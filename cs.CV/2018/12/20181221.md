# Arxiv Papers in cs.CV on 2018-12-21
### Instant Automated Inference of Perceived Mental Stress through Smartphone PPG and Thermal Imaging
- **Arxiv ID**: http://arxiv.org/abs/1901.00449v1
- **DOI**: 10.2196/10140
- **Categories**: **physics.med-ph**, cs.CV, cs.HC, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1901.00449v1)
- **Published**: 2018-12-21 00:49:11+00:00
- **Updated**: 2018-12-21 00:49:11+00:00
- **Authors**: Youngjun Cho, Simon J. Julier, Nadia Bianchi-Berthouze
- **Comment**: Accepted by Journal of Medical Internet Research (JMIR) Mental Health
  - Special Issue on Computing and Mental Health (2018)
- **Journal**: None
- **Summary**: Background: A smartphone is a promising tool for daily cardiovascular measurement and mental stress monitoring. A smartphone camera-based PhotoPlethysmoGraphy (PPG) and a low-cost thermal camera can be used to create cheap, convenient and mobile monitoring systems. However, to ensure reliable monitoring results, a person has to remain still for several minutes while a measurement is being taken. This is very cumbersome and makes its use in real-life mobile situations quite impractical.   Objective: We propose a system which combines PPG and thermography with the aim of improving cardiovascular signal quality and capturing stress responses quickly.   Methods: Using a smartphone camera with a low cost thermal camera added on, we built a novel system which continuously and reliably measures two different types of cardiovascular events: i) blood volume pulse and ii) vasoconstriction/dilation-induced temperature changes of the nose tip. 17 healthy participants, involved in a series of stress-inducing mental workload tasks, measured their physiological responses to stressors over a short window of time (20 seconds) immediately after each task. Participants reported their level of perceived mental stress using a 10-cm Visual Analogue Scale (VAS). We used normalized K-means clustering to reduce interpersonal differences in the self-reported ratings. For the instant stress inference task, we built novel low-level feature sets representing variability of cardiovascular patterns. We then used the automatic feature learning capability of artificial Neural Networks (NN) to improve the mapping between the extracted set of features and the self-reported ratings. We compared our proposed method with existing hand-engineered features-based machine learning methods.   Results, Conclusions: ... due to limited space here, we refer to our manuscript.



### Deep Learning and Glaucoma Specialists: The Relative Importance of Optic Disc Features to Predict Glaucoma Referral in Fundus Photos
- **Arxiv ID**: http://arxiv.org/abs/1812.08911v2
- **DOI**: 10.1016/j.ophtha.2019.07.024
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.08911v2)
- **Published**: 2018-12-21 02:05:29+00:00
- **Updated**: 2019-08-31 00:46:03+00:00
- **Authors**: Sonia Phene, R. Carter Dunn, Naama Hammel, Yun Liu, Jonathan Krause, Naho Kitade, Mike Schaekermann, Rory Sayres, Derek J. Wu, Ashish Bora, Christopher Semturs, Anita Misra, Abigail E. Huang, Arielle Spitze, Felipe A. Medeiros, April Y. Maa, Monica Gandhi, Greg S. Corrado, Lily Peng, Dale R. Webster
- **Comment**: None
- **Journal**: Ophthalmology (2019)
- **Summary**: Glaucoma is the leading cause of preventable, irreversible blindness world-wide. The disease can remain asymptomatic until severe, and an estimated 50%-90% of people with glaucoma remain undiagnosed. Glaucoma screening is recommended for early detection and treatment. A cost-effective tool to detect glaucoma could expand screening access to a much larger patient population, but such a tool is currently unavailable. We trained a deep learning algorithm using a retrospective dataset of 86,618 images, assessed for glaucomatous optic nerve head features and referable glaucomatous optic neuropathy (GON). The algorithm was validated using 3 datasets. For referable GON, the algorithm had an AUC of 0.945 (95% CI, 0.929-0.960) in dataset A (1205 images, 1 image/patient; 18.1% referable), images adjudicated by panels of Glaucoma Specialists (GSs); 0.855 (95% CI, 0.841-0.870) in dataset B (9642 images, 1 image/patient; 9.2% referable), images from Atlanta Veterans Affairs Eye Clinic diabetic teleretinal screening program; and 0.881 (95% CI, 0.838-0.918) in dataset C (346 images, 1 image/patient; 81.7% referable), images from Dr. Shroff's Charity Eye Hospital's glaucoma clinic. The algorithm showed significantly higher sensitivity than 7 of 10 graders not involved in determining the reference standard, including 2 of 3 GSs, and showed higher specificity than 3 graders, while remaining comparable to others. For both GSs and the algorithm, the most crucial features related to referable GON were: presence of vertical cup-to-disc ratio of 0.7 or more, neuroretinal rim notching, retinal nerve fiber layer defect, and bared circumlinear vessels. An algorithm trained on fundus images alone can detect referable GON with higher sensitivity than and comparable specificity to eye care providers. The algorithm maintained good performance on an independent dataset with diagnoses based on a full glaucoma workup.



### Efficient Misalignment-Robust Multi-Focus Microscopical Images Fusion
- **Arxiv ID**: http://arxiv.org/abs/1812.08915v1
- **DOI**: 10.1016/j.sigpro.2019.03.020
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.08915v1)
- **Published**: 2018-12-21 02:17:35+00:00
- **Updated**: 2018-12-21 02:17:35+00:00
- **Authors**: Yixiong Liang, Yuan Mao, Zhihong Tang, Meng Yan, Yuqian Zhao, Jianfeng Liu
- **Comment**: 14 pages,11 figures
- **Journal**: None
- **Summary**: In this paper we propose a very efficient method to fuse the unregistered multi-focus microscopical images based on the speed-up robust features (SURF). Our method follows the pipeline of first registration and then fusion. However, instead of treating the registration and fusion as two completely independent stage, we propose to reuse the determinant of the approximate Hessian generated in SURF detection stage as the corresponding salient response for the final image fusion, thus it enables nearly cost-free saliency map generation. In addition, due to the adoption of SURF scale space representation, our method can generate scale-invariant saliency map which is desired for scale-invariant image fusion. We present an extensive evaluation on the dataset consisting of several groups of unregistered multi-focus 4K ultra HD microscopic images with size of 4112 x 3008. Compared with the state-of-the-art multi-focus image fusion methods, our method is much faster and achieve better results in the visual performance. Our method provides a flexible and efficient way to integrate complementary and redundant information from multiple multi-focus ultra HD unregistered images into a fused image that contains better description than any of the individual input images. Code is available at https://github.com/yiqingmy/JointRF.



### Slimmable Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1812.08928v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1812.08928v1)
- **Published**: 2018-12-21 03:36:48+00:00
- **Updated**: 2018-12-21 03:36:48+00:00
- **Authors**: Jiahui Yu, Linjie Yang, Ning Xu, Jianchao Yang, Thomas Huang
- **Comment**: Accepted in ICLR 2019
- **Journal**: None
- **Summary**: We present a simple and general method to train a single neural network executable at different widths (number of channels in a layer), permitting instant and adaptive accuracy-efficiency trade-offs at runtime. Instead of training individual networks with different width configurations, we train a shared network with switchable batch normalization. At runtime, the network can adjust its width on the fly according to on-device benchmarks and resource constraints, rather than downloading and offloading different models. Our trained networks, named slimmable neural networks, achieve similar (and in many cases better) ImageNet classification accuracy than individually trained models of MobileNet v1, MobileNet v2, ShuffleNet and ResNet-50 at different widths respectively. We also demonstrate better performance of slimmable models compared with individual ones across a wide range of applications including COCO bounding-box object detection, instance segmentation and person keypoint detection without tuning hyper-parameters. Lastly we visualize and discuss the learned features of slimmable networks. Code and models are available at: https://github.com/JiahuiYu/slimmable_networks



### ChamNet: Towards Efficient Network Design through Platform-Aware Model Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1812.08934v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1812.08934v1)
- **Published**: 2018-12-21 03:59:34+00:00
- **Updated**: 2018-12-21 03:59:34+00:00
- **Authors**: Xiaoliang Dai, Peizhao Zhang, Bichen Wu, Hongxu Yin, Fei Sun, Yanghan Wang, Marat Dukhan, Yunqing Hu, Yiming Wu, Yangqing Jia, Peter Vajda, Matt Uyttendaele, Niraj K. Jha
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes an efficient neural network (NN) architecture design methodology called Chameleon that honors given resource constraints. Instead of developing new building blocks or using computationally-intensive reinforcement learning algorithms, our approach leverages existing efficient network building blocks and focuses on exploiting hardware traits and adapting computation resources to fit target latency and/or energy constraints. We formulate platform-aware NN architecture search in an optimization framework and propose a novel algorithm to search for optimal architectures aided by efficient accuracy and resource (latency and/or energy) predictors. At the core of our algorithm lies an accuracy predictor built atop Gaussian Process with Bayesian optimization for iterative sampling. With a one-time building cost for the predictors, our algorithm produces state-of-the-art model architectures on different platforms under given constraints in just minutes. Our results show that adapting computation resources to building blocks is critical to model performance. Without the addition of any bells and whistles, our models achieve significant accuracy improvements against state-of-the-art hand-crafted and automatically designed architectures. We achieve 73.8% and 75.3% top-1 accuracy on ImageNet at 20ms latency on a mobile CPU and DSP. At reduced latency, our models achieve up to 8.5% (4.8%) and 6.6% (9.3%) absolute top-1 accuracy improvements compared to MobileNetV2 and MnasNet, respectively, on a mobile CPU (DSP), and 2.7% (4.6%) and 5.6% (2.6%) accuracy gains over ResNet-101 and ResNet-152, respectively, on an Nvidia GPU (Intel CPU).



### Densely Semantically Aligned Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1812.08967v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.08967v2)
- **Published**: 2018-12-21 06:23:34+00:00
- **Updated**: 2019-04-10 15:01:04+00:00
- **Authors**: Zhizheng Zhang, Cuiling Lan, Wenjun Zeng, Zhibo Chen
- **Comment**: IEEE Conference on Computer Vision and Pattern Recognition (CVPR2019)
- **Journal**: None
- **Summary**: We propose a densely semantically aligned person re-identification framework. It fundamentally addresses the body misalignment problem caused by pose/viewpoint variations, imperfect person detection, occlusion, etc. By leveraging the estimation of the dense semantics of a person image, we construct a set of densely semantically aligned part images (DSAP-images), where the same spatial positions have the same semantics across different images. We design a two-stream network that consists of a main full image stream (MF-Stream) and a densely semantically-aligned guiding stream (DSAG-Stream). The DSAG-Stream, with the DSAP-images as input, acts as a regulator to guide the MF-Stream to learn densely semantically aligned features from the original image. In the inference, the DSAG-Stream is discarded and only the MF-Stream is needed, which makes the inference system computationally efficient and robust. To the best of our knowledge, we are the first to make use of fine grained semantics to address the misalignment problems for re-ID. Our method achieves rank-1 accuracy of 78.9% (new protocol) on the CUHK03 dataset, 90.4% on the CUHK01 dataset, and 95.7% on the Market1501 dataset, outperforming state-of-the-art methods.



### Saliency Guided Hierarchical Robust Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/1812.08973v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1812.08973v1)
- **Published**: 2018-12-21 06:56:56+00:00
- **Updated**: 2018-12-21 06:56:56+00:00
- **Authors**: Fangwen Tu, Shuzhi Sam Ge, Yazhe Tang, Chang Chieh Hang
- **Comment**: None
- **Journal**: None
- **Summary**: A saliency guided hierarchical visual tracking (SHT) algorithm containing global and local search phases is proposed in this paper. In global search, a top-down saliency model is novelly developed to handle abrupt motion and appearance variation problems. Nineteen feature maps are extracted first and combined with online learnt weights to produce the final saliency map and estimated target locations. After the evaluation of integration mechanism, the optimum candidate patch is passed to the local search. In local search, a superpixel based HSV histogram matching is performed jointly with an L2-RLS tracker to take both color distribution and holistic appearance feature of the object into consideration. Furthermore, a linear refinement search process with fast iterative solver is implemented to attenuate the possible negative influence of dominant particles. Both qualitative and quantitative experiments are conducted on a series of challenging image sequences. The superior performance of the proposed method over other state-of-the-art algorithms is demonstrated by comparative study.



### Multi-component Image Translation for Deep Domain Generalization
- **Arxiv ID**: http://arxiv.org/abs/1812.08974v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1812.08974v1)
- **Published**: 2018-12-21 07:03:31+00:00
- **Updated**: 2018-12-21 07:03:31+00:00
- **Authors**: Mohammad Mahfujur Rahman, Clinton Fookes, Mahsa Baktashmotlagh, Sridha Sridharan
- **Comment**: Accepted in WACV 2019
- **Journal**: None
- **Summary**: Domain adaption (DA) and domain generalization (DG) are two closely related methods which are both concerned with the task of assigning labels to an unlabeled data set. The only dissimilarity between these approaches is that DA can access the target data during the training phase, while the target data is totally unseen during the training phase in DG. The task of DG is challenging as we have no earlier knowledge of the target samples. If DA methods are applied directly to DG by a simple exclusion of the target data from training, poor performance will result for a given task. In this paper, we tackle the domain generalization challenge in two ways. In our first approach, we propose a novel deep domain generalization architecture utilizing synthetic data generated by a Generative Adversarial Network (GAN). The discrepancy between the generated images and synthetic images is minimized using existing domain discrepancy metrics such as maximum mean discrepancy or correlation alignment. In our second approach, we introduce a protocol for applying DA methods to a DG scenario by excluding the target data from the training phase, splitting the source data to training and validation parts, and treating the validation data as target data for DA. We conduct extensive experiments on four cross-domain benchmark datasets. Experimental results signify our proposed model outperforms the current state-of-the-art methods for DG.



### A Deep Four-Stream Siamese Convolutional Neural Network with Joint Verification and Identification Loss for Person Re-detection
- **Arxiv ID**: http://arxiv.org/abs/1812.08983v1
- **DOI**: 10.1109/WACV.2018.00146
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.08983v1)
- **Published**: 2018-12-21 07:50:09+00:00
- **Updated**: 2018-12-21 07:50:09+00:00
- **Authors**: Amena Khatun, Simon Denman, Sridha Sridharan, Clinton Fookes
- **Comment**: Published in WACV 2018
- **Journal**: None
- **Summary**: State-of-the-art person re-identification systems that employ a triplet based deep network suffer from a poor generalization capability. In this paper, we propose a four stream Siamese deep convolutional neural network for person redetection that jointly optimises verification and identification losses over a four image input group. Specifically, the proposed method overcomes the weakness of the typical triplet formulation by using groups of four images featuring two matched (i.e. the same identity) and two mismatched images. This allows us to jointly increase the interclass variations and reduce the intra-class variations in the learned feature space. The proposed approach also optimises over both the identification and verification losses, further minimising intra-class variation and maximising inter-class variation, improving overall performance. Extensive experiments on four challenging datasets, VIPeR, CUHK01, CUHK03 and PRID2011, demonstrates that the proposed approach achieves state-of-the-art performance.



### Non-Adversarial Image Synthesis with Generative Latent Nearest Neighbors
- **Arxiv ID**: http://arxiv.org/abs/1812.08985v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.08985v1)
- **Published**: 2018-12-21 07:54:02+00:00
- **Updated**: 2018-12-21 07:54:02+00:00
- **Authors**: Yedid Hoshen, Jitendra Malik
- **Comment**: None
- **Journal**: None
- **Summary**: Unconditional image generation has recently been dominated by generative adversarial networks (GANs). GAN methods train a generator which regresses images from random noise vectors, as well as a discriminator that attempts to differentiate between the generated images and a training set of real images. GANs have shown amazing results at generating realistic looking images. Despite their success, GANs suffer from critical drawbacks including: unstable training and mode-dropping. The weaknesses in GANs have motivated research into alternatives including: variational auto-encoders (VAEs), latent embedding learning methods (e.g. GLO) and nearest-neighbor based implicit maximum likelihood estimation (IMLE). Unfortunately at the moment, GANs still significantly outperform the alternative methods for image generation. In this work, we present a novel method - Generative Latent Nearest Neighbors (GLANN) - for training generative models without adversarial training. GLANN combines the strengths of IMLE and GLO in a way that overcomes the main drawbacks of each method. Consequently, GLANN generates images that are far better than GLO and IMLE. Our method does not suffer from mode collapse which plagues GAN training and is much more stable. Qualitative results show that GLANN outperforms a baseline consisting of 800 GANs and VAEs on commonly used datasets. Our models are also shown to be effective for training truly non-adversarial unsupervised image translation.



### Face Hallucination Revisited: An Exploratory Study on Dataset Bias
- **Arxiv ID**: http://arxiv.org/abs/1812.09010v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.09010v1)
- **Published**: 2018-12-21 09:25:55+00:00
- **Updated**: 2018-12-21 09:25:55+00:00
- **Authors**: Klemen Grm, Martin Pernuš, Leo Cluzel, Walter Scheirer, Simon Dobrišek, Vitomir Štruc
- **Comment**: None
- **Journal**: None
- **Summary**: Contemporary face hallucination (FH) models exhibit considerable ability to reconstruct high-resolution (HR) details from low-resolution (LR) face images. This ability is commonly learned from examples of corresponding HR-LR image pairs, created by artificially down-sampling the HR ground truth data. This down-sampling (or degradation) procedure not only defines the characteristics of the LR training data, but also determines the type of image degradations the learned FH models are eventually able to handle. If the image characteristics encountered with real-world LR images differ from the ones seen during training, FH models are still expected to perform well, but in practice may not produce the desired results. In this paper we study this problem and explore the bias introduced into FH models by the characteristics of the training data. We systematically analyze the generalization capabilities of several FH models in various scenarios, where the image the degradation function does not match the training setup and conduct experiments with synthetically downgraded as well as real-life low-quality images. We make several interesting findings that provide insight into existing problems with FH models and point to future research directions.



### Detection of distal radius fractures trained by a small set of X-ray images and Faster R-CNN
- **Arxiv ID**: http://arxiv.org/abs/1812.09025v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.09025v1)
- **Published**: 2018-12-21 10:13:14+00:00
- **Updated**: 2018-12-21 10:13:14+00:00
- **Authors**: Erez Yahalomi, Michael Chernofsky, Michael Werman
- **Comment**: None
- **Journal**: Computing Conference 2019
- **Summary**: Distal radius fractures are the most common fractures of the upper extremity in humans. As such, they account for a significant portion of the injuries that present to emergency rooms and clinics throughout the world. We trained a Faster R-CNN, a machine vision neural network for object detection, to identify and locate distal radius fractures in anteroposterior X-ray images. We achieved an accuracy of 96\% in identifying fractures and mean Average Precision, mAP, of 0.866. This is significantly more accurate than the detection achieved by physicians and radiologists. These results were obtained by training the deep learning network with only 38 original images of anteroposterior hands X-ray images with fractures. This opens the possibility to detect with this type of neural network rare diseases or rare symptoms of common diseases , where only a small set of diagnosed X-ray images could be collected for each disease.



### A Multi-task Neural Approach for Emotion Attribution, Classification and Summarization
- **Arxiv ID**: http://arxiv.org/abs/1812.09041v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.09041v2)
- **Published**: 2018-12-21 10:53:44+00:00
- **Updated**: 2019-07-24 04:21:23+00:00
- **Authors**: Guoyun Tu, Yanwei Fu, Boyang Li, Jiarui Gao, Yu-Gang Jiang, Xiangyang Xue
- **Comment**: Authors' manuscript; published at the IEEE Transactions on Multimedia
- **Journal**: None
- **Summary**: Emotional content is a crucial ingredient in user-generated videos. However, the sparsity of emotional expressions in the videos poses an obstacle to visual emotion analysis. In this paper, we propose a new neural approach, Bi-stream Emotion Attribution-Classification Network (BEAC-Net), to solve three related emotion analysis tasks: emotion recognition, emotion attribution, and emotion-oriented summarization, in a single integrated framework. BEAC-Net has two major constituents, an attribution network and a classification network. The attribution network extracts the main emotional segment that classification should focus on in order to mitigate the sparsity issue. The classification network utilizes both the extracted segment and the original video in a bi-stream architecture. We contribute a new dataset for the emotion attribution task with human-annotated ground-truth labels for emotion segments. Experiments on two video datasets demonstrate superior performance of the proposed framework and the complementary nature of the dual classification streams.



### 3D multirater RCNN for multimodal multiclass detection and characterisation of extremely small objects
- **Arxiv ID**: http://arxiv.org/abs/1812.09046v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.09046v1)
- **Published**: 2018-12-21 11:03:14+00:00
- **Updated**: 2018-12-21 11:03:14+00:00
- **Authors**: Carole H. Sudre, Beatriz Gomez Anson, Silvia Ingala, Chris D. Lane, Daniel Jimenez, Lukas Haider, Thomas Varsavsky, Lorna Smith, H. Rolf Jäger, M. Jorge Cardoso
- **Comment**: None
- **Journal**: None
- **Summary**: Extremely small objects (ESO) have become observable on clinical routine magnetic resonance imaging acquisitions, thanks to a reduction in acquisition time at higher resolution. Despite their small size (usually $<$10 voxels per object for an image of more than $10^6$ voxels), these markers reflect tissue damage and need to be accounted for to investigate the complete phenotype of complex pathological pathways. In addition to their very small size, variability in shape and appearance leads to high labelling variability across human raters, resulting in a very noisy gold standard. Such objects are notably present in the context of cerebral small vessel disease where enlarged perivascular spaces and lacunes, commonly observed in the ageing population, are thought to be associated with acceleration of cognitive decline and risk of dementia onset. In this work, we redesign the RCNN model to scale to 3D data, and to jointly detect and characterise these important markers of age-related neurovascular changes. We also propose training strategies enforcing the detection of extremely small objects, ensuring a tractable and stable training process.



### 3DSRnet: Video Super-resolution using 3D Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1812.09079v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.09079v2)
- **Published**: 2018-12-21 12:31:42+00:00
- **Updated**: 2019-06-20 07:18:50+00:00
- **Authors**: Soo Ye Kim, Jeongyeon Lim, Taeyoung Na, Munchurl Kim
- **Comment**: Extension of our paper accepted at ICIP 2019
- **Journal**: None
- **Summary**: In video super-resolution, the spatio-temporal coherence between, and among the frames must be exploited appropriately for accurate prediction of the high resolution frames. Although 2D convolutional neural networks (CNNs) are powerful in modelling images, 3D-CNNs are more suitable for spatio-temporal feature extraction as they can preserve temporal information. To this end, we propose an effective 3D-CNN for video super-resolution, called the 3DSRnet that does not require motion alignment as preprocessing. Our 3DSRnet maintains the temporal depth of spatio-temporal feature maps to maximally capture the temporally nonlinear characteristics between low and high resolution frames, and adopts residual learning in conjunction with the sub-pixel outputs. It outperforms the most state-of-the-art method with average 0.45 and 0.36 dB higher in PSNR for scales 3 and 4, respectively, in the Vidset4 benchmark. Our 3DSRnet first deals with the performance drop due to scene change, which is important in practice but has not been previously considered.



### Generative Models from the perspective of Continual Learning
- **Arxiv ID**: http://arxiv.org/abs/1812.09111v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1812.09111v1)
- **Published**: 2018-12-21 13:35:32+00:00
- **Updated**: 2018-12-21 13:35:32+00:00
- **Authors**: Timothée Lesort, Hugo Caselles-Dupré, Michael Garcia-Ortiz, Andrei Stoian, David Filliat
- **Comment**: None
- **Journal**: None
- **Summary**: Which generative model is the most suitable for Continual Learning? This paper aims at evaluating and comparing generative models on disjoint sequential image generation tasks. We investigate how several models learn and forget, considering various strategies: rehearsal, regularization, generative replay and fine-tuning. We used two quantitative metrics to estimate the generation quality and memory ability. We experiment with sequential tasks on three commonly used benchmarks for Continual Learning (MNIST, Fashion MNIST and CIFAR10). We found that among all models, the original GAN performs best and among Continual Learning strategies, generative replay outperforms all other methods. Even if we found satisfactory combinations on MNIST and Fashion MNIST, training generative models sequentially on CIFAR10 is particularly instable, and remains a challenge. Our code is available online \footnote{\url{https://github.com/TLESORT/Generative\_Continual\_Learning}}.



### Cascaded Coarse-to-Fine Deep Kernel Networks for Efficient Satellite Image Change Detection
- **Arxiv ID**: http://arxiv.org/abs/1812.09119v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.09119v1)
- **Published**: 2018-12-21 13:52:17+00:00
- **Updated**: 2018-12-21 13:52:17+00:00
- **Authors**: Hichem Sahbi
- **Comment**: None
- **Journal**: None
- **Summary**: Deep networks are nowadays becoming popular in many computer vision and pattern recognition tasks. Among these networks, deep kernels are particularly interesting and effective, however, their computational complexity is a major issue especially on cheap hardware resources. In this paper, we address the issue of efficient computation in deep kernel networks. We propose a novel framework that reduces dramatically the complexity of evaluating these deep kernels. Our method is based on a coarse-to-fine cascade of networks designed for efficient computation; early stages of the cascade are cheap and reject many patterns efficiently while deep stages are more expensive and accurate. The design principle of these reduced complexity networks is based on a variant of the cross-entropy criterion that reduces the complexity of the networks in the cascade while preserving all the positive responses of the original kernel network. Experiments conducted - on the challenging and time demanding change detection task, on very large satellite images - show that our proposed coarse-to-fine approach is effective and highly efficient.



### A Multiscale Image Denoising Algorithm Based On Dilated Residual Convolution Network
- **Arxiv ID**: http://arxiv.org/abs/1812.09131v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1812.09131v1)
- **Published**: 2018-12-21 14:13:07+00:00
- **Updated**: 2018-12-21 14:13:07+00:00
- **Authors**: Chang Liu, Zhaowei Shang, Anyong Qin
- **Comment**: None
- **Journal**: None
- **Summary**: Image denoising is a classical problem in low level computer vision. Model-based optimization methods and deep learning approaches have been the two main strategies for solving the problem. Model-based optimization methods are flexible for handling different inverse problems but are usually time-consuming. In contrast, deep learning methods have fast testing speed but the performance of these CNNs is still inferior. To address this issue, here we propose a novel deep residual learning model that combines the dilated residual convolution and multi-scale convolution groups. Due to the complex patterns and structures of inside an image, the multiscale convolution group is utilized to learn those patterns and enlarge the receptive field. Specifically, the residual connection and batch normalization are utilized to speed up the training process and maintain the denoising performance. In order to decrease the gridding artifacts, we integrate the hybrid dilated convolution design into our model. To this end, this paper aims to train a lightweight and effective denoiser based on multiscale convolution group. Experimental results have demonstrated that the enhanced denoiser can not only achieve promising denoising results, but also become a strong competitor in practical application.



### Quicker ADC : Unlocking the hidden potential of Product Quantization with SIMD
- **Arxiv ID**: http://arxiv.org/abs/1812.09162v2
- **DOI**: 10.1109/TPAMI.2019.2952606
- **Categories**: **cs.CV**, cs.AI, cs.DB, cs.PF
- **Links**: [PDF](http://arxiv.org/pdf/1812.09162v2)
- **Published**: 2018-12-21 14:51:27+00:00
- **Updated**: 2019-11-14 21:39:48+00:00
- **Authors**: Fabien André, Anne-Marie Kermarrec, Nicolas Le Scouarnec
- **Comment**: Open-source implementation at
  http://github.com/nlescoua/faiss-quickeradc
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence,
  2019 Early Access
- **Summary**: Efficient Nearest Neighbor (NN) search in high-dimensional spaces is a foundation of many multimedia retrieval systems. A common approach is to rely on Product Quantization, which allows the storage of large vector databases in memory and efficient distance computations. Yet, implementations of nearest neighbor search with Product Quantization have their performance limited by the many memory accesses they perform. Following this observation, Andr\'e et al. proposed Quick ADC with up to $6\times$ faster implementations of $m\times{}4$ product quantizers (PQ) leveraging specific SIMD instructions. Quicker ADC is a generalization of Quick ADC not limited to $m\times{}4$ codes and supporting AVX-512, the latest revision of SIMD instruction set. In doing so, Quicker ADC faces the challenge of using efficiently 5,6 and 7-bit shuffles that do not align to computer bytes or words. To this end, we introduce (i) irregular product quantizers combining sub-quantizers of different granularity and (ii) split tables allowing lookup tables larger than registers. We evaluate Quicker ADC with multiple indexes including Inverted Multi-Indexes and IVF HNSW and show that it outperforms the reference optimized implementations (i.e., FAISS and polysemous codes) for numerous configurations. Finally, we release an open-source fork of FAISS enhanced with Quicker ADC at http://github.com/nlescoua/faiss-quickeradc.



### Learning Compositional Representations for Few-Shot Recognition
- **Arxiv ID**: http://arxiv.org/abs/1812.09213v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.09213v3)
- **Published**: 2018-12-21 15:58:02+00:00
- **Updated**: 2019-08-17 14:38:02+00:00
- **Authors**: Pavel Tokmakov, Yu-Xiong Wang, Martial Hebert
- **Comment**: None
- **Journal**: None
- **Summary**: One of the key limitations of modern deep learning approaches lies in the amount of data required to train them. Humans, by contrast, can learn to recognize novel categories from just a few examples. Instrumental to this rapid learning ability is the compositional structure of concept representations in the human brain --- something that deep learning models are lacking. In this work, we make a step towards bridging this gap between human and machine learning by introducing a simple regularization technique that allows the learned representation to be decomposable into parts. Our method uses category-level attribute annotations to disentangle the feature space of a network into subspaces corresponding to the attributes. These attributes can be either purely visual, like object parts, or more abstract, like openness and symmetry. We demonstrate the value of compositional representations on three datasets: CUB-200-2011, SUN397, and ImageNet, and show that they require fewer examples to learn classifiers for novel categories.



### Learning from Web Data: the Benefit of Unsupervised Object Localization
- **Arxiv ID**: http://arxiv.org/abs/1812.09232v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.09232v1)
- **Published**: 2018-12-21 16:25:20+00:00
- **Updated**: 2018-12-21 16:25:20+00:00
- **Authors**: Xiaoxiao Sun, Liang Zheng, Yu-Kun Lai, Jufeng Yang
- **Comment**: 13 pages, 9 figures, 6 tables
- **Journal**: None
- **Summary**: Annotating a large number of training images is very time-consuming. In this background, this paper focuses on learning from easy-to-acquire web data and utilizes the learned model for fine-grained image classification in labeled datasets. Currently, the performance gain from training with web data is incremental, like a common saying "better than nothing, but not by much". Conventionally, the community looks to correcting the noisy web labels to select informative samples. In this work, we first systematically study the built-in gap between the web and standard datasets, i.e. different data distributions between the two kinds of data. Then, in addition to using web labels, we present an unsupervised object localization method, which provides critical insights into the object density and scale in web images. Specifically, we design two constraints on web data to substantially reduce the difference of data distributions for the web and standard datasets. First, we present a method to control the scale, localization and number of objects in the detected region. Second, we propose to select the regions containing objects that are consistent with the web tag. Based on the two constraints, we are able to process web images to reduce the gap, and the processed web data is used to better assist the standard dataset to train CNNs. Experiments on several fine-grained image classification datasets confirm that our method performs favorably against the state-of-the-art methods.



### Polygonal approximation of digital planar curve using novel significant measure
- **Arxiv ID**: http://arxiv.org/abs/1812.09271v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.09271v1)
- **Published**: 2018-12-21 17:25:58+00:00
- **Updated**: 2018-12-21 17:25:58+00:00
- **Authors**: Mangayarkarasi Ramaiah, Dilip K. Prasad
- **Comment**: 17 pages,15 figures
- **Journal**: None
- **Summary**: This paper presents an iterative smoothing technique for polygonal approximation of digital image boundary. The technique starts with finest initial segmentation points of a curve. The contribution of initially segmented points towards preserving the original shape of the image boundary is determined by computing the significant measure of every initial segmentation points which is sensitive to sharp turns, which may be missed easily when conventional significant measures are used for detecting dominant points. The proposed method differentiates between the situations when a point on the curve between two points on a curve projects directly upon the line segment or beyond this line segment. It not only identifies these situations, but also computes its significant contribution for these situations differently. This situation-specific treatment allows preservation of points with high curvature even as revised set of dominant points are derived. The experimental results show that the proposed technique competes well with the state of the art techniques.



### Multimodal Sensor Fusion In Single Thermal image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1812.09276v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.09276v1)
- **Published**: 2018-12-21 17:36:35+00:00
- **Updated**: 2018-12-21 17:36:35+00:00
- **Authors**: Feras Almasri, Olivier Debeir
- **Comment**: None
- **Journal**: None
- **Summary**: With the fast growth in the visual surveillance and security sectors, thermal infrared images have become increasingly necessary ina large variety of industrial applications. This is true even though IR sensors are still more expensive than their RGB counterpart having the same resolution. In this paper, we propose a deep learning solution to enhance the thermal image resolution. The following results are given:(I) Introduction of a multimodal, visual-thermal fusion model that ad-dresses thermal image super-resolution, via integrating high-frequency information from the visual image. (II) Investigation of different net-work architecture schemes in the literature, their up-sampling methods,learning procedures, and their optimization functions by showing their beneficial contribution to the super-resolution problem. (III) A bench-mark ULB17-VT dataset that contains thermal images and their visual images counterpart is presented. (IV) Presentation of a qualitative evaluation of a large test set with 58 samples and 22 raters which shows that our proposed model performs better against state-of-the-arts.



### Canonical Correlation Analysis for Misaligned Satellite Image Change Detection
- **Arxiv ID**: http://arxiv.org/abs/1812.09280v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.09280v1)
- **Published**: 2018-12-21 17:43:16+00:00
- **Updated**: 2018-12-21 17:43:16+00:00
- **Authors**: Hichem Sahbi
- **Comment**: None
- **Journal**: None
- **Summary**: Canonical correlation analysis (CCA) is a statistical learning method that seeks to build view-independent latent representations from multi-view data. This method has been successfully applied to several pattern analysis tasks such as image-to-text mapping and view-invariant object/action recognition. However, this success is highly dependent on the quality of data pairing (i.e., alignments) and mispairing adversely affects the generalization ability of the learned CCA representations. In this paper, we address the issue of alignment errors using a new variant of canonical correlation analysis referred to as alignment-agnostic (AA) CCA. Starting from erroneously paired data taken from different views, this CCA finds transformation matrices by optimizing a constrained maximization problem that mixes a data correlation term with context regularization; the particular design of these two terms mitigates the effect of alignment errors when learning the CCA transformations. Experiments conducted on multi-view tasks, including multi-temporal satellite image change detection, show that our AA CCA method is highly effective and resilient to mispairing errors.



### Cluster Loss for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1812.10325v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.10325v1)
- **Published**: 2018-12-21 18:14:19+00:00
- **Updated**: 2018-12-21 18:14:19+00:00
- **Authors**: Doney Alex, Zishan Sami, Sumandeep Banerjee, Subrat Panda
- **Comment**: Published in 11th Indian Conference on Computer Vision, Graphics and
  Image Processing (ICVGIP 2018). arXiv admin note: text overlap with
  arXiv:1610.02984, arXiv:1703.07737, arXiv:1704.01719, arXiv:1511.09123 by
  other authors
- **Journal**: None
- **Summary**: Person re-identification (ReID) is an important problem in computer vision, especially for video surveillance applications. The problem focuses on identifying people across different cameras or across different frames of the same camera. The main challenge lies in identifying the similarity of the same person against large appearance and structure variations, while differentiating between individuals. Recently, deep learning networks with triplet loss have become a common framework for person ReID. However, triplet loss focuses on obtaining correct orders on the training set. We demonstrate that it performs inferior in a clustering task. In this paper, we design a cluster loss, which can lead to the model output with a larger inter-class variation and a smaller intra-class variation compared to the triplet loss. As a result, our model has a better generalization ability and can achieve higher accuracy on the test set especially for a clustering task. We also introduce a batch hard training mechanism for improving the results and faster convergence of training.



### An Empirical Analysis of Deep Audio-Visual Models for Speech Recognition
- **Arxiv ID**: http://arxiv.org/abs/1812.09336v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1812.09336v1)
- **Published**: 2018-12-21 19:02:52+00:00
- **Updated**: 2018-12-21 19:02:52+00:00
- **Authors**: Devesh Walawalkar, Yihui He, Rohit Pillai
- **Comment**: None
- **Journal**: None
- **Summary**: In this project, we worked on speech recognition, specifically predicting individual words based on both the video frames and audio. Empowered by convolutional neural networks, the recent speech recognition and lip reading models are comparable to human level performance. We re-implemented and made derivations of the state-of-the-art model. Then, we conducted rich experiments including the effectiveness of attention mechanism, more accurate residual network as the backbone with pre-trained weights and the sensitivity of our model with respect to audio input with/without noise.



### Sparse One-Time Grab Sampling of Inliers
- **Arxiv ID**: http://arxiv.org/abs/1901.02338v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1901.02338v1)
- **Published**: 2018-12-21 20:25:33+00:00
- **Updated**: 2018-12-21 20:25:33+00:00
- **Authors**: Maryam Jaberi, Marianna Pensky, Hassan Foroosh
- **Comment**: WiML2017
- **Journal**: None
- **Summary**: Estimating structures in "big data" and clustering them are among the most fundamental problems in computer vision, pattern recognition, data mining, and many other other research fields. Over the past few decades, many studies have been conducted focusing on different aspects of these problems. One of the main approaches that is explored in the literature to tackle the problems of size and dimensionality is sampling subsets of the data in order to estimate the characteristics of the whole population, e.g. estimating the underlying clusters or structures in the data. In this paper, we propose a `one-time-grab' sampling algorithm\cite{jaberi2015swift,jaberi2018sparse}. This method can be used as the front end to any supervised or unsupervised clustering method. Rather than focusing on the strategy of maximizing the probability of sampling inliers, our goal is to minimize the number of samples needed to instantiate all underlying model instances. More specifically, our goal is to answer the following question: {\em `Given a very large population of points with $C$ embedded structures and gross outliers, what is the minimum number of points $r$ to be selected randomly in one grab in order to make sure with probability $P$ that at least $\varepsilon$ points are selected on each structure, where $\varepsilon$ is the number of degrees of freedom of each structure.'}



### Wireless Software Synchronization of Multiple Distributed Cameras
- **Arxiv ID**: http://arxiv.org/abs/1812.09366v2
- **DOI**: 10.1109/ICCPHOT.2019.8747340
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.09366v2)
- **Published**: 2018-12-21 20:48:11+00:00
- **Updated**: 2019-06-11 21:21:24+00:00
- **Authors**: Sameer Ansari, Neal Wadhwa, Rahul Garg, Jiawen Chen
- **Comment**: Main: 9 pages, 10 figures. Supplemental: 3 pages, 5 figures
- **Journal**: None
- **Summary**: We present a method for precisely time-synchronizing the capture of image sequences from a collection of smartphone cameras connected over WiFi. Our method is entirely software-based, has only modest hardware requirements, and achieves an accuracy of less than 250 microseconds on unmodified commodity hardware. It does not use image content and synchronizes cameras prior to capture. The algorithm operates in two stages. In the first stage, we designate one device as the leader and synchronize each client device's clock to it by estimating network delay. Once clocks are synchronized, the second stage initiates continuous image streaming, estimates the relative phase of image timestamps between each client and the leader, and shifts the streams into alignment. We quantitatively validate our results on a multi-camera rig imaging a high-precision LED array and qualitatively demonstrate significant improvements to multi-view stereo depth estimation and stitching of dynamic scenes. We release as open source 'libsoftwaresync', an Android implementation of our system, to inspire new types of collective capture applications.



### Multi-Frame Super-Resolution Reconstruction with Applications to Medical Imaging
- **Arxiv ID**: http://arxiv.org/abs/1812.09375v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.09375v1)
- **Published**: 2018-12-21 21:21:11+00:00
- **Updated**: 2018-12-21 21:21:11+00:00
- **Authors**: Thomas Köhler
- **Comment**: Ph.D. thesis at the Friedrich-Alexander-Universit\"at (FAU)
  Erlangen-N\"urnberg; source code is available at
  https://www5.cs.fau.de/de/forschung/software/multi-frame-super-resolution-toolbox/
  . https://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/9145
- **Journal**: None
- **Summary**: The optical resolution of a digital camera is one of its most crucial parameters with broad relevance for consumer electronics, surveillance systems, remote sensing, or medical imaging. However, resolution is physically limited by the optics and sensor characteristics. In addition, practical and economic reasons often stipulate the use of out-dated or low-cost hardware. Super-resolution is a class of retrospective techniques that aims at high-resolution imagery by means of software. Multi-frame algorithms approach this task by fusing multiple low-resolution frames to reconstruct high-resolution images. This work covers novel super-resolution methods along with new applications in medical imaging.



### Multi-Step Prediction of Occupancy Grid Maps with Recurrent Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1812.09395v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.09395v3)
- **Published**: 2018-12-21 22:27:10+00:00
- **Updated**: 2019-01-22 20:29:59+00:00
- **Authors**: Nima Mohajerin, Mohsen Rohani
- **Comment**: None
- **Journal**: None
- **Summary**: We investigate the multi-step prediction of the drivable space, represented by Occupancy Grid Maps (OGMs), for autonomous vehicles. Our motivation is that accurate multi-step prediction of the drivable space can efficiently improve path planning and navigation resulting in safe, comfortable and optimum paths in autonomous driving. We train a variety of Recurrent Neural Network (RNN) based architectures on the OGM sequences from the KITTI dataset. The results demonstrate significant improvement of the prediction accuracy using our proposed difference learning method, incorporating motion related features, over the state of the art. We remove the egomotion from the OGM sequences by transforming them into a common frame. Although in the transformed sequences the KITTI dataset is heavily biased toward static objects, by learning the difference between subsequent OGMs, our proposed method provides accurate prediction over both the static and moving objects.



### Residual Attention based Network for Hand Bone Age Assessment
- **Arxiv ID**: http://arxiv.org/abs/1901.05876v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1901.05876v1)
- **Published**: 2018-12-21 23:09:32+00:00
- **Updated**: 2018-12-21 23:09:32+00:00
- **Authors**: Eric Wu, Bin Kong, Xin Wang, Junjie Bai, Yi Lu, Feng Gao, Shaoting Zhang, Kunlin Cao, Qi Song, Siwei Lyu, Youbing Yin
- **Comment**: None
- **Journal**: None
- **Summary**: Computerized automatic methods have been employed to boost the productivity as well as objectiveness of hand bone age assessment. These approaches make predictions according to the whole X-ray images, which include other objects that may introduce distractions. Instead, our framework is inspired by the clinical workflow (Tanner-Whitehouse) of hand bone age assessment, which focuses on the key components of the hand. The proposed framework is composed of two components: a Mask R-CNN subnet of pixelwise hand segmentation and a residual attention network for hand bone age assessment. The Mask R-CNN subnet segments the hands from X-ray images to avoid the distractions of other objects (e.g., X-ray tags). The hierarchical attention components of the residual attention subnet force our network to focus on the key components of the X-ray images and generate the final predictions as well as the associated visual supports, which is similar to the assessment procedure of clinicians. We evaluate the performance of the proposed pipeline on the RSNA pediatric bone age dataset and the results demonstrate its superiority over the previous methods.



