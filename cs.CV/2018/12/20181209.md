# Arxiv Papers in cs.CV on 2018-12-09
### Beyond Domain Adaptation: Unseen Domain Encapsulation via Universal Non-volume Preserving Models
- **Arxiv ID**: http://arxiv.org/abs/1812.03407v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.03407v1)
- **Published**: 2018-12-09 00:10:38+00:00
- **Updated**: 2018-12-09 00:10:38+00:00
- **Authors**: Thanh-Dat Truong, Chi Nhan Duong, Khoa Luu, Minh-Triet Tran, Minh Do
- **Comment**: None
- **Journal**: None
- **Summary**: Recognition across domains has recently become an active topic in the research community. However, it has been largely overlooked in the problem of recognition in new unseen domains. Under this condition, the delivered deep network models are unable to be updated, adapted or fine-tuned. Therefore, recent deep learning techniques, such as: domain adaptation, feature transferring, and fine-tuning, cannot be applied. This paper presents a novel Universal Non-volume Preserving approach to the problem of domain generalization in the context of deep learning. The proposed method can be easily incorporated with any other ConvNet framework within an end-to-end deep network design to improve the performance. On digit recognition, we benchmark on four popular digit recognition databases, i.e. MNIST, USPS, SVHN and MNIST-M. The proposed method is also experimented on face recognition on Extended Yale-B, CMU-PIE and CMU-MPIE databases and compared against other the state-of-the-art methods. In the problem of pedestrian detection, we empirically observe that the proposed method learns models that improve performance across a priori unknown data distributions.



### Feature Denoising for Improving Adversarial Robustness
- **Arxiv ID**: http://arxiv.org/abs/1812.03411v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.03411v2)
- **Published**: 2018-12-09 01:55:31+00:00
- **Updated**: 2019-03-25 17:44:19+00:00
- **Authors**: Cihang Xie, Yuxin Wu, Laurens van der Maaten, Alan Yuille, Kaiming He
- **Comment**: CVPR 2019, code is available at:
  https://github.com/facebookresearch/ImageNet-Adversarial-Training
- **Journal**: None
- **Summary**: Adversarial attacks to image classification systems present challenges to convolutional networks and opportunities for understanding them. This study suggests that adversarial perturbations on images lead to noise in the features constructed by these networks. Motivated by this observation, we develop new network architectures that increase adversarial robustness by performing feature denoising. Specifically, our networks contain blocks that denoise the features using non-local means or other filters; the entire networks are trained end-to-end. When combined with adversarial training, our feature denoising networks substantially improve the state-of-the-art in adversarial robustness in both white-box and black-box attack settings. On ImageNet, under 10-iteration PGD white-box attacks where prior art has 27.9% accuracy, our method achieves 55.7%; even under extreme 2000-iteration PGD white-box attacks, our method secures 42.6% accuracy. Our method was ranked first in Competition on Adversarial Attacks and Defenses (CAAD) 2018 --- it achieved 50.6% classification accuracy on a secret, ImageNet-like test dataset against 48 unknown attackers, surpassing the runner-up approach by ~10%. Code is available at https://github.com/facebookresearch/ImageNet-Adversarial-Training.



### Learning Transferable Adversarial Examples via Ghost Networks
- **Arxiv ID**: http://arxiv.org/abs/1812.03413v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.03413v3)
- **Published**: 2018-12-09 02:11:03+00:00
- **Updated**: 2019-11-25 15:34:55+00:00
- **Authors**: Yingwei Li, Song Bai, Yuyin Zhou, Cihang Xie, Zhishuai Zhang, Alan Yuille
- **Comment**: To appear in AAAI-20
- **Journal**: None
- **Summary**: Recent development of adversarial attacks has proven that ensemble-based methods outperform traditional, non-ensemble ones in black-box attack. However, as it is computationally prohibitive to acquire a family of diverse models, these methods achieve inferior performance constrained by the limited number of models to be ensembled.   In this paper, we propose Ghost Networks to improve the transferability of adversarial examples. The critical principle of ghost networks is to apply feature-level perturbations to an existing model to potentially create a huge set of diverse models. After that, models are subsequently fused by longitudinal ensemble. Extensive experimental results suggest that the number of networks is essential for improving the transferability of adversarial examples, but it is less necessary to independently train different networks and ensemble them in an intensive aggregation way. Instead, our work can be used as a computationally cheap and easily applied plug-in to improve adversarial approaches both in single-model and multi-model attack, compatible with residual and non-residual networks. By reproducing the NeurIPS 2017 adversarial competition, our method outperforms the No.1 attack submission by a large margin, demonstrating its effectiveness and efficiency. Code is available at https://github.com/LiYingwei/ghost-network.



### Real-Time Referring Expression Comprehension by Single-Stage Grounding Network
- **Arxiv ID**: http://arxiv.org/abs/1812.03426v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.03426v1)
- **Published**: 2018-12-09 04:30:11+00:00
- **Updated**: 2018-12-09 04:30:11+00:00
- **Authors**: Xinpeng Chen, Lin Ma, Jingyuan Chen, Zequn Jie, Wei Liu, Jiebo Luo
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel end-to-end model, namely Single-Stage Grounding network (SSG), to localize the referent given a referring expression within an image. Different from previous multi-stage models which rely on object proposals or detected regions, our proposed model aims to comprehend a referring expression through one single stage without resorting to region proposals as well as the subsequent region-wise feature extraction. Specifically, a multimodal interactor is proposed to summarize the local region features regarding the referring expression attentively. Subsequently, a grounder is proposed to localize the referring expression within the given image directly. For further improving the localization accuracy, a guided attention mechanism is proposed to enforce the grounder to focus on the central region of the referent. Moreover, by exploiting and predicting visual attribute information, the grounder can further distinguish the referent objects within an image and thereby improve the model performance. Experiments on RefCOCO, RefCOCO+, and RefCOCOg datasets demonstrate that our proposed SSG without relying on any region proposals can achieve comparable performance with other advanced models. Furthermore, our SSG outperforms the previous models and achieves the state-of-art performance on the ReferItGame dataset. More importantly, our SSG is time efficient and can ground a referring expression in a 416*416 image from the RefCOCO dataset in 25ms (40 referents per second) on average with a Nvidia Tesla P40, accomplishing more than 9* speedups over the existing multi-stage models.



### Application of Deep Learning in Fundus Image Processing for Ophthalmic Diagnosis -- A Review
- **Arxiv ID**: http://arxiv.org/abs/1812.07101v3
- **DOI**: 10.1016/j.artmed.2019.101758
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.07101v3)
- **Published**: 2018-12-09 05:57:17+00:00
- **Updated**: 2019-07-05 19:57:45+00:00
- **Authors**: Sourya Sengupta, Amitojdeep Singh, Henry A. Leopold, Tanmay Gulati, Vasudevan Lakshminarayanan
- **Comment**: None
- **Journal**: Artificial Intelligence in Medicine Volume 102, January 2020,
  101758
- **Summary**: An overview of the applications of deep learning in ophthalmic diagnosis using retinal fundus images is presented. We also review various retinal image datasets that can be used for deep learning purposes. Applications of deep learning for segmentation of optic disk, blood vessels and retinal layer as well as detection of lesions are reviewed. Recent deep learning models for classification of diseases such as age-related macular degeneration, glaucoma,diabetic macular edema and diabetic retinopathy are also reported.



### Area-preserving mapping of 3D ultrasound carotid artery images using density-equalizing reference map
- **Arxiv ID**: http://arxiv.org/abs/1812.03434v1
- **DOI**: 10.1109/TBME.2019.2963783
- **Categories**: **cs.CG**, cs.CV, math.NA, physics.med-ph, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1812.03434v1)
- **Published**: 2018-12-09 06:35:15+00:00
- **Updated**: 2018-12-09 06:35:15+00:00
- **Authors**: Gary P. T. Choi, Bernard Chiu, Chris H. Rycroft
- **Comment**: None
- **Journal**: IEEE Transactions on Biomedical Engineering, 67(9), 1507-1517
  (2020)
- **Summary**: Carotid atherosclerosis is a focal disease at the bifurcations of the carotid artery. To quantitatively monitor the local changes in the vessel-wall-plus-plaque thickness (VWT) and compare the VWT distributions for different patients or for the same patients at different ultrasound scanning sessions, a mapping technique is required to adjust for the geometric variability of different carotid artery models. In this work, we propose a novel method called density-equalizing reference map (DERM) for mapping 3D carotid surfaces to a standardized 2D carotid template, with an emphasis on preserving the local geometry of the carotid surface by minimizing the local area distortion. The initial map was generated by a previously described arc-length scaling (ALS) mapping method, which projects a 3D carotid surface onto a 2D non-convex L-shaped domain. A smooth and area-preserving flattened map was subsequently constructed by deforming the ALS map using the proposed algorithm that combines the density-equalizing map and the reference map techniques. This combination allows, for the first time, one-to-one mapping from a 3D surface to a standardized non-convex planar domain in an area-preserving manner. Evaluations using 20 carotid surface models show that the proposed method reduced the area distortion of the flattening maps by over 80% as compared to the ALS mapping method.



### FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/1812.03443v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.03443v3)
- **Published**: 2018-12-09 08:24:50+00:00
- **Updated**: 2019-05-24 05:47:40+00:00
- **Authors**: Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu, Yuandong Tian, Peter Vajda, Yangqing Jia, Kurt Keutzer
- **Comment**: None
- **Journal**: None
- **Summary**: Designing accurate and efficient ConvNets for mobile devices is challenging because the design space is combinatorially large. Due to this, previous neural architecture search (NAS) methods are computationally expensive. ConvNet architecture optimality depends on factors such as input resolution and target devices. However, existing approaches are too expensive for case-by-case redesigns. Also, previous work focuses primarily on reducing FLOPs, but FLOP count does not always reflect actual latency. To address these, we propose a differentiable neural architecture search (DNAS) framework that uses gradient-based methods to optimize ConvNet architectures, avoiding enumerating and training individual architectures separately as in previous methods. FBNets, a family of models discovered by DNAS surpass state-of-the-art models both designed manually and generated automatically. FBNet-B achieves 74.1% top-1 accuracy on ImageNet with 295M FLOPs and 23.1 ms latency on a Samsung S8 phone, 2.4x smaller and 1.5x faster than MobileNetV2-1.3 with similar accuracy. Despite higher accuracy and lower latency than MnasNet, we estimate FBNet-B's search cost is 420x smaller than MnasNet's, at only 216 GPU-hours. Searched for different resolutions and channel sizes, FBNets achieve 1.5% to 6.4% higher accuracy than MobileNetV2. The smallest FBNet achieves 50.2% accuracy and 2.9 ms latency (345 frames per second) on a Samsung S8. Over a Samsung-optimized FBNet, the iPhone-X-optimized model achieves a 1.4x speedup on an iPhone X.



### A New Variational Model for Joint Image Reconstruction and Motion Estimation in Spatiotemporal Imaging
- **Arxiv ID**: http://arxiv.org/abs/1812.03446v2
- **DOI**: 10.1137/18M1234047
- **Categories**: **math.NA**, cs.CV, 65F22, 65R32, 65R30, 65D18, 65J22, 65J20, 65L09, 68U10, 94A12,
  94A08, 92C55, 54C56, 57N25, 47A52
- **Links**: [PDF](http://arxiv.org/pdf/1812.03446v2)
- **Published**: 2018-12-09 08:28:27+00:00
- **Updated**: 2018-12-18 14:20:52+00:00
- **Authors**: Chong Chen, Barbara Gris, Ozan Öktem
- **Comment**: 35 pages, 5 figures, 3 tables, revised
- **Journal**: SIAM Journal on Imaging Sciences 2019
- **Summary**: We propose a new variational model for joint image reconstruction and motion estimation in spatiotemporal imaging, which is investigated along a general framework that we present with shape theory. This model consists of two components, one for conducting modified static image reconstruction, and the other performs sequentially indirect image registration. For the latter, we generalize the large deformation diffeomorphic metric mapping framework into the sequentially indirect registration setting. The proposed model is compared theoretically against alternative approaches (optical flow based model and diffeomorphic motion models), and we demonstrate that the proposed model has desirable properties in terms of the optimal solution. The theoretical derivations and efficient algorithms are also presented for a time-discretized scenario of the proposed model, which show that the optimal solution of the time-discretized version is consistent with that of the time-continuous one, and most of the computational components is the easy-implemented linearized deformation. The complexity of the algorithm is analyzed as well. This work is concluded by some numerical examples in 2D space + time tomography with very sparse and/or highly noisy data.



### A Comparison of Embedded Deep Learning Methods for Person Detection
- **Arxiv ID**: http://arxiv.org/abs/1812.03451v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.03451v2)
- **Published**: 2018-12-09 09:29:28+00:00
- **Updated**: 2019-01-08 15:26:19+00:00
- **Authors**: Chloe Eunhyang Kim, Mahdi Maktab Dar Oghaz, Jiri Fajtl, Vasileios Argyriou, Paolo Remagnino
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advancements in parallel computing, GPU technology and deep learning provide a new platform for complex image processing tasks such as person detection to flourish. Person detection is fundamental preliminary operation for several high level computer vision tasks. One industry that can significantly benefit from person detection is retail. In recent years, various studies attempt to find an optimal solution for person detection using neural networks and deep learning. This study conducts a comparison among the state of the art deep learning base object detector with the focus on person detection performance in indoor environments. Performance of various implementations of YOLO, SSD, RCNN, R-FCN and SqueezeDet have been assessed using our in-house proprietary dataset which consists of over 10 thousands indoor images captured form shopping malls, retails and stores. Experimental results indicate that, Tiny YOLO-416 and SSD (VGG-300) are the fastest and Faster-RCNN (Inception ResNet-v2) and R-FCN (ResNet-101) are the most accurate detectors investigated in this study. Further analysis shows that YOLO v3-416 delivers relatively accurate result in a reasonable amount of time, which makes it an ideal model for person detection in embedded platforms.



### Deep Learning with Attention to Predict Gestational Age of the Fetal Brain
- **Arxiv ID**: http://arxiv.org/abs/1812.07102v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.07102v1)
- **Published**: 2018-12-09 09:30:11+00:00
- **Updated**: 2018-12-09 09:30:11+00:00
- **Authors**: Liyue Shen, Katie Shpanskaya, Edward Lee, Emily McKenna, Maryam Maleki, Quin Lu, Safwan Halabi, John Pauly, Kristen Yeom
- **Comment**: NIPS Machine Learning for Health Workshop 2018, spotlight
  presentation
- **Journal**: None
- **Summary**: Fetal brain imaging is a cornerstone of prenatal screening and early diagnosis of congenital anomalies. Knowledge of fetal gestational age is the key to the accurate assessment of brain development. This study develops an attention-based deep learning model to predict gestational age of the fetal brain. The proposed model is an end-to-end framework that combines key insights from multi-view MRI including axial, coronal, and sagittal views. The model also uses age-activated weakly-supervised attention maps to enable rotation-invariant localization of the fetal brain among background noise. We evaluate our methods on the collected fetal brain MRI cohort with a large age distribution from 125 to 273 days. Our extensive experiments show age prediction performance with R2 = 0.94 using multi-view MRI and attention.



### Comixify: Transform video into a comics
- **Arxiv ID**: http://arxiv.org/abs/1812.03473v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.03473v1)
- **Published**: 2018-12-09 12:36:38+00:00
- **Updated**: 2018-12-09 12:36:38+00:00
- **Authors**: Maciej Pęśko, Adam Svystun, Paweł Andruszkiewicz, Przemysław Rokita, Tomasz Trzciński
- **Comment**: 14 pages. arXiv admin note: substantial text overlap with
  arXiv:1809.01726
- **Journal**: None
- **Summary**: In this paper, we propose a solution to transform a video into a comics. We approach this task using a neural style algorithm based on Generative Adversarial Networks (GANs). Several recent works in the field of Neural Style Transfer showed that producing an image in the style of another image is feasible. In this paper, we build up on these works and extend the existing set of style transfer use cases with a working application of video comixification. To that end, we train an end-to-end solution that transforms input video into a comics in two stages. In the first stage, we propose a state-of-the-art keyframes extraction algorithm that selects a subset of frames from the video to provide the most comprehensive video context and we filter those frames using image aesthetic estimation engine. In the second stage, the style of selected keyframes is transferred into a comics. To provide the most aesthetically compelling results, we selected the most state-of-the art style transfer solution and based on that implement our own ComixGAN framework. The final contribution of our work is a Web-based working application of video comixification available at http://comixify.ii.pw.edu.pl.



### Joint Vertebrae Identification and Localization in Spinal CT Images by Combining Short- and Long-Range Contextual Information
- **Arxiv ID**: http://arxiv.org/abs/1812.03500v1
- **DOI**: 10.1109/TMI.2018.2798293
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.03500v1)
- **Published**: 2018-12-09 15:25:43+00:00
- **Updated**: 2018-12-09 15:25:43+00:00
- **Authors**: Haofu Liao, Addisu Mesfin, Jiebo Luo
- **Comment**: None
- **Journal**: IEEE Transactions on Medical Imaging, vol. 37, no. 5, pp.
  1266-1275, May 2018
- **Summary**: Automatic vertebrae identification and localization from arbitrary CT images is challenging. Vertebrae usually share similar morphological appearance. Because of pathology and the arbitrary field-of-view of CT scans, one can hardly rely on the existence of some anchor vertebrae or parametric methods to model the appearance and shape. To solve the problem, we argue that one should make use of the short-range contextual information, such as the presence of some nearby organs (if any), to roughly estimate the target vertebrae; due to the unique anatomic structure of the spine column, vertebrae have fixed sequential order which provides the important long-range contextual information to further calibrate the results.   We propose a robust and efficient vertebrae identification and localization system that can inherently learn to incorporate both the short-range and long-range contextual information in a supervised manner. To this end, we develop a multi-task 3D fully convolutional neural network (3D FCN) to effectively extract the short-range contextual information around the target vertebrae. For the long-range contextual information, we propose a multi-task bidirectional recurrent neural network (Bi-RNN) to encode the spatial and contextual information among the vertebrae of the visible spine column. We demonstrate the effectiveness of the proposed approach on a challenging dataset and the experimental results show that our approach outperforms the state-of-the-art methods by a significant margin.



### Adversarial Sparse-View CBCT Artifact Reduction
- **Arxiv ID**: http://arxiv.org/abs/1812.03503v1
- **DOI**: 10.1007/978-3-030-00928-1_18
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.03503v1)
- **Published**: 2018-12-09 15:45:59+00:00
- **Updated**: 2018-12-09 15:45:59+00:00
- **Authors**: Haofu Liao, Zhimin Huo, William J. Sehnert, Shaohua Kevin Zhou, Jiebo Luo
- **Comment**: None
- **Journal**: Medical Image Computing and Computer Assisted Intervention
  (MICCAI) 2018. Lecture Notes in Computer Science, vol 11070. Springer, Cham
- **Summary**: We present an effective post-processing method to reduce the artifacts from sparsely reconstructed cone-beam CT (CBCT) images. The proposed method is based on the state-of-the-art, image-to-image generative models with a perceptual loss as regulation. Unlike the traditional CT artifact-reduction approaches, our method is trained in an adversarial fashion that yields more perceptually realistic outputs while preserving the anatomical structures. To address the streak artifacts that are inherently local and appear across various scales, we further propose a novel discriminator architecture based on feature pyramid networks and a differentially modulated focus map to induce the adversarial training. Our experimental results show that the proposed method can greatly correct the cone-beam artifacts from clinical CBCT images reconstructed using 1/3 projections, and outperforms strong baseline methods both quantitatively and qualitatively.



### From Coarse to Fine: Robust Hierarchical Localization at Large Scale
- **Arxiv ID**: http://arxiv.org/abs/1812.03506v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.03506v2)
- **Published**: 2018-12-09 15:56:36+00:00
- **Updated**: 2019-04-08 14:25:53+00:00
- **Authors**: Paul-Edouard Sarlin, Cesar Cadena, Roland Siegwart, Marcin Dymczyk
- **Comment**: Camera-ready for CVPR 2019
- **Journal**: None
- **Summary**: Robust and accurate visual localization is a fundamental capability for numerous applications, such as autonomous driving, mobile robotics, or augmented reality. It remains, however, a challenging task, particularly for large-scale environments and in presence of significant appearance changes. State-of-the-art methods not only struggle with such scenarios, but are often too resource intensive for certain real-time applications. In this paper we propose HF-Net, a hierarchical localization approach based on a monolithic CNN that simultaneously predicts local features and global descriptors for accurate 6-DoF localization. We exploit the coarse-to-fine localization paradigm: we first perform a global retrieval to obtain location hypotheses and only later match local features within those candidate places. This hierarchical approach incurs significant runtime savings and makes our system suitable for real-time operation. By leveraging learned descriptors, our method achieves remarkable localization robustness across large variations of appearance and sets a new state-of-the-art on two challenging benchmarks for large-scale localization.



### More Knowledge is Better: Cross-Modality Volume Completion and 3D+2D Segmentation for Intracardiac Echocardiography Contouring
- **Arxiv ID**: http://arxiv.org/abs/1812.03507v3
- **DOI**: 10.1007/978-3-030-00934-2_60
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.03507v3)
- **Published**: 2018-12-09 16:03:38+00:00
- **Updated**: 2022-03-23 16:25:19+00:00
- **Authors**: Haofu Liao, Yucheng Tang, Gareth Funka-Lea, Jiebo Luo, Shaohua Kevin Zhou
- **Comment**: None
- **Journal**: Medical Image Computing and Computer Assisted Intervention
  (MICCAI) 2018. Lecture Notes in Computer Science, vol 11071. Springer, Cham
- **Summary**: Using catheter ablation to treat atrial fibrillation increasingly relies on intracardiac echocardiography (ICE) for an anatomical delineation of the left atrium and the pulmonary veins that enter the atrium. However, it is a challenge to build an automatic contouring algorithm because ICE is noisy and provides only a limited 2D view of the 3D anatomy. This work provides the first automatic solution to segment the left atrium and the pulmonary veins from ICE. In this solution, we demonstrate the benefit of building a cross-modality framework that can leverage a database of diagnostic images to supplement the less available interventional images. To this end, we develop a novel deep neural network approach that uses the (i) 3D geometrical information provided by a position sensor embedded in the ICE catheter and the (ii) 3D image appearance information from a set of computed tomography cardiac volumes. We evaluate the proposed approach over 11,000 ICE images collected from 150 clinical patients. Experimental results show that our model is significantly better than a direct 2D image-to-image deep neural network segmentation, especially for less-observed structures.



### Skin Disease Classification versus Skin Lesion Characterization: Achieving Robust Diagnosis using Multi-label Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1812.03520v3
- **DOI**: 10.1109/ICPR.2016.7899659
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.03520v3)
- **Published**: 2018-12-09 16:56:14+00:00
- **Updated**: 2022-03-23 16:23:53+00:00
- **Authors**: Haofu Liao, Yuncheng Li, Jiebo Luo
- **Comment**: None
- **Journal**: 2016 23rd International Conference on Pattern Recognition (ICPR),
  Cancun, 2016, pp. 355-360
- **Summary**: In this study, we investigate what a practically useful approach is in order to achieve robust skin disease diagnosis. A direct approach is to target the ground truth diagnosis labels, while an alternative approach instead focuses on determining skin lesion characteristics that are more visually consistent and discernible. We argue that, for computer-aided skin disease diagnosis, it is both more realistic and more useful that lesion type tags should be considered as the target of an automated diagnosis system such that the system can first achieve a high accuracy in describing skin lesions, and in turn facilitate disease diagnosis using lesion characteristics in conjunction with other evidence. To further meet such an objective, we employ convolutional neural networks (CNNs) for both the disease-targeted and lesion-targeted classifications. We have collected a large-scale and diverse dataset of 75,665 skin disease images from six publicly available dermatology atlantes. Then we train and compare both disease-targeted and lesion-targeted classifiers, respectively. For disease-targeted classification, only 27.6% top-1 accuracy and 57.9% top-5 accuracy are achieved with a mean average precision (mAP) of 0.42. In contrast, for lesion-targeted classification, we can achieve a much higher mAP of 0.70.



### A Deep Multi-task Learning Approach to Skin Lesion Classification
- **Arxiv ID**: http://arxiv.org/abs/1812.03527v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.03527v3)
- **Published**: 2018-12-09 17:19:54+00:00
- **Updated**: 2022-03-23 16:20:32+00:00
- **Authors**: Haofu Liao, Jiebo Luo
- **Comment**: AAAI 2017 Joint Workshop on Health Intelligence W3PHIAI 2017 (W3PHI &
  HIAI), San Francisco, CA, 2017
- **Journal**: None
- **Summary**: Skin lesion identification is a key step toward dermatological diagnosis. When describing a skin lesion, it is very important to note its body site distribution as many skin diseases commonly affect particular parts of the body. To exploit the correlation between skin lesions and their body site distributions, in this study, we investigate the possibility of improving skin lesion classification using the additional context information provided by body location. Specifically, we build a deep multi-task learning (MTL) framework to jointly optimize skin lesion classification and body location classification (the latter is used as an inductive bias). Our MTL framework uses the state-of-the-art ImageNet pretrained model with specialized loss functions for the two related tasks. Our experiments show that the proposed MTL based method performs more robustly than its standalone (single-task) counterpart.



### A Structured Model For Action Detection
- **Arxiv ID**: http://arxiv.org/abs/1812.03544v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.03544v5)
- **Published**: 2018-12-09 18:57:33+00:00
- **Updated**: 2019-06-05 17:55:42+00:00
- **Authors**: Yubo Zhang, Pavel Tokmakov, Martial Hebert, Cordelia Schmid
- **Comment**: None
- **Journal**: None
- **Summary**: A dominant paradigm for learning-based approaches in computer vision is training generic models, such as ResNet for image recognition, or I3D for video understanding, on large datasets and allowing them to discover the optimal representation for the problem at hand. While this is an obviously attractive approach, it is not applicable in all scenarios. We claim that action detection is one such challenging problem - the models that need to be trained are large, and labeled data is expensive to obtain. To address this limitation, we propose to incorporate domain knowledge into the structure of the model, simplifying optimization. In particular, we augment a standard I3D network with a tracking module to aggregate long term motion patterns, and use a graph convolutional network to reason about interactions between actors and objects. Evaluated on the challenging AVA dataset, the proposed approach improves over the I3D baseline by 5.5% mAP and over the state-of-the-art by 4.8% mAP.



### Artificial Intelligence Assisted Infrastructure Assessment Using Mixed Reality Systems
- **Arxiv ID**: http://arxiv.org/abs/1812.05659v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.05659v1)
- **Published**: 2018-12-09 19:46:00+00:00
- **Updated**: 2018-12-09 19:46:00+00:00
- **Authors**: Enes Karaaslan, Ulas Bagci, F. Necati Catbas
- **Comment**: 5,240 word texts, 3 tables, 14 figures. Transportation Research
  Record: Journal of the Transportation Research Board, 2019
- **Journal**: None
- **Summary**: Conventional methods for visual assessment of civil infrastructures have certain limitations, such as subjectivity of the collected data, long inspection time, and high cost of labor. Although some new technologies i.e. robotic techniques that are currently in practice can collect objective, quantified data, the inspectors own expertise is still critical in many instances since these technologies are not designed to work interactively with human inspector. This study aims to create a smart, human centered method that offers significant contributions to infrastructure inspection, maintenance, management practice, and safety for the bridge owners. By developing a smart Mixed Reality framework, which can be integrated into a wearable holographic headset device, a bridge inspector, for example, can automatically analyze a certain defect such as a crack that he or she sees on an element, display its dimension information in real-time along with the condition state. Such systems can potentially decrease the time and cost of infrastructure inspections by accelerating essential tasks of the inspector such as defect measurement, condition assessment and data processing to management systems. The human centered artificial intelligence will help the inspector collect more quantified and objective data while incorporating inspectors professional judgement. This study explains in detail the described system and related methodologies of implementing attention guided semi supervised deep learning into mixed reality technology, which interacts with the human inspector during assessment. Thereby, the inspector and the AI will collaborate or communicate for improved visual inspection.



### Deep Spectral Reflectance and Illuminant Estimation from Self-Interreflections
- **Arxiv ID**: http://arxiv.org/abs/1812.03559v1
- **DOI**: 10.1364/JOSAA.36.000105
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.03559v1)
- **Published**: 2018-12-09 21:02:28+00:00
- **Updated**: 2018-12-09 21:02:28+00:00
- **Authors**: Rada Deeb, Joost Van De Weijer, Damien Muselet, Mathieu Hebert, Alain Tremeau
- **Comment**: Accepted by JOSA A
- **Journal**: None
- **Summary**: In this work, we propose a CNN-based approach to estimate the spectral reflectance of a surface and the spectral power distribution of the light from a single RGB image of a V-shaped surface. Interreflections happening in a concave surface lead to gradients of RGB values over its area. These gradients carry a lot of information concerning the physical properties of the surface and the illuminant. Our network is trained with only simulated data constructed using a physics-based interreflection model. Coupling interreflection effects with deep learning helps to retrieve the spectral reflectance under an unknown light and to estimate the spectral power distribution of this light as well. In addition, it is more robust to the presence of image noise than the classical approaches. Our results show that the proposed approach outperforms the state of the art learning-based approaches on simulated data. In addition, it gives better results on real data compared to other interreflection-based approaches.



### Learning Style Compatibility for Furniture
- **Arxiv ID**: http://arxiv.org/abs/1812.03570v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.03570v1)
- **Published**: 2018-12-09 22:54:41+00:00
- **Updated**: 2018-12-09 22:54:41+00:00
- **Authors**: Divyansh Aggarwal, Elchin Valiyev, Fadime Sener, Angela Yao
- **Comment**: German Conference on Pattern Recognition(GCPR)
- **Journal**: None
- **Summary**: When judging style, a key question that often arises is whether or not a pair of objects are compatible with each other. In this paper we investigate how Siamese networks can be used efficiently for assessing the style compatibility between images of furniture items. We show that the middle layers of pretrained CNNs can capture essential information about furniture style, which allows for efficient applications of such networks for this task. We also use a joint image-text embedding method that allows for the querying of stylistically compatible furniture items, along with additional attribute constraints based on text. To evaluate our methods, we collect and present a large scale dataset of images of furniture of different style categories accompanied by text attributes.



