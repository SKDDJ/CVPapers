# Arxiv Papers in cs.CV on 2018-12-03
### Adversarial Domain Randomization
- **Arxiv ID**: http://arxiv.org/abs/1812.00491v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.00491v2)
- **Published**: 2018-12-03 00:00:28+00:00
- **Updated**: 2021-08-29 21:13:26+00:00
- **Authors**: Rawal Khirodkar, Kris M. Kitani
- **Comment**: None
- **Journal**: None
- **Summary**: Domain Randomization (DR) is known to require a significant amount of training data for good performance. We argue that this is due to DR's strategy of random data generation using a uniform distribution over simulation parameters, as a result, DR often generates samples which are uninformative for the learner. In this work, we theoretically analyze DR using ideas from multi-source domain adaptation. Based on our findings, we propose Adversarial Domain Randomization (ADR) as an efficient variant of DR which generates adversarial samples with respect to the learner during training. We implement ADR as a policy whose action space is the quantized simulation parameter space. At each iteration, the policy's action generates labeled data and the reward is set as negative of learner's loss on this data. As a result, we observe ADR frequently generates novel samples for the learner like truncated and occluded objects for object detection and confusing classes for image classification. We perform evaluations on datasets like CLEVR, Syn2Real, and VIRAT for various tasks where we demonstrate that ADR outperforms DR by generating fewer data samples.



### Deep Learning Architect: Classification for Architectural Design through the Eye of Artificial Intelligence
- **Arxiv ID**: http://arxiv.org/abs/1812.01714v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1812.01714v1)
- **Published**: 2018-12-03 00:30:59+00:00
- **Updated**: 2018-12-03 00:30:59+00:00
- **Authors**: Yuji Yoshimura, Bill Cai, Zhoutong Wang, Carlo Ratti
- **Comment**: 22 pages, 5 figures, 4 tables
- **Journal**: None
- **Summary**: This paper applies state-of-the-art techniques in deep learning and computer vision to measure visual similarities between architectural designs by different architects. Using a dataset consisting of web scraped images and an original collection of images of architectural works, we first train a deep convolutional neural network (DCNN) model capable of achieving 73% accuracy in classifying works belonging to 34 different architects. Through examining the weights in the trained DCNN model, we are able to quantitatively measure the visual similarities between architects that are implicitly learned by our model. Using this measure, we cluster architects that are identified to be similar and compare our findings to conventional classification made by architectural historians and theorists. Our clustering of architectural designs remarkably corroborates conventional views in architectural history, and the learned architectural features also coheres with the traditional understanding of architectural designs.



### Multi-task Learning of Hierarchical Vision-Language Representation
- **Arxiv ID**: http://arxiv.org/abs/1812.00500v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.00500v1)
- **Published**: 2018-12-03 00:37:31+00:00
- **Updated**: 2018-12-03 00:37:31+00:00
- **Authors**: Duy-Kien Nguyen, Takayuki Okatani
- **Comment**: None
- **Journal**: None
- **Summary**: It is still challenging to build an AI system that can perform tasks that involve vision and language at human level. So far, researchers have singled out individual tasks separately, for each of which they have designed networks and trained them on its dedicated datasets. Although this approach has seen a certain degree of success, it comes with difficulties of understanding relations among different tasks and transferring the knowledge learned for a task to others. We propose a multi-task learning approach that enables to learn vision-language representation that is shared by many tasks from their diverse datasets. The representation is hierarchical, and prediction for each task is computed from the representation at its corresponding level of the hierarchy. We show through experiments that our method consistently outperforms previous single-task-learning methods on image caption retrieval, visual question answering, and visual grounding. We also analyze the learned hierarchical representation by visualizing attention maps generated in our network.



### Elastic Boundary Projection for 3D Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1812.00518v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.00518v2)
- **Published**: 2018-12-03 02:05:57+00:00
- **Updated**: 2020-06-06 12:24:29+00:00
- **Authors**: Tianwei Ni, Lingxi Xie, Huangjie Zheng, Elliot K. Fishman, Alan L. Yuille
- **Comment**: Accepted to CVPR 2019
- **Journal**: None
- **Summary**: We focus on an important yet challenging problem: using a 2D deep network to deal with 3D segmentation for medical image analysis. Existing approaches either applied multi-view planar (2D) networks or directly used volumetric (3D) networks for this purpose, but both of them are not ideal: 2D networks cannot capture 3D contexts effectively, and 3D networks are both memory-consuming and less stable arguably due to the lack of pre-trained models.   In this paper, we bridge the gap between 2D and 3D using a novel approach named Elastic Boundary Projection (EBP). The key observation is that, although the object is a 3D volume, what we really need in segmentation is to find its boundary which is a 2D surface. Therefore, we place a number of pivot points in the 3D space, and for each pivot, we determine its distance to the object boundary along a dense set of directions. This creates an elastic shell around each pivot which is initialized as a perfect sphere. We train a 2D deep network to determine whether each ending point falls within the object, and gradually adjust the shell so that it gradually converges to the actual shape of the boundary and thus achieves the goal of segmentation. EBP allows boundary-based segmentation without cutting a 3D volume into slices or patches, which stands out from conventional 2D and 3D approaches. EBP achieves promising accuracy in abdominal organ segmentation. Our code has been open-sourced https://github.com/twni2016/Elastic-Boundary-Projection.



### Automated Segmentation of Cervical Nuclei in Pap Smear Images using Deformable Multi-path Ensemble Model
- **Arxiv ID**: http://arxiv.org/abs/1812.00527v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.00527v2)
- **Published**: 2018-12-03 02:30:45+00:00
- **Updated**: 2019-05-14 03:43:45+00:00
- **Authors**: Jie Zhao, Quanzheng Li, Xiang Li, Hongfeng Li, Li Zhang
- **Comment**: ISBI2019 Oral
- **Journal**: None
- **Summary**: Pap smear testing has been widely used for detecting cervical cancers based on the morphology properties of cell nuclei in microscopic image. An accurate nuclei segmentation could thus improve the success rate of cervical cancer screening. In this work, a method of automated cervical nuclei segmentation using Deformable Multipath Ensemble Model (D-MEM) is proposed. The approach adopts a U-shaped convolutional network as a backbone network, in which dense blocks are used to transfer feature information more effectively. To increase the flexibility of the model, we then use deformable convolution to deal with different nuclei irregular shapes and sizes. To reduce the predictive bias, we further construct multiple networks with different settings, which form an ensemble model. The proposed segmentation framework has achieved state-of-the-art accuracy on Herlev dataset with Zijdenbos similarity index (ZSI) of 0.933, and has the potential to be extended for solving other medical image segmentation tasks.



### Beyond Inferring Class Representatives: User-Level Privacy Leakage From Federated Learning
- **Arxiv ID**: http://arxiv.org/abs/1812.00535v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1812.00535v3)
- **Published**: 2018-12-03 03:12:39+00:00
- **Updated**: 2018-12-05 01:17:05+00:00
- **Authors**: Zhibo Wang, Mengkai Song, Zhifei Zhang, Yang Song, Qian Wang, Hairong Qi
- **Comment**: The 38th Annual IEEE International Conference on Computer
  Communications (INFOCOM 2019)
- **Journal**: None
- **Summary**: Federated learning, i.e., a mobile edge computing framework for deep learning, is a recent advance in privacy-preserving machine learning, where the model is trained in a decentralized manner by the clients, i.e., data curators, preventing the server from directly accessing those private data from the clients. This learning mechanism significantly challenges the attack from the server side. Although the state-of-the-art attacking techniques that incorporated the advance of Generative adversarial networks (GANs) could construct class representatives of the global data distribution among all clients, it is still challenging to distinguishably attack a specific client (i.e., user-level privacy leakage), which is a stronger privacy threat to precisely recover the private data from a specific client. This paper gives the first attempt to explore user-level privacy leakage against the federated learning by the attack from a malicious server. We propose a framework incorporating GAN with a multi-task discriminator, which simultaneously discriminates category, reality, and client identity of input samples. The novel discrimination on client identity enables the generator to recover user specified private data. Unlike existing works that tend to interfere the training process of the federated learning, the proposed method works "invisibly" on the server side. The experimental results demonstrate the effectiveness of the proposed attacking approach and the superior to the state-of-the-art.



### Towards Accurate Generative Models of Video: A New Metric & Challenges
- **Arxiv ID**: http://arxiv.org/abs/1812.01717v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.01717v2)
- **Published**: 2018-12-03 03:57:42+00:00
- **Updated**: 2019-03-27 16:43:17+00:00
- **Authors**: Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, Sylvain Gelly
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in deep generative models have lead to remarkable progress in synthesizing high quality images. Following their successful application in image processing and representation learning, an important next step is to consider videos. Learning generative models of video is a much harder task, requiring a model to capture the temporal dynamics of a scene, in addition to the visual presentation of objects. While recent attempts at formulating generative models of video have had some success, current progress is hampered by (1) the lack of qualitative metrics that consider visual quality, temporal coherence, and diversity of samples, and (2) the wide gap between purely synthetic video data sets and challenging real-world data sets in terms of complexity. To this extent we propose Fr\'{e}chet Video Distance (FVD), a new metric for generative models of video, and StarCraft 2 Videos (SCV), a benchmark of game play from custom starcraft 2 scenarios that challenge the current capabilities of generative models of video. We contribute a large-scale human study, which confirms that FVD correlates well with qualitative human judgment of generated videos, and provide initial benchmark results on SCV.



### XNet: A convolutional neural network (CNN) implementation for medical X-Ray image segmentation suitable for small datasets
- **Arxiv ID**: http://arxiv.org/abs/1812.00548v2
- **DOI**: 10.1117/12.2512451
- **Categories**: **cs.CV**, cs.AI, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1812.00548v2)
- **Published**: 2018-12-03 04:17:27+00:00
- **Updated**: 2019-04-20 10:22:52+00:00
- **Authors**: Joseph Bullock, Carolina Cuesta-Lazaro, Arnau Quera-Bofarull
- **Comment**: 11 pages, 5 figures, 2 tables
- **Journal**: Proc. SPIE 10953, Medical Imaging 2019: Biomedical Applications in
  Molecular, Structural, and Functional Imaging, 109531Z (15 March 2019)
- **Summary**: X-Ray image enhancement, along with many other medical image processing applications, requires the segmentation of images into bone, soft tissue, and open beam regions. We apply a machine learning approach to this problem, presenting an end-to-end solution which results in robust and efficient inference. Since medical institutions frequently do not have the resources to process and label the large quantity of X-Ray images usually needed for neural network training, we design an end-to-end solution for small datasets, while achieving state-of-the-art results. Our implementation produces an overall accuracy of 92%, F1 score of 0.92, and an AUC of 0.98, surpassing classical image processing techniques, such as clustering and entropy based methods, while improving upon the output of existing neural networks used for segmentation in non-medical contexts. The code used for this project is available online.



### Learning to Unlearn: Building Immunity to Dataset Bias in Medical Imaging Studies
- **Arxiv ID**: http://arxiv.org/abs/1812.01716v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.01716v1)
- **Published**: 2018-12-03 04:46:35+00:00
- **Updated**: 2018-12-03 04:46:35+00:00
- **Authors**: Ahmed Ashraf, Shehroz Khan, Nikhil Bhagwat, Mallar Chakravarty, Babak Taati
- **Comment**: Machine Learning for Health (ML4H) Workshop at NeurIPS 2018
  arXiv:1811.07216 Submission Id: 207
- **Journal**: None
- **Summary**: Medical imaging machine learning algorithms are usually evaluated on a single dataset. Although training and testing are performed on different subsets of the dataset, models built on one study show limited capability to generalize to other studies. While database bias has been recognized as a serious problem in the computer vision community, it has remained largely unnoticed in medical imaging research. Transfer learning thus remains confined to the re-use of feature representations requiring re-training on the new dataset. As a result, machine learning models do not generalize even when trained on imaging datasets that were captured to study the same variable of interest. The ability to transfer knowledge gleaned from one study to another, without the need for re-training, if possible, would provide reassurance that the models are learning knowledge fundamental to the problem under study instead of latching onto the idiosyncracies of a dataset. In this paper, we situate the problem of dataset bias in the context of medical imaging studies. We show empirical evidence that such a problem exists in medical datasets. We then present a framework to unlearn study membership as a means to handle the problem of database bias. Our main idea is to take the data from the original feature space to an intermediate space where the data points are indistinguishable in terms of which study they come from, while maintaining the recognition capability with respect to the variable of interest. This will promote models which learn the more general properties of the etiology under study instead of aligning to dataset-specific peculiarities. Essentially, our proposed model learns to unlearn the dataset bias.



### Universal Perturbation Attack Against Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1812.00552v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.00552v2)
- **Published**: 2018-12-03 04:52:06+00:00
- **Updated**: 2019-09-11 02:27:12+00:00
- **Authors**: Jie Li, Rongrong Ji, Hong Liu, Xiaopeng Hong, Yue Gao, Qi Tian
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: Universal adversarial perturbations (UAPs), a.k.a. input-agnostic perturbations, has been proved to exist and be able to fool cutting-edge deep learning models on most of the data samples. Existing UAP methods mainly focus on attacking image classification models. Nevertheless, little attention has been paid to attacking image retrieval systems. In this paper, we make the first attempt in attacking image retrieval systems. Concretely, image retrieval attack is to make the retrieval system return irrelevant images to the query at the top ranking list. It plays an important role to corrupt the neighbourhood relationships among features in image retrieval attack. To this end, we propose a novel method to generate retrieval-against UAP to break the neighbourhood relationships of image features via degrading the corresponding ranking metric. To expand the attack method to scenarios with varying input sizes or untouchable network parameters, a multi-scale random resizing scheme and a ranking distillation strategy are proposed. We evaluate the proposed method on four widely-used image retrieval datasets, and report a significant performance drop in terms of different metrics, such as mAP and mP@10. Finally, we test our attack methods on the real-world visual search engine, i.e., Google Images, which demonstrates the practical potentials of our methods.



### A Pixel-Based Framework for Data-Driven Clothing
- **Arxiv ID**: http://arxiv.org/abs/1812.01677v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.01677v1)
- **Published**: 2018-12-03 04:52:10+00:00
- **Updated**: 2018-12-03 04:52:10+00:00
- **Authors**: Ning Jin, Yilin Zhu, Zhenglin Geng, Ronald Fedkiw
- **Comment**: None
- **Journal**: None
- **Summary**: With the aim of creating virtual cloth deformations more similar to real world clothing, we propose a new computational framework that recasts three dimensional cloth deformation as an RGB image in a two dimensional pattern space. Then a three dimensional animation of cloth is equivalent to a sequence of two dimensional RGB images, which in turn are driven/choreographed via animation parameters such as joint angles. This allows us to leverage popular CNNs to learn cloth deformations in image space. The two dimensional cloth pixels are extended into the real world via standard body skinning techniques, after which the RGB values are interpreted as texture offsets and displacement maps. Notably, we illustrate that our approach does not require accurate unclothed body shapes or robust skinning techniques. Additionally, we discuss how standard image based techniques such as image partitioning for higher resolution, GANs for merging partitioned image regions back together, etc., can readily be incorporated into our framework.



### SUSAN: Segment Unannotated image Structure using Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/1812.00555v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1812.00555v1)
- **Published**: 2018-12-03 05:00:07+00:00
- **Updated**: 2018-12-03 05:00:07+00:00
- **Authors**: Fang Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Segmentation of magnetic resonance (MR) images is a fundamental step in many medical imaging-based applications. The recent implementation of deep convolutional neural networks (CNNs) in image processing has been shown to have significant impacts on medical image segmentation. Network training of segmentation CNNs typically requires images and paired annotation data representing pixel-wise tissue labels referred to as masks. However, the supervised training of highly efficient CNNs with deeper structure and more network parameters requires a large number of training images and paired tissue masks. Thus, there is great need to develop a generalized CNN-based segmentation method which would be applicable for a wide variety of MR image datasets with different tissue contrasts. The purpose of this study was to develop and evaluate a generalized CNN-based method for fully-automated segmentation of different MR image datasets using a single set of annotated training data. A technique called cycle-consistent generative adversarial network (CycleGAN) is applied as the core of the proposed method to perform image-to-image translation between MR image datasets with different tissue contrasts. A joint segmentation network is incorporated into the adversarial network to obtain additional segmentation functionality. The proposed method was evaluated for segmenting bone and cartilage on two clinical knee MR image datasets acquired at our institution using only a single set of annotated data from a publicly available knee MR image dataset. The new technique may further improve the applicability and efficiency of CNN-based segmentation of medical images while eliminating the need for large amounts of annotated training data.



### Visual Foresight: Model-Based Deep Reinforcement Learning for Vision-Based Robotic Control
- **Arxiv ID**: http://arxiv.org/abs/1812.00568v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.00568v1)
- **Published**: 2018-12-03 06:06:25+00:00
- **Updated**: 2018-12-03 06:06:25+00:00
- **Authors**: Frederik Ebert, Chelsea Finn, Sudeep Dasari, Annie Xie, Alex Lee, Sergey Levine
- **Comment**: None
- **Journal**: None
- **Summary**: Deep reinforcement learning (RL) algorithms can learn complex robotic skills from raw sensory inputs, but have yet to achieve the kind of broad generalization and applicability demonstrated by deep learning methods in supervised domains. We present a deep RL method that is practical for real-world robotics tasks, such as robotic manipulation, and generalizes effectively to never-before-seen tasks and objects. In these settings, ground truth reward signals are typically unavailable, and we therefore propose a self-supervised model-based approach, where a predictive model learns to directly predict the future from raw sensory readings, such as camera images. At test time, we explore three distinct goal specification methods: designated pixels, where a user specifies desired object manipulation tasks by selecting particular pixels in an image and corresponding goal positions, goal images, where the desired goal state is specified with an image, and image classifiers, which define spaces of goal states. Our deep predictive models are trained using data collected autonomously and continuously by a robot interacting with hundreds of objects, without human supervision. We demonstrate that visual MPC can generalize to never-before-seen objects---both rigid and deformable---and solve a range of user-defined object manipulation tasks using the same model.



### Practical Window Setting Optimization for Medical Image Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1812.00572v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.00572v1)
- **Published**: 2018-12-03 06:23:52+00:00
- **Updated**: 2018-12-03 06:23:52+00:00
- **Authors**: Hyunkwang Lee, Myeongchan Kim, Synho Do
- **Comment**: Machine Learning for Health (ML4H) Workshop at NeurIPS 2018
  arXiv:cs/0101200
- **Journal**: None
- **Summary**: The recent advancements in deep learning have allowed for numerous applications in computed tomography (CT), with potential to improve diagnostic accuracy, speed of interpretation, and clinical efficiency. However, the deep learning community has to date neglected window display settings - a key feature of clinical CT interpretation and opportunity for additional optimization. Here we propose a window setting optimization (WSO) module that is fully trainable with convolutional neural networks (CNNs) to find optimal window settings for clinical performance. Our approach was inspired by the method commonly used by practicing radiologists to interpret CT images by adjusting window settings to increase the visualization of certain pathologies. Our approach provides optimal window ranges to enhance the conspicuity of abnormalities, and was used to enable performance enhancement for intracranial hemorrhage and urinary stone detection. On each task, the WSO model outperformed models trained over the full range of Hounsfield unit values in CT images, as well as images windowed with pre-defined settings. The WSO module can be readily applied to any analysis of CT images, and can be further generalized to tasks on other medical imaging modalities.



### Towards Visual Feature Translation
- **Arxiv ID**: http://arxiv.org/abs/1812.00573v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.00573v2)
- **Published**: 2018-12-03 06:37:02+00:00
- **Updated**: 2019-03-29 05:07:00+00:00
- **Authors**: Jie Hu, Rongrong Ji, Hong Liu, Shengchuan Zhang, Cheng Deng, Qi Tian
- **Comment**: Accepted as a CVPR 2019 paper
- **Journal**: None
- **Summary**: Most existing visual search systems are deployed based upon fixed kinds of visual features, which prohibits the feature reusing across different systems or when upgrading systems with a new type of feature. Such a setting is obviously inflexible and time/memory consuming, which is indeed mendable if visual features can be "translated" across systems. In this paper, we make the first attempt towards visual feature translation to break through the barrier of using features across different visual search systems. To this end, we propose a Hybrid Auto-Encoder (HAE) to translate visual features, which learns a mapping by minimizing the translation and reconstruction errors. Based upon HAE, an Undirected Affinity Measurement (UAM) is further designed to quantify the affinity among different types of visual features. Extensive experiments have been conducted on several public datasets with sixteen different types of widely-used features in visual search systems. Quantitative results show the encouraging possibilities of feature translation. For the first time, the affinity among widely-used features like SIFT and DELF is reported.



### A Smart Security System with Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1812.09127v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1812.09127v1)
- **Published**: 2018-12-03 07:57:14+00:00
- **Updated**: 2018-12-03 07:57:14+00:00
- **Authors**: Trung Nguyen, Barth Lakshmanan, Weihua Sheng
- **Comment**: None
- **Journal**: None
- **Summary**: Web-based technology has improved drastically in the past decade. As a result, security technology has become a major help to protect our daily life. In this paper, we propose a robust security based on face recognition system (SoF). In particular, we develop this system to giving access into a home for authenticated users. The classifier is trained by using a new adaptive learning method. The training data are initially collected from social networks. The accuracy of the classifier is incrementally improved as the user starts using the system. A novel method has been introduced to improve the classifier model by human interaction and social media. By using a deep learning framework - TensorFlow, it will be easy to reuse the framework to adopt with many devices and applications.



### Spatial-temporal Fusion Convolutional Neural Network for Simulated Driving Behavior Recognition
- **Arxiv ID**: http://arxiv.org/abs/1812.00615v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.00615v1)
- **Published**: 2018-12-03 09:18:18+00:00
- **Updated**: 2018-12-03 09:18:18+00:00
- **Authors**: Yaocong Hu, MingQi Lu, Xiaobo Lu
- **Comment**: International Conference on Control, Automation, Robotics and Vision
  (ICARCV2018)
- **Journal**: None
- **Summary**: Abnormal driving behaviour is one of the leading cause of terrible traffic accidents endangering human life. Therefore, study on driving behaviour surveillance has become essential to traffic security and public management. In this paper, we conduct this promising research and employ a two stream CNN framework for video-based driving behaviour recognition, in which spatial stream CNN captures appearance information from still frames, whilst temporal stream CNN captures motion information with pre-computed optical flow displacement between a few adjacent video frames. We investigate different spatial-temporal fusion strategies to combine the intra frame static clues and inter frame dynamic clues for final behaviour recognition. So as to validate the effectiveness of the designed spatial-temporal deep learning based model, we create a simulated driving behaviour dataset, containing 1237 videos with 6 different driving behavior for recognition. Experiment result shows that our proposed method obtains noticeable performance improvements compared to the existing methods.



### Unsupervised Deep Slow Feature Analysis for Change Detection in Multi-Temporal Remote Sensing Images
- **Arxiv ID**: http://arxiv.org/abs/1812.00645v2
- **DOI**: 10.1109/TGRS.2019.2930682
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.00645v2)
- **Published**: 2018-12-03 10:22:59+00:00
- **Updated**: 2019-07-15 11:11:30+00:00
- **Authors**: Bo Du, Lixiang Ru, Chen Wu, Liangpei Zhang
- **Comment**: 17 pages, 14 figures, accepted by IEEE Transactions of Geoscience and
  Remote Sensing
- **Journal**: None
- **Summary**: Change detection has been a hotspot in remote sensing technology for a long time. With the increasing availability of multi-temporal remote sensing images, numerous change detection algorithms have been proposed. Among these methods, image transformation methods with feature extraction and mapping could effectively highlight the changed information and thus has better change detection performance. However, changes of multi-temporal images are usually complex, existing methods are not effective enough. In recent years, deep network has shown its brilliant performance in many fields including feature extraction and projection. Therefore, in this paper, based on deep network and slow feature analysis (SFA) theory, we proposed a new change detection algorithm for multi-temporal remotes sensing images called Deep Slow Feature Analysis (DSFA). In DSFA model, two symmetric deep networks are utilized for projecting the input data of bi-temporal imagery. Then, the SFA module is deployed to suppress the unchanged components and highlight the changed components of the transformed features. The CVA pre-detection is employed to find unchanged pixels with high confidence as training samples. Finally, the change intensity is calculated with chi-square distance and the changes are determined by threshold algorithms. The experiments are performed on two real-world datasets and a public hyperspectral dataset. The visual comparison and quantitative evaluation have both shown that DSFA could outperform the other state-of-the-art algorithms, including other SFA-based and deep learning methods.



### Deep Hierarchical Machine: a Flexible Divide-and-Conquer Architecture
- **Arxiv ID**: http://arxiv.org/abs/1812.00647v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.00647v1)
- **Published**: 2018-12-03 10:34:01+00:00
- **Updated**: 2018-12-03 10:34:01+00:00
- **Authors**: Shichao Li, Xin Yang, Tim Cheng
- **Comment**: None
- **Journal**: None
- **Summary**: We propose Deep Hierarchical Machine (DHM), a model inspired from the divide-and-conquer strategy while emphasizing representation learning ability and flexibility. A stochastic routing framework as used by recent deep neural decision/regression forests is incorporated, but we remove the need to evaluate unnecessary computation paths by utilizing a different topology and introducing a probabilistic pruning technique. We also show a specified version of DHM (DSHM) for efficiency, which inherits the sparse feature extraction process as in traditional decision tree with pixel-difference feature. To achieve sparse feature extraction, we propose to utilize sparse convolution operation in DSHM and show one possibility of introducing sparse convolution kernels by using local binary convolution layer. DHM can be applied to both classification and regression problems, and we validate it on standard image classification and face alignment tasks to show its advantages over past architectures.



### Proceedings of the fourth "international Traveling Workshop on Interactions between low-complexity data models and Sensing Techniques" (iTWIST'18)
- **Arxiv ID**: http://arxiv.org/abs/1812.00648v2
- **DOI**: None
- **Categories**: **cs.IT**, cs.CV, eess.SP, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/1812.00648v2)
- **Published**: 2018-12-03 10:34:09+00:00
- **Updated**: 2018-12-20 08:49:14+00:00
- **Authors**: Sandrine Anthoine, Yannick Boursier, Laurent Jacques
- **Comment**: Final version, conference website:
  https://sites.google.com/view/itwist18
- **Journal**: None
- **Summary**: The iTWIST workshop series aim at fostering collaboration between international scientific teams for developing new theories, applications and generalizations of low-complexity models. These events emphasize dissemination of ideas through both specific oral and poster presentations, as well as free discussions. For this fourth edition, iTWIST'18 gathered in CIRM, Marseille, France, 74 international participants and featured 7 invited talks, 16 oral presentations, and 21 posters.   From iTWIST'18, the scientific committee has decided that the workshop proceedings will adopt the episcience.org philosophy, combined with arXiv.org: in a nutshell, "the proceedings are equivalent to an overlay page, built above arXiv.org; they add value to these archives by attaching a scientific caution to the validated papers."   This means that all papers listed in the HTML page of this arxiv publication (see the menu on the right) have been thoroughly evaluated and approved by two independent reviewers, and authors have revised their work according to the comments provided by these reviewers.



### Knowledge Distillation with Feature Maps for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1812.00660v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1812.00660v1)
- **Published**: 2018-12-03 11:03:04+00:00
- **Updated**: 2018-12-03 11:03:04+00:00
- **Authors**: Wei-Chun Chen, Chia-Che Chang, Chien-Yu Lu, Che-Rung Lee
- **Comment**: Knowledge Distillation, Model Compression, and Generative Adversarial
  Network, ACCV 2018
- **Journal**: None
- **Summary**: The model reduction problem that eases the computation costs and latency of complex deep learning architectures has received an increasing number of investigations owing to its importance in model deployment. One promising method is knowledge distillation (KD), which creates a fast-to-execute student model to mimic a large teacher network. In this paper, we propose a method, called KDFM (Knowledge Distillation with Feature Maps), which improves the effectiveness of KD by learning the feature maps from the teacher network. Two major techniques used in KDFM are shared classifier and generative adversarial network. Experimental results show that KDFM can use a four layers CNN to mimic DenseNet-40 and use MobileNet to mimic DenseNet-100. Both student networks have less than 1\% accuracy loss comparing to their teacher models for CIFAR-100 datasets. The student networks are 2-6 times faster than their teacher models for inference, and the model size of MobileNet is less than half of DenseNet-100's.



### An Interpretable Machine Vision Approach to Human Activity Recognition using Photoplethysmograph Sensor Data
- **Arxiv ID**: http://arxiv.org/abs/1812.00668v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, K.3.8
- **Links**: [PDF](http://arxiv.org/pdf/1812.00668v1)
- **Published**: 2018-12-03 11:10:59+00:00
- **Updated**: 2018-12-03 11:10:59+00:00
- **Authors**: Eoin Brophy, José Juan Dominguez Veiga, Zhengwei Wang, Alan F. Smeaton, Tomas E. Ward
- **Comment**: 26th AIAI Irish Conference on Artificial Intelligence and Cognitive
  Science
- **Journal**: None
- **Summary**: The current gold standard for human activity recognition (HAR) is based on the use of cameras. However, the poor scalability of camera systems renders them impractical in pursuit of the goal of wider adoption of HAR in mobile computing contexts. Consequently, researchers instead rely on wearable sensors and in particular inertial sensors. A particularly prevalent wearable is the smart watch which due to its integrated inertial and optical sensing capabilities holds great potential for realising better HAR in a non-obtrusive way. This paper seeks to simplify the wearable approach to HAR through determining if the wrist-mounted optical sensor alone typically found in a smartwatch or similar device can be used as a useful source of data for activity recognition. The approach has the potential to eliminate the need for the inertial sensing element which would in turn reduce the cost of and complexity of smartwatches and fitness trackers. This could potentially commoditise the hardware requirements for HAR while retaining the functionality of both heart rate monitoring and activity capture all from a single optical sensor. Our approach relies on the adoption of machine vision for activity recognition based on suitably scaled plots of the optical signals. We take this approach so as to produce classifications that are easily explainable and interpretable by non-technical users. More specifically, images of photoplethysmography signal time series are used to retrain the penultimate layer of a convolutional neural network which has initially been trained on the ImageNet database. We then use the 2048 dimensional features from the penultimate layer as input to a support vector machine. Results from the experiment yielded an average classification accuracy of 92.3%. This result outperforms that of an optical and inertial sensor combined (78%) and illustrates the capability of HAR systems using...



### Tensor N-tubal rank and its convex relaxation for low-rank tensor recovery
- **Arxiv ID**: http://arxiv.org/abs/1812.00688v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.00688v1)
- **Published**: 2018-12-03 11:51:59+00:00
- **Updated**: 2018-12-03 11:51:59+00:00
- **Authors**: Yu-Bang Zheng, Ting-Zhu Huang, Xi-Le Zhao, Tai-Xiang Jiang, Teng-Yu Ji, Tian-Hui Ma
- **Comment**: None
- **Journal**: None
- **Summary**: As low-rank modeling has achieved great success in tensor recovery, many research efforts devote to defining the tensor rank. Among them, the recent popular tensor tubal rank, defined based on the tensor singular value decomposition (t-SVD), obtains promising results. However, the framework of the t-SVD and the tensor tubal rank are applicable only to three-way tensors and lack of flexibility to handle different correlations along different modes. To tackle these two issues, we define a new tensor unfolding operator, named mode-$k_1k_2$ tensor unfolding, as the process of lexicographically stacking the mode-$k_1k_2$ slices of an $N$-way tensor into a three-way tensor, which is a three-way extension of the well-known mode-$k$ tensor matricization. Based on it, we define a novel tensor rank, the tensor $N$-tubal rank, as a vector whose elements contain the tubal rank of all mode-$k_1k_2$ unfolding tensors, to depict the correlations along different modes. To efficiently minimize the proposed $N$-tubal rank, we establish its convex relaxation: the weighted sum of tensor nuclear norm (WSTNN). Then, we apply WSTNN to low-rank tensor completion (LRTC) and tensor robust principal component analysis (TRPCA). The corresponding WSTNN-based LRTC and TRPCA models are proposed, and two efficient alternating direction method of multipliers (ADMM)-based algorithms are developed to solve the proposed models. Numerical experiments demonstrate that the proposed models significantly outperform the compared ones.



### An Analysis by Synthesis Approach for Automatic Vertebral Shape Identification in Clinical QCT
- **Arxiv ID**: http://arxiv.org/abs/1812.00693v1
- **DOI**: 10.1007/978-3-030-12939-2_6
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1812.00693v1)
- **Published**: 2018-12-03 12:07:01+00:00
- **Updated**: 2018-12-03 12:07:01+00:00
- **Authors**: Stefan Reinhold. Timo Damm, Lukas Huber, Reimer Andresen, Reinhard Barkmann, Claus-C. Glüer, Reinhard Koch
- **Comment**: Presented on German Conference on Pattern Recognition (GCPR) 2018 in
  Stuttgart
- **Journal**: None
- **Summary**: Quantitative computed tomography (QCT) is a widely used tool for osteoporosis diagnosis and monitoring. The assessment of cortical markers like cortical bone mineral density (BMD) and thickness is a demanding task, mainly because of the limited spatial resolution of QCT. We propose a direct model based method to automatically identify the surface through the center of the cortex of human vertebra. We develop a statistical bone model and analyze its probability distribution after the imaging process. Using an as-rigid-as-possible deformation we find the cortical surface that maximizes the likelihood of our model given the input volume. Using the European Spine Phantom (ESP) and a high resolution \mu CT scan of a cadaveric vertebra, we show that the proposed method is able to accurately identify the real center of cortex ex-vivo. To demonstrate the in-vivo applicability of our method we use manually obtained surfaces for comparison.



### Nesti-Net: Normal Estimation for Unstructured 3D Point Clouds using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1812.00709v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.00709v1)
- **Published**: 2018-12-03 12:27:02+00:00
- **Updated**: 2018-12-03 12:27:02+00:00
- **Authors**: Yizhak Ben-Shabat, Michael Lindenbaum, Anath Fischer
- **Comment**: Code will be available after publication. Figure quality reduced to
  fit size requirement. Higher quality images will be available in the final
  paper
- **Journal**: None
- **Summary**: In this paper, we propose a normal estimation method for unstructured 3D point clouds. This method, called Nesti-Net, builds on a new local point cloud representation which consists of multi-scale point statistics (MuPS), estimated on a local coarse Gaussian grid. This representation is a suitable input to a CNN architecture. The normals are estimated using a mixture-of-experts (MoE) architecture, which relies on a data-driven approach for selecting the optimal scale around each point and encourages sub-network specialization. Interesting insights into the network's resource distribution are provided. The scale prediction significantly improves robustness to different noise levels, point density variations and different levels of detail. We achieve state-of-the-art results on a benchmark synthetic dataset and present qualitative results on real scanned scenes.



### Deep Learning for Classical Japanese Literature
- **Arxiv ID**: http://arxiv.org/abs/1812.01718v1
- **DOI**: 10.20676/00000341
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.01718v1)
- **Published**: 2018-12-03 12:37:31+00:00
- **Updated**: 2018-12-03 12:37:31+00:00
- **Authors**: Tarin Clanuwat, Mikel Bober-Irizar, Asanobu Kitamoto, Alex Lamb, Kazuaki Yamamoto, David Ha
- **Comment**: To appear at Neural Information Processing Systems 2018 Workshop on
  Machine Learning for Creativity and Design
- **Journal**: None
- **Summary**: Much of machine learning research focuses on producing models which perform well on benchmark tasks, in turn improving our understanding of the challenges associated with those tasks. From the perspective of ML researchers, the content of the task itself is largely irrelevant, and thus there have increasingly been calls for benchmark tasks to more heavily focus on problems which are of social or cultural relevance. In this work, we introduce Kuzushiji-MNIST, a dataset which focuses on Kuzushiji (cursive Japanese), as well as two larger, more challenging datasets, Kuzushiji-49 and Kuzushiji-Kanji. Through these datasets, we wish to engage the machine learning community into the world of classical Japanese literature. Dataset available at https://github.com/rois-codh/kmnist



### SUSiNet: See, Understand and Summarize it
- **Arxiv ID**: http://arxiv.org/abs/1812.00722v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.00722v2)
- **Published**: 2018-12-03 13:21:51+00:00
- **Updated**: 2019-04-13 17:58:25+00:00
- **Authors**: Petros Koutras, Petros Maragos
- **Comment**: CVPR Workshops 2019 (Mutual benefits of cognitive and computer
  vision)
- **Journal**: None
- **Summary**: In this work we propose a multi-task spatio-temporal network, called SUSiNet, that can jointly tackle the spatio-temporal problems of saliency estimation, action recognition and video summarization. Our approach employs a single network that is jointly end-to-end trained for all tasks with multiple and diverse datasets related to the exploring tasks. The proposed network uses a unified architecture that includes global and task specific layer and produces multiple output types, i.e., saliency maps or classification labels, by employing the same video input. Moreover, one additional contribution is that the proposed network can be deeply supervised through an attention module that is related to human attention as it is expressed by eye-tracking data. From the extensive evaluation, on seven different datasets, we have observed that the multi-task network performs as well as the state-of-the-art single-task methods (or in some cases better), while it requires less computational budget than having one independent network per each task.



### Knowing what you know in brain segmentation using Bayesian deep neural networks
- **Arxiv ID**: http://arxiv.org/abs/1812.01719v5
- **DOI**: 10.3389/fninf.2019.00067
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.01719v5)
- **Published**: 2018-12-03 13:23:30+00:00
- **Updated**: 2019-09-18 10:30:08+00:00
- **Authors**: Patrick McClure, Nao Rho, John A. Lee, Jakub R. Kaczmarzyk, Charles Zheng, Satrajit S. Ghosh, Dylan Nielson, Adam G. Thomas, Peter Bandettini, Francisco Pereira
- **Comment**: Submitted to Frontiers in Neuroinformatics
- **Journal**: None
- **Summary**: In this paper, we describe a Bayesian deep neural network (DNN) for predicting FreeSurfer segmentations of structural MRI volumes, in minutes rather than hours. The network was trained and evaluated on a large dataset (n = 11,480), obtained by combining data from more than a hundred different sites, and also evaluated on another completely held-out dataset (n = 418). The network was trained using a novel spike-and-slab dropout-based variational inference approach. We show that, on these datasets, the proposed Bayesian DNN outperforms previously proposed methods, in terms of the similarity between the segmentation predictions and the FreeSurfer labels, and the usefulness of the estimate uncertainty of these predictions. In particular, we demonstrated that the prediction uncertainty of this network at each voxel is a good indicator of whether the network has made an error and that the uncertainty across the whole brain can predict the manual quality control ratings of a scan. The proposed Bayesian DNN method should be applicable to any new network architecture for addressing the segmentation problem.



### EnsNet: Ensconce Text in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1812.00723v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.00723v1)
- **Published**: 2018-12-03 13:25:26+00:00
- **Updated**: 2018-12-03 13:25:26+00:00
- **Authors**: Shuaitao Zhang, Yuliang Liu, Lianwen Jin, Yaoxiong Huang, Songxuan Lai
- **Comment**: 8 pages, 8 figures, 2 tables, accepted to appear in AAAI 2019
- **Journal**: None
- **Summary**: A new method is proposed for removing text from natural images. The challenge is to first accurately localize text on the stroke-level and then replace it with a visually plausible background. Unlike previous methods that require image patches to erase scene text, our method, namely ensconce network (EnsNet), can operate end-to-end on a single image without any prior knowledge. The overall structure is an end-to-end trainable FCN-ResNet-18 network with a conditional generative adversarial network (cGAN). The feature of the former is first enhanced by a novel lateral connection structure and then refined by four carefully designed losses: multiscale regression loss and content loss, which capture the global discrepancy of different level features; texture loss and total variation loss, which primarily target filling the text region and preserving the reality of the background. The latter is a novel local-sensitive GAN, which attentively assesses the local consistency of the text erased regions. Both qualitative and quantitative sensitivity experiments on synthetic images and the ICDAR 2013 dataset demonstrate that each component of the EnsNet is essential to achieve a good performance. Moreover, our EnsNet can significantly outperform previous state-of-the-art methods in terms of all metrics. In addition, a qualitative experiment conducted on the SMBNet dataset further demonstrates that the proposed method can also preform well on general object (such as pedestrians) removal tasks. EnsNet is extremely fast, which can preform at 333 fps on an i5-8600 CPU device.



### CRAVES: Controlling Robotic Arm with a Vision-based Economic System
- **Arxiv ID**: http://arxiv.org/abs/1812.00725v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.00725v2)
- **Published**: 2018-12-03 13:28:29+00:00
- **Updated**: 2019-07-02 13:26:04+00:00
- **Authors**: Yiming Zuo, Weichao Qiu, Lingxi Xie, Fangwei Zhong, Yizhou Wang, Alan L. Yuille
- **Comment**: 10 pages, 6 figures
- **Journal**: In Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (2019) 4214-4223
- **Summary**: Training a robotic arm to accomplish real-world tasks has been attracting increasing attention in both academia and industry. This work discusses the role of computer vision algorithms in this field. We focus on low-cost arms on which no sensors are equipped and thus all decisions are made upon visual recognition, e.g., real-time 3D pose estimation. This requires annotating a lot of training data, which is not only time-consuming but also laborious.   In this paper, we present an alternative solution, which uses a 3D model to create a large number of synthetic data, trains a vision model in this virtual domain, and applies it to real-world images after domain adaptation. To this end, we design a semi-supervised approach, which fully leverages the geometric constraints among keypoints. We apply an iterative algorithm for optimization. Without any annotations on real images, our algorithm generalizes well and produces satisfying results on 3D pose estimation, which is evaluated on two real-world datasets. We also construct a vision-based control system for task accomplishment, for which we train a reinforcement learning agent in a virtual environment and apply it to the real-world. Moreover, our approach, with merely a 3D model being required, has the potential to generalize to other types of multi-rigid-body dynamic systems.



### Attention-based Adaptive Selection of Operations for Image Restoration in the Presence of Unknown Combined Distortions
- **Arxiv ID**: http://arxiv.org/abs/1812.00733v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.00733v2)
- **Published**: 2018-12-03 13:50:40+00:00
- **Updated**: 2019-04-07 11:51:26+00:00
- **Authors**: Masanori Suganuma, Xing Liu, Takayuki Okatani
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: Many studies have been conducted so far on image restoration, the problem of restoring a clean image from its distorted version. There are many different types of distortion which affect image quality. Previous studies have focused on single types of distortion, proposing methods for removing them. However, image quality degrades due to multiple factors in the real world. Thus, depending on applications, e.g., vision for autonomous cars or surveillance cameras, we need to be able to deal with multiple combined distortions with unknown mixture ratios. For this purpose, we propose a simple yet effective layer architecture of neural networks. It performs multiple operations in parallel, which are weighted by an attention mechanism to enable selection of proper operations depending on the input. The layer can be stacked to form a deep network, which is differentiable and thus can be trained in an end-to-end fashion by gradient descent. The experimental results show that the proposed method works better than previous methods by a good margin on tasks of restoring images with multiple combined distortions.



### Nose, eyes and ears: Head pose estimation by locating facial keypoints
- **Arxiv ID**: http://arxiv.org/abs/1812.00739v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.00739v1)
- **Published**: 2018-12-03 14:04:04+00:00
- **Updated**: 2018-12-03 14:04:04+00:00
- **Authors**: Aryaman Gupta, Kalpit Thakkar, Vineet Gandhi, P J Narayanan
- **Comment**: 4 pages, ICASSP 2019
- **Journal**: None
- **Summary**: Monocular head pose estimation requires learning a model that computes the intrinsic Euler angles for pose (yaw, pitch, roll) from an input image of human face. Annotating ground truth head pose angles for images in the wild is difficult and requires ad-hoc fitting procedures (which provides only coarse and approximate annotations). This highlights the need for approaches which can train on data captured in controlled environment and generalize on the images in the wild (with varying appearance and illumination of the face). Most present day deep learning approaches which learn a regression function directly on the input images fail to do so. To this end, we propose to use a higher level representation to regress the head pose while using deep learning architectures. More specifically, we use the uncertainty maps in the form of 2D soft localization heatmap images over five facial keypoints, namely left ear, right ear, left eye, right eye and nose, and pass them through an convolutional neural network to regress the head-pose. We show head pose estimation results on two challenging benchmarks BIWI and AFLW and our approach surpasses the state of the art on both the datasets.



### Disentangling Adversarial Robustness and Generalization
- **Arxiv ID**: http://arxiv.org/abs/1812.00740v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.00740v2)
- **Published**: 2018-12-03 14:04:35+00:00
- **Updated**: 2019-04-10 10:25:38+00:00
- **Authors**: David Stutz, Matthias Hein, Bernt Schiele
- **Comment**: Conference on Computer Vision and Pattern Recognition 2019
- **Journal**: None
- **Summary**: Obtaining deep networks that are robust against adversarial examples and generalize well is an open problem. A recent hypothesis even states that both robust and accurate models are impossible, i.e., adversarial robustness and generalization are conflicting goals. In an effort to clarify the relationship between robustness and generalization, we assume an underlying, low-dimensional data manifold and show that: 1. regular adversarial examples leave the manifold; 2. adversarial examples constrained to the manifold, i.e., on-manifold adversarial examples, exist; 3. on-manifold adversarial examples are generalization errors, and on-manifold adversarial training boosts generalization; 4. regular robustness and generalization are not necessarily contradicting goals. These assumptions imply that both robust and accurate models are possible. However, different models (architectures, training strategies etc.) can exhibit different robustness and generalization characteristics. To confirm our claims, we present extensive experiments on synthetic data (with known manifold) as well as on EMNIST, Fashion-MNIST and CelebA.



### Towards Spectral Estimation from a Single RGB Image in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1812.00805v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.00805v1)
- **Published**: 2018-12-03 14:58:26+00:00
- **Updated**: 2018-12-03 14:58:26+00:00
- **Authors**: Berk Kaya, Yigit Baran Can, Radu Timofte
- **Comment**: None
- **Journal**: None
- **Summary**: In contrast to the current literature, we address the problem of estimating the spectrum from a single common trichromatic RGB image obtained under unconstrained settings (e.g. unknown camera parameters, unknown scene radiance, unknown scene contents). For this we use a reference spectrum as provided by a hyperspectral image camera, and propose efficient deep learning solutions for sensitivity function estimation and spectral reconstruction from a single RGB image. We further expand the concept of spectral reconstruction such that to work for RGB images taken in the wild and propose a solution based on a convolutional network conditioned on the estimated sensitivity function. Besides the proposed solutions, we study also generic and sensitivity specialized models and discuss their limitations. We achieve state-of-the-art competitive results on the standard example-based spectral reconstruction benchmarks: ICVL, CAVE, NUS and NTIRE. Moreover, our experiments show that, for the first time, accurate spectral estimation from a single RGB image in the wild is within our reach.



### A Wasserstein GAN model with the total variational regularization
- **Arxiv ID**: http://arxiv.org/abs/1812.00810v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.00810v1)
- **Published**: 2018-12-03 15:00:33+00:00
- **Updated**: 2018-12-03 15:00:33+00:00
- **Authors**: Lijun Zhang, Yujin Zhang, Yongbin Gao
- **Comment**: None
- **Journal**: None
- **Summary**: It is well known that the generative adversarial nets (GANs) are remarkably difficult to train. The recently proposed Wasserstein GAN (WGAN) creates principled research directions towards addressing these issues. But we found in practice that gradient penalty WGANs (GP-WGANs) still suffer from training instability. In this paper, we combine a Total Variational (TV) regularizing term into the WGAN formulation instead of weight clipping or gradient penalty, which implies that the Lipschitz constraint is enforced on the critic network. Our proposed method is more stable at training than GP-WGANs and works well across varied GAN architectures. We also present a method to control the trade-off between image diversity and visual quality. It does not bring any computation burden.



### Novel Quality Metric for Duration Variability Compensation in Speaker Verification using i-Vectors
- **Arxiv ID**: http://arxiv.org/abs/1812.00828v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1812.00828v1)
- **Published**: 2018-12-03 15:20:59+00:00
- **Updated**: 2018-12-03 15:20:59+00:00
- **Authors**: Arnab Poddar, Md Sahidullah, Goutam Saha
- **Comment**: Accepted and presented in ICAPR 2017, Bangalore, India
- **Journal**: None
- **Summary**: Automatic speaker verification (ASV) is the process to recognize persons using voice as biometric. The ASV systems show considerable recognition performance with sufficient amount of speech from matched condition. One of the crucial challenges of ASV technology is to improve recognition performance with speech segments of short duration. In short duration condition, the model parameters are not properly estimated due to inadequate speech information, and this results poor recognition accuracy even with the state-of-the-art i-vector based ASV system. We hypothesize that considering the estimation quality during recognition process would help to improve the ASV performance. This can be incorporated as a quality measure during fusion of ASV systems. This paper investigates a new quality measure for i-vector representation of speech utterances computed directly from Baum-Welch statistics. The proposed metric is subsequently used as quality measure during fusion of ASV systems. In experiments with the NIST SRE 2008 corpus, We have shown that inclusion of proposed quality metric exhibits considerable improvement in speaker verification performance. The results also indicate the potentiality of the proposed method in real-world scenario with short test utterances.



### Iterative Potts minimization for the recovery of signals with discontinuities from indirect measurements -- the multivariate case
- **Arxiv ID**: http://arxiv.org/abs/1812.00862v2
- **DOI**: None
- **Categories**: **math.NA**, cs.CV, cs.NA
- **Links**: [PDF](http://arxiv.org/pdf/1812.00862v2)
- **Published**: 2018-12-03 16:19:05+00:00
- **Updated**: 2021-03-10 10:01:06+00:00
- **Authors**: Lukas Kiefer, Martin Storath, Andreas Weinmann
- **Comment**: 44 pages, 5 figures
- **Journal**: None
- **Summary**: Signals and images with discontinuities appear in many problems in such diverse areas as biology, medicine, mechanics, and electrical engineering. The concrete data are often discrete, indirect and noisy measurements of some quantities describing the signal under consideration. A frequent task is to find the segments of the signal or image which corresponds to finding the discontinuities or jumps in the data. Methods based on minimizing the piecewise constant Mumford-Shah functional -- whose discretized version is known as Potts functional -- are advantageous in this scenario, in particular, in connection with segmentation. However, due to their non-convexity, minimization of such functionals is challenging. In this paper we propose a new iterative minimization strategy for the multivariate Potts functional dealing with indirect, noisy measurements. We provide a convergence analysis and underpin our findings with numerical experiments.



### What can I do here? Leveraging Deep 3D saliency and geometry for fast and scalable multiple affordance detection
- **Arxiv ID**: http://arxiv.org/abs/1812.00889v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1812.00889v1)
- **Published**: 2018-12-03 16:39:32+00:00
- **Updated**: 2018-12-03 16:39:32+00:00
- **Authors**: Eduardo Ruiz, Walterio Mayol-Cuevas
- **Comment**: 10 pages,9 figures
- **Journal**: None
- **Summary**: This paper develops and evaluates a novel method that allows for the detection of affordances in a scalable and multiple-instance manner on visually recovered pointclouds. Our approach has many advantages over alternative methods, as it is based on highly parallelizable, one-shot learning that is fast in commodity hardware. The approach is hybrid in that it uses a geometric representation together with a state-of-the-art deep learning method capable of identifying 3D scene saliency. The geometric component allows for a compact and efficient representation, boosting the performance of the deep network architecture which proved insufficient on its own. Moreover, our approach allows not only to predict whether an input scene affords or not the interactions, but also the pose of the objects that allow these interactions to take place. Our predictions align well with crowd-sourced human judgment as they are preferred with 87% probability, show high rates of improvement with almost four times (4x) better performance over a deep learning-only baseline and are seven times (7x) faster than previous art.



### Domain Alignment with Triplets
- **Arxiv ID**: http://arxiv.org/abs/1812.00893v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.00893v2)
- **Published**: 2018-12-03 16:46:29+00:00
- **Updated**: 2019-01-22 11:59:40+00:00
- **Authors**: Weijian Deng, Liang Zheng, Jianbin Jiao
- **Comment**: 10 pages;This version is not fully edited and will be updated soon
- **Journal**: None
- **Summary**: Deep domain adaptation methods can reduce the distribution discrepancy by learning domain-invariant embedddings. However, these methods only focus on aligning the whole data distributions, without considering the class-level relations among source and target images. Thus, a target embeddings of a bird might be aligned to source embeddings of an airplane. This semantic misalignment can directly degrade the classifier performance on the target dataset. To alleviate this problem, we present a similarity constrained alignment (SCA) method for unsupervised domain adaptation. When aligning the distributions in the embedding space, SCA enforces a similarity-preserving constraint to maintain class-level relations among the source and target images, i.e., if a source image and a target image are of the same class label, their corresponding embeddings are supposed to be aligned nearby, and vise versa. In the absence of target labels, we assign pseudo labels for target images. Given labeled source images and pseudo-labeled target images, the similarity-preserving constraint can be implemented by minimizing the triplet loss. With the joint supervision of domain alignment loss and similarity-preserving constraint, we train a network to obtain domain-invariant embeddings with two critical characteristics, intra-class compactness and inter-class separability. Extensive experiments conducted on the two datasets well demonstrate the effectiveness of SCA.



### Generating Diverse Programs with Instruction Conditioned Reinforced Adversarial Learning
- **Arxiv ID**: http://arxiv.org/abs/1812.00898v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.00898v1)
- **Published**: 2018-12-03 16:51:35+00:00
- **Updated**: 2018-12-03 16:51:35+00:00
- **Authors**: Aishwarya Agrawal, Mateusz Malinowski, Felix Hill, Ali Eslami, Oriol Vinyals, Tejas Kulkarni
- **Comment**: None
- **Journal**: None
- **Summary**: Advances in Deep Reinforcement Learning have led to agents that perform well across a variety of sensory-motor domains. In this work, we study the setting in which an agent must learn to generate programs for diverse scenes conditioned on a given symbolic instruction. Final goals are specified to our agent via images of the scenes. A symbolic instruction consistent with the goal images is used as the conditioning input for our policies. Since a single instruction corresponds to a diverse set of different but still consistent end-goal images, the agent needs to learn to generate a distribution over programs given an instruction. We demonstrate that with simple changes to the reinforced adversarial learning objective, we can learn instruction conditioned policies to achieve the corresponding diverse set of goals. Most importantly, our agent's stochastic policy is shown to more accurately capture the diversity in the goal distribution than a fixed pixel-based reward function baseline. We demonstrate the efficacy of our approach on two domains: (1) drawing MNIST digits with a paint software conditioned on instructions and (2) constructing scenes in a 3D editor that satisfies a certain instruction.



### The Right (Angled) Perspective: Improving the Understanding of Road Scenes Using Boosted Inverse Perspective Mapping
- **Arxiv ID**: http://arxiv.org/abs/1812.00913v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1812.00913v2)
- **Published**: 2018-12-03 17:12:41+00:00
- **Updated**: 2019-05-02 08:39:02+00:00
- **Authors**: Tom Bruls, Horia Porav, Lars Kunze, Paul Newman
- **Comment**: equal contribution of first two authors, 8 full pages, 6 figures,
  accepted at IV 2019
- **Journal**: None
- **Summary**: Many tasks performed by autonomous vehicles such as road marking detection, object tracking, and path planning are simpler in bird's-eye view. Hence, Inverse Perspective Mapping (IPM) is often applied to remove the perspective effect from a vehicle's front-facing camera and to remap its images into a 2D domain, resulting in a top-down view. Unfortunately, however, this leads to unnatural blurring and stretching of objects at further distance, due to the resolution of the camera, limiting applicability. In this paper, we present an adversarial learning approach for generating a significantly improved IPM from a single camera image in real time. The generated bird's-eye-view images contain sharper features (e.g. road markings) and a more homogeneous illumination, while (dynamic) objects are automatically removed from the scene, thus revealing the underlying road layout in an improved fashion. We demonstrate our framework using real-world data from the Oxford RobotCar Dataset and show that scene understanding tasks directly benefit from our boosted IPM approach.



### SPLAT: Semantic Pixel-Level Adaptation Transforms for Detection
- **Arxiv ID**: http://arxiv.org/abs/1812.00929v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.00929v1)
- **Published**: 2018-12-03 17:38:52+00:00
- **Updated**: 2018-12-03 17:38:52+00:00
- **Authors**: Eric Tzeng, Kaylee Burns, Kate Saenko, Trevor Darrell
- **Comment**: None
- **Journal**: None
- **Summary**: Domain adaptation of visual detectors is a critical challenge, yet existing methods have overlooked pixel appearance transformations, focusing instead on bootstrapping and/or domain confusion losses. We propose a Semantic Pixel-Level Adaptation Transform (SPLAT) approach to detector adaptation that efficiently generates cross-domain image pairs. Our model uses aligned-pair and/or pseudo-label losses to adapt an object detector to the target domain, and can learn transformations with or without densely labeled data in the source (e.g. semantic segmentation annotations). Without dense labels, as is the case when only detection labels are available in the source, transformations are learned using CycleGAN alignment. Otherwise, when dense labels are available we introduce a more efficient cycle-free method, which exploits pixel-level semantic labels to condition the training of the transformation network. The end task is then trained using detection box labels from the source, potentially including labels inferred on unlabeled source data. We show both that pixel-level transforms outperform prior approaches to detector domain adaptation, and that our cycle-free method outperforms prior models for unconstrained cycle-based learning of generic transformations while running 3.8 times faster. Our combined model improves on prior detection baselines by 12.5 mAP adapting from Sim 10K to Cityscapes, recovering over 50% of the missing performance between the unadapted baseline and the labeled-target upper bound.



### Visual Memory for Robust Path Following
- **Arxiv ID**: http://arxiv.org/abs/1812.00940v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1812.00940v1)
- **Published**: 2018-12-03 18:06:53+00:00
- **Updated**: 2018-12-03 18:06:53+00:00
- **Authors**: Ashish Kumar, Saurabh Gupta, David Fouhey, Sergey Levine, Jitendra Malik
- **Comment**: Neural Information Processing Systems (NeurIPS) 2018. Oral
  Presentation
- **Journal**: None
- **Summary**: Humans routinely retrace paths in a novel environment both forwards and backwards despite uncertainty in their motion. This paper presents an approach for doing so. Given a demonstration of a path, a first network generates a path abstraction. Equipped with this abstraction, a second network observes the world and decides how to act to retrace the path under noisy actuation and a changing environment. The two networks are optimized end-to-end at training time. We evaluate the method in two realistic simulators, performing path following and homing under actuation noise and environmental changes. Our experiments show that our approach outperforms classical approaches and other learning based baselines.



### Context Encoding Chest X-rays
- **Arxiv ID**: http://arxiv.org/abs/1812.00964v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.00964v2)
- **Published**: 2018-12-03 18:37:46+00:00
- **Updated**: 2019-04-09 10:05:27+00:00
- **Authors**: Davide Belli, Shi Hu, Ecem Sogancioglu, Bram van Ginneken
- **Comment**: 11 pages, 8 figures; fixed sentence in abstract, updated title
- **Journal**: None
- **Summary**: Chest X-rays are one of the most commonly used technologies for medical diagnosis. Many deep learning models have been proposed to improve and automate the abnormality detection task on this type of data. In this paper, we propose a different approach based on image inpainting under adversarial training first introduced by Goodfellow et al. We configure the context encoder model for this task and train it over 1.1M 128x128 images from healthy X-rays. The goal of our model is to reconstruct the missing central 64x64 patch. Once the model has learned how to inpaint healthy tissue, we test its performance on images with and without abnormalities. We discuss and motivate our results considering PSNR, MSE and SSIM scores as evaluation metrics. In addition, we conduct a 2AFC observer study showing that in half of the times an expert is unable to distinguish real images from the ones reconstructed using our model. By computing and visualizing the pixel-wise difference between the source and the reconstructed images, we can highlight abnormalities to simplify further detection and classification tasks.



### Learning to Learn How to Learn: Self-Adaptive Visual Navigation Using Meta-Learning
- **Arxiv ID**: http://arxiv.org/abs/1812.00971v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1812.00971v2)
- **Published**: 2018-12-03 18:46:02+00:00
- **Updated**: 2019-03-26 23:55:19+00:00
- **Authors**: Mitchell Wortsman, Kiana Ehsani, Mohammad Rastegari, Ali Farhadi, Roozbeh Mottaghi
- **Comment**: None
- **Journal**: None
- **Summary**: Learning is an inherently continuous phenomenon. When humans learn a new task there is no explicit distinction between training and inference. As we learn a task, we keep learning about it while performing the task. What we learn and how we learn it varies during different stages of learning. Learning how to learn and adapt is a key property that enables us to generalize effortlessly to new settings. This is in contrast with conventional settings in machine learning where a trained model is frozen during inference. In this paper we study the problem of learning to learn at both training and test time in the context of visual navigation. A fundamental challenge in navigation is generalization to unseen scenes. In this paper we propose a self-adaptive visual navigation method (SAVN) which learns to adapt to new environments without any explicit supervision. Our solution is a meta-reinforcement learning approach where an agent learns a self-supervised interaction loss that encourages effective navigation. Our experiments, performed in the AI2-THOR framework, show major improvements in both success rate and SPL for visual navigation in novel scenes. Our code and data are available at: https://github.com/allenai/savn .



### DeepVoxels: Learning Persistent 3D Feature Embeddings
- **Arxiv ID**: http://arxiv.org/abs/1812.01024v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.01024v2)
- **Published**: 2018-12-03 19:01:01+00:00
- **Updated**: 2019-04-11 01:10:03+00:00
- **Authors**: Vincent Sitzmann, Justus Thies, Felix Heide, Matthias Nießner, Gordon Wetzstein, Michael Zollhöfer
- **Comment**: Video: https://www.youtube.com/watch?v=HM_WsZhoGXw Supplemental
  material:
  https://drive.google.com/file/d/1BnZRyNcVUty6-LxAstN83H79ktUq8Cjp/view?usp=sharing
  Code: https://github.com/vsitzmann/deepvoxels Project page:
  https://vsitzmann.github.io/deepvoxels/
- **Journal**: None
- **Summary**: In this work, we address the lack of 3D understanding of generative neural networks by introducing a persistent 3D feature embedding for view synthesis. To this end, we propose DeepVoxels, a learned representation that encodes the view-dependent appearance of a 3D scene without having to explicitly model its geometry. At its core, our approach is based on a Cartesian 3D grid of persistent embedded features that learn to make use of the underlying 3D scene structure. Our approach combines insights from 3D geometric computer vision with recent advances in learning image-to-image mappings based on adversarial loss functions. DeepVoxels is supervised, without requiring a 3D reconstruction of the scene, using a 2D re-rendering loss and enforces perspective and multi-view geometry in a principled manner. We apply our persistent 3D scene representation to the problem of novel view synthesis demonstrating high-quality results for a variety of challenging scenes.



### TwoStreamVAN: Improving Motion Modeling in Video Generation
- **Arxiv ID**: http://arxiv.org/abs/1812.01037v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.01037v2)
- **Published**: 2018-12-03 19:11:45+00:00
- **Updated**: 2020-01-10 00:07:12+00:00
- **Authors**: Ximeng Sun, Huijuan Xu, Kate Saenko
- **Comment**: WACV 2020
- **Journal**: None
- **Summary**: Video generation is an inherently challenging task, as it requires modeling realistic temporal dynamics as well as spatial content. Existing methods entangle the two intrinsically different tasks of motion and content creation in a single generator network, but this approach struggles to simultaneously generate plausible motion and content. To im-prove motion modeling in video generation tasks, we propose a two-stream model that disentangles motion generation from content generation, called a Two-Stream Variational Adversarial Network (TwoStreamVAN). Given an action label and a noise vector, our model is able to create clear and consistent motion, and thus yields photorealistic videos. The key idea is to progressively generate and fuse multi-scale motion with its corresponding spatial content. Our model significantly outperforms existing methods on the standard Weizmann Human Action, MUG Facial Expression, and VoxCeleb datasets, as well as our new dataset of diverse human actions with challenging and complex motion. Our code is available at https://github.com/sunxm2357/TwoStreamVAN/.



### Identification and Recognition of Rice Diseases and Pests Using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1812.01043v3
- **DOI**: 10.1016/j.biosystemseng.2020.03.020
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.01043v3)
- **Published**: 2018-12-03 19:30:07+00:00
- **Updated**: 2020-03-04 08:31:24+00:00
- **Authors**: Chowdhury Rafeed Rahman, Preetom Saha Arko, Mohammed Eunus Ali, Mohammad Ashik Iqbal Khan, Sajid Hasan Apon, Farzana Nowrin, Abu Wasif
- **Comment**: None
- **Journal**: None
- **Summary**: An accurate and timely detection of diseases and pests in rice plants can help farmers in applying timely treatment on the plants and thereby can reduce the economic losses substantially. Recent developments in deep learning based convolutional neural networks (CNN) have greatly improved the image classification accuracy. Being motivated by the success of CNNs in image classification, deep learning based approaches have been developed in this paper for detecting diseases and pests from rice plant images. The contribution of this paper is two fold: (i) State-of-the-art large scale architectures such as VGG16 and InceptionV3 have been adopted and fine tuned for detecting and recognizing rice diseases and pests. Experimental results show the effectiveness of these models with real datasets. (ii) Since large scale architectures are not suitable for mobile devices, a two-stage small CNN architecture has been proposed, and compared with the state-of-the-art memory efficient CNN architectures such as MobileNet, NasNet Mobile and SqueezeNet. Experimental results show that the proposed architecture can achieve the desired accuracy of 93.3\% with a significantly reduced model size (e.g., 99\% less size compared to that of VGG16).



### Brain Tumor Segmentation using an Ensemble of 3D U-Nets and Overall Survival Prediction using Radiomic Features
- **Arxiv ID**: http://arxiv.org/abs/1812.01049v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.01049v1)
- **Published**: 2018-12-03 19:33:36+00:00
- **Updated**: 2018-12-03 19:33:36+00:00
- **Authors**: Xue Feng, Nicholas Tustison, Craig Meyer
- **Comment**: arXiv admin note: text overlap with arXiv:1810.04274 by other authors
- **Journal**: None
- **Summary**: Accurate segmentation of different sub-regions of gliomas including peritumoral edema, necrotic core, enhancing and non-enhancing tumor core from multimodal MRI scans has important clinical relevance in diagnosis, prognosis and treatment of brain tumors. However, due to the highly heterogeneous appearance and shape, segmentation of the sub-regions is very challenging. Recent development using deep learning models has proved its effectiveness in the past several brain segmentation challenges as well as other semantic and medical image segmentation problems. Most models in brain tumor segmentation use a 2D/3D patch to predict the class label for the center voxel and variant patch sizes and scales are used to improve the model performance. However, it has low computation efficiency and also has limited receptive field. U-Net is a widely used network structure for end-to-end segmentation and can be used on the entire image or extracted patches to provide classification labels over the entire input voxels so that it is more efficient and expect to yield better performance with larger input size. Furthermore, instead of picking the best network structure, an ensemble of multiple models, trained on different dataset or different hyper-parameters, can generally improve the segmentation performance. In this study we propose to use an ensemble of 3D U-Nets with different hyper-parameters for brain tumor segmentation. Preliminary results showed effectiveness of this model. In addition, we developed a linear model for survival prediction using extracted imaging and non-imaging features, which, despite the simplicity, can effectively reduce overfitting and regression errors.



### Disentangling Latent Hands for Image Synthesis and Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1812.01002v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.01002v2)
- **Published**: 2018-12-03 19:40:41+00:00
- **Updated**: 2019-04-25 19:18:10+00:00
- **Authors**: Linlin Yang, Angela Yao
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: Hand image synthesis and pose estimation from RGB images are both highly challenging tasks due to the large discrepancy between factors of variation ranging from image background content to camera viewpoint. To better analyze these factors of variation, we propose the use of disentangled representations and a disentangled variational autoencoder (dVAE) that allows for specific sampling and inference of these factors. The derived objective from the variational lower bound as well as the proposed training strategy are highly flexible, allowing us to handle cross-modal encoders and decoders as well as semi-supervised learning scenarios. Experiments show that our dVAE can synthesize highly realistic images of the hand specifiable by both pose and image background content and also estimate 3D hand poses from RGB images with accuracy competitive with state-of-the-art on two public benchmarks.



### MS-ASL: A Large-Scale Data Set and Benchmark for Understanding American Sign Language
- **Arxiv ID**: http://arxiv.org/abs/1812.01053v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.01053v2)
- **Published**: 2018-12-03 19:41:16+00:00
- **Updated**: 2019-11-20 22:42:52+00:00
- **Authors**: Hamid Reza Vaezi Joze, Oscar Koller
- **Comment**: None
- **Journal**: British Machine Vision Conference, September 2019, Cardiff, UK
- **Summary**: Sign language recognition is a challenging and often underestimated problem comprising multi-modal articulators (handshape, orientation, movement, upper body and face) that integrate asynchronously on multiple streams. Learning powerful statistical models in such a scenario requires much data, particularly to apply recent advances of the field. However, labeled data is a scarce resource for sign language due to the enormous cost of transcribing these unwritten languages.   We propose the first real-life large-scale sign language data set comprising over 25,000 annotated videos, which we thoroughly evaluate with state-of-the-art methods from sign and related action recognition. Unlike the current state-of-the-art, the data set allows to investigate the generalization to unseen individuals (signer-independent test) in a realistic setting with over 200 signers. Previous work mostly deals with limited vocabulary tasks, while here, we cover a large class count of 1000 signs in challenging and unconstrained real-life recording conditions. We further propose I3D, known from video classifications, as a powerful and suitable architecture for sign language recognition, outperforming the current state-of-the-art by a large margin. The data set is publicly available to the community.



### A Hybrid Instance-based Transfer Learning Method
- **Arxiv ID**: http://arxiv.org/abs/1812.01063v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.01063v1)
- **Published**: 2018-12-03 20:15:05+00:00
- **Updated**: 2018-12-03 20:15:05+00:00
- **Authors**: Azin Asgarian, Parinaz Sobhani, Ji Chao Zhang, Madalin Mihailescu, Ariel Sibilia, Ahmed Bilal Ashraf, Babak Taati
- **Comment**: Machine Learning for Health (ML4H) Workshop at NeurIPS 2018
  arXiv:cs/0101200
- **Journal**: None
- **Summary**: In recent years, supervised machine learning models have demonstrated tremendous success in a variety of application domains. Despite the promising results, these successful models are data hungry and their performance relies heavily on the size of training data. However, in many healthcare applications it is difficult to collect sufficiently large training datasets. Transfer learning can help overcome this issue by transferring the knowledge from readily available datasets (source) to a new dataset (target). In this work, we propose a hybrid instance-based transfer learning method that outperforms a set of baselines including state-of-the-art instance-based transfer learning approaches. Our method uses a probabilistic weighting strategy to fuse information from the source domain to the model learned in the target domain. Our method is generic, applicable to multiple source domains, and robust with respect to negative transfer. We demonstrate the effectiveness of our approach through extensive experiments for two different applications.



### QR code denoising using parallel Hopfield networks
- **Arxiv ID**: http://arxiv.org/abs/1812.01065v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.01065v2)
- **Published**: 2018-12-03 20:24:51+00:00
- **Updated**: 2018-12-12 20:22:30+00:00
- **Authors**: Ishan Bhatnagar, Shubhang Bhatnagar
- **Comment**: 12 pages, 20 figures
- **Journal**: None
- **Summary**: We propose a novel algorithm for using Hopfield networks to denoise QR codes. Hopfield networks have mostly been used as a noise tolerant memory or to solve difficult combinatorial problems. One of the major drawbacks in their use in noise tolerant associative memory is their low capacity of storage, scaling only linearly with the number of nodes in the network. A larger capacity therefore requires a larger number of nodes, thereby reducing the speed of convergence of the network in addition to increasing hardware costs for acquiring more precise data to be fed to a larger number of nodes. Our paper proposes a new algorithm to allow the use of several Hopfield networks in parallel thereby increasing the cumulative storage capacity of the system many times as compared to a single Hopfield network. Our algorithm would also be much faster than a larger single Hopfield network with the same total capacity. This enables their use in applications like denoising QR codes, which we have demonstrated in our paper. We then test our network on a large set of QR code images with different types of noise and demonstrate that such a system of Hopfield networks can be used to denoise and recognize QR codes in real time.



### Machine Friendly Machine Learning: Interpretation of Computed Tomography Without Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1812.01068v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.01068v1)
- **Published**: 2018-12-03 20:26:39+00:00
- **Updated**: 2018-12-03 20:26:39+00:00
- **Authors**: Hyunkwang Lee, Chao Huang, Sehyo Yune, Shahein H. Tajmir, Myeongchan Kim, Synho Do
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advancements in deep learning for automated image processing and classification have accelerated many new applications for medical image analysis. However, most deep learning applications have been developed using reconstructed, human-interpretable medical images. While image reconstruction from raw sensor data is required for the creation of medical images, the reconstruction process only uses a partial representation of all the data acquired. Here we report the development of a system to directly process raw computed tomography (CT) data in sinogram-space, bypassing the intermediary step of image reconstruction. Two classification tasks were evaluated for their feasibility for sinogram-space machine learning: body region identification and intracranial hemorrhage (ICH) detection. Our proposed SinoNet performed favorably compared to conventional reconstructed image-space-based systems for both tasks, regardless of scanning geometries in terms of projections or detectors. Further, SinoNet performed significantly better when using sparsely sampled sinograms than conventional networks operating in image-space. As a result, sinogram-space algorithms could be used in field settings for binary diagnosis testing, triage, and in clinical settings where low radiation dose is desired. These findings also demonstrate another strength of deep learning where it can analyze and interpret sinograms that are virtually impossible for human experts.



### Semantic Image Inpainting Through Improved Wasserstein Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1812.01071v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.01071v1)
- **Published**: 2018-12-03 20:28:17+00:00
- **Updated**: 2018-12-03 20:28:17+00:00
- **Authors**: Patricia Vitoria, Joan Sintes, Coloma Ballester
- **Comment**: Accepted as Oral Presentation in VISAPP 2019
- **Journal**: None
- **Summary**: Image inpainting is the task of filling-in missing regions of a damaged or incomplete image. In this work we tackle this problem not only by using the available visual data but also by incorporating image semantics through the use of generative models. Our contribution is twofold: First, we learn a data latent space by training an improved version of the Wasserstein generative adversarial network, for which we incorporate a new generator and discriminator architecture. Second, the learned semantic information is combined with a new optimization loss for inpainting whose minimization infers the missing content conditioned by the available data. It takes into account powerful contextual and perceptual content inherent in the image itself. The benefits include the ability to recover large regions by accumulating semantic information even it is not fully present in the damaged image. Experiments show that the presented method obtains qualitative and quantitative top-tier results in different experimental situations and also achieves accurate photo-realism comparable to state-of-the-art works.



### Crowd Sourcing based Active Learning Approach for Parking Sign Recognition
- **Arxiv ID**: http://arxiv.org/abs/1812.01081v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.01081v1)
- **Published**: 2018-12-03 21:04:41+00:00
- **Updated**: 2018-12-03 21:04:41+00:00
- **Authors**: Humayun Irshad, Qazaleh Mirsharif, Jennifer Prendki
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning models have been used extensively to solve real-world problems in recent years. The performance of such models relies heavily on large amounts of labeled data for training. While the advances of data collection technology have enabled the acquisition of a massive volume of data, labeling the data remains an expensive and time-consuming task. Active learning techniques are being progressively adopted to accelerate the development of machine learning solutions by allowing the model to query the data they learn from. In this paper, we introduce a real-world problem, the recognition of parking signs, and present a framework that combines active learning techniques with a transfer learning approach and crowd-sourcing tools to create and train a machine learning solution to the problem. We discuss how such a framework contributes to building an accurate model in a cost-effective and fast way to solve the parking sign recognition problem in spite of the unevenness of the data associated with the fact that street-level images (such as parking signs) vary in shape, color, orientation and scale, and often appear on top of different types of background.



### ZerNet: Convolutional Neural Networks on Arbitrary Surfaces via Zernike Local Tangent Space Estimation
- **Arxiv ID**: http://arxiv.org/abs/1812.01082v3
- **DOI**: 10.1111/cgf.14012
- **Categories**: **cs.CV**, cs.CG
- **Links**: [PDF](http://arxiv.org/pdf/1812.01082v3)
- **Published**: 2018-12-03 21:11:48+00:00
- **Updated**: 2019-10-04 07:40:15+00:00
- **Authors**: Zhiyu Sun, Ethan Rooke, Jerome Charton, Yusen He, Jia Lu, Stephen Baek
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel formulation to extend CNNs to two-dimensional (2D) manifolds using orthogonal basis functions, called Zernike polynomials. In many areas, geometric features play a key role in understanding scientific phenomena. Thus, an ability to codify geometric features into a mathematical quantity can be critical. Recently, convolutional neural networks (CNNs) have demonstrated the promising capability of extracting and codifying features from visual information. However, the progress has been concentrated in computer vision applications where there exists an inherent grid-like structure. In contrast, many geometry processing problems are defined on curved surfaces, and the generalization of CNNs is not quite trivial. The difficulties are rooted in the lack of key ingredients such as the canonical grid-like representation, the notion of consistent orientation, and a compatible local topology across the domain. In this paper, we prove that the convolution of two functions can be represented as a simple dot product between Zernike polynomial coefficients; and the rotation of a convolution kernel is essentially a set of 2-by-2 rotation matrices applied to the coefficients. As such, the key contribution of this work resides in a concise but rigorous mathematical generalization of the CNN building blocks.



### Disease Detection in Weakly Annotated Volumetric Medical Images using a Convolutional LSTM Network
- **Arxiv ID**: http://arxiv.org/abs/1812.01087v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, q-bio.QM, stat.AP, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.01087v1)
- **Published**: 2018-12-03 21:32:28+00:00
- **Updated**: 2018-12-03 21:32:28+00:00
- **Authors**: Nathaniel Braman, David Beymer, Ehsan Dehghan
- **Comment**: Machine Learning for Health (ML4H) Workshop at NeurIPS 2018
  arXiv:1811.07216 Medical Imaging Meets NeurIPS Workshop at NeurIPS 2018
- **Journal**: None
- **Summary**: We explore a solution for learning disease signatures from weakly, yet easily obtainable, annotated volumetric medical imaging data by analyzing 3D volumes as a sequence of 2D images. We demonstrate the performance of our solution in the detection of emphysema in lung cancer screening low-dose CT images. Our approach utilizes convolutional long short-term memory (LSTM) to "scan" sequentially through an imaging volume for the presence of disease in a portion of scanned region. This framework allowed effective learning given only volumetric images and binary disease labels, thus enabling training from a large dataset of 6,631 un-annotated image volumes from 4,486 patients. When evaluated in a testing set of 2,163 volumes from 2,163 patients, our model distinguished emphysema with area under the receiver operating characteristic curve (AUC) of .83. This approach was found to outperform 2D convolutional neural networks (CNN) implemented with various multiple-instance learning schemes (AUC=0.69-0.76) and a 3D CNN (AUC=.77).



