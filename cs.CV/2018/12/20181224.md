# Arxiv Papers in cs.CV on 2018-12-24
### Guessing Smart: Biased Sampling for Efficient Black-Box Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/1812.09803v3
- **DOI**: 10.1109/ICCV.2019.00506
- **Categories**: **stat.ML**, cs.CR, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.09803v3)
- **Published**: 2018-12-24 00:48:31+00:00
- **Updated**: 2019-05-05 13:05:16+00:00
- **Authors**: Thomas Brunner, Frederik Diehl, Michael Truong Le, Alois Knoll
- **Comment**: For source code and videos, see
  https://github.com/ttbrunner/biased_boundary_attack
- **Journal**: None
- **Summary**: We consider adversarial examples for image classification in the black-box decision-based setting. Here, an attacker cannot access confidence scores, but only the final label. Most attacks for this scenario are either unreliable or inefficient. Focusing on the latter, we show that a specific class of attacks, Boundary Attacks, can be reinterpreted as a biased sampling framework that gains efficiency from domain knowledge. We identify three such biases, image frequency, regional masks and surrogate gradients, and evaluate their performance against an ImageNet classifier. We show that the combination of these biases outperforms the state of the art by a wide margin. We also showcase an efficient way to attack the Google Cloud Vision API, where we craft convincing perturbations with just a few hundred queries. Finally, the methods we propose have also been found to work very well against strong defenses: Our targeted attack won second place in the NeurIPS 2018 Adversarial Vision Challenge.



### Writer-Aware CNN for Parsimonious HMM-Based Offline Handwritten Chinese Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/1812.09809v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.09809v2)
- **Published**: 2018-12-24 02:38:08+00:00
- **Updated**: 2019-09-20 02:50:28+00:00
- **Authors**: Zi-Rui Wang, Jun Du, Jia-Ming Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, the hybrid convolutional neural network hidden Markov model (CNN-HMM) has been introduced for offline handwritten Chinese text recognition (HCTR) and has achieved state-of-the-art performance. However, modeling each of the large vocabulary of Chinese characters with a uniform and fixed number of hidden states requires high memory and computational costs and makes the tens of thousands of HMM state classes confusing. Another key issue of CNN-HMM for HCTR is the diversified writing style, which leads to model strain and a significant performance decline for specific writers. To address these issues, we propose a writer-aware CNN based on parsimonious HMM (WCNN-PHMM). First, PHMM is designed using a data-driven state-tying algorithm to greatly reduce the total number of HMM states, which not only yields a compact CNN by state sharing of the same or similar radicals among different Chinese characters but also improves the recognition accuracy due to the more accurate modeling of tied states and the lower confusion among them. Second, WCNN integrates each convolutional layer with one adaptive layer fed by a writer-dependent vector, namely, the writer code, to extract the irrelevant variability in writer information to improve recognition performance. The parameters of writer-adaptive layers are jointly optimized with other network parameters in the training stage, while a multiple-pass decoding strategy is adopted to learn the writer code and generate recognition results. Validated on the ICDAR 2013 competition of CASIA-HWDB database, the more compact WCNN-PHMM of a 7360-class vocabulary can achieve a relative character error rate (CER) reduction of 16.6% over the conventional CNN-HMM without considering language modeling. By adopting a powerful hybrid language model (N-gram language model and recurrent neural network language model), the CER of WCNN-PHMM is reduced to 3.17%.



### Precision Highway for Ultra Low-Precision Quantization
- **Arxiv ID**: http://arxiv.org/abs/1812.09818v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.09818v1)
- **Published**: 2018-12-24 03:29:36+00:00
- **Updated**: 2018-12-24 03:29:36+00:00
- **Authors**: Eunhyeok Park, Dongyoung Kim, Sungjoo Yoo, Peter Vajda
- **Comment**: None
- **Journal**: None
- **Summary**: Neural network quantization has an inherent problem called accumulated quantization error, which is the key obstacle towards ultra-low precision, e.g., 2- or 3-bit precision. To resolve this problem, we propose precision highway, which forms an end-to-end high-precision information flow while performing the ultra low-precision computation. First, we describe how the precision highway reduce the accumulated quantization error in both convolutional and recurrent neural networks. We also provide the quantitative analysis of the benefit of precision highway and evaluate the overhead on the state-of-the-art hardware accelerator. In the experiments, our proposed method outperforms the best existing quantization methods while offering 3-bit weight/activation quantization with no accuracy loss and 2-bit quantization with a 2.45 % top-1 accuracy loss in ResNet-50. We also report that the proposed method significantly outperforms the existing method in the 2-bit quantization of an LSTM for language modeling.



### Texture Deformation Based Generative Adversarial Networks for Face Editing
- **Arxiv ID**: http://arxiv.org/abs/1812.09832v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.09832v1)
- **Published**: 2018-12-24 05:03:58+00:00
- **Updated**: 2018-12-24 05:03:58+00:00
- **Authors**: WenTing Chen, Xinpeng Xie, Xi Jia, Linlin Shen
- **Comment**: 10 pages, 6 figures, 3 tables
- **Journal**: None
- **Summary**: Despite the significant success in image-to-image translation and latent representation based facial attribute editing and expression synthesis, the existing approaches still have limitations in the sharpness of details, distinct image translation and identity preservation. To address these issues, we propose a Texture Deformation Based GAN, namely TDB-GAN, to disentangle texture from original image and transfers domains based on the extracted texture. The approach utilizes the texture to transfer facial attributes and expressions without the consideration of the object pose. This leads to shaper details and more distinct visual effect of the synthesized faces. In addition, it brings the faster convergence during training. The effectiveness of the proposed method is validated through extensive ablation studies. We also evaluate our approach qualitatively and quantitatively on facial attribute and facial expression synthesis. The results on both the CelebA and RaFD datasets suggest that Texture Deformation Based GAN achieves better performance.



### Holistic Decomposition Convolution for Effective Semantic Segmentation of 3D MR Images
- **Arxiv ID**: http://arxiv.org/abs/1812.09834v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.09834v1)
- **Published**: 2018-12-24 05:18:18+00:00
- **Updated**: 2018-12-24 05:18:18+00:00
- **Authors**: Guodong Zeng, Guoyan Zheng
- **Comment**: 12 pages, 6 figures
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) have achieved state-of-the-art performance in many different 2D medical image analysis tasks. In clinical practice, however, a large part of the medical imaging data available is in 3D. This has motivated the development of 3D CNNs for volumetric image segmentation in order to benefit from more spatial context. Due to GPU memory restrictions caused by moving to fully 3D, state-of-the-art methods depend on subvolume/patch processing and the size of the input patch is usually small, limiting the incorporation of larger context information for a better performance. In this paper, we propose a novel Holistic Decomposition Convolution (HDC), for an effective and efficient semantic segmentation of volumetric images. HDC consists of a periodic down-shuffling operation followed by a conventional 3D convolution. HDC has the advantage of significantly reducing the size of the data for sub-sequential processing while using all the information available in the input irrespective of the down-shuffling factors. Results obtained from comprehensive experiments conducted on hip T1 MR images and intervertebral disc T2 MR images demonstrate the efficacy of the present approach.



### Perceptual deep depth super-resolution
- **Arxiv ID**: http://arxiv.org/abs/1812.09874v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.09874v3)
- **Published**: 2018-12-24 09:43:25+00:00
- **Updated**: 2019-09-09 14:12:39+00:00
- **Authors**: Oleg Voynov, Alexey Artemov, Vage Egiazarian, Alexander Notchenko, Gleb Bobrovskikh, Denis Zorin, Evgeny Burnaev
- **Comment**: 26 pages
- **Journal**: None
- **Summary**: RGBD images, combining high-resolution color and lower-resolution depth from various types of depth sensors, are increasingly common. One can significantly improve the resolution of depth maps by taking advantage of color information; deep learning methods make combining color and depth information particularly easy. However, fusing these two sources of data may lead to a variety of artifacts. If depth maps are used to reconstruct 3D shapes, e.g., for virtual reality applications, the visual quality of upsampled images is particularly important. The main idea of our approach is to measure the quality of depth map upsampling using renderings of resulting 3D surfaces. We demonstrate that a simple visual appearance-based loss, when used with either a trained CNN or simply a deep prior, yields significantly improved 3D shapes, as measured by a number of existing perceptual metrics. We compare this approach with a number of existing optimization and learning-based techniques.



### Latent Filter Scaling for Multimodal Unsupervised Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/1812.09877v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.09877v3)
- **Published**: 2018-12-24 10:07:50+00:00
- **Updated**: 2019-04-07 07:47:18+00:00
- **Authors**: Yazeed Alharbi, Neil Smith, Peter Wonka
- **Comment**: Accepted to CVPR2019
- **Journal**: None
- **Summary**: In multimodal unsupervised image-to-image translation tasks, the goal is to translate an image from the source domain to many images in the target domain. We present a simple method that produces higher quality images than current state-of-the-art while maintaining the same amount of multimodal diversity. Previous methods follow the unconditional approach of trying to map the latent code directly to a full-size image. This leads to complicated network architectures with several introduced hyperparameters to tune. By treating the latent code as a modifier of the convolutional filters, we produce multimodal output while maintaining the traditional Generative Adversarial Network (GAN) loss and without additional hyperparameters. The only tuning required by our method controls the tradeoff between variability and quality of generated images. Furthermore, we achieve disentanglement between source domain content and target domain style for free as a by-product of our formulation. We perform qualitative and quantitative experiments showing the advantages of our method compared with the state-of-the art on multiple benchmark image-to-image translation datasets.



### Coupled Analysis Dictionary Learning to inductively learn inversion: Application to real-time reconstruction of Biomedical signals
- **Arxiv ID**: http://arxiv.org/abs/1812.09878v1
- **DOI**: 10.1109/IJCNN.2018.8489148
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.09878v1)
- **Published**: 2018-12-24 10:17:20+00:00
- **Updated**: 2018-12-24 10:17:20+00:00
- **Authors**: Kavya Gupta, Brojeshwar Bhowmick, Angshul Majumdar
- **Comment**: 2018 International Joint Conference on Neural Networks (IJCNN)(July
  2018)
- **Journal**: None
- **Summary**: This work addresses the problem of reconstructing biomedical signals from their lower dimensional projections. Traditionally Compressed Sensing (CS) based techniques have been employed for this task. These are transductive inversion processes, the problem with these approaches is that the inversion is time-consuming and hence not suitable for real-time applications. With the recent advent of deep learning, Stacked Sparse Denoising Autoencoder (SSDAE) has been used for learning inversion in an inductive setup. The training period for inductive learning is large but is very fast during application -- capable of real-time speed. This work proposes a new approach for inductive learning of the inversion process. It is based on Coupled Analysis Dictionary Learning. Results on Biomedical signal reconstruction show that our proposed approach is very fast and yields result far better than CS and SSDAE.



### Motion Blur removal via Coupled Autoencoder
- **Arxiv ID**: http://arxiv.org/abs/1812.09888v1
- **DOI**: 10.1109/ICIP.2017.8296327
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.09888v1)
- **Published**: 2018-12-24 10:58:30+00:00
- **Updated**: 2018-12-24 10:58:30+00:00
- **Authors**: Kavya Gupta, Brojeshwar Bhowmick, Angshul Majumdar
- **Comment**: None
- **Journal**: 2017 IEEE International Conference on Image Processing
  (ICIP)(pages - 480-484, September 2017)
- **Summary**: In this paper a joint optimization technique has been proposed for coupled autoencoder which learns the autoencoder weights and coupling map (between source and target) simultaneously. The technique is applicable to any transfer learning problem. In this work, we propose a new formulation that recasts deblurring as a transfer learning problem, it is solved using the proposed coupled autoencoder. The proposed technique can operate on-the-fly, since it does not require solving any costly inverse problem. Experiments have been carried out on state-of-the-art techniques, our method yields better quality images in shorter operating times.



### Learning a Disentangled Embedding for Monocular 3D Shape Retrieval and Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1812.09899v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.09899v2)
- **Published**: 2018-12-24 11:44:36+00:00
- **Updated**: 2019-03-26 18:19:43+00:00
- **Authors**: Kyaw Zaw Lin, Weipeng Xu, Qianru Sun, Christian Theobalt, Tat-Seng Chua
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel approach to jointly perform 3D shape retrieval and pose estimation from monocular images.In order to make the method robust to real-world image variations, e.g. complex textures and backgrounds, we learn an embedding space from 3D data that only includes the relevant information, namely the shape and pose. Our approach explicitly disentangles a shape vector and a pose vector, which alleviates both pose bias for 3D shape retrieval and categorical bias for pose estimation. We then train a CNN to map the images to this embedding space, and then retrieve the closest 3D shape from the database and estimate the 6D pose of the object. Our method achieves 10.3 median error for pose estimation and 0.592 top-1-accuracy for category agnostic 3D object retrieval on the Pascal3D+ dataset, outperforming the previous state-of-the-art methods on both tasks.



### TextNet: Irregular Text Reading from Images with an End-to-End Trainable Network
- **Arxiv ID**: http://arxiv.org/abs/1812.09900v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1812.09900v1)
- **Published**: 2018-12-24 11:44:55+00:00
- **Updated**: 2018-12-24 11:44:55+00:00
- **Authors**: Yipeng Sun, Chengquan Zhang, Zuming Huang, Jiaming Liu, Junyu Han, Errui Ding
- **Comment**: Asian conference on computer vision, 2018, oral presentation
- **Journal**: None
- **Summary**: Reading text from images remains challenging due to multi-orientation, perspective distortion and especially the curved nature of irregular text. Most of existing approaches attempt to solve the problem in two or multiple stages, which is considered to be the bottleneck to optimize the overall performance. To address this issue, we propose an end-to-end trainable network architecture, named TextNet, which is able to simultaneously localize and recognize irregular text from images. Specifically, we develop a scale-aware attention mechanism to learn multi-scale image features as a backbone network, sharing fully convolutional features and computation for localization and recognition. In text detection branch, we directly generate text proposals in quadrangles, covering oriented, perspective and curved text regions. To preserve text features for recognition, we introduce a perspective RoI transform layer, which can align quadrangle proposals into small feature maps. Furthermore, in order to extract effective features for recognition, we propose to encode the aligned RoI features by RNN into context information, combining spatial attention mechanism to generate text sequences. This overall pipeline is capable of handling both regular and irregular cases. Finally, text localization and recognition tasks can be jointly trained in an end-to-end fashion with designed multi-task loss. Experiments on standard benchmarks show that the proposed TextNet can achieve state-of-the-art performance, and outperform existing approaches on irregular datasets by a large margin.



### Adaptive Confidence Smoothing for Generalized Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1812.09903v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.09903v3)
- **Published**: 2018-12-24 11:54:41+00:00
- **Updated**: 2019-10-07 16:01:33+00:00
- **Authors**: Yuval Atzmon, Gal Chechik
- **Comment**: (1) Accepted to CVPR 2019. (2) Previous title was "Domain-Aware
  Generalized Zero-Shot Learning". (3) This arxiv version is as the CVPR final
  version with the following modifications: (a) corrected typos found in Table
  3 (b) updated "Related Work" with [52, 10, 20] (c) add a paragraph to the
  abstract (d) add a probabilistic explanation for the smoothing term
- **Journal**: None
- **Summary**: Generalized zero-shot learning (GZSL) is the problem of learning a classifier where some classes have samples and others are learned from side information, like semantic attributes or text description, in a zero-shot learning fashion (ZSL). Training a single model that operates in these two regimes simultaneously is challenging. Here we describe a probabilistic approach that breaks the model into three modular components, and then combines them in a consistent way. Specifically, our model consists of three classifiers: A "gating" model that makes soft decisions if a sample is from a "seen" class, and two experts: a ZSL expert, and an expert model for seen classes.   We address two main difficulties in this approach: How to provide an accurate estimate of the gating probability without any training samples for unseen classes; and how to use expert predictions when it observes samples outside of its domain. The key insight to our approach is to pass information between the three models to improve each one's accuracy, while maintaining the modular structure. We test our approach, adaptive confidence smoothing (COSMO), on four standard GZSL benchmark datasets and find that it largely outperforms state-of-the-art GZSL models. COSMO is also the first model that closes the gap and surpasses the performance of generative models for GZSL, even-though it is a light-weight model that is much easier to train and tune.   Notably, COSMO offers a new view for developing zero-shot models. Thanks to COSMO's modular structure, instead of trying to perform well both on seen and on unseen classes, models can focus on accurate classification of unseen classes, and later consider seen class models.



### Multiple Sclerosis Lesion Inpainting Using Non-Local Partial Convolutions
- **Arxiv ID**: http://arxiv.org/abs/1901.00055v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.00055v3)
- **Published**: 2018-12-24 12:56:56+00:00
- **Updated**: 2019-09-06 07:24:41+00:00
- **Authors**: Hao Xiong, Chaoyue Wang, Dacheng Tao, Michael Barnett, Chenyu Wang
- **Comment**: We make significant changes to the paper and do not plan to submit
  current version to arXiv until it is accepted
- **Journal**: None
- **Summary**: Multiple sclerosis (MS) is an inflammatory demyelinating disease of the central nervous system (CNS) that results in focal injury to the grey and white matter. The presence of white matter lesions biases morphometric analyses such as registration, individual longitudinal measurements and tissue segmentation for brain volume measurements. Lesion-inpainting with intensities derived from surrounding healthy tissue represents one approach to alleviate such problems. However, existing methods inpaint lesions based on texture information derived from local surrounding tissue, often leading to inconsistent inpainting and the generation of artifacts such as intensity discrepancy and blurriness. Based on these observations, we propose non-local partial convolutions (NLPC) that integrates a Unet-like network with the non-local module. The non-local module is exploited to capture long range dependencies between the lesion area and remaining normal-appearing brain regions. Then, the lesion area is filled by referring to normal-appearing regions with more similar features. This method generates inpainted regions that appear more realistic and natural. Our quantitative experimental results also demonstrate superiority of this technique of existing state-of-the-art inpainting methods.



### Image-to-Image Translation via Group-wise Deep Whitening-and-Coloring Transformation
- **Arxiv ID**: http://arxiv.org/abs/1812.09912v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.09912v2)
- **Published**: 2018-12-24 13:03:24+00:00
- **Updated**: 2019-06-09 08:58:05+00:00
- **Authors**: Wonwoong Cho, Sungha Choi, David Keetae Park, Inkyu Shin, Jaegul Choo
- **Comment**: CVPR 2019 (oral)
- **Journal**: None
- **Summary**: Recently, unsupervised exemplar-based image-to-image translation, conditioned on a given exemplar without the paired data, has accomplished substantial advancements. In order to transfer the information from an exemplar to an input image, existing methods often use a normalization technique, e.g., adaptive instance normalization, that controls the channel-wise statistics of an input activation map at a particular layer, such as the mean and the variance. Meanwhile, style transfer approaches similar task to image translation by nature, demonstrated superior performance by using the higher-order statistics such as covariance among channels in representing a style. In detail, it works via whitening (given a zero-mean input feature, transforming its covariance matrix into the identity). followed by coloring (changing the covariance matrix of the whitened feature to those of the style feature). However, applying this approach in image translation is computationally intensive and error-prone due to the expensive time complexity and its non-trivial backpropagation. In response, this paper proposes an end-to-end approach tailored for image translation that efficiently approximates this transformation with our novel regularization methods. We further extend our approach to a group-wise form for memory and time efficiency as well as image quality. Extensive qualitative and quantitative experiments demonstrate that our proposed method is fast, both in training and inference, and highly effective in reflecting the style of an exemplar. Finally, our code is available at https://github.com/WonwoongCho/GDWCT.



### Improving MMD-GAN Training with Repulsive Loss Function
- **Arxiv ID**: http://arxiv.org/abs/1812.09916v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.09916v4)
- **Published**: 2018-12-24 13:23:18+00:00
- **Updated**: 2019-02-08 06:28:35+00:00
- **Authors**: Wei Wang, Yuan Sun, Saman Halgamuge
- **Comment**: Published as a conference paper at ICLR 2019
- **Journal**: None
- **Summary**: Generative adversarial nets (GANs) are widely used to learn the data sampling process and their performance may heavily depend on the loss functions, given a limited computational budget. This study revisits MMD-GAN that uses the maximum mean discrepancy (MMD) as the loss function for GAN and makes two contributions. First, we argue that the existing MMD loss function may discourage the learning of fine details in data as it attempts to contract the discriminator outputs of real data. To address this issue, we propose a repulsive loss function to actively learn the difference among the real data by simply rearranging the terms in MMD. Second, inspired by the hinge loss, we propose a bounded Gaussian kernel to stabilize the training of MMD-GAN with the repulsive loss function. The proposed methods are applied to the unsupervised image generation tasks on CIFAR-10, STL-10, CelebA, and LSUN bedroom datasets. Results show that the repulsive loss function significantly improves over the MMD loss at no additional computational cost and outperforms other representative loss functions. The proposed methods achieve an FID score of 16.21 on the CIFAR-10 dataset using a single DCGAN network and spectral normalization.



### Dynamic Runtime Feature Map Pruning
- **Arxiv ID**: http://arxiv.org/abs/1812.09922v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1812.09922v2)
- **Published**: 2018-12-24 13:54:50+00:00
- **Updated**: 2019-04-02 02:22:22+00:00
- **Authors**: Tailin Liang, Lei Wang, Shaobo Shi, John Glossner
- **Comment**: None
- **Journal**: None
- **Summary**: High bandwidth requirements are an obstacle for accelerating the training and inference of deep neural networks. Most previous research focuses on reducing the size of kernel maps for inference. We analyze parameter sparsity of six popular convolutional neural networks - AlexNet, MobileNet, ResNet-50, SqueezeNet, TinyNet, and VGG16. Of the networks considered, those using ReLU (AlexNet, SqueezeNet, VGG16) contain a high percentage of 0-valued parameters and can be statically pruned. Networks with Non-ReLU activation functions in some cases may not contain any 0-valued parameters (ResNet-50, TinyNet). We also investigate runtime feature map usage and find that input feature maps comprise the majority of bandwidth requirements when depth-wise convolution and point-wise convolutions used. We introduce dynamic runtime pruning of feature maps and show that 10% of dynamic feature map execution can be removed without loss of accuracy. We then extend dynamic pruning to allow for values within an epsilon of zero and show a further 5% reduction of feature map loading with a 1% loss of accuracy in top-1.



### Dual Principal Component Pursuit: Probability Analysis and Efficient Algorithms
- **Arxiv ID**: http://arxiv.org/abs/1812.09924v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/1812.09924v1)
- **Published**: 2018-12-24 14:07:18+00:00
- **Updated**: 2018-12-24 14:07:18+00:00
- **Authors**: Zhihui Zhu, Yifan Wang, Daniel P. Robinson, Daniel Q. Naiman, Rene Vidal, Manolis C. Tsakiris
- **Comment**: None
- **Journal**: None
- **Summary**: Recent methods for learning a linear subspace from data corrupted by outliers are based on convex $\ell_1$ and nuclear norm optimization and require the dimension of the subspace and the number of outliers to be sufficiently small. In sharp contrast, the recently proposed Dual Principal Component Pursuit (DPCP) method can provably handle subspaces of high dimension by solving a non-convex $\ell_1$ optimization problem on the sphere. However, its geometric analysis is based on quantities that are difficult to interpret and are not amenable to statistical analysis. In this paper we provide a refined geometric analysis and a new statistical analysis that show that DPCP can tolerate as many outliers as the square of the number of inliers, thus improving upon other provably correct robust PCA methods. We also propose a scalable Projected Sub-Gradient Method method (DPCP-PSGM) for solving the DPCP problem and show it admits linear convergence even though the underlying optimization problem is non-convex and non-smooth. Experiments on road plane detection from 3D point cloud data demonstrate that DPCP-PSGM can be more efficient than the traditional RANSAC algorithm, which is one of the most popular methods for such computer vision applications.



### Color Image Enhancement Method Based on Weighted Image Guided Filtering
- **Arxiv ID**: http://arxiv.org/abs/1812.09930v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.09930v1)
- **Published**: 2018-12-24 14:46:08+00:00
- **Updated**: 2018-12-24 14:46:08+00:00
- **Authors**: Qi Mu, Yanyan Wei, Zhanli Li
- **Comment**: 15 pages
- **Journal**: None
- **Summary**: A novel color image enhancement method is proposed based on Retinex to enhance color images under non-uniform illumination or poor visibility conditions. Different from the conventional Retinex algorithms, the Weighted Guided Image Filter is used as a surround function instead of the Gaussian filter to estimate the background illumination, which can overcome the drawbacks of local blur and halo artifact that may appear by Gaussian filter. To avoid color distortion, the image is converted to the HSI color model, and only the intensity channel is enhanced. Then a linear color restoration algorithm is adopted to convert the enhanced intensity image back to the RGB color model, which ensures the hue is constant and undistorted. Experimental results show that the proposed method is effective to enhance both color and gray images with low exposure and non-uniform illumination, resulting in better visual quality than traditional method. At the same time, the objective evaluation indicators are also superior to the conventional methods. In addition, the efficiency of the proposed method is also improved thanks to the linear color restoration algorithm.



### A Curriculum Domain Adaptation Approach to the Semantic Segmentation of Urban Scenes
- **Arxiv ID**: http://arxiv.org/abs/1812.09953v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.09953v3)
- **Published**: 2018-12-24 16:50:49+00:00
- **Updated**: 2019-01-10 01:11:19+00:00
- **Authors**: Yang Zhang, Philip David, Hassan Foroosh, Boqing Gong
- **Comment**: This is the journal version of arXiv:1707.09465
- **Journal**: None
- **Summary**: During the last half decade, convolutional neural networks (CNNs) have triumphed over semantic segmentation, which is one of the core tasks in many applications such as autonomous driving and augmented reality. However, to train CNNs requires a considerable amount of data, which is difficult to collect and laborious to annotate. Recent advances in computer graphics make it possible to train CNNs on photo-realistic synthetic imagery with computer-generated annotations. Despite this, the domain mismatch between the real images and the synthetic data hinders the models' performance. Hence, we propose a curriculum-style learning approach to minimizing the domain gap in urban scene semantic segmentation. The curriculum domain adaptation solves easy tasks first to infer necessary properties about the target domain; in particular, the first task is to learn global label distributions over images and local distributions over landmark superpixels. These are easy to estimate because images of urban scenes have strong idiosyncrasies (e.g., the size and spatial relations of buildings, streets, cars, etc.). We then train a segmentation network, while regularizing its predictions in the target domain to follow those inferred properties. In experiments, our method outperforms the baselines on two datasets and two backbone networks. We also report extensive ablation studies about our approach.



