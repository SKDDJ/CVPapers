# Arxiv Papers in cs.CV on 2018-12-14
### Pay Voice: Point of Sale Recognition for Visually Impaired People
- **Arxiv ID**: http://arxiv.org/abs/1812.05740v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.05740v1)
- **Published**: 2018-12-14 00:04:32+00:00
- **Updated**: 2018-12-14 00:04:32+00:00
- **Authors**: Guilherme Folego, Filipe Costa, Bruno Costa, Alan Godoy, Luiz Pita
- **Comment**: None
- **Journal**: XIV Workshop de Visao Computacional (WVC 2018)
- **Summary**: Millions of visually impaired people depend on relatives and friends to perform their everyday tasks. One relevant step towards self-sufficiency is to provide them with means to verify the value and operation presented in payment machines. In this work, we developed and released a smartphone application, named Pay Voice, that uses image processing, optical character recognition (OCR) and voice synthesis to recognize the value and operation presented in POS and PIN pad machines, and thus informing the user with auditive and visual feedback. The proposed approach presented significant results for value and operation recognition, especially for POS, due to the higher display quality. Importantly, we achieved the key performance indicators, namely, more than 80% of accuracy in a real-world scenario, and less than $5$ seconds of processing time for recognition. Pay Voice is publicly available on Google Play and App Store for free.



### Action Machine: Rethinking Action Recognition in Trimmed Videos
- **Arxiv ID**: http://arxiv.org/abs/1812.05770v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.05770v2)
- **Published**: 2018-12-14 03:43:54+00:00
- **Updated**: 2018-12-17 08:12:06+00:00
- **Authors**: Jiagang Zhu, Wei Zou, Liang Xu, Yiming Hu, Zheng Zhu, Manyu Chang, Junjie Huang, Guan Huang, Dalong Du
- **Comment**: None
- **Journal**: None
- **Summary**: Existing methods in video action recognition mostly do not distinguish human body from the environment and easily overfit the scenes and objects. In this work, we present a conceptually simple, general and high-performance framework for action recognition in trimmed videos, aiming at person-centric modeling. The method, called Action Machine, takes as inputs the videos cropped by person bounding boxes. It extends the Inflated 3D ConvNet (I3D) by adding a branch for human pose estimation and a 2D CNN for pose-based action recognition, being fast to train and test. Action Machine can benefit from the multi-task training of action recognition and pose estimation, the fusion of predictions from RGB images and poses. On NTU RGB-D, Action Machine achieves the state-of-the-art performance with top-1 accuracies of 97.2% and 94.3% on cross-view and cross-subject respectively. Action Machine also achieves competitive performance on another three smaller action recognition datasets: Northwestern UCLA Multiview Action3D, MSR Daily Activity3D and UTD-MHAD. Code will be made available.



### PointPillars: Fast Encoders for Object Detection from Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1812.05784v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.05784v2)
- **Published**: 2018-12-14 05:15:08+00:00
- **Updated**: 2019-05-07 02:00:24+00:00
- **Authors**: Alex H. Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, Oscar Beijbom
- **Comment**: 9 pages. v1 is initial submission to CVPR 2019. v2 is final version
  accepted for publication at CVPR 2019
- **Journal**: None
- **Summary**: Object detection in point clouds is an important aspect of many robotics applications such as autonomous driving. In this paper we consider the problem of encoding a point cloud into a format appropriate for a downstream detection pipeline. Recent literature suggests two types of encoders; fixed encoders tend to be fast but sacrifice accuracy, while encoders that are learned from data are more accurate, but slower. In this work we propose PointPillars, a novel encoder which utilizes PointNets to learn a representation of point clouds organized in vertical columns (pillars). While the encoded features can be used with any standard 2D convolutional detection architecture, we further propose a lean downstream network. Extensive experimentation shows that PointPillars outperforms previous encoders with respect to both speed and accuracy by a large margin. Despite only using lidar, our full detection pipeline significantly outperforms the state of the art, even among fusion methods, with respect to both the 3D and bird's eye view KITTI benchmarks. This detection performance is achieved while running at 62 Hz: a 2 - 4 fold runtime improvement. A faster version of our method matches the state of the art at 105 Hz. These benchmarks suggest that PointPillars is an appropriate encoding for object detection in point clouds.



### Deep Active Learning for Video-based Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1812.05785v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.05785v1)
- **Published**: 2018-12-14 05:16:03+00:00
- **Updated**: 2018-12-14 05:16:03+00:00
- **Authors**: Menglin Wang, Baisheng Lai, Zhongming Jin, Xiaojin Gong, Jianqiang Huang, Xiansheng Hua
- **Comment**: None
- **Journal**: None
- **Summary**: It is prohibitively expensive to annotate a large-scale video-based person re-identification (re-ID) dataset, which makes fully supervised methods inapplicable to real-world deployment. How to maximally reduce the annotation cost while retaining the re-ID performance becomes an interesting problem. In this paper, we address this problem by integrating an active learning scheme into a deep learning framework. Noticing that the truly matched tracklet-pairs, also denoted as true positives (TP), are the most informative samples for our re-ID model, we propose a sampling criterion to choose the most TP-likely tracklet-pairs for annotation. A view-aware sampling strategy considering view-specific biases is designed to facilitate candidate selection, followed by an adaptive resampling step to leave out the selected candidates that are unnecessary to annotate. Our method learns the re-ID model and updates the annotation set iteratively. The re-ID model is supervised by the tracklets' pesudo labels that are initialized by treating each tracklet as a distinct class. With the gained annotations of the actively selected candidates, the tracklets' pesudo labels are updated by label merging and further used to re-train our re-ID model. While being simple, the proposed method demonstrates its effectiveness on three video-based person re-ID datasets. Experimental results show that less than 3\% pairwise annotations are needed for our method to reach comparable performance with the fully-supervised setting.



### AU R-CNN: Encoding Expert Prior Knowledge into R-CNN for Action Unit Detection
- **Arxiv ID**: http://arxiv.org/abs/1812.05788v2
- **DOI**: 10.1016/j.neucom.2019.03.082
- **Categories**: **cs.CV**, 2010
- **Links**: [PDF](http://arxiv.org/pdf/1812.05788v2)
- **Published**: 2018-12-14 05:23:49+00:00
- **Updated**: 2019-08-25 11:56:47+00:00
- **Authors**: Chen Ma, Li Chen, Junhai Yong
- **Comment**: 14 pages,10 figures, published on Neurocomputing
- **Journal**: Neurocomputing 355 (2019) 35-47
- **Summary**: Detecting action units (AUs) on human faces is challenging because various AUs make subtle facial appearance change over various regions at different scales. Current works have attempted to recognize AUs by emphasizing important regions. However, the incorporation of expert prior knowledge into region definition remains under-exploited, and current AU detection approaches do not use regional convolutional neural networks (R-CNN) with expert prior knowledge to directly focus on AU-related regions adaptively. By incorporating expert prior knowledge, we propose a novel R-CNN based model named AU R-CNN. The proposed solution offers two main contributions: (1) AU R-CNN directly observes different facial regions, where various AUs are located. Specifically, we define an AU partition rule which encodes the expert prior knowledge into the region definition and RoI-level label definition. This design produces considerably better detection performance than existing approaches. (2) We integrate various dynamic models (including convolutional long short-term memory, two stream network, conditional random field, and temporal action localization network) into AU R-CNN and then investigate and analyze the reason behind the performance of dynamic models. Experiment results demonstrate that \textit{only} static RGB image information and no optical flow-based AU R-CNN surpasses the one fused with dynamic models. AU R-CNN is also superior to traditional CNNs that use the same backbone on varying image resolutions. State-of-the-art recognition performance of AU detection is achieved. The complete network is end-to-end trainable. Experiments on BP4D and DISFA datasets show the effectiveness of our approach. The implementation code is available online.



### Pyramid Network with Online Hard Example Mining for Accurate Left Atrium Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1812.05802v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.05802v1)
- **Published**: 2018-12-14 07:28:07+00:00
- **Updated**: 2018-12-14 07:28:07+00:00
- **Authors**: Cheng Bian, Xin Yang, Jianqiang Ma, Shen Zheng, Yu-An Liu, Reza Nezafat, Pheng-Ann Heng, Yefeng Zheng
- **Comment**: 9 pages, 4 figures. MICCAI Workshop on STACOM 2018
- **Journal**: None
- **Summary**: Accurately segmenting left atrium in MR volume can benefit the ablation procedure of atrial fibrillation. Traditional automated solutions often fail in relieving experts from the labor-intensive manual labeling. In this paper, we propose a deep neural network based solution for automated left atrium segmentation in gadolinium-enhanced MR volumes with promising performance. We firstly argue that, for this volumetric segmentation task, networks in 2D fashion can present great superiorities in time efficiency and segmentation accuracy than networks with 3D fashion. Considering the highly varying shape of atrium and the branchy structure of associated pulmonary veins, we propose to adopt a pyramid module to collect semantic cues in feature maps from multiple scales for fine-grained segmentation. Also, to promote our network in classifying the hard examples, we propose an Online Hard Negative Example Mining strategy to identify voxels in slices with low classification certainties and penalize the wrong predictions on them. Finally, we devise a competitive training scheme to further boost the generalization ability of networks. Extensively verified on 20 testing volumes, our proposed framework achieves an average Dice of 92.83% in segmenting the left atria and pulmonary veins.



### A Self-Supervised Bootstrap Method for Single-Image 3D Face Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1812.05806v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.05806v2)
- **Published**: 2018-12-14 07:46:02+00:00
- **Updated**: 2018-12-17 19:22:22+00:00
- **Authors**: Yifan Xing, Rahul Tewari, Paulo R. S. Mendonca
- **Comment**: None
- **Journal**: None
- **Summary**: State-of-the-art methods for 3D reconstruction of faces from a single image require 2D-3D pairs of ground-truth data for supervision. Such data is costly to acquire, and most datasets available in the literature are restricted to pairs for which the input 2D images depict faces in a near fronto-parallel pose. Therefore, many data-driven methods for single-image 3D facial reconstruction perform poorly on profile and near-profile faces. We propose a method to improve the performance of single-image 3D facial reconstruction networks by utilizing the network to synthesize its own training data for fine-tuning, comprising: (i) single-image 3D reconstruction of faces in near-frontal images without ground-truth 3D shape; (ii) application of a rigid-body transformation to the reconstructed face model; (iii) rendering of the face model from new viewpoints; and (iv) use of the rendered image and corresponding 3D reconstruction as additional data for supervised fine-tuning. The new 2D-3D pairs thus produced have the same high-quality observed for near fronto-parallel reconstructions, thereby nudging the network towards more uniform performance as a function of the viewing angle of input faces. Application of the proposed technique to the fine-tuning of a state-of-the-art single-image 3D-reconstruction network for faces demonstrates the usefulness of the method, with particularly significant gains for profile or near-profile views.



### Combating Uncertainty with Novel Losses for Automatic Left Atrium Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1812.05807v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.05807v1)
- **Published**: 2018-12-14 07:47:10+00:00
- **Updated**: 2018-12-14 07:47:10+00:00
- **Authors**: Xin Yang, Na Wang, Yi Wang, Xu Wang, Reza Nezafat, Dong Ni, Pheng-Ann Heng
- **Comment**: 9 pages, 4 figures. MICCAI Workshop on STACOM 2018
- **Journal**: None
- **Summary**: Segmenting left atrium in MR volume holds great potentials in promoting the treatment of atrial fibrillation. However, the varying anatomies, artifacts and low contrasts among tissues hinder the advance of both manual and automated solutions. In this paper, we propose a fully-automated framework to segment left atrium in gadolinium-enhanced MR volumes. The region of left atrium is firstly automatically localized by a detection module. Our framework then originates with a customized 3D deep neural network to fully explore the spatial dependency in the region for segmentation. To alleviate the risk of low training efficiency and potential overfitting, we enhance our deep network with the transfer learning and deep supervision strategy. Main contribution of our network design lies in the composite loss function to combat the boundary ambiguity and hard examples. We firstly adopt the Overlap loss to encourage network reduce the overlap between the foreground and background and thus sharpen the predictions on boundary. We then propose a novel Focal Positive loss to guide the learning of voxel-specific threshold and emphasize the foreground to improve classification sensitivity. Further improvement is obtained with an recursive training scheme. With ablation studies, all the introduced modules prove to be effective. The proposed framework achieves an average Dice of 92.24 in segmenting left atrium with pulmonary veins on 20 testing volumes.



### ESIR: End-to-end Scene Text Recognition via Iterative Image Rectification
- **Arxiv ID**: http://arxiv.org/abs/1812.05824v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.05824v3)
- **Published**: 2018-12-14 08:32:36+00:00
- **Updated**: 2019-04-02 09:13:15+00:00
- **Authors**: Fangneng Zhan, Shijian Lu
- **Comment**: Accepted to CVPR 2019
- **Journal**: None
- **Summary**: Automated recognition of texts in scenes has been a research challenge for years, largely due to the arbitrary variation of text appearances in perspective distortion, text line curvature, text styles and different types of imaging artifacts. The recent deep networks are capable of learning robust representations with respect to imaging artifacts and text style changes, but still face various problems while dealing with scene texts with perspective and curvature distortions. This paper presents an end-to-end trainable scene text recognition system (ESIR) that iteratively removes perspective distortion and text line curvature as driven by better scene text recognition performance. An innovative rectification network is developed which employs a novel line-fitting transformation to estimate the pose of text lines in scenes. In addition, an iterative rectification pipeline is developed where scene text distortions are corrected iteratively towards a fronto-parallel view. The ESIR is also robust to parameter initialization and the training needs only scene text images and word-level annotations as required by most scene text recognition systems. Extensive experiments over a number of public datasets show that the proposed ESIR is capable of rectifying scene text distortions accurately, achieving superior recognition performance for both normal scene text images and those suffering from perspective and curvature distortions.



### Combining Deep and Depth: Deep Learning and Face Depth Maps for Driver Attention Monitoring
- **Arxiv ID**: http://arxiv.org/abs/1812.05831v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.05831v1)
- **Published**: 2018-12-14 09:10:17+00:00
- **Updated**: 2018-12-14 09:10:17+00:00
- **Authors**: Guido Borghi
- **Comment**: White paper about my research activity
- **Journal**: None
- **Summary**: Recently, deep learning approaches have achieved promising results in various fields of computer vision. In this paper, we investigate the combination of deep learning based methods and depth maps as input images to tackle the problem of driver attention monitoring. Moreover, we assume the concept of attention as Head Pose Estimation and Facial Landmark Detection tasks. Differently from other proposals in the literature, the proposed systems are able to work directly and based only on raw depth data. All presented methods are trained and tested on two new public datasets, namely Pandora and MotorMark, achieving state-of-art results and running with real time performance.



### Rethinking Layer-wise Feature Amounts in Convolutional Neural Network Architectures
- **Arxiv ID**: http://arxiv.org/abs/1812.05836v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.05836v1)
- **Published**: 2018-12-14 09:28:05+00:00
- **Updated**: 2018-12-14 09:28:05+00:00
- **Authors**: Martin Mundt, Sagnik Majumder, Tobias Weis, Visvanathan Ramesh
- **Comment**: Accepted at the Critiquing and Correcting Trends in Machine Learning
  (CRACT) Workshop at the 32nd Conference on Neural Information Processing
  Systems (NeurIPS 2018)
- **Journal**: None
- **Summary**: We characterize convolutional neural networks with respect to the relative amount of features per layer. Using a skew normal distribution as a parametrized framework, we investigate the common assumption of monotonously increasing feature-counts with higher layers of architecture designs. Our evaluation on models with VGG-type layers on the MNIST, Fashion-MNIST and CIFAR-10 image classification benchmarks provides evidence that motivates rethinking of our common assumption: architectures that favor larger early layers seem to yield better accuracy.



### Spatial Fusion GAN for Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1812.05840v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.05840v3)
- **Published**: 2018-12-14 09:38:07+00:00
- **Updated**: 2019-04-02 08:58:47+00:00
- **Authors**: Fangneng Zhan, Hongyuan Zhu, Shijian Lu
- **Comment**: Accepted to CVPR 2019
- **Journal**: None
- **Summary**: Recent advances in generative adversarial networks (GANs) have shown great potentials in realistic image synthesis whereas most existing works address synthesis realism in either appearance space or geometry space but few in both. This paper presents an innovative Spatial Fusion GAN (SF-GAN) that combines a geometry synthesizer and an appearance synthesizer to achieve synthesis realism in both geometry and appearance spaces. The geometry synthesizer learns contextual geometries of background images and transforms and places foreground objects into the background images unanimously. The appearance synthesizer adjusts the color, brightness and styles of the foreground objects and embeds them into background images harmoniously, where a guided filter is introduced for detail preserving. The two synthesizers are inter-connected as mutual references which can be trained end-to-end without supervision. The SF-GAN has been evaluated in two tasks: (1) realistic scene text image synthesis for training better recognition models; (2) glass and hat wearing for realistic matching glasses and hats with real portraits. Qualitative and quantitative comparisons with the state-of-the-art demonstrate the superiority of the proposed SF-GAN.



### Imitation Learning for End to End Vehicle Longitudinal Control with Forward Camera
- **Arxiv ID**: http://arxiv.org/abs/1812.05841v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1812.05841v1)
- **Published**: 2018-12-14 09:45:34+00:00
- **Updated**: 2018-12-14 09:45:34+00:00
- **Authors**: Laurent George, Thibault Buhet, Emilie Wirbel, Gaetan Le-Gall, Xavier Perrotton
- **Comment**: NeurIps 2018 Imitation Learning Workshop
- **Journal**: None
- **Summary**: In this paper we present a complete study of an end-to-end imitation learning system for speed control of a real car, based on a neural network with a Long Short Term Memory (LSTM). To achieve robustness and generalization from expert demonstrations, we propose data augmentation and label augmentation that are relevant for imitation learning in longitudinal control context. Based on front camera image only, our system is able to correctly control the speed of a car in simulation environment, and in a real car on a challenging test track. The system also shows promising results in open road context.



### Multi-hypothesis contextual modeling for semantic segmentation
- **Arxiv ID**: http://arxiv.org/abs/1812.05850v1
- **DOI**: 10.1016/j.patrec.2018.12.011
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.05850v1)
- **Published**: 2018-12-14 10:33:36+00:00
- **Updated**: 2018-12-14 10:33:36+00:00
- **Authors**: Hasan F. Ates, Sercan Sunetci
- **Comment**: 8 pages and 3 figure, accepted to Pattern Recognition Letters,
  Elsevier
- **Journal**: Pattern Recog. Letters, 117 (2019) 104-110
- **Summary**: Semantic segmentation (i.e. image parsing) aims to annotate each image pixel with its corresponding semantic class label. Spatially consistent labeling of the image requires an accurate description and modeling of the local contextual information. Segmentation result is typically improved by Markov Random Field (MRF) optimization on the initial labels. However this improvement is limited by the accuracy of initial result and how the contextual neighborhood is defined. In this paper, we develop generalized and flexible contextual models for segmentation neighborhoods in order to improve parsing accuracy. Instead of using a fixed segmentation and neighborhood definition, we explore various contextual models for fusion of complementary information available in alternative segmentations of the same image. In other words, we propose a novel MRF framework that describes and optimizes the contextual dependencies between multiple segmentations. Simulation results on two common datasets demonstrate significant improvement in parsing accuracy over the baseline approaches.



### The Coherent Point Drift for Clustered Point Sets
- **Arxiv ID**: http://arxiv.org/abs/1812.05869v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.05869v1)
- **Published**: 2018-12-14 11:52:21+00:00
- **Updated**: 2018-12-14 11:52:21+00:00
- **Authors**: Dmitry Lachinov, Vadim Turlapov
- **Comment**: None
- **Journal**: None
- **Summary**: The problem of non-rigid point set registration is a key problem for many computer vision tasks. In many cases the nature of the data or capabilities of the point detection algorithms can give us some prior information on point sets distribution. In non-rigid case this information is able to drastically improve registration results by limiting number of possible solutions. In this paper we explore use of prior information about point sets clustering, such information can be obtained with preliminary segmentation. We extend existing probabilistic framework for fitting two level Gaussian mixture model and derive closed form solution for maximization step of the EM algorithm. This enables us to improve method accuracy with almost no performance loss. We evaluate our approach and compare the Cluster Coherent Point Drift with other existing non-rigid point set registration methods and show it's advantages for digital medicine tasks, especially for heart template model personalization using patient's medical data.



### Advanced Super-Resolution using Lossless Pooling Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1812.06023v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.06023v1)
- **Published**: 2018-12-14 16:58:02+00:00
- **Updated**: 2018-12-14 16:58:02+00:00
- **Authors**: Farzad Toutounchi, Ebroul Izquierdo
- **Comment**: Accepted paper: 2019 IEEE Winter Conference on Applications of
  Computer Vision
- **Journal**: None
- **Summary**: In this paper, we present a novel deep learning-based approach for still image super-resolution, that unlike the mainstream models does not rely solely on the input low resolution image for high quality upsampling, and takes advantage of a set of artificially created auxiliary self-replicas of the input image that are incorporated in the neural network to create an enhanced and accurate upscaling scheme. Inclusion of the proposed lossless pooling layers, and the fusion of the input self-replicas enable the model to exploit the high correlation between multiple instances of the same content, and eventually result in significant improvements in the quality of the super-resolution, which is confirmed by extensive evaluations.



### Fast Mitochondria Detection for Connectomics
- **Arxiv ID**: http://arxiv.org/abs/1812.06024v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.06024v2)
- **Published**: 2018-12-14 16:58:05+00:00
- **Updated**: 2020-06-19 04:25:23+00:00
- **Authors**: Vincent Casser, Kai Kang, Hanspeter Pfister, Daniel Haehn
- **Comment**: None
- **Journal**: None
- **Summary**: High-resolution connectomics data allows for the identification of dysfunctional mitochondria which are linked to a variety of diseases such as autism or bipolar. However, manual analysis is not feasible since datasets can be petabytes in size. We present a fully automatic mitochondria detector based on a modified U-Net architecture that yields high accuracy and fast processing times. We evaluate our method on multiple real-world connectomics datasets, including an improved version of the EPFL mitochondria benchmark. Our results show an Jaccard index of up to 0.90 with inference times lower than 16ms for a 512x512px image tile. This speed is faster than the acquisition speed of modern electron microscopes, enabling mitochondria detection in real-time. Our detector ranks first for real-time detection when compared to previous works and data, results, and code are openly available.



### Automatic quantification of the LV function and mass: a deep learning approach for cardiovascular MRI
- **Arxiv ID**: http://arxiv.org/abs/1812.06061v1
- **DOI**: 10.1016/j.cmpb.2018.12.002
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.06061v1)
- **Published**: 2018-12-14 18:20:39+00:00
- **Updated**: 2018-12-14 18:20:39+00:00
- **Authors**: Ariel H. Curiale, Flavio D. Colavecchia, German Mato
- **Comment**: Accepted in Computer Methods and Programs in Biomedicine.
  https://www.sciencedirect.com/science/article/pii/S0169260718311696?via%3Dihub
- **Journal**: None
- **Summary**: Objective: This paper proposes a novel approach for automatic left ventricle (LV) quantification using convolutional neural networks (CNN).   Methods: The general framework consists of one CNN for detecting the LV, and another for tissue classification. Also, three new deep learning architectures were proposed for LV quantification. These new CNNs introduce the ideas of sparsity and depthwise separable convolution into the U-net architecture, as well as, a residual learning strategy level-to-level. To this end, we extend the classical U-net architecture and use the generalized Jaccard distance as optimization objective function.   Results: The CNNs were trained and evaluated with 140 patients from two public cardiovascular magnetic resonance datasets (Sunnybrook and Cardiac Atlas Project) by using a 5-fold cross-validation strategy. Our results demonstrate a suitable accuracy for myocardial segmentation ($\sim$0.9 Dice's coefficient), and a strong correlation with the most relevant physiological measures: 0.99 for end-diastolic and end-systolic volume, 0.97 for the left myocardial mass, 0.95 for the ejection fraction and 0.93 for the stroke volume and cardiac output.   Conclusion: Our simulation and clinical evaluation results demonstrate the capability and merits of the proposed CNN to estimate different structural and functional features such as LV mass and EF which are commonly used for both diagnosis and treatment of different pathologies.   Significance: This paper suggests a new approach for automatic LV quantification based on deep learning where errors are comparable to the inter- and intra-operator ranges for manual contouring. Also, this approach may have important applications on motion quantification.



### On Attention Modules for Audio-Visual Synchronization
- **Arxiv ID**: http://arxiv.org/abs/1812.06071v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1812.06071v1)
- **Published**: 2018-12-14 18:37:12+00:00
- **Updated**: 2018-12-14 18:37:12+00:00
- **Authors**: Naji Khosravan, Shervin Ardeshir, Rohit Puri
- **Comment**: None
- **Journal**: None
- **Summary**: With the development of media and networking technologies, multimedia applications ranging from feature presentation in a cinema setting to video on demand to interactive video conferencing are in great demand. Good synchronization between audio and video modalities is a key factor towards defining the quality of a multimedia presentation. The audio and visual signals of a multimedia presentation are commonly managed by independent workflows - they are often separately authored, processed, stored and even delivered to the playback system. This opens up the possibility of temporal misalignment between the two modalities - such a tendency is often more pronounced in the case of produced content (such as movies).   To judge whether audio and video signals of a multimedia presentation are synchronized, we as humans often pay close attention to discriminative spatio-temporal blocks of the video (e.g. synchronizing the lip movement with the utterance of words, or the sound of a bouncing ball at the moment it hits the ground). At the same time, we ignore large portions of the video in which no discriminative sounds exist (e.g. background music playing in a movie). Inspired by this observation, we study leveraging attention modules for automatically detecting audio-visual synchronization. We propose neural network based attention modules, capable of weighting different portions (spatio-temporal blocks) of the video based on their respective discriminative power. Our experiments indicate that incorporating attention modules yields state-of-the-art results for the audio-visual synchronization classification problem.



### Axially-shifted pattern illumination for macroscale turbidity suppression and virtual volumetric confocal imaging without axial scanning
- **Arxiv ID**: http://arxiv.org/abs/1812.06125v1
- **DOI**: 10.1364/OL.44.000811
- **Categories**: **cs.CV**, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/1812.06125v1)
- **Published**: 2018-12-14 19:25:40+00:00
- **Updated**: 2018-12-14 19:25:40+00:00
- **Authors**: Shaowei Jiang, Jun Liao, Zichao Bian, Pengming Song, Garrett Soler, Kazunori Hoshino, Guoan Zheng
- **Comment**: None
- **Journal**: Optics Letters, 44(4), (2019)
- **Summary**: Structured illumination has been widely used for optical sectioning and 3D surface recovery. In a typical implementation, multiple images under non-uniform pattern illumination are used to recover a single object section. Axial scanning of the sample or the objective lens is needed for acquiring the 3D volumetric data. Here we demonstrate the use of axially-shifted pattern illumination (asPI) for virtual volumetric confocal imaging without axial scanning. In the reported approach, we project illumination patterns at a tilted angle with respect to the detection optics. As such, the illumination patterns shift laterally at different z sections and the sample information at different z-sections can be recovered based on the captured 2D images. We demonstrate the reported approach for virtual confocal imaging through a diffusing layer and underwater 3D imaging through diluted milk. We show that we can acquire the entire confocal volume in ~1s with a throughput of 420 megapixels per second. Our approach may provide new insights for developing confocal light ranging and detection systems in degraded visual environments.



### Improving the Performance of Unimodal Dynamic Hand-Gesture Recognition with Multimodal Training
- **Arxiv ID**: http://arxiv.org/abs/1812.06145v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.LG, stat.ML, 68T45, 62H30, I.5.3; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/1812.06145v2)
- **Published**: 2018-12-14 20:08:24+00:00
- **Updated**: 2019-08-12 09:51:14+00:00
- **Authors**: Mahdi Abavisani, Hamid Reza Vaezi Joze, Vishal M. Patel
- **Comment**: None
- **Journal**: The IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR), 2019, pp. 1165-1174
- **Summary**: We present an efficient approach for leveraging the knowledge from multiple modalities in training unimodal 3D convolutional neural networks (3D-CNNs) for the task of dynamic hand gesture recognition. Instead of explicitly combining multimodal information, which is commonplace in many state-of-the-art methods, we propose a different framework in which we embed the knowledge of multiple modalities in individual networks so that each unimodal network can achieve an improved performance. In particular, we dedicate separate networks per available modality and enforce them to collaborate and learn to develop networks with common semantics and better representations. We introduce a "spatiotemporal semantic alignment" loss (SSA) to align the content of the features from different networks. In addition, we regularize this loss with our proposed "focal regularization parameter" to avoid negative knowledge transfer. Experimental results show that our framework improves the test time recognition accuracy of unimodal networks, and provides the state-of-the-art performance on various dynamic hand gesture recognition datasets.



### Siamese Cascaded Region Proposal Networks for Real-Time Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/1812.06148v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.06148v1)
- **Published**: 2018-12-14 20:10:36+00:00
- **Updated**: 2018-12-14 20:10:36+00:00
- **Authors**: Heng Fan, Haibin Ling
- **Comment**: Tech Report
- **Journal**: None
- **Summary**: Region proposal networks (RPN) have been recently combined with the Siamese network for tracking, and shown excellent accuracy with high efficiency. Nevertheless, previously proposed one-stage Siamese-RPN trackers degenerate in presence of similar distractors and large scale variation. Addressing these issues, we propose a multi-stage tracking framework, Siamese Cascaded RPN (C-RPN), which consists of a sequence of RPNs cascaded from deep high-level to shallow low-level layers in a Siamese network. Compared to previous solutions, C-RPN has several advantages: (1) Each RPN is trained using the outputs of RPN in the previous stage. Such process stimulates hard negative sampling, resulting in more balanced training samples. Consequently, the RPNs are sequentially more discriminative in distinguishing difficult background (i.e., similar distractors). (2) Multi-level features are fully leveraged through a novel feature transfer block (FTB) for each RPN, further improving the discriminability of C-RPN using both high-level semantic and low-level spatial information. (3) With multiple steps of regressions, C-RPN progressively refines the location and shape of the target in each RPN with adjusted anchor boxes in the previous stage, which makes localization more accurate. C-RPN is trained end-to-end with the multi-task loss function. In inference, C-RPN is deployed as it is, without any temporal adaption, for real-time tracking. In extensive experiments on OTB-2013, OTB-2015, VOT-2016, VOT-2017, LaSOT and TrackingNet, C-RPN consistently achieves state-of-the-art results and runs in real-time.



### A Parametric Top-View Representation of Complex Road Scenes
- **Arxiv ID**: http://arxiv.org/abs/1812.06152v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.06152v2)
- **Published**: 2018-12-14 20:18:38+00:00
- **Updated**: 2019-04-18 19:25:38+00:00
- **Authors**: Ziyan Wang, Buyu Liu, Samuel Schulter, Manmohan Chandraker
- **Comment**: Accepted by CVPR 2019
- **Journal**: None
- **Summary**: In this paper, we address the problem of inferring the layout of complex road scenes given a single camera as input. To achieve that, we first propose a novel parameterized model of road layouts in a top-view representation, which is not only intuitive for human visualization but also provides an interpretable interface for higher-level decision making. Moreover, the design of our top-view scene model allows for efficient sampling and thus generation of large-scale simulated data, which we leverage to train a deep neural network to infer our scene model's parameters. Specifically, our proposed training procedure uses supervised domain-adaptation techniques to incorporate both simulated as well as manually annotated data. Finally, we design a Conditional Random Field (CRF) that enforces coherent predictions for a single frame and encourages temporal smoothness among video frames. Experiments on two public data sets show that: (1) Our parametric top-view model is representative enough to describe complex road scenes; (2) The proposed method outperforms baselines trained on manually-annotated or simulated data only, thus getting the best of both; (3) Our CRF is able to generate temporally smoothed while semantically meaningful results.



### Inverse Cooking: Recipe Generation from Food Images
- **Arxiv ID**: http://arxiv.org/abs/1812.06164v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.06164v2)
- **Published**: 2018-12-14 20:59:33+00:00
- **Updated**: 2019-06-15 10:56:03+00:00
- **Authors**: Amaia Salvador, Michal Drozdzal, Xavier Giro-i-Nieto, Adriana Romero
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: People enjoy food photography because they appreciate food. Behind each meal there is a story described in a complex recipe and, unfortunately, by simply looking at a food image we do not have access to its preparation process. Therefore, in this paper we introduce an inverse cooking system that recreates cooking recipes given food images. Our system predicts ingredients as sets by means of a novel architecture, modeling their dependencies without imposing any order, and then generates cooking instructions by attending to both image and its inferred ingredients simultaneously. We extensively evaluate the whole system on the large-scale Recipe1M dataset and show that (1) we improve performance w.r.t. previous baselines for ingredient prediction; (2) we are able to obtain high quality recipes by leveraging both image and ingredients; (3) our system is able to produce more compelling recipes than retrieval-based approaches according to human judgment. We make code and models publicly available.



### Efficient Interpretation of Deep Learning Models Using Graph Structure and Cooperative Game Theory: Application to ASD Biomarker Discovery
- **Arxiv ID**: http://arxiv.org/abs/1812.06181v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.06181v2)
- **Published**: 2018-12-14 21:50:02+00:00
- **Updated**: 2019-03-13 18:38:11+00:00
- **Authors**: Xiaoxiao Li, Nicha C. Dvornek, Yuan Zhou, Juntang Zhuang, Pamela Ventola, James S. Duncan
- **Comment**: 12 pages, 7 figures, accpeted as a full paper in IPMI 2019
- **Journal**: None
- **Summary**: Discovering imaging biomarkers for autism spectrum disorder (ASD) is critical to help explain ASD and predict or monitor treatment outcomes. Toward this end, deep learning classifiers have recently been used for identifying ASD from functional magnetic resonance imaging (fMRI) with higher accuracy than traditional learning strategies. However, a key challenge with deep learning models is understanding just what image features the network is using, which can in turn be used to define the biomarkers. Current methods extract biomarkers, i.e., important features, by looking at how the prediction changes if "ignoring" one feature at a time. In this work, we go beyond looking at only individual features by using Shapley value explanation (SVE) from cooperative game theory. Cooperative game theory is advantageous here because it directly considers the interaction between features and can be applied to any machine learning method, making it a novel, more accurate way of determining instance-wise biomarker importance from deep learning models. A barrier to using SVE is its computational complexity: $2^N$ given $N$ features. We explicitly reduce the complexity of SVE computation by two approaches based on the underlying graph structure of the input data: 1) only consider the centralized coalition of each feature; 2) a hierarchical pipeline which first clusters features into small communities, then applies SVE in each community. Monte Carlo approximation can be used for large permutation sets. We first validate our methods on the MNIST dataset and compare to human perception. Next, to insure plausibility of our biomarker results, we train a Random Forest (RF) to classify ASD/control subjects from fMRI and compare SVE results to standard RF-based feature importance. Finally, we show initial results on ranked fMRI biomarkers using SVE on a deep learning classifier for the ASD/control dataset.



### Learning Latent Subspaces in Variational Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/1812.06190v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.06190v1)
- **Published**: 2018-12-14 22:10:50+00:00
- **Updated**: 2018-12-14 22:10:50+00:00
- **Authors**: Jack Klys, Jake Snell, Richard Zemel
- **Comment**: Published as a conference paper at NeurIPS 2018. 15 pages
- **Journal**: None
- **Summary**: Variational autoencoders (VAEs) are widely used deep generative models capable of learning unsupervised latent representations of data. Such representations are often difficult to interpret or control. We consider the problem of unsupervised learning of features correlated to specific labels in a dataset. We propose a VAE-based generative model which we show is capable of extracting features correlated to binary labels in the data and structuring it in a latent subspace which is easy to interpret. Our model, the Conditional Subspace VAE (CSVAE), uses mutual information minimization to learn a low-dimensional latent subspace associated with each label that can easily be inspected and independently manipulated. We demonstrate the utility of the learned representations for attribute manipulation tasks on both the Toronto Face and CelebA datasets.



### TAN: Temporal Aggregation Network for Dense Multi-label Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1812.06203v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.06203v1)
- **Published**: 2018-12-14 23:28:39+00:00
- **Updated**: 2018-12-14 23:28:39+00:00
- **Authors**: Xiyang Dai, Bharat Singh, Joe Yue-Hei Ng, Larry S. Davis
- **Comment**: WACV 2019
- **Journal**: None
- **Summary**: We present Temporal Aggregation Network (TAN) which decomposes 3D convolutions into spatial and temporal aggregation blocks. By stacking spatial and temporal convolutions repeatedly, TAN forms a deep hierarchical representation for capturing spatio-temporal information in videos. Since we do not apply 3D convolutions in each layer but only apply temporal aggregation blocks once after each spatial downsampling layer in the network, we significantly reduce the model complexity. The use of dilated convolutions at different resolutions of the network helps in aggregating multi-scale spatio-temporal information efficiently. Experiments show that our model is well suited for dense multi-label action recognition, which is a challenging sub-topic of action recognition that requires predicting multiple action labels in each frame. We outperform state-of-the-art methods by 5% and 3% on the Charades and Multi-THUMOS dataset respectively.



