# Arxiv Papers in cs.CV on 2018-12-25
### Similarity R-C3D for Few-shot Temporal Activity Detection
- **Arxiv ID**: http://arxiv.org/abs/1812.10000v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.10000v1)
- **Published**: 2018-12-25 00:35:31+00:00
- **Updated**: 2018-12-25 00:35:31+00:00
- **Authors**: Huijuan Xu, Bingyi Kang, Ximeng Sun, Jiashi Feng, Kate Saenko, Trevor Darrell
- **Comment**: None
- **Journal**: None
- **Summary**: Many activities of interest are rare events, with only a few labeled examples available. Therefore models for temporal activity detection which are able to learn from a few examples are desirable. In this paper, we present a conceptually simple and general yet novel framework for few-shot temporal activity detection which detects the start and end time of the few-shot input activities in an untrimmed video. Our model is end-to-end trainable and can benefit from more few-shot examples. At test time, each proposal is assigned the label of the few-shot activity class corresponding to the maximum similarity score. Our Similarity R-C3D method outperforms previous work on three large-scale benchmarks for temporal activity detection (THUMOS14, ActivityNet1.2, and ActivityNet1.3 datasets) in the few-shot setting. Our code will be made available.



### DUP-Net: Denoiser and Upsampler Network for 3D Adversarial Point Clouds Defense
- **Arxiv ID**: http://arxiv.org/abs/1812.11017v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.11017v2)
- **Published**: 2018-12-25 00:38:20+00:00
- **Updated**: 2019-07-29 15:12:54+00:00
- **Authors**: Hang Zhou, Kejiang Chen, Weiming Zhang, Han Fang, Wenbo Zhou, Nenghai Yu
- **Comment**: Published in IEEE ICCV2019
- **Journal**: None
- **Summary**: Neural networks are vulnerable to adversarial examples, which poses a threat to their application in security sensitive systems. We propose a Denoiser and UPsampler Network (DUP-Net) structure as defenses for 3D adversarial point cloud classification, where the two modules reconstruct surface smoothness by dropping or adding points. In this paper, statistical outlier removal (SOR) and a data-driven upsampling network are considered as denoiser and upsampler respectively. Compared with baseline defenses, DUP-Net has three advantages. First, with DUP-Net as a defense, the target model is more robust to white-box adversarial attacks. Second, the statistical outlier removal provides added robustness since it is a non-differentiable denoising operation. Third, the upsampler network can be trained on a small dataset and defends well against adversarial attacks generated from other point cloud datasets. We conduct various experiments to validate that DUP-Net is very effective as defense in practice. Our best defense eliminates 83.8% of C&W and l_2 loss based attack (point shifting), 50.0% of C&W and Hausdorff distance loss based attack (point adding) and 9.0% of saliency map based attack (point dropping) under 200 dropped points on PointNet.



### A Unified Framework for Mutual Improvement of SLAM and Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1812.10016v2
- **DOI**: 10.1109/ICRA.2019.8793499
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.10016v2)
- **Published**: 2018-12-25 02:46:22+00:00
- **Updated**: 2019-03-22 10:06:50+00:00
- **Authors**: Kai Wang, Yimin Lin, Luowei Wang, Liming Han, Minjie Hua, Xiang Wang, Shiguo Lian, Bill Huang
- **Comment**: 7 pages, 5 figures.This work has been accepted by ICRA 2019. The demo
  video can be found at https://youtu.be/Bkt53dAehjY
- **Journal**: None
- **Summary**: This paper presents a novel framework for simultaneously implementing localization and segmentation, which are two of the most important vision-based tasks for robotics. While the goals and techniques used for them were considered to be different previously, we show that by making use of the intermediate results of the two modules, their performance can be enhanced at the same time. Our framework is able to handle both the instantaneous motion and long-term changes of instances in localization with the help of the segmentation result, which also benefits from the refined 3D pose information. We conduct experiments on various datasets, and prove that our framework works effectively on improving the precision and robustness of the two tasks and outperforms existing localization and segmentation algorithms.



### Residual Dense Network for Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/1812.10477v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.10477v2)
- **Published**: 2018-12-25 03:45:44+00:00
- **Updated**: 2020-01-23 01:10:07+00:00
- **Authors**: Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, Yun Fu
- **Comment**: To appear in TPAMI. arXiv admin note: substantial text overlap with
  arXiv:1802.08797
- **Journal**: None
- **Summary**: Convolutional neural network has recently achieved great success for image restoration (IR) and also offered hierarchical features. However, most deep CNN based IR models do not make full use of the hierarchical features from the original low-quality images, thereby achieving relatively-low performance. In this paper, we propose a novel residual dense network (RDN) to address this problem in IR. We fully exploit the hierarchical features from all the convolutional layers. Specifically, we propose residual dense block (RDB) to extract abundant local features via densely connected convolutional layers. RDB further allows direct connections from the state of preceding RDB to all the layers of current RDB, leading to a contiguous memory mechanism. To adaptively learn more effective features from preceding and current local features and stabilize the training of wider network, we proposed local feature fusion in RDB. After fully obtaining dense local features, we use global feature fusion to jointly and adaptively learn global hierarchical features in a holistic way. We demonstrate the effectiveness of RDN with several representative IR applications, single image super-resolution, Gaussian image denoising, image compression artifact reduction, and image deblurring. Experiments on benchmark and real-world datasets show that our RDN achieves favorable performance against state-of-the-art methods for each IR task quantitatively and visually.



### Attention Branch Network: Learning of Attention Mechanism for Visual Explanation
- **Arxiv ID**: http://arxiv.org/abs/1812.10025v2
- **DOI**: None
- **Categories**: **cs.CV**, 65D19
- **Links**: [PDF](http://arxiv.org/pdf/1812.10025v2)
- **Published**: 2018-12-25 04:25:54+00:00
- **Updated**: 2019-04-10 11:18:02+00:00
- **Authors**: Hiroshi Fukui, Tsubasa Hirakawa, Takayoshi Yamashita, Hironobu Fujiyoshi
- **Comment**: Accepted to CVPR2019 10 pages, 7 figures
- **Journal**: None
- **Summary**: Visual explanation enables human to understand the decision making of Deep Convolutional Neural Network (CNN), but it is insufficient to contribute the performance improvement. In this paper, we focus on the attention map for visual explanation, which represents high response value as the important region in image recognition. This region significantly improves the performance of CNN by introducing an attention mechanism that focuses on a specific region in an image. In this work, we propose Attention Branch Network (ABN), which extends the top-down visual explanation model by introducing a branch structure with an attention mechanism. ABN can be applicable to several image recognition tasks by introducing a branch for attention mechanism and is trainable for the visual explanation and image recognition in end-to-end manner. We evaluate ABN on several image recognition tasks such as image classification, fine-grained recognition, and multiple facial attributes recognition. Experimental results show that ABN can outperform the accuracy of baseline models on these image recognition tasks while generating an attention map for visual explanation. Our code is available at https://github.com/machine-perception-robotics-group/attention_branch_network.



### A Survey of FPGA Based Deep Learning Accelerators: Challenges and Opportunities
- **Arxiv ID**: http://arxiv.org/abs/1901.04988v2
- **DOI**: None
- **Categories**: **cs.DC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1901.04988v2)
- **Published**: 2018-12-25 04:43:25+00:00
- **Updated**: 2019-12-25 01:34:12+00:00
- **Authors**: Teng Wang, Chao Wang, Xuehai Zhou, Huaping Chen
- **Comment**: Some part in the section of introduction dont have the labeling
  reference. And there are some wrong of data in figure
- **Journal**: None
- **Summary**: With the rapid development of in-depth learning, neural network and deep learning algorithms have been widely used in various fields, e.g., image, video and voice processing. However, the neural network model is getting larger and larger, which is expressed in the calculation of model parameters. Although a wealth of existing efforts on GPU platforms currently used by researchers for improving computing performance, dedicated hardware solutions are essential and emerging to provide advantages over pure software solutions. In this paper, we systematically investigate the neural network accelerator based on FPGA. Specifically, we respectively review the accelerators designed for specific problems, specific algorithms, algorithm features, and general templates. We also compared the design and implementation of the accelerator based on FPGA under different devices and network models and compared it with the versions of CPU and GPU. Finally, we present to discuss the advantages and disadvantages of accelerators on FPGA platforms and to further explore the opportunities for future research.



### MMFNet: A Multi-modality MRI Fusion Network for Segmentation of Nasopharyngeal Carcinoma
- **Arxiv ID**: http://arxiv.org/abs/1812.10033v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.10033v6)
- **Published**: 2018-12-25 05:19:00+00:00
- **Updated**: 2019-10-22 08:53:21+00:00
- **Authors**: Huai Chen, Yuxiao Qi, Yong Yin, Tengxiang Li, Xiaoqing Liu, Xiuli Li, Guanzhong Gong, Lisheng Wang
- **Comment**: 34 pages, 12 figures
- **Journal**: None
- **Summary**: Segmentation of nasopharyngeal carcinoma (NPC) from Magnetic Resonance Images (MRI) is a crucial prerequisite for NPC radiotherapy. However, manually segmenting of NPC is time-consuming and labor-intensive. Additionally, single-modality MRI generally cannot provide enough information for its accurate delineation. Therefore, a multi-modality MRI fusion network (MMFNet) based on three modalities of MRI (T1, T2 and contrast-enhanced T1) is proposed to complete accurate segmentation of NPC. The backbone of MMFNet is designed as a multi-encoder-based network, consisting of several encoders to capture modality-specific features and one single decoder to fuse them and obtain high-level features for NPC segmentation. A fusion block is presented to effectively fuse features from multi-modality MRI. It firstly recalibrates low-level features captured from modality-specific encoders to highlight both informative features and regions of interest, then fuses weighted features by a residual fusion block to keep balance between fused ones and high-level features from decoder. Moreover, a training strategy named self-transfer, which utilizes pre-trained modality-specific encoders to initialize multi-encoder-based network, is proposed to make full mining of information from different modalities of MRI. The proposed method based on multi-modality MRI can effectively segment NPC and its advantages are validated by extensive experiments.



### Selectivity or Invariance: Boundary-aware Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1812.10066v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.10066v3)
- **Published**: 2018-12-25 08:31:47+00:00
- **Updated**: 2019-09-11 08:08:17+00:00
- **Authors**: Jinming Su, Jia Li, Yu Zhang, Changqun Xia, Yonghong Tian
- **Comment**: None
- **Journal**: None
- **Summary**: Typically, a salient object detection (SOD) model faces opposite requirements in processing object interiors and boundaries. The features of interiors should be invariant to strong appearance change so as to pop-out the salient object as a whole, while the features of boundaries should be selective to slight appearance change to distinguish salient objects and background. To address this selectivity-invariance dilemma, we propose a novel boundary-aware network with successive dilation for image-based SOD. In this network, the feature selectivity at boundaries is enhanced by incorporating a boundary localization stream, while the feature invariance at interiors is guaranteed with a complex interior perception stream. Moreover, a transition compensation stream is adopted to amend the probable failures in transitional regions between interiors and boundaries. In particular, an integrated successive dilation module is proposed to enhance the feature invariance at interiors and transitional regions. Extensive experiments on six datasets show that the proposed approach outperforms 16 state-of-the-art methods.



### Learning based Facial Image Compression with Semantic Fidelity Metric
- **Arxiv ID**: http://arxiv.org/abs/1812.10067v2
- **DOI**: 10.1016/j.neucom.2019.01.086
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1812.10067v2)
- **Published**: 2018-12-25 08:42:55+00:00
- **Updated**: 2019-03-07 11:54:39+00:00
- **Authors**: Zhibo Chen, Tianyu He
- **Comment**: Accepted by Neurocomputing
- **Journal**: None
- **Summary**: Surveillance and security scenarios usually require high efficient facial image compression scheme for face recognition and identification. While either traditional general image codecs or special facial image compression schemes only heuristically refine codec separately according to face verification accuracy metric. We propose a Learning based Facial Image Compression (LFIC) framework with a novel Regionally Adaptive Pooling (RAP) module whose parameters can be automatically optimized according to gradient feedback from an integrated hybrid semantic fidelity metric, including a successfully exploration to apply Generative Adversarial Network (GAN) as metric directly in image compression scheme. The experimental results verify the framework's efficiency by demonstrating performance improvement of 71.41%, 48.28% and 52.67% bitrate saving separately over JPEG2000, WebP and neural network-based codecs under the same face verification accuracy distortion metric. We also evaluate LFIC's superior performance gain compared with latest specific facial image codecs. Visual experiments also show some interesting insight on how LFIC can automatically capture the information in critical areas based on semantic distortion metrics for optimized compression, which is quite different from the heuristic way of optimization in traditional image compression algorithms.



### Coupled Recurrent Network (CRN)
- **Arxiv ID**: http://arxiv.org/abs/1812.10071v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.10071v2)
- **Published**: 2018-12-25 09:04:24+00:00
- **Updated**: 2019-03-25 19:11:57+00:00
- **Authors**: Lin Sun, Kui Jia, Yuejia Shen, Silvio Savarese, Dit Yan Yeung, Bertram E. Shi
- **Comment**: None
- **Journal**: None
- **Summary**: Many semantic video analysis tasks can benefit from multiple, heterogenous signals. For example, in addition to the original RGB input sequences, sequences of optical flow are usually used to boost the performance of human action recognition in videos. To learn from these heterogenous input sources, existing methods reply on two-stream architectural designs that contain independent, parallel streams of Recurrent Neural Networks (RNNs). However, two-stream RNNs do not fully exploit the reciprocal information contained in the multiple signals, let alone exploit it in a recurrent manner. To this end, we propose in this paper a novel recurrent architecture, termed Coupled Recurrent Network (CRN), to deal with multiple input sources. In CRN, the parallel streams of RNNs are coupled together. Key design of CRN is a Recurrent Interpretation Block (RIB) that supports learning of reciprocal feature representations from multiple signals in a recurrent manner. Different from RNNs which stack the training loss at each time step or the last time step, we propose an effective and efficient training strategy for CRN. Experiments show the efficacy of the proposed CRN. In particular, we achieve the new state of the art on the benchmark datasets of human action recognition and multi-person pose estimation.



### A Data-driven Adversarial Examples Recognition Framework via Adversarial Feature Genome
- **Arxiv ID**: http://arxiv.org/abs/1812.10085v3
- **DOI**: 10.1002/int.22850
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.10085v3)
- **Published**: 2018-12-25 10:31:36+00:00
- **Updated**: 2022-01-26 16:50:45+00:00
- **Authors**: Li Chen, Qi Li, Weiye Chen, Zeyu Wang, Haifeng Li
- **Comment**: 27 pages, 9 figures, 13 tables
- **Journal**: International Journal of Intelligent Systems. 2022
- **Summary**: Adversarial examples pose many security threats to convolutional neural networks (CNNs). Most defense algorithms prevent these threats by finding differences between the original images and adversarial examples. However, the found differences do not contain features about the classes, so these defense algorithms can only detect adversarial examples without recovering the correct labels. In this regard, we propose the Adversarial Feature Genome (AFG), a novel type of data that contains both the differences and features about classes. This method is inspired by an observed phenomenon, namely the Adversarial Feature Separability (AFS), where the difference between the feature maps of the original images and adversarial examples becomes larger with deeper layers. On top of that, we further develop an adversarial example recognition framework that detects adversarial examples and can recover the correct labels. In the experiments, the detection and classification of adversarial examples by AFGs has an accuracy of more than 90.01\% in various attack scenarios. To the best of our knowledge, our method is the first method that focuses on both attack detecting and recovering. AFG gives a new data-driven perspective to improve the robustness of CNNs. The source code is available at https://github.com/GeoX-Lab/Adv_Fea_Genome.



### Classification of X-Ray Protein Crystallization Using Deep Convolutional Neural Networks with a Finder Module
- **Arxiv ID**: http://arxiv.org/abs/1812.10087v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.10087v1)
- **Published**: 2018-12-25 10:48:26+00:00
- **Updated**: 2018-12-25 10:48:26+00:00
- **Authors**: Yusei Miura, Tetsuya Sakurai, Claus Aranha, Toshiya Senda, Ryuichi Kato, Yusuke Yamada
- **Comment**: 7 pages, 16 figures
- **Journal**: None
- **Summary**: Recently, deep convolutional neural networks have shown good results for image recognition. In this paper, we use convolutional neural networks with a finder module, which discovers the important region for recognition and extracts that region. We propose applying our method to the recognition of protein crystals for X-ray structural analysis. In this analysis, it is necessary to recognize states of protein crystallization from a large number of images. There are several methods that realize protein crystallization recognition by using convolutional neural networks. In each method, large-scale data sets are required to recognize with high accuracy. In our data set, the number of images is not good enough for training CNN. The amount of data for CNN is a serious issue in various fields. Our method realizes high accuracy recognition with few images by discovering the region where the crystallization drop exists. We compared our crystallization image recognition method with a high precision method using Inception-V3. We demonstrate that our method is effective for crystallization images using several experiments. Our method gained the AUC value that is about 5% higher than the compared method.



### Usage of analytic hierarchy process for steganographic inserts detection in images
- **Arxiv ID**: http://arxiv.org/abs/1902.11100v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV, cs.GR, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1902.11100v1)
- **Published**: 2018-12-25 12:34:21+00:00
- **Updated**: 2018-12-25 12:34:21+00:00
- **Authors**: S. V. Belim, D. E. Vilkhovskiy
- **Comment**: None
- **Journal**: 2016 Dynamics of Systems, Mechanisms and Machines (Dynamics),
  Omsk, Russia, pp. 1-5
- **Summary**: This article presents the method of steganography detection, which is formed by replacing the least significant bit (LSB). Detection is performed by dividing the image into layers and making an analysis of zero-layer of adjacent bits for every bit. First-layer and second-layer are analyzed too. Hierarchies analysis method is used for making decision if current bit is changed. Weighting coefficients as part of the analytic hierarchy process are formed on the values of bits. Then a matrix of corrupted pixels is generated. Visualization of matrix with corrupted pixels allows to determine size, location and presence of the embedded message. Computer experiment was performed. Message was embedded in a bounded rectangular area of the image. This method demonstrated efficiency even at low filling container, less than 10\%. Widespread statistical methods are unable to detect this steganographic insert. The location and size of the embedded message can be determined with an error which is not exceeding to five pixels.



### The algorithm of the impulse noise filtration in images based on an algorithm of community detection in graphs
- **Arxiv ID**: http://arxiv.org/abs/1812.10098v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1812.10098v1)
- **Published**: 2018-12-25 12:40:12+00:00
- **Updated**: 2018-12-25 12:40:12+00:00
- **Authors**: S. V. Belim, S. B. Larionov
- **Comment**: None
- **Journal**: 2017 Dynamics of Systems, Mechanisms and Machines (Dynamics),
  Omsk, Russia, pp. 1-5
- **Summary**: This article suggests an algorithm of impulse noise filtration, based on the community detection in graphs. The image is representing as non-oriented weighted graph. Each pixel of an image is corresponding to a vertex of the graph. Community detection algorithm is running on the given graph. Assumed that communities that contain only one pixel are corresponding to noised pixels of an image. Suggested method was tested with help of computer experiment. This experiment was conducted on grayscale, and on colored images, on artificial images and on photos. It is shown that the suggested method is better than median filter by 20% regardless of noise percent. Higher efficiency is justified by the fact that most of filters are changing all of image pixels, but suggested method is finding and restoring only noised pixels. The dependence of the effectiveness of the proposed method on the percentage of noise in the image is shown.



### A Survey on Non-rigid 3D Shape Analysis
- **Arxiv ID**: http://arxiv.org/abs/1812.10111v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CG, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1812.10111v1)
- **Published**: 2018-12-25 14:33:42+00:00
- **Updated**: 2018-12-25 14:33:42+00:00
- **Authors**: Hamid Laga
- **Comment**: None
- **Journal**: None
- **Summary**: Shape is an important physical property of natural and manmade 3D objects that characterizes their external appearances. Understanding differences between shapes and modeling the variability within and across shape classes, hereinafter referred to as \emph{shape analysis}, are fundamental problems to many applications, ranging from computer vision and computer graphics to biology and medicine. This chapter provides an overview of some of the recent techniques that studied the shape of 3D objects that undergo non-rigid deformations including bending and stretching. Recent surveys that covered some aspects such classification, retrieval, recognition, and rigid or nonrigid registration, focused on methods that use shape descriptors. Descriptors, however, provide abstract representations that do not enable the exploration of shape variability. In this chapter, we focus on recent techniques that treated the shape of 3D objects as points in some high dimensional space where paths describe deformations. Equipping the space with a suitable metric enables the quantification of the range of deformations of a given shape, which in turn enables (1) comparing and classifying 3D objects based on their shape, (2) computing smooth deformations, i.e. geodesics, between pairs of objects, and (3) modeling and exploring continuous shape variability in a collection of 3D models. This article surveys and classifies recent developments in this field, outlines fundamental issues, discusses their potential applications in computer vision and graphics, and highlights opportunities for future research. Our primary goal is to bridge the gap between various techniques that have been often independently proposed by different communities including mathematics and statistics, computer vision and graphics, and medical image analysis.



### Motion Selective Prediction for Video Frame Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1812.10157v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.10157v1)
- **Published**: 2018-12-25 19:15:29+00:00
- **Updated**: 2018-12-25 19:15:29+00:00
- **Authors**: Veronique Prinet
- **Comment**: None
- **Journal**: None
- **Summary**: Existing conditional video prediction approaches train a network from large databases and generalize to previously unseen data. We take the opposite stance, and introduce a model that learns from the first frames of a given video and extends its content and motion, to, eg, double its length. To this end, we propose a dual network that can use in a flexible way both dynamic and static convolutional motion kernels, to predict future frames. The construct of our model gives us the the means to efficiently analyze its functioning and interpret its output. We demonstrate experimentally the robustness of our approach on challenging videos in-the-wild and show that it is competitive wrt related baselines.



### Finger-GAN: Generating Realistic Fingerprint Images Using Connectivity Imposed GAN
- **Arxiv ID**: http://arxiv.org/abs/1812.10482v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.10482v1)
- **Published**: 2018-12-25 20:41:35+00:00
- **Updated**: 2018-12-25 20:41:35+00:00
- **Authors**: Shervin Minaee, Amirali Abdolrashidi
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1812.04822
- **Journal**: None
- **Summary**: Generating realistic biometric images has been an interesting and, at the same time, challenging problem. Classical statistical models fail to generate realistic-looking fingerprint images, as they are not powerful enough to capture the complicated texture representation in fingerprint images. In this work, we present a machine learning framework based on generative adversarial networks (GAN), which is able to generate fingerprint images sampled from a prior distribution (learned from a set of training images). We also add a suitable regularization term to the loss function, to impose the connectivity of generated fingerprint images. This is highly desirable for fingerprints, as the lines in each finger are usually connected. We apply this framework to two popular fingerprint databases, and generate images which look very realistic, and similar to the samples in those databases. Through experimental results, we show that the generated fingerprint images have a good diversity, and are able to capture different parts of the prior distribution. We also evaluate the Frechet Inception distance (FID) of our proposed model, and show that our model is able to achieve good quantitative performance in terms of this score.



