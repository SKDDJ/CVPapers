# Arxiv Papers in cs.CV on 2018-12-06
### Simultaneous Recognition of Horizontal and Vertical Text in Natural Images
- **Arxiv ID**: http://arxiv.org/abs/1812.07059v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.07059v1)
- **Published**: 2018-12-06 01:16:08+00:00
- **Updated**: 2018-12-06 01:16:08+00:00
- **Authors**: Chankyu Choi, Youngmin Yoon, Junsu Lee, Junseok Kim
- **Comment**: Presented at the IWRR workshop at ACCV 2018
- **Journal**: None
- **Summary**: Recent state-of-the-art scene text recognition methods have primarily focused on horizontal text in images. However, in several Asian countries, including China, large amounts of text in signs, books, and TV commercials are vertically directed. Because the horizontal and vertical texts exhibit different characteristics, developing an algorithm that can simultaneously recognize both types of text in real environments is necessary. To address this problem, we adopted the direction encoding mask (DEM) and selective attention network (SAN) methods based on supervised learning. DEM contains directional information to compensate in cases that lack text direction; therefore, our network is trained using this information to handle the vertical text. The SAN method is designed to work individually for both types of text. To train the network to recognize both types of text and to evaluate the effectiveness of the designed model, we prepared a new synthetic vertical text dataset and collected an actual vertical text dataset (VTD142) from the Web. Using these datasets, we proved that our proposed model can accurately recognize both vertical and horizontal text and can achieve state-of-the-art results in experiments using benchmark datasets, including the street view test (SVT), IIIT-5k, and ICDAR. Although our model is relatively simple as compared to its predecessors, it maintains the accuracy and is trained in an end-to-end manner.



### Urban-Rural Environmental Gradient in a Developing City: Testing ENVI GIS Functionality
- **Arxiv ID**: http://arxiv.org/abs/1812.10378v1
- **DOI**: 10.6084/m9.figshare.7210286
- **Categories**: **cs.CY**, cs.CV, 68U05, I.5.3
- **Links**: [PDF](http://arxiv.org/pdf/1812.10378v1)
- **Published**: 2018-12-06 02:10:53+00:00
- **Updated**: 2018-12-06 02:10:53+00:00
- **Authors**: Polina Lemenkova
- **Comment**: 5 pages, 2 figures, 1 table
- **Journal**: Conference Proceedings 'Abishevskie Readings. Innovation in the
  Complex Processing of Mineral Raw Materials', 21-22 Jan 2016
- **Summary**: The research performs urban ecosystem analysis supported by ENVI GIS by integrated studies on land cover types and geospatial modeling of Taipei city. The paper deals with the role of anthropogenic pressure on the structure of the landscape and change of land cover types. Methods included assessment of the impact from anthropogenic activities on the natural ecosystems, evaluation of the rate and scale of landscape dynamics using remote sensing data and GIS. The research aims to assist environmentalists and city planners to evaluate strategies for specific objectives of urban development in Taiwan, China.



### Topology, homogeneity and scale factors for object detection: application of eCognition software for urban mapping using multispectral satellite image
- **Arxiv ID**: http://arxiv.org/abs/1901.00726v1
- **DOI**: 10.6084/m9.figshare.7211588
- **Categories**: **eess.IV**, cs.CV, 68U10, I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/1901.00726v1)
- **Published**: 2018-12-06 02:24:48+00:00
- **Updated**: 2018-12-06 02:24:48+00:00
- **Authors**: Polina Lemenkova
- **Comment**: 6 pages, 12 figures, INSO2015, Ed. by A. Girgvliani et al. Akaki
  Tsereteli State University, Kutaisi (Imereti), Georgia
- **Journal**: Proceedings of 7th International Conference 'Internet and Society.
  Modelling' INSO2015, 2015 (80-85)
- **Summary**: The research scope of this paper is to apply spatial object based image analysis (OBIA) method for processing panchromatic multispectral image covering study area of Brussels for urban mapping. The aim is to map different land cover types and more specifically, built-up areas from the very high resolution (VHR) satellite image using OBIA approach. A case study covers urban landscapes in the eastern areas of the city of Brussels, Belgium. Technically, this research was performed in eCognition raster processing software demonstrating excellent results of image segmentation and classification. The tools embedded in eCognition enabled to perform image segmentation and objects classification processes in a semi-automated regime, which is useful for the city planning, spatial analysis and urban growth analysis. The combination of the OBIA method together with technical tools of the eCognition demonstrated applicability of this method for urban mapping in densely populated areas, e.g. in megapolis and capital cities. The methodology included multiresolution segmentation and classification of the created objects.



### Skin Lesions Classification Using Convolutional Neural Networks in Clinical Images
- **Arxiv ID**: http://arxiv.org/abs/1812.02316v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.02316v1)
- **Published**: 2018-12-06 02:54:28+00:00
- **Updated**: 2018-12-06 02:54:28+00:00
- **Authors**: Danilo Barros Mendes, Nilton Correia da Silva
- **Comment**: None
- **Journal**: None
- **Summary**: Skin lesions are conditions that appear on a patient due to many different reasons. One of these can be because of an abnormal growth in skin tissue, defined as cancer. This disease plagues more than 14.1 million patients and had been the cause of more than 8.2 million deaths, worldwide. Therefore, the construction of a classification model for 12 lesions, including Malignant Melanoma and Basal Cell Carcinoma, is proposed. Furthermore, in this work, it is used a ResNet-152 architecture, which was trained over 3,797 images, later augmented by a factor of 29 times, using positional, scale, and lighting transformations. Finally, the network was tested with 956 images and achieve an area under the curve (AUC) of 0.96 for Melanoma and 0.91 for Basal Cell Carcinoma.



### Discriminative Supervised Hashing for Cross-Modal similarity Search
- **Arxiv ID**: http://arxiv.org/abs/1812.07660v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1812.07660v3)
- **Published**: 2018-12-06 04:28:31+00:00
- **Updated**: 2019-04-18 02:54:33+00:00
- **Authors**: Jun Yu, Xiao-Jun Wu, Josef Kittler
- **Comment**: 7 pages,3 figures,4 tables;The paper is under consideration at Image
  and Vision Computing
- **Journal**: None
- **Summary**: With the advantage of low storage cost and high retrieval efficiency, hashing techniques have recently been an emerging topic in cross-modal similarity search. As multiple modal data reflect similar semantic content, many researches aim at learning unified binary codes. However, discriminative hashing features learned by these methods are not adequate. This results in lower accuracy and robustness. We propose a novel hashing learning framework which jointly performs classifier learning, subspace learning and matrix factorization to preserve class-specific semantic content, termed Discriminative Supervised Hashing (DSH), to learn the discrimative unified binary codes for multi-modal data. Besides, reducing the loss of information and preserving the non-linear structure of data, DSH non-linearly projects different modalities into the common space in which the similarity among heterogeneous data points can be measured. Extensive experiments conducted on the three publicly available datasets demonstrate that the framework proposed in this paper outperforms several state-of -the-art methods.



### Arbitrary Style Transfer with Style-Attentional Networks
- **Arxiv ID**: http://arxiv.org/abs/1812.02342v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.02342v5)
- **Published**: 2018-12-06 04:31:54+00:00
- **Updated**: 2019-05-23 10:57:50+00:00
- **Authors**: Dae Young Park, Kwang Hee Lee
- **Comment**: Accepted by CVPR2019
- **Journal**: None
- **Summary**: Arbitrary style transfer aims to synthesize a content image with the style of an image to create a third image that has never been seen before. Recent arbitrary style transfer algorithms find it challenging to balance the content structure and the style patterns. Moreover, simultaneously maintaining the global and local style patterns is difficult due to the patch-based mechanism. In this paper, we introduce a novel style-attentional network (SANet) that efficiently and flexibly integrates the local style patterns according to the semantic spatial distribution of the content image. A new identity loss function and multi-level feature embeddings enable our SANet and decoder to preserve the content structure as much as possible while enriching the style patterns. Experimental results demonstrate that our algorithm synthesizes stylized images in real-time that are higher in quality than those produced by the state-of-the-art algorithms.



### Counterfactual Critic Multi-Agent Training for Scene Graph Generation
- **Arxiv ID**: http://arxiv.org/abs/1812.02347v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.02347v3)
- **Published**: 2018-12-06 04:54:14+00:00
- **Updated**: 2019-08-09 13:08:58+00:00
- **Authors**: Long Chen, Hanwang Zhang, Jun Xiao, Xiangnan He, Shiliang Pu, Shih-Fu Chang
- **Comment**: International Conference on Computer Vision (ICCV), 2019 (oral)
- **Journal**: None
- **Summary**: Scene graphs -- objects as nodes and visual relationships as edges -- describe the whereabouts and interactions of the things and stuff in an image for comprehensive scene understanding. To generate coherent scene graphs, almost all existing methods exploit the fruitful visual context by modeling message passing among objects, fitting the dynamic nature of reasoning with visual context, eg, "person" on "bike" can help to determine the relationship "ride", which in turn contributes to the category confidence of the two objects. However, we argue that the scene dynamics is not properly learned by using the prevailing cross-entropy based supervised learning paradigm, which is not sensitive to graph inconsistency: errors at the hub or non-hub nodes are unfortunately penalized equally. To this end, we propose a Counterfactual critic Multi-Agent Training (CMAT) approach to resolve the mismatch. CMAT is a multi-agent policy gradient method that frames objects as cooperative agents, and then directly maximizes a graph-level metric as the reward. In particular, to assign the reward properly to each agent, CMAT uses a counterfactual baseline that disentangles the agent-specific reward by fixing the dynamics of other agents. Extensive validations on the challenging Visual Genome benchmark show that CMAT achieves a state-of-the-art by significant performance gains under various settings and metrics.



### Context-Aware Synthesis and Placement of Object Instances
- **Arxiv ID**: http://arxiv.org/abs/1812.02350v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.02350v2)
- **Published**: 2018-12-06 05:04:35+00:00
- **Updated**: 2018-12-07 16:46:05+00:00
- **Authors**: Donghoon Lee, Sifei Liu, Jinwei Gu, Ming-Yu Liu, Ming-Hsuan Yang, Jan Kautz
- **Comment**: None
- **Journal**: None
- **Summary**: Learning to insert an object instance into an image in a semantically coherent manner is a challenging and interesting problem. Solving it requires (a) determining a location to place an object in the scene and (b) determining its appearance at the location. Such an object insertion model can potentially facilitate numerous image editing and scene parsing applications. In this paper, we propose an end-to-end trainable neural network for the task of inserting an object instance mask of a specified class into the semantic label map of an image. Our network consists of two generative modules where one determines where the inserted object mask should be (i.e., location and scale) and the other determines what the object mask shape (and pose) should look like. The two modules are connected together via a spatial transformation network and jointly trained. We devise a learning procedure that leverage both supervised and unsupervised data and show our model can insert an object at diverse locations with various appearances. We conduct extensive experimental validations with comparisons to strong baselines to verify the effectiveness of the proposed network.



### Handcrafted and Deep Trackers: Recent Visual Object Tracking Approaches and Trends
- **Arxiv ID**: http://arxiv.org/abs/1812.07368v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.07368v2)
- **Published**: 2018-12-06 06:51:58+00:00
- **Updated**: 2019-02-12 01:27:06+00:00
- **Authors**: Mustansar Fiaz, Arif Mahmood, Sajid Javed, Soon Ki Jung
- **Comment**: 27pages, 26 figures. arXiv admin note: substantial text overlap with
  arXiv:1802.03098
- **Journal**: None
- **Summary**: In recent years visual object tracking has become a very active research area. An increasing number of tracking algorithms are being proposed each year. It is because tracking has wide applications in various real world problems such as human-computer interaction, autonomous vehicles, robotics, surveillance and security just to name a few. In the current study, we review latest trends and advances in the tracking area and evaluate the robustness of different trackers based on the feature extraction methods. The first part of this work comprises a comprehensive survey of the recently proposed trackers. We broadly categorize trackers into Correlation Filter based Trackers (CFTs) and Non-CFTs. Each category is further classified into various types based on the architecture and the tracking mechanism. In the second part, we experimentally evaluated 24 recent trackers for robustness, and compared handcrafted and deep feature based trackers. We observe that trackers using deep features performed better, though in some cases a fusion of both increased performance significantly. In order to overcome the drawbacks of the existing benchmarks, a new benchmark Object Tracking and Temple Color (OTTC) has also been proposed and used in the evaluation of different algorithms. We analyze the performance of trackers over eleven different challenges in OTTC, and three other benchmarks. Our study concludes that Discriminative Correlation Filter (DCF) based trackers perform better than the others. Our study also reveals that inclusion of different types of regularizations over DCF often results in boosted tracking performance. Finally, we sum up our study by pointing out some insights and indicating future trends in visual object tracking field.



### DNQ: Dynamic Network Quantization
- **Arxiv ID**: http://arxiv.org/abs/1812.02375v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1812.02375v1)
- **Published**: 2018-12-06 07:06:17+00:00
- **Updated**: 2018-12-06 07:06:17+00:00
- **Authors**: Yuhui Xu, Shuai Zhang, Yingyong Qi, Jiaxian Guo, Weiyao Lin, Hongkai Xiong
- **Comment**: None
- **Journal**: None
- **Summary**: Network quantization is an effective method for the deployment of neural networks on memory and energy constrained mobile devices. In this paper, we propose a Dynamic Network Quantization (DNQ) framework which is composed of two modules: a bit-width controller and a quantizer. Unlike most existing quantization methods that use a universal quantization bit-width for the whole network, we utilize policy gradient to train an agent to learn the bit-width of each layer by the bit-width controller. This controller can make a trade-off between accuracy and compression ratio. Given the quantization bit-width sequence, the quantizer adopts the quantization distance as the criterion of the weights importance during quantization. We extensively validate the proposed approach on various main-stream neural networks and obtain impressive results.



### Auto-Encoding Scene Graphs for Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/1812.02378v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.02378v3)
- **Published**: 2018-12-06 07:13:33+00:00
- **Updated**: 2018-12-11 01:32:43+00:00
- **Authors**: Xu Yang, Kaihua Tang, Hanwang Zhang, Jianfei Cai
- **Comment**: None
- **Journal**: None
- **Summary**: We propose Scene Graph Auto-Encoder (SGAE) that incorporates the language inductive bias into the encoder-decoder image captioning framework for more human-like captions. Intuitively, we humans use the inductive bias to compose collocations and contextual inference in discourse. For example, when we see the relation `person on bike', it is natural to replace `on' with `ride' and infer `person riding bike on a road' even the `road' is not evident. Therefore, exploiting such bias as a language prior is expected to help the conventional encoder-decoder models less likely overfit to the dataset bias and focus on reasoning. Specifically, we use the scene graph --- a directed graph ($\mathcal{G}$) where an object node is connected by adjective nodes and relationship nodes --- to represent the complex structural layout of both image ($\mathcal{I}$) and sentence ($\mathcal{S}$). In the textual domain, we use SGAE to learn a dictionary ($\mathcal{D}$) that helps to reconstruct sentences in the $\mathcal{S}\rightarrow \mathcal{G} \rightarrow \mathcal{D} \rightarrow \mathcal{S}$ pipeline, where $\mathcal{D}$ encodes the desired language prior; in the vision-language domain, we use the shared $\mathcal{D}$ to guide the encoder-decoder in the $\mathcal{I}\rightarrow \mathcal{G}\rightarrow \mathcal{D} \rightarrow \mathcal{S}$ pipeline. Thanks to the scene graph representation and shared dictionary, the inductive bias is transferred across domains in principle. We validate the effectiveness of SGAE on the challenging MS-COCO image captioning benchmark, e.g., our SGAE-based single-model achieves a new state-of-the-art $127.8$ CIDEr-D on the Karpathy split, and a competitive $125.5$ CIDEr-D (c40) on the official server even compared to other ensemble models.



### Meta-Transfer Learning for Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1812.02391v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.02391v3)
- **Published**: 2018-12-06 07:45:08+00:00
- **Updated**: 2019-04-09 13:39:11+00:00
- **Authors**: Qianru Sun, Yaoyao Liu, Tat-Seng Chua, Bernt Schiele
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: Meta-learning has been proposed as a framework to address the challenging few-shot learning setting. The key idea is to leverage a large number of similar few-shot tasks in order to learn how to adapt a base-learner to a new task for which only a few labeled samples are available. As deep neural networks (DNNs) tend to overfit using a few samples only, meta-learning typically uses shallow neural networks (SNNs), thus limiting its effectiveness. In this paper we propose a novel few-shot learning method called meta-transfer learning (MTL) which learns to adapt a deep NN for few shot learning tasks. Specifically, "meta" refers to training multiple tasks, and "transfer" is achieved by learning scaling and shifting functions of DNN weights for each task. In addition, we introduce the hard task (HT) meta-batch scheme as an effective learning curriculum for MTL. We conduct experiments using (5-class, 1-shot) and (5-class, 5-shot) recognition tasks on two challenging few-shot learning benchmarks: miniImageNet and Fewshot-CIFAR100. Extensive comparisons to related works validate that our meta-transfer learning approach trained with the proposed HT meta-batch scheme achieves top performance. An ablation study also shows that both components contribute to fast convergence and high accuracy.



### Adaptive Scenario Discovery for Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/1812.02393v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.02393v2)
- **Published**: 2018-12-06 07:51:06+00:00
- **Updated**: 2019-02-09 12:01:16+00:00
- **Authors**: Xingjiao Wu, Yingbin Zheng, Hao Ye, Wenxin Hu, Jing Yang, Liang He
- **Comment**: None
- **Journal**: IEEE ICASSP 2019
- **Summary**: Crowd counting, i.e., estimation number of the pedestrian in crowd images, is emerging as an important research problem with the public security applications. A key component for the crowd counting systems is the construction of counting models which are robust to various scenarios under facts such as camera perspective and physical barriers. In this paper, we present an adaptive scenario discovery framework for crowd counting. The system is structured with two parallel pathways that are trained with different sizes of the receptive field to represent different scales and crowd densities. After ensuring that these components are present in the proper geometric configuration, a third branch is designed to adaptively recalibrate the pathway-wise responses by discovering and modeling the dynamic scenarios implicitly. Our system is able to represent highly variable crowd images and achieves state-of-the-art results in two challenging benchmarks.



### DSNet for Real-Time Driving Scene Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1812.07049v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.07049v2)
- **Published**: 2018-12-06 07:59:02+00:00
- **Updated**: 2019-12-06 01:57:07+00:00
- **Authors**: Wenfu Wang, Zhijie Pan
- **Comment**: We have discovered some reported numbers unreproducible, and decided
  to redesign the methods, and rewrite most of the paper
- **Journal**: None
- **Summary**: We focus on the very challenging task of semantic segmentation for autonomous driving system. It must deliver decent semantic segmentation result for traffic critical objects real-time. In this paper, we propose a very efficient yet powerful deep neural network for driving scene semantic segmentation termed as Driving Segmentation Network (DSNet). DSNet achieves state-of-the-art balance between accuracy and inference speed through efficient units and architecture design inspired by ShuffleNet V2 and ENet. More importantly, DSNet highlights classes most critical with driving decision making through our novel Driving Importance-weighted Loss. We evaluate DSNet on Cityscapes dataset, our DSNet achieves 71.8% mean Intersection-over-Union (IoU) on validation set and 69.3% on test set. Class-wise IoU scores show that Driving Importance-weighted Loss could improve most driving critical classes by a large margin. Compared with ENet, DSNet is 18.9% more accurate and 1.1+ times faster which implies great potential for autonomous driving application.



### Trained Rank Pruning for Efficient Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1812.02402v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.02402v3)
- **Published**: 2018-12-06 08:37:54+00:00
- **Updated**: 2020-01-23 21:01:43+00:00
- **Authors**: Yuhui Xu, Yuxi Li, Shuai Zhang, Wei Wen, Botao Wang, Yingyong Qi, Yiran Chen, Weiyao Lin, Hongkai Xiong
- **Comment**: Accepted by NIPS2019 EMC2 workshop, the same version as the withdrawn
  arXiv:1910.04576
- **Journal**: None
- **Summary**: The performance of Deep Neural Networks (DNNs) keeps elevating in recent years with increasing network depth and width. To enable DNNs on edge devices like mobile phones, researchers proposed several network compression methods including pruning, quantization and factorization. Among the factorization-based approaches, low-rank approximation has been widely adopted because of its solid theoretical rationale and efficient implementations. Several previous works attempted to directly approximate a pre-trained model by low-rank decomposition; however, small approximation errors in parameters can ripple a large prediction loss. As a result, performance usually drops significantly and a sophisticated fine-tuning is required to recover accuracy. We argue that it is not optimal to separate low-rank approximation from training. Unlike previous works, this paper integrates low rank approximation and regularization into the training. We propose Trained Rank Pruning (TRP), which iterates low rank approximation and training. TRP maintains the capacity of original network while imposes low-rank constraints during training. A stochastic sub-gradient descent optimized nuclear regularization is utilized to further encourage low rank in TRP. The TRP trained network has low-rank structure in nature, and can be approximated with negligible performance loss, eliminating fine-tuning after low rank approximation. The methods are comprehensively evaluated on CIFAR-10 and ImageNet, outperforming previous compression methods using low rank approximation. Code is available: https://github.com/yuhuixu1993/Trained-Rank-Pruning



### Web Applicable Computer-aided Diagnosis of Glaucoma Using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1812.02405v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.02405v2)
- **Published**: 2018-12-06 08:55:53+00:00
- **Updated**: 2019-04-04 01:56:26+00:00
- **Authors**: Mijung Kim, Olivier Janssens, Ho-min Park, Jasper Zuallaert, Sofie Van Hoecke, Wesley De Neve
- **Comment**: Machine Learning for Health (ML4H) Workshop at NeurIPS 2018
  arXiv:cs/0101200
- **Journal**: None
- **Summary**: Glaucoma is a major eye disease, leading to vision loss in the absence of proper medical treatment. Current diagnosis of glaucoma is performed by ophthalmologists who are often analyzing several types of medical images generated by different types of medical equipment. Capturing and analyzing these medical images is labor-intensive and expensive. In this paper, we present a novel computational approach towards glaucoma diagnosis and localization, only making use of eye fundus images that are analyzed by state-of-the-art deep learning techniques. Specifically, our approach leverages Convolutional Neural Networks (CNNs) and Gradient-weighted Class Activation Mapping (Grad-CAM) for glaucoma diagnosis and localization, respectively. Quantitative and qualitative results, as obtained for a small-sized dataset with no segmentation ground truth, demonstrate that the proposed approach is promising, for instance achieving an accuracy of 0.91$\pm0.02$ and an ROC-AUC score of 0.94 for the diagnosis task. Furthermore, we present a publicly available prototype web application that integrates our predictive model, with the goal of making effective glaucoma diagnosis available to a wide audience.



### Self-supervised Learning of Dense Shape Correspondence
- **Arxiv ID**: http://arxiv.org/abs/1812.02415v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.02415v1)
- **Published**: 2018-12-06 09:26:03+00:00
- **Updated**: 2018-12-06 09:26:03+00:00
- **Authors**: Oshri Halimi, Or Litany, Emanuele Rodolà, Alex Bronstein, Ron Kimmel
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce the first completely unsupervised correspondence learning approach for deformable 3D shapes. Key to our model is the understanding that natural deformations (such as changes in pose) approximately preserve the metric structure of the surface, yielding a natural criterion to drive the learning process toward distortion-minimizing predictions. On this basis, we overcome the need for annotated data and replace it by a purely geometric criterion. The resulting learning model is class-agnostic, and is able to leverage any type of deformable geometric data for the training phase. In contrast to existing supervised approaches which specialize on the class seen at training time, we demonstrate stronger generalization as well as applicability to a variety of challenging settings. We showcase our method on a wide selection of correspondence benchmarks, where we outperform other methods in terms of accuracy, generalization, and efficiency.



### MEAL: Multi-Model Ensemble via Adversarial Learning
- **Arxiv ID**: http://arxiv.org/abs/1812.02425v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.02425v2)
- **Published**: 2018-12-06 09:48:49+00:00
- **Updated**: 2019-07-25 06:32:03+00:00
- **Authors**: Zhiqiang Shen, Zhankui He, Xiangyang Xue
- **Comment**: To appear in AAAI 2019. Code and models are available at:
  https://github.com/AaronHeee/MEAL
- **Journal**: None
- **Summary**: Often the best performing deep neural models are ensembles of multiple base-level networks. Unfortunately, the space required to store these many networks, and the time required to execute them at test-time, prohibits their use in applications where test sets are large (e.g., ImageNet). In this paper, we present a method for compressing large, complex trained ensembles into a single network, where knowledge from a variety of trained deep neural networks (DNNs) is distilled and transferred to a single DNN. In order to distill diverse knowledge from different trained (teacher) models, we propose to use adversarial-based learning strategy where we define a block-wise training loss to guide and optimize the predefined student network to recover the knowledge in teacher models, and to promote the discriminator network to distinguish teacher vs. student features simultaneously. The proposed ensemble method (MEAL) of transferring distilled knowledge with adversarial learning exhibits three important advantages: (1) the student network that learns the distilled knowledge with discriminators is optimized better than the original model; (2) fast inference is realized by a single forward pass, while the performance is even better than traditional ensembles from multi-original models; (3) the student network can learn the distilled knowledge from a teacher model that has arbitrary structures. Extensive experiments on CIFAR-10/100, SVHN and ImageNet datasets demonstrate the effectiveness of our MEAL method. On ImageNet, our ResNet-50 based MEAL achieves top-1/5 21.79%/5.99% val error, which outperforms the original model by 2.06%/1.14%. Code and models are available at: https://github.com/AaronHeee/MEAL



### Segmentation of Head and Neck Organs at Risk Using CNN with Batch Dice Loss
- **Arxiv ID**: http://arxiv.org/abs/1812.02427v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.02427v1)
- **Published**: 2018-12-06 09:50:49+00:00
- **Updated**: 2018-12-06 09:50:49+00:00
- **Authors**: Oldřich Kodym, Michal Španěl, Adam Herout
- **Comment**: None
- **Journal**: None
- **Summary**: This paper deals with segmentation of organs at risk (OAR) in head and neck area in CT images which is a crucial step for reliable intensity modulated radiotherapy treatment. We introduce a convolution neural network with encoder-decoder architecture and a new loss function, the batch soft Dice loss function, used to train the network. The resulting model produces segmentations of every OAR in the public MICCAI 2015 Head And Neck Auto-Segmentation Challenge dataset. Despite the heavy class imbalance in the data, we improve accuracy of current state-of-the-art methods by 0.33 mm in terms of average surface distance and by 0.11 in terms of Dice overlap coefficient on average.



### Pseudo-Rehearsal: Achieving Deep Reinforcement Learning without Catastrophic Forgetting
- **Arxiv ID**: http://arxiv.org/abs/1812.02464v6
- **DOI**: 10.1016/j.neucom.2020.11.050
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1812.02464v6)
- **Published**: 2018-12-06 11:20:18+00:00
- **Updated**: 2020-12-16 21:38:56+00:00
- **Authors**: Craig Atkinson, Brendan McCane, Lech Szymanski, Anthony Robins
- **Comment**: None
- **Journal**: None
- **Summary**: Neural networks can achieve excellent results in a wide variety of applications. However, when they attempt to sequentially learn, they tend to learn the new task while catastrophically forgetting previous ones. We propose a model that overcomes catastrophic forgetting in sequential reinforcement learning by combining ideas from continual learning in both the image classification domain and the reinforcement learning domain. This model features a dual memory system which separates continual learning from reinforcement learning and a pseudo-rehearsal system that "recalls" items representative of previous tasks via a deep generative network. Our model sequentially learns Atari 2600 games without demonstrating catastrophic forgetting and continues to perform above human level on all three games. This result is achieved without: demanding additional storage requirements as the number of tasks increases, storing raw data or revisiting past tasks. In comparison, previous state-of-the-art solutions are substantially more vulnerable to forgetting on these complex deep reinforcement learning tasks.



### Fast and Accurate Person Re-Identification with RMNet
- **Arxiv ID**: http://arxiv.org/abs/1812.02465v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.02465v1)
- **Published**: 2018-12-06 11:20:37+00:00
- **Updated**: 2018-12-06 11:20:37+00:00
- **Authors**: Evgeny Izutov
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we introduce a new neural network architecture designed to use in embedded vision applications. It merges the best working practices of network architectures like MobileNets and ResNets to our named RMNet architecture. We also focus on key moments of building mobile architectures to carry out in the limited computation budget. Additionally, to demonstrate the effectiveness of our architecture we evaluate the RMNet backbone on Person Re-identification task. The proposed approach is in top 3 of state of the art solutions on Market-1501 challenge, however our method significantly outperforms them by the inference speed.



### Deep Embedding using Bayesian Risk Minimization with Application to Sketch Recognition
- **Arxiv ID**: http://arxiv.org/abs/1812.02466v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.02466v1)
- **Published**: 2018-12-06 11:20:58+00:00
- **Updated**: 2018-12-06 11:20:58+00:00
- **Authors**: Anand Mishra, Ajeet Kumar Singh
- **Comment**: Accepted at ACCV 2018
- **Journal**: None
- **Summary**: In this paper, we address the problem of hand-drawn sketch recognition. Inspired by the Bayesian decision theory, we present a deep metric learning loss with the objective to minimize the Bayesian risk of misclassification. We estimate this risk for every mini-batch during training, and learn robust deep embeddings by backpropagating it to a deep neural network in an end-to-end trainable paradigm. Our learnt embeddings are discriminative and robust despite of intra-class variations and inter-class similarities naturally present in hand-drawn sketch images. Outperforming the state of the art on sketch recognition, our method achieves 82.2% and 88.7% on TU-Berlin-250 and TU-Berlin-160 benchmarks respectively.



### Unsupervised Single Image Dehazing Using Dark Channel Prior Loss
- **Arxiv ID**: http://arxiv.org/abs/1812.07051v2
- **DOI**: 10.1109/TIP.2019.2952032
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.07051v2)
- **Published**: 2018-12-06 11:29:38+00:00
- **Updated**: 2019-08-20 11:40:23+00:00
- **Authors**: Alona Golts, Daniel Freedman, Michael Elad
- **Comment**: None
- **Journal**: None
- **Summary**: Single image dehazing is a critical stage in many modern-day autonomous vision applications. Early prior-based methods often involved a time-consuming minimization of a hand-crafted energy function. Recent learning-based approaches utilize the representational power of deep neural networks (DNNs) to learn the underlying transformation between hazy and clear images. Due to inherent limitations in collecting matching clear and hazy images, these methods resort to training on synthetic data; constructed from indoor images and corresponding depth information. This may result in a possible domain shift when treating outdoor scenes. We propose a completely unsupervised method of training via minimization of the well-known, Dark Channel Prior (DCP) energy function. Instead of feeding the network with synthetic data, we solely use real-world outdoor images and tune the network's parameters by directly minimizing the DCP. Although our "Deep DCP" technique can be regarded as a fast approximator of DCP, it actually improves its results significantly. This suggests an additional regularization obtained via the network and learning process. Experiments show that our method performs on par with large-scale supervised methods.



### Binary Document Image Super Resolution for Improved Readability and OCR Performance
- **Arxiv ID**: http://arxiv.org/abs/1812.02475v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.02475v1)
- **Published**: 2018-12-06 11:41:07+00:00
- **Updated**: 2018-12-06 11:41:07+00:00
- **Authors**: Ram Krishna Pandey, K Vignesh, A G Ramakrishnan, Chandrahasa B
- **Comment**: None
- **Journal**: None
- **Summary**: There is a need for information retrieval from large collections of low-resolution (LR) binary document images, which can be found in digital libraries across the world, where the high-resolution (HR) counterpart is not available. This gives rise to the problem of binary document image super-resolution (BDISR). The objective of this paper is to address the interesting and challenging problem of super resolution of binary Tamil document images for improved readability and better optical character recognition (OCR). We propose multiple deep neural network architectures to address this problem and analyze their performance. The proposed models are all single image super-resolution techniques, which learn a generalized spatial correspondence between the LR and HR binary document images. We employ convolutional layers for feature extraction followed by transposed convolution and sub-pixel convolution layers for upscaling the features. Since the outputs of the neural networks are gray scale, we utilize the advantage of power law transformation as a post-processing technique to improve the character level pixel connectivity. The performance of our models is evaluated by comparing the OCR accuracies and the mean opinion scores given by human evaluators on LR images and the corresponding model-generated HR images.



### Learning to Infer the Depth Map of a Hand from its Color Image
- **Arxiv ID**: http://arxiv.org/abs/1812.02486v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.02486v1)
- **Published**: 2018-12-06 12:16:37+00:00
- **Updated**: 2018-12-06 12:16:37+00:00
- **Authors**: Vassilis C. Nicodemou, Iason Oikonomidis, Georgios Tzimiropoulos, Antonis Argyros
- **Comment**: None
- **Journal**: None
- **Summary**: We propose the first approach to the problem of inferring the depth map of a human hand based on a single RGB image. We achieve this with a Convolutional Neural Network (CNN) that employs a stacked hourglass model as its main building block. Intermediate supervision is used in several outputs of the proposed architecture in a staged approach. To aid the process of training and inference, hand segmentation masks are also estimated in such an intermediate supervision step, and used to guide the subsequent depth estimation process. In order to train and evaluate the proposed method we compile and make publicly available HandRGBD, a new dataset of 20,601 views of hands, each consisting of an RGB image and an aligned depth map. Based on HandRGBD, we explore variants of the proposed approach in an ablative study and determine the best performing one. The results of an extensive experimental evaluation demonstrate that hand depth estimation from a single RGB frame can be achieved with an accuracy of 22mm, which is comparable to the accuracy achieved by contemporary low-cost depth cameras. Such a 3D reconstruction of hands based on RGB information is valuable as a final result on its own right, but also as an input to several other hand analysis and perception algorithms that require depth input. Essentially, in such a context, the proposed approach bridges the gap between RGB and RGBD, by making all existing RGBD-based methods applicable to RGB input.



### Prediction of final infarct volume from native CT perfusion and treatment parameters using deep learning
- **Arxiv ID**: http://arxiv.org/abs/1812.02496v2
- **DOI**: 10.1016/j.media.2019.101589
- **Categories**: **cs.CV**, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1812.02496v2)
- **Published**: 2018-12-06 12:34:18+00:00
- **Updated**: 2019-10-14 12:21:53+00:00
- **Authors**: David Robben, Anna M. M. Boers, Henk A. Marquering, Lucianne L. C. M. Langezaal, Yvo B. W. E. M. Roos, Robert J. van Oostenbrugge, Wim H. van Zwam, Diederik W. J. Dippel, Charles B. L. M. Majoie, Aad van der Lugt, Robin Lemmens, Paul Suetens
- **Comment**: Accepted for publication in Medical Image Analysis
- **Journal**: None
- **Summary**: CT Perfusion (CTP) imaging has gained importance in the diagnosis of acute stroke. Conventional perfusion analysis performs a deconvolution of the measurements and thresholds the perfusion parameters to determine the tissue status. We pursue a data-driven and deconvolution-free approach, where a deep neural network learns to predict the final infarct volume directly from the native CTP images and metadata such as the time parameters and treatment. This would allow clinicians to simulate various treatments and gain insight into predicted tissue status over time. We demonstrate on a multicenter dataset that our approach is able to predict the final infarct and effectively uses the metadata. An ablation study shows that using the native CTP measurements instead of the deconvolved measurements improves the prediction.



### Zero-Shot Anticipation for Instructional Activities
- **Arxiv ID**: http://arxiv.org/abs/1812.02501v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.02501v3)
- **Published**: 2018-12-06 12:48:45+00:00
- **Updated**: 2019-10-20 22:44:45+00:00
- **Authors**: Fadime Sener, Angela Yao
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: How can we teach a robot to predict what will happen next for an activity it has never seen before? We address this problem of zero-shot anticipation by presenting a hierarchical model that generalizes instructional knowledge from large-scale text-corpora and transfers the knowledge to the visual domain. Given a portion of an instructional video, our model predicts coherent and plausible actions multiple steps into the future, all in rich natural language. To demonstrate the anticipation capabilities of our model, we introduce the Tasty Videos dataset, a collection of 2511 recipes for zero-shot learning, recognition and anticipation.



### ForensicTransfer: Weakly-supervised Domain Adaptation for Forgery Detection
- **Arxiv ID**: http://arxiv.org/abs/1812.02510v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.02510v2)
- **Published**: 2018-12-06 13:11:09+00:00
- **Updated**: 2019-11-27 10:02:18+00:00
- **Authors**: Davide Cozzolino, Justus Thies, Andreas Rössler, Christian Riess, Matthias Nießner, Luisa Verdoliva
- **Comment**: None
- **Journal**: None
- **Summary**: Distinguishing manipulated from real images is becoming increasingly difficult as new sophisticated image forgery approaches come out by the day. Naive classification approaches based on Convolutional Neural Networks (CNNs) show excellent performance in detecting image manipulations when they are trained on a specific forgery method. However, on examples from unseen manipulation approaches, their performance drops significantly. To address this limitation in transferability, we introduce Forensic-Transfer (FT). We devise a learning-based forensic detector which adapts well to new domains, i.e., novel manipulation methods and can handle scenarios where only a handful of fake examples are available during training. To this end, we learn a forensic embedding based on a novel autoencoder-based architecture that can be used to distinguish between real and fake imagery. The learned embedding acts as a form of anomaly detector; namely, an image manipulated from an unseen method will be detected as fake provided it maps sufficiently far away from the cluster of real images. Comparing to prior works, FT shows significant improvements in transferability, which we demonstrate in a series of experiments on cutting-edge benchmarks. For instance, on unseen examples, we achieve up to 85% in terms of accuracy, and with only a handful of seen examples, our performance already reaches around 95%.



### Automatically Segmenting the Left Atrium from Cardiac Images Using Successive 3D U-Nets and a Contour Loss
- **Arxiv ID**: http://arxiv.org/abs/1812.02518v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.02518v1)
- **Published**: 2018-12-06 13:34:24+00:00
- **Updated**: 2018-12-06 13:34:24+00:00
- **Authors**: Shuman Jia, Antoine Despinasse, Zihao Wang, Hervé Delingette, Xavier Pennec, Pierre Jaïs, Hubert Cochet, Maxime Sermesant
- **Comment**: None
- **Journal**: None
- **Summary**: Radiological imaging offers effective measurement of anatomy, which is useful in disease diagnosis and assessment. Previous study has shown that the left atrial wall remodeling can provide information to predict treatment outcome in atrial fibrillation. Nevertheless, the segmentation of the left atrial structures from medical images is still very time-consuming. Current advances in neural network may help creating automatic segmentation models that reduce the workload for clinicians. In this preliminary study, we propose automated, two-stage, three-dimensional U-Nets with convolutional neural network, for the challenging task of left atrial segmentation. Unlike previous two-dimensional image segmentation methods, we use 3D U-Nets to obtain the heart cavity directly in 3D. The dual 3D U-Net structure consists of, a first U-Net to coarsely segment and locate the left atrium, and a second U-Net to accurately segment the left atrium under higher resolution. In addition, we introduce a Contour loss based on additional distance information to adjust the final segmentation. We randomly split the data into training datasets (80 subjects) and validation datasets (20 subjects) to train multiple models, with different augmentation setting. Experiments show that the average Dice coefficients for validation datasets are around 0.91 - 0.92, the sensitivity around 0.90-0.94 and the specificity 0.99. Compared with traditional Dice loss, models trained with Contour loss in general offer smaller Hausdorff distance with similar Dice coefficient, and have less connected components in predictions. Finally, we integrate several trained models in an ensemble prediction to segment testing datasets.



### Representing pictures with emotions
- **Arxiv ID**: http://arxiv.org/abs/1812.02523v2
- **DOI**: None
- **Categories**: **cs.IR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1812.02523v2)
- **Published**: 2018-12-06 13:50:40+00:00
- **Updated**: 2018-12-07 13:35:05+00:00
- **Authors**: António Filipe Fonseca
- **Comment**: V1.1
- **Journal**: None
- **Summary**: Modern research in content-based image retrieval systems (CIBR) has become progressively more focused on the richness of human semantics. Several approaches may be used to reduced the 'semantic gap' between the high-level human experience and the low level visual features of pictures. Object ontology, among others, is one of the methods. In this paper we investigate the use of a codified emotion ontology over global color features of images to annotate the images at a high semantic level. In order to speed up the annotation process the images are sampled so that each digital image is represented by a random subset of its content. We test within controlled conditions how this random subset may represent the adequate high level emotional concept presented in the image. We monitor this information reducing process with entropy measures, showing that controlled random sampling can capture with significant relevance high level concepts for picture representation.



### Towards Leveraging the Information of Gradients in Optimization-based Adversarial Attack
- **Arxiv ID**: http://arxiv.org/abs/1812.02524v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.02524v1)
- **Published**: 2018-12-06 13:55:23+00:00
- **Updated**: 2018-12-06 13:55:23+00:00
- **Authors**: Jingyang Zhang, Hsin-Pai Cheng, Chunpeng Wu, Hai Li, Yiran Chen
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, deep neural networks demonstrated state-of-the-art performance in a large variety of tasks and therefore have been adopted in many applications. On the other hand, the latest studies revealed that neural networks are vulnerable to adversarial examples obtained by carefully adding small perturbation to legitimate samples. Based upon the observation, many attack methods were proposed. Among them, the optimization-based CW attack is the most powerful as the produced adversarial samples present much less distortion compared to other methods. The better attacking effect, however, comes at the cost of running more iterations and thus longer computation time to reach desirable results. In this work, we propose to leverage the information of gradients as a guidance during the search of adversaries. More specifically, directly incorporating the gradients into the perturbation can be regarded as a constraint added to the optimization process. We intuitively and empirically prove the rationality of our method in reducing the search space. Our experiments show that compared to the original CW attack, the proposed method requires fewer iterations towards adversarial samples, obtaining a higher success rate and resulting in smaller $\ell_2$ distortion.



### Segmentation-driven 6D Object Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1812.02541v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.02541v3)
- **Published**: 2018-12-06 14:15:24+00:00
- **Updated**: 2019-04-06 05:33:21+00:00
- **Authors**: Yinlin Hu, Joachim Hugonot, Pascal Fua, Mathieu Salzmann
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: The most recent trend in estimating the 6D pose of rigid objects has been to train deep networks to either directly regress the pose from the image or to predict the 2D locations of 3D keypoints, from which the pose can be obtained using a PnP algorithm. In both cases, the object is treated as a global entity, and a single pose estimate is computed. As a consequence, the resulting techniques can be vulnerable to large occlusions.   In this paper, we introduce a segmentation-driven 6D pose estimation framework where each visible part of the objects contributes a local pose prediction in the form of 2D keypoint locations. We then use a predicted measure of confidence to combine these pose candidates into a robust set of 3D-to-2D correspondences, from which a reliable pose estimate can be obtained. We outperform the state-of-the-art on the challenging Occluded-LINEMOD and YCB-Video datasets, which is evidence that our approach deals well with multiple poorly-textured objects occluding each other. Furthermore, it relies on a simple enough architecture to achieve real-time performance.



### Computer Vision for Autonomous Vehicles
- **Arxiv ID**: http://arxiv.org/abs/1812.02542v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.02542v1)
- **Published**: 2018-12-06 14:16:56+00:00
- **Updated**: 2018-12-06 14:16:56+00:00
- **Authors**: Rohit Gandikota
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we try to implement Image Processing techniques in the area of autonomous vehicles, both indoor and outdoor. The challenges for both are different and the ways to tackle them vary too. We also showed deep learning makes things easier and precise. We also made base models for all the problems we tackle while building an autonomous car for Indian Institute of Space science and Technology.



### BiHMP-GAN: Bidirectional 3D Human Motion Prediction GAN
- **Arxiv ID**: http://arxiv.org/abs/1812.02591v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.02591v1)
- **Published**: 2018-12-06 15:07:56+00:00
- **Updated**: 2018-12-06 15:07:56+00:00
- **Authors**: Jogendra Nath Kundu, Maharshi Gor, R. Venkatesh Babu
- **Comment**: Accepted at AAAI 2019
- **Journal**: None
- **Summary**: Human motion prediction model has applications in various fields of computer vision. Without taking into account the inherent stochasticity in the prediction of future pose dynamics, such methods often converges to a deterministic undesired mean of multiple probable outcomes. Devoid of this, we propose a novel probabilistic generative approach called Bidirectional Human motion prediction GAN, or BiHMP-GAN. To be able to generate multiple probable human-pose sequences, conditioned on a given starting sequence, we introduce a random extrinsic factor r, drawn from a predefined prior distribution. Furthermore, to enforce a direct content loss on the predicted motion sequence and also to avoid mode-collapse, a novel bidirectional framework is incorporated by modifying the usual discriminator architecture. The discriminator is trained also to regress this extrinsic factor r, which is used alongside with the intrinsic factor (encoded starting pose sequence) to generate a particular pose sequence. To further regularize the training, we introduce a novel recursive prediction strategy. In spite of being in a probabilistic framework, the enhanced discriminator architecture allows predictions of an intermediate part of pose sequence to be used as a conditioning for prediction of the latter part of the sequence. The bidirectional setup also provides a new direction to evaluate the prediction quality against a given test sequence. For a fair assessment of BiHMP-GAN, we report performance of the generated motion sequence using (i) a critic model trained to discriminate between real and fake motion sequence, and (ii) an action classifier trained on real human motion dynamics. Outcomes of both qualitative and quantitative evaluations, on the probabilistic generations of the model, demonstrate the superiority of BiHMP-GAN over previously available methods.



### Unsupervised Feature Learning of Human Actions as Trajectories in Pose Embedding Manifold
- **Arxiv ID**: http://arxiv.org/abs/1812.02592v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.02592v1)
- **Published**: 2018-12-06 15:08:02+00:00
- **Updated**: 2018-12-06 15:08:02+00:00
- **Authors**: Jogendra Nath Kundu, Maharshi Gor, Phani Krishna Uppala, R. Venkatesh Babu
- **Comment**: Accepted at WACV 2019
- **Journal**: None
- **Summary**: An unsupervised human action modeling framework can provide useful pose-sequence representation, which can be utilized in a variety of pose analysis applications. In this work we propose a novel temporal pose-sequence modeling framework, which can embed the dynamics of 3D human-skeleton joints to a continuous latent space in an efficient manner. In contrast to end-to-end framework explored by previous works, we disentangle the task of individual pose representation learning from the task of learning actions as a trajectory in pose embedding space. In order to realize a continuous pose embedding manifold with improved reconstructions, we propose an unsupervised, manifold learning procedure named Encoder GAN, (or EnGAN). Further, we use the pose embeddings generated by EnGAN to model human actions using a bidirectional RNN auto-encoder architecture, PoseRNN. We introduce first-order gradient loss to explicitly enforce temporal regularity in the predicted motion sequence. A hierarchical feature fusion technique is also investigated for simultaneous modeling of local skeleton joints along with global pose variations. We demonstrate state-of-the-art transfer-ability of the learned representation against other supervisedly and unsupervisedly learned motion embeddings for the task of fine-grained action recognition on SBU interaction dataset. Further, we show the qualitative strengths of the proposed framework by visualizing skeleton pose reconstructions and interpolations in pose-embedding space, and low dimensional principal component projections of the reconstructed pose trajectories.



### Disjoint Label Space Transfer Learning with Common Factorised Space
- **Arxiv ID**: http://arxiv.org/abs/1812.02605v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.02605v1)
- **Published**: 2018-12-06 15:33:36+00:00
- **Updated**: 2018-12-06 15:33:36+00:00
- **Authors**: Xiaobin Chang, Yongxin Yang, Tao Xiang, Timothy M. Hospedales
- **Comment**: AAAI-19
- **Journal**: None
- **Summary**: In this paper, a unified approach is presented to transfer learning that addresses several source and target domain label-space and annotation assumptions with a single model. It is particularly effective in handling a challenging case, where source and target label-spaces are disjoint, and outperforms alternatives in both unsupervised and semi-supervised settings. The key ingredient is a common representation termed Common Factorised Space. It is shared between source and target domains, and trained with an unsupervised factorisation loss and a graph-based loss. With a wide range of experiments, we demonstrate the flexibility, relevance and efficacy of our method, both in the challenging cases with disjoint label spaces, and in the more conventional cases such as unsupervised domain adaptation, where the source and target domains share the same label-sets.



### OMNIA Faster R-CNN: Detection in the wild through dataset merging and soft distillation
- **Arxiv ID**: http://arxiv.org/abs/1812.02611v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.02611v2)
- **Published**: 2018-12-06 15:38:43+00:00
- **Updated**: 2019-03-25 17:28:03+00:00
- **Authors**: Alexandre Rame, Emilien Garreau, Hedi Ben-Younes, Charles Ollion
- **Comment**: 9 pages, 5 figures, 4 tables
- **Journal**: None
- **Summary**: Object detectors tend to perform poorly in new or open domains, and require exhaustive yet costly annotations from fully labeled datasets. We aim at benefiting from several datasets with different categories but without additional labelling, not only to increase the number of categories detected, but also to take advantage from transfer learning and to enhance domain independence.   Our dataset merging procedure starts with training several initial Faster R-CNN on the different datasets while considering the complementary datasets' images for domain adaptation. Similarly to self-training methods, the predictions of these initial detectors mitigate the missing annotations on the complementary datasets. The final OMNIA Faster R-CNN is trained with all categories on the union of the datasets enriched by predictions. The joint training handles unsafe targets with a new classification loss called SoftSig in a softly supervised way.   Experimental results show that in the case of fashion detection for images in the wild, merging Modanet with COCO increases the final performance from 45.5% to 57.4% in mAP. Applying our soft distillation to the task of detection with domain shift between GTA and Cityscapes enables to beat the state-of-the-art by 5.3 points. Our methodology could unlock object detection for real-world applications without immense datasets.



### Tube-CNN: Modeling temporal evolution of appearance for object detection in video
- **Arxiv ID**: http://arxiv.org/abs/1812.02619v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.02619v1)
- **Published**: 2018-12-06 15:48:54+00:00
- **Updated**: 2018-12-06 15:48:54+00:00
- **Authors**: Tuan-Hung Vu, Anton Osokin, Ivan Laptev
- **Comment**: 13 pages, 8 figures, technical report
- **Journal**: None
- **Summary**: Object detection in video is crucial for many applications. Compared to images, video provides additional cues which can help to disambiguate the detection problem. Our goal in this paper is to learn discriminative models for the temporal evolution of object appearance and to use such models for object detection. To model temporal evolution, we introduce space-time tubes corresponding to temporal sequences of bounding boxes. We propose two CNN architectures for generating and classifying tubes, respectively. Our tube proposal network (TPN) first generates a large number of spatio-temporal tube proposals maximizing object recall. The Tube-CNN then implements a tube-level object detector in the video. Our method improves state of the art on two large-scale datasets for object detection in video: HollywoodHeads and ImageNet VID. Tube models show particular advantages in difficult dynamic scenes.



### Guided Zoom: Questioning Network Evidence for Fine-grained Classification
- **Arxiv ID**: http://arxiv.org/abs/1812.02626v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.02626v2)
- **Published**: 2018-12-06 16:00:05+00:00
- **Updated**: 2020-03-23 11:16:06+00:00
- **Authors**: Sarah Adel Bargal, Andrea Zunino, Vitali Petsiuk, Jianming Zhang, Kate Saenko, Vittorio Murino, Stan Sclaroff
- **Comment**: BMVC 2019 Camera Ready Version
- **Journal**: None
- **Summary**: We propose Guided Zoom, an approach that utilizes spatial grounding of a model's decision to make more informed predictions. It does so by making sure the model has "the right reasons" for a prediction, defined as reasons that are coherent with those used to make similar correct decisions at training time. The reason/evidence upon which a deep convolutional neural network makes a prediction is defined to be the spatial grounding, in the pixel space, for a specific class conditional probability in the model output. Guided Zoom examines how reasonable such evidence is for each of the top-k predicted classes, rather than solely trusting the top-1 prediction. We show that Guided Zoom improves the classification accuracy of a deep convolutional neural network model and obtains state-of-the-art results on three fine-grained classification benchmark datasets.



### Traversing Latent Space using Decision Ferns
- **Arxiv ID**: http://arxiv.org/abs/1812.02636v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.02636v1)
- **Published**: 2018-12-06 16:15:24+00:00
- **Updated**: 2018-12-06 16:15:24+00:00
- **Authors**: Yan Zuo, Gil Avraham, Tom Drummond
- **Comment**: None
- **Journal**: None
- **Summary**: The practice of transforming raw data to a feature space so that inference can be performed in that space has been popular for many years. Recently, rapid progress in deep neural networks has given both researchers and practitioners enhanced methods that increase the richness of feature representations, be it from images, text or speech. In this work we show how a constructed latent space can be explored in a controlled manner and argue that this complements well founded inference methods. For constructing the latent space a Variational Autoencoder is used. We present a novel controller module that allows for smooth traversal in the latent space and construct an end-to-end trainable framework. We explore the applicability of our method for performing spatial transformations as well as kinematics for predicting future latent vectors of a video sequence.



### Pathological Evidence Exploration in Deep Retinal Image Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/1812.02640v1
- **DOI**: 10.1609/aaai.v33i01.33011093
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.02640v1)
- **Published**: 2018-12-06 16:18:33+00:00
- **Updated**: 2018-12-06 16:18:33+00:00
- **Authors**: Yuhao Niu, Lin Gu, Feng Lu, Feifan Lv, Zongji Wang, Imari Sato, Zijian Zhang, Yangyan Xiao, Xunzhang Dai, Tingting Cheng
- **Comment**: to appear in AAAI (2019). The first two authors contributed equally
  to the paper. Corresponding Author: Feng Lu
- **Journal**: AAAI 2019: 1093-1101
- **Summary**: Though deep learning has shown successful performance in classifying the label and severity stage of certain disease, most of them give few evidence on how to make prediction. Here, we propose to exploit the interpretability of deep learning application in medical diagnosis. Inspired by Koch's Postulates, a well-known strategy in medical research to identify the property of pathogen, we define a pathological descriptor that can be extracted from the activated neurons of a diabetic retinopathy detector. To visualize the symptom and feature encoded in this descriptor, we propose a GAN based method to synthesize pathological retinal image given the descriptor and a binary vessel segmentation. Besides, with this descriptor, we can arbitrarily manipulate the position and quantity of lesions. As verified by a panel of 5 licensed ophthalmologists, our synthesized images carry the symptoms that are directly related to diabetic retinopathy diagnosis. The panel survey also shows that our generated images is both qualitatively and quantitatively superior to existing methods.



### Recursive Visual Attention in Visual Dialog
- **Arxiv ID**: http://arxiv.org/abs/1812.02664v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.02664v2)
- **Published**: 2018-12-06 17:00:16+00:00
- **Updated**: 2019-04-06 15:02:24+00:00
- **Authors**: Yulei Niu, Hanwang Zhang, Manli Zhang, Jianhong Zhang, Zhiwu Lu, Ji-Rong Wen
- **Comment**: CVPR 2019 (Oral)
- **Journal**: None
- **Summary**: Visual dialog is a challenging vision-language task, which requires the agent to answer multi-round questions about an image. It typically needs to address two major problems: (1) How to answer visually-grounded questions, which is the core challenge in visual question answering (VQA); (2) How to infer the co-reference between questions and the dialog history. An example of visual co-reference is: pronouns (\eg, ``they'') in the question (\eg, ``Are they on or off?'') are linked with nouns (\eg, ``lamps'') appearing in the dialog history (\eg, ``How many lamps are there?'') and the object grounded in the image. In this work, to resolve the visual co-reference for visual dialog, we propose a novel attention mechanism called Recursive Visual Attention (RvA). Specifically, our dialog agent browses the dialog history until the agent has sufficient confidence in the visual co-reference resolution, and refines the visual attention recursively. The quantitative and qualitative experimental results on the large-scale VisDial v0.9 and v1.0 datasets demonstrate that the proposed RvA not only outperforms the state-of-the-art methods, but also achieves reasonable recursion and interpretable attention maps without additional annotations. The code is available at \url{https://github.com/yuleiniu/rva}.



### Theoretical Guarantees of Deep Embedding Losses Under Label Noise
- **Arxiv ID**: http://arxiv.org/abs/1812.02676v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1812.02676v2)
- **Published**: 2018-12-06 17:19:01+00:00
- **Updated**: 2019-01-02 17:43:16+00:00
- **Authors**: Nam Le, Jean-Marc Odobez
- **Comment**: None
- **Journal**: None
- **Summary**: Collecting labeled data to train deep neural networks is costly and even impractical for many tasks. Thus, research effort has been focused in automatically curated datasets or unsupervised and weakly supervised learning. The common problem in these directions is learning with unreliable label information. In this paper, we address the tolerance of deep embedding learning losses against label noise, i.e. when the observed labels are different from the true labels. Specifically, we provide the sufficient conditions to achieve theoretical guarantees for the 2 common loss functions: marginal loss and triplet loss. From these theoretical results, we can estimate how sampling strategies and initialization can affect the level of resistance against label noise. The analysis also helps providing more effective guidelines in unsupervised and weakly supervised deep embedding learning.



### Online Model Distillation for Efficient Video Inference
- **Arxiv ID**: http://arxiv.org/abs/1812.02699v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.02699v2)
- **Published**: 2018-12-06 18:29:59+00:00
- **Updated**: 2020-01-27 21:57:10+00:00
- **Authors**: Ravi Teja Mullapudi, Steven Chen, Keyi Zhang, Deva Ramanan, Kayvon Fatahalian
- **Comment**: None
- **Journal**: ICCV 2019
- **Summary**: High-quality computer vision models typically address the problem of understanding the general distribution of real-world images. However, most cameras observe only a very small fraction of this distribution. This offers the possibility of achieving more efficient inference by specializing compact, low-cost models to the specific distribution of frames observed by a single camera. In this paper, we employ the technique of model distillation (supervising a low-cost student model using the output of a high-cost teacher) to specialize accurate, low-cost semantic segmentation models to a target video stream. Rather than learn a specialized student model on offline data from the video stream, we train the student in an online fashion on the live video, intermittently running the teacher to provide a target for learning. Online model distillation yields semantic segmentation models that closely approximate their Mask R-CNN teacher with 7 to 17$\times$ lower inference runtime cost (11 to 26$\times$ in FLOPs), even when the target video's distribution is non-stationary. Our method requires no offline pretraining on the target video stream, achieves higher accuracy and lower cost than solutions based on flow or video object segmentation, and can exhibit better temporal stability than the original teacher. We also provide a new video dataset for evaluating the efficiency of inference over long running video streams.



### Video Action Transformer Network
- **Arxiv ID**: http://arxiv.org/abs/1812.02707v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.02707v2)
- **Published**: 2018-12-06 18:42:25+00:00
- **Updated**: 2019-05-17 14:17:25+00:00
- **Authors**: Rohit Girdhar, João Carreira, Carl Doersch, Andrew Zisserman
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: We introduce the Action Transformer model for recognizing and localizing human actions in video clips. We repurpose a Transformer-style architecture to aggregate features from the spatiotemporal context around the person whose actions we are trying to classify. We show that by using high-resolution, person-specific, class-agnostic queries, the model spontaneously learns to track individual people and to pick up on semantic context from the actions of others. Additionally its attention mechanism learns to emphasize hands and faces, which are often crucial to discriminate an action - all without explicit supervision other than boxes and class labels. We train and test our Action Transformer network on the Atomic Visual Actions (AVA) dataset, outperforming the state-of-the-art by a significant margin using only raw RGB frames as input.



### PartNet: A Large-scale Benchmark for Fine-grained and Hierarchical Part-level 3D Object Understanding
- **Arxiv ID**: http://arxiv.org/abs/1812.02713v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.02713v1)
- **Published**: 2018-12-06 18:48:25+00:00
- **Updated**: 2018-12-06 18:48:25+00:00
- **Authors**: Kaichun Mo, Shilin Zhu, Angel X. Chang, Li Yi, Subarna Tripathi, Leonidas J. Guibas, Hao Su
- **Comment**: None
- **Journal**: None
- **Summary**: We present PartNet: a consistent, large-scale dataset of 3D objects annotated with fine-grained, instance-level, and hierarchical 3D part information. Our dataset consists of 573,585 part instances over 26,671 3D models covering 24 object categories. This dataset enables and serves as a catalyst for many tasks such as shape analysis, dynamic 3D scene modeling and simulation, affordance analysis, and others. Using our dataset, we establish three benchmarking tasks for evaluating 3D part recognition: fine-grained semantic segmentation, hierarchical semantic segmentation, and instance segmentation. We benchmark four state-of-the-art 3D deep learning algorithms for fine-grained semantic segmentation and three baseline methods for hierarchical semantic segmentation. We also propose a novel method for part instance segmentation and demonstrate its superior performance over existing methods.



### Cross-Domain 3D Equivariant Image Embeddings
- **Arxiv ID**: http://arxiv.org/abs/1812.02716v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.02716v2)
- **Published**: 2018-12-06 18:51:12+00:00
- **Updated**: 2019-05-14 19:21:59+00:00
- **Authors**: Carlos Esteves, Avneesh Sud, Zhengyi Luo, Kostas Daniilidis, Ameesh Makadia
- **Comment**: Accepted to the International Conference on Machine Learning, ICML
  2019
- **Journal**: None
- **Summary**: Spherical convolutional networks have been introduced recently as tools to learn powerful feature representations of 3D shapes. Spherical CNNs are equivariant to 3D rotations making them ideally suited to applications where 3D data may be observed in arbitrary orientations. In this paper we learn 2D image embeddings with a similar equivariant structure: embedding the image of a 3D object should commute with rotations of the object. We introduce a cross-domain embedding from 2D images into a spherical CNN latent space. This embedding encodes images with 3D shape properties and is equivariant to 3D rotations of the observed object. The model is supervised only by target embeddings obtained from a spherical CNN pretrained for 3D shape classification. We show that learning a rich embedding for images with appropriate geometric structure is sufficient for tackling varied applications, such as relative pose estimation and novel view synthesis, without requiring additional task-specific supervision.



### Visual Object Networks: Image Generation with Disentangled 3D Representation
- **Arxiv ID**: http://arxiv.org/abs/1812.02725v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.02725v1)
- **Published**: 2018-12-06 18:58:34+00:00
- **Updated**: 2018-12-06 18:58:34+00:00
- **Authors**: Jun-Yan Zhu, Zhoutong Zhang, Chengkai Zhang, Jiajun Wu, Antonio Torralba, Joshua B. Tenenbaum, William T. Freeman
- **Comment**: NeurIPS 2018. Code: https://github.com/junyanz/VON Website:
  http://von.csail.mit.edu/
- **Journal**: None
- **Summary**: Recent progress in deep generative models has led to tremendous breakthroughs in image generation. However, while existing models can synthesize photorealistic images, they lack an understanding of our underlying 3D world. We present a new generative model, Visual Object Networks (VON), synthesizing natural images of objects with a disentangled 3D representation. Inspired by classic graphics rendering pipelines, we unravel our image formation process into three conditionally independent factors---shape, viewpoint, and texture---and present an end-to-end adversarial learning framework that jointly models 3D shapes and 2D images. Our model first learns to synthesize 3D shapes that are indistinguishable from real shapes. It then renders the object's 2.5D sketches (i.e., silhouette and depth map) from its shape under a sampled viewpoint. Finally, it learns to add realistic texture to these 2.5D sketches to generate natural images. The VON not only generates images that are more realistic than state-of-the-art 2D image synthesis methods, but also enables many 3D operations such as changing the viewpoint of a generated image, editing of shape and texture, linear interpolation in texture and shape space, and transferring appearance across different objects and viewpoints.



### Knockoff Nets: Stealing Functionality of Black-Box Models
- **Arxiv ID**: http://arxiv.org/abs/1812.02766v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.02766v1)
- **Published**: 2018-12-06 19:34:33+00:00
- **Updated**: 2018-12-06 19:34:33+00:00
- **Authors**: Tribhuvanesh Orekondy, Bernt Schiele, Mario Fritz
- **Comment**: None
- **Journal**: None
- **Summary**: Machine Learning (ML) models are increasingly deployed in the wild to perform a wide range of tasks. In this work, we ask to what extent can an adversary steal functionality of such "victim" models based solely on blackbox interactions: image in, predictions out. In contrast to prior work, we present an adversary lacking knowledge of train/test data used by the model, its internals, and semantics over model outputs. We formulate model functionality stealing as a two-step approach: (i) querying a set of input images to the blackbox model to obtain predictions; and (ii) training a "knockoff" with queried image-prediction pairs. We make multiple remarkable observations: (a) querying random images from a different distribution than that of the blackbox training data results in a well-performing knockoff; (b) this is possible even when the knockoff is represented using a different architecture; and (c) our reinforcement learning approach additionally improves query sample efficiency in certain settings and provides performance gains. We validate model functionality stealing on a range of datasets and tasks, as well as on a popular image analysis API where we create a reasonable knockoff for as little as $30.



### Neural Word Search in Historical Manuscript Collections
- **Arxiv ID**: http://arxiv.org/abs/1812.02771v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.02771v2)
- **Published**: 2018-12-06 19:48:42+00:00
- **Updated**: 2020-03-31 18:51:13+00:00
- **Authors**: Tomas Wilkinson, Jonas Lindström, Anders Brun
- **Comment**: Extension of arXiv:1703.07645. This version adds results on two
  additional benchmark datasets (Botany and Konzilsprotokolle) and improves the
  experiment done in section 5.3.1
- **Journal**: None
- **Summary**: We address the problem of segmenting and retrieving word images in collections of historical manuscripts given a text query. This is commonly referred to as "word spotting". To this end, we first propose an end-to-end trainable model based on deep neural networks that we dub Ctrl-F-Net. The model simultaneously generates region proposals and embeds them into a word embedding space, wherein a search is performed. We further introduce a simplified version called Ctrl-F-Mini. It is faster with similar performance, though it is limited to more easily segmented manuscripts. We evaluate both models on common benchmark datasets and surpass the previous state of the art. Finally, in collaboration with historians, we employ the Ctrl-F-Net to search within a large manuscript collection of over 100 thousand pages, written across two centuries. With only 11 training pages, we enable large scale data collection in manuscript-based historical research. This results in a speed up of data collection and the number of manuscripts processed by orders of magnitude. Given the time consuming manual work required to study old manuscripts in the humanities, quick and robust tools for word spotting has the potential to revolutionise domains like history, religion and language.



### Object Discovery in Videos as Foreground Motion Clustering
- **Arxiv ID**: http://arxiv.org/abs/1812.02772v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.02772v2)
- **Published**: 2018-12-06 19:51:45+00:00
- **Updated**: 2019-04-05 01:52:22+00:00
- **Authors**: Christopher Xie, Yu Xiang, Zaid Harchaoui, Dieter Fox
- **Comment**: None
- **Journal**: None
- **Summary**: We consider the problem of providing dense segmentation masks for object discovery in videos. We formulate the object discovery problem as foreground motion clustering, where the goal is to cluster foreground pixels in videos into different objects. We introduce a novel pixel-trajectory recurrent neural network that learns feature embeddings of foreground pixel trajectories linked across time. By clustering the pixel trajectories using the learned feature embeddings, our method establishes correspondences between foreground object masks across video frames. To demonstrate the effectiveness of our framework for object discovery, we conduct experiments on commonly used datasets for motion segmentation, where we achieve state-of-the-art performance.



### ROI-10D: Monocular Lifting of 2D Detection to 6D Pose and Metric Shape
- **Arxiv ID**: http://arxiv.org/abs/1812.02781v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.02781v3)
- **Published**: 2018-12-06 20:08:39+00:00
- **Updated**: 2019-04-10 11:43:18+00:00
- **Authors**: Fabian Manhardt, Wadim Kehl, Adrien Gaidon
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: We present a deep learning method for end-to-end monocular 3D object detection and metric shape retrieval. We propose a novel loss formulation by lifting 2D detection, orientation, and scale estimation into 3D space. Instead of optimizing these quantities separately, the 3D instantiation allows to properly measure the metric misalignment of boxes. We experimentally show that our 10D lifting of sparse 2D Regions of Interests (RoIs) achieves great results both for 6D pose and recovery of the textured metric geometry of instances. This further enables 3D synthetic data augmentation via inpainting recovered meshes directly onto the 2D scenes. We evaluate on KITTI3D against other strong monocular methods and demonstrate that our approach doubles the AP on the 3D pose metrics on the official test set, defining the new state of the art.



### StoryGAN: A Sequential Conditional GAN for Story Visualization
- **Arxiv ID**: http://arxiv.org/abs/1812.02784v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.02784v2)
- **Published**: 2018-12-06 20:10:23+00:00
- **Updated**: 2019-04-18 04:59:05+00:00
- **Authors**: Yitong Li, Zhe Gan, Yelong Shen, Jingjing Liu, Yu Cheng, Yuexin Wu, Lawrence Carin, David Carlson, Jianfeng Gao
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new task, called Story Visualization. Given a multi-sentence paragraph, the story is visualized by generating a sequence of images, one for each sentence. In contrast to video generation, story visualization focuses less on the continuity in generated images (frames), but more on the global consistency across dynamic scenes and characters -- a challenge that has not been addressed by any single-image or video generation methods. We therefore propose a new story-to-image-sequence generation model, StoryGAN, based on the sequential conditional GAN framework. Our model is unique in that it consists of a deep Context Encoder that dynamically tracks the story flow, and two discriminators at the story and image levels, to enhance the image quality and the consistency of the generated sequences. To evaluate the model, we modified existing datasets to create the CLEVR-SV and Pororo-SV datasets. Empirically, StoryGAN outperforms state-of-the-art models in image quality, contextual consistency metrics, and human evaluation.



### Tri-axial Self-Attention for Concurrent Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/1812.02817v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.02817v1)
- **Published**: 2018-12-06 21:39:14+00:00
- **Updated**: 2018-12-06 21:39:14+00:00
- **Authors**: Yanyi Zhang, Xinyu Li, Kaixiang Huang, Yehan Wang, Shuhong Chen, Ivan Marsic
- **Comment**: None
- **Journal**: None
- **Summary**: We present a system for concurrent activity recognition. To extract features associated with different activities, we propose a feature-to-activity attention that maps the extracted global features to sub-features associated with individual activities. To model the temporal associations of individual activities, we propose a transformer-network encoder that models independent temporal associations for each activity. To make the concurrent activity prediction aware of the potential associations between activities, we propose self-attention with an association mask. Our system achieved state-of-the-art or comparable performance on three commonly used concurrent activity detection datasets. Our visualizations demonstrate that our system is able to locate the important spatial-temporal features for final decision making. We also showed that our system can be applied to general multilabel classification problems.



### Learning Implicit Fields for Generative Shape Modeling
- **Arxiv ID**: http://arxiv.org/abs/1812.02822v5
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.02822v5)
- **Published**: 2018-12-06 21:52:33+00:00
- **Updated**: 2019-09-16 20:29:07+00:00
- **Authors**: Zhiqin Chen, Hao Zhang
- **Comment**: Accepted to CVPR 2019. Code:
  https://github.com/czq142857/implicit-decoder Project page:
  https://www.sfu.ca/~zhiqinc/imgan/Readme.html
- **Journal**: None
- **Summary**: We advocate the use of implicit fields for learning generative models of shapes and introduce an implicit field decoder, called IM-NET, for shape generation, aimed at improving the visual quality of the generated shapes. An implicit field assigns a value to each point in 3D space, so that a shape can be extracted as an iso-surface. IM-NET is trained to perform this assignment by means of a binary classifier. Specifically, it takes a point coordinate, along with a feature vector encoding a shape, and outputs a value which indicates whether the point is outside the shape or not. By replacing conventional decoders by our implicit decoder for representation learning (via IM-AE) and shape generation (via IM-GAN), we demonstrate superior results for tasks such as generative shape modeling, interpolation, and single-view 3D reconstruction, particularly in terms of visual quality. Code and supplementary material are available at https://github.com/czq142857/implicit-decoder.



### Neural Image Decompression: Learning to Render Better Image Previews
- **Arxiv ID**: http://arxiv.org/abs/1812.02831v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1812.02831v1)
- **Published**: 2018-12-06 22:12:39+00:00
- **Updated**: 2018-12-06 22:12:39+00:00
- **Authors**: Shumeet Baluja, Dave Marwood, Nick Johnston, Michele Covell
- **Comment**: None
- **Journal**: None
- **Summary**: A rapidly increasing portion of Internet traffic is dominated by requests from mobile devices with limited- and metered-bandwidth constraints. To satisfy these requests, it has become standard practice for websites to transmit small and extremely compressed image previews as part of the initial page-load process. Recent work, based on an adaptive triangulation of the target image, has shown the ability to generate thumbnails of full images at extreme compression rates: 200 bytes or less with impressive gains (in terms of PSNR and SSIM) over both JPEG and WebP standards. However, qualitative assessments and preservation of semantic content can be less favorable. We present a novel method to significantly improve the reconstruction quality of the original image with no changes to the encoded information. Our neural-based decoding not only achieves higher PSNR and SSIM scores than the original methods, but also yields a substantial increase in semantic-level content preservation. In addition, by keeping the same encoding stream, our solution is completely inter-operable with the original decoder. The end result is suitable for a range of small-device deployments, as it involves only a single forward-pass through a small, scalable network.



### High-Quality Face Capture Using Anatomical Muscles
- **Arxiv ID**: http://arxiv.org/abs/1812.02836v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.02836v1)
- **Published**: 2018-12-06 22:30:31+00:00
- **Updated**: 2018-12-06 22:30:31+00:00
- **Authors**: Michael Bao, Matthew Cong, Stéphane Grabli, Ronald Fedkiw
- **Comment**: None
- **Journal**: None
- **Summary**: Muscle-based systems have the potential to provide both anatomical accuracy and semantic interpretability as compared to blendshape models; however, a lack of expressivity and differentiability has limited their impact. Thus, we propose modifying a recently developed rather expressive muscle-based system in order to make it fully-differentiable; in fact, our proposed modifications allow this physically robust and anatomically accurate muscle model to conveniently be driven by an underlying blendshape basis. Our formulation is intuitive, natural, as well as monolithically and fully coupled such that one can differentiate the model from end to end, which makes it viable for both optimization and learning-based approaches for a variety of applications. We illustrate this with a number of examples including both shape matching of three-dimensional geometry as as well as the automatic determination of a three-dimensional facial pose from a single two-dimensional RGB image without using markers or depth information.



### Fooling Network Interpretation in Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1812.02843v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.02843v2)
- **Published**: 2018-12-06 22:53:53+00:00
- **Updated**: 2019-09-24 23:48:28+00:00
- **Authors**: Akshayvarun Subramanya, Vipin Pillai, Hamed Pirsiavash
- **Comment**: Accepted at ICCV 2019
- **Journal**: None
- **Summary**: Deep neural networks have been shown to be fooled rather easily using adversarial attack algorithms. Practical methods such as adversarial patches have been shown to be extremely effective in causing misclassification. However, these patches are highlighted using standard network interpretation algorithms, thus revealing the identity of the adversary. We show that it is possible to create adversarial patches which not only fool the prediction, but also change what we interpret regarding the cause of the prediction. Moreover, we introduce our attack as a controlled setting to measure the accuracy of interpretation algorithms. We show this using extensive experiments for Grad-CAM interpretation that transfers to occluding patch interpretation as well. We believe our algorithms can facilitate developing more robust network interpretation tools that truly explain the network's underlying decision making process.



