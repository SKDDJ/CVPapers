# Arxiv Papers in cs.CV on 2018-12-22
### Image Embedding of PMU Data for Deep Learning towards Transient Disturbance Classification
- **Arxiv ID**: http://arxiv.org/abs/1812.09427v1
- **DOI**: 10.1109/ICEI.2018.00038
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.09427v1)
- **Published**: 2018-12-22 01:08:47+00:00
- **Updated**: 2018-12-22 01:08:47+00:00
- **Authors**: Yongli Zhu, Chengxi Liu, Kai Sun
- **Comment**: An updated version of this manuscript has been accepted by the 2018
  IEEE International Conference on Energy Internet (ICEI), Beijing, China
- **Journal**: None
- **Summary**: This paper presents a study on power grid disturbance classification by Deep Learning (DL). A real synchrophasor set composing of three different types of disturbance events from the Frequency Monitoring Network (FNET) is used. An image embedding technique called Gramian Angular Field is applied to transform each time series of event data to a two-dimensional image for learning. Two main DL algorithms, i.e. CNN (Convolutional Neural Network) and RNN (Recurrent Neural Network) are tested and compared with two widely used data mining tools, the Support Vector Machine and Decision Tree. The test results demonstrate the superiority of the both DL algorithms over other methods in the application of power system transient disturbance classification.



### Dissociable neural representations of adversarially perturbed images in convolutional neural networks and the human brain
- **Arxiv ID**: http://arxiv.org/abs/1812.09431v3
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1812.09431v3)
- **Published**: 2018-12-22 01:56:04+00:00
- **Updated**: 2020-07-20 01:28:40+00:00
- **Authors**: Chi Zhang, Xiaohan Duan, Linyuan Wang, Yongli Li, Bin Yan, Guoen Hu, Ruyuan Zhang, Li Tong
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the remarkable similarities between convolutional neural networks (CNN) and the human brain, CNNs still fall behind humans in many visual tasks, indicating that there still exist considerable differences between the two systems. Here, we leverage adversarial noise (AN) and adversarial interference (AI) images to quantify the consistency between neural representations and perceptual outcomes in the two systems. Humans can successfully recognize AI images as corresponding categories but perceive AN images as meaningless noise. In contrast, CNNs can correctly recognize AN images but mistakenly classify AI images into wrong categories with surprisingly high confidence. We use functional magnetic resonance imaging to measure brain activity evoked by regular and adversarial images in the human brain, and compare it to the activity of artificial neurons in a prototypical CNN-AlexNet. In the human brain, we find that the representational similarity between regular and adversarial images largely echoes their perceptual similarity in all early visual areas. In AlexNet, however, the neural representations of adversarial images are inconsistent with network outputs in all intermediate processing layers, providing no neural foundations for perceptual similarity. Furthermore, we show that voxel-encoding models trained on regular images can successfully generalize to the neural responses to AI images but not AN images. These remarkable differences between the human brain and AlexNet in the representation-perception relation suggest that future CNNs should emulate both behavior and the internal neural presentations of the human brain.



### Fully Automatic Segmentation of Sublingual Veins from Retrained U-Net Model for Few Near Infrared Images
- **Arxiv ID**: http://arxiv.org/abs/1812.09477v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1812.09477v1)
- **Published**: 2018-12-22 08:27:45+00:00
- **Updated**: 2018-12-22 08:27:45+00:00
- **Authors**: Tingxiao Yang, Yuichiro Yoshimura, Akira Morita, Takao Namiki, Toshiya Nakaguchi
- **Comment**: IMQA 2018 Conference
- **Journal**: None
- **Summary**: Sublingual vein is commonly used to diagnose the health status. The width of main sublingual veins gives information of the blood circulation. Therefore, it is necessary to segment the main sublingual veins from the tongue automatically. In general, the dataset in the medical field is small, which is a challenge for training the deep learning model. In order to train the model with a small data set, the proposed method for automatically segmenting the sublingual veins is to re-train U-net model with different sets of the limited number of labels for the same training images. With pre-knowledge of the segmentation, the loss of the trained model will be convergence easier. To improve the performance of the segmentation further, a novel strategy of data augmentation was utilized. The operation for masking output of the model with the input was randomly switched on or switched off in each training step. This approach will force the model to learn the contrast invariance and avoid overfitting. Images of dataset were taken with the developed device using eight near infrared LEDs. The final segmentation results were evaluated on the validation dataset by the IoU metric.



### Disentangling Latent Space for VAE by Label Relevant/Irrelevant Dimensions
- **Arxiv ID**: http://arxiv.org/abs/1812.09502v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.09502v4)
- **Published**: 2018-12-22 11:09:50+00:00
- **Updated**: 2019-03-15 12:01:42+00:00
- **Authors**: Zhilin Zheng, Li Sun
- **Comment**: Accepted by CVPR 2019
- **Journal**: None
- **Summary**: VAE requires the standard Gaussian distribution as a prior in the latent space. Since all codes tend to follow the same prior, it often suffers the so-called "posterior collapse". To avoid this, this paper introduces the class specific distribution for the latent code. But different from CVAE, we present a method for disentangling the latent space into the label relevant and irrelevant dimensions, $\bm{\mathrm{z}}_s$ and $\bm{\mathrm{z}}_u$, for a single input. We apply two separated encoders to map the input into $\bm{\mathrm{z}}_s$ and $\bm{\mathrm{z}}_u$ respectively, and then give the concatenated code to the decoder to reconstruct the input. The label irrelevant code $\bm{\mathrm{z}}_u$ represent the common characteristics of all inputs, hence they are constrained by the standard Gaussian, and their encoder is trained in amortized variational inference way, like VAE. While $\bm{\mathrm{z}}_s$ is assumed to follow the Gaussian mixture distribution in which each component corresponds to a particular class. The parameters for the Gaussian components in $\bm{\mathrm{z}}_s$ encoder are optimized by the label supervision in a global stochastic way. In theory, we show that our method is actually equivalent to adding a KL divergence term on the joint distribution of $\bm{\mathrm{z}}_s$ and the class label $c$, and it can directly increase the mutual information between $\bm{\mathrm{z}}_s$ and the label $c$. Our model can also be extended to GAN by adding a discriminator in the pixel domain so that it produces high quality and diverse images.



### Dimensionality Reduction of Hyperspectral Imagery Based on Spatial-spectral Manifold Learning
- **Arxiv ID**: http://arxiv.org/abs/1812.09530v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.09530v1)
- **Published**: 2018-12-22 14:06:21+00:00
- **Updated**: 2018-12-22 14:06:21+00:00
- **Authors**: Hong Huang, Guangyao Shi, Haibo He, Yule Duan, Fulin Luo
- **Comment**: under review in IEEE Transactions On Cybernetics
- **Journal**: None
- **Summary**: The graph embedding (GE) methods have been widely applied for dimensionality reduction of hyperspectral imagery (HSI). However, a major challenge of GE is how to choose proper neighbors for graph construction and explore the spatial information of HSI data. In this paper, we proposed an unsupervised dimensionality reduction algorithm termed spatial-spectral manifold reconstruction preserving embedding (SSMRPE) for HSI classification. At first, a weighted mean filter (WMF) is employed to preprocess the image, which aims to reduce the influence of background noise. According to the spatial consistency property of HSI, the SSMRPE method utilizes a new spatial-spectral combined distance (SSCD) to fuse the spatial structure and spectral information for selecting effective spatial-spectral neighbors of HSI pixels. Then, it explores the spatial relationship between each point and its neighbors to adjusts the reconstruction weights for improving the efficiency of manifold reconstruction. As a result, the proposed method can extract the discriminant features and subsequently improve the classification performance of HSI. The experimental results on PaviaU and Salinas hyperspectral datasets indicate that SSMRPE can achieve better classification accuracies in comparison with some state-of-the-art methods.



### Temporal Hockey Action Recognition via Pose and Optical Flows
- **Arxiv ID**: http://arxiv.org/abs/1812.09533v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.09533v1)
- **Published**: 2018-12-22 14:15:15+00:00
- **Updated**: 2018-12-22 14:15:15+00:00
- **Authors**: Zixi Cai, Helmut Neher, Kanav Vats, David Clausi, John Zelek
- **Comment**: None
- **Journal**: None
- **Summary**: Recognizing actions in ice hockey using computer vision poses challenges due to bulky equipment and inadequate image quality. A novel two-stream framework has been designed to improve action recognition accuracy for hockey using three main components. First, pose is estimated via the Part Affinity Fields model to extract meaningful cues from the player. Second, optical flow (using LiteFlowNet) is used to extract temporal features. Third, pose and optical flow streams are fused and passed to fully-connected layers to estimate the hockey player's action. A novel publicly available dataset named HARPET (Hockey Action Recognition Pose Estimation, Temporal) was created, composed of sequences of annotated actions and pose of hockey players including their hockey sticks as an extension of human body pose. Three contributions are recognized. (1) The novel two-stream architecture achieves 85% action recognition accuracy, with the inclusion of optical flows increasing accuracy by about 10%. (2) The unique localization of hand-held objects (e.g., hockey sticks) as part of pose increases accuracy by about 13%. (3) For pose estimation, a bigger and more general dataset, MSCOCO, is successfully used for transfer learning to a smaller and more specific dataset, HARPET, achieving a PCKh of 87%.



### The algorithm of formation of a training set for an artificial neural network for image segmentation
- **Arxiv ID**: http://arxiv.org/abs/1812.09569v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1812.09569v1)
- **Published**: 2018-12-22 17:21:59+00:00
- **Updated**: 2018-12-22 17:21:59+00:00
- **Authors**: S. V. Belim, S. B. Larionov
- **Comment**: None
- **Journal**: None
- **Summary**: This article suggests an algorithm of formation a training set for artificial neural network in case of image segmentation. The distinctive feature of this algorithm is that it using only one image for segmentation. The segmentation performs using three-layer perceptron. The main method of the segmentation is a method of region growing. Neural network is using for get a decision to include pixel into an area or not. Impulse noise is using for generation of a training set. Pixels damaged by noise are not related to the same region. Suggested method has been tested with help of computer experiment in automatic and interactive modes.



### EgoReID Dataset: Person Re-identification in Videos Acquired by Mobile Devices with First-Person Point-of-View
- **Arxiv ID**: http://arxiv.org/abs/1812.09570v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.09570v4)
- **Published**: 2018-12-22 17:32:36+00:00
- **Updated**: 2019-09-05 21:28:51+00:00
- **Authors**: Emrah Basaran, Yonatan Tariku Tesfaye, Mubarak Shah
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, we have seen the performance of video-based person Re-Identification (ReID) methods have improved considerably. However, most of the work in this area has dealt with videos acquired by fixed cameras with wider field of view. Recently, widespread use of wearable cameras and recording devices such as cellphones have opened the door to interesting research in first-person Point-of-view (POV) videos (egocentric videos). Nonetheless, analysis of such videos is challenging due to factors such as poor video quality due to ego-motion, blurriness, severe changes in lighting conditions and perspective distortions. To facilitate the research towards conquering these challenges, this paper contributes a new dataset called EgoReID. The dataset is captured using 3 mobile cellphones with non-overlapping field-of-view. It contains 900 IDs and around 10,200 tracks with a total of 176,000 detections. The dataset also contains 12-sensor meta data e.g. camera orientation pitch and rotation for each video.   In addition, we propose a new framework which takes advantage of both visual and sensor meta data to successfully perform Person ReID. We extend image-based re-ID method employing human body parsing trained on ten datasets to video-based re-ID. In our method, first frame level local features are extracted for each semantic region, then 3D convolutions are applied to encode the temporal information in each sequence of semantic regions. Additionally, we employ sensor meta data to predict targets' next camera and their estimated time of arrival, which considerably improves our ReID performance as it significantly reduces our search space.



