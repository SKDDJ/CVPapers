# Arxiv Papers in cs.CV on 2018-12-19
### Unsupervised Video Object Segmentation with Distractor-Aware Online Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1812.07712v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.07712v1)
- **Published**: 2018-12-19 00:45:10+00:00
- **Updated**: 2018-12-19 00:45:10+00:00
- **Authors**: Ye Wang, Jongmoo Choi, Yueru Chen, Siyang Li, Qin Huang, Kaitai Zhang, Ming-Sui Lee, C. -C. Jay Kuo
- **Comment**: 11 pages, 6 figures, 4 tables, conference
- **Journal**: None
- **Summary**: Unsupervised video object segmentation is a crucial application in video analysis without knowing any prior information about the objects. It becomes tremendously challenging when multiple objects occur and interact in a given video clip. In this paper, a novel unsupervised video object segmentation approach via distractor-aware online adaptation (DOA) is proposed. DOA models spatial-temporal consistency in video sequences by capturing background dependencies from adjacent frames. Instance proposals are generated by the instance segmentation network for each frame and then selected by motion information as hard negatives if they exist and positives. To adopt high-quality hard negatives, the block matching algorithm is then applied to preceding frames to track the associated hard negatives. General negatives are also introduced in case that there are no hard negatives in the sequence and experiments demonstrate both kinds of negatives (distractors) are complementary. Finally, we conduct DOA using the positive, negative, and hard negative masks to update the foreground/background segmentation. The proposed approach achieves state-of-the-art results on two benchmark datasets, DAVIS 2016 and FBMS-59 datasets.



### Learning Symmetry Consistent Deep CNNs for Face Completion
- **Arxiv ID**: http://arxiv.org/abs/1812.07741v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.07741v1)
- **Published**: 2018-12-19 03:25:45+00:00
- **Updated**: 2018-12-19 03:25:45+00:00
- **Authors**: Xiaoming Li, Ming Liu, Jieru Zhu, Wangmeng Zuo, Meng Wang, Guosheng Hu, Lei Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolutional networks (CNNs) have achieved great success in face completion to generate plausible facial structures. These methods, however, are limited in maintaining global consistency among face components and recovering fine facial details. On the other hand, reflectional symmetry is a prominent property of face image and benefits face recognition and consistency modeling, yet remaining uninvestigated in deep face completion. In this work, we leverage two kinds of symmetry-enforcing subnets to form a symmetry-consistent CNN model (i.e., SymmFCNet) for effective face completion. For missing pixels on only one of the half-faces, an illumination-reweighted warping subnet is developed to guide the warping and illumination reweighting of the other half-face. As for missing pixels on both of half-faces, we present a generative reconstruction subnet together with a perceptual symmetry loss to enforce symmetry consistency of recovered structures. The SymmFCNet is constructed by stacking generative reconstruction subnet upon illumination-reweighted warping subnet, and can be end-to-end learned from training set of unaligned face images. Experiments show that SymmFCNet can generate high quality results on images with synthetic and real occlusion, and performs favorably against state-of-the-arts.



### Cross-Database Micro-Expression Recognition: A Benchmark
- **Arxiv ID**: http://arxiv.org/abs/1812.07742v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.07742v2)
- **Published**: 2018-12-19 03:26:44+00:00
- **Updated**: 2019-11-11 10:38:43+00:00
- **Authors**: Yuan Zong, Tong Zhang, Wenming Zheng, Xiaopeng Hong, Chuangao Tang, Zhen Cui, Guoying Zhao
- **Comment**: 13 pages
- **Journal**: None
- **Summary**: Cross-database micro-expression recognition (CDMER) is one of recently emerging and interesting problem in micro-expression analysis. CDMER is more challenging than the conventional micro-expression recognition (MER), because the training and testing samples in CDMER come from different micro-expression databases, resulting in the inconsistency of the feature distributions between the training and testing sets. In this paper, we contribute to this topic from three aspects. First, we establish a CDMER experimental evaluation protocol aiming to allow the researchers to conveniently work on this topic and provide a standard platform for evaluating their proposed methods. Second, we conduct benchmark experiments by using NINE state-of-the-art domain adaptation (DA) methods and SIX popular spatiotemporal descriptors for respectively investigating CDMER problem from two different perspectives. Third, we propose a novel DA method called region selective transfer regression (RSTR) to deal with the CDMER task. Our RSTR takes advantage of one important cue for recognizing micro-expressions, i.e., the different contributions of the facial local regions in MER. The overall superior performance of RSTR demonstrates that taking into consideration the important cues benefiting MER, e.g., the facial local region information, contributes to develop effective DA methods for dealing with CDMER problem.



### Discriminative analysis of the human cortex using spherical CNNs - a study on Alzheimer's disease diagnosis
- **Arxiv ID**: http://arxiv.org/abs/1812.07749v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.07749v1)
- **Published**: 2018-12-19 04:16:18+00:00
- **Updated**: 2018-12-19 04:16:18+00:00
- **Authors**: Xinyang Feng, Jie Yang, Andrew F. Laine, Elsa D. Angelini
- **Comment**: None
- **Journal**: None
- **Summary**: In neuroimaging studies, the human cortex is commonly modeled as a sphere to preserve the topological structure of the cortical surface. Cortical neuroimaging measures hence can be modeled in spherical representation. In this work, we explore analyzing the human cortex using spherical CNNs in an Alzheimer's disease (AD) classification task using cortical morphometric measures derived from structural MRI. Our results show superior performance in classifying AD versus cognitively normal and in predicting MCI progression within two years, using structural MRI information only. This work demonstrates for the first time the potential of the spherical CNNs framework in the discriminative analysis of the human cortex and could be extended to other modalities and other neurological diseases.



### Generative One-Shot Learning (GOL): A Semi-Parametric Approach to One-Shot Learning in Autonomous Vision
- **Arxiv ID**: http://arxiv.org/abs/1812.07567v1
- **DOI**: 10.1109/ICRA.2018.8461174
- **Categories**: **cs.CV**, cs.LG, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.07567v1)
- **Published**: 2018-12-19 04:22:15+00:00
- **Updated**: 2018-12-19 04:22:15+00:00
- **Authors**: Sorin Grigorescu
- **Comment**: Web-site: http://rovislab.com/gol.html
- **Journal**: Int. Conf. on Robotics and Automation ICRA 2018
- **Summary**: Highly Autonomous Driving (HAD) systems rely on deep neural networks for the visual perception of the driving environment. Such networks are trained on large manually annotated databases. In this work, a semi-parametric approach to one-shot learning is proposed, with the aim of bypassing the manual annotation step required for training perceptions systems used in autonomous driving. The proposed generative framework, coined Generative One-Shot Learning (GOL), takes as input single one-shot objects, or generic patterns, and a small set of so-called regularization samples used to drive the generative process. New synthetic data is generated as Pareto optimal solutions from one-shot objects using a set of generalization functions built into a generalization generator. GOL has been evaluated on environment perception challenges encountered in autonomous vision.



### Learning On-Road Visual Control for Self-Driving Vehicles with Auxiliary Tasks
- **Arxiv ID**: http://arxiv.org/abs/1812.07760v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.07760v1)
- **Published**: 2018-12-19 05:29:53+00:00
- **Updated**: 2018-12-19 05:29:53+00:00
- **Authors**: Yilun Chen, Praveen Palanisamy, Priyantha Mudalige, Katharina Muelling, John M. Dolan
- **Comment**: 2019 IEEE Winter Conference on Applications of Computer Vision (WACV
  2019)
- **Journal**: None
- **Summary**: A safe and robust on-road navigation system is a crucial component of achieving fully automated vehicles. NVIDIA recently proposed an End-to-End algorithm that can directly learn steering commands from raw pixels of a front camera by using one convolutional neural network. In this paper, we leverage auxiliary information aside from raw images and design a novel network structure, called Auxiliary Task Network (ATN), to help boost the driving performance while maintaining the advantage of minimal training data and an End-to-End training method. In this network, we introduce human prior knowledge into vehicle navigation by transferring features from image recognition tasks. Image semantic segmentation is applied as an auxiliary task for navigation. We consider temporal information by introducing an LSTM module and optical flow to the network. Finally, we combine vehicle kinematics with a sensor fusion step. We discuss the benefits of our method over state-of-the-art visual navigation methods both in the Udacity simulation environment and on the real-world Comma.ai dataset.



### Real-Time, Highly Accurate Robotic Grasp Detection using Fully Convolutional Neural Network with Rotation Ensemble Module
- **Arxiv ID**: http://arxiv.org/abs/1812.07762v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1812.07762v3)
- **Published**: 2018-12-19 05:38:47+00:00
- **Updated**: 2019-09-18 04:50:27+00:00
- **Authors**: Dongwon Park, Yonghyeok Seo, Se Young Chun
- **Comment**: 7 pages, 9 figures, 4 tables
- **Journal**: None
- **Summary**: Rotation invariance has been an important topic in computer vision tasks. Ideally, robot grasp detection should be rotation-invariant. However, rotation-invariance in robotic grasp detection has been only recently studied by using rotation anchor box that are often time-consuming and unreliable for multiple objects. In this paper, we propose a rotation ensemble module (REM) for robotic grasp detection using convolutions that rotates network weights. Our proposed REM was able to outperform current state-of-the-art methods by achieving up to 99.2% (image-wise), 98.6% (object-wise) accuracies on the Cornell dataset with real-time computation (50 frames per second). Our proposed method was also able to yield reliable grasps for multiple objects and up to 93.8% success rate for the real-time robotic grasping task with a 4-axis robot arm for small novel objects that was significantly higher than the baseline methods by 11-56%.



### Light Weight Color Image Warping with Inter-Channel Information
- **Arxiv ID**: http://arxiv.org/abs/1812.07763v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.07763v1)
- **Published**: 2018-12-19 05:43:49+00:00
- **Updated**: 2018-12-19 05:43:49+00:00
- **Authors**: Chuangye Zhang, Yan Niu, Tieru Wu, Ximing Li
- **Comment**: None
- **Journal**: None
- **Summary**: Image warping is a necessary step in many multimedia applications such as texture mapping, image-based rendering, panorama stitching, image resizing and optical flow computation etc. Traditionally, color image warping interpolation is performed in each color channel independently. In this paper, we show that the warping quality can be significantly enhanced by exploiting the cross-channel correlation. We design a warping scheme that integrates intra-channel interpolation with cross-channel variation at very low computational cost, which is required for interactive multimedia applications on mobile devices. The effectiveness and efficiency of our method are validated by extensive experiments.



### Mini-Unmanned Aerial Vehicle-Based Remote Sensing: Techniques, Applications, and Prospects
- **Arxiv ID**: http://arxiv.org/abs/1812.07770v3
- **DOI**: 10.1109/MGRS.2019.2918840
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.07770v3)
- **Published**: 2018-12-19 06:12:09+00:00
- **Updated**: 2020-03-15 13:08:45+00:00
- **Authors**: Tian-Zhu Xiang, Gui-Song Xia, Liangpei Zhang
- **Comment**: None
- **Journal**: IEEE Geoscience and Remote Sensing Magazine, 2019, Vol. 7, No. 3,
  pp. 29-63
- **Summary**: The past few decades have witnessed the great progress of unmanned aircraft vehicles (UAVs) in civilian fields, especially in photogrammetry and remote sensing. In contrast with the platforms of manned aircraft and satellite, the UAV platform holds many promising characteristics: flexibility, efficiency, high-spatial/temporal resolution, low cost, easy operation, etc., which make it an effective complement to other remote-sensing platforms and a cost-effective means for remote sensing. Considering the popularity and expansion of UAV-based remote sensing in recent years, this paper provides a systematic survey on the recent advances and future prospectives of UAVs in the remote-sensing community. Specifically, the main challenges and key technologies of remote-sensing data processing based on UAVs are discussed and summarized firstly. Then, we provide an overview of the widespread applications of UAVs in remote sensing. Finally, some prospects for future work are discussed. We hope this paper will provide remote-sensing researchers an overall picture of recent UAV-based remote sensing developments and help guide the further research on this topic.



### Towards Visible and Thermal Drone Monitoring with Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1812.08333v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.08333v1)
- **Published**: 2018-12-19 06:26:19+00:00
- **Updated**: 2018-12-19 06:26:19+00:00
- **Authors**: Ye Wang, Yueru Chen, Jongmoo Choi, C. -C. Jay Kuo
- **Comment**: 12 pages, 18 figures, journal. arXiv admin note: substantial text
  overlap with arXiv:1712.00863
- **Journal**: None
- **Summary**: This paper reports a visible and thermal drone monitoring system that integrates deep-learning-based detection and tracking modules. The biggest challenge in adopting deep learning methods for drone detection is the paucity of training drone images especially thermal drone images. To address this issue, we develop two data augmentation techniques. One is a model-based drone augmentation technique that automatically generates visible drone images with a bounding box label on the drone's location. The other is exploiting an adversarial data augmentation methodology to create thermal drone images. To track a small flying drone, we utilize the residual information between consecutive image frames. Finally, we present an integrated detection and tracking system that outperforms the performance of each individual module containing detection or tracking only. The experiments show that even being trained on synthetic data, the proposed system performs well on real-world drone images with complex background. The USC drone detection and tracking dataset with user labeled bounding boxes is available to the public.



### Found in Translation: Learning Robust Joint Representations by Cyclic Translations Between Modalities
- **Arxiv ID**: http://arxiv.org/abs/1812.07809v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV, cs.HC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.07809v2)
- **Published**: 2018-12-19 08:38:21+00:00
- **Updated**: 2020-02-28 09:20:33+00:00
- **Authors**: Hai Pham, Paul Pu Liang, Thomas Manzini, Louis-Philippe Morency, Barnabas Poczos
- **Comment**: AAAI 2019, code available at https://github.com/hainow/MCTN
- **Journal**: None
- **Summary**: Multimodal sentiment analysis is a core research area that studies speaker sentiment expressed from the language, visual, and acoustic modalities. The central challenge in multimodal learning involves inferring joint representations that can process and relate information from these modalities. However, existing work learns joint representations by requiring all modalities as input and as a result, the learned representations may be sensitive to noisy or missing modalities at test time. With the recent success of sequence to sequence (Seq2Seq) models in machine translation, there is an opportunity to explore new ways of learning joint representations that may not require all input modalities at test time. In this paper, we propose a method to learn robust joint representations by translating between modalities. Our method is based on the key insight that translation from a source to a target modality provides a method of learning joint representations using only the source modality as input. We augment modality translations with a cycle consistency loss to ensure that our joint representations retain maximal information from all modalities. Once our translation model is trained with paired multimodal data, we only need data from the source modality at test time for final sentiment prediction. This ensures that our model remains robust from perturbations or missing information in the other modalities. We train our model with a coupled translation-prediction objective and it achieves new state-of-the-art results on multimodal sentiment analysis datasets: CMU-MOSI, ICT-MMMO, and YouTube. Additional experiments show that our model learns increasingly discriminative joint representations with more input modalities while maintaining robustness to missing or perturbed modalities.



### Fast and Accurate 3D Medical Image Segmentation with Data-swapping Method
- **Arxiv ID**: http://arxiv.org/abs/1812.07816v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.PF, stat.ML, C.4; I.2.6; I.2.10; I.4.6; I.4.9; J.4
- **Links**: [PDF](http://arxiv.org/pdf/1812.07816v1)
- **Published**: 2018-12-19 08:49:50+00:00
- **Updated**: 2018-12-19 08:49:50+00:00
- **Authors**: Haruki Imai, Samuel Matzek, Tung D. Le, Yasushi Negishi, Kiyokuni Kawachiya
- **Comment**: 13 pages
- **Journal**: None
- **Summary**: Deep neural network models used for medical image segmentation are large because they are trained with high-resolution three-dimensional (3D) images. Graphics processing units (GPUs) are widely used to accelerate the trainings. However, the memory on a GPU is not large enough to train the models. A popular approach to tackling this problem is patch-based method, which divides a large image into small patches and trains the models with these small patches. However, this method would degrade the segmentation quality if a target object spans multiple patches. In this paper, we propose a novel approach for 3D medical image segmentation that utilizes the data-swapping, which swaps out intermediate data from GPU memory to CPU memory to enlarge the effective GPU memory size, for training high-resolution 3D medical images without patching. We carefully tuned parameters in the data-swapping method to obtain the best training performance for 3D U-Net, a widely used deep neural network model for medical image segmentation. We applied our tuning to train 3D U-Net with full-size images of 192 x 192 x 192 voxels in brain tumor dataset. As a result, communication overhead, which is the most important issue, was reduced by 17.1%. Compared with the patch-based method for patches of 128 x 128 x 128 voxels, our training for full-size images achieved improvement on the mean Dice score by 4.48% and 5.32 % for detecting whole tumor sub-region and tumor core sub-region, respectively. The total training time was reduced from 164 hours to 47 hours, resulting in 3.53 times of acceleration.



### Semi-Supervised Deep Learning for Abnormality Classification in Retinal Images
- **Arxiv ID**: http://arxiv.org/abs/1812.07832v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.07832v1)
- **Published**: 2018-12-19 09:18:56+00:00
- **Updated**: 2018-12-19 09:18:56+00:00
- **Authors**: Bruno Lecouat, Ken Chang, Chuan-Sheng Foo, Balagopal Unnikrishnan, James M. Brown, Houssam Zenati, Andrew Beers, Vijay Chandrasekhar, Jayashree Kalpathy-Cramer, Pavitra Krishnaswamy
- **Comment**: Machine Learning for Health (ML4H) Workshop at NeurIPS 2018
  arXiv:1811.07216
- **Journal**: None
- **Summary**: Supervised deep learning algorithms have enabled significant performance gains in medical image classification tasks. But these methods rely on large labeled datasets that require resource-intensive expert annotation. Semi-supervised generative adversarial network (GAN) approaches offer a means to learn from limited labeled data alongside larger unlabeled datasets, but have not been applied to discern fine-scale, sparse or localized features that define medical abnormalities. To overcome these limitations, we propose a patch-based semi-supervised learning approach and evaluate performance on classification of diabetic retinopathy from funduscopic images. Our semi-supervised approach achieves high AUC with just 10-20 labeled training images, and outperforms the supervised baselines by upto 15% when less than 30% of the training dataset is labeled. Further, our method implicitly enables interpretation of the SSL predictions. As this approach enables good accuracy, resolution and interpretability with lower annotation burden, it sets the pathway for scalable applications of deep learning in clinical imaging.



### Physical Attribute Prediction Using Deep Residual Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1812.07857v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.07857v1)
- **Published**: 2018-12-19 10:19:19+00:00
- **Updated**: 2018-12-19 10:19:19+00:00
- **Authors**: Rashidedin Jahandideh, Alireza Tavakoli Targhi, Maryam Tahmasbi
- **Comment**: None
- **Journal**: None
- **Summary**: Images taken from the Internet have been used alongside Deep Learning for many different tasks such as: smile detection, ethnicity, hair style, hair colour, gender and age prediction. After witnessing these usages, we were wondering what other attributes can be predicted from facial images available on the Internet. In this paper we tackle the prediction of physical attributes from face images using Convolutional Neural Networks trained on our dataset named FIRW. We crawled around 61, 000 images from the web, then use face detection to crop faces from these real world images. We choose ResNet-50 as our base network architecture. This network was pretrained for the task of face recognition by using the VGG-Face dataset, and we finetune it by using our own dataset to predict physical attributes. Separate networks are trained for the prediction of body type, ethnicity, gender, height and weight; our models achieve the following accuracies for theses tasks, respectively: 84.58%, 87.34%, 97.97%, 70.51%, 63.99%. To validate our choice of ResNet-50 as the base architecture, we also tackle the famous CelebA dataset. Our models achieve an averagy accuracy of 91.19% on CelebA, which is comparable to state-of-the-art approaches.



### Crack Detection Using Enhanced Thresholding on UAV based Collected Images
- **Arxiv ID**: http://arxiv.org/abs/1812.07868v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1812.07868v1)
- **Published**: 2018-12-19 10:49:15+00:00
- **Updated**: 2018-12-19 10:49:15+00:00
- **Authors**: Q. Zhu, T. H. Dinh, V. T. Hoang, M. D. Phung, Q. P. Ha
- **Comment**: In Proceedings of Australian Conference on Robotics and Automation
  2018 (ACRA)
- **Journal**: None
- **Summary**: This paper proposes a thresholding approach for crack detection in an unmanned aerial vehicle (UAV) based infrastructure inspection system. The proposed algorithm performs recursively on the intensity histogram of UAV-taken images to exploit their crack-pixels appearing at the low intensity interval. A quantified criterion of interclass contrast is proposed and employed as an object cost and stop condition for the recursive process. Experiments on different datasets show that our algorithm outperforms different segmentation approaches to accurately extract crack features of some commercial buildings.



### Deep Global-Relative Networks for End-to-End 6-DoF Visual Localization and Odometry
- **Arxiv ID**: http://arxiv.org/abs/1812.07869v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.07869v2)
- **Published**: 2018-12-19 10:51:09+00:00
- **Updated**: 2019-05-10 08:11:42+00:00
- **Authors**: Yimin Lin, Zhaoxiang Liu, Jianfeng Huang, Chaopeng Wang, Guoguang Du, Jinqiang Bai, Shiguo Lian, Bill Huang
- **Comment**: 7 pages, 6 figures
- **Journal**: 2019 The Pacific Rim International Conferences on Artificial
  Intelligence (PRICAI)
- **Summary**: Although a wide variety of deep neural networks for robust Visual Odometry (VO) can be found in the literature, they are still unable to solve the drift problem in long-term robot navigation. Thus, this paper aims to propose novel deep end-to-end networks for long-term 6-DoF VO task. It mainly fuses relative and global networks based on Recurrent Convolutional Neural Networks (RCNNs) to improve the monocular localization accuracy. Indeed, the relative sub-networks are implemented to smooth the VO trajectory, while global subnetworks are designed to avoid drift problem. All the parameters are jointly optimized using Cross Transformation Constraints (CTC), which represents temporal geometric consistency of the consecutive frames, and Mean Square Error (MSE) between the predicted pose and ground truth. The experimental results on both indoor and outdoor datasets show that our method outperforms other state-of-the-art learning-based VO methods in terms of pose accuracy.



### Removing rain streaks by a linear model
- **Arxiv ID**: http://arxiv.org/abs/1812.07870v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.07870v1)
- **Published**: 2018-12-19 10:51:50+00:00
- **Updated**: 2018-12-19 10:51:50+00:00
- **Authors**: Yinglong Wang, Shuaicheng Liu, Bing Zeng
- **Comment**: 12 pages, 12 figures
- **Journal**: None
- **Summary**: Removing rain streaks from a single image continues to draw attentions today in outdoor vision systems. In this paper, we present an efficient method to remove rain streaks. First, the location map of rain pixels needs to be known as precisely as possible, to which we implement a relatively accurate detection of rain streaks by utilizing two characteristics of rain streaks.The key component of our method is to represent the intensity of each detected rain pixel using a linear model: $p=\alpha s + \beta$, where $p$ is the observed intensity of a rain pixel and $s$ represents the intensity of the background (i.e., before rain-affected). To solve $\alpha$ and $\beta$ for each detected rain pixel, we concentrate on a window centered around it and form an $L_2$-norm cost function by considering all detected rain pixels within the window, where the corresponding rain-removed intensity of each detected rain pixel is estimated by some neighboring non-rain pixels. By minimizing this cost function, we determine $\alpha$ and $\beta$ so as to construct the final rain-removed pixel intensity. Compared with several state-of-the-art works, our proposed method can remove rain streaks from a single color image much more efficiently - it offers not only a better visual quality but also a speed-up of several times to one degree of magnitude.



### Sequential Gating Ensemble Network for Noise Robust Multi-Scale Face Restoration
- **Arxiv ID**: http://arxiv.org/abs/1812.11834v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.11834v1)
- **Published**: 2018-12-19 11:53:40+00:00
- **Updated**: 2018-12-19 11:53:40+00:00
- **Authors**: Zhibo Chen, Jianxin Lin, Tiankuang Zhou, Feng Wu
- **Comment**: 11 pages, 15 figures. arXiv admin note: substantial text overlap with
  arXiv:1805.02164
- **Journal**: None
- **Summary**: Face restoration from low resolution and noise is important for applications of face analysis recognition. However, most existing face restoration models omit the multiple scale issues in face restoration problem, which is still not well-solved in research area. In this paper, we propose a Sequential Gating Ensemble Network (SGEN) for multi-scale noise robust face restoration issue. To endow the network with multi-scale representation ability, we first employ the principle of ensemble learning for SGEN network architecture designing. The SGEN aggregates multi-level base-encoders and base-decoders into the network, which enables the network to contain multiple scales of receptive field. Instead of combining these base-en/decoders directly with non-sequential operations, the SGEN takes base-en/decoders from different levels as sequential data. Specifically, it is visualized that SGEN learns to sequentially extract high level information from base-encoders in bottom-up manner and restore low level information from base-decoders in top-down manner. Besides, we propose to realize bottom-up and top-down information combination and selection with Sequential Gating Unit (SGU). The SGU sequentially takes information from two different levels as inputs and decides the output based on one active input. Experiment results on benchmark dataset demonstrate that our SGEN is more effective at multi-scale human face restoration with more image details and less noise than state-of-the-art image restoration models. Further utilizing adversarial training scheme, SGEN also produces more visually preferred results than other models under subjective evaluation.



### PnP-AdaNet: Plug-and-Play Adversarial Domain Adaptation Network with a Benchmark at Cross-modality Cardiac Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1812.07907v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.07907v1)
- **Published**: 2018-12-19 12:21:36+00:00
- **Updated**: 2018-12-19 12:21:36+00:00
- **Authors**: Qi Dou, Cheng Ouyang, Cheng Chen, Hao Chen, Ben Glocker, Xiahai Zhuang, Pheng-Ann Heng
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolutional networks have demonstrated the state-of-the-art performance on various medical image computing tasks. Leveraging images from different modalities for the same analysis task holds clinical benefits. However, the generalization capability of deep models on test data with different distributions remain as a major challenge. In this paper, we propose the PnPAdaNet (plug-and-play adversarial domain adaptation network) for adapting segmentation networks between different modalities of medical images, e.g., MRI and CT. We propose to tackle the significant domain shift by aligning the feature spaces of source and target domains in an unsupervised manner. Specifically, a domain adaptation module flexibly replaces the early encoder layers of the source network, and the higher layers are shared between domains. With adversarial learning, we build two discriminators whose inputs are respectively multi-level features and predicted segmentation masks. We have validated our domain adaptation method on cardiac structure segmentation in unpaired MRI and CT. The experimental results with comprehensive ablation studies demonstrate the excellent efficacy of our proposed PnP-AdaNet. Moreover, we introduce a novel benchmark on the cardiac dataset for the task of unsupervised cross-modality domain adaptation. We will make our code and database publicly available, aiming to promote future studies on this challenging yet important research topic in medical imaging.



### Dynamic Programming Approach to Template-based OCR
- **Arxiv ID**: http://arxiv.org/abs/1812.07933v1
- **DOI**: 10.1117/12.2522974
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.07933v1)
- **Published**: 2018-12-19 13:22:00+00:00
- **Updated**: 2018-12-19 13:22:00+00:00
- **Authors**: M. A. Povolotskiy, D. V. Tropin
- **Comment**: 8 pages, 5 figures, 1 table
- **Journal**: None
- **Summary**: In this paper we propose a dynamic programming solution to the template-based recognition task in OCR case. We formulate a problem of optimal position search for complex objects consisting of parts forming a sequence. We limit the distance between every two adjacent elements with predefined upper and lower thresholds. We choose the sum of penalties for each part in given position as a function to be minimized. We show that such a choice of restrictions allows a faster algorithm to be used than the one for the general form of deformation penalties. We named this algorithm Dynamic Squeezeboxes Packing (DSP) and applied it to solve the two OCR problems: text fields extraction from an image of document Visual Inspection Zone (VIZ) and license plate segmentation. The quality and the performance of resulting solutions were experimentally proved to meet the requirements of the state-of-the-art industrial recognition systems.



### MID-Fusion: Octree-based Object-Level Multi-Instance Dynamic SLAM
- **Arxiv ID**: http://arxiv.org/abs/1812.07976v4
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1812.07976v4)
- **Published**: 2018-12-19 14:43:05+00:00
- **Updated**: 2019-03-21 23:41:56+00:00
- **Authors**: Binbin Xu, Wenbin Li, Dimos Tzoumanikas, Michael Bloesch, Andrew Davison, Stefan Leutenegger
- **Comment**: Accepted to International Conference on Robotics and Automation
  (ICRA) 2019. 7 (6 + 1) pages. Please also see video Link:
  https://youtu.be/gturboNl9gg
- **Journal**: None
- **Summary**: We propose a new multi-instance dynamic RGB-D SLAM system using an object-level octree-based volumetric representation. It can provide robust camera tracking in dynamic environments and at the same time, continuously estimate geometric, semantic, and motion properties for arbitrary objects in the scene. For each incoming frame, we perform instance segmentation to detect objects and refine mask boundaries using geometric and motion information. Meanwhile, we estimate the pose of each existing moving object using an object-oriented tracking method and robustly track the camera pose against the static scene. Based on the estimated camera pose and object poses, we associate segmented masks with existing models and incrementally fuse corresponding colour, depth, semantic, and foreground object probabilities into each object model. In contrast to existing approaches, our system is the first system to generate an object-level dynamic volumetric map from a single RGB-D camera, which can be used directly for robotic tasks. Our method can run at 2-3 Hz on a CPU, excluding the instance segmentation part. We demonstrate its effectiveness by quantitatively and qualitatively testing it on both synthetic and real-world sequences.



### A Gated Peripheral-Foveal Convolutional Neural Network for Unified Image Aesthetic Prediction
- **Arxiv ID**: http://arxiv.org/abs/1812.07989v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1812.07989v2)
- **Published**: 2018-12-19 14:57:06+00:00
- **Updated**: 2019-06-26 08:04:30+00:00
- **Authors**: Xiaodan Zhang, Xinbo Gao, Wen Lu, Lihuo He
- **Comment**: Add more experiments
- **Journal**: None
- **Summary**: Learning fine-grained details is a key issue in image aesthetic assessment. Most of the previous methods extract the fine-grained details via random cropping strategy, which may undermine the integrity of semantic information. Extensive studies show that humans perceive fine-grained details with a mixture of foveal vision and peripheral vision. Fovea has the highest possible visual acuity and is responsible for seeing the details. The peripheral vision is used for perceiving the broad spatial scene and selecting the attended regions for the fovea. Inspired by these observations, we propose a Gated Peripheral-Foveal Convolutional Neural Network (GPF-CNN). It is a dedicated double-subnet neural network, i.e. a peripheral subnet and a foveal subnet. The former aims to mimic the functions of peripheral vision to encode the holistic information and provide the attended regions. The latter aims to extract fine-grained features on these key regions. Considering that the peripheral vision and foveal vision play different roles in processing different visual stimuli, we further employ a gated information fusion (GIF) network to weight their contributions. The weights are determined through the fully connected layers followed by a sigmoid function. We conduct comprehensive experiments on the standard AVA and Photo.net datasets for unified aesthetic prediction tasks: (i) aesthetic quality classification; (ii) aesthetic score regression; and (iii) aesthetic score distribution prediction. The experimental results demonstrate the effectiveness of the proposed method.



### Accurate Hand Keypoint Localization on Mobile Devices
- **Arxiv ID**: http://arxiv.org/abs/1812.08028v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.08028v1)
- **Published**: 2018-12-19 15:39:13+00:00
- **Updated**: 2018-12-19 15:39:13+00:00
- **Authors**: Filippos Gouidis, Paschalis Panteleris, Iason Oikonomidis, Antonis Argyros
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel approach for 2D hand keypoint localization from regular color input. The proposed approach relies on an appropriately designed Convolutional Neural Network (CNN) that computes a set of heatmaps, one per hand keypoint of interest. Extensive experiments with the proposed method compare it against state of the art approaches and demonstrate its accuracy and computational performance on standard, publicly available datasets. The obtained results demonstrate that the proposed method matches or outperforms the competing methods in accuracy, but clearly outperforms them in computational efficiency, making it a suitable building block for applications that require hand keypoint estimation on mobile devices.



### Learning beamforming in ultrasound imaging
- **Arxiv ID**: http://arxiv.org/abs/1812.08043v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1812.08043v2)
- **Published**: 2018-12-19 15:59:19+00:00
- **Updated**: 2020-04-05 06:43:29+00:00
- **Authors**: Sanketh Vedula, Ortal Senouf, Grigoriy Zurakhov, Alex Bronstein, Oleg Michailovich, Michael Zibulevsky
- **Comment**: None
- **Journal**: Proceedings of The 2nd International Conference on Medical Imaging
  with Deep Learning, PMLR 102:493-511, 2019
- **Summary**: Medical ultrasound (US) is a widespread imaging modality owing its popularity to cost efficiency, portability, speed, and lack of harmful ionizing radiation. In this paper, we demonstrate that replacing the traditional ultrasound processing pipeline with a data-driven, learnable counterpart leads to significant improvement in image quality. Moreover, we demonstrate that greater improvement can be achieved through a learning-based design of the transmitted beam patterns simultaneously with learning an image reconstruction pipeline. We evaluate our method on an in-vivo first-harmonic cardiac ultrasound dataset acquired from volunteers and demonstrate the significance of the learned pipeline and transmit beam patterns on the image quality when compared to standard transmit and receive beamformers used in high frame-rate US imaging. We believe that the presented methodology provides a fundamentally different perspective on the classical problem of ultrasound beam pattern design.



### Multitask Painting Categorization by Deep Multibranch Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1812.08052v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.08052v1)
- **Published**: 2018-12-19 16:12:29+00:00
- **Updated**: 2018-12-19 16:12:29+00:00
- **Authors**: Simone Bianco, Davide Mazzini, Paolo Napoletano, Raimondo Schettini
- **Comment**: 11 pages, under review
- **Journal**: None
- **Summary**: In this work we propose a new deep multibranch neural network to solve the tasks of artist, style, and genre categorization in a multitask formulation. In order to gather clues from low-level texture details and, at the same time, exploit the coarse layout of the painting, the branches of the proposed networks are fed with crops at different resolutions. We propose and compare two different crop strategies: the first one is a random-crop strategy that permits to manage the tradeoff between accuracy and speed; the second one is a smart extractor based on Spatial Transformer Networks trained to extract the most representative subregions. Furthermore, inspired by the results obtained in other domains, we experiment the joint use of hand-crafted features directly computed on the input images along with neural ones. Experiments are performed on a new dataset originally sourced from wikiart.org and hosted by Kaggle, and made suitable for artist, style and genre multitask learning. The dataset here proposed, named MultitaskPainting100k, is composed by 100K paintings, 1508 artists, 125 styles and 41 genres. Our best method, tested on the MultitaskPainting100k dataset, achieves accuracy levels of 56.5%, 57.2%, and 63.6% on the tasks of artist, style and genre prediction respectively.



### Shallow Cue Guided Deep Visual Tracking via Mixed Models
- **Arxiv ID**: http://arxiv.org/abs/1812.08094v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.08094v1)
- **Published**: 2018-12-19 17:13:20+00:00
- **Updated**: 2018-12-19 17:13:20+00:00
- **Authors**: Fangwen Tu, Shuzhi Sam Ge, Chang Chieh Hang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, a robust visual tracking approach via mixed model based convolutional neural networks (SDT) is developed. In order to handle abrupt or fast motion, a prior map is generated to facilitate the localization of region of interest (ROI) before the deep tracker is performed. A top-down saliency model with nineteen shallow cues are employed to construct the prior map with online learnt combination weights. Moreover, apart from a holistic deep learner, four local networks are also trained to learn different components of the target. The generated four local heat maps will facilitate to rectify the holistic map by eliminating the distracters to avoid drifting. Furthermore, to guarantee the instance for online update of high quality, a prioritised update strategy is implemented by casting the problem into a label noise problem. The selection probability is designed by considering both confidence values and bio-inspired memory for temporal information integration. Experiments are conducted qualitatively and quantitatively on a set of challenging image sequences. Comparative study demonstrates that the proposed algorithm outperforms other state-of-the-art methods.



### Window detection in aerial texture images of the Berlin 3D CityGML Model
- **Arxiv ID**: http://arxiv.org/abs/1812.08095v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, 68U10
- **Links**: [PDF](http://arxiv.org/pdf/1812.08095v1)
- **Published**: 2018-12-19 17:15:02+00:00
- **Updated**: 2018-12-19 17:15:02+00:00
- **Authors**: Franziska Lippoldt, Marius Erdt
- **Comment**: 5 pages
- **Journal**: None
- **Summary**: This article explores the usage of the state-of-art neural network Mask R-CNN to be used for window detection of texture files from the CityGML model of Berlin. As texture files are very irregular in terms of size, exposure settings and orientation, we use several parameter optimisation methods to improve the precision. Those textures are cropped from aerial photos, which implies that the angle of the facade, the exposure as well as contrast are calibrated towards the mean and not towards the single facade. The analysis of a single texture image with the human eye itself is challenging: A combination of window and facade estimation and perspective analysis is necessary in order to determine the facades and windows. We train and detect bounding boxes and masks from two data sets with image size 128 and 256. We explore various configuration optimisation methods and the relation of the Region Proposal Network, detected ROIs and the mask output. Our final results shows that the we can improve the average precision scores for both data set sizes, yet the initial AP score varies and leads to different resulting scores.



### Lattice Identification and Separation: Theory and Algorithm
- **Arxiv ID**: http://arxiv.org/abs/1901.02520v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, math.MG, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/1901.02520v1)
- **Published**: 2018-12-19 17:25:32+00:00
- **Updated**: 2018-12-19 17:25:32+00:00
- **Authors**: Yuchen He, Sung Ha Kang
- **Comment**: 30 Pages plus 4 pages of Appendix. 4 Pages of References. 24 Figures
- **Journal**: None
- **Summary**: Motivated by lattice mixture identification and grain boundary detection, we present a framework for lattice pattern representation and comparison, and propose an efficient algorithm for lattice separation. We define new scale and shape descriptors, which helps to considerably reduce the size of equivalence classes of lattice bases. These finitely many equivalence relations are fully characterized by modular group theory. We construct the lattice space $\mathscr{L}$ based on the equivalent descriptors and define a metric $d_{\mathscr{L}}$ to accurately quantify the visual similarities and differences between lattices. Furthermore, we introduce the Lattice Identification and Separation Algorithm (LISA), which identifies each lattice patterns from superposed lattices. LISA finds lattice candidates from the high responses in the image spectrum, then sequentially extracts different layers of lattice patterns one by one. Analyzing the frequency components, we reveal the intricate dependency of LISA's performances on particle radius, lattice density, and relative translations. Various numerical experiments are designed to show LISA's robustness against a large number of lattice layers, moir\'{e} patterns and missing particles.



### The Random Forest Classifier in WEKA: Discussion and New Developments for Imbalanced Data
- **Arxiv ID**: http://arxiv.org/abs/1812.08102v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.08102v2)
- **Published**: 2018-12-19 17:27:04+00:00
- **Updated**: 2019-01-04 09:45:48+00:00
- **Authors**: Mario Amrehn, Firas Mualla, Elli Angelopoulou, Stefan Steidl, Andreas Maier
- **Comment**: None
- **Journal**: None
- **Summary**: Data analysis and machine learning have become an integrative part of the modern scientific methodology, providing automated techniques to predict further information based on observations. One of these classification and regression techniques is the random forest approach. Those decision tree based predictors are best known for their good computational performance and scalability. However, in case of severely imbalanced training data, as often seen in medical studies' data with large control groups, the training algorithm or the sampling process has to be altered in order to improve the prediction quality for minority classes. In this work, a balanced random forest approach for WEKA is proposed. Furthermore, the prediction quality of the unmodified random forest implementation and the new balanced random forest version for WEKA are evaluated against reference implementations in R. Two-class problems on balanced data sets and imbalanced medical studies' data are investigated. A superior prediction quality using the proposed method for imbalanced data is shown compared to the other three techniques.



### MoDL-MUSSELS: Model-Based Deep Learning for Multi-Shot Sensitivity Encoded Diffusion MRI
- **Arxiv ID**: http://arxiv.org/abs/1812.08115v3
- **DOI**: 10.1109/TMI.2019.2946501
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.08115v3)
- **Published**: 2018-12-19 17:46:43+00:00
- **Updated**: 2019-10-22 17:32:33+00:00
- **Authors**: Hemant Kumar Aggarwal, Merry P. Mani, Mathews Jacob
- **Comment**: None
- **Journal**: IEEE Transactions on Medical Imaging, 2019
- **Summary**: We introduce a model-based deep learning architecture termed MoDL-MUSSELS for the correction of phase errors in multishot diffusion-weighted echo-planar MRI images. The proposed algorithm is a generalization of existing MUSSELS algorithm with similar performance but with significantly reduced computational complexity. In this work, we show that an iterative re-weighted least-squares implementation of MUSSELS alternates between a multichannel filter bank and the enforcement of data consistency. The multichannel filter bank projects the data to the signal subspace thus exploiting the phase relations between shots. Due to the high computational complexity of self-learned filter bank, we propose to replace it with a convolutional neural network (CNN) whose parameters are learned from exemplary data. The proposed CNN is a hybrid model involving a multichannel CNN in the k-space and another CNN in the image space. The k-space CNN exploits the phase relations between the shot images, while the image domain network is used to project the data to an image manifold. The experiments show that the proposed scheme can yield reconstructions that are comparable to state of the art methods while offering several orders of magnitude reduction in run-time.



### Adam Induces Implicit Weight Sparsity in Rectifier Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1812.08119v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.08119v1)
- **Published**: 2018-12-19 17:59:08+00:00
- **Updated**: 2018-12-19 17:59:08+00:00
- **Authors**: Atsushi Yaguchi, Taiji Suzuki, Wataru Asano, Shuhei Nitta, Yukinobu Sakata, Akiyuki Tanizawa
- **Comment**: 8 pages, 7 figures, 6 tables, 2018 17th IEEE International Conference
  on Machine Learning and Applications (ICMLA)
- **Journal**: None
- **Summary**: In recent years, deep neural networks (DNNs) have been applied to various machine leaning tasks, including image recognition, speech recognition, and machine translation. However, large DNN models are needed to achieve state-of-the-art performance, exceeding the capabilities of edge devices. Model reduction is thus needed for practical use. In this paper, we point out that deep learning automatically induces group sparsity of weights, in which all weights connected to an output channel (node) are zero, when training DNNs under the following three conditions: (1) rectified-linear-unit (ReLU) activations, (2) an $L_2$-regularized objective function, and (3) the Adam optimizer. Next, we analyze this behavior both theoretically and experimentally, and propose a simple model reduction method: eliminate the zero weights after training the DNN. In experiments on MNIST and CIFAR-10 datasets, we demonstrate the sparsity with various training setups. Finally, we show that our method can efficiently reduce the model size and performs well relative to methods that use a sparsity-inducing regularizer.



### Very Power Efficient Neural Time-of-Flight
- **Arxiv ID**: http://arxiv.org/abs/1812.08125v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.08125v1)
- **Published**: 2018-12-19 18:08:48+00:00
- **Updated**: 2018-12-19 18:08:48+00:00
- **Authors**: Yan Chen, Jimmy Ren, Xuanye Cheng, Keyuan Qian, Jinwei Gu
- **Comment**: preprint
- **Journal**: None
- **Summary**: Time-of-Flight (ToF) cameras require active illumination to obtain depth information thus the power of illumination directly affects the performance of ToF cameras. Traditional ToF imaging algorithms is very sensitive to illumination and the depth accuracy degenerates rapidly with the power of it. Therefore, the design of a power efficient ToF camera always creates a painful dilemma for the illumination and the performance trade-off. In this paper, we show that despite the weak signals in many areas under extreme short exposure setting, these signals as a whole can be well utilized through a learning process which directly translates the weak and noisy ToF camera raw to depth map. This creates an opportunity to tackle the aforementioned dilemma and make a very power efficient ToF camera possible. To enable the learning, we collect a comprehensive dataset under a variety of scenes and photographic conditions by a specialized ToF camera. Experiments show that our method is able to robustly process ToF camera raw with the exposure time of one order of magnitude shorter than that used in conventional ToF cameras. In addition to evaluating our approach both quantitatively and qualitatively, we also discuss its implication to designing the next generation power efficient ToF cameras. We will make our dataset and code publicly available.



### Generating Diverse and Meaningful Captions
- **Arxiv ID**: http://arxiv.org/abs/1812.08126v1
- **DOI**: 10.1007/978-3-030-01418-6_18
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.08126v1)
- **Published**: 2018-12-19 18:10:18+00:00
- **Updated**: 2018-12-19 18:10:18+00:00
- **Authors**: Annika Lindh, Robert J. Ross, Abhijit Mahalunkar, Giancarlo Salton, John D. Kelleher
- **Comment**: Accepted for presentation at The 27th International Conference on
  Artificial Neural Networks (ICANN 2018)
- **Journal**: Artificial Neural Networks and Machine Learning - ICANN 2018 (pp.
  176-187). Springer International Publishing
- **Summary**: Image Captioning is a task that requires models to acquire a multi-modal understanding of the world and to express this understanding in natural language text. While the state-of-the-art for this task has rapidly improved in terms of n-gram metrics, these models tend to output the same generic captions for similar images. In this work, we address this limitation and train a model that generates more diverse and specific captions through an unsupervised training approach that incorporates a learning signal from an Image Retrieval model. We summarize previous results and improve the state-of-the-art on caption diversity and novelty. We make our source code publicly available online.



### A Tour of Unsupervised Deep Learning for Medical Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/1812.07715v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.07715v1)
- **Published**: 2018-12-19 18:42:05+00:00
- **Updated**: 2018-12-19 18:42:05+00:00
- **Authors**: Khalid Raza, Nripendra Kumar Singh
- **Comment**: 29 pages, 6 figures, 8 tables
- **Journal**: None
- **Summary**: Interpretation of medical images for diagnosis and treatment of complex disease from high-dimensional and heterogeneous data remains a key challenge in transforming healthcare. In the last few years, both supervised and unsupervised deep learning achieved promising results in the area of medical imaging and image analysis. Unlike supervised learning which is biased towards how it is being supervised and manual efforts to create class label for the algorithm, unsupervised learning derive insights directly from the data itself, group the data and help to make data driven decisions without any external bias. This review systematically presents various unsupervised models applied to medical image analysis, including autoencoders and its several variants, Restricted Boltzmann machines, Deep belief networks, Deep Boltzmann machine and Generative adversarial network. Future research opportunities and challenges of unsupervised techniques for medical image analysis have also been discussed.



### Magnetic Resonance Fingerprinting using Recurrent Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1812.08155v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.08155v1)
- **Published**: 2018-12-19 18:50:15+00:00
- **Updated**: 2018-12-19 18:50:15+00:00
- **Authors**: Ilkay Oksuz, Gastao Cruz, James Clough, Aurelien Bustin, Nicolo Fuin, Rene M. Botnar, Claudia Prieto, Andrew P. King, Julia A. Schnabel
- **Comment**: Accepted for ISBI 2018
- **Journal**: None
- **Summary**: Magnetic Resonance Fingerprinting (MRF) is a new approach to quantitative magnetic resonance imaging that allows simultaneous measurement of multiple tissue properties in a single, time-efficient acquisition. Standard MRF reconstructs parametric maps using dictionary matching and lacks scalability due to computational inefficiency. We propose to perform MRF map reconstruction using a recurrent neural network, which exploits the time-dependent information of the MRF signal evolution. We evaluate our method on multiparametric synthetic signals and compare it to existing MRF map reconstruction approaches, including those based on neural networks. Our method achieves state-of-the-art estimates of T1 and T2 values. In addition, the reconstruction time is significantly reduced compared to dictionary-matching based approaches.



### Unsupervised Event-based Learning of Optical Flow, Depth, and Egomotion
- **Arxiv ID**: http://arxiv.org/abs/1812.08156v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.08156v1)
- **Published**: 2018-12-19 18:50:54+00:00
- **Updated**: 2018-12-19 18:50:54+00:00
- **Authors**: Alex Zihao Zhu, Liangzhe Yuan, Kenneth Chaney, Kostas Daniilidis
- **Comment**: 9 pages, 7 figures
- **Journal**: None
- **Summary**: In this work, we propose a novel framework for unsupervised learning for event cameras that learns motion information from only the event stream. In particular, we propose an input representation of the events in the form of a discretized volume that maintains the temporal distribution of the events, which we pass through a neural network to predict the motion of the events. This motion is used to attempt to remove any motion blur in the event image. We then propose a loss function applied to the motion compensated event image that measures the motion blur in this image. We train two networks with this framework, one to predict optical flow, and one to predict egomotion and depths, and evaluate these networks on the Multi Vehicle Stereo Event Camera dataset, along with qualitative results from a variety of different scenes.



### RankGAN: A Maximum Margin Ranking GAN for Generating Faces
- **Arxiv ID**: http://arxiv.org/abs/1812.08196v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.08196v1)
- **Published**: 2018-12-19 19:09:32+00:00
- **Updated**: 2018-12-19 19:09:32+00:00
- **Authors**: Rahul Dey, Felix Juefei-Xu, Vishnu Naresh Boddeti, Marios Savvides
- **Comment**: Best Student Paper Award at Asian Conference on Computer Vision
  (ACCV), 2018 at Perth, Australia. Includes main paper and supplementary
  material. Total 32 pages including references
- **Journal**: None
- **Summary**: We present a new stage-wise learning paradigm for training generative adversarial networks (GANs). The goal of our work is to progressively strengthen the discriminator and thus, the generators, with each subsequent stage without changing the network architecture. We call this proposed method the RankGAN. We first propose a margin-based loss for the GAN discriminator. We then extend it to a margin-based ranking loss to train the multiple stages of RankGAN. We focus on face images from the CelebA dataset in our work and show visual as well as quantitative improvements in face generation and completion tasks over other GAN approaches, including WGAN and LSGAN.



### Unconstrained Iris Segmentation using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1812.08245v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.08245v1)
- **Published**: 2018-12-19 21:10:08+00:00
- **Updated**: 2018-12-19 21:10:08+00:00
- **Authors**: Sohaib Ahmad, Benjamin Fuller
- **Comment**: None
- **Journal**: None
- **Summary**: The extraction of consistent and identifiable features from an image of the human iris is known as iris recognition. Identifying which pixels belong to the iris, known as segmentation, is the first stage of iris recognition. Errors in segmentation propagate to later stages. Current segmentation approaches are tuned to specific environments. We propose using a convolution neural network for iris segmentation. Our algorithm is accurate when trained in a single environment and tested in multiple environments. Our network builds on the Mask R-CNN framework (He et al., ICCV 2017). Our approach segments faster than previous approaches including the Mask R-CNN network. Our network is accurate when trained on a single environment and tested with a different sensors (either visible light or near-infrared). Its accuracy degrades when trained with a visible light sensor and tested with a near-infrared sensor (and vice versa). A small amount of retraining of the visible light model (using a few samples from a near-infrared dataset) yields a tuned network accurate in both settings. For training and testing, this work uses the Casia v4 Interval, Notre Dame 0405, Ubiris v2, and IITD datasets.



### Detecting GAN-generated Imagery using Color Cues
- **Arxiv ID**: http://arxiv.org/abs/1812.08247v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.08247v1)
- **Published**: 2018-12-19 21:12:00+00:00
- **Updated**: 2018-12-19 21:12:00+00:00
- **Authors**: Scott McCloskey, Michael Albright
- **Comment**: None
- **Journal**: None
- **Summary**: Image forensics is an increasingly relevant problem, as it can potentially address online disinformation campaigns and mitigate problematic aspects of social media. Of particular interest, given its recent successes, is the detection of imagery produced by Generative Adversarial Networks (GANs), e.g. `deepfakes'. Leveraging large training sets and extensive computing resources, recent work has shown that GANs can be trained to generate synthetic imagery which is (in some ways) indistinguishable from real imagery. We analyze the structure of the generating network of a popular GAN implementation, and show that the network's treatment of color is markedly different from a real camera in two ways. We further show that these two cues can be used to distinguish GAN-generated imagery from camera imagery, demonstrating effective discrimination between GAN imagery and real camera images used to train the GAN.



### D3D: Distilled 3D Networks for Video Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1812.08249v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.08249v2)
- **Published**: 2018-12-19 21:19:41+00:00
- **Updated**: 2019-02-05 19:01:06+00:00
- **Authors**: Jonathan C. Stroud, David A. Ross, Chen Sun, Jia Deng, Rahul Sukthankar
- **Comment**: Added link to code and models at https://www.jonathancstroud.com/d3d
- **Journal**: None
- **Summary**: State-of-the-art methods for video action recognition commonly use an ensemble of two networks: the spatial stream, which takes RGB frames as input, and the temporal stream, which takes optical flow as input. In recent work, both of these streams consist of 3D Convolutional Neural Networks, which apply spatiotemporal filters to the video clip before performing classification. Conceptually, the temporal filters should allow the spatial stream to learn motion representations, making the temporal stream redundant. However, we still see significant benefits in action recognition performance by including an entirely separate temporal stream, indicating that the spatial stream is "missing" some of the signal captured by the temporal stream. In this work, we first investigate whether motion representations are indeed missing in the spatial stream of 3D CNNs. Second, we demonstrate that these motion representations can be improved by distillation, by tuning the spatial stream to predict the outputs of the temporal stream, effectively combining both models into a single stream. Finally, we show that our Distilled 3D Network (D3D) achieves performance on par with two-stream approaches, using only a single model and with no need to compute optical flow.



### A comparative study of texture attributes for characterizing subsurface structures in seismic volumes
- **Arxiv ID**: http://arxiv.org/abs/1812.08263v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.08263v1)
- **Published**: 2018-12-19 21:55:59+00:00
- **Updated**: 2018-12-19 21:55:59+00:00
- **Authors**: Zhiling Long, Yazeed Alaudah, Muhammad Ali Qureshi, Yuting Hu, Zhen Wang, Motaz Alfarraj, Ghassan AlRegib, Asjad Amin, Mohamed Deriche, Suhail Al-Dharrab, Haibin Di
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we explore how to computationally characterize subsurface geological structures presented in seismic volumes using texture attributes. For this purpose, we conduct a comparative study of typical texture attributes presented in the image processing literature. We focus on spatial attributes in this study and examine them in a new application for seismic interpretation, i.e., seismic volume labeling. For this application, a data volume is automatically segmented into various structures, each assigned with its corresponding label. If the labels are assigned with reasonable accuracy, such volume labeling will help initiate an interpretation process in a more effective manner. Our investigation proves the feasibility of accomplishing this task using texture attributes. Through the study, we also identify advantages and disadvantages associated with each attribute.



