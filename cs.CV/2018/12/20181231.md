# Arxiv Papers in cs.CV on 2018-12-31
### Path-Invariant Map Networks
- **Arxiv ID**: http://arxiv.org/abs/1812.11647v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.11647v3)
- **Published**: 2018-12-31 00:38:26+00:00
- **Updated**: 2019-06-04 08:14:47+00:00
- **Authors**: Zaiwei Zhang, Zhenxiao Liang, Lemeng Wu, Xiaowei Zhou, Qixing Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Optimizing a network of maps among a collection of objects/domains (or map synchronization) is a central problem across computer vision and many other relevant fields. Compared to optimizing pairwise maps in isolation, the benefit of map synchronization is that there are natural constraints among a map network that can improve the quality of individual maps. While such self-supervision constraints are well-understood for undirected map networks (e.g., the cycle-consistency constraint), they are under-explored for directed map networks, which naturally arise when maps are given by parametric maps (e.g., a feed-forward neural network). In this paper, we study a natural self-supervision constraint for directed map networks called path-invariance, which enforces that composite maps along different paths between a fixed pair of source and target domains are identical. We introduce path-invariance bases for efficient encoding of the path-invariance constraint and present an algorithm that outputs a path-variance basis with polynomial time and space complexities. We demonstrate the effectiveness of our approach on optimizing object correspondences, estimating dense image maps via neural networks, and semantic segmentation of 3D scenes via map networks of diverse 3D representations. In particular, for 3D semantic segmentation, our approach only requires 8% labeled data from ScanNet to achieve the same performance as training a single 3D segmentation network with 30% to 100% labeled data.



### Unsupervised monocular stereo matching
- **Arxiv ID**: http://arxiv.org/abs/1812.11671v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.11671v1)
- **Published**: 2018-12-31 02:13:55+00:00
- **Updated**: 2018-12-31 02:13:55+00:00
- **Authors**: Zhimin Zhang, Jianzhong Qiao, Shukuan Lin
- **Comment**: None
- **Journal**: None
- **Summary**: At present, deep learning has been applied more and more in monocular image depth estimation and has shown promising results. The current more ideal method for monocular depth estimation is the supervised learning based on ground truth depth, but this method requires an abundance of expensive ground truth depth as the supervised labels. Therefore, researchers began to work on unsupervised depth estimation methods. Although the accuracy of unsupervised depth estimation method is still lower than that of supervised method, it is a promising research direction.   In this paper, Based on the experimental results that the stereo matching models outperforms monocular depth estimation models under the same unsupervised depth estimation model, we proposed an unsupervised monocular vision stereo matching method. In order to achieve the monocular stereo matching, we constructed two unsupervised deep convolution network models, one was to reconstruct the right view from the left view, and the other was to estimate the depth map using the reconstructed right view and the original left view. The two network models are piped together during the test phase. The output results of this method outperforms the current mainstream unsupervised depth estimation method in the challenging KITTI dataset.



### ADMM-NN: An Algorithm-Hardware Co-Design Framework of DNNs Using Alternating Direction Method of Multipliers
- **Arxiv ID**: http://arxiv.org/abs/1812.11677v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.AR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1812.11677v1)
- **Published**: 2018-12-31 02:26:48+00:00
- **Updated**: 2018-12-31 02:26:48+00:00
- **Authors**: Ao Ren, Tianyun Zhang, Shaokai Ye, Jiayu Li, Wenyao Xu, Xuehai Qian, Xue Lin, Yanzhi Wang
- **Comment**: None
- **Journal**: None
- **Summary**: To facilitate efficient embedded and hardware implementations of deep neural networks (DNNs), two important categories of DNN model compression techniques: weight pruning and weight quantization are investigated. The former leverages the redundancy in the number of weights, whereas the latter leverages the redundancy in bit representation of weights. However, there lacks a systematic framework of joint weight pruning and quantization of DNNs, thereby limiting the available model compression ratio. Moreover, the computation reduction, energy efficiency improvement, and hardware performance overhead need to be accounted for besides simply model size reduction.   To address these limitations, we present ADMM-NN, the first algorithm-hardware co-optimization framework of DNNs using Alternating Direction Method of Multipliers (ADMM), a powerful technique to deal with non-convex optimization problems with possibly combinatorial constraints. The first part of ADMM-NN is a systematic, joint framework of DNN weight pruning and quantization using ADMM. It can be understood as a smart regularization technique with regularization target dynamically updated in each ADMM iteration, thereby resulting in higher performance in model compression than prior work. The second part is hardware-aware DNN optimizations to facilitate hardware-level implementations.   Without accuracy loss, we can achieve 85$\times$ and 24$\times$ pruning on LeNet-5 and AlexNet models, respectively, significantly higher than prior work. The improvement becomes more significant when focusing on computation reductions. Combining weight pruning and quantization, we achieve 1,910$\times$ and 231$\times$ reductions in overall model size on these two benchmarks, when focusing on data storage. Highly promising results are also observed on other representative DNNs such as VGGNet and ResNet-50.



### Deep Residual Learning in the JPEG Transform Domain
- **Arxiv ID**: http://arxiv.org/abs/1812.11690v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.11690v3)
- **Published**: 2018-12-31 03:55:09+00:00
- **Updated**: 2019-08-27 14:41:31+00:00
- **Authors**: Max Ehrlich, Larry Davis
- **Comment**: Published in ICCV 2019. Code and notes are available on our website
  at https://maxehr.umiacs.io/jpeg_domain_resnet/jpeg_domain_resnet_html.html
- **Journal**: None
- **Summary**: We introduce a general method of performing Residual Network inference and learning in the JPEG transform domain that allows the network to consume compressed images as input. Our formulation leverages the linearity of the JPEG transform to redefine convolution and batch normalization with a tune-able numerical approximation for ReLu. The result is mathematically equivalent to the spatial domain network up to the ReLu approximation accuracy. A formulation for image classification and a model conversion algorithm for spatial domain networks are given as examples of the method. We show that the sparsity of the JPEG format allows for faster processing of images with little to no penalty in the network accuracy.



### Sex-Classification from Cell-Phones Periocular Iris Images
- **Arxiv ID**: http://arxiv.org/abs/1812.11702v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.11702v1)
- **Published**: 2018-12-31 06:08:07+00:00
- **Updated**: 2018-12-31 06:08:07+00:00
- **Authors**: Juan Tapia, Claudia Arellano, Ignacio Viedma
- **Comment**: Pre-print version accepted to be published On Selfie Biometrics
  Book-2019
- **Journal**: None
- **Summary**: Selfie soft biometrics has great potential for various applications ranging from marketing, security and online banking. However, it faces many challenges since there is limited control in data acquisition conditions. This chapter presents a Super-Resolution-Convolutional Neural Networks (SRCNNs) approach that increases the resolution of low quality periocular iris images cropped from selfie images of subject's faces. This work shows that increasing image resolution (2x and 3x) can improve the sex-classification rate when using a Random Forest classifier. The best sex-classification rate was 90.15% for the right and 87.15% for the left eye. This was achieved when images were upscaled from 150x150 to 450x450 pixels. These results compare well with the state of the art and show that when improving image resolution with the SRCNN the sex-classification rate increases. Additionally, a novel selfie database captured from 150 subjects with an iPhone X was created (available upon request).



### SiamRPN++: Evolution of Siamese Visual Tracking with Very Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/1812.11703v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.11703v1)
- **Published**: 2018-12-31 06:14:11+00:00
- **Updated**: 2018-12-31 06:14:11+00:00
- **Authors**: Bo Li, Wei Wu, Qiang Wang, Fangyi Zhang, Junliang Xing, Junjie Yan
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Siamese network based trackers formulate tracking as convolutional feature cross-correlation between target template and searching region. However, Siamese trackers still have accuracy gap compared with state-of-the-art algorithms and they cannot take advantage of feature from deep networks, such as ResNet-50 or deeper. In this work we prove the core reason comes from the lack of strict translation invariance. By comprehensive theoretical analysis and experimental validations, we break this restriction through a simple yet effective spatial aware sampling strategy and successfully train a ResNet-driven Siamese tracker with significant performance gain. Moreover, we propose a new model architecture to perform depth-wise and layer-wise aggregations, which not only further improves the accuracy but also reduces the model size. We conduct extensive ablation studies to demonstrate the effectiveness of the proposed tracker, which obtains currently the best results on four large tracking benchmarks, including OTB2015, VOT2018, UAV123, and LaSOT. Our model will be released to facilitate further studies based on this problem.



### Total Variation with Overlapping Group Sparsity and Lp Quasinorm for Infrared Image Deblurring under Salt-and-Pepper Noise
- **Arxiv ID**: http://arxiv.org/abs/1812.11725v2
- **DOI**: 10.1117/1.JEI.28.4.043031
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.11725v2)
- **Published**: 2018-12-31 08:54:33+00:00
- **Updated**: 2019-01-01 12:36:36+00:00
- **Authors**: Xingguo Liu, Yinping Chen, Zhenming Peng, Juan Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Because of the limitations of the infrared imaging principle and the properties of infrared imaging systems, infrared images have some drawbacks, including a lack of details, indistinct edges, and a large amount of salt-andpepper noise. To improve the sparse characteristics of the image while maintaining the image edges and weakening staircase artifacts, this paper proposes a method that uses the Lp quasinorm instead of the L1 norm and for infrared image deblurring with an overlapping group sparse total variation method. The Lp quasinorm introduces another degree of freedom, better describes image sparsity characteristics, and improves image restoration. Furthermore, we adopt the accelerated alternating direction method of multipliers and fast Fourier transform theory in the proposed method to improve the efficiency and robustness of our algorithm. Experiments show that under different conditions for blur and salt-and-pepper noise, the proposed method leads to excellent performance in terms of objective evaluation and subjective visual results.



### The meaning of "most" for visual question answering models
- **Arxiv ID**: http://arxiv.org/abs/1812.11737v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.11737v2)
- **Published**: 2018-12-31 09:41:04+00:00
- **Updated**: 2019-06-04 08:22:29+00:00
- **Authors**: Alexander Kuhnle, Ann Copestake
- **Comment**: None
- **Journal**: None
- **Summary**: The correct interpretation of quantifier statements in the context of a visual scene requires non-trivial inference mechanisms. For the example of "most", we discuss two strategies which rely on fundamentally different cognitive concepts. Our aim is to identify what strategy deep learning models for visual question answering learn when trained on such questions. To this end, we carefully design data to replicate experiments from psycholinguistics where the same question was investigated for humans. Focusing on the FiLM visual question answering model, our experiments indicate that a form of approximate number system emerges whose performance declines with more difficult scenes as predicted by Weber's law. Moreover, we identify confounding factors, like spatial arrangement of the scene, which impede the effectiveness of this system.



### Predicting Group Cohesiveness in Images
- **Arxiv ID**: http://arxiv.org/abs/1812.11771v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.11771v4)
- **Published**: 2018-12-31 12:01:19+00:00
- **Updated**: 2019-04-07 06:16:27+00:00
- **Authors**: Shreya Ghosh, Abhinav Dhall, Nicu Sebe, Tom Gedeon
- **Comment**: None
- **Journal**: None
- **Summary**: The cohesiveness of a group is an essential indicator of the emotional state, structure and success of a group of people. We study the factors that influence the perception of group-level cohesion and propose methods for estimating the human-perceived cohesion on the group cohesiveness scale. In order to identify the visual cues (attributes) for cohesion, we conducted a user survey. Image analysis is performed at a group-level via a multi-task convolutional neural network. For analyzing the contribution of facial expressions of the group members for predicting the Group Cohesion Score (GCS), a capsule network is explored. We add GCS to the Group Affect database and propose the `GAF-Cohesion database'. The proposed model performs well on the database and is able to achieve near human-level performance in predicting a group's cohesion score. It is interesting to note that group cohesion as an attribute, when jointly trained for group-level emotion prediction, helps in increasing the performance for the later task. This suggests that group-level emotion and cohesion are correlated.



### Weakly Supervised Active Learning with Cluster Annotation
- **Arxiv ID**: http://arxiv.org/abs/1812.11780v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.11780v2)
- **Published**: 2018-12-31 13:06:09+00:00
- **Updated**: 2019-01-25 11:37:04+00:00
- **Authors**: Fábio Perez, Rémi Lebret, Karl Aberer
- **Comment**: Poster session at the Bayesian Deep Learning Workshop - NeurIPS 2018
- **Journal**: None
- **Summary**: In this work, we introduce a novel framework that employs cluster annotation to boost active learning by reducing the number of human interactions required to train deep neural networks. Instead of annotating single samples individually, humans can also label clusters, producing a higher number of annotated samples with the cost of a small label error. Our experiments show that the proposed framework requires 82% and 87% less human interactions for CIFAR-10 and EuroSAT datasets respectively when compared with the fully-supervised training while maintaining similar performance on the test set.



### PVNet: Pixel-wise Voting Network for 6DoF Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1812.11788v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.11788v1)
- **Published**: 2018-12-31 13:24:10+00:00
- **Updated**: 2018-12-31 13:24:10+00:00
- **Authors**: Sida Peng, Yuan Liu, Qixing Huang, Hujun Bao, Xiaowei Zhou
- **Comment**: The first two authors contributed equally to this paper. Project
  page: https://zju-3dv.github.io/pvnet/
- **Journal**: None
- **Summary**: This paper addresses the challenge of 6DoF pose estimation from a single RGB image under severe occlusion or truncation. Many recent works have shown that a two-stage approach, which first detects keypoints and then solves a Perspective-n-Point (PnP) problem for pose estimation, achieves remarkable performance. However, most of these methods only localize a set of sparse keypoints by regressing their image coordinates or heatmaps, which are sensitive to occlusion and truncation. Instead, we introduce a Pixel-wise Voting Network (PVNet) to regress pixel-wise unit vectors pointing to the keypoints and use these vectors to vote for keypoint locations using RANSAC. This creates a flexible representation for localizing occluded or truncated keypoints. Another important feature of this representation is that it provides uncertainties of keypoint locations that can be further leveraged by the PnP solver. Experiments show that the proposed approach outperforms the state of the art on the LINEMOD, Occlusion LINEMOD and YCB-Video datasets by a large margin, while being efficient for real-time pose estimation. We further create a Truncation LINEMOD dataset to validate the robustness of our approach against truncation. The code will be avaliable at https://zju-3dv.github.io/pvnet/.



### Pixel personality for dense object tracking in a 2D honeybee hive
- **Arxiv ID**: http://arxiv.org/abs/1812.11797v1
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.QM, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.11797v1)
- **Published**: 2018-12-31 13:46:21+00:00
- **Updated**: 2018-12-31 13:46:21+00:00
- **Authors**: Katarzyna Bozek, Laetitia Hebert, Alexander S Mikheyev, Greg J Stephens
- **Comment**: 13 pages, 4 main and 9 supplementary figures as well as a link to
  supplementary movies
- **Journal**: None
- **Summary**: Tracking large numbers of densely-arranged, interacting objects is challenging due to occlusions and the resulting complexity of possible trajectory combinations, as well as the sparsity of relevant, labeled datasets. Here we describe a novel technique of collective tracking in the model environment of a 2D honeybee hive in which sample colonies consist of $N\sim10^3$ highly similar individuals, tightly packed, and in rapid, irregular motion. Such a system offers universal challenges for multi-object tracking, while being conveniently accessible for image recording. We first apply an accurate, segmentation-based object detection method to build initial short trajectory segments by matching object configurations based on class, position and orientation. We then join these tracks into full single object trajectories by creating an object recognition model which is adaptively trained to recognize honeybee individuals through their visual appearance across multiple frames, an attribute we denote as pixel personality. Overall, we reconstruct ~46% of the trajectories in 5 min recordings from two different hives and over 71% of the tracks for at least 2 min. We provide validated trajectories spanning 3000 video frames of 876 unmarked moving bees in two distinct colonies in different locations and filmed with different pixel resolutions, which we expect to be useful in the further development of general-purpose tracking solutions.



### Regularized Binary Network Training
- **Arxiv ID**: http://arxiv.org/abs/1812.11800v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1812.11800v3)
- **Published**: 2018-12-31 14:07:08+00:00
- **Updated**: 2020-04-21 04:57:46+00:00
- **Authors**: Sajad Darabi, Mouloud Belbahri, Matthieu Courbariaux, Vahid Partovi Nia
- **Comment**: NeurIPS19 Workshop on Energy Efficient Machine Learning and Cognitive
  Computing (2019)
- **Journal**: None
- **Summary**: There is a significant performance gap between Binary Neural Networks (BNNs) and floating point Deep Neural Networks (DNNs). We propose to improve the binary training method, by introducing a new regularization function that encourages training weights around binary values. In addition, we add trainable scaling factors to our regularization functions. Additionally, an improved approximation of the derivative of the sign activation function in the backward computation. These modifications are based on linear operations that are easily implementable into the binary training framework. Experimental results on ImageNet shows our method outperforms the traditional BNN method and XNOR-net.



### An introduction to domain adaptation and transfer learning
- **Arxiv ID**: http://arxiv.org/abs/1812.11806v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.11806v2)
- **Published**: 2018-12-31 14:19:20+00:00
- **Updated**: 2019-01-14 13:06:25+00:00
- **Authors**: Wouter M. Kouw, Marco Loog
- **Comment**: Technical Report. 41 pages, 5 figures
- **Journal**: None
- **Summary**: In machine learning, if the training data is an unbiased sample of an underlying distribution, then the learned classification function will make accurate predictions for new samples. However, if the training data is not an unbiased sample, then there will be differences between how the training data is distributed and how the test data is distributed. Standard classifiers cannot cope with changes in data distributions between training and test phases, and will not perform well. Domain adaptation and transfer learning are sub-fields within machine learning that are concerned with accounting for these types of changes. Here, we present an introduction to these fields, guided by the question: when and how can a classifier generalize from a source to a target domain? We will start with a brief introduction into risk minimization, and how transfer learning and domain adaptation expand upon this framework. Following that, we discuss three special cases of data set shift, namely prior, covariate and concept shift. For more complex domain shifts, there are a wide variety of approaches. These are categorized into: importance-weighting, subspace mapping, domain-invariant spaces, feature augmentation, minimax estimators and robust algorithms. A number of points will arise, which we will discuss in the last section. We conclude with the remark that many open questions will have to be addressed before transfer learners and domain-adaptive classifiers become practical.



### Towards a topological-geometrical theory of group equivariant non-expansive operators for data analysis and machine learning
- **Arxiv ID**: http://arxiv.org/abs/1812.11832v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, math.AT, math.OA, stat.ML, 55N35 (Primary), 47H09, 54H15, 57S10, 68U05, 65D18 (Secondary)
- **Links**: [PDF](http://arxiv.org/pdf/1812.11832v3)
- **Published**: 2018-12-31 15:17:07+00:00
- **Updated**: 2019-03-03 19:18:26+00:00
- **Authors**: Mattia G. Bergomi, Patrizio Frosini, Daniela Giorgi, Nicola Quercioli
- **Comment**: Added references. Extended Section 7. Added 3 figures. Corrected
  typos. 42 pages, 7 figures
- **Journal**: None
- **Summary**: The aim of this paper is to provide a general mathematical framework for group equivariance in the machine learning context. The framework builds on a synergy between persistent homology and the theory of group actions. We define group-equivariant non-expansive operators (GENEOs), which are maps between function spaces associated with groups of transformations. We study the topological and metric properties of the space of GENEOs to evaluate their approximating power and set the basis for general strategies to initialise and compose operators. We begin by defining suitable pseudo-metrics for the function spaces, the equivariance groups, and the set of non-expansive operators. Basing on these pseudo-metrics, we prove that the space of GENEOs is compact and convex, under the assumption that the function spaces are compact and convex. These results provide fundamental guarantees in a machine learning perspective. We show examples on the MNIST and fashion-MNIST datasets. By considering isometry-equivariant non-expansive operators, we describe a simple strategy to select and sample operators, and show how the selected and sampled operators can be used to perform both classical metric learning and an effective initialisation of the kernels of a convolutional neural network.



### Do GANs leave artificial fingerprints?
- **Arxiv ID**: http://arxiv.org/abs/1812.11842v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.11842v1)
- **Published**: 2018-12-31 15:32:33+00:00
- **Updated**: 2018-12-31 15:32:33+00:00
- **Authors**: Francesco Marra, Diego Gragnaniello, Luisa Verdoliva, Giovanni Poggi
- **Comment**: None
- **Journal**: None
- **Summary**: In the last few years, generative adversarial networks (GAN) have shown tremendous potential for a number of applications in computer vision and related fields. With the current pace of progress, it is a sure bet they will soon be able to generate high-quality images and videos, virtually indistinguishable from real ones. Unfortunately, realistic GAN-generated images pose serious threats to security, to begin with a possible flood of fake multimedia, and multimedia forensic countermeasures are in urgent need. In this work, we show that each GAN leaves its specific fingerprint in the images it generates, just like real-world cameras mark acquired images with traces of their photo-response non-uniformity pattern. Source identification experiments with several popular GANs show such fingerprints to represent a precious asset for forensic analyses.



### Learning Spatial Common Sense with Geometry-Aware Recurrent Networks
- **Arxiv ID**: http://arxiv.org/abs/1901.00003v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.00003v3)
- **Published**: 2018-12-31 15:37:18+00:00
- **Updated**: 2019-04-09 00:39:11+00:00
- **Authors**: Hsiao-Yu Fish Tung, Ricson Cheng, Katerina Fragkiadaki
- **Comment**: None
- **Journal**: None
- **Summary**: We integrate two powerful ideas, geometry and deep visual representation learning, into recurrent network architectures for mobile visual scene understanding. The proposed networks learn to "lift" and integrate 2D visual features over time into latent 3D feature maps of the scene. They are equipped with differentiable geometric operations, such as projection, unprojection, egomotion estimation and stabilization, in order to compute a geometrically-consistent mapping between the world scene and their 3D latent feature state. We train the proposed architectures to predict novel camera views given short frame sequences as input. Their predictions strongly generalize to scenes with a novel number of objects, appearances and configurations; they greatly outperform previous works that do not consider egomotion stabilization or a space-aware latent feature state. We train the proposed architectures to detect and segment objects in 3D using the latent 3D feature map as input--as opposed to per frame features. The resulting object detections persist over time: they continue to exist even when an object gets occluded or leaves the field of view. Our experiments suggest the proposed space-aware latent feature memory and egomotion-stabilized convolutions are essential architectural choices for spatial common sense to emerge in artificial embodied visual agents.



### Fast Perceptual Image Enhancement
- **Arxiv ID**: http://arxiv.org/abs/1812.11852v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.11852v1)
- **Published**: 2018-12-31 15:52:29+00:00
- **Updated**: 2018-12-31 15:52:29+00:00
- **Authors**: Etienne de Stoutz, Andrey Ignatov, Nikolay Kobyshev, Radu Timofte, Luc Van Gool
- **Comment**: None
- **Journal**: None
- **Summary**: The vast majority of photos taken today are by mobile phones. While their quality is rapidly growing, due to physical limitations and cost constraints, mobile phone cameras struggle to compare in quality with DSLR cameras. This motivates us to computationally enhance these images. We extend upon the results of Ignatov et al., where they are able to translate images from compact mobile cameras into images with comparable quality to high-resolution photos taken by DSLR cameras. However, the neural models employed require large amounts of computational resources and are not lightweight enough to run on mobile devices. We build upon the prior work and explore different network architectures targeting an increase in image quality and speed. With an efficient network architecture which does most of its processing in a lower spatial resolution, we achieve a significantly higher mean opinion score (MOS) than the baseline while speeding up the computation by 6.3 times on a consumer-grade CPU. This suggests a promising direction for neural-network-based photo enhancement using the phone hardware of the future.



### Accurate, Data-Efficient, Unconstrained Text Recognition with Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1812.11894v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.11894v1)
- **Published**: 2018-12-31 16:53:21+00:00
- **Updated**: 2018-12-31 16:53:21+00:00
- **Authors**: Mohamed Yousef, Khaled F. Hussain, Usama S. Mohammed
- **Comment**: Submitted for publication
- **Journal**: None
- **Summary**: Unconstrained text recognition is an important computer vision task, featuring a wide variety of different sub-tasks, each with its own set of challenges. One of the biggest promises of deep neural networks has been the convergence and automation of feature extractors from input raw signals, allowing for the highest possible performance with minimum required domain knowledge. To this end, we propose a data-efficient, end-to-end neural network model for generic, unconstrained text recognition. In our proposed architecture we strive for simplicity and efficiency without sacrificing recognition accuracy. Our proposed architecture is a fully convolutional network without any recurrent connections trained with the CTC loss function. Thus it operates on arbitrary input sizes and produces strings of arbitrary length in a very efficient and parallelizable manner. We show the generality and superiority of our proposed text recognition architecture by achieving state of the art results on seven public benchmark datasets, covering a wide spectrum of text recognition tasks, namely: Handwriting Recognition, CAPTCHA recognition, OCR, License Plate Recognition, and Scene Text Recognition. Our proposed architecture has won the ICFHR2018 Competition on Automated Text Recognition on a READ Dataset.



### Large-Scale Object Detection of Images from Network Cameras in Variable Ambient Lighting Conditions
- **Arxiv ID**: http://arxiv.org/abs/1812.11901v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.11901v1)
- **Published**: 2018-12-31 17:06:44+00:00
- **Updated**: 2018-12-31 17:06:44+00:00
- **Authors**: Caleb Tung, Matthew R. Kelleher, Ryan J. Schlueter, Binhan Xu, Yung-Hsiang Lu, George K. Thiruvathukal, Yen-Kuang Chen, Yang Lu
- **Comment**: Submitted to MIPR 2019 (Accepted)
- **Journal**: None
- **Summary**: Computer vision relies on labeled datasets for training and evaluation in detecting and recognizing objects. The popular computer vision program, YOLO ("You Only Look Once"), has been shown to accurately detect objects in many major image datasets. However, the images found in those datasets, are independent of one another and cannot be used to test YOLO's consistency at detecting the same object as its environment (e.g. ambient lighting) changes. This paper describes a novel effort to evaluate YOLO's consistency for large-scale applications. It does so by working (a) at large scale and (b) by using consecutive images from a curated network of public video cameras deployed in a variety of real-world situations, including traffic intersections, national parks, shopping malls, university campuses, etc. We specifically examine YOLO's ability to detect objects in different scenarios (e.g., daytime vs. night), leveraging the cameras' ability to rapidly retrieve many successive images for evaluating detection consistency. Using our camera network and advanced computing resources (supercomputers), we analyzed more than 5 million images captured by 140 network cameras in 24 hours. Compared with labels marked by humans (considered as "ground truth"), YOLO struggles to consistently detect the same humans and cars as their positions change from one frame to the next; it also struggles to detect objects at night time. Our findings suggest that state-of-the art vision solutions should be trained by data from network camera with contextual information before they can be deployed in applications that demand high consistency on object detection.



### High Quality Monocular Depth Estimation via Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/1812.11941v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.11941v2)
- **Published**: 2018-12-31 18:25:21+00:00
- **Updated**: 2019-03-10 07:46:03+00:00
- **Authors**: Ibraheem Alhashim, Peter Wonka
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: Accurate depth estimation from images is a fundamental task in many applications including scene understanding and reconstruction. Existing solutions for depth estimation often produce blurry approximations of low resolution. This paper presents a convolutional neural network for computing a high-resolution depth map given a single RGB image with the help of transfer learning. Following a standard encoder-decoder architecture, we leverage features extracted using high performing pre-trained networks when initializing our encoder along with augmentation and training strategies that lead to more accurate results. We show how, even for a very simple decoder, our method is able to achieve detailed high-resolution depth maps. Our network, with fewer parameters and training iterations, outperforms state-of-the-art on two datasets and also produces qualitatively better results that capture object boundaries more faithfully. Code and corresponding pre-trained weights are made publicly available.



### Image Super-Resolution via RL-CSC: When Residual Learning Meets Convolutional Sparse Coding
- **Arxiv ID**: http://arxiv.org/abs/1812.11950v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.4
- **Links**: [PDF](http://arxiv.org/pdf/1812.11950v1)
- **Published**: 2018-12-31 18:44:26+00:00
- **Updated**: 2018-12-31 18:44:26+00:00
- **Authors**: Menglei Zhang, Zhou Liu, Lei Yu
- **Comment**: 10 pages, 8 figures, 4 tables
- **Journal**: None
- **Summary**: We propose a simple yet effective model for Single Image Super-Resolution (SISR), by combining the merits of Residual Learning and Convolutional Sparse Coding (RL-CSC). Our model is inspired by the Learned Iterative Shrinkage-Threshold Algorithm (LISTA). We extend LISTA to its convolutional version and build the main part of our model by strictly following the convolutional form, which improves the network's interpretability. Specifically, the convolutional sparse codings of input feature maps are learned in a recursive manner, and high-frequency information can be recovered from these CSCs. More importantly, residual learning is applied to alleviate the training difficulty when the network goes deeper. Extensive experiments on benchmark datasets demonstrate the effectiveness of our method. RL-CSC (30 layers) outperforms several recent state-of-the-arts, e.g., DRRN (52 layers) and MemNet (80 layers) in both accuracy and visual qualities. Codes and more results are available at https://github.com/axzml/RL-CSC.



### The role of visual saliency in the automation of seismic interpretation
- **Arxiv ID**: http://arxiv.org/abs/1812.11960v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.11960v1)
- **Published**: 2018-12-31 18:55:26+00:00
- **Updated**: 2018-12-31 18:55:26+00:00
- **Authors**: Muhammad Amir Shafiq, Tariq Alshawi, Zhiling Long, Ghassan AlRegib
- **Comment**: None
- **Journal**: Geophysical Prospecting, vol. 66, issue S1, pp. 132-143, Mar. 2018
- **Summary**: In this paper, we propose a workflow based on SalSi for the detection and delineation of geological structures such as salt domes. SalSi is a seismic attribute designed based on the modeling of human visual system that detects the salient features and captures the spatial correlation within seismic volumes for delineating seismic structures. Using SalSi, we can not only highlight the neighboring regions of salt domes to assist a seismic interpreter but also delineate such structures using a region growing method and post-processing. The proposed delineation workflow detects the salt-dome boundary with very good precision and accuracy. Experimental results show the effectiveness of the proposed workflow on a real seismic dataset acquired from the North Sea, F3 block. For the subjective evaluation of the results of different salt-dome delineation algorithms, we have used a reference salt-dome boundary interpreted by a geophysicist. For the objective evaluation of results, we have used five different metrics based on pixels, shape, and curvedness to establish the effectiveness of the proposed workflow. The proposed workflow is not only fast but also yields better results as compared to other salt-dome delineation algorithms and shows a promising potential in seismic interpretation.



### Mid-Level Visual Representations Improve Generalization and Sample Efficiency for Learning Visuomotor Policies
- **Arxiv ID**: http://arxiv.org/abs/1812.11971v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.NE, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1812.11971v3)
- **Published**: 2018-12-31 18:59:25+00:00
- **Updated**: 2019-04-22 07:12:34+00:00
- **Authors**: Alexander Sax, Bradley Emi, Amir R. Zamir, Leonidas Guibas, Silvio Savarese, Jitendra Malik
- **Comment**: See project website, demos, and code at http://perceptual.actor
- **Journal**: None
- **Summary**: How much does having visual priors about the world (e.g. the fact that the world is 3D) assist in learning to perform downstream motor tasks (e.g. delivering a package)? We study this question by integrating a generic perceptual skill set (e.g. a distance estimator, an edge detector, etc.) within a reinforcement learning framework--see Figure 1. This skill set (hereafter mid-level perception) provides the policy with a more processed state of the world compared to raw images.   We find that using a mid-level perception confers significant advantages over training end-to-end from scratch (i.e. not leveraging priors) in navigation-oriented tasks. Agents are able to generalize to situations where the from-scratch approach fails and training becomes significantly more sample efficient. However, we show that realizing these gains requires careful selection of the mid-level perceptual skills. Therefore, we refine our findings into an efficient max-coverage feature set that can be adopted in lieu of raw images. We perform our study in completely separate buildings for training and testing and compare against visually blind baseline policies and state-of-the-art feature learning methods.



### DCI: Discriminative and Contrast Invertible Descriptor
- **Arxiv ID**: http://arxiv.org/abs/1901.00027v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.00027v1)
- **Published**: 2018-12-31 19:32:18+00:00
- **Updated**: 2018-12-31 19:32:18+00:00
- **Authors**: Zhenwei Miao, Kim-Hui Yap, Xudong Jiang, Subbhuraam Sinduja, Zhenhua Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Local feature descriptors have been widely used in fine-grained visual object search thanks to their robustness in scale and rotation variation and cluttered background. However, the performance of such descriptors drops under severe illumination changes. In this paper, we proposed a Discriminative and Contrast Invertible (DCI) local feature descriptor. In order to increase the discriminative ability of the descriptor under illumination changes, a Laplace gradient based histogram is proposed. A robust contrast flipping estimate is proposed based on the divergence of a local region. Experiments on fine-grained object recognition and retrieval applications demonstrate the superior performance of DCI descriptor to others.



### Interest Point Detection based on Adaptive Ternary Coding
- **Arxiv ID**: http://arxiv.org/abs/1901.00031v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.00031v1)
- **Published**: 2018-12-31 20:00:00+00:00
- **Updated**: 2018-12-31 20:00:00+00:00
- **Authors**: Zhenwei Miao, Kim-Hui Yap, Xudong Jiang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, an adaptive pixel ternary coding mechanism is proposed and a contrast invariant and noise resistant interest point detector is developed on the basis of this mechanism. Every pixel in a local region is adaptively encoded into one of the three statuses: bright, uncertain and dark. The blob significance of the local region is measured by the spatial distribution of the bright and dark pixels. Interest points are extracted from this blob significance measurement. By labeling the statuses of ternary bright, uncertain, and dark, the proposed detector shows more robustness to image noise and quantization errors. Moreover, the adaptive strategy for the ternary cording, which relies on two thresholds that automatically converge to the median of the local region in measurement, enables this coding to be insensitive to the image local contrast. As a result, the proposed detector is invariant to illumination changes. The state-of-the-art results are achieved on the standard datasets, and also in the face recognition application.



### Deep Information Theoretic Registration
- **Arxiv ID**: http://arxiv.org/abs/1901.00040v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IT, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/1901.00040v1)
- **Published**: 2018-12-31 20:45:45+00:00
- **Updated**: 2018-12-31 20:45:45+00:00
- **Authors**: Alireza Sedghi, Jie Luo, Alireza Mehrtash, Steve Pieper, Clare M. Tempany, Tina Kapur, Parvin Mousavi, William M. Wells III
- **Comment**: None
- **Journal**: None
- **Summary**: This paper establishes an information theoretic framework for deep metric based image registration techniques. We show an exact equivalence between maximum profile likelihood and minimization of joint entropy, an important early information theoretic registration method. We further derive deep classifier-based metrics that can be used with iterated maximum likelihood to achieve Deep Information Theoretic Registration on patches rather than pixels. This alleviates a major shortcoming of previous information theoretic registration approaches, namely the implicit pixel-wise independence assumptions. Our proposed approach does not require well-registered training data; this brings previous fully supervised deep metric registration approaches to the realm of weak supervision. We evaluate our approach on several image registration tasks and show significantly better performance compared to mutual information, specifically when images have substantially different contrasts. This work enables general-purpose registration in applications where current methods are not successful.



### SiCloPe: Silhouette-Based Clothed People
- **Arxiv ID**: http://arxiv.org/abs/1901.00049v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.00049v2)
- **Published**: 2018-12-31 21:14:44+00:00
- **Updated**: 2019-04-10 11:30:39+00:00
- **Authors**: Ryota Natsume, Shunsuke Saito, Zeng Huang, Weikai Chen, Chongyang Ma, Hao Li, Shigeo Morishima
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a new silhouette-based representation for modeling clothed human bodies using deep generative models. Our method can reconstruct a complete and textured 3D model of a person wearing clothes from a single input picture. Inspired by the visual hull algorithm, our implicit representation uses 2D silhouettes and 3D joints of a body pose to describe the immense shape complexity and variations of clothed people. Given a segmented 2D silhouette of a person and its inferred 3D joints from the input picture, we first synthesize consistent silhouettes from novel view points around the subject. The synthesized silhouettes which are the most consistent with the input segmentation are fed into a deep visual hull algorithm for robust 3D shape prediction. We then infer the texture of the subject's back view using the frontal image and segmentation mask as input to a conditional generative adversarial network. Our experiments demonstrate that our silhouette-based model is an effective representation and the appearance of the back view can be predicted reliably using an image-to-image translation network. While classic methods based on parametric models often fail for single-view images of subjects with challenging clothing, our approach can still produce successful results, which are comparable to those obtained from multi-view input.



### Deep Frame Prediction for Video Coding
- **Arxiv ID**: http://arxiv.org/abs/1901.00062v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1901.00062v3)
- **Published**: 2018-12-31 23:41:50+00:00
- **Updated**: 2019-06-20 22:21:51+00:00
- **Authors**: Hyomin Choi, Ivan V. Bajic
- **Comment**: This paper is accepted by IEEE Transactions on Circuits and Systems
  for Video Technology in 2019
- **Journal**: None
- **Summary**: We propose a novel frame prediction method using a deep neural network (DNN), with the goal of improving video coding efficiency. The proposed DNN makes use of decoded frames, at both encoder and decoder, to predict textures of the current coding block. Unlike conventional inter-prediction, the proposed method does not require any motion information to be transferred between the encoder and the decoder. Still, both uni-directional and bi-directional prediction are possible using the proposed DNN, which is enabled by the use of the temporal index channel, in addition to color channels. In this study, we developed a jointly trained DNN for both uni- and bi- directional prediction, as well as separate networks for uni- and bi-directional prediction, and compared the efficacy of both approaches. The proposed DNNs were compared with the conventional motion-compensated prediction in the latest video coding standard, HEVC, in terms of BD-Bitrate. The experiments show that the proposed joint DNN (for both uni- and bi-directional prediction) reduces the luminance bitrate by about 4.4%, 2.4%, and 2.3% in the Low delay P, Low delay, and Random access configurations, respectively. In addition, using the separately trained DNNs brings further bit savings of about 0.3%-0.5%.



### Extreme Relative Pose Estimation for RGB-D Scans via Scene Completion
- **Arxiv ID**: http://arxiv.org/abs/1901.00063v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.00063v2)
- **Published**: 2018-12-31 23:43:16+00:00
- **Updated**: 2019-01-05 22:33:42+00:00
- **Authors**: Zhenpei Yang, Jeffrey Z. Pan, Linjie Luo, Xiaowei Zhou, Kristen Grauman, Qixing Huang
- **Comment**: fixed issues with bibtex file
- **Journal**: None
- **Summary**: Estimating the relative rigid pose between two RGB-D scans of the same underlying environment is a fundamental problem in computer vision, robotics, and computer graphics. Most existing approaches allow only limited maximum relative pose changes since they require considerable overlap between the input scans. We introduce a novel deep neural network that extends the scope to extreme relative poses, with little or even no overlap between the input scans. The key idea is to infer more complete scene information about the underlying environment and match on the completed scans. In particular, instead of only performing scene completion from each individual scan, our approach alternates between relative pose estimation and scene completion. This allows us to perform scene completion by utilizing information from both input scans at late iterations, resulting in better results for both scene completion and relative pose estimation. Experimental results on benchmark datasets show that our approach leads to considerable improvements over state-of-the-art approaches for relative pose estimation. In particular, our approach provides encouraging relative pose estimates even between non-overlapping scans.



