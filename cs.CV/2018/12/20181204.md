# Arxiv Papers in cs.CV on 2018-12-04
### Cross-Classification Clustering: An Efficient Multi-Object Tracking Technique for 3-D Instance Segmentation in Connectomics
- **Arxiv ID**: http://arxiv.org/abs/1812.01157v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.01157v2)
- **Published**: 2018-12-04 01:18:05+00:00
- **Updated**: 2019-06-15 19:43:19+00:00
- **Authors**: Yaron Meirovitch, Lu Mi, Hayk Saribekyan, Alexander Matveev, David Rolnick, Nir Shavit
- **Comment**: 11 figures
- **Journal**: The IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR), 2019, pp. 8425-8435
- **Summary**: Pixel-accurate tracking of objects is a key element in many computer vision applications, often solved by iterated individual object tracking or instance segmentation followed by object matching. Here we introduce cross-classification clustering (3C), a technique that simultaneously tracks complex, interrelated objects in an image stack. The key idea in cross-classification is to efficiently turn a clustering problem into a classification problem by running a logarithmic number of independent classifications per image, letting the cross-labeling of these classifications uniquely classify each pixel to the object labels. We apply the 3C mechanism to achieve state-of-the-art accuracy in connectomics -- the nanoscale mapping of neural tissue from electron microscopy volumes. Our reconstruction system increases scalability by an order of magnitude over existing single-object tracking methods (such as flood-filling networks). This scalability is important for the deployment of connectomics pipelines, since currently the best performing techniques require computing infrastructures that are beyond the reach of most laboratories. Our algorithm may offer benefits in other domains that require pixel-accurate tracking of multiple objects, such as segmentation of videos and medical imagery.



### Deep Generative Modeling of LiDAR Data
- **Arxiv ID**: http://arxiv.org/abs/1812.01180v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.01180v4)
- **Published**: 2018-12-04 02:56:00+00:00
- **Updated**: 2019-12-02 20:44:35+00:00
- **Authors**: Lucas Caccia, Herke van Hoof, Aaron Courville, Joelle Pineau
- **Comment**: Presented at IROS 2019
- **Journal**: None
- **Summary**: Building models capable of generating structured output is a key challenge for AI and robotics. While generative models have been explored on many types of data, little work has been done on synthesizing lidar scans, which play a key role in robot mapping and localization. In this work, we show that one can adapt deep generative models for this task by unravelling lidar scans into a 2D point map. Our approach can generate high quality samples, while simultaneously learning a meaningful latent representation of the data. We demonstrate significant improvements against state-of-the-art point cloud generation methods. Furthermore, we propose a novel data representation that augments the 2D signal with absolute positional information. We show that this helps robustness to noisy and imputed input; the learned model can recover the underlying lidar scan from seemingly uninformative data



### Bag of Tricks for Image Classification with Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1812.01187v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.01187v2)
- **Published**: 2018-12-04 03:07:35+00:00
- **Updated**: 2018-12-05 22:17:01+00:00
- **Authors**: Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, Mu Li
- **Comment**: 10 pages, 9 tables, 4 figures
- **Journal**: None
- **Summary**: Much of the recent progress made in image classification research can be credited to training procedure refinements, such as changes in data augmentations and optimization methods. In the literature, however, most refinements are either briefly mentioned as implementation details or only visible in source code. In this paper, we will examine a collection of such refinements and empirically evaluate their impact on the final model accuracy through ablation study. We will show that, by combining these refinements together, we are able to improve various CNN models significantly. For example, we raise ResNet-50's top-1 validation accuracy from 75.3% to 79.29% on ImageNet. We will also demonstrate that improvement on image classification accuracy leads to better transfer learning performance in other application domains such as object detection and semantic segmentation.



### Learning to Fuse Things and Stuff
- **Arxiv ID**: http://arxiv.org/abs/1812.01192v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.01192v2)
- **Published**: 2018-12-04 03:13:07+00:00
- **Updated**: 2019-05-16 20:23:31+00:00
- **Authors**: Jie Li, Allan Raventos, Arjun Bhargava, Takaaki Tagawa, Adrien Gaidon
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an end-to-end learning approach for panoptic segmentation, a novel task unifying instance (things) and semantic (stuff) segmentation. Our model, TASCNet, uses feature maps from a shared backbone network to predict in a single feed-forward pass both things and stuff segmentations. We explicitly constrain these two output distributions through a global things and stuff binary mask to enforce cross-task consistency. Our proposed unified network is competitive with the state of the art on several benchmarks for panoptic segmentation as well as on the individual semantic and instance segmentation tasks.



### Walking on Thin Air: Environment-Free Physics-based Markerless Motion Capture
- **Arxiv ID**: http://arxiv.org/abs/1812.01203v1
- **DOI**: None
- **Categories**: **cs.CV**, 65D19, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/1812.01203v1)
- **Published**: 2018-12-04 03:47:13+00:00
- **Updated**: 2018-12-04 03:47:13+00:00
- **Authors**: Micha Livne, Leonid Sigal, Marcus A. Brubaker, David J. Fleet
- **Comment**: 8 pages, 9 figures, accepted to CRV 2018 (Conference on Computer and
  Robot Vision)
- **Journal**: None
- **Summary**: We propose a generative approach to physics-based motion capture. Unlike prior attempts to incorporate physics into tracking that assume the subject and scene geometry are calibrated and known a priori, our approach is automatic and online. This distinction is important since calibration of the environment is often difficult, especially for motions with props, uneven surfaces, or outdoor scenes. The use of physics in this context provides a natural framework to reason about contact and the plausibility of recovered motions. We propose a fast data-driven parametric body model, based on linear-blend skinning, which decouples deformations due to pose, anthropometrics and body shape. Pose (and shape) parameters are estimated using robust ICP optimization with physics-based dynamic priors that incorporate contact. Contact is estimated from torque trajectories and predictions of which contact points were active. To our knowledge, this is the first approach to take physics into account without explicit {\em a priori} knowledge of the environment or body dimensions. We demonstrate effective tracking from a noisy single depth camera, improving on state-of-the-art results quantitatively and producing better qualitative results, reducing visual artifacts like foot-skate and jitter.



### Zoom-In-to-Check: Boosting Video Interpolation via Instance-level Discrimination
- **Arxiv ID**: http://arxiv.org/abs/1812.01210v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.01210v2)
- **Published**: 2018-12-04 04:17:42+00:00
- **Updated**: 2019-04-28 02:12:48+00:00
- **Authors**: Liangzhe Yuan, Yibo Chen, Hantian Liu, Tao Kong, Jianbo Shi
- **Comment**: CVPR 2019 camera-ready, supplementary video:
  https://youtu.be/q-_wIRq26DY
- **Journal**: None
- **Summary**: We propose a light-weight video frame interpolation algorithm. Our key innovation is an instance-level supervision that allows information to be learned from the high-resolution version of similar objects. Our experiment shows that the proposed method can generate state-of-the-art results across different datasets, with fractional computation resources (time and memory) of competing methods. Given two image frames, a cascade network creates an intermediate frame with 1) a flow-warping module that computes coarse bi-directional optical flow and creates an interpolated image via flow-based warping, followed by 2) an image synthesis module to make fine-scale corrections. In the learning stage, object detection proposals are generated on the interpolated image.Lower resolution objects are zoomed into, and the learning algorithms using an adversarial loss trained on high-resolution objects to guide the system towards the instance-level refinement corrects details of object shape and boundaries.



### Prototype-based Neural Network Layers: Incorporating Vector Quantization
- **Arxiv ID**: http://arxiv.org/abs/1812.01214v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.01214v2)
- **Published**: 2018-12-04 04:33:12+00:00
- **Updated**: 2019-01-18 11:07:39+00:00
- **Authors**: Sascha Saralajew, Lars Holdijk, Maike Rees, Thomas Villmann
- **Comment**: None
- **Journal**: None
- **Summary**: Neural networks currently dominate the machine learning community and they do so for good reasons. Their accuracy on complex tasks such as image classification is unrivaled at the moment and with recent improvements they are reasonably easy to train. Nevertheless, neural networks are lacking robustness and interpretability. Prototype-based vector quantization methods on the other hand are known for being robust and interpretable. For this reason, we propose techniques and strategies to merge both approaches. This contribution will particularly highlight the similarities between them and outline how to construct a prototype-based classification layer for multilayer networks. Additionally, we provide an alternative, prototype-based, approach to the classical convolution operation. Numerical results are not part of this report, instead the focus lays on establishing a strong theoretical framework. By publishing our framework and the respective theoretical considerations and justifications before finalizing our numerical experiments we hope to jump-start the incorporation of prototype-based learning in neural networks and vice versa.



### Ladder Networks for Semi-Supervised Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1812.01222v1
- **DOI**: 10.13140/RG.2.2.33254.27208
- **Categories**: **cs.CV**, cs.LG, stat.ML, 68T10
- **Links**: [PDF](http://arxiv.org/pdf/1812.01222v1)
- **Published**: 2018-12-04 05:24:47+00:00
- **Updated**: 2018-12-04 05:24:47+00:00
- **Authors**: Julian Büchel, Okan Ersoy
- **Comment**: Technical Report, 5 pages, 8 figures
- **Journal**: None
- **Summary**: We used the Ladder Network [Rasmus et al. (2015)] to perform Hyperspectral Image Classification in a semi-supervised setting. The Ladder Network distinguishes itself from other semi-supervised methods by jointly optimizing a supervised and unsupervised cost. In many settings this has proven to be more successful than other semi-supervised techniques, such as pretraining using unlabeled data. We furthermore show that the convolutional Ladder Network outperforms most of the current techniques used in hyperspectral image classification and achieves new state-of-the-art performance on the Pavia University dataset given only 5 labeled data points per class.



### The Alignment of the Spheres: Globally-Optimal Spherical Mixture Alignment for Camera Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1812.01232v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.01232v2)
- **Published**: 2018-12-04 05:54:17+00:00
- **Updated**: 2019-06-18 09:05:54+00:00
- **Authors**: Dylan Campbell, Lars Petersson, Laurent Kneip, Hongdong Li, Stephen Gould
- **Comment**: None
- **Journal**: None
- **Summary**: Determining the position and orientation of a calibrated camera from a single image with respect to a 3D model is an essential task for many applications. When 2D-3D correspondences can be obtained reliably, perspective-n-point solvers can be used to recover the camera pose. However, without the pose it is non-trivial to find cross-modality correspondences between 2D images and 3D models, particularly when the latter only contains geometric information. Consequently, the problem becomes one of estimating pose and correspondences jointly. Since outliers and local optima are so prevalent, robust objective functions and global search strategies are desirable. Hence, we cast the problem as a 2D-3D mixture model alignment task and propose the first globally-optimal solution to this formulation under the robust $L_2$ distance between mixture distributions. We search the 6D camera pose space using branch-and-bound, which requires novel bounds, to obviate the need for a pose estimate and guarantee global optimality. To accelerate convergence, we integrate local optimization, implement GPU bound computations, and provide an intuitive way to incorporate side information such as semantic labels. The algorithm is evaluated on challenging synthetic and real datasets, outperforming existing approaches and reliably converging to the global optimum.



### Spatio-Temporal Action Graph Networks
- **Arxiv ID**: http://arxiv.org/abs/1812.01233v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.01233v2)
- **Published**: 2018-12-04 05:58:20+00:00
- **Updated**: 2019-09-29 16:57:16+00:00
- **Authors**: Roei Herzig, Elad Levi, Huijuan Xu, Hang Gao, Eli Brosh, Xiaolong Wang, Amir Globerson, Trevor Darrell
- **Comment**: IEEE/CVF International Conference on Computer Vision Workshop
  (ICCVW), 2019
- **Journal**: None
- **Summary**: Events defined by the interaction of objects in a scene are often of critical importance; yet important events may have insufficient labeled examples to train a conventional deep model to generalize to future object appearance. Activity recognition models that represent object interactions explicitly have the potential to learn in a more efficient manner than those that represent scenes with global descriptors. We propose a novel inter-object graph representation for activity recognition based on a disentangled graph embedding with direct observation of edge appearance. We employ a novel factored embedding of the graph structure, disentangling a representation hierarchy formed over spatial dimensions from that found over temporal variation. We demonstrate the effectiveness of our model on the Charades activity recognition benchmark, as well as a new dataset of driving activities focusing on multi-object interactions with near-collision events. Our model offers significantly improved performance compared to baseline approaches without object-graph representations, or with previous graph-based models.



### Efficient Attention: Attention with Linear Complexities
- **Arxiv ID**: http://arxiv.org/abs/1812.01243v9
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, I.2.6; I.2.10; I.4.8; I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/1812.01243v9)
- **Published**: 2018-12-04 06:41:46+00:00
- **Updated**: 2020-11-11 03:40:08+00:00
- **Authors**: Zhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, Hongsheng Li
- **Comment**: To appear at WACV 2021
- **Journal**: None
- **Summary**: Dot-product attention has wide applications in computer vision and natural language processing. However, its memory and computational costs grow quadratically with the input size. Such growth prohibits its application on high-resolution inputs. To remedy this drawback, this paper proposes a novel efficient attention mechanism equivalent to dot-product attention but with substantially less memory and computational costs. Its resource efficiency allows more widespread and flexible integration of attention modules into a network, which leads to better accuracies. Empirical evaluations demonstrated the effectiveness of its advantages. Efficient attention modules brought significant performance boosts to object detectors and instance segmenters on MS-COCO 2017. Further, the resource efficiency democratizes attention to complex models, where high costs prohibit the use of dot-product attention. As an exemplar, a model with efficient attention achieved state-of-the-art accuracies for stereo depth estimation on the Scene Flow dataset. Code is available at https://github.com/cmsflash/efficient-attention.



### Conditional Video Generation Using Action-Appearance Captions
- **Arxiv ID**: http://arxiv.org/abs/1812.01261v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.01261v2)
- **Published**: 2018-12-04 07:54:39+00:00
- **Updated**: 2018-12-05 04:19:27+00:00
- **Authors**: Shohei Yamamoto, Antonio Tejero-de-Pablos, Yoshitaka Ushiku, Tatsuya Harada
- **Comment**: None
- **Journal**: None
- **Summary**: The field of automatic video generation has received a boost thanks to the recent Generative Adversarial Networks (GANs). However, most existing methods cannot control the contents of the generated video using a text caption, losing their usefulness to a large extent. This particularly affects human videos due to their great variety of actions and appearances. This paper presents Conditional Flow and Texture GAN (CFT-GAN), a GAN-based video generation method from action-appearance captions. We propose a novel way of generating video by encoding a caption (e.g., "a man in blue jeans is playing golf") in a two-stage generation pipeline. Our CFT-GAN uses such caption to generate an optical flow (action) and a texture (appearance) for each frame. As a result, the output video reflects the content specified in the caption in a plausible way. Moreover, to train our method, we constructed a new dataset for human video generation with captions. We evaluated the proposed method qualitatively and quantitatively via an ablation study and a user study. The results demonstrate that CFT-GAN is able to successfully generate videos containing the action and appearances indicated in the captions.



### Multimodal Explanations by Predicting Counterfactuality in Videos
- **Arxiv ID**: http://arxiv.org/abs/1812.01263v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.01263v2)
- **Published**: 2018-12-04 08:02:23+00:00
- **Updated**: 2019-05-20 03:18:25+00:00
- **Authors**: Atsushi Kanehira, Kentaro Takemoto, Sho Inayoshi, Tatsuya Harada
- **Comment**: Camera ready version of CVPR'19
- **Journal**: None
- **Summary**: This study addresses generating counterfactual explanations with multimodal information. Our goal is not only to classify a video into a specific category, but also to provide explanations on why it is not categorized to a specific class with combinations of visual-linguistic information. Requirements that the expected output should satisfy are referred to as counterfactuality in this paper: (1) Compatibility of visual-linguistic explanations, and (2) Positiveness/negativeness for the specific positive/negative class. Exploiting a spatio-temporal region (tube) and an attribute as visual and linguistic explanations respectively, the explanation model is trained to predict the counterfactuality for possible combinations of multimodal information in a post-hoc manner. The optimization problem, which appears during training/inference, can be efficiently solved by inserting a novel neural network layer, namely the maximum subpath layer. We demonstrated the effectiveness of this method by comparison with a baseline of the action recognition datasets extended for this task. Moreover, we provide information-theoretical insight into the proposed method.



### Image Dehazing via Joint Estimation of Transmittance Map and Environmental Illumination
- **Arxiv ID**: http://arxiv.org/abs/1812.01273v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.01273v1)
- **Published**: 2018-12-04 08:28:23+00:00
- **Updated**: 2018-12-04 08:28:23+00:00
- **Authors**: Sanchayan Santra, Ranjan Mondal, Pranoy Panda, Nishant Mohanty, Shubham Bhuyan
- **Comment**: 6 pages, 9 figures, Presented at the Ninth International Conference
  on Advances in Pattern Recognition(ICAPR), December 2017, Bengaluru, India
- **Journal**: None
- **Summary**: Haze limits the visibility of outdoor images, due to the existence of fog, smoke and dust in the atmosphere. Image dehazing methods try to recover haze-free image by removing the effect of haze from a given input image. In this paper, we present an end to end system, which takes a hazy image as its input and returns a dehazed image. The proposed method learns the mapping between a hazy image and its corresponding transmittance map and the environmental illumination, by using a multi-scale Convolutional Neural Network. Although most of the time haze appears grayish in color, its color may vary depending on the color of the environmental illumination. Very few of the existing image dehazing methods have laid stress on its accurate estimation. But the color of the dehazed image and the estimated transmittance depends on the environmental illumination. Our proposed method exploits the relationship between the transmittance values and the environmental illumination as per the haze imaging model and estimates both of them. Qualitative and quantitative evaluations show, the estimates are accurate enough.



### Learning to Explain with Complemental Examples
- **Arxiv ID**: http://arxiv.org/abs/1812.01280v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.01280v2)
- **Published**: 2018-12-04 08:52:05+00:00
- **Updated**: 2019-05-20 10:00:37+00:00
- **Authors**: Atsushi Kanehira, Tatsuya Harada
- **Comment**: Camera ready version of CVPR'19
- **Journal**: None
- **Summary**: This paper addresses the generation of explanations with visual examples. Given an input sample, we build a system that not only classifies it to a specific category, but also outputs linguistic explanations and a set of visual examples that render the decision interpretable. Focusing especially on the complementarity of the multimodal information, i.e., linguistic and visual examples, we attempt to achieve it by maximizing the interaction information, which provides a natural definition of complementarity from an information theoretical viewpoint. We propose a novel framework to generate complemental explanations, on which the joint distribution of the variables to explain, and those to be explained is parameterized by three different neural networks: predictor, linguistic explainer, and example selector. Explanation models are trained collaboratively to maximize the interaction information to ensure the generated explanation are complemental to each other for the target. The results of experiments conducted on several datasets demonstrate the effectiveness of the proposed method.



### Towards Continuous Domain adaptation for Healthcare
- **Arxiv ID**: http://arxiv.org/abs/1812.01281v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.01281v1)
- **Published**: 2018-12-04 08:59:03+00:00
- **Updated**: 2018-12-04 08:59:03+00:00
- **Authors**: Rahul Venkataramani, Hariharan Ravishankar, Saihareesh Anamandra
- **Comment**: Machine Learning for Health (ML4H) Workshop at NeurIPS 2018
  arXiv:1811.07216
- **Journal**: None
- **Summary**: Deep learning algorithms have demonstrated tremendous success on challenging medical imaging problems. However, post-deployment, these algorithms are susceptible to data distribution variations owing to \emph{limited data issues} and \emph{diversity} in medical images. In this paper, we propose \emph{ContextNets}, a generic memory-augmented neural network framework for semantic segmentation to achieve continuous domain adaptation without the necessity of retraining. Unlike existing methods which require access to entire source and target domain images, our algorithm can adapt to a target domain with a few similar images. We condition the inference on any new input with features computed on its support set of images (and masks, if available) through contextual embeddings to achieve site-specific adaptation. We demonstrate state-of-the-art domain adaptation performance on the X-ray lung segmentation problem from three independent cohorts that differ in disease type, gender, contrast and intensity variations.



### Rare Event Detection using Disentangled Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/1812.01285v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.01285v1)
- **Published**: 2018-12-04 09:05:34+00:00
- **Updated**: 2018-12-04 09:05:34+00:00
- **Authors**: Ryuhei Hamaguchi, Ken Sakurada, Ryosuke Nakamura
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a novel method for rare event detection from an image pair with class-imbalanced datasets. A straightforward approach for event detection tasks is to train a detection network from a large-scale dataset in an end-to-end manner. However, in many applications such as building change detection on satellite images, few positive samples are available for the training. Moreover, scene image pairs contain many trivial events, such as in illumination changes or background motions. These many trivial events and the class imbalance problem lead to false alarms for rare event detection. In order to overcome these difficulties, we propose a novel method to learn disentangled representations from only low-cost negative samples. The proposed method disentangles different aspects in a pair of observations: variant and invariant factors that represent trivial events and image contents, respectively. The effectiveness of the proposed approach is verified by the quantitative evaluations on four change detection datasets, and the qualitative analysis shows that the proposed method can acquire the representations that disentangle rare events from trivial ones.



### FaceFeat-GAN: a Two-Stage Approach for Identity-Preserving Face Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1812.01288v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.01288v1)
- **Published**: 2018-12-04 09:11:46+00:00
- **Updated**: 2018-12-04 09:11:46+00:00
- **Authors**: Yujun Shen, Bolei Zhou, Ping Luo, Xiaoou Tang
- **Comment**: 12 pages and 6 figures
- **Journal**: None
- **Summary**: The advance of Generative Adversarial Networks (GANs) enables realistic face image synthesis. However, synthesizing face images that preserve facial identity as well as have high diversity within each identity remains challenging. To address this problem, we present FaceFeat-GAN, a novel generative model that improves both image quality and diversity by using two stages. Unlike existing single-stage models that map random noise to image directly, our two-stage synthesis includes the first stage of diverse feature generation and the second stage of feature-to-image rendering. The competitions between generators and discriminators are carefully designed in both stages with different objective functions. Specially, in the first stage, they compete in the feature domain to synthesize various facial features rather than images. In the second stage, they compete in the image domain to render photo-realistic images that contain high diversity but preserve identity. Extensive experiments show that FaceFeat-GAN generates images that not only retain identity information but also have high diversity and quality, significantly outperforming previous methods.



### Timeception for Complex Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1812.01289v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.01289v2)
- **Published**: 2018-12-04 09:17:38+00:00
- **Updated**: 2019-04-28 09:53:54+00:00
- **Authors**: Noureldien Hussein, Efstratios Gavves, Arnold W. M. Smeulders
- **Comment**: IEEE CVPR 2019 (Oral)
- **Journal**: None
- **Summary**: This paper focuses on the temporal aspect for recognizing human activities in videos; an important visual cue that has long been undervalued. We revisit the conventional definition of activity and restrict it to Complex Action: a set of one-actions with a weak temporal pattern that serves a specific purpose. Related works use spatiotemporal 3D convolutions with fixed kernel size, too rigid to capture the varieties in temporal extents of complex actions, and too short for long-range temporal modeling. In contrast, we use multi-scale temporal convolutions, and we reduce the complexity of 3D convolutions. The outcome is Timeception convolution layers, which reasons about minute-long temporal patterns, a factor of 8 longer than best related works. As a result, Timeception achieves impressive accuracy in recognizing the human activities of Charades, Breakfast Actions, and MultiTHUMOS. Further, we demonstrate that Timeception learns long-range temporal dependencies and tolerate temporal extents of complex actions.



### Channel-wise pruning of neural networks with tapering resource constraint
- **Arxiv ID**: http://arxiv.org/abs/1812.07060v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.07060v1)
- **Published**: 2018-12-04 10:41:09+00:00
- **Updated**: 2018-12-04 10:41:09+00:00
- **Authors**: Alexey Kruglov
- **Comment**: None
- **Journal**: None
- **Summary**: Neural network pruning is an important step in design process of efficient neural networks for edge devices with limited computational power. Pruning is a form of knowledge transfer from the weights of the original network to a smaller target subnetwork. We propose a new method for compute-constrained structured channel-wise pruning of convolutional neural networks. The method iteratively fine-tunes the network, while gradually tapering the computation resources available to the pruned network via a holonomic constraint in the method of Lagrangian multipliers framework. An explicit and adaptive automatic control over the rate of tapering is provided. The trainable parameters of our pruning method are separate from the weights of the neural network, which allows us to avoid the interference with the neural network solver (e.g. avoid the direct dependence of pruning speed on neural network learning rates). Our method combines the `rigoristic' approach by the direct application of constrained optimization, avoiding the pitfalls of ADMM-based methods, like their need to define the target amount of resources for each pruning run, and direct dependence of pruning speed and priority of pruning on the relative scale of weights between layers. For VGG-16 @ ILSVRC-2012, we achieve reduction of 15.47 -> 3.87 GMAC with only 1% top-1 accuracy reduction (68.4% -> 67.4%). For AlexNet @ ILSVRC-2012, we achieve 0.724 -> 0.411 GMAC with 1% top-1 accuracy reduction (56.8% -> 55.8%).



### From biological vision to unsupervised hierarchical sparse coding
- **Arxiv ID**: http://arxiv.org/abs/1812.01335v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.01335v1)
- **Published**: 2018-12-04 11:07:11+00:00
- **Updated**: 2018-12-04 11:07:11+00:00
- **Authors**: Victor Boutin, Angelo Franciosini, Franck Ruffier, Laurent. U Perrinet
- **Comment**: in Proceedings of iTWIST'18, Paper-ID: 17, Marseille, France,
  November, 21-23, 2018
- **Journal**: None
- **Summary**: The formation of connections between neural cells is emerging essentially from an unsupervised learning process. For instance, during the development of the primary visual cortex of mammals (V1), we observe the emergence of cells selective to localized and oriented features. This leads to the development of a rough contour-based representation of the retinal image in area V1. We propose a biological model of the formation of this representation along the thalamo-cortical pathway. To achieve this goal, we replicated the Multi-Layer Convolutional Sparse Coding (ML-CSC) algorithm developed by Michael Elad's group.



### Weakly Supervised Convolutional LSTM Approach for Tool Tracking in Laparoscopic Videos
- **Arxiv ID**: http://arxiv.org/abs/1812.01366v2
- **DOI**: 10.1007/s11548-019-01958-6
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.01366v2)
- **Published**: 2018-12-04 12:21:13+00:00
- **Updated**: 2019-02-15 10:57:00+00:00
- **Authors**: Chinedu Innocent Nwoye, Didier Mutter, Jacques Marescaux, Nicolas Padoy
- **Comment**: 14 pages, 3 figures, 3 tables, Supplementary video:
  https://youtu.be/vnMwlS5tvHE and https://youtu.be/SNhd1yzOe50
- **Journal**: International Journal of Computer Assisted Radiology and Surgery
  14, 1059-1067 (2019)
- **Summary**: Purpose: Real-time surgical tool tracking is a core component of the future intelligent operating room (OR), because it is highly instrumental to analyze and understand the surgical activities. Current methods for surgical tool tracking in videos need to be trained on data in which the spatial positions of the tools are manually annotated. Generating such training data is difficult and time-consuming. Instead, we propose to use solely binary presence annotations to train a tool tracker for laparoscopic videos. Methods: The proposed approach is composed of a CNN + Convolutional LSTM (ConvLSTM) neural network trained end-to-end, but weakly supervised on tool binary presence labels only. We use the ConvLSTM to model the temporal dependencies in the motion of the surgical tools and leverage its spatio-temporal ability to smooth the class peak activations in the localization heat maps (Lh-maps).   Results: We build a baseline tracker on top of the CNN model and demonstrate that our approach based on the ConvLSTM outperforms the baseline in tool presence detection, spatial localization, and motion tracking by over 5.0%, 13.9%, and 12.6%, respectively.   Conclusions: In this paper, we demonstrate that binary presence labels are sufficient for training a deep learning tracking model using our proposed method. We also show that the ConvLSTM can leverage the spatio-temporal coherence of consecutive image frames across a surgical video to improve tool presence detection, spatial localization, and motion tracking.   keywords: Surgical workflow analysis, tool tracking, weak supervision, spatio-temporal coherence, ConvLSTM, endoscopic videos



### Estimating 6D Pose From Localizing Designated Surface Keypoints
- **Arxiv ID**: http://arxiv.org/abs/1812.01387v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1812.01387v1)
- **Published**: 2018-12-04 12:55:06+00:00
- **Updated**: 2018-12-04 12:55:06+00:00
- **Authors**: Zelin Zhao, Gao Peng, Haoyu Wang, Hao-Shu Fang, Chengkun Li, Cewu Lu
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present an accurate yet effective solution for 6D pose estimation from an RGB image. The core of our approach is that we first designate a set of surface points on target object model as keypoints and then train a keypoint detector (KPD) to localize them. Finally a PnP algorithm can recover the 6D pose according to the 2D-3D relationship of keypoints. Different from recent state-of-the-art CNN-based approaches that rely on a time-consuming post-processing procedure, our method can achieve competitive accuracy without any refinement after pose prediction. Meanwhile, we obtain a 30% relative improvement in terms of ADD accuracy among methods without using refinement. Moreover, we succeed in handling heavy occlusion by selecting the most confident keypoints to recover the 6D pose. For the sake of reproducibility, we will make our code and models publicly available soon.



### A multi-class structured dictionary learning method using discriminant atom selection
- **Arxiv ID**: http://arxiv.org/abs/1812.01389v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.01389v1)
- **Published**: 2018-12-04 13:02:40+00:00
- **Updated**: 2018-12-04 13:02:40+00:00
- **Authors**: R. E. Rolón, L. E. Di Persia, R. D. Spies, H. L. Rufiner
- **Comment**: 18 pages, 8 figures and 3 tables
- **Journal**: None
- **Summary**: In the last decade, traditional dictionary learning methods have been successfully applied to various pattern classification tasks. Although these methods produce sparse representations of signals which are robust against distortions and missing data, such representations quite often turn out to be unsuitable if the final objective is signal classification. In order to overcome or at least to attenuate such a weakness, several new methods which incorporate discriminative information into sparse-inducing models have emerged in recent years. In particular, methods for discriminative dictionary learning have shown to be more accurate (in terms of signal classification) than the traditional ones, which are only focused on minimizing the total representation error. In this work, we present both a novel multi-class discriminative measure and an innovative dictionary learning method. For a given dictionary, this new measure, which takes into account not only when a particular atom is used for representing signals coming from a certain class and the magnitude of its corresponding representation coefficient, but also the effect that such an atom has in the total representation error, is capable of efficiently quantifying the degree of discriminability of each one of the atoms. On the other hand, the new dictionary construction method yields dictionaries which are highly suitable for multi-class classification tasks. Our method was tested with a widely used database for handwritten digit recognition and compared with three state-of-the-art classification methods. The results show that our method significantly outperforms the other three achieving good recognition rates and additionally, reducing the computational cost of the classifier.



### Semi-Supervised Cross-Modal Retrieval with Label Prediction
- **Arxiv ID**: http://arxiv.org/abs/1812.01391v2
- **DOI**: 10.1109/TMM.2019.2954741
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.01391v2)
- **Published**: 2018-12-04 13:07:15+00:00
- **Updated**: 2020-01-01 07:23:40+00:00
- **Authors**: Devraj Mandal, Pramod Rao, Soma Biswas
- **Comment**: Updated Version of the Paper has been accepted in IEEE Transactions
  on Multimedia {https://ieeexplore.ieee.org/document/8907496/}
- **Journal**: None
- **Summary**: Due to abundance of data from multiple modalities, cross-modal retrieval tasks with image-text, audio-image, etc. are gaining increasing importance. Of the different approaches proposed, supervised methods usually give significant improvement over their unsupervised counterparts at the additional cost of labeling or annotation of the training data. Semi-supervised methods are recently becoming popular as they provide an elegant framework to balance the conflicting requirement of labeling cost and accuracy. In this work, we propose a novel deep semi-supervised framework which can seamlessly handle both labeled as well as unlabeled data. The network has two important components: (a) the label prediction component predicts the labels for the unlabeled portion of the data and then (b) a common modality-invariant representation is learned for cross-modal retrieval. The two parts of the network are trained sequentially one after the other. Extensive experiments on three standard benchmark datasets, Wiki, Pascal VOC and NUS-WIDE demonstrate that the proposed framework outperforms the state-of-the-art for both supervised and semi-supervised settings.



### TextField: Learning A Deep Direction Field for Irregular Scene Text Detection
- **Arxiv ID**: http://arxiv.org/abs/1812.01393v2
- **DOI**: 10.1109/TIP.2019.2900589
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.01393v2)
- **Published**: 2018-12-04 13:12:58+00:00
- **Updated**: 2019-07-29 06:18:33+00:00
- **Authors**: Yongchao Xu, Yukang Wang, Wei Zhou, Yongpan Wang, Zhibo Yang, Xiang Bai
- **Comment**: To appear in IEEE TIP
- **Journal**: None
- **Summary**: Scene text detection is an important step of scene text reading system. The main challenges lie on significantly varied sizes and aspect ratios, arbitrary orientations and shapes. Driven by recent progress in deep learning, impressive performances have been achieved for multi-oriented text detection. Yet, the performance drops dramatically in detecting curved texts due to the limited text representation (e.g., horizontal bounding boxes, rotated rectangles, or quadrilaterals). It is of great interest to detect curved texts, which are actually very common in natural scenes. In this paper, we present a novel text detector named TextField for detecting irregular scene texts. Specifically, we learn a direction field pointing away from the nearest text boundary to each text point. This direction field is represented by an image of two-dimensional vectors and learned via a fully convolutional neural network. It encodes both binary text mask and direction information used to separate adjacent text instances, which is challenging for classical segmentation-based approaches. Based on the learned direction field, we apply a simple yet effective morphological-based post-processing to achieve the final detection. Experimental results show that the proposed TextField outperforms the state-of-the-art methods by a large margin (28% and 8%) on two curved text datasets: Total-Text and CTW1500, respectively, and also achieves very competitive performance on multi-oriented datasets: ICDAR 2015 and MSRA-TD500. Furthermore, TextField is robust in generalizing to unseen datasets. The code is available at https://github.com/YukangWang/TextField.



### Meta Learning Deep Visual Words for Fast Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1812.01397v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.01397v3)
- **Published**: 2018-12-04 13:23:05+00:00
- **Updated**: 2020-08-17 00:02:54+00:00
- **Authors**: Harkirat Singh Behl, Mohammad Najafi, Anurag Arnab, Philip H. S. Torr
- **Comment**: None
- **Journal**: In Proceedings of International Conference on Intelligent Robots
  and Systems (IROS) 2020
- **Summary**: Personal robots and driverless cars need to be able to operate in novel environments and thus quickly and efficiently learn to recognise new object classes. We address this problem by considering the task of video object segmentation. Previous accurate methods for this task finetune a model using the first annotated frame, and/or use additional inputs such as optical flow and complex post-processing. In contrast, we develop a fast, causal algorithm that requires no finetuning, auxiliary inputs or post-processing, and segments a variable number of objects in a single forward-pass. We represent an object with clusters, or "visual words", in the embedding space, which correspond to object parts in the image space. This allows us to robustly match to the reference objects throughout the video, because although the global appearance of an object changes as it undergoes occlusions and deformations, the appearance of more local parts may stay consistent. We learn these visual words in an unsupervised manner, using meta-learning to ensure that our training objective matches our inference procedure. We achieve comparable accuracy to finetuning based methods (whilst being 1 to 2 orders of magnitude faster), and state-of-the-art in terms of speed/accuracy trade-offs on four video segmentation datasets. Code is available at https://github.com/harkiratbehl/MetaVOS.



### Inferring Point Clouds from Single Monocular Images by Depth Intermediation
- **Arxiv ID**: http://arxiv.org/abs/1812.01402v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.01402v3)
- **Published**: 2018-12-04 13:32:51+00:00
- **Updated**: 2020-10-26 12:30:49+00:00
- **Authors**: Wei Zeng, Sezer Karaoglu, Theo Gevers
- **Comment**: Statement: This paper is under consideration at Computer Vision and
  Image Understanding
- **Journal**: None
- **Summary**: In this paper, we propose a pipeline to generate 3D point cloud of an object from a single-view RGB image. Most previous work predict the 3D point coordinates from single RGB images directly. We decompose this problem into depth estimation from single images and point cloud completion from partial point clouds.   Our method sequentially predicts the depth maps from images and then infers the complete 3D object point clouds based on the predicted partial point clouds. We explicitly impose the camera model geometrical constraint in our pipeline and enforce the alignment of the generated point clouds and estimated depth maps.   Experimental results for the single image 3D object reconstruction task show that the proposed method outperforms existing state-of-the-art methods. Both the qualitative and quantitative results demonstrate the generality and suitability of our method.



### Compressive Classification (Machine Learning without learning)
- **Arxiv ID**: http://arxiv.org/abs/1812.01410v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.01410v1)
- **Published**: 2018-12-04 13:50:11+00:00
- **Updated**: 2018-12-04 13:50:11+00:00
- **Authors**: Vincent Schellekens, Laurent Jacques
- **Comment**: in Proceedings of iTWIST'18, Paper-ID: 8, Marseille, France,
  November, 21-23, 2018
- **Journal**: None
- **Summary**: Compressive learning is a framework where (so far unsupervised) learning tasks use not the entire dataset but a compressed summary (sketch) of it. We propose a compressive learning classification method, and a novel sketch function for images.



### The Visual Centrifuge: Model-Free Layered Video Representations
- **Arxiv ID**: http://arxiv.org/abs/1812.01461v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.01461v2)
- **Published**: 2018-12-04 14:47:23+00:00
- **Updated**: 2019-04-04 10:44:27+00:00
- **Authors**: Jean-Baptiste Alayrac, João Carreira, Andrew Zisserman
- **Comment**: Appears in: 2019 IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR 2019). This arXiv contains the CVPR Camera Ready version of
  the paper (although we have included larger figures) as well as an appendix
  detailing the model architecture
- **Journal**: None
- **Summary**: True video understanding requires making sense of non-lambertian scenes where the color of light arriving at the camera sensor encodes information about not just the last object it collided with, but about multiple mediums -- colored windows, dirty mirrors, smoke or rain. Layered video representations have the potential of accurately modelling realistic scenes but have so far required stringent assumptions on motion, lighting and shape. Here we propose a learning-based approach for multi-layered video representation: we introduce novel uncertainty-capturing 3D convolutional architectures and train them to separate blended videos. We show that these models then generalize to single videos, where they exhibit interesting abilities: color constancy, factoring out shadows and separating reflections. We present quantitative and qualitative results on real world videos.



### Feasibility of Colon Cancer Detection in Confocal Laser Microscopy Images Using Convolution Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1812.01464v2
- **DOI**: 10.1007/978-3-658-25326-4_72
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.01464v2)
- **Published**: 2018-12-04 14:49:39+00:00
- **Updated**: 2018-12-05 10:00:00+00:00
- **Authors**: Nils Gessert, Lukas Wittig, Daniel Drömann, Tobias Keck, Alexander Schlaefer, David B. Ellebrecht
- **Comment**: Accepted at BVM Workshop 2019
- **Journal**: None
- **Summary**: Histological evaluation of tissue samples is a typical approach to identify colorectal cancer metastases in the peritoneum. For immediate assessment, reliable and real-time in-vivo imaging would be required. For example, intraoperative confocal laser microscopy has been shown to be suitable for distinguishing organs and also malignant and benign tissue. So far, the analysis is done by human experts. We investigate the feasibility of automatic colon cancer classification from confocal laser microscopy images using deep learning models. We overcome very small dataset sizes through transfer learning with state-of-the-art architectures. We achieve an accuracy of 89.1% for cancer detection in the peritoneum which indicates viability as an intraoperative decision support system.



### Cross-spectral Periocular Recognition: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1812.01465v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.01465v1)
- **Published**: 2018-12-04 14:52:15+00:00
- **Updated**: 2018-12-04 14:52:15+00:00
- **Authors**: S. S. Behera, Bappaditya Mandal, N. B. Puhan
- **Comment**: 12 pages, 4 figures, 1 table, accepted in the Third International
  Conference on Emerging Research in Electronics, Computer science and
  Technology (ICERECT), during August 2018
- **Journal**: None
- **Summary**: Among many biometrics such as face, iris, fingerprint and others, periocular region has the advantages over other biometrics because it is non-intrusive and serves as a balance between iris or eye region (very stringent, small area) and the whole face region (very relaxed large area). Research have shown that this is the region which does not get affected much because of various poses, aging, expression, facial changes and other artifacts, which otherwise would change to a large variation. Active research has been carried out on this topic since past few years due to its obvious advantages over face and iris biometrics in unconstrained and uncooperative scenarios. Many researchers have explored periocular biometrics involving both visible (VIS) and infra-red (IR) spectrum images. For a system to work for 24/7 (such as in surveillance scenarios), the registration process may depend on the day time VIS periocular images (or any mug shot image) and the testing or recognition process may occur in the night time involving only IR periocular images. This gives rise to a challenging research problem called the cross-spectral matching of images where VIS images are used for registration or as gallery images and IR images are used for testing or recognition process and vice versa. After intensive research of more than two decades on face and iris biometrics in cross-spectral domain, a number of researchers have now focused their work on matching heterogeneous (cross-spectral) periocular images. Though a number of surveys have been made on existing periocular biometric research, no study has been done on its cross-spectral aspect. This paper analyses and reviews current state-of-the-art techniques in cross-spectral periocular recognition including various methodologies, databases, their protocols and current-state-of-the-art recognition performances.



### Generating High Fidelity Images with Subscale Pixel Networks and Multidimensional Upscaling
- **Arxiv ID**: http://arxiv.org/abs/1812.01608v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.01608v1)
- **Published**: 2018-12-04 15:47:44+00:00
- **Updated**: 2018-12-04 15:47:44+00:00
- **Authors**: Jacob Menick, Nal Kalchbrenner
- **Comment**: None
- **Journal**: None
- **Summary**: The unconditional generation of high fidelity images is a longstanding benchmark for testing the performance of image decoders. Autoregressive image models have been able to generate small images unconditionally, but the extension of these methods to large images where fidelity can be more readily assessed has remained an open problem. Among the major challenges are the capacity to encode the vast previous context and the sheer difficulty of learning a distribution that preserves both global semantic coherence and exactness of detail. To address the former challenge, we propose the Subscale Pixel Network (SPN), a conditional decoder architecture that generates an image as a sequence of sub-images of equal size. The SPN compactly captures image-wide spatial dependencies and requires a fraction of the memory and the computation required by other fully autoregressive models. To address the latter challenge, we propose to use Multidimensional Upscaling to grow an image in both size and depth via intermediate stages utilising distinct SPNs. We evaluate SPNs on the unconditional generation of CelebAHQ of size 256 and of ImageNet from size 32 to 256. We achieve state-of-the-art likelihood results in multiple settings, set up new benchmark results in previously unexplored settings and are able to generate very high fidelity large scale samples on the basis of both datasets.



### Sturm: Sparse Tubal-Regularized Multilinear Regression for fMRI
- **Arxiv ID**: http://arxiv.org/abs/1812.01496v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.01496v1)
- **Published**: 2018-12-04 15:52:51+00:00
- **Updated**: 2018-12-04 15:52:51+00:00
- **Authors**: Wenwen Li, Jian Lou, Shuo Zhou, Haiping Lu
- **Comment**: 10 pages, 4 figures
- **Journal**: None
- **Summary**: While functional magnetic resonance imaging (fMRI) is important for healthcare/neuroscience applications, it is challenging to classify or interpret due to its multi-dimensional structure, high dimensionality, and small number of samples available. Recent sparse multilinear regression methods based on tensor are emerging as promising solutions for fMRI, yet existing works rely on unfolding/folding operations and a tensor rank relaxation with limited tightness. The newly proposed tensor singular value decomposition (t-SVD) sheds light on new directions. In this work, we study t-SVD for sparse multilinear regression and propose a Sparse tubal-regularized multilinear regression (Sturm) method for fMRI. Specifically, the Sturm model performs multilinear regression with two regularization terms: a tubal tensor nuclear norm based on t-SVD and a standard L1 norm. We further derive the algorithm under the alternating direction method of multipliers framework. We perform experiments on four classification problems, including both resting-state fMRI for disease diagnosis and task-based fMRI for neural decoding. The results show the superior performance of Sturm in classifying fMRI using just a small number of voxels.



### Topological Map Extraction from Overhead Images
- **Arxiv ID**: http://arxiv.org/abs/1812.01497v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.01497v3)
- **Published**: 2018-12-04 15:52:52+00:00
- **Updated**: 2019-11-29 12:34:35+00:00
- **Authors**: Zuoyue Li, Jan Dirk Wegner, Aurélien Lucchi
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: We propose a new approach, named PolyMapper, to circumvent the conventional pixel-wise segmentation of (aerial) images and predict objects in a vector representation directly. PolyMapper directly extracts the topological map of a city from overhead images as collections of building footprints and road networks. In order to unify the shape representation for different types of objects, we also propose a novel sequentialization method that reformulates a graph structure as closed polygons. Experiments are conducted on both existing and self-collected large-scale datasets of several cities. Our empirical results demonstrate that our end-to-end learnable model is capable of drawing polygons of building footprints and road networks that very closely approximate the structure of existing online map services, in a fully automated manner. Quantitative and qualitative comparison to the state-of-the-art also shows that our approach achieves good levels of performance. To the best of our knowledge, the automatic extraction of large-scale topological maps is a novel contribution in the remote sensing community that we believe will help develop models with more informed geometrical constraints.



### Content Authentication for Neural Imaging Pipelines: End-to-end Optimization of Photo Provenance in Complex Distribution Channels
- **Arxiv ID**: http://arxiv.org/abs/1812.01516v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1812.01516v2)
- **Published**: 2018-12-04 16:38:47+00:00
- **Updated**: 2019-02-25 16:46:19+00:00
- **Authors**: Pawel Korus, Nasir Memon
- **Comment**: Camera ready + supplement, CVPR'19
- **Journal**: None
- **Summary**: Forensic analysis of digital photo provenance relies on intrinsic traces left in the photograph at the time of its acquisition. Such analysis becomes unreliable after heavy post-processing, such as down-sampling and re-compression applied upon distribution in the Web. This paper explores end-to-end optimization of the entire image acquisition and distribution workflow to facilitate reliable forensic analysis at the end of the distribution channel. We demonstrate that neural imaging pipelines can be trained to replace the internals of digital cameras, and jointly optimized for high-fidelity photo development and reliable provenance analysis. In our experiments, the proposed approach increased image manipulation detection accuracy from 45% to over 90%. The findings encourage further research towards building more reliable imaging pipelines with explicit provenance-guaranteeing properties.



### SurfConv: Bridging 3D and 2D Convolution for RGBD Images
- **Arxiv ID**: http://arxiv.org/abs/1812.01519v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.01519v1)
- **Published**: 2018-12-04 16:42:29+00:00
- **Updated**: 2018-12-04 16:42:29+00:00
- **Authors**: Hang Chu, Wei-Chiu Ma, Kaustav Kundu, Raquel Urtasun, Sanja Fidler
- **Comment**: Published at CVPR 2018
- **Journal**: CVPR (2018) 3002-3011
- **Summary**: We tackle the problem of using 3D information in convolutional neural networks for down-stream recognition tasks. Using depth as an additional channel alongside the RGB input has the scale variance problem present in image convolution based approaches. On the other hand, 3D convolution wastes a large amount of memory on mostly unoccupied 3D space, which consists of only the surface visible to the sensor. Instead, we propose SurfConv, which "slides" compact 2D filters along the visible 3D surface. SurfConv is formulated as a simple depth-aware multi-scale 2D convolution, through a new Data-Driven Depth Discretization (D4) scheme. We demonstrate the effectiveness of our method on indoor and outdoor 3D semantic segmentation datasets. Our method achieves state-of-the-art performance with less than 30% parameters used by the 3D convolution-based approaches.



### A Face-to-Face Neural Conversation Model
- **Arxiv ID**: http://arxiv.org/abs/1812.01525v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1812.01525v1)
- **Published**: 2018-12-04 16:55:25+00:00
- **Updated**: 2018-12-04 16:55:25+00:00
- **Authors**: Hang Chu, Daiqing Li, Sanja Fidler
- **Comment**: Published at CVPR 2018
- **Journal**: CVPR (2018) 7113-7121
- **Summary**: Neural networks have recently become good at engaging in dialog. However, current approaches are based solely on verbal text, lacking the richness of a real face-to-face conversation. We propose a neural conversation model that aims to read and generate facial gestures alongside with text. This allows our model to adapt its response based on the "mood" of the conversation. In particular, we introduce an RNN encoder-decoder that exploits the movement of facial muscles, as well as the verbal conversation. The decoder consists of two layers, where the lower layer aims at generating the verbal response and coarse facial expressions, while the second layer fills in the subtle gestures, making the generated output more smooth and natural. We train our neural network by having it "watch" 250 movies. We showcase our joint face-text model in generating more natural conversations through automatic metrics and a human study. We demonstrate an example application with a face-to-face chatting avatar.



### Towards generative adversarial networks as a new paradigm for radiology education
- **Arxiv ID**: http://arxiv.org/abs/1812.01547v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.01547v1)
- **Published**: 2018-12-04 17:32:16+00:00
- **Updated**: 2018-12-04 17:32:16+00:00
- **Authors**: Samuel G. Finlayson, Hyunkwang Lee, Isaac S. Kohane, Luke Oakden-Rayner
- **Comment**: Machine Learning for Health (ML4H) Workshop at NeurIPS 2018
  arXiv:cs/0101200
- **Journal**: None
- **Summary**: Medical students and radiology trainees typically view thousands of images in order to "train their eye" to detect the subtle visual patterns necessary for diagnosis. Nevertheless, infrastructural and legal constraints often make it difficult to access and quickly query an abundance of images with a user-specified feature set. In this paper, we use a conditional generative adversarial network (GAN) to synthesize $1024\times1024$ pixel pelvic radiographs that can be queried with conditioning on fracture status. We demonstrate that the conditional GAN learns features that distinguish fractures from non-fractures by training a convolutional neural network exclusively on images sampled from the GAN and achieving an AUC of $>0.95$ on a held-out set of real images. We conduct additional analysis of the images sampled from the GAN and describe ongoing work to validate educational efficacy.



### A novel database of Children's Spontaneous Facial Expressions (LIRIS-CSE)
- **Arxiv ID**: http://arxiv.org/abs/1812.01555v2
- **DOI**: 10.1016/j.imavis.2019.02.004
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.01555v2)
- **Published**: 2018-12-04 17:49:25+00:00
- **Updated**: 2019-01-20 16:39:21+00:00
- **Authors**: Rizwan Ahmed Khan, Crenn Arthur, Alexandre Meyer, Saida Bouakaz
- **Comment**: None
- **Journal**: Image and Vision Computing, 2019
- **Summary**: Computing environment is moving towards human-centered designs instead of computer centered designs and human's tend to communicate wealth of information through affective states or expressions. Traditional Human Computer Interaction (HCI) based systems ignores bulk of information communicated through those affective states and just caters for user's intentional input. Generally, for evaluating and benchmarking different facial expression analysis algorithms, standardized databases are needed to enable a meaningful comparison. In the absence of comparative tests on such standardized databases it is difficult to find relative strengths and weaknesses of different facial expression recognition algorithms. In this article we present a novel video database for Children's Spontaneous facial Expressions (LIRIS-CSE). Proposed video database contains six basic spontaneous facial expressions shown by 12 ethnically diverse children between the ages of 6 and 12 years with mean age of 7.3 years. To the best of our knowledge, this database is first of its kind as it records and shows spontaneous facial expressions of children. Previously there were few database of children expressions and all of them show posed or exaggerated expressions which are different from spontaneous or natural expressions. Thus, this database will be a milestone for human behavior researchers. This database will be a excellent resource for vision community for benchmarking and comparing results. In this article, we have also proposed framework for automatic expression recognition based on convolutional neural network (CNN) architecture with transfer learning approach. Proposed architecture achieved average classification accuracy of 75% on our proposed database i.e. LIRIS-CSE.



### Detect-to-Retrieve: Efficient Regional Aggregation for Image Search
- **Arxiv ID**: http://arxiv.org/abs/1812.01584v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.01584v2)
- **Published**: 2018-12-04 18:40:20+00:00
- **Updated**: 2019-05-14 00:47:47+00:00
- **Authors**: Marvin Teichmann, Andre Araujo, Menglong Zhu, Jack Sim
- **Comment**: CVPR 2019. Code and dataset available:
  https://github.com/tensorflow/models/tree/master/research/delf
- **Journal**: None
- **Summary**: Retrieving object instances among cluttered scenes efficiently requires compact yet comprehensive regional image representations. Intuitively, object semantics can help build the index that focuses on the most relevant regions. However, due to the lack of bounding-box datasets for objects of interest among retrieval benchmarks, most recent work on regional representations has focused on either uniform or class-agnostic region selection. In this paper, we first fill the void by providing a new dataset of landmark bounding boxes, based on the Google Landmarks dataset, that includes $86k$ images with manually curated boxes from $15k$ unique landmarks. Then, we demonstrate how a trained landmark detector, using our new dataset, can be leveraged to index image regions and improve retrieval accuracy while being much more efficient than existing regional methods. In addition, we introduce a novel regional aggregated selective match kernel (R-ASMK) to effectively combine information from detected regions into an improved holistic image representation. R-ASMK boosts image retrieval accuracy substantially with no dimensionality increase, while even outperforming systems that index image regions independently. Our complete image retrieval system improves upon the previous state-of-the-art by significant margins on the Revisited Oxford and Paris datasets. Code and data available at the project webpage: https://github.com/tensorflow/models/tree/master/research/delf.



### Improving Semantic Segmentation via Video Propagation and Label Relaxation
- **Arxiv ID**: http://arxiv.org/abs/1812.01593v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.MM, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1812.01593v3)
- **Published**: 2018-12-04 18:49:54+00:00
- **Updated**: 2019-07-03 03:16:39+00:00
- **Authors**: Yi Zhu, Karan Sapra, Fitsum A. Reda, Kevin J. Shih, Shawn Newsam, Andrew Tao, Bryan Catanzaro
- **Comment**: CVPR 2019 Oral. Code link:
  https://github.com/NVIDIA/semantic-segmentation. YouTube link:
  https://www.youtube.com/watch?v=aEbXjGZDZSQ
- **Journal**: None
- **Summary**: Semantic segmentation requires large amounts of pixel-wise annotations to learn accurate models. In this paper, we present a video prediction-based methodology to scale up training sets by synthesizing new training samples in order to improve the accuracy of semantic segmentation networks. We exploit video prediction models' ability to predict future frames in order to also predict future labels. A joint propagation strategy is also proposed to alleviate mis-alignments in synthesized samples. We demonstrate that training segmentation models on datasets augmented by the synthesized samples leads to significant improvements in accuracy. Furthermore, we introduce a novel boundary label relaxation technique that makes training robust to annotation noise and propagation artifacts along object boundaries. Our proposed methods achieve state-of-the-art mIoUs of 83.5% on Cityscapes and 82.9% on CamVid. Our single model, without model ensembles, achieves 72.8% mIoU on the KITTI semantic segmentation test set, which surpasses the winning entry of the ROB challenge 2018. Our code and videos can be found at https://nv-adlr.github.io/publication/2018-Segmentation.



### Monocular Total Capture: Posing Face, Body, and Hands in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1812.01598v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1812.01598v1)
- **Published**: 2018-12-04 18:55:33+00:00
- **Updated**: 2018-12-04 18:55:33+00:00
- **Authors**: Donglai Xiang, Hanbyul Joo, Yaser Sheikh
- **Comment**: 17 pages, 16 figures
- **Journal**: None
- **Summary**: We present the first method to capture the 3D total motion of a target person from a monocular view input. Given an image or a monocular video, our method reconstructs the motion from body, face, and fingers represented by a 3D deformable mesh model. We use an efficient representation called 3D Part Orientation Fields (POFs), to encode the 3D orientations of all body parts in the common 2D image space. POFs are predicted by a Fully Convolutional Network (FCN), along with the joint confidence maps. To train our network, we collect a new 3D human motion dataset capturing diverse total body motion of 40 subjects in a multiview system. We leverage a 3D deformable human model to reconstruct total body pose from the CNN outputs by exploiting the pose and shape prior in the model. We also present a texture-based tracking method to obtain temporally coherent motion capture output. We perform thorough quantitative evaluations including comparison with the existing body-specific and hand-specific methods, and performance analysis on camera viewpoint and human pose changes. Finally, we demonstrate the results of our total body motion capture on various challenging in-the-wild videos. Our code and newly collected human motion dataset will be publicly shared.



### AutoFocus: Efficient Multi-Scale Inference
- **Arxiv ID**: http://arxiv.org/abs/1812.01600v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.01600v2)
- **Published**: 2018-12-04 18:57:08+00:00
- **Updated**: 2019-08-01 17:47:18+00:00
- **Authors**: Mahyar Najibi, Bharat Singh, Larry S. Davis
- **Comment**: To appear in Proceedings of International Conference on Computer
  Vision (ICCV), 2019
- **Journal**: None
- **Summary**: This paper describes AutoFocus, an efficient multi-scale inference algorithm for deep-learning based object detectors. Instead of processing an entire image pyramid, AutoFocus adopts a coarse to fine approach and only processes regions which are likely to contain small objects at finer scales. This is achieved by predicting category agnostic segmentation maps for small objects at coarser scales, called FocusPixels. FocusPixels can be predicted with high recall, and in many cases, they only cover a small fraction of the entire image. To make efficient use of FocusPixels, an algorithm is proposed which generates compact rectangular FocusChips which enclose FocusPixels. The detector is only applied inside FocusChips, which reduces computation while processing finer scales. Different types of error can arise when detections from FocusChips of multiple scales are combined, hence techniques to correct them are proposed. AutoFocus obtains an mAP of 47.9% (68.3% at 50% overlap) on the COCO test-dev set while processing 6.4 images per second on a Titan X (Pascal) GPU. This is 2.5X faster than our multi-scale baseline detector and matches its mAP. The number of pixels processed in the pyramid can be reduced by 5X with a 1% drop in mAP. AutoFocus obtains more than 10% mAP gain compared to RetinaNet but runs at the same speed with the same ResNet-101 backbone.



### Learning 3D Human Dynamics from Video
- **Arxiv ID**: http://arxiv.org/abs/1812.01601v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.01601v4)
- **Published**: 2018-12-04 18:57:10+00:00
- **Updated**: 2019-09-16 23:15:29+00:00
- **Authors**: Angjoo Kanazawa, Jason Y. Zhang, Panna Felsen, Jitendra Malik
- **Comment**: To appear in CVPR 2019. Changelog: v3. +an experiment to compare
  improvement from pseudo-gt data on single view vs temporal context model. v2.
  camready ver: Minor update in model training where the gaussian shape prior
  is used, updated results (similar results, same trends), added more ablation
  study in the appendix. v1. +evaluation protocol subsection in appendix,
  updated results due to bug fix
- **Journal**: None
- **Summary**: From an image of a person in action, we can easily guess the 3D motion of the person in the immediate past and future. This is because we have a mental model of 3D human dynamics that we have acquired from observing visual sequences of humans in motion. We present a framework that can similarly learn a representation of 3D dynamics of humans from video via a simple but effective temporal encoding of image features. At test time, from video, the learned temporal representation give rise to smooth 3D mesh predictions. From a single image, our model can recover the current 3D mesh as well as its 3D past and future motion. Our approach is designed so it can learn from videos with 2D pose annotations in a semi-supervised manner. Though annotated data is always limited, there are millions of videos uploaded daily on the Internet. In this work, we harvest this Internet-scale source of unlabeled data by training our model on unlabeled video with pseudo-ground truth 2D pose obtained from an off-the-shelf 2D pose detector. Our experiments show that adding more videos with pseudo-ground truth 2D pose monotonically improves 3D prediction performance. We evaluate our model, Human Mesh and Motion Recovery (HMMR), on the recent challenging dataset of 3D Poses in the Wild and obtain state-of-the-art performance on the 3D prediction task without any fine-tuning. The project website with video, code, and data can be found at https://akanazawa.github.io/human_dynamics/.



### Learning to Sample
- **Arxiv ID**: http://arxiv.org/abs/1812.01659v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.01659v2)
- **Published**: 2018-12-04 19:58:44+00:00
- **Updated**: 2019-04-01 16:10:37+00:00
- **Authors**: Oren Dovrat, Itai Lang, Shai Avidan
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: Processing large point clouds is a challenging task. Therefore, the data is often sampled to a size that can be processed more easily. The question is how to sample the data? A popular sampling technique is Farthest Point Sampling (FPS). However, FPS is agnostic to a downstream application (classification, retrieval, etc.). The underlying assumption seems to be that minimizing the farthest point distance, as done by FPS, is a good proxy to other objective functions.   We show that it is better to learn how to sample. To do that, we propose a deep network to simplify 3D point clouds. The network, termed S-NET, takes a point cloud and produces a smaller point cloud that is optimized for a particular task. The simplified point cloud is not guaranteed to be a subset of the original point cloud. Therefore, we match it to a subset of the original points in a post-processing step. We contrast our approach with FPS by experimenting on two standard data sets and show significantly better results for a variety of applications. Our code is publicly available at: https://github.com/orendv/learning_to_sample



### Decompose to manipulate: Manipulable Object Synthesis in 3D Medical Images with Structured Image Decomposition
- **Arxiv ID**: http://arxiv.org/abs/1812.01737v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.01737v2)
- **Published**: 2018-12-04 22:52:57+00:00
- **Updated**: 2019-02-07 15:38:08+00:00
- **Authors**: Siqi Liu, Eli Gibson, Sasa Grbic, Zhoubing Xu, Arnaud Arindra Adiyoso Setio, Jie Yang, Bogdan Georgescu, Dorin Comaniciu
- **Comment**: None
- **Journal**: None
- **Summary**: The performance of medical image analysis systems is constrained by the quantity of high-quality image annotations. Such systems require data to be annotated by experts with years of training, especially when diagnostic decisions are involved. Such datasets are thus hard to scale up. In this context, it is hard for supervised learning systems to generalize to the cases that are rare in the training set but would be present in real-world clinical practices. We believe that the synthetic image samples generated by a system trained on the real data can be useful for improving the supervised learning tasks in the medical image analysis applications. Allowing the image synthesis to be manipulable could help synthetic images provide complementary information to the training data rather than simply duplicating the real-data manifold. In this paper, we propose a framework for synthesizing 3D objects, such as pulmonary nodules, in 3D medical images with manipulable properties. The manipulation is enabled by decomposing of the object of interests into its segmentation mask and a 1D vector containing the residual information. The synthetic object is refined and blended into the image context with two adversarial discriminators. We evaluate the proposed framework on lung nodules in 3D chest CT images and show that the proposed framework could generate realistic nodules with manipulable shapes, textures and locations, etc. By sampling from both the synthetic nodules and the real nodules from 2800 3D CT volumes during the classifier training, we show the synthetic patches could improve the overall nodule detection performance by average 8.44% competition performance metric (CPM) score.



### Multiview Cross-supervision for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1812.01738v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.01738v1)
- **Published**: 2018-12-04 22:57:22+00:00
- **Updated**: 2018-12-04 22:57:22+00:00
- **Authors**: Yuan Yao, Hyun Soo Park
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a semi-supervised learning framework for a customized semantic segmentation task using multiview image streams. A key challenge of the customized task lies in the limited accessibility of the labeled data due to the requirement of prohibitive manual annotation effort. We hypothesize that it is possible to leverage multiview image streams that are linked through the underlying 3D geometry, which can provide an additional supervisionary signal to train a segmentation model. We formulate a new cross-supervision method using a shape belief transfer---the segmentation belief in one image is used to predict that of the other image through epipolar geometry analogous to shape-from-silhouette. The shape belief transfer provides the upper and lower bounds of the segmentation for the unlabeled data where its gap approaches asymptotically to zero as the number of the labeled views increases. We integrate this theory to design a novel network that is agnostic to camera calibration, network model, and semantic category and bypasses the intermediate process of suboptimal 3D reconstruction. We validate this network by recognizing a customized semantic category per pixel from realworld visual data including non-human species and a subject of interest in social videos where attaining large-scale annotation data is infeasible.



### Domain-Adaptive Single-View 3D Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1812.01742v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.01742v2)
- **Published**: 2018-12-04 23:01:00+00:00
- **Updated**: 2019-08-26 17:15:06+00:00
- **Authors**: Pedro O. Pinheiro, Negar Rostamzadeh, Sungjin Ahn
- **Comment**: None
- **Journal**: None
- **Summary**: Single-view 3D shape reconstruction is an important but challenging problem, mainly for two reasons. First, as shape annotation is very expensive to acquire, current methods rely on synthetic data, in which ground-truth 3D annotation is easy to obtain. However, this results in domain adaptation problem when applied to natural images. The second challenge is that there are multiple shapes that can explain a given 2D image. In this paper, we propose a framework to improve over these challenges using adversarial training. On one hand, we impose domain confusion between natural and synthetic image representations to reduce the distribution gap. On the other hand, we impose the reconstruction to be `realistic' by forcing it to lie on a (learned) manifold of realistic object shapes. Our experiments show that these constraints improve performance by a large margin over baseline reconstruction models. We achieve results competitive with the state of the art with a much simpler architecture.



### Complete the Look: Scene-based Complementary Product Recommendation
- **Arxiv ID**: http://arxiv.org/abs/1812.01748v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1812.01748v2)
- **Published**: 2018-12-04 23:30:22+00:00
- **Updated**: 2019-04-15 21:32:32+00:00
- **Authors**: Wang-Cheng Kang, Eric Kim, Jure Leskovec, Charles Rosenberg, Julian McAuley
- **Comment**: Accepted to CVPR'19
- **Journal**: None
- **Summary**: Modeling fashion compatibility is challenging due to its complexity and subjectivity. Existing work focuses on predicting compatibility between product images (e.g. an image containing a t-shirt and an image containing a pair of jeans). However, these approaches ignore real-world 'scene' images (e.g. selfies); such images are hard to deal with due to their complexity, clutter, variations in lighting and pose (etc.) but on the other hand could potentially provide key context (e.g. the user's body type, or the season) for making more accurate recommendations. In this work, we propose a new task called 'Complete the Look', which seeks to recommend visually compatible products based on scene images. We design an approach to extract training data for this task, and propose a novel way to learn the scene-product compatibility from fashion or interior design images. Our approach measures compatibility both globally and locally via CNNs and attention mechanisms. Extensive experiments show that our method achieves significant performance gains over alternative systems. Human evaluation and qualitative analysis are also conducted to further understand model behavior. We hope this work could lead to useful applications which link large corpora of real-world scenes with shoppable products.



### Cerebrovascular Network Segmentation on MRA Images with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1812.01752v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1812.01752v1)
- **Published**: 2018-12-04 23:38:54+00:00
- **Updated**: 2018-12-04 23:38:54+00:00
- **Authors**: Pedro Sanches, Cyril Meyer, Vincent Vigon, Benoît Naegel
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning has been shown to produce state of the art results in many tasks in biomedical imaging, especially in segmentation. Moreover, segmentation of the cerebrovascular structure from magnetic resonance angiography is a challenging problem because its complex geometry and topology have a large inter-patient variability. Therefore, in this work, we present a convolutional neural network approach for this problem. Particularly, a new network topology inspired by the U-net 3D and by the Inception modules, entitled Uception. In addition, a discussion about the best objective function for sparse data also guided most choices during the project. State of the art models are also implemented for a comparison purpose and final results show that the proposed architecture has the best performance in this particular context.



### Moment Matching for Multi-Source Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1812.01754v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.01754v4)
- **Published**: 2018-12-04 23:43:37+00:00
- **Updated**: 2019-08-27 19:47:46+00:00
- **Authors**: Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, Bo Wang
- **Comment**: Accepted As Oral Paper in the IEEE International Conference on
  Computer Vision 2019
- **Journal**: None
- **Summary**: Conventional unsupervised domain adaptation (UDA) assumes that training data are sampled from a single domain. This neglects the more practical scenario where training data are collected from multiple sources, requiring multi-source domain adaptation. We make three major contributions towards addressing this problem. First, we collect and annotate by far the largest UDA dataset, called DomainNet, which contains six domains and about 0.6 million images distributed among 345 categories, addressing the gap in data availability for multi-source UDA research. Second, we propose a new deep learning approach, Moment Matching for Multi-Source Domain Adaptation M3SDA, which aims to transfer knowledge learned from multiple labeled source domains to an unlabeled target domain by dynamically aligning moments of their feature distributions. Third, we provide new theoretical insights specifically for moment matching approaches in both single and multiple source domain adaptation. Extensive experiments are conducted to demonstrate the power of our new dataset in benchmarking state-of-the-art multi-source domain adaptation methods, as well as the advantage of our proposed model. Dataset and Code are available at \url{http://ai.bu.edu/M3SDA/}.



