# Arxiv Papers in cs.CV on 2018-12-10
### PoseFix: Model-agnostic General Human Pose Refinement Network
- **Arxiv ID**: http://arxiv.org/abs/1812.03595v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.03595v3)
- **Published**: 2018-12-10 01:58:12+00:00
- **Updated**: 2019-03-10 14:11:09+00:00
- **Authors**: Gyeongsik Moon, Ju Yong Chang, Kyoung Mu Lee
- **Comment**: Published at CVPR 2019
- **Journal**: None
- **Summary**: Multi-person pose estimation from a 2D image is an essential technique for human behavior understanding. In this paper, we propose a human pose refinement network that estimates a refined pose from a tuple of an input image and input pose. The pose refinement was performed mainly through an end-to-end trainable multi-stage architecture in previous methods. However, they are highly dependent on pose estimation models and require careful model design. By contrast, we propose a model-agnostic pose refinement method. According to a recent study, state-of-the-art 2D human pose estimation methods have similar error distributions. We use this error statistics as prior information to generate synthetic poses and use the synthesized poses to train our model. In the testing stage, pose estimation results of any other methods can be input to the proposed method. Moreover, the proposed model does not require code or knowledge about other methods, which allows it to be easily used in the post-processing step. We show that the proposed approach achieves better performance than the conventional multi-stage refinement models and consistently improves the performance of various state-of-the-art pose estimation methods on the commonly used benchmark. The code is available in this https URL\footnote{\url{https://github.com/mks0601/PoseFix_RELEASE}}.



### Task-Free Continual Learning
- **Arxiv ID**: http://arxiv.org/abs/1812.03596v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.03596v3)
- **Published**: 2018-12-10 02:07:57+00:00
- **Updated**: 2019-08-19 10:42:15+00:00
- **Authors**: Rahaf Aljundi, Klaas Kelchtermans, Tinne Tuytelaars
- **Comment**: Accepted as a conference paper in CVPR 2019. Rahaf Aljundi and Klaas
  Kelchtermans have contributed equally to this work
- **Journal**: None
- **Summary**: Methods proposed in the literature towards continual deep learning typically operate in a task-based sequential learning setup. A sequence of tasks is learned, one at a time, with all data of current task available but not of previous or future tasks. Task boundaries and identities are known at all times. This setup, however, is rarely encountered in practical applications. Therefore we investigate how to transform continual learning to an online setup. We develop a system that keeps on learning over time in a streaming fashion, with data distributions gradually changing and without the notion of separate tasks. To this end, we build on the work on Memory Aware Synapses, and show how this method can be made online by providing a protocol to decide i) when to update the importance weights, ii) which data to use to update them, and iii) how to accumulate the importance weights at each update step. Experimental results show the validity of the approach in the context of two applications: (self-)supervised learning of a face recognition model by watching soap series and learning a robot to avoid collisions.



### Learning Non-Uniform Hypergraph for Multi-Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/1812.03621v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.03621v1)
- **Published**: 2018-12-10 04:53:02+00:00
- **Updated**: 2018-12-10 04:53:02+00:00
- **Authors**: Longyin Wen, Dawei Du, Shengkun Li, Xiao Bian, Siwei Lyu
- **Comment**: 11 pages, 4 figures, accepted by AAAI 2019
- **Journal**: None
- **Summary**: The majority of Multi-Object Tracking (MOT) algorithms based on the tracking-by-detection scheme do not use higher order dependencies among objects or tracklets, which makes them less effective in handling complex scenarios. In this work, we present a new near-online MOT algorithm based on non-uniform hypergraph, which can model different degrees of dependencies among tracklets in a unified objective. The nodes in the hypergraph correspond to the tracklets and the hyperedges with different degrees encode various kinds of dependencies among them. Specifically, instead of setting the weights of hyperedges with different degrees empirically, they are learned automatically using the structural support vector machine algorithm (SSVM). Several experiments are carried out on various challenging datasets (i.e., PETS09, ParkingLot sequence, SubwayFace, and MOT16 benchmark), to demonstrate that our method achieves favorable performance against the state-of-the-art MOT methods.



### 3D Scene Parsing via Class-Wise Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1812.03622v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.03622v2)
- **Published**: 2018-12-10 04:53:14+00:00
- **Updated**: 2019-03-01 03:29:53+00:00
- **Authors**: Daichi Ono, Hiroyuki Yabe, Tsutomu Horikawa
- **Comment**: None
- **Journal**: None
- **Summary**: We propose the method that uses only computer graphics datasets to parse the real world 3D scenes. 3D scene parsing based on semantic segmentation is required to implement the categorical interaction in the virtual world. Convolutional Neural Networks (CNNs) have recently shown state-of-theart performance on computer vision tasks including semantic segmentation. However, collecting and annotating a huge amount of data are needed to train CNNs. Especially in the case of semantic segmentation, annotating pixel by pixel takes a significant amount of time and often makes mistakes. In contrast, computer graphics can generate a lot of accurate annotated data and easily scale up by changing camera positions, textures and lights. Despite these advantages, models trained on computer graphics datasets cannot perform well on real data, which is known as the domain shift. To address this issue, we first present that depth modal and synthetic noise are effective to reduce the domain shift. Then, we develop the class-wise adaptation which obtains domain invariant features of CNNs. To reduce the domain shift, we create computer graphics rooms with a lot of props, and provide photo-realistic rendered images.We also demonstrate the application which is combined semantic segmentation with Simultaneous Localization and Mapping (SLAM). Our application performs accurate 3D scene parsing in real-time on an actual room.



### EDF: Ensemble, Distill, and Fuse for Easy Video Labeling
- **Arxiv ID**: http://arxiv.org/abs/1812.03626v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.03626v1)
- **Published**: 2018-12-10 05:18:57+00:00
- **Updated**: 2018-12-10 05:18:57+00:00
- **Authors**: Giulio Zhou, Subramanya Dulloor, David G. Andersen, Michael Kaminsky
- **Comment**: None
- **Journal**: None
- **Summary**: We present a way to rapidly bootstrap object detection on unseen videos using minimal human annotations. We accomplish this by combining two complementary sources of knowledge (one generic and the other specific) using bounding box merging and model distillation. The first (generic) knowledge source is obtained from ensembling pre-trained object detectors using a novel bounding box merging and confidence reweighting scheme. We make the observation that model distillation with data augmentation can train a specialized detector that outperforms the noisy labels it was trained on, and train a Student Network on the ensemble detections that obtains higher mAP than the ensemble itself. The second (specialized) knowledge source comes from training a detector (which we call the Supervised Labeler) on a labeled subset of the video to generate detections on the unlabeled portion. We demonstrate on two popular vehicular datasets that these techniques work to emit bounding boxes for all vehicles in the frame with higher mean average precision (mAP) than any of the reference networks used, and that the combination of ensembled and human-labeled data produces object detections that outperform either alone.



### Spatial Knowledge Distillation to aid Visual Reasoning
- **Arxiv ID**: http://arxiv.org/abs/1812.03631v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.03631v2)
- **Published**: 2018-12-10 05:36:23+00:00
- **Updated**: 2018-12-11 16:42:29+00:00
- **Authors**: Somak Aditya, Rudra Saha, Yezhou Yang, Chitta Baral
- **Comment**: Equal contribution by first two authors. Accepted in WACV 2019
- **Journal**: None
- **Summary**: For tasks involving language and vision, the current state-of-the-art methods tend not to leverage any additional information that might be present to gather relevant (commonsense) knowledge. A representative task is Visual Question Answering where large diagnostic datasets have been proposed to test a system's capability of answering questions about images. The training data is often accompanied by annotations of individual object properties and spatial locations. In this work, we take a step towards integrating this additional privileged information in the form of spatial knowledge to aid in visual reasoning. We propose a framework that combines recent advances in knowledge distillation (teacher-student framework), relational reasoning and probabilistic logical languages to incorporate such knowledge in existing neural networks for the task of Visual Question Answering. Specifically, for a question posed against an image, we use a probabilistic logical language to encode the spatial knowledge and the spatial understanding about the question in the form of a mask that is directly provided to the teacher network. The student network learns from the ground-truth information as well as the teachers prediction via distillation. We also demonstrate the impact of predicting such a mask inside the teachers network using attention. Empirically, we show that both the methods improve the test accuracy over a state-of-the-art approach on a publicly available dataset.



### Few-Shot Learning via Embedding Adaptation with Set-to-Set Functions
- **Arxiv ID**: http://arxiv.org/abs/1812.03664v6
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1812.03664v6)
- **Published**: 2018-12-10 07:55:56+00:00
- **Updated**: 2021-06-13 06:16:30+00:00
- **Authors**: Han-Jia Ye, Hexiang Hu, De-Chuan Zhan, Fei Sha
- **Comment**: Accepted by CVPR 2020; The code is available at
  https://github.com/Sha-Lab/FEAT
- **Journal**: None
- **Summary**: Learning with limited data is a key challenge for visual recognition. Many few-shot learning methods address this challenge by learning an instance embedding function from seen classes and apply the function to instances from unseen classes with limited labels. This style of transfer learning is task-agnostic: the embedding function is not learned optimally discriminative with respect to the unseen classes, where discerning among them leads to the target task. In this paper, we propose a novel approach to adapt the instance embeddings to the target classification task with a set-to-set function, yielding embeddings that are task-specific and are discriminative. We empirically investigated various instantiations of such set-to-set functions and observed the Transformer is most effective -- as it naturally satisfies key properties of our desired model. We denote this model as FEAT (few-shot embedding adaptation w/ Transformer) and validate it on both the standard few-shot classification benchmark and four extended few-shot learning settings with essential use cases, i.e., cross-domain, transductive, generalized few-shot learning, and low-shot learning. It archived consistent improvements over baseline models as well as previous methods and established the new state-of-the-art results on two benchmarks.



### Neural Probabilistic System for Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/1812.03680v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.03680v6)
- **Published**: 2018-12-10 09:12:01+00:00
- **Updated**: 2019-07-19 06:57:58+00:00
- **Authors**: Najoua Rahal, Maroua Tounsi, Adel M. Alimi
- **Comment**: None
- **Journal**: None
- **Summary**: Unconstrained text recognition is a stimulating field in the branch of pattern recognition. This field is still an open search due to the unlimited vocabulary, multi styles, mixed-font and their great morphological variability. Recent trends show a potential improvement of recognition by adoption a novel representation of extracted features. In the present paper, we propose a novel feature extraction model by learning a Bag of Features Framework for text recognition based on Sparse Auto-Encoder. The Hidden Markov Models are then used for sequences modeling. For features learned quality evaluation, our proposed system was tested on two printed text datasets PKHATT text line images and APTI word images benchmark. Our method achieves promising recognition on both datasets.



### SMIT: Stochastic Multi-Label Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/1812.03704v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.03704v3)
- **Published**: 2018-12-10 10:00:24+00:00
- **Updated**: 2019-09-05 10:18:24+00:00
- **Authors**: Andrés Romero, Pablo Arbeláez, Luc Van Gool, Radu Timofte
- **Comment**: ICCV Workshops, 2019
- **Journal**: None
- **Summary**: Cross-domain mapping has been a very active topic in recent years. Given one image, its main purpose is to translate it to the desired target domain, or multiple domains in the case of multiple labels. This problem is highly challenging due to three main reasons: (i) unpaired datasets, (ii) multiple attributes, and (iii) the multimodality (e.g., style) associated with the translation. Most of the existing state-of-the-art has focused only on two reasons, i.e. either on (i) and (ii), or (i) and (iii). In this work, we propose a joint framework (i, ii, iii) of diversity and multi-mapping image-to-image translations, using a single generator to conditionally produce countless and unique fake images that hold the underlying characteristics of the source image. Our system does not use style regularization, instead, it uses an embedding representation that we call domain embedding for both domain and style. Extensive experiments over different datasets demonstrate the effectiveness of our proposed approach in comparison with the state-of-the-art in both multi-label and multimodal problems. Additionally, our method is able to generalize under different scenarios: continuous style interpolation, continuous label interpolation, and fine-grained mapping. Code and pretrained models are available at https://github.com/BCV-Uniandes/SMIT.



### Defending Against Universal Perturbations With Shared Adversarial Training
- **Arxiv ID**: http://arxiv.org/abs/1812.03705v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.03705v2)
- **Published**: 2018-12-10 10:02:45+00:00
- **Updated**: 2019-08-13 11:58:27+00:00
- **Authors**: Chaithanya Kumar Mummadi, Thomas Brox, Jan Hendrik Metzen
- **Comment**: ICCV 2019, 8 main pages, 9 appendix pages, 16 figures, 2 tables
- **Journal**: None
- **Summary**: Classifiers such as deep neural networks have been shown to be vulnerable against adversarial perturbations on problems with high-dimensional input space. While adversarial training improves the robustness of image classifiers against such adversarial perturbations, it leaves them sensitive to perturbations on a non-negligible fraction of the inputs. In this work, we show that adversarial training is more effective in preventing universal perturbations, where the same perturbation needs to fool a classifier on many inputs. Moreover, we investigate the trade-off between robustness against universal perturbations and performance on unperturbed data and propose an extension of adversarial training that handles this trade-off more gracefully. We present results for image classification and semantic segmentation to showcase that universal perturbations that fool a model hardened with adversarial training become clearly perceptible and show patterns of the target scene.



### Improving Nighttime Retrieval-Based Localization
- **Arxiv ID**: http://arxiv.org/abs/1812.03707v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.03707v3)
- **Published**: 2018-12-10 10:05:40+00:00
- **Updated**: 2019-04-05 08:50:02+00:00
- **Authors**: Hugo Germain, Guillaume Bourmaud, Vincent Lepetit
- **Comment**: None
- **Journal**: None
- **Summary**: Outdoor visual localization is a crucial component to many computer vision systems. We propose an approach to localization from images that is designed to explicitly handle the strong variations in appearance happening between daytime and nighttime. As revealed by recent long-term localization benchmarks, both traditional feature-based and retrieval-based approaches still struggle to handle such changes. Our novel localization method combines a state-of-the-art image retrieval architecture with condition-specific sub-networks allowing the computation of global image descriptors that are explicitly dependent of the capturing conditions. We show that our approach improves localization by a factor of almost 300\% compared to the popular VLAD-based methods on nighttime localization.



### Can we learn where people go?
- **Arxiv ID**: http://arxiv.org/abs/1812.03719v2
- **DOI**: 10.17815/CD.2020.43
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.03719v2)
- **Published**: 2018-12-10 10:24:51+00:00
- **Updated**: 2018-12-20 10:13:58+00:00
- **Authors**: Marion Gödel, Gerta Köster, Daniel Lehmberg, Manfred Gruber, Angelika Kneidl, Florian Sesser
- **Comment**: Proceedings of the 9th International Conference on Pedestrian and
  Evacuation Dynamics (PED2018) in Lund, Sweden, August 21-23, 2018 Paper No.
  50, 8 pages, 5 figures
- **Journal**: None
- **Summary**: In most agent-based simulators, pedestrians navigate from origins to destinations. Consequently, destinations are essential input parameters to the simulation. While many other relevant parameters as positions, speeds and densities can be obtained from sensors, like cameras, destinations cannot be observed directly. Our research question is: Can we obtain this information from video data using machine learning methods? We use density heatmaps, which indicate the pedestrian density within a given camera cutout, as input to predict the destination distributions. For our proof of concept, we train a Random Forest predictor on an exemplary data set generated with the Vadere microscopic simulator. The scenario is a crossroad where pedestrians can head left, straight or right. In addition, we gain first insights on suitable placement of the camera. The results motivate an in-depth analysis of the methodology.



### Style Transfer and Extraction for the Handwritten Letters Using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1812.07103v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.07103v1)
- **Published**: 2018-12-10 13:38:46+00:00
- **Updated**: 2018-12-10 13:38:46+00:00
- **Authors**: Omar Mohammed, Gerard Bailly, Damien Pellier
- **Comment**: Accepted in ICAART 2019
- **Journal**: None
- **Summary**: How can we learn, transfer and extract handwriting styles using deep neural networks? This paper explores these questions using a deep conditioned autoencoder on the IRON-OFF handwriting data-set. We perform three experiments that systematically explore the quality of our style extraction procedure. First, We compare our model to handwriting benchmarks using multidimensional performance metrics. Second, we explore the quality of style transfer, i.e. how the model performs on new, unseen writers. In both experiments, we improve the metrics of state of the art methods by a large margin. Lastly, we analyze the latent space of our model, and we see that it separates consistently writing styles.



### Unsupervised Deep Learning for Structured Shape Matching
- **Arxiv ID**: http://arxiv.org/abs/1812.03794v3
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1812.03794v3)
- **Published**: 2018-12-10 13:50:34+00:00
- **Updated**: 2019-08-22 10:53:58+00:00
- **Authors**: Jean-Michel Roufosse, Abhishek Sharma, Maks Ovsjanikov
- **Comment**: Oral Presentation at ICCV'19
- **Journal**: None
- **Summary**: We present a novel method for computing correspondences across 3D shapes using unsupervised learning. Our method computes a non-linear transformation of given descriptor functions, while optimizing for global structural properties of the resulting maps, such as their bijectivity or approximate isometry. To this end, we use the functional maps framework, and build upon the recent FMNet architecture for descriptor learning. Unlike that approach, however, we show that learning can be done in a purely \emph{unsupervised setting}, without having access to any ground truth correspondences. This results in a very general shape matching method that we call SURFMNet for Spectral Unsupervised FMNet, and which can be used to establish correspondences within 3D shape collections without any prior information. We demonstrate on a wide range of challenging benchmarks, that our approach leads to state-of-the-art results compared to the existing unsupervised methods and achieves results that are comparable even to the supervised learning techniques. Moreover, our framework is an order of magnitude faster, and does not rely on geodesic distance computation or expensive post-processing.



### Mapping, Localization and Path Planning for Image-based Navigation using Visual Features and Map
- **Arxiv ID**: http://arxiv.org/abs/1812.03795v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.03795v2)
- **Published**: 2018-12-10 13:52:58+00:00
- **Updated**: 2019-07-11 12:38:53+00:00
- **Authors**: Janine Thoma, Danda Pani Paudel, Ajad Chhatkuli, Thomas Probst, Luc Van Gool
- **Comment**: CVPR 2019, for implementation see https://github.com/janinethoma
- **Journal**: None
- **Summary**: Building on progress in feature representations for image retrieval, image-based localization has seen a surge of research interest. Image-based localization has the advantage of being inexpensive and efficient, often avoiding the use of 3D metric maps altogether. That said, the need to maintain a large number of reference images as an effective support of localization in a scene, nonetheless calls for them to be organized in a map structure of some kind.   The problem of localization often arises as part of a navigation process. We are, therefore, interested in summarizing the reference images as a set of landmarks, which meet the requirements for image-based navigation. A contribution of this paper is to formulate such a set of requirements for the two sub-tasks involved: map construction and self-localization. These requirements are then exploited for compact map representation and accurate self-localization, using the framework of a network flow problem. During this process, we formulate the map construction and self-localization problems as convex quadratic and second-order cone programs, respectively. We evaluate our methods on publicly available indoor and outdoor datasets, where they outperform existing methods significantly.



### Learning to Drive from Simulation without Real World Labels
- **Arxiv ID**: http://arxiv.org/abs/1812.03823v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.03823v2)
- **Published**: 2018-12-10 14:31:58+00:00
- **Updated**: 2018-12-13 17:33:07+00:00
- **Authors**: Alex Bewley, Jessica Rigley, Yuxuan Liu, Jeffrey Hawke, Richard Shen, Vinh-Dieu Lam, Alex Kendall
- **Comment**: None
- **Journal**: None
- **Summary**: Simulation can be a powerful tool for understanding machine learning systems and designing methods to solve real-world problems. Training and evaluating methods purely in simulation is often "doomed to succeed" at the desired task in a simulated environment, but the resulting models are incapable of operation in the real world. Here we present and evaluate a method for transferring a vision-based lane following driving policy from simulation to operation on a rural road without any real-world labels. Our approach leverages recent advances in image-to-image translation to achieve domain transfer while jointly learning a single-camera control policy from simulation control labels. We assess the driving performance of this method using both open-loop regression metrics, and closed-loop performance operating an autonomous vehicle on rural and urban roads.



### Occupancy Networks: Learning 3D Reconstruction in Function Space
- **Arxiv ID**: http://arxiv.org/abs/1812.03828v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.03828v2)
- **Published**: 2018-12-10 14:36:52+00:00
- **Updated**: 2019-04-30 14:43:13+00:00
- **Authors**: Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, Andreas Geiger
- **Comment**: To be presented at CVPR 2019. Supplementary material and code is
  available at http://avg.is.tuebingen.mpg.de/publications/occupancy-networks
- **Journal**: None
- **Summary**: With the advent of deep neural networks, learning-based approaches for 3D reconstruction have gained popularity. However, unlike for images, in 3D there is no canonical representation which is both computationally and memory efficient yet allows for representing high-resolution geometry of arbitrary topology. Many of the state-of-the-art learning-based 3D reconstruction approaches can hence only represent very coarse 3D geometry or are limited to a restricted domain. In this paper, we propose Occupancy Networks, a new representation for learning-based 3D reconstruction methods. Occupancy networks implicitly represent the 3D surface as the continuous decision boundary of a deep neural network classifier. In contrast to existing approaches, our representation encodes a description of the 3D output at infinite resolution without excessive memory footprint. We validate that our representation can efficiently encode 3D structure and can be inferred from various kinds of input. Our experiments demonstrate competitive results, both qualitatively and quantitatively, for the challenging tasks of 3D reconstruction from single images, noisy point clouds and coarse discrete voxel grids. We believe that occupancy networks will become a useful tool in a wide variety of learning-based 3D tasks.



### Weakly Supervised Dense Event Captioning in Videos
- **Arxiv ID**: http://arxiv.org/abs/1812.03849v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.03849v1)
- **Published**: 2018-12-10 14:58:24+00:00
- **Updated**: 2018-12-10 14:58:24+00:00
- **Authors**: Xuguang Duan, Wenbing Huang, Chuang Gan, Jingdong Wang, Wenwu Zhu, Junzhou Huang
- **Comment**: NeurIPS 2018
- **Journal**: None
- **Summary**: Dense event captioning aims to detect and describe all events of interest contained in a video. Despite the advanced development in this area, existing methods tackle this task by making use of dense temporal annotations, which is dramatically source-consuming. This paper formulates a new problem: weakly supervised dense event captioning, which does not require temporal segment annotations for model training. Our solution is based on the one-to-one correspondence assumption, each caption describes one temporal segment, and each temporal segment has one caption, which holds in current benchmark datasets and most real-world cases. We decompose the problem into a pair of dual problems: event captioning and sentence localization and present a cycle system to train our model. Extensive experimental results are provided to demonstrate the ability of our model on both dense event captioning and sentence localization in videos.



### Facial Landmark Machines: A Backbone-Branches Architecture with Progressive Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/1812.03887v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.03887v1)
- **Published**: 2018-12-10 15:50:37+00:00
- **Updated**: 2018-12-10 15:50:37+00:00
- **Authors**: Lingbo Liu, Guanbin Li, Yuan Xie, Yizhou Yu, Qing Wang, Liang Lin
- **Comment**: None
- **Journal**: None
- **Summary**: Facial landmark localization plays a critical role in face recognition and analysis. In this paper, we propose a novel cascaded backbone-branches fully convolutional neural network~(BB-FCN) for rapidly and accurately localizing facial landmarks in unconstrained and cluttered settings. Our proposed BB-FCN generates facial landmark response maps directly from raw images without any preprocessing. BB-FCN follows a coarse-to-fine cascaded pipeline, which consists of a backbone network for roughly detecting the locations of all facial landmarks and one branch network for each type of detected landmark for further refining their locations. Furthermore, to facilitate the facial landmark localization under unconstrained settings, we propose a large-scale benchmark named SYSU16K, which contains 16000 faces with large variations in pose, expression, illumination and resolution. Extensive experimental evaluations demonstrate that our proposed BB-FCN can significantly outperform the state-of-the-art under both constrained (i.e., within detected facial regions only) and unconstrained settings. We further confirm that high-quality facial landmarks localized with our proposed network can also improve the precision and recall of face detection.



### Deep Learning with Mixed Supervision for Brain Tumor Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1812.04571v1
- **DOI**: 10.1117/1.JMI.6.3.034002
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.04571v1)
- **Published**: 2018-12-10 16:03:27+00:00
- **Updated**: 2018-12-10 16:03:27+00:00
- **Authors**: Pawel Mlynarski, Hervé Delingette, Antonio Criminisi, Nicholas Ayache
- **Comment**: Submitted to SPIE Journal of Medical Imaging
- **Journal**: None
- **Summary**: Most of the current state-of-the-art methods for tumor segmentation are based on machine learning models trained on manually segmented images. This type of training data is particularly costly, as manual delineation of tumors is not only time-consuming but also requires medical expertise. On the other hand, images with a provided global label (indicating presence or absence of a tumor) are less informative but can be obtained at a substantially lower cost. In this paper, we propose to use both types of training data (fully-annotated and weakly-annotated) to train a deep learning model for segmentation. The idea of our approach is to extend segmentation networks with an additional branch performing image-level classification. The model is jointly trained for segmentation and classification tasks in order to exploit information contained in weakly-annotated images while preventing the network to learn features which are irrelevant for the segmentation task. We evaluate our method on the challenging task of brain tumor segmentation in Magnetic Resonance images from BRATS 2018 challenge. We show that the proposed approach provides a significant improvement of segmentation performance compared to the standard supervised learning. The observed improvement is proportional to the ratio between weakly-annotated and fully-annotated images available for training.



### Attention-guided Unified Network for Panoptic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1812.03904v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.03904v2)
- **Published**: 2018-12-10 16:25:10+00:00
- **Updated**: 2019-04-17 04:32:23+00:00
- **Authors**: Yanwei Li, Xinze Chen, Zheng Zhu, Lingxi Xie, Guan Huang, Dalong Du, Xingang Wang
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: This paper studies panoptic segmentation, a recently proposed task which segments foreground (FG) objects at the instance level as well as background (BG) contents at the semantic level. Existing methods mostly dealt with these two problems separately, but in this paper, we reveal the underlying relationship between them, in particular, FG objects provide complementary cues to assist BG understanding. Our approach, named the Attention-guided Unified Network (AUNet), is a unified framework with two branches for FG and BG segmentation simultaneously. Two sources of attentions are added to the BG branch, namely, RPN and FG segmentation mask to provide object-level and pixel-level attentions, respectively. Our approach is generalized to different backbones with consistent accuracy gain in both FG and BG segmentation, and also sets new state-of-the-arts both in the MS-COCO (46.5% PQ) and Cityscapes (59.0% PQ) benchmarks.



### Self-Contained Stylization via Steganography for Reverse and Serial Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/1812.03910v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.03910v3)
- **Published**: 2018-12-10 16:43:49+00:00
- **Updated**: 2020-01-09 16:21:59+00:00
- **Authors**: Hung-Yu Chen, I-Sheng Fang, Wei-Chen Chiu
- **Comment**: 21 pages, 21 figures
- **Journal**: None
- **Summary**: Style transfer has been widely applied to give real-world images a new artistic look. However, given a stylized image, the attempts to use typical style transfer methods for de-stylization or transferring it again into another style usually lead to artifacts or undesired results. We realize that these issues are originated from the content inconsistency between the original image and its stylized output. Therefore, in this paper we advance to keep the content information of the input image during the process of style transfer by the power of steganography, with two approaches proposed: a two-stage model and an end-to-end model. We conduct extensive experiments to successfully verify the capacity of our models, in which both of them are able to not only generate stylized images of quality comparable with the ones produced by typical style transfer methods, but also effectively eliminate the artifacts introduced in reconstructing original input from a stylized image as well as performing multiple times of style transfer in series.



### Learning Representations of Sets through Optimized Permutations
- **Arxiv ID**: http://arxiv.org/abs/1812.03928v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.03928v3)
- **Published**: 2018-12-10 17:26:25+00:00
- **Updated**: 2019-01-15 03:18:33+00:00
- **Authors**: Yan Zhang, Jonathon Hare, Adam Prügel-Bennett
- **Comment**: Published in ICLR 2019
- **Journal**: None
- **Summary**: Representations of sets are challenging to learn because operations on sets should be permutation-invariant. To this end, we propose a Permutation-Optimisation module that learns how to permute a set end-to-end. The permuted set can be further processed to learn a permutation-invariant representation of that set, avoiding a bottleneck in traditional set models. We demonstrate our model's ability to learn permutations and set representations with either explicit or implicit supervision on four datasets, on which we achieve state-of-the-art results: number sorting, image mosaics, classification from image mosaics, and visual question answering.



### Data Fine-tuning
- **Arxiv ID**: http://arxiv.org/abs/1812.03944v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.03944v1)
- **Published**: 2018-12-10 17:57:59+00:00
- **Updated**: 2018-12-10 17:57:59+00:00
- **Authors**: Saheb Chhabra, Puspita Majumdar, Mayank Vatsa, Richa Singh
- **Comment**: Accepted in AAAI 2019
- **Journal**: None
- **Summary**: In real-world applications, commercial off-the-shelf systems are utilized for performing automated facial analysis including face recognition, emotion recognition, and attribute prediction. However, a majority of these commercial systems act as black boxes due to the inaccessibility of the model parameters which makes it challenging to fine-tune the models for specific applications. Stimulated by the advances in adversarial perturbations, this research proposes the concept of Data Fine-tuning to improve the classification accuracy of a given model without changing the parameters of the model. This is accomplished by modeling it as data (image) perturbation problem. A small amount of "noise" is added to the input with the objective of minimizing the classification loss without affecting the (visual) appearance. Experiments performed on three publicly available datasets LFW, CelebA, and MUCT, demonstrate the effectiveness of the proposed concept.



### A New Ensemble Learning Framework for 3D Biomedical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1812.03945v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.03945v1)
- **Published**: 2018-12-10 17:58:00+00:00
- **Updated**: 2018-12-10 17:58:00+00:00
- **Authors**: Hao Zheng, Yizhe Zhang, Lin Yang, Peixian Liang, Zhuo Zhao, Chaoli Wang, Danny Z. Chen
- **Comment**: To appear in AAAI-2019. The first three authors contributed equally
  to the paper
- **Journal**: None
- **Summary**: 3D image segmentation plays an important role in biomedical image analysis. Many 2D and 3D deep learning models have achieved state-of-the-art segmentation performance on 3D biomedical image datasets. Yet, 2D and 3D models have their own strengths and weaknesses, and by unifying them together, one may be able to achieve more accurate results. In this paper, we propose a new ensemble learning framework for 3D biomedical image segmentation that combines the merits of 2D and 3D models. First, we develop a fully convolutional network based meta-learner to learn how to improve the results from 2D and 3D models (base-learners). Then, to minimize over-fitting for our sophisticated meta-learner, we devise a new training method that uses the results of the base-learners as multiple versions of "ground truths". Furthermore, since our new meta-learner training scheme does not depend on manual annotation, it can utilize abundant unlabeled 3D image data to further improve the model. Extensive experiments on two public datasets (the HVSMR 2016 Challenge dataset and the mouse piriform cortex dataset) show that our approach is effective under fully-supervised, semi-supervised, and transductive settings, and attains superior performance over state-of-the-art image segmentation methods.



### An Intelligent Safety System for Human-Centered Semi-Autonomous Vehicles
- **Arxiv ID**: http://arxiv.org/abs/1812.03953v2
- **DOI**: 10.1007/978-3-030-37309-2_26
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1812.03953v2)
- **Published**: 2018-12-10 18:08:18+00:00
- **Updated**: 2019-02-20 22:57:02+00:00
- **Authors**: Hadi Abdi Khojasteh, Alireza Abbas Alipour, Ebrahim Ansari, Parvin Razzaghi
- **Comment**: 15 pages and 5 figures, Submitted to the international conference on
  Contemporary issues in Data Science (CiDaS 2019), Learn more about this
  project at https://iasbs.ac.ir/~ansari/faraz
- **Journal**: Nature Switzerland AG - Springer LNDECT 45(2020) 322-336
- **Summary**: Nowadays, automobile manufacturers make efforts to develop ways to make cars fully safe. Monitoring driver's actions by computer vision techniques to detect driving mistakes in real-time and then planning for autonomous driving to avoid vehicle collisions is one of the most important issues that has been investigated in the machine vision and Intelligent Transportation Systems (ITS). The main goal of this study is to prevent accidents caused by fatigue, drowsiness, and driver distraction. To avoid these incidents, this paper proposes an integrated safety system that continuously monitors the driver's attention and vehicle surroundings, and finally decides whether the actual steering control status is safe or not. For this purpose, we equipped an ordinary car called FARAZ with a vision system consisting of four mounted cameras along with a universal car tool for communicating with surrounding factory-installed sensors and other car systems, and sending commands to actuators. The proposed system leverages a scene understanding pipeline using deep convolutional encoder-decoder networks and a driver state detection pipeline. We have been identifying and assessing domestic capabilities for the development of technologies specifically of the ordinary vehicles in order to manufacture smart cars and eke providing an intelligent system to increase safety and to assist the driver in various conditions/situations.



### Facial Expression Recognition using Facial Landmark Detection and Feature Extraction via Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1812.04510v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.04510v3)
- **Published**: 2018-12-10 18:34:19+00:00
- **Updated**: 2020-07-15 21:22:22+00:00
- **Authors**: Fuzail Khan
- **Comment**: None
- **Journal**: None
- **Summary**: The proposed framework in this paper has the primary objective of classifying the facial expression shown by a person. These classifiable expressions can be any one of the six universal emotions along with the neutral emotion. After the initial facial localization is performed, facial landmark detection and feature extraction are applied where in the landmarks are determined to be the fiducial features: the eyebrows, eyes, nose and lips. This is primarily done using state-of-the-art facial landmark detection algorithms as well as traditional edge and corner point detection methods using Sobel filters and Shi Tomasi corner point detection methods respectively. This leads to generation of input feature vectors being formulated using Euclidean distances and trained into a Multi-Layer Perceptron (MLP) neural network in order to classify the expression being displayed. The results achieved have further dealt with higher uniformity in certain emotions and the inherently subjective nature of expression.



### Accuracy, Uncertainty, and Adaptability of Automatic Myocardial ASL Segmentation using Deep CNN
- **Arxiv ID**: http://arxiv.org/abs/1812.03974v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.03974v4)
- **Published**: 2018-12-10 18:49:32+00:00
- **Updated**: 2019-11-05 04:42:31+00:00
- **Authors**: Hung P. Do, Yi Guo, Andrew J. Yoon, Krishna S. Nayak
- **Comment**: None
- **Journal**: None
- **Summary**: PURPOSE: To apply deep CNN to the segmentation task in myocardial arterial spin labeled (ASL) perfusion imaging and to develop methods that measure uncertainty and that adapt the CNN model to a specific false positive vs. false negative tradeoff.   METHODS: The Monte Carlo dropout (MCD) U-Net was trained on data from 22 subjects and tested on data from 6 heart transplant recipients. Manual segmentation and regional myocardial blood flow (MBF) were available for comparison. We consider two global uncertainty measures, named Dice Uncertainty and MCD Uncertainty, which were calculated with and without the use of manual segmentation, respectively. Tversky loss function with a hyperparameter $\beta$ was used to adapt the model to a specific false positive vs. false negative tradeoff.   RESULTS: The MCD U-Net achieved Dice coefficient of mean(std) = 0.91(0.04) on the test set. MBF measured using automatic segmentations was highly correlated to that measured using the manual segmentation ($R^2$ = 0.96). Dice Uncertainty and MCD Uncertainty were in good agreement ($R^2$ = 0.64). As $\beta$ increased, the false positive rate systematically decreased and false negative rate systematically increased.   CONCLUSION: We demonstrate the feasibility of deep CNN for automatic segmentation of myocardial ASL, with good accuracy. We also introduce two simple methods for assessing model uncertainty. Finally, we demonstrate the ability to adapt the CNN model to a specific false positive vs. false negative tradeoff. These findings are directly relevant to automatic segmentation in quantitative cardiac MRI and are broadly applicable to automatic segmentation problems in diagnostic imaging.



### SlowFast Networks for Video Recognition
- **Arxiv ID**: http://arxiv.org/abs/1812.03982v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.03982v3)
- **Published**: 2018-12-10 18:59:07+00:00
- **Updated**: 2019-10-29 06:26:37+00:00
- **Authors**: Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, Kaiming He
- **Comment**: Technical report
- **Journal**: None
- **Summary**: We present SlowFast networks for video recognition. Our model involves (i) a Slow pathway, operating at low frame rate, to capture spatial semantics, and (ii) a Fast pathway, operating at high frame rate, to capture motion at fine temporal resolution. The Fast pathway can be made very lightweight by reducing its channel capacity, yet can learn useful temporal information for video recognition. Our models achieve strong performance for both action classification and detection in video, and large improvements are pin-pointed as contributions by our SlowFast concept. We report state-of-the-art accuracy on major video recognition benchmarks, Kinetics, Charades and AVA. Code has been made available at: https://github.com/facebookresearch/SlowFast



### Supervised Deep Kriging for Single-Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1812.04042v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.04042v1)
- **Published**: 2018-12-10 19:32:23+00:00
- **Updated**: 2018-12-10 19:32:23+00:00
- **Authors**: Gianni Franchi, Angela Yao, Andreas Kolb
- **Comment**: 3 figures, for a better quality read the hal or GCPR version
- **Journal**: None
- **Summary**: We propose a novel single-image super-resolution approach based on the geostatistical method of kriging. Kriging is a zero-bias minimum-variance estimator that performs spatial interpolation based on a weighted average of known observations. Rather than solving for the kriging weights via the traditional method of inverting covariance matrices, we propose a supervised form in which we learn a deep network to generate said weights. We combine the kriging weight generation and kriging process into a joint network that can be learned end-to-end. Our network achieves competitive super-resolution results as other state-of-the-art methods. In addition, since the super-resolution process follows a known statistical framework, we are able to estimate bias and variance, something which is rarely possible for other deep networks.



### Accelerating Convolutional Neural Networks via Activation Map Compression
- **Arxiv ID**: http://arxiv.org/abs/1812.04056v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.04056v2)
- **Published**: 2018-12-10 19:50:44+00:00
- **Updated**: 2019-03-27 17:42:08+00:00
- **Authors**: Georgios Georgiadis
- **Comment**: None
- **Journal**: None
- **Summary**: The deep learning revolution brought us an extensive array of neural network architectures that achieve state-of-the-art performance in a wide variety of Computer Vision tasks including among others, classification, detection and segmentation. In parallel, we have also been observing an unprecedented demand in computational and memory requirements, rendering the efficient use of neural networks in low-powered devices virtually unattainable. Towards this end, we propose a three-stage compression and acceleration pipeline that sparsifies, quantizes and entropy encodes activation maps of Convolutional Neural Networks. Sparsification increases the representational power of activation maps leading to both acceleration of inference and higher model accuracy. Inception-V3 and MobileNet-V1 can be accelerated by as much as $1.6\times$ with an increase in accuracy of $0.38\%$ and $0.54\%$ on the ImageNet and CIFAR-10 datasets respectively. Quantizing and entropy coding the sparser activation maps lead to higher compression over the baseline, reducing the memory cost of the network execution. Inception-V3 and MobileNet-V1 activation maps, quantized to $16$ bits, are compressed by as much as $6\times$ with an increase in accuracy of $0.36\%$ and $0.55\%$ respectively.



### An Automatic System for Unconstrained Video-Based Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1812.04058v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.04058v3)
- **Published**: 2018-12-10 19:51:38+00:00
- **Updated**: 2019-08-09 23:45:46+00:00
- **Authors**: Jingxiao Zheng, Rajeev Ranjan, Ching-Hui Chen, Jun-Cheng Chen, Carlos D. Castillo, Rama Chellappa
- **Comment**: None
- **Journal**: None
- **Summary**: Although deep learning approaches have achieved performance surpassing humans for still image-based face recognition, unconstrained video-based face recognition is still a challenging task due to large volume of data to be processed and intra/inter-video variations on pose, illumination, occlusion, scene, blur, video quality, etc. In this work, we consider challenging scenarios for unconstrained video-based face recognition from multiple-shot videos and surveillance videos with low-quality frames. To handle these problems, we propose a robust and efficient system for unconstrained video-based face recognition, which is composed of modules for face/fiducial detection, face association, and face recognition. First, we use multi-scale single-shot face detectors to efficiently localize faces in videos. The detected faces are then grouped respectively through carefully designed face association methods, especially for multi-shot videos. Finally, the faces are recognized by the proposed face matcher based on an unsupervised subspace learning approach and a subspace-to-subspace similarity metric. Extensive experiments on challenging video datasets, such as Multiple Biometric Grand Challenge (MBGC), Face and Ocular Challenge Series (FOCS), IARPA Janus Surveillance Video Benchmark (IJB-S) for low-quality surveillance videos and IARPA JANUS Benchmark B (IJB-B) for multiple-shot videos, demonstrate that the proposed system can accurately detect and associate faces from unconstrained videos and effectively learn robust and discriminative features for recognition.



### PlaneRCNN: 3D Plane Detection and Reconstruction from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/1812.04072v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.04072v2)
- **Published**: 2018-12-10 20:35:55+00:00
- **Updated**: 2019-01-08 00:24:15+00:00
- **Authors**: Chen Liu, Kihwan Kim, Jinwei Gu, Yasutaka Furukawa, Jan Kautz
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a deep neural architecture, PlaneRCNN, that detects and reconstructs piecewise planar surfaces from a single RGB image. PlaneRCNN employs a variant of Mask R-CNN to detect planes with their plane parameters and segmentation masks. PlaneRCNN then jointly refines all the segmentation masks with a novel loss enforcing the consistency with a nearby view during training. The paper also presents a new benchmark with more fine-grained plane segmentations in the ground-truth, in which, PlaneRCNN outperforms existing state-of-the-art methods with significant margins in the plane detection, segmentation, and reconstruction metrics. PlaneRCNN makes an important step towards robust plane extraction, which would have an immediate impact on a wide range of applications including Robotics, Augmented Reality, and Virtual Reality.



### Visual Depth Mapping from Monocular Images using Recurrent Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1812.04082v1
- **DOI**: 10.2514/6.2019-1189
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1812.04082v1)
- **Published**: 2018-12-10 20:53:49+00:00
- **Updated**: 2018-12-10 20:53:49+00:00
- **Authors**: John Mern, Kyle Julian, Rachael E. Tompa, Mykel J. Kochenderfer
- **Comment**: None
- **Journal**: None
- **Summary**: A reliable sense-and-avoid system is critical to enabling safe autonomous operation of unmanned aircraft. Existing sense-and-avoid methods often require specialized sensors that are too large or power intensive for use on small unmanned vehicles. This paper presents a method to estimate object distances based on visual image sequences, allowing for the use of low-cost, on-board monocular cameras as simple collision avoidance sensors. We present a deep recurrent convolutional neural network and training method to generate depth maps from video sequences. Our network is trained using simulated camera and depth data generated with Microsoft's AirSim simulator. Empirically, we show that our model achieves superior performance compared to models generated using prior methods.We further demonstrate that the method can be used for sense-and-avoid of obstacles in simulation.



### The Effects of Super-Resolution on Object Detection Performance in Satellite Imagery
- **Arxiv ID**: http://arxiv.org/abs/1812.04098v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.04098v3)
- **Published**: 2018-12-10 21:19:28+00:00
- **Updated**: 2019-04-09 16:58:39+00:00
- **Authors**: Jacob Shermeyer, Adam Van Etten
- **Comment**: To appear in EarthVision 2019,IEEE
- **Journal**: None
- **Summary**: We explore the application of super-resolution techniques to satellite imagery, and the effects of these techniques on object detection algorithm performance. Specifically, we enhance satellite imagery beyond its native resolution, and test if we can identify various types of vehicles, planes, and boats with greater accuracy than native resolution. Using the Very Deep Super-Resolution (VDSR) framework and a custom Random Forest Super-Resolution (RFSR) framework we generate enhancement levels of 2x, 4x, and 8x over five distinct resolutions ranging from 30 cm to 4.8 meters. Using both native and super-resolved data, we then train several custom detection models using the SIMRDWN object detection framework. SIMRDWN combines a number of popular object detection algorithms (e.g. SSD, YOLO) into a unified framework that is designed to rapidly detect objects in large satellite images. This approach allows us to quantify the effects of super-resolution techniques on object detection performance across multiple classes and resolutions. We also quantify the performance of object detection as a function of native resolution and object pixel size. For our test set we note that performance degrades from mean average precision (mAP) = 0.53 at 30 cm resolution, down to mAP = 0.11 at 4.8 m resolution. Super-resolving native 30 cm imagery to 15 cm yields the greatest benefit; a 13-36% improvement in mAP. Super-resolution is less beneficial at coarser resolutions, though still provides a small improvement in performance.



### Non-local U-Net for Biomedical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1812.04103v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.AP, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.04103v2)
- **Published**: 2018-12-10 21:28:55+00:00
- **Updated**: 2020-02-18 21:00:45+00:00
- **Authors**: Zhengyang Wang, Na Zou, Dinggang Shen, Shuiwang Ji
- **Comment**: In Proceedings of the 34th AAAI Conference on Artificial Intelligence
  (AAAI), 2019
- **Journal**: None
- **Summary**: Deep learning has shown its great promise in various biomedical image segmentation tasks. Existing models are typically based on U-Net and rely on an encoder-decoder architecture with stacked local operators to aggregate long-range information gradually. However, only using the local operators limits the efficiency and effectiveness. In this work, we propose the non-local U-Nets, which are equipped with flexible global aggregation blocks, for biomedical image segmentation. These blocks can be inserted into U-Net as size-preserving processes, as well as down-sampling and up-sampling layers. We perform thorough experiments on the 3D multimodality isointense infant brain MR image segmentation task to evaluate the non-local U-Nets. Results show that our proposed models achieve top performances with fewer parameters and faster computation.



### Vision-based Navigation with Language-based Assistance via Imitation Learning with Indirect Intervention
- **Arxiv ID**: http://arxiv.org/abs/1812.04155v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.04155v4)
- **Published**: 2018-12-10 23:48:25+00:00
- **Updated**: 2019-04-06 02:02:42+00:00
- **Authors**: Khanh Nguyen, Debadeepta Dey, Chris Brockett, Bill Dolan
- **Comment**: In CVPR 2019, 16 pages, appendix included
- **Journal**: None
- **Summary**: We present Vision-based Navigation with Language-based Assistance (VNLA), a grounded vision-language task where an agent with visual perception is guided via language to find objects in photorealistic indoor environments. The task emulates a real-world scenario in that (a) the requester may not know how to navigate to the target objects and thus makes requests by only specifying high-level end-goals, and (b) the agent is capable of sensing when it is lost and querying an advisor, who is more qualified at the task, to obtain language subgoals to make progress. To model language-based assistance, we develop a general framework termed Imitation Learning with Indirect Intervention (I3L), and propose a solution that is effective on the VNLA task. Empirical results show that this approach significantly improves the success rate of the learning agent over other baselines in both seen and unseen environments. Our code and data are publicly available at https://github.com/debadeepta/vnla .



