# Arxiv Papers in cs.CV on 2018-12-01
### From Third Person to First Person: Dataset and Baselines for Synthesis and Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1812.00104v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.00104v1)
- **Published**: 2018-12-01 00:10:03+00:00
- **Updated**: 2018-12-01 00:10:03+00:00
- **Authors**: Mohamed Elfeki, Krishna Regmi, Shervin Ardeshir, Ali Borji
- **Comment**: None
- **Journal**: None
- **Summary**: First-person (egocentric) and third person (exocentric) videos are drastically different in nature. The relationship between these two views have been studied in recent years, however, it has yet to be fully explored. In this work, we introduce two datasets (synthetic and natural/real) containing simultaneously recorded egocentric and exocentric videos. We also explore relating the two domains (egocentric and exocentric) in two aspects. First, we synthesize images in the egocentric domain from the exocentric domain using a conditional generative adversarial network (cGAN). We show that with enough training data, our network is capable of hallucinating how the world would look like from an egocentric perspective, given an exocentric video. Second, we address the cross-view retrieval problem across the two views. Given an egocentric query frame (or its momentary optical flow), we retrieve its corresponding exocentric frame (or optical flow) from a gallery set. We show that using synthetic data could be beneficial in retrieving real data. We show that performing domain adaptation from the synthetic domain to the natural/real domain, is helpful in tasks such as retrieval. We believe that the presented datasets and the proposed baselines offer new opportunities for further research in this direction. The code and dataset are publicly available.



### Multi-Stream Dynamic Video Summarization
- **Arxiv ID**: http://arxiv.org/abs/1812.00108v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.00108v4)
- **Published**: 2018-12-01 00:44:12+00:00
- **Updated**: 2021-10-14 21:52:40+00:00
- **Authors**: Mohamed Elfeki, Liqiang Wang, Ali Borji
- **Comment**: None
- **Journal**: None
- **Summary**: With vast amounts of video content being uploaded to the Internet every minute, video summarization becomes critical for efficient browsing, searching, and indexing of visual content. Nonetheless, the spread of social and egocentric cameras creates an abundance of sparse scenarios captured by several devices, and ultimately required to be jointly summarized. In this paper, we discuss the problem of summarizing videos recorded independently by several dynamic cameras that intermittently share the field of view. We present a robust framework that (a) identifies a diverse set of important events among moving cameras that often are not capturing the same scene, and (b) selects the most representative view(s) at each event to be included in a universal summary. Due to the lack of an applicable alternative, we collected a new multi-view egocentric dataset, Multi-Ego. Our dataset is recorded simultaneously by three cameras, covering a wide variety of real-life scenarios. The footage is annotated by multiple individuals under various summarization configurations, with a consensus analysis ensuring a reliable ground truth. We conduct extensive experiments on the compiled dataset in addition to three other standard benchmarks that show the robustness and the advantage of our approach in both supervised and unsupervised settings. Additionally, we show that our approach learns collectively from data of varied number-of-views and orthogonal to other summarization methods, deeming it scalable and generic.



### Assigning a Grade: Accurate Measurement of Road Quality Using Satellite Imagery
- **Arxiv ID**: http://arxiv.org/abs/1812.01699v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.01699v2)
- **Published**: 2018-12-01 01:43:26+00:00
- **Updated**: 2018-12-06 02:38:23+00:00
- **Authors**: Gabriel Cadamuro, Aggrey Muhebwa, Jay Taneja
- **Comment**: Presented at NIPS 2018 Workshop on Machine Learning for the
  Developing World
- **Journal**: None
- **Summary**: Roads are critically important infrastructure to societal and economic development, with huge investments made by governments every year. However, methods for monitoring those investments tend to be time-consuming, laborious, and expensive, placing them out of reach for many developing regions. In this work, we develop a model for monitoring the quality of road infrastructure using satellite imagery. For this task, we harness two trends: the increasing availability of high-resolution, often-updated satellite imagery, and the enormous improvement in speed and accuracy of convolutional neural network-based methods for performing computer vision tasks. We employ a unique dataset of road quality information on 7000km of roads in Kenya combined with 50cm resolution satellite imagery. We create models for a binary classification task as well as a comprehensive 5-category classification task, with accuracy scores of 88 and 73 percent respectively. We also provide evidence of the robustness of our methods with challenging held-out scenarios, though we note some improvement is still required for confident analysis of a never before seen road. We believe these results are well-positioned to have substantial impact on a broad set of transport applications.



### Snapshot Distillation: Teacher-Student Optimization in One Generation
- **Arxiv ID**: http://arxiv.org/abs/1812.00123v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.00123v1)
- **Published**: 2018-12-01 02:08:38+00:00
- **Updated**: 2018-12-01 02:08:38+00:00
- **Authors**: Chenglin Yang, Lingxi Xie, Chi Su, Alan L. Yuille
- **Comment**: None
- **Journal**: None
- **Summary**: Optimizing a deep neural network is a fundamental task in computer vision, yet direct training methods often suffer from over-fitting. Teacher-student optimization aims at providing complementary cues from a model trained previously, but these approaches are often considerably slow due to the pipeline of training a few generations in sequence, i.e., time complexity is increased by several times.   This paper presents snapshot distillation (SD), the first framework which enables teacher-student optimization in one generation. The idea of SD is very simple: instead of borrowing supervision signals from previous generations, we extract such information from earlier epochs in the same generation, meanwhile make sure that the difference between teacher and student is sufficiently large so as to prevent under-fitting. To achieve this goal, we implement SD in a cyclic learning rate policy, in which the last snapshot of each cycle is used as the teacher for all iterations in the next cycle, and the teacher signal is smoothed to provide richer information. In standard image classification benchmarks such as CIFAR100 and ILSVRC2012, SD achieves consistent accuracy gain without heavy computational overheads. We also verify that models pre-trained with SD transfers well to object detection and semantic segmentation in the PascalVOC dataset.



### NOTE-RCNN: NOise Tolerant Ensemble RCNN for Semi-Supervised Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1812.00124v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.00124v1)
- **Published**: 2018-12-01 02:14:57+00:00
- **Updated**: 2018-12-01 02:14:57+00:00
- **Authors**: JIyang Gao, Jiang Wang, Shengyang Dai, Li-Jia Li, Ram Nevatia
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: The labeling cost of large number of bounding boxes is one of the main challenges for training modern object detectors. To reduce the dependence on expensive bounding box annotations, we propose a new semi-supervised object detection formulation, in which a few seed box level annotations and a large scale of image level annotations are used to train the detector. We adopt a training-mining framework, which is widely used in weakly supervised object detection tasks. However, the mining process inherently introduces various kinds of labelling noises: false negatives, false positives and inaccurate boundaries, which can be harmful for training the standard object detectors (e.g. Faster RCNN). We propose a novel NOise Tolerant Ensemble RCNN (NOTE-RCNN) object detector to handle such noisy labels. Comparing to standard Faster RCNN, it contains three highlights: an ensemble of two classification heads and a distillation head to avoid overfitting on noisy labels and improve the mining precision, masking the negative sample loss in box predictor to avoid the harm of false negative labels, and training box regression head only on seed annotations to eliminate the harm from inaccurate boundaries of mined bounding boxes. We evaluate the methods on ILSVRC 2013 and MSCOCO 2017 dataset; we observe that the detection accuracy consistently improves as we iterate between mining and training steps, and state-of-the-art performance is achieved.



### Deep Inception Generative Network for Cognitive Image Inpainting
- **Arxiv ID**: http://arxiv.org/abs/1812.01458v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.01458v1)
- **Published**: 2018-12-01 03:20:47+00:00
- **Updated**: 2018-12-01 03:20:47+00:00
- **Authors**: Qingguo Xiao, Guangyao Li, Qiaochuan Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in deep learning have shown exciting promise in filling large holes and lead to another orientation for image inpainting. However, existing learning-based methods often create artifacts and fallacious textures because of insufficient cognition understanding. Previous generative networks are limited with single receptive type and give up pooling in consideration of detail sharpness. Human cognition is constant regardless of the target attribute. As multiple receptive fields improve the ability of abstract image characterization and pooling can keep feature invariant, specifically, deep inception learning is adopted to promote high-level feature representation and enhance model learning capacity for local patches. Moreover, approaches for generating diverse mask images are introduced and a random mask dataset is created. We benchmark our methods on ImageNet, Places2 dataset, and CelebA-HQ. Experiments for regular, irregular, and custom regions completion are all performed and free-style image inpainting is also presented. Quantitative comparisons with previous state-of-the-art methods show that ours obtain much more natural image completions.



### Automated segmentaiton and classification of arterioles and venules using Cascading Dilated Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1812.00137v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.00137v1)
- **Published**: 2018-12-01 04:05:14+00:00
- **Updated**: 2018-12-01 04:05:14+00:00
- **Authors**: Meng Li, Yan Zhang, Haicheng She, Jinqiong Zhou, Jia Jia, Danmei He, Li Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: The change of retinal vasculature is an early sign of many vascular and systematic diseases, such as diabetes and hypertension. Different behaviors of retinal arterioles and venules form an important metric to measure the disease severity. Therefore, an accurate classification of arterioles and venules is of great necessity. In this work, we propose a novel architecture of deep convolutional neural network for segmenting and classifying arterioles and venules on retinal fundus images. This network takes the original color fundus image as inputs and multi-class labels as outputs. We adopt the encoding-decoding structure (Unet) as the backbone network of our proposed model. To improve the classification accuracy, we develop a special encoding path that couples InceptionV4 modules and Cascading Dilated Convolutions (CDCs) on top of the backbone network. The model is thus able to extract and fuse high-level semantic features from multi-scale receptive fields. The proposed method has outperformed the previous state-of-the-art method on DRIVE dataset with an accuracy of 0.955 $\pm$ 0.002.



### Learning RoI Transformer for Detecting Oriented Objects in Aerial Images
- **Arxiv ID**: http://arxiv.org/abs/1812.00155v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.00155v1)
- **Published**: 2018-12-01 06:05:35+00:00
- **Updated**: 2018-12-01 06:05:35+00:00
- **Authors**: Jian Ding, Nan Xue, Yang Long, Gui-Song Xia, Qikai Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Object detection in aerial images is an active yet challenging task in computer vision because of the birdview perspective, the highly complex backgrounds, and the variant appearances of objects. Especially when detecting densely packed objects in aerial images, methods relying on horizontal proposals for common object detection often introduce mismatches between the Region of Interests (RoIs) and objects. This leads to the common misalignment between the final object classification confidence and localization accuracy. Although rotated anchors have been used to tackle this problem, the design of them always multiplies the number of anchors and dramatically increases the computational complexity. In this paper, we propose a RoI Transformer to address these problems. More precisely, to improve the quality of region proposals, we first designed a Rotated RoI (RRoI) learner to transform a Horizontal Region of Interest (HRoI) into a Rotated Region of Interest (RRoI). Based on the RRoIs, we then proposed a Rotated Position Sensitive RoI Align (RPS-RoI-Align) module to extract rotation-invariant features from them for boosting subsequent classification and regression. Our RoI Transformer is with light weight and can be easily embedded into detectors for oriented object detection. A simple implementation of the RoI Transformer has achieved state-of-the-art performances on two common and challenging aerial datasets, i.e., DOTA and HRSC2016, with a neglectable reduction to detection speed. Our RoI Transformer exceeds the deformable Position Sensitive RoI pooling when oriented bounding-box annotations are available. Extensive experiments have also validated the flexibility and effectiveness of our RoI Transformer. The results demonstrate that it can be easily integrated with other detector architectures and significantly improve the performances.



### Vision-Based Gait Analysis for Senior Care
- **Arxiv ID**: http://arxiv.org/abs/1812.00169v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.00169v1)
- **Published**: 2018-12-01 07:41:18+00:00
- **Updated**: 2018-12-01 07:41:18+00:00
- **Authors**: David Xue, Anin Sayana, Evan Darke, Kelly Shen, Jun-Ting Hsieh, Zelun Luo, Li-Jia Li, N. Lance Downing, Arnold Milstein, Li Fei-Fei
- **Comment**: Machine Learning for Health (ML4H) Workshop at NeurIPS 2018
  arXiv:1811.07216
- **Journal**: None
- **Summary**: As the senior population rapidly increases, it is challenging yet crucial to provide effective long-term care for seniors who live at home or in senior care facilities. Smart senior homes, which have gained widespread interest in the healthcare community, have been proposed to improve the well-being of seniors living independently. In particular, non-intrusive, cost-effective sensors placed in these senior homes enable gait characterization, which can provide clinically relevant information including mobility level and early neurodegenerative disease risk. In this paper, we present a method to perform gait analysis from a single camera placed within the home. We show that we can accurately calculate various gait parameters, demonstrating the potential for our system to monitor the long-term gait of seniors and thus aid clinicians in understanding a patient's medical profile.



### Racial Faces in-the-Wild: Reducing Racial Bias by Information Maximization Adaptation Network
- **Arxiv ID**: http://arxiv.org/abs/1812.00194v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.00194v2)
- **Published**: 2018-12-01 12:10:39+00:00
- **Updated**: 2019-07-27 09:43:36+00:00
- **Authors**: Mei Wang, Weihong Deng, Jiani Hu, Xunqiang Tao, Yaohai Huang
- **Comment**: None
- **Journal**: ICCV 2019: 692-702
- **Summary**: Racial bias is an important issue in biometric, but has not been thoroughly studied in deep face recognition. In this paper, we first contribute a dedicated dataset called Racial Faces in-the-Wild (RFW) database, on which we firmly validated the racial bias of four commercial APIs and four state-of-the-art (SOTA) algorithms. Then, we further present the solution using deep unsupervised domain adaptation and propose a deep information maximization adaptation network (IMAN) to alleviate this bias by using Caucasian as source domain and other races as target domains. This unsupervised method simultaneously aligns global distribution to decrease race gap at domain-level, and learns the discriminative target representations at cluster level. A novel mutual information loss is proposed to further enhance the discriminative ability of network output without label information. Extensive experiments on RFW, GBU, and IJB-A databases show that IMAN successfully learns features that generalize well across different races and across different databases.



### Traversing the Continuous Spectrum of Image Retrieval with Deep Dynamic Models
- **Arxiv ID**: http://arxiv.org/abs/1812.00202v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.00202v2)
- **Published**: 2018-12-01 12:40:01+00:00
- **Updated**: 2019-03-31 13:57:47+00:00
- **Authors**: Ziad Al-Halah, Andreas M. Lehrmann, Leonid Sigal
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce the first work to tackle the image retrieval problem as a continuous operation. While the proposed approaches in the literature can be roughly categorized into two main groups: category- and instance-based retrieval, in this work we show that the retrieval task is much richer and more complex. Image similarity goes beyond this discrete vantage point and spans a continuous spectrum among the classical operating points of category and instance similarity. However, current retrieval models are static and incapable of exploring this rich structure of the retrieval space since they are trained and evaluated with a single operating point as a target objective. Hence, we introduce a novel retrieval model that for a given query is capable of producing a dynamic embedding that can target an arbitrary point along the continuous retrieval spectrum. Our model disentangles the visual signal of a query image into its basic components of categorical and attribute information. Furthermore, using a continuous control parameter our model learns to reconstruct a dynamic embedding of the query by mixing these components with different proportions to target a specific point along the retrieval simplex. We demonstrate our idea in a comprehensive evaluation of the proposed model and highlight the advantages of our approach against a set of well-established discrete retrieval models.



### FineFool: Fine Object Contour Attack via Attention
- **Arxiv ID**: http://arxiv.org/abs/1812.01713v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.01713v1)
- **Published**: 2018-12-01 12:58:56+00:00
- **Updated**: 2018-12-01 12:58:56+00:00
- **Authors**: Jinyin Chen, Haibin Zheng, Hui Xiong, Mengmeng Su
- **Comment**: None
- **Journal**: None
- **Summary**: Machine learning models have been shown vulnerable to adversarial attacks launched by adversarial examples which are carefully crafted by attacker to defeat classifiers. Deep learning models cannot escape the attack either. Most of adversarial attack methods are focused on success rate or perturbations size, while we are more interested in the relationship between adversarial perturbation and the image itself. In this paper, we put forward a novel adversarial attack based on contour, named FineFool. Finefool not only has better attack performance compared with other state-of-art white-box attacks in aspect of higher attack success rate and smaller perturbation, but also capable of visualization the optimal adversarial perturbation via attention on object contour. To the best of our knowledge, Finefool is for the first time combines the critical feature of the original clean image with the optimal perturbations in a visible manner. Inspired by the correlations between adversarial perturbations and object contour, slighter perturbations is produced via focusing on object contour features, which is more imperceptible and difficult to be defended, especially network add-on defense methods with the trade-off between perturbations filtering and contour feature loss. Compared with existing state-of-art attacks, extensive experiments are conducted to show that Finefool is capable of efficient attack against defensive deep models.



### InGAN: Capturing and Remapping the "DNA" of a Natural Image
- **Arxiv ID**: http://arxiv.org/abs/1812.00231v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.00231v2)
- **Published**: 2018-12-01 17:48:02+00:00
- **Updated**: 2019-04-24 11:38:55+00:00
- **Authors**: Assaf Shocher, Shai Bagon, Phillip Isola, Michal Irani
- **Comment**: None
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) typically learn a distribution of images in a large image dataset, and are then able to generate new images from this distribution. However, each natural image has its own internal statistics, captured by its unique distribution of patches. In this paper we propose an "Internal GAN" (InGAN) - an image-specific GAN - which trains on a single input image and learns its internal distribution of patches. It is then able to synthesize a plethora of new natural images of significantly different sizes, shapes and aspect-ratios - all with the same internal patch-distribution (same "DNA") as the input image. In particular, despite large changes in global size/shape of the image, all elements inside the image maintain their local size/shape. InGAN is fully unsupervised, requiring no additional data other than the input image itself. Once trained on the input image, it can remap the input to any size or shape in a single feedforward pass, while preserving the same internal patch distribution. InGAN provides a unified framework for a variety of tasks, bridging the gap between textures and natural images.



### Learning to Caption Images through a Lifetime by Asking Questions
- **Arxiv ID**: http://arxiv.org/abs/1812.00235v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1812.00235v3)
- **Published**: 2018-12-01 18:12:35+00:00
- **Updated**: 2019-03-21 16:11:45+00:00
- **Authors**: Kevin Shen, Amlan Kar, Sanja Fidler
- **Comment**: Fixed typos and added contribution list in intro, results remain the
  same
- **Journal**: None
- **Summary**: In order to bring artificial agents into our lives, we will need to go beyond supervised learning on closed datasets to having the ability to continuously expand knowledge. Inspired by a student learning in a classroom, we present an agent that can continuously learn by posing natural language questions to humans. Our agent is composed of three interacting modules, one that performs captioning, another that generates questions and a decision maker that learns when to ask questions by implicitly reasoning about the uncertainty of the agent and expertise of the teacher. As compared to current active learning methods which query images for full captions, our agent is able to ask pointed questions to improve the generated captions. The agent trains on the improved captions, expanding its knowledge. We show that our approach achieves better performance using less human supervision than the baselines on the challenging MSCOCO dataset.



### LSTM-based Network for Human Gait Stability Prediction in an Intelligent Robotic Rollator
- **Arxiv ID**: http://arxiv.org/abs/1812.00252v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.00252v2)
- **Published**: 2018-12-01 19:50:56+00:00
- **Updated**: 2019-03-05 11:27:43+00:00
- **Authors**: Georgia Chalvatzaki, Petros Koutras, Jack Hadfield, Xanthi S. Papageorgiou, Costas S. Tzafestas, Petros Maragos
- **Comment**: 8 pages, 4 figures accepted to ICRA 2019
- **Journal**: None
- **Summary**: In this work, we present a novel framework for on-line human gait stability prediction of the elderly users of an intelligent robotic rollator using Long Short Term Memory (LSTM) networks, fusing multimodal RGB-D and Laser Range Finder (LRF) data from non-wearable sensors. A Deep Learning (DL) based approach is used for the upper body pose estimation. The detected pose is used for estimating the body Center of Mass (CoM) using Unscented Kalman Filter (UKF). An Augmented Gait State Estimation framework exploits the LRF data to estimate the legs' positions and the respective gait phase. These estimates are the inputs of an encoder-decoder sequence to sequence model which predicts the gait stability state as Safe or Fall Risk walking. It is validated with data from real patients, by exploring different network architectures, hyperparameter settings and by comparing the proposed method with other baselines. The presented LSTM-based human gait stability predictor is shown to provide robust predictions of the human stability state, and thus has the potential to be integrated into a general user-adaptive control architecture as a fall-risk alarm.



### A Deep Learning Approach for Multi-View Engagement Estimation of Children in a Child-Robot Joint Attention task
- **Arxiv ID**: http://arxiv.org/abs/1812.00253v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.00253v1)
- **Published**: 2018-12-01 19:51:16+00:00
- **Updated**: 2018-12-01 19:51:16+00:00
- **Authors**: Jack Hadfield, Georgia Chalvatzaki, Petros Koutras, Mehdi Khamassi, Costas S. Tzafestas, Petros Maragos
- **Comment**: 7 pages, 6 figures
- **Journal**: None
- **Summary**: In this work we tackle the problem of child engagement estimation while children freely interact with a robot in their room. We propose a deep-based multi-view solution that takes advantage of recent developments in human pose detection. We extract the child's pose from different RGB-D cameras placed elegantly in the room, fuse the results and feed them to a deep neural network trained for classifying engagement levels. The deep network contains a recurrent layer, in order to exploit the rich temporal information contained in the pose data. The resulting method outperforms a number of baseline classifiers, and provides a promising tool for better automatic understanding of a child's attitude, interest and attention while cooperating with a robot. The goal is to integrate this model in next generation social robots as an attention monitoring tool during various CRI tasks both for Typically Developed (TD) children and children affected by autism (ASD).



### Cross-Modulation Networks for Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1812.00273v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.00273v1)
- **Published**: 2018-12-01 22:02:04+00:00
- **Updated**: 2018-12-01 22:02:04+00:00
- **Authors**: Hugo Prol, Vincent Dumoulin, Luis Herranz
- **Comment**: Accepted at NIPS 2018 Workshop on Meta-Learning. Source code
  available at https://github.com/hprop/cross-modulation-nets
- **Journal**: None
- **Summary**: A family of recent successful approaches to few-shot learning relies on learning an embedding space in which predictions are made by computing similarities between examples. This corresponds to combining information between support and query examples at a very late stage of the prediction pipeline. Inspired by this observation, we hypothesize that there may be benefits to combining the information at various levels of abstraction along the pipeline. We present an architecture called Cross-Modulation Networks which allows support and query examples to interact throughout the feature extraction process via a feature-wise modulation mechanism. We adapt the Matching Networks architecture to take advantage of these interactions and show encouraging initial results on miniImageNet in the 5-way, 1-shot setting, where we close the gap with state-of-the-art.



### HUMBI: A Large Multiview Dataset of Human Body Expressions
- **Arxiv ID**: http://arxiv.org/abs/1812.00281v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.00281v3)
- **Published**: 2018-12-01 23:11:22+00:00
- **Updated**: 2020-05-23 01:22:21+00:00
- **Authors**: Zhixuan Yu, Jae Shin Yoon, In Kyu Lee, Prashanth Venkatesh, Jaesik Park, Jihun Yu, Hyun Soo Park
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a new large multiview dataset called HUMBI for human body expressions with natural clothing. The goal of HUMBI is to facilitate modeling view-specific appearance and geometry of gaze, face, hand, body, and garment from assorted people. 107 synchronized HD cameras are used to capture 772 distinctive subjects across gender, ethnicity, age, and physical condition. With the multiview image streams, we reconstruct high fidelity body expressions using 3D mesh models, which allows representing view-specific appearance using their canonical atlas. We demonstrate that HUMBI is highly effective in learning and reconstructing a complete human model and is complementary to the existing datasets of human body expressions with limited views and subjects such as MPII-Gaze, Multi-PIE, Human3.6M, and Panoptic Studio datasets.



### Explaining the Ambiguity of Object Detection and 6D Pose From Visual Data
- **Arxiv ID**: http://arxiv.org/abs/1812.00287v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.00287v2)
- **Published**: 2018-12-01 23:31:50+00:00
- **Updated**: 2019-08-20 14:08:33+00:00
- **Authors**: Fabian Manhardt, Diego Martin Arroyo, Christian Rupprecht, Benjamin Busam, Tolga Birdal, Nassir Navab, Federico Tombari
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: 3D object detection and pose estimation from a single image are two inherently ambiguous problems. Oftentimes, objects appear similar from different viewpoints due to shape symmetries, occlusion and repetitive textures. This ambiguity in both detection and pose estimation means that an object instance can be perfectly described by several different poses and even classes. In this work we propose to explicitly deal with this uncertainty. For each object instance we predict multiple pose and class outcomes to estimate the specific pose distribution generated by symmetries and repetitive textures. The distribution collapses to a single outcome when the visual appearance uniquely identifies just one valid pose. We show the benefits of our approach which provides not only a better explanation for pose ambiguity, but also a higher accuracy in terms of pose estimation.



### Classifying a specific image region using convolutional nets with an ROI mask as input
- **Arxiv ID**: http://arxiv.org/abs/1812.00291v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.00291v2)
- **Published**: 2018-12-01 23:52:37+00:00
- **Updated**: 2018-12-05 22:38:14+00:00
- **Authors**: Sagi Eppel
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural nets (CNN) are the leading computer vision method for classifying images. In some cases, it is desirable to classify only a specific region of the image that corresponds to a certain object. Hence, assuming that the region of the object in the image is known in advance and is given as a binary region of interest (ROI) mask, the goal is to classify the object in this region using a convolutional neural net. This goal is achieved using a standard image classification net with the addition of a side branch, which converts the ROI mask into an attention map. This map is then combined with the image classification net. This allows the net to focus the attention on the object region while still extracting contextual cues from the background. This approach was evaluated using the COCO object dataset and the OpenSurfaces materials dataset. In both cases, it gave superior results to methods that completely ignore the background region. In addition, it was found that combining the attention map at the first layer of the net gave better results than combining it at higher layers of the net. The advantages of this method are most apparent in the classification of small regions which demands a great deal of contextual information from the background.



