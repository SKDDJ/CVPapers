# Arxiv Papers in cs.CV on 2018-12-20
### SQuantizer: Simultaneous Learning for Both Sparse and Low-precision Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1812.08301v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1812.08301v2)
- **Published**: 2018-12-20 00:55:55+00:00
- **Updated**: 2019-03-23 20:47:47+00:00
- **Authors**: Mi Sun Park, Xiaofan Xu, Cormac Brick
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks have achieved state-of-the-art accuracies in a wide range of computer vision, speech recognition, and machine translation tasks. However the limits of memory bandwidth and computational power constrain the range of devices capable of deploying these modern networks. To address this problem, we propose SQuantizer, a new training method that jointly optimizes for both sparse and low-precision neural networks while maintaining high accuracy and providing a high compression rate. This approach brings sparsification and low-bit quantization into a single training pass, employing these techniques in an order demonstrated to be optimal. Our method achieves state-of-the-art accuracies using 4-bit and 2-bit precision for ResNet18, MobileNet-v2 and ResNet50, even with high degree of sparsity. The compression rates of 18x for ResNet18 and 17x for ResNet50, and 9x for MobileNet-v2 are obtained when SQuantizing both weights and activations within 1% and 2% loss in accuracy for ResNets and MobileNet-v2 respectively. An extension of these techniques to object detection also demonstrates high accuracy on YOLO-v3. Additionally, our method allows for fast single pass training, which is important for rapid prototyping and neural architecture search techniques. Finally extensive results from this simultaneous training approach allows us to draw some useful insights into the relative merits of sparsity and quantization.



### Rain Removal By Image Quasi-Sparsity Priors
- **Arxiv ID**: http://arxiv.org/abs/1812.08348v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.08348v1)
- **Published**: 2018-12-20 03:39:39+00:00
- **Updated**: 2018-12-20 03:39:39+00:00
- **Authors**: Yinglong Wang, Shuaicheng Liu, Chen Chen, Dehua Xie, Bing Zeng
- **Comment**: 12 pages, 12 figures
- **Journal**: None
- **Summary**: Rain streaks will inevitably be captured by some outdoor vision systems, which lowers the image visual quality and also interferes various computer vision applications. We present a novel rain removal method in this paper, which consists of two steps, i.e., detection of rain streaks and reconstruction of the rain-removed image. An accurate detection of rain streaks determines the quality of the overall performance. To this end, we first detect rain streaks according to pixel intensities, motivated by the observation that rain streaks often possess higher intensities compared to other neighboring image structures. Some mis-detected locations are then refined through a morphological processing and the principal component analysis (PCA) such that only locations corresponding to real rain streaks are retained. In the second step, we separate image gradients into a background layer and a rain streak layer, thanks to the image quasi-sparsity prior, so that a rain image can be decomposed into a background layer and a rain layer. We validate the effectiveness of our method through quantitative and qualitative evaluations. We show that our method can remove rain (even for some relatively bright rain) from images robustly and outperforms some state-of-the-art rain removal algorithms.



### Plug-and-Play: Improve Depth Estimation via Sparse Data Propagation
- **Arxiv ID**: http://arxiv.org/abs/1812.08350v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1812.08350v2)
- **Published**: 2018-12-20 03:49:49+00:00
- **Updated**: 2019-04-11 14:32:53+00:00
- **Authors**: Tsun-Hsuan Wang, Fu-En Wang, Juan-Ting Lin, Yi-Hsuan Tsai, Wei-Chen Chiu, Min Sun
- **Comment**: 7 pages. 7 figures. ver.2
- **Journal**: None
- **Summary**: We propose a novel plug-and-play (PnP) module for improving depth prediction with taking arbitrary patterns of sparse depths as input. Given any pre-trained depth prediction model, our PnP module updates the intermediate feature map such that the model outputs new depths consistent with the given sparse depths. Our method requires no additional training and can be applied to practical applications such as leveraging both RGB and sparse LiDAR points to robustly estimate dense depth map. Our approach achieves consistent improvements on various state-of-the-art methods on indoor (i.e., NYU-v2) and outdoor (i.e., KITTI) datasets. Various types of LiDARs are also synthesized in our experiments to verify the general applicability of our PnP module in practice. For project page, see https://zswang666.github.io/PnP-Depth-Project-Page/



### Robustness Meets Deep Learning: An End-to-End Hybrid Pipeline for Unsupervised Learning of Egomotion
- **Arxiv ID**: http://arxiv.org/abs/1812.08351v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.08351v3)
- **Published**: 2018-12-20 03:51:47+00:00
- **Updated**: 2019-02-13 01:31:15+00:00
- **Authors**: Alex Zihao Zhu, Wenxin Liu, Ziyun Wang, Vijay Kumar, Kostas Daniilidis
- **Comment**: 10 pages, 5 figures, 7 tables
- **Journal**: None
- **Summary**: In this work, we propose a method that combines unsupervised deep learning predictions for optical flow and monocular disparity with a model based optimization procedure for instantaneous camera pose. Given the flow and disparity predictions from the network, we apply a RANSAC outlier rejection scheme to find an inlier set of flows and disparities, which we use to solve for the relative camera pose in a least squares fashion. We show that this pipeline is fully differentiable, allowing us to combine the pose with the network outputs as an additional unsupervised training loss to further refine the predicted flows and disparities. This method not only allows us to directly regress relative pose from the network outputs, but also automatically segments away pixels that do not fit the rigid scene assumptions that many unsupervised structure from motion methods apply, such as on independently moving objects. We evaluate our method on the KITTI dataset, and demonstrate state of the art results, even in the presence of challenging independently moving objects.



### Sequential Attention GAN for Interactive Image Editing
- **Arxiv ID**: http://arxiv.org/abs/1812.08352v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.08352v4)
- **Published**: 2018-12-20 03:55:33+00:00
- **Updated**: 2020-08-05 22:13:20+00:00
- **Authors**: Yu Cheng, Zhe Gan, Yitong Li, Jingjing Liu, Jianfeng Gao
- **Comment**: ACM MM 2020
- **Journal**: None
- **Summary**: Most existing text-to-image synthesis tasks are static single-turn generation, based on pre-defined textual descriptions of images. To explore more practical and interactive real-life applications, we introduce a new task - Interactive Image Editing, where users can guide an agent to edit images via multi-turn textual commands on-the-fly. In each session, the agent takes a natural language description from the user as the input and modifies the image generated in the previous turn to a new design, following the user description. The main challenges in this sequential and interactive image generation task are two-fold: 1) contextual consistency between a generated image and the provided textual description; 2) step-by-step region-level modification to maintain visual consistency across the generated image sequence in each session. To address these challenges, we propose a novel Sequential Attention Generative Adversarial Net-work (SeqAttnGAN), which applies a neural state tracker to encode the previous image and the textual description in each turn of the sequence, and uses a GAN framework to generate a modified version of the image that is consistent with the preceding images and coherent with the description. To achieve better region-specific refinement, we also introduce a sequential attention mechanism into the model. To benchmark on the new task, we introduce two new datasets, Zap-Seq and DeepFashion-Seq, which contain multi-turn sessions with image-description sequences in the fashion domain. Experiments on both datasets show that the proposed SeqAttnGANmodel outperforms state-of-the-art approaches on the interactive image editing task across all evaluation metrics including visual quality, image sequence coherence, and text-image consistency.



### SfMLearner++: Learning Monocular Depth & Ego-Motion using Meaningful Geometric Constraints
- **Arxiv ID**: http://arxiv.org/abs/1812.08370v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1812.08370v1)
- **Published**: 2018-12-20 06:17:37+00:00
- **Updated**: 2018-12-20 06:17:37+00:00
- **Authors**: Vignesh Prasad, Brojeshwar Bhowmick
- **Comment**: Accepted at WACV 2019
- **Journal**: None
- **Summary**: Most geometric approaches to monocular Visual Odometry (VO) provide robust pose estimates, but sparse or semi-dense depth estimates. Off late, deep methods have shown good performance in generating dense depths and VO from monocular images by optimizing the photometric consistency between images. Despite being intuitive, a naive photometric loss does not ensure proper pixel correspondences between two views, which is the key factor for accurate depth and relative pose estimations. It is a well known fact that simply minimizing such an error is prone to failures.   We propose a method using Epipolar constraints to make the learning more geometrically sound. We use the Essential matrix, obtained using Nister's Five Point Algorithm, for enforcing meaningful geometric constraints on the loss, rather than using it as labels for training. Our method, although simplistic but more geometrically meaningful, using lesser number of parameters, gives a comparable performance to state-of-the-art methods which use complex losses and large networks showing the effectiveness of using epipolar constraints. Such a geometrically constrained learning method performs successfully even in cases where simply minimizing the photometric error would fail.



### DAC: Data-free Automatic Acceleration of Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1812.08374v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.08374v2)
- **Published**: 2018-12-20 06:26:08+00:00
- **Updated**: 2018-12-27 22:55:58+00:00
- **Authors**: Xin Li, Shuai Zhang, Bolan Jiang, Yingyong Qi, Mooi Choo Chuah, Ning Bi
- **Comment**: Accepted by IEEE Winter Conference on Applications of Computer Vision
  (WACV 2019)
- **Journal**: None
- **Summary**: Deploying a deep learning model on mobile/IoT devices is a challenging task. The difficulty lies in the trade-off between computation speed and accuracy. A complex deep learning model with high accuracy runs slowly on resource-limited devices, while a light-weight model that runs much faster loses accuracy. In this paper, we propose a novel decomposition method, namely DAC, that is capable of factorizing an ordinary convolutional layer into two layers with much fewer parameters. DAC computes the corresponding weights for the newly generated layers directly from the weights of the original convolutional layer. Thus, no training (or fine-tuning) or any data is needed. The experimental results show that DAC reduces a large number of floating-point operations (FLOPs) while maintaining high accuracy of a pre-trained model. If 2% accuracy drop is acceptable, DAC saves 53% FLOPs of VGG16 image classification model on ImageNet dataset, 29% FLOPS of SSD300 object detection model on PASCAL VOC2007 dataset, and 46% FLOPS of a multi-person pose estimation model on Microsoft COCO dataset. Compared to other existing decomposition methods, DAC achieves better performance.



### SFA: Small Faces Attention Face Detector
- **Arxiv ID**: http://arxiv.org/abs/1812.08402v1
- **DOI**: 10.1109/ACCESS.2019.2955757
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.08402v1)
- **Published**: 2018-12-20 07:46:23+00:00
- **Updated**: 2018-12-20 07:46:23+00:00
- **Authors**: Shi Luo, Xiongfei Li, Rui Zhu, Xiaoli Zhang
- **Comment**: 10 pages, 0 figures, 0 tables, 41 references
- **Journal**: IEEE Access 2019
- **Summary**: In recent year, tremendous strides have been made in face detection thanks to deep learning. However, most published face detectors deteriorate dramatically as the faces become smaller. In this paper, we present the Small Faces Attention (SFA) face detector to better detect faces with small scale. First, we propose a new scale-invariant face detection architecture which pays more attention to small faces, including 4-branch detection architecture and small faces sensitive anchor design. Second, feature maps fusion strategy is applied in SFA by partially combining high-level features into low-level features to further improve the ability of finding hard faces. Third, we use multi-scale training and testing strategy to enhance face detection performance in practice. Comprehensive experiments show that SFA significantly improves face detection performance, especially on small faces. Our real-time SFA face detector can run at 5 FPS on a single GPU as well as maintain high performance. Besides, our final SFA face detector achieves state-of-the-art detection performance on challenging face detection benchmarks, including WIDER FACE and FDDB datasets, with competitive runtime speed. Both our code and models will be available to the research community.



### Unsupervised Meta-learning of Figure-Ground Segmentation via Imitating Visual Effects
- **Arxiv ID**: http://arxiv.org/abs/1812.08442v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.08442v1)
- **Published**: 2018-12-20 09:39:47+00:00
- **Updated**: 2018-12-20 09:39:47+00:00
- **Authors**: Ding-Jie Chen, Jui-Ting Chien, Hwann-Tzong Chen, Tyng-Luh Liu
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a "learning to learn" approach to figure-ground image segmentation. By exploring webly-abundant images of specific visual effects, our method can effectively learn the visual-effect internal representations in an unsupervised manner and uses this knowledge to differentiate the figure from the ground in an image. Specifically, we formulate the meta-learning process as a compositional image editing task that learns to imitate a certain visual effect and derive the corresponding internal representation. Such a generative process can help instantiate the underlying figure-ground notion and enables the system to accomplish the intended image segmentation. Whereas existing generative methods are mostly tailored to image synthesis or style transfer, our approach offers a flexible learning mechanism to model a general concept of figure-ground segmentation from unorganized images that have no explicit pixel-level annotations. We validate our approach via extensive experiments on six datasets to demonstrate that the proposed model can be end-to-end trained without ground-truth pixel labeling yet outperforms the existing methods of unsupervised segmentation tasks.



### One-Class Feature Learning Using Intra-Class Splitting
- **Arxiv ID**: http://arxiv.org/abs/1812.08468v5
- **DOI**: 10.23919/EUSIPCO.2019.8902848
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.08468v5)
- **Published**: 2018-12-20 10:32:46+00:00
- **Updated**: 2019-11-20 13:51:36+00:00
- **Authors**: Patrick Schlachter, Yiwen Liao, Bin Yang
- **Comment**: IEEE European Signal Processing Conference 2019 (EUSIPCO 2019)
- **Journal**: None
- **Summary**: This paper proposes a novel generic one-class feature learning method based on intra-class splitting. In one-class classification, feature learning is challenging, because only samples of one class are available during training. Hence, state-of-the-art methods require reference multi-class datasets to pretrain feature extractors. In contrast, the proposed method realizes feature learning by splitting the given normal class into typical and atypical normal samples. By introducing closeness loss and dispersion loss, an intra-class joint training procedure between the two subsets after splitting enables the extraction of valuable features for one-class classification. Various experiments on three well-known image classification datasets demonstrate the effectiveness of our method which outperformed other baseline models in average.



### Computational Anatomy for Multi-Organ Analysis in Medical Imaging: A Review
- **Arxiv ID**: http://arxiv.org/abs/1812.08577v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.08577v1)
- **Published**: 2018-12-20 14:05:15+00:00
- **Updated**: 2018-12-20 14:05:15+00:00
- **Authors**: Juan J. Cerrolaza, Mirella Lopez-Picazo, Ludovic Humbert, Yoshinobu Sato, Daniel Rueckert, Miguel Angel Gonzalez Ballester, Marius George Linguraru
- **Comment**: Paper under review
- **Journal**: None
- **Summary**: The medical image analysis field has traditionally been focused on the development of organ-, and disease-specific methods. Recently, the interest in the development of more 20 comprehensive computational anatomical models has grown, leading to the creation of multi-organ models. Multi-organ approaches, unlike traditional organ-specific strategies, incorporate inter-organ relations into the model, thus leading to a more accurate representation of the complex human anatomy. Inter-organ relations are not only spatial, but also functional and physiological. Over the years, the strategies 25 proposed to efficiently model multi-organ structures have evolved from the simple global modeling, to more sophisticated approaches such as sequential, hierarchical, or machine learning-based models. In this paper, we present a review of the state of the art on multi-organ analysis and associated computation anatomy methodology. The manuscript follows a methodology-based classification of the different techniques 30 available for the analysis of multi-organs and multi-anatomical structures, from techniques using point distribution models to the most recent deep learning-based approaches. With more than 300 papers included in this review, we reflect on the trends and challenges of the field of computational anatomy, the particularities of each anatomical region, and the potential of multi-organ analysis to increase the impact of 35 medical imaging applications on the future of healthcare.



### Automated detection of block falls in the north polar region of Mars
- **Arxiv ID**: http://arxiv.org/abs/1812.08624v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.08624v1)
- **Published**: 2018-12-20 15:05:21+00:00
- **Updated**: 2018-12-20 15:05:21+00:00
- **Authors**: L. Fanara, K. Gwinner, E. Hauber, J. Oberst
- **Comment**: None
- **Journal**: None
- **Summary**: We developed a change detection method for the identification of ice block falls using NASA's HiRISE images of the north polar scarps on Mars. Our method is based on a Support Vector Machine (SVM), trained using Histograms of Oriented Gradients (HOG), and on blob detection. The SVM detects potential new blocks between a set of images; the blob detection, then, confirms the identification of a block inside the area indicated by the SVM and derives the shape of the block. The results from the automatic analysis were compared with block statistics from visual inspection. We tested our method in 6 areas consisting of 1000x1000 pixels, where several hundreds of blocks were identified. The results for the given test areas produced a true positive rate of ~75% for blocks with sizes larger than 0.7 m (i.e., approx. 3 times the available ground pixel size) and a false discovery rate of ~8.5%. Using blob detection we also recover the size of each block within 3 pixels of their actual size.



### nocaps: novel object captioning at scale
- **Arxiv ID**: http://arxiv.org/abs/1812.08658v3
- **DOI**: 10.1109/ICCV.2019.00904
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.08658v3)
- **Published**: 2018-12-20 16:04:05+00:00
- **Updated**: 2019-09-30 20:10:33+00:00
- **Authors**: Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Stefan Lee, Peter Anderson
- **Comment**: None
- **Journal**: IEEE International Conference on Computer Vision (ICCV) 2019
- **Summary**: Image captioning models have achieved impressive results on datasets containing limited visual concepts and large amounts of paired image-caption training data. However, if these models are to ever function in the wild, a much larger variety of visual concepts must be learned, ideally from less supervision. To encourage the development of image captioning models that can learn visual concepts from alternative data sources, such as object detection datasets, we present the first large-scale benchmark for this task. Dubbed 'nocaps', for novel object captioning at scale, our benchmark consists of 166,100 human-generated captions describing 15,100 images from the OpenImages validation and test sets. The associated training data consists of COCO image-caption pairs, plus OpenImages image-level labels and object bounding boxes. Since OpenImages contains many more classes than COCO, nearly 400 object classes seen in test images have no or very few associated training captions (hence, nocaps). We extend existing novel object captioning models to establish strong baselines for this benchmark and provide analysis to guide future work on this task.



### DeepFakes: a New Threat to Face Recognition? Assessment and Detection
- **Arxiv ID**: http://arxiv.org/abs/1812.08685v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.08685v1)
- **Published**: 2018-12-20 16:36:39+00:00
- **Updated**: 2018-12-20 16:36:39+00:00
- **Authors**: Pavel Korshunov, Sebastien Marcel
- **Comment**: http://publications.idiap.ch/index.php/publications/show/3988
- **Journal**: None
- **Summary**: It is becoming increasingly easy to automatically replace a face of one person in a video with the face of another person by using a pre-trained generative adversarial network (GAN). Recent public scandals, e.g., the faces of celebrities being swapped onto pornographic videos, call for automated ways to detect these Deepfake videos. To help developing such methods, in this paper, we present the first publicly available set of Deepfake videos generated from videos of VidTIMIT database. We used open source software based on GANs to create the Deepfakes, and we emphasize that training and blending parameters can significantly impact the quality of the resulted videos. To demonstrate this impact, we generated videos with low and high visual quality (320 videos each) using differently tuned parameter sets. We showed that the state of the art face recognition systems based on VGG and Facenet neural networks are vulnerable to Deepfake videos, with 85.62% and 95.00% false acceptance rates respectively, which means methods for detecting Deepfake videos are necessary. By considering several baseline approaches, we found that audio-visual approach based on lip-sync inconsistency detection was not able to distinguish Deepfake videos. The best performing method, which is based on visual quality metrics and is often used in presentation attack detection domain, resulted in 8.97% equal error rate on high quality Deepfakes. Our experiments demonstrate that GAN-generated Deepfake videos are challenging for both face recognition systems and existing detection methods, and the further development of face swapping technology will make it even more so.



### Subsurface structure analysis using computational interpretation and learning: A visual signal processing perspective
- **Arxiv ID**: http://arxiv.org/abs/1812.08756v1
- **DOI**: 10.1109/MSP.2017.2785979
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.08756v1)
- **Published**: 2018-12-20 18:38:11+00:00
- **Updated**: 2018-12-20 18:38:11+00:00
- **Authors**: G. AlRegib, M. Deriche, Z. Long, H. Di, Z. Wang, Y. Alaudah, M. Shafiq, M. Alfarraj
- **Comment**: None
- **Journal**: None
- **Summary**: Understanding Earth's subsurface structures has been and continues to be an essential component of various applications such as environmental monitoring, carbon sequestration, and oil and gas exploration. By viewing the seismic volumes that are generated through the processing of recorded seismic traces, researchers were able to learn from applying advanced image processing and computer vision algorithms to effectively analyze and understand Earth's subsurface structures. In this paper, first, we summarize the recent advances in this direction that relied heavily on the fields of image processing and computer vision. Second, we discuss the challenges in seismic interpretation and provide insights and some directions to address such challenges using emerging machine learning algorithms.



### Deep Paper Gestalt
- **Arxiv ID**: http://arxiv.org/abs/1812.08775v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.08775v1)
- **Published**: 2018-12-20 18:55:26+00:00
- **Updated**: 2018-12-20 18:55:26+00:00
- **Authors**: Jia-Bin Huang
- **Comment**: Project page: https://github.com/vt-vl-lab/paper-gestalt
- **Journal**: None
- **Summary**: Recent years have witnessed a significant increase in the number of paper submissions to computer vision conferences. The sheer volume of paper submissions and the insufficient number of competent reviewers cause a considerable burden for the current peer review system. In this paper, we learn a classifier to predict whether a paper should be accepted or rejected based solely on the visual appearance of the paper (i.e., the gestalt of a paper). Experimental results show that our classifier can safely reject 50% of the bad papers while wrongly reject only 0.4% of the good papers, and thus dramatically reduce the workload of the reviewers. We also provide tools for providing suggestions to authors so that they can improve the gestalt of their papers.



### Deep Metric Transfer for Label Propagation with Limited Annotated Data
- **Arxiv ID**: http://arxiv.org/abs/1812.08781v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.08781v2)
- **Published**: 2018-12-20 18:57:28+00:00
- **Updated**: 2019-07-29 06:46:33+00:00
- **Authors**: Bin Liu, Zhirong Wu, Han Hu, Stephen Lin
- **Comment**: Code is availble at
  http://github.com/Microsoft/metric-transfer.pytorch
- **Journal**: None
- **Summary**: We study object recognition under the constraint that each object class is only represented by very few observations. Semi-supervised learning, transfer learning, and few-shot recognition all concern with achieving fast generalization with few labeled data. In this paper, we propose a generic framework that utilizes unlabeled data to aid generalization for all three tasks. Our approach is to create much more training data through label propagation from the few labeled examples to a vast collection of unannotated images. The main contribution of the paper is that we show such a label propagation scheme can be highly effective when the similarity metric used for propagation is transferred from other related domains. We test various combinations of supervised and unsupervised metric learning methods with various label propagation algorithms. We find that our framework is very generic without being sensitive to any specific techniques. By taking advantage of unlabeled data in this way, we achieve significant improvements on all three tasks.



### Steerable $e$PCA: Rotationally Invariant Exponential Family PCA
- **Arxiv ID**: http://arxiv.org/abs/1812.08789v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.08789v3)
- **Published**: 2018-12-20 18:59:58+00:00
- **Updated**: 2019-12-17 17:27:54+00:00
- **Authors**: Zhizhen Zhao, Lydia T. Liu, Amit Singer
- **Comment**: None
- **Journal**: None
- **Summary**: In photon-limited imaging, the pixel intensities are affected by photon count noise. Many applications, such as 3-D reconstruction using correlation analysis in X-ray free electron laser (XFEL) single molecule imaging, require an accurate estimation of the covariance of the underlying 2-D clean images. Accurate estimation of the covariance from low-photon count images must take into account that pixel intensities are Poisson distributed, hence the classical sample covariance estimator is sub-optimal. Moreover, in single molecule imaging, including in-plane rotated copies of all images could further improve the accuracy of covariance estimation. In this paper we introduce an efficient and accurate algorithm for covariance matrix estimation of count noise 2-D images, including their uniform planar rotations and possibly reflections. Our procedure, steerable $e$PCA, combines in a novel way two recently introduced innovations. The first is a methodology for principal component analysis (PCA) for Poisson distributions, and more generally, exponential family distributions, called $e$PCA. The second is steerable PCA, a fast and accurate procedure for including all planar rotations for PCA. The resulting principal components are invariant to the rotation and reflection of the input images. We demonstrate the efficiency and accuracy of steerable $e$PCA in numerical experiments involving simulated XFEL datasets and rotated Yale B face data.



### SMILER: Saliency Model Implementation Library for Experimental Research
- **Arxiv ID**: http://arxiv.org/abs/1812.08848v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.08848v1)
- **Published**: 2018-12-20 21:20:11+00:00
- **Updated**: 2018-12-20 21:20:11+00:00
- **Authors**: Calden Wloka, Toni Kunić, Iuliia Kotseruba, Ramin Fahimi, Nicholas Frosst, Neil D. B. Bruce, John K. Tsotsos
- **Comment**: None
- **Journal**: None
- **Summary**: The Saliency Model Implementation Library for Experimental Research (SMILER) is a new software package which provides an open, standardized, and extensible framework for maintaining and executing computational saliency models. This work drastically reduces the human effort required to apply saliency algorithms to new tasks and datasets, while also ensuring consistency and procedural correctness for results and conclusions produced by different parties. At its launch SMILER already includes twenty three saliency models (fourteen models based in MATLAB and nine supported through containerization), and the open design of SMILER encourages this number to grow with future contributions from the community. The project may be downloaded and contributed to through its GitHub page: https://github.com/tsotsoslab/smiler



### Three Dimensional Reconstruction of Botanical Trees with Simulatable Geometry
- **Arxiv ID**: http://arxiv.org/abs/1812.08849v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.08849v1)
- **Published**: 2018-12-20 21:22:43+00:00
- **Updated**: 2018-12-20 21:22:43+00:00
- **Authors**: Ed Quigley, Winnie Lin, Yilin Zhu, Ronald Fedkiw
- **Comment**: None
- **Journal**: None
- **Summary**: We tackle the challenging problem of creating full and accurate three dimensional reconstructions of botanical trees with the topological and geometric accuracy required for subsequent physical simulation, e.g. in response to wind forces. Although certain aspects of our approach would benefit from various improvements, our results exceed the state of the art especially in geometric and topological complexity and accuracy. Starting with two dimensional RGB image data acquired from cameras attached to drones, we create point clouds, textured triangle meshes, and a simulatable and skinned cylindrical articulated rigid body model. We discuss the pros and cons of each step of our pipeline, and in order to stimulate future research we make the raw and processed data from every step of the pipeline as well as the final geometric reconstructions publicly available.



### A Scale Invariant Approach for Sparse Signal Recovery
- **Arxiv ID**: http://arxiv.org/abs/1812.08852v4
- **DOI**: None
- **Categories**: **math.NA**, cs.CV, cs.NA
- **Links**: [PDF](http://arxiv.org/pdf/1812.08852v4)
- **Published**: 2018-12-20 21:29:44+00:00
- **Updated**: 2019-08-16 14:34:47+00:00
- **Authors**: Yaghoub Rahimi, Chao Wang, Hongbo Dong, Yifei Lou
- **Comment**: 24 pages
- **Journal**: None
- **Summary**: In this paper, we study the ratio of the $L_1 $ and $L_2 $ norms, denoted as $L_1/L_2$, to promote sparsity. Due to the non-convexity and non-linearity, there has been little attention to this scale-invariant model. Compared to popular models in the literature such as the $L_p$ model for $p\in(0,1)$ and the transformed $L_1$ (TL1), this ratio model is parameter free. Theoretically, we present a strong null space property (sNSP) and prove that any sparse vector is a local minimizer of the $L_1 /L_2 $ model provided with this sNSP condition. Computationally, we focus on a constrained formulation that can be solved via the alternating direction method of multipliers (ADMM). Experiments show that the proposed approach is comparable to the state-of-the-art methods in sparse recovery. In addition, a variant of the $L_1/L_2$ model to apply on the gradient is also discussed with a proof-of-concept example of the MRI reconstruction.



### Animating Arbitrary Objects via Deep Motion Transfer
- **Arxiv ID**: http://arxiv.org/abs/1812.08861v3
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.08861v3)
- **Published**: 2018-12-20 21:45:56+00:00
- **Updated**: 2019-08-30 23:48:13+00:00
- **Authors**: Aliaksandr Siarohin, Stéphane Lathuilière, Sergey Tulyakov, Elisa Ricci, Nicu Sebe
- **Comment**: CVPR-2019 (oral)
- **Journal**: None
- **Summary**: This paper introduces a novel deep learning framework for image animation. Given an input image with a target object and a driving video sequence depicting a moving object, our framework generates a video in which the target object is animated according to the driving sequence. This is achieved through a deep architecture that decouples appearance and motion information. Our framework consists of three main modules: (i) a Keypoint Detector unsupervisely trained to extract object keypoints, (ii) a Dense Motion prediction network for generating dense heatmaps from sparse keypoints, in order to better encode motion information and (iii) a Motion Transfer Network, which uses the motion heatmaps and appearance information extracted from the input image to synthesize the output frames. We demonstrate the effectiveness of our method on several benchmark datasets, spanning a wide variety of object appearances, and show that our approach outperforms state-of-the-art image animation and video generation methods. Our source code is publicly available.



### An Optical Flow-Based Approach for Minimally-Divergent Velocimetry Data Interpolation
- **Arxiv ID**: http://arxiv.org/abs/1812.08882v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.08882v1)
- **Published**: 2018-12-20 23:01:30+00:00
- **Updated**: 2018-12-20 23:01:30+00:00
- **Authors**: Berkay Kanberoglu, Dhritiman Das, Priya Nair, Pavan Turaga, David Frakes
- **Comment**: 24 pages, 10 figures, International Journal of Biomedical Imaging,
  accepted manuscript
- **Journal**: None
- **Summary**: Three-dimensional (3D) biomedical image sets are often acquired with in-plane pixel spacings that are far less than the out-of-plane spacings between images. The resultant anisotropy, which can be detrimental in many applications, can be decreased using image interpolation. Optical flow and/or other registration-based interpolators have proven useful in such interpolation roles in the past. When acquired images are comprised of signals that describe the flow velocity of fluids, additional information is available to guide the interpolation process. In this paper, we present an optical-flow based framework for image interpolation that also minimizes resultant divergence in the interpolated data.



### Cluster validity index based on Jeffrey divergence
- **Arxiv ID**: http://arxiv.org/abs/1812.08891v1
- **DOI**: 10.1007/s10044-015-0453-7
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1812.08891v1)
- **Published**: 2018-12-20 23:41:14+00:00
- **Updated**: 2018-12-20 23:41:14+00:00
- **Authors**: Ahmed Ben Said, Rachid Hadjidj, Sebti Foufou
- **Comment**: None
- **Journal**: PATTERN.ANAL.APPL. 20 (2015) 21-31
- **Summary**: Cluster validity indexes are very important tools designed for two purposes: comparing the performance of clustering algorithms and determining the number of clusters that best fits the data. These indexes are in general constructed by combining a measure of compactness and a measure of separation. A classical measure of compactness is the variance. As for separation, the distance between cluster centers is used. However, such a distance does not always reflect the quality of the partition between clusters and sometimes gives misleading results. In this paper, we propose a new cluster validity index for which Jeffrey divergence is used to measure separation between clusters. Experimental results are conducted using different types of data and comparison with widely used cluster validity indexes demonstrates the outperformance of the proposed index.



