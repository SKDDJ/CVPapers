# Arxiv Papers in cs.CV on 2018-12-26
### Deep Convolutional Generative Adversarial Network Based Food Recognition Using Partially Labeled Data
- **Arxiv ID**: http://arxiv.org/abs/1812.10179v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.10179v1)
- **Published**: 2018-12-26 00:10:26+00:00
- **Updated**: 2018-12-26 00:10:26+00:00
- **Authors**: Bappaditya Mandal, N. B. Puhan, Avijit Verma
- **Comment**: 5 pages, 2 figures, 2 tables, submitted to IEEE Sensors Letters
- **Journal**: None
- **Summary**: Traditional machine learning algorithms using hand-crafted feature extraction techniques (such as local binary pattern) have limited accuracy because of high variation in images of the same class (or intra-class variation) for food recognition task. In recent works, convolutional neural networks (CNN) have been applied to this task with better results than all previously reported methods. However, they perform best when trained with large amount of annotated (labeled) food images. This is problematic when obtained in large volume, because they are expensive, laborious and impractical. Our work aims at developing an efficient deep CNN learning-based method for food recognition alleviating these limitations by using partially labeled training data on generative adversarial networks (GANs). We make new enhancements to the unsupervised training architecture introduced by Goodfellow et al. (2014), which was originally aimed at generating new data by sampling a dataset. In this work, we make modifications to deep convolutional GANs to make them robust and efficient for classifying food images. Experimental results on benchmarking datasets show the superiority of our proposed method as compared to the current-state-of-the-art methodologies even when trained with partially labeled training data.



### FPD-M-net: Fingerprint Image Denoising and Inpainting Using M-Net Based Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1812.10191v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.10191v2)
- **Published**: 2018-12-26 00:42:47+00:00
- **Updated**: 2019-03-22 11:53:36+00:00
- **Authors**: Sukesh Adiga V, Jayanthi Sivaswamy
- **Comment**: 11 pages, Accepted in CiML; 3rd Rank in ECCV 2018 Satellite Event by
  Chalearn LAP In-painting Competition Track-3;
- **Journal**: None
- **Summary**: Fingerprint is a common biometric used for authentication and verification of an individual. These images are degraded when fingers are wet, dirty, dry or wounded and due to the failure of the sensors, etc. The extraction of the fingerprint from a degraded image requires denoising and inpainting. We propose to address these problems with an end-to-end trainable Convolutional Neural Network based architecture called FPD-M-net, by posing the fingerprint denoising and inpainting problem as a segmentation (foreground) task. Our architecture is based on the M-net with a change: structure similarity loss function, used for better extraction of the fingerprint from the noisy background. Our method outperforms the baseline method and achieves an overall 3rd rank in the Chalearn LAP Inpainting Competition Track 3 - Fingerprint Denoising and Inpainting, ECCV 2018



### RegNet: Learning the Optimization of Direct Image-to-Image Pose Registration
- **Arxiv ID**: http://arxiv.org/abs/1812.10212v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.10212v1)
- **Published**: 2018-12-26 03:47:31+00:00
- **Updated**: 2018-12-26 03:47:31+00:00
- **Authors**: Lei Han, Mengqi Ji, Lu Fang, Matthias Nie√üner
- **Comment**: 8 pages, 6 figures
- **Journal**: None
- **Summary**: Direct image-to-image alignment that relies on the optimization of photometric error metrics suffers from limited convergence range and sensitivity to lighting conditions. Deep learning approaches has been applied to address this problem by learning better feature representations using convolutional neural networks, yet still require a good initialization. In this paper, we demonstrate that the inaccurate numerical Jacobian limits the convergence range which could be improved greatly using learned approaches. Based on this observation, we propose a novel end-to-end network, RegNet, to learn the optimization of image-to-image pose registration. By jointly learning feature representation for each pixel and partial derivatives that replace handcrafted ones (e.g., numerical differentiation) in the optimization step, the neural network facilitates end-to-end optimization. The energy landscape is constrained on both the feature representation and the learned Jacobian, hence providing more flexibility for the optimization as a consequence leads to more robust and faster convergence. In a series of experiments, including a broad ablation study, we demonstrate that RegNet is able to converge for large-baseline image pairs with fewer iterations.



### End-to-End Latent Fingerprint Search
- **Arxiv ID**: http://arxiv.org/abs/1812.10213v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.10213v1)
- **Published**: 2018-12-26 03:48:12+00:00
- **Updated**: 2018-12-26 03:48:12+00:00
- **Authors**: Kai Cao, Dinh-Luan Nguyen, Cori Tymoszek, A. K. Jain
- **Comment**: None
- **Journal**: None
- **Summary**: Latent fingerprints are one of the most important and widely used sources of evidence in law enforcement and forensic agencies. Yet the performance of the state-of-the-art latent recognition systems is far from satisfactory, and they often require manual markups to boost the latent search performance. Further, the COTS systems are proprietary and do not output the true comparison scores between a latent and reference prints to conduct quantitative evidential analysis. We present an end-to-end latent fingerprint search system, including automated region of interest (ROI) cropping, latent image preprocessing, feature extraction, feature comparison , and outputs a candidate list. Two separate minutiae extraction models provide complementary minutiae templates. To compensate for the small number of minutiae in small area and poor quality latents, a virtual minutiae set is generated to construct a texture template. A 96-dimensional descriptor is extracted for each minutia from its neighborhood. For computational efficiency, the descriptor length for virtual minutiae is further reduced to 16 using product quantization. Our end-to-end system is evaluated on three latent databases: NIST SD27 (258 latents); MSP (1,200 latents), WVU (449 latents) and N2N (10,000 latents) against a background set of 100K rolled prints, which includes the true rolled mates of the latents with rank-1 retrieval rates of 65.7%, 69.4%, 65.5%, and 7.6% respectively. A multi-core solution implemented on 24 cores obtains 1ms per latent to rolled comparison.



### Seeing isn't Believing: Practical Adversarial Attack Against Object Detectors
- **Arxiv ID**: http://arxiv.org/abs/1812.10217v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/1812.10217v3)
- **Published**: 2018-12-26 04:14:08+00:00
- **Updated**: 2019-09-04 08:54:39+00:00
- **Authors**: Yue Zhao, Hong Zhu, Ruigang Liang, Qintao Shen, Shengzhi Zhang, Kai Chen
- **Comment**: 13 pages, 12 figures
- **Journal**: None
- **Summary**: In this paper, we presented systematic solutions to build robust and practical AEs against real world object detectors. Particularly, for Hiding Attack (HA), we proposed the feature-interference reinforcement (FIR) method and the enhanced realistic constraints generation (ERG) to enhance robustness, and for Appearing Attack (AA), we proposed the nested-AE, which combines two AEs together to attack object detectors in both long and short distance. We also designed diverse styles of AEs to make AA more surreptitious. Evaluation results show that our AEs can attack the state-of-the-art real-time object detectors (i.e., YOLO V3 and faster-RCNN) at the success rate up to 92.4% with varying distance from 1m to 25m and angles from -60{\deg} to 60{\deg}. Our AEs are also demonstrated to be highly transferable, capable of attacking another three state-of-the-art black-box models with high success rate.



### 3D PersonVLAD: Learning Deep Global Representations for Video-based Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1812.10222v3
- **DOI**: 10.1109/TNNLS.2019.2891244
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.10222v3)
- **Published**: 2018-12-26 04:51:55+00:00
- **Updated**: 2019-02-06 02:51:47+00:00
- **Authors**: Lin Wu, Yang Wang, Ling Shao, Meng Wang
- **Comment**: Accepted to appear at IEEE Transactions on Neural Networks and
  Learning Systems
- **Journal**: None
- **Summary**: In this paper, we introduce a global video representation to video-based person re-identification (re-ID) that aggregates local 3D features across the entire video extent. Most of the existing methods rely on 2D convolutional networks (ConvNets) to extract frame-wise deep features which are pooled temporally to generate the video-level representations. However, 2D ConvNets lose temporal input information immediately after the convolution, and a separate temporal pooling is limited in capturing human motion in shorter sequences. To this end, we present a \textit{global} video representation (3D PersonVLAD), complementary to 3D ConvNets as a novel layer to capture the appearance and motion dynamics in full-length videos. However, encoding each video frame in its entirety and computing an aggregate global representation across all frames is tremendously challenging due to occlusions and misalignments. To resolve this, our proposed network is further augmented with 3D part alignment module to learn local features through soft-attention module. These attended features are statistically aggregated to yield identity-discriminative representations. Our global 3D features are demonstrated to achieve state-of-the-art results on three benchmark datasets: MARS \cite{MARS}, iLIDS-VID \cite{VideoRanking}, and PRID 2011



### Studying the Plasticity in Deep Convolutional Neural Networks using Random Pruning
- **Arxiv ID**: http://arxiv.org/abs/1812.10240v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1812.10240v1)
- **Published**: 2018-12-26 06:26:06+00:00
- **Updated**: 2018-12-26 06:26:06+00:00
- **Authors**: Deepak Mittal, Shweta Bhardwaj, Mitesh M. Khapra, Balaraman Ravindran
- **Comment**: To appear in the Journal of Machine Vision and Applications,
  Springer. This work is an extended version of our previous work
  arXiv:1801.10447, "Recovering from Random Pruning: On the Plasticity of Deep
  Convolutional Neural Networks", accepted at WACV 2018
- **Journal**: None
- **Summary**: Recently there has been a lot of work on pruning filters from deep convolutional neural networks (CNNs) with the intention of reducing computations.The key idea is to rank the filters based on a certain criterion (say, l1-norm) and retain only the top ranked filters. Once the low scoring filters are pruned away the remainder of the network is fine tuned and is shown to give performance comparable to the original unpruned network. In this work, we report experiments which suggest that the comparable performance of the pruned network is not due to the specific criterion chosen but due to the inherent plasticity of deep neural networks which allows them to recover from the loss of pruned filters once the rest of the filters are fine-tuned. Specifically we show counter-intuitive results wherein by randomly pruning 25-50% filters from deep CNNs we are able to obtain the same performance as obtained by using state-of-the-art pruning methods. We empirically validate our claims by doing an exhaustive evaluation with VGG-16 and ResNet-50. We also evaluate a real world scenario where a CNN trained on all 1000 ImageNet classes needs to be tested on only a small set of classes at test time (say, only animals). We create a new benchmark dataset from ImageNet to evaluate such class specific pruning and show that even here a random pruning strategy gives close to state-of-the-art performance. Unlike existing approaches which mainly focus on the task of image classification, in this work we also report results on object detection and image segmentation. We show that using a simple random pruning strategy we can achieve significant speed up in object detection (74% improvement in fps) while retaining the same accuracy as that of the original Faster RCNN model. Similarly we show that the performance of a pruned Segmentation Network (SegNet) is actually very similar to that of the original unpruned SegNet.



### Multi-resolution neural networks for tracking seismic horizons from few training images
- **Arxiv ID**: http://arxiv.org/abs/1812.11092v1
- **DOI**: None
- **Categories**: **physics.geo-ph**, cs.CV, cs.LG, stat.ML, 68T45 (Primary)
- **Links**: [PDF](http://arxiv.org/pdf/1812.11092v1)
- **Published**: 2018-12-26 07:35:24+00:00
- **Updated**: 2018-12-26 07:35:24+00:00
- **Authors**: Bas Peters, Justin Granek, Eldad Haber
- **Comment**: 24 pages, 13 figures
- **Journal**: None
- **Summary**: Detecting a specific horizon in seismic images is a valuable tool for geological interpretation. Because hand-picking the locations of the horizon is a time-consuming process, automated computational methods were developed starting three decades ago. Older techniques for such picking include interpolation of control points however, in recent years neural networks have been used for this task. Until now, most networks trained on small patches from larger images. This limits the networks ability to learn from large-scale geologic structures. Moreover, currently available networks and training strategies require label patches that have full and continuous annotations, which are also time-consuming to generate.   We propose a projected loss-function for training convolutional networks with a multi-resolution structure, including variants of the U-net. Our networks learn from a small number of large seismic images without creating patches. The projected loss-function enables training on labels with just a few annotated pixels and has no issue with the other unknown label pixels. Training uses all data without reserving some for validation. Only the labels are split into training/testing. Contrary to other work on horizon tracking, we train the network to perform non-linear regression, and not classification. As such, we propose labels as the convolution of a Gaussian kernel and the known horizon locations that indicate uncertainty in the labels. The network output is the probability of the horizon location. We demonstrate the proposed computational ingredients on two different datasets, for horizon extrapolation and interpolation. We show that the predictions of our methodology are accurate even in areas far from known horizon locations because our learning strategy exploits all data in large seismic images.



### A Whole Slide Image Grading Benchmark and Tissue Classification for Cervical Cancer Precursor Lesions with Inter-Observer Variability
- **Arxiv ID**: http://arxiv.org/abs/1812.10256v1
- **DOI**: 10.1007/s11517-021-02388-w
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.10256v1)
- **Published**: 2018-12-26 07:40:37+00:00
- **Updated**: 2018-12-26 07:40:37+00:00
- **Authors**: Abdulkadir Albayrak, Asli Unlu, Nurullah Calik, Abdulkerim Capar, Gokhan Bilgin, Behcet Ugur Toreyin, Bahar Muezzinoglu, Ilknur Turkmen, Lutfiye Durak-Ata
- **Comment**: None
- **Journal**: None
- **Summary**: The cervical cancer developing from the precancerous lesions caused by the Human Papilloma Virus (HPV) has been one of the preventable cancers with the help of periodic screening. There are two types of grading conventions widely accepted among pathologists. On the other hand, inter-observer variability is an important issue for final diagnosis. In this paper, a whole-slide image grading benchmark for cervical cancer precursor lesions is introduced. The papillae of the cervical epithelium and overlapping cell problems are handled and a tissue classification method with a novel morphological feature exploiting the relative orientation between the BM and the major axis of all nuclei is developed and its performance is evaluated. Besides, the inter-observer variability is also revealed by a thorough comparison among pathologists' decisions, as well as, the final diagnosis.



### A Survey of Deep Facial Attribute Analysis
- **Arxiv ID**: http://arxiv.org/abs/1812.10265v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.10265v3)
- **Published**: 2018-12-26 09:24:07+00:00
- **Updated**: 2019-10-27 03:13:51+00:00
- **Authors**: Xin Zheng, Yanqing Guo, Huaibo Huang, Yi Li, Ran He
- **Comment**: submitted to International Journal of Computer Vision (IJCV)
- **Journal**: None
- **Summary**: Facial attribute analysis has received considerable attention when deep learning techniques made remarkable breakthroughs in this field over the past few years. Deep learning based facial attribute analysis consists of two basic sub-issues: facial attribute estimation (FAE), which recognizes whether facial attributes are present in given images, and facial attribute manipulation (FAM), which synthesizes or removes desired facial attributes. In this paper, we provide a comprehensive survey of deep facial attribute analysis from the perspectives of both estimation and manipulation. First, we summarize a general pipeline that deep facial attribute analysis follows, which comprises two stages: data preprocessing and model construction. Additionally, we introduce the underlying theories of this two-stage pipeline for both FAE and FAM. Second, the datasets and performance metrics commonly used in facial attribute analysis are presented. Third, we create a taxonomy of state-of-the-art methods and review deep FAE and FAM algorithms in detail. Furthermore, several additional facial attribute related issues are introduced, as well as relevant real-world applications. Finally, we discuss possible challenges and promising future research directions.



### Hierarchical LSTMs with Adaptive Attention for Visual Captioning
- **Arxiv ID**: http://arxiv.org/abs/1812.11004v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.11004v1)
- **Published**: 2018-12-26 09:51:58+00:00
- **Updated**: 2018-12-26 09:51:58+00:00
- **Authors**: Jingkuan Song, Xiangpeng Li, Lianli Gao, Heng Tao Shen
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1706.01231
- **Journal**: None
- **Summary**: Recent progress has been made in using attention based encoder-decoder framework for image and video captioning. Most existing decoders apply the attention mechanism to every generated word including both visual words (e.g., "gun" and "shooting") and non-visual words (e.g. "the", "a"). However, these non-visual words can be easily predicted using natural language model without considering visual signals or attention. Imposing attention mechanism on non-visual words could mislead and decrease the overall performance of visual captioning. Furthermore, the hierarchy of LSTMs enables more complex representation of visual data, capturing information at different scales. To address these issues, we propose a hierarchical LSTM with adaptive attention (hLSTMat) approach for image and video captioning. Specifically, the proposed framework utilizes the spatial or temporal attention for selecting specific regions or frames to predict the related words, while the adaptive attention is for deciding whether to depend on the visual information or the language context information. Also, a hierarchical LSTMs is designed to simultaneously consider both low-level visual information and high-level language context information to support the caption generation. We initially design our hLSTMat for video captioning task. Then, we further refine it and apply it to image captioning task. To demonstrate the effectiveness of our proposed framework, we test our method on both video and image captioning tasks. Experimental results show that our approach achieves the state-of-the-art performance for most of the evaluation metrics on both tasks. The effect of important components is also well exploited in the ablation study.



### Spatial and Temporal Mutual Promotion for Video-based Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1812.10305v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.10305v1)
- **Published**: 2018-12-26 13:24:31+00:00
- **Updated**: 2018-12-26 13:24:31+00:00
- **Authors**: Yiheng Liu, Zhenxun Yuan, Wengang Zhou, Houqiang Li
- **Comment**: Accepted by AAAI19 as spotlight
- **Journal**: None
- **Summary**: Video-based person re-identification is a crucial task of matching video sequences of a person across multiple camera views. Generally, features directly extracted from a single frame suffer from occlusion, blur, illumination and posture changes. This leads to false activation or missing activation in some regions, which corrupts the appearance and motion representation. How to explore the abundant spatial-temporal information in video sequences is the key to solve this problem. To this end, we propose a Refining Recurrent Unit (RRU) that recovers the missing parts and suppresses noisy parts of the current frame's features by referring historical frames. With RRU, the quality of each frame's appearance representation is improved. Then we use the Spatial-Temporal clues Integration Module (STIM) to mine the spatial-temporal information from those upgraded features. Meanwhile, the multi-level training objective is used to enhance the capability of RRU and STIM. Through the cooperation of those modules, the spatial and temporal features mutually promote each other and the final spatial-temporal feature representation is more discriminative and robust. Extensive experiments are conducted on three challenging datasets, i.e., iLIDS-VID, PRID-2011 and MARS. The experimental results demonstrate that our approach outperforms existing state-of-the-art methods of video-based person re-identification on iLIDS-VID and MARS and achieves favorable results on PRID-2011.



### Spotting Micro-Expressions on Long Videos Sequences
- **Arxiv ID**: http://arxiv.org/abs/1812.10306v2
- **DOI**: 10.1109/FG.2019.8756626
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.10306v2)
- **Published**: 2018-12-26 13:25:46+00:00
- **Updated**: 2019-07-13 09:13:34+00:00
- **Authors**: Jingting Li, Catherine Soladie, Renaud Sguier, Sujing Wang, Moi Hoon Yap
- **Comment**: 4 pages, 3 figures and 3 tables
- **Journal**: The 14th IEEE International Conference on Automatic Face & Gesture
  Recognition (FG 2019)
- **Summary**: This paper presents baseline results for the first Micro-Expression Spotting Challenge 2019 by evaluating local temporal pattern (LTP) on SAMM and CAS(ME)2. The proposed LTP patterns are extracted by applying PCA in a temporal window on several facial local regions. The micro-expression sequences are then spotted by a local classification of LTP and a global fusion. The performance is evaluated by Leave-One-Subject-Out cross validation. Furthermore, we define the criteria of determining true positives in one video by overlap rate and set the metric F1-score for spotting performance of the whole database. The F1-score of baseline results for SAMM and CAS(ME)2 are 0.0316 and 0.0179, respectively.



### Structure-Aware 3D Hourglass Network for Hand Pose Estimation from Single Depth Image
- **Arxiv ID**: http://arxiv.org/abs/1812.10320v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.10320v1)
- **Published**: 2018-12-26 14:08:08+00:00
- **Updated**: 2018-12-26 14:08:08+00:00
- **Authors**: Fuyang Huang, Ailing Zeng, Minhao Liu, Jing Qin, Qiang Xu
- **Comment**: BMVC 2018
- **Journal**: None
- **Summary**: In this paper, we propose a novel structure-aware 3D hourglass network for hand pose estimation from a single depth image, which achieves state-of-the-art results on MSRA and NYU datasets. Compared to existing works that perform image-to-coordination regression, our network takes 3D voxel as input and directly regresses 3D heatmap for each joint. To be specific, we use hourglass network as our backbone network and modify it into 3D form. We explicitly model tree-like finger bone into the network as well as in the loss function in an end-to-end manner, in order to take the skeleton constraints into consideration. Final estimation can then be easily obtained from voxel density map with simple post-processing. Experimental results show that the proposed structure-aware 3D hourglass network is able to achieve a mean joint error of 7.4 mm in MSRA and 8.9 mm in NYU datasets, respectively.



### A Multi-Stream Convolutional Neural Network Framework for Group Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/1812.10328v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.10328v1)
- **Published**: 2018-12-26 14:30:46+00:00
- **Updated**: 2018-12-26 14:30:46+00:00
- **Authors**: Sina Mokhtarzadeh Azar, Mina Ghadimi Atigh, Ahmad Nickabadi
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we present a framework based on multi-stream convolutional neural networks (CNNs) for group activity recognition. Streams of CNNs are separately trained on different modalities and their predictions are fused at the end. Each stream has two branches to predict the group activity based on person and scene level representations. A new modality based on the human pose estimation is presented to add extra information to the model. We evaluate our method on the Volleyball and Collective Activity datasets. Experimental results show that the proposed framework is able to achieve state-of-the-art results when multiple or single frames are given as input to the model with 90.50% and 86.61% accuracy on Volleyball dataset, respectively, and 87.01% accuracy of multiple frames group activity on Collective Activity dataset.



### Region Proposal Networks with Contextual Selective Attention for Real-Time Organ Detection
- **Arxiv ID**: http://arxiv.org/abs/1812.10330v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.10330v1)
- **Published**: 2018-12-26 14:33:59+00:00
- **Updated**: 2018-12-26 14:33:59+00:00
- **Authors**: Awais Mansoor, Antonio R. Porras, Marius George Linguraru
- **Comment**: Accepted at the IEEE International Symposium on Biomedical Imaging
  (ISBI) 2019, Venice-Italy
- **Journal**: None
- **Summary**: State-of-the-art methods for object detection use region proposal networks (RPN) to hypothesize object location. These networks simultaneously predicts object bounding boxes and \emph{objectness} scores at each location in the image. Unlike natural images for which RPN algorithms were originally designed, most medical images are acquired following standard protocols, thus organs in the image are typically at a similar location and possess similar geometrical characteristics (e.g. scale, aspect-ratio, etc.). Therefore, medical image acquisition protocols hold critical localization and geometric information that can be incorporated for faster and more accurate detection. This paper presents a novel attention mechanism for the detection of organs by incorporating imaging protocol information. Our novel selective attention approach (i) effectively shrinks the search space inside the feature map, (ii) appends useful localization information to the hypothesized proposal for the detection architecture to learn where to look for each organ, and (iii) modifies the pyramid of regression references in the RPN by incorporating organ- and modality-specific information, which results in additional time reduction. We evaluated the proposed framework on a dataset of 768 chest X-ray images obtained from a diverse set of sources. Our results demonstrate superior performance for the detection of the lung field compared to the state-of-the-art, both in terms of detection accuracy, demonstrating an improvement of $>7\%$ in Dice score, and reduced processing time by $27.53\%$ due to fewer hypotheses.



### Learning Not to Learn: Training Deep Neural Networks with Biased Data
- **Arxiv ID**: http://arxiv.org/abs/1812.10352v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.10352v2)
- **Published**: 2018-12-26 16:01:29+00:00
- **Updated**: 2019-04-15 08:42:54+00:00
- **Authors**: Byungju Kim, Hyunwoo Kim, Kyungsu Kim, Sungjin Kim, Junmo Kim
- **Comment**: CVPR 2019, Accepted
- **Journal**: None
- **Summary**: We propose a novel regularization algorithm to train deep neural networks, in which data at training time is severely biased. Since a neural network efficiently learns data distribution, a network is likely to learn the bias information to categorize input data. It leads to poor performance at test time, if the bias is, in fact, irrelevant to the categorization. In this paper, we formulate a regularization loss based on mutual information between feature embedding and bias. Based on the idea of minimizing this mutual information, we propose an iterative algorithm to unlearn the bias information. We employ an additional network to predict the bias distribution and train the network adversarially against the feature embedding network. At the end of learning, the bias prediction network is not able to predict the bias not because it is poorly trained, but because the feature embedding network successfully unlearns the bias information. We also demonstrate quantitative and qualitative experimental results which show that our algorithm effectively removes the bias information from feature embedding.



### Informative Object Annotations: Tell Me Something I Don't Know
- **Arxiv ID**: http://arxiv.org/abs/1812.10358v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.10358v1)
- **Published**: 2018-12-26 16:12:30+00:00
- **Updated**: 2018-12-26 16:12:30+00:00
- **Authors**: Lior Bracha, Gal Chechik
- **Comment**: None
- **Journal**: None
- **Summary**: Capturing the interesting components of an image is a key aspect of image understanding. When a speaker annotates an image, selecting labels that are informative greatly depends on the prior knowledge of a prospective listener. Motivated by cognitive theories of categorization and communication, we present a new unsupervised approach to model this prior knowledge and quantify the informativeness of a description. Specifically, we compute how knowledge of a label reduces uncertainty over the space of labels and utilize this to rank candidate labels for describing an image. While the full estimation problem is intractable, we describe an efficient algorithm to approximate entropy reduction using a tree-structured graphical model. We evaluate our approach on the open-images dataset using a new evaluation set of 10K ground-truth ratings and find that it achieves ~65% agreement with human raters, largely outperforming other unsupervised baseline approaches.



### A Poisson-Gaussian Denoising Dataset with Real Fluorescence Microscopy Images
- **Arxiv ID**: http://arxiv.org/abs/1812.10366v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.10366v2)
- **Published**: 2018-12-26 16:42:02+00:00
- **Updated**: 2019-04-05 22:26:25+00:00
- **Authors**: Yide Zhang, Yinhao Zhu, Evan Nichols, Qingfei Wang, Siyuan Zhang, Cody Smith, Scott Howard
- **Comment**: Camera-ready version for CVPR 2019. The Fluorescence Microscopy
  Denoising (FMD) dataset is available at
  https://drive.google.com/drive/folders/1aygMzSDdoq63IqSk-ly8cMq0_owup8UM
- **Journal**: None
- **Summary**: Fluorescence microscopy has enabled a dramatic development in modern biology. Due to its inherently weak signal, fluorescence microscopy is not only much noisier than photography, but also presented with Poisson-Gaussian noise where Poisson noise, or shot noise, is the dominating noise source. To get clean fluorescence microscopy images, it is highly desirable to have effective denoising algorithms and datasets that are specifically designed to denoise fluorescence microscopy images. While such algorithms exist, no such datasets are available. In this paper, we fill this gap by constructing a dataset - the Fluorescence Microscopy Denoising (FMD) dataset - that is dedicated to Poisson-Gaussian denoising. The dataset consists of 12,000 real fluorescence microscopy images obtained with commercial confocal, two-photon, and wide-field microscopes and representative biological samples such as cells, zebrafish, and mouse brain tissues. We use image averaging to effectively obtain ground truth images and 60,000 noisy images with different noise levels. We use this dataset to benchmark 10 representative denoising algorithms and find that deep learning methods have the best performance. To our knowledge, this is the first real microscopy image dataset for Poisson-Gaussian denoising purposes and it could be an important tool for high-quality, real-time denoising applications in biomedical research.



### Exploring the Challenges towards Lifelong Fact Learning
- **Arxiv ID**: http://arxiv.org/abs/1812.10524v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.10524v1)
- **Published**: 2018-12-26 20:09:49+00:00
- **Updated**: 2018-12-26 20:09:49+00:00
- **Authors**: Mohamed Elhoseiny, Francesca Babiloni, Rahaf Aljundi, Marcus Rohrbach, Manohar Paluri, Tinne Tuytelaars
- **Comment**: This work got published at ACCV 2018 as a main conference paper
- **Journal**: None
- **Summary**: So far life-long learning (LLL) has been studied in relatively small-scale and relatively artificial setups. Here, we introduce a new large-scale alternative. What makes the proposed setup more natural and closer to human-like visual systems is threefold: First, we focus on concepts (or facts, as we call them) of varying complexity, ranging from single objects to more complex structures such as objects performing actions, and objects interacting with other objects. Second, as in real-world settings, our setup has a long-tail distribution, an aspect which has mostly been ignored in the LLL context. Third, facts across tasks may share structure (e.g., <person, riding, wave> and <dog, riding, wave>). Facts can also be semantically related (e.g., "liger" relates to seen categories like "tiger" and "lion"). Given the large number of possible facts, a LLL setup seems a natural choice. To avoid model size growing over time and to optimally exploit the semantic relations and structure, we combine it with a visual semantic embedding instead of discrete class labels. We adapt existing datasets with the properties mentioned above into new benchmarks, by dividing them semantically or randomly into disjoint tasks. This leads to two large-scale benchmarks with 906,232 images and 165,150 unique facts, on which we evaluate and analyze state-of-the-art LLL methods.



### A Unified Learning Based Framework for Light Field Reconstruction from Coded Projections
- **Arxiv ID**: http://arxiv.org/abs/1812.10532v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.10532v2)
- **Published**: 2018-12-26 20:57:43+00:00
- **Updated**: 2019-10-18 21:35:45+00:00
- **Authors**: Anil Kumar Vadathya, Sharath Girish, Kaushik Mitra
- **Comment**: None
- **Journal**: None
- **Summary**: Light field presents a rich way to represent the 3D world by capturing the spatio-angular dimensions of the visual signal. However, the popular way of capturing light field (LF) via a plenoptic camera presents spatio-angular resolution trade-off. Computational imaging techniques such as compressive light field and programmable coded aperture reconstruct full sensor resolution LF from coded projections obtained by multiplexing the incoming spatio-angular light field. Here, we present a unified learning framework that can reconstruct LF from a variety of multiplexing schemes with minimal number of coded images as input. We consider three light field capture schemes: heterodyne capture scheme with code placed near the sensor, coded aperture scheme with code at the camera aperture and finally the dual exposure scheme of capturing a focus-defocus pair where there is no explicit coding. Our algorithm consists of three stages 1) we recover the all-in-focus image from the coded image 2) we estimate the disparity maps for all the LF views from the coded image and the all-in-focus image, 3) we then render the LF by warping the all-in-focus image using disparity maps and refine it. For these three stages we propose three deep neural networks - ViewNet, DispairtyNet and RefineNet. Our reconstructions show that our learning algorithm achieves state-of-the-art results for all the three multiplexing schemes. Especially, our LF reconstructions from focus-defocus pair is comparable to other learning-based view synthesis approaches from multiple images. Thus, our work paves the way for capturing high-resolution LF (~ a megapixel) using conventional cameras such as DSLRs. Please check our supplementary materials $\href{https://docs.google.com/presentation/d/1Vr-F8ZskrSd63tvnLfJ2xmEXY6OBc1Rll3XeOAtc11I/}{online}$ to better appreciate the reconstructed light fields.



### Learning to Recognize 3D Human Action from A New Skeleton-based Representation Using Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1812.10550v1
- **DOI**: 10.1049/iet-cvi.2018.5014
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.10550v1)
- **Published**: 2018-12-26 21:47:08+00:00
- **Updated**: 2018-12-26 21:47:08+00:00
- **Authors**: Huy-Hieu Pham, Louahdi Khoudour, Alain Crouzil, Pablo Zegers, Sergio A. Velastin
- **Comment**: This paper is a preprint of a paper published to IET Computer Vision.
  The copy of the record will be available at the IET Digital Library
- **Journal**: None
- **Summary**: Recognizing human actions in untrimmed videos is an important challenging task. An effective 3D motion representation and a powerful learning model are two key factors influencing recognition performance. In this paper we introduce a new skeleton-based representation for 3D action recognition in videos. The key idea of the proposed representation is to transform 3D joint coordinates of the human body carried in skeleton sequences into RGB images via a color encoding process. By normalizing the 3D joint coordinates and dividing each skeleton frame into five parts, where the joints are concatenated according to the order of their physical connections, the color-coded representation is able to represent spatio-temporal evolutions of complex 3D motions, independently of the length of each sequence. We then design and train different Deep Convolutional Neural Networks (D-CNNs) based on the Residual Network architecture (ResNet) on the obtained image-based representations to learn 3D motion features and classify them into classes. Our method is evaluated on two widely used action recognition benchmarks: MSR Action3D and NTU-RGB+D, a very large-scale dataset for 3D human action recognition. The experimental results demonstrate that the proposed method outperforms previous state-of-the-art approaches whilst requiring less computation for training and prediction.



### Solving Archaeological Puzzles
- **Arxiv ID**: http://arxiv.org/abs/1812.10553v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.10553v1)
- **Published**: 2018-12-26 21:57:55+00:00
- **Updated**: 2018-12-26 21:57:55+00:00
- **Authors**: Niv Derech, Ayellet Tal, Ilan Shimshoni
- **Comment**: None
- **Journal**: None
- **Summary**: Puzzle solving is a difficult problem in its own right, even when the pieces are all square and build up a natural image. But what if these ideal conditions do not hold? One such application domain is archaeology, where restoring an artifact from its fragments is highly important. From the point of view of computer vision, archaeological puzzle solving is very challenging, due to three additional difficulties: the fragments are of general shape; they are abraded, especially at the boundaries (where the strongest cues for matching should exist); and the domain of valid transformations between the pieces is continuous. The key contribution of this paper is a fully-automatic and general algorithm that addresses puzzle solving in this intriguing domain. We show that our state-of-the-art approach manages to correctly reassemble dozens of broken artifacts and frescoes.



### Deception Detection by 2D-to-3D Face Reconstruction from Videos
- **Arxiv ID**: http://arxiv.org/abs/1812.10558v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.10558v1)
- **Published**: 2018-12-26 22:11:52+00:00
- **Updated**: 2018-12-26 22:11:52+00:00
- **Authors**: Minh Ng√¥, Burak Mandira, Selim Fƒ±rat Yƒ±lmaz, Ward Heij, Sezer Karaoglu, Henri Bouma, Hamdi Dibeklioglu, Theo Gevers
- **Comment**: 9 pages, 3 figures
- **Journal**: None
- **Summary**: Lies and deception are common phenomena in society, both in our private and professional lives. However, humans are notoriously bad at accurate deception detection. Based on the literature, human accuracy of distinguishing between lies and truthful statements is 54% on average, in other words it is slightly better than a random guess. While people do not much care about this issue, in high-stakes situations such as interrogations for series crimes and for evaluating the testimonies in court cases, accurate deception detection methods are highly desirable. To achieve a reliable, covert, and non-invasive deception detection, we propose a novel method that jointly extracts reliable low- and high-level facial features namely, 3D facial geometry, skin reflectance, expression, head pose, and scene illumination in a video sequence. Then these features are modeled using a Recurrent Neural Network to learn temporal characteristics of deceptive and honest behavior. We evaluate the proposed method on the Real-Life Trial (RLT) dataset that contains high-stake deceptive and honest videos recorded in courtrooms. Our results show that the proposed method (with an accuracy of 72.8%) improves the state of the art as well as outperforming the use of manually coded facial attributes 67.6%) in deception detection.



