# Arxiv Papers in cs.CV on 2018-12-27
### Learning Dynamic Generator Model by Alternating Back-Propagation Through Time
- **Arxiv ID**: http://arxiv.org/abs/1812.10587v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.10587v1)
- **Published**: 2018-12-27 01:34:08+00:00
- **Updated**: 2018-12-27 01:34:08+00:00
- **Authors**: Jianwen Xie, Ruiqi Gao, Zilong Zheng, Song-Chun Zhu, Ying Nian Wu
- **Comment**: 10 pages
- **Journal**: The Thirty-Third AAAI Conference on Artificial Intelligence (AAAI)
  2019
- **Summary**: This paper studies the dynamic generator model for spatial-temporal processes such as dynamic textures and action sequences in video data. In this model, each time frame of the video sequence is generated by a generator model, which is a non-linear transformation of a latent state vector, where the non-linear transformation is parametrized by a top-down neural network. The sequence of latent state vectors follows a non-linear auto-regressive model, where the state vector of the next frame is a non-linear transformation of the state vector of the current frame as well as an independent noise vector that provides randomness in the transition. The non-linear transformation of this transition model can be parametrized by a feedforward neural network. We show that this model can be learned by an alternating back-propagation through time algorithm that iteratively samples the noise vectors and updates the parameters in the transition model and the generator model. We show that our training method can learn realistic models for dynamic textures and action patterns.



### Eyes on the Prize: Improved Biological Surface Registration via Forward Propagation
- **Arxiv ID**: http://arxiv.org/abs/1812.10592v2
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV, I.3.5; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/1812.10592v2)
- **Published**: 2018-12-27 02:11:36+00:00
- **Updated**: 2020-12-14 17:55:18+00:00
- **Authors**: Robert J. Ravier
- **Comment**: 20 pages, 11 figures, 1 table
- **Journal**: None
- **Summary**: Many algorithms for surface registration risk producing significant errors if surfaces are significantly nonisometric. Manifold learning has been shown to be effective at improving registration quality, using information from an entire collection of surfaces to correct issues present in pairwise registrations. These methods, however, are not robust to changes in the collection of surfaces, or do not produce accurate registrations at a resolution high enough for subsequent downstream analysis. We propose a novel algorithm for efficiently registering such collections given initial correspondences with varying degrees of accuracy. By combining the initial information with recent developments in manifold learning, we employ a simple metric condition to construct a measure on the space of correspondences between any pair of shapes in our collection, which we then use to distill soft correspondences. We demonstrate that this measure can improve correspondence accuracy between feature points compared to currently employed, less robust methods on a diverse dataset of surfaces from evolutionary biology. We then show how our methods can be used, in combination with recent sampling and interpolation methods, to compute accurate and consistent homeomorphisms between surfaces.



### Deep Learning based Early Detection and Grading of Diabetic Retinopathy Using Retinal Fundus Images
- **Arxiv ID**: http://arxiv.org/abs/1812.10595v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.10595v1)
- **Published**: 2018-12-27 02:24:24+00:00
- **Updated**: 2018-12-27 02:24:24+00:00
- **Authors**: Sheikh Muhammad Saiful Islam, Md Mahedi Hasan, Sohaib Abdullah
- **Comment**: Accepted in MIND 2019
- **Journal**: None
- **Summary**: Diabetic Retinopathy (DR) is a constantly deteriorating disease, being one of the leading causes of vision impairment and blindness. Subtle distinction among different grades and existence of many significant small features make the task of recognition very challenging. In addition, the present approach of retinopathy detection is a very laborious and time-intensive task, which heavily relies on the skill of a physician. Automated detection of diabetic retinopathy is essential to tackle these problems. Early-stage detection of diabetic retinopathy is also very important for diagnosis, which can prevent blindness with proper treatment. In this paper, we developed a novel deep convolutional neural network, which performs the early-stage detection by identifying all microaneurysms (MAs), the first signs of DR, along with correctly assigning labels to retinal fundus images which are graded into five categories. We have tested our network on the largest publicly available Kaggle diabetic retinopathy dataset, and achieved 0.851 quadratic weighted kappa score and 0.844 AUC score, which achieves the state-of-the-art performance on severity grading. In the early-stage detection, we have achieved a sensitivity of 98% and specificity of above 94%, which demonstrates the effectiveness of our proposed method. Our proposed architecture is at the same time very simple and efficient with respect to computational time and space are concerned.



### Low-Cost Device Prototype for Automatic Medical Diagnosis Using Deep Learning Methods
- **Arxiv ID**: http://arxiv.org/abs/1901.00751v2
- **DOI**: None
- **Categories**: **cs.CY**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.00751v2)
- **Published**: 2018-12-27 03:27:56+00:00
- **Updated**: 2019-01-04 01:59:35+00:00
- **Authors**: Neil Deshmukh
- **Comment**: Best Machine Learning Paper at the 9th IEEE Columbia Ubiquitous
  Computing, Electronics & Mobile Communication Conference (UEMCON) and
  presented at the IEEE MIT Undergraduate Research Technology Conference (URTC)
- **Journal**: None
- **Summary**: This paper introduces a novel low-cost device prototype for the automatic diagnosis of diseases, utilizing inputted symptoms and personal background. The engineering goal is to solve the problem of limited healthcare access with a single device. Diagnosing diseases automatically is an immense challenge, owing to their variable properties and symptoms. On the other hand, Neural Networks have developed into a powerful tool in the field of machine learning, one that is showing to be extremely promising at computing diagnosis even with inconsistent variables.   In this research, a cheap device was created to allow for straightforward diagnosis and treatment of human diseases. By utilizing Deep Neural Networks (DNNs) and Convolutional Neural Networks (CNNs), outfitted on a Raspberry Pi Zero processor ($5), the device is able to detect up to 1537 different diseases and conditions and utilize a CNN for on-device visual diagnostics. The user can input the symptoms using the buttons on the device and can take pictures using the same mechanism. The algorithm processes inputted symptoms, providing diagnosis and possible treatment options for common conditions. The purpose of this work was to be able to diagnose diseases through an affordable processor with high accuracy, as it is currently achieving an accuracy of 90% for Top-5 symptom-based diagnoses, and 91% for visual skin diseases. The NNs achieve performance far above any other tested system, and its efficiency and ease of use will prove it to be a helpful tool for people around the world. This device could potentially provide low-cost universal access to vital diagnostics and treatment options.



### Chart-Text: A Fully Automated Chart Image Descriptor
- **Arxiv ID**: http://arxiv.org/abs/1812.10636v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.10636v1)
- **Published**: 2018-12-27 05:52:06+00:00
- **Updated**: 2018-12-27 05:52:06+00:00
- **Authors**: Abhijit Balaji, Thuvaarakkesh Ramanathan, Venkateshwarlu Sonathi
- **Comment**: None
- **Journal**: None
- **Summary**: Images greatly help in understanding, interpreting and visualizing data. Adding textual description to images is the first and foremost principle of web accessibility. Visually impaired users using screen readers will use these textual descriptions to get better understanding of images present in digital contents. In this paper, we propose Chart-Text a novel fully automated system that creates textual description of chart images. Given a PNG image of a chart, our Chart-Text system creates a complete textual description of it. First, the system classifies the type of chart and then it detects and classifies the labels and texts in the charts. Finally, it uses specific image processing algorithms to extract relevant information from the chart images. Our proposed system achieves an accuracy of 99.72% in classifying the charts and an accuracy of 78.9% in extracting the data and creating the corresponding textual description.



### No-Reference Color Image Quality Assessment: From Entropy to Perceptual Quality
- **Arxiv ID**: http://arxiv.org/abs/1812.10695v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.10695v1)
- **Published**: 2018-12-27 11:01:29+00:00
- **Updated**: 2018-12-27 11:01:29+00:00
- **Authors**: Xiaoqiao Chen, Qingyi Zhang, Manhui Lin, Guangyi Yang, Chu He
- **Comment**: 12 pages, 8 figures
- **Journal**: None
- **Summary**: This paper presents a high-performance general-purpose no-reference (NR) image quality assessment (IQA) method based on image entropy. The image features are extracted from two domains. In the spatial domain, the mutual information between the color channels and the two-dimensional entropy are calculated. In the frequency domain, the two-dimensional entropy and the mutual information of the filtered sub-band images are computed as the feature set of the input color image. Then, with all the extracted features, the support vector classifier (SVC) for distortion classification and support vector regression (SVR) are utilized for the quality prediction, to obtain the final quality assessment score. The proposed method, which we call entropy-based no-reference image quality assessment (ENIQA), can assess the quality of different categories of distorted images, and has a low complexity. The proposed ENIQA method was assessed on the LIVE and TID2013 databases and showed a superior performance. The experimental results confirmed that the proposed ENIQA method has a high consistency of objective and subjective assessment on color images, which indicates the good overall performance and generalization ability of ENIQA. The source code is available on github https://github.com/jacob6/ENIQA.



### Surface Networks via General Covers
- **Arxiv ID**: http://arxiv.org/abs/1812.10705v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.10705v3)
- **Published**: 2018-12-27 12:18:59+00:00
- **Updated**: 2019-08-18 12:52:09+00:00
- **Authors**: Niv Haim, Nimrod Segol, Heli Ben-Hamu, Haggai Maron, Yaron Lipman
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: Developing deep learning techniques for geometric data is an active and fruitful research area. This paper tackles the problem of sphere-type surface learning by developing a novel surface-to-image representation. Using this representation we are able to quickly adapt successful CNN models to the surface setting.   The surface-image representation is based on a covering map from the image domain to the surface. Namely, the map wraps around the surface several times, making sure that every part of the surface is well represented in the image. Differently from previous surface-to-image representations, we provide a low distortion coverage of all surface parts in a single image. Specifically, for the use case of learning spherical signals, our representation provides a low distortion alternative to several popular spherical parameterizations used in deep learning.   We have used the surface-to-image representation to apply standard CNN architectures to 3D models as well as spherical signals. We show that our method achieves state of the art or comparable results on the tasks of shape retrieval, shape classification and semantic shape segmentation.



### S4-Net: Geometry-Consistent Semi-Supervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1812.10717v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.10717v2)
- **Published**: 2018-12-27 13:24:56+00:00
- **Updated**: 2019-01-09 13:48:44+00:00
- **Authors**: Sinisa Stekovic, Friedrich Fraundorfer, Vincent Lepetit
- **Comment**: 8 pages, 5 figures
- **Journal**: None
- **Summary**: We show that it is possible to learn semantic segmentation from very limited amounts of manual annotations, by enforcing geometric 3D constraints between multiple views. More exactly, image locations corresponding to the same physical 3D point should all have the same label. We show that introducing such constraints during learning is very effective, even when no manual label is available for a 3D point, and can be done simply by employing techniques from 'general' semi-supervised learning to the context of semantic segmentation. To demonstrate this idea, we use RGB-D image sequences of rigid scenes, for a 4-class segmentation problem derived from the ScanNet dataset. Starting from RGB-D sequences with a few annotated frames, we show that we can incorporate RGB-D sequences without any manual annotations to improve the performance, which makes our approach very convenient. Furthermore, we demonstrate our approach for semantic segmentation of objects on the LabelFusion dataset, where we show that one manually labeled image in a scene is sufficient for high performance on the whole scene.



### Finite State Machines for Semantic Scene Parsing and Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1812.10745v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.10745v1)
- **Published**: 2018-12-27 15:41:22+00:00
- **Updated**: 2018-12-27 15:41:22+00:00
- **Authors**: Hichem Sahbi
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce in this work a novel stochastic inference process, for scene annotation and object class segmentation, based on finite state machines (FSMs). The design principle of our framework is generative and based on building, for a given scene, finite state machines that encode annotation lattices, and inference consists in finding and scoring the best configurations in these lattices. Different novel operations are defined using our FSM framework including reordering, segmentation, visual transduction, and label dependency modeling. All these operations are combined together in order to achieve annotation as well as object class segmentation.



### Off-the-grid model based deep learning (O-MODL)
- **Arxiv ID**: http://arxiv.org/abs/1812.10747v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.10747v1)
- **Published**: 2018-12-27 15:48:10+00:00
- **Updated**: 2018-12-27 15:48:10+00:00
- **Authors**: Aniket Pramanik, Hemant Kumar Aggarwal, Mathews Jacob
- **Comment**: ISBI 2019
- **Journal**: None
- **Summary**: We introduce a model based off-the-grid image reconstruction algorithm using deep learned priors. The main difference of the proposed scheme with current deep learning strategies is the learning of non-linear annihilation relations in Fourier space. We rely on a model based framework, which allows us to use a significantly smaller deep network, compared to direct approaches that also learn how to invert the forward model. Preliminary comparisons against image domain MoDL approach demonstrates the potential of the off-the-grid formulation. The main benefit of the proposed scheme compared to structured low-rank methods is the quite significant reduction in computational complexity.



### SMPLR: Deep SMPL reverse for 3D human pose and shape recovery
- **Arxiv ID**: http://arxiv.org/abs/1812.10766v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.10766v2)
- **Published**: 2018-12-27 16:47:11+00:00
- **Updated**: 2019-08-08 10:56:36+00:00
- **Authors**: Meysam Madadi, Hugo Bertiche, Sergio Escalera
- **Comment**: None
- **Journal**: None
- **Summary**: Current state-of-the-art in 3D human pose and shape recovery relies on deep neural networks and statistical morphable body models, such as the Skinned Multi-Person Linear model (SMPL). However, regardless of the advantages of having both body pose and shape, SMPL-based solutions have shown difficulties to predict 3D bodies accurately. This is mainly due to the unconstrained nature of SMPL, which may generate unrealistic body meshes. Because of this, regression of SMPL parameters is a difficult task, often addressed with complex regularization terms. In this paper we propose to embed SMPL within a deep model to accurately estimate 3D pose and shape from a still RGB image. We use CNN-based 3D joint predictions as an intermediate representation to regress SMPL pose and shape parameters. Later, 3D joints are reconstructed again in the SMPL output. This module can be seen as an autoencoder where the encoder is a deep neural network and the decoder is SMPL model. We refer to this as SMPL reverse (SMPLR). By implementing SMPLR as an encoder-decoder we avoid the need of complex constraints on pose and shape. Furthermore, given that in-the-wild datasets usually lack accurate 3D annotations, it is desirable to lift 2D joints to 3D without pairing 3D annotations with RGB images. Therefore, we also propose a denoising autoencoder (DAE) module between CNN and SMPLR, able to lift 2D joints to 3D and partially recover from structured error. We evaluate our method on SURREAL and Human3.6M datasets, showing improvement over SMPL-based state-of-the-art alternatives by about 4 and 25 millimeters, respectively.



### 3D Point Capsule Networks
- **Arxiv ID**: http://arxiv.org/abs/1812.10775v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1812.10775v2)
- **Published**: 2018-12-27 17:16:48+00:00
- **Updated**: 2019-07-11 23:39:55+00:00
- **Authors**: Yongheng Zhao, Tolga Birdal, Haowen Deng, Federico Tombari
- **Comment**: As published in CVPR 2019 (camera ready version), with supplementary
  material
- **Journal**: None
- **Summary**: In this paper, we propose 3D point-capsule networks, an auto-encoder designed to process sparse 3D point clouds while preserving spatial arrangements of the input data. 3D capsule networks arise as a direct consequence of our novel unified 3D auto-encoder formulation. Their dynamic routing scheme and the peculiar 2D latent space deployed by our approach bring in improvements for several common point cloud-related tasks, such as object classification, object reconstruction and part segmentation as substantiated by our extensive evaluations. Moreover, it enables new applications such as part interpolation and replacement.



### Semantic Driven Multi-Camera Pedestrian Detection
- **Arxiv ID**: http://arxiv.org/abs/1812.10779v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.10779v3)
- **Published**: 2018-12-27 17:36:37+00:00
- **Updated**: 2022-04-07 10:14:58+00:00
- **Authors**: Alejandro López-Cifuentes, Marcos Escudero-Viñolo, Jesús Bescós, Pablo Carballeira
- **Comment**: Preprint accepted in Springer Knowledge and Information Systems
  (KAIS)
- **Journal**: None
- **Summary**: In the current worldwide situation, pedestrian detection has reemerged as a pivotal tool for intelligent video-based systems aiming to solve tasks such as pedestrian tracking, social distancing monitoring or pedestrian mass counting. Pedestrian detection methods, even the top performing ones, are highly sensitive to occlusions among pedestrians, which dramatically degrades their performance in crowded scenarios. The generalization of multi-camera set-ups permits to better confront occlusions by combining information from different viewpoints. In this paper, we present a multi-camera approach to globally combine pedestrian detections leveraging automatically extracted scene context. Contrarily to the majority of the methods of the state-of-the-art, the proposed approach is scene-agnostic, not requiring a tailored adaptation to the target scenario\textemdash e.g., via fine-tunning. This noteworthy attribute does not require \textit{ad hoc} training with labelled data, expediting the deployment of the proposed method in real-world situations. Context information, obtained via semantic segmentation, is used 1) to automatically generate a common Area of Interest for the scene and all the cameras, avoiding the usual need of manually defining it; and 2) to obtain detections for each camera by solving a global optimization problem that maximizes coherence of detections both in each 2D image and in the 3D scene. This process yields tightly-fitted bounding boxes that circumvent occlusions or miss-detections. Experimental results on five publicly available datasets show that the proposed approach outperforms state-of-the-art multi-camera pedestrian detectors, even some specifically trained on the target scenario, signifying the versatility and robustness of the proposed method without requiring ad-hoc annotations nor human-guided configuration.



### Can Image Enhancement be Beneficial to Find Smoke Images in Laparoscopic Surgery?
- **Arxiv ID**: http://arxiv.org/abs/1812.10784v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.10784v1)
- **Published**: 2018-12-27 18:07:05+00:00
- **Updated**: 2018-12-27 18:07:05+00:00
- **Authors**: Congcong Wang, Vivek Sharma, Yu Fan, Faouzi Alaya Cheikh, Azeddine Beghdadi, Ole Jacob Elle, Rainer Stiefelhagen
- **Comment**: In proceedings of IST, Color and Imaging Conference (CIC 26).
  Congcong Wang and Vivek Sharma contributed equally to this work and listed in
  alphabetical order
- **Journal**: None
- **Summary**: Laparoscopic surgery has a limited field of view. Laser ablation in a laproscopic surgery causes smoke, which inevitably influences the surgeon's visibility. Therefore, it is of vital importance to remove the smoke, such that a clear visualization is possible. In order to employ a desmoking technique, one needs to know beforehand if the image contains smoke or not, to this date, there exists no accurate method that could classify the smoke/non-smoke images completely. In this work, we propose a new enhancement method which enhances the informative details in the RGB images for discrimination of smoke/non-smoke images. Our proposed method utilizes weighted least squares optimization framework~(WLS). For feature extraction, we use statistical features based on bivariate histogram distribution of gradient magnitude~(GM) and Laplacian of Gaussian~(LoG). We then train a SVM classifier with binary smoke/non-smoke classification task. We demonstrate the effectiveness of our method on Cholec80 dataset. Experiments using our proposed enhancement method show promising results with improvements of 4\% in accuracy and 4\% in F1-Score over the baseline performance of RGB images. In addition, our approach improves over the saturation histogram based classification methodologies Saturation Analysis~(SAN) and Saturation Peak Analysis~(SPA) by 1/5\% and 1/6\% in accuracy/F1-Score metrics.



### Future semantic segmentation of time-lapsed videos with large temporal displacement
- **Arxiv ID**: http://arxiv.org/abs/1812.10786v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.10786v1)
- **Published**: 2018-12-27 18:17:28+00:00
- **Updated**: 2018-12-27 18:17:28+00:00
- **Authors**: Talha Siddiqui, Samarth Bharadwaj
- **Comment**: 12 pages, 7 figures, https://bit.ly/2Bw7HGP
- **Journal**: None
- **Summary**: An important aspect of video understanding is the ability to predict the evolution of its content in the future. This paper presents a future frame semantic segmentation technique for predicting semantic masks of the current and future frames in a time-lapsed video. We specifically focus on time-lapsed videos with large temporal displacement to highlight the model's ability to capture large motions in time. We first introduce a unique semantic segmentation prediction dataset with over 120,000 time-lapsed sky-video frames and all corresponding semantic masks captured over a span of five years in North America region. The dataset has immense practical value for cloud cover analysis, which are treated as non-rigid objects of interest. %Here the model provides both semantic segmentation of cloud region and solar irradiance emitted from a region from the sky-videos. Next, our proposed recurrent network architecture departs from existing trend of using temporal convolutional networks (TCN) (or feed-forward networks), by explicitly learning an internal representations for the evolution of video content with time. Experimental evaluation shows an improvement of mean IoU over TCNs in the segmentation task by 10.8% for 10 mins (21% over 60 mins) ahead of time predictions. Further, our model simultaneously measures both the current and future solar irradiance from the same video frames with a normalized-MAE of 10.5% over two years. These results indicate that recurrent memory networks with attention mechanism are able to capture complex advective and diffused flow characteristic of dense fluids even with sparse temporal sampling and are more suitable for future frame prediction tasks for longer duration videos.



### Hyperspectral Unmixing Based on Clustered Multitask Networks
- **Arxiv ID**: http://arxiv.org/abs/1812.10788v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.10788v1)
- **Published**: 2018-12-27 18:31:25+00:00
- **Updated**: 2018-12-27 18:31:25+00:00
- **Authors**: Sara Khoshsokhan, Roozbeh Rajabi, Hadi Zayyani
- **Comment**: 4 pages, ICSPIS 2018 Conference Paper
- **Journal**: None
- **Summary**: Hyperspectral remote sensing is a prominent research topic in data processing. Most of the spectral unmixing algorithms are developed by adopting the linear mixing models. Nonnegative matrix factorization (NMF) and its developments are used widely for estimation of signatures and fractional abundances in the SU problem. Sparsity constraints was added to NMF, and was regularized by $ L_ {q} $ norm. In this paper, at first hyperspectral images are clustered by fuzzy c- means method, and then a new algorithm based on sparsity constrained distributed optimization is used for spectral unmixing. In the proposed algorithm, a network including clusters is employed. Each pixel in the hyperspectral images considered as a node in this network. The proposed algorithm is optimized with diffusion LMS strategy, and then the update equations for fractional abundance and signature matrices are obtained. Simulation results based on defined performance metrics illustrate advantage of the proposed algorithm in spectral unmixing of hyperspectral data compared with other methods.



### DeepBillboard: Systematic Physical-World Testing of Autonomous Driving Systems
- **Arxiv ID**: http://arxiv.org/abs/1812.10812v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.10812v1)
- **Published**: 2018-12-27 19:55:54+00:00
- **Updated**: 2018-12-27 19:55:54+00:00
- **Authors**: Husheng Zhou, Wei Li, Yuankun Zhu, Yuqun Zhang, Bei Yu, Lingming Zhang, Cong Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Neural Networks (DNNs) have been widely applied in many autonomous systems such as autonomous driving. Recently, DNN testing has been intensively studied to automatically generate adversarial examples, which inject small-magnitude perturbations into inputs to test DNNs under extreme situations. While existing testing techniques prove to be effective, they mostly focus on generating digital adversarial perturbations (particularly for autonomous driving), e.g., changing image pixels, which may never happen in physical world. There is a critical missing piece in the literature on autonomous driving testing: understanding and exploiting both digital and physical adversarial perturbation generation for impacting steering decisions. In this paper, we present DeepBillboard, a systematic physical-world testing approach targeting at a common and practical driving scenario: drive-by billboards. DeepBillboard is capable of generating a robust and resilient printable adversarial billboard, which works under dynamic changing driving conditions including viewing angle, distance, and lighting. The objective is to maximize the possibility, degree, and duration of the steering-angle errors of an autonomous vehicle driving by the generated adversarial billboard. We have extensively evaluated the efficacy and robustness of DeepBillboard through conducting both digital and physical-world experiments. Results show that DeepBillboard is effective for various steering models and scenes. Furthermore, DeepBillboard is sufficiently robust and resilient for generating physical-world adversarial billboard tests for real-world driving under various weather conditions. To the best of our knowledge, this is the first study demonstrating the possibility of generating realistic and continuous physical-world tests for practical autonomous driving systems.



### Classification of radiology reports by modality and anatomy: A comparative study
- **Arxiv ID**: http://arxiv.org/abs/1812.10818v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.10818v1)
- **Published**: 2018-12-27 20:21:36+00:00
- **Updated**: 2018-12-27 20:21:36+00:00
- **Authors**: Marina Bendersky, Joy Wu, Tanveer Syeda-Mahmood
- **Comment**: 8 pages, 4 figures, BIBM 2018
- **Journal**: None
- **Summary**: Data labeling is currently a time-consuming task that often requires expert knowledge. In research settings, the availability of correctly labeled data is crucial to ensure that model predictions are accurate and useful. We propose relatively simple machine learning-based models that achieve high performance metrics in the binary and multiclass classification of radiology reports. We compare the performance of these algorithms to that of a data-driven approach based on NLP, and find that the logistic regression classifier outperforms all other models, in both the binary and multiclass classification tasks. We then choose the logistic regression binary classifier to predict chest X-ray (CXR)/ non-chest X-ray (non-CXR) labels in reports from different datasets, unseen during any training phase of any of the models. Even in unseen report collections, the binary logistic regression classifier achieves average precision values of above 0.9. Based on the regression coefficient values, we also identify frequent tokens in CXR and non-CXR reports that are features with possibly high predictive power.



### Adaptive Image Sampling using Deep Learning and its Application on X-Ray Fluorescence Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1812.10836v3
- **DOI**: 10.1109/TMM.2019.2958760
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.10836v3)
- **Published**: 2018-12-27 21:53:14+00:00
- **Updated**: 2019-11-17 23:53:17+00:00
- **Authors**: Qiqin Dai, Henry Chopp, Emeline Pouyet, Oliver Cossairt, Marc Walton, Aggelos K. Katsaggelos
- **Comment**: journal preprint v3
- **Journal**: None
- **Summary**: This paper presents an adaptive image sampling algorithm based on Deep Learning (DL). The adaptive sampling mask generation network is jointly trained with an image inpainting network. The sampling rate is controlled in the mask generation network, and a binarization strategy is investigated to make the sampling mask binary. Besides the image sampling and reconstruction application, we show that the proposed adaptive sampling algorithm is able to speed up raster scan processes such as the X-Ray fluorescence (XRF) image scanning process. Recently XRF laboratory-based systems have evolved to lightweight and portable instruments thanks to technological advancements in both X-Ray generation and detection. However, the scanning time of an XRF image is usually long due to the long exposures requires (e.g., $100 \mu s-1ms$ per point). We propose an XRF image inpainting approach to address the issue of long scanning time, thus speeding up the scanning process while still maintaining the possibility to reconstruct a high quality XRF image. The proposed adaptive image sampling algorithm is applied to the RGB image of the scanning target to generate the sampling mask. The XRF scanner is then driven according to the sampling mask to scan a subset of the total image pixels. Finally, we inpaint the scanned XRF image by fusing the RGB image to reconstruct the full scan XRF image. The experiments show that the proposed adaptive sampling algorithm is able to effectively sample the image and achieve a better reconstruction accuracy than that of the existing methods.



