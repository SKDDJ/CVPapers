# Arxiv Papers in cs.CV on 2018-12-05
### Multi$^{\mathbf{3}}$Net: Segmenting Flooded Buildings via Fusion of Multiresolution, Multisensor, and Multitemporal Satellite Imagery
- **Arxiv ID**: http://arxiv.org/abs/1812.01756v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, I.2.10; I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/1812.01756v1)
- **Published**: 2018-12-05 00:05:01+00:00
- **Updated**: 2018-12-05 00:05:01+00:00
- **Authors**: Tim G. J. Rudner, Marc Rußwurm, Jakub Fil, Ramona Pelich, Benjamin Bischke, Veronika Kopackova, Piotr Bilinski
- **Comment**: To appear in Proceedings of the Thirty-Third AAAI Conference on
  Artificial Intelligence (AAAI-19)
- **Journal**: None
- **Summary**: We propose a novel approach for rapid segmentation of flooded buildings by fusing multiresolution, multisensor, and multitemporal satellite imagery in a convolutional neural network. Our model significantly expedites the generation of satellite imagery-based flood maps, crucial for first responders and local authorities in the early stages of flood events. By incorporating multitemporal satellite imagery, our model allows for rapid and accurate post-disaster damage assessment and can be used by governments to better coordinate medium- and long-term financial assistance programs for affected areas. The network consists of multiple streams of encoder-decoder architectures that extract spatiotemporal information from medium-resolution images and spatial information from high-resolution images before fusing the resulting representations into a single medium-resolution segmentation map of flooded buildings. We compare our model to state-of-the-art methods for building footprint segmentation as well as to alternative fusion approaches for the segmentation of flooded buildings and find that our model performs best on both tasks. We also demonstrate that our model produces highly accurate segmentation maps of flooded buildings using only publicly available medium-resolution data instead of significantly more detailed but sparsely available very high-resolution data. We release the first open-source dataset of fully preprocessed and labeled multiresolution, multispectral, and multitemporal satellite images of disaster sites along with our source code.



### Capture Dense: Markerless Motion Capture Meets Dense Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1812.01783v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.01783v2)
- **Published**: 2018-12-05 02:19:24+00:00
- **Updated**: 2018-12-11 04:42:25+00:00
- **Authors**: Xiu Li, Yebin Liu, Hanbyul Joo, Qionghai Dai, Yaser Sheikh
- **Comment**: Withdraw due to incomplete experiment
- **Journal**: None
- **Summary**: We present a method to combine markerless motion capture and dense pose feature estimation into a single framework. We demonstrate that dense pose information can help for multiview/single-view motion capture, and multiview motion capture can help the collection of a high-quality dataset for training the dense pose detector. Specifically, we first introduce a novel markerless motion capture method that can take advantage of dense parsing capability provided by the dense pose detector. Thanks to the introduced dense human parsing ability, our method is demonstrated much more efficient, and accurate compared with the available state-of-the-art markerless motion capture approach. Second, we improve the performance of available dense pose detector by using multiview markerless motion capture data. Such dataset is beneficial to dense pose training because they are more dense and accurate and consistent, and can compensate for the corner cases such as unusual viewpoints. We quantitatively demonstrate the improved performance of our dense pose detector over the available DensePose. Our dense pose dataset and detector will be made public.



### Generalized Zero- and Few-Shot Learning via Aligned Variational Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/1812.01784v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.01784v4)
- **Published**: 2018-12-05 02:20:12+00:00
- **Updated**: 2019-04-05 12:52:25+00:00
- **Authors**: Edgar Schönfeld, Sayna Ebrahimi, Samarth Sinha, Trevor Darrell, Zeynep Akata
- **Comment**: Accepted at CVPR 2019
- **Journal**: None
- **Summary**: Many approaches in generalized zero-shot learning rely on cross-modal mapping between the image feature space and the class embedding space. As labeled images are expensive, one direction is to augment the dataset by generating either images or image features. However, the former misses fine-grained details and the latter requires learning a mapping associated with class embeddings. In this work, we take feature generation one step further and propose a model where a shared latent space of image features and class embeddings is learned by modality-specific aligned variational autoencoders. This leaves us with the required discriminative information about the image and classes in the latent features, on which we train a softmax classifier. The key to our approach is that we align the distributions learned from images and from side-information to construct latent features that contain the essential multi-modal information associated with unseen classes. We evaluate our learned latent features on several benchmark datasets, i.e. CUB, SUN, AWA1 and AWA2, and establish a new state of the art on generalized zero-shot as well as on few-shot learning. Moreover, our results on ImageNet with various zero-shot splits show that our latent features generalize well in large-scale settings.



### Visual Attention for Behavioral Cloning in Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1812.01802v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.01802v1)
- **Published**: 2018-12-05 03:30:54+00:00
- **Updated**: 2018-12-05 03:30:54+00:00
- **Authors**: Sourav Pal, Tharun Mohandoss, Pabitra Mitra
- **Comment**: Paper Accepted at ICMV (2018)
- **Journal**: None
- **Summary**: The goal of our work is to use visual attention to enhance autonomous driving performance. We present two methods of predicting visual attention maps. The first method is a supervised learning approach in which we collect eye-gaze data for the task of driving and use this to train a model for predicting the attention map. The second method is a novel unsupervised approach where we train a model to learn to predict attention as it learns to drive a car. Finally, we present a comparative study of our results and show that the supervised approach for predicting attention when incorporated performs better than other approaches.



### An Embarrassingly Simple Approach for Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/1812.01819v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.01819v2)
- **Published**: 2018-12-05 05:09:45+00:00
- **Updated**: 2019-09-08 16:46:52+00:00
- **Authors**: Mengya Gao, Yujun Shen, Quanquan Li, Junjie Yan, Liang Wan, Dahua Lin, Chen Change Loy, Xiaoou Tang
- **Comment**: 8 pages and 5 figures
- **Journal**: None
- **Summary**: Knowledge Distillation (KD) aims at improving the performance of a low-capacity student model by inheriting knowledge from a high-capacity teacher model. Previous KD methods typically train a student by minimizing a task-related loss and the KD loss simultaneously, using a pre-defined loss weight to balance these two terms. In this work, we propose to first transfer the backbone knowledge from a teacher to the student, and then only learn the task-head of the student network. Such a decomposition of the training process circumvents the need of choosing an appropriate loss weight, which is often difficult in practice, and thus makes it easier to apply to different datasets and tasks. Importantly, the decomposition permits the core of our method, Stage-by-Stage Knowledge Distillation (SSKD), which facilitates progressive feature mimicking from teacher to student. Extensive experiments on CIFAR-100 and ImageNet suggest that SSKD significantly narrows down the performance gap between student and teacher, outperforming state-of-the-art approaches. We also demonstrate the generalization ability of SSKD on other challenging benchmarks, including face recognition on IJB-A dataset as well as object detection on COCO dataset.



### Regularized Ensembles and Transferability in Adversarial Learning
- **Arxiv ID**: http://arxiv.org/abs/1812.01821v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.01821v1)
- **Published**: 2018-12-05 05:32:00+00:00
- **Updated**: 2018-12-05 05:32:00+00:00
- **Authors**: Yifan Chen, Yevgeniy Vorobeychik
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Despite the considerable success of convolutional neural networks in a broad array of domains, recent research has shown these to be vulnerable to small adversarial perturbations, commonly known as adversarial examples. Moreover, such examples have shown to be remarkably portable, or transferable, from one model to another, enabling highly successful black-box attacks. We explore this issue of transferability and robustness from two dimensions: first, considering the impact of conventional $l_p$ regularization as well as replacing the top layer with a linear support vector machine (SVM), and second, the value of combining regularized models into an ensemble. We show that models trained with different regularizers present barriers to transferability, as does partial information about the models comprising the ensemble.



### Explainable and Explicit Visual Reasoning over Scene Graphs
- **Arxiv ID**: http://arxiv.org/abs/1812.01855v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.01855v2)
- **Published**: 2018-12-05 08:35:05+00:00
- **Updated**: 2019-03-19 12:55:14+00:00
- **Authors**: Jiaxin Shi, Hanwang Zhang, Juanzi Li
- **Comment**: CVPR2019
- **Journal**: None
- **Summary**: We aim to dismantle the prevalent black-box neural architectures used in complex visual reasoning tasks, into the proposed eXplainable and eXplicit Neural Modules (XNMs), which advance beyond existing neural module networks towards using scene graphs --- objects as nodes and the pairwise relationships as edges --- for explainable and explicit reasoning with structured knowledge. XNMs allow us to pay more attention to teach machines how to "think", regardless of what they "look". As we will show in the paper, by using scene graphs as an inductive bias, 1) we can design XNMs in a concise and flexible fashion, i.e., XNMs merely consist of 4 meta-types, which significantly reduce the number of parameters by 10 to 100 times, and 2) we can explicitly trace the reasoning-flow in terms of graph attentions. XNMs are so generic that they support a wide range of scene graph implementations with various qualities. For example, when the graphs are detected perfectly, XNMs achieve 100% accuracy on both CLEVR and CLEVR CoGenT, establishing an empirical performance upper-bound for visual reasoning; when the graphs are noisily detected from real-world images, XNMs are still robust to achieve a competitive 67.5% accuracy on VQAv2.0, surpassing the popular bag-of-objects attention models without graph structures.



### Enhancing Label-Driven Deep Deformable Image Registration with Local Distance Metrics for State-of-the-Art Cardiac Motion Tracking
- **Arxiv ID**: http://arxiv.org/abs/1812.01859v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.01859v1)
- **Published**: 2018-12-05 08:49:47+00:00
- **Updated**: 2018-12-05 08:49:47+00:00
- **Authors**: Alessa Hering, Sven Kuckertz, Stefan Heldmann, Mattias Heinrich
- **Comment**: Accepted at BVM Workshop 2019
- **Journal**: None
- **Summary**: While deep learning has achieved significant advances in accuracy for medical image segmentation, its benefits for deformable image registration have so far remained limited to reduced computation times. Previous work has either focused on replacing the iterative optimization of distance and smoothness terms with CNN-layers or using supervised approaches driven by labels. Our method is the first to combine the complementary strengths of global semantic information (represented by segmentation labels) and local distance metrics that help align surrounding structures. We demonstrate significant higher Dice scores (of 86.5\%) for deformable cardiac image registration compared to classic registration (79.0\%) as well as label-driven deep learning frameworks (83.4\%).



### Few-shot Object Detection via Feature Reweighting
- **Arxiv ID**: http://arxiv.org/abs/1812.01866v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.01866v2)
- **Published**: 2018-12-05 09:23:41+00:00
- **Updated**: 2019-10-21 08:50:16+00:00
- **Authors**: Bingyi Kang, Zhuang Liu, Xin Wang, Fisher Yu, Jiashi Feng, Trevor Darrell
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: Conventional training of a deep CNN based object detector demands a large number of bounding box annotations, which may be unavailable for rare categories. In this work we develop a few-shot object detector that can learn to detect novel objects from only a few annotated examples. Our proposed model leverages fully labeled base classes and quickly adapts to novel classes, using a meta feature learner and a reweighting module within a one-stage detection architecture. The feature learner extracts meta features that are generalizable to detect novel object classes, using training data from base classes with sufficient samples. The reweighting module transforms a few support examples from the novel classes to a global vector that indicates the importance or relevance of meta features for detecting the corresponding objects. These two modules, together with a detection prediction module, are trained end-to-end based on an episodic few-shot learning scheme and a carefully designed loss function. Through extensive experiments we demonstrate that our model outperforms well-established baselines by a large margin for few-shot object detection, on multiple datasets and settings. We also present analysis on various aspects of our proposed model, aiming to provide some inspiration for future few-shot detection works.



### Learning to Take Directions One Step at a Time
- **Arxiv ID**: http://arxiv.org/abs/1812.01874v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1812.01874v3)
- **Published**: 2018-12-05 09:42:12+00:00
- **Updated**: 2020-08-14 06:39:39+00:00
- **Authors**: Qiyang Hu, Adrian Wälchli, Tiziano Portenier, Matthias Zwicker, Paolo Favaro
- **Comment**: None
- **Journal**: None
- **Summary**: We present a method to generate a video sequence given a single image. Because items in an image can be animated in arbitrarily many different ways, we introduce as control signal a sequence of motion strokes. Such control signal can be automatically transferred from other videos, e.g., via bounding box tracking. Each motion stroke provides the direction to the moving object in the input image and we aim to train a network to generate an animation following a sequence of such directions. To address this task we design a novel recurrent architecture, which can be trained easily and effectively thanks to an explicit separation of past, future and current states. As we demonstrate in the experiments, our proposed architecture is capable of generating an arbitrary number of frames from a single image and a sequence of motion strokes. Key components of our architecture are an autoencoding constraint to ensure consistency with the past and a generative adversarial scheme to ensure that images look realistic and are temporally smooth. We demonstrate the effectiveness of our approach on the MNIST, KTH, Human3.6M, Push and Weizmann datasets.



### Learning to Compose Dynamic Tree Structures for Visual Contexts
- **Arxiv ID**: http://arxiv.org/abs/1812.01880v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.01880v1)
- **Published**: 2018-12-05 09:51:19+00:00
- **Updated**: 2018-12-05 09:51:19+00:00
- **Authors**: Kaihua Tang, Hanwang Zhang, Baoyuan Wu, Wenhan Luo, Wei Liu
- **Comment**: None
- **Journal**: None
- **Summary**: We propose to compose dynamic tree structures that place the objects in an image into a visual context, helping visual reasoning tasks such as scene graph generation and visual Q&A. Our visual context tree model, dubbed VCTree, has two key advantages over existing structured object representations including chains and fully-connected graphs: 1) The efficient and expressive binary tree encodes the inherent parallel/hierarchical relationships among objects, e.g., "clothes" and "pants" are usually co-occur and belong to "person"; 2) the dynamic structure varies from image to image and task to task, allowing more content-/task-specific message passing among objects. To construct a VCTree, we design a score function that calculates the task-dependent validity between each object pair, and the tree is the binary version of the maximum spanning tree from the score matrix. Then, visual contexts are encoded by bidirectional TreeLSTM and decoded by task-specific models. We develop a hybrid learning procedure which integrates end-task supervised learning and the tree structure reinforcement learning, where the former's evaluation result serves as a self-critic for the latter's structure exploration. Experimental results on two benchmarks, which require reasoning over contexts: Visual Genome for scene graph generation and VQA2.0 for visual Q&A, show that VCTree outperforms state-of-the-art results while discovering interpretable visual context structures.



### Interactive Full Image Segmentation by Considering All Regions Jointly
- **Arxiv ID**: http://arxiv.org/abs/1812.01888v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.01888v2)
- **Published**: 2018-12-05 10:09:15+00:00
- **Updated**: 2019-04-10 15:51:32+00:00
- **Authors**: Eirikur Agustsson, Jasper R. R. Uijlings, Vittorio Ferrari
- **Comment**: Accepted to CVPR 2019
- **Journal**: None
- **Summary**: We address interactive full image annotation, where the goal is to accurately segment all object and stuff regions in an image. We propose an interactive, scribble-based annotation framework which operates on the whole image to produce segmentations for all regions. This enables sharing scribble corrections across regions, and allows the annotator to focus on the largest errors made by the machine across the whole image. To realize this, we adapt Mask-RCNN into a fast interactive segmentation framework and introduce an instance-aware loss measured at the pixel-level in the full image canvas, which lets predictions for nearby regions properly compete for space. Finally, we compare to interactive single object segmentation on the COCO panoptic dataset. We demonstrate that our interactive full image segmentation approach leads to a 5% IoU gain, reaching 90% IoU at a budget of four extreme clicks and four corrective scribbles per region.



### Learning to generate filters for convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/1812.01894v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.01894v1)
- **Published**: 2018-12-05 10:16:38+00:00
- **Updated**: 2018-12-05 10:16:38+00:00
- **Authors**: Wei Shen, Rujie Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Conventionally, convolutional neural networks (CNNs) process different images with the same set of filters. However, the variations in images pose a challenge to this fashion. In this paper, we propose to generate sample-specific filters for convolutional layers in the forward pass. Since the filters are generated on-the-fly, the model becomes more flexible and can better fit the training data compared to traditional CNNs. In order to obtain sample-specific features, we extract the intermediate feature maps from an autoencoder. As filters are usually high dimensional, we propose to learn a set of coefficients instead of a set of filters. These coefficients are used to linearly combine the base filters from a filter repository to generate the final filters for a CNN. The proposed method is evaluated on MNIST, MTFL and CIFAR10 datasets. Experiment results demonstrate that the classification accuracy of the baseline model can be improved by using the proposed filter generation method.



### Local Temporal Bilinear Pooling for Fine-grained Action Parsing
- **Arxiv ID**: http://arxiv.org/abs/1812.01922v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.01922v3)
- **Published**: 2018-12-05 11:25:31+00:00
- **Updated**: 2019-05-26 10:15:18+00:00
- **Authors**: Yan Zhang, Siyu Tang, Krikamol Muandet, Christian Jarvers, Heiko Neumann
- **Comment**: 11 pages, 2 figures. Cam.R
- **Journal**: None
- **Summary**: Fine-grained temporal action parsing is important in many applications, such as daily activity understanding, human motion analysis, surgical robotics and others requiring subtle and precise operations in a long-term period. In this paper we propose a novel bilinear pooling operation, which is used in intermediate layers of a temporal convolutional encoder-decoder net. In contrast to other work, our proposed bilinear pooling is learnable and hence can capture more complex local statistics than the conventional counterpart. In addition, we introduce exact lower-dimension representations of our bilinear forms, so that the dimensionality is reduced with neither information loss nor extra computation. We perform intensive experiments to quantitatively analyze our model and show the superior performances to other state-of-the-art work on various datasets.



### An Empirical Study towards Understanding How Deep Convolutional Nets Recognize Falls
- **Arxiv ID**: http://arxiv.org/abs/1812.01923v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.01923v1)
- **Published**: 2018-12-05 11:27:12+00:00
- **Updated**: 2018-12-05 11:27:12+00:00
- **Authors**: Yan Zhang, Heiko Neumann
- **Comment**: published at the sixth International Workshop on Assistive Computer
  Vision and Robotics (ACVR), in conjunction with European Conference on
  Computer Vision (ECCV), Munich, 2018
- **Journal**: None
- **Summary**: Detecting unintended falls is essential for ambient intelligence and healthcare of elderly people living alone. In recent years, deep convolutional nets are widely used in human action analysis, based on which a number of fall detection methods have been proposed. Despite their highly effective performances, the behaviors of how the convolutional nets recognize falls are still not clear. In this paper, instead of proposing a novel approach, we perform a systematical empirical study, attempting to investigate the underlying fall recognition process. We propose four tasks to investigate, which involve five types of input modalities, seven net instances and different training samples. The obtained quantitative and qualitative results reveal the patterns that the nets tend to learn, and several factors that can heavily influence the performances on fall recognition. We expect that our conclusions are favorable to proposing better deep learning solutions to fall detection systems.



### Stacked Dense U-Nets with Dual Transformers for Robust Face Alignment
- **Arxiv ID**: http://arxiv.org/abs/1812.01936v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.01936v1)
- **Published**: 2018-12-05 12:02:11+00:00
- **Updated**: 2018-12-05 12:02:11+00:00
- **Authors**: Jia Guo, Jiankang Deng, Niannan Xue, Stefanos Zafeiriou
- **Comment**: None
- **Journal**: None
- **Summary**: Facial landmark localisation in images captured in-the-wild is an important and challenging problem. The current state-of-the-art revolves around certain kinds of Deep Convolutional Neural Networks (DCNNs) such as stacked U-Nets and Hourglass networks. In this work, we innovatively propose stacked dense U-Nets for this task. We design a novel scale aggregation network topology structure and a channel aggregation building block to improve the model's capacity without sacrificing the computational complexity and model size. With the assistance of deformable convolutions inside the stacked dense U-Nets and coherent loss for outside data transformation, our model obtains the ability to be spatially invariant to arbitrary input face images. Extensive experiments on many in-the-wild datasets, validate the robustness of the proposed method under extreme poses, exaggerated expressions and heavy occlusions. Finally, we show that accurate 3D face alignment can assist pose-invariant face recognition where we achieve a new state-of-the-art accuracy on CFP-FP.



### Automatic Generation of Dense Non-rigid Optical Flow
- **Arxiv ID**: http://arxiv.org/abs/1812.01946v5
- **DOI**: 10.1016/j.cviu.2021.103274
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.01946v5)
- **Published**: 2018-12-05 12:10:06+00:00
- **Updated**: 2021-09-07 16:45:02+00:00
- **Authors**: Hoàng-Ân Lê, Tushar Nimbhorkar, Thomas Mensink, Anil S. Baslamisli, Sezer Karaoglu, Theo Gevers
- **Comment**: The paper is accepted for publication for Computer Vision and Image
  Understanding (CVIU)
- **Journal**: Volume 212, November 2021, 103274
- **Summary**: There hardly exists any large-scale datasets with dense optical flow of non-rigid motion from real-world imagery as of today. The reason lies mainly in the required setup to derive ground truth optical flows: a series of images with known camera poses along its trajectory, and an accurate 3D model from a textured scene. Human annotation is not only too tedious for large databases, it can simply hardly contribute to accurate optical flow. To circumvent the need for manual annotation, we propose a framework to automatically generate optical flow from real-world videos. The method extracts and matches objects from video frames to compute initial constraints, and applies a deformation over the objects of interest to obtain dense optical flow fields. We propose several ways to augment the optical flow variations. Extensive experimental results show that training on our automatically generated optical flow outperforms methods that are trained on rigid synthetic data using FlowNet-S, LiteFlowNet, PWC-Net, and RAFT. Datasets and implementation of our optical flow generation framework are released at https://github.com/lhoangan/arap_flow



### On Min-Max affine approximants of convex or concave real valued functions from $\mathbb R^k$, Chebyshev equioscillation and graphics
- **Arxiv ID**: http://arxiv.org/abs/1812.02302v10
- **DOI**: 10.1007/978-3-030-69637-5_19
- **Categories**: **math.OC**, cs.CV, math.CA, 49J30, 49J35, 49J10, 49J21 (Primary), 94A08 (Secondary)
- **Links**: [PDF](http://arxiv.org/pdf/1812.02302v10)
- **Published**: 2018-12-05 12:17:25+00:00
- **Updated**: 2021-08-17 23:32:47+00:00
- **Authors**: Steven B. Damelin, David L. Ragozin, Michael Werman
- **Comment**: None
- **Journal**: In: Hirn, M., Li, S., Okoudjou, K.A., Saliani, S. (eds.)
  Excursions in Harmonic Analysis. Applied and Numerical Harmonic Analysis,
  vol. 6. Springer, Cham (2021)
- **Summary**: We study Min-Max affine approximants of a continuous convex or concave function $f:\Delta\subset \mathbb R^k\xrightarrow{} \mathbb R$ where $\Delta$ is a convex compact subset of $\mathbb R^k$. In the case when $\Delta$ is a simplex we prove that there is a vertical translate of the supporting hyperplane in $\mathbb R^{k+1}$ of the graph of $f$ at the vertices which is the unique best affine approximant to $f$ on $\Delta$. For $k=1$, this result provides an extension of the Chebyshev equioscillation theorem for linear approximants. Our result has interesting connections to the computer graphics problem of rapid rendering of projective transformations.



### Training Competitive Binary Neural Networks from Scratch
- **Arxiv ID**: http://arxiv.org/abs/1812.01965v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.01965v1)
- **Published**: 2018-12-05 12:48:50+00:00
- **Updated**: 2018-12-05 12:48:50+00:00
- **Authors**: Joseph Bethge, Marvin Bornstein, Adrian Loy, Haojin Yang, Christoph Meinel
- **Comment**: Our source code can be found at https://github.com/hpi-xnor/BMXNet-v2
- **Journal**: None
- **Summary**: Convolutional neural networks have achieved astonishing results in different application areas. Various methods that allow us to use these models on mobile and embedded devices have been proposed. Especially binary neural networks are a promising approach for devices with low computational power. However, training accurate binary models from scratch remains a challenge. Previous work often uses prior knowledge from full-precision models and complex training strategies. In our work, we focus on increasing the performance of binary neural networks without such prior knowledge and a much simpler training strategy. In our experiments we show that we are able to achieve state-of-the-art results on standard benchmark datasets. Further, to the best of our knowledge, we are the first to successfully adopt a network architecture with dense connections for binary networks, which lets us improve the state-of-the-art even further.



### Summarizing Videos with Attention
- **Arxiv ID**: http://arxiv.org/abs/1812.01969v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.01969v2)
- **Published**: 2018-12-05 13:00:04+00:00
- **Updated**: 2019-02-21 09:52:11+00:00
- **Authors**: Jiri Fajtl, Hajar Sadeghi Sokeh, Vasileios Argyriou, Dorothy Monekosso, Paolo Remagnino
- **Comment**: Presented at ACCV2018 AIU2018 workshop
- **Journal**: None
- **Summary**: In this work we propose a novel method for supervised, keyshots based video summarization by applying a conceptually simple and computationally efficient soft, self-attention mechanism. Current state of the art methods leverage bi-directional recurrent networks such as BiLSTM combined with attention. These networks are complex to implement and computationally demanding compared to fully connected networks. To that end we propose a simple, self-attention based network for video summarization which performs the entire sequence to sequence transformation in a single feed forward pass and single backward pass during training. Our method sets a new state of the art results on two benchmarks TvSum and SumMe, commonly used in this domain.



### VideoMem: Constructing, Analyzing, Predicting Short-term and Long-term Video Memorability
- **Arxiv ID**: http://arxiv.org/abs/1812.01973v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1812.01973v1)
- **Published**: 2018-12-05 13:03:11+00:00
- **Updated**: 2018-12-05 13:03:11+00:00
- **Authors**: Romain Cohendet, Claire-Hélène Demarty, Ngoc Q. K. Duong, Martin Engilberge
- **Comment**: None
- **Journal**: None
- **Summary**: Humans share a strong tendency to memorize/forget some of the visual information they encounter. This paper focuses on providing computational models for the prediction of the intrinsic memorability of visual content. To address this new challenge, we introduce a large scale dataset (VideoMem) composed of 10,000 videos annotated with memorability scores. In contrast to previous work on image memorability -- where memorability was measured a few minutes after memorization -- memory performance is measured twice: a few minutes after memorization and again 24-72 hours later. Hence, the dataset comes with short-term and long-term memorability annotations. After an in-depth analysis of the dataset, we investigate several deep neural network based models for the prediction of video memorability. Our best model using a ranking loss achieves a Spearman's rank correlation of 0.494 for short-term memorability prediction, while our proposed model with attention mechanism provides insights of what makes a content memorable. The VideoMem dataset with pre-extracted features is publicly available.



### Dynamic Spatio-temporal Graph-based CNNs for Traffic Prediction
- **Arxiv ID**: http://arxiv.org/abs/1812.02019v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.02019v4)
- **Published**: 2018-12-05 14:34:10+00:00
- **Updated**: 2020-03-05 14:35:59+00:00
- **Authors**: Ken Chen, Fei Chen, Baisheng Lai, Zhongming Jin, Yong Liu, Kai Li, Long Wei, Pengfei Wang, Yandong Tang, Jianqiang Huang, Xian-Sheng Hua
- **Comment**: None
- **Journal**: None
- **Summary**: Forecasting future traffic flows from previous ones is a challenging problem because of their complex and dynamic nature of spatio-temporal structures. Most existing graph-based CNNs attempt to capture the static relations while largely neglecting the dynamics underlying sequential data. In this paper, we present dynamic spatio-temporal graph-based CNNs (DST-GCNNs) by learning expressive features to represent spatio-temporal structures and predict future traffic flows from surveillance video data. In particular, DST-GCNN is a two stream network. In the flow prediction stream, we present a novel graph-based spatio-temporal convolutional layer to extract features from a graph representation of traffic flows. Then several such layers are stacked together to predict future flows over time. Meanwhile, the relations between traffic flows in the graph are often time variant as the traffic condition changes over time. To capture the graph dynamics, we use the graph prediction stream to predict the dynamic graph structures, and the predicted structures are fed into the flow prediction stream. Experiments on real datasets demonstrate that the proposed model achieves competitive performances compared with the other state-of-the-art methods.



### Learn to See by Events: Color Frame Synthesis from Event and RGB Cameras
- **Arxiv ID**: http://arxiv.org/abs/1812.02041v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.02041v2)
- **Published**: 2018-12-05 15:22:47+00:00
- **Updated**: 2019-12-10 15:00:10+00:00
- **Authors**: Stefano Pini, Guido Borghi, Roberto Vezzani
- **Comment**: Accepted as full oral at the 15th International Conference on
  Computer Vision Theory and Applications (VISAPP) 2020
- **Journal**: None
- **Summary**: Event cameras are biologically-inspired sensors that gather the temporal evolution of the scene. They capture pixel-wise brightness variations and output a corresponding stream of asynchronous events. Despite having multiple advantages with respect to traditional cameras, their use is partially prevented by the limited applicability of traditional data processing and vision algorithms. To this aim, we present a framework which exploits the output stream of event cameras to synthesize RGB frames, relying on an initial or a periodic set of color key-frames and the sequence of intermediate events. Differently from existing work, we propose a deep learning-based frame synthesis method, consisting of an adversarial architecture combined with a recurrent module. Qualitative results and quantitative per-pixel, perceptual, and semantic evaluation on four public datasets confirm the quality of the synthesized images.



### Point-to-Pose Voting based Hand Pose Estimation using Residual Permutation Equivariant Layer
- **Arxiv ID**: http://arxiv.org/abs/1812.02050v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.02050v1)
- **Published**: 2018-12-05 15:31:26+00:00
- **Updated**: 2018-12-05 15:31:26+00:00
- **Authors**: Shile Li, Dongheui Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, 3D input data based hand pose estimation methods have shown state-of-the-art performance, because 3D data capture more spatial information than the depth image. Whereas 3D voxel-based methods need a large amount of memory, PointNet based methods need tedious preprocessing steps such as K-nearest neighbour search for each point. In this paper, we present a novel deep learning hand pose estimation method for an unordered point cloud. Our method takes 1024 3D points as input and does not require additional information. We use Permutation Equivariant Layer (PEL) as the basic element, where a residual network version of PEL is proposed for the hand pose estimation task. Furthermore, we propose a voting based scheme to merge information from individual points to the final pose output. In addition to the pose estimation task, the voting-based scheme can also provide point cloud segmentation result without ground-truth for segmentation. We evaluate our method on both NYU dataset and the Hands2017Challenge dataset. Our method outperforms recent state-of-the-art methods, where our pose accuracy is currently the best for the Hands2017Challenge dataset.



### Brain Segmentation from k-space with End-to-end Recurrent Attention Network
- **Arxiv ID**: http://arxiv.org/abs/1812.02068v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.02068v2)
- **Published**: 2018-12-05 16:01:28+00:00
- **Updated**: 2019-07-21 18:56:39+00:00
- **Authors**: Qiaoying Huang, Xiao Chen, Dimitris Metaxas, Mariappan S. Nadar
- **Comment**: Accepted by MICCAI 2019
- **Journal**: None
- **Summary**: The task of medical image segmentation commonly involves an image reconstruction step to convert acquired raw data to images before any analysis. However, noises, artifacts and loss of information due to the reconstruction process are almost inevitable, which compromises the final performance of segmentation. We present a novel learning framework that performs magnetic resonance brain image segmentation directly from k-space data. The end-to-end framework consists of a unique task-driven attention module that recurrently utilizes intermediate segmentation estimation to facilitate image-domain feature extraction from the raw data, thus closely bridging the reconstruction and the segmentation tasks. In addition, to address the challenge of manual labeling, we introduce a novel workflow to generate labeled training data for segmentation by exploiting imaging modality simulators and digital phantoms. Extensive experimental results show that the proposed method outperforms several state-of-the-art methods.



### Understanding Individual Decisions of CNNs via Contrastive Backpropagation
- **Arxiv ID**: http://arxiv.org/abs/1812.02100v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.02100v2)
- **Published**: 2018-12-05 16:43:04+00:00
- **Updated**: 2019-09-03 00:16:47+00:00
- **Authors**: Jindong Gu, Yinchong Yang, Volker Tresp
- **Comment**: 16 pages, 5 figures, ACCV
- **Journal**: the 14th Asian Conference on Computer Vision (ACCV) 2018
- **Summary**: A number of backpropagation-based approaches such as DeConvNets, vanilla Gradient Visualization and Guided Backpropagation have been proposed to better understand individual decisions of deep convolutional neural networks. The saliency maps produced by them are proven to be non-discriminative. Recently, the Layer-wise Relevance Propagation (LRP) approach was proposed to explain the classification decisions of rectifier neural networks. In this work, we evaluate the discriminativeness of the generated explanations and analyze the theoretical foundation of LRP, i.e. Deep Taylor Decomposition. The experiments and analysis conclude that the explanations generated by LRP are not class-discriminative. Based on LRP, we propose Contrastive Layer-wise Relevance Propagation (CLRP), which is capable of producing instance-specific, class-discriminative, pixel-wise explanations. In the experiments, we use the CLRP to explain the decisions and understand the difference between neurons in individual classification decisions. We also evaluate the explanations quantitatively with a Pointing Game and an ablation study. Both qualitative and quantitative evaluations show that the CLRP generates better explanations than the LRP. The code is available.



### Learning Attraction Field Representation for Robust Line Segment Detection
- **Arxiv ID**: http://arxiv.org/abs/1812.02122v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.02122v2)
- **Published**: 2018-12-05 17:24:02+00:00
- **Updated**: 2019-03-05 23:36:21+00:00
- **Authors**: Nan Xue, Song Bai, Fudong Wang, Gui-Song Xia, Tianfu Wu, Liangpei Zhang
- **Comment**: Accepted by CVPR 2019
- **Journal**: None
- **Summary**: This paper presents a region-partition based attraction field dual representation for line segment maps, and thus poses the problem of line segment detection (LSD) as the region coloring problem. The latter is then addressed by learning deep convolutional neural networks (ConvNets) for accuracy, robustness and efficiency. For a 2D line segment map, our dual representation consists of three components: (i) A region-partition map in which every pixel is assigned to one and only one line segment; (ii) An attraction field map in which every pixel in a partition region is encoded by its 2D projection vector w.r.t. the associated line segment; and (iii) A squeeze module which squashes the attraction field to a line segment map that almost perfectly recovers the input one. By leveraging the duality, we learn ConvNets to compute the attraction field maps for raw in-put images, followed by the squeeze module for LSD, in an end-to-end manner. Our method rigorously addresses several challenges in LSD such as local ambiguity and class imbalance. Our method also harnesses the best practices developed in ConvNets based semantic segmentation methods such as the encoder-decoder architecture and the a-trous convolution. In experiments, our method is tested on the WireFrame dataset and the YorkUrban dataset with state-of-the-art performance obtained. Especially, we advance the performance by 4.5 percents on the WireFrame dataset. Our method is also fast with 6.6~10.4 FPS, outperforming most of existing line segment detectors.



### SADA: Semantic Adversarial Diagnostic Attacks for Autonomous Applications
- **Arxiv ID**: http://arxiv.org/abs/1812.02132v3
- **DOI**: 10.1609/aaai.v34i07.6722
- **Categories**: **cs.CV**, cs.CR, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1812.02132v3)
- **Published**: 2018-12-05 17:42:36+00:00
- **Updated**: 2019-12-02 16:20:42+00:00
- **Authors**: Abdullah Hamdi, Matthias Müller, Bernard Ghanem
- **Comment**: Accepted at AAAI'20
- **Journal**: AAAI 2020
- **Summary**: One major factor impeding more widespread adoption of deep neural networks (DNNs) is their lack of robustness, which is essential for safety-critical applications such as autonomous driving. This has motivated much recent work on adversarial attacks for DNNs, which mostly focus on pixel-level perturbations void of semantic meaning. In contrast, we present a general framework for adversarial attacks on trained agents, which covers semantic perturbations to the environment of the agent performing the task as well as pixel-level attacks. To do this, we re-frame the adversarial attack problem as learning a distribution of parameters that always fools the agent. In the semantic case, our proposed adversary (denoted as BBGAN) is trained to sample parameters that describe the environment with which the black-box agent interacts, such that the agent performs its dedicated task poorly in this environment. We apply BBGAN on three different tasks, primarily targeting aspects of autonomous navigation: object detection, self-driving, and autonomous UAV racing. On these tasks, BBGAN can generate failure cases that consistently fool a trained agent.



### An Unpaired Shape Transforming Method for Image Translation and Cross-Domain Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1812.02134v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.02134v3)
- **Published**: 2018-12-05 17:47:07+00:00
- **Updated**: 2021-08-18 12:16:08+00:00
- **Authors**: Kaili Wang, Liqian Ma, Jose Oramas, Luc Van Gool, Tinne Tuytelaars
- **Comment**: This manuscript is a pre-print currently under review at the Elsevier
  Journal Computer Vision and Image Under-standing (CVIU)
- **Journal**: None
- **Summary**: We address the problem of unpaired geometric image-to-image translation. Rather than transferring the style of an image as a whole, our goal is to translate the geometry of an object as depicted in different domains while preserving its appearance characteristics. Our model is trained in an unpaired fashion, i.e. without the need of paired images during training. It performs all steps of the shape transfer within a single model and without additional post-processing stages. Extensive experiments on the VITON, CMU-Multi-PIE and our own FashionStyle datasets show the effectiveness of the method. In addition, we show that despite their low-dimensionality, the features learned by our model are useful to the item retrieval task.



### Dissecting Person Re-identification from the Viewpoint of Viewpoint
- **Arxiv ID**: http://arxiv.org/abs/1812.02162v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.02162v6)
- **Published**: 2018-12-05 18:56:10+00:00
- **Updated**: 2019-06-17 14:29:25+00:00
- **Authors**: Xiaoxiao Sun, Liang Zheng
- **Comment**: 10 pages, 9 figures
- **Journal**: None
- **Summary**: Variations in visual factors such as viewpoint, pose, illumination and background, are usually viewed as important challenges in person re-identification (re-ID). In spite of acknowledging these factors to be influential, quantitative studies on how they affect a re-ID system are still lacking. To derive insights in this scientific campaign, this paper makes an early attempt in studying a particular factor, viewpoint. We narrow the viewpoint problem down to the pedestrian rotation angle to obtain focused conclusions. In this regard, this paper makes two contributions to the community. First, we introduce a large-scale synthetic data engine, PersonX. Composed of hand-crafted 3D person models, the salient characteristic of this engine is "controllable". That is, we are able to synthesize pedestrians by setting the visual variables to arbitrary values. Second, on the 3D data engine, we quantitatively analyze the influence of pedestrian rotation angle on re-ID accuracy. Comprehensively, the person rotation angles are precisely customized from 0 to 360, allowing us to investigate its effect on the training, query, and gallery sets. Extensive experiment helps us have a deeper understanding of the fundamental problems in person re-ID. Our research also provides useful insights for dataset building and future practical usage, e.g., a person of a side view makes a better query.



### Photo Wake-Up: 3D Character Animation from a Single Photo
- **Arxiv ID**: http://arxiv.org/abs/1812.02246v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1812.02246v1)
- **Published**: 2018-12-05 22:09:52+00:00
- **Updated**: 2018-12-05 22:09:52+00:00
- **Authors**: Chung-Yi Weng, Brian Curless, Ira Kemelmacher-Shlizerman
- **Comment**: The project page is at
  https://grail.cs.washington.edu/projects/wakeup/, and the supplementary video
  is at https://youtu.be/G63goXc5MyU
- **Journal**: None
- **Summary**: We present a method and application for animating a human subject from a single photo. E.g., the character can walk out, run, sit, or jump in 3D. The key contributions of this paper are: 1) an application of viewing and animating humans in single photos in 3D, 2) a novel 2D warping method to deform a posable template body model to fit the person's complex silhouette to create an animatable mesh, and 3) a method for handling partial self occlusions. We compare to state-of-the-art related methods and evaluate results with human studies. Further, we present an interactive interface that allows re-posing the person in 3D, and an augmented reality setup where the animated 3D person can emerge from the photo into the real world. We demonstrate the method on photos, posters, and art.



