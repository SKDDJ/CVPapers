# Arxiv Papers in cs.CV on 2018-12-11
### Learning Discriminative Motion Features Through Detection
- **Arxiv ID**: http://arxiv.org/abs/1812.04172v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.04172v1)
- **Published**: 2018-12-11 01:06:56+00:00
- **Updated**: 2018-12-11 01:06:56+00:00
- **Authors**: Gedas Bertasius, Christoph Feichtenhofer, Du Tran, Jianbo Shi, Lorenzo Torresani
- **Comment**: None
- **Journal**: None
- **Summary**: Despite huge success in the image domain, modern detection models such as Faster R-CNN have not been used nearly as much for video analysis. This is arguably due to the fact that detection models are designed to operate on single frames and as a result do not have a mechanism for learning motion representations directly from video. We propose a learning procedure that allows detection models such as Faster R-CNN to learn motion features directly from the RGB video data while being optimized with respect to a pose estimation task. Given a pair of video frames---Frame A and Frame B---we force our model to predict human pose in Frame A using the features from Frame B. We do so by leveraging deformable convolutions across space and time. Our network learns to spatially sample features from Frame B in order to maximize pose detection accuracy in Frame A. This naturally encourages our network to learn motion offsets encoding the spatial correspondences between the two frames. We refer to these motion offsets as DiMoFs (Discriminative Motion Features).   In our experiments we show that our training scheme helps learn effective motion cues, which can be used to estimate and localize salient human motion. Furthermore, we demonstrate that as a byproduct, our model also learns features that lead to improved pose detection in still-images, and better keypoint tracking. Finally, we show how to leverage our learned model for the tasks of spatiotemporal action localization and fine-grained action recognition.



### Material Based Object Tracking in Hyperspectral Videos: Benchmark and Algorithms
- **Arxiv ID**: http://arxiv.org/abs/1812.04179v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1812.04179v5)
- **Published**: 2018-12-11 01:35:15+00:00
- **Updated**: 2019-07-10 14:34:15+00:00
- **Authors**: Fengchao Xiong, Jun Zhou, Yuntao Qian
- **Comment**: Update results
- **Journal**: None
- **Summary**: Traditional color images only depict color intensities in red, green and blue channels, often making object trackers fail in challenging scenarios, e.g., background clutter and rapid changes of target appearance. Alternatively, material information of targets contained in a large amount of bands of hyperspectral images (HSI) is more robust to these difficult conditions. In this paper, we conduct a comprehensive study on how material information can be utilized to boost object tracking from three aspects: benchmark dataset, material feature representation and material based tracking. In terms of benchmark, we construct a dataset of fully-annotated videos, which contain both hyperspectral and color sequences of the same scene. Material information is represented by spectral-spatial histogram of multidimensional gradient, which describes the 3D local spectral-spatial structure in an HSI, and fractional abundances of constituted material components which encode the underlying material distribution. These two types of features are embedded into correlation filters, yielding material based tracking. Experimental results on the collected benchmark dataset show the potentials and advantages of material based object tracking.



### Channel selection using Gumbel Softmax
- **Arxiv ID**: http://arxiv.org/abs/1812.04180v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.04180v4)
- **Published**: 2018-12-11 01:41:07+00:00
- **Updated**: 2020-11-23 22:35:18+00:00
- **Authors**: Charles Herrmann, Richard Strong Bowen, Ramin Zabih
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: Important applications such as mobile computing require reducing the computational costs of neural network inference. Ideally, applications would specify their preferred tradeoff between accuracy and speed, and the network would optimize this end-to-end, using classification error to remove parts of the network. Increasing speed can be done either during training - e.g., pruning filters - or during inference - e.g., conditionally executing a subset of the layers. We propose a single end-to-end framework that can improve inference efficiency in both settings. We use a combination of batch activation loss and classification loss, and Gumbel reparameterization to learn network structure. We train end-to-end, and the same technique supports pruning as well as conditional computation. We obtain promising experimental results for ImageNet classification with ResNet (45-52% less computation).



### Loss Guided Activation for Action Recognition in Still Images
- **Arxiv ID**: http://arxiv.org/abs/1812.04194v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.04194v1)
- **Published**: 2018-12-11 02:43:45+00:00
- **Updated**: 2018-12-11 02:43:45+00:00
- **Authors**: Lu Liu, Robby T. Tan, Shaodi You
- **Comment**: Accepted to appear in ACCV 2018
- **Journal**: None
- **Summary**: One significant problem of deep-learning based human action recognition is that it can be easily misled by the presence of irrelevant objects or backgrounds. Existing methods commonly address this problem by employing bounding boxes on the target humans as part of the input, in both training and testing stages. This requirement of bounding boxes as part of the input is needed to enable the methods to ignore irrelevant contexts and extract only human features. However, we consider this solution is inefficient, since the bounding boxes might not be available. Hence, instead of using a person bounding box as an input, we introduce a human-mask loss to automatically guide the activations of the feature maps to the target human who is performing the action, and hence suppress the activations of misleading contexts. We propose a multi-task deep learning method that jointly predicts the human action class and human location heatmap. Extensive experiments demonstrate our approach is more robust compared to the baseline methods under the presence of irrelevant misleading contexts. Our method achieves 94.06\% and 40.65\% (in terms of mAP) on Stanford40 and MPII dataset respectively, which are 3.14\% and 12.6\% relative improvements over the best results reported in the literature, and thus set new state-of-the-art results. Additionally, unlike some existing methods, we eliminate the requirement of using a person bounding box as an input during testing.



### 2.5D Visual Sound
- **Arxiv ID**: http://arxiv.org/abs/1812.04204v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.04204v4)
- **Published**: 2018-12-11 03:23:10+00:00
- **Updated**: 2019-04-09 16:11:53+00:00
- **Authors**: Ruohan Gao, Kristen Grauman
- **Comment**: Published in CVPR 2019, project page:
  http://vision.cs.utexas.edu/projects/2.5D_visual_sound/
- **Journal**: None
- **Summary**: Binaural audio provides a listener with 3D sound sensation, allowing a rich perceptual experience of the scene. However, binaural recordings are scarcely available and require nontrivial expertise and equipment to obtain. We propose to convert common monaural audio into binaural audio by leveraging video. The key idea is that visual frames reveal significant spatial cues that, while explicitly lacking in the accompanying single-channel audio, are strongly linked to it. Our multi-modal approach recovers this link from unlabeled video. We devise a deep convolutional neural network that learns to decode the monaural (single-channel) soundtrack into its binaural counterpart by injecting visual information about object and scene configurations. We call the resulting output 2.5D visual sound---the visual stream helps "lift" the flat single channel audio into spatialized sound. In addition to sound generation, we show the self-supervised representation learned by our network benefits audio-visual source separation. Our video results: http://vision.cs.utexas.edu/projects/2.5D_visual_sound/



### Identity-Enhanced Network for Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/1812.04207v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.04207v1)
- **Published**: 2018-12-11 03:40:52+00:00
- **Updated**: 2018-12-11 03:40:52+00:00
- **Authors**: Yanwei Li, Xingang Wang, Shilei Zhang, Lingxi Xie, Wenqi Wu, Hongyuan Yu, Zheng Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Facial expression recognition is a challenging task, arguably because of large intra-class variations and high inter-class similarities. The core drawback of the existing approaches is the lack of ability to discriminate the changes in appearance caused by emotions and identities. In this paper, we present a novel identity-enhanced network (IDEnNet) to eliminate the negative impact of identity factor and focus on recognizing facial expressions. Spatial fusion combined with self-constrained multi-task learning are adopted to jointly learn the expression representations and identity-related information. We evaluate our approach on three popular datasets, namely Oulu-CASIA, CK+ and MMI. IDEnNet improves the baseline consistently, and achieves the best or comparable state-of-the-art on all three datasets.



### A Main/Subsidiary Network Framework for Simplifying Binary Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1812.04210v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.04210v1)
- **Published**: 2018-12-11 03:55:00+00:00
- **Updated**: 2018-12-11 03:55:00+00:00
- **Authors**: Yinghao Xu, Xin Dong, Yudian Li, Hao Su
- **Comment**: 9 pages and 9 figures
- **Journal**: None
- **Summary**: To reduce memory footprint and run-time latency, techniques such as neural network pruning and binarization have been explored separately. However, it is unclear how to combine the best of the two worlds to get extremely small and efficient models. In this paper, we, for the first time, define the filter-level pruning problem for binary neural networks, which cannot be solved by simply migrating existing structural pruning methods for full-precision models. A novel learning-based approach is proposed to prune filters in our main/subsidiary network framework, where the main network is responsible for learning representative features to optimize the prediction performance, and the subsidiary component works as a filter selector on the main network. To avoid gradient mismatch when training the subsidiary component, we propose a layer-wise and bottom-up scheme. We also provide the theoretical and experimental comparison between our learning-based and greedy rule-based methods. Finally, we empirically demonstrate the effectiveness of our approach applied on several binary models, including binarized NIN, VGG-11, and ResNet-18, on various image classification datasets.



### Automatic Feature Weight Determination using Indexing and Pseudo-Relevance Feedback for Multi-feature Content-Based Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1812.04215v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.04215v1)
- **Published**: 2018-12-11 04:21:07+00:00
- **Updated**: 2018-12-11 04:21:07+00:00
- **Authors**: Asheet Kumar, Shivam Choudhary, Vaibhav Singh Khokhar, Vikas Meena, Chiranjoy Chattopadhyay
- **Comment**: 9 pages, 6 figures
- **Journal**: None
- **Summary**: Content-based image retrieval (CBIR) is one of the most active research areas in multimedia information retrieval. Given a query image, the task is to search relevant images in a repository. Low level features like color, texture, and shape feature vectors of an image are always considered to be an important attribute in CBIR system. Thus the performance of the CBIR system can be enhanced by combining these feature vectors. In this paper, we propose a novel CBIR framework by applying to index using multiclass SVM and finding the appropriate weights of the individual features automatically using the relevance ratio and mean difference. We have taken four feature descriptors to represent color, texture and shape features. During retrieval, feature vectors of query image are combined, weighted and compared with feature vectors of images in the database to rank order the results. Experiments were performed on four benchmark datasets and performance is compared with existing techniques to validate the superiority of our proposed framework.



### LPD-Net: 3D Point Cloud Learning for Large-Scale Place Recognition and Environment Analysis
- **Arxiv ID**: http://arxiv.org/abs/1812.07050v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.07050v2)
- **Published**: 2018-12-11 04:42:24+00:00
- **Updated**: 2019-08-19 08:02:47+00:00
- **Authors**: Zhe Liu, Shunbo Zhou, Chuanzhe Suo, Yingtian Liu, Peng Yin, Hesheng Wang, Yun-Hui Liu
- **Comment**: This paper has been accepted by ICCV-2019
- **Journal**: None
- **Summary**: Point cloud based place recognition is still an open issue due to the difficulty in extracting local features from the raw 3D point cloud and generating the global descriptor, and it's even harder in the large-scale dynamic environments. In this paper, we develop a novel deep neural network, named LPD-Net (Large-scale Place Description Network), which can extract discriminative and generalizable global descriptors from the raw 3D point cloud. Two modules, the adaptive local feature extraction module and the graph-based neighborhood aggregation module, are proposed, which contribute to extract the local structures and reveal the spatial distribution of local features in the large-scale point cloud, with an end-to-end manner. We implement the proposed global descriptor in solving point cloud based retrieval tasks to achieve the large-scale place recognition. Comparison results show that our LPD-Net is much better than PointNetVLAD and reaches the state-of-the-art. We also compare our LPD-Net with the vision-based solutions to show the robustness of our approach to different weather and light conditions.



### Coarse-to-fine: A RNN-based hierarchical attention model for vehicle re-identification
- **Arxiv ID**: http://arxiv.org/abs/1812.04239v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.04239v1)
- **Published**: 2018-12-11 07:05:15+00:00
- **Updated**: 2018-12-11 07:05:15+00:00
- **Authors**: Xiu-Shen Wei, Chen-Lin Zhang, Lingqiao Liu, Chunhua Shen, Jianxin Wu
- **Comment**: ACCV 2018
- **Journal**: None
- **Summary**: Vehicle re-identification is an important problem and becomes desirable with the rapid expansion of applications in video surveillance and intelligent transportation. By recalling the identification process of human vision, we are aware that there exists a native hierarchical dependency when humans identify different vehicles. Specifically, humans always firstly determine one vehicle's coarse-grained category, i.e., the car model/type. Then, under the branch of the predicted car model/type, they are going to identify specific vehicles by relying on subtle visual cues, e.g., customized paintings and windshield stickers, at the fine-grained level. Inspired by the coarse-to-fine hierarchical process, we propose an end-to-end RNN-based Hierarchical Attention (RNN-HA) classification model for vehicle re-identification. RNN-HA consists of three mutually coupled modules: the first module generates image representations for vehicle images, the second hierarchical module models the aforementioned hierarchical dependent relationship, and the last attention module focuses on capturing the subtle visual information distinguishing specific vehicles from each other. By conducting comprehensive experiments on two vehicle re-identification benchmark datasets VeRi and VehicleID, we demonstrate that the proposed model achieves superior performance over state-of-the-art methods.



### Unsupervised Degradation Learning for Single Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1812.04240v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.04240v2)
- **Published**: 2018-12-11 07:07:58+00:00
- **Updated**: 2018-12-13 07:10:41+00:00
- **Authors**: Tianyu Zhao, Wenqi Ren, Changqing Zhang, Dongwei Ren, Qinghua Hu
- **Comment**: 9 pages, 9 figures
- **Journal**: None
- **Summary**: Deep Convolution Neural Networks (CNN) have achieved significant performance on single image super-resolution (SR) recently. However, existing CNN-based methods use artificially synthetic low-resolution (LR) and high-resolution (HR) image pairs to train networks, which cannot handle real-world cases since the degradation from HR to LR is much more complex than manually designed. To solve this problem, we propose a real-world LR images guided bi-cycle network for single image super-resolution, in which the bidirectional structural consistency is exploited to train both the degradation and SR reconstruction networks in an unsupervised way. Specifically, we propose a degradation network to model the real-world degradation process from HR to LR via generative adversarial networks, and these generated realistic LR images paired with real-world HR images are exploited for training the SR reconstruction network, forming the first cycle. Then in the second reverse cycle, consistency of real-world LR images are exploited to further stabilize the training of SR reconstruction and degradation networks. Extensive experiments on both synthetic and real-world images demonstrate that the proposed algorithm performs favorably against state-of-the-art single image SR methods.



### Non-local Meets Global: An Integrated Paradigm for Hyperspectral Denoising
- **Arxiv ID**: http://arxiv.org/abs/1812.04243v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.04243v2)
- **Published**: 2018-12-11 07:26:07+00:00
- **Updated**: 2019-03-27 04:00:22+00:00
- **Authors**: Wei He, Quanming Yao, Chao Li, Naoto Yokoya, Qibin Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Non-local low-rank tensor approximation has been developed as a state-of-the-art method for hyperspectral image (HSI) denoising. Unfortunately, with more spectral bands for HSI, while the running time of these methods significantly increases, their denoising performance benefits little. In this paper, we claim that the HSI underlines a global spectral low-rank subspace, and the spectral subspaces of each full band patch groups should underlie this global low-rank subspace. This motivates us to propose a unified spatial-spectral paradigm for HSI denoising. As the new model is hard to optimize, we further propose an efficient algorithm for optimization, which is motivated by alternating minimization. This is done by first learning a low-dimensional projection and the related reduced image from the noisy HSI. Then, the non-local low-rank denoising and iterative regularization are developed to refine the reduced image and projection, respectively. Finally, experiments on synthetic and both real datasets demonstrate the superiority against the other state-of-the-arts HSI denoising methods.



### PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud
- **Arxiv ID**: http://arxiv.org/abs/1812.04244v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.04244v2)
- **Published**: 2018-12-11 07:27:56+00:00
- **Updated**: 2019-05-16 15:50:01+00:00
- **Authors**: Shaoshuai Shi, Xiaogang Wang, Hongsheng Li
- **Comment**: Accepted by CVPR 2019
- **Journal**: None
- **Summary**: In this paper, we propose PointRCNN for 3D object detection from raw point cloud. The whole framework is composed of two stages: stage-1 for the bottom-up 3D proposal generation and stage-2 for refining proposals in the canonical coordinates to obtain the final detection results. Instead of generating proposals from RGB image or projecting point cloud to bird's view or voxels as previous methods do, our stage-1 sub-network directly generates a small number of high-quality 3D proposals from point cloud in a bottom-up manner via segmenting the point cloud of the whole scene into foreground points and background. The stage-2 sub-network transforms the pooled points of each proposal to canonical coordinates to learn better local spatial features, which is combined with global semantic features of each point learned in stage-1 for accurate box refinement and confidence prediction. Extensive experiments on the 3D detection benchmark of KITTI dataset show that our proposed architecture outperforms state-of-the-art methods with remarkable margins by using only point cloud as input. The code is available at https://github.com/sshaoshuai/PointRCNN.



### Classification-Reconstruction Learning for Open-Set Recognition
- **Arxiv ID**: http://arxiv.org/abs/1812.04246v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.04246v3)
- **Published**: 2018-12-11 07:34:28+00:00
- **Updated**: 2019-10-06 07:55:48+00:00
- **Authors**: Ryota Yoshihashi, Wen Shao, Rei Kawakami, Shaodi You, Makoto Iida, Takeshi Naemura
- **Comment**: 11 pages, 7 figures
- **Journal**: None
- **Summary**: Open-set classification is a problem of handling `unknown' classes that are not contained in the training dataset, whereas traditional classifiers assume that only known classes appear in the test environment. Existing open-set classifiers rely on deep networks trained in a supervised manner on known classes in the training set; this causes specialization of learned representations to known classes and makes it hard to distinguish unknowns from knowns. In contrast, we train networks for joint classification and reconstruction of input data. This enhances the learned representation so as to preserve information useful for separating unknowns from knowns, as well as to discriminate classes of knowns. Our novel Classification-Reconstruction learning for Open-Set Recognition (CROSR) utilizes latent representations for reconstruction and enables robust unknown detection without harming the known-class classification accuracy. Extensive experiments reveal that the proposed method outperforms existing deep open-set classifiers in multiple standard datasets and is robust to diverse outliers. The code is available in https://nae-lab.org/~rei/research/crosr/.



### Prior-Knowledge and Attention-based Meta-Learning for Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1812.04955v5
- **DOI**: 10.1016/j.knosys.2020.106609
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.04955v5)
- **Published**: 2018-12-11 08:56:47+00:00
- **Updated**: 2020-02-15 06:23:11+00:00
- **Authors**: Yunxiao Qin, Weiguo Zhang, Chenxu Zhao, Zezheng Wang, Xiangyu Zhu, Guojun Qi, Jingping Shi, Zhen Lei
- **Comment**: 15 pages
- **Journal**: knowledge-based systems-2021
- **Summary**: Recently, meta-learning has been shown as a promising way to solve few-shot learning. In this paper, inspired by the human cognition process which utilizes both prior-knowledge and vision attention in learning new knowledge, we present a novel paradigm of meta-learning approach with three developments to introduce attention mechanism and prior-knowledge for meta-learning. In our approach, prior-knowledge is responsible for helping meta-learner expressing the input data into high-level representation space, and attention mechanism enables meta-learner focusing on key features of the data in the representation space. Compared with existing meta-learning approaches that pay little attention to prior-knowledge and vision attention, our approach alleviates the meta-learner's few-shot cognition burden. Furthermore, a Task-Over-Fitting (TOF) problem, which indicates that the meta-learner has poor generalization on different K-shot learning tasks, is discovered and we propose a Cross-Entropy across Tasks (CET) metric to model and solve the TOF problem. Extensive experiments demonstrate that we improve the meta-learner with state-of-the-art performance on several few-shot learning benchmarks, and at the same time the TOF problem can also be released greatly.



### Domain-Aware SE Network for Sketch-based Image Retrieval with Multiplicative Euclidean Margin Softmax
- **Arxiv ID**: http://arxiv.org/abs/1812.04275v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.04275v2)
- **Published**: 2018-12-11 08:57:31+00:00
- **Updated**: 2021-07-26 05:44:41+00:00
- **Authors**: Peng Lu, Gao Huang, Hangyu Lin, Wenming Yang, Guodong Guo, Yanwei Fu
- **Comment**: Accepted in ACMMM 21
- **Journal**: None
- **Summary**: This paper proposes a novel approach for Sketch-Based Image Retrieval (SBIR), for which the key is to bridge the gap between sketches and photos in terms of the data representation. Inspired by channel-wise attention explored in recent years, we present a Domain-Aware Squeeze-and-Excitation (DASE) network, which seamlessly incorporates the prior knowledge of sample sketch or photo into SE module and make the SE module capable of emphasizing appropriate channels according to domain signal. Accordingly, the proposed network can switch its mode to achieve a better domain feature with lower intra-class discrepancy. Moreover, while previous works simply focus on minimizing intra-class distance and maximizing inter-class distance, we introduce a loss function, named Multiplicative Euclidean Margin Softmax (MEMS), which introduces multiplicative Euclidean margin into feature space and ensure that the maximum intra-class distance is smaller than the minimum inter-class distance. This facilitates learning a highly discriminative feature space and ensures a more accurate image retrieval result. Extensive experiments are conducted on two widely used SBIR benchmark datasets. Our approach achieves better results on both datasets, surpassing the state-of-the-art methods by a large margin.



### Deep Density-based Image Clustering
- **Arxiv ID**: http://arxiv.org/abs/1812.04287v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.04287v1)
- **Published**: 2018-12-11 09:27:20+00:00
- **Updated**: 2018-12-11 09:27:20+00:00
- **Authors**: Yazhou Ren, Ni Wang, Mingxia Li, Zenglin Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, deep clustering, which is able to perform feature learning that favors clustering tasks via deep neural networks, has achieved remarkable performance in image clustering applications. However, the existing deep clustering algorithms generally need the number of clusters in advance, which is usually unknown in real-world tasks. In addition, the initial cluster centers in the learned feature space are generated by $k$-means. This only works well on spherical clusters and probably leads to unstable clustering results. In this paper, we propose a two-stage deep density-based image clustering (DDC) framework to address these issues. The first stage is to train a deep convolutional autoencoder (CAE) to extract low-dimensional feature representations from high-dimensional image data, and then apply t-SNE to further reduce the data to a 2-dimensional space favoring density-based clustering algorithms. The second stage is to apply the developed density-based clustering technique on the 2-dimensional embedded data to automatically recognize an appropriate number of clusters with arbitrary shapes. Concretely, a number of local clusters are generated to capture the local structures of clusters, and then are merged via their density relationship to form the final clustering result. Experiments demonstrate that the proposed DDC achieves comparable or even better clustering performance than state-of-the-art deep clustering methods, even though the number of clusters is not given.



### Deep RBFNet: Point Cloud Feature Learning using Radial Basis Functions
- **Arxiv ID**: http://arxiv.org/abs/1812.04302v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.04302v2)
- **Published**: 2018-12-11 09:45:04+00:00
- **Updated**: 2019-04-06 11:39:29+00:00
- **Authors**: Weikai Chen, Xiaoguang Han, Guanbin Li, Chao Chen, Jun Xing, Yajie Zhao, Hao Li
- **Comment**: Technical Report
- **Journal**: None
- **Summary**: Three-dimensional object recognition has recently achieved great progress thanks to the development of effective point cloud-based learning frameworks, such as PointNet and its extensions. However, existing methods rely heavily on fully connected layers, which introduce a significant amount of parameters, making the network harder to train and prone to overfitting problems. In this paper, we propose a simple yet effective framework for point set feature learning by leveraging a nonlinear activation layer encoded by Radial Basis Function (RBF) kernels. Unlike PointNet variants, that fail to recognize local point patterns, our approach explicitly models the spatial distribution of point clouds by aggregating features from sparsely distributed RBF kernels. A typical RBF kernel, e.g. Gaussian function, naturally penalizes long-distance response and is only activated by neighboring points. Such localized response generates highly discriminative features given different point distributions. In addition, our framework allows the joint optimization of kernel distribution and its receptive field, automatically evolving kernel configurations in an end-to-end manner. We demonstrate that the proposed network with a single RBF layer can outperform the state-of-the-art Pointnet++ in terms of classification accuracy for 3D object recognition tasks. Moreover, the introduction of nonlinear mappings significantly reduces the number of network parameters and computational cost, enabling significantly faster training and a deployable point cloud recognition solution on portable devices with limited resources.



### Analytic heuristics for a fast DSC-MRI
- **Arxiv ID**: http://arxiv.org/abs/1812.04303v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.NA, 94A08, 65T60
- **Links**: [PDF](http://arxiv.org/pdf/1812.04303v1)
- **Published**: 2018-12-11 09:46:45+00:00
- **Updated**: 2018-12-11 09:46:45+00:00
- **Authors**: Marco Virgulin, Marco Castellaro, Enrico Grisan, Fabio Marcuzzi
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we propose a deterministic approach for the reconstruction of Dynamic Susceptibility Contrast magnetic resonance imaging data and compare it with the compressed sensing solution existing in the literature for the same problem. Our study is based on the mathematical analysis of the problem, which is computationally intractable because of its non polynomial complexity, but suggests simple heuristics that perform quite well. We give results on real images and on artificial phantoms with added noise.



### Multichannel Semantic Segmentation with Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1812.04351v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.04351v1)
- **Published**: 2018-12-11 12:26:38+00:00
- **Updated**: 2018-12-11 12:26:38+00:00
- **Authors**: Kohei Watanabe, Kuniaki Saito, Yoshitaka Ushiku, Tatsuya Harada
- **Comment**: published on AUTONUE Workshops of ECCV 2018
- **Journal**: None
- **Summary**: Most contemporary robots have depth sensors, and research on semantic segmentation with RGBD images has shown that depth images boost the accuracy of segmentation. Since it is time-consuming to annotate images with semantic labels per pixel, it would be ideal if we could avoid this laborious work by utilizing an existing dataset or a synthetic dataset which we can generate on our own. Robot motions are often tested in a synthetic environment, where multichannel (eg, RGB + depth + instance boundary) images plus their pixel-level semantic labels are available. However, models trained simply on synthetic images tend to demonstrate poor performance on real images. In order to address this, we propose two approaches that can efficiently exploit multichannel inputs combined with an unsupervised domain adaptation (UDA) algorithm. One is a fusion-based approach that uses depth images as inputs. The other is a multitask learning approach that uses depth images as outputs. We demonstrated that the segmentation results were improved by using a multitask learning approach with a post-process and created a benchmark for this task.



### Proximal Mean-field for Neural Network Quantization
- **Arxiv ID**: http://arxiv.org/abs/1812.04353v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.04353v3)
- **Published**: 2018-12-11 12:27:54+00:00
- **Updated**: 2019-08-19 23:27:28+00:00
- **Authors**: Thalaiyasingam Ajanthan, Puneet K. Dokania, Richard Hartley, Philip H. S. Torr
- **Comment**: None
- **Journal**: ICCV, 2019
- **Summary**: Compressing large Neural Networks (NN) by quantizing the parameters, while maintaining the performance is highly desirable due to reduced memory and time complexity. In this work, we cast NN quantization as a discrete labelling problem, and by examining relaxations, we design an efficient iterative optimization procedure that involves stochastic gradient descent followed by a projection. We prove that our simple projected gradient descent approach is, in fact, equivalent to a proximal version of the well-known mean-field method. These findings would allow the decades-old and theoretically grounded research on MRF optimization to be used to design better network quantization schemes. Our experiments on standard classification datasets (MNIST, CIFAR10/100, TinyImageNet) with convolutional and residual architectures show that our algorithm obtains fully-quantized networks with accuracies very close to the floating-point reference networks.



### Exploiting Kernel Sparsity and Entropy for Interpretable CNN Compression
- **Arxiv ID**: http://arxiv.org/abs/1812.04368v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.04368v2)
- **Published**: 2018-12-11 12:52:22+00:00
- **Updated**: 2019-04-01 08:44:02+00:00
- **Authors**: Yuchao Li, Shaohui Lin, Baochang Zhang, Jianzhuang Liu, David Doermann, Yongjian Wu, Feiyue Huang, Rongrong Ji
- **Comment**: 10 pagers
- **Journal**: None
- **Summary**: Compressing convolutional neural networks (CNNs) has received ever-increasing research focus. However, most existing CNN compression methods do not interpret their inherent structures to distinguish the implicit redundancy. In this paper, we investigate the problem of CNN compression from a novel interpretable perspective. The relationship between the input feature maps and 2D kernels is revealed in a theoretical framework, based on which a kernel sparsity and entropy (KSE) indicator is proposed to quantitate the feature map importance in a feature-agnostic manner to guide model compression. Kernel clustering is further conducted based on the KSE indicator to accomplish high-precision CNN compression. KSE is capable of simultaneously compressing each layer in an efficient way, which is significantly faster compared to previous data-driven feature map pruning methods. We comprehensively evaluate the compression and speedup of the proposed method on CIFAR-10, SVHN and ImageNet 2012. Our method demonstrates superior performance gains over previous ones. In particular, it achieves 4.7 \times FLOPs reduction and 2.9 \times compression on ResNet-50 with only a Top-5 accuracy drop of 0.35% on ImageNet 2012, which significantly outperforms state-of-the-art methods.



### Deep Reader: Information extraction from Document images via relation extraction and Natural Language
- **Arxiv ID**: http://arxiv.org/abs/1812.04377v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1812.04377v2)
- **Published**: 2018-12-11 13:09:13+00:00
- **Updated**: 2018-12-14 16:46:59+00:00
- **Authors**: Vishwanath D, Rohit Rahul, Gunjan Sehgal, Swati, Arindam Chowdhury, Monika Sharma, Lovekesh Vig, Gautam Shroff, Ashwin Srinivasan
- **Comment**: Published in 3rd International Workshop on Robust Reading at Asian
  Conference of Computer Vision 2018
- **Journal**: None
- **Summary**: Recent advancements in the area of Computer Vision with state-of-art Neural Networks has given a boost to Optical Character Recognition (OCR) accuracies. However, extracting characters/text alone is often insufficient for relevant information extraction as documents also have a visual structure that is not captured by OCR. Extracting information from tables, charts, footnotes, boxes, headings and retrieving the corresponding structured representation for the document remains a challenge and finds application in a large number of real-world use cases. In this paper, we propose a novel enterprise based end-to-end framework called DeepReader which facilitates information extraction from document images via identification of visual entities and populating a meta relational model across different entities in the document image. The model schema allows for an easy to understand abstraction of the entities detected by the deep vision models and the relationships between them. DeepReader has a suite of state-of-the-art vision algorithms which are applied to recognize handwritten and printed text, eliminate noisy effects, identify the type of documents and detect visual entities like tables, lines and boxes. Deep Reader maps the extracted entities into a rich relational schema so as to capture all the relevant relationships between entities (words, textboxes, lines etc) detected in the document. Relevant information and fields can then be extracted from the document by writing SQL queries on top of the relationship tables. A natural language based interface is added on top of the relationship schema so that a non-technical user, specifying the queries in natural language, can fetch the information with minimal effort. In this paper, we also demonstrate many different capabilities of Deep Reader and report results on a real-world use case.



### Reading Industrial Inspection Sheets by Inferring Visual Relations
- **Arxiv ID**: http://arxiv.org/abs/1812.07104v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1812.07104v1)
- **Published**: 2018-12-11 13:10:14+00:00
- **Updated**: 2018-12-11 13:10:14+00:00
- **Authors**: Rohit Rahul, Arindam Chowdhury, Animesh, Samarth Mittal, Lovekesh Vig
- **Comment**: Published in 3rd International Workshop on Robust Reading at Asian
  Conference on Computer Vision 2018
- **Journal**: None
- **Summary**: The traditional mode of recording faults in heavy factory equipment has been via hand marked inspection sheets, wherein a machine engineer manually marks the faulty machine regions on a paper outline of the machine. Over the years, millions of such inspection sheets have been recorded and the data within these sheets has remained inaccessible. However, with industries going digital and waking up to the potential value of fault data for machine health monitoring, there is an increased impetus towards digitization of these hand marked inspection records. To target this digitization, we propose a novel visual pipeline combining state of the art deep learning models, with domain knowledge and low level vision techniques, followed by inference of visual relationships. Our framework is robust to the presence of both static and non-static background in the document, variability in the machine template diagrams, unstructured shape of graphical objects to be identified and variability in the strokes of handwritten text. The proposed pipeline incorporates a capsule and spatial transformer network based classifier for accurate text reading, and a customized CTPN network for text detection in addition to hybrid techniques for arrow detection and dialogue cloud removal. We have tested our approach on a real world dataset of 50 inspection sheets for large containers and boilers. The results are visually appealing and the pipeline achieved an accuracy of 87.1% for text detection and 94.6% for text reading.



### Object-centric Auto-encoders and Dummy Anomalies for Abnormal Event Detection in Video
- **Arxiv ID**: http://arxiv.org/abs/1812.04960v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.04960v2)
- **Published**: 2018-12-11 13:22:02+00:00
- **Updated**: 2019-04-07 09:14:28+00:00
- **Authors**: Radu Tudor Ionescu, Fahad Shahbaz Khan, Mariana-Iuliana Georgescu, Ling Shao
- **Comment**: Accepted at CVPR 2019
- **Journal**: None
- **Summary**: Abnormal event detection in video is a challenging vision problem. Most existing approaches formulate abnormal event detection as an outlier detection task, due to the scarcity of anomalous data during training. Because of the lack of prior information regarding abnormal events, these methods are not fully-equipped to differentiate between normal and abnormal events. In this work, we formalize abnormal event detection as a one-versus-rest binary classification problem. Our contribution is two-fold. First, we introduce an unsupervised feature learning framework based on object-centric convolutional auto-encoders to encode both motion and appearance information. Second, we propose a supervised classification approach based on clustering the training samples into normality clusters. A one-versus-rest abnormal event classifier is then employed to separate each normality cluster from the rest. For the purpose of training the classifier, the other clusters act as dummy anomalies. During inference, an object is labeled as abnormal if the highest classification score assigned by the one-versus-rest classifiers is negative. Comprehensive experiments are performed on four benchmarks: Avenue, ShanghaiTech, UCSD and UMN. Our approach provides superior results on all four data sets. On the large-scale ShanghaiTech data set, our method provides an absolute gain of 8.4% in terms of frame-level AUC compared to the state-of-the-art method [Sultani et al., CVPR 2018].



### Towards Automatic Identification of Elephants in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1812.04418v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.04418v1)
- **Published**: 2018-12-11 14:13:16+00:00
- **Updated**: 2018-12-11 14:13:16+00:00
- **Authors**: Matthias Körschens, Björn Barz, Joachim Denzler
- **Comment**: Presented at the AI for Wildlife Conservation (AIWC) 2018 workshop in
  Stockholm (https://sites.google.com/a/usc.edu/aiwc/home)
- **Journal**: None
- **Summary**: Identifying animals from a large group of possible individuals is very important for biodiversity monitoring and especially for collecting data on a small number of particularly interesting individuals, as these have to be identified first before this can be done. Identifying them can be a very time-consuming task. This is especially true, if the animals look very similar and have only a small number of distinctive features, like elephants do. In most cases the animals stay at one place only for a short period of time during which the animal needs to be identified for knowing whether it is important to collect new data on it. For this reason, a system supporting the researchers in identifying elephants to speed up this process would be of great benefit. In this paper, we present such a system for identifying elephants in the face of a large number of individuals with only few training images per individual. For that purpose, we combine object part localization, off-the-shelf CNN features, and support vector machine classification to provide field researches with proposals of possible individuals given new images of an elephant. The performance of our system is demonstrated on a dataset comprising a total of 2078 images of 276 individual elephants, where we achieve 56% top-1 test accuracy and 80% top-10 accuracy. To deal with occlusion, varying viewpoints, and different poses present in the dataset, we furthermore enable the analysts to provide the system with multiple images of the same elephant to be identified and aggregate confidence values generated by the classifier. With that, our system achieves a top-1 accuracy of 74% and a top-10 accuracy of 88% on the held-out test dataset.



### Zero-Shot Learning with Sparse Attribute Propagation
- **Arxiv ID**: http://arxiv.org/abs/1812.04427v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.04427v2)
- **Published**: 2018-12-11 14:28:20+00:00
- **Updated**: 2019-03-18 07:25:39+00:00
- **Authors**: Nanyi Fei, Jiechao Guan, Zhiwu Lu, Tao Xiang, Ji-Rong Wen
- **Comment**: None
- **Journal**: None
- **Summary**: Zero-shot learning (ZSL) aims to recognize a set of unseen classes without any training images. The standard approach to ZSL requires a set of training images annotated with seen class labels and a semantic descriptor for seen/unseen classes (attribute vector is the most widely used). Class label/attribute annotation is expensive; it thus severely limits the scalability of ZSL. In this paper, we define a new ZSL setting where only a few annotated images are collected from each seen class. This is clearly more challenging yet more realistic than the conventional ZSL setting. To overcome the resultant image-level attribute sparsity, we propose a novel inductive ZSL model termed sparse attribute propagation (SAP) by propagating attribute annotations to more unannotated images using sparse coding. This is followed by learning bidirectional projections between features and attributes for ZSL. An efficient solver is provided, together with rigorous theoretic algorithm analysis. With our SAP, we show that a ZSL training dataset can now be augmented by the abundant web images returned by image search engine, to further improve the model performance. Moreover, the general applicability of SAP is demonstrated on solving the social image annotation (SIA) problem. Extensive experiments show that our model achieves superior performance on both ZSL and SIA.



### Face-Focused Cross-Stream Network for Deception Detection in Videos
- **Arxiv ID**: http://arxiv.org/abs/1812.04429v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.04429v1)
- **Published**: 2018-12-11 14:32:20+00:00
- **Updated**: 2018-12-11 14:32:20+00:00
- **Authors**: Mingyu Ding, An Zhao, Zhiwu Lu, Tao Xiang, Ji-Rong Wen
- **Comment**: None
- **Journal**: None
- **Summary**: Automated deception detection (ADD) from real-life videos is a challenging task. It specifically needs to address two problems: (1) Both face and body contain useful cues regarding whether a subject is deceptive. How to effectively fuse the two is thus key to the effectiveness of an ADD model. (2) Real-life deceptive samples are hard to collect; learning with limited training data thus challenges most deep learning based ADD models. In this work, both problems are addressed. Specifically, for face-body multimodal learning, a novel face-focused cross-stream network (FFCSN) is proposed. It differs significantly from the popular two-stream networks in that: (a) face detection is added into the spatial stream to capture the facial expressions explicitly, and (b) correlation learning is performed across the spatial and temporal streams for joint deep feature learning across both face and body. To address the training data scarcity problem, our FFCSN model is trained with both meta learning and adversarial learning. Extensive experiments show that our FFCSN model achieves state-of-the-art results. Further, the proposed FFCSN model as well as its robust training strategy are shown to be generally applicable to other human-centric video analysis tasks such as emotion recognition from user-generated videos.



### Coconditional Autoencoding Adversarial Networks for Chinese Font Feature Learning
- **Arxiv ID**: http://arxiv.org/abs/1812.04451v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.04451v2)
- **Published**: 2018-12-11 15:08:23+00:00
- **Updated**: 2018-12-12 14:22:34+00:00
- **Authors**: Zhizhan Zheng, Feiyun Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we propose a novel framework named Coconditional Autoencoding Adversarial Networks (CocoAAN) for Chinese font learning, which jointly learns a generation network and two encoding networks of different feature domains using an adversarial process. The encoding networks map the glyph images into style and content features respectively via the pairwise substitution optimization strategy, and the generation network maps these two kinds of features to glyph samples. Together with a discriminative network conditioned on the extracted features, our framework succeeds in producing realistic-looking Chinese glyph images flexibly. Unlike previous models relying on the complex segmentation of Chinese components or strokes, our model can "parse" structures in an unsupervised way, through which the content feature representation of each character is captured. Experiments demonstrate our framework has a powerful generalization capacity to other unseen fonts and characters.



### Grounded Human-Object Interaction Hotspots from Video
- **Arxiv ID**: http://arxiv.org/abs/1812.04558v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.04558v2)
- **Published**: 2018-12-11 17:25:57+00:00
- **Updated**: 2019-04-02 23:53:56+00:00
- **Authors**: Tushar Nagarajan, Christoph Feichtenhofer, Kristen Grauman
- **Comment**: None
- **Journal**: None
- **Summary**: Learning how to interact with objects is an important step towards embodied visual intelligence, but existing techniques suffer from heavy supervision or sensing requirements. We propose an approach to learn human-object interaction "hotspots" directly from video. Rather than treat affordances as a manually supervised semantic segmentation task, our approach learns about interactions by watching videos of real human behavior and anticipating afforded actions. Given a novel image or video, our model infers a spatial hotspot map indicating how an object would be manipulated in a potential interaction-- even if the object is currently at rest. Through results with both first and third person video, we show the value of grounding affordances in real human-object interactions. Not only are our weakly supervised hotspots competitive with strongly supervised affordance methods, but they can also anticipate object interaction for novel object categories.



### Adversarial Framing for Image and Video Classification
- **Arxiv ID**: http://arxiv.org/abs/1812.04599v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.04599v3)
- **Published**: 2018-12-11 18:39:29+00:00
- **Updated**: 2019-10-17 14:03:07+00:00
- **Authors**: Konrad Zolna, Michal Zajac, Negar Rostamzadeh, Pedro O. Pinheiro
- **Comment**: This is an extended version of the paper published at 33rd AAAI
  Conference on Artificial Intelligence (see
  https://doi.org/10.1609/aaai.v33i01.330110077 )
- **Journal**: None
- **Summary**: Neural networks are prone to adversarial attacks. In general, such attacks deteriorate the quality of the input by either slightly modifying most of its pixels, or by occluding it with a patch. In this paper, we propose a method that keeps the image unchanged and only adds an adversarial framing on the border of the image. We show empirically that our method is able to successfully attack state-of-the-art methods on both image and video classification problems. Notably, the proposed method results in a universal attack which is very fast at test time. Source code can be found at https://github.com/zajaczajac/adv_framing .



### Diagnostic Visualization for Deep Neural Networks Using Stochastic Gradient Langevin Dynamics
- **Arxiv ID**: http://arxiv.org/abs/1812.04604v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1812.04604v1)
- **Published**: 2018-12-11 18:43:52+00:00
- **Updated**: 2018-12-11 18:43:52+00:00
- **Authors**: Biye Jiang, David M. Chan, Tianhao Zhang, John F. Canny
- **Comment**: None
- **Journal**: None
- **Summary**: The internal states of most deep neural networks are difficult to interpret, which makes diagnosis and debugging during training challenging. Activation maximization methods are widely used, but lead to multiple optima and are hard to interpret (appear noise-like) for complex neurons. Image-based methods use maximally-activating image regions which are easier to interpret, but do not provide pixel-level insight into why the neuron responds to them. In this work we introduce an MCMC method: Langevin Dynamics Activation Maximization (LDAM), which is designed for diagnostic visualization. LDAM provides two affordances in combination: the ability to explore the set of maximally activating pre-images, and the ability to trade-off interpretability and pixel-level accuracy using a GAN-style discriminator as a regularizer. We present case studies on MNIST, CIFAR and ImageNet datasets exploring these trade-offs. Finally we show that diagnostic visualization using LDAM leads to a novel insight into the parameter averaging method for deep net training.



### DeepV2D: Video to Depth with Differentiable Structure from Motion
- **Arxiv ID**: http://arxiv.org/abs/1812.04605v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.04605v4)
- **Published**: 2018-12-11 18:47:12+00:00
- **Updated**: 2020-04-27 19:17:43+00:00
- **Authors**: Zachary Teed, Jia Deng
- **Comment**: None
- **Journal**: None
- **Summary**: We propose DeepV2D, an end-to-end deep learning architecture for predicting depth from video. DeepV2D combines the representation ability of neural networks with the geometric principles governing image formation. We compose a collection of classical geometric algorithms, which are converted into trainable modules and combined into an end-to-end differentiable architecture. DeepV2D interleaves two stages: motion estimation and depth estimation. During inference, motion and depth estimation are alternated and converge to accurate depth. Code is available https://github.com/princeton-vl/DeepV2D.



### Deep Anomaly Detection with Outlier Exposure
- **Arxiv ID**: http://arxiv.org/abs/1812.04606v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.04606v3)
- **Published**: 2018-12-11 18:49:50+00:00
- **Updated**: 2019-01-28 20:34:44+00:00
- **Authors**: Dan Hendrycks, Mantas Mazeika, Thomas Dietterich
- **Comment**: ICLR 2019; PyTorch code available at
  https://github.com/hendrycks/outlier-exposure
- **Journal**: None
- **Summary**: It is important to detect anomalous inputs when deploying machine learning systems. The use of larger and more complex inputs in deep learning magnifies the difficulty of distinguishing between anomalous and in-distribution examples. At the same time, diverse image and text data are available in enormous quantities. We propose leveraging these data to improve deep anomaly detection by training anomaly detectors against an auxiliary dataset of outliers, an approach we call Outlier Exposure (OE). This enables anomaly detectors to generalize and detect unseen anomalies. In extensive experiments on natural language processing and small- and large-scale vision tasks, we find that Outlier Exposure significantly improves detection performance. We also observe that cutting-edge generative models trained on CIFAR-10 may assign higher likelihoods to SVHN images than to CIFAR-10 images; we use OE to mitigate this issue. We also analyze the flexibility and robustness of Outlier Exposure, and identify characteristics of the auxiliary dataset that improve performance.



### Evaluating the Impact of Intensity Normalization on MR Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1812.04652v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.04652v1)
- **Published**: 2018-12-11 19:08:44+00:00
- **Updated**: 2018-12-11 19:08:44+00:00
- **Authors**: Jacob C. Reinhold, Blake E. Dewey, Aaron Carass, Jerry L. Prince
- **Comment**: SPIE Medical Imaging 2019
- **Journal**: None
- **Summary**: Image synthesis learns a transformation from the intensity features of an input image to yield a different tissue contrast of the output image. This process has been shown to have application in many medical image analysis tasks including imputation, registration, and segmentation. To carry out synthesis, the intensities of the input images are typically scaled--i.e., normalized--both in training to learn the transformation and in testing when applying the transformation, but it is not presently known what type of input scaling is optimal. In this paper, we consider seven different intensity normalization algorithms and three different synthesis methods to evaluate the impact of normalization. Our experiments demonstrate that intensity normalization as a preprocessing step improves the synthesis results across all investigated synthesis algorithms. Furthermore, we show evidence that suggests intensity normalization is vital for successful deep learning-based MR image synthesis.



### Femural Head Autosegmentation for 3D Radiotherapy Planning: Preliminary Results
- **Arxiv ID**: http://arxiv.org/abs/1812.04682v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.04682v1)
- **Published**: 2018-12-11 20:48:18+00:00
- **Updated**: 2018-12-11 20:48:18+00:00
- **Authors**: Bruno A. G. da Silva, Alvaro L. Fazenda, Fabiano C. Paixao
- **Comment**: None
- **Journal**: None
- **Summary**: Contouring of organs at risk is an important but time consuming part of radiotherapy treatment planning. Several authors proposed methods for automatic delineation but the clinical experts eye remains the gold standard method. In this paper, we present a totally visual software for automated delineation of the femural head. The software was successfully characterized in pelvic CT Scan of prostate patients (n=11). The automatic delineation was compared with manual and approved delineation through blind test evaluated by a panel of seniors radiation oncologists (n=9). Clinical experts evaluated that no any contouring correction were need in 77.8% and 67.8% of manual and automatic delineation respectively. Our results show that the software is robust, the automated delineation was reproducible in all patient, and its performance was similar to manually delineation.



### ECG Arrhythmia Classification Using Transfer Learning from 2-Dimensional Deep CNN Features
- **Arxiv ID**: http://arxiv.org/abs/1812.04693v1
- **DOI**: 10.1109/BIOCAS.2018.8584808
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.04693v1)
- **Published**: 2018-12-11 21:11:30+00:00
- **Updated**: 2018-12-11 21:11:30+00:00
- **Authors**: Milad Salem, Shayan Taheri, Jiann Shiun-Yuan
- **Comment**: Accepted and presented for IEEE Biomedical Circuits and Systems
  (BioCAS) on 17th-19th October 2018 in Ohio, USA
- **Journal**: None
- **Summary**: Due to the recent advances in the area of deep learning, it has been demonstrated that a deep neural network, trained on a huge amount of data, can recognize cardiac arrhythmias better than cardiologists. Moreover, traditionally feature extraction was considered an integral part of ECG pattern recognition; however, recent findings have shown that deep neural networks can carry out the task of feature extraction directly from the data itself. In order to use deep neural networks for their accuracy and feature extraction, high volume of training data is required, which in the case of independent studies is not pragmatic. To arise to this challenge, in this work, the identification and classification of four ECG patterns are studied from a transfer learning perspective, transferring knowledge learned from the image classification domain to the ECG signal classification domain. It is demonstrated that feature maps learned in a deep neural network trained on great amounts of generic input images can be used as general descriptors for the ECG signal spectrograms and result in features that enable classification of arrhythmias. Overall, an accuracy of 97.23 percent is achieved in classifying near 7000 instances by ten-fold cross validation.



### Anomaly Generation using Generative Adversarial Networks in Host Based Intrusion Detection
- **Arxiv ID**: http://arxiv.org/abs/1812.04697v1
- **DOI**: 10.1109/UEMCON.2018.8796769
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.04697v1)
- **Published**: 2018-12-11 21:21:09+00:00
- **Updated**: 2018-12-11 21:21:09+00:00
- **Authors**: Milad Salem, Shayan Taheri, Jiann Shiun Yuan
- **Comment**: Accepted and presented at IEEE Annual Ubiquitous Computing,
  Electronics, and Mobile Communications Conference (IEEE UEMCON) on 8th-10th
  November 2018
- **Journal**: None
- **Summary**: Generative adversarial networks have been able to generate striking results in various domains. This generation capability can be general while the networks gain deep understanding regarding the data distribution. In many domains, this data distribution consists of anomalies and normal data, with the anomalies commonly occurring relatively less, creating datasets that are imbalanced. The capabilities that generative adversarial networks offer can be leveraged to examine these anomalies and help alleviate the challenge that imbalanced datasets propose via creating synthetic anomalies. This anomaly generation can be specifically beneficial in domains that have costly data creation processes as well as inherently imbalanced datasets. One of the domains that fits this description is the host-based intrusion detection domain. In this work, ADFA-LD dataset is chosen as the dataset of interest containing system calls of small foot-print next generation attacks. The data is first converted into images, and then a Cycle-GAN is used to create images of anomalous data from images of normal data. The generated data is combined with the original dataset and is used to train a model to detect anomalies. By doing so, it is shown that the classification results are improved, with the AUC rising from 0.55 to 0.71, and the anomaly detection rate rising from 17.07% to 80.49%. The results are also compared to SMOTE, showing the potential presented by generative adversarial networks in anomaly generation.



### Rotation Invariant Descriptors for Galaxy Morphological Classification
- **Arxiv ID**: http://arxiv.org/abs/1812.04706v2
- **DOI**: None
- **Categories**: **cs.CV**, astro-ph.GA
- **Links**: [PDF](http://arxiv.org/pdf/1812.04706v2)
- **Published**: 2018-12-11 21:45:52+00:00
- **Updated**: 2019-08-20 21:14:27+00:00
- **Authors**: Hubert Cecotti
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: The detection of objects that are multi-oriented is a difficult pattern recognition problem. In this paper, we propose to evaluate the performance of different families of descriptors for the classification of galaxy morphologies. We investigate the performance of the Hu moments, Flusser moments, Zernike moments, Fourier-Mellin moments, and ring projection techniques based on 1D moment and the Fourier transform. We consider two main datasets for the performance evaluation. The first dataset is an artificial dataset based on representative templates from 11 types of galaxies, which are evaluated with different transformations (noise, smoothing), alone or combined. The evaluation is based on image retrieval performance to estimate the robustness of the rotation invariant descriptors with this type of images. The second dataset is composed of real images extracted from the Galaxy Zoo 2 project. The binary classification of elliptical and spiral galaxies is achieved with pre-processing steps including morphological filtering and a Laplacian pyramid. For the binary classification, we compare the different set of features with Support Vector Machines (SVM), Extreme Learning Machine, and different types of linear discriminant analysis techniques. The results support the conclusion that the proposed framework for the binary classification of elliptical and spiral galaxies provides an area under the ROC curve reaching 99.54%, proving the robustness of the approach for helping astronomers to study galaxies.



### Non-local Operational Anisotropic Diffusion Filter
- **Arxiv ID**: http://arxiv.org/abs/1812.04708v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1812.04708v1)
- **Published**: 2018-12-11 21:49:07+00:00
- **Updated**: 2018-12-11 21:49:07+00:00
- **Authors**: Fábio A. M. Cappabianco, Petrus P. C. E. da Silva
- **Comment**: 7 pages, 10 figures
- **Journal**: None
- **Summary**: High-frequency noise is present in several modalities of medical images. It originates from the acquisition process and may be related to the scanner configurations, the scanned body, or to other external factors. This way, prospective filters are an important tool to improve the image quality. In this paper, we propose a non-local weighted operational anisotropic diffusion filter and evaluate its effect on magnetic resonance images and on kV/CBCT radiotherapy images. We also provide a detailed analysis of non-local parameter settings. Results show that the new filter enhances previous local implementations and has potential application in radiotherapy treatments.



