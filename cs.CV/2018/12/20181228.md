# Arxiv Papers in cs.CV on 2018-12-28
### Signal Classification under structure sparsity constraints
- **Arxiv ID**: http://arxiv.org/abs/1812.10859v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.10859v1)
- **Published**: 2018-12-28 01:20:43+00:00
- **Updated**: 2018-12-28 01:20:43+00:00
- **Authors**: Tiep Huu Vu
- **Comment**: PhD Thesis
- **Journal**: None
- **Summary**: Object Classification is a key direction of research in signal and image processing, computer vision and artificial intelligence. The goal is to come up with algorithms that automatically analyze images and put them in predefined categories. This dissertation focuses on the theory and application of sparse signal processing and learning algorithms for image processing and computer vision, especially object classification problems. A key emphasis of this work is to formulate novel optimization problems for learning dictionary and structured sparse representations. Tractable solutions are proposed subsequently for the corresponding optimization problems.   An important goal of this dissertation is to demonstrate the wide applications of these algorithmic tools for real-world applications. To that end, we explored important problems in the areas of:   1. Medical imaging: histopathological images acquired from mammalian tissues, human breast tissues, and human brain tissues.   2. Low-frequency (UHF to L-band) ultra-wideband (UWB) synthetic aperture radar: detecting bombs and mines buried under rough surfaces.   3. General object classification: face, flowers, objects, dogs, indoor scenes, etc.



### Coarse-to-fine Semantic Segmentation from Image-level Labels
- **Arxiv ID**: http://arxiv.org/abs/1812.10885v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.10885v1)
- **Published**: 2018-12-28 04:04:06+00:00
- **Updated**: 2018-12-28 04:04:06+00:00
- **Authors**: Longlong Jing, Yucheng Chen, Yingli Tian
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural network-based semantic segmentation generally requires large-scale cost extensive annotations for training to obtain better performance. To avoid pixel-wise segmentation annotations which are needed for most methods, recently some researchers attempted to use object-level labels (e.g. bounding boxes) or image-level labels (e.g. image categories). In this paper, we propose a novel recursive coarse-to-fine semantic segmentation framework based on only image-level category labels. For each image, an initial coarse mask is first generated by a convolutional neural network-based unsupervised foreground segmentation model and then is enhanced by a graph model. The enhanced coarse mask is fed to a fully convolutional neural network to be recursively refined. Unlike existing image-level label-based semantic segmentation methods which require to label all categories for images contain multiple types of objects, our framework only needs one label for each image and can handle images contains multi-category objects. With only trained on ImageNet, our framework achieves comparable performance on PASCAL VOC dataset as other image-level label-based state-of-the-arts of semantic segmentation. Furthermore, our framework can be easily extended to foreground object segmentation task and achieves comparable performance with the state-of-the-art supervised methods on the Internet Object dataset.



### InstaGAN: Instance-aware Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/1812.10889v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.10889v2)
- **Published**: 2018-12-28 04:30:47+00:00
- **Updated**: 2019-01-02 09:29:21+00:00
- **Authors**: Sangwoo Mo, Minsu Cho, Jinwoo Shin
- **Comment**: Accepted to ICLR 2019. High resolution images are available in
  https://github.com/sangwoomo/instagan
- **Journal**: None
- **Summary**: Unsupervised image-to-image translation has gained considerable attention due to the recent impressive progress based on generative adversarial networks (GANs). However, previous methods often fail in challenging cases, in particular, when an image has multiple target instances and a translation task involves significant changes in shape, e.g., translating pants to skirts in fashion images. To tackle the issues, we propose a novel method, coined instance-aware GAN (InstaGAN), that incorporates the instance information (e.g., object segmentation masks) and improves multi-instance transfiguration. The proposed method translates both an image and the corresponding set of instance attributes while maintaining the permutation invariance property of the instances. To this end, we introduce a context preserving loss that encourages the network to learn the identity function outside of target instances. We also propose a sequential mini-batch inference/training technique that handles multiple instances with a limited GPU memory and enhances the network to generalize better for multiple instances. Our comparative evaluation demonstrates the effectiveness of the proposed method on different image datasets, in particular, in the aforementioned challenging cases. Code and results are available in https://github.com/sangwoomo/instagan



### Deep Convolutional Neural Networks in the Face of Caricature: Identity and Image Revealed
- **Arxiv ID**: http://arxiv.org/abs/1812.10902v1
- **DOI**: 10.1038/s42256-019-0111-7
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.10902v1)
- **Published**: 2018-12-28 06:16:23+00:00
- **Updated**: 2018-12-28 06:16:23+00:00
- **Authors**: Matthew Q. Hill, Connor J. Parde, Carlos D. Castillo, Y. Ivette Colon, Rajeev Ranjan, Jun-Cheng Chen, Volker Blanz, Alice J. O'Toole
- **Comment**: 8 pages, 5 figures
- **Journal**: None
- **Summary**: Real-world face recognition requires an ability to perceive the unique features of an individual face across multiple, variable images. The primate visual system solves the problem of image invariance using cascades of neurons that convert images of faces into categorical representations of facial identity. Deep convolutional neural networks (DCNNs) also create generalizable face representations, but with cascades of simulated neurons. DCNN representations can be examined in a multidimensional "face space", with identities and image parameters quantified via their projections onto the axes that define the space. We examined the organization of viewpoint, illumination, gender, and identity in this space. We show that the network creates a highly organized, hierarchically nested, face similarity structure in which information about face identity and imaging characteristics coexist. Natural image variation is accommodated in this hierarchy, with face identity nested under gender, illumination nested under identity, and viewpoint nested under illumination. To examine identity, we caricatured faces and found that network identification accuracy increased with caricature level, and--mimicking human perception--a caricatured distortion of a face "resembled" its veridical counterpart. Caricatures improved performance by moving the identity away from other identities in the face space and minimizing the effects of illumination and viewpoint. Deep networks produce face representations that solve long-standing computational problems in generalized face recognition. They also provide a unitary theoretical framework for reconciling decades of behavioral and neural results that emphasized either the image or the object/face in representations, without understanding how a neural code could seamlessly accommodate both.



### Divergence Triangle for Joint Training of Generator Model, Energy-based Model, and Inference Model
- **Arxiv ID**: http://arxiv.org/abs/1812.10907v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.10907v2)
- **Published**: 2018-12-28 06:35:39+00:00
- **Updated**: 2019-01-31 09:35:03+00:00
- **Authors**: Tian Han, Erik Nijkamp, Xiaolin Fang, Mitch Hill, Song-Chun Zhu, Ying Nian Wu
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes the divergence triangle as a framework for joint training of generator model, energy-based model and inference model. The divergence triangle is a compact and symmetric (anti-symmetric) objective function that seamlessly integrates variational learning, adversarial learning, wake-sleep algorithm, and contrastive divergence in a unified probabilistic formulation. This unification makes the processes of sampling, inference, energy evaluation readily available without the need for costly Markov chain Monte Carlo methods. Our experiments demonstrate that the divergence triangle is capable of learning (1) an energy-based model with well-formed energy landscape, (2) direct sampling in the form of a generator network, and (3) feed-forward inference that faithfully reconstructs observed as well as synthesized data. The divergence triangle is a robust training method that can learn from incomplete data.



### Spatiotemporal Data Fusion for Precipitation Nowcasting
- **Arxiv ID**: http://arxiv.org/abs/1812.10915v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.10915v1)
- **Published**: 2018-12-28 07:51:08+00:00
- **Updated**: 2018-12-28 07:51:08+00:00
- **Authors**: Vladimir Ivashkin, Vadim Lebedev
- **Comment**: None
- **Journal**: None
- **Summary**: Precipitation nowcasting using neural networks and ground-based radars has become one of the key components of modern weather prediction services, but it is limited to the regions covered by ground-based radars. Truly global precipitation nowcasting requires fusion of radar and satellite observations. We propose the data fusion pipeline based on computer vision techniques, including novel inpainting algorithm with soft masking.



### Salient Object Detection via High-to-Low Hierarchical Context Aggregation
- **Arxiv ID**: http://arxiv.org/abs/1812.10956v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.10956v2)
- **Published**: 2018-12-28 11:34:12+00:00
- **Updated**: 2019-04-01 01:29:52+00:00
- **Authors**: Yun Liu, Yu Qiu, Le Zhang, JiaWang Bian, Guang-Yu Nie, Ming-Ming Cheng
- **Comment**: We made a significant change and re-submitted as "DNA:
  Deeply-supervised Nonlinear Aggregation for Salient Object Detection",
  arXiv:1903.12476
- **Journal**: None
- **Summary**: Recent progress on salient object detection mainly aims at exploiting how to effectively integrate convolutional side-output features in convolutional neural networks (CNN). Based on this, most of the existing state-of-the-art saliency detectors design complex network structures to fuse the side-output features of the backbone feature extraction networks. However, should the fusion strategies be more and more complex for accurate salient object detection? In this paper, we observe that the contexts of a natural image can be well expressed by a high-to-low self-learning of side-output convolutional features. As we know, the contexts of an image usually refer to the global structures, and the top layers of CNN usually learn to convey global information. On the other hand, it is difficult for the intermediate side-output features to express contextual information. Here, we design an hourglass network with intermediate supervision to learn contextual features in a high-to-low manner. The learned hierarchical contexts are aggregated to generate the hybrid contextual expression for an input image. At last, the hybrid contextual features can be used for accurate saliency estimation. We extensively evaluate our method on six challenging saliency datasets, and our simple method achieves state-of-the-art performance under various evaluation metrics. Code will be released upon paper acceptance.



### TROVE Feature Detection for Online Pose Recovery by Binocular Cameras
- **Arxiv ID**: http://arxiv.org/abs/1812.10967v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1812.10967v1)
- **Published**: 2018-12-28 12:07:37+00:00
- **Updated**: 2018-12-28 12:07:37+00:00
- **Authors**: Yuance Liu, Michael Z. Q. Chen
- **Comment**: 18 pages, 21 figures
- **Journal**: None
- **Summary**: This paper proposes a new and efficient method to estimate 6-DoF ego-states: attitudes and positions in real time. The proposed method extract information of ego-states by observing a feature called "TROVE" (Three Rays and One VErtex). TROVE features are projected from structures that are ubiquitous on man-made constructions and objects. The proposed method does not search for conventional corner-type features nor use Perspective-n-Point (PnP) methods, and it achieves a real-time estimation of attitudes and positions up to 60 Hz. The accuracy of attitude estimates can reach 0.3 degrees and that of position estimates can reach 2 cm in an indoor environment. The result shows a promising approach for unmanned robots to localize in an environment that is rich in man-made structures.



### Car Detection using Unmanned Aerial Vehicles: Comparison between Faster R-CNN and YOLOv3
- **Arxiv ID**: http://arxiv.org/abs/1812.10968v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.10968v1)
- **Published**: 2018-12-28 12:08:55+00:00
- **Updated**: 2018-12-28 12:08:55+00:00
- **Authors**: Bilel Benjdira, Taha Khursheed, Anis Koubaa, Adel Ammar, Kais Ouni
- **Comment**: This paper is accepted in The 1st Unmanned Vehicle Systems conference
  in Oman, Feb 2019
- **Journal**: The 1st Unmanned Vehicle Systems conference in Oman, Feb 2019
- **Summary**: Unmanned Aerial Vehicles are increasingly being used in surveillance and traffic monitoring thanks to their high mobility and ability to cover areas at different altitudes and locations. One of the major challenges is to use aerial images to accurately detect cars and count them in real-time for traffic monitoring purposes. Several deep learning techniques were recently proposed based on convolution neural network (CNN) for real-time classification and recognition in computer vision. However, their performance depends on the scenarios where they are used. In this paper, we investigate the performance of two state-of-the-art CNN algorithms, namely Faster R-CNN and YOLOv3, in the context of car detection from aerial images. We trained and tested these two models on a large car dataset taken from UAVs. We demonstrated in this paper that YOLOv3 outperforms Faster R-CNN in sensitivity and processing time, although they are comparable in the precision metric.



### Reasoning About Physical Interactions with Object-Oriented Prediction and Planning
- **Arxiv ID**: http://arxiv.org/abs/1812.10972v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.10972v2)
- **Published**: 2018-12-28 12:18:23+00:00
- **Updated**: 2019-01-07 08:27:03+00:00
- **Authors**: Michael Janner, Sergey Levine, William T. Freeman, Joshua B. Tenenbaum, Chelsea Finn, Jiajun Wu
- **Comment**: ICLR 2019, project page:
  https://people.eecs.berkeley.edu/~janner/o2p2/
- **Journal**: None
- **Summary**: Object-based factorizations provide a useful level of abstraction for interacting with the world. Building explicit object representations, however, often requires supervisory signals that are difficult to obtain in practice. We present a paradigm for learning object-centric representations for physical scene understanding without direct supervision of object properties. Our model, Object-Oriented Prediction and Planning (O2P2), jointly learns a perception function to map from image observations to object representations, a pairwise physics interaction function to predict the time evolution of a collection of objects, and a rendering function to map objects back to pixels. For evaluation, we consider not only the accuracy of the physical predictions of the model, but also its utility for downstream tasks that require an actionable representation of intuitive physics. After training our model on an image prediction task, we can use its learned representations to build block towers more complicated than those observed during training.



### Honey Authentication with Machine Learning Augmented Bright-Field Microscopy
- **Arxiv ID**: http://arxiv.org/abs/1901.00516v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1901.00516v1)
- **Published**: 2018-12-28 13:01:25+00:00
- **Updated**: 2018-12-28 13:01:25+00:00
- **Authors**: Chloe He, Alexis Gkantiragas, Gerard Glowacki
- **Comment**: Accepted at the 'AI for Social Good' workshop at the 32nd Conference
  on Neural Information Processing Systems (NeurIPS2018), Montr\'eal, Canada
- **Journal**: None
- **Summary**: Honey has been collected and used by humankind as both a food and medicine for thousands of years. However, in the modern economy, honey has become subject to mislabelling and adulteration making it the third most faked food product in the world. The international scale of fraudulent honey has had both economic and environmental ramifications. In this paper, we propose a novel method of identifying fraudulent honey using machine learning augmented microscopy.



### Image Processing in Quantum Computers
- **Arxiv ID**: http://arxiv.org/abs/1812.11042v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.11042v3)
- **Published**: 2018-12-28 15:29:06+00:00
- **Updated**: 2019-02-11 19:03:12+00:00
- **Authors**: Aditya Dendukuri, Khoa Luu
- **Comment**: 6 pages, 7 figures
- **Journal**: None
- **Summary**: Quantum Image Processing (QIP)is an exciting new field showing a lot of promise as a powerful addition to the arsenal of Image Processing techniques. Representing image pixel by pixel using classical information requires an enormous amount of computational resources. Hence, exploring methods to represent images in a different paradigm of information is important. In this work, we study the representation of images in Quantum Information. The main motivation for this pursuit is the ability of storing N bits of classical information in only log(2N) quantum bits (qubits). The promising first step was the exponentially efficient implementation of the Fourier transform in quantum computers as compared to Fast Fourier Transform in classical computers. In addition, images encoded in quantum information could obey unique quantum properties like superposition or entanglement.



### Artistic Object Recognition by Unsupervised Style Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1812.11139v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.11139v1)
- **Published**: 2018-12-28 18:08:23+00:00
- **Updated**: 2018-12-28 18:08:23+00:00
- **Authors**: Christopher Thomas, Adriana Kovashka
- **Comment**: None
- **Journal**: Asian Conference on Computer Vision 2018 (ACCV)
- **Summary**: Computer vision systems currently lack the ability to reliably recognize artistically rendered objects, especially when such data is limited. In this paper, we propose a method for recognizing objects in artistic modalities (such as paintings, cartoons, or sketches), without requiring any labeled data from those modalities. Our method explicitly accounts for stylistic domain shifts between and within domains. To do so, we introduce a complementary training modality constructed to be similar in artistic style to the target domain, and enforce that the network learns features that are invariant between the two training modalities. We show how such artificial labeled source domains can be generated automatically through the use of style transfer techniques, using diverse target images to represent the style in the target domain. Unlike existing methods which require a large amount of unlabeled target data, our method can work with as few as ten unlabeled images. We evaluate it on a number of cross-domain object and scene classification tasks and on a new dataset we release. Our experiments show that our approach, though conceptually simple, significantly improves the accuracy that existing domain adaptation techniques obtain for artistic object recognition.



### Center Emphasized Visual Saliency and a Contrast-based Full Reference Image Quality Index
- **Arxiv ID**: http://arxiv.org/abs/1812.11163v3
- **DOI**: 10.3390/sym11030296
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.11163v3)
- **Published**: 2018-12-28 18:50:40+00:00
- **Updated**: 2019-02-26 09:24:50+00:00
- **Authors**: Md Abu Layek, Sanjida Afroz, TaeChoong Chung, Eui-Nam Huh
- **Comment**: This work was supported by Institute for Information & communications
  Technology Promotion(IITP) grant funded by the Korea government(MSIT)
  (No.2017-0-00294, Service mobility support distributed cloud technology)
- **Journal**: Symmetry 2019, 11, 296. Online:
  https://www.mdpi.com/2073-8994/11/3/296
- **Summary**: Objective image quality assessment (IQA) is imperative in the current multimedia-intensive world, in order to assess the visual quality of an image at close to a human level of ability. Many~parameters such as color intensity, structure, sharpness, contrast, presence of an object, etc., draw human attention to an image. Psychological vision research suggests that human vision is biased to the center area of an image and display screen. As a result, if the center part contains any visually salient information, it draws human attention even more and any distortion in that part will be better perceived than other parts. To the best of our knowledge, previous IQA methods have not considered this fact. In this paper, we propose a full reference image quality assessment (FR-IQA) approach using visual saliency and contrast; however, we give extra attention to the center by increasing the sensitivity of the similarity maps in that region. We evaluated our method on three large-scale popular benchmark databases used by most of the current IQA researchers (TID2008, CSIQ~and LIVE), having a total of 3345 distorted images with 28~different kinds of distortions. Our~method is compared with 13 state-of-the-art approaches. This comparison reveals the stronger correlation of our method with human-evaluated values. The prediction-of-quality score is consistent for distortion specific as well as distortion independent cases. Moreover, faster processing makes it applicable to any real-time application. The MATLAB code is publicly available to test the algorithm and can be found online at http://layek.khu.ac.kr/CEQI.



### Learning to Reconstruct Shapes from Unseen Classes
- **Arxiv ID**: http://arxiv.org/abs/1812.11166v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1812.11166v1)
- **Published**: 2018-12-28 18:52:50+00:00
- **Updated**: 2018-12-28 18:52:50+00:00
- **Authors**: Xiuming Zhang, Zhoutong Zhang, Chengkai Zhang, Joshua B. Tenenbaum, William T. Freeman, Jiajun Wu
- **Comment**: NeurIPS 2018 (Oral). The first two authors contributed equally to
  this paper. Project page: http://genre.csail.mit.edu/
- **Journal**: None
- **Summary**: From a single image, humans are able to perceive the full 3D shape of an object by exploiting learned shape priors from everyday life. Contemporary single-image 3D reconstruction algorithms aim to solve this task in a similar fashion, but often end up with priors that are highly biased by training classes. Here we present an algorithm, Generalizable Reconstruction (GenRe), designed to capture more generic, class-agnostic shape priors. We achieve this with an inference network and training procedure that combine 2.5D representations of visible surfaces (depth and silhouette), spherical shape representations of both visible and non-visible surfaces, and 3D voxel-based representations, in a principled manner that exploits the causal structure of how 3D shapes give rise to 2D images. Experiments demonstrate that GenRe performs well on single-view shape reconstruction, and generalizes to diverse novel objects from categories not seen during training.



### Class-Aware Adversarial Lung Nodule Synthesis in CT Images
- **Arxiv ID**: http://arxiv.org/abs/1812.11204v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.11204v1)
- **Published**: 2018-12-28 19:33:38+00:00
- **Updated**: 2018-12-28 19:33:38+00:00
- **Authors**: Jie Yang, Siqi Liu, Sasa Grbic, Arnaud Arindra Adiyoso Setio, Zhoubing Xu, Eli Gibson, Guillaume Chabin, Bogdan Georgescu, Andrew F. Laine, Dorin Comaniciu
- **Comment**: None
- **Journal**: None
- **Summary**: Though large-scale datasets are essential for training deep learning systems, it is expensive to scale up the collection of medical imaging datasets. Synthesizing the objects of interests, such as lung nodules, in medical images based on the distribution of annotated datasets can be helpful for improving the supervised learning tasks, especially when the datasets are limited by size and class balance. In this paper, we propose the class-aware adversarial synthesis framework to synthesize lung nodules in CT images. The framework is built with a coarse-to-fine patch in-painter (generator) and two class-aware discriminators. By conditioning on the random latent variables and the target nodule labels, the trained networks are able to generate diverse nodules given the same context. By evaluating on the public LIDC-IDRI dataset, we demonstrate an example application of the proposed framework for improving the accuracy of the lung nodule malignancy estimation as a binary classification problem, which is important in the lung screening scenario. We show that combining the real image patches and the synthetic lung nodules in the training set can improve the mean AUC classification score across different network architectures by 2%.



### CFA Bayer image sequence denoising and demosaicking chain
- **Arxiv ID**: http://arxiv.org/abs/1812.11207v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.11207v1)
- **Published**: 2018-12-28 19:51:39+00:00
- **Updated**: 2018-12-28 19:51:39+00:00
- **Authors**: Antoni Buades, Joan Duran
- **Comment**: None
- **Journal**: None
- **Summary**: The demosaicking provokes the spatial and color correlation of noise, which is afterwards enhanced by the imaging pipeline. The correct removal previous or simultaneously with the demosaicking process is not usually considered in the literature. We present a novel imaging chain including a denoising of the Bayer CFA and a demosaicking method for image sequences. The proposed algorithm uses a spatio-temporal patch method for the noise removal and demosaicking of the CFA. The experimentation, including real examples, illustrates the superior performance of the proposed chain, avoiding the creation of artifacts and colored spots in the final image.



### CamLoc: Pedestrian Location Detection from Pose Estimation on Resource-constrained Smart-cameras
- **Arxiv ID**: http://arxiv.org/abs/1812.11209v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.11209v1)
- **Published**: 2018-12-28 19:57:48+00:00
- **Updated**: 2018-12-28 19:57:48+00:00
- **Authors**: Adrian Cosma, Ion Emilian Radoi, Valentin Radu
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advancements in energy-efficient hardware technology is driving the exponential growth we are experiencing in the Internet of Things (IoT) space, with more pervasive computations being performed near to data generation sources. A range of intelligent devices and applications performing local detection is emerging (activity recognition, fitness monitoring, etc.) bringing with them obvious advantages such as reducing detection latency for improved interaction with devices and safeguarding user data by not leaving the device. Video processing holds utility for many emerging applications and data labelling in the IoT space. However, performing this video processing with deep neural networks at the edge of the Internet is not trivial. In this paper we show that pedestrian location estimation using deep neural networks is achievable on fixed cameras with limited compute resources. Our approach uses pose estimation from key body points detection to extend pedestrian skeleton when whole body not in image (occluded by obstacles or partially outside of frame), which achieves better location estimation performance (infrence time and memory footprint) compared to fitting a bounding box over pedestrian and scaling. We collect a sizable dataset comprising of over 2100 frames in videos from one and two surveillance cameras pointing from different angles at the scene, and annotate each frame with the exact position of person in image, in 42 different scenarios of activity and occlusion. We compare our pose estimation based location detection with a popular detection algorithm, YOLOv2, for overlapping bounding-box generation, our solution achieving faster inference time (15x speedup) at half the memory footprint, within resource capabilities on embedded devices, which demonstrate that CamLoc is an efficient solution for location estimation in videos on smart-cameras.



### Kymatio: Scattering Transforms in Python
- **Arxiv ID**: http://arxiv.org/abs/1812.11214v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.SD, eess.AS, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.11214v3)
- **Published**: 2018-12-28 20:53:29+00:00
- **Updated**: 2022-05-31 09:46:58+00:00
- **Authors**: Mathieu Andreux, Tomás Angles, Georgios Exarchakis, Roberto Leonarduzzi, Gaspar Rochette, Louis Thiry, John Zarka, Stéphane Mallat, Joakim andén, Eugene Belilovsky, Joan Bruna, Vincent Lostanlen, Muawiz Chaudhary, Matthew J. Hirn, Edouard Oyallon, Sixin Zhang, Carmine Cella, Michael Eickenberg
- **Comment**: None
- **Journal**: None
- **Summary**: The wavelet scattering transform is an invariant signal representation suitable for many signal processing and machine learning applications. We present the Kymatio software package, an easy-to-use, high-performance Python implementation of the scattering transform in 1D, 2D, and 3D that is compatible with modern deep learning frameworks. All transforms may be executed on a GPU (in addition to CPU), offering a considerable speed up over CPU implementations. The package also has a small memory footprint, resulting inefficient memory usage. The source code, documentation, and examples are available undera BSD license at https://www.kymat.io/



