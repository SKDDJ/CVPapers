# Arxiv Papers in cs.CV on 2018-12-16
### Resource-Scalable CNN Synthesis for IoT Applications
- **Arxiv ID**: http://arxiv.org/abs/1901.00738v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.00738v1)
- **Published**: 2018-12-16 01:21:57+00:00
- **Updated**: 2018-12-16 01:21:57+00:00
- **Authors**: Mohammad Motamedi, Felix Portillo, Mahya Saffarpour, Daniel Fong, Soheil Ghiasi
- **Comment**: 7 Pages, 3 Figures, 4 Tables
- **Journal**: None
- **Summary**: State-of-the-art image recognition systems use sophisticated Convolutional Neural Networks (CNNs) that are designed and trained to identify numerous object classes. Such networks are fairly resource intensive to compute, prohibiting their deployment on resource-constrained embedded platforms. On one hand, the ability to classify an exhaustive list of categories is excessive for the demands of most IoT applications. On the other hand, designing a new custom-designed CNN for each new IoT application is impractical, due to the inherent difficulty in developing competitive models and time-to-market pressure. To address this problem, we investigate the question of: "Can one utilize an existing optimized CNN model to automatically build a competitive CNN for an IoT application whose objects of interest are a fraction of categories that the original CNN was designed to classify, such that the resource requirement is proportionally scaled down?" We use the term resource scalability to refer to this concept, and develop a methodology for automated synthesis of resource scalable CNNs from an existing optimized baseline CNN. The synthesized CNN has sufficient learning capacity for handling the given IoT application requirements, and yields competitive accuracy. The proposed approach is fast, and unlike the presently common practice of CNN design, does not require iterative rounds of training trial and error.



### Distill-Net: Application-Specific Distillation of Deep Convolutional Neural Networks for Resource-Constrained IoT Platforms
- **Arxiv ID**: http://arxiv.org/abs/1812.07390v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.07390v1)
- **Published**: 2018-12-16 02:37:03+00:00
- **Updated**: 2018-12-16 02:37:03+00:00
- **Authors**: Mohammad Motamedi, Felix Portillo, Daniel Fong, Soheil Ghiasi
- **Comment**: None
- **Journal**: None
- **Summary**: Many Internet-of-Things (IoT) applications demand fast and accurate understanding of a few key events in their surrounding environment. Deep Convolutional Neural Networks (CNNs) have emerged as an effective approach to understand speech, images, and similar high dimensional data types. Algorithmic performance of modern CNNs, however, fundamentally relies on learning class-agnostic hierarchical features that only exist in comprehensive training datasets with many classes. As a result, fast inference using CNNs trained on such datasets is prohibitive for most resource-constrained IoT platforms. To bridge this gap, we present a principled and practical methodology for distilling a complex modern CNN that is trained to effectively recognize many different classes of input data into an application-dependent essential core that not only recognizes the few classes of interest to the application accurately, but also runs efficiently on platforms with limited resources. Experimental results confirm that our approach strikes a favorable balance between classification accuracy (application constraint), inference efficiency (platform constraint), and productive development of new applications (business constraint).



### Efficient Super Resolution Using Binarized Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1812.06378v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1812.06378v1)
- **Published**: 2018-12-16 02:44:20+00:00
- **Updated**: 2018-12-16 02:44:20+00:00
- **Authors**: Yinglan Ma, Hongyu Xiong, Zhe Hu, Lizhuang Ma
- **Comment**: 10 Pages, 7 figures
- **Journal**: None
- **Summary**: Deep convolutional neural networks (DCNNs) have recently demonstrated high-quality results in single-image super-resolution (SR). DCNNs often suffer from over-parametrization and large amounts of redundancy, which results in inefficient inference and high memory usage, preventing massive applications on mobile devices. As a way to significantly reduce model size and computation time, binarized neural network has only been shown to excel on semantic-level tasks such as image classification and recognition. However, little effort of network quantization has been spent on image enhancement tasks like SR, as network quantization is usually assumed to sacrifice pixel-level accuracy. In this work, we explore an network-binarization approach for SR tasks without sacrificing much reconstruction accuracy. To achieve this, we binarize the convolutional filters in only residual blocks, and adopt a learnable weight for each binary filter. We evaluate this idea on several state-of-the-art DCNN-based architectures, and show that binarized SR networks achieve comparable qualitative and quantitative results as their real-weight counterparts. Moreover, the proposed binarized strategy could help reduce model size by 80% when applying on SRResNet, and could potentially speed up inference by 5 times.



### TET-GAN: Text Effects Transfer via Stylization and Destylization
- **Arxiv ID**: http://arxiv.org/abs/1812.06384v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.06384v2)
- **Published**: 2018-12-16 03:19:08+00:00
- **Updated**: 2018-12-27 04:08:18+00:00
- **Authors**: Shuai Yang, Jiaying Liu, Wenjing Wang, Zongming Guo
- **Comment**: Accepted by AAAI 2019. Code and dataset will be available at
  http://www.icst.pku.edu.cn/struct/Projects/TETGAN.html
- **Journal**: None
- **Summary**: Text effects transfer technology automatically makes the text dramatically more impressive. However, previous style transfer methods either study the model for general style, which cannot handle the highly-structured text effects along the glyph, or require manual design of subtle matching criteria for text effects. In this paper, we focus on the use of the powerful representation abilities of deep neural features for text effects transfer. For this purpose, we propose a novel Texture Effects Transfer GAN (TET-GAN), which consists of a stylization subnetwork and a destylization subnetwork. The key idea is to train our network to accomplish both the objective of style transfer and style removal, so that it can learn to disentangle and recombine the content and style features of text effects images. To support the training of our network, we propose a new text effects dataset with as much as 64 professionally designed styles on 837 characters. We show that the disentangled feature representations enable us to transfer or remove all these styles on arbitrary glyphs using one network. Furthermore, the flexible network design empowers TET-GAN to efficiently extend to a new text style via one-shot learning where only one example is required. We demonstrate the superiority of the proposed method in generating high-quality stylized text over the state-of-the-art methods.



### Pre-Trained Convolutional Neural Network Features for Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/1812.06387v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.06387v1)
- **Published**: 2018-12-16 04:23:59+00:00
- **Updated**: 2018-12-16 04:23:59+00:00
- **Authors**: Aravind Ravi
- **Comment**: None
- **Journal**: None
- **Summary**: Facial expression recognition has been an active area in computer vision with application areas including animation, social robots, personalized banking, etc. In this study, we explore the problem of image classification for detecting facial expressions based on features extracted from pre-trained convolutional neural networks trained on ImageNet database. Features are extracted and transferred to a Linear Support Vector Machine for classification. All experiments are performed on two publicly available datasets such as JAFFE and CK+ database. The results show that representations learned from pre-trained networks for a task such as object recognition can be transferred, and used for facial expression recognition. Furthermore, for a small dataset, using features from earlier layers of the VGG19 network provides better classification accuracy. Accuracies of 92.26% and 92.86% were achieved for the CK+ and JAFFE datasets respectively.



### Unified Graph based Multi-Cue Feature Fusion for Robust Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/1812.06407v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.06407v3)
- **Published**: 2018-12-16 07:08:41+00:00
- **Updated**: 2019-05-23 22:46:02+00:00
- **Authors**: Kapil Sharma, Himanshu Ahuja, Ashish Kumar, Nipun Bansal, Gurjit Singh Walia
- **Comment**: The information on this paper is not complete in entirety and may be
  misinterpreted
- **Journal**: None
- **Summary**: Visual Tracking is a complex problem due to unconstrained appearance variations and dynamic environment. Extraction of complementary information from the object environment via multiple features and adaption to the target's appearance variations are the key problems of this work. To this end, we propose a robust object tracking framework based on Unified Graph Fusion (UGF) of multi-cue to adapt to the object's appearance. The proposed cross-diffusion of sparse and dense features not only suppresses the individual feature deficiencies but also extracts the complementary information from multi-cue. This iterative process builds robust unified features which are invariant to object deformations, fast motion, and occlusion. Robustness of the unified feature also enables the random forest classifier to precisely distinguish the foreground from the background, adding resilience to background clutter. In addition, we present a novel kernel-based adaptation strategy using outlier detection and a transductive reliability metric.



### Human Pose and Path Estimation from Aerial Video using Dynamic Classifier Selection
- **Arxiv ID**: http://arxiv.org/abs/1812.06408v1
- **DOI**: 10.1007/s12559-018-9577-6
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.06408v1)
- **Published**: 2018-12-16 07:27:26+00:00
- **Updated**: 2018-12-16 07:27:26+00:00
- **Authors**: Asanka G Perera, Yee Wei Law, Javaan Chahl
- **Comment**: For associated dataset, see
  https://asankagp.github.io/aerialgaitdataset/
- **Journal**: Cogn Comput 10 (2018) 1019-1041
- **Summary**: We consider the problem of estimating human pose and trajectory by an aerial robot with a monocular camera in near real time. We present a preliminary solution whose distinguishing feature is a dynamic classifier selection architecture. In our solution, each video frame is corrected for perspective using projective transformation. Then, two alternative feature sets are used: (i) Histogram of Oriented Gradients (HOG) of the silhouette, (ii) Convolutional Neural Network (CNN) features of the RGB image. The features (HOG or CNN) are classified using a dynamic classifier. A class is defined as a pose-viewpoint pair, and a total of 64 classes are defined to represent a forward walking and turning gait sequence. Our solution provides three main advantages: (i) Classification is efficient due to dynamic selection (4-class vs. 64-class classification). (ii) Classification errors are confined to neighbors of the true view-points. (iii) The robust temporal relationship between poses is used to resolve the left-right ambiguities of human silhouettes. Experiments conducted on both fronto-parallel videos and aerial videos confirm our solution can achieve accurate pose and trajectory estimation for both scenarios. We found using HOG features provides higher accuracy than using CNN features. For example, applying the HOG-based variant of our scheme to the 'walking on a figure 8-shaped path' dataset (1652 frames) achieved estimation accuracies of 99.6% for viewpoints and 96.2% for number of poses.



### Visual Dialogue without Vision or Dialogue
- **Arxiv ID**: http://arxiv.org/abs/1812.06417v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.06417v3)
- **Published**: 2018-12-16 08:18:37+00:00
- **Updated**: 2019-10-22 10:09:41+00:00
- **Authors**: Daniela Massiceti, Puneet K. Dokania, N. Siddharth, Philip H. S. Torr
- **Comment**: 2018 NeurIPS Workshop on Critiquing and Correcting Trends in Machine
  Learning
- **Journal**: None
- **Summary**: We characterise some of the quirks and shortcomings in the exploration of Visual Dialogue - a sequential question-answering task where the questions and corresponding answers are related through given visual stimuli. To do so, we develop an embarrassingly simple method based on Canonical Correlation Analysis (CCA) that, on the standard dataset, achieves near state-of-the-art performance on mean rank (MR). In direct contrast to current complex and over-parametrised architectures that are both compute and time intensive, our method ignores the visual stimuli, ignores the sequencing of dialogue, does not need gradients, uses off-the-shelf feature extractors, has at least an order of magnitude fewer parameters, and learns in practically no time. We argue that these results are indicative of issues in current approaches to Visual Dialogue and conduct analyses to highlight implicit dataset biases and effects of over-constrained evaluation metrics. Our code is publicly available.



### Model-free Tracking with Deep Appearance and Motion Features Integration
- **Arxiv ID**: http://arxiv.org/abs/1812.06418v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.06418v1)
- **Published**: 2018-12-16 08:24:57+00:00
- **Updated**: 2018-12-16 08:24:57+00:00
- **Authors**: Xiaolong Jiang, Peizhao Li, Xiantong Zhen, Xianbin Cao
- **Comment**: 10 pages, 10 figures. Accepted in IEEE Winter Conference on
  Applications of Computer Vision (WACV), 2019
- **Journal**: None
- **Summary**: Being able to track an anonymous object, a model-free tracker is comprehensively applicable regardless of the target type. However, designing such a generalized framework is challenged by the lack of object-oriented prior information. As one solution, a real-time model-free object tracking approach is designed in this work relying on Convolutional Neural Networks (CNNs). To overcome the object-centric information scarcity, both appearance and motion features are deeply integrated by the proposed AMNet, which is an end-to-end offline trained two-stream network. Between the two parallel streams, the ANet investigates appearance features with a multi-scale Siamese atrous CNN, enabling the tracking-by-matching strategy. The MNet achieves deep motion detection to localize anonymous moving objects by processing generic motion features. The final tracking result at each frame is generated by fusing the output response maps from both sub-networks. The proposed AMNet reports leading performance on both OTB and VOT benchmark datasets with favorable real-time processing speed.



### Classifier and Exemplar Synthesis for Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1812.06423v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.06423v2)
- **Published**: 2018-12-16 08:44:11+00:00
- **Updated**: 2019-07-18 04:12:19+00:00
- **Authors**: Soravit Changpinyo, Wei-Lun Chao, Boqing Gong, Fei Sha
- **Comment**: Extended version of arXiv:1603.00550 (CVPR 2016) and arXiv:1605.08151
  (ICCV 2017); Accepted for publication in International Journal of Computer
  Vision (IJCV)
- **Journal**: None
- **Summary**: Zero-shot learning (ZSL) enables solving a task without the need to see its examples. In this paper, we propose two ZSL frameworks that learn to synthesize parameters for novel unseen classes. First, we propose to cast the problem of ZSL as learning manifold embeddings from graphs composed of object classes, leading to a flexible approach that synthesizes "classifiers" for the unseen classes. Then, we define an auxiliary task of synthesizing "exemplars" for the unseen classes to be used as an automatic denoising mechanism for any existing ZSL approaches or as an effective ZSL model by itself. On five visual recognition benchmark datasets, we demonstrate the superior performances of our proposed frameworks in various scenarios of both conventional and generalized ZSL. Finally, we provide valuable insights through a series of empirical analyses, among which are a comparison of semantic representations on the full ImageNet benchmark as well as a comparison of metrics used in generalized ZSL. Our code and data are publicly available at https://github.com/pujols/Zero-shot-learning-journal



### Auto-tuning Neural Network Quantization Framework for Collaborative Inference Between the Cloud and Edge
- **Arxiv ID**: http://arxiv.org/abs/1812.06426v1
- **DOI**: None
- **Categories**: **cs.DC**, cs.AI, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1812.06426v1)
- **Published**: 2018-12-16 09:05:44+00:00
- **Updated**: 2018-12-16 09:05:44+00:00
- **Authors**: Guangli Li, Lei Liu, Xueying Wang, Xiao Dong, Peng Zhao, Xiaobing Feng
- **Comment**: Published at ICANN 2018
- **Journal**: None
- **Summary**: Recently, deep neural networks (DNNs) have been widely applied in mobile intelligent applications. The inference for the DNNs is usually performed in the cloud. However, it leads to a large overhead of transmitting data via wireless network. In this paper, we demonstrate the advantages of the cloud-edge collaborative inference with quantization. By analyzing the characteristics of layers in DNNs, an auto-tuning neural network quantization framework for collaborative inference is proposed. We study the effectiveness of mixed-precision collaborative inference of state-of-the-art DNNs by using ImageNet dataset. The experimental results show that our framework can generate reasonable network partitions and reduce the storage on mobile devices with trivial loss of accuracy.



### A novel 3D display based on micro-volumetric scanning and real time reconstruction of holograms principle
- **Arxiv ID**: http://arxiv.org/abs/1901.05064v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1901.05064v1)
- **Published**: 2018-12-16 13:06:49+00:00
- **Updated**: 2018-12-16 13:06:49+00:00
- **Authors**: Guangjun Wang
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1706.03231
- **Journal**: None
- **Summary**: The present study proposes a novel 3D display contains a micro-volumetric scanning system (MVS) and a real time reconstruction hologram system (RTRH).



### HoVer-Net: Simultaneous Segmentation and Classification of Nuclei in Multi-Tissue Histology Images
- **Arxiv ID**: http://arxiv.org/abs/1812.06499v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.06499v5)
- **Published**: 2018-12-16 16:53:41+00:00
- **Updated**: 2019-11-13 17:25:43+00:00
- **Authors**: Simon Graham, Quoc Dang Vu, Shan E Ahmed Raza, Ayesha Azam, Yee Wah Tsang, Jin Tae Kwak, Nasir Rajpoot
- **Comment**: None
- **Journal**: None
- **Summary**: Nuclear segmentation and classification within Haematoxylin & Eosin stained histology images is a fundamental prerequisite in the digital pathology work-flow. The development of automated methods for nuclear segmentation and classification enables the quantitative analysis of tens of thousands of nuclei within a whole-slide pathology image, opening up possibilities of further analysis of large-scale nuclear morphometry. However, automated nuclear segmentation and classification is faced with a major challenge in that there are several different types of nuclei, some of them exhibiting large intra-class variability such as the tumour cells. Additionally, some of the nuclei are often clustered together. To address these challenges, we present a novel convolutional neural network for simultaneous nuclear segmentation and classification that leverages the instance-rich information encoded within the vertical and horizontal distances of nuclear pixels to their centres of mass. These distances are then utilised to separate clustered nuclei, resulting in an accurate segmentation, particularly in areas with overlapping instances. Then for each segmented instance, the network predicts the type of nucleus via a devoted up-sampling branch. We demonstrate state-of-the-art performance compared to other methods on multiple independent multi-tissue histology image datasets. As part of this work, we introduce a new dataset of Haematoxylin & Eosin stained colorectal adenocarcinoma image tiles, containing 24,319 exhaustively annotated nuclei with associated class labels.



### Non-invasive measuring method of skin temperature based on skin sensitivity index and deep learning
- **Arxiv ID**: http://arxiv.org/abs/1812.06509v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.06509v1)
- **Published**: 2018-12-16 17:56:16+00:00
- **Updated**: 2018-12-16 17:56:16+00:00
- **Authors**: Xiaogang Cheng, Bin Yang, Kaige Tan, Erik Isaksson, Liren Li, Anders Hedman, Thomas Olofsson, Haibo Li
- **Comment**: 13 pages, 5 figure
- **Journal**: None
- **Summary**: In human-centered intelligent building, real-time measurements of human thermal comfort play critical roles and supply feedback control signals for building heating, ventilation, and air conditioning (HVAC) systems. Due to the challenges of intra- and inter-individual differences and skin subtleness variations, there is no satisfactory solution for thermal comfort measurements until now. In this paper, a non-invasive measuring method based on skin sensitivity index and deep learning (NISDL) was proposed to measure real-time skin temperature. A new evaluating index, named skin sensitivity index (SSI), was defined to overcome individual differences and skin subtleness variations. To illustrate the effectiveness of SSI proposed, two multi-layers deep learning framework (NISDL method I and II) was designed and the DenseNet201 was used for extracting features from skin images. The partly personal saturation temperature (NIPST) algorithm was use for algorithm comparisons. Another deep learning algorithm without SSI (DL) was also generated for algorithm comparisons. Finally, a total of 1.44 million image data was used for algorithm validation. The results show that 55.6180% and 52.2472% error values (NISDL method I, II) are scattered at [0, 0.25), and the same error intervals distribution of NIPST is 35.3933%.



### Towards Robust Human Activity Recognition from RGB Video Stream with Limited Labeled Data
- **Arxiv ID**: http://arxiv.org/abs/1812.06544v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.06544v1)
- **Published**: 2018-12-16 22:19:43+00:00
- **Updated**: 2018-12-16 22:19:43+00:00
- **Authors**: Krishanu Sarker, Mohamed Masoud, Saeid Belkasim, Shihao Ji
- **Comment**: To appear in ICMLA 2018
- **Journal**: None
- **Summary**: Human activity recognition based on video streams has received numerous attentions in recent years. Due to lack of depth information, RGB video based activity recognition performs poorly compared to RGB-D video based solutions. On the other hand, acquiring depth information, inertia etc. is costly and requires special equipment, whereas RGB video streams are available in ordinary cameras. Hence, our goal is to investigate whether similar or even higher accuracy can be achieved with RGB-only modality. In this regard, we propose a novel framework that couples skeleton data extracted from RGB video and deep Bidirectional Long Short Term Memory (BLSTM) model for activity recognition. A big challenge of training such a deep network is the limited training data, and exploring RGB-only stream significantly exaggerates the difficulty. We therefore propose a set of algorithmic techniques to train this model effectively, e.g., data augmentation, dynamic frame dropout and gradient injection. The experiments demonstrate that our RGB-only solution surpasses the state-of-the-art approaches that all exploit RGB-D video streams by a notable margin. This makes our solution widely deployable with ordinary cameras.



