# Arxiv Papers in cs.CV on 2018-12-07
### Privacy Partitioning: Protecting User Data During the Deep Learning Inference Phase
- **Arxiv ID**: http://arxiv.org/abs/1812.02863v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.02863v1)
- **Published**: 2018-12-07 00:42:06+00:00
- **Updated**: 2018-12-07 00:42:06+00:00
- **Authors**: Jianfeng Chi, Emmanuel Owusu, Xuwang Yin, Tong Yu, William Chan, Patrick Tague, Yuan Tian
- **Comment**: None
- **Journal**: None
- **Summary**: We present a practical method for protecting data during the inference phase of deep learning based on bipartite topology threat modeling and an interactive adversarial deep network construction. We term this approach \emph{Privacy Partitioning}. In the proposed framework, we split the machine learning models and deploy a few layers into users' local devices, and the rest of the layers into a remote server. We propose an approach to protect user's data during the inference phase, while still achieve good classification accuracy.   We conduct an experimental evaluation of this approach on benchmark datasets of three computer vision tasks. The experimental results indicate that this approach can be used to significantly attenuate the capacity for an adversary with access to the state-of-the-art deep network's intermediate states to learn privacy-sensitive inputs to the network. For example, we demonstrate that our approach can prevent attackers from inferring the private attributes such as gender from the Face image dataset without sacrificing the classification accuracy of the original machine learning task such as Face Identification.



### An Attempt towards Interpretable Audio-Visual Video Captioning
- **Arxiv ID**: http://arxiv.org/abs/1812.02872v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.02872v1)
- **Published**: 2018-12-07 01:57:42+00:00
- **Updated**: 2018-12-07 01:57:42+00:00
- **Authors**: Yapeng Tian, Chenxiao Guan, Justin Goodman, Marc Moore, Chenliang Xu
- **Comment**: 11 pages, 4 figures
- **Journal**: None
- **Summary**: Automatically generating a natural language sentence to describe the content of an input video is a very challenging problem. It is an essential multimodal task in which auditory and visual contents are equally important. Although audio information has been exploited to improve video captioning in previous works, it is usually regarded as an additional feature fed into a black box fusion machine. How are the words in the generated sentences associated with the auditory and visual modalities? The problem is still not investigated. In this paper, we make the first attempt to design an interpretable audio-visual video captioning network to discover the association between words in sentences and audio-visual sequences. To achieve this, we propose a multimodal convolutional neural network-based audio-visual video captioning framework and introduce a modality-aware module for exploring modality selection during sentence generation. Besides, we collect new audio captioning and visual captioning datasets for further exploring the interactions between auditory and visual modalities for high-level video understanding. Extensive experiments demonstrate that the modality-aware module makes our model interpretable on modality selection during sentence generation. Even with the added interpretability, our video captioning network can still achieve comparable performance with recent state-of-the-art methods.



### Adversarial Defense of Image Classification Using a Variational Auto-Encoder
- **Arxiv ID**: http://arxiv.org/abs/1812.02891v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.02891v1)
- **Published**: 2018-12-07 03:33:14+00:00
- **Updated**: 2018-12-07 03:33:14+00:00
- **Authors**: Yi Luo, Henry Pfister
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks are known to be vulnerable to adversarial attacks. This exposes them to potential exploits in security-sensitive applications and highlights their lack of robustness. This paper uses a variational auto-encoder (VAE) to defend against adversarial attacks for image classification tasks. This VAE defense has a few nice properties: (1) it is quite flexible and its use of randomness makes it harder to attack; (2) it can learn disentangled representations that prevent blurry reconstruction; and (3) a patch-wise VAE defense strategy is used that does not require retraining for different size images. For moderate to severe attacks, this system outperforms or closely matches the performance of JPEG compression, with the best quality parameter. It also has more flexibility and potential for improvement via training.



### Star Tracking using an Event Camera
- **Arxiv ID**: http://arxiv.org/abs/1812.02895v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.02895v2)
- **Published**: 2018-12-07 03:50:09+00:00
- **Updated**: 2019-04-13 01:51:23+00:00
- **Authors**: Tat-Jun Chin, Samya Bagchi, Anders Eriksson, Andre van Schaik
- **Comment**: None
- **Journal**: IEEE Conference on Computer Vision and Pattern Recognition
  Workshops (CVPRW), 2019
- **Summary**: Star trackers are primarily optical devices that are used to estimate the attitude of a spacecraft by recognising and tracking star patterns. Currently, most star trackers use conventional optical sensors. In this application paper, we propose the usage of event sensors for star tracking. There are potentially two benefits of using event sensors for star tracking: lower power consumption and higher operating speeds. Our main contribution is to formulate an algorithmic pipeline for star tracking from event data that includes novel formulations of rotation averaging and bundle adjustment. In addition, we also release with this paper a dataset for star tracking using event cameras. With this work, we introduce the problem of star tracking using event cameras to the computer vision community, whose expertise in SLAM and geometric optimisation can be brought to bear on this commercially important application.



### Improved Search Strategies with Application to Estimating Facial Blendshape Parameters
- **Arxiv ID**: http://arxiv.org/abs/1812.02897v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.02897v3)
- **Published**: 2018-12-07 03:57:39+00:00
- **Updated**: 2020-08-16 22:29:00+00:00
- **Authors**: Michael Bao, David Hyde, Xinru Hua, Ronald Fedkiw
- **Comment**: None
- **Journal**: None
- **Summary**: It is well known that popular optimization techniques can lead to overfitting or even a lack of convergence altogether; thus, practitioners often utilize ad hoc regularization terms added to the energy functional. When carefully crafted, these regularizations can produce compelling results. However, regularization changes both the energy landscape and the solution to the optimization problem, which can result in underfitting. Surprisingly, many practitioners both add regularization and claim that their model lacks the expressivity to fit the data. Motivated by a geometric interpretation of the linearized search space, we propose an approach that ameliorates overfitting without the need for regularization terms that restrict the expressiveness of the underlying model. We illustrate the efficacy of our approach on minimization problems related to three-dimensional facial expression estimation where overfitting clouds semantic understanding and regularization may lead to underfitting that misses or misinterprets subtle expressions.



### TDAN: Temporally Deformable Alignment Network for Video Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1812.02898v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.02898v1)
- **Published**: 2018-12-07 03:58:43+00:00
- **Updated**: 2018-12-07 03:58:43+00:00
- **Authors**: Yapeng Tian, Yulun Zhang, Yun Fu, Chenliang Xu
- **Comment**: 10 pages, 6 figures, demo link
  http://www.youtube.com/watch?v=eZExENE50I0
- **Journal**: None
- **Summary**: Video super-resolution (VSR) aims to restore a photo-realistic high-resolution (HR) video frame from both its corresponding low-resolution (LR) frame (reference frame) and multiple neighboring frames (supporting frames). Due to varying motion of cameras or objects, the reference frame and each support frame are not aligned. Therefore, temporal alignment is a challenging yet important problem for VSR. Previous VSR methods usually utilize optical flow between the reference frame and each supporting frame to wrap the supporting frame for temporal alignment. Therefore, the performance of these image-level wrapping-based models will highly depend on the prediction accuracy of optical flow, and inaccurate optical flow will lead to artifacts in the wrapped supporting frames, which also will be propagated into the reconstructed HR video frame. To overcome the limitation, in this paper, we propose a temporal deformable alignment network (TDAN) to adaptively align the reference frame and each supporting frame at the feature level without computing optical flow. The TDAN uses features from both the reference frame and each supporting frame to dynamically predict offsets of sampling convolution kernels. By using the corresponding kernels, TDAN transforms supporting frames to align with the reference frame. To predict the HR video frame, a reconstruction network taking aligned frames and the reference frame is utilized. Experimental results demonstrate the effectiveness of the proposed TDAN-based VSR model.



### Deep Energies for Estimating Three-Dimensional Facial Pose and Expression
- **Arxiv ID**: http://arxiv.org/abs/1812.02899v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.02899v1)
- **Published**: 2018-12-07 04:00:53+00:00
- **Updated**: 2018-12-07 04:00:53+00:00
- **Authors**: Michael Bao, Jane Wu, Xinwei Yao, Ronald Fedkiw
- **Comment**: None
- **Journal**: None
- **Summary**: While much progress has been made in capturing high-quality facial performances using motion capture markers and shape-from-shading, high-end systems typically also rely on rotoscope curves hand-drawn on the image. These curves are subjective and difficult to draw consistently; moreover, ad-hoc procedural methods are required for generating matching rotoscope curves on synthetic renders embedded in the optimization used to determine three-dimensional facial pose and expression. We propose an alternative approach whereby these curves and other keypoints are detected automatically on both the image and the synthetic renders using trained neural networks, eliminating artist subjectivity and the ad-hoc procedures meant to mimic it. More generally, we propose using machine learning networks to implicitly define deep energies which when minimized using classical optimization techniques lead to three-dimensional facial pose and expression estimation.



### Spatial-Spectral Regularized Local Scaling Cut for Dimensionality Reduction in Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1812.08047v1
- **DOI**: 10.1109/LGRS.2018.2885809
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.08047v1)
- **Published**: 2018-12-07 04:13:31+00:00
- **Updated**: 2018-12-07 04:13:31+00:00
- **Authors**: Ramanarayan Mohanty, S L Happy, Aurobinda Routray
- **Comment**: arXiv admin note: text overlap with arXiv:1811.08223
- **Journal**: IEEE Geoscience and Remote Sensing Letters, 2018
- **Summary**: Dimensionality reduction (DR) methods have attracted extensive attention to provide discriminative information and reduce the computational burden of the hyperspectral image (HSI) classification. However, the DR methods face many challenges due to limited training samples with high dimensional spectra. To address this issue, a graph-based spatial and spectral regularized local scaling cut (SSRLSC) for DR of HSI data is proposed. The underlying idea of the proposed method is to utilize the information from both the spectral and spatial domains to achieve better classification accuracy than its spectral domain counterpart. In SSRLSC, a guided filter is initially used to smoothen and homogenize the pixels of the HSI data in order to preserve the pixel consistency. This is followed by generation of between-class and within-class dissimilarity matrices in both spectral and spatial domains by regularized local scaling cut (RLSC) and neighboring pixel local scaling cut (NPLSC) respectively. Finally, we obtain the projection matrix by optimizing the updated spatial-spectral between-class and total-class dissimilarity. The effectiveness of the proposed DR algorithm is illustrated with two popular real-world HSI datasets.



### SeFM: A Sequential Feature Point Matching Algorithm for Object 3D Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1812.02925v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.02925v3)
- **Published**: 2018-12-07 06:13:07+00:00
- **Updated**: 2019-11-24 10:37:23+00:00
- **Authors**: Zhihao Fang, He Ma, Xuemin Zhu, Xutao Guo, Ruixin Zhou
- **Comment**: This is a pre-print of a contribution published in Frontier Computing
  - Theory, Technologies and Applications (FC 2019) published by Springer
- **Journal**: None
- **Summary**: 3D reconstruction is a fundamental issue in many applications and the feature point matching problem is a key step while reconstructing target objects. Conventional algorithms can only find a small number of feature points from two images which is quite insufficient for reconstruction. To overcome this problem, we propose SeFM a sequential feature point matching algorithm. We first utilize the epipolar geometry to find the epipole of each image. Rotating along the epipole, we generate a set of the epipolar lines and reserve those intersecting with the input image. Next, a rough matching phase, followed by a dense matching phase, is applied to find the matching dot-pairs using dynamic programming. Furthermore, we also remove wrong matching dot-pairs by calculating the validity. Experimental results illustrate that SeFM can achieve around 1,000 to 10,000 times matching dot-pairs, depending on individual image, compared to conventional algorithms and the object reconstruction with only two images is semantically visible. Moreover, it outperforms conventional algorithms, such as SIFT and SURF, regarding precision and recall.



### Optimizing speed/accuracy trade-off for person re-identification via knowledge distillation
- **Arxiv ID**: http://arxiv.org/abs/1812.02937v2
- **DOI**: 10.1016/j.engappai.2019.103309
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.02937v2)
- **Published**: 2018-12-07 08:11:06+00:00
- **Updated**: 2019-12-05 16:40:11+00:00
- **Authors**: Idoia Ruiz, Bogdan Raducanu, Rakesh Mehta, Jaume Amores
- **Comment**: Published on the journal "Engineering Applications of Artificial
  Intelligence"
- **Journal**: Engineering Applications of Artificial Intelligence, Volume 87,
  January 2020, 103309
- **Summary**: Finding a person across a camera network plays an important role in video surveillance. For a real-world person re-identification application, in order to guarantee an optimal time response, it is crucial to find the balance between accuracy and speed. We analyse this trade-off, comparing a classical method, that comprises hand-crafted feature description and metric learning, in particular, LOMO and XQDA, to deep learning based techniques, using image classification networks, ResNet and MobileNets. Additionally, we propose and analyse network distillation as a learning strategy to reduce the computational cost of the deep learning approach at test time. We evaluate both methods on the Market-1501 and DukeMTMC-reID large-scale datasets, showing that distillation helps reducing the computational cost at inference time while even increasing the accuracy performance.



### Rigid Body Structure and Motion From Two-Frame Point-Correspondences Under Perspective Projection
- **Arxiv ID**: http://arxiv.org/abs/1812.07971v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.07971v1)
- **Published**: 2018-12-07 08:22:20+00:00
- **Updated**: 2018-12-07 08:22:20+00:00
- **Authors**: Mieczysław A. Kłopotek
- **Comment**: arXiv admin note: text overlap with arXiv:1705.03986
- **Journal**: M.A. K{\l}opotek: Rigid Body Structure and Motion From Two-Frame
  Point-Correspondences Under Perspective Projection. Machine Graphics & Vision
  4 (1995)3-4, pp. 187-202
- **Summary**: This paper is concerned with possibility of recovery of motion and structure parameters from multiframes under perspective projection when only points on a rigid body are traced. Free (unrestricted and uncontrolled) pattern of motion between frames is assumed. The major question is how many points and/or how many frames are necessary for the task. It has been shown in an earlier paper {Klopotek:95b} that for orthogonal projection two frames are insufficient for the task. The paper demonstrates that, under perspective projection, that total uncertainty about relative position of focal point versus projection plane makes the recovery of structure and motion from two frames impossible.



### Video Colorization using CNNs and Keyframes extraction: An application in saving bandwidth
- **Arxiv ID**: http://arxiv.org/abs/1812.03858v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.03858v3)
- **Published**: 2018-12-07 09:24:39+00:00
- **Updated**: 2018-12-18 11:46:11+00:00
- **Authors**: Ankur Singh, Anurag Chanani, Harish Karnick
- **Comment**: arXiv admin note: text overlap with arXiv:1712.03400 by other authors
- **Journal**: None
- **Summary**: In this paper, we tackle the problem of colorization of grayscale videos to reduce bandwidth usage. For this task, we use some colored keyframes as reference images from the colored version of the grayscale video. We propose a model that extracts keyframes from a colored video and trains a Convolutional Neural Network from scratch on these colored frames. Through the extracted keyframes we get a good knowledge of the colors that have been used in the video which helps us in colorizing the grayscale version of the video efficiently. An application of the technique that we propose in this paper, is in saving bandwidth while sending raw colored videos that haven't gone through any compression. A raw colored video takes up around three times more memory size than its grayscale version. We can exploit this fact and send a grayscale video along with out trained model instead of a colored video. Later on, in this paper we show how this technique can help to save bandwidth usage to upto three times while transmitting raw colored videos.



### LNEMLC: Label Network Embeddings for Multi-Label Classification
- **Arxiv ID**: http://arxiv.org/abs/1812.02956v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.02956v2)
- **Published**: 2018-12-07 09:30:18+00:00
- **Updated**: 2019-01-01 21:11:09+00:00
- **Authors**: Piotr Szymański, Tomasz Kajdanowicz, Nitesh Chawla
- **Comment**: submitted to TPAMI
- **Journal**: None
- **Summary**: Multi-label classification aims to classify instances with discrete non-exclusive labels. Most approaches on multi-label classification focus on effective adaptation or transformation of existing binary and multi-class learning approaches but fail in modelling the joint probability of labels or do not preserve generalization abilities for unseen label combinations. To address these issues we propose a new multi-label classification scheme, LNEMLC - Label Network Embedding for Multi-Label Classification, that embeds the label network and uses it to extend input space in learning and inference of any base multi-label classifier. The approach allows capturing of labels' joint probability at low computational complexity providing results comparable to the best methods reported in the literature. We demonstrate how the method reveals statistically significant improvements over the simple kNN baseline classifier. We also provide hints for selecting the robust configuration that works satisfactorily across data domains.



### EventNet: Asynchronous Recursive Event Processing
- **Arxiv ID**: http://arxiv.org/abs/1812.07045v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.07045v2)
- **Published**: 2018-12-07 09:47:35+00:00
- **Updated**: 2019-04-01 07:08:14+00:00
- **Authors**: Yusuke Sekikawa, Kosuke Hara, Hideo Saito
- **Comment**: None
- **Journal**: None
- **Summary**: Event cameras are bio-inspired vision sensors that mimic retinas to asynchronously report per-pixel intensity changes rather than outputting an actual intensity image at regular intervals. This new paradigm of image sensor offers significant potential advantages; namely, sparse and non-redundant data representation. Unfortunately, however, most of the existing artificial neural network architectures, such as a CNN, require dense synchronous input data, and therefore, cannot make use of the sparseness of the data. We propose EventNet, a neural network designed for real-time processing of asynchronous event streams in a recursive and event-wise manner. EventNet models dependence of the output on tens of thousands of causal events recursively using a novel temporal coding scheme. As a result, at inference time, our network operates in an event-wise manner that is realized with very few sum-of-the-product operations---look-up table and temporal feature aggregation---which enables processing of 1 mega or more events per second on standard CPU. In experiments using real data, we demonstrated the real-time performance and robustness of our framework.



### Scale-aware multi-level guidance for interactive instance segmentation
- **Arxiv ID**: http://arxiv.org/abs/1812.02967v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.02967v1)
- **Published**: 2018-12-07 10:19:01+00:00
- **Updated**: 2018-12-07 10:19:01+00:00
- **Authors**: Soumajit Majumder, Angela Yao
- **Comment**: None
- **Journal**: None
- **Summary**: In interactive instance segmentation, users give feedback to iteratively refine segmentation masks. The user-provided clicks are transformed into guidance maps which provide the network with necessary cues on the whereabouts of the object of interest. Guidance maps used in current systems are purely distance-based and are either too localized or non-informative. We propose a novel transformation of user clicks to generate scale-aware guidance maps that leverage the hierarchical structural information present in an image. Using our guidance maps, even the most basic FCNs are able to outperform existing approaches that require state-of-the-art segmentation networks pre-trained on large scale segmentation datasets. We demonstrate the effectiveness of our proposed transformation strategy through comprehensive experimentation in which we significantly raise state-of-the-art on four standard interactive segmentation benchmarks.



### Back to square one: probabilistic trajectory forecasting without bells and whistles
- **Arxiv ID**: http://arxiv.org/abs/1812.02984v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.02984v1)
- **Published**: 2018-12-07 11:31:05+00:00
- **Updated**: 2018-12-07 11:31:05+00:00
- **Authors**: Ehsan Pajouheshgar, Christoph H. Lampert
- **Comment**: 4 pages, 3 figures, Workshop on Modeling and Decision-Making in the
  Spatiotemporal Domain, 32nd Conference on Neural Information Processing
  Systems (NIPS 2018), Montreal, Canada
- **Journal**: None
- **Summary**: We introduce a spatio-temporal convolutional neural network model for trajectory forecasting from visual sources. Applied in an auto-regressive way it provides an explicit probability distribution over continuations of a given initial trajectory segment. We discuss it in relation to (more complicated) existing work and report on experiments on two standard datasets for trajectory forecasting: MNISTseq and Stanford Drones, achieving results on-par with or better than previous methods.



### Interval type-2 Beta Fuzzy Near set based approach to content based image retrieval
- **Arxiv ID**: http://arxiv.org/abs/1812.07098v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1812.07098v1)
- **Published**: 2018-12-07 11:34:23+00:00
- **Updated**: 2018-12-07 11:34:23+00:00
- **Authors**: Yosr Ghozzi, Nesrine Baklouti, Hani Hagras, Mounir Ben Ayed, Adel M. Alimi
- **Comment**: 10 pages, 7 figures, 1 table
- **Journal**: None
- **Summary**: In an automated search system, similarity is a key concept in solving a human task. Indeed, human process is usually a natural categorization that underlies many natural abilities such as image recovery, language comprehension, decision making, or pattern recognition. In the image search axis, there are several ways to measure the similarity between images in an image database, to a query image. Image search by content is based on the similarity of the visual characteristics of the images. The distance function used to evaluate the similarity between images depends on the criteria of the search but also on the representation of the characteristics of the image; this is the main idea of the near and fuzzy sets approaches. In this article, we introduce a new category of beta type-2 fuzzy sets for the description of image characteristics as well as the near sets approach for image recovery. Finally, we illustrate our work with examples of image recovery problems used in the real world.



### Real-time Indoor Scene Reconstruction with RGBD and Inertia Input
- **Arxiv ID**: http://arxiv.org/abs/1812.03015v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.03015v1)
- **Published**: 2018-12-07 13:07:36+00:00
- **Updated**: 2018-12-07 13:07:36+00:00
- **Authors**: Zunjie Zhu, Feng Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Camera motion estimation is a key technique for 3D scene reconstruction and Simultaneous localization and mapping (SLAM). To make it be feasibly achieved, previous works usually assume slow camera motions, which limits its usage in many real cases. We propose an end-to-end 3D reconstruction system which combines color, depth and inertial measurements to achieve robust reconstruction with fast sensor motions. Our framework extends Kalman filter to fuse the three kinds of information and involve an iterative method to jointly optimize feature correspondences, camera poses and scene geometry. We also propose a novel geometry-aware patch deformation technique to adapt the feature appearance in image domain, leading to a more accurate feature matching under fast camera motions. Experiments show that our patch deformation method improves the accuracy of feature tracking, and our 3D reconstruction outperforms the state-of-the-art solutions under fast camera motions.



### A High-Order Scheme for Image Segmentation via a modified Level-Set method
- **Arxiv ID**: http://arxiv.org/abs/1812.03026v2
- **DOI**: None
- **Categories**: **math.NA**, cs.CV, cs.NA
- **Links**: [PDF](http://arxiv.org/pdf/1812.03026v2)
- **Published**: 2018-12-07 13:50:17+00:00
- **Updated**: 2020-01-07 13:27:25+00:00
- **Authors**: Maurizio Falcone, Giulio Paolucci, Silvia Tozza
- **Comment**: Accepted version for publication in SIAM Journal on Imaging Sciences,
  86 figures
- **Journal**: None
- **Summary**: In this paper we propose a high-order accurate scheme for image segmentation based on the level-set method. In this approach, the curve evolution is described as the 0-level set of a representation function but we modify the velocity that drives the curve to the boundary of the object in order to obtain a new velocity with additional properties that are extremely useful to develop a more stable high-order approximation with a small additional cost. The approximation scheme proposed here is the first 2D version of an adaptive "filtered" scheme recently introduced and analyzed by the authors in 1D. This approach is interesting since the implementation of the filtered scheme is rather efficient and easy. The scheme combines two building blocks (a monotone scheme and a high-order scheme) via a filter function and smoothness indicators that allow to detect the regularity of the approximate solution adapting the scheme in an automatic way. Some numerical tests on synthetic and real images confirm the accuracy of the proposed method and the advantages given by the new velocity.



### Graph Cut Segmentation Methods Revisited with a Quantum Algorithm
- **Arxiv ID**: http://arxiv.org/abs/1812.03050v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.03050v2)
- **Published**: 2018-12-07 14:53:16+00:00
- **Updated**: 2019-03-30 16:32:18+00:00
- **Authors**: Lisa Tse, Peter Mountney, Paul Klein, Simone Severini
- **Comment**: None
- **Journal**: None
- **Summary**: The design and performance of computer vision algorithms are greatly influenced by the hardware on which they are implemented. CPUs, multi-core CPUs, FPGAs and GPUs have inspired new algorithms and enabled existing ideas to be realized. This is notably the case with GPUs, which has significantly changed the landscape of computer vision research through deep learning. As the end of Moores law approaches, researchers and hardware manufacturers are exploring alternative hardware computing paradigms. Quantum computers are a very promising alternative and offer polynomial or even exponential speed-ups over conventional computing for some problems. This paper presents a novel approach to image segmentation that uses new quantum computing hardware. Segmentation is formulated as a graph cut problem that can be mapped to the quantum approximate optimization algorithm (QAOA). This algorithm can be implemented on current and near-term quantum computers. Encouraging results are presented on artificial and medical imaging data. This represents an important, practical step towards leveraging quantum computers for computer vision.



### ChauffeurNet: Learning to Drive by Imitating the Best and Synthesizing the Worst
- **Arxiv ID**: http://arxiv.org/abs/1812.03079v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.03079v1)
- **Published**: 2018-12-07 16:04:00+00:00
- **Updated**: 2018-12-07 16:04:00+00:00
- **Authors**: Mayank Bansal, Alex Krizhevsky, Abhijit Ogale
- **Comment**: Video results: https://sites.google.com/view/waymo-learn-to-drive
- **Journal**: None
- **Summary**: Our goal is to train a policy for autonomous driving via imitation learning that is robust enough to drive a real vehicle. We find that standard behavior cloning is insufficient for handling complex driving scenarios, even when we leverage a perception system for preprocessing the input and a controller for executing the output on the car: 30 million examples are still not enough. We propose exposing the learner to synthesized data in the form of perturbations to the expert's driving, which creates interesting situations such as collisions and/or going off the road. Rather than purely imitating all data, we augment the imitation loss with additional losses that penalize undesirable events and encourage progress -- the perturbations then provide an important signal for these losses and lead to robustness of the learned model. We show that the ChauffeurNet model can handle complex situations in simulation, and present ablation experiments that emphasize the importance of each of our proposed changes and show that the model is responding to the appropriate causal factors. Finally, we demonstrate the model driving a car in the real world.



### Color Constancy by GANs: An Experimental Survey
- **Arxiv ID**: http://arxiv.org/abs/1812.03085v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.03085v1)
- **Published**: 2018-12-07 16:20:51+00:00
- **Updated**: 2018-12-07 16:20:51+00:00
- **Authors**: Partha Das, Anil S. Baslamisli, Yang Liu, Sezer Karaoglu, Theo Gevers
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we formulate the color constancy task as an image-to-image translation problem using GANs. By conducting a large set of experiments on different datasets, an experimental survey is provided on the use of different types of GANs to solve for color constancy i.e. CC-GANs (Color Constancy GANs). Based on the experimental review, recommendations are given for the design of CC-GAN architectures based on different criteria, circumstances and datasets.



### Kernel Transformer Networks for Compact Spherical Convolution
- **Arxiv ID**: http://arxiv.org/abs/1812.03115v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.03115v2)
- **Published**: 2018-12-07 17:26:28+00:00
- **Updated**: 2019-04-09 15:46:38+00:00
- **Authors**: Yu-Chuan Su, Kristen Grauman
- **Comment**: In Proceedings of the IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR), 2019
- **Journal**: None
- **Summary**: Ideally, 360{\deg} imagery could inherit the deep convolutional neural networks (CNNs) already trained with great success on perspective projection images. However, existing methods to transfer CNNs from perspective to spherical images introduce significant computational costs and/or degradations in accuracy. In this work, we present the Kernel Transformer Network (KTN). KTNs efficiently transfer convolution kernels from perspective images to the equirectangular projection of 360{\deg} images. Given a source CNN for perspective images as input, the KTN produces a function parameterized by a polar angle and kernel as output. Given a novel 360{\deg} image, that function in turn can compute convolutions for arbitrary layers and kernels as would the source CNN on the corresponding tangent plane projections. Distinct from all existing methods, KTNs allow model transfer: the same model can be applied to different source CNNs with the same base architecture. This enables application to multiple recognition tasks without re-training the KTN. Validating our approach with multiple source CNNs and datasets, we show that KTNs improve the state of the art for spherical convolution. KTNs successfully preserve the source CNN's accuracy, while offering transferability, scalability to typical image resolutions, and, in many cases, a substantially lower memory footprint.



### Backdooring Convolutional Neural Networks via Targeted Weight Perturbations
- **Arxiv ID**: http://arxiv.org/abs/1812.03128v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.03128v1)
- **Published**: 2018-12-07 17:41:40+00:00
- **Updated**: 2018-12-07 17:41:40+00:00
- **Authors**: Jacob Dumford, Walter Scheirer
- **Comment**: None
- **Journal**: None
- **Summary**: We present a new type of backdoor attack that exploits a vulnerability of convolutional neural networks (CNNs) that has been previously unstudied. In particular, we examine the application of facial recognition. Deep learning techniques are at the top of the game for facial recognition, which means they have now been implemented in many production-level systems. Alarmingly, unlike other commercial technologies such as operating systems and network devices, deep learning-based facial recognition algorithms are not presently designed with security requirements or audited for security vulnerabilities before deployment. Given how young the technology is and how abstract many of the internal workings of these algorithms are, neural network-based facial recognition systems are prime targets for security breaches. As more and more of our personal information begins to be guarded by facial recognition (e.g., the iPhone X), exploring the security vulnerabilities of these systems from a penetration testing standpoint is crucial. Along these lines, we describe a general methodology for backdooring CNNs via targeted weight perturbations. Using a five-layer CNN and ResNet-50 as case studies, we show that an attacker is able to significantly increase the chance that inputs they supply will be falsely accepted by a CNN while simultaneously preserving the error rates for legitimate enrolled classes.



### Removal of Parameter Adjustment of Frangi Filters in Case of Coronary Angiograms
- **Arxiv ID**: http://arxiv.org/abs/1812.03186v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.03186v1)
- **Published**: 2018-12-07 19:10:01+00:00
- **Updated**: 2018-12-07 19:10:01+00:00
- **Authors**: Dhruv Gosain, Rishabh Joshi
- **Comment**: None
- **Journal**: None
- **Summary**: Frangi Filters are one of the widely used filters for enhancing vessels in medical images. Since they were first proposed, the threshold of the vesselness function of Frangi Filters is to be arranged for each individual application. These thresholds are changed manually for individual fluoroscope, for enhancing coronary angiogram images. Hence it is felt, there is a need of mitigating the tuning procedure of threshold values for every fluoroscope. The current papers approach has been devised in order to treat the coronary angiogram images uniformly, irrespective of the fluoroscopes through which they were obtained and the patient demographics for further stenosis detection. This problem to the best of our knowledge has not been addressed yet. In the approach, before feeding the image to Frangi Filters, non uniform illumination of the input image is removed using homomorphic filters and the image is enhanced using Non Subsampled Contourlet Transform (NSCT). The experiment was conducted on the data that has been accumulated from various hospitals in India and the results obtained verifies dependency removal of parameters without compromising the results obtained by Frangi filters.



### Harmonic Networks: Integrating Spectral Information into CNNs
- **Arxiv ID**: http://arxiv.org/abs/1812.03205v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.03205v1)
- **Published**: 2018-12-07 20:21:48+00:00
- **Updated**: 2018-12-07 20:21:48+00:00
- **Authors**: Matej Ulicny, Vladimir A. Krylov, Rozenn Dahyot
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) learn filters in order to capture local correlation patterns in feature space. In contrast, in this paper we propose harmonic blocks that produce features by learning optimal combinations of spectral filters defined by the Discrete Cosine Transform. The harmonic blocks are used to replace conventional convolutional layers to construct partial or fully harmonic CNNs. We extensively validate our approach and show that the introduction of harmonic blocks into state-of-the-art CNN baseline architectures results in comparable or better performance in classification tasks on small NORB, CIFAR10 and CIFAR100 datasets.



### PIRC Net : Using Proposal Indexing, Relationships and Context for Phrase Grounding
- **Arxiv ID**: http://arxiv.org/abs/1812.03213v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.03213v1)
- **Published**: 2018-12-07 20:50:59+00:00
- **Updated**: 2018-12-07 20:50:59+00:00
- **Authors**: Rama Kovvuri, Ram Nevatia
- **Comment**: Accepted in ACCV 2018
- **Journal**: None
- **Summary**: Phrase Grounding aims to detect and localize objects in images that are referred to and are queried by natural language phrases. Phrase grounding finds applications in tasks such as Visual Dialog, Visual Search and Image-text co-reference resolution. In this paper, we present a framework that leverages information such as phrase category, relationships among neighboring phrases in a sentence and context to improve the performance of phrase grounding systems. We propose three modules: Proposal Indexing Network(PIN); Inter-phrase Regression Network(IRN) and Proposal Ranking Network(PRN) each of which analyze the region proposals of an image at increasing levels of detail by incorporating the above information. Also, in the absence of ground-truth spatial locations of the phrases(weakly-supervised), we propose knowledge transfer mechanisms that leverages the framework of PIN module. We demonstrate the effectiveness of our approach on the Flickr 30k Entities and ReferItGame datasets, for which we achieve improvements over state-of-the-art approaches in both supervised and weakly-supervised variants.



