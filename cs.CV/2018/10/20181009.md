# Arxiv Papers in cs.CV on 2018-10-09
### SPIGAN: Privileged Adversarial Learning from Simulation
- **Arxiv ID**: http://arxiv.org/abs/1810.03756v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.03756v3)
- **Published**: 2018-10-09 00:17:24+00:00
- **Updated**: 2019-02-18 06:33:11+00:00
- **Authors**: Kuan-Hui Lee, German Ros, Jie Li, Adrien Gaidon
- **Comment**: Accepted by ICLR 2019
- **Journal**: None
- **Summary**: Deep Learning for Computer Vision depends mainly on the source of supervision.Photo-realistic simulators can generate large-scale automatically labeled syntheticdata, but introduce a domain gap negatively impacting performance. We propose anew unsupervised domain adaptation algorithm, called SPIGAN, relying on Sim-ulator Privileged Information (PI) and Generative Adversarial Networks (GAN).We use internal data from the simulator as PI during the training of a target tasknetwork. We experimentally evaluate our approach on semantic segmentation. Wetrain the networks on real-world Cityscapes and Vistas datasets, using only unla-beled real-world images and synthetic labeled data with z-buffer (depth) PI fromthe SYNTHIA dataset. Our method improves over no adaptation and state-of-the-art unsupervised domain adaptation techniques.



### A Summary of the 4th International Workshop on Recovering 6D Object Pose
- **Arxiv ID**: http://arxiv.org/abs/1810.03758v1
- **DOI**: 10.1007/978-3-030-11009-3_36
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1810.03758v1)
- **Published**: 2018-10-09 00:31:59+00:00
- **Updated**: 2018-10-09 00:31:59+00:00
- **Authors**: Tomas Hodan, Rigas Kouskouridas, Tae-Kyun Kim, Federico Tombari, Kostas Bekris, Bertram Drost, Thibault Groueix, Krzysztof Walas, Vincent Lepetit, Ales Leonardis, Carsten Steger, Frank Michel, Caner Sahin, Carsten Rother, Jiri Matas
- **Comment**: In: Computer Vision - ECCV 2018 Workshops - Munich, Germany,
  September 8-9 and 14, 2018, Proceedings
- **Journal**: None
- **Summary**: This document summarizes the 4th International Workshop on Recovering 6D Object Pose which was organized in conjunction with ECCV 2018 in Munich. The workshop featured four invited talks, oral and poster presentations of accepted workshop papers, and an introduction of the BOP benchmark for 6D object pose estimation. The workshop was attended by 100+ people working on relevant topics in both academia and industry who shared up-to-date advances and discussed open problems.



### Context-Aware Text-Based Binary Image Stylization and Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1810.03767v1
- **DOI**: 10.1109/TIP.2018.2873064
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.03767v1)
- **Published**: 2018-10-09 01:35:46+00:00
- **Updated**: 2018-10-09 01:35:46+00:00
- **Authors**: Shuai Yang, Jiaying Liu, Wenhan Yang, Zongming Guo
- **Comment**: Accepted by IEEE Trans. on Image Processing. Project page:
  http://www.icst.pku.edu.cn/struct/Projects/UTS.html
- **Journal**: None
- **Summary**: In this work, we present a new framework for the stylization of text-based binary images. First, our method stylizes the stroke-based geometric shape like text, symbols and icons in the target binary image based on an input style image. Second, the composition of the stylized geometric shape and a background image is explored. To accomplish the task, we propose legibility-preserving structure and texture transfer algorithms, which progressively narrow the visual differences between the binary image and the style image. The stylization is then followed by a context-aware layout design algorithm, where cues for both seamlessness and aesthetics are employed to determine the optimal layout of the shape in the background. Given the layout, the binary image is seamlessly embedded into the background by texture synthesis under a context-aware boundary constraint. According to the contents of binary images, our method can be applied to many fields. We show that the proposed method is capable of addressing the unsupervised text stylization problem and is superior to state-of-the-art style transfer methods in automatic artistic typography creation. Besides, extensive experiments on various tasks, such as visual-textual presentation synthesis, icon/symbol rendering and structure-guided image inpainting, demonstrate the effectiveness of the proposed method.



### Skeleton Driven Non-rigid Motion Tracking and 3D Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1810.03774v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.03774v1)
- **Published**: 2018-10-09 02:11:03+00:00
- **Updated**: 2018-10-09 02:11:03+00:00
- **Authors**: Shafeeq Elanattil, Peyman Moghadam, Simon Denman, Sridha Sridharan, Clinton Fookes
- **Comment**: Accepted in DICTA 2018
- **Journal**: None
- **Summary**: This paper presents a method which can track and 3D reconstruct the non-rigid surface motion of human performance using a moving RGB-D camera. 3D reconstruction of marker-less human performance is a challenging problem due to the large range of articulated motions and considerable non-rigid deformations. Current approaches use local optimization for tracking. These methods need many iterations to converge and may get stuck in local minima during sudden articulated movements. We propose a puppet model-based tracking approach using skeleton prior, which provides a better initialization for tracking articulated movements. The proposed approach uses an aligned puppet model to estimate correct correspondences for human performance capture. We also contribute a synthetic dataset which provides ground truth locations for frame-by-frame geometry and skeleton joints of human subjects. Experimental results show that our approach is more robust when faced with sudden articulated motions, and provides better 3D reconstruction compared to the existing state-of-the-art approaches.



### Unsupervised Online Video Object Segmentation with Motion Property Understanding
- **Arxiv ID**: http://arxiv.org/abs/1810.03783v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1810.03783v2)
- **Published**: 2018-10-09 02:48:18+00:00
- **Updated**: 2019-08-06 03:12:29+00:00
- **Authors**: Tao Zhuo, Zhiyong Cheng, Peng Zhang, Yongkang Wong, Mohan Kankanhalli
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised video object segmentation aims to automatically segment moving objects over an unconstrained video without any user annotation. So far, only few unsupervised online methods have been reported in literature and their performance is still far from satisfactory, because the complementary information from future frames cannot be processed under online setting. To solve this challenging problem, in this paper, we propose a novel Unsupervised Online Video Object Segmentation (UOVOS) framework by construing the motion property to mean moving in concurrence with a generic object for segmented regions. By incorporating salient motion detection and object proposal, a pixel-wise fusion strategy is developed to effectively remove detection noise such as dynamic background and stationary objects. Furthermore, by leveraging the obtained segmentation from immediately preceding frames, a forward propagation algorithm is employed to deal with unreliable motion detection and object proposals. Experimental results on several benchmark datasets demonstrate the efficacy of the proposed method. Compared to the state-of-the-art unsupervised online segmentation algorithms, the proposed method achieves an absolute gain of 6.2%. Moreover, our method achieves better performance than the best unsupervised offline algorithm on the DAVIS-2016 benchmark dataset. Our code is available on the project website: https://github.com/visiontao/uovos.



### Visual Localization of Key Positions for Visually Impaired People
- **Arxiv ID**: http://arxiv.org/abs/1810.03790v1
- **DOI**: 10.1109/ICPR.2018.8545141
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.03790v1)
- **Published**: 2018-10-09 03:12:40+00:00
- **Updated**: 2018-10-09 03:12:40+00:00
- **Authors**: Ruiqi Cheng, Kaiwei Wang, Longqing Lin, Kailun Yang
- **Comment**: This paper has been accepted by International Conference on Pattern
  Recognition (ICPR) 2018
- **Journal**: None
- **Summary**: On the off-the-shelf navigational assistance devices, the localization precision is limited to the signal error of global navigation satellite system (GNSS). During travelling outdoors, the inaccurately localization perplexes visually impaired people, especially at key positions, such as gates, bus stations or intersections. The visual localization is a feasible approach to improving the positioning precision of assistive devices. Using multiple image descriptors, the paper proposes a robust and efficient visual localization algorithm, which takes advantage of priori GNSS signals and multi-modal images to achieve the accurate localization of key positions. In the experiments, we implement the approach on the wearable system and test the performance of visual localization under practical scenarios.



### Knowing Where to Look? Analysis on Attention of Visual Question Answering System
- **Arxiv ID**: http://arxiv.org/abs/1810.03821v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.03821v1)
- **Published**: 2018-10-09 05:51:08+00:00
- **Updated**: 2018-10-09 05:51:08+00:00
- **Authors**: Wei Li, Zehuan Yuan, Xiangzhong Fang, Changhu Wang
- **Comment**: ECCV SiVL Workshop paper
- **Journal**: None
- **Summary**: Attention mechanisms have been widely used in Visual Question Answering (VQA) solutions due to their capacity to model deep cross-domain interactions. Analyzing attention maps offers us a perspective to find out limitations of current VQA systems and an opportunity to further improve them. In this paper, we select two state-of-the-art VQA approaches with attention mechanisms to study their robustness and disadvantages by visualizing and analyzing their estimated attention maps. We find that both methods are sensitive to features, and simultaneously, they perform badly for counting and multi-object related questions. We believe that the findings and analytical method will help researchers identify crucial challenges on the way to improve their own VQA systems.



### DeepWeeds: A Multiclass Weed Species Image Dataset for Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1810.05726v3
- **DOI**: 10.1038/s41598-018-38343-3
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.05726v3)
- **Published**: 2018-10-09 05:53:26+00:00
- **Updated**: 2019-02-14 11:20:57+00:00
- **Authors**: Alex Olsen, Dmitry A. Konovalov, Bronson Philippa, Peter Ridd, Jake C. Wood, Jamie Johns, Wesley Banks, Benjamin Girgenti, Owen Kenny, James Whinney, Brendan Calvert, Mostafa Rahimi Azghadi, Ronald D. White
- **Comment**: 14 pages, 8 figures, 4 tables
- **Journal**: Sci.Rep. 9, 2058 (2019)
- **Summary**: Robotic weed control has seen increased research of late with its potential for boosting productivity in agriculture. Majority of works focus on developing robotics for croplands, ignoring the weed management problems facing rangeland stock farmers. Perhaps the greatest obstacle to widespread uptake of robotic weed control is the robust classification of weed species in their natural environment. The unparalleled successes of deep learning make it an ideal candidate for recognising various weed species in the complex rangeland environment. This work contributes the first large, public, multiclass image dataset of weed species from the Australian rangelands; allowing for the development of robust classification methods to make robotic weed control viable. The DeepWeeds dataset consists of 17,509 labelled images of eight nationally significant weed species native to eight locations across northern Australia. This paper presents a baseline for classification performance on the dataset using the benchmark deep learning models, Inception-v3 and ResNet-50. These models achieved an average classification accuracy of 95.1% and 95.7%, respectively. We also demonstrate real time performance of the ResNet-50 architecture, with an average inference time of 53.4 ms per image. These strong results bode well for future field implementation of robotic weed control methods in the Australian rangelands.



### Deep Attentive Tracking via Reciprocative Learning
- **Arxiv ID**: http://arxiv.org/abs/1810.03851v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.03851v2)
- **Published**: 2018-10-09 08:25:49+00:00
- **Updated**: 2018-10-15 06:46:50+00:00
- **Authors**: Shi Pu, Yibing Song, Chao Ma, Honggang Zhang, Ming-Hsuan Yang
- **Comment**: In NIPS 2018
- **Journal**: None
- **Summary**: Visual attention, derived from cognitive neuroscience, facilitates human perception on the most pertinent subset of the sensory data. Recently, significant efforts have been made to exploit attention schemes to advance computer vision systems. For visual tracking, it is often challenging to track target objects undergoing large appearance changes. Attention maps facilitate visual tracking by selectively paying attention to temporal robust features. Existing tracking-by-detection approaches mainly use additional attention modules to generate feature weights as the classifiers are not equipped with such mechanisms. In this paper, we propose a reciprocative learning algorithm to exploit visual attention for training deep classifiers. The proposed algorithm consists of feed-forward and backward operations to generate attention maps, which serve as regularization terms coupled with the original classification loss function for training. The deep classifier learns to attend to the regions of target objects robust to appearance changes. Extensive experiments on large-scale benchmark datasets show that the proposed attentive tracking method performs favorably against the state-of-the-art approaches.



### Functionally Modular and Interpretable Temporal Filtering for Robust Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1810.03867v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1810.03867v2)
- **Published**: 2018-10-09 09:13:36+00:00
- **Updated**: 2018-10-15 16:42:20+00:00
- **Authors**: Jörg Wagner, Volker Fischer, Michael Herman, Sven Behnke
- **Comment**: In Proceedings of 29th British Machine Vision Conference (BMVC),
  Newcastle upon Tyne, UK, 2018
- **Journal**: None
- **Summary**: The performance of autonomous systems heavily relies on their ability to generate a robust representation of the environment. Deep neural networks have greatly improved vision-based perception systems but still fail in challenging situations, e.g. sensor outages or heavy weather. These failures are often introduced by data-inherent perturbations, which significantly reduce the information provided to the perception system. We propose a functionally modularized temporal filter, which stabilizes an abstract feature representation of a single-frame segmentation model using information of previous time steps. Our filter module splits the filter task into multiple less complex and more interpretable subtasks. The basic structure of the filter is inspired by a Bayes estimator consisting of a prediction and an update step. To make the prediction more transparent, we implement it using a geometric projection and estimate its parameters. This additionally enables the decomposition of the filter task into static representation filtering and low-dimensional motion filtering. Our model can cope with missing frames and is trainable in an end-to-end fashion. Using photorealistic, synthetic video data, we show the ability of the proposed architecture to overcome data-inherent perturbations. The experiments especially highlight advantages introduced by an interpretable and explicit filter module.



### Conditional Generative Refinement Adversarial Networks for Unbalanced Medical Image Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1810.03871v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.03871v1)
- **Published**: 2018-10-09 09:17:47+00:00
- **Updated**: 2018-10-09 09:17:47+00:00
- **Authors**: Mina Rezaei, Haojin Yang, Christoph Meinel
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new generative adversarial architecture to mitigate imbalance data problem in medical image semantic segmentation where the majority of pixels belongs to a healthy region and few belong to lesion or non-health region. A model trained with imbalanced data tends to bias toward healthy data which is not desired in clinical applications and predicted outputs by these networks have high precision and low sensitivity. We propose a new conditional generative refinement network with three components: a generative, a discriminative, and a refinement network to mitigate unbalanced data problem through ensemble learning. The generative network learns to a segment at the pixel level by getting feedback from the discriminative network according to the true positive and true negative maps. On the other hand, the refinement network learns to predict the false positive and the false negative masks produced by the generative network that has significant value, especially in medical application. The final semantic segmentation masks are then composed by the output of the three networks. The proposed architecture shows state-of-the-art results on LiTS-2017 for liver lesion segmentation, and two microscopic cell segmentation datasets MDA231, PhC-HeLa. We have achieved competitive results on BraTS-2017 for brain tumour segmentation.



### Understanding and Predicting the Memorability of Outdoor Natural Scenes
- **Arxiv ID**: http://arxiv.org/abs/1810.06679v7
- **DOI**: 10.1109/TIP.2020.2975957
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.06679v7)
- **Published**: 2018-10-09 09:25:07+00:00
- **Updated**: 2021-11-23 13:05:39+00:00
- **Authors**: Jiaxin Lu, Mai Xu, Ren Yang, Zulin Wang
- **Comment**: Added links to dataset and code; Modified footnote 6 in experiments.
  arXiv admin note: text overlap with arXiv:1808.08754
- **Journal**: IEEE Transactions on Image Processing, 2020
- **Summary**: Memorability measures how easily an image is to be memorized after glancing, which may contribute to designing magazine covers, tourism publicity materials, and so forth. Recent works have shed light on the visual features that make generic images, object images or face photographs memorable. However, these methods are not able to effectively predict the memorability of outdoor natural scene images. To overcome this shortcoming of previous works, in this paper, we provide an attempt to answer: "what exactly makes outdoor natural scenes memorable". To this end, we first establish a large-scale outdoor natural scene image memorability (LNSIM) database, containing 2,632 outdoor natural scene images with their ground truth memorability scores and the multi-label scene category annotations. Then, similar to previous works, we mine our database to investigate how low-, middle- and high-level handcrafted features affect the memorability of outdoor natural scenes. In particular, we find that the high-level feature of scene category is rather correlated with outdoor natural scene memorability, and the deep features learnt by deep neural network (DNN) are also effective in predicting the memorability scores. Moreover, combining the deep features with the category feature can further boost the performance of memorability prediction. Therefore, we propose an end-to-end DNN based outdoor natural scene memorability (DeepNSM) predictor, which takes advantage of the learned category-related features. Then, the experimental results validate the effectiveness of our DeepNSM model, exceeding the state-of-the-art methods. Finally, we try to understand the reason of the good performance for our DeepNSM model, and also study the cases that our DeepNSM model succeeds or fails to accurately predict the memorability of outdoor natural scenes. Code: github.com/JiaxinLu-home/Natural-Scene-Memorability-Dataset.



### Image Segmentation using Unsupervised Watershed Algorithm with an Over-segmentation Reduction Technique
- **Arxiv ID**: http://arxiv.org/abs/1810.03908v1
- **DOI**: 10.13140/RG.2.2.35055.28323
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.03908v1)
- **Published**: 2018-10-09 11:02:00+00:00
- **Updated**: 2018-10-09 11:02:00+00:00
- **Authors**: Ravimal Bandara
- **Comment**: Source code related to this technical report is available at
  www.codeproject.com/Articles/751744/Image-Segmentation-using-Unsupervised-Watershed-Al
- **Journal**: None
- **Summary**: Image segmentation is the process of partitioning an image into meaningful segments. The meaning of the segments is subjective due to the definition of homogeneity is varied based on the users perspective hence the automation of the segmentation is challenging. Watershed is a popular segmentation technique which assumes topographic map in an image, with the brightness of each pixel representing its height, and finds the lines that run along the tops of ridges. The results from the algorithm typically suffer from over segmentation due to the lack of knowledge of the objects being classified. This paper presents an approach to reduce the over segmentation of watershed algorithm by assuming that the different adjacent segments of an object have similar color distribution. The approach demonstrates an improvement over conventional watershed algorithm.



### Convolutional Neural Networks In Convolution
- **Arxiv ID**: http://arxiv.org/abs/1810.03946v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1810.03946v1)
- **Published**: 2018-10-09 12:59:12+00:00
- **Updated**: 2018-10-09 12:59:12+00:00
- **Authors**: Xiaobo Huang
- **Comment**: 9 pages, 2 figures
- **Journal**: None
- **Summary**: Currently, increasingly deeper neural networks have been applied to improve their accuracy. In contrast, We propose a novel wider Convolutional Neural Networks (CNN) architecture, motivated by the Multi-column Deep Neural Networks and the Network In Network(NIN), aiming for higher accuracy without input data transmutation. In our architecture, namely "CNN In Convolution"(CNNIC), a small CNN, instead of the original generalized liner model(GLM) based filters, is convoluted as kernel on the original image, serving as feature extracting layer of this networks. And further classifications are then carried out by a global average pooling layer and a softmax layer. Dropout and orthonormal initialization are applied to overcome training difficulties including slow convergence and over-fitting. Persuasive classification performance is demonstrated on MNIST.



### 3D model silhouette-based tracking in depth images for puppet suit dynamic video-mapping
- **Arxiv ID**: http://arxiv.org/abs/1810.03956v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.03956v1)
- **Published**: 2018-10-09 13:28:48+00:00
- **Updated**: 2018-10-09 13:28:48+00:00
- **Authors**: Guillaume Caron, Mounya Belghiti, Anthony Dessaux
- **Comment**: 8 pages, 8 figures
- **Journal**: None
- **Summary**: Video-mapping is the process of coherent video-projection of images, animations or movies on static objects or buildings for shows. This paper focuses on the dynamic video-mapping of the suit of a puppet being moved by its puppeteer on the theater stage. This may allow changing the costume dynamically and simulate light interaction and more.   Contrary to common video-mapping, the image warping cannot be done once, offline, before the show. It must be done in real-time, and considering a non-flat projection surface, so that the video-projected suit always maps perfectly the puppet, automatically.   Hence, we propose a new visual tracking method of articulated object, for the puppet tracking, exploiting the silhouette of a 3D model of it, in the depth images of a Kinect v2. Then, considering the precise calibration between the latter and the video-projector, that we propose, coherent dynamic video-mapping is made possible as the presented results show.



### Automatic Segmentation of Thoracic Aorta Segments in Low-Dose Chest CT
- **Arxiv ID**: http://arxiv.org/abs/1810.05727v1
- **DOI**: 10.1117/12.2293114
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.05727v1)
- **Published**: 2018-10-09 13:31:10+00:00
- **Updated**: 2018-10-09 13:31:10+00:00
- **Authors**: Julia M. H. Noothout, Bob D. de Vos, Jelmer M. Wolterink, Ivana Isgum
- **Comment**: None
- **Journal**: SPIE Medical Imaging, 2018, vol. 10574, pp. 105741S
- **Summary**: Morphological analysis and identification of pathologies in the aorta are important for cardiovascular diagnosis and risk assessment in patients. Manual annotation is time-consuming and cumbersome in CT scans acquired without contrast enhancement and with low radiation dose. Hence, we propose an automatic method to segment the ascending aorta, the aortic arch and the thoracic descending aorta in low-dose chest CT without contrast enhancement. Segmentation was performed using a dilated convolutional neural network (CNN), with a receptive field of 131X131 voxels, that classified voxels in axial, coronal and sagittal image slices. To obtain a final segmentation, the obtained probabilities of the three planes were averaged per class, and voxels were subsequently assigned to the class with the highest class probability. Two-fold cross-validation experiments were performed where ten scans were used to train the network and another ten to evaluate the performance. Dice coefficients of 0.83, 0.86 and 0.88, and Average Symmetrical Surface Distances (ASSDs) of 2.44, 1.56 and 1.87 mm were obtained for the ascending aorta, the aortic arch, and the descending aorta, respectively. The results indicate that the proposed method could be used in large-scale studies analyzing the anatomical location of pathology and morphology of the thoracic aorta.



### UOLO - automatic object detection and segmentation in biomedical images
- **Arxiv ID**: http://arxiv.org/abs/1810.05729v1
- **DOI**: 10.1007/978-3-030-00889-5_19
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.05729v1)
- **Published**: 2018-10-09 13:53:13+00:00
- **Updated**: 2018-10-09 13:53:13+00:00
- **Authors**: Teresa Araújo, Guilherme Aresta, Adrian Galdran, Pedro Costa, Ana Maria Mendonça, Aurélio Campilho
- **Comment**: Publised on DLMIA 2018. Licensed under the Creative Commons
  CC-BY-NC-ND 4.0 license: http://creativecommons.org/licenses/by-nc-nd/4.0/
- **Journal**: 4th International Workshop, DLMIA 2018, and 8th International
  Workshop, ML-CDS 2018, Held in Conjunction with MICCAI 2018, Granada, Spain,
  September 20, 2018, Proceedings. 165-173
- **Summary**: We propose UOLO, a novel framework for the simultaneous detection and segmentation of structures of interest in medical images. UOLO consists of an object segmentation module which intermediate abstract representations are processed and used as input for object detection. The resulting system is optimized simultaneously for detecting a class of objects and segmenting an optionally different class of structures. UOLO is trained on a set of bounding boxes enclosing the objects to detect, as well as pixel-wise segmentation information, when available. A new loss function is devised, taking into account whether a reference segmentation is accessible for each training image, in order to suitably backpropagate the error. We validate UOLO on the task of simultaneous optic disc (OD) detection, fovea detection, and OD segmentation from retinal images, achieving state-of-the-art performance on public datasets.



### Glioma Segmentation with Cascaded Unet
- **Arxiv ID**: http://arxiv.org/abs/1810.04008v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.04008v1)
- **Published**: 2018-10-09 14:07:28+00:00
- **Updated**: 2018-10-09 14:07:28+00:00
- **Authors**: Dmitry Lachinov, Evgeny Vasiliev, Vadim Turlapov
- **Comment**: None
- **Journal**: None
- **Summary**: MRI analysis takes central position in brain tumor diagnosis and treatment, thus it's precise evaluation is crucially important. However, it's 3D nature imposes several challenges, so the analysis is often performed on 2D projections that reduces the complexity, but increases bias. On the other hand, time consuming 3D evaluation, like, segmentation, is able to provide precise estimation of a number of valuable spatial characteristics, giving us understanding about the course of the disease.\newline Recent studies, focusing on the segmentation task, report superior performance of Deep Learning methods compared to classical computer vision algorithms. But still, it remains a challenging problem. In this paper we present deep cascaded approach for automatic brain tumor segmentation. Similar to recent methods for object detection, our implementation is based on neural networks; we propose modifications to the 3D UNet architecture and augmentation strategy to efficiently handle multimodal MRI input, besides this we introduce approach to enhance segmentation quality with context obtained from models of the same topology operating on downscaled data. We evaluate presented approach on BraTS 2018 dataset and discuss results.



### Learning Converged Propagations with Deep Prior Ensemble for Image Enhancement
- **Arxiv ID**: http://arxiv.org/abs/1810.04012v1
- **DOI**: 10.1109/TIP.2018.2875568
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.04012v1)
- **Published**: 2018-10-09 14:15:01+00:00
- **Updated**: 2018-10-09 14:15:01+00:00
- **Authors**: Risheng Liu, Long Ma, Yiyang Wang, Lei Zhang
- **Comment**: This paper has been accepted in the IEEE Transactions on Image
  Processing
- **Journal**: None
- **Summary**: Enhancing visual qualities of images plays very important roles in various vision and learning applications. In the past few years, both knowledge-driven maximum a posterior (MAP) with prior modelings and fully data-dependent convolutional neural network (CNN) techniques have been investigated to address specific enhancement tasks. In this paper, by exploiting the advantages of these two types of mechanisms within a complementary propagation perspective, we propose a unified framework, named deep prior ensemble (DPE), for solving various image enhancement tasks. Specifically, we first establish the basic propagation scheme based on the fundamental image modeling cues and then introduce residual CNNs to help predicting the propagation direction at each stage. By designing prior projections to perform feedback control, we theoretically prove that even with experience-inspired CNNs, DPE is definitely converged and the output will always satisfy our fundamental task constraints. The main advantage against conventional optimization-based MAP approaches is that our descent directions are learned from collected training data, thus are much more robust to unwanted local minimums. While, compared with existing CNN type networks, which are often designed in heuristic manners without theoretical guarantees, DPE is able to gain advantages from rich task cues investigated on the bases of domain knowledges. Therefore, DPE actually provides a generic ensemble methodology to integrate both knowledge and data-based cues for different image enhancement tasks. More importantly, our theoretical investigations verify that the feedforward propagations of DPE are properly controlled toward our desired solution. Experimental results demonstrate that the proposed DPE outperforms state-of-the-arts on a variety of image enhancement tasks in terms of both quantitative measure and visual perception quality.



### Comparison of U-net-based Convolutional Neural Networks for Liver Segmentation in CT
- **Arxiv ID**: http://arxiv.org/abs/1810.04017v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.04017v1)
- **Published**: 2018-10-09 14:20:57+00:00
- **Updated**: 2018-10-09 14:20:57+00:00
- **Authors**: Hans Meine, Grzegorz Chlebus, Mohsen Ghafoorian, Itaru Endo, Andrea Schenk
- **Comment**: None
- **Journal**: None
- **Summary**: Various approaches for liver segmentation in CT have been proposed: Besides statistical shape models, which played a major role in this research area, novel approaches on the basis of convolutional neural networks have been introduced recently. Using a set of 219 liver CT datasets with reference segmentations from liver surgery planning, we evaluate the performance of several neural network classifiers based on 2D and 3D U-net architectures. An interesting observation is that slice-wise approaches perform surprisingly well, with mean and median Dice coefficients above 0.97, and may be preferable over 3D approaches given current hardware and software limitations.



### Selective Distillation of Weakly Annotated GTD for Vision-based Slab Identification System
- **Arxiv ID**: http://arxiv.org/abs/1810.04029v2
- **DOI**: 10.1109/ACCESS.2019.2899109
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.04029v2)
- **Published**: 2018-10-09 14:32:45+00:00
- **Updated**: 2018-12-13 14:31:57+00:00
- **Authors**: Sang Jun Lee, Sang Woo Kim, Wookyong Kwon, Gyogwon Koo, Jong Pil Yun
- **Comment**: 10 pages, 12 figures, submitted to a journal
- **Journal**: IEEE Access 7 (2019) 23177-23186
- **Summary**: This paper proposes an algorithm for recognizing slab identification numbers in factory scenes. In the development of a deep-learning based system, manual labeling to make ground truth data (GTD) is an important but expensive task. Furthermore, the quality of GTD is closely related to the performance of a supervised learning algorithm. To reduce manual work in the labeling process, we generated weakly annotated GTD by marking only character centroids. Whereas bounding-boxes for characters require at least a drag-and-drop operation or two clicks to annotate a character location, the weakly annotated GTD requires a single click to record a character location. The main contribution of this paper is on selective distillation to improve the quality of the weakly annotated GTD. Because manual GTD are usually generated by many people, it may contain personal bias or human error. To address this problem, the information in manual GTD is integrated and refined by selective distillation. In the process of selective distillation, a fully convolutional network is trained using the weakly annotated GTD, and its prediction maps are selectively used to revise locations and boundaries of semantic regions of characters in the initial GTD. The modified GTD are used in the main training stage, and a post-processing is conducted to retrieve text information. Experiments were thoroughly conducted on actual industry data collected at a steelmaking factory to demonstrate the effectiveness of the proposed method.



### Geometry meets semantics for semi-supervised monocular depth estimation
- **Arxiv ID**: http://arxiv.org/abs/1810.04093v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.04093v2)
- **Published**: 2018-10-09 15:55:11+00:00
- **Updated**: 2018-10-26 14:39:37+00:00
- **Authors**: Pierluigi Zama Ramirez, Matteo Poggi, Fabio Tosi, Stefano Mattoccia, Luigi Di Stefano
- **Comment**: 16 pages, Accepted to ACCV 2018
- **Journal**: None
- **Summary**: Depth estimation from a single image represents a very exciting challenge in computer vision. While other image-based depth sensing techniques leverage on the geometry between different viewpoints (e.g., stereo or structure from motion), the lack of these cues within a single image renders ill-posed the monocular depth estimation task. For inference, state-of-the-art encoder-decoder architectures for monocular depth estimation rely on effective feature representations learned at training time. For unsupervised training of these models, geometry has been effectively exploited by suitable images warping losses computed from views acquired by a stereo rig or a moving camera. In this paper, we make a further step forward showing that learning semantic information from images enables to improve effectively monocular depth estimation as well. In particular, by leveraging on semantically labeled images together with unsupervised signals gained by geometry through an image warping loss, we propose a deep learning approach aimed at joint semantic segmentation and depth estimation. Our overall learning framework is semi-supervised, as we deploy groundtruth data only in the semantic domain. At training time, our network learns a common feature representation for both tasks and a novel cross-task loss function is proposed. The experimental findings show how, jointly tackling depth prediction and semantic segmentation, allows to improve depth estimation accuracy. In particular, on the KITTI dataset our network outperforms state-of-the-art methods for monocular depth estimation.



### Image Captioning as Neural Machine Translation Task in SOCKEYE
- **Arxiv ID**: http://arxiv.org/abs/1810.04101v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.04101v3)
- **Published**: 2018-10-09 16:16:48+00:00
- **Updated**: 2018-10-15 14:27:17+00:00
- **Authors**: Loris Bazzani, Tobias Domhan, Felix Hieber
- **Comment**: None
- **Journal**: None
- **Summary**: Image captioning is an interdisciplinary research problem that stands between computer vision and natural language processing. The task is to generate a textual description of the content of an image. The typical model used for image captioning is an encoder-decoder deep network, where the encoder captures the essence of an image while the decoder is responsible for generating a sentence describing the image. Attention mechanisms can be used to automatically focus the decoder on parts of the image which are relevant to predict the next word. In this paper, we explore different decoders and attentional models popular in neural machine translation, namely attentional recurrent neural networks, self-attentional transformers, and fully-convolutional networks, which represent the current state of the art of neural machine translation. The image captioning module is available as part of SOCKEYE at https://github.com/awslabs/sockeye which tutorial can be found at https://awslabs.github.io/sockeye/image_captioning.html .



### Real time expert system for anomaly detection of aerators based on computer vision technology and existing surveillance cameras
- **Arxiv ID**: http://arxiv.org/abs/1810.04108v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.04108v3)
- **Published**: 2018-10-09 16:28:14+00:00
- **Updated**: 2018-10-17 15:34:59+00:00
- **Authors**: Yeqi Liu, Yingyi Chen, Huihui Yu, Xiaomin Fang, Chuanyang Gong
- **Comment**: 17 figures
- **Journal**: None
- **Summary**: Aerators are essential and crucial auxiliary devices in intensive culture, especially in industrial culture in China. The traditional methods cannot accurately detect abnormal condition of aerators in time. Surveillance cameras are widely used as visual perception modules of the Internet of Things, and then using these widely existing surveillance cameras to realize real-time anomaly detection of aerators is a cost-free and easy-to-promote method. However, it is difficult to develop such an expert system due to some technical and applied challenges, e.g., illumination, occlusion, complex background, etc. To tackle these aforementioned challenges, we propose a real-time expert system based on computer vision technology and existing surveillance cameras for anomaly detection of aerators, which consists of two modules, i.e., object region detection and working state detection. First, it is difficult to detect the working state for some small object regions in whole images, and the time complexity of global feature comparison is also high, so we present an object region detection method based on the region proposal idea. Moreover, we propose a novel algorithm called reference frame Kanade-Lucas-Tomasi (RF-KLT) algorithm for motion feature extraction in fixed regions. Then, we present a dimension reduction method of time series for establishing a feature dataset with obvious boundaries between classes. Finally, we use machine learning algorithms to build the feature classifier. The experimental results in both the actual video dataset and the augmented video dataset show that the accuracy for detecting object region and working state of aerators is 100% and 99.9% respectively, and the detection speed is 77-333 frames per second (FPS) according to the different types of surveillance cameras.



### Seeing Beyond Appearance - Mapping Real Images into Geometrical Domains for Unsupervised CAD-based Recognition
- **Arxiv ID**: http://arxiv.org/abs/1810.04158v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.04158v1)
- **Published**: 2018-10-09 17:59:16+00:00
- **Updated**: 2018-10-09 17:59:16+00:00
- **Authors**: Benjamin Planche, Sergey Zakharov, Ziyan Wu, Andreas Hutter, Harald Kosch, Slobodan Ilic
- **Comment**: paper + supplementary material; previous work: "Keep it Unreal:
  Bridging the Realism Gap for 2.5D Recognition with Geometry Priors Only"
- **Journal**: None
- **Summary**: While convolutional neural networks are dominating the field of computer vision, one usually does not have access to the large amount of domain-relevant data needed for their training. It thus became common to use available synthetic samples along domain adaptation schemes to prepare algorithms for the target domain. Tackling this problem from a different angle, we introduce a pipeline to map unseen target samples into the synthetic domain used to train task-specific methods. Denoising the data and retaining only the features these recognition algorithms are familiar with, our solution greatly improves their performance. As this mapping is easier to learn than the opposite one (ie to learn to generate realistic features to augment the source samples), we demonstrate how our whole solution can be trained purely on augmented synthetic data, and still perform better than methods trained with domain-relevant information (eg real images or realistic textures for the 3D models). Applying our approach to object recognition from texture-less CAD data, we present a custom generative network which fully utilizes the purely geometrical information to learn robust features and achieve a more refined mapping for unseen color images.



### Penetrating the Fog: the Path to Efficient CNN Models
- **Arxiv ID**: http://arxiv.org/abs/1810.04231v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.04231v2)
- **Published**: 2018-10-09 20:16:29+00:00
- **Updated**: 2018-10-11 00:40:37+00:00
- **Authors**: Kun Wan, Boyuan Feng, Shu Yang, Yufei Ding
- **Comment**: 13 pages, 1 figure
- **Journal**: None
- **Summary**: With the increasing demand to deploy convolutional neural networks (CNNs) on mobile platforms, the sparse kernel approach was proposed, which could save more parameters than the standard convolution while maintaining accuracy. However, despite the great potential, no prior research has pointed out how to craft an sparse kernel design with such potential (i.e., effective design), and all prior works just adopt simple combinations of existing sparse kernels such as group convolution. Meanwhile due to the large design space it is also impossible to try all combinations of existing sparse kernels. In this paper, we are the first in the field to consider how to craft an effective sparse kernel design by eliminating the large design space. Specifically, we present a sparse kernel scheme to illustrate how to reduce the space from three aspects. First, in terms of composition we remove designs composed of repeated layers. Second, to remove designs with large accuracy degradation, we find an unified property named information field behind various sparse kernel designs, which could directly indicate the final accuracy. Last, we remove designs in two cases where a better parameter efficiency could be achieved. Additionally, we provide detailed efficiency analysis on the final four designs in our scheme. Experimental results validate the idea of our scheme by showing that our scheme is able to find designs which are more efficient in using parameters and computation with similar or higher accuracy.



### Deep clustering: On the link between discriminative models and K-means
- **Arxiv ID**: http://arxiv.org/abs/1810.04246v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.04246v2)
- **Published**: 2018-10-09 21:17:09+00:00
- **Updated**: 2019-12-15 23:28:05+00:00
- **Authors**: Mohammed Jabi, Marco Pedersoli, Amar Mitiche, Ismail Ben Ayed
- **Comment**: None
- **Journal**: None
- **Summary**: In the context of recent deep clustering studies, discriminative models dominate the literature and report the most competitive performances. These models learn a deep discriminative neural network classifier in which the labels are latent. Typically, they use multinomial logistic regression posteriors and parameter regularization, as is very common in supervised learning. It is generally acknowledged that discriminative objective functions (e.g., those based on the mutual information or the KL divergence) are more flexible than generative approaches (e.g., K-means) in the sense that they make fewer assumptions about the data distributions and, typically, yield much better unsupervised deep learning results. On the surface, several recent discriminative models may seem unrelated to K-means. This study shows that these models are, in fact, equivalent to K-means under mild conditions and common posterior models and parameter regularization. We prove that, for the commonly used logistic regression posteriors, maximizing the $L_2$ regularized mutual information via an approximate alternating direction method (ADM) is equivalent to a soft and regularized K-means loss. Our theoretical analysis not only connects directly several recent state-of-the-art discriminative models to K-means, but also leads to a new soft and regularized deep K-means algorithm, which yields competitive performance on several image clustering benchmarks.



### Bird Species Classification using Transfer Learning with Multistage Training
- **Arxiv ID**: http://arxiv.org/abs/1810.04250v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1810.04250v2)
- **Published**: 2018-10-09 21:29:08+00:00
- **Updated**: 2018-10-11 16:30:29+00:00
- **Authors**: Sourya Dipta Das, Akash Kumar
- **Comment**: None
- **Journal**: None
- **Summary**: Bird species classification has received more and more attention in the field of computer vision, for its promising applications in biology and environmental studies. Recognizing bird species is difficult due to the challenges of discriminative region localization and fine-grained feature learning. In this paper, we have introduced a Transfer learning based method with multistage training. We have used both Pre-Trained Mask-RCNN and an ensemble model consisting of Inception Nets (InceptionV3 & InceptionResNetV2 ) to get localization and species of the bird from the images respectively. Our final model achieves an F1 score of 0.5567 or 55.67 % on the dataset provided in CVIP 2018 Challenge.



### Inter-Scanner Harmonization of High Angular Resolution DW-MRI using Null Space Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1810.04260v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.04260v1)
- **Published**: 2018-10-09 21:52:10+00:00
- **Updated**: 2018-10-09 21:52:10+00:00
- **Authors**: Vishwesh Nath, Prasanna Parvathaneni, Colin B. Hansen, Allison E. Hainline, Camilo Bermudez, Samuel Remedios, Justin A. Blaber, Kurt G. Schilling, Ilwoo Lyu, Vaibhav Janve, Yurui Gao, Iwona Stepniewska, Baxter P. Rogers, Allen T. Newton, L. Taylor Davis, Jeff Luci, Adam W. Anderson, Bennett A. Landman
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: Diffusion-weighted magnetic resonance imaging (DW-MRI) allows for non-invasive imaging of the local fiber architecture of the human brain at a millimetric scale. Multiple classical approaches have been proposed to detect both single (e.g., tensors) and multiple (e.g., constrained spherical deconvolution, CSD) fiber population orientations per voxel. However, existing techniques generally exhibit low reproducibility across MRI scanners. Herein, we propose a data-driven tech-nique using a neural network design which exploits two categories of data. First, training data were acquired on three squirrel monkey brains using ex-vivo DW-MRI and histology of the brain. Second, repeated scans of human subjects were acquired on two different scanners to augment the learning of the network pro-posed. To use these data, we propose a new network architecture, the null space deep network (NSDN), to simultaneously learn on traditional observed/truth pairs (e.g., MRI-histology voxels) along with repeated observations without a known truth (e.g., scan-rescan MRI). The NSDN was tested on twenty percent of the histology voxels that were kept completely blind to the network. NSDN significantly improved absolute performance relative to histology by 3.87% over CSD and 1.42% over a recently proposed deep neural network approach. More-over, it improved reproducibility on the paired data by 21.19% over CSD and 10.09% over a recently proposed deep approach. Finally, NSDN improved gen-eralizability of the model to a third in vivo human scanner (which was not used in training) by 16.08% over CSD and 10.41% over a recently proposed deep learn-ing approach. This work suggests that data-driven approaches for local fiber re-construction are more reproducible, informative and precise and offers a novel, practical method for determining these models.



### A Tale of Three Probabilistic Families: Discriminative, Descriptive and Generative Models
- **Arxiv ID**: http://arxiv.org/abs/1810.04261v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1810.04261v2)
- **Published**: 2018-10-09 21:54:54+00:00
- **Updated**: 2018-12-04 00:33:15+00:00
- **Authors**: Ying Nian Wu, Ruiqi Gao, Tian Han, Song-Chun Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: The pattern theory of Grenander is a mathematical framework where patterns are represented by probability models on random variables of algebraic structures. In this paper, we review three families of probability models, namely, the discriminative models, the descriptive models, and the generative models. A discriminative model is in the form of a classifier. It specifies the conditional probability of the class label given the input signal. A descriptive model specifies the probability distribution of the signal, based on an energy function defined on the signal. A generative model assumes that the signal is generated by some latent variables via a transformation. We shall review these models within a common framework and explore their connections. We shall also review the recent developments that take advantage of the high approximation capacities of deep neural networks.



