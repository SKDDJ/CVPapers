# Arxiv Papers in cs.CV on 2018-10-08
### Exposition and Interpretation of the Topology of Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1810.03234v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.03234v3)
- **Published**: 2018-10-08 00:34:25+00:00
- **Updated**: 2019-10-18 04:24:28+00:00
- **Authors**: Rickard Br√ºel Gabrielsson, Gunnar Carlsson
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks (CNN's) are powerful and widely used tools. However, their interpretability is far from ideal. One such shortcoming is the difficulty of deducing a network's ability to generalize to unseen data. We use topological data analysis to show that the information encoded in the weights of a CNN can be organized in terms of a topological data model and demonstrate how such information can be interpreted and utilized. We show that the weights of convolutional layers at depths from 1 through 13 learn simple global structures. We also demonstrate the change of the simple structures over the course of training. In particular, we define and analyze the spaces of spatial filters of convolutional layers and show the recurrence, among all networks, depths, and during training, of a simple circle consisting of rotating edges, as well as a less recurring unanticipated complex circle that combines lines, edges, and non-linear patterns. We also demonstrate that topological structure correlates with a network's ability to generalize to unseen data and that topological information can be used to improve a network's performance. We train over a thousand CNN's on MNIST, CIFAR-10, SVHN, and ImageNet.



### Task-Embedded Control Networks for Few-Shot Imitation Learning
- **Arxiv ID**: http://arxiv.org/abs/1810.03237v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1810.03237v1)
- **Published**: 2018-10-08 00:57:24+00:00
- **Updated**: 2018-10-08 00:57:24+00:00
- **Authors**: Stephen James, Michael Bloesch, Andrew J. Davison
- **Comment**: Published at the Conference on Robot Learning (CoRL) 2018
- **Journal**: None
- **Summary**: Much like humans, robots should have the ability to leverage knowledge from previously learned tasks in order to learn new tasks quickly in new and unfamiliar environments. Despite this, most robot learning approaches have focused on learning a single task, from scratch, with a limited notion of generalisation, and no way of leveraging the knowledge to learn other tasks more efficiently. One possible solution is meta-learning, but many of the related approaches are limited in their ability to scale to a large number of tasks and to learn further tasks without forgetting previously learned ones. With this in mind, we introduce Task-Embedded Control Networks, which employ ideas from metric learning in order to create a task embedding that can be used by a robot to learn new tasks from one or more demonstrations. In the area of visually-guided manipulation, we present simulation results in which we surpass the performance of a state-of-the-art method when using only visual information from each demonstration. Additionally, we demonstrate that our approach can also be used in conjunction with domain randomisation to train our few-shot learning ability in simulation and then deploy in the real world without any additional training. Once deployed, the robot can learn new tasks from a single real-world demonstration.



### Diagnosing Convolutional Neural Networks using their Spectral Response
- **Arxiv ID**: http://arxiv.org/abs/1810.03241v1
- **DOI**: None
- **Categories**: **cs.CV**, I.5.2; I.4.7
- **Links**: [PDF](http://arxiv.org/pdf/1810.03241v1)
- **Published**: 2018-10-08 01:27:34+00:00
- **Updated**: 2018-10-08 01:27:34+00:00
- **Authors**: Victor Stamatescu, Mark D. McDonnell
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) are a class of artificial neural networks whose computational blocks use convolution, together with other linear and non-linear operations, to perform classification or regression. This paper explores the spectral response of CNNs and its potential use in diagnosing problems with their training. We measure the gain of CNNs trained for image classification on ImageNet and observe that the best models are also the most sensitive to perturbations of their input. Further, we perform experiments on MNIST and CIFAR-10 to find that the gain rises as the network learns and then saturates as the network converges. Moreover, we find that strong gain fluctuations can point to overfitting and learning problems caused by a poor choice of learning rate. We argue that the gain of CNNs can act as a diagnostic tool and potential replacement for the validation loss when hold-out validation data are not available.



### Arc-support Line Segments Revisited: An Efficient and High-quality Ellipse Detection
- **Arxiv ID**: http://arxiv.org/abs/1810.03243v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.03243v5)
- **Published**: 2018-10-08 01:41:44+00:00
- **Updated**: 2019-08-04 14:49:23+00:00
- **Authors**: Changsheng Lu, Siyu Xia, Ming Shao, Yun Fu
- **Comment**: IEEE Transactions on Image Processing; The paper has been revised to
  a more appropriate title "Arc-support Line Segments Revisited: An Efficient
  and High-quality Ellipse Detection"; In addition, please contacts us if you
  want a higher quality pdf due to the limited uploading size of files in arXiv
  where the resolution of figures might be not very high
- **Journal**: None
- **Summary**: Over the years many ellipse detection algorithms spring up and are studied broadly, while the critical issue of detecting ellipses accurately and efficiently in real-world images remains a challenge. In this paper, we propose a valuable industry-oriented ellipse detector by arc-support line segments, which simultaneously reaches high detection accuracy and efficiency. To simplify the complicated curves in an image while retaining the general properties including convexity and polarity, the arc-support line segments are extracted, which grounds the successful detection of ellipses. The arc-support groups are formed by iteratively and robustly linking the arc-support line segments that latently belong to a common ellipse. Afterward, two complementary approaches, namely, locally selecting the arc-support group with higher saliency and globally searching all the valid paired groups, are adopted to fit the initial ellipses in a fast way. Then, the ellipse candidate set can be formulated by hierarchical clustering of 5D parameter space of initial ellipses. Finally, the salient ellipse candidates are selected and refined as detections subject to the stringent and effective verification. Extensive experiments on three public datasets are implemented and our method achieves the best F-measure scores compared to the state-of-the-art methods. The source code is available at https://github.com/AlanLuSun/High-quality-ellipse-detection.



### Triple Attention Mixed Link Network for Single Image Super Resolution
- **Arxiv ID**: http://arxiv.org/abs/1810.03254v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.03254v1)
- **Published**: 2018-10-08 03:04:07+00:00
- **Updated**: 2018-10-08 03:04:07+00:00
- **Authors**: Xi Cheng, Xiang Li, Jian Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Single image super resolution is of great importance as a low-level computer vision task. Recent approaches with deep convolutional neural networks have achieved im-pressive performance. However, existing architectures have limitations due to the less sophisticated structure along with less strong representational power. In this work, to significantly enhance the feature representation, we proposed Triple Attention mixed link Network (TAN) which consists of 1) three different aspects (i.e., kernel, spatial and channel) of attention mechanisms and 2) fu-sion of both powerful residual and dense connections (i.e., mixed link). Specifically, the network with multi kernel learns multi hierarchical representations under different receptive fields. The output features are recalibrated by the effective kernel and channel attentions and feed into next layer partly residual and partly dense, which filters the information and enable the network to learn more powerful representations. The features finally pass through the spatial attention in the reconstruction network which generates a fusion of local and global information, let the network restore more details and improves the quality of reconstructed images. Thanks to the diverse feature recalibrations and the advanced information flow topology, our proposed model is strong enough to per-form against the state-of-the-art methods on the bench-mark evaluations.



### On Learning and Learned Data Representation by Capsule Networks
- **Arxiv ID**: http://arxiv.org/abs/1810.04041v3
- **DOI**: 10.1109/ACCESS.2019.2911622
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.04041v3)
- **Published**: 2018-10-08 03:14:53+00:00
- **Updated**: 2020-06-22 03:22:39+00:00
- **Authors**: Ancheng Lin, Jun Li, Zhenyuan Ma
- **Comment**: None
- **Journal**: IEEE Access, vol. 7, pp. 50808-50822, 2019
- **Summary**: In this work, we investigate the following: 1) how the routing affects the CapsNet model fitting; 2) how the representation using capsules helps discover global structures in data distribution, and; 3) how the learned data representation adapts and generalizes to new tasks. Our investigation yielded the results some of which have been mentioned in the original paper of CapsNet, they are: 1) the routing operation determines the certainty with which a layer of capsules pass information to the layer above and the appropriate level of certainty is related to the model fitness; 2) in a designed experiment using data with a known 2D structure, capsule representations enable a more meaningful 2D manifold embedding than neurons do in a standard convolutional neural network (CNN), and; 3) compared with neurons of the standard CNN, capsules of successive layers are less coupled and more adaptive to new data distribution.



### Patient-Specific 3D Volumetric Reconstruction of Bioresorbable Stents: A Method to Generate 3D Geometries for Computational Analysis of Coronaries Treated with Bioresorbable Stents
- **Arxiv ID**: http://arxiv.org/abs/1810.03270v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.03270v1)
- **Published**: 2018-10-08 04:58:49+00:00
- **Updated**: 2018-10-08 04:58:49+00:00
- **Authors**: Boyi Yang, Marina Piccinelli, Gaetano Esposito, Tianli Han, Yasir Bouchi, Bill Gogas, Don Giddens, Habib Samady, Alessandro Veneziani
- **Comment**: 26 pages, 12 figures
- **Journal**: None
- **Summary**: As experts continue to debate the optimal surgery practice for coronary disease - percutaneous coronary intervention (PCI) or coronary aortic bypass graft (CABG) - computational tools may provide a quantitative assessment of each option. Computational fluid dynamics (CFD) has been used to assess the interplay between hemodynamics and stent struts; it is of particular interest in Bioresorbable Vascular Stents (BVS), since their thicker struts may result in impacted flow patterns and possible pathological consequences. Many proofs of concept are presented in the literature; however, a practical method for extracting patient-specific stented coronary artery geometries from images over a large number of patients remains an open problem.   This work provides a possible pipeline for the reconstruction of the BVS. Using Optical Coherence Tomographies (OCT) and Invasive Coronary Angiographies (ICA), we can reconstruct the 3D geometry of deployed BVS in vivo. We illustrate the stent reconstruction process: (i) automatic strut detection, (ii) identification of stent components, (iii) 3D registration of stent curvature, and (iv) final stent volume reconstruction. The methodology is designed for use on clinical OCT images, as opposed to approaches that relied on a small number of virtually deployed stents.   The proposed reconstruction process is validated with a virtual phantom stent, providing quantitative assessment of the methodology, and with selected clinical cases, confirming feasibility. Using multimodality image analysis, we obtain reliable reconstructions within a reasonable timeframe. This work is the first step toward a fully automated reconstruction and simulation procedure aiming at an extensive quantitative analysis of the impact of BVS struts on hemodynamics via CFD in clinical trials, going beyond the proof-of-concept stage.



### Light-Weight RefineNet for Real-Time Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1810.03272v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.03272v1)
- **Published**: 2018-10-08 05:18:46+00:00
- **Updated**: 2018-10-08 05:18:46+00:00
- **Authors**: Vladimir Nekrasov, Chunhua Shen, Ian Reid
- **Comment**: Models are available here:
  https://github.com/drsleep/light-weight-refinenet, BMVC 2018
- **Journal**: None
- **Summary**: We consider an important task of effective and efficient semantic image segmentation. In particular, we adapt a powerful semantic segmentation architecture, called RefineNet, into the more compact one, suitable even for tasks requiring real-time performance on high-resolution inputs. To this end, we identify computationally expensive blocks in the original setup, and propose two modifications aimed to decrease the number of parameters and floating point operations. By doing that, we achieve more than twofold model reduction, while keeping the performance levels almost intact. Our fastest model undergoes a significant speed-up boost from 20 FPS to 55 FPS on a generic GPU card on 512x512 inputs with solid 81.1% mean iou performance on the test set of PASCAL VOC, while our slowest model with 32 FPS (from original 17 FPS) shows 82.7% mean iou on the same dataset. Alternatively, we showcase that our approach is easily mixable with light-weight classification networks: we attain 79.2% mean iou on PASCAL VOC using a model that contains only 3.3M parameters and performs only 9.3B floating point operations.



### TV-regularized CT Reconstruction and Metal Artifact Reduction Using Inequality Constraints with Preconditioning
- **Arxiv ID**: http://arxiv.org/abs/1810.03275v1
- **DOI**: None
- **Categories**: **math.NA**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1810.03275v1)
- **Published**: 2018-10-08 05:43:44+00:00
- **Updated**: 2018-10-08 05:43:44+00:00
- **Authors**: Clemens Schiffer
- **Comment**: Master's Thesis, as submitted at the University of Graz
- **Journal**: None
- **Summary**: Total variation(TV) regularization is applied to X-Ray computed tomography(CT) in an effort to reduce metal artifacts. Tikhonov regularization with $L^2$ data fidelity term and total variation regularization is augmented in this novel model by inequality constraints on sinogram data affected by metal to model errors caused by metal. The formulated problem is discretized and solved using the Chambolle-Pock algorithm. Faster convergence is achieved using preconditioning in a Douglas-Rachford spitting method as well as Advanced Direction Method of Multipliers(ADMM). The methods are applied to real and synthetic data demonstrating feasibility of the model to reduce metal artifacts. Technical details of CT data used and its processing are given in the appendix.



### Inter-BMV: Interpolation with Block Motion Vectors for Fast Semantic Segmentation on Video
- **Arxiv ID**: http://arxiv.org/abs/1810.04047v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.04047v1)
- **Published**: 2018-10-08 06:22:30+00:00
- **Updated**: 2018-10-08 06:22:30+00:00
- **Authors**: Samvit Jain, Joseph E. Gonzalez
- **Comment**: 12 pages. arXiv admin note: substantial text overlap with
  arXiv:1803.07742
- **Journal**: None
- **Summary**: Models optimized for accuracy on single images are often prohibitively slow to run on each frame in a video. Recent work exploits the use of optical flow to warp image features forward from select keyframes, as a means to conserve computation on video. This approach, however, achieves only limited speedup, even when optimized, due to the accuracy degradation introduced by repeated forward warping, and the inference cost of optical flow estimation. To address these problems, we propose a new scheme that propagates features using the block motion vectors (BMV) present in compressed video (e.g. H.264 codecs), instead of optical flow, and bi-directionally warps and fuses features from enclosing keyframes to capture scene context on each video frame. Our technique, interpolation-BMV, enables us to accurately estimate the features of intermediate frames, while keeping inference costs low. We evaluate our system on the CamVid and Cityscapes datasets, comparing to both a strong single-frame baseline and related work. We find that we are able to substantially accelerate segmentation on video, achieving near real-time frame rates (20+ frames per second) on large images (e.g. 960 x 720 pixels), while maintaining competitive accuracy. This represents an improvement of almost 6x over the single-frame baseline and 2.5x over the fastest prior work.



### Guiding Intelligent Surveillance System by learning-by-synthesis gaze estimation
- **Arxiv ID**: http://arxiv.org/abs/1810.03286v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.03286v1)
- **Published**: 2018-10-08 07:02:06+00:00
- **Updated**: 2018-10-08 07:02:06+00:00
- **Authors**: Tongtong Zhao, Yuxiao Yan, Jinjia Peng, Zetian Mi, Xianping Fu
- **Comment**: Submit to the journal of Pattern Recognition Letters
- **Journal**: None
- **Summary**: We describe a novel learning-by-synthesis method for estimating gaze direction of an automated intelligent surveillance system. Recently, progress in learning-by-synthesis has proposed training models on synthetic images, which can effectively reduce the cost of manpower and material resources. However, learning from synthetic images still fails to achieve the desired performance compared to naturalistic images due to the different distribution of synthetic images. In an attempt to address this issue, previous method is to improve the realism of synthetic images by learning a model. However, the disadvantage of the method is that the distortion has not been improved and the authenticity level is unstable. To solve this problem, we put forward a new structure to improve synthetic images, via the reference to the idea of style transformation, through which we can efficiently reduce the distortion of pictures and minimize the need of real data annotation. We estimate that this enables generation of highly realistic images, which we demonstrate both qualitatively and with a user study. We quantitatively evaluate the generated images by training models for gaze estimation. We show a significant improvement over using synthetic images, and achieve state-of-the-art results on various datasets including MPIIGaze dataset.



### Sanity Checks for Saliency Maps
- **Arxiv ID**: http://arxiv.org/abs/1810.03292v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.03292v3)
- **Published**: 2018-10-08 07:27:11+00:00
- **Updated**: 2020-11-06 13:40:14+00:00
- **Authors**: Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, Been Kim
- **Comment**: Updating Guided Backprop experiments due to bug. The results and
  conclusions remain the same
- **Journal**: None
- **Summary**: Saliency methods have emerged as a popular tool to highlight features in an input deemed relevant for the prediction of a learned model. Several saliency methods have been proposed, often guided by visual appeal on image data. In this work, we propose an actionable methodology to evaluate what kinds of explanations a given method can and cannot provide. We find that reliance, solely, on visual assessment can be misleading. Through extensive experiments we show that some existing saliency methods are independent both of the model and of the data generating process. Consequently, methods that fail the proposed tests are inadequate for tasks that are sensitive to either data or model, such as, finding outliers in the data, explaining the relationship between inputs and outputs that the model learned, and debugging the model. We interpret our findings through an analogy with edge detection in images, a technique that requires neither training data nor model. Theory in the case of a linear model and a single-layer convolutional neural network supports our experimental findings.



### Local Explanation Methods for Deep Neural Networks Lack Sensitivity to Parameter Values
- **Arxiv ID**: http://arxiv.org/abs/1810.03307v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.03307v1)
- **Published**: 2018-10-08 08:18:14+00:00
- **Updated**: 2018-10-08 08:18:14+00:00
- **Authors**: Julius Adebayo, Justin Gilmer, Ian Goodfellow, Been Kim
- **Comment**: Workshop Track International Conference on Learning Representations
  (ICLR)
- **Journal**: None
- **Summary**: Explaining the output of a complicated machine learning model like a deep neural network (DNN) is a central challenge in machine learning. Several proposed local explanation methods address this issue by identifying what dimensions of a single input are most responsible for a DNN's output. The goal of this work is to assess the sensitivity of local explanations to DNN parameter values. Somewhat surprisingly, we find that DNNs with randomly-initialized weights produce explanations that are both visually and quantitatively similar to those produced by DNNs with learned weights. Our conjecture is that this phenomenon occurs because these explanations are dominated by the lower level features of a DNN, and that a DNN's architecture provides a strong prior which significantly affects the representations learned at these lower layers. NOTE: This work is now subsumed by our recent manuscript, Sanity Checks for Saliency Maps (to appear NIPS 2018), where we expand on findings and address concerns raised in Sundararajan et. al. (2018).



### A Survey on Periocular Biometrics Research
- **Arxiv ID**: http://arxiv.org/abs/1810.03360v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.03360v1)
- **Published**: 2018-10-08 10:15:15+00:00
- **Updated**: 2018-10-08 10:15:15+00:00
- **Authors**: Fernando Alonso-Fernandez, Josef Bigun
- **Comment**: Published in Pattern Recognition Letters
- **Journal**: Pattern Recognition Letters, Special Issue An Insight on Eye
  Biometrics, vol. 82, part 2, pp. 92-105, October 2016, ISSN: 0167-8655
- **Summary**: Periocular refers to the facial region in the vicinity of the eye, including eyelids, lashes and eyebrows. While face and irises have been extensively studied, the periocular region has emerged as a promising trait for unconstrained biometrics, following demands for increased robustness of face or iris systems. With a surprisingly high discrimination ability, this region can be easily obtained with existing setups for face and iris, and the requirement of user cooperation can be relaxed, thus facilitating the interaction with biometric systems. It is also available over a wide range of distances even when the iris texture cannot be reliably obtained (low resolution) or under partial face occlusion (close distances). Here, we review the state of the art in periocular biometrics research. A number of aspects are described, including: i) existing databases, ii) algorithms for periocular detection and/or segmentation, iii) features employed for recognition, iv) identification of the most discriminative regions of the periocular area, v) comparison with iris and face modalities, vi) soft-biometrics (gender/ethnicity classification), and vii) impact of gender transformation and plastic surgery on the recognition accuracy. This work is expected to provide an insight of the most relevant issues in periocular biometrics, giving a comprehensive coverage of the existing literature and current state of the art.



### Deep learning cardiac motion analysis for human survival prediction
- **Arxiv ID**: http://arxiv.org/abs/1810.03382v1
- **DOI**: 10.1038/s42256-019-0019-2
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.03382v1)
- **Published**: 2018-10-08 11:34:38+00:00
- **Updated**: 2018-10-08 11:34:38+00:00
- **Authors**: Ghalib A. Bello, Timothy J. W. Dawes, Jinming Duan, Carlo Biffi, Antonio de Marvao, Luke S. G. E. Howard, J. Simon R. Gibbs, Martin R. Wilkins, Stuart A. Cook, Daniel Rueckert, Declan P. O'Regan
- **Comment**: None
- **Journal**: Nature Machine Intelligence, 1, 95-104 (2019)
- **Summary**: Motion analysis is used in computer vision to understand the behaviour of moving objects in sequences of images. Optimising the interpretation of dynamic biological systems requires accurate and precise motion tracking as well as efficient representations of high-dimensional motion trajectories so that these can be used for prediction tasks. Here we use image sequences of the heart, acquired using cardiac magnetic resonance imaging, to create time-resolved three-dimensional segmentations using a fully convolutional network trained on anatomical shape priors. This dense motion model formed the input to a supervised denoising autoencoder (4Dsurvival), which is a hybrid network consisting of an autoencoder that learns a task-specific latent code representation trained on observed outcome data, yielding a latent representation optimised for survival prediction. To handle right-censored survival outcomes, our network used a Cox partial likelihood loss function. In a study of 302 patients the predictive accuracy (quantified by Harrell's C-index) was significantly higher (p < .0001) for our model C=0.73 (95$\%$ CI: 0.68 - 0.78) than the human benchmark of C=0.59 (95$\%$ CI: 0.53 - 0.65). This work demonstrates how a complex computer vision task using high-dimensional medical image data can efficiently predict human survival.



### Deep LDA Hashing
- **Arxiv ID**: http://arxiv.org/abs/1810.03402v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1810.03402v1)
- **Published**: 2018-10-08 12:27:35+00:00
- **Updated**: 2018-10-08 12:27:35+00:00
- **Authors**: Di Hu, Feiping Nie, Xuelong Li
- **Comment**: 10 pages, 3 figures
- **Journal**: None
- **Summary**: The conventional supervised hashing methods based on classification do not entirely meet the requirements of hashing technique, but Linear Discriminant Analysis (LDA) does. In this paper, we propose to perform a revised LDA objective over deep networks to learn efficient hashing codes in a truly end-to-end fashion. However, the complicated eigenvalue decomposition within each mini-batch in every epoch has to be faced with when simply optimizing the deep network w.r.t. the LDA objective. In this work, the revised LDA objective is transformed into a simple least square problem, which naturally overcomes the intractable problems and can be easily solved by the off-the-shelf optimizer. Such deep extension can also overcome the weakness of LDA Hashing in the limited linear projection and feature learning. Amounts of experiments are conducted on three benchmark datasets. The proposed Deep LDA Hashing shows nearly 70 points improvement over the conventional one on the CIFAR-10 dataset. It also beats several state-of-the-art methods on various metrics.



### Robust 6D Object Pose Estimation in Cluttered Scenes using Semantic Segmentation and Pose Regression Networks
- **Arxiv ID**: http://arxiv.org/abs/1810.03410v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.03410v1)
- **Published**: 2018-10-08 12:48:02+00:00
- **Updated**: 2018-10-08 12:48:02+00:00
- **Authors**: Arul Selvam Periyasamy, Max Schwarz, Sven Behnke
- **Comment**: Accepted for IEEE/RSJ International Conference on Intelligent Robots
  and Systems (IROS), Madrid, Spain, to appear October 2018
- **Journal**: None
- **Summary**: Object pose estimation is a crucial prerequisite for robots to perform autonomous manipulation in clutter. Real-world bin-picking settings such as warehouses present additional challenges, e.g., new objects are added constantly. Most of the existing object pose estimation methods assume that 3D models of the objects is available beforehand. We present a pipeline that requires minimal human intervention and circumvents the reliance on the availability of 3D models by a fast data acquisition method and a synthetic data generation procedure. This work builds on previous work on semantic segmentation of cluttered bin-picking scenes to isolate individual objects in clutter. An additional network is trained on synthetic scenes to estimate object poses from a cropped object-centered encoding extracted from the segmentation results. The proposed method is evaluated on a synthetic validation dataset and cluttered real-world scenes.



### Dense Multimodal Fusion for Hierarchically Joint Representation
- **Arxiv ID**: http://arxiv.org/abs/1810.03414v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1810.03414v1)
- **Published**: 2018-10-08 12:52:36+00:00
- **Updated**: 2018-10-08 12:52:36+00:00
- **Authors**: Di Hu, Feiping Nie, Xuelong Li
- **Comment**: 10 pages, 4 figures
- **Journal**: None
- **Summary**: Multiple modalities can provide more valuable information than single one by describing the same contents in various ways. Hence, it is highly expected to learn effective joint representation by fusing the features of different modalities. However, previous methods mainly focus on fusing the shallow features or high-level representations generated by unimodal deep networks, which only capture part of the hierarchical correlations across modalities. In this paper, we propose to densely integrate the representations by greedily stacking multiple shared layers between different modality-specific networks, which is named as Dense Multimodal Fusion (DMF). The joint representations in different shared layers can capture the correlations in different levels, and the connection between shared layers also provides an efficient way to learn the dependence among hierarchical correlations. These two properties jointly contribute to the multiple learning paths in DMF, which results in faster convergence, lower training loss, and better performance. We evaluate our model on three typical multimodal learning tasks, including audiovisual speech recognition, cross-modal retrieval, and multimodal classification. The noticeable performance in the experiments demonstrates that our model can learn more effective joint representation.



### MRI Super-Resolution using Multi-Channel Total Variation
- **Arxiv ID**: http://arxiv.org/abs/1810.03422v6
- **DOI**: 10.1007/978-3-030-00928-1_97
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.03422v6)
- **Published**: 2018-10-08 13:14:28+00:00
- **Updated**: 2019-09-09 16:42:17+00:00
- **Authors**: Mikael Brudfors, Yael Balbastre, Parashkev Nachev, John Ashburner
- **Comment**: None
- **Journal**: MIUA 2018. Communications in Computer and Information Science, vol
  894
- **Summary**: This paper presents a generative model for super-resolution in routine clinical magnetic resonance images (MRI), of arbitrary orientation and contrast. The model recasts the recovery of high resolution images as an inverse problem, in which a forward model simulates the slice-select profile of the MR scanner. The paper introduces a prior based on multi-channel total variation for MRI super-resolution. Bias-variance trade-off is handled by estimating hyper-parameters from the low resolution input scans. The model was validated on a large database of brain images. The validation showed that the model can improve brain segmentation, that it can recover anatomical information between images of different MR contrasts, and that it generalises well to the large variability present in MR images of different subjects. The implementation is freely available at https://github.com/brudfors/spm_superres



### State of the Art Optical Character Recognition of 19th Century Fraktur Scripts using Open Source Engines
- **Arxiv ID**: http://arxiv.org/abs/1810.03436v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.03436v1)
- **Published**: 2018-10-08 13:32:06+00:00
- **Updated**: 2018-10-08 13:32:06+00:00
- **Authors**: Christian Reul, Uwe Springmann, Christoph Wick, Frank Puppe
- **Comment**: Submitted to DHd 2019 (https://dhd2019.org/) which demands a...
  creative... submission format. Consequently, some captions might look weird
  and some links aren't clickable. Extended version with more technical details
  and some fixes to follow
- **Journal**: None
- **Summary**: In this paper we evaluate Optical Character Recognition (OCR) of 19th century Fraktur scripts without book-specific training using mixed models, i.e. models trained to recognize a variety of fonts and typesets from previously unseen sources. We describe the training process leading to strong mixed OCR models and compare them to freely available models of the popular open source engines OCRopus and Tesseract as well as the commercial state of the art system ABBYY. For evaluation, we use a varied collection of unseen data from books, journals, and a dictionary from the 19th century. The experiments show that training mixed models with real data is superior to training with synthetic data and that the novel OCR engine Calamari outperforms the other engines considerably, on average reducing ABBYYs character error rate (CER) by over 70%, resulting in an average CER below 1%.



### NSGA-Net: Neural Architecture Search using Multi-Objective Genetic Algorithm
- **Arxiv ID**: http://arxiv.org/abs/1810.03522v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1810.03522v2)
- **Published**: 2018-10-08 15:14:33+00:00
- **Updated**: 2019-04-18 23:07:16+00:00
- **Authors**: Zhichao Lu, Ian Whalen, Vishnu Boddeti, Yashesh Dhebar, Kalyanmoy Deb, Erik Goodman, Wolfgang Banzhaf
- **Comment**: GECCO 2019
- **Journal**: None
- **Summary**: This paper introduces NSGA-Net -- an evolutionary approach for neural architecture search (NAS). NSGA-Net is designed with three goals in mind: (1) a procedure considering multiple and conflicting objectives, (2) an efficient procedure balancing exploration and exploitation of the space of potential neural network architectures, and (3) a procedure finding a diverse set of trade-off network architectures achieved in a single run. NSGA-Net is a population-based search algorithm that explores a space of potential neural network architectures in three steps, namely, a population initialization step that is based on prior-knowledge from hand-crafted architectures, an exploration step comprising crossover and mutation of architectures, and finally an exploitation step that utilizes the hidden useful knowledge stored in the entire history of evaluated neural architectures in the form of a Bayesian Network. Experimental results suggest that combining the dual objectives of minimizing an error metric and computational complexity, as measured by FLOPs, allows NSGA-Net to find competitive neural architectures. Moreover, NSGA-Net achieves error rate on the CIFAR-10 dataset on par with other state-of-the-art NAS methods while using orders of magnitude less computational resources. These results are encouraging and shows the promise to further use of EC methods in various deep-learning paradigms.



### Trace Quotient with Sparsity Priors for Learning Low Dimensional Image Representations
- **Arxiv ID**: http://arxiv.org/abs/1810.03523v1
- **DOI**: None
- **Categories**: **cs.CV**, 14J60, F.2.2
- **Links**: [PDF](http://arxiv.org/pdf/1810.03523v1)
- **Published**: 2018-10-08 15:19:13+00:00
- **Updated**: 2018-10-08 15:19:13+00:00
- **Authors**: Xian Wei, Hao Shen, Martin Kleinsteuber
- **Comment**: 17 pages
- **Journal**: None
- **Summary**: This work studies the problem of learning appropriate low dimensional image representations. We propose a generic algorithmic framework, which leverages two classic representation learning paradigms, i.e., sparse representation and the trace quotient criterion. The former is a well-known powerful tool to identify underlying self-explanatory factors of data, while the latter is known for disentangling underlying low dimensional discriminative factors in data. Our developed solutions disentangle sparse representations of images by employing the trace quotient criterion. We construct a unified cost function, coined as the SPARse LOW dimensional representation (SparLow) function, for jointly learning both a sparsifying dictionary and a dimensionality reduction transformation. The SparLow function is widely applicable for developing various algorithms in three classic machine learning scenarios, namely, unsupervised, supervised, and semi-supervised learning. In order to develop efficient joint learning algorithms for maximizing the SparLow function, we deploy a framework of sparse coding with appropriate convex priors to ensure the sparse representations to be locally differentiable. Moreover, we develop an efficient geometric conjugate gradient algorithm to maximize the SparLow function on its underlying Riemannian manifold. Performance of the proposed SparLow algorithmic framework is investigated on several image processing tasks, such as 3D data visualization, face/digit recognition, and object/scene categorization.



### Bootstrapped CNNs for Building Segmentation on RGB-D Aerial Imagery
- **Arxiv ID**: http://arxiv.org/abs/1810.03570v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.03570v1)
- **Published**: 2018-10-08 16:41:36+00:00
- **Updated**: 2018-10-08 16:41:36+00:00
- **Authors**: Clint Sebastian, Bas Boom, Thijs van Lankveld, Egor Bondarev, Peter H. N. De With
- **Comment**: Published at ISPRS Annals of the Photogrammetry, Remote Sensing and
  Spatial Information Sciences
- **Journal**: None
- **Summary**: Detection of buildings and other objects from aerial images has various applications in urban planning and map making. Automated building detection from aerial imagery is a challenging task, as it is prone to varying lighting conditions, shadows and occlusions. Convolutional Neural Networks (CNNs) are robust against some of these variations, although they fail to distinguish easy and difficult examples. We train a detection algorithm from RGB-D images to obtain a segmented mask by using the CNN architecture DenseNet.First, we improve the performance of the model by applying a statistical re-sampling technique called Bootstrapping and demonstrate that more informative examples are retained. Second, the proposed method outperforms the non-bootstrapped version by utilizing only one-sixth of the original training data and it obtains a precision-recall break-even of 95.10% on our aerial imagery dataset.



### SFV: Reinforcement Learning of Physical Skills from Videos
- **Arxiv ID**: http://arxiv.org/abs/1810.03599v2
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1810.03599v2)
- **Published**: 2018-10-08 17:55:39+00:00
- **Updated**: 2018-10-15 17:15:34+00:00
- **Authors**: Xue Bin Peng, Angjoo Kanazawa, Jitendra Malik, Pieter Abbeel, Sergey Levine
- **Comment**: None
- **Journal**: None
- **Summary**: Data-driven character animation based on motion capture can produce highly naturalistic behaviors and, when combined with physics simulation, can provide for natural procedural responses to physical perturbations, environmental changes, and morphological discrepancies. Motion capture remains the most popular source of motion data, but collecting mocap data typically requires heavily instrumented environments and actors. In this paper, we propose a method that enables physically simulated characters to learn skills from videos (SFV). Our approach, based on deep pose estimation and deep reinforcement learning, allows data-driven animation to leverage the abundance of publicly available video clips from the web, such as those from YouTube. This has the potential to enable fast and easy design of character controllers simply by querying for video recordings of the desired behavior. The resulting controllers are robust to perturbations, can be adapted to new settings, can perform basic object interactions, and can be retargeted to new morphologies via reinforcement learning. We further demonstrate that our method can predict potential human motions from still images, by forward simulation of learned controllers initialized from the observed pose. Our framework is able to learn a broad range of dynamic skills, including locomotion, acrobatics, and martial arts.



### Overcoming Language Priors in Visual Question Answering with Adversarial Regularization
- **Arxiv ID**: http://arxiv.org/abs/1810.03649v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.03649v2)
- **Published**: 2018-10-08 18:29:05+00:00
- **Updated**: 2018-11-08 20:51:44+00:00
- **Authors**: Sainandan Ramakrishnan, Aishwarya Agrawal, Stefan Lee
- **Comment**: NIPS 2018. 11 pages ( with references ), 4 figures, 2 tables
- **Journal**: None
- **Summary**: Modern Visual Question Answering (VQA) models have been shown to rely heavily on superficial correlations between question and answer words learned during training such as overwhelmingly reporting the type of room as kitchen or the sport being played as tennis, irrespective of the image. Most alarmingly, this shortcoming is often not well reflected during evaluation because the same strong priors exist in test distributions; however, a VQA system that fails to ground questions in image content would likely perform poorly in real-world settings. In this work, we present a novel regularization scheme for VQA that reduces this effect. We introduce a question-only model that takes as input the question encoding from the VQA model and must leverage language biases in order to succeed. We then pose training as an adversarial game between the VQA model and this question-only adversary -- discouraging the VQA model from capturing language biases in its question encoding. Further,we leverage this question-only model to estimate the increase in model confidence after considering the image, which we maximize explicitly to encourage visual grounding. Our approach is a model agnostic training procedure and simple to implement. We show empirically that it can improve performance significantly on a bias-sensitive split of the VQA dataset for multiple base models -- achieving state-of-the-art on this task. Further, on standard VQA tasks, our approach shows significantly less drop in accuracy compared to existing bias-reducing VQA models.



### Joint Unsupervised Learning of Optical Flow and Depth by Watching Stereo Videos
- **Arxiv ID**: http://arxiv.org/abs/1810.03654v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.03654v1)
- **Published**: 2018-10-08 18:49:46+00:00
- **Updated**: 2018-10-08 18:49:46+00:00
- **Authors**: Yang Wang, Zhenheng Yang, Peng Wang, Yi Yang, Chenxu Luo, Wei Xu
- **Comment**: Submitted to AAAI 2019
- **Journal**: None
- **Summary**: Learning depth and optical flow via deep neural networks by watching videos has made significant progress recently. In this paper, we jointly solve the two tasks by exploiting the underlying geometric rules within stereo videos. Specifically, given two consecutive stereo image pairs from a video, we first estimate depth, camera ego-motion and optical flow from three neural networks. Then the whole scene is decomposed into moving foreground and static background by compar- ing the estimated optical flow and rigid flow derived from the depth and ego-motion. We propose a novel consistency loss to let the optical flow learn from the more accurate rigid flow in static regions. We also design a rigid alignment module which helps refine ego-motion estimation by using the estimated depth and optical flow. Experiments on the KITTI dataset show that our results significantly outperform other state-of- the-art algorithms. Source codes can be found at https: //github.com/baidu-research/UnDepthflow



### Domain Transfer for 3D Pose Estimation from Color Images without Manual Annotations
- **Arxiv ID**: http://arxiv.org/abs/1810.03707v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.03707v2)
- **Published**: 2018-10-08 21:24:39+00:00
- **Updated**: 2019-02-21 10:22:05+00:00
- **Authors**: Mahdi Rad, Markus Oberweger, Vincent Lepetit
- **Comment**: ACCV 2018 (oral)
- **Journal**: None
- **Summary**: We introduce a novel learning method for 3D pose estimation from color images. While acquiring annotations for color images is a difficult task, our approach circumvents this problem by learning a mapping from paired color and depth images captured with an RGB-D camera. We jointly learn the pose from synthetic depth images that are easy to generate, and learn to align these synthetic depth images with the real depth images. We show our approach for the task of 3D hand pose estimation and 3D object pose estimation, both from color images only. Our method achieves performances comparable to state-of-the-art methods on popular benchmark datasets, without requiring any annotations for the color images.



### Saliency Prediction in the Deep Learning Era: Successes, Limitations, and Future Challenges
- **Arxiv ID**: http://arxiv.org/abs/1810.03716v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.03716v3)
- **Published**: 2018-10-08 21:50:27+00:00
- **Updated**: 2019-05-24 23:29:41+00:00
- **Authors**: Ali Borji
- **Comment**: None
- **Journal**: None
- **Summary**: Visual saliency models have enjoyed a big leap in performance in recent years, thanks to advances in deep learning and large scale annotated data. Despite enormous effort and huge breakthroughs, however, models still fall short in reaching human-level accuracy. In this work, I explore the landscape of the field emphasizing on new deep saliency models, benchmarks, and datasets. A large number of image and video saliency models are reviewed and compared over two image benchmarks and two large scale video datasets. Further, I identify factors that contribute to the gap between models and humans and discuss remaining issues that need to be addressed to build the next generation of more powerful saliency models. Some specific questions that are addressed include: in what ways current models fail, how to remedy them, what can be learned from cognitive studies of attention, how explicit saliency judgments relate to fixations, how to conduct fair model comparison, and what are the emerging applications of saliency models.



### Probabilistic Semantic Inpainting with Pixel Constrained CNNs
- **Arxiv ID**: http://arxiv.org/abs/1810.03728v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.03728v2)
- **Published**: 2018-10-08 22:19:08+00:00
- **Updated**: 2019-02-23 22:38:27+00:00
- **Authors**: Emilien Dupont, Suhas Suresha
- **Comment**: AISTATS camera ready version
- **Journal**: None
- **Summary**: Semantic inpainting is the task of inferring missing pixels in an image given surrounding pixels and high level image semantics. Most semantic inpainting algorithms are deterministic: given an image with missing regions, a single inpainted image is generated. However, there are often several plausible inpaintings for a given missing region. In this paper, we propose a method to perform probabilistic semantic inpainting by building a model, based on PixelCNNs, that learns a distribution of images conditioned on a subset of visible pixels. Experiments on the MNIST and CelebA datasets show that our method produces diverse and realistic inpaintings.



### Novel Single View Constraints for Manhattan 3D Line Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1810.03737v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.03737v1)
- **Published**: 2018-10-08 22:51:50+00:00
- **Updated**: 2018-10-08 22:51:50+00:00
- **Authors**: Siddhant Ranade, Srikumar Ramalingam
- **Comment**: Accepted at 3DV 2018
- **Journal**: None
- **Summary**: This paper proposes a novel and exact method to reconstruct line-based 3D structure from a single image using Manhattan world assumption. This problem is a distinctly unsolved problem because there can be multiple 3D reconstructions from a single image. Thus, we are often forced to look for priors like Manhattan world assumption and common scene structures. In addition to the standard orthogonality, perspective projection, and parallelism constraints, we investigate a few novel constraints based on the physical realizability of the 3D scene structure. We treat the line segments in the image to be part of a graph similar to straws and connectors game, where the goal is to back-project the line segments in 3D space and while ensuring that some of these 3D line segments connect with each other (i.e., truly intersect in 3D space) to form the 3D structure. We consider three sets of novel constraints while solving the reconstruction: (1) constraints on a series of Manhattan line intersections that form cycles, but are not all physically realizable, (2) constraints on true and false intersections in the case of nearby lines lying on the same Manhattan plane, and (3) constraints from the intersections on boundary and non-boundary line segments. The reconstruction is achieved using mixed integer linear programming (MILP), and we show compelling results on real images. Along with this paper, we will release a challenging Single View Line Reconstruction dataset with ground truth 3D line models for research purposes.



### Neural Networks Models for Analyzing Magic: the Gathering Cards
- **Arxiv ID**: http://arxiv.org/abs/1810.03744v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.03744v1)
- **Published**: 2018-10-08 23:25:18+00:00
- **Updated**: 2018-10-08 23:25:18+00:00
- **Authors**: Felipe Zilio, Marcelo Prates, Luis Lamb
- **Comment**: 10 pages, 1 figure, 9 tables. Accepted at ICONIP 2018
- **Journal**: None
- **Summary**: Historically, games of all kinds have often been the subject of study in scientific works of Computer Science, including the field of machine learning. By using machine learning techniques and applying them to a game with defined rules or a structured dataset, it's possible to learn and improve on the already existing techniques and methods to tackle new challenges and solve problems that are out of the ordinary. The already existing work on card games tends to focus on gameplay and card mechanics. This work aims to apply neural networks models, including Convolutional Neural Networks and Recurrent Neural Networks, in order to analyze Magic: the Gathering cards, both in terms of card text and illustrations; the card images and texts are used to train the networks in order to be able to classify them into multiple categories. The ultimate goal was to develop a methodology that could generate card text matching it to an input image, which was attained by relating the prediction values of the images and generated text across the different categories.



### Deep residual networks for automatic sleep stage classification of raw polysomnographic waveforms
- **Arxiv ID**: http://arxiv.org/abs/1810.03745v1
- **DOI**: 10.1109/EMBC.2018.8513080
- **Categories**: **cs.CV**, eess.SP, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.03745v1)
- **Published**: 2018-10-08 23:28:33+00:00
- **Updated**: 2018-10-08 23:28:33+00:00
- **Authors**: Alexander Neergaard Olesen, Poul Jennum, Paul Peppard, Emmanuel Mignot, Helge Bjarup Dissing Sorensen
- **Comment**: None
- **Journal**: 2018 40th Annual International Conference of the IEEE Engineering
  in Medicine and Biology Society (EMBC), Honolulu, HI, 2018, pp. 1-4
- **Summary**: We have developed an automatic sleep stage classification algorithm based on deep residual neural networks and raw polysomnogram signals. Briefly, the raw data is passed through 50 convolutional layers before subsequent classification into one of five sleep stages. Three model configurations were trained on 1850 polysomnogram recordings and subsequently tested on 230 independent recordings. Our best performing model yielded an accuracy of 84.1% and a Cohen's kappa of 0.746, improving on previous reported results by other groups also using only raw polysomnogram data. Most errors were made on non-REM stage 1 and 3 decisions, errors likely resulting from the definition of these stages. Further testing on independent cohorts is needed to verify performance for clinical use.



### Neural Network based classification of bone metastasis by primary cacinoma
- **Arxiv ID**: http://arxiv.org/abs/1810.05725v1
- **DOI**: 10.5121/csit.2018.80707
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/1810.05725v1)
- **Published**: 2018-10-08 23:44:59+00:00
- **Updated**: 2018-10-08 23:44:59+00:00
- **Authors**: Marija Prokopijeviƒá, Aleksandar Stanƒçiƒá, Jelena Vasiljeviƒá, ≈Ωeljko Stojkoviƒá, Goran Dimiƒá, Jelena Sopta, Dalibor Ristiƒá, Dhinaharan Nagamalai
- **Comment**: 13 pages, 9 figures
- **Journal**: Computer Science & Information Technology (CS & IT), 7th
  International Conference on Information Technology Convergence and Services
  (ITCSE 2018), Vienna, Austria, May 26~27, 2018
- **Summary**: Neural networks have been known for a long time as a tool for different types of classification, but only just in the last decade they have showed their entire power. Along with appearing of hardware that is capable to support demanding matrix operations and parallel algorithms, the neural network, as a universal function approximation framework, turns out to be the most successful classification method widely used in all fields of science. On the other side, multifractal (MF) approach is an efficient way for quantitative description of complex structures [1] such as metastatic carcinoma, which recommends this method as an accurate tool for medical diagnostics. The only part that is missing is classification method. The goal of this research is to describe and apply a feed-forward neural network as an auxiliary diagnostic method for classification of multifractal parameters in order to determine primary cancer.



