# Arxiv Papers in cs.CV on 2018-10-29
### Text-Adaptive Generative Adversarial Networks: Manipulating Images with Natural Language
- **Arxiv ID**: http://arxiv.org/abs/1810.11919v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.11919v2)
- **Published**: 2018-10-29 01:47:09+00:00
- **Updated**: 2018-11-28 02:01:11+00:00
- **Authors**: Seonghyeon Nam, Yunji Kim, Seon Joo Kim
- **Comment**: NeurIPS 2018
- **Journal**: None
- **Summary**: This paper addresses the problem of manipulating images using natural language description. Our task aims to semantically modify visual attributes of an object in an image according to the text describing the new visual appearance. Although existing methods synthesize images having new attributes, they do not fully preserve text-irrelevant contents of the original image. In this paper, we propose the text-adaptive generative adversarial network (TAGAN) to generate semantically manipulated images while preserving text-irrelevant contents. The key to our method is the text-adaptive discriminator that creates word-level local discriminators according to input text to classify fine-grained attributes independently. With this discriminator, the generator learns to generate images where only regions that correspond to the given text are modified. Experimental results show that our method outperforms existing methods on CUB and Oxford-102 datasets, and our results were mostly preferred on a user study. Extensive analysis shows that our method is able to effectively disentangle visual attributes and produce pleasing outputs.



### Attention-Mechanism-based Tracking Method for Intelligent Internet of Vehicles
- **Arxiv ID**: http://arxiv.org/abs/1811.02682v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.02682v1)
- **Published**: 2018-10-29 04:13:25+00:00
- **Updated**: 2018-10-29 04:13:25+00:00
- **Authors**: Xu Kang, Bin Song, Jie Guo, Xiaojiang Du, Mohsen Guizani
- **Comment**: None
- **Journal**: None
- **Summary**: Vehicle tracking task plays an important role on the internet of vehicles and intelligent transportation system. Beyond the traditional GPS sensor, the image sensor can capture different kinds of vehicles, analyze their driving situation and can interact with them. Aiming at the problem that the traditional convolutional neural network is vulnerable to background interference, this paper proposes vehicle tracking method based on human attention mechanism for self-selection of deep features with an inter-channel fully connected layer. It mainly includes the following contents: 1) A fully convolutional neural network fused attention mechanism with the selection of the deep features for convolution. 2) A separation method for template and semantic background region to separate target vehicles from the background in the initial frame adaptively. 3) A two-stage method for model training using our traffic dataset. The experimental results show that the proposed method improves the tracking accuracy without an increase in tracking time. Meanwhile, it strengthens the robustness of algorithm under the condition of the complex background region. The success rate of the proposed method in overall traffic datasets is higher than Siamese network by about 10 percent, and the overall precision is higher than Siamese network by 8 percent.



### Vehicle Tracking Using Surveillance with Multimodal Data Fusion
- **Arxiv ID**: http://arxiv.org/abs/1811.02627v1
- **DOI**: 10.1109/TITS.2017.2787101
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1811.02627v1)
- **Published**: 2018-10-29 04:14:05+00:00
- **Updated**: 2018-10-29 04:14:05+00:00
- **Authors**: Yue Zhang, Bin Song, Xiaojiang Du, Mohsen Guizani
- **Comment**: 8 pages,6 figures,33 conferences
- **Journal**: None
- **Summary**: Vehicle location prediction or vehicle tracking is a significant topic within connected vehicles. This task, however, is difficult if only a single modal data is available, probably causing bias and impeding the accuracy. With the development of sensor networks in connected vehicles, multimodal data are becoming accessible. Therefore, we propose a framework for vehicle tracking with multimodal data fusion. Specifically, we fuse the results of two modalities, images and velocity, in our vehicle-tracking task. Images, being processed in the module of vehicle detection, provide direct information about the features of vehicles, whereas velocity estimation can further evaluate the possible location of the target vehicles, which reduces the number of features being compared, and decreases the time consumption and computational cost. Vehicle detection is designed with a color-faster R-CNN, which takes both the shape and color of the vehicles into consideration. Meanwhile, velocity estimation is through the Kalman filter, which is a classical method for tracking. Finally, a multimodal data fusion method is applied to integrate these outcomes so that vehicle-tracking tasks can be achieved. Experimental results suggest the efficiency of our methods, which can track vehicles using a series of surveillance cameras in urban areas.



### Evolutionary Self-Expressive Models for Subspace Clustering
- **Arxiv ID**: http://arxiv.org/abs/1810.11957v1
- **DOI**: 10.1109/JSTSP.2018.2877478
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.11957v1)
- **Published**: 2018-10-29 05:08:16+00:00
- **Updated**: 2018-10-29 05:08:16+00:00
- **Authors**: Abolfazl Hashemi, Haris Vikalo
- **Comment**: None
- **Journal**: IEEE Journal of Selected Topics in Signal Processing, Special
  Issue on Data Science: Robust Subspace Learning and Tracking, vol. 12, no. 6,
  December 2018
- **Summary**: The problem of organizing data that evolves over time into clusters is encountered in a number of practical settings. We introduce evolutionary subspace clustering, a method whose objective is to cluster a collection of evolving data points that lie on a union of low-dimensional evolving subspaces. To learn the parsimonious representation of the data points at each time step, we propose a non-convex optimization framework that exploits the self-expressiveness property of the evolving data while taking into account representation from the preceding time step. To find an approximate solution to the aforementioned non-convex optimization problem, we develop a scheme based on alternating minimization that both learns the parsimonious representation as well as adaptively tunes and infers a smoothing parameter reflective of the rate of data evolution. The latter addresses a fundamental challenge in evolutionary clustering -- determining if and to what extent one should consider previous clustering solutions when analyzing an evolving data collection. Our experiments on both synthetic and real-world datasets demonstrate that the proposed framework outperforms state-of-the-art static subspace clustering algorithms and existing evolutionary clustering schemes in terms of both accuracy and running time, in a range of scenarios.



### GOT-10k: A Large High-Diversity Benchmark for Generic Object Tracking in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1810.11981v3
- **DOI**: 10.1109/TPAMI.2019.2957464
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.11981v3)
- **Published**: 2018-10-29 07:22:46+00:00
- **Updated**: 2019-11-20 07:49:29+00:00
- **Authors**: Lianghua Huang, Xin Zhao, Kaiqi Huang
- **Comment**: Accepted by IEEE Transactions on Pattern Analysis and Machine
  Intelligence (TPAMI)
- **Journal**: None
- **Summary**: We introduce here a large tracking database that offers an unprecedentedly wide coverage of common moving objects in the wild, called GOT-10k. Specifically, GOT-10k is built upon the backbone of WordNet structure and it populates the majority of over 560 classes of moving objects and 87 motion patterns, magnitudes wider than the most recent similar-scale counterparts. The contributions of this paper are summarized in the following: (1) GOT-10k offers over 10,000 video segments with more than 1.5 million manually labeled bounding boxes, enabling unified training and stable evaluation of deep trackers. (2) GOT-10k is by far the first video trajectory dataset that uses the semantic hierarchy of WordNet to guide class population. (3) For the first time, GOT-10k introduces the one-shot protocol for tracker evaluation, where the training and test classes are zero-overlapped. The protocol avoids biased evaluation results towards familiar objects and it promotes generalization in tracker development. (4) We conduct extensive tracking experiments with 39 typical tracking algorithms on GOT-10k and analyze their results in this paper. (5) Finally, we develop a comprehensive platform for the tracking community that offers full-featured evaluation toolkits, an online evaluation server, and a responsive leaderboard. The annotations of GOT-10k's test data are kept private to avoid tuning parameters on it. The database, toolkits, evaluation server and baseline results are available at http://got-10k.aitestunion.com.



### An Augmented Linear Mixing Model to Address Spectral Variability for Hyperspectral Unmixing
- **Arxiv ID**: http://arxiv.org/abs/1810.12000v1
- **DOI**: 10.1109/TIP.2018.2878958
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.12000v1)
- **Published**: 2018-10-29 08:35:42+00:00
- **Updated**: 2018-10-29 08:35:42+00:00
- **Authors**: Danfeng Hong, Naoto Yokoya, Jocelyn Chanussot, Xiao Xiang Zhu
- **Comment**: None
- **Journal**: IEEE Transactions on Image Processing, 2019, 28(4): 1923-1938
- **Summary**: Hyperspectral imagery collected from airborne or satellite sources inevitably suffers from spectral variability, making it difficult for spectral unmixing to accurately estimate abundance maps. The classical unmixing model, the linear mixing model (LMM), generally fails to handle this sticky issue effectively. To this end, we propose a novel spectral mixture model, called the augmented linear mixing model (ALMM), to address spectral variability by applying a data-driven learning strategy in inverse problems of hyperspectral unmixing. The proposed approach models the main spectral variability (i.e., scaling factors) generated by variations in illumination or typography separately by means of the endmember dictionary. It then models other spectral variabilities caused by environmental conditions (e.g., local temperature and humidity, atmospheric effects) and instrumental configurations (e.g., sensor noise), as well as material nonlinear mixing effects, by introducing a spectral variability dictionary. To effectively run the data-driven learning strategy, we also propose a reasonable prior knowledge for the spectral variability dictionary, whose atoms are assumed to be low-coherent with spectral signatures of endmembers, which leads to a well-known low coherence dictionary learning problem. Thus, a dictionary learning technique is embedded in the framework of spectral unmixing so that the algorithm can learn the spectral variability dictionary and estimate the abundance maps simultaneously. Extensive experiments on synthetic and real datasets are performed to demonstrate the superiority and effectiveness of the proposed method in comparison with previous state-of-the-art methods.



### Patch-Based Sparse Representation For Bacterial Detection
- **Arxiv ID**: http://arxiv.org/abs/1810.12043v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.12043v2)
- **Published**: 2018-10-29 10:31:06+00:00
- **Updated**: 2019-01-24 11:00:16+00:00
- **Authors**: Ahmed Karam Eldaly, Yoann Altmann, Ahsan Akram, Antonios Perperidis, Kevin Dhaliwal, Stephen McLaughlin
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose an unsupervised approach for bacterial detection in optical endomicroscopy images. This approach splits each image into a set of overlapping patches and assumes that observed intensities are linear combinations of the actual intensity values associated with background image structures, corrupted by additive Gaussian noise and potentially by a sparse outlier term modelling anomalies (which are considered to be candidate bacteria). The actual intensity term representing background structures is modelled as a linear combination of a few atoms drawn from a dictionary which is learned from bacteria-free data and then fixed while analyzing new images. The bacteria detection task is formulated as a minimization problem and an alternating direction method of multipliers (ADMM) is then used to estimate the unknown parameters. Simulations conducted using two ex vivo lung datasets show good detection and correlation performance between bacteria counts identified by a trained clinician and those of the proposed method.



### Visual Re-ranking with Natural Language Understanding for Text Spotting
- **Arxiv ID**: http://arxiv.org/abs/1810.12738v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.12738v1)
- **Published**: 2018-10-29 10:41:45+00:00
- **Updated**: 2018-10-29 10:41:45+00:00
- **Authors**: Ahmed Sabir, Francesc Moreno-Noguer, Lluís Padró
- **Comment**: Accepted by ACCV 2018. arXiv admin note: substantial text overlap
  with arXiv:1810.09776
- **Journal**: None
- **Summary**: Many scene text recognition approaches are based on purely visual information and ignore the semantic relation between scene and text. In this paper, we tackle this problem from natural language processing perspective to fill the gap between language and vision. We propose a post-processing approach to improve scene text recognition accuracy by using occurrence probabilities of words (unigram language model), and the semantic correlation between scene and text. For this, we initially rely on an off-the-shelf deep neural network, already trained with a large amount of data, which provides a series of text hypotheses per input image. These hypotheses are then re-ranked using word frequencies and semantic relatedness with objects or scenes in the image. As a result of this combination, the performance of the original network is boosted with almost no additional cost. We validate our approach on ICDAR'17 dataset.



### PartsNet: A Unified Deep Network for Automotive Engine Precision Parts Defect Detection
- **Arxiv ID**: http://arxiv.org/abs/1810.12061v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.12061v1)
- **Published**: 2018-10-29 11:31:55+00:00
- **Updated**: 2018-10-29 11:31:55+00:00
- **Authors**: Zhenshen Qu, Jianxiong Shen, Ruikun Li, Junyu Liu, Qiuyu Guan
- **Comment**: 2nd International Conference on Computer Science and Artificial
  Intelligence (CSAI 2018)
- **Journal**: None
- **Summary**: Defect detection is a basic and essential task in automatic parts production, especially for automotive engine precision parts. In this paper, we propose a new idea to construct a deep convolutional network combining related knowledge of feature processing and the representation ability of deep learning. Our algorithm consists of a pixel-wise segmentation Deep Neural Network (DNN) and a feature refining network. The fully convolutional DNN is presented to learn basic features of parts defects. After that, several typical traditional methods which are used to refine the segmentation results are transformed into convolutional manners and integrated. We assemble these methods as a shallow network with fixed weights and empirical thresholds. These thresholds are then released to enhance its adaptation ability and realize end-to-end training. Testing results on different datasets show that the proposed method has good portability and outperforms the state-of-the-art algorithms.



### Burst ranking for blind multi-image deblurring
- **Arxiv ID**: http://arxiv.org/abs/1810.12121v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.12121v2)
- **Published**: 2018-10-29 13:38:54+00:00
- **Updated**: 2018-10-30 23:56:41+00:00
- **Authors**: Fidel A. Guerrero Peña, Pedro D. Marrero Fernández, Tsang Ing Ren, Jorge J. G. Leandro, Ricardo Nishihara
- **Comment**: Submitted to IEEE Transactions on Image Processing. 11 pages, 9
  figures
- **Journal**: None
- **Summary**: We propose a new incremental aggregation algorithm for multi-image deblurring with automatic image selection. The primary motivation is that current bursts deblurring methods do not handle well situations in which misalignment or out-of-context frames are present in the burst. These real-life situations result in poor reconstructions or manual selection of the images that will be used to deblur. Automatically selecting best frames within the burst to improve the base reconstruction is challenging because the amount of possible images fusions is equal to the power set cardinal. Here, we approach the multi-image deblurring problem as a two steps process. First, we successfully learn a comparison function to rank a burst of images using a deep convolutional neural network. Then, an incremental Fourier burst accumulation with a reconstruction degradation mechanism is applied fusing only less blurred images that are sufficient to maximize the reconstruction quality. Experiments with the proposed algorithm have shown superior results when compared to other similar approaches, outperforming other methods described in the literature in previously described situations. We validate our findings on several synthetic and real datasets.



### ActionXPose: A Novel 2D Multi-view Pose-based Algorithm for Real-time Human Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1810.12126v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1810.12126v1)
- **Published**: 2018-10-29 13:49:07+00:00
- **Updated**: 2018-10-29 13:49:07+00:00
- **Authors**: Federico Angelini, Zeyu Fu, Yang Long, Ling Shao, Syed Mohsen Naqvi
- **Comment**: None
- **Journal**: None
- **Summary**: We present ActionXPose, a novel 2D pose-based algorithm for posture-level Human Action Recognition (HAR). The proposed approach exploits 2D human poses provided by OpenPose detector from RGB videos. ActionXPose aims to process poses data to be provided to a Long Short-Term Memory Neural Network and to a 1D Convolutional Neural Network, which solve the classification problem. ActionXPose is one of the first algorithms that exploits 2D human poses for HAR. The algorithm has real-time performance and it is robust to camera movings, subject proximity changes, viewpoint changes, subject appearance changes and provide high generalization degree. In fact, extensive simulations show that ActionXPose can be successfully trained using different datasets at once. State-of-the-art performance on popular datasets for posture-related HAR problems (i3DPost, KTH) are provided and results are compared with those obtained by other methods, including the selected ActionXPose baseline. Moreover, we also proposed two novel datasets called MPOSE and ISLD recorded in our Intelligent Sensing Lab, to show ActionXPose generalization performance.



### Unsupervised Data Selection for Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/1810.12142v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.12142v2)
- **Published**: 2018-10-29 14:24:31+00:00
- **Updated**: 2018-12-19 15:44:25+00:00
- **Authors**: Gabriele Valvano, Andrea Leo, Daniele Della Latta, Nicola Martini, Gianmarco Santini, Dante Chiappino, Emiliano Ricciardi
- **Comment**: Technical Report --- 8 pages, 3 figures New tests demonstrated that
  the system, as is, is not able to create reproducible results. Further study
  on the topic should be done
- **Journal**: None
- **Summary**: Recent research put a big effort in the development of deep learning architectures and optimizers obtaining impressive results in areas ranging from vision to language processing. However little attention has been addressed to the need of a methodological process of data collection. In this work we hypothesize that high quality data for supervised learning can be selected in an unsupervised manner and that by doing so one can obtain models capable to generalize better than in the case of random training set construction. However, preliminary results are not robust and further studies on the subject should be carried out.



### Imagination Based Sample Construction for Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1810.12145v1
- **DOI**: 10.1145/3209978.3210096
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.12145v1)
- **Published**: 2018-10-29 14:26:28+00:00
- **Updated**: 2018-10-29 14:26:28+00:00
- **Authors**: Gang Yang, Jinlu Liu, Xirong Li
- **Comment**: Accepted as a short paper in ACM SIGIR 2018
- **Journal**: None
- **Summary**: Zero-shot learning (ZSL) which aims to recognize unseen classes with no labeled training sample, efficiently tackles the problem of missing labeled data in image retrieval. Nowadays there are mainly two types of popular methods for ZSL to recognize images of unseen classes: probabilistic reasoning and feature projection. Different from these existing types of methods, we propose a new method: sample construction to deal with the problem of ZSL. Our proposed method, called Imagination Based Sample Construction (IBSC), innovatively constructs image samples of target classes in feature space by mimicking human associative cognition process. Based on an association between attribute and feature, target samples are constructed from different parts of various samples. Furthermore, dissimilarity representation is employed to select high-quality constructed samples which are used as labeled data to train a specific classifier for those unseen classes. In this way, zero-shot learning is turned into a supervised learning problem. As far as we know, it is the first work to construct samples for ZSL thus, our work is viewed as a baseline for future sample construction methods. Experiments on four benchmark datasets show the superiority of our proposed method.



### Recurrent Transformer Networks for Semantic Correspondence
- **Arxiv ID**: http://arxiv.org/abs/1810.12155v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.12155v1)
- **Published**: 2018-10-29 14:37:29+00:00
- **Updated**: 2018-10-29 14:37:29+00:00
- **Authors**: Seungryong Kim, Stephen Lin, Sangryul Jeon, Dongbo Min, Kwanghoon Sohn
- **Comment**: Neural Information Processing Systems (NIPS) 2018
- **Journal**: None
- **Summary**: We present recurrent transformer networks (RTNs) for obtaining dense correspondences between semantically similar images. Our networks accomplish this through an iterative process of estimating spatial transformations between the input images and using these transformations to generate aligned convolutional activations. By directly estimating the transformations between an image pair, rather than employing spatial transformer networks to independently normalize each individual image, we show that greater accuracy can be achieved. This process is conducted in a recursive manner to refine both the transformation estimates and the feature representations. In addition, a technique is presented for weakly-supervised training of RTNs that is based on a proposed classification loss. With RTNs, state-of-the-art performance is attained on several benchmarks for semantic correspondence.



### Real-Time RGB-D Camera Pose Estimation in Novel Scenes using a Relocalisation Cascade
- **Arxiv ID**: http://arxiv.org/abs/1810.12163v2
- **DOI**: 10.1109/TPAMI.2019.2915068
- **Categories**: **cs.CV**, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/1810.12163v2)
- **Published**: 2018-10-29 14:48:28+00:00
- **Updated**: 2019-07-02 14:16:22+00:00
- **Authors**: Tommaso Cavallari, Stuart Golodetz, Nicholas A. Lord, Julien Valentin, Victor A. Prisacariu, Luigi Di Stefano, Philip H. S. Torr
- **Comment**: Tommaso Cavallari, Stuart Golodetz, Nicholas Lord and Julien Valentin
  assert joint first authorship
- **Journal**: None
- **Summary**: Camera pose estimation is an important problem in computer vision. Common techniques either match the current image against keyframes with known poses, directly regress the pose, or establish correspondences between keypoints in the image and points in the scene to estimate the pose. In recent years, regression forests have become a popular alternative to establish such correspondences. They achieve accurate results, but have traditionally needed to be trained offline on the target scene, preventing relocalisation in new environments. Recently, we showed how to circumvent this limitation by adapting a pre-trained forest to a new scene on the fly. The adapted forests achieved relocalisation performance that was on par with that of offline forests, and our approach was able to estimate the camera pose in close to real time. In this paper, we present an extension of this work that achieves significantly better relocalisation performance whilst running fully in real time. To achieve this, we make several changes to the original approach: (i) instead of accepting the camera pose hypothesis without question, we make it possible to score the final few hypotheses using a geometric approach and select the most promising; (ii) we chain several instantiations of our relocaliser together in a cascade, allowing us to try faster but less accurate relocalisation first, only falling back to slower, more accurate relocalisation as necessary; and (iii) we tune the parameters of our cascade to achieve effective overall performance. These changes allow us to significantly improve upon the performance our original state-of-the-art method was able to achieve on the well-known 7-Scenes and Stanford 4 Scenes benchmarks. As additional contributions, we present a way of visualising the internal behaviour of our forests and show how to entirely circumvent the need to pre-train a forest on a generic scene.



### Causal Inference in Nonverbal Dyadic Communication with Relevant Interval Selection and Granger Causality
- **Arxiv ID**: http://arxiv.org/abs/1810.12171v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.12171v1)
- **Published**: 2018-10-29 14:58:16+00:00
- **Updated**: 2018-10-29 14:58:16+00:00
- **Authors**: Lea Müller, Maha Shadaydeh, Martin Thümmel, Thomas Kessler, Dana Schneider, Joachim Denzler
- **Comment**: Nonverbal emotional communication, Granger causality, maximally
  coherent intervals
- **Journal**: None
- **Summary**: Human nonverbal emotional communication in dyadic dialogs is a process of mutual influence and adaptation. Identifying the direction of influence, or cause-effect relation between participants is a challenging task, due to two main obstacles. First, distinct emotions might not be clearly visible. Second, participants cause-effect relation is transient and variant over time. In this paper, we address these difficulties by using facial expressions that can be present even when strong distinct facial emotions are not visible. We also propose to apply a relevant interval selection approach prior to causal inference to identify those transient intervals where adaptation process occurs. To identify the direction of influence, we apply the concept of Granger causality to the time series of facial expressions on the set of relevant intervals. We tested our approach on synthetic data and then applied it to newly, experimentally obtained data. Here, we were able to show that a more sensitive facial expression detection algorithm and a relevant interval detection approach is most promising to reveal the cause-effect pattern for dyadic communication in various instructed interaction conditions.



### Automatic CNN-based detection of cardiac MR motion artefacts using k-space data augmentation and curriculum learning
- **Arxiv ID**: http://arxiv.org/abs/1810.12185v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.12185v2)
- **Published**: 2018-10-29 15:19:05+00:00
- **Updated**: 2018-12-11 16:20:05+00:00
- **Authors**: Ilkay Oksuz, Bram Ruijsink, Esther Puyol-Anton, James Clough, Gastao Cruz, Aurelien Bustin, Claudia Prieto, Rene Botnar, Daniel Rueckert, Julia A. Schnabel, Andrew P. King
- **Comment**: Submitted to Medical Image Analysis
- **Journal**: None
- **Summary**: Good quality of medical images is a prerequisite for the success of subsequent image analysis pipelines. Quality assessment of medical images is therefore an essential activity and for large population studies such as the UK Biobank (UKBB), manual identification of artefacts such as those caused by unanticipated motion is tedious and time-consuming. Therefore, there is an urgent need for automatic image quality assessment techniques. In this paper, we propose a method to automatically detect the presence of motion-related artefacts in cardiac magnetic resonance (CMR) cine images. We compare two deep learning architectures to classify poor quality CMR images: 1) 3D spatio-temporal Convolutional Neural Networks (3D-CNN), 2) Long-term Recurrent Convolutional Network (LRCN). Though in real clinical setup motion artefacts are common, high-quality imaging of UKBB, which comprises cross-sectional population data of volunteers who do not necessarily have health problems creates a highly imbalanced classification problem. Due to the high number of good quality images compared to the relatively low number of images with motion artefacts, we propose a novel data augmentation scheme based on synthetic artefact creation in k-space. We also investigate a learning approach using a predetermined curriculum based on synthetic artefact severity. We evaluate our pipeline on a subset of the UK Biobank data set consisting of 3510 CMR images. The LRCN architecture outperformed the 3D-CNN architecture and was able to detect 2D+time short axis images with motion artefacts in less than 1ms with high recall. We compare our approach to a range of state-of-the-art quality assessment methods. The novel data augmentation and curriculum learning approaches both improved classification performance achieving overall area under the ROC curve of 0.89.



### DeepSphere: Efficient spherical Convolutional Neural Network with HEALPix sampling for cosmological applications
- **Arxiv ID**: http://arxiv.org/abs/1810.12186v2
- **DOI**: 10.1016/j.ascom.2019.03.004
- **Categories**: **astro-ph.CO**, astro-ph.IM, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1810.12186v2)
- **Published**: 2018-10-29 15:23:18+00:00
- **Updated**: 2019-03-26 17:11:17+00:00
- **Authors**: Nathanaël Perraudin, Michaël Defferrard, Tomasz Kacprzak, Raphael Sgier
- **Comment**: arXiv admin note: text overlap with arXiv:astro-ph/0409513 by other
  authors
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) are a cornerstone of the Deep Learning toolbox and have led to many breakthroughs in Artificial Intelligence. These networks have mostly been developed for regular Euclidean domains such as those supporting images, audio, or video. Because of their success, CNN-based methods are becoming increasingly popular in Cosmology. Cosmological data often comes as spherical maps, which make the use of the traditional CNNs more complicated. The commonly used pixelization scheme for spherical maps is the Hierarchical Equal Area isoLatitude Pixelisation (HEALPix). We present a spherical CNN for analysis of full and partial HEALPix maps, which we call DeepSphere. The spherical CNN is constructed by representing the sphere as a graph. Graphs are versatile data structures that can act as a discrete representation of a continuous manifold. Using the graph-based representation, we define many of the standard CNN operations, such as convolution and pooling. With filters restricted to being radial, our convolutions are equivariant to rotation on the sphere, and DeepSphere can be made invariant or equivariant to rotation. This way, DeepSphere is a special case of a graph CNN, tailored to the HEALPix sampling of the sphere. This approach is computationally more efficient than using spherical harmonics to perform convolutions. We demonstrate the method on a classification problem of weak lensing mass maps from two cosmological models and compare the performance of the CNN with that of two baseline classifiers. The results show that the performance of DeepSphere is always superior or equal to both of these baselines. For high noise levels and for data covering only a smaller fraction of the sphere, DeepSphere achieves typically 10% better classification accuracy than those baselines. Finally, we show how learned filters can be visualized to introspect the neural network.



### Pyramidal Person Re-IDentification via Multi-Loss Dynamic Training
- **Arxiv ID**: http://arxiv.org/abs/1810.12193v3
- **DOI**: None
- **Categories**: **cs.CV**, 14J60
- **Links**: [PDF](http://arxiv.org/pdf/1810.12193v3)
- **Published**: 2018-10-29 15:33:03+00:00
- **Updated**: 2019-05-05 13:35:15+00:00
- **Authors**: Feng Zheng, Cheng Deng, Xing Sun, Xinyang Jiang, Xiaowei Guo, Zongqiao Yu, Feiyue Huang, Rongrong Ji
- **Comment**: Accepted by 2019 Conference on Computer Vision and Pattern
  Recognition
- **Journal**: None
- **Summary**: Most existing Re-IDentification (Re-ID) methods are highly dependent on precise bounding boxes that enable images to be aligned with each other. However, due to the challenging practical scenarios, current detection models often produce inaccurate bounding boxes, which inevitably degenerate the performance of existing Re-ID algorithms. In this paper, we propose a novel coarse-to-fine pyramid model to relax the need of bounding boxes, which not only incorporates local and global information, but also integrates the gradual cues between them. The pyramid model is able to match at different scales and then search for the correct image of the same identity, even when the image pairs are not aligned. In addition, in order to learn discriminative identity representation, we explore a dynamic training scheme to seamlessly unify two losses and extract appropriate shared information between them. Experimental results clearly demonstrate that the proposed method achieves the state-of-the-art results on three datasets. Especially, our approach exceeds the current best method by 9.5% on the most challenging CUHK03 dataset.



### Few-shot 3D Multi-modal Medical Image Segmentation using Generative Adversarial Learning
- **Arxiv ID**: http://arxiv.org/abs/1810.12241v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.12241v1)
- **Published**: 2018-10-29 16:38:57+00:00
- **Updated**: 2018-10-29 16:38:57+00:00
- **Authors**: Arnab Kumar Mondal, Jose Dolz, Christian Desrosiers
- **Comment**: submitted to Medical Image Analysis for review
- **Journal**: None
- **Summary**: We address the problem of segmenting 3D multi-modal medical images in scenarios where very few labeled examples are available for training. Leveraging the recent success of adversarial learning for semi-supervised segmentation, we propose a novel method based on Generative Adversarial Networks (GANs) to train a segmentation model with both labeled and unlabeled images. The proposed method prevents over-fitting by learning to discriminate between true and fake patches obtained by a generator network. Our work extends current adversarial learning approaches, which focus on 2D single-modality images, to the more challenging context of 3D volumes of multiple modalities. The proposed method is evaluated on the problem of segmenting brain MRI from the iSEG-2017 and MRBrainS 2013 datasets. Significant performance improvement is reported, compared to state-of-art segmentation networks trained in a fully-supervised manner. In addition, our work presents a comprehensive analysis of different GAN architectures for semi-supervised segmentation, showing recent techniques like feature matching to yield a higher performance than conventional adversarial training approaches. Our code is publicly available at https://github.com/arnab39/FewShot_GAN-Unet3D



### Compressive Sampling Approach for Image Acquisition with Lensless Endoscope
- **Arxiv ID**: http://arxiv.org/abs/1810.12286v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.12286v2)
- **Published**: 2018-10-29 17:52:32+00:00
- **Updated**: 2018-10-30 13:36:57+00:00
- **Authors**: Stéphanie Guérit, Siddharth Sivankutty, Camille Scotté, John Alto Lee, Hervé Rigneault, Laurent Jacques
- **Comment**: 5 pages, 4 figures, workshop
- **Journal**: None
- **Summary**: The lensless endoscope is a promising device designed to image tissues in vivo at the cellular scale. The traditional acquisition setup consists in raster scanning during which the focused light beam from the optical fiber illuminates sequentially each pixel of the field of view (FOV). The calibration step to focus the beam and the sampling scheme both take time. In this preliminary work, we propose a scanning method based on compressive sampling theory. The method does not rely on a focused beam but rather on the random illumination patterns generated by the single-mode fibers. Experiments are performed on synthetic data for different compression rates (from 10 to 100% of the FOV).



### Gather-Excite: Exploiting Feature Context in Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1810.12348v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.12348v3)
- **Published**: 2018-10-29 18:52:37+00:00
- **Updated**: 2019-01-12 10:34:06+00:00
- **Authors**: Jie Hu, Li Shen, Samuel Albanie, Gang Sun, Andrea Vedaldi
- **Comment**: NeurIPS 2018
- **Journal**: None
- **Summary**: While the use of bottom-up local operators in convolutional neural networks (CNNs) matches well some of the statistics of natural images, it may also prevent such models from capturing contextual long-range feature interactions. In this work, we propose a simple, lightweight approach for better context exploitation in CNNs. We do so by introducing a pair of operators: gather, which efficiently aggregates feature responses from a large spatial extent, and excite, which redistributes the pooled information to local features. The operators are cheap, both in terms of number of added parameters and computational complexity, and can be integrated directly in existing architectures to improve their performance. Experiments on several datasets show that gather-excite can bring benefits comparable to increasing the depth of a CNN at a fraction of the cost. For example, we find ResNet-50 with gather-excite operators is able to outperform its 101-layer counterpart on ImageNet with no additional learnable parameters. We also propose a parametric gather-excite operator pair which yields further performance gains, relate it to the recently-introduced Squeeze-and-Excitation Networks, and analyse the effects of these changes to the CNN feature activation statistics.



### Do Explanations make VQA Models more Predictable to a Human?
- **Arxiv ID**: http://arxiv.org/abs/1810.12366v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1810.12366v1)
- **Published**: 2018-10-29 19:14:26+00:00
- **Updated**: 2018-10-29 19:14:26+00:00
- **Authors**: Arjun Chandrasekaran, Viraj Prabhu, Deshraj Yadav, Prithvijit Chattopadhyay, Devi Parikh
- **Comment**: EMNLP 2018. 16 pages, 11 figures. Content overlaps with "It Takes Two
  to Tango: Towards Theory of AI's Mind" (arXiv:1704.00717)
- **Journal**: None
- **Summary**: A rich line of research attempts to make deep neural networks more transparent by generating human-interpretable 'explanations' of their decision process, especially for interactive tasks like Visual Question Answering (VQA). In this work, we analyze if existing explanations indeed make a VQA model -- its responses as well as failures -- more predictable to a human. Surprisingly, we find that they do not. On the other hand, we find that human-in-the-loop approaches that treat the model as a black-box do.



### Concealing the identity of faces in oblique images with adaptive hopping Gaussian mixtures
- **Arxiv ID**: http://arxiv.org/abs/1810.12435v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.12435v1)
- **Published**: 2018-10-29 22:16:02+00:00
- **Updated**: 2018-10-29 22:16:02+00:00
- **Authors**: Omair Sarwar, Bernhard Rinner, Andrea Cavallaro
- **Comment**: None
- **Journal**: None
- **Summary**: Cameras mounted on Micro Aerial Vehicles (MAVs) are increasingly used for recreational photography. However, aerial photographs of public places often contain faces of bystanders thus leading to a perceived or actual violation of privacy. To address this issue, we propose to pseudo-randomly modify the appearance of face regions in the images using a privacy filter that prevents a human or a face recogniser from inferring the identities of people. The filter, which is applied only when the resolution is high enough for a face to be recognisable, adaptively distorts the face appearance as a function of its resolution. Moreover, the proposed filter locally changes its parameters to discourage attacks that use parameter estimation. The filter exploits both global adaptiveness to reduce distortion and local hopping of the parameters to make their estimation difficult for an attacker. In order to evaluate the efficiency of the proposed approach, we use a state-of-the-art face recognition algorithm and synthetically generated face data with 3D geometric image transformations that mimic faces captured from an MAV at different heights and pitch angles. Experimental results show that the proposed filter protects privacy while reducing distortion and exhibits resilience against attacks.



### TallyQA: Answering Complex Counting Questions
- **Arxiv ID**: http://arxiv.org/abs/1810.12440v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.12440v2)
- **Published**: 2018-10-29 22:29:45+00:00
- **Updated**: 2018-10-31 18:32:07+00:00
- **Authors**: Manoj Acharya, Kushal Kafle, Christopher Kanan
- **Comment**: To appear in AAAI 2019 ( To download the dataset please go to
  http://www.manojacharya.com/ )
- **Journal**: None
- **Summary**: Most counting questions in visual question answering (VQA) datasets are simple and require no more than object detection. Here, we study algorithms for complex counting questions that involve relationships between objects, attribute identification, reasoning, and more. To do this, we created TallyQA, the world's largest dataset for open-ended counting. We propose a new algorithm for counting that uses relation networks with region proposals. Our method lets relation networks be efficiently used with high-resolution imagery. It yields state-of-the-art results compared to baseline and recent systems on both TallyQA and the HowMany-QA benchmark.



### Geometric Median Shapes
- **Arxiv ID**: http://arxiv.org/abs/1810.12445v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.12445v3)
- **Published**: 2018-10-29 22:50:26+00:00
- **Updated**: 2019-04-15 22:52:25+00:00
- **Authors**: Alexandre Cunha
- **Comment**: Accepted ISBI'19
- **Journal**: None
- **Summary**: We present an algorithm to compute the geometric median of shapes which is based on the extension of median to high dimensions. The median finding problem is formulated as an optimization over distances and it is solved directly using the watershed method as an optimizer. We show that computing the geometric median of shapes is robust in the presence of outliers and it is superior to the mean shape which can easily be affected by the presence of outliers. The geometric median shape thus faithfully represents the true central tendency of the data, contaminated or not. Our approach can be applied to manifold and non manifold shapes, with connected or disconnected shapes. The application of distance transforms and watershed algorithm, two well established constructs of image processing, lead to an algorithm that can be quickly implemented to generate fast solutions with linear storage requirements. We demonstrate our methods in synthetic and natural shapes and compare median and mean results under increasing contamination by strong outliers.



### Incremental Learning for Semantic Segmentation of Large-Scale Remote Sensing Data
- **Arxiv ID**: http://arxiv.org/abs/1810.12448v1
- **DOI**: 10.1109/JSTARS.2019.2925416
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.12448v1)
- **Published**: 2018-10-29 22:51:52+00:00
- **Updated**: 2018-10-29 22:51:52+00:00
- **Authors**: Onur Tasar, Yuliya Tarabalka, Pierre Alliez
- **Comment**: None
- **Journal**: IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND
  REMOTE SENSING, 12, 2019, 3524-3537
- **Summary**: In spite of remarkable success of the convolutional neural networks on semantic segmentation, they suffer from catastrophic forgetting: a significant performance drop for the already learned classes when new classes are added on the data, having no annotations for the old classes. We propose an incremental learning methodology, enabling to learn segmenting new classes without hindering dense labeling abilities for the previous classes, although the entire previous data are not accessible. The key points of the proposed approach are adapting the network to learn new as well as old classes on the new training data, and allowing it to remember the previously learned information for the old classes. For adaptation, we keep a frozen copy of the previously trained network, which is used as a memory for the updated network in absence of annotations for the former classes. The updated network minimizes a loss function, which balances the discrepancy between outputs for the previous classes from the memory and updated networks, and the mis-classification rate between outputs for the new classes from the updated network and the new ground-truth. For remembering, we either regularly feed samples from the stored, little fraction of the previous data or use the memory network, depending on whether the new data are collected from completely different geographic areas or from the same city. Our experimental results prove that it is possible to add new classes to the network, while maintaining its performance for the previous classes, despite the whole previous training data are not available.



### Demystifying Neural Network Filter Pruning
- **Arxiv ID**: http://arxiv.org/abs/1811.02639v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.02639v1)
- **Published**: 2018-10-29 23:49:18+00:00
- **Updated**: 2018-10-29 23:49:18+00:00
- **Authors**: Zhuwei Qin, Fuxun Yu, ChenChen Liu, Xiang Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Based on filter magnitude ranking (e.g. L1 norm), conventional filter pruning methods for Convolutional Neural Networks (CNNs) have been proved with great effectiveness in computation load reduction. Although effective, these methods are rarely analyzed in a perspective of filter functionality. In this work, we explore the filter pruning and the retraining through qualitative filter functionality interpretation. We find that the filter magnitude based method fails to eliminate the filters with repetitive functionality. And the retraining phase is actually used to reconstruct the remained filters for functionality compensation for the wrongly-pruned critical filters. With a proposed functionality-oriented pruning method, we further testify that, by precisely addressing the filter functionality redundancy, a CNN can be pruned without considerable accuracy drop, and the retraining phase is unnecessary.



