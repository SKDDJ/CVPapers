# Arxiv Papers in cs.CV on 2018-10-18
### A Novel Focal Tversky loss function with improved Attention U-Net for lesion segmentation
- **Arxiv ID**: http://arxiv.org/abs/1810.07842v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.07842v1)
- **Published**: 2018-10-18 00:07:33+00:00
- **Updated**: 2018-10-18 00:07:33+00:00
- **Authors**: Nabila Abraham, Naimul Mefraz Khan
- **Comment**: submitted to 2019 IEEE International Symposium on Biomedical Imaging
  (ISBI)
- **Journal**: None
- **Summary**: We propose a generalized focal loss function based on the Tversky index to address the issue of data imbalance in medical image segmentation. Compared to the commonly used Dice loss, our loss function achieves a better trade off between precision and recall when training on small structures such as lesions. To evaluate our loss function, we improve the attention U-Net model by incorporating an image pyramid to preserve contextual features. We experiment on the BUS 2017 dataset and ISIC 2018 dataset where lesions occupy 4.84% and 21.4% of the images area and improve segmentation accuracy when compared to the standard U-Net by 25.7% and 3.6%, respectively.



### Automatic Brain Tumor Segmentation using Convolutional Neural Networks with Test-Time Augmentation
- **Arxiv ID**: http://arxiv.org/abs/1810.07884v2
- **DOI**: 10.1007/978-3-030-11726-9_6
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.07884v2)
- **Published**: 2018-10-18 02:55:56+00:00
- **Updated**: 2018-11-28 02:02:01+00:00
- **Authors**: Guotai Wang, Wenqi Li, Sebastien Ourselin, Tom Vercauteren
- **Comment**: 12 pages, 3 figures, MICCAI BrainLes 2018
- **Journal**: None
- **Summary**: Automatic brain tumor segmentation plays an important role for diagnosis, surgical planning and treatment assessment of brain tumors. Deep convolutional neural networks (CNNs) have been widely used for this task. Due to the relatively small data set for training, data augmentation at training time has been commonly used for better performance of CNNs. Recent works also demonstrated the usefulness of using augmentation at test time, in addition to training time, for achieving more robust predictions. We investigate how test-time augmentation can improve CNNs' performance for brain tumor segmentation. We used different underpinning network structures and augmented the image by 3D rotation, flipping, scaling and adding random noise at both training and test time. Experiments with BraTS 2018 training and validation set show that test-time augmentation helps to improve the brain tumor segmentation accuracy and obtain uncertainty estimation of the segmentation results.



### Decoupling Semantic Context and Color Correlation with multi-class cross branch regularization
- **Arxiv ID**: http://arxiv.org/abs/1810.07901v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.07901v2)
- **Published**: 2018-10-18 05:25:13+00:00
- **Updated**: 2018-12-09 10:00:15+00:00
- **Authors**: Vishal Keshav, Tej Pratap GVSL
- **Comment**: In submission
- **Journal**: None
- **Summary**: This paper presents a novel design methodology for architecting a light-weight and faster DNN architecture for vision applications. The effectiveness of the architecture is demonstrated on Color-Constancy use case an inherent block in camera and imaging pipelines. Specifically, we present a multi-branch architecture that disassembles the contextual features and color properties from an image, and later combines them to predict a global property (e.g. Global Illumination). We also propose an implicit regularization technique by designing cross-branch regularization block that enables the network to retain high generalization accuracy. With a conservative use of best computational operators, the proposed architecture achieves state-of-the-art accuracy with 30X lesser model parameters and 70X faster inference time for color constancy. It is also shown that the proposed architecture is generic and achieves similar efficiency in other vision applications such as Low-Light photography.



### Domain Adaptation for Semantic Segmentation via Class-Balanced Self-Training
- **Arxiv ID**: http://arxiv.org/abs/1810.07911v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1810.07911v2)
- **Published**: 2018-10-18 06:20:02+00:00
- **Updated**: 2018-10-25 09:51:52+00:00
- **Authors**: Yang Zou, Zhiding Yu, B. V. K. Vijaya Kumar, Jinsong Wang
- **Comment**: Accepted to ECCV 2018
- **Journal**: None
- **Summary**: Recent deep networks achieved state of the art performance on a variety of semantic segmentation tasks. Despite such progress, these models often face challenges in real world `wild tasks' where large difference between labeled training/source data and unseen test/target data exists. In particular, such difference is often referred to as `domain gap', and could cause significantly decreased performance which cannot be easily remedied by further increasing the representation power. Unsupervised domain adaptation (UDA) seeks to overcome such problem without target domain labels. In this paper, we propose a novel UDA framework based on an iterative self-training procedure, where the problem is formulated as latent variable loss minimization, and can be solved by alternatively generating pseudo labels on target data and re-training the model with these labels. On top of self-training, we also propose a novel class-balanced self-training framework to avoid the gradual dominance of large classes on pseudo-label generation, and introduce spatial priors to refine generated labels. Comprehensive experiments show that the proposed methods achieve state of the art semantic segmentation performance under multiple major UDA settings.



### Unsupervised Domain Adaptation for Learning Eye Gaze from a Million Synthetic Images: An Adversarial Approach
- **Arxiv ID**: http://arxiv.org/abs/1810.07926v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.07926v1)
- **Published**: 2018-10-18 07:15:06+00:00
- **Updated**: 2018-10-18 07:15:06+00:00
- **Authors**: Avisek Lahiri, Abhinav Agarwalla, Prabir Kumar Biswas
- **Comment**: None
- **Journal**: None
- **Summary**: With contemporary advancements of graphics engines, recent trend in deep learning community is to train models on automatically annotated simulated examples and apply on real data during test time. This alleviates the burden of manual annotation. However, there is an inherent difference of distributions between images coming from graphics engine and real world. Such domain difference deteriorates test time performances of models trained on synthetic examples. In this paper we address this issue with unsupervised adversarial feature adaptation across synthetic and real domain for the special use case of eye gaze estimation which is an essential component for various downstream HCI tasks. We initially learn a gaze estimator on annotated synthetic samples rendered from a 3D game engine and then adapt the features of unannotated real samples via a zero-sum minmax adversarial game against a domain discriminator following the recent paradigm of generative adversarial networks. Such adversarial adaptation forces features of both domains to be indistinguishable which enables us to use regression models trained on synthetic domain to be used on real samples. On the challenging MPIIGaze real life dataset, we outperform recent fully supervised methods trained on manually annotated real samples by appreciable margins and also achieve 13\% more relative gain after adaptation compared to the current benchmark method of SimGAN



### Accurate and Scalable Image Clustering Based On Sparse Representation of Camera Fingerprint
- **Arxiv ID**: http://arxiv.org/abs/1810.07945v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.07945v2)
- **Published**: 2018-10-18 08:33:21+00:00
- **Updated**: 2018-11-30 11:56:38+00:00
- **Authors**: Quoc-Tin Phan, Giulia Boato, Francesco G. B. De Natale
- **Comment**: None
- **Journal**: None
- **Summary**: Clustering images according to their acquisition devices is a well-known problem in multimedia forensics, which is typically faced by means of camera Sensor Pattern Noise (SPN). Such an issue is challenging since SPN is a noise-like signal, hard to be estimated and easy to be attenuated or destroyed by many factors. Moreover, the high dimensionality of SPN hinders large-scale applications. Existing approaches are typically based on the correlation among SPNs in the pixel domain, which might not be able to capture intrinsic data structure in union of vector subspaces. In this paper, we propose an accurate clustering framework, which exploits linear dependencies among SPNs in their intrinsic vector subspaces. Such dependencies are encoded under sparse representations which are obtained by solving a LASSO problem with non-negativity constraint. The proposed framework is highly accurate in number of clusters estimation and image association. Moreover, our framework is scalable to the number of images and robust against double JPEG compression as well as the presence of outliers, owning big potential for real-world applications. Experimental results on Dresden and Vision database show that our proposed framework can adapt well to both medium-scale and large-scale contexts, and outperforms state-of-the-art methods.



### S-Net: A Scalable Convolutional Neural Network for JPEG Compression Artifact Reduction
- **Arxiv ID**: http://arxiv.org/abs/1810.07960v1
- **DOI**: 10.1117/1.JEI.27.4.043037
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.07960v1)
- **Published**: 2018-10-18 09:21:44+00:00
- **Updated**: 2018-10-18 09:21:44+00:00
- **Authors**: Bolun Zheng, Rui Sun, Xiang Tian, Yaowu Chen
- **Comment**: accepted by Journal of Electronic Imaging
- **Journal**: None
- **Summary**: Recent studies have used deep residual convolutional neural networks (CNNs) for JPEG compression artifact reduction. This study proposes a scalable CNN called S-Net. Our approach effectively adjusts the network scale dynamically in a multitask system for real-time operation with little performance loss. It offers a simple and direct technique to evaluate the performance gains obtained with increasing network depth, and it is helpful for removing redundant network layers to maximize the network efficiency. We implement our architecture using the Keras framework with the TensorFlow backend on an NVIDIA K80 GPU server. We train our models on the DIV2K dataset and evaluate their performance on public benchmark datasets. To validate the generality and universality of the proposed method, we created and utilized a new dataset, called WIN143, for over-processed images evaluation. Experimental results indicate that our proposed approach outperforms other CNN-based methods and achieves state-of-the-art performance.



### LeukoNet: DCT-based CNN architecture for the classification of normal versus Leukemic blasts in B-ALL Cancer
- **Arxiv ID**: http://arxiv.org/abs/1810.07961v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1810.07961v2)
- **Published**: 2018-10-18 09:24:14+00:00
- **Updated**: 2018-11-04 08:05:41+00:00
- **Authors**: Simmi Mourya, Sonaal Kant, Pulkit Kumar, Anubha Gupta, Ritu Gupta
- **Comment**: None
- **Journal**: None
- **Summary**: Acute lymphoblastic leukemia (ALL) constitutes approximately 25% of the pediatric cancers. In general, the task of identifying immature leukemic blasts from normal cells under the microscope is challenging because morphologically the images of the two cells appear similar. In this paper, we propose a deep learning framework for classifying immature leukemic blasts and normal cells. The proposed model combines the Discrete Cosine Transform (DCT) domain features extracted via CNN with the Optical Density (OD) space features to build a robust classifier. Elaborate experiments have been conducted to validate the proposed LeukoNet classifier.



### Optical Font Recognition in Smartphone-Captured Images, and its Applicability for ID Forgery Detection
- **Arxiv ID**: http://arxiv.org/abs/1810.08016v1
- **DOI**: 10.1117/12.2522955
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.08016v1)
- **Published**: 2018-10-18 12:44:05+00:00
- **Updated**: 2018-10-18 12:44:05+00:00
- **Authors**: Yulia S. Chernyshova, Mikhail A. Aliev, Ekaterina S. Gushchanskaia, Alexander V. Sheshkus
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we consider the problem of detecting counterfeit identity documents in images captured with smartphones. As the number of documents contain special fonts, we study the applicability of convolutional neural networks (CNNs) for detection of the conformance of the fonts used with the ones, corresponding to the government standards. Here, we use multi-task learning to differentiate samples by both fonts and characters and compare the resulting classifier with its analogue trained for binary font classification. We train neural networks for authenticity estimation of the fonts used in machine-readable zones and ID numbers of the Russian national passport and test them on samples of individual characters acquired from 3238 images of the Russian national passport. Our results show that the usage of multi-task learning increases sensitivity and specificity of the classifier. Moreover, the resulting CNNs demonstrate high generalization ability as they correctly classify fonts which were not present in the training set. We conclude that the proposed method is sufficient for authentication of the fonts and can be used as a part of the forgery detection system for images acquired with a smartphone camera.



### Implicit Dual-domain Convolutional Network for Robust Color Image Compression Artifact Reduction
- **Arxiv ID**: http://arxiv.org/abs/1810.08042v3
- **DOI**: 10.1109/TCSVT.2019.2931045
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.08042v3)
- **Published**: 2018-10-18 13:28:06+00:00
- **Updated**: 2019-07-24 02:29:05+00:00
- **Authors**: Bolun Zheng, Yaowu Chen, Xiang Tian, Fan Zhou, Xuesong Liu
- **Comment**: accepted by IEEE Transactions on Circuits and Systems for Video
  Technology(T-CSVT)
- **Journal**: None
- **Summary**: Several dual-domain convolutional neural network-based methods show outstanding performance in reducing image compression artifacts. However, they suffer from handling color images because the compression processes for gray-scale and color images are completely different. Moreover, these methods train a specific model for each compression quality and require multiple models to achieve different compression qualities. To address these problems, we proposed an implicit dual-domain convolutional network (IDCN) with the pixel position labeling map and the quantization tables as inputs. Specifically, we proposed an extractor-corrector framework-based dual-domain correction unit (DCU) as the basic component to formulate the IDCN. A dense block was introduced to improve the performance of extractor in DRU. The implicit dual-domain translation allows the IDCN to handle color images with the discrete cosine transform (DCT)-domain priors. A flexible version of IDCN (IDCN-f) was developed to handle a wide range of compression qualities. Experiments for both objective and subjective evaluations on benchmark datasets show that IDCN is superior to the state-of-the-art methods and IDCN-f exhibits excellent abilities to handle a wide range of compression qualities with little performance sacrifice and demonstrates great potential for practical applications.



### Visions of a generalized probability theory
- **Arxiv ID**: http://arxiv.org/abs/1810.10341v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, math.PR, 60A99, 60D99, 62A99
- **Links**: [PDF](http://arxiv.org/pdf/1810.10341v1)
- **Published**: 2018-10-18 14:00:48+00:00
- **Updated**: 2018-10-18 14:00:48+00:00
- **Authors**: Fabio Cuzzolin
- **Comment**: None
- **Journal**: Lambert Academic Publishing, Sep 24 2014
- **Summary**: In this Book we argue that the fruitful interaction of computer vision and belief calculus is capable of stimulating significant advances in both fields. From a methodological point of view, novel theoretical results concerning the geometric and algebraic properties of belief functions as mathematical objects are illustrated and discussed in Part II, with a focus on both a perspective 'geometric approach' to uncertainty and an algebraic solution to the issue of conflicting evidence. In Part III we show how these theoretical developments arise from important computer vision problems (such as articulated object tracking, data association and object pose estimation) to which, in turn, the evidential formalism is able to provide interesting new solutions. Finally, some initial steps towards a generalization of the notion of total probability to belief functions are taken, in the perspective of endowing the theory of evidence with a complete battery of estimation and inference tools to the benefit of all scientists and practitioners.



### Stochastic Distance Transform
- **Arxiv ID**: http://arxiv.org/abs/1810.08097v1
- **DOI**: 10.1007/978-3-030-14085-4_7
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.08097v1)
- **Published**: 2018-10-18 15:07:58+00:00
- **Updated**: 2018-10-18 15:07:58+00:00
- **Authors**: Johan Öfverstedt, Joakim Lindblad, Nataša Sladoje
- **Comment**: 12 pages, 4 figures, 3 tables
- **Journal**: In Proceedings of the 21th international conference on Discrete
  Geometry for Computer Imagery (DGCI), LNCS-11414, pp. 75-86, Paris, France,
  March 2019
- **Summary**: The distance transform (DT) and its many variations are ubiquitous tools for image processing and analysis. In many imaging scenarios, the images of interest are corrupted by noise. This has a strong negative impact on the accuracy of the DT, which is highly sensitive to spurious noise points. In this study, we consider images represented as discrete random sets and observe statistics of DT computed on such representations. We, thus, define a stochastic distance transform (SDT), which has an adjustable robustness to noise. Both a stochastic Monte Carlo method and a deterministic method for computing the SDT are proposed and compared. Through a series of empirical tests, we demonstrate that the SDT is effective not only in improving the accuracy of the computed distances in the presence of noise, but also in improving the performance of template matching and watershed segmentation of partially overlapping objects, which are examples of typical applications where DTs are utilized.



### DeepLens: Shallow Depth Of Field From A Single Image
- **Arxiv ID**: http://arxiv.org/abs/1810.08100v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.08100v1)
- **Published**: 2018-10-18 15:14:41+00:00
- **Updated**: 2018-10-18 15:14:41+00:00
- **Authors**: Lijun Wang, Xiaohui Shen, Jianming Zhang, Oliver Wang, Zhe Lin, Chih-Yao Hsieh, Sarah Kong, Huchuan Lu
- **Comment**: 11 pages, 15 figures, accepted by SIGGRAPH Asia 2018, low-resolution
  version
- **Journal**: None
- **Summary**: We aim to generate high resolution shallow depth-of-field (DoF) images from a single all-in-focus image with controllable focal distance and aperture size. To achieve this, we propose a novel neural network model comprised of a depth prediction module, a lens blur module, and a guided upsampling module. All modules are differentiable and are learned from data. To train our depth prediction module, we collect a dataset of 2462 RGB-D images captured by mobile phones with a dual-lens camera, and use existing segmentation datasets to improve border prediction. We further leverage a synthetic dataset with known depth to supervise the lens blur and guided upsampling modules. The effectiveness of our system and training strategies are verified in the experiments. Our method can generate high-quality shallow DoF images at high resolution, and produces significantly fewer artifacts than the baselines and existing solutions for single image shallow DoF synthesis. Compared with the iPhone portrait mode, which is a state-of-the-art shallow DoF solution based on a dual-lens depth camera, our method generates comparable results, while allowing for greater flexibility to choose focal points and aperture size, and is not limited to one capture setup.



### Salience Biased Loss for Object Detection in Aerial Images
- **Arxiv ID**: http://arxiv.org/abs/1810.08103v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.08103v1)
- **Published**: 2018-10-18 15:18:58+00:00
- **Updated**: 2018-10-18 15:18:58+00:00
- **Authors**: Peng Sun, Guang Chen, Guerdan Luke, Yi Shang
- **Comment**: None
- **Journal**: None
- **Summary**: Object detection in remote sensing, especially in aerial images, remains a challenging problem due to low image resolution, complex backgrounds, and variation of scale and angles of objects in images. In current implementations, multi-scale based and angle-based networks have been proposed and generate promising results with aerial image detection. In this paper, we propose a novel loss function, called Salience Biased Loss (SBL), for deep neural networks, which uses salience information of the input image to achieve improved performance for object detection. Our novel loss function treats training examples differently based on input complexity in order to avoid the over-contribution of easy cases in the training process. In our experiments, RetinaNet was trained with SBL to generate an one-stage detector, SBL-RetinaNet. SBL-RetinaNet is applied to the largest existing public aerial image dataset, DOTA. Experimental results show our proposed loss function with the RetinaNet architecture outperformed other state-of-art object detection models by at least 4.31 mAP, and RetinaNet by 2.26 mAP with the same inference speed of RetinaNet.



### KTAN: Knowledge Transfer Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/1810.08126v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.08126v1)
- **Published**: 2018-10-18 15:57:02+00:00
- **Updated**: 2018-10-18 15:57:02+00:00
- **Authors**: Peiye Liu, Wu Liu, Huadong Ma, Tao Mei, Mingoo Seok
- **Comment**: 8 pages, 2 figures
- **Journal**: None
- **Summary**: To reduce the large computation and storage cost of a deep convolutional neural network, the knowledge distillation based methods have pioneered to transfer the generalization ability of a large (teacher) deep network to a light-weight (student) network. However, these methods mostly focus on transferring the probability distribution of the softmax layer in a teacher network and thus neglect the intermediate representations. In this paper, we propose a knowledge transfer adversarial network to better train a student network. Our technique holistically considers both intermediate representations and probability distributions of a teacher network. To transfer the knowledge of intermediate representations, we set high-level teacher feature maps as a target, toward which the student feature maps are trained. Specifically, we arrange a Teacher-to-Student layer for enabling our framework suitable for various student structures. The intermediate representation helps the student network better understand the transferred generalization as compared to the probability distribution only. Furthermore, we infuse an adversarial learning process by employing a discriminator network, which can fully exploit the spatial correlation of feature maps in training a student network. The experimental results demonstrate that the proposed method can significantly improve the performance of a student network on both image classification and object detection tasks.



### Exploiting High-Level Semantics for No-Reference Image Quality Assessment of Realistic Blur Images
- **Arxiv ID**: http://arxiv.org/abs/1810.08169v1
- **DOI**: 10.1145/3123266.3123322
- **Categories**: **eess.IV**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1810.08169v1)
- **Published**: 2018-10-18 17:15:26+00:00
- **Updated**: 2018-10-18 17:15:26+00:00
- **Authors**: Dingquan Li, Tingting Jiang, Ming Jiang
- **Comment**: correct typos, e.g., "avarage" -> "average" in the figure of the
  proposed framework
- **Journal**: Proceedings of the 2017 ACM on Multimedia Conference
- **Summary**: To guarantee a satisfying Quality of Experience (QoE) for consumers, it is required to measure image quality efficiently and reliably. The neglect of the high-level semantic information may result in predicting a clear blue sky as bad quality, which is inconsistent with human perception. Therefore, in this paper, we tackle this problem by exploiting the high-level semantics and propose a novel no-reference image quality assessment method for realistic blur images. Firstly, the whole image is divided into multiple overlapping patches. Secondly, each patch is represented by the high-level feature extracted from the pre-trained deep convolutional neural network model. Thirdly, three different kinds of statistical structures are adopted to aggregate the information from different patches, which mainly contain some common statistics (i.e., the mean\&standard deviation, quantiles and moments). Finally, the aggregated features are fed into a linear regression model to predict the image quality. Experiments show that, compared with low-level features, high-level features indeed play a more critical role in resolving the aforementioned challenging problem for quality estimation. Besides, the proposed method significantly outperforms the state-of-the-art methods on two realistic blur image databases and achieves comparable performance on two synthetic blur image databases.



### Convolutional Collaborative Filter Network for Video Based Recommendation Systems
- **Arxiv ID**: http://arxiv.org/abs/1810.08189v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1810.08189v2)
- **Published**: 2018-10-18 17:57:58+00:00
- **Updated**: 2018-10-22 20:43:16+00:00
- **Authors**: Cheng-Kang Hsieh, Miguel Campo, Abhinav Taliyan, Matt Nickens, Mitkumar Pandya, JJ Espinoza
- **Comment**: 8 pages, 3 figures, 1 table include ablation study. arguments /
  results unchanged
- **Journal**: None
- **Summary**: This analysis explores the temporal sequencing of objects in a movie trailer. Temporal sequencing of objects in a movie trailer (e.g., a long shot of an object vs intermittent short shots) can convey information about the type of movie, plot of the movie, role of the main characters, and the filmmakers cinematographic choices. When combined with historical customer data, sequencing analysis can be used to improve predictions of customer behavior. E.g., a customer buys tickets to a new movie and maybe the customer has seen movies in the past that contained similar sequences. To explore object sequencing in movie trailers, we propose a video convolutional network to capture actions and scenes that are predictive of customers' preferences. The model learns the specific nature of sequences for different types of objects (e.g., cars vs faces), and the role of sequences in predicting customer future behavior. We show how such a temporal-aware model outperforms simple feature pooling methods proposed in our previous works and, importantly, demonstrate the additional model explain-ability allowed by such a model.



### MRI Reconstruction via Cascaded Channel-wise Attention Network
- **Arxiv ID**: http://arxiv.org/abs/1810.08229v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1810.08229v2)
- **Published**: 2018-10-18 18:37:37+00:00
- **Updated**: 2019-04-09 21:11:26+00:00
- **Authors**: Qiaoying Huang, Dong Yang, Pengxiang Wu, Hui Qu, Jingru Yi, Dimitris Metaxas
- **Comment**: Accepted by the IEEE International Symposium on Biomedical Imaging
  (ISBI) 2019. Code is available now
- **Journal**: None
- **Summary**: We consider an MRI reconstruction problem with input of k-space data at a very low undersampled rate. This can practically benefit patient due to reduced time of MRI scan, but it is also challenging since quality of reconstruction may be compromised. Currently, deep learning based methods dominate MRI reconstruction over traditional approaches such as Compressed Sensing, but they rarely show satisfactory performance in the case of low undersampled k-space data. One explanation is that these methods treat channel-wise features equally, which results in degraded representation ability of the neural network. To solve this problem, we propose a new model called MRI Cascaded Channel-wise Attention Network (MICCAN), highlighted by three components: (i) a variant of U-net with Channel-wise Attention (UCA) module, (ii) a long skip connection and (iii) a combined loss. Our model is able to attend to salient information by filtering irrelevant features and also concentrate on high-frequency information by enforcing low-frequency information bypassed to the final output. We conduct both quantitative evaluation and qualitative analysis of our method on a cardiac dataset. The experiment shows that our method achieves very promising results in terms of three common metrics on the MRI reconstruction with low undersampled k-space data.



### Deep Learning vs. Human Graders for Classifying Severity Levels of Diabetic Retinopathy in a Real-World Nationwide Screening Program
- **Arxiv ID**: http://arxiv.org/abs/1810.08290v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.08290v1)
- **Published**: 2018-10-18 22:17:45+00:00
- **Updated**: 2018-10-18 22:17:45+00:00
- **Authors**: Paisan Raumviboonsuk, Jonathan Krause, Peranut Chotcomwongse, Rory Sayres, Rajiv Raman, Kasumi Widner, Bilson J L Campana, Sonia Phene, Kornwipa Hemarat, Mongkol Tadarati, Sukhum Silpa-Acha, Jirawut Limwattanayingyong, Chetan Rao, Oscar Kuruvilla, Jesse Jung, Jeffrey Tan, Surapong Orprayoon, Chawawat Kangwanwongpaisan, Ramase Sukulmalpaiboon, Chainarong Luengchaichawang, Jitumporn Fuangkaew, Pipat Kongsap, Lamyong Chualinpha, Sarawuth Saree, Srirat Kawinpanitan, Korntip Mitvongsa, Siriporn Lawanasakol, Chaiyasit Thepchatri, Lalita Wongpichedchai, Greg S Corrado, Lily Peng, Dale R Webster
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning algorithms have been used to detect diabetic retinopathy (DR) with specialist-level accuracy. This study aims to validate one such algorithm on a large-scale clinical population, and compare the algorithm performance with that of human graders. 25,326 gradable retinal images of patients with diabetes from the community-based, nation-wide screening program of DR in Thailand were analyzed for DR severity and referable diabetic macular edema (DME). Grades adjudicated by a panel of international retinal specialists served as the reference standard. Across different severity levels of DR for determining referable disease, deep learning significantly reduced the false negative rate (by 23%) at the cost of slightly higher false positive rates (2%). Deep learning algorithms may serve as a valuable tool for DR screening.



### CURE-OR: Challenging Unreal and Real Environments for Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/1810.08293v2
- **DOI**: None
- **Categories**: **cs.CV**, I.2; I.4; I.5
- **Links**: [PDF](http://arxiv.org/pdf/1810.08293v2)
- **Published**: 2018-10-18 22:23:50+00:00
- **Updated**: 2018-11-13 15:24:17+00:00
- **Authors**: Dogancan Temel, Jinsol Lee, Ghassan AlRegib
- **Comment**: 8 pages, 7 figures, 4 tables
- **Journal**: D. Temel, J. Lee, and G. AlRegib, "CURE-OR: Challenging unreal and
  real environments for object recognition," 2018 17th IEEE International
  Conference on Machine Learning and Applications (ICMLA), Orlando, Florida,
  USA, 2018
- **Summary**: In this paper, we introduce a large-scale, controlled, and multi-platform object recognition dataset denoted as Challenging Unreal and Real Environments for Object Recognition (CURE-OR). In this dataset, there are 1,000,000 images of 100 objects with varying size, color, and texture that are positioned in five different orientations and captured using five devices including a webcam, a DSLR, and three smartphone cameras in real-world (real) and studio (unreal) environments. The controlled challenging conditions include underexposure, overexposure, blur, contrast, dirty lens, image noise, resizing, and loss of color information. We utilize CURE-OR dataset to test recognition APIs-Amazon Rekognition and Microsoft Azure Computer Vision- and show that their performance significantly degrades under challenging conditions. Moreover, we investigate the relationship between object recognition and image quality and show that objective quality algorithms can estimate recognition performance under certain photometric challenging conditions. The dataset is publicly available at https://ghassanalregib.com/cure-or/.



### Predicting optical coherence tomography-derived diabetic macular edema grades from fundus photographs using deep learning
- **Arxiv ID**: http://arxiv.org/abs/1810.10342v4
- **DOI**: 10.1038/s41467-019-13922-8
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.10342v4)
- **Published**: 2018-10-18 23:22:33+00:00
- **Updated**: 2019-07-31 23:39:59+00:00
- **Authors**: Avinash Varadarajan, Pinal Bavishi, Paisan Raumviboonsuk, Peranut Chotcomwongse, Subhashini Venugopalan, Arunachalam Narayanaswamy, Jorge Cuadros, Kuniyoshi Kanai, George Bresnick, Mongkol Tadarati, Sukhum Silpa-archa, Jirawut Limwattanayingyong, Variya Nganthavee, Joe Ledsam, Pearse A Keane, Greg S Corrado, Lily Peng, Dale R Webster
- **Comment**: None
- **Journal**: Nature Communications (2020)
- **Summary**: Diabetic eye disease is one of the fastest growing causes of preventable blindness. With the advent of anti-VEGF (vascular endothelial growth factor) therapies, it has become increasingly important to detect center-involved diabetic macular edema (ci-DME). However, center-involved diabetic macular edema is diagnosed using optical coherence tomography (OCT), which is not generally available at screening sites because of cost and workflow constraints. Instead, screening programs rely on the detection of hard exudates in color fundus photographs as a proxy for DME, often resulting in high false positive or false negative calls. To improve the accuracy of DME screening, we trained a deep learning model to use color fundus photographs to predict ci-DME. Our model had an ROC-AUC of 0.89 (95% CI: 0.87-0.91), which corresponds to a sensitivity of 85% at a specificity of 80%. In comparison, three retinal specialists had similar sensitivities (82-85%), but only half the specificity (45-50%, p<0.001 for each comparison with model). The positive predictive value (PPV) of the model was 61% (95% CI: 56-66%), approximately double the 36-38% by the retinal specialists. In addition to predicting ci-DME, our model was able to detect the presence of intraretinal fluid with an AUC of 0.81 (95% CI: 0.81-0.86) and subretinal fluid with an AUC of 0.88 (95% CI: 0.85-0.91). The ability of deep learning algorithms to make clinically relevant predictions that generally require sophisticated 3D-imaging equipment from simple 2D images has broad relevance to many other applications in medical imaging.



### An Efficient Data Retrieval Parallel Reeb Graph Algorithm
- **Arxiv ID**: http://arxiv.org/abs/1810.08310v4
- **DOI**: None
- **Categories**: **cs.CG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1810.08310v4)
- **Published**: 2018-10-18 23:49:26+00:00
- **Updated**: 2020-10-12 04:30:47+00:00
- **Authors**: Mustafa Hajij, Paul Rosen
- **Comment**: 30 pages, 25 figures
- **Journal**: None
- **Summary**: The Reeb graph of a scalar function defined on a domain gives a topologically meaningful summary of that domain. Reeb graphs have been shown in the past decade to be of great importance in geometric processing, image processing, computer graphics, and computational topology. The demand for analyzing large data sets has increased in the last decade. Hence the parallelization of topological computations needs to be more fully considered. We propose a parallel augmented Reeb graph algorithm on triangulated meshes with and without a boundary. That is, in addition to our parallel algorithm for computing a Reeb graph, we describe a method for extracting the original manifold data from the Reeb graph structure. We demonstrate the running time of our algorithm on standard datasets. As an application, we show how our algorithm can be utilized in mesh segmentation algorithms.



