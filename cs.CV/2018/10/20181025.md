# Arxiv Papers in cs.CV on 2018-10-25
### Understand, Compose and Respond - Answering Visual Questions by a Composition of Abstract Procedures
- **Arxiv ID**: http://arxiv.org/abs/1810.10656v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.10656v1)
- **Published**: 2018-10-25 00:03:09+00:00
- **Updated**: 2018-10-25 00:03:09+00:00
- **Authors**: Ben Zion Vatashsky, Shimon Ullman
- **Comment**: None
- **Journal**: None
- **Summary**: An image related question defines a specific visual task that is required in order to produce an appropriate answer. The answer may depend on a minor detail in the image and require complex reasoning and use of prior knowledge. When humans perform this task, they are able to do it in a flexible and robust manner, integrating modularly any novel visual capability with diverse options for various elaborations of the task. In contrast, current approaches to solve this problem by a machine are based on casting the problem as an end-to-end learning problem, which lacks such abilities.   We present a different approach, inspired by the aforementioned human capabilities. The approach is based on the compositional structure of the question. The underlying idea is that a question has an abstract representation based on its structure, which is compositional in nature. The question can consequently be answered by a composition of procedures corresponding to its substructures. The basic elements of the representation are logical patterns, which are put together to represent the question. These patterns include a parametric representation for object classes, properties and relations. Each basic pattern is mapped into a basic procedure that includes meaningful visual tasks, and the patterns are composed to produce the overall answering procedure.   The UnCoRd (Understand Compose and Respond) system, based on this approach, integrates existing detection and classification schemes for a set of object classes, properties and relations. These schemes are incorporated in a modular manner, providing elaborated answers and corrections for negative answers. In addition, an external knowledge base is queried for required common-knowledge. We performed a qualitative analysis of the system, which demonstrates its representation capabilities and provide suggestions for future developments.



### Sports Camera Calibration via Synthetic Data
- **Arxiv ID**: http://arxiv.org/abs/1810.10658v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.10658v1)
- **Published**: 2018-10-25 00:10:57+00:00
- **Updated**: 2018-10-25 00:10:57+00:00
- **Authors**: Jianhui Chen, James J. Little
- **Comment**: 6 + 1 pages
- **Journal**: None
- **Summary**: Calibrating sports cameras is important for autonomous broadcasting and sports analysis. Here we propose a highly automatic method for calibrating sports cameras from a single image using synthetic data. First, we develop a novel camera pose engine. The camera pose engine has only three significant free parameters so that it can effectively generate a lot of camera poses and corresponding edge (i.e, field marking) images. Then, we learn compact deep features via a siamese network from paired edge image and camera pose and build a feature-pose database. After that, we use a novel two-GAN (generative adversarial network) model to detect field markings in real images. Finally, we query an initial camera pose from the feature-pose database and refine camera poses using truncated distance images. We evaluate our method on both synthetic and real data. Our method not only demonstrates the robustness on the synthetic data but also achieves the state-of-the-art accuracy on a standard soccer dataset and very high performance on a volleyball dataset.



### Engaging Image Captioning Via Personality
- **Arxiv ID**: http://arxiv.org/abs/1810.10665v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1810.10665v2)
- **Published**: 2018-10-25 00:46:16+00:00
- **Updated**: 2019-03-20 16:53:33+00:00
- **Authors**: Kurt Shuster, Samuel Humeau, Hexiang Hu, Antoine Bordes, Jason Weston
- **Comment**: None
- **Journal**: None
- **Summary**: Standard image captioning tasks such as COCO and Flickr30k are factual, neutral in tone and (to a human) state the obvious (e.g., "a man playing a guitar"). While such tasks are useful to verify that a machine understands the content of an image, they are not engaging to humans as captions. With this in mind we define a new task, Personality-Captions, where the goal is to be as engaging to humans as possible by incorporating controllable style and personality traits. We collect and release a large dataset of 201,858 of such captions conditioned over 215 possible traits. We build models that combine existing work from (i) sentence representations (Mazare et al., 2018) with Transformers trained on 1.7 billion dialogue examples; and (ii) image representations (Mahajan et al., 2018) with ResNets trained on 3.5 billion social media images. We obtain state-of-the-art performance on Flickr30k and COCO, and strong performance on our new task. Finally, online evaluations validate that our task and models are engaging to humans, with our best model close to human performance.



### Structure Learning of Deep Networks via DNA Computing Algorithm
- **Arxiv ID**: http://arxiv.org/abs/1810.10687v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1810.10687v1)
- **Published**: 2018-10-25 02:06:24+00:00
- **Updated**: 2018-10-25 02:06:24+00:00
- **Authors**: Guoqiang Zhong, Tao Li, Wenxue Liu, Yang Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Network (CNN) has gained state-of-the-art results in many pattern recognition and computer vision tasks. However, most of the CNN structures are manually designed by experienced researchers. Therefore, auto- matically building high performance networks becomes an important problem. In this paper, we introduce the idea of using DNA computing algorithm to automatically learn high-performance architectures. In DNA computing algorithm, we use short DNA strands to represent layers and long DNA strands to represent overall networks. We found that most of the learned models perform similarly, and only those performing worse during the first runs of training will perform worse finally than others. The indicates that: 1) Using DNA computing algorithm to learn deep architectures is feasible; 2) Local minima should not be a problem of deep networks; 3) We can use early stop to kill the models with the bad performance just after several runs of training. In our experiments, an accuracy 99.73% was obtained on the MNIST data set and an accuracy 95.10% was obtained on the CIFAR-10 data set.



### K for the Price of 1: Parameter-efficient Multi-task and Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/1810.10703v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.10703v2)
- **Published**: 2018-10-25 03:12:37+00:00
- **Updated**: 2019-02-24 02:03:00+00:00
- **Authors**: Pramod Kaushik Mudrakarta, Mark Sandler, Andrey Zhmoginov, Andrew Howard
- **Comment**: published at ICLR 2019
- **Journal**: None
- **Summary**: We introduce a novel method that enables parameter-efficient transfer and multi-task learning with deep neural networks. The basic approach is to learn a model patch - a small set of parameters - that will specialize to each task, instead of fine-tuning the last layer or the entire network. For instance, we show that learning a set of scales and biases is sufficient to convert a pretrained network to perform well on qualitatively different problems (e.g. converting a Single Shot MultiBox Detection (SSD) model into a 1000-class image classification model while reusing 98% of parameters of the SSD feature extractor). Similarly, we show that re-learning existing low-parameter layers (such as depth-wise convolutions) while keeping the rest of the network frozen also improves transfer-learning accuracy significantly. Our approach allows both simultaneous (multi-task) as well as sequential transfer learning. In several multi-task learning problems, despite using much fewer parameters than traditional logits-only fine-tuning, we match single-task performance.



### Convolutional Deblurring for Natural Imaging
- **Arxiv ID**: http://arxiv.org/abs/1810.10725v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1810.10725v2)
- **Published**: 2018-10-25 05:33:26+00:00
- **Updated**: 2019-07-19 16:31:53+00:00
- **Authors**: Mahdi S. Hosseini, Konstantinos N. Plataniotis
- **Comment**: 15 pages, for publication in IEEE Transaction Image Processing
- **Journal**: None
- **Summary**: In this paper, we propose a novel design of image deblurring in the form of one-shot convolution filtering that can directly convolve with naturally blurred images for restoration. The problem of optical blurring is a common disadvantage to many imaging applications that suffer from optical imperfections. Despite numerous deconvolution methods that blindly estimate blurring in either inclusive or exclusive forms, they are practically challenging due to high computational cost and low image reconstruction quality. Both conditions of high accuracy and high speed are prerequisites for high-throughput imaging platforms in digital archiving. In such platforms, deblurring is required after image acquisition before being stored, previewed, or processed for high-level interpretation. Therefore, on-the-fly correction of such images is important to avoid possible time delays, mitigate computational expenses, and increase image perception quality. We bridge this gap by synthesizing a deconvolution kernel as a linear combination of Finite Impulse Response (FIR) even-derivative filters that can be directly convolved with blurry input images to boost the frequency fall-off of the Point Spread Function (PSF) associated with the optical blur. We employ a Gaussian low-pass filter to decouple the image denoising problem for image edge deblurring. Furthermore, we propose a blind approach to estimate the PSF statistics for two Gaussian and Laplacian models that are common in many imaging pipelines. Thorough experiments are designed to test and validate the efficiency of the proposed method using 2054 naturally blurred images across six imaging applications and seven state-of-the-art deconvolution methods.



### Supervised Classification Methods for Flash X-ray single particle diffraction Imaging
- **Arxiv ID**: http://arxiv.org/abs/1810.10786v1
- **DOI**: 10.1364/OE.27.003884
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.10786v1)
- **Published**: 2018-10-25 09:02:03+00:00
- **Updated**: 2018-10-25 09:02:03+00:00
- **Authors**: Jing Liu, Gijs van der Schot, Stefan Engblom
- **Comment**: None
- **Journal**: None
- **Summary**: Current Flash X-ray single-particle diffraction Imaging (FXI) experiments, which operate on modern X-ray Free Electron Lasers (XFELs), can record millions of interpretable diffraction patterns from individual biomolecules per day. Due to the stochastic nature of the XFELs, those patterns will to a varying degree include scatterings from contaminated samples. Also, the heterogeneity of the sample biomolecules is unavoidable and complicates data processing. Reducing the data volumes and selecting high-quality single-molecule patterns are therefore critical steps in the experimental set-up.   In this paper, we present two supervised template-based learning methods for classifying FXI patterns. Our Eigen-Image and Log-Likelihood classifier can find the best-matched template for a single-molecule pattern within a few milliseconds. It is also straightforward to parallelize them so as to fully match the XFEL repetition rate, thereby enabling processing at site.



### Perceptual Visual Interactive Learning
- **Arxiv ID**: http://arxiv.org/abs/1810.10789v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.10789v1)
- **Published**: 2018-10-25 09:06:06+00:00
- **Updated**: 2018-10-25 09:06:06+00:00
- **Authors**: Shenglan Liu, Xiang Liu, Yang Liu, Lin Feng, Hong Qiao, Jian Zhou, Yang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Supervised learning methods are widely used in machine learning. However, the lack of labels in existing data limits the application of these technologies. Visual interactive learning (VIL) compared with computers can avoid semantic gap, and solve the labeling problem of small label quantity (SLQ) samples in a groundbreaking way. In order to fully understand the importance of VIL to the interaction process, we re-summarize the interactive learning related algorithms (e.g. clustering, classification, retrieval etc.) from the perspective of VIL. Note that, perception and cognition are two main visual processes of VIL. On this basis, we propose a perceptual visual interactive learning (PVIL) framework, which adopts gestalt principle to design interaction strategy and multi-dimensionality reduction (MDR) to optimize the process of visualization. The advantage of PVIL framework is that it combines computer's sensitivity of detailed features and human's overall understanding of global tasks. Experimental results validate that the framework is superior to traditional computer labeling methods (such as label propagation) in both accuracy and efficiency, which achieves significant classification results on dense distribution and sparse classes dataset.



### Fast Neural Architecture Search of Compact Semantic Segmentation Models via Auxiliary Cells
- **Arxiv ID**: http://arxiv.org/abs/1810.10804v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.10804v3)
- **Published**: 2018-10-25 09:27:23+00:00
- **Updated**: 2019-05-17 01:31:24+00:00
- **Authors**: Vladimir Nekrasov, Hao Chen, Chunhua Shen, Ian Reid
- **Comment**: To appear in CVPR 2019. CityScapes results with ResNet-50 are added
- **Journal**: None
- **Summary**: Automated design of neural network architectures tailored for a specific task is an extremely promising, albeit inherently difficult, avenue to explore. While most results in this domain have been achieved on image classification and language modelling problems, here we concentrate on dense per-pixel tasks, in particular, semantic image segmentation using fully convolutional networks. In contrast to the aforementioned areas, the design choices of a fully convolutional network require several changes, ranging from the sort of operations that need to be used---e.g., dilated convolutions---to a solving of a more difficult optimisation problem. In this work, we are particularly interested in searching for high-performance compact segmentation architectures, able to run in real-time using limited resources. To achieve that, we intentionally over-parameterise the architecture during the training time via a set of auxiliary cells that provide an intermediate supervisory signal and can be omitted during the evaluation phase. The design of the auxiliary cell is emitted by a controller, a neural network with the fixed structure trained using reinforcement learning. More crucially, we demonstrate how to efficiently search for these architectures within limited time and computational budgets. In particular, we rely on a progressive strategy that terminates non-promising architectures from being further trained, and on Polyak averaging coupled with knowledge distillation to speed-up the convergence. Quantitatively, in 8 GPU-days our approach discovers a set of architectures performing on-par with state-of-the-art among compact models on the semantic segmentation, pose estimation and depth prediction tasks. Code will be made available here: https://github.com/drsleep/nas-segm-pytorch



### HANDS18: Methods, Techniques and Applications for Hand Observation
- **Arxiv ID**: http://arxiv.org/abs/1810.10818v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.10818v1)
- **Published**: 2018-10-25 10:21:52+00:00
- **Updated**: 2018-10-25 10:21:52+00:00
- **Authors**: Iason Oikonomidis, Guillermo Garcia-Hernando, Angela Yao, Antonis Argyros, Vincent Lepetit, Tae-Kyun Kim
- **Comment**: 11 pages, 1 figure, Discussion of the HANDS 2018 workshop held in
  conjunction with ECCV 2018
- **Journal**: None
- **Summary**: This report outlines the proceedings of the Fourth International Workshop on Observing and Understanding Hands in Action (HANDS 2018). The fourth instantiation of this workshop attracted significant interest from both academia and the industry. The program of the workshop included regular papers that are published as the workshop's proceedings, extended abstracts, invited posters, and invited talks. Topics of the submitted works and invited talks and posters included novel methods for hand pose estimation from RGB, depth, or skeletal data, datasets for special cases and real-world applications, and techniques for hand motion re-targeting and hand gesture recognition. The invited speakers are leaders in their respective areas of specialization, coming from both industry and academia. The main conclusions that can be drawn are the turn of the community towards RGB data and the maturation of some methods and techniques, which in turn has led to increasing interest for real-world applications.



### Compressed Sensing Plus Motion (CS+M): A New Perspective for Improving Undersampled MR Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1810.10828v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.10828v2)
- **Published**: 2018-10-25 10:57:03+00:00
- **Updated**: 2020-04-09 16:56:08+00:00
- **Authors**: Angelica I. Aviles-Rivero, Noémie Debroux, Guy Williams, Martin J. Graves, Carola-Bibiane Schonlieb
- **Comment**: None
- **Journal**: None
- **Summary**: We address the problem of reconstructing high quality images from undersampled MRI data. This is a challenging task due to the highly ill-posed nature of the problem. In particular, in dynamic MRI scans, the interaction between the target structure and the physical motion affects the acquired measurements leading to blurring artefacts and loss of fine details. In this work, we propose a framework for dynamic MRI reconstruction framed under a new multi-task optimisation model called Compressed Sensing Plus Motion (CS+M). Firstly, we propose a single optimisation problem that simultaneously computes the MRI reconstruction and the physical motion. Secondly, we show our model can be efficiently solved by breaking it up into two more computationally tractable problems. The potentials and generalisation capabilities of our approach are demonstrated in different clinical applications including cardiac cine, cardiac perfusion and brain perfusion imaging. We show, through numerical and graphical experiments, that the proposed scheme reduces blurring artefacts and preserves the target shape and fine details. We also report the highest quality reconstruction under highly undersampling rates in comparison to several state of the art techniques.



### An Adversarial Learning Approach to Medical Image Synthesis for Lesion Detection
- **Arxiv ID**: http://arxiv.org/abs/1810.10850v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.10850v2)
- **Published**: 2018-10-25 12:57:31+00:00
- **Updated**: 2019-04-08 15:44:12+00:00
- **Authors**: Liyan Sun, Jiexiang Wang, Yue Huang, Xinghao Ding, Hayit Greenspan, John Paisley
- **Comment**: 10 pages, 13 figures
- **Journal**: None
- **Summary**: The identification of lesion within medical image data is necessary for diagnosis, treatment and prognosis. Segmentation and classification approaches are mainly based on supervised learning with well-paired image-level or voxel-level labels. However, labeling the lesion in medical images is laborious requiring highly specialized knowledge. We propose a medical image synthesis model named abnormal-to-normal translation generative adversarial network (ANT-GAN) to generate a normal-looking medical image based on its abnormal-looking counterpart without the need for paired training data. Unlike typical GANs, whose aim is to generate realistic samples with variations, our more restrictive model aims at producing a normal-looking image corresponding to one containing lesions, and thus requires a special design. Being able to provide a "normal" counterpart to a medical image can provide useful side information for medical imaging tasks like lesion segmentation or classification validated by our experiments. In the other aspect, the ANT-GAN model is also capable of producing highly realistic lesion-containing image corresponding to the healthy one, which shows the potential in data augmentation verified in our experiments.



### Training of a Skull-Stripping Neural Network with efficient data augmentation
- **Arxiv ID**: http://arxiv.org/abs/1810.10853v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.10853v1)
- **Published**: 2018-10-25 13:01:27+00:00
- **Updated**: 2018-10-25 13:01:27+00:00
- **Authors**: Gabriele Valvano, Nicola Martini, Andrea Leo, Gianmarco Santini, Daniele Della Latta, Emiliano Ricciardi, Dante Chiappino
- **Comment**: April 2018. Content: 8 pages, 2 figures
- **Journal**: None
- **Summary**: Skull-stripping methods aim to remove the non-brain tissue from acquisition of brain scans in magnetic resonance (MR) imaging. Although several methods sharing this common purpose have been presented in literature, they all suffer from the great variability of the MR images. In this work we propose a novel approach based on Convolutional Neural Networks to automatically perform the brain extraction obtaining cutting-edge performance in the NFBS public database. Additionally, we focus on the efficient training of the neural network designing an effective data augmentation pipeline. Obtained results are evaluated through Dice metric, obtaining a value of 96.5%, and processing time, with 4.5s per volume.



### GAN Augmentation: Augmenting Training Data using Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1810.10863v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.10863v1)
- **Published**: 2018-10-25 13:17:33+00:00
- **Updated**: 2018-10-25 13:17:33+00:00
- **Authors**: Christopher Bowles, Liang Chen, Ricardo Guerrero, Paul Bentley, Roger Gunn, Alexander Hammers, David Alexander Dickie, Maria Valdés Hernández, Joanna Wardlaw, Daniel Rueckert
- **Comment**: None
- **Journal**: None
- **Summary**: One of the biggest issues facing the use of machine learning in medical imaging is the lack of availability of large, labelled datasets. The annotation of medical images is not only expensive and time consuming but also highly dependent on the availability of expert observers. The limited amount of training data can inhibit the performance of supervised machine learning algorithms which often need very large quantities of data on which to train to avoid overfitting. So far, much effort has been directed at extracting as much information as possible from what data is available. Generative Adversarial Networks (GANs) offer a novel way to unlock additional information from a dataset by generating synthetic samples with the appearance of real images. This paper demonstrates the feasibility of introducing GAN derived synthetic data to the training datasets in two brain segmentation tasks, leading to improvements in Dice Similarity Coefficient (DSC) of between 1 and 5 percentage points under different conditions, with the strongest effects seen fewer than ten training image stacks are available.



### Investigating the Automatic Classification of Algae Using Fusion of Spectral and Morphological Characteristics of Algae via Deep Residual Learning
- **Arxiv ID**: http://arxiv.org/abs/1810.10889v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1810.10889v1)
- **Published**: 2018-10-25 14:23:23+00:00
- **Updated**: 2018-10-25 14:23:23+00:00
- **Authors**: Jason L. Deglint, Chao Jin, Alexander Wong
- **Comment**: 19 pages, 8 figures
- **Journal**: None
- **Summary**: Under the impact of global climate changes and human activities, harmful algae blooms in surface waters have become a growing concern due to negative impacts on water related industries. Therefore, reliable and cost effective methods of quantifying the type and concentration of threshold levels of algae cells has become critical for ensuring successful water management. In this work, we present SAMSON, an innovative system to automatically classify multiple types of algae from different phyla groups by combining standard morphological features with their multi-wavelength signals. Two phyla with focused investigation in this study are the Cyanophyta phylum (blue-green algae), and the Chlorophyta phylum (green algae). We use a custom-designed microscopy imaging system which is configured to image water samples at two fluorescent wavelengths and seven absorption wavelengths using discrete-wavelength high-powered light emitting diodes (LEDs). Powered by computer vision and machine learning, we investigate the possibility and effectiveness of automatic classification using a deep residual convolutional neural network. More specifically, a classification accuracy of 96% was achieved in an experiment conducted with six different algae types. This high level of accuracy was achieved using a deep residual convolutional neural network that learns the optimal combination of spectral and morphological features. These findings elude to the possibility of leveraging a unique fingerprint of algae cell (i.e. spectral wavelengths and morphological features) to automatically distinguish different algae types. Our work herein demonstrates that, when coupled with multi-band fluorescence microscopy, machine learning algorithms can potentially be used as a robust and cost-effective tool for identifying and enumerating algae cells.



### Adversarial Semantic Scene Completion from a Single Depth Image
- **Arxiv ID**: http://arxiv.org/abs/1810.10901v1
- **DOI**: 10.1109/3DV.2018.00056
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.10901v1)
- **Published**: 2018-10-25 14:43:12+00:00
- **Updated**: 2018-10-25 14:43:12+00:00
- **Authors**: Yida Wang, David Joseph Tan, Nassir Navab, Federico Tombari
- **Comment**: 2018 International Conference on 3D Vision (3DV)
- **Journal**: 2018 International Conference on 3D Vision (3DV), Verona, Italy,
  2018, pp. 426-434
- **Summary**: We propose a method to reconstruct, complete and semantically label a 3D scene from a single input depth image. We improve the accuracy of the regressed semantic 3D maps by a novel architecture based on adversarial learning. In particular, we suggest using multiple adversarial loss terms that not only enforce realistic outputs with respect to the ground truth, but also an effective embedding of the internal features. This is done by correlating the latent features of the encoder working on partial 2.5D data with the latent features extracted from a variational 3D auto-encoder trained to reconstruct the complete semantic scene. In addition, differently from other approaches that operate entirely through 3D convolutions, at test time we retain the original 2.5D structure of the input during downsampling to improve the effectiveness of the internal representation of our model. We test our approach on the main benchmark datasets for semantic scene completion to qualitatively and quantitatively assess the effectiveness of our proposal.



### Automatic Analysis of Facial Expressions Based on Deep Covariance Trajectories
- **Arxiv ID**: http://arxiv.org/abs/1810.11392v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.11392v3)
- **Published**: 2018-10-25 15:00:20+00:00
- **Updated**: 2019-12-04 15:09:38+00:00
- **Authors**: Naima Otberdout, Anis Kacem, Mohamed Daoudi, Lahoucine Ballihi, Stefano Berretti
- **Comment**: A preliminary version of this work appeared in "Otberdout N, Kacem A,
  Daoudi M, Ballihi L, Berretti S. Deep Covariance Descriptors for Facial
  Expression Recognition, in British Machine Vision Conference 2018, BMVC 2018,
  Northumbria University, Newcastle, UK, September 3-6, 2018. ; 2018 :159."
  arXiv admin note: substantial text overlap with arXiv:1805.03869
- **Journal**: None
- **Summary**: In this paper, we propose a new approach for facial expression recognition using deep covariance descriptors. The solution is based on the idea of encoding local and global Deep Convolutional Neural Network (DCNN) features extracted from still images, in compact local and global covariance descriptors. The space geometry of the covariance matrices is that of Symmetric Positive Definite (SPD) matrices. By conducting the classification of static facial expressions using Support Vector Machine (SVM) with a valid Gaussian kernel on the SPD manifold, we show that deep covariance descriptors are more effective than the standard classification with fully connected layers and softmax. Besides, we propose a completely new and original solution to model the temporal dynamic of facial expressions as deep trajectories on the SPD manifold. As an extension of the classification pipeline of covariance descriptors, we apply SVM with valid positive definite kernels derived from global alignment for deep covariance trajectories classification. By performing extensive experiments on the Oulu-CASIA, CK+, and SFEW datasets, we show that both the proposed static and dynamic approaches achieve state-of-the-art performance for facial expression recognition outperforming many recent approaches.



### Practical Shape Analysis and Segmentation Methods for Point Cloud Models
- **Arxiv ID**: http://arxiv.org/abs/1810.10933v1
- **DOI**: None
- **Categories**: **cs.CG**, cs.CV, cs.GR, J.6; I.4.6; I.5.3; I.3.5
- **Links**: [PDF](http://arxiv.org/pdf/1810.10933v1)
- **Published**: 2018-10-25 15:38:30+00:00
- **Updated**: 2018-10-25 15:38:30+00:00
- **Authors**: Reed M. Williams, Horea T. Ilieş
- **Comment**: 21 pages, 20 figures. To appear in The Journal of Computer Aided
  Geometric Design's Special Issue on Heat Diffusion Equation and Optimal
  Transport in Geometry Processing and Computer Graphics
- **Journal**: None
- **Summary**: Current point cloud processing algorithms do not have the capability to automatically extract semantic information from the observed scenes, except in very specialized cases. Furthermore, existing mesh analysis paradigms cannot be directly employed to automatically perform typical shape analysis tasks directly on point cloud models.   We present a potent framework for shape analysis, similarity, and segmentation of noisy point cloud models for real objects of engineering interest, models that may be incomplete. The proposed framework relies on spectral methods and the heat diffusion kernel to construct compact shape signatures, and we show that the framework supports a variety of clustering techniques that have traditionally been applied only on mesh models. We developed and implemented one practical and convergent estimate of the Laplace-Beltrami operator for point clouds as well as a number of clustering techniques adapted to work directly on point clouds to produce geometric features of engineering interest. The key advantage of this framework is that it supports practical shape analysis capabilities that operate directly on point cloud models of objects without requiring surface reconstruction or global meshing. We show that the proposed technique is robust against typical noise present in possibly incomplete point clouds, and segment point clouds scanned by depth cameras (e.g. Kinect) into semantically-meaningful sub-shapes.



### Alzheimer's Disease Diagnosis Based on Cognitive Methods in Virtual Environments and Emotions Analysis
- **Arxiv ID**: http://arxiv.org/abs/1810.10941v1
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1810.10941v1)
- **Published**: 2018-10-25 15:56:51+00:00
- **Updated**: 2018-10-25 15:56:51+00:00
- **Authors**: Juan Manuel Fernández Montenegro
- **Comment**: PhD Thesis 2018
- **Journal**: None
- **Summary**: Dementia is a syndrome characterised by the decline of different cognitive abilities. Alzheimer's Disease (AD) is the most common dementia affecting cognitive domains such as memory and learning, perceptual-motion or executive function. High rate of deaths and high cost for detection, treatments and patient's care count amongst its consequences. Early detection of AD is considered of high importance for improving the quality of life of patients and their families. The aim of this thesis is to introduce novel non-invasive early diagnosis methods in order to speed the diagnosis, reduce the associated costs and make them widely accessible. Novel AD's screening tests based on virtual environments using new immersive technologies combined with advanced Human Computer Interaction (HCI) systems are introduced. Four tests demonstrate the wide range of screening mechanisms based on cognitive domain impairments that can be designed using virtual environments. The use of emotion recognition to analyse AD symptoms has been also proposed. A novel multimodal dataset was specifically created to remark the autobiographical memory deficits of AD patients. Data from this dataset is used to introduce novel descriptors for Electroencephalogram (EEG) and facial images data. EEG features are based on quaternions in order to keep the correlation information between sensors, whereas, for facial expression recognition, a preprocessing method for motion magnification and descriptors based on origami crease pattern algorithm are proposed to enhance facial micro-expressions. These features have been proved on classifiers such as SVM and Adaboost for the classification of reactions to autobiographical stimuli such as long and short term memories.



### Training Generative Adversarial Networks Via Turing Test
- **Arxiv ID**: http://arxiv.org/abs/1810.10948v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.10948v2)
- **Published**: 2018-10-25 16:08:02+00:00
- **Updated**: 2018-10-27 08:49:53+00:00
- **Authors**: Jianlin Su
- **Comment**: fix some clerical errors, add some experimental data
- **Journal**: None
- **Summary**: In this article, we introduce a new mode for training Generative Adversarial Networks (GANs). Rather than minimizing the distance of evidence distribution $\tilde{p}(x)$ and the generative distribution $q(x)$, we minimize the distance of $\tilde{p}(x_r)q(x_f)$ and $\tilde{p}(x_f)q(x_r)$. This adversarial pattern can be interpreted as a Turing test in GANs. It allows us to use information of real samples during training generator and accelerates the whole training procedure. We even find that just proportionally increasing the size of discriminator and generator, it succeeds on 256x256 resolution without adjusting hyperparameters carefully.



### A Preliminary Study on Hyperparameter Configuration for Human Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/1810.10956v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.10956v1)
- **Published**: 2018-10-25 16:26:30+00:00
- **Updated**: 2018-10-25 16:26:30+00:00
- **Authors**: Kemilly Dearo Garcia, Tiago Carvalho, João Mendes-Moreira, João M. P. Cardoso, André C. P. L. F. de Carvalho
- **Comment**: None
- **Journal**: None
- **Summary**: Human activity recognition (HAR) is a classification task that aims to classify human activities or predict human behavior by means of features extracted from sensors data. Typical HAR systems use wearable sensors and/or handheld and mobile devices with built-in sensing capabilities. Due to the widespread use of smartphones and to the inclusion of various sensors in all contemporary smartphones (e.g., accelerometers and gyroscopes), they are commonly used for extracting and collecting data from sensors and even for implementing HAR systems. When using mobile devices, e.g., smartphones, HAR systems need to deal with several constraints regarding battery, computation and memory. These constraints enforce the need of a system capable of managing its resources and maintain acceptable levels of classification accuracy. Moreover, several factors can influence activity recognition, such as classification models, sensors availability and size of data window for feature extraction, making stable accuracy a difficult task. In this paper, we present a semi-supervised classifier and a study regarding the influence of hyperparameter configuration in classification accuracy, depending on the user and the activities performed by each user. This study focuses on sensing data provided by the PAMAP2 dataset. Experimental results show that it is possible to maintain classification accuracy by adjusting hyperparameters, like window size and windows overlap factor, depending on user and activity performed. These experiments motivate the development of a system able to automatically adapt hyperparameter settings for the activity performed by each user.



### DeepDPM: Dynamic Population Mapping via Deep Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1811.02644v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.02644v2)
- **Published**: 2018-10-25 16:46:38+00:00
- **Updated**: 2018-11-28 03:24:35+00:00
- **Authors**: Zefang Zong, Jie Feng, Kechun Liu, Hongzhi Shi, Yong Li
- **Comment**: AAAI2019
- **Journal**: None
- **Summary**: Dynamic high resolution data on human population distribution is of great importance for a wide spectrum of activities and real-life applications, but is too difficult and expensive to obtain directly. Therefore, generating fine-scaled population distributions from coarse population data is of great significance. However, there are three major challenges: 1) the complexity in spatial relations between high and low resolution population; 2) the dependence of population distributions on other external information; 3) the difficulty in retrieving temporal distribution patterns. In this paper, we first propose the idea to generate dynamic population distributions in full-time series, then we design dynamic population mapping via deep neural network(DeepDPM), a model that describes both spatial and temporal patterns using coarse data and point of interest information. In DeepDPM, we utilize super-resolution convolutional neural network(SRCNN) based model to directly map coarse data into higher resolution data, and a time-embedded long short-term memory model to effectively capture the periodicity nature to smooth the finer-scaled results from the previous static SRCNN model. We perform extensive experiments on a real-life mobile dataset collected from Shanghai. Our results demonstrate that DeepDPM outperforms previous state-of-the-art methods and a suite of frequent data-mining approaches. Moreover, DeepDPM breaks through the limitation from previous works in time dimension so that dynamic predictions in all-day time slots can be obtained.



### Decoding Brain Representations by Multimodal Learning of Neural Activity and Visual Features
- **Arxiv ID**: http://arxiv.org/abs/1810.10974v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1810.10974v2)
- **Published**: 2018-10-25 16:52:20+00:00
- **Updated**: 2020-04-18 17:49:41+00:00
- **Authors**: Simone Palazzo, Concetto Spampinato, Isaak Kavasidis, Daniela Giordano, Joseph Schmidt, Mubarak Shah
- **Comment**: None
- **Journal**: None
- **Summary**: This work presents a novel method of exploring human brain-visual representations, with a view towards replicating these processes in machines. The core idea is to learn plausible computational and biological representations by correlating human neural activity and natural images. Thus, we first propose a model, EEG-ChannelNet, to learn a brain manifold for EEG classification. After verifying that visual information can be extracted from EEG data, we introduce a multimodal approach that uses deep image and EEG encoders, trained in a siamese configuration, for learning a joint manifold that maximizes a compatibility measure between visual features and brain representations. We then carry out image classification and saliency detection on the learned manifold. Performance analyses show that our approach satisfactorily decodes visual information from neural signals. This, in turn, can be used to effectively supervise the training of deep learning models, as demonstrated by the high performance of image classification and saliency detection on out-of-training classes. The obtained results show that the learned brain-visual features lead to improved performance and simultaneously bring deep models more in line with cognitive neuroscience work related to visual perception and attention.



### On the dissection of degenerate cosmologies with machine learning
- **Arxiv ID**: http://arxiv.org/abs/1810.11027v2
- **DOI**: 10.1093/mnras/stz972
- **Categories**: **astro-ph.CO**, cs.CV, gr-qc
- **Links**: [PDF](http://arxiv.org/pdf/1810.11027v2)
- **Published**: 2018-10-25 18:00:02+00:00
- **Updated**: 2019-03-27 09:41:52+00:00
- **Authors**: Julian Merten, Carlo Giocoli, Marco Baldi, Massimo Meneghetti, Austin Peel, Florian Lalande, Jean-Luc Starck, Valeria Pettorino
- **Comment**: 21 pages, 14 figures, 10 tables. Associated code and data respository
  at https://www.bitbucket.org/jmerten82/mydnn . Accepted for publication by
  the MNRAS
- **Journal**: None
- **Summary**: Based on the DUSTGRAIN-pathfinder suite of simulations, we investigate observational degeneracies between nine models of modified gravity and massive neutrinos. Three types of machine learning techniques are tested for their ability to discriminate lensing convergence maps by extracting dimensional reduced representations of the data. Classical map descriptors such as the power spectrum, peak counts and Minkowski functionals are combined into a joint feature vector and compared to the descriptors and statistics that are common to the field of digital image processing. To learn new features directly from the data we use a Convolutional Neural Network (CNN). For the mapping between feature vectors and the predictions of their underlying model, we implement two different classifiers; one based on a nearest-neighbour search and one that is based on a fully connected neural network. We find that the neural network provides a much more robust classification than the nearest-neighbour approach and that the CNN provides the most discriminating representation of the data. It achieves the cleanest separation between the different models and the highest classification success rate of 59% for a single source redshift. Once we perform a tomographic CNN analysis, the total classification accuracy increases significantly to 76% with no observational degeneracies remaining. Visualising the filter responses of the CNN at different network depths provides us with the unique opportunity to learn from very complex models and to understand better why they perform so well.



### One-Shot Hierarchical Imitation Learning of Compound Visuomotor Tasks
- **Arxiv ID**: http://arxiv.org/abs/1810.11043v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.11043v1)
- **Published**: 2018-10-25 18:05:08+00:00
- **Updated**: 2018-10-25 18:05:08+00:00
- **Authors**: Tianhe Yu, Pieter Abbeel, Sergey Levine, Chelsea Finn
- **Comment**: Video results available at https://sites.google.com/view/one-shot-hil
- **Journal**: None
- **Summary**: We consider the problem of learning multi-stage vision-based tasks on a real robot from a single video of a human performing the task, while leveraging demonstration data of subtasks with other objects. This problem presents a number of major challenges. Video demonstrations without teleoperation are easy for humans to provide, but do not provide any direct supervision. Learning policies from raw pixels enables full generality but calls for large function approximators with many parameters to be learned. Finally, compound tasks can require impractical amounts of demonstration data, when treated as a monolithic skill. To address these challenges, we propose a method that learns both how to learn primitive behaviors from video demonstrations and how to dynamically compose these behaviors to perform multi-stage tasks by "watching" a human demonstrator. Our results on a simulated Sawyer robot and real PR2 robot illustrate our method for learning a variety of order fulfillment and kitchen serving tasks with novel objects and raw pixel inputs.



### Radiomic Synthesis Using Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1810.11090v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.11090v2)
- **Published**: 2018-10-25 20:07:39+00:00
- **Updated**: 2019-05-29 13:41:31+00:00
- **Authors**: Vishwa S. Parekh, Michael A. Jacobs
- **Comment**: Submitted to ISBI 2019, 4 pages
- **Journal**: None
- **Summary**: Radiomics is a rapidly growing field that deals with modeling the textural information present in the different tissues of interest for clinical decision support. However, the process of generating radiomic images is computationally very expensive and could take substantial time per radiological image for certain higher order features, such as, gray-level co-occurrence matrix(GLCM), even with high-end GPUs. To that end, we developed RadSynth, a deep convolutional neural network(CNN) model, to efficiently generate radiomic images. RadSynth was tested on a breast cancer patient cohort of twenty-four patients(ten benign, ten malignant and four normal) for computation of GLCM entropy images from post-contrast DCE-MRI. RadSynth produced excellent synthetic entropy images compared to traditional GLCM entropy images. The average percentage difference and correlation between the two techniques were 0.07 $\pm$ 0.06 and 0.97, respectively. In conclusion, RadSynth presents a new powerful tool for fast computation and visualization of the textural information present in the radiological images.



### Improving Document Binarization via Adversarial Noise-Texture Augmentation
- **Arxiv ID**: http://arxiv.org/abs/1810.11120v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.11120v2)
- **Published**: 2018-10-25 22:02:54+00:00
- **Updated**: 2019-05-01 10:33:02+00:00
- **Authors**: Ankan Kumar Bhunia, Ayan Kumar Bhunia, Aneeshan Sain, Partha Pratim Roy
- **Comment**: IEEE International Conference on Image Processing (ICIP), 2019. The
  full source code of the proposed system is publicly available at
  https://github.com/ankanbhunia/AdverseBiNet
- **Journal**: None
- **Summary**: Binarization of degraded document images is an elementary step in most of the problems in document image analysis domain. The paper re-visits the binarization problem by introducing an adversarial learning approach. We construct a Texture Augmentation Network that transfers the texture element of a degraded reference document image to a clean binary image. In this way, the network creates multiple versions of the same textual content with various noisy textures, thus enlarging the available document binarization datasets. At last, the newly generated images are passed through a Binarization network to get back the clean version. By jointly training the two networks we can increase the adversarial robustness of our system. Also, it is noteworthy that our model can learn from unpaired data. Experimental results suggest that the proposed method achieves superior performance over widely used DIBCO datasets.



### Towards improved lossy image compression: Human image reconstruction with public-domain images
- **Arxiv ID**: http://arxiv.org/abs/1810.11137v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.IT, cs.MM, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/1810.11137v3)
- **Published**: 2018-10-25 23:34:37+00:00
- **Updated**: 2019-06-25 03:01:38+00:00
- **Authors**: Ashutosh Bhown, Soham Mukherjee, Sean Yang, Shubham Chandak, Irena Fischer-Hwang, Kedar Tatwawadi, Judith Fan, Tsachy Weissman
- **Comment**: None
- **Journal**: None
- **Summary**: Lossy image compression has been studied extensively in the context of typical loss functions such as RMSE, MS-SSIM, etc. However, compression at low bitrates generally produces unsatisfying results. Furthermore, the availability of massive public image datasets appears to have hardly been exploited in image compression. Here, we present a paradigm for eliciting human image reconstruction in order to perform lossy image compression. In this paradigm, one human describes images to a second human, whose task is to reconstruct the target image using publicly available images and text instructions. The resulting reconstructions are then evaluated by human raters on the Amazon Mechanical Turk platform and compared to reconstructions obtained using state-of-the-art compressor WebP. Our results suggest that prioritizing semantic visual elements may be key to achieving significant improvements in image compression, and that our paradigm can be used to develop a more human-centric loss function.   The images, results and additional data are available at https://compression.stanford.edu/human-compression



