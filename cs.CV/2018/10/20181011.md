# Arxiv Papers in cs.CV on 2018-10-11
### A Novel Domain Adaptation Framework for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1810.05732v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.05732v1)
- **Published**: 2018-10-11 04:03:30+00:00
- **Updated**: 2018-10-11 04:03:30+00:00
- **Authors**: Amir Gholami, Shashank Subramanian, Varun Shenoy, Naveen Himthani, Xiangyu Yue, Sicheng Zhao, Peter Jin, George Biros, Kurt Keutzer
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a segmentation framework that uses deep neural networks and introduce two innovations. First, we describe a biophysics-based domain adaptation method. Second, we propose an automatic method to segment white and gray matter, and cerebrospinal fluid, in addition to tumorous tissue. Regarding our first innovation, we use a domain adaptation framework that combines a novel multispecies biophysical tumor growth model with a generative adversarial model to create realistic looking synthetic multimodal MR images with known segmentation. Regarding our second innovation, we propose an automatic approach to enrich available segmentation data by computing the segmentation for healthy tissues. This segmentation, which is done using diffeomorphic image registration between the BraTS training data and a set of prelabeled atlases, provides more information for training and reduces the class imbalance problem. Our overall approach is not specific to any particular neural network and can be used in conjunction with existing solutions. We demonstrate the performance improvement using a 2D U-Net for the BraTS'18 segmentation challenge. Our biophysics based domain adaptation achieves better results, as compared to the existing state-of-the-art GAN model used to create synthetic data for training.



### A Data-Efficient Framework for Training and Sim-to-Real Transfer of Navigation Policies
- **Arxiv ID**: http://arxiv.org/abs/1810.04871v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1810.04871v1)
- **Published**: 2018-10-11 07:22:54+00:00
- **Updated**: 2018-10-11 07:22:54+00:00
- **Authors**: Homanga Bharadhwaj, Zihan Wang, Yoshua Bengio, Liam Paull
- **Comment**: Under review in ICRA 2019
- **Journal**: None
- **Summary**: Learning effective visuomotor policies for robots purely from data is challenging, but also appealing since a learning-based system should not require manual tuning or calibration. In the case of a robot operating in a real environment the training process can be costly, time-consuming, and even dangerous since failures are common at the start of training. For this reason, it is desirable to be able to leverage \textit{simulation} and \textit{off-policy} data to the extent possible to train the robot. In this work, we introduce a robust framework that plans in simulation and transfers well to the real environment. Our model incorporates a gradient-descent based planning module, which, given the initial image and goal image, encodes the images to a lower dimensional latent state and plans a trajectory to reach the goal. The model, consisting of the encoder and planner modules, is trained through a meta-learning strategy in simulation first. We subsequently perform adversarial domain transfer on the encoder by using a bank of unlabelled but random images from the simulation and real environments to enable the encoder to map images from the real and simulated environments to a similarly distributed latent representation. By fine tuning the entire model (encoder + planner) with far fewer real world expert demonstrations, we show successful planning performances in different navigation tasks.



### Deep Bi-Dense Networks for Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1810.04873v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.04873v1)
- **Published**: 2018-10-11 07:34:39+00:00
- **Updated**: 2018-10-11 07:34:39+00:00
- **Authors**: Yucheng Wang, Jialiang Shen, Jian Zhang
- **Comment**: DICTA 2018
- **Journal**: None
- **Summary**: This paper proposes Deep Bi-Dense Networks (DBDN) for single image super-resolution. Our approach extends previous intra-block dense connection approaches by including novel inter-block dense connections. In this way, feature information propagates from a single dense block to all subsequent blocks, instead of to a single successor. To build a DBDN, we firstly construct intra-dense blocks, which extract and compress abundant local features via densely connected convolutional layers and compression layers for further feature learning. Then, we use an inter-block dense net to connect intra-dense blocks, which allow each intra-dense block propagates its own local features to all successors. Additionally, our bi-dense construction connects each block to the output, alleviating the vanishing gradient problems in training. The evaluation of our proposed method on five benchmark datasets shows that our DBDN outperforms the state of the art in SISR with a moderate number of network parameters.



### Generating Shared Latent Variables for Robots to Imitate Human Movements and Understand their Physical Limitations
- **Arxiv ID**: http://arxiv.org/abs/1810.04879v3
- **DOI**: 10.1007/978-3-030-11012-3_15
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1810.04879v3)
- **Published**: 2018-10-11 08:01:08+00:00
- **Updated**: 2021-11-03 09:25:53+00:00
- **Authors**: Maxime Devanne, Sao Mai Nguyen
- **Comment**: None
- **Journal**: Leal-Taix{\'e} L; Roth S. Lecture Notes in Computer Science,
  11130, Springer, Cham, pp.190-197, 2019, Computer Vision -- ECCV 2018
  Workshops
- **Summary**: Assistive robotics and particularly robot coaches may be very helpful for rehabilitation healthcare. In this context, we propose a method based on Gaussian Process Latent Variable Model (GP-LVM) to transfer knowledge between a physiotherapist, a robot coach and a patient. Our model is able to map visual human body features to robot data in order to facilitate the robot learning and imitation. In addition , we propose to extend the model to adapt robots' understanding to patient's physical limitations during the assessment of rehabilitation exercises. Experimental evaluation demonstrates promising results for both robot imitation and model adaptation according to the patients' limitations.



### Monitoring spatial sustainable development: Semi-automated analysis of satellite and aerial images for energy transition and sustainability indicators
- **Arxiv ID**: http://arxiv.org/abs/1810.04881v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, 00-02
- **Links**: [PDF](http://arxiv.org/pdf/1810.04881v1)
- **Published**: 2018-10-11 08:05:29+00:00
- **Updated**: 2018-10-11 08:05:29+00:00
- **Authors**: R. L. Curier, T. J. A. De Jong, Katharina Strauch, Katharina Cramer, Natalie Rosenski, Clara Schartner, M. Debusschere, Hannah Ziemons, Deniz Iren, Stefano Bromuri
- **Comment**: This document provides the reader with an overview of the various
  datasets which will be used throughout the project. The collection of
  satellite and aerial images as well as auxiliary information such as the
  location of buildings and roofs which is required to train, test and validate
  the machine learning algorithm that is being developed
- **Journal**: None
- **Summary**: Solar panels are installed by a large and growing number of households due to the convenience of having cheap and renewable energy to power house appliances. In contrast to other energy sources solar installations are distributed very decentralized and spread over hundred-thousands of locations. On a global level more than 25% of solar photovoltaic (PV) installations were decentralized. The effect of the quick energy transition from a carbon based economy to a green economy is though still very difficult to quantify. As a matter of fact the quick adoption of solar panels by households is difficult to track, with local registries that miss a large number of the newly built solar panels. This makes the task of assessing the impact of renewable energies an impossible task. Although models of the output of a region exist, they are often black box estimations. This project's aim is twofold: First automate the process to extract the location of solar panels from aerial or satellite images and second, produce a map of solar panels along with statistics on the number of solar panels. Further, this project takes place in a wider framework which investigates how official statistics can benefit from new digital data sources. At project completion, a method for detecting solar panels from aerial images via machine learning will be developed and the methodology initially developed for BE, DE and NL will be standardized for application to other EU countries. In practice, machine learning techniques are used to identify solar panels in satellite and aerial images for the province of Limburg (NL), Flanders (BE) and North Rhine-Westphalia (DE).



### Dense Object Reconstruction from RGBD Images with Embedded Deep Shape Representations
- **Arxiv ID**: http://arxiv.org/abs/1810.04891v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1810.04891v1)
- **Published**: 2018-10-11 08:26:33+00:00
- **Updated**: 2018-10-11 08:26:33+00:00
- **Authors**: Lan Hu, Yuchen Cao, Peng Wu, Laurent Kneip
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: Most problems involving simultaneous localization and mapping can nowadays be solved using one of two fundamentally different approaches. The traditional approach is given by a least-squares objective, which minimizes many local photometric or geometric residuals over explicitly parametrized structure and camera parameters. Unmodeled effects violating the lambertian surface assumption or geometric invariances of individual residuals are encountered through statistical averaging or the addition of robust kernels and smoothness terms. Aiming at more accurate measurement models and the inclusion of higher-order shape priors, the community more recently shifted its attention to deep end-to-end models for solving geometric localization and mapping problems. However, at test-time, these feed-forward models ignore the more traditional geometric or photometric consistency terms, thus leading to a low ability to recover fine details and potentially complete failure in corner case scenarios. With an application to dense object modeling from RGBD images, our work aims at taking the best of both worlds by embedding modern higher-order object shape priors into classical iterative residual minimization objectives. We demonstrate a general ability to improve mapping accuracy with respect to each modality alone, and present a successful application to real data.



### Perfusion parameter estimation using neural networks and data augmentation
- **Arxiv ID**: http://arxiv.org/abs/1810.04898v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.04898v1)
- **Published**: 2018-10-11 08:36:30+00:00
- **Updated**: 2018-10-11 08:36:30+00:00
- **Authors**: David Robben, Paul Suetens
- **Comment**: Presented at the MICCAI 2018 SWITCH workshop (16 September 2018,
  Granada, Spain)
- **Journal**: None
- **Summary**: Perfusion imaging plays a crucial role in acute stroke diagnosis and treatment decision making. Current perfusion analysis relies on deconvolution of the measured signals, an operation that is mathematically ill-conditioned and requires strong regularization. We propose a neural network and a data augmentation approach to predict perfusion parameters directly from the native measurements. A comparison on simulated CT Perfusion data shows that the neural network provides better estimations for both CBF and Tmax than a state of the art deconvolution method, and this over a wide range of noise levels. The proposed data augmentation enables to achieve these results with less than 100 datasets.



### MOANOFS: Multi-Objective Automated Negotiation based Online Feature Selection System for Big Data Classification
- **Arxiv ID**: http://arxiv.org/abs/1810.04903v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.MA, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.04903v2)
- **Published**: 2018-10-11 08:41:30+00:00
- **Updated**: 2019-06-18 15:19:17+00:00
- **Authors**: Fatma BenSaid, Adel M. Alimi
- **Comment**: 15 pages, 8 figures, journal paper
- **Journal**: None
- **Summary**: Feature Selection (FS) plays an important role in learning and classification tasks. The object of FS is to select the relevant and non-redundant features. Considering the huge amount number of features in real-world applications, FS methods using batch learning technique can't resolve big data problem especially when data arrive sequentially. In this paper, we propose an online feature selection system which resolves this problem. More specifically, we treat the problem of online supervised feature selection for binary classification as a decision-making problem. A philosophical vision to this problem leads to a hybridization between two important domains: feature selection using online learning technique (OFS) and automated negotiation (AN). The proposed OFS system called MOANOFS (Multi-Objective Automated Negotiation based Online Feature Selection) uses two levels of decision. In the first level, from n learners (or OFS methods), we decide which are the k trustful ones (with high confidence or trust value). These elected k learners will participate in the second level. In this level, we integrate our proposed Multilateral Automated Negotiation based OFS (MANOFS) method to decide finally which is the best solution or which are relevant features. We show that MOANOFS system is applicable to different domains successfully and achieves high accuracy with several real-world applications.   Index Terms: Feature selection, online learning, multi-objective automated negotiation, trust, classification, big data.



### VIPL-HR: A Multi-modal Database for Pulse Estimation from Less-constrained Face Video
- **Arxiv ID**: http://arxiv.org/abs/1810.04927v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.04927v2)
- **Published**: 2018-10-11 09:32:27+00:00
- **Updated**: 2018-11-27 04:57:32+00:00
- **Authors**: Xuesong Niu, Hu Han, Shiguang Shan, Xilin Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Heart rate (HR) is an important physiological signal that reflects the physical and emotional activities of humans. Traditional HR measurements are mainly based on contact monitors, which are inconvenient and may cause discomfort for the subjects. Recently, methods have been proposed for remote HR estimation from face videos. However, most of the existing methods focus on well-controlled scenarios, their generalization ability into less-constrained scenarios are not known. At the same time, lacking large-scale databases has limited the use of deep representation learning methods in remote HR estimation. In this paper, we introduce a large-scale multi-modal HR database (named as VIPL-HR), which contains 2,378 visible light videos (VIS) and 752 near-infrared (NIR) videos of 107 subjects. Our VIPL-HR database also contains various variations such as head movements, illumination variations, and acquisition device changes. We also learn a deep HR estimator (named as RhythmNet) with the proposed spatial-temporal representation, which achieves promising results on both the public-domain and our VIPL-HR HR estimation databases. We would like to put the VIPL-HR database into the public domain.



### Location Dependency in Video Prediction
- **Arxiv ID**: http://arxiv.org/abs/1810.04937v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.04937v2)
- **Published**: 2018-10-11 10:01:00+00:00
- **Updated**: 2018-10-16 12:11:00+00:00
- **Authors**: Niloofar Azizi, Hafez Farazi, Sven Behnke
- **Comment**: International Conference on Artificial Neural Networks. Springer,
  Cham, 2018
- **Journal**: International Conference on Artificial Neural Networks. Springer,
  Cham, 2018
- **Summary**: Deep convolutional neural networks are used to address many computer vision problems, including video prediction. The task of video prediction requires analyzing the video frames, temporally and spatially, and constructing a model of how the environment evolves. Convolutional neural networks are spatially invariant, though, which prevents them from modeling location-dependent patterns. In this work, the authors propose location-biased convolutional layers to overcome this limitation. The effectiveness of location bias is evaluated on two architectures: Video Ladder Network (VLN) and Convolutional redictive Gating Pyramid (Conv-PGP). The results indicate that encoding location-dependent features is crucial for the task of video prediction. Our proposed methods significantly outperform spatially invariant models.



### Online Visual Robot Tracking and Identification using Deep LSTM Networks
- **Arxiv ID**: http://arxiv.org/abs/1810.04941v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1810.04941v2)
- **Published**: 2018-10-11 10:20:52+00:00
- **Updated**: 2018-10-16 12:04:43+00:00
- **Authors**: Hafez Farazi, Sven Behnke
- **Comment**: IEEE/RSJ International Conference on Intelligent Robots and Systems
  (IROS), Vancouver, Canada, 2017. IROS RoboCup Best Paper Award
- **Journal**: IEEE/RSJ International Conference on Intelligent Robots and
  Systems (IROS), Vancouver, Canada, 2017
- **Summary**: Collaborative robots working on a common task are necessary for many applications. One of the challenges for achieving collaboration in a team of robots is mutual tracking and identification. We present a novel pipeline for online visionbased detection, tracking and identification of robots with a known and identical appearance. Our method runs in realtime on the limited hardware of the observer robot. Unlike previous works addressing robot tracking and identification, we use a data-driven approach based on recurrent neural networks to learn relations between sequential inputs and outputs. We formulate the data association problem as multiple classification problems. A deep LSTM network was trained on a simulated dataset and fine-tuned on small set of real data. Experiments on two challenging datasets, one synthetic and one real, which include long-term occlusions, show promising results.



### Globally Continuous and Non-Markovian Activity Analysis from Videos
- **Arxiv ID**: http://arxiv.org/abs/1810.04954v1
- **DOI**: 10.1007/978-3-319-46454-1_32
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.04954v1)
- **Published**: 2018-10-11 11:15:17+00:00
- **Updated**: 2018-10-11 11:15:17+00:00
- **Authors**: He Wang, Carol O'Sullivan
- **Comment**: Preprint of our ECCV 2016 spotlight paper
- **Journal**: Wang H., O'Sullivan C. (2016) Globally Continuous and
  Non-Markovian Crowd Activity Analysis from Videos. In ECCV 2016. Lecture
  Notes in Computer Science, vol 9909. Springer
- **Summary**: Automatically recognizing activities in video is a classic problem in vision and helps to understand behaviors, describe scenes and detect anomalies. We propose an unsupervised method for such purposes. Given video data, we discover recurring activity patterns that appear, peak, wane and disappear over time. By using non-parametric Bayesian methods, we learn coupled spatial and temporal patterns with minimum prior knowledge. To model the temporal changes of patterns, previous works compute Markovian progressions or locally continuous motifs whereas we model time in a globally continuous and non-Markovian way. Visually, the patterns depict flows of major activities. Temporally, each pattern has its own unique appearance-disappearance cycles. To compute compact pattern representations, we also propose a hybrid sampling method. By combining these patterns with detailed environment information, we interpret the semantics of activities and report anomalies. Also, our method fits data better and detects anomalies that were difficult to detect previously.



### SingleGAN: Image-to-Image Translation by a Single-Generator Network using Multiple Generative Adversarial Learning
- **Arxiv ID**: http://arxiv.org/abs/1810.04991v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.04991v2)
- **Published**: 2018-10-11 13:01:40+00:00
- **Updated**: 2020-04-17 15:46:37+00:00
- **Authors**: Xiaoming Yu, Xing Cai, Zhenqiang Ying, Thomas Li, Ge Li
- **Comment**: Accepted in ACCV 2018. Code is available at
  https://github.com/Xiaoming-Yu/SingleGAN
- **Journal**: None
- **Summary**: Image translation is a burgeoning field in computer vision where the goal is to learn the mapping between an input image and an output image. However, most recent methods require multiple generators for modeling different domain mappings, which are inefficient and ineffective on some multi-domain image translation tasks. In this paper, we propose a novel method, SingleGAN, to perform multi-domain image-to-image translations with a single generator. We introduce the domain code to explicitly control the different generative tasks and integrate multiple optimization goals to ensure the translation. Experimental results on several unpaired datasets show superior performance of our model in translation between two domains. Besides, we explore variants of SingleGAN for different tasks, including one-to-many domain translation, many-to-many domain translation and one-to-one domain translation with multimodality. The extended experiments show the universality and extensibility of our model.



### ISA$^2$: Intelligent Speed Adaptation from Appearance
- **Arxiv ID**: http://arxiv.org/abs/1810.05016v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.05016v1)
- **Published**: 2018-10-11 13:43:36+00:00
- **Updated**: 2018-10-11 13:43:36+00:00
- **Authors**: Carlos Herranz-Perdiguero, Roberto J. López-Sastre
- **Comment**: IROS 2018 Workshop: 10th Planning, Perception and Navigation for
  Intelligent Vehicles (PPNIV'18)
- **Journal**: None
- **Summary**: In this work we introduce a new problem named Intelligent Speed Adaptation from Appearance (ISA$^2$). Technically, the goal of an ISA$^2$ model is to predict for a given image of a driving scenario the proper speed of the vehicle. Note this problem is different from predicting the actual speed of the vehicle. It defines a novel regression problem where the appearance information has to be directly mapped to get a prediction for the speed at which the vehicle should go, taking into account the traffic situation. First, we release a novel dataset for the new problem, where multiple driving video sequences, with the annotated adequate speed per frame, are provided. We then introduce two deep learning based ISA$^2$ models, which are trained to perform the final regression of the proper speed given a test image. We end with a thorough experimental validation where the results show the level of difficulty of the proposed task. The dataset and the proposed models will all be made publicly available to encourage much needed further research on this problem.



### One-Shot High-Fidelity Imitation: Training Large-Scale Deep Nets with RL
- **Arxiv ID**: http://arxiv.org/abs/1810.05017v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1810.05017v1)
- **Published**: 2018-10-11 13:46:18+00:00
- **Updated**: 2018-10-11 13:46:18+00:00
- **Authors**: Tom Le Paine, Sergio Gómez Colmenarejo, Ziyu Wang, Scott Reed, Yusuf Aytar, Tobias Pfaff, Matt W. Hoffman, Gabriel Barth-Maron, Serkan Cabi, David Budden, Nando de Freitas
- **Comment**: None
- **Journal**: None
- **Summary**: Humans are experts at high-fidelity imitation -- closely mimicking a demonstration, often in one attempt. Humans use this ability to quickly solve a task instance, and to bootstrap learning of new tasks. Achieving these abilities in autonomous agents is an open problem. In this paper, we introduce an off-policy RL algorithm (MetaMimic) to narrow this gap. MetaMimic can learn both (i) policies for high-fidelity one-shot imitation of diverse novel skills, and (ii) policies that enable the agent to solve tasks more efficiently than the demonstrators. MetaMimic relies on the principle of storing all experiences in a memory and replaying these to learn massive deep neural network policies by off-policy RL. This paper introduces, to the best of our knowledge, the largest existing neural networks for deep RL and shows that larger networks with normalization are needed to achieve one-shot high-fidelity imitation on a challenging manipulation task. The results also show that both types of policy can be learned from vision, in spite of the task rewards being sparse, and without access to demonstrator actions.



### Deep Learning for Image Denoising: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1810.05052v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.05052v1)
- **Published**: 2018-10-11 14:43:43+00:00
- **Updated**: 2018-10-11 14:43:43+00:00
- **Authors**: Chunwei Tian, Yong Xu, Lunke Fei, Ke Yan
- **Comment**: None
- **Journal**: None
- **Summary**: Since the proposal of big data analysis and Graphic Processing Unit (GPU), the deep learning technology has received a great deal of attention and has been widely applied in the field of imaging processing. In this paper, we have an aim to completely review and summarize the deep learning technologies for image denoising proposed in recent years. Morever, we systematically analyze the conventional machine learning methods for image denoising. Finally, we point out some research directions for the deep learning technologies in image denoising.



### Learning Optimal Deep Projection of $^{18}$F-FDG PET Imaging for Early Differential Diagnosis of Parkinsonian Syndromes
- **Arxiv ID**: http://arxiv.org/abs/1810.05733v1
- **DOI**: 10.1007/978-3-030-00889-5_26
- **Categories**: **cs.CV**, I.2.10; I.2.4; I.4.10; I.2.1
- **Links**: [PDF](http://arxiv.org/pdf/1810.05733v1)
- **Published**: 2018-10-11 15:41:26+00:00
- **Updated**: 2018-10-11 15:41:26+00:00
- **Authors**: Shubham Kumar, Abhijit Guha Roy, Ping Wu, Sailesh Conjeti, R. S. Anand, Jian Wang, Igor Yakushev, Stefan Förster, Markus Schwaiger, Sung-Cheng Huang, Axel Rominger, Chuantao Zuo, Kuangyu Shi
- **Comment**: 8 pages, 3 figures, conference, MICCAI DLMIA, 2018
- **Journal**: Kumar, Shubham, et al. DLMIA, Springer, Cham, 2018. 227-235
- **Summary**: Several diseases of parkinsonian syndromes present similar symptoms at early stage and no objective widely used diagnostic methods have been approved until now. Positron emission tomography (PET) with $^{18}$F-FDG was shown to be able to assess early neuronal dysfunction of synucleinopathies and tauopathies. Tensor factorization (TF) based approaches have been applied to identify characteristic metabolic patterns for differential diagnosis. However, these conventional dimension-reduction strategies assume linear or multi-linear relationships inside data, and are therefore insufficient to distinguish nonlinear metabolic differences between various parkinsonian syndromes. In this paper, we propose a Deep Projection Neural Network (DPNN) to identify characteristic metabolic pattern for early differential diagnosis of parkinsonian syndromes. We draw our inspiration from the existing TF methods. The network consists of a (i) compression part: which uses a deep network to learn optimal 2D projections of 3D scans, and a (ii) classification part: which maps the 2D projections to labels. The compression part can be pre-trained using surplus unlabelled datasets. Also, as the classification part operates on these 2D projections, it can be trained end-to-end effectively with limited labelled data, in contrast to 3D approaches. We show that DPNN is more effective in comparison to existing state-of-the-art and plausible baselines.



### InfiNet: Fully Convolutional Networks for Infant Brain MRI Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1810.05735v1
- **DOI**: 10.1109/ISBI.2018.8363542
- **Categories**: **cs.CV**, I.2.10; I.2.4; I.4.10; I.2.1; I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/1810.05735v1)
- **Published**: 2018-10-11 16:05:00+00:00
- **Updated**: 2018-10-11 16:05:00+00:00
- **Authors**: Shubham Kumar, Sailesh Conjeti, Abhijit Guha Roy, Christian Wachinger, Nassir Navab
- **Comment**: 4 pages, 3 figures, conference, IEEE ISBI, 2018
- **Journal**: Kumar, Shubham, et al. ISBI, IEEE (2018)(pp. 145-148)
- **Summary**: We present a novel, parameter-efficient and practical fully convolutional neural network architecture, termed InfiNet, aimed at voxel-wise semantic segmentation of infant brain MRI images at iso-intense stage, which can be easily extended for other segmentation tasks involving multi-modalities. InfiNet consists of double encoder arms for T1 and T2 input scans that feed into a joint-decoder arm that terminates in the classification layer. The novelty of InfiNet lies in the manner in which the decoder upsamples lower resolution input feature map(s) from multiple encoder arms. Specifically, the pooled indices computed in the max-pooling layers of each of the encoder blocks are related to the corresponding decoder block to perform non-linear learning-free upsampling. The sparse maps are concatenated with intermediate encoder representations (skip connections) and convolved with trainable filters to produce dense feature maps. InfiNet is trained end-to-end to optimize for the Generalized Dice Loss, which is well-suited for high class imbalance. InfiNet achieves the whole-volume segmentation in under 50 seconds and we demonstrate competitive performance against multiple state-of-the art deep architectures and their multi-modal variants.



### Characterizing Adversarial Examples Based on Spatial Consistency Information for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1810.05162v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1810.05162v1)
- **Published**: 2018-10-11 17:03:44+00:00
- **Updated**: 2018-10-11 17:03:44+00:00
- **Authors**: Chaowei Xiao, Ruizhi Deng, Bo Li, Fisher Yu, Mingyan Liu, Dawn Song
- **Comment**: Accepted to ECCV 2018
- **Journal**: None
- **Summary**: Deep Neural Networks (DNNs) have been widely applied in various recognition tasks. However, recently DNNs have been shown to be vulnerable against adversarial examples, which can mislead DNNs to make arbitrary incorrect predictions. While adversarial examples are well studied in classification tasks, other learning problems may have different properties. For instance, semantic segmentation requires additional components such as dilated convolutions and multiscale processing. In this paper, we aim to characterize adversarial examples based on spatial context information in semantic segmentation. We observe that spatial consistency information can be potentially leveraged to detect adversarial examples robustly even when a strong adaptive attacker has access to the model and detection strategies. We also show that adversarial examples based on attacks considered within the paper barely transfer among models, even though transferability is common in classification. Our observations shed new light on developing adversarial attacks and defenses to better understand the vulnerabilities of DNNs.



### Bottom-up Attention, Models of
- **Arxiv ID**: http://arxiv.org/abs/1810.05680v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.05680v3)
- **Published**: 2018-10-11 17:58:35+00:00
- **Updated**: 2019-04-25 03:26:50+00:00
- **Authors**: Ali Borji, Hamed R. Tavakoli, Zoya Bylinskii
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1810.03716
- **Journal**: None
- **Summary**: In this review, we examine the recent progress in saliency prediction and proposed several avenues for future research. In spite of tremendous efforts and huge progress, there is still room for improvement in terms finer-grained analysis of deep saliency models, evaluation measures, datasets, annotation methods, cognitive studies, and new applications. This chapter will appear in Encyclopedia of Computational Neuroscience.



### Bilinear Factor Matrix Norm Minimization for Robust PCA: Algorithms and Applications
- **Arxiv ID**: http://arxiv.org/abs/1810.05186v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, math.OC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.05186v1)
- **Published**: 2018-10-11 18:06:27+00:00
- **Updated**: 2018-10-11 18:06:27+00:00
- **Authors**: Fanhua Shang, James Cheng, Yuanyuan Liu, Zhi-Quan Luo, Zhouchen Lin
- **Comment**: 29 pages, 19 figures
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence,
  40(9): 2066-2080, 2018
- **Summary**: The heavy-tailed distributions of corrupted outliers and singular values of all channels in low-level vision have proven effective priors for many applications such as background modeling, photometric stereo and image alignment. And they can be well modeled by a hyper-Laplacian. However, the use of such distributions generally leads to challenging non-convex, non-smooth and non-Lipschitz problems, and makes existing algorithms very slow for large-scale applications. Together with the analytic solutions to lp-norm minimization with two specific values of p, i.e., p=1/2 and p=2/3, we propose two novel bilinear factor matrix norm minimization models for robust principal component analysis. We first define the double nuclear norm and Frobenius/nuclear hybrid norm penalties, and then prove that they are in essence the Schatten-1/2 and 2/3 quasi-norms, respectively, which lead to much more tractable and scalable Lipschitz optimization problems. Our experimental analysis shows that both our methods yield more accurate solutions than original Schatten quasi-norm minimization, even when the number of observations is very limited. Finally, we apply our penalties to various low-level vision problems, e.g., text removal, moving object detection, image alignment and inpainting, and show that our methods usually outperform the state-of-the-art methods.



### MeshAdv: Adversarial Meshes for Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/1810.05206v2
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.05206v2)
- **Published**: 2018-10-11 19:01:10+00:00
- **Updated**: 2019-06-29 19:43:54+00:00
- **Authors**: Chaowei Xiao, Dawei Yang, Bo Li, Jia Deng, Mingyan Liu
- **Comment**: Published in IEEE CVPR2019
- **Journal**: None
- **Summary**: Highly expressive models such as deep neural networks (DNNs) have been widely applied to various applications. However, recent studies show that DNNs are vulnerable to adversarial examples, which are carefully crafted inputs aiming to mislead the predictions. Currently, the majority of these studies have focused on perturbation added to image pixels, while such manipulation is not physically realistic. Some works have tried to overcome this limitation by attaching printable 2D patches or painting patterns onto surfaces, but can be potentially defended because 3D shape features are intact. In this paper, we propose meshAdv to generate "adversarial 3D meshes" from objects that have rich shape features but minimal textural variation. To manipulate the shape or texture of the objects, we make use of a differentiable renderer to compute accurate shading on the shape and propagate the gradient. Extensive experiments show that the generated 3D meshes are effective in attacking both classifiers and object detectors. We evaluate the attack under different viewpoints. In addition, we design a pipeline to perform black-box attack on a photorealistic renderer with unknown rendering parameters.



### FeatureLego: Volume Exploration Using Exhaustive Clustering of Super-Voxels
- **Arxiv ID**: http://arxiv.org/abs/1810.05220v2
- **DOI**: 10.1109/TVCG.2018.2856744
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.05220v2)
- **Published**: 2018-10-11 19:40:25+00:00
- **Updated**: 2019-10-19 12:30:23+00:00
- **Authors**: Shreeraj Jadhav, Saad Nadeem, Arie Kaufman
- **Comment**: IEEE Transactions on Visualization and Computer Graphics, 2018 (12
  pages, 11 figures). Supplementary video demonstrating FeatureLego can be
  found here: https://www.youtube.com/watch?v=y_a3VnACXfE
- **Journal**: IEEE Transactions on Visualization and Computer Graphics (Volume:
  25, Issue: 9, Pages: 2725 - 2737, Sept. 1 2019)
- **Summary**: We present a volume exploration framework, FeatureLego, that uses a novel voxel clustering approach for efficient selection of semantic features. We partition the input volume into a set of compact super-voxels that represent the finest selection granularity. We then perform an exhaustive clustering of these super-voxels using a graph-based clustering method. Unlike the prevalent brute-force parameter sampling approaches, we propose an efficient algorithm to perform this exhaustive clustering. By computing an exhaustive set of clusters, we aim to capture as many boundaries as possible and ensure that the user has sufficient options for efficiently selecting semantically relevant features. Furthermore, we merge all the computed clusters into a single tree of meta-clusters that can be used for hierarchical exploration. We implement an intuitive user-interface to interactively explore volumes using our clustering approach. Finally, we show the effectiveness of our framework on multiple real-world datasets of different modalities.



### Rethinking the Value of Network Pruning
- **Arxiv ID**: http://arxiv.org/abs/1810.05270v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.05270v2)
- **Published**: 2018-10-11 22:15:28+00:00
- **Updated**: 2019-03-05 05:58:11+00:00
- **Authors**: Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, Trevor Darrell
- **Comment**: ICLR 2019. Significant revisions from the previous version
- **Journal**: None
- **Summary**: Network pruning is widely used for reducing the heavy inference cost of deep models in low-resource settings. A typical pruning algorithm is a three-stage pipeline, i.e., training (a large model), pruning and fine-tuning. During pruning, according to a certain criterion, redundant weights are pruned and important weights are kept to best preserve the accuracy. In this work, we make several surprising observations which contradict common beliefs. For all state-of-the-art structured pruning algorithms we examined, fine-tuning a pruned model only gives comparable or worse performance than training that model with randomly initialized weights. For pruning algorithms which assume a predefined target network architecture, one can get rid of the full pipeline and directly train the target network from scratch. Our observations are consistent for multiple network architectures, datasets, and tasks, which imply that: 1) training a large, over-parameterized model is often not necessary to obtain an efficient final model, 2) learned "important" weights of the large model are typically not useful for the small pruned model, 3) the pruned architecture itself, rather than a set of inherited "important" weights, is more crucial to the efficiency in the final model, which suggests that in some cases pruning can be useful as an architecture search paradigm. Our results suggest the need for more careful baseline evaluations in future research on structured pruning methods. We also compare with the "Lottery Ticket Hypothesis" (Frankle & Carbin 2019), and find that with optimal learning rate, the "winning ticket" initialization as used in Frankle & Carbin (2019) does not bring improvement over random initialization.



### The highD Dataset: A Drone Dataset of Naturalistic Vehicle Trajectories on German Highways for Validation of Highly Automated Driving Systems
- **Arxiv ID**: http://arxiv.org/abs/1810.05642v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.IR, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.05642v1)
- **Published**: 2018-10-11 22:47:33+00:00
- **Updated**: 2018-10-11 22:47:33+00:00
- **Authors**: Robert Krajewski, Julian Bock, Laurent Kloeker, Lutz Eckstein
- **Comment**: IEEE International Conference on Intelligent Transportation Systems
  (ITSC) 2018
- **Journal**: None
- **Summary**: Scenario-based testing for the safety validation of highly automated vehicles is a promising approach that is being examined in research and industry. This approach heavily relies on data from real-world scenarios to derive the necessary scenario information for testing. Measurement data should be collected at a reasonable effort, contain naturalistic behavior of road users and include all data relevant for a description of the identified scenarios in sufficient quality. However, the current measurement methods fail to meet at least one of the requirements. Thus, we propose a novel method to measure data from an aerial perspective for scenario-based validation fulfilling the mentioned requirements. Furthermore, we provide a large-scale naturalistic vehicle trajectory dataset from German highways called highD. We evaluate the data in terms of quantity, variety and contained scenarios. Our dataset consists of 16.5 hours of measurements from six locations with 110 000 vehicles, a total driven distance of 45 000 km and 5600 recorded complete lane changes. The highD dataset is available online at: http://www.highD-dataset.com



