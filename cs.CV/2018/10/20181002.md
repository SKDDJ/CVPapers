# Arxiv Papers in cs.CV on 2018-10-02
### Reinforcement Learning with Perturbed Rewards
- **Arxiv ID**: http://arxiv.org/abs/1810.01032v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.01032v4)
- **Published**: 2018-10-02 01:43:45+00:00
- **Updated**: 2020-02-01 21:15:52+00:00
- **Authors**: Jingkang Wang, Yang Liu, Bo Li
- **Comment**: AAAI 2020 (Spotlight)
- **Journal**: None
- **Summary**: Recent studies have shown that reinforcement learning (RL) models are vulnerable in various noisy scenarios. For instance, the observed reward channel is often subject to noise in practice (e.g., when rewards are collected through sensors), and is therefore not credible. In addition, for applications such as robotics, a deep reinforcement learning (DRL) algorithm can be manipulated to produce arbitrary errors by receiving corrupted rewards. In this paper, we consider noisy RL problems with perturbed rewards, which can be approximated with a confusion matrix. We develop a robust RL framework that enables agents to learn in noisy environments where only perturbed rewards are observed. Our solution framework builds on existing RL/DRL algorithms and firstly addresses the biased noisy reward setting without any assumptions on the true distribution (e.g., zero-mean Gaussian noise as made in previous works). The core ideas of our solution include estimating a reward confusion matrix and defining a set of unbiased surrogate rewards. We prove the convergence and sample complexity of our approach. Extensive experiments on different DRL platforms show that trained policies based on our estimated surrogate reward can achieve higher expected rewards, and converge faster than existing baselines. For instance, the state-of-the-art PPO algorithm is able to obtain 84.6% and 80.8% improvements on average score for five Atari games, with error rates as 10% and 30% respectively.



### Cloud Chaser: Real Time Deep Learning Computer Vision on Low Computing Power Devices
- **Arxiv ID**: http://arxiv.org/abs/1810.01069v2
- **DOI**: 10.1117/12.2523087
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.01069v2)
- **Published**: 2018-10-02 05:08:12+00:00
- **Updated**: 2020-11-08 22:00:53+00:00
- **Authors**: Zhengyi Luo, Austin Small, Liam Dugan, Stephen Lane
- **Comment**: Accepted to The 11th International Conference on Machine Vision (ICMV
  2018). Project site: https://zhengyiluo.github.io/projects/cloudchaser/
- **Journal**: None
- **Summary**: Internet of Things(IoT) devices, mobile phones, and robotic systems are often denied the power of deep learning algorithms due to their limited computing power. However, to provide time-critical services such as emergency response, home assistance, surveillance, etc, these devices often need real-time analysis of their camera data. This paper strives to offer a viable approach to integrate high-performance deep learning-based computer vision algorithms with low-resource and low-power devices by leveraging the computing power of the cloud. By offloading the computation work to the cloud, no dedicated hardware is needed to enable deep neural networks on existing low computing power devices. A Raspberry Pi based robot, Cloud Chaser, is built to demonstrate the power of using cloud computing to perform real-time vision tasks. Furthermore, to reduce latency and improve real-time performance, compression algorithms are proposed and evaluated for streaming real-time video frames to the cloud.



### NU-LiteNet: Mobile Landmark Recognition using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1810.01074v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.01074v1)
- **Published**: 2018-10-02 05:27:22+00:00
- **Updated**: 2018-10-02 05:27:22+00:00
- **Authors**: Chakkrit Termritthikun, Surachet Kanprachar, Paisarn Muneesawang
- **Comment**: 6 pages, 7 figures, this paper presented to NVIDIA's GPU Technology
  Conference (GTC 2017), San Jose McEnery Convention Center, San Jose, CA
- **Journal**: None
- **Summary**: The growth of high-performance mobile devices has resulted in more research into on-device image recognition. The research problems are the latency and accuracy of automatic recognition, which remains obstacles to its real-world usage. Although the recently developed deep neural networks can achieve accuracy comparable to that of a human user, some of them still lack the necessary latency. This paper describes the development of the architecture of a new convolutional neural network model, NU-LiteNet. For this, SqueezeNet was developed to reduce the model size to a degree suitable for smartphones. The model size of NU-LiteNet is therefore 2.6 times smaller than that of SqueezeNet. The recognition accuracy of NU-LiteNet also compared favorably with other recently developed deep neural networks, when experiments were conducted on two standard landmark databases.



### Ancient Coin Classification Using Graph Transduction Games
- **Arxiv ID**: http://arxiv.org/abs/1810.01091v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.01091v1)
- **Published**: 2018-10-02 07:00:46+00:00
- **Updated**: 2018-10-02 07:00:46+00:00
- **Authors**: Sinem Aslan, Sebastiano Vascon, Marcello Pelillo
- **Comment**: None
- **Journal**: None
- **Summary**: Recognizing the type of an ancient coin requires theoretical expertise and years of experience in the field of numismatics. Our goal in this work is automatizing this time consuming and demanding task by a visual classification framework. Specifically, we propose to model ancient coin image classification using Graph Transduction Games (GTG). GTG casts the classification problem as a non-cooperative game where the players (the coin images) decide their strategies (class labels) according to the choices made by the others, which results with a global consensus at the final labeling. Experiments are conducted on the only publicly available dataset which is composed of 180 images of 60 types of Roman coins. We demonstrate that our approach outperforms the literature work on the same dataset with the classification accuracy of 73.6% and 87.3% when there are one and two images per class in the training set, respectively.



### Semi-supervised and Active-learning Scenarios: Efficient Acoustic Model Refinement for a Low Resource Indian Language
- **Arxiv ID**: http://arxiv.org/abs/1810.06635v1
- **DOI**: 10.21437/Interspeech.2018-2486
- **Categories**: **cs.CL**, cs.CV, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1810.06635v1)
- **Published**: 2018-10-02 07:23:42+00:00
- **Updated**: 2018-10-02 07:23:42+00:00
- **Authors**: Maharajan Chellapriyadharshini, Anoop Toffy, Srinivasa Raghavan K. M., V Ramasubramanian
- **Comment**: None
- **Journal**: Proc. Interspeech 2018
- **Summary**: We address the problem of efficient acoustic-model refinement (continuous retraining) using semi-supervised and active learning for a low resource Indian language, wherein the low resource constraints are having i) a small labeled corpus from which to train a baseline `seed' acoustic model and ii) a large training corpus without orthographic labeling or from which to perform a data selection for manual labeling at low costs. The proposed semi-supervised learning decodes the unlabeled large training corpus using the seed model and through various protocols, selects the decoded utterances with high reliability using confidence levels (that correlate to the WER of the decoded utterances) and iterative bootstrapping. The proposed active learning protocol uses confidence level based metric to select the decoded utterances from the large unlabeled corpus for further labeling. The semi-supervised learning protocols can offer a WER reduction, from a poorly trained seed model, by as much as 50% of the best WER-reduction realizable from the seed model's WER, if the large corpus were labeled and used for acoustic-model training. The active learning protocols allow that only 60% of the entire training corpus be manually labeled, to reach the same performance as the entire data.



### Target Aware Network Adaptation for Efficient Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/1810.01104v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1810.01104v1)
- **Published**: 2018-10-02 08:01:48+00:00
- **Updated**: 2018-10-02 08:01:48+00:00
- **Authors**: Yang Zhong, Vladimir Li, Ryuzo Okada, Atsuto Maki
- **Comment**: Accepted by the ECCV'18 Workshops (2nd International Workshop on
  Compact and Efficient Feature Representation and Learning in Computer Vision)
- **Journal**: None
- **Summary**: This paper presents an automatic network adaptation method that finds a ConvNet structure well-suited to a given target task, e.g., image classification, for efficiency as well as accuracy in transfer learning. We call the concept target-aware transfer learning. Given only small-scale labeled data, and starting from an ImageNet pre-trained network, we exploit a scheme of removing its potential redundancy for the target task through iterative operations of filter-wise pruning and network optimization. The basic motivation is that compact networks are on one hand more efficient and should also be more tolerant, being less complex, against the risk of overfitting which would hinder the generalization of learned representations in the context of transfer learning. Further, unlike existing methods involving network simplification, we also let the scheme identify redundant portions across the entire network, which automatically results in a network structure adapted to the task at hand. We achieve this with a few novel ideas: (i) cumulative sum of activation statistics for each layer, and (ii) a priority evaluation of pruning across multiple layers. Experimental results by the method on five datasets (Flower102, CUB200-2011, Dog120, MIT67, and Stanford40) show favorable accuracies over the related state-of-the-art techniques while enhancing the computational and storage efficiency of the transferred model.



### Injective State-Image Mapping facilitates Visual Adversarial Imitation Learning
- **Arxiv ID**: http://arxiv.org/abs/1810.01108v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.01108v2)
- **Published**: 2018-10-02 08:22:41+00:00
- **Updated**: 2019-10-25 09:32:10+00:00
- **Authors**: Subhajit Chaudhury, Daiki Kimura, Asim Munawar, Ryuki Tachibana
- **Comment**: Updated the paper to match with version accepted at IEEE MMSP 2019
- **Journal**: None
- **Summary**: The growing use of virtual autonomous agents in applications like games and entertainment demands better control policies for natural-looking movements and actions. Unlike the conventional approach of hard-coding motion routines, we propose a deep learning method for obtaining control policies by directly mimicking raw video demonstrations. Previous methods in this domain rely on extracting low-dimensional features from expert videos followed by a separate hand-crafted reward estimation step. We propose an imitation learning framework that reduces the dependence on hand-engineered reward functions by jointly learning the feature extraction and reward estimation steps using Generative Adversarial Networks (GANs). Our main contribution in this paper is to show that under injective mapping between low-level joint state (angles and velocities) trajectories and corresponding raw video stream, performing adversarial imitation learning on video demonstrations is equivalent to learning from the state trajectories. Experimental results show that the proposed adversarial learning method from raw videos produces a similar performance to state-of-the-art imitation learning techniques while frequently outperforming existing hand-crafted video imitation methods. Furthermore, we show that our method can learn action policies by imitating video demonstrations on YouTube with similar performance to learned agents from true reward signals. Please see the supplementary video submission at https://ibm.biz/BdzzNA.



### AI Benchmark: Running Deep Neural Networks on Android Smartphones
- **Arxiv ID**: http://arxiv.org/abs/1810.01109v2
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1810.01109v2)
- **Published**: 2018-10-02 08:24:09+00:00
- **Updated**: 2018-10-15 08:09:24+00:00
- **Authors**: Andrey Ignatov, Radu Timofte, William Chou, Ke Wang, Max Wu, Tim Hartley, Luc Van Gool
- **Comment**: None
- **Journal**: None
- **Summary**: Over the last years, the computational power of mobile devices such as smartphones and tablets has grown dramatically, reaching the level of desktop computers available not long ago. While standard smartphone apps are no longer a problem for them, there is still a group of tasks that can easily challenge even high-end devices, namely running artificial intelligence algorithms. In this paper, we present a study of the current state of deep learning in the Android ecosystem and describe available frameworks, programming models and the limitations of running AI on smartphones. We give an overview of the hardware acceleration resources available on four main mobile chipset platforms: Qualcomm, HiSilicon, MediaTek and Samsung. Additionally, we present the real-world performance results of different mobile SoCs collected with AI Benchmark that are covering all main existing hardware configurations.



### Sinkhorn AutoEncoders
- **Arxiv ID**: http://arxiv.org/abs/1810.01118v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.01118v3)
- **Published**: 2018-10-02 08:43:08+00:00
- **Updated**: 2019-07-16 02:04:33+00:00
- **Authors**: Giorgio Patrini, Rianne van den Berg, Patrick Forré, Marcello Carioni, Samarth Bhargav, Max Welling, Tim Genewein, Frank Nielsen
- **Comment**: Accepted for oral presentation at UAI19
- **Journal**: None
- **Summary**: Optimal transport offers an alternative to maximum likelihood for learning generative autoencoding models. We show that minimizing the p-Wasserstein distance between the generator and the true data distribution is equivalent to the unconstrained min-min optimization of the p-Wasserstein distance between the encoder aggregated posterior and the prior in latent space, plus a reconstruction error. We also identify the role of its trade-off hyperparameter as the capacity of the generator: its Lipschitz constant. Moreover, we prove that optimizing the encoder over any class of universal approximators, such as deterministic neural networks, is enough to come arbitrarily close to the optimum. We therefore advertise this framework, which holds for any metric space and prior, as a sweet-spot of current generative autoencoding objectives. We then introduce the Sinkhorn auto-encoder (SAE), which approximates and minimizes the p-Wasserstein distance in latent space via backprogation through the Sinkhorn algorithm. SAE directly works on samples, i.e. it models the aggregated posterior as an implicit distribution, with no need for a reparameterization trick for gradients estimations. SAE is thus able to work with different metric spaces and priors with minimal adaptations. We demonstrate the flexibility of SAE on latent spaces with different geometries and priors and compare with other methods on benchmark data sets.



### Training compact deep learning models for video classification using circulant matrices
- **Arxiv ID**: http://arxiv.org/abs/1810.01140v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.01140v2)
- **Published**: 2018-10-02 09:45:15+00:00
- **Updated**: 2018-10-08 08:40:40+00:00
- **Authors**: Alexandre Araujo, Benjamin Negrevergne, Yann Chevaleyre, Jamal Atif
- **Comment**: The 2nd Workshop on YouTube-8M Large-Scale Video Understanding, ECCV
  2018
- **Journal**: None
- **Summary**: In real world scenarios, model accuracy is hardly the only factor to consider. Large models consume more memory and are computationally more intensive, which makes them difficult to train and to deploy, especially on mobile devices. In this paper, we build on recent results at the crossroads of Linear Algebra and Deep Learning which demonstrate how imposing a structure on large weight matrices can be used to reduce the size of the model. We propose very compact models for video classification based on state-of-the-art network architectures such as Deep Bag-of-Frames, NetVLAD and NetFisherVectors. We then conduct thorough experiments using the large YouTube-8M video classification dataset. As we will show, the circulant DBoF embedding achieves an excellent trade-off between size and accuracy.



### Know What Your Neighbors Do: 3D Semantic Segmentation of Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1810.01151v2
- **DOI**: 10.1007/978-3-030-11015-4_29
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.01151v2)
- **Published**: 2018-10-02 10:06:13+00:00
- **Updated**: 2018-12-08 15:15:08+00:00
- **Authors**: Francis Engelmann, Theodora Kontogianni, Jonas Schult, Bastian Leibe
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a deep learning architecture which addresses the problem of 3D semantic segmentation of unstructured point clouds. Compared to previous work, we introduce grouping techniques which define point neighborhoods in the initial world space and the learned feature space. Neighborhoods are important as they allow to compute local or global point features depending on the spatial extend of the neighborhood. Additionally, we incorporate dedicated loss functions to further structure the learned point feature space: the pairwise distance loss and the centroid loss. We show how to apply these mechanisms to the task of 3D semantic segmentation of point clouds and report state-of-the-art performance on indoor and outdoor datasets.



### Learning Discriminators as Energy Networks in Adversarial Learning
- **Arxiv ID**: http://arxiv.org/abs/1810.01152v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1810.01152v1)
- **Published**: 2018-10-02 10:06:32+00:00
- **Updated**: 2018-10-02 10:06:32+00:00
- **Authors**: Pingbo Pan, Yan Yan, Tianbao Yang, Yi Yang
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel framework for structured prediction via adversarial learning. Existing adversarial learning methods involve two separate networks, i.e., the structured prediction models and the discriminative models, in the training. The information captured by discriminative models complements that in the structured prediction models, but few existing researches have studied on utilizing such information to improve structured prediction models at the inference stage. In this work, we propose to refine the predictions of structured prediction models by effectively integrating discriminative models into the prediction. Discriminative models are treated as energy-based models. Similar to the adversarial learning, discriminative models are trained to estimate scores which measure the quality of predicted outputs, while structured prediction models are trained to predict contrastive outputs with maximal energy scores. In this way, the gradient vanishing problem is ameliorated, and thus we are able to perform inference by following the ascent gradient directions of discriminative models to refine structured prediction models. The proposed method is able to handle a range of tasks, e.g., multi-label classification and image segmentation. Empirical results on these two tasks validate the effectiveness of our learning method.



### An Entropic Optimal Transport Loss for Learning Deep Neural Networks under Label Noise in Remote Sensing Images
- **Arxiv ID**: http://arxiv.org/abs/1810.01163v1
- **DOI**: 10.1016/j.cviu.2019.102863
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.01163v1)
- **Published**: 2018-10-02 10:31:37+00:00
- **Updated**: 2018-10-02 10:31:37+00:00
- **Authors**: Bharath Bhushan Damodaran, Rémi Flamary, Viven Seguy, Nicolas Courty
- **Comment**: Under Consideration at Computer Vision and Image Understanding
- **Journal**: Computer Vision and Image Understanding, Volume 191, 2020, 102863,
  ISSN 1077-3142
- **Summary**: Deep neural networks have established as a powerful tool for large scale supervised classification tasks. The state-of-the-art performances of deep neural networks are conditioned to the availability of large number of accurately labeled samples. In practice, collecting large scale accurately labeled datasets is a challenging and tedious task in most scenarios of remote sensing image analysis, thus cheap surrogate procedures are employed to label the dataset. Training deep neural networks on such datasets with inaccurate labels easily overfits to the noisy training labels and degrades the performance of the classification tasks drastically. To mitigate this effect, we propose an original solution with entropic optimal transportation. It allows to learn in an end-to-end fashion deep neural networks that are, to some extent, robust to inaccurately labeled samples. We empirically demonstrate on several remote sensing datasets, where both scene and pixel-based hyperspectral images are considered for classification. Our method proves to be highly tolerant to significant amounts of label noise and achieves favorable results against state-of-the-art methods.



### Adversarial Examples - A Complete Characterisation of the Phenomenon
- **Arxiv ID**: http://arxiv.org/abs/1810.01185v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1810.01185v2)
- **Published**: 2018-10-02 11:54:51+00:00
- **Updated**: 2019-02-17 21:48:42+00:00
- **Authors**: Alexandru Constantin Serban, Erik Poll, Joost Visser
- **Comment**: None
- **Journal**: None
- **Summary**: We provide a complete characterisation of the phenomenon of adversarial examples - inputs intentionally crafted to fool machine learning models. We aim to cover all the important concerns in this field of study: (1) the conjectures on the existence of adversarial examples, (2) the security, safety and robustness implications, (3) the methods used to generate and (4) protect against adversarial examples and (5) the ability of adversarial examples to transfer between different machine learning models. We provide ample background information in an effort to make this document self-contained. Therefore, this document can be used as survey, tutorial or as a catalog of attacks and defences using adversarial examples.



### Characterization of Visual Object Representations in Rat Primary Visual Cortex
- **Arxiv ID**: http://arxiv.org/abs/1810.01193v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.01193v1)
- **Published**: 2018-10-02 12:08:32+00:00
- **Updated**: 2018-10-02 12:08:32+00:00
- **Authors**: Sebastiano Vascon, Ylenia Parin, Eis Annavini, Mattia D'Andola, Davide Zoccolan, Marcello Pelillo
- **Comment**: Accepted at Brain Driven Computer Vision Workshop at ECCV 2018
- **Journal**: None
- **Summary**: For most animal species, quick and reliable identification of visual objects is critical for survival. This applies also to rodents, which, in recent years, have become increasingly popular models of visual functions. For this reason in this work we analyzed how various properties of visual objects are represented in rat primary visual cortex (V1). The analysis has been carried out through supervised (classification) and unsupervised (clustering) learning methods. We assessed quantitatively the discrimination capabilities of V1 neurons by demonstrating how photometric properties (luminosity and object position in the scene) can be derived directly from the neuronal responses.



### Post-training 4-bit quantization of convolution networks for rapid-deployment
- **Arxiv ID**: http://arxiv.org/abs/1810.05723v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.05723v3)
- **Published**: 2018-10-02 15:10:44+00:00
- **Updated**: 2019-05-29 08:45:02+00:00
- **Authors**: Ron Banner, Yury Nahshan, Elad Hoffer, Daniel Soudry
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks require significant memory bandwidth and storage for intermediate computations, apart from substantial computing resources. Neural network quantization has significant benefits in reducing the amount of intermediate results, but it often requires the full datasets and time-consuming fine tuning to recover the accuracy lost after quantization. This paper introduces the first practical 4-bit post training quantization approach: it does not involve training the quantized model (fine-tuning), nor it requires the availability of the full dataset. We target the quantization of both activations and weights and suggest three complementary methods for minimizing quantization error at the tensor level, two of whom obtain a closed-form analytical solution. Combining these methods, our approach achieves accuracy that is just a few percents less the state-of-the-art baseline across a wide range of convolutional models. The source code to replicate all experiments is available on GitHub: \url{https://github.com/submission2019/cnn-quantization}.



### FutureGAN: Anticipating the Future Frames of Video Sequences using Spatio-Temporal 3d Convolutions in Progressively Growing GANs
- **Arxiv ID**: http://arxiv.org/abs/1810.01325v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.01325v2)
- **Published**: 2018-10-02 15:30:25+00:00
- **Updated**: 2018-11-26 16:14:45+00:00
- **Authors**: Sandra Aigner, Marco Körner
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a new encoder-decoder GAN model, FutureGAN, that predicts future frames of a video sequence conditioned on a sequence of past frames. During training, the networks solely receive the raw pixel values as an input, without relying on additional constraints or dataset specific conditions. To capture both the spatial and temporal components of a video sequence, spatio-temporal 3d convolutions are used in all encoder and decoder modules. Further, we utilize concepts of the existing progressively growing GAN (PGGAN) that achieves high-quality results on generating high-resolution single images. The FutureGAN model extends this concept to the complex task of video prediction. We conducted experiments on three different datasets, MovingMNIST, KTH Action, and Cityscapes. Our results show that the model learned representations to transform the information of an input sequence into a plausible future sequence effectively for all three datasets. The main advantage of the FutureGAN framework is that it is applicable to various different datasets without additional changes, whilst achieving stable results that are competitive to the state-of-the-art in video prediction. Our code is available at https://github.com/TUM-LMF/FutureGAN.



### On Self Modulation for Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1810.01365v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.01365v2)
- **Published**: 2018-10-02 16:50:28+00:00
- **Updated**: 2019-05-02 07:20:50+00:00
- **Authors**: Ting Chen, Mario Lucic, Neil Houlsby, Sylvain Gelly
- **Comment**: None
- **Journal**: None
- **Summary**: Training Generative Adversarial Networks (GANs) is notoriously challenging. We propose and study an architectural modification, self-modulation, which improves GAN performance across different data sets, architectures, losses, regularizers, and hyperparameter settings. Intuitively, self-modulation allows the intermediate feature maps of a generator to change as a function of the input noise vector. While reminiscent of other conditioning techniques, it requires no labeled data. In a large-scale empirical study we observe a relative decrease of $5\%-35\%$ in FID. Furthermore, all else being equal, adding this modification to the generator leads to improved performance in $124/144$ ($86\%$) of the studied settings. Self-modulation is a simple architectural change that requires no additional parameter tuning, which suggests that it can be applied readily to any GAN.



### FFJORD: Free-form Continuous Dynamics for Scalable Reversible Generative Models
- **Arxiv ID**: http://arxiv.org/abs/1810.01367v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.01367v3)
- **Published**: 2018-10-02 16:56:37+00:00
- **Updated**: 2018-10-22 17:56:45+00:00
- **Authors**: Will Grathwohl, Ricky T. Q. Chen, Jesse Bettencourt, Ilya Sutskever, David Duvenaud
- **Comment**: 8 Pages, 6 figures
- **Journal**: None
- **Summary**: A promising class of generative models maps points from a simple distribution to a complex distribution through an invertible neural network. Likelihood-based training of these models requires restricting their architectures to allow cheap computation of Jacobian determinants. Alternatively, the Jacobian trace can be used if the transformation is specified by an ordinary differential equation. In this paper, we use Hutchinson's trace estimator to give a scalable unbiased estimate of the log-density. The result is a continuous-time invertible generative model with unbiased density estimation and one-pass sampling, while allowing unrestricted neural network architectures. We demonstrate our approach on high-dimensional density estimation, image generation, and variational inference, achieving the state-of-the-art among exact likelihood methods with efficient sampling.



### Semi-dense Stereo Matching using Dual CNNs
- **Arxiv ID**: http://arxiv.org/abs/1810.01369v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.01369v1)
- **Published**: 2018-10-02 16:57:58+00:00
- **Updated**: 2018-10-02 16:57:58+00:00
- **Authors**: Wendong Mao, Mingjie Wang, Jun Zhou, Minglun Gong
- **Comment**: None
- **Journal**: None
- **Summary**: A robust solution for semi-dense stereo matching is presented. It utilizes two CNN models for computing stereo matching cost and performing confidence-based filtering, respectively. Compared to existing CNNs-based matching cost generation approaches, our method feeds additional global information into the network so that the learned model can better handle challenging cases, such as lighting changes and lack of textures. Through utilizing non-parametric transforms, our method is also more self-reliant than most existing semi-dense stereo approaches, which rely highly on the adjustment of parameters. The experimental results based on Middlebury Stereo dataset demonstrate that the proposed approach outperforms the state-of-the-art semi-dense stereo approaches.



### Multi-scale Convolution Aggregation and Stochastic Feature Reuse for DenseNets
- **Arxiv ID**: http://arxiv.org/abs/1810.01373v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.01373v1)
- **Published**: 2018-10-02 17:07:35+00:00
- **Updated**: 2018-10-02 17:07:35+00:00
- **Authors**: Mingjie Wang, Jun Zhou, Wendong Mao, Minglun Gong
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, Convolution Neural Networks (CNNs) obtained huge success in numerous vision tasks. In particular, DenseNets have demonstrated that feature reuse via dense skip connections can effectively alleviate the difficulty of training very deep networks and that reusing features generated by the initial layers in all subsequent layers has strong impact on performance. To feed even richer information into the network, a novel adaptive Multi-scale Convolution Aggregation module is presented in this paper. Composed of layers for multi-scale convolutions, trainable cross-scale aggregation, maxout, and concatenation, this module is highly non-linear and can boost the accuracy of DenseNet while using much fewer parameters. In addition, due to high model complexity, the network with extremely dense feature reuse is prone to overfitting. To address this problem, a regularization method named Stochastic Feature Reuse is also presented. Through randomly dropping a set of feature maps to be reused for each mini-batch during the training phase, this regularization method reduces training costs and prevents co-adaptation. Experimental results on CIFAR-10, CIFAR-100 and SVHN benchmarks demonstrated the effectiveness of the proposed methods.



### Super-Resolution via Conditional Implicit Maximum Likelihood Estimation
- **Arxiv ID**: http://arxiv.org/abs/1810.01406v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.GR, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.01406v1)
- **Published**: 2018-10-02 17:58:02+00:00
- **Updated**: 2018-10-02 17:58:02+00:00
- **Authors**: Ke Li, Shichong Peng, Jitendra Malik
- **Comment**: 12 pages, 7 figures
- **Journal**: None
- **Summary**: Single-image super-resolution (SISR) is a canonical problem with diverse applications. Leading methods like SRGAN produce images that contain various artifacts, such as high-frequency noise, hallucinated colours and shape distortions, which adversely affect the realism of the result. In this paper, we propose an alternative approach based on an extension of the method of Implicit Maximum Likelihood Estimation (IMLE). We demonstrate greater effectiveness at noise reduction and preservation of the original colours and shapes, yielding more realistic super-resolved images.



### Scientific image rendering for space scenes with the SurRender software
- **Arxiv ID**: http://arxiv.org/abs/1810.01423v1
- **DOI**: None
- **Categories**: **astro-ph.IM**, astro-ph.EP, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1810.01423v1)
- **Published**: 2018-10-02 18:00:02+00:00
- **Updated**: 2018-10-02 18:00:02+00:00
- **Authors**: Roland Brochard, Jérémy Lebreton, Cyril Robin, Keyvan Kanani, Grégory Jonniaux, Aurore Masson, Noela Despré, Ahmad Berjaoui
- **Comment**: 11 pages, 10 figures, 69th International Astronautical Congress
  (IAC), Bremen, Germany, 1-5 October 2018,
  https://www.airbus.com/space/space-exploration/SurRenderSoftware.html
- **Journal**: None
- **Summary**: Spacecraft autonomy can be enhanced by vision-based navigation (VBN) techniques. Applications range from manoeuvers around Solar System objects and landing on planetary surfaces, to in-orbit servicing or space debris removal. The development and validation of VBN algorithms relies on the availability of physically accurate relevant images. Yet archival data from past missions can rarely serve this purpose and acquiring new data is often costly. The SurRender software is an image simulator that addresses the challenges of realistic image rendering, with high representativeness for space scenes. Images are rendered by raytracing, which implements the physical principles of geometrical light propagation, in physical units. A macroscopic instrument model and scene objects reflectance functions are used. SurRender is specially optimized for space scenes, with huge distances between objects and scenes up to Solar System size. Raytracing conveniently tackles some important effects for VBN algorithms: image quality, eclipses, secondary illumination, subpixel limb imaging, etc. A simulation is easily setup (in MATLAB, Python, and more) by specifying the position of the bodies (camera, Sun, planets, satellites) over time, 3D shapes and material surface properties. SurRender comes with its own modelling tool enabling to go beyond existing models for shapes, materials and sensors (projection, temporal sampling, electronics, etc.). It is natively designed to simulate different kinds of sensors (visible, LIDAR, etc.). Tools are available for manipulating huge datasets to store albedo maps and digital elevation models, or for procedural (fractal) texturing that generates high-quality images for a large range of observing distances (from millions of km to touchdown). We illustrate SurRender performances with a selection of case studies, placing particular emphasis on a 900-km Moon flyby simulation.



### Representation Flow for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1810.01455v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.01455v3)
- **Published**: 2018-10-02 18:57:45+00:00
- **Updated**: 2019-08-01 18:51:22+00:00
- **Authors**: AJ Piergiovanni, Michael S. Ryoo
- **Comment**: CVPR 2019
- **Journal**: CVPR 2019
- **Summary**: In this paper, we propose a convolutional layer inspired by optical flow algorithms to learn motion representations. Our representation flow layer is a fully-differentiable layer designed to capture the `flow' of any representation channel within a convolutional neural network for action recognition. Its parameters for iterative flow optimization are learned in an end-to-end fashion together with the other CNN model parameters, maximizing the action recognition performance. Furthermore, we newly introduce the concept of learning `flow of flow' representations by stacking multiple representation flow layers. We conducted extensive experimental evaluations, confirming its advantages over previous recognition models using traditional optical flows in both computational speed and performance. Code/models available here: https://piergiaj.github.io/rep-flow-site/



### DeepCMB: Lensing Reconstruction of the Cosmic Microwave Background with Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1810.01483v3
- **DOI**: 10.1016/j.ascom.2019.100307
- **Categories**: **astro-ph.CO**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.01483v3)
- **Published**: 2018-10-02 20:04:07+00:00
- **Updated**: 2020-06-12 22:33:45+00:00
- **Authors**: João Caldeira, W. L. Kimmy Wu, Brian Nord, Camille Avestruz, Shubhendu Trivedi, Kyle T. Story
- **Comment**: 19 pages; LaTeX; 12 figures; changes to match published version
- **Journal**: Astronomy and Computing 28 100307 (2019)
- **Summary**: Next-generation cosmic microwave background (CMB) experiments will have lower noise and therefore increased sensitivity, enabling improved constraints on fundamental physics parameters such as the sum of neutrino masses and the tensor-to-scalar ratio r. Achieving competitive constraints on these parameters requires high signal-to-noise extraction of the projected gravitational potential from the CMB maps. Standard methods for reconstructing the lensing potential employ the quadratic estimator (QE). However, the QE performs suboptimally at the low noise levels expected in upcoming experiments. Other methods, like maximum likelihood estimators (MLE), are under active development. In this work, we demonstrate reconstruction of the CMB lensing potential with deep convolutional neural networks (CNN) - ie, a ResUNet. The network is trained and tested on simulated data, and otherwise has no physical parametrization related to the physical processes of the CMB and gravitational lensing. We show that, over a wide range of angular scales, ResUNets recover the input gravitational potential with a higher signal-to-noise ratio than the QE method, reaching levels comparable to analytic approximations of MLE methods. We demonstrate that the network outputs quantifiably different lensing maps when given input CMB maps generated with different cosmologies. We also show we can use the reconstructed lensing map for cosmological parameter estimation. This application of CNN provides a few innovations at the intersection of cosmology and machine learning. First, while training and regressing on images, we predict a continuous-variable field rather than discrete classes. Second, we are able to establish uncertainty measures for the network output that are analogous to standard methods. We expect this approach to excel in capturing hard-to-model non-Gaussian astrophysical foreground and noise contributions.



### Deep Decoder: Concise Image Representations from Untrained Non-convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1810.03982v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.03982v2)
- **Published**: 2018-10-02 20:07:07+00:00
- **Updated**: 2019-02-22 22:13:19+00:00
- **Authors**: Reinhard Heckel, Paul Hand
- **Comment**: International Conference on Learning Representations 2019
- **Journal**: None
- **Summary**: Deep neural networks, in particular convolutional neural networks, have become highly effective tools for compressing images and solving inverse problems including denoising, inpainting, and reconstruction from few and noisy measurements. This success can be attributed in part to their ability to represent and generate natural images well. Contrary to classical tools such as wavelets, image-generating deep neural networks have a large number of parameters---typically a multiple of their output dimension---and need to be trained on large datasets. In this paper, we propose an untrained simple image model, called the deep decoder, which is a deep neural network that can generate natural images from very few weight parameters. The deep decoder has a simple architecture with no convolutions and fewer weight parameters than the output dimensionality. This underparameterization enables the deep decoder to compress images into a concise set of network weights, which we show is on par with wavelet-based thresholding. Further, underparameterization provides a barrier to overfitting, allowing the deep decoder to have state-of-the-art performance for denoising. The deep decoder is simple in the sense that each layer has an identical structure that consists of only one upsampling unit, pixel-wise linear combination of channels, ReLU activation, and channelwise normalization. This simplicity makes the network amenable to theoretical analysis, and it sheds light on the aspects of neural networks that enable them to form effective signal representations.



### CINIC-10 is not ImageNet or CIFAR-10
- **Arxiv ID**: http://arxiv.org/abs/1810.03505v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.03505v1)
- **Published**: 2018-10-02 21:20:09+00:00
- **Updated**: 2018-10-02 21:20:09+00:00
- **Authors**: Luke N. Darlow, Elliot J. Crowley, Antreas Antoniou, Amos J. Storkey
- **Comment**: Dataset compilation, 9 pages, 11 figures, technical report
- **Journal**: None
- **Summary**: In this brief technical report we introduce the CINIC-10 dataset as a plug-in extended alternative for CIFAR-10. It was compiled by combining CIFAR-10 with images selected and downsampled from the ImageNet database. We present the approach to compiling the dataset, illustrate the example images for different classes, give pixel distributions for each part of the repository, and give some standard benchmarks for well known models. Details for download, usage, and compilation can be found in the associated github repository.



