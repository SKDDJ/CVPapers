# Arxiv Papers in cs.CV on 2018-10-22
### VIENA2: A Driving Anticipation Dataset
- **Arxiv ID**: http://arxiv.org/abs/1810.09044v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.09044v2)
- **Published**: 2018-10-22 00:21:28+00:00
- **Updated**: 2018-10-29 22:29:20+00:00
- **Authors**: Mohammad Sadegh Aliakbarian, Fatemeh Sadat Saleh, Mathieu Salzmann, Basura Fernando, Lars Petersson, Lars Andersson
- **Comment**: Accepted in ACCV 2018
- **Journal**: None
- **Summary**: Action anticipation is critical in scenarios where one needs to react before the action is finalized. This is, for instance, the case in automated driving, where a car needs to, e.g., avoid hitting pedestrians and respect traffic lights. While solutions have been proposed to tackle subsets of the driving anticipation tasks, by making use of diverse, task-specific sensors, there is no single dataset or framework that addresses them all in a consistent manner. In this paper, we therefore introduce a new, large-scale dataset, called VIENA2, covering 5 generic driving scenarios, with a total of 25 distinct action classes. It contains more than 15K full HD, 5s long videos acquired in various driving conditions, weathers, daytimes and environments, complemented with a common and realistic set of sensor measurements. This amounts to more than 2.25M frames, each annotated with an action label, corresponding to 600 samples per action class. We discuss our data acquisition strategy and the statistics of our dataset, and benchmark state-of-the-art action anticipation techniques, including a new multi-modal LSTM architecture with an effective loss function for action anticipation in driving scenarios.



### Block Matching Frame based Material Reconstruction for Spectral CT
- **Arxiv ID**: http://arxiv.org/abs/1810.10346v2
- **DOI**: 10.1088/1361-6560/ab51db
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.10346v2)
- **Published**: 2018-10-22 02:05:09+00:00
- **Updated**: 2019-10-30 11:01:23+00:00
- **Authors**: Weiwen Wu, Qian Wang, Fenglin Liu, Yining Zhu, Hengyong Yu
- **Comment**: More details can refer to
  https://iopscience.iop.org/article/10.1088/1361-6560/ab51db/pdf
- **Journal**: Physics in Medicine & Biology,2019
- **Summary**: Spectral computed tomography (CT) has a great potential in material identification and decomposition. To achieve high-quality material composition images and further suppress the x-ray beam hardening artifacts, we first propose a one-step material reconstruction model based on Taylor first-order expansion. Then, we develop a basic material reconstruction method named material simultaneous algebraic reconstruction technique (MSART). Considering the local similarity of each material image, we incorporate a powerful block matching frame (BMF) into the material reconstruction (MR) model and generate a BMF based MR (BMFMR) method. Because the BMFMR model contains the L0-norm problem, we adopt a split-Bregman method for optimization. The numerical simulation and physical phantom experiment results validate the correctness of the material reconstruction algorithms and demonstrate that the BMF regularization outperforms the total variation and no-local mean regularizations.



### Where is this? Video geolocation based on neural network features
- **Arxiv ID**: http://arxiv.org/abs/1810.09068v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1810.09068v2)
- **Published**: 2018-10-22 03:27:43+00:00
- **Updated**: 2018-10-23 00:51:00+00:00
- **Authors**: Salvador Medina, Zhuyun Dai, Yingkai Gao
- **Comment**: None
- **Journal**: None
- **Summary**: In this work we propose a method that geolocates videos within a delimited widespread area based solely on the frames visual content. Our proposed method tackles video-geolocation through traditional image retrieval techniques considering Google Street View as the reference point. To achieve this goal we use the deep learning features obtained from NetVLAD to represent images, since through this feature vectors the similarity is their L2 norm. In this paper, we propose a family of voting-based methods to aggregate frame-wise geolocation results which boost the video geolocation result. The best aggregation found through our experiments considers both NetVLAD and SIFT similarity, as well as the geolocation density of the most similar results. To test our proposed method, we gathered a new video dataset from Pittsburgh Downtown area to benefit and stimulate more work in this area. Our system achieved a precision of 90% while geolocating videos within a range of 150 meters or two blocks away from the original position.



### Atrial fibrosis quantification based on maximum likelihood estimator of multivariate images
- **Arxiv ID**: http://arxiv.org/abs/1810.09075v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.09075v1)
- **Published**: 2018-10-22 04:03:14+00:00
- **Updated**: 2018-10-22 04:03:14+00:00
- **Authors**: Fuping Wu, Lei Li, Guang Yang, Tom Wong, Raad Mohiaddin, David Firmin, Jennifer Keegan, Lingchao Xu, Xiahai Zhuang
- **Comment**: None
- **Journal**: None
- **Summary**: We present a fully-automated segmentation and quantification of the left atrial (LA) fibrosis and scars combining two cardiac MRIs, one is the target late gadolinium-enhanced (LGE) image, and the other is an anatomical MRI from the same acquisition session. We formulate the joint distribution of images using a multivariate mixture model (MvMM), and employ the maximum likelihood estimator (MLE) for texture classification of the images simultaneously. The MvMM can also embed transformations assigned to the images to correct the misregistration. The iterated conditional mode algorithm is adopted for optimization. This method first extracts the anatomical shape of the LA, and then estimates a prior probability map. It projects the resulting segmentation onto the LA surface, for quantification and analysis of scarring. We applied the proposed method to 36 clinical data sets and obtained promising results (Accuracy: $0.809\pm .150$, Dice: $0.556\pm.187$). We compared the method with the conventional algorithms and showed an evidently and statistically better performance ($p<0.03$).



### SG-One: Similarity Guidance Network for One-Shot Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1810.09091v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.09091v4)
- **Published**: 2018-10-22 05:30:04+00:00
- **Updated**: 2020-05-12 11:34:33+00:00
- **Authors**: Xiaolin Zhang, Yunchao Wei, Yi Yang, Thomas Huang
- **Comment**: None
- **Journal**: None
- **Summary**: One-shot image semantic segmentation poses a challenging task of recognizing the object regions from unseen categories with only one annotated example as supervision. In this paper, we propose a simple yet effective Similarity Guidance network to tackle the One-shot (SG-One) segmentation problem. We aim at predicting the segmentation mask of a query image with the reference to one densely labeled support image of the same category. To obtain the robust representative feature of the support image, we firstly adopt a masked average pooling strategy for producing the guidance features by only taking the pixels belonging to the support image into account. We then leverage the cosine similarity to build the relationship between the guidance features and features of pixels from the query image. In this way, the possibilities embedded in the produced similarity maps can be adapted to guide the process of segmenting objects. Furthermore, our SG-One is a unified framework which can efficiently process both support and query images within one network and be learned in an end-to-end manner. We conduct extensive experiments on Pascal VOC 2012. In particular, our SGOne achieves the mIoU score of 46.3%, surpassing the baseline methods.



### Can We Gain More from Orthogonality Regularizations in Training Deep CNNs?
- **Arxiv ID**: http://arxiv.org/abs/1810.09102v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.09102v1)
- **Published**: 2018-10-22 06:22:54+00:00
- **Updated**: 2018-10-22 06:22:54+00:00
- **Authors**: Nitin Bansal, Xiaohan Chen, Zhangyang Wang
- **Comment**: 11 pages, 1 figure, 2 tables. Accepted in NIPS 2018
- **Journal**: None
- **Summary**: This paper seeks to answer the question: as the (near-) orthogonality of weights is found to be a favorable property for training deep convolutional neural networks, how can we enforce it in more effective and easy-to-use ways? We develop novel orthogonality regularizations on training deep CNNs, utilizing various advanced analytical tools such as mutual coherence and restricted isometry property. These plug-and-play regularizations can be conveniently incorporated into training almost any CNN without extra hassle. We then benchmark their effects on state-of-the-art models: ResNet, WideResNet, and ResNeXt, on several most popular computer vision datasets: CIFAR-10, CIFAR-100, SVHN and ImageNet. We observe consistent performance gains after applying those proposed regularizations, in terms of both the final accuracies achieved, and faster and more stable convergences. We have made our codes and pre-trained models publicly available: https://github.com/nbansal90/Can-we-Gain-More-from-Orthogonality.



### Learning to Measure Change: Fully Convolutional Siamese Metric Networks for Scene Change Detection
- **Arxiv ID**: http://arxiv.org/abs/1810.09111v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.09111v3)
- **Published**: 2018-10-22 07:01:45+00:00
- **Updated**: 2018-11-12 03:16:22+00:00
- **Authors**: Enqiang Guo, Xinsha Fu, Jiawei Zhu, Min Deng, Yu Liu, Qing Zhu, Haifeng Li
- **Comment**: 10 pages, 12 figures
- **Journal**: None
- **Summary**: A critical challenge problem of scene change detection is that noisy changes generated by varying illumination, shadows and camera viewpoint make variances of a scene difficult to define and measure since the noisy changes and semantic ones are entangled. Following the intuitive idea of detecting changes by directly comparing dissimilarities between a pair of features, we propose a novel fully Convolutional siamese metric Network(CosimNet) to measure changes by customizing implicit metrics. To learn more discriminative metrics, we utilize contrastive loss to reduce the distance between the unchanged feature pairs and to enlarge the distance between the changed feature pairs. Specifically, to address the issue of large viewpoint differences, we propose Thresholded Contrastive Loss (TCL) with a more tolerant strategy to punish noisy changes. We demonstrate the effectiveness of the proposed approach with experiments on three challenging datasets: CDnet, PCD2015, and VL-CMU-CD. Our approach is robust to lots of challenging conditions, such as illumination changes, large viewpoint difference caused by camera motion and zooming. In addition, we incorporate the distance metric into the segmentation framework and validate the effectiveness through visualization of change maps and feature distribution. The source code is available at https://github.com/gmayday1997/ChangeDet.



### Boosted Convolutional Neural Networks for Motor Imagery EEG Decoding with Multiwavelet-based Time-Frequency Conditional Granger Causality Analysis
- **Arxiv ID**: http://arxiv.org/abs/1810.10353v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1810.10353v1)
- **Published**: 2018-10-22 07:39:12+00:00
- **Updated**: 2018-10-22 07:39:12+00:00
- **Authors**: Yang Li, Mengying Lei, Xianrui Zhang, Weigang Cui, Yuzhu Guo, Ting-Wen Huang, Hua-Liang Wei
- **Comment**: None
- **Journal**: None
- **Summary**: Decoding EEG signals of different mental states is a challenging task for brain-computer interfaces (BCIs) due to nonstationarity of perceptual decision processes. This paper presents a novel boosted convolutional neural networks (ConvNets) decoding scheme for motor imagery (MI) EEG signals assisted by the multiwavelet-based time-frequency (TF) causality analysis. Specifically, multiwavelet basis functions are first combined with Geweke spectral measure to obtain high-resolution TF-conditional Granger causality (CGC) representations, where a regularized orthogonal forward regression (ROFR) algorithm is adopted to detect a parsimonious model with good generalization performance. The causality images for network input preserving time, frequency and location information of connectivity are then designed based on the TF-CGC distributions of alpha band multichannel EEG signals. Further constructed boosted ConvNets by using spatio-temporal convolutions as well as advances in deep learning including cropping and boosting methods, to extract discriminative causality features and classify MI tasks. Our proposed approach outperforms the competition winner algorithm with 12.15% increase in average accuracy and 74.02% decrease in associated inter subject standard deviation for the same binary classification on BCI competition-IV dataset-IIa. Experiment results indicate that the boosted ConvNets with causality images works well in decoding MI-EEG signals and provides a promising framework for developing MI-BCI systems.



### Atrial scars segmentation via potential learning in the graph-cuts framework
- **Arxiv ID**: http://arxiv.org/abs/1810.09123v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.09123v1)
- **Published**: 2018-10-22 07:57:24+00:00
- **Updated**: 2018-10-22 07:57:24+00:00
- **Authors**: Lei Li, Fuping Wu, Guang Yang, Tom Wong, Raad Mohiaddin, David Firmin, Jenny Keegan, Lingchao Xu, Xiahai Zhuang
- **Comment**: 9 pages,4 figures,STACOM2018
- **Journal**: None
- **Summary**: Late Gadolinium Enhancement Magnetic Resonance Imaging (LGE MRI) emerged as a routine scan for patients with atrial fibrillation (AF). However, due to the low image quality automating the quantification and analysis of the atrial scars is challenging. In this study, we pro-posed a fully automated method based on the graph-cuts framework, where the potential of the graph is learned on a surface mesh of the left atrium (LA) using an equidistant projection and a Deep Neural Network (DNN). For validation, we employed 100 datasets with manual delineation. The results showed that the performance of the proposed method improved and converged with respect to the increased size of training patches, which provide important features of the structural and texture information learned by the DNN. The segmentation could be further improved when the contribution from the t-link and n-link is balanced, thanks to inter-relationship learned by the DNN for the graph-cuts algorithm. Compared with the published methods which mostly acquired manual delineation of the LA or LA wall, our method is fully automatic and demonstrated evidently better results with statistical significance. Finally, the accuracy of quantifying the scars assessed by the Dice score was 0.570. The results are promising and the method can be useful in diagnosis and prognosis of AF.



### Exploring Correlations in Multiple Facial Attributes through Graph Attention Network
- **Arxiv ID**: http://arxiv.org/abs/1810.09162v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.09162v1)
- **Published**: 2018-10-22 10:09:00+00:00
- **Updated**: 2018-10-22 10:09:00+00:00
- **Authors**: Yan Zhang, Li Sun
- **Comment**: 9 pages, 5 figures, summit to AAAI2019
- **Journal**: None
- **Summary**: Estimating multiple attributes from a single facial image gives comprehensive descriptions on the high level semantics of the face. It is naturally regarded as a multi-task supervised learning problem with a single deep CNN, in which lower layers are shared, and higher ones are task-dependent with the multi-branch structure. Within the traditional deep multi-task learning (DMTL) framework, this paper intends to fully exploit the correlations among different attributes by constructing a graph. The node in graph represents the feature vector from a particular branch for a given attribute, and the edge can be defined by either the prior knowledge or the similarity between two nodes in the embedding with a fully data-driven manner. We analyze that the attention mechanism actually takes effect in the latter case, and utilize the Graph Attention Layer (GAL) for exploring on the most relevant attribute feature and refining the task-dependant feature by considering other attributes. Experiments show that by mining the correlations among attributes, our method can improve the recognition accuracy on CelebA and LFWA dataset. And it also achieves competitive performance.



### Dating Ancient Paintings of Mogao Grottoes Using Deeply Learnt Visual Codes
- **Arxiv ID**: http://arxiv.org/abs/1810.09168v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.09168v1)
- **Published**: 2018-10-22 10:28:29+00:00
- **Updated**: 2018-10-22 10:28:29+00:00
- **Authors**: Qingquan Li, Qin Zou, De Ma, Qian Wang, Song Wang
- **Comment**: None
- **Journal**: Science China Information Sciences, 61(9), 092105 (2018)
- **Summary**: Cultural heritage is the asset of all the peoples of the world. The preservation and inheritance of cultural heritage is conducive to the progress of human civilization. In northwestern China, there is a world heritage site -- Mogao Grottoes -- that has a plenty of mural paintings showing the historical cultures of ancient China. To study these historical cultures, one critical procedure is to date the mural paintings, i.e., determining the era when they were created. Until now, most mural paintings at Mogao Grottoes have been dated by directly referring to the mural texts or historical documents. However, some are still left with creation-era undetermined due to the lack of reference materials. Considering that the drawing style of mural paintings was changing along the history and the drawing style can be learned and quantified through painting data, we formulate the problem of mural-painting dating into a problem of drawing-style classification. In fact, drawing styles can be expressed not only in color or curvature, but also in some unknown forms -- the forms that have not been observed. To this end, besides sophisticated color and shape descriptors, a deep convolution neural network is designed to encode the implicit drawing styles. 3860 mural paintings collected from 194 different grottoes with determined creation-era labels are used to train the classification model and build the dating method. In experiments, the proposed dating method is applied to seven mural paintings which were previously dated with controversies, and the exciting new dating results are approved by the Dunhuang expert.



### Field Of Interest Proposal for Augmented Mitotic Cell Count: Comparison of two Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1810.09197v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.09197v1)
- **Published**: 2018-10-22 12:06:29+00:00
- **Updated**: 2018-10-22 12:06:29+00:00
- **Authors**: Marc Aubreville, Christof A. Bertram, Robert Klopfleisch, Andreas Maier
- **Comment**: 8 pages, 7 figures and one table, submitted to BIOIMAGING 2019
- **Journal**: None
- **Summary**: Most tumor grading systems for human as for veterinary histopathology are based upon the absolute count of mitotic figures in a certain reference area of a histology slide. Since time for prognostication is limited in a diagnostic setting, the pathologist will often almost arbitrarily choose a certain field of interest assumed to have the highest mitotic activity. However, as mitotic figures are commonly very sparse on the slide and often have a patchy distribution, this poses a sampling problem which is known to be able to influence the tumor prognostication. On the other hand, automatic detection of mitotic figures can't yet be considered reliable enough for clinical application. In order to aid the work of the human expert and at the same time reduce variance in tumor grading, it is beneficial to assess the whole slide image (WSI) for the highest mitotic activity and use this as a reference region for human counting. For this task, we compare two methods for region of interest proposal, both based on convolutional neural networks (CNN). For both approaches, the CNN performs a segmentation of the WSI to assess mitotic activity. The first method performs a segmentation at the original image resolution, while the second approach performs a segmentation operation at a significantly reduced resolution, cutting down on processing complexity. We evaluate the approach using a dataset of 32 completely annotated whole slide images of canine mast cell tumors, where 22 were used for training of the network and 10 for test. Our results indicate that, while the overall correlation to the ground truth mitotic activity is considerably higher (0.94 vs. 0.83) for the approach based upon the fine resolution network, the field of interest choices are only marginally better. Both approaches propose fields of interest that contain a mitotic count in the upper quartile of respective slides.



### Implicit Modeling with Uncertainty Estimation for Intravoxel Incoherent Motion Imaging
- **Arxiv ID**: http://arxiv.org/abs/1810.10358v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.10358v1)
- **Published**: 2018-10-22 14:22:10+00:00
- **Updated**: 2018-10-22 14:22:10+00:00
- **Authors**: Lin Zhang, Valery Vishnevskiy, Andras Jakab, Orcun Goksel
- **Comment**: None
- **Journal**: None
- **Summary**: Intravoxel incoherent motion (IVIM) imaging allows contrast-agent free in vivo perfusion quantification with magnetic resonance imaging (MRI). However, its use is limited by typically low accuracy due to low signal-to-noise ratio (SNR) at large gradient encoding magnitudes as well as dephasing artefacts caused by subject motion, which is particularly challenging in fetal MRI. To mitigate this problem, we propose an implicit IVIM signal acquisition model with which we learn full posterior distribution of perfusion parameters using artificial neural networks. This posterior then encapsulates the uncertainty of the inferred parameter estimates, which we validate herein via numerical experiments with rejection-based Bayesian sampling. Compared to state-of-the-art IVIM estimation method of segmented least-squares fitting, our proposed approach improves parameter estimation accuracy by 65% on synthetic anisotropic perfusion data. On paired rescans of in vivo fetal MRI, our method increases repeatability of parameter estimation in placenta by 46%.



### Baseline Detection in Historical Documents using Convolutional U-Nets
- **Arxiv ID**: http://arxiv.org/abs/1810.09343v1
- **DOI**: 10.1109/DAS.2018.34
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1810.09343v1)
- **Published**: 2018-10-22 15:06:57+00:00
- **Updated**: 2018-10-22 15:06:57+00:00
- **Authors**: Michael Fink, Thomas Layer, Georg Mackenbrock, Michael Sprinzl
- **Comment**: 6 pages, accepted to DAS 2018
- **Journal**: Proc. of the 13th IAPR Int. Workshop on Document Analysis Systems
  (DAS 2018), IEEE Computer Society, pp. 37-42, 2018
- **Summary**: Baseline detection is still a challenging task for heterogeneous collections of historical documents. We present a novel approach to baseline extraction in such settings, turning out the winning entry to the ICDAR 2017 Competition on Baseline detection (cBAD). It utilizes deep convolutional nets (CNNs) for both, the actual extraction of baselines, as well as for a simple form of layout analysis in a pre-processing step. To the best of our knowledge it is the first CNN-based system for baseline extraction applying a U-net architecture and sliding window detection, profiting from a high local accuracy of the candidate lines extracted. Final baseline post-processing complements our approach, compensating for inaccuracies mainly due to missing context information during sliding window detection. We experimentally evaluate the components of our system individually on the cBAD dataset. Moreover, we investigate how it generalizes to different data by means of the dataset used for the baseline extraction task of the ICDAR 2017 Competition on Layout Analysis for Challenging Medieval Manuscripts (HisDoc). A comparison with the results reported for HisDoc shows that it also outperforms the contestants of the latter.



### Generation of Virtual Dual Energy Images from Standard Single-Shot Radiographs using Multi-scale and Conditional Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/1810.09354v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.09354v2)
- **Published**: 2018-10-22 15:17:38+00:00
- **Updated**: 2021-04-14 17:30:27+00:00
- **Authors**: Bo Zhou, Xunyu Lin, Brendan Eck, Jun Hou, David L. Wilson
- **Comment**: 16 pages, 7 figures, accepted by Asian Conference on Computer Vision
  (2018 ACCV), code available at
  https://github.com/bbbbbbzhou/Virtual-Dual-Energy
- **Journal**: None
- **Summary**: Dual-energy (DE) chest radiographs provide greater diagnostic information than standard radiographs by separating the image into bone and soft tissue, revealing suspicious lesions which may otherwise be obstructed from view. However, acquisition of DE images requires two physical scans, necessitating specialized hardware and processing, and images are prone to motion artifact. Generation of virtual DE images from standard, single-shot chest radiographs would expand the diagnostic value of standard radiographs without changing the acquisition procedure. We present a Multi-scale Conditional Adversarial Network (MCA-Net) which produces high-resolution virtual DE bone images from standard, single-shot chest radiographs. Our proposed MCA-Net is trained using the adversarial network so that it learns sharp details for the production of high-quality bone images. Then, the virtual DE soft tissue image is generated by processing the standard radiograph with the virtual bone image using a cross projection transformation. Experimental results from 210 patient DE chest radiographs demonstrated that the algorithm can produce high-quality virtual DE chest radiographs. Important structures were preserved, such as coronary calcium in bone images and lung lesions in soft tissue images. The average structure similarity index and the peak signal to noise ratio of the produced bone images in testing data were 96.4 and 41.5, which are significantly better than results from previous methods. Furthermore, our clinical evaluation results performed on the publicly available dataset indicates the clinical values of our algorithms. Thus, our algorithm can produce high-quality DE images that are potentially useful for radiologists, computer-aided diagnostics, and other diagnostic tasks.



### Brain Tumor Image Retrieval via Multitask Learning
- **Arxiv ID**: http://arxiv.org/abs/1810.09369v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.09369v1)
- **Published**: 2018-10-22 15:38:25+00:00
- **Updated**: 2018-10-22 15:38:25+00:00
- **Authors**: Maxim Pisov, Gleb Makarchuk, Valery Kostjuchenko, Alexandra Dalechina, Andrey Golanov, Mikhail Belyaev
- **Comment**: None
- **Journal**: None
- **Summary**: Classification-based image retrieval systems are built by training convolutional neural networks (CNNs) on a relevant classification problem and using the distance in the resulting feature space as a similarity metric. However, in practical applications, it is often desirable to have representations which take into account several aspects of the data (e.g., brain tumor type and its localization). In our work, we extend the classification-based approach with multitask learning: we train a CNN on brain MRI scans with heterogeneous labels and implement a corresponding tumor image retrieval system. We validate our approach on brain tumor data which contains information about tumor types, shapes and localization. We show that our method allows us to build representations that contain more relevant information about tumors than single-task classification-based approaches.



### Unsupervised Learning of Shape and Pose with Differentiable Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1810.09381v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1810.09381v1)
- **Published**: 2018-10-22 16:01:20+00:00
- **Updated**: 2018-10-22 16:01:20+00:00
- **Authors**: Eldar Insafutdinov, Alexey Dosovitskiy
- **Comment**: None
- **Journal**: None
- **Summary**: We address the problem of learning accurate 3D shape and camera pose from a collection of unlabeled category-specific images. We train a convolutional network to predict both the shape and the pose from a single image by minimizing the reprojection error: given several views of an object, the projections of the predicted shapes to the predicted camera poses should match the provided views. To deal with pose ambiguity, we introduce an ensemble of pose predictors which we then distill to a single "student" model. To allow for efficient learning of high-fidelity shapes, we represent the shapes by point clouds and devise a formulation allowing for differentiable projection of these. Our experiments show that the distilled ensemble of pose predictors learns to estimate the pose accurately, while the point cloud representation allows to predict detailed shape models. The supplementary video can be found at https://www.youtube.com/watch?v=LuIGovKeo60



### Visual Rendering of Shapes on 2D Display Devices Guided by Hand Gestures
- **Arxiv ID**: http://arxiv.org/abs/1810.10581v1
- **DOI**: 10.1016/j.displa.2019.03.001
- **Categories**: **cs.HC**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.10581v1)
- **Published**: 2018-10-22 16:59:52+00:00
- **Updated**: 2018-10-22 16:59:52+00:00
- **Authors**: Abhik Singla, Partha Pratim Roy, Debi Prosad Dogra
- **Comment**: Submitted to Elsevier Displays Journal, 32 pages, 18 figures, 7
  tables
- **Journal**: None
- **Summary**: Designing of touchless user interface is gaining popularity in various contexts. Using such interfaces, users can interact with electronic devices even when the hands are dirty or non-conductive. Also, user with partial physical disability can interact with electronic devices using such systems. Research in this direction has got major boost because of the emergence of low-cost sensors such as Leap Motion, Kinect or RealSense devices. In this paper, we propose a Leap Motion controller-based methodology to facilitate rendering of 2D and 3D shapes on display devices. The proposed method tracks finger movements while users perform natural gestures within the field of view of the sensor. In the next phase, trajectories are analyzed to extract extended Npen++ features in 3D. These features represent finger movements during the gestures and they are fed to unidirectional left-to-right Hidden Markov Model (HMM) for training. A one-to-one mapping between gestures and shapes is proposed. Finally, shapes corresponding to these gestures are rendered over the display using MuPad interface. We have created a dataset of 5400 samples recorded by 10 volunteers. Our dataset contains 18 geometric and 18 non-geometric shapes such as "circle", "rectangle", "flower", "cone", "sphere" etc. The proposed methodology achieves an accuracy of 92.87% when evaluated using 5-fold cross validation method. Our experiments revel that the extended 3D features perform better than existing 3D features in the context of shape representation and classification. The method can be used for developing useful HCI applications for smart display devices.



### Single Image Haze Removal using a Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/1810.09479v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1810.09479v2)
- **Published**: 2018-10-22 18:04:50+00:00
- **Updated**: 2020-08-29 19:04:07+00:00
- **Authors**: Bharath Raj N., Venkateswaran N
- **Comment**: Accepted for the WiSPNET 2020 conference. Please refer to the GitHub
  repository for information on updates to the paper:
  https://github.com/thatbrguy/Dehaze-GAN
- **Journal**: None
- **Summary**: Traditional methods to remove haze from images rely on estimating a transmission map. When dealing with single images, this becomes an ill-posed problem due to the lack of depth information. In this paper, we propose an end-to-end learning based approach which uses a modified conditional Generative Adversarial Network to directly remove haze from an image. We employ the usage of the Tiramisu model in place of the classic U-Net model as the generator owing to its higher parameter efficiency and performance. Moreover, a patch based discriminator was used to reduce artefacts in the output. To further improve the perceptual quality of the output, a hybrid weighted loss function was designed and used to train the model. Experiments on synthetic and real world hazy images demonstrates that our model performs competitively with the state of the art methods.



### Diagnostic Accuracy of Content Based Dermatoscopic Image Retrieval with Deep Classification Features
- **Arxiv ID**: http://arxiv.org/abs/1810.09487v1
- **DOI**: 10.1111/bjd.17189
- **Categories**: **cs.CV**, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1810.09487v1)
- **Published**: 2018-10-22 18:20:01+00:00
- **Updated**: 2018-10-22 18:20:01+00:00
- **Authors**: Philipp Tschandl, Giuseppe Argenziano, Majid Razmara, Jordan Yap
- **Comment**: None
- **Journal**: Tschandl P, Argenziano G, Razmara M, Yap J. Diagnostic Accuracy of
  Content Based Dermatoscopic Image Retrieval with Deep Classification
  Features. Br J Dermatol 2018 Sep 12. doi: 10.1111/bjd.17189
- **Summary**: Background: Automated classification of medical images through neural networks can reach high accuracy rates but lack interpretability.   Objectives: To compare the diagnostic accuracy obtained by using content based image retrieval (CBIR) to retrieve visually similar dermatoscopic images with corresponding disease labels against predictions made by a neural network.   Methods: A neural network was trained to predict disease classes on dermatoscopic images from three retrospectively collected image datasets containing 888, 2750 and 16691 images respectively. Diagnosis predictions were made based on the most commonly occurring diagnosis in visually similar images, or based on the top-1 class prediction of the softmax output from the network. Outcome measures were area under the ROC curve for predicting a malignant lesion (AUC), multiclass-accuracy and mean average precision (mAP), measured on unseen test images of the corresponding dataset.   Results: In all three datasets the skin cancer predictions from CBIR (evaluating the 16 most similar images) showed AUC values similar to softmax predictions (0.842, 0.806 and 0.852 versus 0.830, 0.810 and 0.847 respectively; p-value>0.99 for all). Similarly, the multiclass-accuracy of CBIR was comparable to softmax predictions. Networks trained for detecting only 3 classes performed better on a dataset with 8 classes when using CBIR as compared to softmax predictions (mAP 0.184 vs. 0.368 and 0.198 vs. 0.403 respectively).   Conclusions: Presenting visually similar images based on features from a neural network shows comparable accuracy to the softmax probability-based diagnoses of convolutional neural networks. CBIR may be more helpful than a softmax classifier in improving diagnostic accuracy of clinicians in a routine clinical setting.



### Hierarchical multi-class segmentation of glioma images using networks with multi-level activation function
- **Arxiv ID**: http://arxiv.org/abs/1810.09488v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.09488v2)
- **Published**: 2018-10-22 18:22:34+00:00
- **Updated**: 2018-11-02 09:51:19+00:00
- **Authors**: Xiaobin Hu, Hongwei Li, Yu Zhao, Chao Dong, Bjoern H. Menze, Marie Piraud
- **Comment**: 12pages first version
- **Journal**: None
- **Summary**: For many segmentation tasks, especially for the biomedical image, the topological prior is vital information which is useful to exploit. The containment/nesting is a typical inter-class geometric relationship. In the MICCAI Brain tumor segmentation challenge, with its three hierarchically nested classes 'whole tumor', 'tumor core', 'active tumor', the nested classes relationship is introduced into the 3D-residual-Unet architecture. The network comprises a context aggregation pathway and a localization pathway, which encodes increasingly abstract representation of the input as going deeper into the network, and then recombines these representations with shallower features to precisely localize the interest domain via a localization path. The nested-class-prior is combined by proposing the multi-class activation function and its corresponding loss function. The model is trained on the training dataset of Brats2018, and 20% of the dataset is regarded as the validation dataset to determine parameters. When the parameters are fixed, we retrain the model on the whole training dataset. The performance achieved on the validation leaderboard is 86%, 77% and 72% Dice scores for the whole tumor, enhancing tumor and tumor core classes without relying on ensembles or complicated post-processing steps. Based on the same start-of-the-art network architecture, the accuracy of nested-class (enhancing tumor) is reasonably improved from 69% to 72% compared with the traditional Softmax-based method which blind to topological prior.



### Two view constraints on the epipoles from few correspondences
- **Arxiv ID**: http://arxiv.org/abs/1810.09496v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.09496v1)
- **Published**: 2018-10-22 18:37:36+00:00
- **Updated**: 2018-10-22 18:37:36+00:00
- **Authors**: Yoni Kasten, Michael Werman
- **Comment**: None
- **Journal**: None
- **Summary**: In general it requires at least 7 point correspondences to compute the fundamental matrix between views. We use the cross ratio invariance between corresponding epipolar lines, stemming from epipolar line homography, to derive a simple formulation for the relationship between epipoles and corresponding points. We show how it can be used to reduce the number of required points for the epipolar geometry when some information about the epipoles is available and demonstrate this with a buddy search app.



### A Comparative Study of Fruit Detection and Counting Methods for Yield Mapping in Apple Orchards
- **Arxiv ID**: http://arxiv.org/abs/1810.09499v2
- **DOI**: 10.1002/rob.21902
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1810.09499v2)
- **Published**: 2018-10-22 18:45:34+00:00
- **Updated**: 2019-03-06 05:21:56+00:00
- **Authors**: Nicolai Häni, Pravakar Roy, Volkan Isler
- **Comment**: 28 pages
- **Journal**: None
- **Summary**: We present new methods for apple detection and counting based on recent deep learning approaches and compare them with state-of-the-art results based on classical methods. Our goal is to quantify performance improvements by neural network-based methods compared to methods based on classical approaches. Additionally, we introduce a complete system for counting apples in an entire row. This task is challenging as it requires tracking fruits in images from both sides of the row. We evaluate the performances of three fruit detection methods and two fruit counting methods on six datasets. Results indicate that the classical detection approach still outperforms the deep learning based methods in the majority of the datasets. For fruit counting though, the deep learning based approach performs better for all of the datasets. Combining the classical detection method together with the neural network based counting approach, we achieve remarkable yield accuracies ranging from 95.56% to 97.83%.



### A Weakly Supervised Approach for Estimating Spatial Density Functions from High-Resolution Satellite Imagery
- **Arxiv ID**: http://arxiv.org/abs/1810.09528v1
- **DOI**: 10.1145/3274895.3274934
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.09528v1)
- **Published**: 2018-10-22 20:14:46+00:00
- **Updated**: 2018-10-22 20:14:46+00:00
- **Authors**: Nathan Jacobs, Adam Kraft, Muhammad Usman Rafique, Ranti Dev Sharma
- **Comment**: 10 pages, 8 figures. ACM SIGSPATIAL 2018, Seattle, USA
- **Journal**: 26th ACM SIGSPATIAL International Conference on Advances in
  Geographic Information Systems (SIGSPATIAL 18), 2018, Seattle, WA, USA
- **Summary**: We propose a neural network component, the regional aggregation layer, that makes it possible to train a pixel-level density estimator using only coarse-grained density aggregates, which reflect the number of objects in an image region. Our approach is simple to use and does not require domain-specific assumptions about the nature of the density function. We evaluate our approach on several synthetic datasets. In addition, we use this approach to learn to estimate high-resolution population and housing density from satellite imagery. In all cases, we find that our approach results in better density estimates than a commonly used baseline. We also show how our housing density estimator can be used to classify buildings as residential or non-residential.



### Bioresorbable Scaffold Visualization in IVOCT Images Using CNNs and Weakly Supervised Localization
- **Arxiv ID**: http://arxiv.org/abs/1810.09578v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.09578v1)
- **Published**: 2018-10-22 22:06:09+00:00
- **Updated**: 2018-10-22 22:06:09+00:00
- **Authors**: Nils Gessert, Sarah Latus, Youssef S. Abdelwahed, David M. Leistner, Matthias Lutz, Alexander Schlaefer
- **Comment**: Accepted at SPIE: Medical Imaging 2019
- **Journal**: None
- **Summary**: Bioresorbable scaffolds have become a popular choice for treatment of coronary heart disease, replacing traditional metal stents. Often, intravascular optical coherence tomography is used to assess potential malapposition after implantation and for follow-up examinations later on. Typically, the scaffold is manually reviewed by an expert, analyzing each of the hundreds of image slices. As this is time consuming, automatic stent detection and visualization approaches have been proposed, mostly for metal stent detection based on classic image processing. As bioresorbable scaffolds are harder to detect, recent approaches have used feature extraction and machine learning methods for automatic detection. However, these methods require detailed, pixel-level labels in each image slice and extensive feature engineering for the particular stent type which might limit the approaches' generalization capabilities. Therefore, we propose a deep learning-based method for bioresorbable scaffold visualization using only image-level labels. A convolutional neural network is trained to predict whether an image slice contains a metal stent, a bioresorbable scaffold, or no device. Then, we derive local stent strut information by employing weakly supervised localization using saliency maps with guided backpropagation. As saliency maps are generally diffuse and noisy, we propose a novel patch-based method with image shifting which allows for high resolution stent visualization. Our convolutional neural network model achieves a classification accuracy of 99.0 % for image-level stent classification which can be used for both high quality in-slice stent visualization and 3D rendering of the stent structure.



### Two-path 3D CNNs for calibration of system parameters for OCT-based motion compensation
- **Arxiv ID**: http://arxiv.org/abs/1810.09582v1
- **DOI**: 10.1117/12.2512823
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.09582v1)
- **Published**: 2018-10-22 22:14:38+00:00
- **Updated**: 2018-10-22 22:14:38+00:00
- **Authors**: Nils Gessert, Martin Gromniak, Matthias Schlüter, Alexander Schlaefer
- **Comment**: Accepted at SPIE: Medical Imaging 2019
- **Journal**: None
- **Summary**: Automatic motion compensation and adjustment of an intraoperative imaging modality's field of view is a common problem during interventions. Optical coherence tomography (OCT) is an imaging modality which is used in interventions due to its high spatial resolution of few micrometers and its temporal resolution of potentially several hundred volumes per second. However, performing motion compensation with OCT is problematic due to its small field of view which might lead to tracked objects being lost quickly. We propose a novel deep learning-based approach that directly learns input parameters of motors that move the scan area for motion compensation from optical coherence tomography volumes. We design a two-path 3D convolutional neural network (CNN) architecture that takes two volumes with an object to be tracked as its input and predicts the necessary motor input parameters to compensate the object's movement. In this way, we learn the calibration between object movement and system parameters for motion compensation with arbitrary objects. Thus, we avoid error-prone hand-eye calibration and handcrafted feature tracking from classical approaches. We achieve an average correlation coefficient of 0.998 between predicted and ground-truth motor parameters which leads to sub-voxel accuracy. Furthermore, we show that our deep learning model is real-time capable for use with the system's high volume acquisition frequency.



