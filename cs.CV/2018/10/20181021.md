# Arxiv Papers in cs.CV on 2018-10-21
### A Regressive Convolution Neural network and Support Vector Regression Model for Electricity Consumption Forecasting
- **Arxiv ID**: http://arxiv.org/abs/1810.08878v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.08878v2)
- **Published**: 2018-10-21 02:12:11+00:00
- **Updated**: 2019-04-04 00:31:36+00:00
- **Authors**: Youshan Zhang, Qi Li
- **Comment**: Future of Information and Communications Conference (FICC) 2019
- **Journal**: None
- **Summary**: Electricity consumption forecasting has important implications for the mineral companies on guiding quarterly work, normal power system operation, and the management. However, electricity consumption prediction for the mineral company is different from traditional electricity load prediction since mineral company electricity consumption can be affected by various factors (e.g., ore grade, processing quantity of the crude ore, ball milling fill rate). The problem is non-trivial due to three major challenges for traditional methods: insufficient training data, high computational cost and low prediction accu-racy. To tackle these challenges, we firstly propose a Regressive Convolution Neural Network (RCNN) to predict the electricity consumption. While RCNN still suffers from high computation overhead, we utilize RCNN to extract features from the history data and Regressive Support Vector Machine (SVR) trained with the features to predict the electricity consumption. The experimental results show that the proposed RCNN-SVR model achieves higher accuracy than using the traditional RNN or SVM alone. The MSE, MAPE, and CV-RMSE of RCNN-SVR model are 0.8564, 1.975%, and 0.0687% respectively, which illustrates the low predicting error rate of the proposed model.



### Automated identification of hookahs (waterpipes) on Instagram: an application in feature extraction using Convolutional Neural Network and Support Vector Machine classification
- **Arxiv ID**: http://arxiv.org/abs/1810.08881v1
- **DOI**: 10.2196/10513
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.08881v1)
- **Published**: 2018-10-21 02:48:36+00:00
- **Updated**: 2018-10-21 02:48:36+00:00
- **Authors**: Youshan Zhang, Jon-Patrick Allem, Jennifer B. Unger, Tess Boley Cruz
- **Comment**: Journal of Medical Internet Research
- **Journal**: None
- **Summary**: Background: Instagram, with millions of posts per day, can be used to inform public health surveillance targets and policies. However, current research relying on image-based data often relies on hand coding of images which is time consuming and costly, ultimately limiting the scope of the study. Current best practices in automated image classification (e.g., support vector machine (SVM), Backpropagation (BP) neural network, and artificial neural network) are limited in their capacity to accurately distinguish between objects within images. Objective: This study demonstrates how convolutional neural network (CNN) can be used to extract unique features within an image and how SVM can then be used to classify the image. Methods: Images of waterpipes or hookah (an emerging tobacco product possessing similar harms to that of cigarettes) were collected from Instagram and used in analyses (n=840). CNN was used to extract unique features from images identified to contain waterpipes. A SVM classifier was built to distinguish between images with and without waterpipes. Methods for image classification were then compared to show how a CNN + SVM classifier could improve accuracy. Results: As the number of the validated training images increased, the total number of extracted features increased. Additionally, as the number of features learned by the SVM classifier increased, the average level of accuracy increased. Overall, 99.5% of the 420 images classified were correctly identified as either hookah or non-hookah images. This level of accuracy was an improvement over earlier methods that used SVM, CNN or Bag of Features (BOF) alone. Conclusions: CNN extracts more features of the images allowing a SVM classifier to be better informed, resulting in higher accuracy compared with methods that extract fewer features. Future research can use this method to grow the scope of image-based studies.



### Learning Spectral Transform Network on 3D Surface for Non-rigid Shape Analysis
- **Arxiv ID**: http://arxiv.org/abs/1810.08950v1
- **DOI**: None
- **Categories**: **cs.CV**, 65D19
- **Links**: [PDF](http://arxiv.org/pdf/1810.08950v1)
- **Published**: 2018-10-21 13:56:51+00:00
- **Updated**: 2018-10-21 13:56:51+00:00
- **Authors**: Ruixuan Yu, Jian Sun, Huibin Li
- **Comment**: 16 pages, 3 figures
- **Journal**: None
- **Summary**: Designing a network on 3D surface for non-rigid shape analysis is a challenging task. In this work, we propose a novel spectral transform network on 3D surface to learn shape descriptors. The proposed network architecture consists of four stages: raw descriptor extraction, surface second-order pooling, mixture of power function-based spectral transform, and metric learning. The proposed network is simple and shallow. Quantitative experiments on challenging benchmarks show its effectiveness for non-rigid shape retrieval and classification, e.g., it achieved the highest accuracies on SHREC14, 15 datasets as well as the Range subset of SHREC17 dataset.



### Visualization Framework for Colonoscopy Videos
- **Arxiv ID**: http://arxiv.org/abs/1810.08998v1
- **DOI**: 10.1117/12.2216963
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.08998v1)
- **Published**: 2018-10-21 18:17:39+00:00
- **Updated**: 2018-10-21 18:17:39+00:00
- **Authors**: Saad Nadeem, Arie Kaufman
- **Comment**: SPIE Medical Imaging, 2016 (7 pages, 5 figures)
- **Journal**: SPIE Medical Imaging, Vol. 9786, p. 97861T, 2016
- **Summary**: We present a visualization framework for annotating and comparing colonoscopy videos, where these annotations can then be used for semi-automatic report generation at the end of the procedure. Currently, there are approximately 14 million colonoscopies performed every year in the US. In this work, we create a visualization tool to deal with the deluge of colonoscopy videos in a more effective way. We present an interactive visualization framework for the annotation and tagging of colonoscopy videos in an easy and intuitive way. These annotations and tags can later be used for report generation for electronic medical records and for comparison at an individual as well as group level. We also present important use cases and medical expert feedback for our visualization framework.



### Machine Learning Methods for Track Classification in the AT-TPC
- **Arxiv ID**: http://arxiv.org/abs/1810.10350v3
- **DOI**: 10.1016/j.nima.2019.05.097
- **Categories**: **cs.CV**, cs.LG, nucl-ex
- **Links**: [PDF](http://arxiv.org/pdf/1810.10350v3)
- **Published**: 2018-10-21 18:52:01+00:00
- **Updated**: 2019-04-30 01:54:11+00:00
- **Authors**: Michelle P. Kuchera, Raghuram Ramanujan, Jack Z. Taylor, Ryan R. Strauss, Daniel Bazin, Joshua Bradt, Ruiming Chen
- **Comment**: None
- **Journal**: NIMA 940 (2019) 56-167, ISSN 0168-9002
- **Summary**: We evaluate machine learning methods for event classification in the Active-Target Time Projection Chamber detector at the National Superconducting Cyclotron Laboratory (NSCL) at Michigan State University. An automated method to single out the desired reaction product would result in more accurate physics results as well as a faster analysis process. Binary and multi-class classification methods were tested on data produced by the $^{46}$Ar(p,p) experiment run at the NSCL in September 2015. We found a Convolutional Neural Network to be the most successful classifier of proton scattering events for transfer learning. Results from this investigation and recommendations for event classification in future experiments are presented.



### 3D shape retrieval basing on representatives of classes
- **Arxiv ID**: http://arxiv.org/abs/1810.09008v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.09008v3)
- **Published**: 2018-10-21 19:11:12+00:00
- **Updated**: 2018-12-27 21:14:15+00:00
- **Authors**: M. Benjelloun, E. W. Dadi, E. M. Daoudi
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present an improvement of our proposed technique for 3D shape retrieval in classified databases [2] which is based on representatives of classes. Instead of systematically matching the object-query with all 3D models of the database, our idea presented in [2] consist, for a classified database, to represent each class by one representative that is used to orient the retrieval process to the right class (the class excepted to contain 3D models similar to the query). In order to increase the chance to fall in the right class, our idea in this work is to represent each class by more than one representative. In this case, instead of using only one representative to decide which is the right class we use a set of representatives this will contribute certainly to improving the relevance of retrieval results. The obtained experimental results show that the relevance is significantly improved.



### C2A: Crowd Consensus Analytics for Virtual Colonoscopy
- **Arxiv ID**: http://arxiv.org/abs/1810.09012v1
- **DOI**: 10.1109/VAST.2016.7883508
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.09012v1)
- **Published**: 2018-10-21 19:33:33+00:00
- **Updated**: 2018-10-21 19:33:33+00:00
- **Authors**: Ji Hwan Park, Saad Nadeem, Seyedkoosha Mirhosseini, Arie Kaufman
- **Comment**: IEEE Conference on Visual Analytics Science and Technology (VAST),
  pp. 21-30, 2016 (10 pages, 11 figures)
- **Journal**: IEEE Conference on Visual Analytics Science and Technology (VAST),
  pp. 21-30, 2016
- **Summary**: We present a medical crowdsourcing visual analytics platform called C{$^2$}A to visualize, classify and filter crowdsourced clinical data. More specifically, C$^2$A is used to build consensus on a clinical diagnosis by visualizing crowd responses and filtering out anomalous activity. Crowdsourcing medical applications have recently shown promise where the non-expert users (the crowd) were able to achieve accuracy similar to the medical experts. This has the potential to reduce interpretation/reading time and possibly improve accuracy by building a consensus on the findings beforehand and letting the medical experts make the final diagnosis. In this paper, we focus on a virtual colonoscopy (VC) application with the clinical technicians as our target users, and the radiologists acting as consultants and classifying segments as benign or malignant. In particular, C$^2$A is used to analyze and explore crowd responses on video segments, created from fly-throughs in the virtual colon. C$^2$A provides several interactive visualization components to build crowd consensus on video segments, to detect anomalies in the crowd data and in the VC video segments, and finally, to improve the non-expert user's work quality and performance by A/B testing for the optimal crowdsourcing platform and application-specific parameters. Case studies and domain experts feedback demonstrate the effectiveness of our framework in improving workers' output quality, the potential to reduce the radiologists' interpretation time, and hence, the potential to improve the traditional clinical workflow by marking the majority of the video segments as benign based on the crowd consensus.



### Hierarchical ResNeXt Models for Breast Cancer Histology Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1810.09025v1
- **DOI**: 10.1007/978-3-319-93000-8_90
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.09025v1)
- **Published**: 2018-10-21 20:55:41+00:00
- **Updated**: 2018-10-21 20:55:41+00:00
- **Authors**: Ismaël Koné, Lahsen Boulmane
- **Comment**: None
- **Journal**: Image Analysis and Recognition. ICIAR 2018. LNCS, vol 10882.
  Springer, Cham
- **Summary**: Microscopic histology image analysis is a cornerstone in early detection of breast cancer. However these images are very large and manual analysis is error prone and very time consuming. Thus automating this process is in high demand. We proposed a hierarchical system of convolutional neural networks (CNN) that classifies automatically patches of these images into four pathologies: normal, benign, in situ carcinoma and invasive carcinoma. We evaluated our system on the BACH challenge dataset of image-wise classification and a small dataset that we used to extend it. Using a train/test split of 75%/25%, we achieved an accuracy rate of 0.99 on the test split for the BACH dataset and 0.96 on that of the extension. On the test of the BACH challenge, we've reached an accuracy of 0.81 which rank us to the 8th out of 51 teams.



### Digital holographic particle volume reconstruction using a deep neural network
- **Arxiv ID**: http://arxiv.org/abs/1810.09444v1
- **DOI**: 10.1364/AO.58.001900
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1810.09444v1)
- **Published**: 2018-10-21 23:25:44+00:00
- **Updated**: 2018-10-21 23:25:44+00:00
- **Authors**: Tomoyoshi Shimobaba, Takayuki Takahashi, Yota Yamamoto, Yutaka Endo, Atsushi Shiraki, Takashi Nishitsuji, Naoto Hoshikawa, Takashi Kakue, Tomoyosh Ito
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a particle volume reconstruction directly from an in-line hologram using a deep neural network. Digital holographic volume reconstruction conventionally uses multiple diffraction calculations to obtain sectional reconstructed images from an in-line hologram, followed by detection of the lateral and axial positions, and the sizes of particles by using focus metrics. However, the axial resolution is limited by the numerical aperture of the optical system, and the processes are time-consuming. The method proposed here can simultaneously detect the lateral and axial positions, and the particle sizes via a deep neural network (DNN). We numerically investigated the performance of the DNN in terms of the errors in the detected positions and sizes. The calculation time is faster than conventional diffracted-based approaches.



### Dermatologist Level Dermoscopy Skin Cancer Classification Using Different Deep Learning Convolutional Neural Networks Algorithms
- **Arxiv ID**: http://arxiv.org/abs/1810.10348v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.10348v1)
- **Published**: 2018-10-21 23:27:59+00:00
- **Updated**: 2018-10-21 23:27:59+00:00
- **Authors**: Amirreza Rezvantalab, Habib Safigholi, Somayeh Karimijeshni
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, the effectiveness and capability of convolutional neural networks have been studied in the classification of 8 skin diseases. Different pre-trained state-of-the-art architectures (DenseNet 201, ResNet 152, Inception v3, InceptionResNet v2) were used and applied on 10135 dermoscopy skin images in total (HAM10000: 10015, PH2: 120). The utilized dataset includes 8 diagnostic categories - melanoma, melanocytic nevi, basal cell carcinoma, benign keratosis, actinic keratosis and intraepithelial carcinoma, dermatofibroma, vascular lesions, and atypical nevi. The aim is to compare the ability of deep learning with the performance of highly trained dermatologists. Overall, the mean results show that all deep learning models outperformed dermatologists (at least 11%). The best ROC AUC values for melanoma and basal cell carcinoma are 94.40% (ResNet 152) and 99.30% (DenseNet 201) versus 82.26% and 88.82% of dermatologists, respectively. Also, DenseNet 201 had the highest macro and micro averaged AUC values for overall classification (98.16%, 98.79%, respectively).



