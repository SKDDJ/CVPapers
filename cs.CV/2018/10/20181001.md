# Arxiv Papers in cs.CV on 2018-10-01
### Hybrid Noise Removal in Hyperspectral Imagery With a Spatial-Spectral Gradient Network
- **Arxiv ID**: http://arxiv.org/abs/1810.00495v3
- **DOI**: 10.1109/TGRS.2019.2912909
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.00495v3)
- **Published**: 2018-10-01 00:52:34+00:00
- **Updated**: 2019-04-01 08:02:41+00:00
- **Authors**: Qiang Zhang, Qiangqiang Yuan, Jie Li, Xinxin Liu, Huanfeng Shen, Liangpei Zhang
- **Comment**: Accept by IEEE TGRS
- **Journal**: None
- **Summary**: The existence of hybrid noise in hyperspectral images (HSIs) severely degrades the data quality, reduces the interpretation accuracy of HSIs, and restricts the subsequent HSIs applications. In this paper, the spatial-spectral gradient network (SSGN) is presented for mixed noise removal in HSIs. The proposed method employs a spatial-spectral gradient learning strategy, in consideration of the unique spatial structure directionality of sparse noise and spectral differences with additional complementary information for better extracting intrinsic and deep features of HSIs. Based on a fully cascaded multi-scale convolutional network, SSGN can simultaneously deal with the different types of noise in different HSIs or spectra by the use of the same model. The simulated and real-data experiments undertaken in this study confirmed that the proposed SSGN performs better at mixed noise removal than the other state-of-the-art HSI denoising algorithms, in evaluation indices, visual assessments, and time consumption.



### One Network to Solve All ROIs: Deep Learning CT for Any ROI using Differentiated Backprojection
- **Arxiv ID**: http://arxiv.org/abs/1810.00500v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.00500v2)
- **Published**: 2018-10-01 01:51:33+00:00
- **Updated**: 2019-05-27 04:26:34+00:00
- **Authors**: Yoseob Han, Jong Chul Ye
- **Comment**: Accepted by Medical Physics
- **Journal**: None
- **Summary**: Computed tomography for region-of-interest (ROI) reconstruction has advantages of reducing X-ray radiation dose and using a small detector. However, standard analytic reconstruction methods suffer from severe cupping artifacts, and existing model-based iterative reconstruction methods require extensive computations. Recently, we proposed a deep neural network to learn the cupping artifact, but the network is not well generalized for different ROIs due to the singularities in the corrupted images. Therefore, there is an increasing demand for a neural network that works well for any ROI sizes. In this paper, two types of neural networks are designed. The first type learns ROI size-specific cupping artifacts from the analytic reconstruction images, whereas the second type network is to learn to invert the finite Hilbert transform from the truncated differentiated backprojection (DBP) data. Their generalizability for any ROI sizes is then examined. Experimental results show that the new type of neural network significantly outperforms the existing iterative methods for any ROI size in spite of significantly reduced run-time complexity. Since the proposed method consistently surpasses existing methods for any ROIs, it can be used as a general CT reconstruction engine for many practical applications without compromising possible detector truncation.



### Layer-compensated Pruning for Resource-constrained Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1810.00518v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1810.00518v2)
- **Published**: 2018-10-01 03:41:25+00:00
- **Updated**: 2018-10-18 02:36:01+00:00
- **Authors**: Ting-Wu Chin, Cha Zhang, Diana Marculescu
- **Comment**: 11 pages, 8 figures, work in progress
- **Journal**: None
- **Summary**: Resource-efficient convolution neural networks enable not only the intelligence on edge devices but also opportunities in system-level optimization such as scheduling. In this work, we aim to improve the performance of resource-constrained filter pruning by merging two sub-problems commonly considered, i.e., (i) how many filters to prune for each layer and (ii) which filters to prune given a per-layer pruning budget, into a global filter ranking problem. Our framework entails a novel algorithm, dubbed layer-compensated pruning, where meta-learning is involved to determine better solutions. We show empirically that the proposed algorithm is superior to prior art in both effectiveness and efficiency. Specifically, we reduce the accuracy gap between the pruned and original networks from 0.9% to 0.7% with 8x reduction in time needed for meta-learning, i.e., from 1 hour down to 7 minutes. To this end, we demonstrate the effectiveness of our algorithm using ResNet and MobileNetV2 networks under CIFAR-10, ImageNet, and Bird-200 datasets.



### End-To-End Alzheimer's Disease Diagnosis and Biomarker Identification
- **Arxiv ID**: http://arxiv.org/abs/1810.00523v1
- **DOI**: 10.1007/978-3-030-00919-9_39
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.00523v1)
- **Published**: 2018-10-01 04:01:09+00:00
- **Updated**: 2018-10-01 04:01:09+00:00
- **Authors**: Soheil Esmaeilzadeh, Dimitrios Ioannis Belivanis, Kilian M. Pohl, Ehsan Adeli
- **Comment**: None
- **Journal**: Machine Learning in Medical Imaging. MLMI 2018. Lecture Notes in
  Computer Science, vol 11046. Springer, Cham
- **Summary**: As shown in computer vision, the power of deep learning lies in automatically learning relevant and powerful features for any perdition task, which is made possible through end-to-end architectures. However, deep learning approaches applied for classifying medical images do not adhere to this architecture as they rely on several pre- and post-processing steps. This shortcoming can be explained by the relatively small number of available labeled subjects, the high dimensionality of neuroimaging data, and difficulties in interpreting the results of deep learning methods. In this paper, we propose a simple 3D Convolutional Neural Networks and exploit its model parameters to tailor the end-to-end architecture for the diagnosis of Alzheimer's disease (AD). Our model can diagnose AD with an accuracy of 94.1\% on the popular ADNI dataset using only MRI data, which outperforms the previous state-of-the-art. Based on the learned model, we identify the disease biomarkers, the results of which were in accordance with the literature. We further transfer the learned model to diagnose mild cognitive impairment (MCI), the prodromal stage of AD, which yield better results compared to other methods.



### Interpretable Spatio-temporal Attention for Video Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1810.04511v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.04511v2)
- **Published**: 2018-10-01 04:23:35+00:00
- **Updated**: 2019-06-03 03:09:50+00:00
- **Authors**: Lili Meng, Bo Zhao, Bo Chang, Gao Huang, Wei Sun, Frederich Tung, Leonid Sigal
- **Comment**: None
- **Journal**: None
- **Summary**: Inspired by the observation that humans are able to process videos efficiently by only paying attention where and when it is needed, we propose an interpretable and easy plug-in spatial-temporal attention mechanism for video action recognition. For spatial attention, we learn a saliency mask to allow the model to focus on the most salient parts of the feature maps. For temporal attention, we employ a convolutional LSTM based attention mechanism to identify the most relevant frames from an input video. Further, we propose a set of regularizers to ensure that our attention mechanism attends to coherent regions in space and time. Our model not only improves video action recognition accuracy, but also localizes discriminative regions both spatially and temporally, despite being trained in a weakly-supervised manner with only classification labels (no bounding box labels or time frame temporal labels). We evaluate our approach on several public video action recognition datasets with ablation studies. Furthermore, we quantitatively and qualitatively evaluate our model's ability to localize discriminative regions spatially and critical frames temporally. Experimental results demonstrate the efficacy of our approach, showing superior or comparable accuracy with the state-of-the-art methods while increasing model interpretability.



### Densely Supervised Grasp Detector (DSGD)
- **Arxiv ID**: http://arxiv.org/abs/1810.03962v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.03962v2)
- **Published**: 2018-10-01 04:35:08+00:00
- **Updated**: 2019-01-30 03:09:38+00:00
- **Authors**: Umar Asif, Jianbin Tang, Stefan Harrer
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents Densely Supervised Grasp Detector (DSGD), a deep learning framework which combines CNN structures with layer-wise feature fusion and produces grasps and their confidence scores at different levels of the image hierarchy (i.e., global-, region-, and pixel-levels). % Specifically, at the global-level, DSGD uses the entire image information to predict a grasp. At the region-level, DSGD uses a region proposal network to identify salient regions in the image and predicts a grasp for each salient region. At the pixel-level, DSGD uses a fully convolutional network and predicts a grasp and its confidence at every pixel. % During inference, DSGD selects the most confident grasp as the output. This selection from hierarchically generated grasp candidates overcomes limitations of the individual models. % DSGD outperforms state-of-the-art methods on the Cornell grasp dataset in terms of grasp accuracy. % Evaluation on a multi-object dataset and real-world robotic grasping experiments show that DSGD produces highly stable grasps on a set of unseen objects in new environments. It achieves 97% grasp detection accuracy and 90% robotic grasping success rate with real-time inference speed.



### Learnable Pooling Methods for Video Classification
- **Arxiv ID**: http://arxiv.org/abs/1810.00530v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.00530v1)
- **Published**: 2018-10-01 05:02:47+00:00
- **Updated**: 2018-10-01 05:02:47+00:00
- **Authors**: Sebastian Kmiec, Juhan Bae, Ruijian An
- **Comment**: Presented at Youtube 8M ECCV18 Workshop
- **Journal**: None
- **Summary**: We introduce modifications to state-of-the-art approaches to aggregating local video descriptors by using attention mechanisms and function approximations. Rather than using ensembles of existing architectures, we provide an insight on creating new architectures. We demonstrate our solutions in the "The 2nd YouTube-8M Video Understanding Challenge", by using frame-level video and audio descriptors. We obtain testing accuracy similar to the state of the art, while meeting budget constraints, and touch upon strategies to improve the state of the art. Model implementations are available in https://github.com/pomonam/LearnablePoolingMethods.



### Generative Adversarial Network for Medical Images (MI-GAN)
- **Arxiv ID**: http://arxiv.org/abs/1810.00551v1
- **DOI**: 10.1007/s10916-018-1072-9
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.00551v1)
- **Published**: 2018-10-01 06:59:37+00:00
- **Updated**: 2018-10-01 06:59:37+00:00
- **Authors**: Talha Iqbal, Hazrat Ali
- **Comment**: Journal of Medical Systems
- **Journal**: Med Syst (2018) 42: 231
- **Summary**: Deep learning algorithms produces state-of-the-art results for different machine learning and computer vision tasks. To perform well on a given task, these algorithms require large dataset for training. However, deep learning algorithms lack generalization and suffer from over-fitting whenever trained on small dataset, especially when one is dealing with medical images. For supervised image analysis in medical imaging, having image data along with their corresponding annotated ground-truths is costly as well as time consuming since annotations of the data is done by medical experts manually. In this paper, we propose a new Generative Adversarial Network for Medical Imaging (MI-GAN). The MI-GAN generates synthetic medical images and their segmented masks, which can then be used for the application of supervised analysis of medical images. Particularly, we present MI-GAN for synthesis of retinal images. The proposed method generates precise segmented images better than the existing techniques. The proposed model achieves a dice coefficient of 0.837 on STARE dataset and 0.832 on DRIVE dataset which is state-of-the-art performance on both the datasets.



### Elastic Neural Networks for Classification
- **Arxiv ID**: http://arxiv.org/abs/1810.00589v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.00589v4)
- **Published**: 2018-10-01 09:15:42+00:00
- **Updated**: 2019-05-30 06:26:05+00:00
- **Authors**: Yi Zhou, Yue Bai, Shuvra S. Bhattacharyya, Heikki Huttunen
- **Comment**: 2019 IEEE International Conference on Artificial Intelligence
  Circuits and Systems
- **Journal**: None
- **Summary**: In this work we propose a framework for improving the performance of any deep neural network that may suffer from vanishing gradients. To address the vanishing gradient issue, we study a framework, where we insert an intermediate output branch after each layer in the computational graph and use the corresponding prediction loss for feeding the gradient to the early layers. The framework - which we name Elastic network - is tested with several well-known networks on CIFAR10 and CIFAR100 datasets, and the experimental results show that the proposed framework improves the accuracy on both shallow networks (e.g., MobileNet) and deep convolutional neural networks (e.g., DenseNet). We also identify the types of networks where the framework does not improve the performance and discuss the reasons. Finally, as a side product, the computational complexity of the resulting networks can be adjusted in an elastic manner by selecting the output branch according to current computational budget.



### Unsupervised Trajectory Segmentation and Promoting of Multi-Modal Surgical Demonstrations
- **Arxiv ID**: http://arxiv.org/abs/1810.00599v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.00599v1)
- **Published**: 2018-10-01 09:59:53+00:00
- **Updated**: 2018-10-01 09:59:53+00:00
- **Authors**: Zhenzhou Shao, Hongfa Zhao, Jiexin Xie, Ying Qu, Yong Guan, Jindong Tan
- **Comment**: 7 pages, 6 figures
- **Journal**: None
- **Summary**: To improve the efficiency of surgical trajectory segmentation for robot learning in robot-assisted minimally invasive surgery, this paper presents a fast unsupervised method using video and kinematic data, followed by a promoting procedure to address the over-segmentation issue. Unsupervised deep learning network, stacking convolutional auto-encoder, is employed to extract more discriminative features from videos in an effective way. To further improve the accuracy of segmentation, on one hand, wavelet transform is used to filter out the noises existed in the features from video and kinematic data. On the other hand, the segmentation result is promoted by identifying the adjacent segments with no state transition based on the predefined similarity measurements. Extensive experiments on a public dataset JIGSAWS show that our method achieves much higher accuracy of segmentation than state-of-the-art methods in the shorter time.



### Privado: Practical and Secure DNN Inference with Enclaves
- **Arxiv ID**: http://arxiv.org/abs/1810.00602v2
- **DOI**: None
- **Categories**: **cs.CR**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1810.00602v2)
- **Published**: 2018-10-01 10:13:42+00:00
- **Updated**: 2019-09-05 15:03:14+00:00
- **Authors**: Karan Grover, Shruti Tople, Shweta Shinde, Ranjita Bhagwan, Ramachandran Ramjee
- **Comment**: 13 pages, 5 figures
- **Journal**: None
- **Summary**: Cloud providers are extending support for trusted hardware primitives such as Intel SGX. Simultaneously, the field of deep learning is seeing enormous innovation as well as an increase in adoption. In this paper, we ask a timely question: "Can third-party cloud services use Intel SGX enclaves to provide practical, yet secure DNN Inference-as-a-service?" We first demonstrate that DNN models executing inside enclaves are vulnerable to access pattern based attacks. We show that by simply observing access patterns, an attacker can classify encrypted inputs with 97% and 71% attack accuracy for MNIST and CIFAR10 datasets on models trained to achieve 99% and 79% original accuracy respectively. This motivates the need for PRIVADO, a system we have designed for secure, easy-to-use, and performance efficient inference-as-a-service. PRIVADO is input-oblivious: it transforms any deep learning framework that is written in C/C++ to be free of input-dependent access patterns thus eliminating the leakage. PRIVADO is fully-automated and has a low TCB: with zero developer effort, given an ONNX description of a model, it generates compact and enclave-compatible code which can be deployed on an SGX cloud platform. PRIVADO incurs low performance overhead: we use PRIVADO with Torch framework and show its overhead to be 17.18% on average on 11 different contemporary neural networks.



### One-Click Annotation with Guided Hierarchical Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1810.00609v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.HC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.00609v1)
- **Published**: 2018-10-01 10:41:35+00:00
- **Updated**: 2018-10-01 10:41:35+00:00
- **Authors**: Adithya Subramanian, Anbumani Subramanian
- **Comment**: None
- **Journal**: None
- **Summary**: The increase in data collection has made data annotation an interesting and valuable task in the contemporary world. This paper presents a new methodology for quickly annotating data using click-supervision and hierarchical object detection. The proposed work is semi-automatic in nature where the task of annotations is split between the human and a neural network. We show that our improved method of annotation reduces the time, cost and mental stress on a human annotator. The research also highlights how our method performs better than the current approach in different circumstances such as variation in number of objects, object size and different datasets. Our approach also proposes a new method of using object detectors making it suitable for data annotation task. The experiment conducted on PASCAL VOC dataset revealed that annotation created from our approach achieves a mAP of 0.995 and a recall of 0.903. The Our Approach has shown an overall improvement by 8.5%, 18.6% in mean average precision and recall score for KITTI and 69.6%, 36% for CITYSCAPES dataset. The proposed framework is 3-4 times faster as compared to the standard annotation method.



### Part-Level Convolutional Neural Networks for Pedestrian Detection Using Saliency and Boundary Box Alignment
- **Arxiv ID**: http://arxiv.org/abs/1810.00689v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.00689v1)
- **Published**: 2018-10-01 13:00:30+00:00
- **Updated**: 2018-10-01 13:00:30+00:00
- **Authors**: Inyong Yun, Cheolkon Jung, Xinran Wang, Alfred O Hero, Joongkyu Kim
- **Comment**: 9 pages, 10 figures
- **Journal**: None
- **Summary**: Pedestrians in videos have a wide range of appearances such as body poses, occlusions, and complex backgrounds, and there exists the proposal shift problem in pedestrian detection that causes the loss of body parts such as head and legs. To address it, we propose part-level convolutional neural networks (CNN) for pedestrian detection using saliency and boundary box alignment in this paper. The proposed network consists of two sub-networks: detection and alignment. We use saliency in the detection sub-network to remove false positives such as lamp posts and trees. We adopt bounding box alignment on detection proposals in the alignment sub-network to address the proposal shift problem. First, we combine FCN and CAM to extract deep features for pedestrian detection. Then, we perform part-level CNN to recall the lost body parts. Experimental results on various datasets demonstrate that the proposed method remarkably improves accuracy in pedestrian detection and outperforms existing state-of-the-arts in terms of log average miss rate at false position per image (FPPI).



### A categorisation and implementation of digital pen features for behaviour characterisation
- **Arxiv ID**: http://arxiv.org/abs/1810.03970v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1810.03970v1)
- **Published**: 2018-10-01 14:24:20+00:00
- **Updated**: 2018-10-01 14:24:20+00:00
- **Authors**: Alexander Prange, Michael Barz, Daniel Sonntag
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we provide a categorisation and implementation of digital ink features for behaviour characterisation. Based on four feature sets taken from literature, we provide a categorisation in different classes of syntactic and semantic features. We implemented a publicly available framework to calculate these features and show its deployment in the use case of analysing cognitive assessments performed using a digital pen.



### SurfelMeshing: Online Surfel-Based Mesh Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1810.00729v2
- **DOI**: 10.1109/TPAMI.2019.2947048
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.00729v2)
- **Published**: 2018-10-01 14:41:50+00:00
- **Updated**: 2019-11-20 17:37:00+00:00
- **Authors**: Thomas Schöps, Torsten Sattler, Marc Pollefeys
- **Comment**: Version accepted to IEEE Transactions on Pattern Analysis and Machine
  Intelligence
- **Journal**: None
- **Summary**: We address the problem of mesh reconstruction from live RGB-D video, assuming a calibrated camera and poses provided externally (e.g., by a SLAM system). In contrast to most existing approaches, we do not fuse depth measurements in a volume but in a dense surfel cloud. We asynchronously (re)triangulate the smoothed surfels to reconstruct a surface mesh. This novel approach enables to maintain a dense surface representation of the scene during SLAM which can quickly adapt to loop closures. This is possible by deforming the surfel cloud and asynchronously remeshing the surface where necessary. The surfel-based representation also naturally supports strongly varying scan resolution. In particular, it reconstructs colors at the input camera's resolution. Moreover, in contrast to many volumetric approaches, ours can reconstruct thin objects since objects do not need to enclose a volume. We demonstrate our approach in a number of experiments, showing that it produces reconstructions that are competitive with the state-of-the-art, and we discuss its advantages and limitations. The algorithm (excluding loop closure functionality) is available as open source at https://github.com/puzzlepaint/surfelmeshing .



### Benchmark Analysis of Representative Deep Neural Network Architectures
- **Arxiv ID**: http://arxiv.org/abs/1810.00736v2
- **DOI**: 10.1109/ACCESS.2018.2877890
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.00736v2)
- **Published**: 2018-10-01 14:48:18+00:00
- **Updated**: 2018-10-19 15:39:24+00:00
- **Authors**: Simone Bianco, Remi Cadene, Luigi Celona, Paolo Napoletano
- **Comment**: Will appear in IEEE Access
- **Journal**: None
- **Summary**: This work presents an in-depth analysis of the majority of the deep neural networks (DNNs) proposed in the state of the art for image recognition. For each DNN multiple performance indices are observed, such as recognition accuracy, model complexity, computational complexity, memory usage, and inference time. The behavior of such performance indices and some combinations of them are analyzed and discussed. To measure the indices we experiment the use of DNNs on two different computer architectures, a workstation equipped with a NVIDIA Titan X Pascal and an embedded system based on a NVIDIA Jetson TX1 board. This experimentation allows a direct comparison between DNNs running on machines with very different computational capacity. This study is useful for researchers to have a complete view of what solutions have been explored so far and in which research directions are worth exploring in the future; and for practitioners to select the DNN architecture(s) that better fit the resource constraints of practical deployments and applications. To complete this work, all the DNNs, as well as the software used for the analysis, are available online.



### Improving the Generalization of Adversarial Training with Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1810.00740v7
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.00740v7)
- **Published**: 2018-10-01 14:52:08+00:00
- **Updated**: 2019-03-15 08:37:29+00:00
- **Authors**: Chuanbiao Song, Kun He, Liwei Wang, John E. Hopcroft
- **Comment**: ICLR 2019
- **Journal**: None
- **Summary**: By injecting adversarial examples into training data, adversarial training is promising for improving the robustness of deep learning models. However, most existing adversarial training approaches are based on a specific type of adversarial attack. It may not provide sufficiently representative samples from the adversarial domain, leading to a weak generalization ability on adversarial examples from other attacks. Moreover, during the adversarial training, adversarial perturbations on inputs are usually crafted by fast single-step adversaries so as to scale to large datasets. This work is mainly focused on the adversarial training yet efficient FGSM adversary. In this scenario, it is difficult to train a model with great generalization due to the lack of representative adversarial samples, aka the samples are unable to accurately reflect the adversarial domain. To alleviate this problem, we propose a novel Adversarial Training with Domain Adaptation (ATDA) method. Our intuition is to regard the adversarial training on FGSM adversary as a domain adaption task with limited number of target domain samples. The main idea is to learn a representation that is semantically meaningful and domain invariant on the clean domain as well as the adversarial domain. Empirical evaluations on Fashion-MNIST, SVHN, CIFAR-10 and CIFAR-100 demonstrate that ATDA can greatly improve the generalization of adversarial training and the smoothness of the learned models, and outperforms state-of-the-art methods on standard benchmark datasets. To show the transfer ability of our method, we also extend ATDA to the adversarial training on iterative attacks such as PGD-Adversial Training (PAT) and the defense performance is improved considerably.



### Bayesian Prediction of Future Street Scenes using Synthetic Likelihoods
- **Arxiv ID**: http://arxiv.org/abs/1810.00746v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1810.00746v3)
- **Published**: 2018-10-01 15:02:54+00:00
- **Updated**: 2019-01-18 08:09:47+00:00
- **Authors**: Apratim Bhattacharyya, Mario Fritz, Bernt Schiele
- **Comment**: To appear in ICLR 2019. arXiv admin note: text overlap with
  arXiv:1806.06939
- **Journal**: None
- **Summary**: For autonomous agents to successfully operate in the real world, the ability to anticipate future scene states is a key competence. In real-world scenarios, future states become increasingly uncertain and multi-modal, particularly on long time horizons. Dropout based Bayesian inference provides a computationally tractable, theoretically well grounded approach to learn likely hypotheses/models to deal with uncertain futures and make predictions that correspond well to observations -- are well calibrated. However, it turns out that such approaches fall short to capture complex real-world scenes, even falling behind in accuracy when compared to the plain deterministic approaches. This is because the used log-likelihood estimate discourages diversity. In this work, we propose a novel Bayesian formulation for anticipating future scene states which leverages synthetic likelihoods that encourage the learning of diverse models to accurately capture the multi-modal nature of future scene states. We show that our approach achieves accurate state-of-the-art predictions and calibrated probabilities through extensive experiments for scene anticipation on Cityscapes dataset. Moreover, we show that our approach generalizes across diverse tasks such as digit generation and precipitation forecasting.



### Graph Diffusion-Embedding Networks
- **Arxiv ID**: http://arxiv.org/abs/1810.00797v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.00797v1)
- **Published**: 2018-10-01 16:27:55+00:00
- **Updated**: 2018-10-01 16:27:55+00:00
- **Authors**: Bo Jiang, Doudou Lin, Jin Tang
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel graph diffusion-embedding networks (GDEN) for graph structured data. GDEN is motivated by our closed-form formulation on regularized feature diffusion on graph. GDEN integrates both regularized feature diffusion and low-dimensional embedding simultaneously in a unified network model. Moreover, based on GDEN, we can naturally deal with structured data with multiple graph structures. Experiments on semi-supervised learning tasks on several benchmark datasets demonstrate the better performance of the proposed GDEN when comparing with the traditional GCN models.



### RGB-D Object Detection and Semantic Segmentation for Autonomous Manipulation in Clutter
- **Arxiv ID**: http://arxiv.org/abs/1810.00818v1
- **DOI**: 10.1177/0278364917713117
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.00818v1)
- **Published**: 2018-10-01 16:55:53+00:00
- **Updated**: 2018-10-01 16:55:53+00:00
- **Authors**: Max Schwarz, Anton Milan, Arul Selvam Periyasamy, Sven Behnke
- **Comment**: None
- **Journal**: International Journal of Robotics Research 37(4-5): 437-451 (2018)
- **Summary**: Autonomous robotic manipulation in clutter is challenging. A large variety of objects must be perceived in complex scenes, where they are partially occluded and embedded among many distractors, often in restricted spaces. To tackle these challenges, we developed a deep-learning approach that combines object detection and semantic segmentation. The manipulation scenes are captured with RGB-D cameras, for which we developed a depth fusion method. Employing pretrained features makes learning from small annotated robotic data sets possible. We evaluate our approach on two challenging data sets: one captured for the Amazon Picking Challenge 2016, where our team NimbRo came in second in the Stowing and third in the Picking task, and one captured in disaster-response scenarios. The experiments show that object detection and semantic segmentation complement each other and can be combined to yield reliable object perception.



### How Powerful are Graph Neural Networks?
- **Arxiv ID**: http://arxiv.org/abs/1810.00826v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.00826v3)
- **Published**: 2018-10-01 17:11:31+00:00
- **Updated**: 2019-02-22 19:15:54+00:00
- **Authors**: Keyulu Xu, Weihua Hu, Jure Leskovec, Stefanie Jegelka
- **Comment**: None
- **Journal**: None
- **Summary**: Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.



### Augmented Mitotic Cell Count using Field Of Interest Proposal
- **Arxiv ID**: http://arxiv.org/abs/1810.00850v1
- **DOI**: 10.1007/978-3-658-25326-4_71
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.00850v1)
- **Published**: 2018-10-01 17:40:54+00:00
- **Updated**: 2018-10-01 17:40:54+00:00
- **Authors**: Marc Aubreville, Christof A. Bertram, Robert Klopfleisch, Andreas Maier
- **Comment**: 6 pages, submitted to BVM 2019 (bvm-workshop.org)
- **Journal**: Bildverarbeitung f\"ur die Medizin 2019, pp. 321-326
- **Summary**: Histopathological prognostication of neoplasia including most tumor grading systems are based upon a number of criteria. Probably the most important is the number of mitotic figures which are most commonly determined as the mitotic count (MC), i.e. number of mitotic figures within 10 consecutive high power fields. Often the area with the highest mitotic activity is to be selected for the MC. However, since mitotic activity is not known in advance, an arbitrary choice of this region is considered one important cause for high variability in the prognostication and grading.   In this work, we present an algorithmic approach that first calculates a mitotic cell map based upon a deep convolutional network. This map is in a second step used to construct a mitotic activity estimate. Lastly, we select the image segment representing the size of ten high power fields with the overall highest mitotic activity as a region proposal for an expert MC determination. We evaluate the approach using a dataset of 32 completely annotated whole slide images, where 22 were used for training of the network and 10 for test. We find a correlation of r=0.936 in mitotic count estimate.



### Visual Curiosity: Learning to Ask Questions to Learn Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/1810.00912v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1810.00912v1)
- **Published**: 2018-10-01 18:37:05+00:00
- **Updated**: 2018-10-01 18:37:05+00:00
- **Authors**: Jianwei Yang, Jiasen Lu, Stefan Lee, Dhruv Batra, Devi Parikh
- **Comment**: 18 pages, 10 figures, Oral Presentation in Conference on Robot
  Learning (CoRL) 2018
- **Journal**: None
- **Summary**: In an open-world setting, it is inevitable that an intelligent agent (e.g., a robot) will encounter visual objects, attributes or relationships it does not recognize. In this work, we develop an agent empowered with visual curiosity, i.e. the ability to ask questions to an Oracle (e.g., human) about the contents in images (e.g., What is the object on the left side of the red cube?) and build visual recognition model based on the answers received (e.g., Cylinder). In order to do this, the agent must (1) understand what it recognizes and what it does not, (2) formulate a valid, unambiguous and informative language query (a question) to ask the Oracle, (3) derive the parameters of visual classifiers from the Oracle response and (4) leverage the updated visual classifiers to ask more clarified questions. Specifically, we propose a novel framework and formulate the learning of visual curiosity as a reinforcement learning problem. In this framework, all components of our agent, visual recognition module (to see), question generation policy (to ask), answer digestion module (to understand) and graph memory module (to memorize), are learned entirely end-to-end to maximize the reward derived from the scene graph obtained by the agent as a consequence of the dialog with the Oracle. Importantly, the question generation policy is disentangled from the visual recognition system and specifics of the environment. Consequently, we demonstrate a sort of double generalization. Our question generation policy generalizes to new environments and a new pair of eyes, i.e., new visual system. Trained on a synthetic dataset, our results show that our agent learns new visual concepts significantly faster than several heuristic baselines, even when tested on synthetic environments with novel objects, as well as in a realistic environment.



### Improved robustness to adversarial examples using Lipschitz regularization of the loss
- **Arxiv ID**: http://arxiv.org/abs/1810.00953v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.00953v4)
- **Published**: 2018-10-01 20:02:00+00:00
- **Updated**: 2019-09-13 14:56:57+00:00
- **Authors**: Chris Finlay, Adam Oberman, Bilal Abbasi
- **Comment**: Merged with arXiv:1808.09540
- **Journal**: None
- **Summary**: We augment adversarial training (AT) with worst case adversarial training (WCAT) which improves adversarial robustness by 11% over the current state-of-the-art result in the $\ell_2$ norm on CIFAR-10. We obtain verifiable average case and worst case robustness guarantees, based on the expected and maximum values of the norm of the gradient of the loss. We interpret adversarial training as Total Variation Regularization, which is a fundamental tool in mathematical image processing, and WCAT as Lipschitz regularization.



### Natural measures of alignment
- **Arxiv ID**: http://arxiv.org/abs/1810.00965v1
- **DOI**: None
- **Categories**: **cs.CV**, math.GR
- **Links**: [PDF](http://arxiv.org/pdf/1810.00965v1)
- **Published**: 2018-10-01 20:37:07+00:00
- **Updated**: 2018-10-01 20:37:07+00:00
- **Authors**: R. A. Kycia, Z. Tabor
- **Comment**: 12 pages, 1 figure
- **Journal**: None
- **Summary**: Natural coordinate system will be proposed. In this coordinate system alignment procedure of a device and a detector can be easily performed. This approach is generalization of previous specific formulas in the field of calibration and provide top level description of the procedure. A basic example application to linac therapy plan is also provided.



### Extended Bit-Plane Compression for Convolutional Neural Network Accelerators
- **Arxiv ID**: http://arxiv.org/abs/1810.03979v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.AR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1810.03979v1)
- **Published**: 2018-10-01 21:02:53+00:00
- **Updated**: 2018-10-01 21:02:53+00:00
- **Authors**: Lukas Cavigelli, Luca Benini
- **Comment**: None
- **Journal**: None
- **Summary**: After the tremendous success of convolutional neural networks in image classification, object detection, speech recognition, etc., there is now rising demand for deployment of these compute-intensive ML models on tightly power constrained embedded and mobile systems at low cost as well as for pushing the throughput in data centers. This has triggered a wave of research towards specialized hardware accelerators. Their performance is often constrained by I/O bandwidth and the energy consumption is dominated by I/O transfers to off-chip memory. We introduce and evaluate a novel, hardware-friendly compression scheme for the feature maps present within convolutional neural networks. We show that an average compression ratio of 4.4x relative to uncompressed data and a gain of 60% over existing method can be achieved for ResNet-34 with a compression block requiring <300 bit of sequential cells and minimal combinational logic.



### Gyroscope-Aided Motion Deblurring with Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/1810.00986v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.00986v2)
- **Published**: 2018-10-01 21:32:59+00:00
- **Updated**: 2018-11-23 13:44:09+00:00
- **Authors**: Janne Mustaniemi, Juho Kannala, Simo Särkkä, Jiri Matas, Janne Heikkilä
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a deblurring method that incorporates gyroscope measurements into a convolutional neural network (CNN). With the help of such measurements, it can handle extremely strong and spatially-variant motion blur. At the same time, the image data is used to overcome the limitations of gyro-based blur estimation. To train our network, we also introduce a novel way of generating realistic training data using the gyroscope. The evaluation shows a clear improvement in visual quality over the state-of-the-art while achieving real-time performance. Furthermore, the method is shown to improve the performance of existing feature detectors and descriptors against the motion blur.



### Learning Hash Codes via Hamming Distance Targets
- **Arxiv ID**: http://arxiv.org/abs/1810.01008v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.01008v1)
- **Published**: 2018-10-01 23:03:27+00:00
- **Updated**: 2018-10-01 23:03:27+00:00
- **Authors**: Martin Loncaric, Bowei Liu, Ryan Weber
- **Comment**: 8 pages, overhaul of our previous submission Convolutional Hashing
  for Automated Scene Matching
- **Journal**: None
- **Summary**: We present a powerful new loss function and training scheme for learning binary hash codes with any differentiable model and similarity function. Our loss function improves over prior methods by using log likelihood loss on top of an accurate approximation for the probability that two inputs fall within a Hamming distance target. Our novel training scheme obtains a good estimate of the true gradient by better sampling inputs and evaluating loss terms between all pairs of inputs in each minibatch. To fully leverage the resulting hashes, we use multi-indexing. We demonstrate that these techniques provide large improvements to a similarity search tasks. We report the best results to date on competitive information retrieval tasks for ImageNet and SIFT 1M, improving MAP from 73% to 84% and reducing query cost by a factor of 2-8, respectively.



### CNN-SVO: Improving the Mapping in Semi-Direct Visual Odometry Using Single-Image Depth Prediction
- **Arxiv ID**: http://arxiv.org/abs/1810.01011v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.01011v1)
- **Published**: 2018-10-01 23:20:16+00:00
- **Updated**: 2018-10-01 23:20:16+00:00
- **Authors**: Shing Yan Loo, Ali Jahani Amiri, Syamsiah Mashohor, Sai Hong Tang, Hong Zhang
- **Comment**: 6 pages, 5 figures, submitted to ICRA 2019
- **Journal**: None
- **Summary**: Reliable feature correspondence between frames is a critical step in visual odometry (VO) and visual simultaneous localization and mapping (V-SLAM) algorithms. In comparison with existing VO and V-SLAM algorithms, semi-direct visual odometry (SVO) has two main advantages that lead to state-of-the-art frame rate camera motion estimation: direct pixel correspondence and efficient implementation of probabilistic mapping method. This paper improves the SVO mapping by initializing the mean and the variance of the depth at a feature location according to the depth prediction from a single-image depth prediction network. By significantly reducing the depth uncertainty of the initialized map point (i.e., small variance centred about the depth prediction), the benefits are twofold: reliable feature correspondence between views and fast convergence to the true depth in order to create new map points. We evaluate our method with two outdoor datasets: KITTI dataset and Oxford Robotcar dataset. The experimental results indicate that the improved SVO mapping results in increased robustness and camera tracking accuracy.



