# Arxiv Papers in cs.CV on 2018-10-27
### Monitoring the shape of weather, soundscapes, and dynamical systems: a new statistic for dimension-driven data analysis on large data sets
- **Arxiv ID**: http://arxiv.org/abs/1810.11562v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, physics.data-an, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.11562v1)
- **Published**: 2018-10-27 00:15:34+00:00
- **Updated**: 2018-10-27 00:15:34+00:00
- **Authors**: Henry Kvinge, Elin Farnell, Michael Kirby, Chris Peterson
- **Comment**: Accepted to the 2018 IEEE International Conference on BIG DATA, 9
  pages
- **Journal**: None
- **Summary**: Dimensionality-reduction methods are a fundamental tool in the analysis of large data sets. These algorithms work on the assumption that the "intrinsic dimension" of the data is generally much smaller than the ambient dimension in which it is collected. Alongside their usual purpose of mapping data into a smaller dimension with minimal information loss, dimensionality-reduction techniques implicitly or explicitly provide information about the dimension of the data set.   In this paper, we propose a new statistic that we call the $\kappa$-profile for analysis of large data sets. The $\kappa$-profile arises from a dimensionality-reduction optimization problem: namely that of finding a projection into $k$-dimensions that optimally preserves the secants between points in the data set. From this optimal projection we extract $\kappa,$ the norm of the shortest projected secant from among the set of all normalized secants. This $\kappa$ can be computed for any $k$; thus the tuple of $\kappa$ values (indexed by dimension) becomes a $\kappa$-profile. Algorithms such as the Secant-Avoidance Projection algorithm and the Hierarchical Secant-Avoidance Projection algorithm, provide a computationally feasible means of estimating the $\kappa$-profile for large data sets, and thus a method of understanding and monitoring their behavior. As we demonstrate in this paper, the $\kappa$-profile serves as a useful statistic in several representative settings: weather data, soundscape data, and dynamical systems data.



### A Volumetric Convolutional Neural Network for Brain Tumor Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1811.02654v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.02654v1)
- **Published**: 2018-10-27 02:28:54+00:00
- **Updated**: 2018-10-27 02:28:54+00:00
- **Authors**: Ryan Sherman
- **Comment**: None
- **Journal**: None
- **Summary**: Brain cancer can be very fatal, but chances of survival increase through early detection and treatment. Doctors use Magnetic Resonance Imaging (MRI) to detect and locate tumors in the brain, and very carefully analyze scans to segment brain tumors. Manual segmentation is time consuming and tiring for doctors, and it can be difficult for them to notice extremely small abnormalities. Automated segmentations performed by computers offer quicker diagnoses, the ability to notice small details, and more accurate segmentations. Advances in deep learning and computer hardware have allowed for high-performing automated segmentation approaches. However, several problems persist in practice: increased training time, class imbalance, and low performance. In this paper, I propose applying V-Net, a volumetric, fully convolutional neural network, to segment brain tumors in MRI scans from the BraTS Challenges. With this approach, I achieve a whole tumor dice score of 0.89 and train the network in a short time while addressing class imbalance with the use of a dice loss layer. Then, I propose applying an existing technique to improve automated segmentation performance in practice.



### $A^2$-Nets: Double Attention Networks
- **Arxiv ID**: http://arxiv.org/abs/1810.11579v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.11579v1)
- **Published**: 2018-10-27 02:32:22+00:00
- **Updated**: 2018-10-27 02:32:22+00:00
- **Authors**: Yunpeng Chen, Yannis Kalantidis, Jianshu Li, Shuicheng Yan, Jiashi Feng
- **Comment**: Accepted at NIPS 2018
- **Journal**: None
- **Summary**: Learning to capture long-range relations is fundamental to image/video recognition. Existing CNN models generally rely on increasing depth to model such relations which is highly inefficient. In this work, we propose the "double attention block", a novel component that aggregates and propagates informative global features from the entire spatio-temporal space of input images/videos, enabling subsequent convolution layers to access features from the entire space efficiently. The component is designed with a double attention mechanism in two steps, where the first step gathers features from the entire space into a compact set through second-order attention pooling and the second step adaptively selects and distributes features to each location via another attention. The proposed double attention block is easy to adopt and can be plugged into existing deep neural networks conveniently. We conduct extensive ablation studies and experiments on both image and video recognition tasks for evaluating its performance. On the image recognition task, a ResNet-50 equipped with our double attention blocks outperforms a much larger ResNet-152 architecture on ImageNet-1k dataset with over 40% less the number of parameters and less FLOPs. On the action recognition task, our proposed model achieves the state-of-the-art results on the Kinetics and UCF-101 datasets with significantly higher efficiency than recent works.



### Convolutional neural networks with extra-classical receptive fields
- **Arxiv ID**: http://arxiv.org/abs/1810.11594v1
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1810.11594v1)
- **Published**: 2018-10-27 04:15:50+00:00
- **Updated**: 2018-10-27 04:15:50+00:00
- **Authors**: Brian Hu, Stefan Mihalas
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have had great success in many real-world applications and have also been used to model visual processing in the brain. However, these networks are quite brittle - small changes in the input image can dramatically change a network's output prediction. In contrast to what is known from biology, these networks largely rely on feedforward connections, ignoring the influence of recurrent connections. They also focus on supervised rather than unsupervised learning. To address these issues, we combine traditional supervised learning via backpropagation with a specialized unsupervised learning rule to learn lateral connections between neurons within a convolutional neural network. These connections have been shown to optimally integrate information from the surround, generating extra-classical receptive fields for the neurons in our new proposed model (CNNEx). Models with optimal lateral connections are more robust to noise and achieve better performance on noisy versions of the MNIST and CIFAR-10 datasets. Resistance to noise can be further improved by combining our model with additional regularization techniques such as dropout and weight decay. Although the image statistics of MNIST and CIFAR-10 differ greatly, the same unsupervised learning rule generalized to both datasets. Our results demonstrate the potential usefulness of combining supervised and unsupervised learning techniques and suggest that the integration of lateral connections into convolutional neural networks is an important area of future research.



### Self-Supervised GAN to Counter Forgetting
- **Arxiv ID**: http://arxiv.org/abs/1810.11598v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.11598v2)
- **Published**: 2018-10-27 04:49:25+00:00
- **Updated**: 2018-11-29 23:00:39+00:00
- **Authors**: Ting Chen, Xiaohua Zhai, Neil Houlsby
- **Comment**: NeurIPS'18 Continual Learning workshop
- **Journal**: None
- **Summary**: GANs involve training two networks in an adversarial game, where each network's task depends on its adversary. Recently, several works have framed GAN training as an online or continual learning problem. We focus on the discriminator, which must perform classification under an (adversarially) shifting data distribution. When trained on sequential tasks, neural networks exhibit \emph{forgetting}. For GANs, discriminator forgetting leads to training instability. To counter forgetting, we encourage the discriminator to maintain useful representations by adding a self-supervision. Conditional GANs have a similar effect using labels. However, our self-supervised GAN does not require labels, and closes the performance gap between conditional and unconditional models. We show that, in doing so, the self-supervised discriminator learns better representations than regular GANs.



### A Miniaturized Semantic Segmentation Method for Remote Sensing Image
- **Arxiv ID**: http://arxiv.org/abs/1810.11603v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1810.11603v1)
- **Published**: 2018-10-27 05:58:36+00:00
- **Updated**: 2018-10-27 05:58:36+00:00
- **Authors**: Shou-Yu Chen, Guang-Sheng Chen, Wei-Peng Jing
- **Comment**: 5 pages, 3 figures, 3 tables, this paper is to be submitted to the
  conference
- **Journal**: None
- **Summary**: In order to save the memory, we propose a miniaturization method for neural network to reduce the parameter quantity existed in remote sensing (RS) image semantic segmentation model. The compact convolution optimization method is first used for standard U-Net to reduce the weights quantity. With the purpose of decreasing model performance loss caused by miniaturization and based on the characteristics of remote sensing image, fewer down-samplings and improved cascade atrous convolution are then used to improve the performance of the miniaturized U-Net. Compared with U-Net, our proposed Micro-Net not only achieves 29.26 times model compression, but also basically maintains the performance unchanged on the public dataset. We provide a Keras and Tensorflow hybrid programming implementation for our model: https://github.com/Isnot2bad/Micro-Net



### Soft-Gated Warping-GAN for Pose-Guided Person Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1810.11610v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.11610v2)
- **Published**: 2018-10-27 07:08:18+00:00
- **Updated**: 2019-01-11 18:19:04+00:00
- **Authors**: Haoye Dong, Xiaodan Liang, Ke Gong, Hanjiang Lai, Jia Zhu, Jian Yin
- **Comment**: 17 pages, 14 figures
- **Journal**: None
- **Summary**: Despite remarkable advances in image synthesis research, existing works often fail in manipulating images under the context of large geometric transformations. Synthesizing person images conditioned on arbitrary poses is one of the most representative examples where the generation quality largely relies on the capability of identifying and modeling arbitrary transformations on different body parts. Current generative models are often built on local convolutions and overlook the key challenges (e.g. heavy occlusions, different views or dramatic appearance changes) when distinct geometric changes happen for each part, caused by arbitrary pose manipulations. This paper aims to resolve these challenges induced by geometric variability and spatial displacements via a new Soft-Gated Warping Generative Adversarial Network (Warping-GAN), which is composed of two stages: 1) it first synthesizes a target part segmentation map given a target pose, which depicts the region-level spatial layouts for guiding image synthesis with higher-level structure constraints; 2) the Warping-GAN equipped with a soft-gated warping-block learns feature-level mapping to render textures from the original image into the generated segmentation map. Warping-GAN is capable of controlling different transformation degrees given distinct target poses. Moreover, the proposed warping-block is light-weight and flexible enough to be injected into any networks. Human perceptual studies and quantitative evaluations demonstrate the superiority of our Warping-GAN that significantly outperforms all existing methods on two large datasets.



### Cross-Modal Distillation for RGB-Depth Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1810.11641v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1810.11641v3)
- **Published**: 2018-10-27 13:37:10+00:00
- **Updated**: 2022-02-12 11:18:51+00:00
- **Authors**: Frank Hafner, Amran Bhuiyan, Julian F. P. Kooij, Eric Granger
- **Comment**: None
- **Journal**: Computer Vision and Image Understanding, 103352 (2022)
- **Summary**: Person re-identification is a key challenge for surveillance across multiple sensors. Prompted by the advent of powerful deep learning models for visual recognition, and inexpensive RGB-D cameras and sensor-rich mobile robotic platforms, e.g. self-driving vehicles, we investigate the relatively unexplored problem of cross-modal re-identification of persons between RGB (color) and depth images. The considerable divergence in data distributions across different sensor modalities introduces additional challenges to the typical difficulties like distinct viewpoints, occlusions, and pose and illumination variation. While some work has investigated re-identification across RGB and infrared, we take inspiration from successes in transfer learning from RGB to depth in object detection tasks. Our main contribution is a novel method for cross-modal distillation for robust person re-identification, which learns a shared feature representation space of person's appearance in both RGB and depth images. In addition, we propose a cross-modal attention mechanism where the gating signal from one modality can dynamically activate the most discriminant CNN filters of the other modality. The proposed distillation method is compared to conventional and deep learning approaches proposed for other cross-domain re-identification tasks. Results obtained on the public BIWI and RobotPKU datasets indicate that the proposed method can significantly outperform the state-of-the-art approaches by up to 16.1% in mean Average Precision (mAP), demonstrating the benefit of the distillation paradigm. The experimental results also indicate that using cross-modal attention allows to improve recognition accuracy considerably with respect to the proposed distillation method and relevant state-of-the-art approaches.



### Fabrik: An Online Collaborative Neural Network Editor
- **Arxiv ID**: http://arxiv.org/abs/1810.11649v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1810.11649v1)
- **Published**: 2018-10-27 14:23:38+00:00
- **Updated**: 2018-10-27 14:23:38+00:00
- **Authors**: Utsav Garg, Viraj Prabhu, Deshraj Yadav, Ram Ramrakhya, Harsh Agrawal, Dhruv Batra
- **Comment**: None
- **Journal**: None
- **Summary**: We present Fabrik, an online neural network editor that provides tools to visualize, edit, and share neural networks from within a browser. Fabrik provides a simple and intuitive GUI to import neural networks written in popular deep learning frameworks such as Caffe, Keras, and TensorFlow, and allows users to interact with, build, and edit models via simple drag and drop. Fabrik is designed to be framework agnostic and support high interoperability, and can be used to export models back to any supported framework. Finally, it provides powerful collaborative features to enable users to iterate over model design remotely and at scale.



### 3D MRI brain tumor segmentation using autoencoder regularization
- **Arxiv ID**: http://arxiv.org/abs/1810.11654v3
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1810.11654v3)
- **Published**: 2018-10-27 14:42:13+00:00
- **Updated**: 2018-11-19 17:04:58+00:00
- **Authors**: Andriy Myronenko
- **Comment**: None
- **Journal**: None
- **Summary**: Automated segmentation of brain tumors from 3D magnetic resonance images (MRIs) is necessary for the diagnosis, monitoring, and treatment planning of the disease. Manual delineation practices require anatomical knowledge, are expensive, time consuming and can be inaccurate due to human error. Here, we describe a semantic segmentation network for tumor subregion segmentation from 3D MRIs based on encoder-decoder architecture. Due to a limited training dataset size, a variational auto-encoder branch is added to reconstruct the input image itself in order to regularize the shared decoder and impose additional constraints on its layers. The current approach won 1st place in the BraTS 2018 challenge.



### Accelerated Inference in Markov Random Fields via Smooth Riemannian Optimization
- **Arxiv ID**: http://arxiv.org/abs/1810.11689v2
- **DOI**: None
- **Categories**: **cs.CV**, 65K05, 62F10, 65D19
- **Links**: [PDF](http://arxiv.org/pdf/1810.11689v2)
- **Published**: 2018-10-27 17:53:43+00:00
- **Updated**: 2018-12-14 20:28:05+00:00
- **Authors**: Siyi Hu, Luca Carlone
- **Comment**: 16 pages
- **Journal**: None
- **Summary**: Markov Random Fields (MRFs) are a popular model for several pattern recognition and reconstruction problems in robotics and computer vision. Inference in MRFs is intractable in general and related work resorts to approximation algorithms. Among those techniques, semidefinite programming (SDP) relaxations have been shown to provide accurate estimates while scaling poorly with the problem size and being typically slow for practical applications. Our first contribution is to design a dual ascent method to solve standard SDP relaxations that takes advantage of the geometric structure of the problem to speed up computation. This technique, named Dual Ascent Riemannian Staircase (DARS), is able to solve large problem instances in seconds. Our second contribution is to develop a second and faster approach. The backbone of this second approach is a novel SDP relaxation combined with a fast and scalable solver based on smooth Riemannian optimization. We show that this approach, named Fast Unconstrained SEmidefinite Solver (FUSES), can solve large problems in milliseconds. Contrarily to local MRF solvers, e.g., loopy belief propagation, our approaches do not require an initial guess. Moreover, we leverage recent results from optimization theory to provide per-instance sub-optimality guarantees. We demonstrate the proposed approaches in multi-class image segmentation problems. Extensive experimental evidence shows that (i) FUSES and DARS produce near-optimal solutions, attaining an objective within 0.1% of the optimum, (ii) FUSES and DARS are remarkably faster than general-purpose SDP solvers, and FUSES is more than two orders of magnitude faster than DARS while attaining similar solution quality, (iii) FUSES is faster than local search methods while being a global solver.



### 3D Terrain Segmentation in the SWIR Spectrum
- **Arxiv ID**: http://arxiv.org/abs/1810.11690v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.11690v1)
- **Published**: 2018-10-27 17:59:26+00:00
- **Updated**: 2018-10-27 17:59:26+00:00
- **Authors**: Dalton Rosario, Anthony Ortiz, Olac Fuentes
- **Comment**: Published on: IEEE Workshop on Hyperspectral Image and Signal
  Processing Conference (WHISPERS 2018), Amsterdam, The Netherlands, September
  2018
- **Journal**: None
- **Summary**: We focus on the automatic 3D terrain segmentation problem using hyperspectral shortwave IR (HS-SWIR) imagery and 3D Digital Elevation Models (DEM). The datasets were independently collected, and metadata for the HS-SWIR dataset are unavailable. We explore an overall slope of the SWIR spectrum that correlates with the presence of moisture in soil to propose a band ratio test to be used as a proxy for soil moisture content to distinguish two broad classes of objects: live vegetation from impermeable manmade surface. We show that image based localization techniques combined with the Optimal Randomized RANdom Sample Consensus (RANSAC) algorithm achieve precise spatial matches between HS-SWIR data of a portion of downtown Los Angeles (LA (USA)) and the Visible image of a geo-registered 3D DEM, covering a wider-area of LA. Our spectral-elevation rule based approach yields an overall accuracy of 97.7%, segmenting the object classes into buildings, houses, trees, grass, and roads/parking lots.



### Flash Photography for Data-Driven Hidden Scene Recovery
- **Arxiv ID**: http://arxiv.org/abs/1810.11710v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.11710v1)
- **Published**: 2018-10-27 21:16:55+00:00
- **Updated**: 2018-10-27 21:16:55+00:00
- **Authors**: Matthew Tancik, Guy Satat, Ramesh Raskar
- **Comment**: None
- **Journal**: None
- **Summary**: Vehicles, search and rescue personnel, and endoscopes use flash lights to locate, identify, and view objects in their surroundings. Here we show the first steps of how all these tasks can be done around corners with consumer cameras. Recent techniques for NLOS imaging using consumer cameras have not been able to both localize and identify the hidden object. We introduce a method that couples traditional geometric understanding and data-driven techniques. To avoid the limitation of large dataset gathering, we train the data-driven models on rendered samples to computationally recover the hidden scene on real data. The method has three independent operating modes: 1) a regression output to localize a hidden object in 2D, 2) an identification output to identify the object type or pose, and 3) a generative network to reconstruct the hidden scene from a new viewpoint. The method is able to localize 12cm wide hidden objects in 2D with 1.7cm accuracy. The method also identifies the hidden object class with 87.7% accuracy (compared to 33.3% random accuracy). This paper also provides an analysis on the distribution of information that encodes the occluded object in the accessible scene. We show that, unlike previously thought, the area that extends beyond the corner is essential for accurate object localization and identification.



### The CoSTAR Block Stacking Dataset: Learning with Workspace Constraints
- **Arxiv ID**: http://arxiv.org/abs/1810.11714v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1810.11714v2)
- **Published**: 2018-10-27 21:26:42+00:00
- **Updated**: 2019-03-12 23:17:17+00:00
- **Authors**: Andrew Hundt, Varun Jain, Chia-Hung Lin, Chris Paxton, Gregory D. Hager
- **Comment**: This is a major revision refocusing the topic towards the JHU CoSTAR
  Block Stacking Dataset, workspace constraints, and a comparison of HyperTrees
  with hand-designed algorithms. 12 pages, 10 figures, and 3 tables
- **Journal**: None
- **Summary**: A robot can now grasp an object more effectively than ever before, but once it has the object what happens next? We show that a mild relaxation of the task and workspace constraints implicit in existing object grasping datasets can cause neural network based grasping algorithms to fail on even a simple block stacking task when executed under more realistic circumstances.   To address this, we introduce the JHU CoSTAR Block Stacking Dataset (BSD), where a robot interacts with 5.1 cm colored blocks to complete an order-fulfillment style block stacking task. It contains dynamic scenes and real time-series data in a less constrained environment than comparable datasets. There are nearly 12,000 stacking attempts and over 2 million frames of real data. We discuss the ways in which this dataset provides a valuable resource for a broad range of other topics of investigation.   We find that hand-designed neural networks that work on prior datasets do not generalize to this task. Thus, to establish a baseline for this dataset, we demonstrate an automated search of neural network based models using a novel multiple-input HyperTree MetaModel, and find a final model which makes reasonable 3D pose predictions for grasping and stacking on our dataset.   The CoSTAR BSD, code, and instructions are available at https://sites.google.com/site/costardataset.



### On the role of ML estimation and Bregman divergences in sparse representation of covariance and precision matrices
- **Arxiv ID**: http://arxiv.org/abs/1810.11718v1
- **DOI**: None
- **Categories**: **cs.CV**, I.5.0; I.5.1; I.5.2; I.5.4; I.4.7; F.2.1; G.3; H.1.1; I.6.4
- **Links**: [PDF](http://arxiv.org/pdf/1810.11718v1)
- **Published**: 2018-10-27 22:06:29+00:00
- **Updated**: 2018-10-27 22:06:29+00:00
- **Authors**: Branko Brkljač, Željen Trpovski
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: Sparse representation of structured signals requires modelling strategies that maintain specific signal properties, in addition to preserving original information content and achieving simpler signal representation. Therefore, the major design challenge is to introduce adequate problem formulations and offer solutions that will efficiently lead to desired representations. In this context, sparse representation of covariance and precision matrices, which appear as feature descriptors or mixture model parameters, respectively, will be in the main focus of this paper.



### Real-time Action Recognition with Dissimilarity-based Training of Specialized Module Networks
- **Arxiv ID**: http://arxiv.org/abs/1810.11731v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.11731v1)
- **Published**: 2018-10-27 23:29:13+00:00
- **Updated**: 2018-10-27 23:29:13+00:00
- **Authors**: Marian K. Y. Boktor, Ahmad Al-Kabbany, Radwa Khalil, Said El-Khamy
- **Comment**: 8 pages, 5 Tables, 4 Figures, Appeared in CAINE 2018
- **Journal**: None
- **Summary**: This paper addresses the problem of real-time action recognition in trimmed videos, for which deep neural networks have defined the state-of-the-art performance in the recent literature. For attaining higher recognition accuracies with efficient computations, researchers have addressed the various aspects of limitations in the recognition pipeline. This includes network architecture, the number of input streams (where additional streams augment the color information), the cost function to be optimized, in addition to others. The literature has always aimed, though, at assigning the adopted network (or networks, in case of multiple streams) the task of recognizing the whole number of action classes in the dataset at hand. We propose to train multiple specialized module networks instead. Each module is trained to recognize a subset of the action classes. Towards this goal, we present a dissimilarity-based optimized procedure for distributing the action classes over the modules, which can be trained simultaneously offline. On two standard datasets--UCF-101 and HMDB-51--the proposed method demonstrates a comparable performance, that is superior in some aspects, to the state-of-the-art, and that satisfies the real-time constraint. We achieved 72.5\% accuracy on the challenging HMDB-51 dataset. By assigning fewer and unalike classes to each module network, this research paves the way to benefit from light-weight architectures without compromising recognition accuracy.



