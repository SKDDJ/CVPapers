# Arxiv Papers in cs.CV on 2018-10-24
### A Binary Optimization Approach for Constrained K-Means Clustering
- **Arxiv ID**: http://arxiv.org/abs/1810.10134v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1810.10134v2)
- **Published**: 2018-10-24 00:11:33+00:00
- **Updated**: 2018-10-29 05:51:09+00:00
- **Authors**: Huu Le, Anders Eriksson, Thanh-Toan Do, Michael Milford
- **Comment**: None
- **Journal**: None
- **Summary**: K-Means clustering still plays an important role in many computer vision problems. While the conventional Lloyd method, which alternates between centroid update and cluster assignment, is primarily used in practice, it may converge to a solution with empty clusters. Furthermore, some applications may require the clusters to satisfy a specific set of constraints, e.g., cluster sizes, must-link/cannot-link. Several methods have been introduced to solve constrained K-Means clustering. Due to the non-convex nature of K-Means, however, existing approaches may result in sub-optimal solutions that poorly approximate the true clusters. In this work, we provide a new perspective to tackle this problem. Particularly, we reconsider constrained K-Means as a Binary Optimization Problem and propose a novel optimization scheme to search for feasible solutions in the binary domain. This approach allows us to solve constrained K-Means where multiple types of constraints can be simultaneously enforced. Experimental results on synthetic and real datasets show that our method provides better clustering accuracy with faster runtime compared to several commonly used techniques.



### A Deep-Learning-Based Fashion Attributes Detection Model
- **Arxiv ID**: http://arxiv.org/abs/1810.10148v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.10148v1)
- **Published**: 2018-10-24 01:22:01+00:00
- **Updated**: 2018-10-24 01:22:01+00:00
- **Authors**: Menglin Jia, Yichen Zhou, Mengyun Shi, Bharath Hariharan
- **Comment**: None
- **Journal**: None
- **Summary**: Analyzing fashion attributes is essential in the fashion design process. Current fashion forecasting firms, such as WGSN utilizes information from all around the world (from fashion shows, visual merchandising, blogs, etc). They gather information by experience, by observation, by media scan, by interviews, and by exposed to new things. Such information analyzing process is called abstracting, which recognize similarities or differences across all the garments and collections. In fact, such abstraction ability is useful in many fashion careers with different purposes. Fashion forecasters abstract across design collections and across time to identify fashion change and directions; designers, product developers and buyers abstract across a group of garments and collections to develop a cohesive and visually appeal lines; sales and marketing executives abstract across product line each season to recognize selling points; fashion journalist and bloggers abstract across runway photos to recognize symbolic core concepts that can be translated into editorial features. Fashion attributes analysis for such fashion insiders requires much detailed and in-depth attributes annotation than that for consumers, and requires inference on multiple domains. In this project, we propose a data-driven approach for recognizing fashion attributes. Specifically, a modified version of Faster R-CNN model is trained on images from a large-scale localization dataset with 594 fine-grained attributes under different scenarios, for example in online stores and street snapshots. This model will then be used to detect garment items and classify clothing attributes for runway photos and fashion illustrations.



### AUNet: Attention-guided dense-upsampling networks for breast mass segmentation in whole mammograms
- **Arxiv ID**: http://arxiv.org/abs/1810.10151v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.10151v3)
- **Published**: 2018-10-24 01:55:33+00:00
- **Updated**: 2019-08-06 07:53:47+00:00
- **Authors**: Hui Sun, Cheng Li, Boqiang Liu, Hairong Zheng, David Dagan Feng, Shanshan Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Mammography is one of the most commonly applied tools for early breast cancer screening. Automatic segmentation of breast masses in mammograms is essential but challenging due to the low signal-to-noise ratio and the wide variety of mass shapes and sizes. Existing methods deal with these challenges mainly by extracting mass-centered image patches manually or automatically. However, manual patch extraction is time-consuming and automatic patch extraction brings errors that could not be compensated in the following segmentation step. In this study, we propose a novel attention-guided dense-upsampling network (AUNet) for accurate breast mass segmentation in whole mammograms directly. In AUNet, we employ an asymmetrical encoder-decoder structure and propose an effective upsampling block, attention-guided dense-upsampling block (AU block). Especially, the AU block is designed to have three merits. Firstly, it compensates the information loss of bilinear upsampling by dense upsampling. Secondly, it designs a more effective method to fuse high- and low-level features. Thirdly, it includes a channel-attention function to highlight rich-information channels. We evaluated the proposed method on two publicly available datasets, CBIS-DDSM and INbreast. Compared to three state-of-the-art fully convolutional networks, AUNet achieved the best performances with an average Dice similarity coefficient of 81.8% for CBIS-DDSM and 79.1% for INbreast.



### Background Subtraction using Compressed Low-resolution Images
- **Arxiv ID**: http://arxiv.org/abs/1810.10155v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.10155v1)
- **Published**: 2018-10-24 02:37:08+00:00
- **Updated**: 2018-10-24 02:37:08+00:00
- **Authors**: Min Chen, Andy Song, Shivanthan A. C. Yhanandan, Jing Zhang
- **Comment**: 4 pages,36 figures
- **Journal**: None
- **Summary**: Image processing and recognition are an important part of the modern society, with applications in fields such as advanced artificial intelligence, smart assistants, and security surveillance. The essential first step involved in almost all the visual tasks is background subtraction with a static camera. Ensuring that this critical step is performed in the most efficient manner would therefore improve all aspects related to objects recognition and tracking, behavior comprehension, etc.. Although background subtraction method has been applied for many years, its application suffers from real-time requirement. In this letter, we present a novel approach in implementing the background subtraction. The proposed method uses compressed, low-resolution grayscale image for the background subtraction. These low-resolution grayscale images were found to preserve the salient information very well. To verify the feasibility of our methodology, two prevalent methods, ViBe and GMM, are used in the experiment. The results of the proposed methodology confirm the effectiveness of our approach.



### Resolving Referring Expressions in Images With Labeled Elements
- **Arxiv ID**: http://arxiv.org/abs/1810.10165v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1810.10165v2)
- **Published**: 2018-10-24 03:22:08+00:00
- **Updated**: 2018-10-25 22:17:21+00:00
- **Authors**: Nevan Wichers, Dilek Hakkani-Tur, Jindong Chen
- **Comment**: Accepted into IEEE SLT Workshop
- **Journal**: None
- **Summary**: Images may have elements containing text and a bounding box associated with them, for example, text identified via optical character recognition on a computer screen image, or a natural image with labeled objects. We present an end-to-end trainable architecture to incorporate the information from these elements and the image to segment/identify the part of the image a natural language expression is referring to. We calculate an embedding for each element and then project it onto the corresponding location (i.e., the associated bounding box) of the image feature map. We show that this architecture gives an improvement in resolving referring expressions, over only using the image, and other methods that incorporate the element information. We demonstrate experimental results on the referring expression datasets based on COCO, and on a webpage image referring expression dataset that we developed.



### Fault Area Detection in Leaf Diseases using k-means Clustering
- **Arxiv ID**: http://arxiv.org/abs/1810.10188v1
- **DOI**: 10.1109/ICOEI.2018.8553913
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.10188v1)
- **Published**: 2018-10-24 05:08:08+00:00
- **Updated**: 2018-10-24 05:08:08+00:00
- **Authors**: Subhajit Maity, Sujan Sarkar, Avinaba Tapadar, Ayan Dutta, Sanket Biswas, Sayon Nayek, Pritam Saha
- **Comment**: This article is of 5 pages in IEEE format. It has been presented as a
  full paper in International Conference on Trends in Electronics and
  Informatics (ICOEI 2018) and is currently under the proceedings of the
  conference and yet to be published in IEEE Xplore
- **Journal**: None
- **Summary**: With increasing population the crisis of food is getting bigger day by day.In this time of crisis,the leaf disease of crops is the biggest problem in the food industry.In this paper, we have addressed that problem and proposed an efficient method to detect leaf disease.Leaf diseases can be detected from sample images of the leaf with the help of image processing and segmentation.Using k-means clustering and Otsu's method the faulty region in a leaf is detected which helps to determine proper course of action to be taken.Further the ratio of normal and faulty region if calculated would be able to predict if the leaf can be cured at all.



### Automated Evaluation of Semantic Segmentation Robustness for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1810.10193v1
- **DOI**: 10.1109/TITS.2019.2909066
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.10193v1)
- **Published**: 2018-10-24 05:36:53+00:00
- **Updated**: 2018-10-24 05:36:53+00:00
- **Authors**: Wei Zhou, Julie Stephany Berrio, Stewart Worrall, Eduardo Nebot
- **Comment**: None
- **Journal**: None
- **Summary**: One of the fundamental challenges in the design of perception systems for autonomous vehicles is validating the performance of each algorithm under a comprehensive variety of operating conditions. In the case of vision-based semantic segmentation, there are known issues when encountering new scenarios that are sufficiently different to the training data. In addition, even small variations in environmental conditions such as illumination and precipitation can affect the classification performance of the segmentation model. Given the reliance on visual information, these effects often translate into poor semantic pixel classification which can potentially lead to catastrophic consequences when driving autonomously. This paper presents a novel method for analysing the robustness of semantic segmentation models and provides a number of metrics to evaluate the classification performance over a variety of environmental conditions. The process incorporates an additional sensor (lidar) to automate the process, eliminating the need for labour-intensive hand labelling of validation data. The system integrity can be monitored as the performance of the vision sensors are validated against a different sensor modality. This is necessary for detecting failures that are inherent to vision technology. Experimental results are presented based on multiple datasets collected at different times of the year with different environmental conditions. These results show that the semantic segmentation performance varies depending on the weather, camera parameters, existence of shadows, etc.. The results also demonstrate how the metrics can be used to compare and validate the performance after making improvements to a model, and compare the performance of different networks.



### DSFD: Dual Shot Face Detector
- **Arxiv ID**: http://arxiv.org/abs/1810.10220v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.10220v3)
- **Published**: 2018-10-24 07:26:47+00:00
- **Updated**: 2019-04-06 12:09:49+00:00
- **Authors**: Jian Li, Yabiao Wang, Changan Wang, Ying Tai, Jianjun Qian, Jian Yang, Chengjie Wang, Jilin Li, Feiyue Huang
- **Comment**: Camera-ready version of DSFD for CVPR 2019. Code is available at:
  https://github.com/TencentYoutuResearch/FaceDetection-DSFD
- **Journal**: None
- **Summary**: In this paper, we propose a novel face detection network with three novel contributions that address three key aspects of face detection, including better feature learning, progressive loss design and anchor assign based data augmentation, respectively. First, we propose a Feature Enhance Module (FEM) for enhancing the original feature maps to extend the single shot detector to dual shot detector. Second, we adopt Progressive Anchor Loss (PAL) computed by two different sets of anchors to effectively facilitate the features. Third, we use an Improved Anchor Matching (IAM) by integrating novel anchor assign strategy into data augmentation to provide better initialization for the regressor. Since these techniques are all related to the two-stream design, we name the proposed network as Dual Shot Face Detector (DSFD). Extensive experiments on popular benchmarks, WIDER FACE and FDDB, demonstrate the superiority of DSFD over the state-of-the-art face detectors.



### Cross-Resolution Person Re-identification with Deep Antithetical Learning
- **Arxiv ID**: http://arxiv.org/abs/1810.10221v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.10221v1)
- **Published**: 2018-10-24 07:33:30+00:00
- **Updated**: 2018-10-24 07:33:30+00:00
- **Authors**: Zijie Zhuang, Haizhou Ai, Long Chen, Chong Shang
- **Comment**: None
- **Journal**: None
- **Summary**: Images with different resolutions are ubiquitous in public person re-identification (ReID) datasets and real-world scenes, it is thus crucial for a person ReID model to handle the image resolution variations for improving its generalization ability. However, most existing person ReID methods pay little attention to this resolution discrepancy problem. One paradigm to deal with this problem is to use some complicated methods for mapping all images into an artificial image space, which however will disrupt the natural image distribution and requires heavy image preprocessing. In this paper, we analyze the deficiencies of several widely-used objective functions handling image resolution discrepancies and propose a new framework called deep antithetical learning that directly learns from the natural image space rather than creating an arbitrary one. We first quantify and categorize original training images according to their resolutions. Then we create an antithetical training set and make sure that original training images have counterparts with antithetical resolutions in this new set. At last, a novel Contrastive Center Loss(CCL) is proposed to learn from images with different resolutions without being interfered by their resolution discrepancies. Extensive experimental analyses and evaluations indicate that the proposed framework, even using a vanilla deep ReID network, exhibits remarkable performance improvements. Without bells and whistles, our approach outperforms previous state-of-the-art methods by a large margin.



### Learning color space adaptation from synthetic to real images of cirrus clouds
- **Arxiv ID**: http://arxiv.org/abs/1810.10286v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.10286v2)
- **Published**: 2018-10-24 10:53:03+00:00
- **Updated**: 2020-11-17 03:28:40+00:00
- **Authors**: Qing Lyu, Minghao Chen, Xiang Chen
- **Comment**: 12 pages, 13 figures
- **Journal**: None
- **Summary**: Cloud segmentation plays a crucial role in image analysis for climate modeling. Manually labeling the training data for cloud segmentation is time-consuming and error-prone. We explore to train segmentation networks with synthetic data due to the natural acquisition of pixel-level labels. Nevertheless, the domain gap between synthetic and real images significantly degrades the performance of the trained model. We propose a color space adaptation method to bridge the gap, by training a color-sensitive generator and discriminator to adapt synthetic data to real images in color space. Instead of transforming images by general convolutional kernels, we adopt a set of closed-form operations to make color-space adjustments while preserving the labels. We also construct a synthetic-to-real cirrus cloud dataset SynCloud and demonstrate the adaptation efficacy on the semantic segmentation task of cirrus clouds. With our adapted synthetic data for training the semantic segmentation, we achieve an improvement of 6:59% when applied to real images, superior to alternative methods.



### Mask Propagation Network for Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1810.10289v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.10289v1)
- **Published**: 2018-10-24 10:59:51+00:00
- **Updated**: 2018-10-24 10:59:51+00:00
- **Authors**: Jia Sun, Dongdong Yu, Yinghong Li, Changhu Wang
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we propose a mask propagation network to treat the video segmentation problem as a concept of the guided instance segmentation. Similar to most MaskTrack based video segmentation methods, our method takes the mask probability map of previous frame and the appearance of current frame as inputs, and predicts the mask probability map for the current frame. Specifically, we adopt the Xception backbone based DeepLab v3+ model as the probability map predictor in our prediction pipeline. Besides, instead of the full image and the original mask probability, our network takes the region of interest of the instance, and the new mask probability which warped by the optical flow between the previous and current frames as the inputs. We also ensemble the modified One-Shot Video Segmentation Network to make the final predictions in order to retrieve and segment the missing instance.



### Coarse-to-fine volumetric segmentation of teeth in Cone-Beam CT
- **Arxiv ID**: http://arxiv.org/abs/1810.10293v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1810.10293v1)
- **Published**: 2018-10-24 11:07:44+00:00
- **Updated**: 2018-10-24 11:07:44+00:00
- **Authors**: Matvey Ezhov, Adel Zakirov, Maxim Gusarev
- **Comment**: None
- **Journal**: None
- **Summary**: We consider the problem of localizing and segmenting individual teeth inside 3D Cone-Beam Computed Tomography (CBCT) images. To handle large image sizes we approach this task with a coarse-to-fine framework, where the whole volume is first analyzed as a 33-class semantic segmentation (adults have up to 32 teeth) in coarse resolution, followed by binary semantic segmentation of the cropped region of interest in original resolution. To improve the performance of the challenging 33-class segmentation, we first train the Coarse step model on a large weakly labeled dataset, then fine-tune it on a smaller precisely labeled dataset. The Fine step model is trained with precise labels only. Experiments using our in-house dataset show significant improvement for both weakly-supervised pretraining and for the addition of the Fine step. Empirically, this framework yields precise teeth masks with low localization errors sufficient for many real-world applications.



### Dental pathology detection in 3D cone-beam CT
- **Arxiv ID**: http://arxiv.org/abs/1810.10309v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.10309v1)
- **Published**: 2018-10-24 12:04:11+00:00
- **Updated**: 2018-10-24 12:04:11+00:00
- **Authors**: Adel Zakirov, Matvey Ezhov, Maxim Gusarev, Vladimir Alexandrovsky, Evgeny Shumilov
- **Comment**: None
- **Journal**: None
- **Summary**: Cone-beam computed tomography (CBCT) is a valuable imaging method in dental diagnostics that provides information not available in traditional 2D imaging. However, interpretation of CBCT images is a time-consuming process that requires a physician to work with complicated software. In this work we propose an automated pipeline composed of several deep convolutional neural networks and algorithmic heuristics. Our task is two-fold: a) find locations of each present tooth inside a 3D image volume, and b) detect several common tooth conditions in each tooth. The proposed system achieves 96.3\% accuracy in tooth localization and an average of 0.94 AUROC for 6 common tooth conditions.



### Generative adversarial networks and adversarial methods in biomedical image analysis
- **Arxiv ID**: http://arxiv.org/abs/1810.10352v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.10352v1)
- **Published**: 2018-10-24 12:36:11+00:00
- **Updated**: 2018-10-24 12:36:11+00:00
- **Authors**: Jelmer M. Wolterink, Konstantinos Kamnitsas, Christian Ledig, Ivana Išgum
- **Comment**: None
- **Journal**: None
- **Summary**: Generative adversarial networks (GANs) and other adversarial methods are based on a game-theoretical perspective on joint optimization of two neural networks as players in a game. Adversarial techniques have been extensively used to synthesize and analyze biomedical images. We provide an introduction to GANs and adversarial methods, with an overview of biomedical image analysis tasks that have benefited from such methods. We conclude with a discussion of strengths and limitations of adversarial methods in biomedical image analysis, and propose potential future research directions.



### UAVid: A Semantic Segmentation Dataset for UAV Imagery
- **Arxiv ID**: http://arxiv.org/abs/1810.10438v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.10438v2)
- **Published**: 2018-10-24 15:08:11+00:00
- **Updated**: 2020-05-18 10:20:12+00:00
- **Authors**: Ye Lyu, George Vosselman, Guisong Xia, Alper Yilmaz, Michael Ying Yang
- **Comment**: Accepted by ISPRS Journal of Photogrammetry and Remote Sensing
- **Journal**: None
- **Summary**: Semantic segmentation has been one of the leading research interests in computer vision recently. It serves as a perception foundation for many fields, such as robotics and autonomous driving. The fast development of semantic segmentation attributes enormously to the large scale datasets, especially for the deep learning related methods. There already exist several semantic segmentation datasets for comparison among semantic segmentation methods in complex urban scenes, such as the Cityscapes and CamVid datasets, where the side views of the objects are captured with a camera mounted on the driving car. There also exist semantic labeling datasets for the airborne images and the satellite images, where the top views of the objects are captured. However, only a few datasets capture urban scenes from an oblique Unmanned Aerial Vehicle (UAV) perspective, where both of the top view and the side view of the objects can be observed, providing more information for object recognition. In this paper, we introduce our UAVid dataset, a new high-resolution UAV semantic segmentation dataset as a complement, which brings new challenges, including large scale variation, moving object recognition and temporal consistency preservation. Our UAV dataset consists of 30 video sequences capturing 4K high-resolution images in slanted views. In total, 300 images have been densely labeled with 8 classes for the semantic labeling task. We have provided several deep learning baseline methods with pre-training, among which the proposed Multi-Scale-Dilation net performs the best via multi-scale feature extraction. Our UAVid website and the labeling tool have been published https://uavid.nl/.



### Machine Learning Algorithms for Classification of Microcirculation Images from Septic and Non-Septic Patients
- **Arxiv ID**: http://arxiv.org/abs/1811.02659v2
- **DOI**: 10.1109/ICMLA.2018.00097
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.02659v2)
- **Published**: 2018-10-24 15:34:18+00:00
- **Updated**: 2019-02-20 16:50:47+00:00
- **Authors**: Perikumar Javia, Aman Rana, Nathan Shapiro, Pratik Shah
- **Comment**: Accepted for publication at 2018 IEEE International Conference on
  Machine Learning and Applications (IEEE ICMLA)
- **Journal**: None
- **Summary**: Sepsis is a life-threatening disease and one of the major causes of death in hospitals. Imaging of microcirculatory dysfunction is a promising approach for automated diagnosis of sepsis. We report a machine learning classifier capable of distinguishing non-septic and septic images from dark field microcirculation videos of patients. The classifier achieves an accuracy of 89.45%. The area under the receiver operating characteristics of the classifier was 0.92, the precision was 0.92 and the recall was 0.84. Codes representing the learned feature space of trained classifier were visualized using t-SNE embedding and were separable and distinguished between images from critically ill and non-septic patients. Using an unsupervised convolutional autoencoder, independent of the clinical diagnosis, we also report clustering of learned features from a compressed representation associated with healthy images and those with microcirculatory dysfunction. The feature space used by our trained classifier to distinguish between images from septic and non-septic patients has potential diagnostic application.



### Neighbourhood Consensus Networks
- **Arxiv ID**: http://arxiv.org/abs/1810.10510v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1810.10510v2)
- **Published**: 2018-10-24 17:45:17+00:00
- **Updated**: 2018-11-29 07:35:45+00:00
- **Authors**: Ignacio Rocco, Mircea Cimpoi, Relja Arandjelović, Akihiko Torii, Tomas Pajdla, Josef Sivic
- **Comment**: In Proceedings of the 32nd Conference on Neural Information
  Processing Systems (NeurIPS 2018)
- **Journal**: None
- **Summary**: We address the problem of finding reliable dense correspondences between a pair of images. This is a challenging task due to strong appearance differences between the corresponding scene elements and ambiguities generated by repetitive patterns. The contributions of this work are threefold. First, inspired by the classic idea of disambiguating feature matches using semi-local constraints, we develop an end-to-end trainable convolutional neural network architecture that identifies sets of spatially consistent matches by analyzing neighbourhood consensus patterns in the 4D space of all possible correspondences between a pair of images without the need for a global geometric model. Second, we demonstrate that the model can be trained effectively from weak supervision in the form of matching and non-matching image pairs without the need for costly manual annotation of point to point correspondences. Third, we show the proposed neighbourhood consensus network can be applied to a range of matching tasks including both category- and instance-level matching, obtaining the state-of-the-art results on the PF Pascal dataset and the InLoc indoor visual localization benchmark.



### Spatiotemporal CNNs for Pornography Detection in Videos
- **Arxiv ID**: http://arxiv.org/abs/1810.10519v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.10519v1)
- **Published**: 2018-10-24 17:55:22+00:00
- **Updated**: 2018-10-24 17:55:22+00:00
- **Authors**: Murilo Varges da Silva, Aparecido Nilceu Marana
- **Comment**: None
- **Journal**: Proceedings of 23rd Iberoamerican Congress on Pattern Recognition
  (CIARP) 2018
- **Summary**: With the increasing use of social networks and mobile devices, the number of videos posted on the Internet is growing exponentially. Among the inappropriate contents published on the Internet, pornography is one of the most worrying as it can be accessed by teens and children. Two spatiotemporal CNNs, VGG-C3D CNN and ResNet R(2+1)D CNN, were assessed for pornography detection in videos in the present study. Experimental results using the Pornography-800 dataset showed that these spatiotemporal CNNs performed better than some state-of-the-art methods based on bag of visual words and are competitive with other CNN-based approaches, reaching accuracy of 95.1%.



### Fast and accurate object detection in high resolution 4K and 8K video using GPUs
- **Arxiv ID**: http://arxiv.org/abs/1810.10551v1
- **DOI**: 10.1109/HPEC.2018.8547574
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1810.10551v1)
- **Published**: 2018-10-24 18:00:06+00:00
- **Updated**: 2018-10-24 18:00:06+00:00
- **Authors**: Vít Růžička, Franz Franchetti
- **Comment**: 6 pages, 12 figures, Best Paper Finalist at IEEE High Performance
  Extreme Computing Conference (HPEC) 2018; copyright 2018 IEEE; (DOI will be
  filled when known)
- **Journal**: 2018 IEEE High Performance extreme Computing Conference (HPEC)
- **Summary**: Machine learning has celebrated a lot of achievements on computer vision tasks such as object detection, but the traditionally used models work with relatively low resolution images. The resolution of recording devices is gradually increasing and there is a rising need for new methods of processing high resolution data. We propose an attention pipeline method which uses two staged evaluation of each image or video frame under rough and refined resolution to limit the total number of necessary evaluations. For both stages, we make use of the fast object detection model YOLO v2. We have implemented our model in code, which distributes the work across GPUs. We maintain high accuracy while reaching the average performance of 3-6 fps on 4K video and 2 fps on 8K video.



### Multimodal Polynomial Fusion for Detecting Driver Distraction
- **Arxiv ID**: http://arxiv.org/abs/1810.10565v1
- **DOI**: 10.21437/Interspeech.2018-2011
- **Categories**: **cs.CV**, cs.AI, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1810.10565v1)
- **Published**: 2018-10-24 18:16:42+00:00
- **Updated**: 2018-10-24 18:16:42+00:00
- **Authors**: Yulun Du, Chirag Raman, Alan W Black, Louis-Philippe Morency, Maxine Eskenazi
- **Comment**: INTERSPEECH 2018
- **Journal**: None
- **Summary**: Distracted driving is deadly, claiming 3,477 lives in the U.S. in 2015 alone. Although there has been a considerable amount of research on modeling the distracted behavior of drivers under various conditions, accurate automatic detection using multiple modalities and especially the contribution of using the speech modality to improve accuracy has received little attention. This paper introduces a new multimodal dataset for distracted driving behavior and discusses automatic distraction detection using features from three modalities: facial expression, speech and car signals. Detailed multimodal feature analysis shows that adding more modalities monotonically increases the predictive accuracy of the model. Finally, a simple and effective multimodal fusion technique using a polynomial fusion layer shows superior distraction detection results compared to the baseline SVM and neural network models.



### Band Selection from Hyperspectral Images Using Attention-based Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1811.02667v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.02667v3)
- **Published**: 2018-10-24 19:32:48+00:00
- **Updated**: 2020-01-09 08:50:48+00:00
- **Authors**: Pablo Ribalta Lorenzo, Lukasz Tulczyjew, Michal Marcinkiewicz, Jakub Nalepa
- **Comment**: This is an initial draft of the paper submitted to IEEE ACCESS
- **Journal**: None
- **Summary**: This paper introduces new attention-based convolutional neural networks for selecting bands from hyperspectral images. The proposed approach re-uses convolutional activations at different depths, identifying the most informative regions of the spectrum with the help of gating mechanisms. Our attention techniques are modular and easy to implement, and they can be seamlessly trained end-to-end using gradient descent. Our rigorous experiments showed that deep models equipped with the attention mechanism deliver high-quality classification, and repeatedly identify significant bands in the training data, permitting the creation of refined and extremely compact sets that retain the most meaningful features.



### The speaker-independent lipreading play-off; a survey of lipreading machines
- **Arxiv ID**: http://arxiv.org/abs/1810.10597v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1810.10597v1)
- **Published**: 2018-10-24 20:06:19+00:00
- **Updated**: 2018-10-24 20:06:19+00:00
- **Authors**: Jake Burton, David Frank, Madhi Saleh, Nassir Navab, Helen L. Bear
- **Comment**: To appear at the third IEEE International Conference on Image
  Processing, Applications and Systems 2018
- **Journal**: None
- **Summary**: Lipreading is a difficult gesture classification task. One problem in computer lipreading is speaker-independence. Speaker-independence means to achieve the same accuracy on test speakers not included in the training set as speakers within the training set. Current literature is limited on speaker-independent lipreading, the few independent test speaker accuracy scores are usually aggregated within dependent test speaker accuracies for an averaged performance. This leads to unclear independent results. Here we undertake a systematic survey of experiments with the TCD-TIMIT dataset using both conventional approaches and deep learning methods to provide a series of wholly speaker-independent benchmarks and show that the best speaker-independent machine scores 69.58% accuracy with CNN features and an SVM classifier. This is less than state of the art speaker-dependent lipreading machines, but greater than previously reported in independence experiments.



