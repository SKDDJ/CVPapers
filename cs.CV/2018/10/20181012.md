# Arxiv Papers in cs.CV on 2018-10-12
### Dynamic Channel Pruning: Feature Boosting and Suppression
- **Arxiv ID**: http://arxiv.org/abs/1810.05331v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.05331v2)
- **Published**: 2018-10-12 03:00:59+00:00
- **Updated**: 2019-01-28 05:25:48+00:00
- **Authors**: Xitong Gao, Yiren Zhao, ≈Åukasz Dudziak, Robert Mullins, Cheng-zhong Xu
- **Comment**: 14 pages, 5 figures, 4 tables, published as a conference paper at
  ICLR 2019
- **Journal**: None
- **Summary**: Making deep convolutional neural networks more accurate typically comes at the cost of increased computational and memory resources. In this paper, we reduce this cost by exploiting the fact that the importance of features computed by convolutional layers is highly input-dependent, and propose feature boosting and suppression (FBS), a new method to predictively amplify salient convolutional channels and skip unimportant ones at run-time. FBS introduces small auxiliary connections to existing convolutional layers. In contrast to channel pruning methods which permanently remove channels, it preserves the full network structures and accelerates convolution by dynamically skipping unimportant input and output channels. FBS-augmented networks are trained with conventional stochastic gradient descent, making it readily available for many state-of-the-art CNNs. We compare FBS to a range of existing channel pruning and dynamic execution schemes and demonstrate large improvements on ImageNet classification. Experiments show that FBS can respectively provide $5\times$ and $2\times$ savings in compute on VGG-16 and ResNet-18, both with less than $0.6\%$ top-5 accuracy loss.



### 4D Human Body Correspondences from Panoramic Depth Maps
- **Arxiv ID**: http://arxiv.org/abs/1810.05340v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.05340v1)
- **Published**: 2018-10-12 03:33:20+00:00
- **Updated**: 2018-10-12 03:33:20+00:00
- **Authors**: Zhong Li, Minye Wu, Wangyiteng Zhou, Jingyi Yu
- **Comment**: 10 pages, 12 figures, CVPR 2018 paper
- **Journal**: None
- **Summary**: The availability of affordable 3D full body reconstruction systems has given rise to free-viewpoint video (FVV) of human shapes. Most existing solutions produce temporally uncorrelated point clouds or meshes with unknown point/vertex correspondences. Individually compressing each frame is ineffective and still yields to ultra-large data sizes. We present an end-to-end deep learning scheme to establish dense shape correspondences and subsequently compress the data. Our approach uses sparse set of "panoramic" depth maps or PDMs, each emulating an inward-viewing concentric mosaics. We then develop a learning-based technique to learn pixel-wise feature descriptors on PDMs. The results are fed into an autoencoder-based network for compression. Comprehensive experiments demonstrate our solution is robust and effective on both public and our newly captured datasets.



### Efficient architecture for deep neural networks with heterogeneous sensitivity
- **Arxiv ID**: http://arxiv.org/abs/1810.05358v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.05358v3)
- **Published**: 2018-10-12 05:09:12+00:00
- **Updated**: 2019-06-05 02:40:15+00:00
- **Authors**: Hyunjoong Cho, Jinhyeok Jang, Chanhyeok Lee, Seungjoon Yang
- **Comment**: None
- **Journal**: None
- **Summary**: This work presents a neural network that consists of nodes with heterogeneous sensitivity. Each node in a network is assigned a variable that determines the sensitivity with which it learns to perform a given task. The network is trained by a constrained optimization that maximizes the sparsity of the sensitivity variables while ensuring the network's performance. As a result, the network learns to perform a given task using only a small number of sensitive nodes. Insensitive nodes, the nodes with zero sensitivity, can be removed from a trained network to obtain a computationally efficient network. Removing zero-sensitivity nodes has no effect on the network's performance because the network has already been trained to perform the task without them. The regularization parameter used to solve the optimization problem is found simultaneously during the training of networks. To validate our approach, we design networks with computationally efficient architectures for various tasks such as autoregression, object recognition, facial expression recognition, and object detection using various datasets. In our experiments, the networks designed by the proposed method provide the same or higher performance but with far less computational complexity.



### Unsupervised Facial Geometry Learning for Sketch to Photo Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1810.05361v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.05361v1)
- **Published**: 2018-10-12 05:22:56+00:00
- **Updated**: 2018-10-12 05:22:56+00:00
- **Authors**: Hadi Kazemi, Fariborz Taherkhani, Nasser M. Nasrabadi
- **Comment**: Published as a conference paper in BIOSIG 2018
- **Journal**: None
- **Summary**: Face sketch-photo synthesis is a critical application in law enforcement and digital entertainment industry where the goal is to learn the mapping between a face sketch image and its corresponding photo-realistic image. However, the limited number of paired sketch-photo training data usually prevents the current frameworks to learn a robust mapping between the geometry of sketches and their matching photo-realistic images. Consequently, in this work, we present an approach for learning to synthesize a photo-realistic image from a face sketch in an unsupervised fashion. In contrast to current unsupervised image-to-image translation techniques, our framework leverages a novel perceptual discriminator to learn the geometry of human face. Learning facial prior information empowers the network to remove the geometrical artifacts in the face sketch. We demonstrate that a simultaneous optimization of the face photo generator network, employing the proposed perceptual discriminator in combination with a texture-wise discriminator, results in a significant improvement in quality and recognition rate of the synthesized photos. We evaluate the proposed network by conducting extensive experiments on multiple baseline sketch-photo datasets.



### FPGA-based Acceleration System for Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/1810.05367v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1810.05367v2)
- **Published**: 2018-10-12 05:46:05+00:00
- **Updated**: 2018-10-15 00:31:32+00:00
- **Authors**: Ke Song, Chun Yuan, Peng Gao, Yunxu Sun
- **Comment**: Accepted by IEEE 14th International Conference on Solid-State and
  Integrated Circuit Technology (ICSICT)
- **Journal**: None
- **Summary**: Visual tracking is one of the most important application areas of computer vision. At present, most algorithms are mainly implemented on PCs, and it is difficult to ensure real-time performance when applied in the real scenario. In order to improve the tracking speed and reduce the overall power consumption of visual tracking, this paper proposes a real-time visual tracking algorithm based on DSST(Discriminative Scale Space Tracking) approach. We implement a hardware system on Xilinx XC7K325T FPGA platform based on our proposed visual tracking algorithm. Our hardware system can run at more than 153 frames per second. In order to reduce the resource occupation, our system adopts the batch processing method in the feature extraction module. In the filter processing module, the FFT IP core is time-division multiplexed. Therefore, our hardware system utilizes LUTs and storage blocks of 33% and 40%, respectively. Test results show that the proposed visual tracking hardware system has excellent performance.



### Sequential Learning of Movement Prediction in Dynamic Environments using LSTM Autoencoder
- **Arxiv ID**: http://arxiv.org/abs/1810.05394v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.RO, stat.ML, 68T05
- **Links**: [PDF](http://arxiv.org/pdf/1810.05394v1)
- **Published**: 2018-10-12 08:11:13+00:00
- **Updated**: 2018-10-12 08:11:13+00:00
- **Authors**: Meenakshi Sarkar, Debasish Ghose
- **Comment**: 4 pages
- **Journal**: None
- **Summary**: Predicting movement of objects while the action of learning agent interacts with the dynamics of the scene still remains a key challenge in robotics. We propose a multi-layer Long Short Term Memory (LSTM) autoendocer network that predicts future frames for a robot navigating in a dynamic environment with moving obstacles. The autoencoder network is composed of a state and action conditioned decoder network that reconstructs the future frames of video, conditioned on the action taken by the agent. The input image frames are first transformed into low dimensional feature vectors with a pre-trained encoder network and then reconstructed with the LSTM autoencoder network to generate the future frames. A virtual environment, based on the OpenAi-Gym framework for robotics, is used to gather training data and test the proposed network. The initial experiments show promising results indicating that these predicted frames can be used by an appropriate reinforcement learning framework in future to navigate around dynamic obstacles.



### Point Cloud Colorization Based on Densely Annotated 3D Shape Dataset
- **Arxiv ID**: http://arxiv.org/abs/1810.05396v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.05396v1)
- **Published**: 2018-10-12 08:18:24+00:00
- **Updated**: 2018-10-12 08:18:24+00:00
- **Authors**: Xu Cao, Katashi Nagao
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces DensePoint, a densely sampled and annotated point cloud dataset containing over 10,000 single objects across 16 categories, by merging different kind of information from two existing datasets. Each point cloud in DensePoint contains 40,000 points, and each point is associated with two sorts of information: RGB value and part annotation. In addition, we propose a method for point cloud colorization by utilizing Generative Adversarial Networks (GANs). The network makes it possible to generate colours for point clouds of single objects by only giving the point cloud itself. Experiments on DensePoint show that there exist clear boundaries in point clouds between different parts of an object, suggesting that the proposed network is able to generate reasonably good colours. Our dataset is publicly available on the project page.



### Thermal Infrared Colorization via Conditional Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/1810.05399v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.05399v2)
- **Published**: 2018-10-12 08:21:04+00:00
- **Updated**: 2018-11-05 03:11:40+00:00
- **Authors**: Xiaodong Kuang, Xiubao Sui, Chengwei Liu, Yuan Liu, Qian Chen, Guohua Gu
- **Comment**: None
- **Journal**: None
- **Summary**: Transforming a thermal infrared image into a realistic RGB image is a challenging task. In this paper we propose a deep learning method to bridge this gap. We propose learning the transformation mapping using a coarse-to-fine generator that preserves the details. Since the standard mean squared loss cannot penalize the distance between colorized and ground truth images well, we propose a composite loss function that combines content, adversarial, perceptual and total variation losses. The content loss is used to recover global image information while the latter three losses are used to synthesize local realistic textures. Quantitative and qualitative experiments demonstrate that our approach significantly outperforms existing approaches.



### A Gentle Introduction to Deep Learning in Medical Image Processing
- **Arxiv ID**: http://arxiv.org/abs/1810.05401v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.05401v2)
- **Published**: 2018-10-12 08:27:53+00:00
- **Updated**: 2018-12-21 13:37:23+00:00
- **Authors**: Andreas Maier, Christopher Syben, Tobias Lasser, Christian Riess
- **Comment**: Accepted by Journal of Medical Physics; Final Version after review
- **Journal**: None
- **Summary**: This paper tries to give a gentle introduction to deep learning in medical image processing, proceeding from theoretical foundations to applications. We first discuss general reasons for the popularity of deep learning, including several major breakthroughs in computer science. Next, we start reviewing the fundamental basics of the perceptron and neural networks, along with some fundamental theory that is often omitted. Doing so allows us to understand the reasons for the rise of deep learning in many application domains. Obviously medical image processing is one of these areas which has been largely affected by this rapid progress, in particular in image detection and recognition, image segmentation, image registration, and computer-aided diagnosis. There are also recent trends in physical simulation, modelling, and reconstruction that have led to astonishing results. Yet, some of these approaches neglect prior knowledge and hence bear the risk of producing implausible results. These apparent weaknesses highlight current limitations of deep learning. However, we also briefly discuss promising approaches that might be able to resolve these problems in the future.



### Cryo-CARE: Content-Aware Image Restoration for Cryo-Transmission Electron Microscopy Data
- **Arxiv ID**: http://arxiv.org/abs/1810.05420v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1810.05420v2)
- **Published**: 2018-10-12 09:15:06+00:00
- **Updated**: 2018-10-15 18:49:50+00:00
- **Authors**: Tim-Oliver Buchholz, Mareike Jordan, Gaia Pigino, Florian Jug
- **Comment**: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible. This version fixed flipped graph labels in Figure 5
- **Journal**: None
- **Summary**: Multiple approaches to use deep learning for image restoration have recently been proposed. Training such approaches requires well registered pairs of high and low quality images. While this is easily achievable for many imaging modalities, e.g. fluorescence light microscopy, for others it is not. Cryo-transmission electron microscopy (cryo-TEM) could profoundly benefit from improved denoising methods, unfortunately it is one of the latter. Here we show how recent advances in network training for image restoration tasks, i.e. denoising, can be applied to cryo-TEM data. We describe our proposed method and show how it can be applied to single cryo-TEM projections and whole cryo-tomographic image volumes. Our proposed restoration method dramatically increases contrast in cryo-TEM images, which improves the interpretability of the acquired data. Furthermore we show that automated downstream processing on restored image data, demonstrated on a dense segmentation task, leads to improved results.



### DeepScores and Deep Watershed Detection: current state and open issues
- **Arxiv ID**: http://arxiv.org/abs/1810.05423v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.05423v1)
- **Published**: 2018-10-12 09:16:42+00:00
- **Updated**: 2018-10-12 09:16:42+00:00
- **Authors**: Ismail Elezi, Lukas Tuggener, Marcello Pelillo, Thilo Stadelmann
- **Comment**: Published on WORMS workshop (ISMIR affiliated workshop)
- **Journal**: None
- **Summary**: This paper gives an overview of our current Optical Music Recognition (OMR) research. We recently released the OMR dataset \emph{DeepScores} as well as the object detection method \emph{Deep Watershed Detector}. We are currently taking some additional steps to improve both of them. Here we summarize current and future efforts, aimed at improving usefulness on real-world task and tackling extreme class imbalance.



### Real-time self-adaptive deep stereo
- **Arxiv ID**: http://arxiv.org/abs/1810.05424v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.05424v2)
- **Published**: 2018-10-12 09:17:53+00:00
- **Updated**: 2019-04-05 09:57:29+00:00
- **Authors**: Alessio Tonioni, Fabio Tosi, Matteo Poggi, Stefano Mattoccia, Luigi Di Stefano
- **Comment**: Accepted at CVPR2019 as oral presentation. Code Available
  https://github.com/CVLAB-Unibo/Real-time-self-adaptive-deep-stereo
- **Journal**: None
- **Summary**: Deep convolutional neural networks trained end-to-end are the state-of-the-art methods to regress dense disparity maps from stereo pairs. These models, however, suffer from a notable decrease in accuracy when exposed to scenarios significantly different from the training set, e.g., real vs synthetic images, etc.). We argue that it is extremely unlikely to gather enough samples to achieve effective training/tuning in any target domain, thus making this setup impractical for many applications. Instead, we propose to perform unsupervised and continuous online adaptation of a deep stereo network, which allows for preserving its accuracy in any environment. However, this strategy is extremely computationally demanding and thus prevents real-time inference. We address this issue introducing a new lightweight, yet effective, deep stereo architecture, Modularly ADaptive Network (MADNet) and developing a Modular ADaptation (MAD) algorithm, which independently trains sub-portions of the network. By deploying MADNet together with MAD we introduce the first real-time self-adaptive deep stereo system enabling competitive performance on heterogeneous datasets.



### MPTV: Matching Pursuit Based Total Variation Minimization for Image Deconvolution
- **Arxiv ID**: http://arxiv.org/abs/1810.05438v1
- **DOI**: 10.1109/TIP.2018.2875352
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.05438v1)
- **Published**: 2018-10-12 10:07:05+00:00
- **Updated**: 2018-10-12 10:07:05+00:00
- **Authors**: Dong Gong, Mingkui Tan, Qinfeng Shi, Anton van den Hengel, Yanning Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Total variation (TV) regularization has proven effective for a range of computer vision tasks through its preferential weighting of sharp image edges. Existing TV-based methods, however, often suffer from the over-smoothing issue and solution bias caused by the homogeneous penalization. In this paper, we consider addressing these issues by applying inhomogeneous regularization on different image components. We formulate the inhomogeneous TV minimization problem as a convex quadratic constrained linear programming problem. Relying on this new model, we propose a matching pursuit based total variation minimization method (MPTV), specifically for image deconvolution. The proposed MPTV method is essentially a cutting-plane method, which iteratively activates a subset of nonzero image gradients, and then solves a subproblem focusing on those activated gradients only. Compared to existing methods, MPTV is less sensitive to the choice of the trade-off parameter between data fitting and regularization. Moreover, the inhomogeneity of MPTV alleviates the over-smoothing and ringing artifacts, and improves the robustness to errors in blur kernel. Extensive experiments on different tasks demonstrate the superiority of the proposed method over the current state-of-the-art.



### Cats or CAT scans: transfer learning from natural or medical image source datasets?
- **Arxiv ID**: http://arxiv.org/abs/1810.05444v2
- **DOI**: 10.1016/j.cobme.2018.12.005
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1810.05444v2)
- **Published**: 2018-10-12 10:35:21+00:00
- **Updated**: 2019-01-10 08:45:08+00:00
- **Authors**: Veronika Cheplygina
- **Comment**: Accepted to Current Opinion in Biomedical Engineering
- **Journal**: None
- **Summary**: Transfer learning is a widely used strategy in medical image analysis. Instead of only training a network with a limited amount of data from the target task of interest, we can first train the network with other, potentially larger source datasets, creating a more robust model. The source datasets do not have to be related to the target task. For a classification task in lung CT images, we could use both head CT images, or images of cats, as the source. While head CT images appear more similar to lung CT images, the number and diversity of cat images might lead to a better model overall. In this survey we review a number of papers that have performed similar comparisons. Although the answer to which strategy is best seems to be "it depends", we discuss a number of research directions we need to take as a community, to gain more understanding of this topic.



### Modeling Varying Camera-IMU Time Offset in Optimization-Based Visual-Inertial Odometry
- **Arxiv ID**: http://arxiv.org/abs/1810.05456v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1810.05456v1)
- **Published**: 2018-10-12 11:35:27+00:00
- **Updated**: 2018-10-12 11:35:27+00:00
- **Authors**: Yonggen Ling, Linchao Bao, Zequn Jie, Fengming Zhu, Ziyang Li, Shanmin Tang, Yongsheng Liu, Wei Liu, Tong Zhang
- **Comment**: European Conference on Computer Vision 2018
- **Journal**: None
- **Summary**: Combining cameras and inertial measurement units (IMUs) has been proven effective in motion tracking, as these two sensing modalities offer complementary characteristics that are suitable for fusion. While most works focus on global-shutter cameras and synchronized sensor measurements, consumer-grade devices are mostly equipped with rolling-shutter cameras and suffer from imperfect sensor synchronization. In this work, we propose a nonlinear optimization-based monocular visual inertial odometry (VIO) with varying camera-IMU time offset modeled as an unknown variable. Our approach is able to handle the rolling-shutter effects and imperfect sensor synchronization in a unified way. Additionally, we introduce an efficient algorithm based on dynamic programming and red-black tree to speed up IMU integration over variable-length time intervals during the optimization. An uncertainty-aware initialization is also presented to launch the VIO robustly. Comparisons with state-of-the-art methods on the Euroc dataset and mobile phone data are shown to validate the effectiveness of our approach.



### Embedding Geographic Locations for Modelling the Natural Environment using Flickr Tags and Structured Data
- **Arxiv ID**: http://arxiv.org/abs/1810.12091v1
- **DOI**: None
- **Categories**: **cs.IR**, cs.CL, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.12091v1)
- **Published**: 2018-10-12 12:22:34+00:00
- **Updated**: 2018-10-12 12:22:34+00:00
- **Authors**: Shelan S. Jeawak, Christopher B. Jones, Steven Schockaert
- **Comment**: None
- **Journal**: None
- **Summary**: Meta-data from photo-sharing websites such as Flickr can be used to obtain rich bag-of-words descriptions of geographic locations, which have proven valuable, among others, for modelling and predicting ecological features. One important insight from previous work is that the descriptions obtained from Flickr tend to be complementary to the structured information that is available from traditional scientific resources. To better integrate these two diverse sources of information, in this paper we consider a method for learning vector space embeddings of geographic locations. We show experimentally that this method improves on existing approaches, especially in cases where structured information is available.



### Effects of Image Degradations to CNN-based Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1810.05552v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.05552v1)
- **Published**: 2018-10-12 14:38:52+00:00
- **Updated**: 2018-10-12 14:38:52+00:00
- **Authors**: Yanting Pei, Yaping Huang, Qi Zou, Hao Zang, Xingyuan Zhang, Song Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Just like many other topics in computer vision, image classification has achieved significant progress recently by using deep-learning neural networks, especially the Convolutional Neural Networks (CNN). Most of the existing works are focused on classifying very clear natural images, evidenced by the widely used image databases such as Caltech-256, PASCAL VOCs and ImageNet. However, in many real applications, the acquired images may contain certain degradations that lead to various kinds of blurring, noise, and distortions. One important and interesting problem is the effect of such degradations to the performance of CNN-based image classification. More specifically, we wonder whether image-classification performance drops with each kind of degradation, whether this drop can be avoided by including degraded images into training, and whether existing computer vision algorithms that attempt to remove such degradations can help improve the image-classification performance. In this paper, we empirically study this problem for four kinds of degraded images -- hazy images, underwater images, motion-blurred images and fish-eye images. For this study, we synthesize a large number of such degraded images by applying respective physical models to the clear natural images and collect a new hazy image dataset from the Internet. We expect this work can draw more interests from the community to study the classification of degraded images.



### PointGrow: Autoregressively Learned Point Cloud Generation with Self-Attention
- **Arxiv ID**: http://arxiv.org/abs/1810.05591v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.05591v3)
- **Published**: 2018-10-12 16:08:32+00:00
- **Updated**: 2019-11-06 18:17:41+00:00
- **Authors**: Yongbin Sun, Yue Wang, Ziwei Liu, Joshua E. Siegel, Sanjay E. Sarma
- **Comment**: None
- **Journal**: None
- **Summary**: Generating 3D point clouds is challenging yet highly desired. This work presents a novel autoregressive model, PointGrow, which can generate diverse and realistic point cloud samples from scratch or conditioned on semantic contexts. This model operates recurrently, with each point sampled according to a conditional distribution given its previously-generated points, allowing inter-point correlations to be well-exploited and 3D shape generative processes to be better interpreted. Since point cloud object shapes are typically encoded by long-range dependencies, we augment our model with dedicated self-attention modules to capture such relations. Extensive evaluations show that PointGrow achieves satisfying performance on both unconditional and conditional point cloud generation tasks, with respect to realism and diversity. Several important applications, such as unsupervised feature learning and shape arithmetic operations, are also demonstrated.



### Functionality-Oriented Convolutional Filter Pruning
- **Arxiv ID**: http://arxiv.org/abs/1810.07322v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.07322v2)
- **Published**: 2018-10-12 20:39:47+00:00
- **Updated**: 2019-09-12 03:24:06+00:00
- **Authors**: Zhuwei Qin, Fuxun Yu, Chenchen Liu, Xiang Chen
- **Comment**: None
- **Journal**: None
- **Summary**: The sophisticated structure of Convolutional Neural Network (CNN) allows for outstanding performance, but at the cost of intensive computation. As significant redundancies inevitably present in such a structure, many works have been proposed to prune the convolutional filters for computation cost reduction. Although extremely effective, most works are based only on quantitative characteristics of the convolutional filters, and highly overlook the qualitative interpretation of individual filter's specific functionality. In this work, we interpreted the functionality and redundancy of the convolutional filters from different perspectives, and proposed a functionality-oriented filter pruning method. With extensive experiment results, we proved the convolutional filters' qualitative significance regardless of magnitude, demonstrated significant neural network redundancy due to repetitive filter functions, and analyzed the filter functionality defection under inappropriate retraining process. Such an interpretable pruning approach not only offers outstanding computation cost optimization over previous filter pruning methods, but also interprets filter pruning process.



### Does Haze Removal Help CNN-based Image Classification?
- **Arxiv ID**: http://arxiv.org/abs/1810.05716v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.05716v1)
- **Published**: 2018-10-12 20:46:29+00:00
- **Updated**: 2018-10-12 20:46:29+00:00
- **Authors**: Yanting Pei, Yaping Huang, Qi Zou, Yuhang Lu, Song Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Hazy images are common in real scenarios and many dehazing methods have been developed to automatically remove the haze from images. Typically, the goal of image dehazing is to produce clearer images from which human vision can better identify the object and structural details present in the images. When the ground-truth haze-free image is available for a hazy image, quantitative evaluation of image dehazing is usually based on objective metrics, such as Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity (SSIM). However, in many applications, large-scale images are collected not for visual examination by human. Instead, they are used for many high-level vision tasks, such as automatic classification, recognition and categorization. One fundamental problem here is whether various dehazing methods can produce clearer images that can help improve the performance of the high-level tasks. In this paper, we empirically study this problem in the important task of image classification by using both synthetic and real hazy image datasets. From the experimental results, we find that the existing image-dehazing methods cannot improve much the image-classification performance and sometimes even reduce the image-classification performance.



### Graph HyperNetworks for Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/1810.05749v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.05749v3)
- **Published**: 2018-10-12 22:21:05+00:00
- **Updated**: 2020-12-18 18:01:04+00:00
- **Authors**: Chris Zhang, Mengye Ren, Raquel Urtasun
- **Comment**: ICLR 2019
- **Journal**: None
- **Summary**: Neural architecture search (NAS) automatically finds the best task-specific neural network topology, outperforming many manual architecture designs. However, it can be prohibitively expensive as the search requires training thousands of different networks, while each can last for hours. In this work, we propose the Graph HyperNetwork (GHN) to amortize the search cost: given an architecture, it directly generates the weights by running inference on a graph neural network. GHNs model the topology of an architecture and therefore can predict network performance more accurately than regular hypernetworks and premature early stopping. To perform NAS, we randomly sample architectures and use the validation accuracy of networks with GHN generated weights as the surrogate search signal. GHNs are fast -- they can search nearly 10 times faster than other random search methods on CIFAR-10 and ImageNet. GHNs can be further extended to the anytime prediction setting, where they have found networks with better speed-accuracy tradeoff than the state-of-the-art manual designs.



