# Arxiv Papers in cs.CV on 2018-10-26
### Data-specific Adaptive Threshold for Face Recognition and Authentication
- **Arxiv ID**: http://arxiv.org/abs/1810.11160v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.11160v1)
- **Published**: 2018-10-26 01:34:40+00:00
- **Updated**: 2018-10-26 01:34:40+00:00
- **Authors**: Hsin-Rung Chou, Jia-Hong Lee, Yi-Ming Chan, Chu-Song Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Many face recognition systems boost the performance using deep learning models, but only a few researches go into the mechanisms for dealing with online registration. Although we can obtain discriminative facial features through the state-of-the-art deep model training, how to decide the best threshold for practical use remains a challenge. We develop a technique of adaptive threshold mechanism to improve the recognition accuracy. We also design a face recognition system along with the registering procedure to handle online registration. Furthermore, we introduce a new evaluation protocol to better evaluate the performance of an algorithm for real-world scenarios. Under our proposed protocol, our method can achieve a 22\% accuracy improvement on the LFW dataset.



### Neural Modular Control for Embodied Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1810.11181v2
- **DOI**: None
- **Categories**: **cs.AI**, cs.CL, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1810.11181v2)
- **Published**: 2018-10-26 03:58:26+00:00
- **Updated**: 2019-05-02 23:41:47+00:00
- **Authors**: Abhishek Das, Georgia Gkioxari, Stefan Lee, Devi Parikh, Dhruv Batra
- **Comment**: 10 pages, 3 figures, 2 tables. Published at CoRL 2018. Webpage:
  https://embodiedqa.org/
- **Journal**: None
- **Summary**: We present a modular approach for learning policies for navigation over long planning horizons from language input. Our hierarchical policy operates at multiple timescales, where the higher-level master policy proposes subgoals to be executed by specialized sub-policies. Our choice of subgoals is compositional and semantic, i.e. they can be sequentially combined in arbitrary orderings, and assume human-interpretable descriptions (e.g. 'exit room', 'find kitchen', 'find refrigerator', etc.).   We use imitation learning to warm-start policies at each level of the hierarchy, dramatically increasing sample efficiency, followed by reinforcement learning. Independent reinforcement learning at each level of hierarchy enables sub-policies to adapt to consequences of their actions and recover from errors. Subsequent joint hierarchical training enables the master policy to adapt to the sub-policies.   On the challenging EQA (Das et al., 2018) benchmark in House3D (Wu et al., 2018), requiring navigating diverse realistic indoor environments, our approach outperforms prior work by a significant margin, both in terms of navigation and question answering.



### Fine-grained Video Categorization with Redundancy Reduction Attention
- **Arxiv ID**: http://arxiv.org/abs/1810.11189v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.11189v1)
- **Published**: 2018-10-26 05:03:34+00:00
- **Updated**: 2018-10-26 05:03:34+00:00
- **Authors**: Chen Zhu, Xiao Tan, Feng Zhou, Xiao Liu, Kaiyu Yue, Errui Ding, Yi Ma
- **Comment**: Correcting a typo in ECCV version
- **Journal**: None
- **Summary**: For fine-grained categorization tasks, videos could serve as a better source than static images as videos have a higher chance of containing discriminative patterns. Nevertheless, a video sequence could also contain a lot of redundant and irrelevant frames. How to locate critical information of interest is a challenging task. In this paper, we propose a new network structure, known as Redundancy Reduction Attention (RRA), which learns to focus on multiple discriminative patterns by sup- pressing redundant feature channels. Specifically, it firstly summarizes the video by weight-summing all feature vectors in the feature maps of selected frames with a spatio-temporal soft attention, and then predicts which channels to suppress or to enhance according to this summary with a learned non-linear transform. Suppression is achieved by modulating the feature maps and threshing out weak activations. The updated feature maps are then used in the next iteration. Finally, the video is classified based on multiple summaries. The proposed method achieves out- standing performances in multiple video classification datasets. Further- more, we have collected two large-scale video datasets, YouTube-Birds and YouTube-Cars, for future researches on fine-grained video categorization. The datasets are available at http://www.cs.umd.edu/~chenzhu/fgvc.



### Deep learning based 2.5D flow field estimation for maximum intensity projections of 4D optical coherence tomography
- **Arxiv ID**: http://arxiv.org/abs/1810.11205v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.11205v2)
- **Published**: 2018-10-26 07:10:51+00:00
- **Updated**: 2019-02-19 00:35:59+00:00
- **Authors**: Max-Heinrich Laves, LÃ¼der A. Kahrs, Tobias Ortmaier
- **Comment**: Accepted for SPIE Medical Imaging 2019
- **Journal**: None
- **Summary**: In microsurgery, lasers have emerged as precise tools for bone ablation. A challenge is automatic control of laser bone ablation with 4D optical coherence tomography (OCT). OCT as high resolution imaging modality provides volumetric images of tissue and foresees information of bone position and orientation (pose) as well as thickness. However, existing approaches for OCT based laser ablation control rely on external tracking systems or invasively ablated artificial landmarks for tracking the pose of the OCT probe relative to the tissue. This can be superseded by estimating the scene flow caused by relative movement between OCT-based laser ablation system and patient.   Therefore, this paper deals with 2.5D scene flow estimation of volumetric OCT images for application in laser ablation. We present a semi-supervised convolutional neural network based tracking scheme for subsequent 3D OCT volumes and apply it to a realistic semi-synthetic data set of ex vivo human temporal bone specimen. The scene flow is estimated in a two-stage approach. In the first stage, 2D lateral scene flow is computed on census-transformed en-face arguments-of-maximum intensity projections. Subsequent to this, the projections are warped by predicted lateral flow and 1D depth flow is estimated. The neural network is trained semi-supervised by combining error to ground truth and the reconstruction error of warped images with assumptions of spatial flow smoothness. Quantitative evaluation reveals a mean endpoint error of $ (4.7\pm{}3.5) $ voxel or $ 27.5 \pm 20.5 \mu\mathrm{m} $ for scene flow estimation caused by simulated relative movement between the OCT probe and bone. The scene flow estimation for 4D OCT enables its use for markerless tracking of mastoid bone structures for image guidance in general, and automated laser ablation control.



### Capsule-Forensics: Using Capsule Networks to Detect Forged Images and Videos
- **Arxiv ID**: http://arxiv.org/abs/1810.11215v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1810.11215v1)
- **Published**: 2018-10-26 07:50:29+00:00
- **Updated**: 2018-10-26 07:50:29+00:00
- **Authors**: Huy H. Nguyen, Junichi Yamagishi, Isao Echizen
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in media generation techniques have made it easier for attackers to create forged images and videos. State-of-the-art methods enable the real-time creation of a forged version of a single video obtained from a social network. Although numerous methods have been developed for detecting forged images and videos, they are generally targeted at certain domains and quickly become obsolete as new kinds of attacks appear. The method introduced in this paper uses a capsule network to detect various kinds of spoofs, from replay attacks using printed images or recorded videos to computer-generated videos using deep convolutional neural networks. It extends the application of capsule networks beyond their original intention to the solving of inverse graphics problems.



### Building Footprint Generation Using Improved Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1810.11224v1
- **DOI**: 10.1109/LGRS.2018.2878486
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.11224v1)
- **Published**: 2018-10-26 08:25:41+00:00
- **Updated**: 2018-10-26 08:25:41+00:00
- **Authors**: Yilei Shi, Qingyu Li, Xiao Xiang Zhu
- **Comment**: 5 pages
- **Journal**: None
- **Summary**: Building footprint information is an essential ingredient for 3-D reconstruction of urban models. The automatic generation of building footprints from satellite images presents a considerable challenge due to the complexity of building shapes. In this work, we have proposed improved generative adversarial networks (GANs) for the automatic generation of building footprints from satellite images. We used a conditional GAN with a cost function derived from the Wasserstein distance and added a gradient penalty term. The achieved results indicated that the proposed method can significantly improve the quality of building footprint generation compared to conditional generative adversarial networks, the U-Net, and other networks. In addition, our method nearly removes all hyperparameters tuning.



### Beyond the Leaderboard: Insight and Deployment Challenges to Address Research Problems
- **Arxiv ID**: http://arxiv.org/abs/1811.03014v1
- **DOI**: None
- **Categories**: **cs.CY**, cs.CV, cs.LG, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/1811.03014v1)
- **Published**: 2018-10-26 10:18:07+00:00
- **Updated**: 2018-10-26 10:18:07+00:00
- **Authors**: Adrienne M. Mendrik, Stephen R. Aylward
- **Comment**: This two-page abstract was accepted for the NIPS 2018 Challenges in
  Machine Learning (CiML) workshop "Machine Learning competitions "in the
  wild": Playing in the real world or in real time" on Saturday December 8,
  2018 in Palais des congres de Montreal, Canada
- **Journal**: None
- **Summary**: In the medical image analysis field, organizing challenges with associated workshops at international conferences began in 2007 and has grown to include over 150 challenges. Several of these challenges have had a major impact in the field. However, whereas well-designed challenges have the potential to unite and focus the field on creating solutions to important problems, poorly designed and documented challenges can equally impede a field and lead to pursuing incremental improvements in metric scores with no theoretic or clinical significance. This is supported by a critical assessment of challenges at the international MICCAI conference. In this assessment the main observation was that small changes to the underlying challenge data can drastically change the ranking order on the leaderboard. Related to this is the practice of leaderboard climbing, which is characterized by participants focusing on incrementally improving metric results rather than advancing science or solving the driving problem of a challenge. In this abstract we look beyond the leaderboard of a challenge and instead look at the conclusions that can be drawn from a challenge with respect to the research problem that it is addressing. Research study design is well described in other research areas and can be translated to challenge design when viewing challenges as research studies on algorithm performance that address a research problem. Based on the two main types of scientific research study design, we propose two main challenge types, which we think would benefit other research areas as well: 1) an insight challenge that is based on a qualitative study design and 2) a deployment challenge that is based on a quantitative study design. In addition we briefly touch upon related considerations with respect to statistical significance versus practical significance, generalizability and data saturation.



### Video-based Person Re-identification Using Spatial-Temporal Attention Networks
- **Arxiv ID**: http://arxiv.org/abs/1810.11261v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.11261v1)
- **Published**: 2018-10-26 10:54:39+00:00
- **Updated**: 2018-10-26 10:54:39+00:00
- **Authors**: Shivansh Rao, Tanzila Rahman, Mrigank Rochan, Yang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: We consider the problem of video-based person re-identification. The goal is to identify a person from videos captured under different cameras. In this paper, we propose an efficient spatial-temporal attention based model for person re-identification from videos. Our method generates an attention score for each frame based on frame-level features. The attention scores of all frames in a video are used to produce a weighted feature vector for the input video. Unlike most existing deep learning methods that use global representation, our approach focuses on attention scores. Extensive experiments on two benchmark datasets demonstrate that our method achieves the state-of-the-art performance. This is a technical report.



### Texture variation adaptive image denoising with nonlocal PCA
- **Arxiv ID**: http://arxiv.org/abs/1810.11282v1
- **DOI**: 10.1109/TIP.2019.2916976
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.11282v1)
- **Published**: 2018-10-26 11:53:39+00:00
- **Updated**: 2018-10-26 11:53:39+00:00
- **Authors**: Wenzhao Zhao, Qiegen Liu, Yisong Lv, Binjie Qin
- **Comment**: None
- **Journal**: None
- **Summary**: Image textures, as a kind of local variations, provide important information for human visual system. Many image textures, especially the small-scale or stochastic textures are rich in high-frequency variations, and are difficult to be preserved. Current state-of-the-art denoising algorithms typically adopt a nonlocal approach consisting of image patch grouping and group-wise denoising filtering. To achieve a better image denoising while preserving the variations in texture, we first adaptively group high correlated image patches with the same kinds of texture elements (texels) via an adaptive clustering method. This adaptive clustering method is applied in an over-clustering-and-iterative-merging approach, where its noise robustness is improved with a custom merging threshold relating to the noise level and cluster size. For texture-preserving denoising of each cluster, considering that the variations in texture are captured and wrapped in not only the between-dimension energy variations but also the within-dimension variations of PCA transform coefficients, we further propose a PCA-transform-domain variation adaptive filtering method to preserve the local variations in textures. Experiments on natural images show the superiority of the proposed transform-domain variation adaptive filtering to traditional PCA-based hard or soft threshold filtering. As a whole, the proposed denoising method achieves a favorable texture preserving performance both quantitatively and visually, especially for stochastic textures, which is further verified in camera raw image denoising.



### Outlier Detection using Generative Models with Theoretical Performance Guarantees
- **Arxiv ID**: http://arxiv.org/abs/1810.11335v1
- **DOI**: None
- **Categories**: **cs.IT**, cs.CV, eess.IV, math.IT, math.OC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.11335v1)
- **Published**: 2018-10-26 14:11:04+00:00
- **Updated**: 2018-10-26 14:11:04+00:00
- **Authors**: Jirong Yi, Anh Duc Le, Tianming Wang, Xiaodong Wu, Weiyu Xu
- **Comment**: 38 Pages, 15 Figures, 10 Lemmas or Theorems with Proofs
- **Journal**: None
- **Summary**: This paper considers the problem of recovering signals from compressed measurements contaminated with sparse outliers, which has arisen in many applications. In this paper, we propose a generative model neural network approach for reconstructing the ground truth signals under sparse outliers. We propose an iterative alternating direction method of multipliers (ADMM) algorithm for solving the outlier detection problem via $\ell_1$ norm minimization, and a gradient descent algorithm for solving the outlier detection problem via squared $\ell_1$ norm minimization. We establish the recovery guarantees for reconstruction of signals using generative models in the presence of outliers, and give an upper bound on the number of outliers allowed for recovery. Our results are applicable to both the linear generator neural network and the nonlinear generator neural network with an arbitrary number of layers. We conduct extensive experiments using variational auto-encoder and deep convolutional generative adversarial networks, and the experimental results show that the signals can be successfully reconstructed under outliers using our approach. Our approach outperforms the traditional Lasso and $\ell_2$ minimization approach.



### Security Event Recognition for Visual Surveillance
- **Arxiv ID**: http://arxiv.org/abs/1810.11348v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.11348v1)
- **Published**: 2018-10-26 14:37:45+00:00
- **Updated**: 2018-10-26 14:37:45+00:00
- **Authors**: Michael Ying Yang, Wentong Liao, Chun Yang, Yanpeng Cao, Bodo Rosenhahn
- **Comment**: None
- **Journal**: None
- **Summary**: With rapidly increasing deployment of surveillance cameras, the reliable methods for automatically analyzing the surveillance video and recognizing special events are demanded by different practical applications. This paper proposes a novel effective framework for security event analysis in surveillance videos. First, convolutional neural network (CNN) framework is used to detect objects of interest in the given videos. Second, the owners of the objects are recognized and monitored in real-time as well. If anyone moves any object, this person will be verified whether he/she is its owner. If not, this event will be further analyzed and distinguished between two different scenes: moving the object away or stealing it. To validate the proposed approach, a new video dataset consisting of various scenarios is constructed for more complex tasks. For comparison purpose, the experiments are also carried out on the benchmark databases related to the task on abandoned luggage detection. The experimental results show that the proposed approach outperforms the state-of-the-art methods and effective in recognizing complex security events.



### Anytime Stereo Image Depth Estimation on Mobile Devices
- **Arxiv ID**: http://arxiv.org/abs/1810.11408v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.11408v2)
- **Published**: 2018-10-26 16:22:27+00:00
- **Updated**: 2019-03-05 14:53:54+00:00
- **Authors**: Yan Wang, Zihang Lai, Gao Huang, Brian H. Wang, Laurens van der Maaten, Mark Campbell, Kilian Q. Weinberger
- **Comment**: Accepted by ICRA2019
- **Journal**: None
- **Summary**: Many applications of stereo depth estimation in robotics require the generation of accurate disparity maps in real time under significant computational constraints. Current state-of-the-art algorithms force a choice between either generating accurate mappings at a slow pace, or quickly generating inaccurate ones, and additionally these methods typically require far too many parameters to be usable on power- or memory-constrained devices. Motivated by these shortcomings, we propose a novel approach for disparity prediction in the anytime setting. In contrast to prior work, our end-to-end learned approach can trade off computation and accuracy at inference time. Depth estimation is performed in stages, during which the model can be queried at any time to output its current best estimate. Our final model can process 1242$ \times $375 resolution images within a range of 10-35 FPS on an NVIDIA Jetson TX2 module with only marginal increases in error -- using two orders of magnitude fewer parameters than the most competitive baseline. The source code is available at https://github.com/mileyan/AnyNet .



### Computational Histological Staining and Destaining of Prostate Core Biopsy RGB Images with Generative Adversarial Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1811.02642v2
- **DOI**: 10.1109/ICMLA.2018.00133
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.02642v2)
- **Published**: 2018-10-26 17:36:00+00:00
- **Updated**: 2019-02-20 16:37:49+00:00
- **Authors**: Aman Rana, Gregory Yauney, Alarice Lowe, Pratik Shah
- **Comment**: Accepted for publication at 2018 IEEE International Conference on
  Machine Learning and Applications (ICMLA)
- **Journal**: None
- **Summary**: Histopathology tissue samples are widely available in two states: paraffin-embedded unstained and non-paraffin-embedded stained whole slide RGB images (WSRI). Hematoxylin and eosin stain (H&E) is one of the principal stains in histology but suffers from several shortcomings related to tissue preparation, staining protocols, slowness and human error. We report two novel approaches for training machine learning models for the computational H&E staining and destaining of prostate core biopsy RGB images. The staining model uses a conditional generative adversarial network that learns hierarchical non-linear mappings between whole slide RGB image (WSRI) pairs of prostate core biopsy before and after H&E staining. The trained staining model can then generate computationally H&E-stained prostate core WSRIs using previously unseen non-stained biopsy images as input. The destaining model, by learning mappings between an H&E stained WSRI and a non-stained WSRI of the same biopsy, can computationally destain previously unseen H&E-stained images. Structural and anatomical details of prostate tissue and colors, shapes, geometries, locations of nuclei, stroma, vessels, glands and other cellular components were generated by both models with structural similarity indices of 0.68 (staining) and 0.84 (destaining). The proposed staining and destaining models can engender computational H&E staining and destaining of WSRI biopsies without additional equipment and devices.



### American Sign Language fingerspelling recognition in the wild
- **Arxiv ID**: http://arxiv.org/abs/1810.11438v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1810.11438v3)
- **Published**: 2018-10-26 17:43:44+00:00
- **Updated**: 2019-02-17 22:45:52+00:00
- **Authors**: Bowen Shi, Aurora Martinez Del Rio, Jonathan Keane, Jonathan Michaux, Diane Brentari, Greg Shakhnarovich, Karen Livescu
- **Comment**: accepted in SLT 2018
- **Journal**: None
- **Summary**: We address the problem of American Sign Language fingerspelling recognition in the wild, using videos collected from websites. We introduce the largest data set available so far for the problem of fingerspelling recognition, and the first using naturally occurring video data. Using this data set, we present the first attempt to recognize fingerspelling sequences in this challenging setting. Unlike prior work, our video data is extremely challenging due to low frame rates and visual variability. To tackle the visual challenges, we train a special-purpose signing hand detector using a small subset of our data. Given the hand detector output, a sequence model decodes the hypothesized fingerspelled letter sequence. For the sequence model, we explore attention-based recurrent encoder-decoders and CTC-based approaches. As the first attempt at fingerspelling recognition in the wild, this work is intended to serve as a baseline for future work on sign language recognition in realistic conditions. We find that, as expected, letter error rates are much higher than in previous work on more controlled data, and we analyze the sources of error and effects of model variants.



### Noise Sensitivity of Local Descriptors vs ConvNets: An application to Facial Recognition
- **Arxiv ID**: http://arxiv.org/abs/1810.11515v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.11515v1)
- **Published**: 2018-10-26 20:01:22+00:00
- **Updated**: 2018-10-26 20:01:22+00:00
- **Authors**: Yasin Musa Ayami, Aboubayda Shabat, Delene Heukelman
- **Comment**: 6 pages, 5 figures, 6 Tables,conference
- **Journal**: None
- **Summary**: The Local Binary Patterns (LBP) is a local descriptor proposed by Ojala et al to discriminate texture due to its discriminative power. However, the LBP is sensitive to noise and illumination changes. Consequently, several extensions to the LBP such as Median Binary Pattern (MBP) and methods such as Local Directional Pattern (LDP) have been proposed to address its drawbacks. Though studies by Zhou et al, suggest that the LDP exhibits poor performance in presence of random noise. Recently, convolution neural networks (ConvNets) were introduced which are increasingly becoming popular for feature extraction due to their discriminative power. This study aimed at evaluating the sensitivity of ResNet50, a ConvNet pre-trained model and local descriptors (LBP and LDP) to noise using the Extended Yale B face dataset with 5 different levels of noise added to the dataset. In our findings, it was observed that despite adding different levels of noise to the dataset, ResNet50 proved to be more robust than the local descriptors (LBP and LDP).



### An Acceleration Scheme to The Local Directional Pattern
- **Arxiv ID**: http://arxiv.org/abs/1810.11518v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1810.11518v1)
- **Published**: 2018-10-26 20:12:20+00:00
- **Updated**: 2018-10-26 20:12:20+00:00
- **Authors**: Yasin Musa Ayami, Aboubayda Shabat
- **Comment**: 5 pages, 3 figures, 3 tables, conference
- **Journal**: None
- **Summary**: This study seeks to improve the running time of the Local Directional Pattern (LDP) during feature extraction using a newly proposed acceleration scheme to LDP. LDP is considered to be computationally expensive. To confirm this, the running time of the LDP to gray level co-occurrence matrix (GLCM) were it was established that the running time for LDP was two orders of magnitude higher than that of the GLCM. In this study, the performance of the newly proposed acceleration scheme was evaluated against LDP and Local Binary patter (LBP) using images from the publicly available extended Cohn-Kanade (CK+) dataset. Based on our findings, the proposed acceleration scheme significantly improves the running time of the LDP by almost 3 times during feature extraction



### Unsupervised Multi-Target Domain Adaptation: An Information Theoretic Approach
- **Arxiv ID**: http://arxiv.org/abs/1810.11547v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.11547v1)
- **Published**: 2018-10-26 22:27:52+00:00
- **Updated**: 2018-10-26 22:27:52+00:00
- **Authors**: Behnam Gholami, Pritish Sahu, Ognjen Rudovic, Konstantinos Bousmalis, Vladimir Pavlovic
- **Comment**: 19 pages, 5 Figures, 5 Tables
- **Journal**: None
- **Summary**: Unsupervised domain adaptation (uDA) models focus on pairwise adaptation settings where there is a single, labeled, source and a single target domain. However, in many real-world settings one seeks to adapt to multiple, but somewhat similar, target domains. Applying pairwise adaptation approaches to this setting may be suboptimal, as they fail to leverage shared information among multiple domains. In this work we propose an information theoretic approach for domain adaptation in the novel context of multiple target domains with unlabeled instances and one source domain with labeled instances. Our model aims to find a shared latent space common to all domains, while simultaneously accounting for the remaining private, domain-specific factors. Disentanglement of shared and private information is accomplished using a unified information-theoretic approach, which also serves to establish a stronger link between the latent representations and the observed data. The resulting model, accompanied by an efficient optimization algorithm, allows simultaneous adaptation from a single source to multiple target domains. We test our approach on three challenging publicly-available datasets, showing that it outperforms several popular domain adaptation methods.



### Deep Convolutional Neural Network Applied to Quality Assessment for Video Tracking
- **Arxiv ID**: http://arxiv.org/abs/1810.11550v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.11550v1)
- **Published**: 2018-10-26 22:45:48+00:00
- **Updated**: 2018-10-26 22:45:48+00:00
- **Authors**: Roger Gomez Nieto, Eugenio Tamura Morimitsu
- **Comment**: 4 pages, 2 figures
- **Journal**: None
- **Summary**: Surveillance videos often suffer from blur and exposure distortions that occur during acquisition and storage, which can adversely influence following automatic image analysis results on video-analytic tasks. The purpose of this paper is to deploy an algorithm that can automatically assess the presence of exposure distortion in videos. In this work we to design and build one architecture for deep learning applied to recognition of distortions in a video. The goal is to know if the video present exposure distortions. Such an algorithm could be used to enhance or restoration image or to create an object tracker distortion-aware.



