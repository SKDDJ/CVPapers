# Arxiv Papers in cs.CV on 2018-10-10
### Least Squares Normalized Cross Correlation
- **Arxiv ID**: http://arxiv.org/abs/1810.04320v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.04320v3)
- **Published**: 2018-10-10 00:55:19+00:00
- **Updated**: 2022-03-30 00:05:19+00:00
- **Authors**: Oliver J. Woodford
- **Comment**: Final version. Rejected from TPAMI twice
- **Journal**: None
- **Summary**: Direct methods are widely used for alignment of models to images, due to their accuracy, since they minimize errors in the domain of measurement noise. They have leveraged least squares minimizations, for simple, efficient, variational optimization, since the seminal 1981 work of Lucas & Kanade, and normalized cross correlation (NCC), for robustness to intensity variations, since at least 1972. Despite the complementary benefits of these two well known methods, they have not been effectively combined to address local variations in intensity. Many ad-hoc NCC frameworks, sub-optimal least squares methods and image transformation approaches have thus been proposed instead, each with their own limitations. This work shows that a least squares optimization of NCC without approximation is not only possible, but straightforward and efficient. A robust, locally normalized formulation is introduced to mitigate local intensity variations and partial occlusions. Finally, sparse features with oriented patches are proposed for further efficiency. The resulting framework is simple to implement, computationally efficient and robust to local intensity variations. It is evaluated on the image alignment problem, showing improvements in both convergence rate and computation time over existing lighting invariant methods.



### Learning Deep Representations for Semantic Image Parsing: a Comprehensive Overview
- **Arxiv ID**: http://arxiv.org/abs/1810.04377v1
- **DOI**: 10.1007/s11704-018-7195-8
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.04377v1)
- **Published**: 2018-10-10 05:29:47+00:00
- **Updated**: 2018-10-10 05:29:47+00:00
- **Authors**: Lili Huang, Jiefeng Peng, Ruimao Zhang, Guanbin Li, Liang Lin
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic image parsing, which refers to the process of decomposing images into semantic regions and constructing the structure representation of the input, has recently aroused widespread interest in the field of computer vision. The recent application of deep representation learning has driven this field into a new stage of development. In this paper, we summarize three aspects of the progress of research on semantic image parsing, i.e., category-level semantic segmentation, instance-level semantic segmentation, and beyond segmentation. Specifically, we first review the general frameworks for each task and introduce the relevant variants. The advantages and limitations of each method are also discussed. Moreover, we present a comprehensive comparison of different benchmark datasets and evaluation metrics. Finally, we explore the future trends and challenges of semantic image parsing.



### Unpaired High-Resolution and Scalable Style Transfer Using Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1810.05724v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.05724v1)
- **Published**: 2018-10-10 07:02:47+00:00
- **Updated**: 2018-10-10 07:02:47+00:00
- **Authors**: Andrej Junginger, Markus Hanselmann, Thilo Strauss, Sebastian Boblest, Jens Buchner, Holger Ulmer
- **Comment**: 10 pages, 8 figures
- **Journal**: None
- **Summary**: Neural networks have proven their capabilities by outperforming many other approaches on regression or classification tasks on various kinds of data. Other astonishing results have been achieved using neural nets as data generators, especially in settings of generative adversarial networks (GANs). One special application is the field of image domain translations. Here, the goal is to take an image with a certain style (e.g. a photography) and transform it into another one (e.g. a painting). If such a task is performed for unpaired training examples, the corresponding GAN setting is complex, the neural networks are large, and this leads to a high peak memory consumption during, both, training and evaluation phase. This sets a limit to the highest processable image size. We address this issue by the idea of not processing the whole image at once, but to train and evaluate the domain translation on the level of overlapping image subsamples. This new approach not only enables us to translate high-resolution images that otherwise cannot be processed by the neural network at once, but also allows us to work with comparably small neural networks and with limited hardware resources. Additionally, the number of images required for the training process is significantly reduced. We present high-quality results on images with a total resolution of up to over 50 megapixels and emonstrate that our method helps to preserve local image details while it also keeps global consistency.



### Prediction of the Influence of Navigation Scan-path on Perceived Quality of Free-Viewpoint Videos
- **Arxiv ID**: http://arxiv.org/abs/1810.04409v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.04409v1)
- **Published**: 2018-10-10 08:12:06+00:00
- **Updated**: 2018-10-10 08:12:06+00:00
- **Authors**: Suiyi Ling, Jesús Gutiérrez, Gu Ke, Patrick Le Callet
- **Comment**: 11 pages, 7 figures
- **Journal**: None
- **Summary**: Free-Viewpoint Video (FVV) systems allow the viewers to freely change the viewpoints of the scene. In such systems, view synthesis and compression are the two main sources of artifacts influencing the perceived quality. To assess this influence, quality evaluation studies are often carried out using conventional displays and generating predefined navigation trajectories mimicking the possible movement of the viewers when exploring the content. Nevertheless, as different trajectories may lead to different conclusions in terms of visual quality when benchmarking the performance of the systems, methods to identify critical trajectories are needed. This paper aims at exploring the impact of exploration trajectories (defined as Hypothetical Rendering Trajectories: HRT) on perceived quality of FVV subjectively and objectively, providing two main contributions. Firstly, a subjective assessment test including different HRTs was carried out and analyzed. The results demonstrate and quantify the influence of HRT in the perceived quality. Secondly, we propose a new objective video quality assessment measure to objectively predict the impact of HRT. This measure, based on Sketch-Token representation, models how the categories of the contours change spatially and temporally from a higher semantic level. Performance in comparison with existing quality metrics for FVV, highlight promising results for automatic detection of most critical HRTs for the benchmark of immersive systems.



### On the Brain Networks of Complex Problem Solving
- **Arxiv ID**: http://arxiv.org/abs/1810.05077v1
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1810.05077v1)
- **Published**: 2018-10-10 09:22:21+00:00
- **Updated**: 2018-10-10 09:22:21+00:00
- **Authors**: Abdullah Alchihabi, Omer Ekmekci, Baran B. Kivilcim, Sharlene D. Newman, Fatos T. Yarman Vural
- **Comment**: None
- **Journal**: None
- **Summary**: Complex problem solving is a high level cognitive process which has been thoroughly studied over the last decade. The Tower of London (TOL) is a task that has been widely used to study problem-solving. In this study, we aim to explore the underlying cognitive network dynamics among anatomical regions of complex problem solving and its sub-phases, namely planning and execution. A new brain network construction model establishing dynamic functional brain networks using fMRI is proposed. The first step of the model is a preprocessing pipeline that manages to decrease the spatial redundancy while increasing the temporal resolution of the fMRI recordings. Then, dynamic brain networks are estimated using artificial neural networks. The network properties of the estimated brain networks are studied in order to identify regions of interest, such as hubs and subgroups of densely connected brain regions. The major similarities and dissimilarities of the network structure of planning and execution phases are highlighted. Our findings show the hubs and clusters of densely interconnected regions during both subtasks. It is observed that there are more hubs during the planning phase compared to the execution phase, and the clusters are more strongly connected during planning compared to execution.



### AI Learns to Recognize Bengali Handwritten Digits: Bengali.AI Computer Vision Challenge 2018
- **Arxiv ID**: http://arxiv.org/abs/1810.04452v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.04452v1)
- **Published**: 2018-10-10 10:59:28+00:00
- **Updated**: 2018-10-10 10:59:28+00:00
- **Authors**: Sharif Amit Kamran, Ahmed Imtiaz Humayun, Samiul Alam, Rashed Mohammad Doha, Manash Kumar Mandal, Tahsin Reasat, Fuad Rahman
- **Comment**: 5 pages, 3 figures
- **Journal**: None
- **Summary**: Solving problems with Artificial intelligence in a competitive manner has long been absent in Bangladesh and Bengali-speaking community. On the other hand, there has not been a well structured database for Bengali Handwritten digits for mass public use. To bring out the best minds working in machine learning and use their expertise to create a model which can easily recognize Bengali Handwritten digits, we organized Bengali.AI Computer Vision Challenge.The challenge saw both local and international teams participating with unprecedented efforts.



### Invariance Analysis of Saliency Models versus Human Gaze During Scene Free Viewing
- **Arxiv ID**: http://arxiv.org/abs/1810.04456v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.04456v1)
- **Published**: 2018-10-10 11:10:28+00:00
- **Updated**: 2018-10-10 11:10:28+00:00
- **Authors**: Zhaohui Che, Ali Borji, Guangtao Zhai, Xiongkuo Min
- **Comment**: None
- **Journal**: None
- **Summary**: Most of current studies on human gaze and saliency modeling have used high-quality stimuli. In real world, however, captured images undergo various types of distortions during the whole acquisition, transmission, and displaying chain. Some distortion types include motion blur, lighting variations and rotation. Despite few efforts, influences of ubiquitous distortions on visual attention and saliency models have not been systematically investigated. In this paper, we first create a large-scale database including eye movements of 10 observers over 1900 images degraded by 19 types of distortions. Second, by analyzing eye movements and saliency models, we find that: a) observers look at different locations over distorted versus original images, and b) performances of saliency models are drastically hindered over distorted images, with the maximum performance drop belonging to Rotation and Shearing distortions. Finally, we investigate the effectiveness of different distortions when serving as data augmentation transformations. Experimental results verify that some useful data augmentation transformations which preserve human gaze of reference images can improve deep saliency models against distortions, while some invalid transformations which severely change human gaze will degrade the performance.



### Let's take a Walk on Superpixels Graphs: Deformable Linear Objects Segmentation and Model Estimation
- **Arxiv ID**: http://arxiv.org/abs/1810.04461v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.04461v1)
- **Published**: 2018-10-10 11:31:14+00:00
- **Updated**: 2018-10-10 11:31:14+00:00
- **Authors**: Daniele De Gregorio, Gianluca Palli, Luigi Di Stefano
- **Comment**: Accepted as Oral to ACCV 2018, Perth
- **Journal**: None
- **Summary**: While robotic manipulation of rigid objects is quite straightforward, coping with deformable objects is an open issue. More specifically, tasks like tying a knot, wiring a connector or even surgical suturing deal with the domain of Deformable Linear Objects (DLOs). In particular the detection of a DLO is a non-trivial problem especially under clutter and occlusions (as well as self-occlusions). The pose estimation of a DLO results into the identification of its parameters related to a designed model, e.g. a basis spline. It follows that the stand-alone segmentation of a DLO might not be sufficient to conduct a full manipulation task. This is why we propose a novel framework able to perform both a semantic segmentation and b-spline modeling of multiple deformable linear objects simultaneously without strict requirements about environment (i.e. the background). The core algorithm is based on biased random walks over the Region Adiacency Graph built on a superpixel oversegmentation of the source image. The algorithm is initialized by a Convolutional Neural Networks that detects the DLO's endcaps. An open source implementation of the proposed approach is also provided to easy the reproduction of the whole detection pipeline along with a novel cables dataset in order to encourage further experiments.



### CRH: A Simple Benchmark Approach to Continuous Hashing
- **Arxiv ID**: http://arxiv.org/abs/1810.05730v1
- **DOI**: 10.1109/GlobalSIP.2015.7418363
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1810.05730v1)
- **Published**: 2018-10-10 14:18:00+00:00
- **Updated**: 2018-10-10 14:18:00+00:00
- **Authors**: Miao Cheng, Ah Chung Tsoi
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: In recent years, the distinctive advancement of handling huge data promotes the evolution of ubiquitous computing and analysis technologies. With the constantly upward system burden and computational complexity, adaptive coding has been a fascinating topic for pattern analysis, with outstanding performance. In this work, a continuous hashing method, termed continuous random hashing (CRH), is proposed to encode sequential data stream, while ignorance of previously hashing knowledge is possible. Instead, a random selection idea is adopted to adaptively approximate the differential encoding patterns of data stream, e.g., streaming media, and iteration is avoided for stepwise learning. Experimental results demonstrate our method is able to provide outstanding performance, as a benchmark approach to continuous hashing.



### A Similarity Measure for Weaving Patterns in Textiles
- **Arxiv ID**: http://arxiv.org/abs/1810.04604v1
- **DOI**: None
- **Categories**: **cs.DB**, cs.CV, cs.DL, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1810.04604v1)
- **Published**: 2018-10-10 15:50:03+00:00
- **Updated**: 2018-10-10 15:50:03+00:00
- **Authors**: Sven Helmer, Vuong M. Ngo
- **Comment**: 10 papes, will be published in SIGIR 2015
- **Journal**: SIGIR 2015
- **Summary**: We propose a novel approach for measuring the similarity between weaving patterns that can provide similarity-based search functionality for textile archives. We represent textile structures using hypergraphs and extract multisets of k-neighborhoods from these graphs. The resulting multisets are then compared using Jaccard coefficients, Hamming distances, and cosine measures. We evaluate the different variants of our similarity measure experimentally, showing that it can be implemented efficiently and illustrating its quality using it to cluster and query a data set containing more than a thousand textile samples.



### A Closer Look at Structured Pruning for Neural Network Compression
- **Arxiv ID**: http://arxiv.org/abs/1810.04622v3
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1810.04622v3)
- **Published**: 2018-10-10 16:30:02+00:00
- **Updated**: 2019-06-07 14:23:14+00:00
- **Authors**: Elliot J. Crowley, Jack Turner, Amos Storkey, Michael O'Boyle
- **Comment**: Preprint. First two authors contributed equally. Paper title has
  changed
- **Journal**: None
- **Summary**: Structured pruning is a popular method for compressing a neural network: given a large trained network, one alternates between removing channel connections and fine-tuning; reducing the overall width of the network. However, the efficacy of structured pruning has largely evaded scrutiny. In this paper, we examine ResNets and DenseNets obtained through structured pruning-and-tuning and make two interesting observations: (i) reduced networks---smaller versions of the original network trained from scratch---consistently outperform pruned networks; (ii) if one takes the architecture of a pruned network and then trains it from scratch it is significantly more competitive. Furthermore, these architectures are easy to approximate: we can prune once and obtain a family of new, scalable network architectures that can simply be trained from scratch. Finally, we compare the inference speed of reduced and pruned networks on hardware, and show that reduced networks are significantly faster. Code is available at https://github.com/BayesWatch/pytorch-prunes.



### Learning Embeddings for Product Visual Search with Triplet Loss and Online Sampling
- **Arxiv ID**: http://arxiv.org/abs/1810.04652v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1810.04652v1)
- **Published**: 2018-10-10 17:19:08+00:00
- **Updated**: 2018-10-10 17:19:08+00:00
- **Authors**: Eric Dodds, Huy Nguyen, Simao Herdade, Jack Culpepper, Andrew Kae, Pierre Garrigues
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose learning an embedding function for content-based image retrieval within the e-commerce domain using the triplet loss and an online sampling method that constructs triplets from within a minibatch. We compare our method to several strong baselines as well as recent works on the DeepFashion and Stanford Online Product datasets. Our approach significantly outperforms the state-of-the-art on the DeepFashion dataset. With a modification to favor sampling minibatches from a single product category, the same approach demonstrates competitive results when compared to the state-of-the-art for the Stanford Online Products dataset.



### Deep Inertial Poser: Learning to Reconstruct Human Pose from Sparse Inertial Measurements in Real Time
- **Arxiv ID**: http://arxiv.org/abs/1810.04703v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1810.04703v1)
- **Published**: 2018-10-10 18:45:55+00:00
- **Updated**: 2018-10-10 18:45:55+00:00
- **Authors**: Yinghao Huang, Manuel Kaufmann, Emre Aksan, Michael J. Black, Otmar Hilliges, Gerard Pons-Moll
- **Comment**: SIGGRAPH Asia 2018. First two authors contributed equally to this
  work. Project page: http://dip.is.tue.mpg.de/
- **Journal**: None
- **Summary**: We demonstrate a novel deep neural network capable of reconstructing human full body pose in real-time from 6 Inertial Measurement Units (IMUs) worn on the user's body. In doing so, we address several difficult challenges. First, the problem is severely under-constrained as multiple pose parameters produce the same IMU orientations. Second, capturing IMU data in conjunction with ground-truth poses is expensive and difficult to do in many target application scenarios (e.g., outdoors). Third, modeling temporal dependencies through non-linear optimization has proven effective in prior work but makes real-time prediction infeasible. To address this important limitation, we learn the temporal pose priors using deep learning. To learn from sufficient data, we synthesize IMU data from motion capture datasets. A bi-directional RNN architecture leverages past and future information that is available at training time. At test time, we deploy the network in a sliding window fashion, retaining real time capabilities. To evaluate our method, we recorded DIP-IMU, a dataset consisting of $10$ subjects wearing 17 IMUs for validation in $64$ sequences with $330\,000$ time instants; this constitutes the largest IMU dataset publicly available. We quantitatively evaluate our approach on multiple datasets and show results from a real-time implementation. DIP-IMU and the code are available for research purposes.



### Image Super-Resolution Using VDSR-ResNeXt and SRCGAN
- **Arxiv ID**: http://arxiv.org/abs/1810.05731v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.05731v1)
- **Published**: 2018-10-10 19:20:15+00:00
- **Updated**: 2018-10-10 19:20:15+00:00
- **Authors**: Saifuddin Hitawala, Yao Li, Xian Wang, Dongyang Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Over the past decade, many Super Resolution techniques have been developed using deep learning. Among those, generative adversarial networks (GAN) and very deep convolutional networks (VDSR) have shown promising results in terms of HR image quality and computational speed. In this paper, we propose two approaches based on these two algorithms: VDSR-ResNeXt, which is a deep multi-branch convolutional network inspired by VDSR and ResNeXt; and SRCGAN, which is a conditional GAN that explicitly passes class labels as input to the GAN. The two methods were implemented on common SR benchmark datasets for both quantitative and qualitative assessment.



### A Multimodal Approach towards Emotion Recognition of Music using Audio and Lyrical Content
- **Arxiv ID**: http://arxiv.org/abs/1811.05760v1
- **DOI**: None
- **Categories**: **eess.AS**, cs.CL, cs.CV, cs.MM, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/1811.05760v1)
- **Published**: 2018-10-10 20:51:03+00:00
- **Updated**: 2018-10-10 20:51:03+00:00
- **Authors**: Aniruddha Bhattacharya, K. V. Kadambari
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: We propose MoodNet - A Deep Convolutional Neural Network based architecture to effectively predict the emotion associated with a piece of music given its audio and lyrical content.We evaluate different architectures consisting of varying number of two-dimensional convolutional and subsampling layers,followed by dense layers.We use Mel-Spectrograms to represent the audio content and word embeddings-specifically 100 dimensional word vectors, to represent the textual content represented by the lyrics.We feed input data from both modalities to our MoodNet architecture.The output from both the modalities are then fused as a fully connected layer and softmax classfier is used to predict the category of emotion.Using F1-score as our metric,our results show excellent performance of MoodNet over the two datasets we experimented on-The MIREX Multimodal dataset and the Million Song Dataset.Our experiments reflect the hypothesis that more complex models perform better with more training data.We also observe that lyrics outperform audio as a better expressed modality and conclude that combining and using features from multiple modalities for prediction tasks result in superior performance in comparison to using a single modality as input.



### Computational ghost imaging using a field-programmable gate array
- **Arxiv ID**: http://arxiv.org/abs/1810.05670v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AR, cs.CV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/1810.05670v1)
- **Published**: 2018-10-10 21:32:10+00:00
- **Updated**: 2018-10-10 21:32:10+00:00
- **Authors**: Ikuo Hoshi, Tomoyoshi Shimobaba, Takashi Kakue, Tomoyoshi Ito
- **Comment**: None
- **Journal**: None
- **Summary**: Computational ghost imaging is a promising technique for single-pixel imaging because it is robust to disturbance and can be operated over broad wavelength bands, unlike common cameras. However, one disadvantage of this method is that it has a long calculation time for image reconstruction. In this paper, we have designed a dedicated calculation circuit that accelerated the process of computational ghost imaging. We implemented this circuit by using a field-programmable gate array, which reduced the calculation time for the circuit compared to a CPU. The dedicated circuit reconstructs images at a frame rate of 300 Hz.



### Deep Recurrent Level Set for Segmenting Brain Tumors
- **Arxiv ID**: http://arxiv.org/abs/1810.04752v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.04752v1)
- **Published**: 2018-10-10 21:33:13+00:00
- **Updated**: 2018-10-10 21:33:13+00:00
- **Authors**: T. Hoang Ngan Le, Raajitha Gummadi, Marios Savvides
- **Comment**: None
- **Journal**: booktitle="Medical Image Computing and Computer Assisted
  Intervention -- MICCAI 2018", year="2018", publisher="Springer International
  Publishing",
- **Summary**: Variational Level Set (VLS) has been a widely used method in medical segmentation. However, segmentation accuracy in the VLS method dramatically decreases when dealing with intervening factors such as lighting, shadows, colors, etc. Additionally, results are quite sensitive to initial settings and are highly dependent on the number of iterations. In order to address these limitations, the proposed method incorporates VLS into deep learning by defining a novel end-to-end trainable model called as Deep Recurrent Level Set (DRLS). The proposed DRLS consists of three layers, i.e, Convolutional layers, Deconvolutional layers with skip connections and LevelSet layers. Brain tumor segmentation is taken as an instant to illustrate the performance of the proposed DRLS. Convolutional layer learns visual representation of brain tumor at different scales. Since brain tumors occupy a small portion of the image, deconvolutional layers are designed with skip connections to obtain a high quality feature map. Level-Set Layer drives the contour towards the brain tumor. In each step, the Convolutional Layer is fed with the LevelSet map to obtain a brain tumor feature map. This in turn serves as input for the LevelSet layer in the next step. The experimental results have been obtained on BRATS2013, BRATS2015 and BRATS2017 datasets. The proposed DRLS model improves both computational time and segmentation accuracy when compared to the the classic VLS-based method. Additionally, a fully end-to-end system DRLS achieves state-of-the-art segmentation on brain tumors.



