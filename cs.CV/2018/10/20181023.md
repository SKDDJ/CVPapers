# Arxiv Papers in cs.CV on 2018-10-23
### How to Read Paintings: Semantic Art Understanding with Multi-Modal Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1810.09617v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.09617v1)
- **Published**: 2018-10-23 00:54:42+00:00
- **Updated**: 2018-10-23 00:54:42+00:00
- **Authors**: Noa Garcia, George Vogiatzis
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic art analysis has been mostly focused on classifying artworks into different artistic styles. However, understanding an artistic representation involves more complex processes, such as identifying the elements in the scene or recognizing author influences. We present SemArt, a multi-modal dataset for semantic art understanding. SemArt is a collection of fine-art painting images in which each image is associated to a number of attributes and a textual artistic comment, such as those that appear in art catalogues or museum collections. To evaluate semantic art understanding, we envisage the Text2Art challenge, a multi-modal retrieval task where relevant paintings are retrieved according to an artistic text, and vice versa. We also propose several models for encoding visual and textual artistic representations into a common semantic space. Our best approach is able to find the correct image within the top 10 ranked images in the 45.5% of the test samples. Moreover, our models show remarkable levels of art understanding when compared against human evaluation.



### Sparse DNNs with Improved Adversarial Robustness
- **Arxiv ID**: http://arxiv.org/abs/1810.09619v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.09619v2)
- **Published**: 2018-10-23 01:05:41+00:00
- **Updated**: 2019-11-06 01:32:50+00:00
- **Authors**: Yiwen Guo, Chao Zhang, Changshui Zhang, Yurong Chen
- **Comment**: l1 regularization on weights --> l1 regularization on activations
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) are computationally/memory-intensive and vulnerable to adversarial attacks, making them prohibitive in some real-world applications. By converting dense models into sparse ones, pruning appears to be a promising solution to reducing the computation/memory cost. This paper studies classification models, especially DNN-based ones, to demonstrate that there exists intrinsic relationships between their sparsity and adversarial robustness. Our analyses reveal, both theoretically and empirically, that nonlinear DNN-based classifiers behave differently under $l_2$ attacks from some linear ones. We further demonstrate that an appropriately higher model sparsity implies better robustness of nonlinear DNNs, whereas over-sparsified models can be more difficult to resist adversarial examples.



### A Neural Compositional Paradigm for Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/1810.09630v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1810.09630v1)
- **Published**: 2018-10-23 02:16:12+00:00
- **Updated**: 2018-10-23 02:16:12+00:00
- **Authors**: Bo Dai, Sanja Fidler, Dahua Lin
- **Comment**: 32nd Conference on Neural Information Processing Systems (NIPS 2018),
  Montr\'eal, Canada
- **Journal**: None
- **Summary**: Mainstream captioning models often follow a sequential structure to generate captions, leading to issues such as introduction of irrelevant semantics, lack of diversity in the generated captions, and inadequate generalization performance. In this paper, we present an alternative paradigm for image captioning, which factorizes the captioning procedure into two stages: (1) extracting an explicit semantic representation from the given image; and (2) constructing the caption based on a recursive compositional procedure in a bottom-up manner. Compared to conventional ones, our paradigm better preserves the semantic content through an explicit factorization of semantics and syntax. By using the compositional generation procedure, caption construction follows a recursive structure, which naturally fits the properties of human language. Moreover, the proposed compositional procedure requires less data to train, generalizes better, and yields more diverse captions.



### Point-cloud-based place recognition using CNN feature extraction
- **Arxiv ID**: http://arxiv.org/abs/1810.09631v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1810.09631v1)
- **Published**: 2018-10-23 02:23:48+00:00
- **Updated**: 2018-10-23 02:23:48+00:00
- **Authors**: Ting Sun, Ming Liu, Haoyang Ye, Dit-Yan Yeung
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a novel point-cloud-based place recognition system that adopts a deep learning approach for feature extraction. By using a convolutional neural network pre-trained on color images to extract features from a range image without fine-tuning on extra range images, significant improvement has been observed when compared to using hand-crafted features. The resulting system is illumination invariant, rotation invariant and robust against moving objects that are unrelated to the place identity. Apart from the system itself, we also bring to the community a new place recognition dataset containing both point cloud and grayscale images covering a full $360^\circ$ environmental view. In addition, the dataset is organized in such a way that it facilitates experimental validation with respect to rotation invariance or robustness against unrelated moving objects separately.



### Visual Attention is Beyond One Single Saliency Map
- **Arxiv ID**: http://arxiv.org/abs/1811.02650v1
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1811.02650v1)
- **Published**: 2018-10-23 04:10:50+00:00
- **Updated**: 2018-10-23 04:10:50+00:00
- **Authors**: Jian Li
- **Comment**: arXiv admin note: text overlap with arXiv:1605.01999
- **Journal**: None
- **Summary**: Of later years, numerous bottom-up attention models have been proposed on different assumptions. However, the produced saliency maps may be different from each other even from the same input image. We also observe that human fixation map varies across time greatly. When people freely view an image, they tend to allocate attention at salient regions of large scale at first, and then search more and more detailed regions. In this paper, we argue that, for one input image visual attention cannot be described by only one single saliency map, and this mechanism should be modeled as a dynamic process. Under the frequency domain paradigm, we proposed a global inhibition model to mimic this process by suppressing the {\it non-saliency} in the input image; we also show that the dynamic process is influenced by one parameter in the frequency domain. Experiments illustrate that the proposed model is capable of predicting human dynamic fixation distribution.



### One Bit Matters: Understanding Adversarial Examples as the Abuse of Redundancy
- **Arxiv ID**: http://arxiv.org/abs/1810.09650v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, cs.IT, math.IT, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.09650v1)
- **Published**: 2018-10-23 04:23:25+00:00
- **Updated**: 2018-10-23 04:23:25+00:00
- **Authors**: Jingkang Wang, Ruoxi Jia, Gerald Friedland, Bo Li, Costas Spanos
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the great success achieved in machine learning (ML), adversarial examples have caused concerns with regards to its trustworthiness: A small perturbation of an input results in an arbitrary failure of an otherwise seemingly well-trained ML model. While studies are being conducted to discover the intrinsic properties of adversarial examples, such as their transferability and universality, there is insufficient theoretic analysis to help understand the phenomenon in a way that can influence the design process of ML experiments. In this paper, we deduce an information-theoretic model which explains adversarial attacks as the abuse of feature redundancies in ML algorithms. We prove that feature redundancy is a necessary condition for the existence of adversarial examples. Our model helps to explain some major questions raised in many anecdotal studies on adversarial examples. Our theory is backed up by empirical measurements of the information content of benign and adversarial examples on both image and text datasets. Our measurements show that typical adversarial examples introduce just enough redundancy to overflow the decision making of an ML model trained on corresponding benign examples. We conclude with actionable recommendations to improve the robustness of machine learners against adversarial examples.



### Face Recognition from Sequential Sparse 3D Data via Deep Registration
- **Arxiv ID**: http://arxiv.org/abs/1810.09658v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.09658v3)
- **Published**: 2018-10-23 04:58:48+00:00
- **Updated**: 2019-04-04 10:56:37+00:00
- **Authors**: Yang Tan, Hongxin Lin, Zelin Xiao, Shengyong Ding, Hongyang Chao
- **Comment**: To be appeared in ICB2019
- **Journal**: None
- **Summary**: Previous works have shown that face recognition with high accurate 3D data is more reliable and insensitive to pose and illumination variations. Recently, low-cost and portable 3D acquisition techniques like ToF(Time of Flight) and DoE based structured light systems enable us to access 3D data easily, e.g., via a mobile phone. However, such devices only provide sparse(limited speckles in structured light system) and noisy 3D data which can not support face recognition directly. In this paper, we aim at achieving high-performance face recognition for devices equipped with such modules which is very meaningful in practice as such devices will be very popular. We propose a framework to perform face recognition by fusing a sequence of low-quality 3D data. As 3D data are sparse and noisy which can not be well handled by conventional methods like the ICP algorithm, we design a PointNet-like Deep Registration Network(DRNet) which works with ordered 3D point coordinates while preserving the ability of mining local structures via convolution. Meanwhile we develop a novel loss function to optimize our DRNet based on the quaternion expression which obviously outperforms other widely used functions. For face recognition, we design a deep convolutional network which takes the fused 3D depth-map as input based on AMSoftmax model. Experiments show that our DRNet can achieve rotation error 0.95{\deg} and translation error 0.28mm for registration. The face recognition on fused data also achieves rank-1 accuracy 99.2% , FAR-0.001 97.5% on Bosphorus dataset which is comparable with state-of-the-art high-quality data based recognition performance.



### Large scale visual place recognition with sub-linear storage growth
- **Arxiv ID**: http://arxiv.org/abs/1810.09660v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.09660v1)
- **Published**: 2018-10-23 05:04:36+00:00
- **Updated**: 2018-10-23 05:04:36+00:00
- **Authors**: Huu Le, Michael Milford
- **Comment**: None
- **Journal**: None
- **Summary**: Robotic and animal mapping systems share many of the same objectives and challenges, but differ in one key aspect: where much of the research in robotic mapping has focused on solving the data association problem, the grid cell neurons underlying maps in the mammalian brain appear to intentionally break data association by encoding many locations with a single grid cell neuron. One potential benefit of this intentional aliasing is both sub-linear map storage and computational requirements growth with environment size, which we demonstrated in a previous proof-of-concept study that detected and encoded mutually complementary co-prime pattern frequencies in the visual map data. In this research, we solve several of the key theoretical and practical limitations of that prototype model and achieve significantly better sub-linear storage growth, a factor reduction in storage requirements per map location, scalability to large datasets on standard compute equipment and improved robustness to environments with visually challenging appearance change. These improvements are achieved through several innovations including a flexible user-driven choice mechanism for the periodic patterns underlying the new encoding method, a parallelized chunking technique that splits the map into sub-sections processed in parallel and a novel feature selection approach that selects only the image information most relevant to the encoded temporal patterns. We evaluate our techniques on two large benchmark datasets with the comparison to the previous state-of-the-art system, as well as providing a detailed analysis of system performance with respect to parameters such as required precision performance and the number of cyclic patterns encoded.



### Action-Agnostic Human Pose Forecasting
- **Arxiv ID**: http://arxiv.org/abs/1810.09676v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.09676v1)
- **Published**: 2018-10-23 06:17:53+00:00
- **Updated**: 2018-10-23 06:17:53+00:00
- **Authors**: Hsu-kuang Chiu, Ehsan Adeli, Borui Wang, De-An Huang, Juan Carlos Niebles
- **Comment**: Accepted for publication in WACV 2019
- **Journal**: None
- **Summary**: Predicting and forecasting human dynamics is a very interesting but challenging task with several prospective applications in robotics, health-care, etc. Recently, several methods have been developed for human pose forecasting; however, they often introduce a number of limitations in their settings. For instance, previous work either focused only on short-term or long-term predictions, while sacrificing one or the other. Furthermore, they included the activity labels as part of the training process, and require them at testing time. These limitations confine the usage of pose forecasting models for real-world applications, as often there are no activity-related annotations for testing scenarios. In this paper, we propose a new action-agnostic method for short- and long-term human pose forecasting. To this end, we propose a new recurrent neural network for modeling the hierarchical and multi-scale characteristics of the human dynamics, denoted by triangular-prism RNN (TP-RNN). Our model captures the latent hierarchical structure embedded in temporal human pose sequences by encoding the temporal dependencies with different time-scales. For evaluation, we run an extensive set of experiments on Human 3.6M and Penn Action datasets and show that our method outperforms baseline and state-of-the-art methods quantitatively and qualitatively. Codes are available at https://github.com/eddyhkchiu/pose_forecast_wacv/



### Consistency-aware Shading Orders Selective Fusion for Intrinsic Image Decomposition
- **Arxiv ID**: http://arxiv.org/abs/1810.09706v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.09706v1)
- **Published**: 2018-10-23 07:53:30+00:00
- **Updated**: 2018-10-23 07:53:30+00:00
- **Authors**: Yuanliu Liu, Ang Li, Zejian Yuan, Badong Chen, Nanning Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: We address the problem of decomposing a single image into reflectance and shading. The difficulty comes from the fact that the components of image---the surface albedo, the direct illumination, and the ambient illumination---are coupled heavily in observed image. We propose to infer the shading by ordering pixels by their relative brightness, without knowing the absolute values of the image components beforehand. The pairwise shading orders are estimated in two ways: brightness order and low-order fittings of local shading field. The brightness order is a non-local measure, which can be applied to any pair of pixels including those whose reflectance and shading are both different. The low-order fittings are used for pixel pairs within local regions of smooth shading. Together, they can capture both global order structure and local variations of the shading. We propose a Consistency-aware Selective Fusion (CSF) to integrate the pairwise orders into a globally consistent order. The iterative selection process solves the conflicts between the pairwise orders obtained by different estimation methods. Inconsistent or unreliable pairwise orders will be automatically excluded from the fusion to avoid polluting the global order. Experiments on the MIT Intrinsic Image dataset show that the proposed model is effective at recovering the shading including deep shadows. Our model also works well on natural images from the IIW dataset, the UIUC Shadow dataset and the NYU-Depth dataset, where the colors of direct lights and ambient lights are quite different.



### Color naming guided intrinsic image decomposition
- **Arxiv ID**: http://arxiv.org/abs/1810.09720v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.09720v1)
- **Published**: 2018-10-23 08:38:39+00:00
- **Updated**: 2018-10-23 08:38:39+00:00
- **Authors**: Yuanliu Liu, Zejian Yuan
- **Comment**: None
- **Journal**: None
- **Summary**: Intrinsic image decomposition is a severely under-constrained problem. User interactions can help to reduce the ambiguity of the decomposition considerably. The traditional way of user interaction is to draw scribbles that indicate regions with constant reflectance or shading. However the effect scopes of the scribbles are quite limited, so dozens of scribbles are often needed to rectify the whole decomposition, which is time consuming. In this paper we propose an efficient way of user interaction that users need only to annotate the color composition of the image. Color composition reveals the global distribution of reflectance, so it can help to adapt the whole decomposition directly. We build a generative model of the process that the albedo of the material produces both the reflectance through imaging and the color labels by color naming. Our model fuses effectively the physical properties of image formation and the top-down information from human color perception. Experimental results show that color naming can improve the performance of intrinsic image decomposition, especially in cleaning the shadows left in reflectance and solving the color constancy problem.



### CEREALS - Cost-Effective REgion-based Active Learning for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1810.09726v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.09726v1)
- **Published**: 2018-10-23 08:44:49+00:00
- **Updated**: 2018-10-23 08:44:49+00:00
- **Authors**: Radek Mackowiak, Philip Lenz, Omair Ghori, Ferran Diego, Oliver Lange, Carsten Rother
- **Comment**: Published at British Machine Vision Conference 2018 (BMVC)
- **Journal**: None
- **Summary**: State of the art methods for semantic image segmentation are trained in a supervised fashion using a large corpus of fully labeled training images. However, gathering such a corpus is expensive, due to human annotation effort, in contrast to gathering unlabeled data. We propose an active learning-based strategy, called CEREALS, in which a human only has to hand-label a few, automatically selected, regions within an unlabeled image corpus. This minimizes human annotation effort while maximizing the performance of a semantic image segmentation method. The automatic selection procedure is achieved by: a) using a suitable information measure combined with an estimate about human annotation effort, which is inferred from a learned cost model, and b) exploiting the spatial coherency of an image. The performance of CEREALS is demonstrated on Cityscapes, where we are able to reduce the annotation effort to 17%, while keeping 95% of the mean Intersection over Union (mIoU) of a model that was trained with the fully annotated training set of Cityscapes.



### Domain Adaptive Segmentation in Volume Electron Microscopy Imaging
- **Arxiv ID**: http://arxiv.org/abs/1810.09734v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.09734v2)
- **Published**: 2018-10-23 09:12:41+00:00
- **Updated**: 2018-12-18 17:51:31+00:00
- **Authors**: Joris Roels, Julian Hennies, Yvan Saeys, Wilfried Philips, Anna Kreshuk
- **Comment**: ISBI 2019 (accepted)
- **Journal**: None
- **Summary**: In the last years, automated segmentation has become a necessary tool for volume electron microscopy (EM) imaging. So far, the best performing techniques have been largely based on fully supervised encoder-decoder CNNs, requiring a substantial amount of annotated images. Domain Adaptation (DA) aims to alleviate the annotation burden by 'adapting' the networks trained on existing groundtruth data (source domain) to work on a different (target) domain with as little additional annotation as possible. Most DA research is focused on the classification task, whereas volume EM segmentation remains rather unexplored. In this work, we extend recently proposed classification DA techniques to an encoder-decoder layout and propose a novel method that adds a reconstruction decoder to the classical encoder-decoder segmentation in order to align source and target encoder features. The method has been validated on the task of segmenting mitochondria in EM volumes. We have performed DA from brain EM images to HeLa cells and from isotropic FIB/SEM volumes to anisotropic TEM volumes. In all cases, the proposed method has outperformed the extended classification DA techniques and the finetuning baseline. An implementation of our work can be found on https://github.com/JorisRoels/domain-adaptive-segmentation.



### Convolutional Neural Network Pruning to Accelerate Membrane Segmentation in Electron Microscopy
- **Arxiv ID**: http://arxiv.org/abs/1810.09735v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.09735v1)
- **Published**: 2018-10-23 09:18:16+00:00
- **Updated**: 2018-10-23 09:18:16+00:00
- **Authors**: Joris Roels, Jonas De Vylder, Jan Aelterman, Yvan Saeys, Wilfried Philips
- **Comment**: 5 pages, 4 figures, ISBI 2017
- **Journal**: None
- **Summary**: Biological membranes are one of the most basic structures and regions of interest in cell biology. In the study of membranes, segment extraction is a well-known and difficult problem because of impeding noise, directional and thickness variability, etc. Recent advances in electron microscopy membrane segmentation are able to cope with such difficulties by training convolutional neural networks. However, because of the massive amount of features that have to be extracted while propagating forward, the practical usability diminishes, even with state-of-the-art GPU's. A significant part of these network features typically contains redundancy through correlation and sparsity. In this work, we propose a pruning method for convolutional neural networks that ensures the training loss increase is minimized. We show that the pruned networks, after retraining, are more efficient in terms of time and memory, without significantly affecting the network accuracy. This way, we manage to obtain real-time membrane segmentation performance, for our specific electron microscopy setup.



### Bayesian Deconvolution of Scanning Electron Microscopy Images Using Point-spread Function Estimation and Non-local Regularization
- **Arxiv ID**: http://arxiv.org/abs/1810.09739v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.09739v1)
- **Published**: 2018-10-23 09:25:55+00:00
- **Updated**: 2018-10-23 09:25:55+00:00
- **Authors**: Joris Roels, Jan Aelterman, Jonas De Vylder, Hiep Luong, Yvan Saeys, Wilfried Philips
- **Comment**: 5 pages, 4 figures, EMBC 2016
- **Journal**: None
- **Summary**: Microscopy is one of the most essential imaging techniques in life sciences. High-quality images are required in order to solve (potentially life-saving) biomedical research problems. Many microscopy techniques do not achieve sufficient resolution for these purposes, being limited by physical diffraction and hardware deficiencies. Electron microscopy addresses optical diffraction by measuring emitted or transmitted electrons instead of photons, yielding nanometer resolution. Despite pushing back the diffraction limit, blur should still be taken into account because of practical hardware imperfections and remaining electron diffraction. Deconvolution algorithms can remove some of the blur in post-processing but they depend on knowledge of the point-spread function (PSF) and should accurately regularize noise. Any errors in the estimated PSF or noise model will reduce their effectiveness. This paper proposes a new procedure to estimate the lateral component of the point spread function of a 3D scanning electron microscope more accurately. We also propose a Bayesian maximum a posteriori deconvolution algorithm with a non-local image prior which employs this PSF estimate and previously developed noise statistics. We demonstrate visual quality improvements and show that applying our method improves the quality of subsequent segmentation steps.



### LoGAN: Generating Logos with a Generative Adversarial Neural Network Conditioned on color
- **Arxiv ID**: http://arxiv.org/abs/1810.10395v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1810.10395v1)
- **Published**: 2018-10-23 10:22:17+00:00
- **Updated**: 2018-10-23 10:22:17+00:00
- **Authors**: Ajkel Mino, Gerasimos Spanakis
- **Comment**: 6 page, ICMLA18
- **Journal**: None
- **Summary**: Designing a logo is a long, complicated, and expensive process for any designer. However, recent advancements in generative algorithms provide models that could offer a possible solution. Logos are multi-modal, have very few categorical properties, and do not have a continuous latent space. Yet, conditional generative adversarial networks can be used to generate logos that could help designers in their creative process. We propose LoGAN: an improved auxiliary classifier Wasserstein generative adversarial neural network (with gradient penalty) that is able to generate logos conditioned on twelve different colors. In 768 generated instances (12 classes and 64 logos per class), when looking at the most prominent color, the conditional generation part of the model has an overall precision and recall of 0.8 and 0.7 respectively. LoGAN's results offer a first glance at how artificial intelligence can be used to assist designers in their creative process and open promising future directions, such as including more descriptive labels which will provide a more exhaustive and easy-to-use system.



### Visual Semantic Re-ranker for Text Spotting
- **Arxiv ID**: http://arxiv.org/abs/1810.09776v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.09776v2)
- **Published**: 2018-10-23 11:12:18+00:00
- **Updated**: 2018-10-27 11:31:43+00:00
- **Authors**: Ahmed Sabir, Francesc Moreno-Noguer, Lluís Padró
- **Comment**: None
- **Journal**: None
- **Summary**: Many current state-of-the-art methods for text recognition are based on purely local information and ignore the semantic correlation between text and its surrounding visual context. In this paper, we propose a post-processing approach to improve the accuracy of text spotting by using the semantic relation between the text and the scene. We initially rely on an off-the-shelf deep neural network that provides a series of text hypotheses for each input image. These text hypotheses are then re-ranked using the semantic relatedness with the object in the image. As a result of this combination, the performance of the original network is boosted with a very low computational cost. The proposed framework can be used as a drop-in complement for any text-spotting algorithm that outputs a ranking of word hypotheses. We validate our approach on ICDAR'17 shared task dataset.



### Expression Recognition Using the Periocular Region: A Feasibility Study
- **Arxiv ID**: http://arxiv.org/abs/1810.09798v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.09798v1)
- **Published**: 2018-10-23 11:56:20+00:00
- **Updated**: 2018-10-23 11:56:20+00:00
- **Authors**: Fernando Alonso-Fernandez, Josef Bigun, Cristofer Englund
- **Comment**: Accepted for publication at Intl Conf on Signal Image Technology &
  Internet Based Systems, SITIS 2018
- **Journal**: Proc. Intl Conf on Signal Image Technology & Internet Based
  Systems, SITIS, Gran Canaria, Spain, 26-29 Nov 2018
- **Summary**: This paper investigates the feasibility of using the periocular region for expression recognition. Most works have tried to solve this by analyzing the whole face. Periocular is the facial region in the immediate vicinity of the eye. It has the advantage of being available over a wide range of distances and under partial face occlusion, thus making it suitable for unconstrained or uncooperative scenarios. We evaluate five different image descriptors on a dataset of 1,574 images from 118 subjects. The experimental results show an average/overall accuracy of 67.0%/78.0% by fusion of several descriptors. While this accuracy is still behind that attained with full-face methods, it is noteworthy to mention that our initial approach employs only one frame to predict the expression, in contraposition to state of the art, exploiting several order more data comprising spatial-temporal data which is often not available.



### Improving Automated Latent Fingerprint Identification using Extended Minutia Types
- **Arxiv ID**: http://arxiv.org/abs/1810.09801v1
- **DOI**: 10.1016/j.inffus.2018.10.001
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.09801v1)
- **Published**: 2018-10-23 12:02:22+00:00
- **Updated**: 2018-10-23 12:02:22+00:00
- **Authors**: Ram P. Krish, Julian Fierrez, Daniel Ramos, Fernando Alonso-Fernandez, Josef Bigun
- **Comment**: To appear in Information Fusion journal (Elsevier)
- **Journal**: Information Fusion, Volume 50, p. 9-19, 2019. ISSN 1566-2535
- **Summary**: Latent fingerprints are usually processed with Automated Fingerprint Identification Systems (AFIS) by law enforcement agencies to narrow down possible suspects from a criminal database. AFIS do not commonly use all discriminatory features available in fingerprints but typically use only some types of features automatically extracted by a feature extraction algorithm. In this work, we explore ways to improve rank identification accuracies of AFIS when only a partial latent fingerprint is available. Towards solving this challenge, we propose a method that exploits extended fingerprint features (unusual/rare minutiae) not commonly considered in AFIS. This new method can be combined with any existing minutiae-based matcher. We first compute a similarity score based on least squares between latent and tenprint minutiae points, with rare minutiae features as reference points. Then the similarity score of the reference minutiae-based matcher at hand is modified based on a fitting error from the least square similarity stage. We use a realistic forensic fingerprint casework database in our experiments which contains rare minutiae features obtained from Guardia Civil, the Spanish law enforcement agency. Experiments are conducted using three minutiae-based matchers as a reference, namely: NIST-Bozorth3, VeriFinger-SDK and MCC-SDK. We report significant improvements in the rank identification accuracies when these minutiae matchers are augmented with our proposed algorithm based on rare minutiae features.



### Action and intention recognition of pedestrians in urban traffic
- **Arxiv ID**: http://arxiv.org/abs/1810.09805v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.09805v1)
- **Published**: 2018-10-23 12:07:32+00:00
- **Updated**: 2018-10-23 12:07:32+00:00
- **Authors**: Dimitrios Varytimidis, Fernando Alonso-Fernandez, Boris Duran, Cristofer Englund
- **Comment**: Accepted for publication at Intl Conf on Signal Image Technology &
  Internet Based Systems, SITIS 2018
- **Journal**: Proc. Intl Conf on Signal Image Technology & Internet Based
  Systems, SITIS, Gran Canaria, Spain, 26-29 Nov 2018
- **Summary**: Action and intention recognition of pedestrians in urban settings are challenging problems for Advanced Driver Assistance Systems as well as future autonomous vehicles to maintain smooth and safe traffic. This work investigates a number of feature extraction methods in combination with several machine learning algorithms to build knowledge on how to automatically detect the action and intention of pedestrians in urban traffic. We focus on the motion and head orientation to predict whether the pedestrian is about to cross the street or not. The work is based on the Joint Attention for Autonomous Driving (JAAD) dataset, which contains 346 videoclips of various traffic scenarios captured with cameras mounted in the windshield of a car. An accuracy of 72% for head orientation estimation and 85% for motion detection is obtained in our experiments.



### Fruit and Vegetable Identification Using Machine Learning for Retail Applications
- **Arxiv ID**: http://arxiv.org/abs/1810.09811v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.09811v1)
- **Published**: 2018-10-23 12:24:03+00:00
- **Updated**: 2018-10-23 12:24:03+00:00
- **Authors**: Frida Femling, Adam Olsson, Fernando Alonso-Fernandez
- **Comment**: Accepted for publication at Intl Conf on Signal Image Technology &
  Internet Based Systems, SITIS 2018
- **Journal**: Proc. Intl Conf on Signal Image Technology & Internet Based
  Systems, SITIS, Gran Canaria, Spain, 26-29 Nov 2018
- **Summary**: This paper describes an approach of creating a system identifying fruit and vegetables in the retail market using images captured with a video camera attached to the system. The system helps the customers to label desired fruits and vegetables with a price according to its weight. The purpose of the system is to minimize the number of human computer interactions, speed up the identification process and improve the usability of the graphical user interface compared to existing manual systems. The hardware of the system is constituted by a Raspberry Pi, camera, display, load cell and a case. To classify an object, different convolutional neural networks have been tested and retrained. To test the usability, a heuristic evaluation has been performed with several users, concluding that the implemented system is more user friendly compared to existing systems.



### Self-Erasing Network for Integral Object Attention
- **Arxiv ID**: http://arxiv.org/abs/1810.09821v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.09821v1)
- **Published**: 2018-10-23 12:53:56+00:00
- **Updated**: 2018-10-23 12:53:56+00:00
- **Authors**: Qibin Hou, Peng-Tao Jiang, Yunchao Wei, Ming-Ming Cheng
- **Comment**: Accepted by NIPS2018
- **Journal**: None
- **Summary**: Recently, adversarial erasing for weakly-supervised object attention has been deeply studied due to its capability in localizing integral object regions. However, such a strategy raises one key problem that attention regions will gradually expand to non-object regions as training iterations continue, which significantly decreases the quality of the produced attention maps. To tackle such an issue as well as promote the quality of object attention, we introduce a simple yet effective Self-Erasing Network (SeeNet) to prohibit attentions from spreading to unexpected background regions. In particular, SeeNet leverages two self-erasing strategies to encourage networks to use reliable object and background cues for learning to attention. In this way, integral object regions can be effectively highlighted without including much more background regions. To test the quality of the generated attention maps, we employ the mined object regions as heuristic cues for learning semantic segmentation models. Experiments on Pascal VOC well demonstrate the superiority of our SeeNet over other state-of-the-art methods.



### DropFilter: Dropout for Convolutions
- **Arxiv ID**: http://arxiv.org/abs/1810.09849v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.09849v1)
- **Published**: 2018-10-23 13:42:25+00:00
- **Updated**: 2018-10-23 13:42:25+00:00
- **Authors**: Zhengsu Chen Jianwei Niu Qi Tian
- **Comment**: None
- **Journal**: None
- **Summary**: Using a large number of parameters , deep neural networks have achieved remarkable performance on computer vison and natural language processing tasks. However the networks usually suffer from overfitting by using too much parameters. Dropout is a widely use method to deal with overfitting. Although dropout can significantly regularize densely connected layers in neural networks, it leads to suboptimal results when using for convolutional layers. To track this problem, we propose DropFilter, a new dropout method for convolutional layers. DropFilter randomly suppresses the outputs of some filters. Because it is observed that co-adaptions are more likely to occurs inter filters rather than intra filters in convolutional layers. Using DropFilter, we remarkably improve the performance of convolutional networks on CIFAR and ImageNet.



### Brand > Logo: Visual Analysis of Fashion Brands
- **Arxiv ID**: http://arxiv.org/abs/1810.09941v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.09941v1)
- **Published**: 2018-10-23 16:06:22+00:00
- **Updated**: 2018-10-23 16:06:22+00:00
- **Authors**: M. Hadi Kiapour, Robinson Piramuthu
- **Comment**: ECCV 2018 First Workshop on Computer Vision For Fashion, Art and
  Design accepted paper
- **Journal**: None
- **Summary**: While lots of people may think branding begins and ends with a logo, fashion brands communicate their uniqueness through a wide range of visual cues such as color, patterns and shapes. In this work, we analyze learned visual representations by deep networks that are trained to recognize fashion brands. In particular, the activation strength and extent of neurons are studied to provide interesting insights about visual brand expressions. The proposed method identifies where a brand stands in the spectrum of branding strategy, i.e., from trademark-emblazoned goods with bold logos to implicit no logo marketing. By quantifying attention maps, we are able to interpret the visual characteristics of a brand present in a single image and model the general design direction of a brand as a whole. We further investigate versatility of neurons and discover "specialists" that are highly brand-specific and "generalists" that detect diverse visual features. A human experiment based on three main visual scenarios of fashion brands is conducted to verify the alignment of our quantitative measures with the human perception of brands. This paper demonstrate how deep networks go beyond logos in order to recognize clothing brands in an image.



### Analyzing Neuroimaging Data Through Recurrent Deep Learning Models
- **Arxiv ID**: http://arxiv.org/abs/1810.09945v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, q-bio.NC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.09945v2)
- **Published**: 2018-10-23 16:23:27+00:00
- **Updated**: 2019-04-05 07:31:32+00:00
- **Authors**: Armin W. Thomas, Hauke R. Heekeren, Klaus-Robert Müller, Wojciech Samek
- **Comment**: 36 pages, 9 figures
- **Journal**: None
- **Summary**: The application of deep learning (DL) models to neuroimaging data poses several challenges, due to the high dimensionality, low sample size and complex temporo-spatial dependency structure of these datasets. Even further, DL models act as as black-box models, impeding insight into the association of cognitive state and brain activity. To approach these challenges, we introduce the DeepLight framework, which utilizes long short-term memory (LSTM) based DL models to analyze whole-brain functional Magnetic Resonance Imaging (fMRI) data. To decode a cognitive state (e.g., seeing the image of a house), DeepLight separates the fMRI volume into a sequence of axial brain slices, which is then sequentially processed by an LSTM. To maintain interpretability, DeepLight adapts the layer-wise relevance propagation (LRP) technique. Thereby, decomposing its decoding decision into the contributions of the single input voxels to this decision. Importantly, the decomposition is performed on the level of single fMRI volumes, enabling DeepLight to study the associations between cognitive state and brain activity on several levels of data granularity, from the level of the group down to the level of single time points. To demonstrate the versatility of DeepLight, we apply it to a large fMRI dataset of the Human Connectome Project. We show that DeepLight outperforms conventional approaches of uni- and multivariate fMRI analysis in decoding the cognitive states and in identifying the physiologically appropriate brain regions associated with these states. We further demonstrate DeepLight's ability to study the fine-grained temporo-spatial variability of brain activity over sequences of single fMRI samples.



### GhostVLAD for set-based face recognition
- **Arxiv ID**: http://arxiv.org/abs/1810.09951v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.09951v1)
- **Published**: 2018-10-23 16:31:10+00:00
- **Updated**: 2018-10-23 16:31:10+00:00
- **Authors**: Yujie Zhong, Relja Arandjelović, Andrew Zisserman
- **Comment**: Accepted by ACCV 2018
- **Journal**: None
- **Summary**: The objective of this paper is to learn a compact representation of image sets for template-based face recognition. We make the following contributions: first, we propose a network architecture which aggregates and embeds the face descriptors produced by deep convolutional neural networks into a compact fixed-length representation. This compact representation requires minimal memory storage and enables efficient similarity computation. Second, we propose a novel GhostVLAD layer that includes {\em ghost clusters}, that do not contribute to the aggregation. We show that a quality weighting on the input faces emerges automatically such that informative images contribute more than those with low quality, and that the ghost clusters enhance the network's ability to deal with poor quality images. Third, we explore how input feature dimension, number of clusters and different training techniques affect the recognition performance. Given this analysis, we train a network that far exceeds the state-of-the-art on the IJB-B face recognition dataset. This is currently one of the most challenging public benchmarks, and we surpass the state-of-the-art on both the identification and verification protocols.



### Meta-Learning Multi-task Communication
- **Arxiv ID**: http://arxiv.org/abs/1810.09988v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1810.09988v1)
- **Published**: 2018-10-23 17:42:17+00:00
- **Updated**: 2018-10-23 17:42:17+00:00
- **Authors**: Pengfei Liu, Xuanjing Huang
- **Comment**: A related blog can be found on the author's homepage
- **Journal**: None
- **Summary**: In this paper, we describe a general framework: Parameters Read-Write Networks (PRaWNs) to systematically analyze current neural models for multi-task learning, in which we find that existing models expect to disentangle features into different spaces while features learned in practice are still entangled in shared space, leaving potential hazards for other training or unseen tasks.   We propose to alleviate this problem by incorporating an inductive bias into the process of multi-task learning, that each task can keep informed of not only the knowledge stored in other tasks but the way how other tasks maintain their knowledge.   In practice, we achieve above inductive bias by allowing different tasks to communicate by passing both hidden variables and gradients explicitly.   Experimentally, we evaluate proposed methods on three groups of tasks and two types of settings (\textsc{in-task} and \textsc{out-of-task}). Quantitative and qualitative results show their effectiveness.



### Classifying and Visualizing Emotions with Emotional DAN
- **Arxiv ID**: http://arxiv.org/abs/1810.10529v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.10529v1)
- **Published**: 2018-10-23 18:24:46+00:00
- **Updated**: 2018-10-23 18:24:46+00:00
- **Authors**: Ivona Tautkute, Tomasz Trzcinski
- **Comment**: Under review at Special Issue on Deep Neural Networks for Digital
  Media Algorithms, Fundamenta Informaticae. arXiv admin note: text overlap
  with arXiv:1805.00326
- **Journal**: None
- **Summary**: Classification of human emotions remains an important and challenging task for many computer vision algorithms, especially in the era of humanoid robots which coexist with humans in their everyday life. Currently proposed methods for emotion recognition solve this task using multi-layered convolutional networks that do not explicitly infer any facial features in the classification phase. In this work, we postulate a fundamentally different approach to solve emotion recognition task that relies on incorporating facial landmarks as a part of the classification loss function. To that end, we extend a recently proposed Deep Alignment Network (DAN) with a term related to facial features. Thanks to this simple modification, our model called EmotionalDAN is able to outperform state-of-the-art emotion classification methods on two challenging benchmark dataset by up to 5%. Furthermore, we visualize image regions analyzed by the network when making a decision and the results indicate that our EmotionalDAN model is able to correctly identify facial landmarks responsible for expressing the emotions.



### DeepLSR: a deep learning approach for laser speckle reduction
- **Arxiv ID**: http://arxiv.org/abs/1810.10039v4
- **DOI**: 10.1364/BOE.10.002869
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.10039v4)
- **Published**: 2018-10-23 18:36:35+00:00
- **Updated**: 2019-04-23 00:20:01+00:00
- **Authors**: Taylor L. Bobrow, Faisal Mahmood, Miguel Inserni, Nicholas J. Durr
- **Comment**: None
- **Journal**: None
- **Summary**: Speckle artifacts degrade image quality in virtually all modalities that utilize coherent energy, including optical coherence tomography, reflectance confocal microscopy, ultrasound, and widefield imaging with laser illumination. We present an adversarial deep learning framework for laser speckle reduction, called DeepLSR (https://durr.jhu.edu/DeepLSR), that transforms images from a source domain of coherent illumination to a target domain of speckle-free, incoherent illumination. We apply this method to widefield images of objects and tissues illuminated with a multi-wavelength laser, using light emitting diode-illuminated images as ground truth. In images of gastrointestinal tissues, DeepLSR reduces laser speckle noise by 6.4 dB, compared to a 2.9 dB reduction from optimized non-local means processing, a 3.0 dB reduction from BM3D, and a 3.7 dB reduction from an optical speckle reducer utilizing an oscillating diffuser. Further, DeepLSR can be combined with optical speckle reduction to reduce speckle noise by 9.4 dB. This dramatic reduction in speckle noise may enable the use of coherent light sources in applications that require small illumination sources and high-quality imaging, including medical endoscopy.



### A Fusion Approach for Multi-Frame Optical Flow Estimation
- **Arxiv ID**: http://arxiv.org/abs/1810.10066v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.10066v2)
- **Published**: 2018-10-23 19:46:57+00:00
- **Updated**: 2018-11-29 18:10:01+00:00
- **Authors**: Zhile Ren, Orazio Gallo, Deqing Sun, Ming-Hsuan Yang, Erik B. Sudderth, Jan Kautz
- **Comment**: Work accepted at IEEE Winter Conference on Applications of Computer
  Vision (WACV 2019)
- **Journal**: None
- **Summary**: To date, top-performing optical flow estimation methods only take pairs of consecutive frames into account. While elegant and appealing, the idea of using more than two frames has not yet produced state-of-the-art results. We present a simple, yet effective fusion approach for multi-frame optical flow that benefits from longer-term temporal cues. Our method first warps the optical flow from previous frames to the current, thereby yielding multiple plausible estimates. It then fuses the complementary information carried by these estimates into a new optical flow field. At the time of writing, our method ranks first among published results in the MPI Sintel and KITTI 2015 benchmarks. Our models will be available on https://github.com/NVlabs/PWC-Net.



### NestDNN: Resource-Aware Multi-Tenant On-Device Deep Learning for Continuous Mobile Vision
- **Arxiv ID**: http://arxiv.org/abs/1810.10090v1
- **DOI**: 10.1145/3241539.3241559
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.10090v1)
- **Published**: 2018-10-23 21:07:42+00:00
- **Updated**: 2018-10-23 21:07:42+00:00
- **Authors**: Biyi Fang, Xiao Zeng, Mi Zhang
- **Comment**: 12 pages
- **Journal**: Fang, Biyi, Xiao Zeng, and Mi Zhang. "NestDNN: Resource-Aware
  Multi-Tenant On-Device Deep Learning for Continuous Mobile Vision."
  Proceedings of the 24th Annual International Conference on Mobile Computing
  and Networking. ACM, 2018
- **Summary**: Mobile vision systems such as smartphones, drones, and augmented-reality headsets are revolutionizing our lives. These systems usually run multiple applications concurrently and their available resources at runtime are dynamic due to events such as starting new applications, closing existing applications, and application priority changes. In this paper, we present NestDNN, a framework that takes the dynamics of runtime resources into account to enable resource-aware multi-tenant on-device deep learning for mobile vision systems. NestDNN enables each deep learning model to offer flexible resource-accuracy trade-offs. At runtime, it dynamically selects the optimal resource-accuracy trade-off for each deep learning model to fit the model's resource demand to the system's available runtime resources. In doing so, NestDNN efficiently utilizes the limited resources in mobile vision systems to jointly maximize the performance of all the concurrently running applications. Our experiments show that compared to the resource-agnostic status quo approach, NestDNN achieves as much as 4.2% increase in inference accuracy, 2.0x increase in video frame processing rate and 1.7x reduction on energy consumption.



### Structured Domain Randomization: Bridging the Reality Gap by Context-Aware Synthetic Data
- **Arxiv ID**: http://arxiv.org/abs/1810.10093v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.10093v2)
- **Published**: 2018-10-23 21:15:55+00:00
- **Updated**: 2020-08-18 14:16:52+00:00
- **Authors**: Aayush Prakash, Shaad Boochoon, Mark Brophy, David Acuna, Eric Cameracci, Gavriel State, Omer Shapira, Stan Birchfield
- **Comment**: ICRA 2019; for video, see https://youtu.be/1WdjWJYx9AY
- **Journal**: None
- **Summary**: We present structured domain randomization (SDR), a variant of domain randomization (DR) that takes into account the structure and context of the scene. In contrast to DR, which places objects and distractors randomly according to a uniform probability distribution, SDR places objects and distractors randomly according to probability distributions that arise from the specific problem at hand. In this manner, SDR-generated imagery enables the neural network to take the context around an object into consideration during detection. We demonstrate the power of SDR for the problem of 2D bounding box car detection, achieving competitive results on real data after training only on synthetic data. On the KITTI easy, moderate, and hard tasks, we show that SDR outperforms other approaches to generating synthetic data (VKITTI, Sim 200k, or DR), as well as real data collected in a different domain (BDD100K). Moreover, synthetic SDR data combined with real KITTI data outperforms real KITTI data alone.



### Resource-Constrained Simultaneous Detection and Labeling of Objects in High-Resolution Satellite Images
- **Arxiv ID**: http://arxiv.org/abs/1810.10110v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.10110v1)
- **Published**: 2018-10-23 22:19:09+00:00
- **Updated**: 2018-10-23 22:19:09+00:00
- **Authors**: Gilbert Rotich, Rodrigo Minetto, Sudeep Sarkar
- **Comment**: None
- **Journal**: None
- **Summary**: We describe a strategy for detection and classification of man-made objects in large high-resolution satellite photos under computational resource constraints. We detect and classify candidate objects by using five pipelines of convolutional neural network processing (CNN), run in parallel. Each pipeline has its own unique strategy for fine tunning parameters, proposal region filtering, and dealing with image scales. The conflicting region proposals are merged based on region confidence and not just based on overlap areas, which improves the quality of the final bounding-box regions selected. We demonstrate this strategy using the recent xView challenge, which is a complex benchmark with more than 1,100 high-resolution images, spanning 800,000 aerial objects around the world covering a total area of 1,400 square kilometers at 0.3 meter ground sample distance. To tackle the resource-constrained problem posed by the xView challenge, where inferences are restricted to be on CPU with 8GB memory limit, we used lightweight CNN's trained with the single shot detector algorithm. Our approach was competitive on sequestered sets; it was ranked third.



### End-to-End Diagnosis and Segmentation Learning from Cardiac Magnetic Resonance Imaging
- **Arxiv ID**: http://arxiv.org/abs/1810.10117v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.10117v1)
- **Published**: 2018-10-23 22:40:13+00:00
- **Updated**: 2018-10-23 22:40:13+00:00
- **Authors**: Gerard Snaauw, Dong Gong, Gabriel Maicas, Anton van den Hengel, Wiro J. Niessen, Johan Verjans, Gustavo Carneiro
- **Comment**: submitted to 2019 IEEE International Symposium on Biomedical Imaging
  (ISBI)
- **Journal**: None
- **Summary**: Cardiac magnetic resonance (CMR) is used extensively in the diagnosis and management of cardiovascular disease. Deep learning methods have proven to deliver segmentation results comparable to human experts in CMR imaging, but there have been no convincing results for the problem of end-to-end segmentation and diagnosis from CMR. This is in part due to a lack of sufficiently large datasets required to train robust diagnosis models. In this paper, we propose a learning method to train diagnosis models, where our approach is designed to work with relatively small datasets. In particular, the optimisation loss is based on multi-task learning that jointly trains for the tasks of segmentation and diagnosis classification. We hypothesize that segmentation has a regularizing effect on the learning of features relevant for diagnosis. Using the 100 training and 50 testing samples available from the Automated Cardiac Diagnosis Challenge (ACDC) dataset, which has a balanced distribution of 5 cardiac diagnoses, we observe a reduction of the classification error from 32% to 22%, and a faster convergence compared to a baseline without segmentation. To the best of our knowledge, this is the best diagnosis results from CMR using an end-to-end diagnosis and segmentation learning method.



