# Arxiv Papers in cs.CV on 2018-10-06
### FD-GAN: Pose-guided Feature Distilling GAN for Robust Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1810.02936v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.02936v2)
- **Published**: 2018-10-06 05:17:18+00:00
- **Updated**: 2018-12-12 14:38:17+00:00
- **Authors**: Yixiao Ge, Zhuowan Li, Haiyu Zhao, Guojun Yin, Shuai Yi, Xiaogang Wang, Hongsheng Li
- **Comment**: Accepted in Proceedings of 32nd Conference on Neural Information
  Processing Systems (NeurIPS 2018). Code available:
  https://github.com/yxgeee/FD-GAN
- **Journal**: None
- **Summary**: Person re-identification (reID) is an important task that requires to retrieve a person's images from an image dataset, given one image of the person of interest. For learning robust person features, the pose variation of person images is one of the key challenges. Existing works targeting the problem either perform human alignment, or learn human-region-based representations. Extra pose information and computational cost is generally required for inference. To solve this issue, a Feature Distilling Generative Adversarial Network (FD-GAN) is proposed for learning identity-related and pose-unrelated representations. It is a novel framework based on a Siamese structure with multiple novel discriminators on human poses and identities. In addition to the discriminators, a novel same-pose loss is also integrated, which requires appearance of a same person's generated images to be similar. After learning pose-unrelated person features with pose guidance, no auxiliary pose information and additional computational cost is required during testing. Our proposed FD-GAN achieves state-of-the-art performance on three person reID datasets, which demonstrates that the effectiveness and robust feature distilling capability of the proposed FD-GAN.



### Camera Model Identification Using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1810.02981v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1810.02981v2)
- **Published**: 2018-10-06 11:11:31+00:00
- **Updated**: 2018-12-09 07:47:43+00:00
- **Authors**: Artur Kuzin, Artur Fattakhov, Ilya Kibardin, Vladimir Iglovikov, Ruslan Dautov
- **Comment**: 5 pages, 2 figures
- **Journal**: None
- **Summary**: Source camera identification is the process of determining which camera or model has been used to capture an image. In the recent years, there has been a rapid growth of research interest in the domain of forensics. In the current work, we describe our Deep Learning approach to the camera detection task of 10 cameras as a part of the Camera Model Identification Challenge hosted by Kaggle.com where our team finished 2nd out of 582 teams with the accuracy on the unseen data of 98%. We used aggressive data augmentations that allowed a model to stay robust against transformations. A number of experiments are carried out on datasets collected by organizers and scraped from the web.



### Context-Aware Deep Spatio-Temporal Network for Hand Pose Estimation from Depth Images
- **Arxiv ID**: http://arxiv.org/abs/1810.02994v1
- **DOI**: 10.1109/TCYB.2018.2873733
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.02994v1)
- **Published**: 2018-10-06 12:25:50+00:00
- **Updated**: 2018-10-06 12:25:50+00:00
- **Authors**: Yiming Wu, Wei Ji, Xi Li, Gang Wang, Jianwei Yin, Fei Wu
- **Comment**: IEEE Transactions On Cybernetics
- **Journal**: None
- **Summary**: As a fundamental and challenging problem in computer vision, hand pose estimation aims to estimate the hand joint locations from depth images. Typically, the problem is modeled as learning a mapping function from images to hand joint coordinates in a data-driven manner. In this paper, we propose Context-Aware Deep Spatio-Temporal Network (CADSTN), a novel method to jointly model the spatio-temporal properties for hand pose estimation. Our proposed network is able to learn the representations of the spatial information and the temporal structure from the image sequences. Moreover, by adopting adaptive fusion method, the model is capable of dynamically weighting different predictions to lay emphasis on sufficient context. Our method is examined on two common benchmarks, the experimental results demonstrate that our proposed approach achieves the best or the second-best performance with state-of-the-art methods and runs in 60fps.



### A Comprehensive Survey of Deep Learning for Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/1810.04020v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.04020v2)
- **Published**: 2018-10-06 16:31:52+00:00
- **Updated**: 2018-10-14 04:55:06+00:00
- **Authors**: Md. Zakir Hossain, Ferdous Sohel, Mohd Fairuz Shiratuddin, Hamid Laga
- **Comment**: 36 Pages, Accepted as a Journal Paper in ACM Computing Surveys
  (October 2018)
- **Journal**: None
- **Summary**: Generating a description of an image is called image captioning. Image captioning requires to recognize the important objects, their attributes and their relationships in an image. It also needs to generate syntactically and semantically correct sentences. Deep learning-based techniques are capable of handling the complexities and challenges of image captioning. In this survey paper, we aim to present a comprehensive review of existing deep learning-based image captioning techniques. We discuss the foundation of the techniques to analyze their performances, strengths and limitations. We also discuss the datasets and the evaluation metrics popularly used in deep learning based automatic image captioning.



### Deep Geodesic Learning for Segmentation and Anatomical Landmarking
- **Arxiv ID**: http://arxiv.org/abs/1810.04021v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.04021v1)
- **Published**: 2018-10-06 17:37:39+00:00
- **Updated**: 2018-10-06 17:37:39+00:00
- **Authors**: Neslisah Torosdagli, Denise K. Liberton, Payal Verma, Murat Sincan, Janice S. Lee, Ulas Bagci
- **Comment**: 14 pages, 12 Figures, IEEE Transactions on Medical Imaging 2018,
  TMI-2018-0898.R1
- **Journal**: None
- **Summary**: In this paper, we propose a novel deep learning framework for anatomy segmentation and automatic landmark- ing. Specifically, we focus on the challenging problem of mandible segmentation from cone-beam computed tomography (CBCT) scans and identification of 9 anatomical landmarks of the mandible on the geodesic space. The overall approach employs three inter-related steps. In step 1, we propose a deep neu- ral network architecture with carefully designed regularization, and network hyper-parameters to perform image segmentation without the need for data augmentation and complex post- processing refinement. In step 2, we formulate the landmark localization problem directly on the geodesic space for sparsely- spaced anatomical landmarks. In step 3, we propose to use a long short-term memory (LSTM) network to identify closely- spaced landmarks, which is rather difficult to obtain using other standard detection networks. The proposed fully automated method showed superior efficacy compared to the state-of-the- art mandible segmentation and landmarking approaches in craniofacial anomalies and diseased states. We used a very challenging CBCT dataset of 50 patients with a high-degree of craniomaxillofacial (CMF) variability that is realistic in clinical practice. Complementary to the quantitative analysis, the qualitative visual inspection was conducted for distinct CBCT scans from 250 patients with high anatomical variability. We have also shown feasibility of the proposed work in an independent dataset from MICCAI Head-Neck Challenge (2015) achieving the state-of-the-art performance. Lastly, we present an in-depth analysis of the proposed deep networks with respect to the choice of hyper-parameters such as pooling and activation functions.



### Robustness via Retrying: Closed-Loop Robotic Manipulation with Self-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/1810.03043v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1810.03043v1)
- **Published**: 2018-10-06 19:51:46+00:00
- **Updated**: 2018-10-06 19:51:46+00:00
- **Authors**: Frederik Ebert, Sudeep Dasari, Alex X. Lee, Sergey Levine, Chelsea Finn
- **Comment**: accepted at the Conference on Robot Learning (CoRL) 2018
- **Journal**: None
- **Summary**: Prediction is an appealing objective for self-supervised learning of behavioral skills, particularly for autonomous robots. However, effectively utilizing predictive models for control, especially with raw image inputs, poses a number of major challenges. How should the predictions be used? What happens when they are inaccurate? In this paper, we tackle these questions by proposing a method for learning robotic skills from raw image observations, using only autonomously collected experience. We show that even an imperfect model can complete complex tasks if it can continuously retry, but this requires the model to not lose track of the objective (e.g., the object of interest). To enable a robot to continuously retry a task, we devise a self-supervised algorithm for learning image registration, which can keep track of objects of interest for the duration of the trial. We demonstrate that this idea can be combined with a video-prediction based controller to enable complex behaviors to be learned from scratch using only raw visual inputs, including grasping, repositioning objects, and non-prehensile manipulation. Our real-world experiments demonstrate that a model trained with 160 robot hours of autonomously collected, unlabeled data is able to successfully perform complex manipulation tasks with a wide range of objects not seen during training.



### Provable Subspace Tracking from Missing Data and Matrix Completion
- **Arxiv ID**: http://arxiv.org/abs/1810.03051v2
- **DOI**: 10.1109/TSP.2019.2924595
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.03051v2)
- **Published**: 2018-10-06 20:54:25+00:00
- **Updated**: 2019-05-30 04:49:34+00:00
- **Authors**: Praneeth Narayanamurthy, Vahid Daneshpajooh, Namrata Vaswani
- **Comment**: Writing changes; includes a detailed discussion of noise analysis;
  contains discussion for Matrix Completion; Accepted to IEEE Transactions on
  Signal Processing
- **Journal**: IEEE Transactions on Signal Processing (Volume: 67 , Issue: 16 ,
  Aug, 15 2019)
- **Summary**: We study the problem of subspace tracking in the presence of missing data (ST-miss). In recent work, we studied a related problem called robust ST. In this work, we show that a simple modification of our robust ST solution also provably solves ST-miss and robust ST-miss. To our knowledge, our result is the first `complete' guarantee for ST-miss. This means that we can prove that under assumptions on only the algorithm inputs, the output subspace estimates are close to the true data subspaces at all times. Our guarantees hold under mild and easily interpretable assumptions, and allow the underlying subspace to change with time in a piecewise constant fashion. In contrast, all existing guarantees for ST are partial results and assume a fixed unknown subspace. Extensive numerical experiments are shown to back up our theoretical claims. Finally, our solution can be interpreted as a provably correct mini-batch and memory-efficient solution to low-rank Matrix Completion (MC).



