# Arxiv Papers in cs.CV on 2018-10-17
### Progressive Weight Pruning of Deep Neural Networks using ADMM
- **Arxiv ID**: http://arxiv.org/abs/1810.07378v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.07378v2)
- **Published**: 2018-10-17 03:51:38+00:00
- **Updated**: 2018-11-04 16:41:06+00:00
- **Authors**: Shaokai Ye, Tianyun Zhang, Kaiqi Zhang, Jiayu Li, Kaidi Xu, Yunfei Yang, Fuxun Yu, Jian Tang, Makan Fardad, Sijia Liu, Xiang Chen, Xue Lin, Yanzhi Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) although achieving human-level performance in many domains, have very large model size that hinders their broader applications on edge computing devices. Extensive research work have been conducted on DNN model compression or pruning. However, most of the previous work took heuristic approaches. This work proposes a progressive weight pruning approach based on ADMM (Alternating Direction Method of Multipliers), a powerful technique to deal with non-convex optimization problems with potentially combinatorial constraints. Motivated by dynamic programming, the proposed method reaches extremely high pruning rate by using partial prunings with moderate pruning rates. Therefore, it resolves the accuracy degradation and long convergence time problems when pursuing extremely high pruning ratios. It achieves up to 34 times pruning rate for ImageNet dataset and 167 times pruning rate for MNIST dataset, significantly higher than those reached by the literature work. Under the same number of epochs, the proposed method also achieves faster convergence and higher compression rates. The codes and pruned DNN models are released in the link bit.ly/2zxdlss



### The Role of Emotion in Problem Solving: First Results from Observing Chess
- **Arxiv ID**: http://arxiv.org/abs/1810.11094v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1810.11094v1)
- **Published**: 2018-10-17 06:35:31+00:00
- **Updated**: 2018-10-17 06:35:31+00:00
- **Authors**: Thomas Guntz, James Crowley, Dominique Vaufreydaz, Raffaella Balzarini, Philippe Dessus
- **Comment**: None
- **Journal**: ICMI 2018 - Workshop at 20th ACM International Conference on
  Multimodal Interaction, Oct 2018, Boulder, Colorado, United States. pp.1-13
- **Summary**: In this paper we present results from recent experiments that suggest that chess players associate emotions to game situations and reactively use these associations to guide search for planning and problem solving. We describe the design of an instrument for capturing and interpreting multimodal signals of humans engaged in solving challenging problems. We review results from a pilot experiment with human experts engaged in solving challenging problems in Chess that revealed an unexpected observation of rapid changes in emotion as players attempt to solve challenging problems. We propose a cognitive model that describes the process by which subjects select chess chunks for use in interpretation of the game situation and describe initial results from a second experiment designed to test this model.



### Recognizing Partial Biometric Patterns
- **Arxiv ID**: http://arxiv.org/abs/1810.07399v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.07399v1)
- **Published**: 2018-10-17 06:56:45+00:00
- **Updated**: 2018-10-17 06:56:45+00:00
- **Authors**: Lingxiao He, Zhenan Sun, Yuhao Zhu, Yunbo Wang
- **Comment**: 13 pages, 11 figures
- **Journal**: None
- **Summary**: Biometric recognition on partial captured targets is challenging, where only several partial observations of objects are available for matching. In this area, deep learning based methods are widely applied to match these partial captured objects caused by occlusions, variations of postures or just partial out of view in person re-identification and partial face recognition. However, most current methods are not able to identify an individual in case that some parts of the object are not obtainable, while the rest are specialized to certain constrained scenarios. To this end, we propose a robust general framework for arbitrary biometric matching scenarios without the limitations of alignment as well as the size of inputs. We introduce a feature post-processing step to handle the feature maps from FCN and a dictionary learning based Spatial Feature Reconstruction (SFR) to match different sized feature maps in this work. Moreover, the batch hard triplet loss function is applied to optimize the model. The applicability and effectiveness of the proposed method are demonstrated by the results from experiments on three person re-identification datasets (Market1501, CUHK03, DukeMTMC-reID), two partial person datasets (Partial REID and Partial iLIDS) and two partial face datasets (CASIA-NIR-Distance and Partial LFW), on which state-of-the-art performance is ensured in comparison with several state-of-the-art approaches. The code is released online and can be found on the website: https://github.com/lingxiao-he/Partial-Person-ReID.



### Coherence Constraints in Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/1810.10326v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.10326v1)
- **Published**: 2018-10-17 07:51:46+00:00
- **Updated**: 2018-10-17 07:51:46+00:00
- **Authors**: Lisa Graziani, Stefano Melacci, Marco Gori
- **Comment**: None
- **Journal**: None
- **Summary**: Recognizing facial expressions from static images or video sequences is a widely studied but still challenging problem. The recent progresses obtained by deep neural architectures, or by ensembles of heterogeneous models, have shown that integrating multiple input representations leads to state-of-the-art results. In particular, the appearance and the shape of the input face, or the representations of some face parts, are commonly used to boost the quality of the recognizer. This paper investigates the application of Convolutional Neural Networks (CNNs) with the aim of building a versatile recognizer of expressions in static images that can be further applied to video sequences. We first study the importance of different face parts in the recognition task, focussing on appearance and shape-related features. Then we cast the learning problem in the Semi-Supervised setting, exploiting video data, where only a few frames are supervised. The unsupervised portion of the training data is used to enforce three types of coherence, namely temporal coherence, coherence among the predictions on the face parts and coherence between appearance and shape-based representation. Our experimental analysis shows that coherence constraints can improve the quality of the expression recognizer, thus offering a suitable basis to profitably exploit unsupervised video sequences. Finally we present some examples with occlusions where the shape-based predictor performs better than the appearance one.



### Embarrassingly Simple Model for Early Action Proposal
- **Arxiv ID**: http://arxiv.org/abs/1810.07420v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.07420v2)
- **Published**: 2018-10-17 08:09:09+00:00
- **Updated**: 2018-10-18 07:43:00+00:00
- **Authors**: Marcos Baptista-Ríos, Roberto J. López-Sastre, Franciso Javier Acevedo-Rodríguez, Saturnino Maldonado-Bascón
- **Comment**: Published in the Anticipating Human Behavior Workshop, ECCV 2018
- **Journal**: None
- **Summary**: Early action proposal consists in generating high quality candidate temporal segments that are likely to contain an action in a video stream, as soon as they happen. Many sophisticated approaches have been proposed for the action proposal problem but from the off-line perspective. On the contrary, we focus on the on-line version of the problem, proposing a simple classifier-based model, using standard 3D CNNs, that performs significantly better than the state of the art.



### Learning an MR acquisition-invariant representation using Siamese neural networks
- **Arxiv ID**: http://arxiv.org/abs/1810.07430v1
- **DOI**: 10.1109/ISBI.2019.8759281
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.07430v1)
- **Published**: 2018-10-17 08:37:09+00:00
- **Updated**: 2018-10-17 08:37:09+00:00
- **Authors**: Wouter M. Kouw, Marco Loog, Wilbert Bartels, Adriënne M. Mendrik
- **Comment**: 3 figures, submitted to International Symposium on Biomedical Imaging
  2019
- **Journal**: 16th IEEE International Symposium on Biomedical Imaging (ISBI),
  Venice, 2019, pp. 364-367
- **Summary**: Generalization of voxelwise classifiers is hampered by differences between MRI-scanners, e.g. different acquisition protocols and field strengths. To address this limitation, we propose a Siamese neural network (MRAI-NET) that extracts acquisition-invariant feature vectors. These can consequently be used by task-specific methods, such as voxelwise classifiers for tissue segmentation. MRAI-NET is tested on both simulated and real patient data. Experiments show that MRAI-NET outperforms voxelwise classifiers trained on the source or target scanner data when a small number of labeled samples is available.



### Learning to quantify emphysema extent: What labels do we need?
- **Arxiv ID**: http://arxiv.org/abs/1810.07433v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.07433v1)
- **Published**: 2018-10-17 08:48:27+00:00
- **Updated**: 2018-10-17 08:48:27+00:00
- **Authors**: Silas Nyboe Ørting, Jens Petersen, Laura H. Thomsen, Mathilde M. W. Wille, Marleen de Bruijne
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate assessment of pulmonary emphysema is crucial to assess disease severity and subtype, to monitor disease progression and to predict lung cancer risk. However, visual assessment is time-consuming and subject to substantial inter-rater variability and standard densitometry approaches to quantify emphysema remain inferior to visual scoring. We explore if machine learning methods that learn from a large dataset of visually assessed CT scans can provide accurate estimates of emphysema extent. We further investigate if machine learning algorithms that learn from a scoring of emphysema extent can outperform algorithms that learn only from a scoring of emphysema presence. We compare four Multiple Instance Learning classifiers that are trained on emphysema presence labels, and five Learning with Label Proportions classifiers that are trained on emphysema extent labels. We evaluate performance on 600 low-dose CT scans from the Danish Lung Cancer Screening Trial and find that learning from emphysema presence labels, which are much easier to obtain, gives equally good performance to learning from emphysema extent labels. The best classifiers achieve intra-class correlation coefficients around 0.90 and average overall agreement with raters of 78% and 79% on six emphysema extent classes versus inter-rater agreement of 83%.



### Strategies for Training Stain Invariant CNNs
- **Arxiv ID**: http://arxiv.org/abs/1810.10338v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.10338v2)
- **Published**: 2018-10-17 09:41:23+00:00
- **Updated**: 2018-11-03 23:13:02+00:00
- **Authors**: Thomas Lampert, Odyssée Merveille, Jessica Schmitz, Germain Forestier, Friedrich Feuerhake, Cédric Wemmert
- **Comment**: None
- **Journal**: None
- **Summary**: An important part of Digital Pathology is the analysis of multiple digitised whole slide images from differently stained tissue sections. It is common practice to mount consecutive sections containing corresponding microscopic structures on glass slides, and to stain them differently to highlight specific tissue components. These multiple staining modalities result in very different images but include a significant amount of consistent image information. Deep learning approaches have recently been proposed to analyse these images in order to automatically identify objects of interest for pathologists. These supervised approaches require a vast amount of annotations, which are difficult and expensive to acquire---a problem that is multiplied with multiple stainings. This article presents several training strategies that make progress towards stain invariant networks. By training the network on one commonly used staining modality and applying it to images that include corresponding but differently stained tissue structures, the presented unsupervised strategies demonstrate significant improvements over standard training strategies.



### Characterization of Brain Cortical Morphology Using Localized Topology-Encoding Graphs
- **Arxiv ID**: http://arxiv.org/abs/1810.10339v1
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1810.10339v1)
- **Published**: 2018-10-17 11:45:09+00:00
- **Updated**: 2018-10-17 11:45:09+00:00
- **Authors**: Sevil Maghsadhagh, Mousa Shamsi, Anders Eklund, Hamid Behjat
- **Comment**: None
- **Journal**: None
- **Summary**: The human brain cortical layer has a convoluted morphology that is unique to each individual. Characterization of the cortical morphology is necessary in longitudinal studies of structural brain change, as well as in discriminating individuals in health and disease. A method for encoding the cortical morphology in the form of a graph is presented. The design of graphs that encode the global cerebral hemisphere cortices as well as localized cortical regions is proposed. Spectral features of these graphs are then studied and proposed as descriptors of cortical morphology. As proof-of-concept of their applicability in characterizing cortical morphology, the descriptors are studied in the context of discriminating individuals based on their sex.



### Offline Signature Verification by Combining Graph Edit Distance and Triplet Networks
- **Arxiv ID**: http://arxiv.org/abs/1810.07491v1
- **DOI**: 10.1007/978-3-319-97785-0_45
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.07491v1)
- **Published**: 2018-10-17 11:55:04+00:00
- **Updated**: 2018-10-17 11:55:04+00:00
- **Authors**: Paul Maergner, Vinaychandran Pondenkandath, Michele Alberti, Marcus Liwicki, Kaspar Riesen, Rolf Ingold, Andreas Fischer
- **Comment**: None
- **Journal**: Structural, Syntactic, and Statistical Pattern Recognition. S+SSPR
  2018. Lecture Notes in Computer Science, vol 11004. Springer, Cham
- **Summary**: Biometric authentication by means of handwritten signatures is a challenging pattern recognition task, which aims to infer a writer model from only a handful of genuine signatures. In order to make it more difficult for a forger to attack the verification system, a promising strategy is to combine different writer models. In this work, we propose to complement a recent structural approach to offline signature verification based on graph edit distance with a statistical approach based on metric learning with deep neural networks. On the MCYT and GPDS benchmark datasets, we demonstrate that combining the structural and statistical models leads to significant improvements in performance, profiting from their complementary properties.



### When does Bone Suppression and Lung Field Segmentation Improve Chest X-Ray Disease Classification?
- **Arxiv ID**: http://arxiv.org/abs/1810.07500v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1810.07500v1)
- **Published**: 2018-10-17 12:30:08+00:00
- **Updated**: 2018-10-17 12:30:08+00:00
- **Authors**: Ivo M. Baltruschat, Leonhard Steinmeister, Harald Ittrich, Gerhard Adam, Hannes Nickisch, Axel Saalbach, Jens von Berg, Michael Grass, Tobias Knopp
- **Comment**: None
- **Journal**: None
- **Summary**: Chest radiography is the most common clinical examination type. To improve the quality of patient care and to reduce workload, methods for automatic pathology classification have been developed. In this contribution we investigate the usefulness of two advanced image pre-processing techniques, initially developed for image reading by radiologists, for the performance of Deep Learning methods. First, we use bone suppression, an algorithm to artificially remove the rib cage. Secondly, we employ an automatic lung field detection to crop the image to the lung area. Furthermore, we consider the combination of both in the context of an ensemble approach. In a five-times re-sampling scheme, we use Receiver Operating Characteristic (ROC) statistics to evaluate the effect of the pre-processing approaches. Using a Convolutional Neural Network (CNN), optimized for X-ray analysis, we achieve a good performance with respect to all pathologies on average. Superior results are obtained for selected pathologies when using pre-processing, i.e. for mass the area under the ROC curve increased by 9.95%. The ensemble with pre-processed trained models yields the best overall results.



### Virtual Wave Optics for Non-Line-of-Sight Imaging
- **Arxiv ID**: http://arxiv.org/abs/1810.07535v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.07535v2)
- **Published**: 2018-10-17 13:38:37+00:00
- **Updated**: 2019-08-06 14:01:27+00:00
- **Authors**: Xiaochun Liu, Ibón Guillén, Marco La Manna, Ji Hyun Nam, Syed Azer Reza, Toan Huu Le, Diego Gutierrez, Adrian Jarabo, Andreas Velten
- **Comment**: 19 pages, 6 figures
- **Journal**: None
- **Summary**: Non-Line-of-Sight (NLOS) imaging allows to observe objects partially or fully occluded from direct view, by analyzing indirect diffuse reflections off a secondary, relay surface. Despite its many potential applications, existing methods lack practical usability due to several shared limitations, including the assumption of single scattering only, lack of occlusions, and Lambertian reflectance. We lift these limitations by transforming the NLOS problem into a virtual Line-Of-Sight (LOS) one. Since imaging information cannot be recovered from the irradiance arriving at the relay surface, we introduce the concept of the phasor field, a mathematical construct representing a fast variation in irradiance. We show that NLOS light transport can be modeled as the propagation of a phasor field wave, which can be solved accurately by the Rayleigh-Sommerfeld diffraction integral. We demonstrate for the first time NLOS reconstruction of complex scenes with strong multiply scattered and ambient light, arbitrary materials, large depth range, and occlusions. Our method handles these challenging cases without explicitly developing a light transport model. By leveraging existing fast algorithms, we outperform existing methods in terms of execution speed, computational complexity, and memory use. We believe that our approach will help unlock the potential of NLOS imaging, and the development of novel applications not restricted to lab conditions. For example, we demonstrate both refocusing and transient NLOS videos of real-world, complex scenes with large depth.



### Learning and Tracking the 3D Body Shape of Freely Moving Infants from RGB-D sequences
- **Arxiv ID**: http://arxiv.org/abs/1810.07538v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.07538v1)
- **Published**: 2018-10-17 13:41:10+00:00
- **Updated**: 2018-10-17 13:41:10+00:00
- **Authors**: Nikolas Hesse, Sergi Pujades, Michael J. Black, Michael Arens, Ulrich G. Hofmann, A. Sebastian Schroeder
- **Comment**: 12 pages, supplemental video at https://youtu.be/aahF1xGurmM
- **Journal**: None
- **Summary**: Statistical models of the human body surface are generally learned from thousands of high-quality 3D scans in predefined poses to cover the wide variety of human body shapes and articulations. Acquisition of such data requires expensive equipment, calibration procedures, and is limited to cooperative subjects who can understand and follow instructions, such as adults. We present a method for learning a statistical 3D Skinned Multi-Infant Linear body model (SMIL) from incomplete, low-quality RGB-D sequences of freely moving infants. Quantitative experiments show that SMIL faithfully represents the RGB-D data and properly factorizes the shape and pose of the infants. To demonstrate the applicability of SMIL, we fit the model to RGB-D sequences of freely moving infants and show, with a case study, that our method captures enough motion detail for General Movements Assessment (GMA), a method used in clinical practice for early detection of neurodevelopmental disorders in infants. SMIL provides a new tool for analyzing infant shape and movement and is a step towards an automated system for GMA.



### Orthogonal Deep Features Decomposition for Age-Invariant Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1810.07599v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.07599v1)
- **Published**: 2018-10-17 15:08:11+00:00
- **Updated**: 2018-10-17 15:08:11+00:00
- **Authors**: Yitong Wang, Dihong Gong, Zheng Zhou, Xing Ji, Hao Wang, Zhifeng Li, Wei Liu, Tong Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: As facial appearance is subject to significant intra-class variations caused by the aging process over time, age-invariant face recognition (AIFR) remains a major challenge in face recognition community. To reduce the intra-class discrepancy caused by the aging, in this paper we propose a novel approach (namely, Orthogonal Embedding CNNs, or OE-CNNs) to learn the age-invariant deep face features. Specifically, we decompose deep face features into two orthogonal components to represent age-related and identity-related features. As a result, identity-related features that are robust to aging are then used for AIFR. Besides, for complementing the existing cross-age datasets and advancing the research in this field, we construct a brand-new large-scale Cross-Age Face dataset (CAF). Extensive experiments conducted on the three public domain face aging datasets (MORPH Album 2, CACD-VS and FG-NET) have shown the effectiveness of the proposed approach and the value of the constructed CAF dataset on AIFR. Benchmarking our algorithm on one of the most popular general face recognition (GFR) dataset LFW additionally demonstrates the comparable generalization performance on GFR.



### Pruning Deep Neural Networks using Partial Least Squares
- **Arxiv ID**: http://arxiv.org/abs/1810.07610v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.07610v3)
- **Published**: 2018-10-17 15:24:21+00:00
- **Updated**: 2019-09-19 14:33:14+00:00
- **Authors**: Artur Jordao, Ricardo Kloss, Fernando Yamada, William Robson Schwartz
- **Comment**: None
- **Journal**: British Machine Vision Conference Workshop, 2019
- **Summary**: Modern pattern recognition methods are based on convolutional networks since they are able to learn complex patterns that benefit the classification. However, convolutional networks are computationally expensive and require a considerable amount of memory, which limits their deployment on low-power and resource-constrained systems. To handle these problems, recent approaches have proposed pruning strategies that find and remove unimportant neurons (i.e., filters) in these networks. Despite achieving remarkable results, existing pruning approaches are ineffective since the accuracy of the original network is degraded. In this work, we propose a novel approach to efficiently remove filters from convolutional networks. Our approach estimates the filter importance based on its relationship with the class label on a low-dimensional space. This relationship is computed using Partial Least Squares (PLS) and Variable Importance in Projection (VIP). Our method is able to reduce up to 67% of the floating point operations (FLOPs) without penalizing the network accuracy. With a negligible drop in accuracy, we can reduce up to 90% of FLOPs. Additionally, sometimes the method is even able to improve the accuracy compared to original, unpruned, network. We show that employing PLS+VIP as the criterion for detecting the filters to be removed is better than recent feature selection techniques, which have been employed by state-of-the-art pruning methods. Finally, we show that the proposed method achieves the highest FLOPs reduction and the smallest drop in accuracy when compared to state-of-the-art pruning approaches. Codes are available at: https://github.com/arturjordao/PruningNeuralNetworks



### Video Object Segmentation using Teacher-Student Adaptation in a Human Robot Interaction (HRI) Setting
- **Arxiv ID**: http://arxiv.org/abs/1810.07733v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.07733v4)
- **Published**: 2018-10-17 18:42:53+00:00
- **Updated**: 2019-03-12 21:18:32+00:00
- **Authors**: Mennatullah Siam, Chen Jiang, Steven Lu, Laura Petrich, Mahmoud Gamal, Mohamed Elhoseiny, Martin Jagersand
- **Comment**: Accepted in ICRA'19, https://msiam.github.io/ivos/
- **Journal**: None
- **Summary**: Video object segmentation is an essential task in robot manipulation to facilitate grasping and learning affordances. Incremental learning is important for robotics in unstructured environments, since the total number of objects and their variations can be intractable. Inspired by the children learning process, human robot interaction (HRI) can be utilized to teach robots about the world guided by humans similar to how children learn from a parent or a teacher. A human teacher can show potential objects of interest to the robot, which is able to self adapt to the teaching signal without providing manual segmentation labels. We propose a novel teacher-student learning paradigm to teach robots about their surrounding environment. A two-stream motion and appearance "teacher" network provides pseudo-labels to adapt an appearance "student" network. The student network is able to segment the newly learned objects in other scenes, whether they are static or in motion. We also introduce a carefully designed dataset that serves the proposed HRI setup, denoted as (I)nteractive (V)ideo (O)bject (S)egmentation. Our IVOS dataset contains teaching videos of different objects, and manipulation tasks. Unlike previous datasets, IVOS provides manipulation tasks sequences with segmentation annotation along with the waypoints for the robot trajectories. It also provides segmentation annotation for the different transformations such as translation, scale, planar rotation, and out-of-plane rotation. Our proposed adaptation method outperforms the state-of-the-art on DAVIS and FBMS with 6.8% and 1.2% in F-measure respectively. It improves over the baseline on IVOS dataset with 46.1% and 25.9% in mIoU.



### A Convolutional Autoencoder Approach to Learn Volumetric Shape Representations for Brain Structures
- **Arxiv ID**: http://arxiv.org/abs/1810.07746v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.07746v1)
- **Published**: 2018-10-17 19:34:59+00:00
- **Updated**: 2018-10-17 19:34:59+00:00
- **Authors**: Evan M. Yu, Mert R. Sabuncu
- **Comment**: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
- **Journal**: None
- **Summary**: We propose a novel machine learning strategy for studying neuroanatomical shape variation. Our model works with volumetric binary segmentation images, and requires no pre-processing such as the extraction of surface points or a mesh. The learned shape descriptor is invariant to affine transformations, including shifts, rotations and scaling. Thanks to the adopted autoencoder framework, inter-subject differences are automatically enhanced in the learned representation, while intra-subject variances are minimized. Our experimental results on a shape retrieval task showed that the proposed representation outperforms a state-of-the-art benchmark for brain structures extracted from MRI scans.



### Investigating Object Compositionality in Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1810.10340v3
- **DOI**: 10.1016/j.neunet.2020.07.007
- **Categories**: **cs.CV**, cs.NE, I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/1810.10340v3)
- **Published**: 2018-10-17 20:17:11+00:00
- **Updated**: 2020-07-24 13:10:53+00:00
- **Authors**: Sjoerd van Steenkiste, Karol Kurach, Jürgen Schmidhuber, Sylvain Gelly
- **Comment**: A preliminary version of this work (arXiv v1) appeared under the
  title "A Case for Object Compositionality in Deep Generative Models of
  Images" as a workshop paper at the NeurIPS2018 workshop on "Modeling the
  Physical World: Perception, Learning, and Control", and at the NeurIPS2018
  workshop on "Relational Representation Learning"
- **Journal**: None
- **Summary**: Deep generative models seek to recover the process with which the observed data was generated. They may be used to synthesize new samples or to subsequently extract representations. Successful approaches in the domain of images are driven by several core inductive biases. However, a bias to account for the compositional way in which humans structure a visual scene in terms of objects has frequently been overlooked. In this work, we investigate object compositionality as an inductive bias for Generative Adversarial Networks (GANs). We present a minimal modification of a standard generator to incorporate this inductive bias and find that it reliably learns to generate images as compositions of objects. Using this general design as a backbone, we then propose two useful extensions to incorporate dependencies among objects and background. We extensively evaluate our approach on several multi-object image datasets and highlight the merits of incorporating structure for representation learning purposes. In particular, we find that our structured GANs are better at generating multi-object images that are more faithful to the reference distribution. More so, we demonstrate how, by leveraging the structure of the learned generative process, one can `invert' the learned generative model to perform unsupervised instance segmentation. On the challenging CLEVR dataset, it is shown how our approach is able to improve over other recent purely unsupervised object-centric approaches to image generation.



### LadderNet: Multi-path networks based on U-Net for medical image segmentation
- **Arxiv ID**: http://arxiv.org/abs/1810.07810v4
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1810.07810v4)
- **Published**: 2018-10-17 21:33:27+00:00
- **Updated**: 2019-08-28 17:16:55+00:00
- **Authors**: Juntang Zhuang
- **Comment**: None
- **Journal**: None
- **Summary**: U-Net has been providing state-of-the-art performance in many medical image segmentation problems. Many modifications have been proposed for U-Net, such as attention U-Net, recurrent residual convolutional U-Net (R2-UNet), and U-Net with residual blocks or blocks with dense connections. However, all these modifications have an encoder-decoder structure with skip connections, and the number of paths for information flow is limited. We propose LadderNet in this paper, which can be viewed as a chain of multiple U-Nets. Instead of only one pair of encoder branch and decoder branch in U-Net, a LadderNet has multiple pairs of encoder-decoder branches, and has skip connections between every pair of adjacent decoder and decoder branches in each level. Inspired by the success of ResNet and R2-UNet, we use modified residual blocks where two convolutional layers in one block share the same weights. A LadderNet has more paths for information flow because of skip connections and residual blocks, and can be viewed as an ensemble of Fully Convolutional Networks (FCN). The equivalence to an ensemble of FCNs improves segmentation accuracy, while the shared weights within each residual block reduce parameter number. Semantic segmentation is essential for retinal disease detection. We tested LadderNet on two benchmark datasets for blood vessel segmentation in retinal images, and achieved superior performance over methods in the literature. The implementation is provided \url{https://github.com/juntang-zhuang/LadderNet}



