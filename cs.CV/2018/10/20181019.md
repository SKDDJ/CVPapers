# Arxiv Papers in cs.CV on 2018-10-19
### A Comparative Analysis of Registration Tools: Traditional vs Deep Learning Approach on High Resolution Tissue Cleared Data
- **Arxiv ID**: http://arxiv.org/abs/1810.08315v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.08315v1)
- **Published**: 2018-10-19 00:22:07+00:00
- **Updated**: 2018-10-19 00:22:07+00:00
- **Authors**: Abdullah Nazib, Clinton Fookes, Dimitri Perrin
- **Comment**: None
- **Journal**: None
- **Summary**: Image registration plays an important role in comparing images. It is particularly important in analyzing medical images like CT, MRI, PET, etc. to quantify different biological samples, to monitor disease progression and to fuse different modalities to support better diagnosis. The recent emergence of tissue clearing protocols enable us to take images at cellular level resolution. Image registration tools developed for other modalities are currently unable to manage images of such extreme high resolution. The recent popularity of deep learning based methods in the computer vision community justifies a rigorous investigation of deep-learning based methods on tissue cleared images along with their traditional counterparts. In this paper, we investigate and compare the performance of a deep learning based registration method with traditional optimization based methods on samples from tissue-clearing methods. From the comparative results it is found that a deep-learning based method outperforms all traditional registration tools in terms of registration time and has achieved promising registration accuracy.



### Sequenced-Replacement Sampling for Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1810.08322v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.08322v1)
- **Published**: 2018-10-19 00:55:47+00:00
- **Updated**: 2018-10-19 00:55:47+00:00
- **Authors**: Chiu Man Ho, Dae Hoon Park, Wei Yang, Yi Chang
- **Comment**: None
- **Journal**: None
- **Summary**: We propose sequenced-replacement sampling (SRS) for training deep neural networks. The basic idea is to assign a fixed sequence index to each sample in the dataset. Once a mini-batch is randomly drawn in each training iteration, we refill the original dataset by successively adding samples according to their sequence index. Thus we carry out replacement sampling but in a batched and sequenced way. In a sense, SRS could be viewed as a way of performing "mini-batch augmentation". It is particularly useful for a task where we have a relatively small images-per-class such as CIFAR-100. Together with a longer period of initial large learning rate, it significantly improves the classification accuracy in CIFAR-100 over the current state-of-the-art results. Our experiments indicate that training deeper networks with SRS is less prone to over-fitting. In the best case, we achieve an error rate as low as 10.10%.



### Domain-Invariant Projection Learning for Zero-Shot Recognition
- **Arxiv ID**: http://arxiv.org/abs/1810.08326v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1810.08326v1)
- **Published**: 2018-10-19 01:08:05+00:00
- **Updated**: 2018-10-19 01:08:05+00:00
- **Authors**: An Zhao, Mingyu Ding, Jiechao Guan, Zhiwu Lu, Tao Xiang, Ji-Rong Wen
- **Comment**: Accepted to NIPS 2018
- **Journal**: None
- **Summary**: Zero-shot learning (ZSL) aims to recognize unseen object classes without any training samples, which can be regarded as a form of transfer learning from seen classes to unseen ones. This is made possible by learning a projection between a feature space and a semantic space (e.g. attribute space). Key to ZSL is thus to learn a projection function that is robust against the often large domain gap between the seen and unseen classes. In this paper, we propose a novel ZSL model termed domain-invariant projection learning (DIPL). Our model has two novel components: (1) A domain-invariant feature self-reconstruction task is introduced to the seen/unseen class data, resulting in a simple linear formulation that casts ZSL into a min-min optimization problem. Solving the problem is non-trivial, and a novel iterative algorithm is formulated as the solver, with rigorous theoretic algorithm analysis provided. (2) To further align the two domains via the learned projection, shared semantic structure among seen and unseen classes is explored via forming superclasses in the semantic space. Extensive experiments show that our model outperforms the state-of-the-art alternatives by significant margins.



### Transferrable Feature and Projection Learning with Class Hierarchy for Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1810.08329v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1810.08329v1)
- **Published**: 2018-10-19 01:21:08+00:00
- **Updated**: 2018-10-19 01:21:08+00:00
- **Authors**: Aoxue Li, Zhiwu Lu, Jiechao Guan, Tao Xiang, Liwei Wang, Ji-Rong Wen
- **Comment**: Submitted to IJCV
- **Journal**: None
- **Summary**: Zero-shot learning (ZSL) aims to transfer knowledge from seen classes to unseen ones so that the latter can be recognised without any training samples. This is made possible by learning a projection function between a feature space and a semantic space (e.g. attribute space). Considering the seen and unseen classes as two domains, a big domain gap often exists which challenges ZSL. Inspired by the fact that an unseen class is not exactly `unseen' if it belongs to the same superclass as a seen class, we propose a novel inductive ZSL model that leverages superclasses as the bridge between seen and unseen classes to narrow the domain gap. Specifically, we first build a class hierarchy of multiple superclass layers and a single class layer, where the superclasses are automatically generated by data-driven clustering over the semantic representations of all seen and unseen class names. We then exploit the superclasses from the class hierarchy to tackle the domain gap challenge in two aspects: deep feature learning and projection function learning. First, to narrow the domain gap in the feature space, we integrate a recurrent neural network (RNN) defined with the superclasses into a convolutional neural network (CNN), in order to enforce the superclass hierarchy. Second, to further learn a transferrable projection function for ZSL, a novel projection function learning method is proposed by exploiting the superclasses to align the two domains. Importantly, our transferrable feature and projection learning methods can be easily extended to a closely related task -- few-shot learning (FSL). Extensive experiments show that the proposed model significantly outperforms the state-of-the-art alternatives in both ZSL and FSL tasks.



### Improving Annotation for 3D Pose Dataset of Fine-Grained Object Categories
- **Arxiv ID**: http://arxiv.org/abs/1810.09263v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.09263v1)
- **Published**: 2018-10-19 01:48:05+00:00
- **Updated**: 2018-10-19 01:48:05+00:00
- **Authors**: Yaming Wang, Xiao Tan, Yi Yang, Ziyu Li, Xiao Liu, Feng Zhou, Larry S. Davis
- **Comment**: arXiv admin note: text overlap with arXiv:1806.04314
- **Journal**: None
- **Summary**: Existing 3D pose datasets of object categories are limited to generic object types and lack of fine-grained information. In this work, we introduce a new large-scale dataset that consists of 409 fine-grained categories and 31,881 images with accurate 3D pose annotation. Specifically, we augment three existing fine-grained object recognition datasets (StanfordCars, CompCars and FGVC-Aircraft) by finding a specific 3D model for each sub-category from ShapeNet and manually annotating each 2D image by adjusting a full set of 7 continuous perspective parameters. Since the fine-grained shapes allow 3D models to better fit the images, we further improve the annotation quality by initializing from the human annotation and conducting local search of the pose parameters with the objective of maximizing the IoUs between the projected mask and the segmentation reference estimated from state-of-the-art deep Convolutional Neural Networks (CNNs). We provide full statistics of the annotations with qualitative and quantitative comparisons suggesting that our dataset can be a complementary source for studying 3D pose estimation. The dataset can be downloaded at http://users.umiacs.umd.edu/~wym/3dpose.html.



### Zero and Few Shot Learning with Semantic Feature Synthesis and Competitive Learning
- **Arxiv ID**: http://arxiv.org/abs/1810.08332v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1810.08332v1)
- **Published**: 2018-10-19 01:52:03+00:00
- **Updated**: 2018-10-19 01:52:03+00:00
- **Authors**: Zhiwu Lu, Jiechao Guan, Aoxue Li, Tao Xiang, An Zhao, Ji-Rong Wen
- **Comment**: Submitted to IEEE TPAMI
- **Journal**: None
- **Summary**: Zero-shot learning (ZSL) is made possible by learning a projection function between a feature space and a semantic space (e.g.,~an attribute space). Key to ZSL is thus to learn a projection that is robust against the often large domain gap between the seen and unseen class domains. In this work, this is achieved by unseen class data synthesis and robust projection function learning. Specifically, a novel semantic data synthesis strategy is proposed, by which semantic class prototypes (e.g., attribute vectors) are used to simply perturb seen class data for generating unseen class ones. As in any data synthesis/hallucination approach, there are ambiguities and uncertainties on how well the synthesised data can capture the targeted unseen class data distribution. To cope with this, the second contribution of this work is a novel projection learning model termed competitive bidirectional projection learning (BPL) designed to best utilise the ambiguous synthesised data. Specifically, we assume that each synthesised data point can belong to any unseen class; and the most likely two class candidates are exploited to learn a robust projection function in a competitive fashion. As a third contribution, we show that the proposed ZSL model can be easily extended to few-shot learning (FSL) by again exploiting semantic (class prototype guided) feature synthesis and competitive BPL. Extensive experiments show that our model achieves the state-of-the-art results on both problems.



### Multi-Domain Pose Network for Multi-Person Pose Estimation and Tracking
- **Arxiv ID**: http://arxiv.org/abs/1810.08338v1
- **DOI**: 10.1007/978-3-030-11012-3_17
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.08338v1)
- **Published**: 2018-10-19 02:51:14+00:00
- **Updated**: 2018-10-19 02:51:14+00:00
- **Authors**: Hengkai Guo, Tang Tang, Guozhong Luo, Riwei Chen, Yongchen Lu, Linfu Wen
- **Comment**: Extended abstract for the ECCV 2018 PoseTrack Workshop
- **Journal**: None
- **Summary**: Multi-person human pose estimation and tracking in the wild is important and challenging. For training a powerful model, large-scale training data are crucial. While there are several datasets for human pose estimation, the best practice for training on multi-dataset has not been investigated. In this paper, we present a simple network called Multi-Domain Pose Network (MDPN) to address this problem. By treating the task as multi-domain learning, our methods can learn a better representation for pose prediction. Together with prediction heads fine-tuning and multi-branch combination, it shows significant improvement over baselines and achieves the best performance on PoseTrack ECCV 2018 Challenge without additional datasets other than MPII and COCO.



### Super-pixel cloud detection using Hierarchical Fusion CNN
- **Arxiv ID**: http://arxiv.org/abs/1810.08352v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.08352v1)
- **Published**: 2018-10-19 04:37:46+00:00
- **Updated**: 2018-10-19 04:37:46+00:00
- **Authors**: Han Liu, Dan Zeng, Qi Tian
- **Comment**: None
- **Journal**: None
- **Summary**: Cloud detection plays a very important role in the process of remote sensing images. This paper designs a super-pixel level cloud detection method based on convolutional neural network (CNN) and deep forest. Firstly, remote sensing images are segmented into super-pixels through the combination of SLIC and SEEDS. Structured forests is carried out to compute edge probability of each pixel, based on which super-pixels are segmented more precisely. Segmented super-pixels compose a super-pixel level remote sensing database. Though cloud detection is essentially a binary classification problem, our database is labeled into four categories: thick cloud, cirrus cloud, building and other culture, to improve the generalization ability of our proposed models. Secondly, super-pixel level database is used to train our cloud detection models based on CNN and deep forest. Considering super-pixel level remote sensing images contain less semantic information compared with general object classification database, we propose a Hierarchical Fusion CNN (HFCNN). It takes full advantage of low-level features like color and texture information and is more applicable to cloud detection task. In test phase, every super-pixel in remote sensing images is classified by our proposed models and then combined to recover final binary mask by our proposed distance metric, which is used to determine ambiguous super-pixels. Experimental results show that, compared with conventional methods, HFCNN can achieve better precision and recall.



### Temporal Action Detection by Joint Identification-Verification
- **Arxiv ID**: http://arxiv.org/abs/1810.08375v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.08375v1)
- **Published**: 2018-10-19 07:22:40+00:00
- **Updated**: 2018-10-19 07:22:40+00:00
- **Authors**: Wen Wang, Yongjian Wu, Haijun Liu, Shiguang Wang, Jian Cheng
- **Comment**: None
- **Journal**: None
- **Summary**: Temporal action detection aims at not only recognizing action category but also detecting start time and end time for each action instance in an untrimmed video. The key challenge of this task is to accurately classify the action and determine the temporal boundaries of each action instance. In temporal action detection benchmark: THUMOS 2014, large variations exist in the same action category while many similarities exist in different action categories, which always limit the performance of temporal action detection. To address this problem, we propose to use joint Identification-Verification network to reduce the intra-action variations and enlarge inter-action differences. The joint Identification-Verification network is a siamese network based on 3D ConvNets, which can simultaneously predict the action categories and the similarity scores for the input pairs of video proposal segments. Extensive experimental results on the challenging THUMOS 2014 dataset demonstrate the effectiveness of our proposed method compared to the existing state-of-art methods for temporal action detection in untrimmed videos.



### Saliency guided deep network for weakly-supervised image segmentation
- **Arxiv ID**: http://arxiv.org/abs/1810.08378v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.08378v1)
- **Published**: 2018-10-19 07:35:40+00:00
- **Updated**: 2018-10-19 07:35:40+00:00
- **Authors**: Fengdong Sun, Wenhui Li
- **Comment**: None
- **Journal**: None
- **Summary**: Weakly-supervised image segmentation is an important task in computer vision. A key problem is how to obtain high quality objects location from image-level category. Classification activation mapping is a common method which can be used to generate high-precise object location cues. However these location cues are generally very sparse and small such that they can not provide effective information for image segmentation. In this paper, we propose a saliency guided image segmentation network to resolve this problem. We employ a self-attention saliency method to generate subtle saliency maps, and render the location cues grow as seeds by seeded region growing method to expand pixel-level labels extent. In the process of seeds growing, we use the saliency values to weight the similarity between pixels to control the growing. Therefore saliency information could help generate discriminative object regions, and the effects of wrong salient pixels can be suppressed efficiently. Experimental results on a common segmentation dataset PASCAL VOC2012 demonstrate the effectiveness of our method.



### DGC-Net: Dense Geometric Correspondence Network
- **Arxiv ID**: http://arxiv.org/abs/1810.08393v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.08393v2)
- **Published**: 2018-10-19 08:28:59+00:00
- **Updated**: 2018-10-22 14:54:57+00:00
- **Authors**: Iaroslav Melekhov, Aleksei Tiulpin, Torsten Sattler, Marc Pollefeys, Esa Rahtu, Juho Kannala
- **Comment**: Supplementary material included; Affiliation section has been changed
- **Journal**: None
- **Summary**: This paper addresses the challenge of dense pixel correspondence estimation between two images. This problem is closely related to optical flow estimation task where ConvNets (CNNs) have recently achieved significant progress. While optical flow methods produce very accurate results for the small pixel translation and limited appearance variation scenarios, they hardly deal with the strong geometric transformations that we consider in this work. In this paper, we propose a coarse-to-fine CNN-based framework that can leverage the advantages of optical flow approaches and extend them to the case of large transformations providing dense and subpixel accurate estimates. It is trained on synthetic transformations and demonstrates very good performance to unseen, realistic, data. Further, we apply our method to the problem of relative camera pose estimation and demonstrate that the model outperforms existing dense approaches.



### CVABS: Moving Object Segmentation with Common Vector Approach for Videos
- **Arxiv ID**: http://arxiv.org/abs/1810.08412v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.08412v1)
- **Published**: 2018-10-19 09:10:42+00:00
- **Updated**: 2018-10-19 09:10:42+00:00
- **Authors**: Şahin Işık, Kemal Özkan, Ömer Nezih Gerek
- **Comment**: 12 Pages, 4 Figures, 1 Table
- **Journal**: None
- **Summary**: Background modelling is a fundamental step for several real-time computer vision applications that requires security systems and monitoring. An accurate background model helps detecting activity of moving objects in the video. In this work, we have developed a new subspace based background modelling algorithm using the concept of Common Vector Approach with Gram-Schmidt orthogonalization. Once the background model that involves the common characteristic of different views corresponding to the same scene is acquired, a smart foreground detection and background updating procedure is applied based on dynamic control parameters. A variety of experiments is conducted on different problem types related to dynamic backgrounds. Several types of metrics are utilized as objective measures and the obtained visual results are judged subjectively. It was observed that the proposed method stands successfully for all problem types reported on CDNet2014 dataset by updating the background frames with a self-learning feedback mechanism.



### ScratchDet: Training Single-Shot Object Detectors from Scratch
- **Arxiv ID**: http://arxiv.org/abs/1810.08425v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.08425v4)
- **Published**: 2018-10-19 09:57:50+00:00
- **Updated**: 2019-05-06 03:33:25+00:00
- **Authors**: Rui Zhu, Shifeng Zhang, Xiaobo Wang, Longyin Wen, Hailin Shi, Liefeng Bo, Tao Mei
- **Comment**: CVPR2019 Oral Presentation. Camera Ready Version
- **Journal**: None
- **Summary**: Current state-of-the-art object objectors are fine-tuned from the off-the-shelf networks pretrained on large-scale classification dataset ImageNet, which incurs some additional problems: 1) The classification and detection have different degrees of sensitivity to translation, resulting in the learning objective bias; 2) The architecture is limited by the classification network, leading to the inconvenience of modification. To cope with these problems, training detectors from scratch is a feasible solution. However, the detectors trained from scratch generally perform worse than the pretrained ones, even suffer from the convergence issue in training. In this paper, we explore to train object detectors from scratch robustly. By analysing the previous work on optimization landscape, we find that one of the overlooked points in current trained-from-scratch detector is the BatchNorm. Resorting to the stable and predictable gradient brought by BatchNorm, detectors can be trained from scratch stably while keeping the favourable performance independent to the network architecture. Taking this advantage, we are able to explore various types of networks for object detection, without suffering from the poor convergence. By extensive experiments and analyses on downsampling factor, we propose the Root-ResNet backbone network, which makes full use of the information from original images. Our ScratchDet achieves the state-of-the-art accuracy on PASCAL VOC 2007, 2012 and MS COCO among all the train-from-scratch detectors and even performs better than several one-stage pretrained methods. Codes will be made publicly available at https://github.com/KimSoybean/ScratchDet.



### Fast Graph-Cut Based Optimization for Practical Dense Deformable Registration of Volume Images
- **Arxiv ID**: http://arxiv.org/abs/1810.08427v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.08427v1)
- **Published**: 2018-10-19 10:09:54+00:00
- **Updated**: 2018-10-19 10:09:54+00:00
- **Authors**: Simon Ekström, Filip Malmberg, Håkan Ahlström, Joel Kullberg, Robin Strand
- **Comment**: None
- **Journal**: None
- **Summary**: Objective: Deformable image registration is a fundamental problem in medical image analysis, with applications such as longitudinal studies, population modeling, and atlas based image segmentation. Registration is often phrased as an optimization problem, i.e., finding a deformation field that is optimal according to a given objective function. Discrete, combinatorial, optimization techniques have successfully been employed to solve the resulting optimization problem. Specifically, optimization based on $\alpha$-expansion with minimal graph cuts has been proposed as a powerful tool for image registration. The high computational cost of the graph-cut based optimization approach, however, limits the utility of this approach for registration of large volume images. Methods: Here, we propose to accelerate graph-cut based deformable registration by dividing the image into overlapping sub-regions and restricting the $\alpha$-expansion moves to a single sub-region at a time. Results: We demonstrate empirically that this approach can achieve a large reduction in computation time -- from days to minutes -- with only a small penalty in terms of solution quality. Conclusion: The reduction in computation time provided by the proposed method makes graph cut based deformable registration viable for large volume images. Significance: Graph cut based image registration has previously been shown to produce excellent results, but the high computational cost has hindered the adoption of the method for registration of large medical volume images. Our proposed method lifts this restriction, requiring only a small fraction of the computational cost to produce results of comparable quality.



### Learning with privileged information via adversarial discriminative modality distillation
- **Arxiv ID**: http://arxiv.org/abs/1810.08437v2
- **DOI**: 10.1109/TPAMI.2019.2929038
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.08437v2)
- **Published**: 2018-10-19 10:49:11+00:00
- **Updated**: 2019-07-26 13:03:29+00:00
- **Authors**: Nuno C. Garcia, Pietro Morerio, Vittorio Murino
- **Comment**: Accepted to IEEE Transactions on Pattern Analysis and Machine
  Intelligence
- **Journal**: None
- **Summary**: Heterogeneous data modalities can provide complementary cues for several tasks, usually leading to more robust algorithms and better performance. However, while training data can be accurately collected to include a variety of sensory modalities, it is often the case that not all of them are available in real life (testing) scenarios, where a model has to be deployed. This raises the challenge of how to extract information from multimodal data in the training stage, in a form that can be exploited at test time, considering limitations such as noisy or missing modalities. This paper presents a new approach in this direction for RGB-D vision tasks, developed within the adversarial learning and privileged information frameworks. We consider the practical case of learning representations from depth and RGB videos, while relying only on RGB data at test time. We propose a new approach to train a hallucination network that learns to distill depth information via adversarial learning, resulting in a clean approach without several losses to balance or hyperparameters. We report state-of-the-art results on object classification on the NYUD dataset and video action recognition on the largest multimodal dataset available for this task, the NTU RGB+D, as well as on the Northwestern-UCLA.



### Multitask Learning for Large-scale Semantic Change Detection
- **Arxiv ID**: http://arxiv.org/abs/1810.08452v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1810.08452v2)
- **Published**: 2018-10-19 12:01:51+00:00
- **Updated**: 2019-08-28 15:29:38+00:00
- **Authors**: Rodrigo Caye Daudt, Bertrand Le Saux, Alexandre Boulch, Yann Gousseau
- **Comment**: Preprint submitted to Computer Vision and Image Understanding
- **Journal**: None
- **Summary**: Change detection is one of the main problems in remote sensing, and is essential to the accurate processing and understanding of the large scale Earth observation data available through programs such as Sentinel and Landsat. Most of the recently proposed change detection methods bring deep learning to this context, but openly available change detection datasets are still very scarce, which limits the methods that can be proposed and tested. In this paper we present the first large scale high resolution semantic change detection (HRSCD) dataset, which enables the usage of deep learning methods for semantic change detection. The dataset contains coregistered RGB image pairs, pixel-wise change information and land cover information. We then propose several methods using fully convolutional neural networks to perform semantic change detection. Most notably, we present a network architecture that performs change detection and land cover mapping simultaneously, while using the predicted land cover information to help to predict changes. We also describe a sequential training scheme that allows this network to be trained without setting a hyperparameter that balances different loss functions and achieves the best overall results.



### Fully Convolutional Siamese Networks for Change Detection
- **Arxiv ID**: http://arxiv.org/abs/1810.08462v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1810.08462v1)
- **Published**: 2018-10-19 12:27:49+00:00
- **Updated**: 2018-10-19 12:27:49+00:00
- **Authors**: Rodrigo Caye Daudt, Bertrand Le Saux, Alexandre Boulch
- **Comment**: To appear inProc. ICIP 2018, October 07-10, 2018, Athens, Greece
- **Journal**: None
- **Summary**: This paper presents three fully convolutional neural network architectures which perform change detection using a pair of coregistered images. Most notably, we propose two Siamese extensions of fully convolutional networks which use heuristics about the current problem to achieve the best results in our tests on two open change detection datasets, using both RGB and multispectral images. We show that our system is able to learn from scratch using annotated change detection images. Our architectures achieve better performance than previously proposed methods, while being at least 500 times faster than related systems. This work is a step towards efficient processing of data from large scale Earth observation systems such as Copernicus or Landsat.



### Urban Change Detection for Multispectral Earth Observation Using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1810.08468v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1810.08468v1)
- **Published**: 2018-10-19 12:40:28+00:00
- **Updated**: 2018-10-19 12:40:28+00:00
- **Authors**: Rodrigo Caye Daudt, Bertrand Le Saux, Alexandre Boulch, Yann Gousseau
- **Comment**: To appear inProc. IGARSS 2018, July 22-27, 2018, Valencia, Spain
- **Journal**: None
- **Summary**: The Copernicus Sentinel-2 program now provides multispectral images at a global scale with a high revisit rate. In this paper we explore the usage of convolutional neural networks for urban change detection using such multispectral images. We first present the new change detection dataset that was used for training the proposed networks, which will be openly available to serve as a benchmark. The Onera Satellite Change Detection (OSCD) dataset is composed of pairs of multispectral aerial images, and the changes were manually annotated at pixel level. We then propose two architectures to detect changes, Siamese and Early Fusion, and compare the impact of using different numbers of spectral channels as inputs. These architectures are trained from scratch using the provided dataset.



### Improving Fast Segmentation With Teacher-student Learning
- **Arxiv ID**: http://arxiv.org/abs/1810.08476v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.08476v1)
- **Published**: 2018-10-19 12:59:32+00:00
- **Updated**: 2018-10-19 12:59:32+00:00
- **Authors**: Jiafeng Xie, Bing Shuai, Jian-Fang Hu, Jingyang Lin, Wei-Shi Zheng
- **Comment**: 13 pages, 3 figures, conference
- **Journal**: BMVC 2018
- **Summary**: Recently, segmentation neural networks have been significantly improved by demonstrating very promising accuracies on public benchmarks. However, these models are very heavy and generally suffer from low inference speed, which limits their application scenarios in practice. Meanwhile, existing fast segmentation models usually fail to obtain satisfactory segmentation accuracies on public benchmarks. In this paper, we propose a teacher-student learning framework that transfers the knowledge gained by a heavy and better performed segmentation network (i.e. teacher) to guide the learning of fast segmentation networks (i.e. student). Specifically, both zero-order and first-order knowledge depicted in the fine annotated images and unlabeled auxiliary data are transferred to regularize our student learning. The proposed method can improve existing fast segmentation models without incurring extra computational overhead, so it can still process images with the same fast speed. Extensive experiments on the Pascal Context, Cityscape and VOC 2012 datasets demonstrate that the proposed teacher-student learning framework is able to significantly boost the performance of student network.



### Hybrid deep neural networks for all-cause Mortality Prediction from LDCT Images
- **Arxiv ID**: http://arxiv.org/abs/1810.08503v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.08503v1)
- **Published**: 2018-10-19 13:48:58+00:00
- **Updated**: 2018-10-19 13:48:58+00:00
- **Authors**: Pingkun Yan, Hengtao Guo, Ge Wang, Ruben De Man, Mannudeep K. Kalra
- **Comment**: IEEE conference format
- **Journal**: None
- **Summary**: Known for its high morbidity and mortality rates, lung cancer poses a significant threat to human health and well-being. However, the same population is also at high risk for other deadly diseases, such as cardiovascular disease. Since Low-Dose CT (LDCT) has been shown to significantly improve the lung cancer diagnosis accuracy, it will be very useful for clinical practice to predict the all-cause mortality for lung cancer patients to take corresponding actions. In this paper, we propose a deep learning based method, which takes both chest LDCT image patches and coronary artery calcification risk scores as input, for direct prediction of mortality risk of lung cancer subjects. The proposed method is called Hybrid Risk Network (HyRiskNet) for mortality risk prediction, which is an end-to-end framework utilizing hybrid imaging features, instead of completely relying on automatic feature extraction. Our work demonstrates the feasibility of using deep learning techniques for all-cause lung cancer mortality prediction from chest LDCT images. The experimental results show that the proposed HyRiskNet can achieve superior performance compared with the neural networks with only image input and with other traditional semi-automatic scoring methods. The study also indicates that radiologist defined features can well complement convolutional neural networks for more comprehensive feature extraction.



### Transfer Learning versus Multi-agent Learning regarding Distributed Decision-Making in Highway Traffic
- **Arxiv ID**: http://arxiv.org/abs/1810.08515v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.08515v1)
- **Published**: 2018-10-19 14:16:25+00:00
- **Updated**: 2018-10-19 14:16:25+00:00
- **Authors**: Mark Schutera, Niklas Goby, Dirk Neumann, Markus Reischl
- **Comment**: Proc. of the 10th International Workshop on Agents in Traffic and
  Transportation (ATT 2018), co-located with ECAI/IJCAI, AAMAS and ICML 2018
  conferences (FAIM 2018)
- **Journal**: CEUR Workshop Proceedings 2018
- **Summary**: Transportation and traffic are currently undergoing a rapid increase in terms of both scale and complexity. At the same time, an increasing share of traffic participants are being transformed into agents driven or supported by artificial intelligence resulting in mixed-intelligence traffic. This work explores the implications of distributed decision-making in mixed-intelligence traffic. The investigations are carried out on the basis of an online-simulated highway scenario, namely the MIT \emph{DeepTraffic} simulation. In the first step traffic agents are trained by means of a deep reinforcement learning approach, being deployed inside an elitist evolutionary algorithm for hyperparameter search. The resulting architectures and training parameters are then utilized in order to either train a single autonomous traffic agent and transfer the learned weights onto a multi-agent scenario or else to conduct multi-agent learning directly. Both learning strategies are evaluated on different ratios of mixed-intelligence traffic. The strategies are assessed according to the average speed of all agents driven by artificial intelligence. Traffic patterns that provoke a reduction in traffic flow are analyzed with respect to the different strategies.



### MsCGAN: Multi-scale Conditional Generative Adversarial Networks for Person Image Generation
- **Arxiv ID**: http://arxiv.org/abs/1810.08534v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.08534v3)
- **Published**: 2018-10-19 15:04:13+00:00
- **Updated**: 2020-03-05 16:19:24+00:00
- **Authors**: Wei Tang, Gui Li, Xinyuan Bao, Teng Li
- **Comment**: None
- **Journal**: None
- **Summary**: To synthesize high-quality person images with arbitrary poses is challenging. In this paper, we propose a novel Multi-scale Conditional Generative Adversarial Networks (MsCGAN), aiming to convert the input conditional person image to a synthetic image of any given target pose, whose appearance and the texture are consistent with the input image. MsCGAN is a multi-scale adversarial network consisting of two generators and two discriminators. One generator transforms the conditional person image into a coarse image of the target pose globally, and the other is to enhance the detailed quality of the synthetic person image through a local reinforcement network. The outputs of the two generators are then merged into a synthetic, discriminant and high-resolution image. On the other hand, the synthetic image is downsampled to multiple resolutions as the input to multi-scale discriminator networks. The proposed multi-scale generators and discriminators handling different levels of visual features can benefit to synthesizing high-resolution person images with realistic appearance and texture. Experiments are conducted on the Market-1501 and DeepFashion datasets to evaluate the proposed model, and both qualitative and quantitative results demonstrate the superior performance of the proposed MsCGAN.



### Deep Person Re-identification for Probabilistic Data Association in Multiple Pedestrian Tracking
- **Arxiv ID**: http://arxiv.org/abs/1810.08565v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.08565v1)
- **Published**: 2018-10-19 15:57:38+00:00
- **Updated**: 2018-10-19 15:57:38+00:00
- **Authors**: Brian H. Wang, Yan Wang, Kilian Q. Weinberger, Mark Campbell
- **Comment**: None
- **Journal**: None
- **Summary**: We present a data association method for vision-based multiple pedestrian tracking, using deep convolutional features to distinguish between different people based on their appearances. These re-identification (re-ID) features are learned such that they are invariant to transformations such as rotation, translation, and changes in the background, allowing consistent identification of a pedestrian moving through a scene. We incorporate re-ID features into a general data association likelihood model for multiple person tracking, experimentally validate this model by using it to perform tracking in two evaluation video sequences, and examine the performance improvements gained as compared to several baseline approaches. Our results demonstrate that using deep person re-ID for data association greatly improves tracking robustness to challenges such as occlusions and path crossings.



### Detecting cities in aerial night-time images by learning structural invariants using single reference augmentation
- **Arxiv ID**: http://arxiv.org/abs/1810.08597v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1810.08597v1)
- **Published**: 2018-10-19 17:32:07+00:00
- **Updated**: 2018-10-19 17:32:07+00:00
- **Authors**: Philipp Sadler
- **Comment**: Project in Image Classification, Winter 2018, Prof. Dr. Tatjana
  Scheffler
- **Journal**: None
- **Summary**: This paper examines, if it is possible to learn structural invariants of city images by using only a single reference picture when producing transformations along the variants in the dataset. Previous work explored the problem of learning from only a few examples and showed that data augmentation techniques benefit performance and generalization for machine learning approaches. First a principal component analysis in conjunction with a Fourier transform is trained on a single reference augmentation training dataset using the city images. Secondly a convolutional neural network is trained on a similar dataset with more samples. The findings are that the convolutional neural network is capable of finding images of the same category whereas the applied principal component analysis in conjunction with a Fourier transform failed to solve this task.



### Fast and Robust Multiple ColorChecker Detection using Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1810.08639v1
- **DOI**: 10.1016/j.imavis.2018.11.001
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.08639v1)
- **Published**: 2018-10-19 18:39:53+00:00
- **Updated**: 2018-10-19 18:39:53+00:00
- **Authors**: Pedro D. Marrero Fernandez, Fidel A. Guerrero-Peña, Tsang Ing Ren, Jorge J. G. Leandro
- **Comment**: Submitted to Image and Vision Computing
- **Journal**: None
- **Summary**: ColorCheckers are reference standards that professional photographers and filmmakers use to ensure predictable results under every lighting condition. The objective of this work is to propose a new fast and robust method for automatic ColorChecker detection. The process is divided into two steps: (1) ColorCheckers localization and (2) ColorChecker patches recognition. For the ColorChecker localization, we trained a detection convolutional neural network using synthetic images. The synthetic images are created with the 3D models of the ColorChecker and different background images. The output of the neural networks are the bounding box of each possible ColorChecker candidates in the input image. Each bounding box defines a cropped image which is evaluated by a recognition system, and each image is canonized with regards to color and dimensions. Subsequently, all possible color patches are extracted and grouped with respect to the center's distance. Each group is evaluated as a candidate for a ColorChecker part, and its position in the scene is estimated. Finally, a cost function is applied to evaluate the accuracy of the estimation. The method is tested using real and synthetic images. The proposed method is fast, robust to overlaps and invariant to affine projections. The algorithm also performs well in case of multiple ColorCheckers detection.



### Understanding Deep Convolutional Networks through Gestalt Theory
- **Arxiv ID**: http://arxiv.org/abs/1810.08697v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.08697v1)
- **Published**: 2018-10-19 21:48:51+00:00
- **Updated**: 2018-10-19 21:48:51+00:00
- **Authors**: Angelos Amanatiadis, Vasileios Kaburlasos, Elias Kosmatopoulos
- **Comment**: None
- **Journal**: None
- **Summary**: The superior performance of deep convolutional networks over high-dimensional problems have made them very popular for several applications. Despite their wide adoption, their underlying mechanisms still remain unclear with their improvement procedures still relying mainly on a trial and error process. We introduce a novel sensitivity analysis based on the Gestalt theory for giving insights into the classifier function and intermediate layers. Since Gestalt psychology stipulates that perception can be a product of complex interactions among several elements, we perform an ablation study based on this concept to discover which principles and image context significantly contribute in the network classification. Our results reveal that ConvNets follow most of the visual cortical perceptual mechanisms defined by the Gestalt principles at several levels. The proposed framework stimulates specific feature maps in classification problems and reveal important network attributes that can produce more explainable network models.



### Synscapes: A Photorealistic Synthetic Dataset for Street Scene Parsing
- **Arxiv ID**: http://arxiv.org/abs/1810.08705v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.08705v1)
- **Published**: 2018-10-19 22:30:44+00:00
- **Updated**: 2018-10-19 22:30:44+00:00
- **Authors**: Magnus Wrenninge, Jonas Unger
- **Comment**: For more information and download see: https://7dlabs.com
- **Journal**: None
- **Summary**: We introduce Synscapes -- a synthetic dataset for street scene parsing created using photorealistic rendering techniques, and show state-of-the-art results for training and validation as well as new types of analysis. We study the behavior of networks trained on real data when performing inference on synthetic data: a key factor in determining the equivalence of simulation environments. We also compare the behavior of networks trained on synthetic data and evaluated on real-world data. Additionally, by analyzing pre-trained, existing segmentation and detection models, we illustrate how uncorrelated images along with a detailed set of annotations open up new avenues for analysis of computer vision systems, providing fine-grain information about how a model's performance changes according to factors such as distance, occlusion and relative object orientation.



