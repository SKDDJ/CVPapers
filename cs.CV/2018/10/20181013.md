# Arxiv Papers in cs.CV on 2018-10-13
### CPNet: A Context Preserver Convolutional Neural Network for Detecting Shadows in Single RGB Images
- **Arxiv ID**: http://arxiv.org/abs/1810.05778v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.05778v1)
- **Published**: 2018-10-13 01:26:10+00:00
- **Updated**: 2018-10-13 01:26:10+00:00
- **Authors**: Sorour Mohajerani, Parvaneh Saeedi
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic detection of shadow regions in an image is a difficult task due to the lack of prior information about the illumination source and the dynamic of the scene objects. To address this problem, in this paper, a deep-learning based segmentation method is proposed that identifies shadow regions at the pixel-level in a single RGB image. We exploit a novel Convolutional Neural Network (CNN) architecture to identify and extract shadow features in an end-to-end manner. This network preserves learned contexts during the training and observes the entire image to detect global and local shadow patterns simultaneously. The proposed method is evaluated on two publicly available datasets of SBU and UCF. We have improved the state-of-the-art Balanced Error Rate (BER) on these datasets by 22\% and 14\%, respectively.



### Pose Estimation for Objects with Rotational Symmetry
- **Arxiv ID**: http://arxiv.org/abs/1810.05780v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.05780v1)
- **Published**: 2018-10-13 01:40:15+00:00
- **Updated**: 2018-10-13 01:40:15+00:00
- **Authors**: Enric Corona, Kaustav Kundu, Sanja Fidler
- **Comment**: Accepted at IROS 2018. More details available at
  http://www.cs.utoronto.ca/~ecorona/symmetry_pose_estimation
- **Journal**: None
- **Summary**: Pose estimation is a widely explored problem, enabling many robotic tasks such as grasping and manipulation. In this paper, we tackle the problem of pose estimation for objects that exhibit rotational symmetry, which are common in man-made and industrial environments. In particular, our aim is to infer poses for objects not seen at training time, but for which their 3D CAD models are available at test time. Previous work has tackled this problem by learning to compare captured views of real objects with the rendered views of their 3D CAD models, by embedding them in a joint latent space using neural networks. We show that sidestepping the issue of symmetry in this scenario during training leads to poor performance at test time. We propose a model that reasons about rotational symmetry during training by having access to only a small set of symmetry-labeled objects, whereby exploiting a large collection of unlabeled CAD models. We demonstrate that our approach significantly outperforms a naively trained neural network on a new pose dataset containing images of tools and hardware.



### Cloud Detection Algorithm for Remote Sensing Images Using Fully Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1810.05782v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.05782v1)
- **Published**: 2018-10-13 01:53:49+00:00
- **Updated**: 2018-10-13 01:53:49+00:00
- **Authors**: Sorour Mohajerani, Thomas A. Krammer, Parvaneh Saeedi
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a deep-learning based framework for addressing the problem of accurate cloud detection in remote sensing images. This framework benefits from a Fully Convolutional Neural Network (FCN), which is capable of pixel-level labeling of cloud regions in a Landsat 8 image. Also, a gradient-based identification approach is proposed to identify and exclude regions of snow/ice in the ground truths of the training set. We show that using the hybrid of the two methods (threshold-based and deep-learning) improves the performance of the cloud identification process without the need to manually correct automatically generated ground truths. In average the Jaccard index and recall measure are improved by 4.36% and 3.62%, respectively.



### Learning to Globally Edit Images with Textual Description
- **Arxiv ID**: http://arxiv.org/abs/1810.05786v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.05786v1)
- **Published**: 2018-10-13 02:14:15+00:00
- **Updated**: 2018-10-13 02:14:15+00:00
- **Authors**: Hai Wang, Jason D. Williams, SingBing Kang
- **Comment**: None
- **Journal**: None
- **Summary**: We show how we can globally edit images using textual instructions: given a source image and a textual instruction for the edit, generate a new image transformed under this instruction. To tackle this novel problem, we develop three different trainable models based on RNN and Generative Adversarial Network (GAN). The models (bucket, filter bank, and end-to-end) differ in how much expert knowledge is encoded, with the most general version being purely end-to-end. To train these systems, we use Amazon Mechanical Turk to collect textual descriptions for around 2000 image pairs sampled from several datasets. Experimental results evaluated on our dataset validate our approaches. In addition, given that the filter bank model is a good compromise between generality and performance, we investigate it further by replacing RNN with Graph RNN, and show that Graph RNN improves performance. To the best of our knowledge, this is the first computational photography work on global image editing that is purely based on free-form textual instructions.



### Deep learning based cloud detection for medium and high resolution remote sensing images of different sensors
- **Arxiv ID**: http://arxiv.org/abs/1810.05801v3
- **DOI**: 10.1016/j.isprsjprs.2019.02.017
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.05801v3)
- **Published**: 2018-10-13 05:58:47+00:00
- **Updated**: 2019-03-05 02:49:17+00:00
- **Authors**: Zhiwei Li, Huanfeng Shen, Qing Cheng, Yuhao Liu, Shucheng You, Zongyi He
- **Comment**: This manuscript has been accepted for publication in ISPRS Journal of
  Photogrammetry and Remote Sensing, vol. 150, pp.197-212, 2019.
  (https://doi.org/10.1016/j.isprsjprs.2019.02.017)
- **Journal**: ISPRS Journal of Photogrammetry and Remote Sensing, vol. 150,
  pp.197-212, 2019
- **Summary**: Cloud detection is an important preprocessing step for the precise application of optical satellite imagery. In this paper, we propose a deep learning based cloud detection method named multi-scale convolutional feature fusion (MSCFF) for remote sensing images of different sensors. In the network architecture of MSCFF, the symmetric encoder-decoder module, which provides both local and global context by densifying feature maps with trainable convolutional filter banks, is utilized to extract multi-scale and high-level spatial features. The feature maps of multiple scales are then up-sampled and concatenated, and a novel multi-scale feature fusion module is designed to fuse the features of different scales for the output. The two output feature maps of the network are cloud and cloud shadow maps, which are in turn fed to binary classifiers outside the model to obtain the final cloud and cloud shadow mask. The MSCFF method was validated on hundreds of globally distributed optical satellite images, with spatial resolutions ranging from 0.5 to 50 m, including Landsat-5/7/8, Gaofen-1/2/4, Sentinel-2, Ziyuan-3, CBERS-04, Huanjing-1, and collected high-resolution images exported from Google Earth. The experimental results show that MSCFF achieves a higher accuracy than the traditional rule-based cloud detection methods and the state-of-the-art deep learning models, especially in bright surface covered areas. The effectiveness of MSCFF means that it has great promise for the practical application of cloud detection for multiple types of medium and high-resolution remote sensing images. Our established global high-resolution cloud detection validation dataset has been made available online.



### Efficient Multi-level Correlating for Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/1810.05810v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1810.05810v1)
- **Published**: 2018-10-13 06:58:46+00:00
- **Updated**: 2018-10-13 06:58:46+00:00
- **Authors**: Yipeng Ma, Chun Yuan, Peng Gao, Fei Wang
- **Comment**: Accepted by ACCV'2018
- **Journal**: None
- **Summary**: Correlation filter (CF) based tracking algorithms have demonstrated favorable performance recently. Nevertheless, the top performance trackers always employ complicated optimization methods which constraint their real-time applications. How to accelerate the tracking speed while retaining the tracking accuracy is a significant issue. In this paper, we propose a multi-level CF-based tracking approach named MLCFT which further explores the potential capacity of CF with two-stage detection: primal detection and oriented re-detection. The cascaded detection scheme is simple but competent to prevent model drift and accelerate the speed. An effective fusion method based on relative entropy is introduced to combine the complementary features extracted from deep and shallow layers of convolutional neural networks (CNN). Moreover, a novel online model update strategy is utilized in our tracker, which enhances the tracking performance further. Experimental results demonstrate that our proposed approach outperforms the most state-of-the-art trackers while tracking at speed of exceeded 16 frames per second on challenging benchmarks.



### Characterising epithelial tissues using persistent entropy
- **Arxiv ID**: http://arxiv.org/abs/1810.05835v1
- **DOI**: 10.1007/978-3-030-10828-1_14
- **Categories**: **eess.IV**, cs.CV, q-bio.QM, 68T10, 92B99, 65D18 94A17, 55N99, 5504
- **Links**: [PDF](http://arxiv.org/pdf/1810.05835v1)
- **Published**: 2018-10-13 09:43:14+00:00
- **Updated**: 2018-10-13 09:43:14+00:00
- **Authors**: N. Atienza, L. M. Escudero, M. J. Jimenez, M. Soriano-Trigueros
- **Comment**: 12 pages, 7 figures, 4 tables
- **Journal**: None
- **Summary**: In this paper, we apply persistent entropy, a novel topological statistic, for characterization of images of epithelial tissues. We have found out that persistent entropy is able to summarize topological and geometric information encoded by \alpha-complexes and persistent homology. After using some statistical tests, we can guarantee the existence of significant differences in the studied tissues.



### Exploiting Semantics in Adversarial Training for Image-Level Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1810.05852v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.05852v1)
- **Published**: 2018-10-13 12:20:44+00:00
- **Updated**: 2018-10-13 12:20:44+00:00
- **Authors**: Pierluigi Zama Ramirez, Alessio Tonioni, Luigi Di Stefano
- **Comment**: 6 pages, Accepted to IPAS 2018
- **Journal**: None
- **Summary**: Performance achievable by modern deep learning approaches are directly related to the amount of data used at training time. Unfortunately, the annotation process is notoriously tedious and expensive, especially for pixel-wise tasks like semantic segmentation. Recent works have proposed to rely on synthetically generated imagery to ease the training set creation. However, models trained on these kind of data usually under-perform on real images due to the well known issue of domain shift. We address this problem by learning a domain-to-domain image translation GAN to shrink the gap between real and synthetic images. Peculiarly to our method, we introduce semantic constraints into the generation process to both avoid artifacts and guide the synthesis. To prove the effectiveness of our proposal, we show how a semantic segmentation CNN trained on images from the synthetic GTA dataset adapted by our method can improve performance by more than 16% mIoU with respect to the same model trained on synthetic images.



### Equivalent Constraints for Two-View Geometry: Pose Solution/Pure Rotation Identification and 3D Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1810.05863v1
- **DOI**: 10.1007/s11263-018-1136-9
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1810.05863v1)
- **Published**: 2018-10-13 13:51:15+00:00
- **Updated**: 2018-10-13 13:51:15+00:00
- **Authors**: Qi Cai, Yuanxin Wu, Lilian Zhang, Peike Zhang
- **Comment**: 15 pages, 13 figures
- **Journal**: International Journal of Computer Vision, 2019
- **Summary**: Two-view relative pose estimation and structure reconstruction is a classical problem in computer vision. The typical methods usually employ the singular value decomposition of the essential matrix to get multiple solutions of the relative pose, from which the right solution is picked out by reconstructing the three-dimension (3D) feature points and imposing the constraint of positive depth. This paper revisits the two-view geometry problem and discovers that the two-view imaging geometry is equivalently governed by a Pair of new Pose-Only (PPO) constraints: the same-side constraint and the intersection constraint. From the perspective of solving equation, the complete pose solutions of the essential matrix are explicitly derived and we rigorously prove that the orientation part of the pose can still be recovered in the case of pure rotation. The PPO constraints are simplified and formulated in the form of inequalities to directly identify the right pose solution with no need of 3D reconstruction and the 3D reconstruction can be analytically achieved from the identified right pose. Furthermore, the intersection inequality also enables a robust criterion for pure rotation identification. Experiment results validate the correctness of analyses and the robustness of the derived pose solution/pure rotation identification and analytical 3D reconstruction.



### Attention Driven Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1810.05866v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.05866v1)
- **Published**: 2018-10-13 14:25:47+00:00
- **Updated**: 2018-10-13 14:25:47+00:00
- **Authors**: Fan Yang, Ke Yan, Shijian Lu, Huizhu Jia, Xiaodong Xie, Wen Gao
- **Comment**: Accepted in the Pattern Recognition (PR)
- **Journal**: None
- **Summary**: Person re-identification (ReID) is a challenging task due to arbitrary human pose variations, background clutters, etc. It has been studied extensively in recent years, but the multifarious local and global features are still not fully exploited by either ignoring the interplay between whole-body images and body-part images or missing in-depth examination of specific body-part images. In this paper, we propose a novel attention-driven multi-branch network that learns robust and discriminative human representation from global whole-body images and local body-part images simultaneously. Within each branch, an intra-attention network is designed to search for informative and discriminative regions within the whole-body or body-part images, where attention is elegantly decomposed into spatial-wise attention and channel-wise attention for effective and efficient learning. In addition, a novel inter-attention module is designed which fuses the output of intra-attention networks adaptively for optimal person ReID. The proposed technique has been evaluated over three widely used datasets CUHK03, Market-1501 and DukeMTMC-ReID, and experiments demonstrate its superior robustness and effectiveness as compared with the state of the arts.



### Embedded deep learning in ophthalmology: Making ophthalmic imaging smarter
- **Arxiv ID**: http://arxiv.org/abs/1810.05874v2
- **DOI**: 10.1177/2515841419827172
- **Categories**: **cs.CV**, 68T45, I.2.10; I.2.11; I.4.5; I.4.9; J.3
- **Links**: [PDF](http://arxiv.org/pdf/1810.05874v2)
- **Published**: 2018-10-13 15:20:32+00:00
- **Updated**: 2019-02-21 09:17:47+00:00
- **Authors**: Petteri Teikari, Raymond P. Najjar, Leopold Schmetterer, Dan Milea
- **Comment**: This work has been submitted to "Therapeutic Advances in
  Ophthalmology" for possible publication 17 pages, 5 figures
- **Journal**: Therapeutic advances in ophthalmology 11 (2019): 2515841419827172
- **Summary**: Deep learning has recently gained high interest in ophthalmology, due to its ability to detect clinically significant features for diagnosis and prognosis. Despite these significant advances, little is known about the ability of various deep learning systems to be embedded within ophthalmic imaging devices, allowing automated image acquisition. In this work, we will review the existing and future directions for "active acquisition" embedded deep learning, leading to as high quality images with little intervention by the human operator. In clinical practice, the improved image quality should translate into more robust deep learning-based clinical diagnostics. Embedded deep learning will be enabled by the constantly improving hardware performance with low cost. We will briefly review possible computation methods in larger clinical systems. Briefly, they can be included in a three-layer framework composed of edge, fog and cloud layers, the former being performed at a device-level. Improved edge layer performance via "active acquisition" serves as an automatic data curation operator translating to better quality data in electronic health records (EHRs), as well as on the cloud layer, for improved deep learning-based clinical data mining.



### Incremental Deep Learning for Robust Object Detection in Unknown Cluttered Environments
- **Arxiv ID**: http://arxiv.org/abs/1810.10323v1
- **DOI**: 10.1109/ACCESS.2018.2875720
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.10323v1)
- **Published**: 2018-10-13 17:10:41+00:00
- **Updated**: 2018-10-13 17:10:41+00:00
- **Authors**: Dong Kyun Shin, Minhaz Uddin Ahmed, Phill Kyu Rhee
- **Comment**: 14 pages, 17 figures
- **Journal**: None
- **Summary**: Object detection in streaming images is a major step in different detection-based applications, such as object tracking, action recognition, robot navigation, and visual surveillance applications. In mostcases, image quality is noisy and biased, and as a result, the data distributions are disturbed and imbalanced. Most object detection approaches, such as the faster region-based convolutional neural network (Faster RCNN), Single Shot Multibox Detector with 300x300 inputs (SSD300), and You Only Look Once version 2 (YOLOv2), rely on simple sampling without considering distortions and noise under real-world changing environments, despite poor object labeling. In this paper, we propose an Incremental active semi-supervised learning (IASSL) technology for unseen object detection. It combines batch-based active learning (AL) and bin-based semi-supervised learning (SSL) to leverage the strong points of AL's exploration and SSL's exploitation capabilities. A collaborative sampling method is also adopted to measure the uncertainty and diversity of AL and the confidence in SSL. Batch-based AL allows us to select more informative, confident, and representative samples with low cost. Bin-based SSL divides streaming image samples into several bins, and each bin repeatedly transfers the discriminative knowledge of convolutional neural network (CNN) deep learning to the next bin until the performance criterion is reached. IASSL can overcome noisy and biased labels in unknown, cluttered data distributions. We obtain superior performance, compared to state-of-the-art technologies such as Faster RCNN, SSD300, and YOLOv2.



### Multi-scale Geometric Summaries for Similarity-based Sensor Fusion
- **Arxiv ID**: http://arxiv.org/abs/1810.10324v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, 65T60, H.5.5; H.5.1; I.5
- **Links**: [PDF](http://arxiv.org/pdf/1810.10324v2)
- **Published**: 2018-10-13 18:52:07+00:00
- **Updated**: 2019-01-05 01:27:01+00:00
- **Authors**: Christopher J. Tralie, Paul Bendich, John Harer
- **Comment**: 9 pages, 13 Figures
- **Journal**: None
- **Summary**: In this work, we address fusion of heterogeneous sensor data using wavelet-based summaries of fused self-similarity information from each sensor. The technique we develop is quite general, does not require domain specific knowledge or physical models, and requires no training. Nonetheless, it can perform surprisingly well at the general task of differentiating classes of time-ordered behavior sequences which are sensed by more than one modality. As a demonstration of our capabilities in the audio to video context, we focus on the differentiation of speech sequences.   Data from two or more modalities first are represented using self-similarity matrices(SSMs) corresponding to time-ordered point clouds in feature spaces of each of these data sources; we note that these feature spaces can be of entirely different scale and dimensionality.   A fused similarity template is then derived from the modality-specific SSMs using a technique called similarity network fusion (SNF). We investigate pipelines using SNF as both an upstream (feature-level) and a downstream (ranking-level) fusion technique. Multiscale geometric features of this template are then extracted using a recently-developed technique called the scattering transform, and these features are then used to differentiate speech sequences. This method outperforms unsupervised techniques which operate directly on the raw data, and it also outperforms stovepiped methods which operate on SSMs separately derived from the distinct modalities. The benefits of this method become even more apparent as the simulated peak signal to noise ratio decreases.



### No-reference Image Denoising Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/1810.05919v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1810.05919v1)
- **Published**: 2018-10-13 19:47:09+00:00
- **Updated**: 2018-10-13 19:47:09+00:00
- **Authors**: Si Lu
- **Comment**: 17 pages, 41 figures, accepted by Computer Vision Conference (CVC)
  2019
- **Journal**: None
- **Summary**: A wide variety of image denoising methods are available now. However, the performance of a denoising algorithm often depends on individual input noisy images as well as its parameter setting. In this paper, we present a no-reference image denoising quality assessment method that can be used to select for an input noisy image the right denoising algorithm with the optimal parameter setting. This is a challenging task as no ground truth is available. This paper presents a data-driven approach to learn to predict image denoising quality. Our method is based on the observation that while individual existing quality metrics and denoising models alone cannot robustly rank denoising results, they often complement each other. We accordingly design denoising quality features based on these existing metrics and models and then use Random Forests Regression to aggregate them into a more powerful unified metric. Our experiments on images with various types and levels of noise show that our no-reference denoising quality assessment method significantly outperforms the state-of-the-art quality metrics. This paper also provides a method that leverages our quality assessment method to automatically tune the parameter settings of a denoising algorithm for an input noisy image to produce an optimal denoising result.



### Porosity Amount Estimation in Stones Based on Combination of One Dimensional Local Binary Patterns and Image Normalization Technique
- **Arxiv ID**: http://arxiv.org/abs/1810.05922v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1810.05922v1)
- **Published**: 2018-10-13 20:14:00+00:00
- **Updated**: 2018-10-13 20:14:00+00:00
- **Authors**: Shervan Fekri-Ershad
- **Comment**: 16 pages, in Farsi. 9 Figures, Computing Science Journal, Vol. 9,
  2018
- **Journal**: None
- **Summary**: Since now, many approaches has been proposed for surface defect detection based on image texture analysis techniques. One of the efficient texture analysis operations is local binary patterns which provides good accuracy.



### Varifocal-Net: A Chromosome Classification Approach using Deep Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1810.05943v4
- **DOI**: 10.1109/TMI.2019.2905841
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.05943v4)
- **Published**: 2018-10-13 23:39:39+00:00
- **Updated**: 2019-03-20 05:20:11+00:00
- **Authors**: Yulei Qin, Juan Wen, Hao Zheng, Xiaolin Huang, Jie Yang, Ning Song, Yue-Min Zhu, Lingqian Wu, Guang-Zhong Yang
- **Comment**: This paper is accepted to IEEE TMI for future publication. 13 pages,
  11 figures, 9 tables
- **Journal**: None
- **Summary**: Chromosome classification is critical for karyotyping in abnormality diagnosis. To expedite the diagnosis, we present a novel method named Varifocal-Net for simultaneous classification of chromosome's type and polarity using deep convolutional networks. The approach consists of one global-scale network (G-Net) and one local-scale network (L-Net). It follows three stages. The first stage is to learn both global and local features. We extract global features and detect finer local regions via the G-Net. By proposing a varifocal mechanism, we zoom into local parts and extract local features via the L-Net. Residual learning and multi-task learning strategies are utilized to promote high-level feature extraction. The detection of discriminative local parts is fulfilled by a localization subnet of the G-Net, whose training process involves both supervised and weakly-supervised learning. The second stage is to build two multi-layer perceptron classifiers that exploit features of both two scales to boost classification performance. The third stage is to introduce a dispatch strategy of assigning each chromosome to a type within each patient case, by utilizing the domain knowledge of karyotyping. Evaluation results from 1909 karyotyping cases showed that the proposed Varifocal-Net achieved the highest accuracy per patient case (%) 99.2 for both type and polarity tasks. It outperformed state-of-the-art methods, demonstrating the effectiveness of our varifocal mechanism, multi-scale feature ensemble, and dispatch strategy. The proposed method has been applied to assist practical karyotype diagnosis.



