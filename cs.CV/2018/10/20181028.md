# Arxiv Papers in cs.CV on 2018-10-28
### Distilling Critical Paths in Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1811.02643v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.02643v2)
- **Published**: 2018-10-28 00:56:45+00:00
- **Updated**: 2018-11-08 06:41:04+00:00
- **Authors**: Fuxun Yu, Zhuwei Qin, Xiang Chen
- **Comment**: Accepted in NIPS18 CDNNRIA workshop
- **Journal**: None
- **Summary**: Neural network compression and acceleration are widely demanded currently due to the resource constraints on most deployment targets. In this paper, through analyzing the filter activation, gradients, and visualizing the filters' functionality in convolutional neural networks, we show that the filters in higher layers learn extremely task-specific features, which are exclusive for only a small subset of the overall tasks, or even a single class. Based on such findings, we reveal the critical paths of information flow for different classes. And by their intrinsic property of exclusiveness, we propose a critical path distillation method, which can effectively customize the convolutional neural networks to small ones with much smaller model size and less computation.



### Towards Human Pulse Rate Estimation from Face Video: Automatic Component Selection and Comparison of Blind Source Separation Methods
- **Arxiv ID**: http://arxiv.org/abs/1810.11770v1
- **DOI**: 10.1109/IS.2018.8710532
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.11770v1)
- **Published**: 2018-10-28 07:15:35+00:00
- **Updated**: 2018-10-28 07:15:35+00:00
- **Authors**: Vladislav Ostankovich, Geesara Prathap, Ilya Afanasyev
- **Comment**: None
- **Journal**: None
- **Summary**: Human heartbeat can be measured using several different ways appropriately based on the patient condition which includes contact base such as measured by using instruments and non-contact base such as computer vision assisted techniques. Non-contact based approached are getting popular due to those techniques are capable of mitigating some of the limitations of contact-based techniques especially in clinical section. However, existing vision guided approaches are not able to prove high accurate result due to various reason such as the property of camera, illumination changes, skin tones in face image, etc. We propose a technique that uses video as an input and returns pulse rate in output. Initially, key point detection is carried out on two facial subregions: forehead and nose-mouth. After removing unstable features, the temporal filtering is applied to isolate frequencies of interest. Then four component analysis methods are employed in order to distinguish the cardiovascular pulse signal from extraneous noise caused by respiration, vestibular activity and other changes in facial expression. Afterwards, proposed peak detection technique is applied for each component which extracted from one of the four different component selection algorithms. This will enable to locate the positions of peaks in each component. Proposed automatic components selection technique is employed in order to select an optimal component which will be used to calculate the heartbeat. Finally, we conclude with a comparison of four component analysis methods (PCA, FastICA, JADE, SHIBBS), processing face video datasets of fifteen volunteers with verification by an ECG/EKG Workstation as a ground truth.



### Deep Affinity Network for Multiple Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/1810.11780v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.11780v2)
- **Published**: 2018-10-28 09:07:40+00:00
- **Updated**: 2019-07-16 01:09:38+00:00
- **Authors**: ShiJie Sun, Naveed Akhtar, HuanSheng Song, Ajmal Mian, Mubarak Shah
- **Comment**: To appear in IEEE TPAMI
- **Journal**: None
- **Summary**: Multiple Object Tracking (MOT) plays an important role in solving many fundamental problems in video analysis in computer vision. Most MOT methods employ two steps: Object Detection and Data Association. The first step detects objects of interest in every frame of a video, and the second establishes correspondence between the detected objects in different frames to obtain their tracks. Object detection has made tremendous progress in the last few years due to deep learning. However, data association for tracking still relies on hand crafted constraints such as appearance, motion, spatial proximity, grouping etc. to compute affinities between the objects in different frames. In this paper, we harness the power of deep learning for data association in tracking by jointly modelling object appearances and their affinities between different frames in an end-to-end fashion. The proposed Deep Affinity Network (DAN) learns compact; yet comprehensive features of pre-detected objects at several levels of abstraction, and performs exhaustive pairing permutations of those features in any two frames to infer object affinities. DAN also accounts for multiple objects appearing and disappearing between video frames. We exploit the resulting efficient affinity computations to associate objects in the current frame deep into the previous frames for reliable on-line tracking. Our technique is evaluated on popular multiple object tracking challenges MOT15, MOT17 and UA-DETRAC. Comprehensive benchmarking under twelve evaluation metrics demonstrates that our approach is among the best performing techniques on the leader board for these challenges. The open source implementation of our work is available at https://github.com/shijieS/SST.git.



### Cascaded Pyramid Mining Network for Weakly Supervised Temporal Action Localization
- **Arxiv ID**: http://arxiv.org/abs/1810.11794v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.11794v1)
- **Published**: 2018-10-28 11:10:33+00:00
- **Updated**: 2018-10-28 11:10:33+00:00
- **Authors**: Haisheng Su, Xu Zhao, Tianwei Lin
- **Comment**: Accepted at ACCV 2018
- **Journal**: None
- **Summary**: Weakly supervised temporal action localization, which aims at temporally locating action instances in untrimmed videos using only video-level class labels during training, is an important yet challenging problem in video analysis. Many current methods adopt the "localization by classification" framework: first do video classification, then locate temporal area contributing to the results most. However, this framework fails to locate the entire action instances and gives little consideration to the local context. In this paper, we present a novel architecture called Cascaded Pyramid Mining Network (CPMN) to address these issues using two effective modules. First, to discover the entire temporal interval of specific action, we design a two-stage cascaded module with proposed Online Adversarial Erasing (OAE) mechanism, where new and complementary regions are mined through feeding the erased feature maps of discovered regions back to the system. Second, to exploit hierarchical contextual information in videos and reduce missing detections, we design a pyramid module which produces a scale-invariant attention map through combining the feature maps from different levels. Final, we aggregate the results of two modules to perform action localization via locating high score areas in temporal Class Activation Sequence (CAS). Extensive experiments conducted on THUMOS14 and ActivityNet-1.3 datasets demonstrate the effectiveness of our method.



### Discrimination-aware Channel Pruning for Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1810.11809v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.11809v3)
- **Published**: 2018-10-28 13:18:50+00:00
- **Updated**: 2019-01-14 01:44:08+00:00
- **Authors**: Zhuangwei Zhuang, Mingkui Tan, Bohan Zhuang, Jing Liu, Yong Guo, Qingyao Wu, Junzhou Huang, Jinhui Zhu
- **Comment**: NeurIPS 2018
- **Journal**: None
- **Summary**: Channel pruning is one of the predominant approaches for deep model compression. Existing pruning methods either train from scratch with sparsity constraints on channels, or minimize the reconstruction error between the pre-trained feature maps and the compressed ones. Both strategies suffer from some limitations: the former kind is computationally expensive and difficult to converge, whilst the latter kind optimizes the reconstruction error but ignores the discriminative power of channels. To overcome these drawbacks, we investigate a simple-yet-effective method, called discrimination-aware channel pruning, to choose those channels that really contribute to discriminative power. To this end, we introduce additional losses into the network to increase the discriminative power of intermediate layers and then select the most discriminative channels for each layer by considering the additional loss and the reconstruction error. Last, we propose a greedy algorithm to conduct channel selection and parameter optimization in an iterative way. Extensive experiments demonstrate the effectiveness of our method. For example, on ILSVRC-12, our pruned ResNet-50 with 30% reduction of channels even outperforms the original model by 0.39% in top-1 accuracy.



### Object Tracking in Hyperspectral Videos with Convolutional Features and Kernelized Correlation Filter
- **Arxiv ID**: http://arxiv.org/abs/1810.11819v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.11819v1)
- **Published**: 2018-10-28 14:35:26+00:00
- **Updated**: 2018-10-28 14:35:26+00:00
- **Authors**: Kun Qian, Jun Zhou, Fengchao Xiong, Huixin Zhou, Juan Du
- **Comment**: Accepted by ICSM 2018
- **Journal**: None
- **Summary**: Target tracking in hyperspectral videos is a new research topic. In this paper, a novel method based on convolutional network and Kernelized Correlation Filter (KCF) framework is presented for tracking objects of interest in hyperspectral videos. We extract a set of normalized three-dimensional cubes from the target region as fixed convolution filters which contain spectral information surrounding a target. The feature maps generated by convolutional operations are combined to form a three-dimensional representation of an object, thereby providing effective encoding of local spectral-spatial information. We show that a simple two-layer convolutional networks is sufficient to learn robust representations without the need of offline training with a large dataset. In the tracking step, KCF is adopted to distinguish targets from neighboring environment. Experimental results demonstrate that the proposed method performs well on sample hyperspectral videos, and outperforms several state-of-the-art methods tested on grayscale and color videos in the same scene.



### Multi-Spectral Imaging via Computed Tomography (MUSIC) - Comparing Unsupervised Spectral Segmentations for Material Differentiation
- **Arxiv ID**: http://arxiv.org/abs/1810.11823v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.11823v1)
- **Published**: 2018-10-28 15:28:36+00:00
- **Updated**: 2018-10-28 15:28:36+00:00
- **Authors**: Christian Kehl, Wail Mustafa, Jan Kehres, Anders Bjorholm Dahl, Ulrik Lund Olsen
- **Comment**: 21 pages, 24 figures (in articles), includes 2 appendices with 8
  additional figures
- **Journal**: None
- **Summary**: Multi-spectral computed tomography is an emerging technology for the non-destructive identification of object materials and the study of their physical properties. Applications of this technology can be found in various scientific and industrial contexts, such as luggage scanning at airports. Material distinction and its identification is challenging, even with spectral x-ray information, due to acquisition noise, tomographic reconstruction artefacts and scanning setup application constraints. We present MUSIC - and open access multi-spectral CT dataset in 2D and 3D - to promote further research in the area of material identification. We demonstrate the value of this dataset on the image analysis challenge of object segmentation purely based on the spectral response of its composing materials. In this context, we compare the segmentation accuracy of fast adaptive mean shift (FAMS) and unconstrained graph cuts on both datasets. We further discuss the impact of reconstruction artefacts and segmentation controls on the achievable results. Dataset, related software packages and further documentation are made available to the imaging community in an open-access manner to promote further data-driven research on the subject



### Enhanced CNN for image denoising
- **Arxiv ID**: http://arxiv.org/abs/1810.11834v4
- **DOI**: 10.1049/trit.2018.1054
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.11834v4)
- **Published**: 2018-10-28 17:00:42+00:00
- **Updated**: 2019-03-04 06:29:31+00:00
- **Authors**: Chunwei Tian, Yong Xu, Lunke Fei, Junqian Wang, Jie Wen, Nan Luo
- **Comment**: CAAI Transactions on Intelligence Technology[J], 2019
- **Journal**: None
- **Summary**: Owing to flexible architectures of deep convolutional neural networks (CNNs), CNNs are successfully used for image denoising. However, they suffer from the following drawbacks: (i) deep network architecture is very difficult to train. (ii) Deeper networks face the challenge of performance saturation. In this study, the authors propose a novel method called enhanced convolutional neural denoising network (ECNDNet). Specifically, they use residual learning and batch normalisation techniques to address the problem of training difficulties and accelerate the convergence of the network. In addition, dilated convolutions are used in the proposed network to enlarge the context information and reduce the computational cost. Extensive experiments demonstrate that the ECNDNet outperforms the state-of-the-art methods for image denoising.



### Convolutional LSTMs for Cloud-Robust Segmentation of Remote Sensing Imagery
- **Arxiv ID**: http://arxiv.org/abs/1811.02471v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.02471v2)
- **Published**: 2018-10-28 17:58:22+00:00
- **Updated**: 2018-12-02 11:30:38+00:00
- **Authors**: Marc Rußwurm, Marco Körner
- **Comment**: Cameraready version to NeurIPS 2018 Spatiotemporal Workshop.
  Openreview: https://openreview.net/forum?id=Sye7df9CK7
- **Journal**: None
- **Summary**: Clouds frequently cover the Earth's surface and pose an omnipresent challenge to optical Earth observation methods. The vast majority of remote sensing approaches either selectively choose single cloud-free observations or employ a pre-classification strategy to identify and mask cloudy pixels. We follow a different strategy and treat cloud coverage as noise that is inherent to the observed satellite data. In prior work, we directly employed a straightforward \emph{convolutional long short-term memory} network for vegetation classification without explicit cloud filtering and achieved state-of-the-art classification accuracies. In this work, we investigate this cloud-robustness further by visualizing internal cell activations and performing an ablation experiment on datasets of different cloud coverage. In the visualizations of network states, we identified some cells in which modulation and input gates closed on cloudy pixels. This indicates that the network has internalized a cloud-filtering mechanism without being specifically trained on cloud labels. Overall, our results question the necessity of sophisticated pre-processing pipelines for multi-temporal deep learning approaches.



### Scale Estimation of Monocular SfM for a Multi-modal Stereo Camera
- **Arxiv ID**: http://arxiv.org/abs/1810.11856v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.11856v1)
- **Published**: 2018-10-28 18:34:50+00:00
- **Updated**: 2018-10-28 18:34:50+00:00
- **Authors**: Shinya Sumikura, Ken Sakurada, Nobuo Kawaguchi, Ryosuke Nakamura
- **Comment**: Accepted to ACCV 2018, please see the additional results here:
  http://youtu.be/xOLtvMZJseU
- **Journal**: None
- **Summary**: This paper proposes a novel method of estimating the absolute scale of monocular SfM for a multi-modal stereo camera. In the fields of computer vision and robotics, scale estimation for monocular SfM has been widely investigated in order to simplify systems. This paper addresses the scale estimation problem for a stereo camera system in which two cameras capture different spectral images (e.g., RGB and FIR), whose feature points are difficult to directly match using descriptors. Furthermore, the number of matching points between FIR images can be comparatively small, owing to the low resolution and lack of thermal scene texture. To cope with these difficulties, the proposed method estimates the scale parameter using batch optimization, based on the epipolar constraint of a small number of feature correspondences between the invisible light images. The accuracy and numerical stability of the proposed method are verified by synthetic and real image experiments.



### Sequential anatomy localization in fetal echocardiography videos
- **Arxiv ID**: http://arxiv.org/abs/1810.11868v2
- **DOI**: None
- **Categories**: **cs.CV**, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1810.11868v2)
- **Published**: 2018-10-28 19:36:30+00:00
- **Updated**: 2018-12-20 17:27:52+00:00
- **Authors**: Arijit Patra, J. A. Noble
- **Comment**: To appear in ISBI 2019
- **Journal**: None
- **Summary**: Fetal heart motion is an important diagnostic indicator for structural detection and functional assessment of congenital heart disease. We propose an approach towards integrating deep convolutional and recurrent architectures that utilize localized spatial and temporal features of different anatomical substructures within a global spatiotemporal context for interpretation of fetal echocardiography videos. We formulate our task as a cardiac structure localization problem with convolutional architectures for aggregating global spatial context and detecting anatomical structures on spatial region proposals. This information is aggregated temporally by recurrent architectures to quantify the progressive motion patterns. We experimentally show that the resulting architecture combines anatomical landmark detection at the frame-level over multiple video sequences-with temporal progress of the associated anatomical motions to encode local spatiotemporal fetal heart dynamics and is validated on a real-world clinical dataset.



