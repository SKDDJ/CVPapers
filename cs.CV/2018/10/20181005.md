# Arxiv Papers in cs.CV on 2018-10-05
### Co-Learning Feature Fusion Maps from PET-CT Images of Lung Cancer
- **Arxiv ID**: http://arxiv.org/abs/1810.02492v2
- **DOI**: 10.1109/TMI.2019.2923601
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.02492v2)
- **Published**: 2018-10-05 02:09:54+00:00
- **Updated**: 2019-10-28 08:33:43+00:00
- **Authors**: Ashnil Kumar, Michael Fulham, Dagan Feng, Jinman Kim
- **Comment**: Source code is available from https://github.com/ashnilkumar/colearn
  . The paper has been accepted for publication in IEEE Transactions on Medical
  Imaging. The final published version of the manuscript can be accessed from
  the IEEE. The paper contains 21 pages (14 main paper, 7 supplementary), 16
  images (8 main paper, 8 supplementary), and 3 tables
- **Journal**: None
- **Summary**: The analysis of multi-modality positron emission tomography and computed tomography (PET-CT) images for computer aided diagnosis applications requires combining the sensitivity of PET to detect abnormal regions with anatomical localization from CT. Current methods for PET-CT image analysis either process the modalities separately or fuse information from each modality based on knowledge about the image analysis task. These methods generally do not consider the spatially varying visual characteristics that encode different information across the different modalities, which have different priorities at different locations. For example, a high abnormal PET uptake in the lungs is more meaningful for tumor detection than physiological PET uptake in the heart. Our aim is to improve fusion of the complementary information in multi-modality PET-CT with a new supervised convolutional neural network (CNN) that learns to fuse complementary information for multi-modality medical image analysis. Our CNN first encodes modality-specific features and then uses them to derive a spatially varying fusion map that quantifies the relative importance of each modality's features across different spatial locations. These fusion maps are then multiplied with the modality-specific feature maps to obtain a representation of the complementary multi-modality information at different locations, which can then be used for image analysis. We evaluated the ability of our CNN to detect and segment multiple regions with different fusion requirements using a dataset of PET-CT images of lung cancer. We compared our method to baseline techniques for multi-modality image fusion and segmentation. Our findings show that our CNN had a significantly higher foreground detection accuracy (99.29%, p < 0.05) than the fusion baselines and a significantly higher Dice score (63.85%) than recent PET-CT tumor segmentation methods.



### Medical Images Analysis in Cancer Diagnostic
- **Arxiv ID**: http://arxiv.org/abs/1810.02495v1
- **DOI**: 10.5121/csit.2018.80107
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.02495v1)
- **Published**: 2018-10-05 02:38:46+00:00
- **Updated**: 2018-10-05 02:38:46+00:00
- **Authors**: Jelena Vasiljević, Ivica Milosavljević, Vladimir Krstić, Nataša Zivić, Lazar Berbakov, Luka Lopušina, Dhinaharan Nagamalai, Milutin Cerović
- **Comment**: 11 pages, 6 figures, 11th International Conference on Security and
  its Applications (CNSA 2018), Zurich, Switzerland,
- **Journal**: Computer Science & Information Technology (CS & IT), 2018
- **Summary**: This paper shows results of computer analysis of images in the purpose of finding differences between medical images in order of their classifications in terms of separation malign tissue from a normal and benign tissue. The diagnostics of malign tissue is of the crucial importance in medicine. Therefore, ascertainment of the correlation between multifractals parameters and "chaotic" cells could be of the great appliance. This paper shows the application of multifractal analysis for additional help in cancer diagnosis, as well as diminishing. of the subjective factor and error probability



### Learning To Simulate
- **Arxiv ID**: http://arxiv.org/abs/1810.02513v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.02513v2)
- **Published**: 2018-10-05 04:11:25+00:00
- **Updated**: 2019-05-14 03:15:27+00:00
- **Authors**: Nataniel Ruiz, Samuel Schulter, Manmohan Chandraker
- **Comment**: Published at International Conference on Learning Representations
  (ICLR) 2019
- **Journal**: None
- **Summary**: Simulation is a useful tool in situations where training data for machine learning models is costly to annotate or even hard to acquire. In this work, we propose a reinforcement learning-based method for automatically adjusting the parameters of any (non-differentiable) simulator, thereby controlling the distribution of synthesized data in order to maximize the accuracy of a model trained on that data. In contrast to prior art that hand-crafts these simulation parameters or adjusts only parts of the available parameters, our approach fully controls the simulator with the actual underlying goal of maximizing accuracy, rather than mimicking the real data distribution or randomly generating a large volume of data. We find that our approach (i) quickly converges to the optimal simulation parameters in controlled experiments and (ii) can indeed discover good sets of parameters for an image rendering simulator in actual computer vision applications.



### Weakly Supervised Object Detection in Artworks
- **Arxiv ID**: http://arxiv.org/abs/1810.02569v1
- **DOI**: 10.1007/978-3-030-11012-3_53
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.02569v1)
- **Published**: 2018-10-05 08:48:53+00:00
- **Updated**: 2018-10-05 08:48:53+00:00
- **Authors**: Nicolas Gonthier, Yann Gousseau, Said Ladjal, Olivier Bonfait
- **Comment**: Accepted at ECCV 2018 Workshop Computer Vision for Art Analysis -
  VISART 2018 14 pages, 5 figures
- **Journal**: None
- **Summary**: We propose a method for the weakly supervised detection of objects in paintings. At training time, only image-level annotations are needed. This, combined with the efficiency of our multiple-instance learning method, enables one to learn new classes on-the-fly from globally annotated databases, avoiding the tedious task of manually marking objects. We show on several databases that dropping the instance-level annotations only yields mild performance losses. We also introduce a new database, IconArt, on which we perform detection experiments on classes that could not be learned on photographs, such as Jesus Child or Saint Sebastian. To the best of our knowledge, these are the first experiments dealing with the automatic (and in our case weakly supervised) detection of iconographic elements in paintings. We believe that such a method is of great benefit for helping art historians to explore large digital databases.



### Dark Model Adaptation: Semantic Image Segmentation from Daytime to Nighttime
- **Arxiv ID**: http://arxiv.org/abs/1810.02575v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.02575v1)
- **Published**: 2018-10-05 09:09:38+00:00
- **Updated**: 2018-10-05 09:09:38+00:00
- **Authors**: Dengxin Dai, Luc Van Gool
- **Comment**: Accepted to International Conference on Intelligent Transportation
  Systems (ITSC 2018)
- **Journal**: None
- **Summary**: This work addresses the problem of semantic image segmentation of nighttime scenes. Although considerable progress has been made in semantic image segmentation, it is mainly related to daytime scenarios. This paper proposes a novel method to progressive adapt the semantic models trained on daytime scenes, along with large-scale annotations therein, to nighttime scenes via the bridge of twilight time -- the time between dawn and sunrise, or between sunset and dusk. The goal of the method is to alleviate the cost of human annotation for nighttime images by transferring knowledge from standard daytime conditions. In addition to the method, a new dataset of road scenes is compiled; it consists of 35,000 images ranging from daytime to twilight time and to nighttime. Also, a subset of the nighttime images are densely annotated for method evaluation. Our experiments show that our method is effective for model adaptation from daytime scenes to nighttime scenes, without using extra human annotation.



### AIRNet: Self-Supervised Affine Registration for 3D Medical Images using Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1810.02583v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.02583v2)
- **Published**: 2018-10-05 09:37:24+00:00
- **Updated**: 2018-10-15 01:34:35+00:00
- **Authors**: Evelyn Chee, Zhenzhou Wu
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we propose a self-supervised learning method for affine image registration on 3D medical images. Unlike optimisation-based methods, our affine image registration network (AIRNet) is designed to directly estimate the transformation parameters between two input images without using any metric, which represents the quality of the registration, as the optimising function. But since it is costly to manually identify the transformation parameters between any two images, we leverage the abundance of cheap unlabelled data to generate a synthetic dataset for the training of the model. Additionally, the structure of AIRNet enables us to learn the discriminative features of the images which are useful for registration purpose. Our proposed method was evaluated on magnetic resonance images of the axial view of human brain and compared with the performance of a conventional image registration method. Experiments demonstrate that our approach achieves better overall performance on registration of images from different patients and modalities with 100x speed-up in execution time.



### Spatially-weighted Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/1810.02607v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1810.02607v1)
- **Published**: 2018-10-05 11:04:06+00:00
- **Updated**: 2018-10-05 11:04:06+00:00
- **Authors**: Minori Narita, Daiki Kimura, Ryuki Tachibana
- **Comment**: 4 pages, SSII 2018 (original paper was written in Japanese)
- **Journal**: None
- **Summary**: Many types of anomaly detection methods have been proposed recently, and applied to a wide variety of fields including medical screening and production quality checking. Some methods have utilized images, and, in some cases, a part of the anomaly images is known beforehand. However, this kind of information is dismissed by previous methods, because the methods can only utilize a normal pattern. Moreover, the previous methods suffer a decrease in accuracy due to negative effects from surrounding noises. In this study, we propose a spatially-weighted anomaly detection method (SPADE) that utilizes all of the known patterns and lessens the vulnerability to ambient noises by applying Grad-CAM, which is the visualization method of a CNN. We evaluated our method quantitatively using two datasets, the MNIST dataset with noise and a dataset based on a brief screening test for dementia.



### SLIC Based Digital Image Enlargement
- **Arxiv ID**: http://arxiv.org/abs/1810.02643v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.02643v1)
- **Published**: 2018-10-05 12:19:21+00:00
- **Updated**: 2018-10-05 12:19:21+00:00
- **Authors**: M. Z. F. Amara, R. Bandara, Thushari Silva
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: Low resolution image enhancement is a classical computer vision problem. Selecting the best method to reconstruct an image to a higher resolution with the limited data available in the low-resolution image is quite a challenge. A major drawback from the existing enlargement techniques is the introduction of color bleeding while interpolating pixels over the edges that separate distinct colors in an image. The color bleeding causes to accentuate the edges with new colors as a result of blending multiple colors over adjacent regions. This paper proposes a novel approach to mitigate the color bleeding by segmenting the homogeneous color regions of the image using Simple Linear Iterative Clustering (SLIC) and applying a higher order interpolation technique separately on the isolated segments. The interpolation at the boundaries of each of the isolated segments is handled by using a morphological operation. The approach is evaluated by comparing against several frequently used image enlargement methods such as bilinear and bicubic interpolation by means of Peak Signal-to-Noise-Ratio (PSNR) value. The results obtained exhibit that the proposed method outperforms the baseline methods by means of PSNR and also mitigates the color bleeding at the edges which improves the overall appearance.



### LiveCap: Real-time Human Performance Capture from Monocular Video
- **Arxiv ID**: http://arxiv.org/abs/1810.02648v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.02648v3)
- **Published**: 2018-10-05 12:39:37+00:00
- **Updated**: 2019-01-25 15:37:49+00:00
- **Authors**: Marc Habermann, Weipeng Xu, Michael Zollhoefer, Gerard Pons-Moll, Christian Theobalt
- **Comment**: None
- **Journal**: None
- **Summary**: We present the first real-time human performance capture approach that reconstructs dense, space-time coherent deforming geometry of entire humans in general everyday clothing from just a single RGB video. We propose a novel two-stage analysis-by-synthesis optimization whose formulation and implementation are designed for high performance. In the first stage, a skinned template model is jointly fitted to background subtracted input video, 2D and 3D skeleton joint positions found using a deep neural network, and a set of sparse facial landmark detections. In the second stage, dense non-rigid 3D deformations of skin and even loose apparel are captured based on a novel real-time capable algorithm for non-rigid tracking using dense photometric and silhouette constraints. Our novel energy formulation leverages automatically identified material regions on the template to model the differing non-rigid deformation behavior of skin and apparel. The two resulting non-linear optimization problems per-frame are solved with specially-tailored data-parallel Gauss-Newton solvers. In order to achieve real-time performance of over 25Hz, we design a pipelined parallel architecture using the CPU and two commodity GPUs. Our method is the first real-time monocular approach for full-body performance capture. Our method yields comparable accuracy with off-line performance capture techniques, while being orders of magnitude faster.



### Generating Diffusion MRI scalar maps from T1 weighted images using generative adversarial networks
- **Arxiv ID**: http://arxiv.org/abs/1810.02683v3
- **DOI**: 10.1007/978-3-030-20205-7_40
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.02683v3)
- **Published**: 2018-10-05 13:53:28+00:00
- **Updated**: 2019-04-09 13:09:40+00:00
- **Authors**: Xuan Gu, Hans Knutsson, Markus Nilsson, Anders Eklund
- **Comment**: None
- **Journal**: Scandinavian Conference on Image Analysis, 2019
- **Summary**: Diffusion magnetic resonance imaging (diffusion MRI) is a non-invasive microstructure assessment technique. Scalar measures, such as FA (fractional anisotropy) and MD (mean diffusivity), quantifying micro-structural tissue properties can be obtained using diffusion models and data processing pipelines. However, it is costly and time consuming to collect high quality diffusion data. Here, we therefore demonstrate how Generative Adversarial Networks (GANs) can be used to generate synthetic diffusion scalar measures from structural T1-weighted images in a single optimized step. Specifically, we train the popular CycleGAN model to learn to map a T1 image to FA or MD, and vice versa. As an application, we show that synthetic FA images can be used as a target for non-linear registration, to correct for geometric distortions common in diffusion MRI.



### Automatic Detection of Arousals during Sleep using Multiple Physiological Signals
- **Arxiv ID**: http://arxiv.org/abs/1810.02726v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/1810.02726v1)
- **Published**: 2018-10-05 14:50:55+00:00
- **Updated**: 2018-10-05 14:50:55+00:00
- **Authors**: Saman Parvaneh, Jonathan Rubin, Ali Samadani, Gajendra Katuwal
- **Comment**: Computing in Cardiology 2018
- **Journal**: None
- **Summary**: The visual scoring of arousals during sleep routinely conducted by sleep experts is a challenging task warranting an automatic approach. This paper presents an algorithm for automatic detection of arousals during sleep. Using the Physionet/CinC Challenge dataset, an 80-20% subject-level split was performed to create in-house training and test sets, respectively. The data for each subject in the training set was split to 30-second epochs with no overlap. A total of 428 features from EEG, EMG, EOG, airflow, and SaO2 in each epoch were extracted and used for creating subject-specific models based on an ensemble of bagged classification trees, resulting in 943 models. For marking arousal and non-arousal regions in the test set, the data in the test set was split to 30-second epochs with 50% overlaps. The average of arousal probabilities from different patient-specific models was assigned to each 30-second epoch and then a sample-wise probability vector with the same length as test data was created for model evaluation. Using the PhysioNet/CinC Challenge 2018 scoring criteria, AUPRCs of 0.25 and 0.21 were achieved for the in-house test and blind test sets, respectively.



### Hierarchical Recurrent Filtering for Fully Convolutional DenseNets
- **Arxiv ID**: http://arxiv.org/abs/1810.02766v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1810.02766v2)
- **Published**: 2018-10-05 15:54:46+00:00
- **Updated**: 2018-10-15 16:35:14+00:00
- **Authors**: Jörg Wagner, Volker Fischer, Michael Herman, Sven Behnke
- **Comment**: In Proceedings of 26th European Symposium on Artificial Neural
  Networks, Computational Intelligence and Machine Learning (ESANN), Bruges,
  Belgium, 2018
- **Journal**: None
- **Summary**: Generating a robust representation of the environment is a crucial ability of learning agents. Deep learning based methods have greatly improved perception systems but still fail in challenging situations. These failures are often not solvable on the basis of a single image. In this work, we present a parameter-efficient temporal filtering concept which extends an existing single-frame segmentation model to work with multiple frames. The resulting recurrent architecture temporally filters representations on all abstraction levels in a hierarchical manner, while decoupling temporal dependencies from scene representation. Using a synthetic dataset, we show the ability of our model to cope with data perturbations and highlight the importance of recurrent and hierarchical filtering.



### Interpretable Convolutional Neural Networks via Feedforward Design
- **Arxiv ID**: http://arxiv.org/abs/1810.02786v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.02786v2)
- **Published**: 2018-10-05 16:44:49+00:00
- **Updated**: 2018-10-21 06:20:48+00:00
- **Authors**: C. -C. Jay Kuo, Min Zhang, Siyang Li, Jiali Duan, Yueru Chen
- **Comment**: 32 pages
- **Journal**: None
- **Summary**: The model parameters of convolutional neural networks (CNNs) are determined by backpropagation (BP). In this work, we propose an interpretable feedforward (FF) design without any BP as a reference. The FF design adopts a data-centric approach. It derives network parameters of the current layer based on data statistics from the output of the previous layer in a one-pass manner. To construct convolutional layers, we develop a new signal transform, called the Saab (Subspace Approximation with Adjusted Bias) transform. It is a variant of the principal component analysis (PCA) with an added bias vector to annihilate activation's nonlinearity. Multiple Saab transforms in cascade yield multiple convolutional layers. As to fully-connected (FC) layers, we construct them using a cascade of multi-stage linear least squared regressors (LSRs). The classification and robustness (against adversarial attacks) performances of BP- and FF-designed CNNs applied to the MNIST and the CIFAR-10 datasets are compared. Finally, we comment on the relationship between BP and FF designs.



### CanvasGAN: A simple baseline for text to image generation by incrementally patching a canvas
- **Arxiv ID**: http://arxiv.org/abs/1810.02833v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.02833v1)
- **Published**: 2018-10-05 18:04:07+00:00
- **Updated**: 2018-10-05 18:04:07+00:00
- **Authors**: Amanpreet Singh, Sharan Agrawal
- **Comment**: CVC 2019
- **Journal**: None
- **Summary**: We propose a new recurrent generative model for generating images from text captions while attending on specific parts of text captions. Our model creates images by incrementally adding patches on a "canvas" while attending on words from text caption at each timestep. Finally, the canvas is passed through an upscaling network to generate images. We also introduce a new method for generating visual-semantic sentence embeddings based on self-attention over text. We compare our model's generated images with those generated Reed et. al.'s model and show that our model is a stronger baseline for text to image generation tasks.



### A Comparison between Background Modelling Methods for Vehicle Segmentation in Highway Traffic Videos
- **Arxiv ID**: http://arxiv.org/abs/1810.02835v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.02835v1)
- **Published**: 2018-10-05 18:07:40+00:00
- **Updated**: 2018-10-05 18:07:40+00:00
- **Authors**: L. A. Marcomini, A. L. Cunha
- **Comment**: 12 pages, 11 figures, 1 table
- **Journal**: None
- **Summary**: The objective of this paper is to compare the performance of three background-modeling algorithms in segmenting and detecting vehicles in highway traffic videos. All algorithms are available in OpenCV and were all coded in Python. We analyzed seven videos, totaling 2 hours of recording. To compare the algorithms, we created 35 ground-truth images, five from each video, and we used three different metrics: accuracy rate, precision rate, and processing time. By using accuracy and precision, we aim to identify how well the algorithms perform in detection and segmentation, while using the processing time to evaluate the impact on the computational system. Results indicate that all three algorithms had more than 90% of precision rate, while obtaining an average of 80% on accuracy. The algorithm with the lowest impact on processing time allowed the computation of 60 frames per second.



### Computationally Efficient Deep Neural Network for Computed Tomography Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1810.03999v3
- **DOI**: 10.1002/mp.13627
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.03999v3)
- **Published**: 2018-10-05 18:26:39+00:00
- **Updated**: 2019-05-24 03:25:59+00:00
- **Authors**: Dufan Wu, Kyungsang Kim, Quanzheng Li
- **Comment**: 33 pages, 14 figures, accepted by Medical Physics
- **Journal**: None
- **Summary**: Deep-neural-network-based image reconstruction has demonstrated promising performance in medical imaging for under-sampled and low-dose scenarios. However, it requires large amount of memory and extensive time for the training. It is especially challenging to train the reconstruction networks for three-dimensional computed tomography (CT) because of the high resolution of CT images. The purpose of this work is to reduce the memory and time consumption of the training of the reconstruction networks for CT to make it practical for current hardware, while maintaining the quality of the reconstructed images.   We unrolled the proximal gradient descent algorithm for iterative image reconstruction to finite iterations and replaced the terms related to the penalty function with trainable convolutional neural networks (CNN). The network was trained greedily iteration by iteration in the image-domain on patches, which requires reasonable amount of memory and time on mainstream graphics processing unit (GPU). To overcome the local-minimum problem caused by greedy learning, we used deep UNet as the CNN and incorporated separable quadratic surrogate with ordered subsets for data fidelity, so that the solution could escape from easy local minimums and achieve better image quality.   The proposed method achieved comparable image quality with state-of-the-art neural network for CT image reconstruction on 2D sparse-view and limited-angle problems on the low-dose CT challenge dataset.



### Deep Generative Video Compression
- **Arxiv ID**: http://arxiv.org/abs/1810.02845v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.02845v2)
- **Published**: 2018-10-05 18:42:02+00:00
- **Updated**: 2019-11-01 22:48:14+00:00
- **Authors**: Jun Han, Salvator Lombardo, Christopher Schroers, Stephan Mandt
- **Comment**: Accepted at NeurIPS 2019, 15 pages, 8 figures
- **Journal**: None
- **Summary**: The usage of deep generative models for image compression has led to impressive performance gains over classical codecs while neural video compression is still in its infancy. Here, we propose an end-to-end, deep generative modeling approach to compress temporal sequences with a focus on video. Our approach builds upon variational autoencoder (VAE) models for sequential data and combines them with recent work on neural image compression. The approach jointly learns to transform the original sequence into a lower-dimensional representation as well as to discretize and entropy code this representation according to predictions of the sequential VAE. Rate-distortion evaluations on small videos from public data sets with varying complexity and diversity show that our model yields competitive results when trained on generic video content. Extreme compression performance is achieved when training the model on specialized content.



### Decoupled Classification Refinement: Hard False Positive Suppression for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1810.04002v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.04002v2)
- **Published**: 2018-10-05 19:34:20+00:00
- **Updated**: 2020-12-09 04:04:10+00:00
- **Authors**: Bowen Cheng, Yunchao Wei, Rogerio Feris, Jinjun Xiong, Wen-mei Hwu, Thomas Huang, Humphrey Shi
- **Comment**: under review. arXiv admin note: substantial text overlap with
  arXiv:1803.06799
- **Journal**: None
- **Summary**: In this paper, we analyze failure cases of state-of-the-art detectors and observe that most hard false positives result from classification instead of localization and they have a large negative impact on the performance of object detectors. We conjecture there are three factors: (1) Shared feature representation is not optimal due to the mismatched goals of feature learning for classification and localization; (2) multi-task learning helps, yet optimization of the multi-task loss may result in sub-optimal for individual tasks; (3) large receptive field for different scales leads to redundant context information for small objects. We demonstrate the potential of detector classification power by a simple, effective, and widely-applicable Decoupled Classification Refinement (DCR) network. In particular, DCR places a separate classification network in parallel with the localization network (base detector). With ROI Pooling placed on the early stage of the classification network, we enforce an adaptive receptive field in DCR. During training, DCR samples hard false positives from the base detector and trains a strong classifier to refine classification results. During testing, DCR refines all boxes from the base detector. Experiments show competitive results on PASCAL VOC and COCO without any bells and whistles. Our codes are available at: https://github.com/bowenc0221/Decoupled-Classification-Refinement.



### Generic Model-Agnostic Convolutional Neural Network for Single Image Dehazing
- **Arxiv ID**: http://arxiv.org/abs/1810.02862v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.02862v2)
- **Published**: 2018-10-05 19:35:28+00:00
- **Updated**: 2019-11-29 15:48:24+00:00
- **Authors**: Zheng Liu, Botao Xiao, Muhammad Alrabeiah, Keyan Wang, Jun Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Haze and smog are among the most common environmental factors impacting image quality and, therefore, image analysis. This paper proposes an end-to-end generative method for image dehazing. It is based on designing a fully convolutional neural network to recognize haze structures in input images and restore clear, haze-free images. The proposed method is agnostic in the sense that it does not explore the atmosphere scattering model. Somewhat surprisingly, it achieves superior performance relative to all existing state-of-the-art methods for image dehazing even on SOTS outdoor images, which are synthesized using the atmosphere scattering model.   Project detail and code can be found here: https://github.com/Seanforfun/GMAN_Net_Haze_Removal



### CDF Transform-and-Shift: An effective way to deal with datasets of inhomogeneous cluster densities
- **Arxiv ID**: http://arxiv.org/abs/1810.02897v3
- **DOI**: 10.1016/j.patcog.2021.107977
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.02897v3)
- **Published**: 2018-10-05 22:32:51+00:00
- **Updated**: 2021-04-12 04:27:35+00:00
- **Authors**: Ye Zhu, Kai Ming Ting, Mark Carman, Maia Angelova
- **Comment**: Pattern Recognition (2021)
- **Journal**: None
- **Summary**: The problem of inhomogeneous cluster densities has been a long-standing issue for distance-based and density-based algorithms in clustering and anomaly detection. These algorithms implicitly assume that all clusters have approximately the same density. As a result, they often exhibit a bias towards dense clusters in the presence of sparse clusters. Many remedies have been suggested; yet, we show that they are partial solutions which do not address the issue satisfactorily. To match the implicit assumption, we propose to transform a given dataset such that the transformed clusters have approximately the same density while all regions of locally low density become globally low density -- homogenising cluster density while preserving the cluster structure of the dataset. We show that this can be achieved by using a new multi-dimensional Cumulative Distribution Function in a transform-and-shift method. The method can be applied to every dataset, before the dataset is used in many existing algorithms to match their implicit assumption without algorithmic modification. We show that the proposed method performs better than existing remedies.



