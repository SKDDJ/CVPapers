# Arxiv Papers in cs.CV on 2018-10-15
### Traffic Signs in the Wild: Highlights from the IEEE Video and Image Processing Cup 2017 Student Competition [SP Competitions]
- **Arxiv ID**: http://arxiv.org/abs/1810.06169v2
- **DOI**: 10.1109/MSP.2017.2783449
- **Categories**: **cs.CV**, I.2; I.4; I.5
- **Links**: [PDF](http://arxiv.org/pdf/1810.06169v2)
- **Published**: 2018-10-15 03:40:48+00:00
- **Updated**: 2018-11-13 15:44:58+00:00
- **Authors**: Dogancan Temel, Ghassan AlRegib
- **Comment**: 11 pages, 5 figures
- **Journal**: IEEE Signal Processing Magazine, vol. 35, no. 2, pp. 154-161,
  March 2018
- **Summary**: Robust and reliable traffic sign detection is necessary to bring autonomous vehicles onto our roads. State-of-the-art algorithms successfully perform traffic sign detection over existing databases that mostly lack severe challenging conditions. VIP Cup 2017 competition focused on detecting such traffic signs under challenging conditions. To facilitate such task and competition, we introduced a video dataset denoted as CURE-TSD that includes a variety of challenging conditions. The goal of this challenge was to implement traffic sign detection algorithms that can robustly perform under such challenging conditions. In this article, we share an overview of the VIP Cup 2017 experience including competition setup, teams, technical approaches, participation statistics, and competition experience through finalist teams members' and organizers' eyes.



### 3D Feature Pyramid Attention Module for Robust Visual Speech Recognition
- **Arxiv ID**: http://arxiv.org/abs/1810.06178v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.06178v4)
- **Published**: 2018-10-15 04:38:23+00:00
- **Updated**: 2019-01-11 11:57:12+00:00
- **Authors**: Jingyun Xiao
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Visual speech recognition is the task to decode the speech content from a video based on visual information, especially the movements of lips. It is also referenced as lipreading. Motivated by two problems existing in lipreading, words with similar pronunciation and the variation of word duration, we propose a novel 3D Feature Pyramid Attention (3D-FPA) module to jointly improve the representation power of features in both the spatial and temporal domains. Specifically, the input features are downsampled for 3 times in both the spatial and temporal dimensions to construct spatiotemporal feature pyramids. Then high-level features are upsampled and combined with low-level features, finally generating a pixel-level soft attention mask to be multiplied with the input features.It enhances the discriminative power of features and exploits the temporal multi-scale information while decoding the visual speeches. Also, this module provides a new method to construct and utilize temporal pyramid structures in video analysis tasks. The field of temporal featrue pyramids are still under exploring compared to the plentiful works on spatial feature pyramids for image analysis tasks. To validate the effectiveness and adaptability of our proposed module, we embed the module in a sentence-level lipreading model, LipNet, with the result of 3.6% absolute decrease in word error rate, and a word-level model, with the result of 1.4% absolute improvement in accuracy.



### Solution for Large-Scale Hierarchical Object Detection Datasets with Incomplete Annotation and Data Imbalance
- **Arxiv ID**: http://arxiv.org/abs/1810.06208v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.06208v1)
- **Published**: 2018-10-15 07:40:53+00:00
- **Updated**: 2018-10-15 07:40:53+00:00
- **Authors**: Yuan Gao, Xingyuan Bu, Yang Hu, Hui Shen, Ti Bai, Xubin Li, Shilei Wen
- **Comment**: 5 pages, 4 figures, ECCV 2018 Open Images workshop
- **Journal**: None
- **Summary**: This report demonstrates our solution for the Open Images 2018 Challenge. Based on our detailed analysis on the Open Images Datasets (OID), it is found that there are four typical features: large-scale, hierarchical tag system, severe annotation incompleteness and data imbalance. Considering these characteristics, an amount of strategies are employed, including SNIPER, soft sampling, class-aware sampling (CAS), hierarchical non-maximum suppression (HNMS) and so on. In virtue of these effective strategies, and further using the powerful SENet154 armed with feature pyramid module and deformable ROIalign as the backbone, our best single model could achieve a mAP of 56.9%. After a further ensemble with 9 models, the final mAP is boosted to 62.2% in the public leaderboard (ranked the 2nd place) and 58.6% in the private leaderboard (ranked the 3rd place, slightly inferior to the 1st place by only 0.04 point).



### The Focus-Aspect-Polarity Model for Predicting Subjective Noun Attributes in Images
- **Arxiv ID**: http://arxiv.org/abs/1810.06219v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1810.06219v1)
- **Published**: 2018-10-15 08:14:38+00:00
- **Updated**: 2018-10-15 08:14:38+00:00
- **Authors**: Tushar Karayil, Philipp Blandfort, JÃ¶rn Hees, Andreas Dengel
- **Comment**: None
- **Journal**: None
- **Summary**: Subjective visual interpretation is a challenging yet important topic in computer vision. Many approaches reduce this problem to the prediction of adjective- or attribute-labels from images. However, most of these do not take attribute semantics into account, or only process the image in a holistic manner. Furthermore, there is a lack of relevant datasets with fine-grained subjective labels. In this paper, we propose the Focus-Aspect-Polarity model to structure the process of capturing subjectivity in image processing, and introduce a novel dataset following this way of modeling. We run experiments on this dataset to compare several deep learning methods and find that incorporating context information based on tensor multiplication in several cases outperforms the default way of information fusion (concatenation).



### Supervised COSMOS Autoencoder: Learning Beyond the Euclidean Loss!
- **Arxiv ID**: http://arxiv.org/abs/1810.06221v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.06221v1)
- **Published**: 2018-10-15 08:19:38+00:00
- **Updated**: 2018-10-15 08:19:38+00:00
- **Authors**: Maneet Singh, Shruti Nagpal, Mayank Vatsa, Richa Singh, Afzel Noore
- **Comment**: None
- **Journal**: None
- **Summary**: Autoencoders are unsupervised deep learning models used for learning representations. In literature, autoencoders have shown to perform well on a variety of tasks spread across multiple domains, thereby establishing widespread applicability. Typically, an autoencoder is trained to generate a model that minimizes the reconstruction error between the input and the reconstructed output, computed in terms of the Euclidean distance. While this can be useful for applications related to unsupervised reconstruction, it may not be optimal for classification. In this paper, we propose a novel Supervised COSMOS Autoencoder which utilizes a multi-objective loss function to learn representations that simultaneously encode the (i) "similarity" between the input and reconstructed vectors in terms of their direction, (ii) "distribution" of pixel values of the reconstruction with respect to the input sample, while also incorporating (iii) "discriminability" in the feature learning pipeline. The proposed autoencoder model incorporates a Cosine similarity and Mahalanobis distance based loss function, along with supervision via Mutual Information based loss. Detailed analysis of each component of the proposed model motivates its applicability for feature learning in different classification tasks. The efficacy of Supervised COSMOS autoencoder is demonstrated via extensive experimental evaluations on different image datasets. The proposed model outperforms existing algorithms on MNIST, CIFAR-10, and SVHN databases. It also yields state-of-the-art results on CelebA, LFWA, Adience, and IJB-A databases for attribute prediction and face recognition, respectively.



### Sparse-View CT Reconstruction via Convolutional Sparse Coding
- **Arxiv ID**: http://arxiv.org/abs/1810.06228v1
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1810.06228v1)
- **Published**: 2018-10-15 08:50:25+00:00
- **Updated**: 2018-10-15 08:50:25+00:00
- **Authors**: Peng Bao, Wenjun Xia, Kang Yang, Jiliu Zhou, Yi Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Traditional dictionary learning based CT reconstruction methods are patch-based and the features learned with these methods often contain shifted versions of the same features. To deal with these problems, the convolutional sparse coding (CSC) has been proposed and introduced into various applications. In this paper, inspired by the successful applications of CSC in the field of signal processing, we propose a novel sparse-view CT reconstruction method based on CSC with gradient regularization on feature maps. By directly working on whole image, which need not to divide the image into overlapped patches like dictionary learning based methods, the proposed method can maintain more details and avoid the artifacts caused by patch aggregation. Experimental results demonstrate that the proposed method has better performance than several existing algorithms in both qualitative and quantitative aspects.



### A Context-aware Capsule Network for Multi-label Classification
- **Arxiv ID**: http://arxiv.org/abs/1810.06231v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.06231v2)
- **Published**: 2018-10-15 09:02:54+00:00
- **Updated**: 2018-10-16 04:58:27+00:00
- **Authors**: Sameera Ramasinghe, C. D. Athuralya, Salman Khan
- **Comment**: None
- **Journal**: None
- **Summary**: Recently proposed Capsule Network is a brain inspired architecture that brings a new paradigm to deep learning by modelling input domain variations through vector based representations. Despite being a seminal contribution, CapsNet does not explicitly model structured relationships between the detected entities and among the capsule features for related inputs. Motivated by the working of cortical network in human visual system, we seek to resolve CapsNet limitations by proposing several intuitive modifications to the CapsNet architecture. We introduce, (1) a novel routing weight initialization technique, (2) an improved CapsNet design that exploits semantic relationships between the primary capsule activations using a densely connected Conditional Random Field and (3) a Cholesky transformation based correlation module to learn a general priority scheme. Our proposed design allows CapsNet to scale better to more complex problems, such as the multi-label classification task, where semantically related categories co-exist with various interdependencies. We present theoretical bases for our extensions and demonstrate significant improvements on ADE20K scene dataset.



### BshapeNet: Object Detection and Instance Segmentation with Bounding Shape Masks
- **Arxiv ID**: http://arxiv.org/abs/1810.10327v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.10327v3)
- **Published**: 2018-10-15 10:12:45+00:00
- **Updated**: 2019-07-31 11:19:23+00:00
- **Authors**: Ba Rom Kang, Ha Young Kim
- **Comment**: 10 pages,6 figures
- **Journal**: None
- **Summary**: Recent object detectors use four-coordinate bounding box (bbox) regression to predict object locations. Providing additional information indicating the object positions and coordinates will improve detection performance. Thus, we propose two types of masks: a bbox mask and a bounding shape (bshape) mask, to represent the object's bbox and boundary shape, respectively. For each of these types, we consider two variants: the Thick model and the Scored model, both of which have the same morphology but differ in ways to make their boundaries thicker. To evaluate the proposed masks, we design extended frameworks by adding a bshape mask (or a bbox mask) branch to a Faster R-CNN framework, and call this BshapeNet (or BboxNet). Further, we propose BshapeNet+, a network that combines a bshape mask branch with a Mask R-CNN to improve instance segmentation as well as detection. Among our proposed models, BshapeNet+ demonstrates the best performance in both tasks and achieves highly competitive results with state of the art (SOTA) models. Particularly, it improves the detection results over Faster R-CNN+RoIAlign (37.3% and 28.9%) with a detection AP of 42.4% and 32.3% on MS COCO test-dev and Cityscapes val, respectively. Furthermore, for small objects, it achieves 24.9% AP on COCO test-dev, a significant improvement over previous SOTA models. For instance segmentation, it is substantially superior to Mask R-CNN on both test datasets.



### Playing for Depth
- **Arxiv ID**: http://arxiv.org/abs/1810.06268v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.06268v1)
- **Published**: 2018-10-15 10:54:19+00:00
- **Updated**: 2018-10-15 10:54:19+00:00
- **Authors**: Mohammad Mahdi Haji-Esmaeili, Gholamali Montazer
- **Comment**: None
- **Journal**: None
- **Summary**: Estimating the relative depth of a scene is a significant step towards understanding the general structure of the depicted scenery, the relations of entities in the scene and their interactions. When faced with the task of estimating depth without the use of Stereo images, we are dependent on the availability of large-scale depth datasets and high-capacity models to capture the intrinsic nature of depth. Unfortunately, creating datasets of depth images is not a trivial task as the requirements for the camera mainly limits us to areas where we can provide the necessities for the camera to work.   In this work, we present a new depth dataset captured from Video Games in an easy and reproducible way. The nature of open-world video games gives us the ability to capture high-quality depth maps in the wild without the constrictions of Stereo cameras. Experiments on this dataset shows that using such synthetic datasets increases the accuracy of Monocular Depth Estimation in the wild where other approaches usually fail to generalize.



### Feature Representation Analysis of Deep Convolutional Neural Network using Two-stage Feature Transfer -An Application for Diffuse Lung Disease Classification-
- **Arxiv ID**: http://arxiv.org/abs/1810.06282v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1810.06282v1)
- **Published**: 2018-10-15 11:35:34+00:00
- **Updated**: 2018-10-15 11:35:34+00:00
- **Authors**: Aiga Suzuki, Hidenori Sakanashi, Shoji Kido, Hayaru Shouno
- **Comment**: Preprint of the journal article to be published in IPSJ TOM-51.
  Notice for the use of this material The copyright of this material is
  retained by the Information Processing Society of Japan (IPSJ). This material
  is published on this web site with the agreement of the author (s) and the
  IPSJ
- **Journal**: None
- **Summary**: Transfer learning is a machine learning technique designed to improve generalization performance by using pre-trained parameters obtained from other learning tasks. For image recognition tasks, many previous studies have reported that, when transfer learning is applied to deep neural networks, performance improves, despite having limited training data. This paper proposes a two-stage feature transfer learning method focusing on the recognition of textural medical images. During the proposed method, a model is successively trained with massive amounts of natural images, some textural images, and the target images. We applied this method to the classification task of textural X-ray computed tomography images of diffuse lung diseases. In our experiment, the two-stage feature transfer achieves the best performance compared to a from-scratch learning and a conventional single-stage feature transfer. We also investigated the robustness of the target dataset, based on size. Two-stage feature transfer shows better robustness than the other two learning methods. Moreover, we analyzed the feature representations obtained from DLDs imagery inputs for each feature transfer models using a visualization method. We showed that the two-stage feature transfer obtains both edge and textural features of DLDs, which does not occur in conventional single-stage feature transfer models.



### Compressively Sensed Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/1810.06323v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.06323v1)
- **Published**: 2018-10-15 12:55:10+00:00
- **Updated**: 2018-10-15 12:55:10+00:00
- **Authors**: Aysen Degerli, Sinem Aslan, Mehmet Yamac, Bulent Sankur, Moncef Gabbouj
- **Comment**: 6 pages, submitted/accepted, EUVIP 2018
- **Journal**: None
- **Summary**: Compressive Sensing (CS) theory asserts that sparse signal reconstruction is possible from a small number of linear measurements. Although CS enables low-cost linear sampling, it requires non-linear and costly reconstruction. Recent literature works show that compressive image classification is possible in CS domain without reconstruction of the signal. In this work, we introduce a DCT base method that extracts binary discriminative features directly from CS measurements. These CS measurements can be obtained by using (i) a random or a pseudo-random measurement matrix, or (ii) a measurement matrix whose elements are learned from the training data to optimize the given classification task. We further introduce feature fusion by concatenating Bag of Words (BoW) representation of our binary features with one of the two state-of-the-art CNN-based feature vectors. We show that our fused feature outperforms the state-of-the-art in both cases.



### Deep Photovoltaic Nowcasting
- **Arxiv ID**: http://arxiv.org/abs/1810.06327v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.06327v1)
- **Published**: 2018-10-15 13:00:31+00:00
- **Updated**: 2018-10-15 13:00:31+00:00
- **Authors**: Jinsong Zhang, Rodrigo Verschae, Shohei Nobuhara, Jean-FranÃ§ois Lalonde
- **Comment**: 28 pages, 10 figure, 4 tables, preprint accepted to Solar Energy
- **Journal**: None
- **Summary**: Predicting the short-term power output of a photovoltaic panel is an important task for the efficient management of smart grids. Short-term forecasting at the minute scale, also known as nowcasting, can benefit from sky images captured by regular cameras and installed close to the solar panel. However, estimating the weather conditions from these images---sun intensity, cloud appearance and movement, etc.---is a very challenging task that the community has yet to solve with traditional computer vision techniques. In this work, we propose to learn the relationship between sky appearance and the future photovoltaic power output using deep learning. We train several variants of convolutional neural networks which take historical photovoltaic power values and sky images as input and estimate photovoltaic power in a very short term future. In particular, we compare three different architectures based on: a multi-layer perceptron (MLP), a convolutional neural network (CNN), and a long short term memory (LSTM) module. We evaluate our approach quantitatively on a dataset of photovoltaic power values and corresponding images gathered in Kyoto, Japan. Our experiments reveal that the MLP network, already used similarly in previous work, achieves an RMSE skill score of 7% over the commonly-used persistence baseline on the 1-minute future photovoltaic power prediction task. Our CNN-based network improves upon this with a 12% skill score. In contrast, our LSTM-based model, which can learn the temporal dependencies in the data, achieves a 21% RMSE skill score, thus outperforming all other approaches.



### Vehicle classification using ResNets, localisation and spatially-weighted pooling
- **Arxiv ID**: http://arxiv.org/abs/1810.10329v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.10329v1)
- **Published**: 2018-10-15 13:28:19+00:00
- **Updated**: 2018-10-15 13:28:19+00:00
- **Authors**: Rohan Watkins, Nick Pears, Suresh Manandhar
- **Comment**: 15 pages
- **Journal**: None
- **Summary**: We investigate whether ResNet architectures can outperform more traditional Convolutional Neural Networks on the task of fine-grained vehicle classification. We train and test ResNet-18, ResNet-34 and ResNet-50 on the Comprehensive Cars dataset without pre-training on other datasets. We then modify the networks to use Spatially Weighted Pooling. Finally, we add a localisation step before the classification process, using a network based on ResNet-50. We find that using Spatially Weighted Pooling and localisation both improve classification accuracy of ResNet50. Spatially Weighted Pooling increases accuracy by 1.5 percent points and localisation increases accuracy by 3.4 percent points. Using both increases accuracy by 3.7 percent points giving a top-1 accuracy of 96.351\% on the Comprehensive Cars dataset. Our method achieves higher accuracy than a range of methods including those that use traditional CNNs. However, our method does not perform quite as well as pre-trained networks that use Spatially Weighted Pooling.



### Real-Time Visual Tracking and Identification for a Team of Homogeneous Humanoid Robots
- **Arxiv ID**: http://arxiv.org/abs/1810.06411v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1810.06411v2)
- **Published**: 2018-10-15 14:45:29+00:00
- **Updated**: 2018-10-16 12:22:34+00:00
- **Authors**: Hafez Farazi, Sven Behnke
- **Comment**: 20th RoboCup International Symposium, Leipzig, Germany, 2016
- **Journal**: 20th RoboCup International Symposium, Leipzig, Germany, 2016
- **Summary**: The use of a team of humanoid robots to collaborate in completing a task is an increasingly important field of research. One of the challenges in achieving collaboration, is mutual identification and tracking of the robots. This work presents a real-time vision-based approach to the detection and tracking of robots of known appearance, based on the images captured by a stationary robot. A Histogram of Oriented Gradients descriptor is used to detect the robots and the robot headings are estimated by a multiclass classifier. The tracked robots report their own heading estimate from magnetometer readings. For tracking, a cost function based on position and heading is applied to each of the tracklets, and a globally optimal labeling of the detected robots is found using the Hungarian algorithm. The complete identification and tracking system was tested using two igus Humanoid Open Platform robots on a soccer field. We expect that a similar system can be used with other humanoid robots, such as Nao and DARwIn-OP



### Virtualization of tissue staining in digital pathology using an unsupervised deep learning approach
- **Arxiv ID**: http://arxiv.org/abs/1810.06415v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.06415v1)
- **Published**: 2018-10-15 14:45:53+00:00
- **Updated**: 2018-10-15 14:45:53+00:00
- **Authors**: Amal Lahiani, Jacob Gildenblat, Irina Klaman, Shadi Albarqouni, Nassir Navab, Eldad Klaiman
- **Comment**: None
- **Journal**: None
- **Summary**: Histopathological evaluation of tissue samples is a key practice in patient diagnosis and drug development, especially in oncology. Historically, Hematoxylin and Eosin (H&E) has been used by pathologists as a gold standard staining. However, in many cases, various target specific stains, including immunohistochemistry (IHC), are needed in order to highlight specific structures in the tissue. As tissue is scarce and staining procedures are tedious, it would be beneficial to generate images of stained tissue virtually. Virtual staining could also generate in-silico multiplexing of different stains on the same tissue segment. In this paper, we present a sample application that generates FAP-CK virtual IHC images from Ki67-CD8 real IHC images using an unsupervised deep learning approach based on CycleGAN. We also propose a method to deal with tiling artifacts caused by normalization layers and we validate our approach by comparing the results of tissue analysis algorithms for virtual and real images.



### Channel Splitting Network for Single MR Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1810.06453v3
- **DOI**: 10.1109/TIP.2019.2921882
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.06453v3)
- **Published**: 2018-10-15 15:15:16+00:00
- **Updated**: 2019-09-15 05:57:51+00:00
- **Authors**: Xiaole Zhao, Yulun Zhang, Tao Zhang, Xueming Zou
- **Comment**: 13 pages, 11 figures and 4 tables
- **Journal**: None
- **Summary**: High resolution magnetic resonance (MR) imaging is desirable in many clinical applications due to its contribution to more accurate subsequent analyses and early clinical diagnoses. Single image super resolution (SISR) is an effective and cost efficient alternative technique to improve the spatial resolution of MR images. In the past few years, SISR methods based on deep learning techniques, especially convolutional neural networks (CNNs), have achieved state-of-the-art performance on natural images. However, the information is gradually weakened and training becomes increasingly difficult as the network deepens. The problem is more serious for medical images because lacking high quality and effective training samples makes deep models prone to underfitting or overfitting. Nevertheless, many current models treat the hierarchical features on different channels equivalently, which is not helpful for the models to deal with the hierarchical features discriminatively and targetedly. To this end, we present a novel channel splitting network (CSN) to ease the representational burden of deep models. The proposed CSN model divides the hierarchical features into two branches, i.e., residual branch and dense branch, with different information transmissions. The residual branch is able to promote feature reuse, while the dense branch is beneficial to the exploration of new features. Besides, we also adopt the merge-and-run mapping to facilitate information integration between different branches. Extensive experiments on various MR images, including proton density (PD), T1 and T2 images, show that the proposed CSN model achieves superior performance over other state-of-the-art SISR methods.



### Refacing: reconstructing anonymized facial features using GANs
- **Arxiv ID**: http://arxiv.org/abs/1810.06455v2
- **DOI**: 10.1109/ISBI.2019.8759515
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.06455v2)
- **Published**: 2018-10-15 15:19:25+00:00
- **Updated**: 2019-01-21 12:33:38+00:00
- **Authors**: David Abramian, Anders Eklund
- **Comment**: None
- **Journal**: IEEE International Symposium on Biomedical Imaging, 2019
- **Summary**: Anonymization of medical images is necessary for protecting the identity of the test subjects, and is therefore an essential step in data sharing. However, recent developments in deep learning may raise the bar on the amount of distortion that needs to be applied to guarantee anonymity. To test such possibilities, we have applied the novel CycleGAN unsupervised image-to-image translation framework on sagittal slices of T1 MR images, in order to reconstruct facial features from anonymized data. We applied the CycleGAN framework on both face-blurred and face-removed images. Our results show that face blurring may not provide adequate protection against malicious attempts at identifying the subjects, while face removal provides more robust anonymization, but is still partially reversible.



### CSV: Image Quality Assessment Based on Color, Structure, and Visual System
- **Arxiv ID**: http://arxiv.org/abs/1810.06464v2
- **DOI**: 10.1016/j.image.2016.08.008
- **Categories**: **cs.CV**, eess.IV, I.4
- **Links**: [PDF](http://arxiv.org/pdf/1810.06464v2)
- **Published**: 2018-10-15 15:33:14+00:00
- **Updated**: 2018-11-13 15:36:27+00:00
- **Authors**: Dogancan Temel, Ghassan AlRegib
- **Comment**: 31 pages, 9 figures, 7 tables
- **Journal**: Signal Processing: Image Communication, Volume 48, 2016, Pages
  92-103, ISSN 0923-5965
- **Summary**: This paper presents a full-reference image quality estimator based on color, structure, and visual system characteristics denoted as CSV. In contrast to the majority of existing methods, we quantify perceptual color degradations rather than absolute pixel-wise changes. We use the CIEDE2000 color difference formulation to quantify low-level color degradations and the Earth Mover's Distance between color name descriptors to measure significant color degradations. In addition to the perceptual color difference, CSV also contains structural and perceptual differences. Structural feature maps are obtained by mean subtraction and divisive normalization, and perceptual feature maps are obtained from contrast sensitivity formulations of retinal ganglion cells. The proposed quality estimator CSV is tested on the LIVE, the Multiply Distorted LIVE, and the TID 2013 databases, and it is always among the top two performing quality estimators in terms of at least ranking, monotonic behavior or linearity.



### Unsupervised Deep Features for Remote Sensing Image Matching via Discriminator Network
- **Arxiv ID**: http://arxiv.org/abs/1810.06470v1
- **DOI**: 10.1109/TGRS.2019.2951820
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.06470v1)
- **Published**: 2018-10-15 15:39:44+00:00
- **Updated**: 2018-10-15 15:39:44+00:00
- **Authors**: Mohbat Tharani, Numan Khurshid, Murtaza Taj
- **Comment**: 13 Pages, 7 Figures
- **Journal**: None
- **Summary**: The advent of deep perceptual networks brought about a paradigm shift in machine vision and image perception. Image apprehension lately carried out by hand-crafted features in the latent space have been replaced by deep features acquired from supervised networks for improved understanding. However, such deep networks require strict supervision with a substantial amount of the labeled data for authentic training process. These methods perform poorly in domains lacking labeled data especially in case of remote sensing image retrieval. Resolving this, we propose an unsupervised encoder-decoder feature for remote sensing image matching (RSIM). Moreover, we replace the conventional distance metrics with a deep discriminator network to identify the similarity of the image pairs. To the best of our knowledge, discriminator network has never been used before for solving RSIM problem. Results have been validated with two publicly available benchmark remote sensing image datasets. The technique has also been investigated for content-based remote sensing image retrieval (CBRSIR); one of the widely used applications of RSIM. Results demonstrate that our technique supersedes the state-of-the-art methods used for unsupervised image matching with mean average precision (mAP) of 81%, and image retrieval with an overall improvement in mAP score of about 12%.



### SynSeg-Net: Synthetic Segmentation Without Target Modality Ground Truth
- **Arxiv ID**: http://arxiv.org/abs/1810.06498v2
- **DOI**: 10.1109/TMI.2018.2876633
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.06498v2)
- **Published**: 2018-10-15 16:23:08+00:00
- **Updated**: 2019-09-27 18:45:26+00:00
- **Authors**: Yuankai Huo, Zhoubing Xu, Hyeonsoo Moon, Shunxing Bao, Albert Assad, Tamara K. Moyo, Michael R. Savona, Richard G. Abramson, Bennett A. Landman
- **Comment**: IEEE Transactions on Medical Imaging (TMI)
- **Journal**: "Synseg-net: Synthetic segmentation without target modality ground
  truth." IEEE transactions on medical imaging 38, no. 4 (2018): 1016-1025
- **Summary**: A key limitation of deep convolutional neural networks (DCNN) based image segmentation methods is the lack of generalizability. Manually traced training images are typically required when segmenting organs in a new imaging modality or from distinct disease cohort. The manual efforts can be alleviated if the manually traced images in one imaging modality (e.g., MRI) are able to train a segmentation network for another imaging modality (e.g., CT). In this paper, we propose an end-to-end synthetic segmentation network (SynSeg-Net) to train a segmentation network for a target imaging modality without having manual labels. SynSeg-Net is trained by using (1) unpaired intensity images from source and target modalities, and (2) manual labels only from source modality. SynSeg-Net is enabled by the recent advances of cycle generative adversarial networks (CycleGAN) and DCNN. We evaluate the performance of the SynSeg-Net on two experiments: (1) MRI to CT splenomegaly synthetic segmentation for abdominal images, and (2) CT to MRI total intracranial volume synthetic segmentation (TICV) for brain images. The proposed end-to-end approach achieved superior performance to two stage methods. Moreover, the SynSeg-Net achieved comparable performance to the traditional segmentation network using target modality labels in certain scenarios. The source code of SynSeg-Net is publicly available (https://github.com/MASILab/SynSeg-Net).



### Deep Surface Light Fields
- **Arxiv ID**: http://arxiv.org/abs/1810.06514v1
- **DOI**: 10.1145/3203192
- **Categories**: **cs.CG**, cs.CV, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1810.06514v1)
- **Published**: 2018-10-15 16:56:58+00:00
- **Updated**: 2018-10-15 16:56:58+00:00
- **Authors**: Anpei Chen, Minye Wu, Yingliang Zhang, Nianyi Li, Jie Lu, Shenghua Gao, Jingyi Yu
- **Comment**: None
- **Journal**: None
- **Summary**: A surface light field represents the radiance of rays originating from any points on the surface in any directions. Traditional approaches require ultra-dense sampling to ensure the rendering quality. In this paper, we present a novel neural network based technique called deep surface light field or DSLF to use only moderate sampling for high fidelity rendering. DSLF automatically fills in the missing data by leveraging different sampling patterns across the vertices and at the same time eliminates redundancies due to the network's prediction capability. For real data, we address the image registration problem as well as conduct texture-aware remeshing for aligning texture edges with vertices to avoid blurring. Comprehensive experiments show that DSLF can further achieve high data compression ratio while facilitating real-time rendering on the GPU.



### Visual Semantic Navigation using Scene Priors
- **Arxiv ID**: http://arxiv.org/abs/1810.06543v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1810.06543v1)
- **Published**: 2018-10-15 17:45:02+00:00
- **Updated**: 2018-10-15 17:45:02+00:00
- **Authors**: Wei Yang, Xiaolong Wang, Ali Farhadi, Abhinav Gupta, Roozbeh Mottaghi
- **Comment**: None
- **Journal**: None
- **Summary**: How do humans navigate to target objects in novel scenes? Do we use the semantic/functional priors we have built over years to efficiently search and navigate? For example, to search for mugs, we search cabinets near the coffee machine and for fruits we try the fridge. In this work, we focus on incorporating semantic priors in the task of semantic navigation. We propose to use Graph Convolutional Networks for incorporating the prior knowledge into a deep reinforcement learning framework. The agent uses the features from the knowledge graph to predict the actions. For evaluation, we use the AI2-THOR framework. Our experiments show how semantic knowledge improves performance significantly. More importantly, we show improvement in generalization to unseen scenes and/or objects. The supplementary video can be accessed at the following link: https://youtu.be/otKjuO805dE .



### Deep Imitative Models for Flexible Inference, Planning, and Control
- **Arxiv ID**: http://arxiv.org/abs/1810.06544v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.06544v4)
- **Published**: 2018-10-15 17:51:03+00:00
- **Updated**: 2019-10-01 00:13:58+00:00
- **Authors**: Nicholas Rhinehart, Rowan McAllister, Sergey Levine
- **Comment**: None
- **Journal**: None
- **Summary**: Imitation Learning (IL) is an appealing approach to learn desirable autonomous behavior. However, directing IL to achieve arbitrary goals is difficult. In contrast, planning-based algorithms use dynamics models and reward functions to achieve goals. Yet, reward functions that evoke desirable behavior are often difficult to specify. In this paper, we propose Imitative Models to combine the benefits of IL and goal-directed planning. Imitative Models are probabilistic predictive models of desirable behavior able to plan interpretable expert-like trajectories to achieve specified goals. We derive families of flexible goal objectives, including constrained goal regions, unconstrained goal sets, and energy-based goals. We show that our method can use these objectives to successfully direct behavior. Our method substantially outperforms six IL approaches and a planning-based approach in a dynamic simulated autonomous driving task, and is efficiently learned from expert demonstrations without online data collection. We also show our approach is robust to poorly specified goals, such as goals on the wrong side of the road.



### Deep learning-based super-resolution in coherent imaging systems
- **Arxiv ID**: http://arxiv.org/abs/1810.06611v1
- **DOI**: 10.1038/s41598-019-40554-1
- **Categories**: **cs.CV**, cs.LG, physics.app-ph, physics.optics, 68T01, 68T05, 68U10, 62M45, 78M32, 92C55, 94A08, I.2; I.2.1; I.2.6; I.2.10; I.4.5; I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/1810.06611v1)
- **Published**: 2018-10-15 18:55:26+00:00
- **Updated**: 2018-10-15 18:55:26+00:00
- **Authors**: Tairan Liu, Kevin de Haan, Yair Rivenson, Zhensong Wei, Xin Zeng, Yibo Zhang, Aydogan Ozcan
- **Comment**: 18 pages, 9 figures, 3 tables
- **Journal**: Scientific Reports (2019)
- **Summary**: We present a deep learning framework based on a generative adversarial network (GAN) to perform super-resolution in coherent imaging systems. We demonstrate that this framework can enhance the resolution of both pixel size-limited and diffraction-limited coherent imaging systems. We experimentally validated the capabilities of this deep learning-based coherent imaging approach by super-resolving complex images acquired using a lensfree on-chip holographic microscope, the resolution of which was pixel size-limited. Using the same GAN-based approach, we also improved the resolution of a lens-based holographic imaging system that was limited in resolution by the numerical aperture of its objective lens. This deep learning-based super-resolution framework can be broadly applied to enhance the space-bandwidth product of coherent imaging systems using image data and convolutional neural networks, and provides a rapid, non-iterative method for solving inverse image reconstruction or enhancement problems in optics.



### Learning to Segment Corneal Tissue Interfaces in OCT Images
- **Arxiv ID**: http://arxiv.org/abs/1810.06612v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.06612v4)
- **Published**: 2018-10-15 18:56:07+00:00
- **Updated**: 2019-01-25 17:28:34+00:00
- **Authors**: Tejas Sudharshan Mathai, Kira Lathrop, John Galeotti
- **Comment**: Accepted to ISBI 2019. 5 pages. First version received by IEEE ISBI
  on 21st Sept 2018. This work has been submitted to the IEEE for possible
  publication. Copyright may be transferred without notice, after which this
  version may no longer be accessible
- **Journal**: None
- **Summary**: Accurate and repeatable delineation of corneal tissue interfaces is necessary for surgical planning during anterior segment interventions, such as Keratoplasty. Designing an approach to identify interfaces, which generalizes to datasets acquired from different Optical Coherence Tomographic (OCT) scanners, is paramount. In this paper, we present a Convolutional Neural Network (CNN) based framework called CorNet that can accurately segment three corneal interfaces across datasets obtained with different scan settings from different OCT scanners. Extensive validation of the approach was conducted across all imaged datasets. To the best of our knowledge, this is the first deep learning based approach to segment both anterior and posterior corneal tissue interfaces. Our errors are 2x lower than non-proprietary state-of-the-art corneal tissue interface segmentation algorithms, which include image analysis-based and deep learning approaches.



### Adversarial Inpainting of Medical Image Modalities
- **Arxiv ID**: http://arxiv.org/abs/1810.06621v1
- **DOI**: 10.1109/ICASSP.2019.8682677
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.06621v1)
- **Published**: 2018-10-15 19:14:16+00:00
- **Updated**: 2018-10-15 19:14:16+00:00
- **Authors**: Karim Armanious, Youssef Mecky, Sergios Gatidis, Bin Yang
- **Comment**: To be submitted to ICASSP 2019
- **Journal**: None
- **Summary**: Numerous factors could lead to partial deteriorations of medical images. For example, metallic implants will lead to localized perturbations in MRI scans. This will affect further post-processing tasks such as attenuation correction in PET/MRI or radiation therapy planning. In this work, we propose the inpainting of medical images via Generative Adversarial Networks (GANs). The proposed framework incorporates two patch-based discriminator networks with additional style and perceptual losses for the inpainting of missing information in realistically detailed and contextually consistent manner. The proposed framework outperformed other natural image inpainting techniques both qualitatively and quantitatively on two different medical modalities.



### UNIQUE: Unsupervised Image Quality Estimation
- **Arxiv ID**: http://arxiv.org/abs/1810.06631v2
- **DOI**: 10.1109/LSP.2016.2601119
- **Categories**: **cs.CV**, eess.IV, I.2; I.4; I.5
- **Links**: [PDF](http://arxiv.org/pdf/1810.06631v2)
- **Published**: 2018-10-15 19:36:34+00:00
- **Updated**: 2018-11-13 15:31:22+00:00
- **Authors**: D. Temel, M. Prabhushankar, G. AlRegib
- **Comment**: 12 pages, 5 figures, 2 tables
- **Journal**: D. Temel, M. Prabhushankar and G. AlRegib, "UNIQUE: Unsupervised
  Image Quality Estimation," in IEEE Signal Processing Letters, vol. 23, no.
  10, pp. 1414-1418, Oct. 2016
- **Summary**: In this paper, we estimate perceived image quality using sparse representations obtained from generic image databases through an unsupervised learning approach. A color space transformation, a mean subtraction, and a whitening operation are used to enhance descriptiveness of images by reducing spatial redundancy; a linear decoder is used to obtain sparse representations; and a thresholding stage is used to formulate suppression mechanisms in a visual system. A linear decoder is trained with 7 GB worth of data, which corresponds to 100,000 8x8 image patches randomly obtained from nearly 1,000 images in the ImageNet 2013 database. A patch-wise training approach is preferred to maintain local information. The proposed quality estimator UNIQUE is tested on the LIVE, the Multiply Distorted LIVE, and the TID 2013 databases and compared with thirteen quality estimators. Experimental results show that UNIQUE is generally a top performing quality estimator in terms of accuracy, consistency, linearity, and monotonic behavior.



### Lesion Focused Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1810.06693v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1810.06693v1)
- **Published**: 2018-10-15 21:02:24+00:00
- **Updated**: 2018-10-15 21:02:24+00:00
- **Authors**: Jin Zhu, Guang Yang, Pietro Lio
- **Comment**: 4 pages, 2 figures, 1 table, Accepted as Oral Presentation by the
  SPIE Medical Imaging Conference 2019
- **Journal**: None
- **Summary**: Super-resolution (SR) for image enhancement has great importance in medical image applications. Broadly speaking, there are two types of SR, one requires multiple low resolution (LR) images from different views of the same object to be reconstructed to the high resolution (HR) output, and the other one relies on the learning from a large amount of training datasets, i.e., LR-HR pairs. In real clinical environment, acquiring images from multi-views is expensive and sometimes infeasible. In this paper, we present a novel Generative Adversarial Networks (GAN) based learning framework to achieve SR from its LR version. By performing simulation based studies on the Multimodal Brain Tumor Segmentation Challenge (BraTS) datasets, we demonstrate the efficacy of our method in application of brain tumor MRI enhancement. Compared to bilinear interpolation and other state-of-the-art SR methods, our model is lesion focused, which is not only resulted in better perceptual image quality without blurring, but also more efficient and directly benefit for the following clinical tasks, e.g., lesion detection and abnormality enhancement. Therefore, we can envisage the application of our SR method to boost image spatial resolution while maintaining crucial diagnostic information for further clinical tasks.



### Multi-Stage Reinforcement Learning For Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1810.10325v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.10325v2)
- **Published**: 2018-10-15 21:41:57+00:00
- **Updated**: 2018-10-26 11:11:02+00:00
- **Authors**: Jonas Koenig, Simon Malberg, Martin Martens, Sebastian Niehaus, Artus Krohn-Grimberghe, Arunselvan Ramaswamy
- **Comment**: Accepted for the Computer Vision Conference (CVC) 2019
- **Journal**: None
- **Summary**: We present a reinforcement learning approach for detecting objects within an image. Our approach performs a step-wise deformation of a bounding box with the goal of tightly framing the object. It uses a hierarchical tree-like representation of predefined region candidates, which the agent can zoom in on. This reduces the number of region candidates that must be evaluated so that the agent can afford to compute new feature maps before each step to enhance detection quality. We compare an approach that is based purely on zoom actions with one that is extended by a second refinement stage to fine-tune the bounding box after each zoom step. We also improve the fitting ability by allowing for different aspect ratios of the bounding box. Finally, we propose different reward functions to lead to a better guidance of the agent while following its search trajectories. Experiments indicate that each of these extensions leads to more correct detections. The best performing approach comprises a zoom stage and a refinement stage, uses aspect-ratio modifying actions and is trained using a combination of three different reward metrics.



