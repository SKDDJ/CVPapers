# Arxiv Papers in cs.CV on 2018-10-03
### Image as Data: Automated Visual Content Analysis for Political Science
- **Arxiv ID**: http://arxiv.org/abs/1810.01544v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/1810.01544v1)
- **Published**: 2018-10-03 00:11:55+00:00
- **Updated**: 2018-10-03 00:11:55+00:00
- **Authors**: Jungseock Joo, Zachary C. Steinert-Threlkeld
- **Comment**: None
- **Journal**: None
- **Summary**: Image data provide unique information about political events, actors, and their interactions which are difficult to measure from or not available in text data. This article introduces a new class of automated methods based on computer vision and deep learning which can automatically analyze visual content data. Scholars have already recognized the importance of visual data and a variety of large visual datasets have become available. The lack of scalable analytic methods, however, has prevented from incorporating large scale image data in political analysis. This article aims to offer an in-depth overview of automated methods for visual content analysis and explains their usages and implementations. We further elaborate on how these methods and results can be validated and interpreted. We then discuss how these methods can contribute to the study of political communication, identity and politics, development, and conflict, by enabling a new set of research questions at scale.



### Performance Evaluation of SIFT Descriptor against Common Image Deformations on Iban Plaited Mat Motifs
- **Arxiv ID**: http://arxiv.org/abs/1810.01562v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.01562v1)
- **Published**: 2018-10-03 01:43:12+00:00
- **Updated**: 2018-10-03 01:43:12+00:00
- **Authors**: Silvia Joseph, Irwandi Hipiny, Hamimah Ujir
- **Comment**: 14th International Borneo Research Council Conference, 6 to 8 August
  2018, UNIMAS, Sarawak
- **Journal**: None
- **Summary**: Borneo indigenous communities are blessed with rich craft heritage. One such examples is the Iban's plaited mat craft. There have been many efforts by UNESCO and the Sarawak Government to preserve and promote the craft. One such method is by developing a mobile app capable of recognising the different mat motifs. As a first step towards this aim, we presents a novel image dataset consisting of seven mat motif classes. Each class possesses a unique variation of chevrons, diagonal shapes, symmetrical, repetitive, geometric and non geometric patterns. In this study, the performance of the Scale invariant feature transform (SIFT) descriptor is evaluated against five common image deformations, i.e., zoom and rotation, viewpoint, image blur, JPEG compression and illumination. Using our dataset, SIFT performed favourably with test sequences belonging to Illumination changes, Viewpoint changes, JPEG compression and Zoom and Rotation. However, it did not performed well with Image blur test sequences with an average of 1.61 percents retained pairwise matching after blurring with a Gaussian kernel of 8.0 radius.



### Assessing Performance of Aerobic Routines using Background Subtraction and Intersected Image Region
- **Arxiv ID**: http://arxiv.org/abs/1810.01564v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.01564v1)
- **Published**: 2018-10-03 02:04:15+00:00
- **Updated**: 2018-10-03 02:04:15+00:00
- **Authors**: Faustine John, Irwandi Hipiny, Hamimah Ujir, Mohd Shahrizal Sunar
- **Comment**: Presented at The International UNIMAS STEM Engineering Conference
  2018 (ENCON2018). Accepted for publication in MATEC Web of Conferences
- **Journal**: None
- **Summary**: It is recommended for a novice to engage a trained and experience person, i.e., a coach before starting an unfamiliar aerobic or weight routine. The coach's task is to provide real-time feedbacks to ensure that the routine is performed in a correct manner. This greatly reduces the risk of injury and maximise physical gains. We present a simple image similarity measure based on intersected image region to assess a subject's performance of an aerobic routine. The method is implemented inside an Augmented Reality (AR) desktop app that employs a single RGB camera to capture still images of the subject as he or she progresses through the routine. The background-subtracted body pose image is compared against the exemplar body pose image (i.e., AR template) at specific intervals. Based on a limited dataset, our pose matching function is reported to have an accuracy of 93.67%.



### Relative Saliency and Ranking: Models, Metrics, Data, and Benchmarks
- **Arxiv ID**: http://arxiv.org/abs/1810.02426v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.02426v2)
- **Published**: 2018-10-03 03:18:20+00:00
- **Updated**: 2019-09-13 02:20:44+00:00
- **Authors**: Mahmoud Kalash, Md Amirul Islam, Neil D. B. Bruce
- **Comment**: Accepted to Transaction on Pattern Analysis and Machine Intelligence.
  arXiv admin note: substantial text overlap with arXiv:1803.05082
- **Journal**: None
- **Summary**: Salient object detection is a problem that has been considered in detail and \textcolor{black}{many solutions have been proposed}. In this paper, we argue that work to date has addressed a problem that is relatively ill-posed. Specifically, there is not universal agreement about what constitutes a salient object when multiple observers are queried. This implies that some objects are more likely to be judged salient than others, and implies a relative rank exists on salient objects. Initially, we present a novel deep learning solution based on a hierarchical representation of relative saliency and stage-wise refinement. Further to this, we present data, analysis and baseline benchmark results towards addressing the problem of salient object ranking. Methods for deriving suitable ranked salient object instances are presented, along with metrics suitable to measuring algorithm performance. In addition, we show how a derived dataset can be successively refined to provide cleaned results that correlate well with pristine ground truth in its characteristics and value for training and testing models. Finally, we provide a comparison among prevailing algorithms that address salient object ranking or detection to establish initial baselines providing a basis for comparison with future efforts addressing this problem. \textcolor{black}{The source code and data are publicly available via our project page:} \textrm{\href{https://ryersonvisionlab.github.io/cocosalrank.html}{ryersonvisionlab.github.io/cocosalrank}}



### Deep Fundamental Matrix Estimation without Correspondences
- **Arxiv ID**: http://arxiv.org/abs/1810.01575v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG, cs.GR, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.01575v1)
- **Published**: 2018-10-03 03:59:15+00:00
- **Updated**: 2018-10-03 03:59:15+00:00
- **Authors**: Omid Poursaeed, Guandao Yang, Aditya Prakash, Qiuren Fang, Hanqing Jiang, Bharath Hariharan, Serge Belongie
- **Comment**: ECCV 2018, Geometry Meets Deep Learning Workshop
- **Journal**: None
- **Summary**: Estimating fundamental matrices is a classic problem in computer vision. Traditional methods rely heavily on the correctness of estimated key-point correspondences, which can be noisy and unreliable. As a result, it is difficult for these methods to handle image pairs with large occlusion or significantly different camera poses. In this paper, we propose novel neural network architectures to estimate fundamental matrices in an end-to-end manner without relying on point correspondences. New modules and layers are introduced in order to preserve mathematical properties of the fundamental matrix as a homogeneous rank-2 matrix with seven degrees of freedom. We analyze performance of the proposed models using various metrics on the KITTI dataset, and show that they achieve competitive performance with traditional methods without the need for extracting correspondences.



### Primitive Fitting Using Deep Boundary Aware Geometric Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1810.01604v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.01604v1)
- **Published**: 2018-10-03 06:50:51+00:00
- **Updated**: 2018-10-03 06:50:51+00:00
- **Authors**: Duanshun Li, Chen Feng
- **Comment**: None
- **Journal**: None
- **Summary**: To identify and fit geometric primitives (e.g., planes, spheres, cylinders, cones) in a noisy point cloud is a challenging yet beneficial task for fields such as robotics and reverse engineering. As a multi-model multi-instance fitting problem, it has been tackled with different approaches including RANSAC, which however often fit inferior models in practice with noisy inputs of cluttered scenes. Inspired by the corresponding human recognition process, and benefiting from the recent advancements in image semantic segmentation using deep neural networks, we propose BAGSFit as a new framework addressing this problem. Firstly, through a fully convolutional neural network, the input point cloud is point-wisely segmented into multiple classes divided by jointly detected instance boundaries without any geometric fitting. Thus, segments can serve as primitive hypotheses with a probability estimation of associating primitive classes. Finally, all hypotheses are sent through a geometric verification to correct any misclassification by fitting primitives respectively. We performed training using simulated range images and tested it with both simulated and real-world point clouds. Quantitative and qualitative experiments demonstrated the superiority of BAGSFit.



### Cascaded Pyramid Network for 3D Human Pose Estimation Challenge
- **Arxiv ID**: http://arxiv.org/abs/1810.01616v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.01616v1)
- **Published**: 2018-10-03 07:58:18+00:00
- **Updated**: 2018-10-03 07:58:18+00:00
- **Authors**: Sungeun Hong, Wonjin Jung, Ilsang Woo, Seung Wook Kim
- **Comment**: Accepted to ECCV Workshop 2018
- **Journal**: None
- **Summary**: Over the past decade, there has been a growing interest in human pose estimation. Although much work has been done on 2D pose estimation, 3D pose estimation has still been relatively studied less. In this paper, we propose a top-bottom based two-stage 3D estimation framework. GloabalNet and RefineNet in our 2D pose estimation process enable us to find occluded or invisible 2D joints while 2D-to-3D pose estimator composed of residual blocks is used to lift 2D joints to 3D joints effectively. The proposed method achieves promising results with mean per joint position error at 42.39 on the validation dataset on `3D Human Pose Estimation within the ECCV 2018 PoseTrack Challenge.'



### Towards WARSHIP: Combining Components of Brain-Inspired Computing of RSH for Image Super Resolution
- **Arxiv ID**: http://arxiv.org/abs/1810.01620v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1810.01620v1)
- **Published**: 2018-10-03 08:10:03+00:00
- **Updated**: 2018-10-03 08:10:03+00:00
- **Authors**: Wendi Xu, Ming Zhang
- **Comment**: 2018 5th IEEE International Conference on Cloud Computing and
  Intelligence Systems
- **Journal**: None
- **Summary**: Evolution of deep learning shows that some algorithmic tricks are more durable , while others are not. To the best of our knowledge, we firstly summarize 5 more durable and complete deep learning components for vision, that is, WARSHIP. Moreover, we give a biological overview of WARSHIP, emphasizing brain-inspired computing of WARSHIP. As a step towards WARSHIP, our case study of image super resolution combines 3 components of RSH to deploy a CNN model of WARSHIP-XZNet, which performs a happy medium between speed and performance.



### Extreme Augmentation : Can deep learning based medical image segmentation be trained using a single manually delineated scan?
- **Arxiv ID**: http://arxiv.org/abs/1810.01621v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.01621v3)
- **Published**: 2018-10-03 08:10:35+00:00
- **Updated**: 2019-09-06 22:45:17+00:00
- **Authors**: Bilwaj Gaonkar, Matthew Edwards, Alex Bui, Matthew Brown, Luke Macyszyn
- **Comment**: None
- **Journal**: None
- **Summary**: Yes, it can. Data augmentation is perhaps the oldest preprocessing step in computer vision literature. Almost every computer vision model trained on imaging data uses some form of augmentation. In this paper, we use the inter-vertebral disk segmentation task alongside a deep residual U-Net as the learning model, to explore the effectiveness of augmentation. In the extreme, we observed that a model trained on patches extracted from just one scan, with each patch augmented 50 times; achieved a Dice score of 0.73 in a validation set of 40 cases. Qualitative evaluation indicated a clinically usable segmentation algorithm, which appropriately segments regions of interest, alongside limited false positive specks. When the initial patches are extracted from nine scans the average Dice coefficient jumps to 0.86 and most of the false positives disappear. While this still falls short of state-of-the-art deep learning based segmentation of discs reported in literature, qualitative examination reveals that it does yield segmentation, which can be amended by expert clinicians with minimal effort to generate additional data for training improved deep models. Extreme augmentation of training data, should thus be construed as a strategy for training deep learning based algorithms, when very little manually annotated data is available to work with. Models trained with extreme augmentation can then be used to accelerate the generation of manually labelled data. Hence, we show that extreme augmentation can be a valuable tool in addressing scaling up small imaging data sets to address medical image segmentation tasks.



### Theory of Generative Deep Learning : Probe Landscape of Empirical Error via Norm Based Capacity Control
- **Arxiv ID**: http://arxiv.org/abs/1810.01622v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1810.01622v1)
- **Published**: 2018-10-03 08:10:51+00:00
- **Updated**: 2018-10-03 08:10:51+00:00
- **Authors**: Wendi Xu, Ming Zhang
- **Comment**: 2018 5th IEEE International Conference on Cloud Computing and
  Intelligence Systems
- **Journal**: None
- **Summary**: Despite its remarkable empirical success as a highly competitive branch of artificial intelligence, deep learning is often blamed for its widely known low interpretation and lack of firm and rigorous mathematical foundation. However, most theoretical endeavor is devoted in discriminative deep learning case, whose complementary part is generative deep learning. To the best of our knowledge, we firstly highlight landscape of empirical error in generative case to complete the full picture through exquisite design of image super resolution under norm based capacity control. Our theoretical advance in interpretation of the training dynamic is achieved from both mathematical and biological sides.



### Optimization Algorithm Inspired Deep Neural Network Structure Design
- **Arxiv ID**: http://arxiv.org/abs/1810.01638v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1810.01638v1)
- **Published**: 2018-10-03 08:59:41+00:00
- **Updated**: 2018-10-03 08:59:41+00:00
- **Authors**: Huan Li, Yibo Yang, Dongmin Chen, Zhouchen Lin
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks have been one of the dominant machine learning approaches in recent years. Several new network structures are proposed and have better performance than the traditional feedforward neural network structure. Representative ones include the skip connection structure in ResNet and the dense connection structure in DenseNet. However, it still lacks a unified guidance for the neural network structure design. In this paper, we propose the hypothesis that the neural network structure design can be inspired by optimization algorithms and a faster optimization algorithm may lead to a better neural network structure. Specifically, we prove that the propagation in the feedforward neural network with the same linear transformation in different layers is equivalent to minimizing some function using the gradient descent algorithm. Based on this observation, we replace the gradient descent algorithm with the heavy ball algorithm and Nesterov's accelerated gradient descent algorithm, which are faster and inspire us to design new and better network structures. ResNet and DenseNet can be considered as two special cases of our framework. Numerical experiments on CIFAR-10, CIFAR-100 and ImageNet verify the advantage of our optimization algorithm inspired structures over ResNet and DenseNet.



### PIRM Challenge on Perceptual Image Enhancement on Smartphones: Report
- **Arxiv ID**: http://arxiv.org/abs/1810.01641v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.01641v1)
- **Published**: 2018-10-03 09:07:28+00:00
- **Updated**: 2018-10-03 09:07:28+00:00
- **Authors**: Andrey Ignatov, Radu Timofte, Thang Van Vu, Tung Minh Luu, Trung X Pham, Cao Van Nguyen, Yongwoo Kim, Jae-Seok Choi, Munchurl Kim, Jie Huang, Jiewen Ran, Chen Xing, Xingguang Zhou, Pengfei Zhu, Mingrui Geng, Yawei Li, Eirikur Agustsson, Shuhang Gu, Luc Van Gool, Etienne de Stoutz, Nikolay Kobyshev, Kehui Nie, Yan Zhao, Gen Li, Tong Tong, Qinquan Gao, Liu Hanwen, Pablo Navarrete Michelini, Zhu Dan, Hu Fengshuo, Zheng Hui, Xiumei Wang, Lirui Deng, Rang Meng, Jinghui Qin, Yukai Shi, Wushao Wen, Liang Lin, Ruicheng Feng, Shixiang Wu, Chao Dong, Yu Qiao, Subeesh Vasu, Nimisha Thekke Madam, Praveen Kandula, A. N. Rajagopalan, Jie Liu, Cheolkon Jung
- **Comment**: None
- **Journal**: None
- **Summary**: This paper reviews the first challenge on efficient perceptual image enhancement with the focus on deploying deep learning models on smartphones. The challenge consisted of two tracks. In the first one, participants were solving the classical image super-resolution problem with a bicubic downscaling factor of 4. The second track was aimed at real-world photo enhancement, and the goal was to map low-quality photos from the iPhone 3GS device to the same photos captured with a DSLR camera. The target metric used in this challenge combined the runtime, PSNR scores and solutions' perceptual results measured in the user study. To ensure the efficiency of the submitted models, we additionally measured their runtime and memory requirements on Android smartphones. The proposed solutions significantly improved baseline results defining the state-of-the-art for image enhancement on smartphones.



### A Brief Survey on Autonomous Vehicle Possible Attacks, Exploits and Vulnerabilities
- **Arxiv ID**: http://arxiv.org/abs/1810.04144v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1810.04144v1)
- **Published**: 2018-10-03 09:31:14+00:00
- **Updated**: 2018-10-03 09:31:14+00:00
- **Authors**: Amara Dinesh Kumar, Koti Naga Renu Chebrolu, Vinayakumar R, Soman KP
- **Comment**: 5 Pages,1 Figure
- **Journal**: None
- **Summary**: Advanced driver assistance systems are advancing at a rapid pace and all major companies started investing in developing the autonomous vehicles. But the security and reliability is still uncertain and debatable. Imagine that a vehicle is compromised by the attackers and then what they can do. An attacker can control brake, accelerate and even steering which can lead to catastrophic consequences. This paper gives a very short and brief overview of most of the possible attacks on autonomous vehicle software and hardware and their potential implications.



### DeepImageSpam: Deep Learning based Image Spam Detection
- **Arxiv ID**: http://arxiv.org/abs/1810.03977v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/1810.03977v1)
- **Published**: 2018-10-03 09:35:01+00:00
- **Updated**: 2018-10-03 09:35:01+00:00
- **Authors**: Amara Dinesh Kumar, Vinayakumar R, Soman KP
- **Comment**: 4 pages
- **Journal**: None
- **Summary**: Hackers and spammers are employing innovative and novel techniques to deceive novice and even knowledgeable internet users. Image spam is one of such technique where the spammer varies and changes some portion of the image such that it is indistinguishable from the original image fooling the users. This paper proposes a deep learning based approach for image spam detection using the convolutional neural networks which uses a dataset with 810 natural images and 928 spam images for classification achieving an accuracy of 91.7% outperforming the existing image processing and machine learning techniques



### A Robot Localization Framework Using CNNs for Object Detection and Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1810.01665v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1810.01665v1)
- **Published**: 2018-10-03 09:58:21+00:00
- **Updated**: 2018-10-03 09:58:21+00:00
- **Authors**: Lukas Hoyer, Christoph Steup, Sanaz Mostaghim
- **Comment**: None
- **Journal**: None
- **Summary**: External localization is an essential part for the indoor operation of small or cost-efficient robots, as they are used, for example, in swarm robotics. We introduce a two-stage localization and instance identification framework for arbitrary robots based on convolutional neural networks. Object detection is performed on an external camera image of the operation zone providing robot bounding boxes for an identification and orientation estimation convolutional neural network. Additionally, we propose a process to generate the necessary training data. The framework was evaluated with 3 different robot types and various identification patterns. We have analyzed the main framework hyperparameters providing recommendations for the framework operation settings. We achieved up to 98% mAP@IOU0.5 and only 1.6{\deg} orientation error, running with a frame rate of 50 Hz on a GPU.



### A Comprehensive Approach for Learning-based Fully-Automated Inter-slice Motion Correction for Short-Axis Cine Cardiac MR Image Stacks
- **Arxiv ID**: http://arxiv.org/abs/1810.02201v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.02201v1)
- **Published**: 2018-10-03 11:40:07+00:00
- **Updated**: 2018-10-03 11:40:07+00:00
- **Authors**: Giacomo Tarroni, Ozan Oktay, Matthew Sinclair, Wenjia Bai, Andreas Schuh, Hideaki Suzuki, Antonio de Marvao, Declan O'Regan, Stuart Cook, Daniel Rueckert
- **Comment**: Accepted for publication at MICCAI 2018. arXiv admin note: text
  overlap with arXiv:1803.09354
- **Journal**: None
- **Summary**: In the clinical routine, short axis (SA) cine cardiac MR (CMR) image stacks are acquired during multiple subsequent breath-holds. If the patient cannot consistently hold the breath at the same position, the acquired image stack will be affected by inter-slice respiratory motion and will not correctly represent the cardiac volume, introducing potential errors in the following analyses and visualisations. We propose an approach to automatically correct inter-slice respiratory motion in SA CMR image stacks. Our approach makes use of probabilistic segmentation maps (PSMs) of the left ventricular (LV) cavity generated with decision forests. PSMs are generated for each slice of the SA stack and rigidly registered in-plane to a target PSM. If long axis (LA) images are available, PSMs are generated for them and combined to create the target PSM; if not, the target PSM is produced from the same stack using a 3D model trained from motion-free stacks. The proposed approach was tested on a dataset of SA stacks acquired from 24 healthy subjects (for which anatomical 3D cardiac images were also available as reference) and compared to two techniques which use LA intensity images and LA segmentations as targets, respectively. The results show the accuracy and robustness of the proposed approach in motion compensation.



### Set Aggregation Network as a Trainable Pooling Layer
- **Arxiv ID**: http://arxiv.org/abs/1810.01868v3
- **DOI**: 10.1007/978-3-030-36711-4_35
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.01868v3)
- **Published**: 2018-10-03 13:20:13+00:00
- **Updated**: 2019-11-25 10:25:02+00:00
- **Authors**: Łukasz Maziarka, Marek Śmieja, Aleksandra Nowak, Jacek Tabor, Łukasz Struski, Przemysław Spurek
- **Comment**: ICONIP 2019
- **Journal**: Neural Information Processing. ICONIP 2019
- **Summary**: Global pooling, such as max- or sum-pooling, is one of the key ingredients in deep neural networks used for processing images, texts, graphs and other types of structured data. Based on the recent DeepSets architecture proposed by Zaheer et al. (NIPS 2017), we introduce a Set Aggregation Network (SAN) as an alternative global pooling layer. In contrast to typical pooling operators, SAN allows to embed a given set of features to a vector representation of arbitrary size. We show that by adjusting the size of embedding, SAN is capable of preserving the whole information from the input. In experiments, we demonstrate that replacing global pooling layer by SAN leads to the improvement of classification accuracy. Moreover, it is less prone to overfitting and can be used as a regularizer.



### 2018 Low-Power Image Recognition Challenge
- **Arxiv ID**: http://arxiv.org/abs/1810.01732v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.01732v1)
- **Published**: 2018-10-03 13:34:52+00:00
- **Updated**: 2018-10-03 13:34:52+00:00
- **Authors**: Sergei Alyamkin, Matthew Ardi, Achille Brighton, Alexander C. Berg, Yiran Chen, Hsin-Pai Cheng, Bo Chen, Zichen Fan, Chen Feng, Bo Fu, Kent Gauen, Jongkook Go, Alexander Goncharenko, Xuyang Guo, Hong Hanh Nguyen, Andrew Howard, Yuanjun Huang, Donghyun Kang, Jaeyoun Kim, Alexander Kondratyev, Seungjae Lee, Suwoong Lee, Junhyeok Lee, Zhiyu Liang, Xin Liu, Juzheng Liu, Zichao Li, Yang Lu, Yung-Hsiang Lu, Deeptanshu Malik, Eunbyung Park, Denis Repin, Tao Sheng, Liang Shen, Fei Sun, David Svitov, George K. Thiruvathukal, Baiwu Zhang, Jingchi Zhang, Xiaopeng Zhang, Shaojie Zhuo
- **Comment**: 13 pages, workshop in 2018 CVPR, competition, low-power, image
  recognition
- **Journal**: None
- **Summary**: The Low-Power Image Recognition Challenge (LPIRC, https://rebootingcomputing.ieee.org/lpirc) is an annual competition started in 2015. The competition identifies the best technologies that can classify and detect objects in images efficiently (short execution time and low energy consumption) and accurately (high precision). Over the four years, the winners' scores have improved more than 24 times. As computer vision is widely used in many battery-powered systems (such as drones and mobile phones), the need for low-power computer vision will become increasingly important. This paper summarizes LPIRC 2018 by describing the three different tracks and the winners' solutions.



### A deep learning pipeline for product recognition on store shelves
- **Arxiv ID**: http://arxiv.org/abs/1810.01733v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.01733v3)
- **Published**: 2018-10-03 13:36:26+00:00
- **Updated**: 2019-01-27 08:52:17+00:00
- **Authors**: Alessio Tonioni, Eugenio Serra, Luigi Di Stefano
- **Comment**: None
- **Journal**: None
- **Summary**: Recognition of grocery products in store shelves poses peculiar challenges. Firstly, the task mandates the recognition of an extremely high number of different items, in the order of several thousands for medium-small shops, with many of them featuring small inter and intra class variability. Then, available product databases usually include just one or a few studio-quality images per product (referred to herein as reference images), whilst at test time recognition is performed on pictures displaying a portion of a shelf containing several products and taken in the store by cheap cameras (referred to as query images). Moreover, as the items on sale in a store as well as their appearance change frequently over time, a practical recognition system should handle seamlessly new products/packages. Inspired by recent advances in object detection and image retrieval, we propose to leverage on state of the art object detectors based on deep learning to obtain an initial productagnostic item detection. Then, we pursue product recognition through a similarity search between global descriptors computed on reference and cropped query images. To maximize performance, we learn an ad-hoc global descriptor by a CNN trained on reference images based on an image embedding loss. Our system is computationally expensive at training time but can perform recognition rapidly and accurately at test time.



### SAVOIAS: A Diverse, Multi-Category Visual Complexity Dataset
- **Arxiv ID**: http://arxiv.org/abs/1810.01771v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.01771v1)
- **Published**: 2018-10-03 14:34:37+00:00
- **Updated**: 2018-10-03 14:34:37+00:00
- **Authors**: Elham Saraee, Mona Jalal, Margrit Betke
- **Comment**: 10 pages, 4 figures, 4 tables
- **Journal**: None
- **Summary**: Visual complexity identifies the level of intricacy and details in an image or the level of difficulty to describe the image. It is an important concept in a variety of areas such as cognitive psychology, computer vision and visualization, and advertisement. Yet, efforts to create large, downloadable image datasets with diverse content and unbiased groundtruthing are lacking. In this work, we introduce Savoias, a visual complexity dataset that compromises of more than 1,400 images from seven image categories relevant to the above research areas, namely Scenes, Advertisements, Visualization and infographics, Objects, Interior design, Art, and Suprematism. The images in each category portray diverse characteristics including various low-level and high-level features, objects, backgrounds, textures and patterns, text, and graphics. The ground truth for Savoias is obtained by crowdsourcing more than 37,000 pairwise comparisons of images using the forced-choice methodology and with more than 1,600 contributors. The resulting relative scores are then converted to absolute visual complexity scores using the Bradley-Terry method and matrix completion. When applying five state-of-the-art algorithms to analyze the visual complexity of the images in the Savoias dataset, we found that the scores obtained from these baseline tools only correlate well with crowdsourced labels for abstract patterns in the Suprematism category (Pearson correlation r=0.84). For the other categories, in particular, the objects and advertisement categories, low correlation coefficients were revealed (r=0.3 and 0.56, respectively). These findings suggest that (1) state-of-the-art approaches are mostly insufficient and (2) Savoias enables category-specific method development, which is likely to improve the impact of visual complexity analysis on specific application areas, including computer vision.



### SAM-GCNN: A Gated Convolutional Neural Network with Segment-Level Attention Mechanism for Home Activity Monitoring
- **Arxiv ID**: http://arxiv.org/abs/1810.03986v2
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1810.03986v2)
- **Published**: 2018-10-03 14:55:32+00:00
- **Updated**: 2018-11-14 09:42:40+00:00
- **Authors**: Yu-Han Shen, Ke-Xin He, Wei-Qiang Zhang
- **Comment**: 6 pages, accepted by ISSPIT 2018
- **Journal**: None
- **Summary**: In this paper, we propose a method for home activity monitoring. We demonstrate our model on dataset of Detection and Classification of Acoustic Scenes and Events (DCASE) 2018 Challenge Task 5. This task aims to classify multi-channel audios into one of the provided pre-defined classes. All of these classes are daily activities performed in a home environment. To tackle this task, we propose a gated convolutional neural network with segment-level attention mechanism (SAM-GCNN). The proposed framework is a convolutional model with two auxiliary modules: a gated convolutional neural network and a segment-level attention mechanism. Furthermore, we adopted model ensemble to enhance the capability of generalization of our model. We evaluated our work on the development dataset of DCASE 2018 Task 5 and achieved competitive performance, with a macro-averaged F-1 score increasing from 83.76% to 89.33%, compared with the convolutional baseline system.



### On the Evaluation and Validation of Off-the-shelf Statistical Shape Modeling Tools: A Clinical Application
- **Arxiv ID**: http://arxiv.org/abs/1810.03987v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.03987v1)
- **Published**: 2018-10-03 15:37:40+00:00
- **Updated**: 2018-10-03 15:37:40+00:00
- **Authors**: Anupama Goparaju, Ibolya Csecs, Alan Morris, Evgueni Kholmovski, Nassir Marrouche, Ross Whitaker, Shireen Elhabian
- **Comment**: To Appear: ShapeMI Workshop: Workshop on Shape in Medical Imaging
- **Journal**: None
- **Summary**: Statistical shape modeling (SSM) has proven useful in many areas of biology and medicine as a new generation of morphometric approaches for the quantitative analysis of anatomical shapes. Recently, the increased availability of high-resolution in vivo images of anatomy has led to the development and distribution of open-source computational tools to model anatomical shapes and their variability within populations with unprecedented detail and statistical power. Nonetheless, there is little work on the evaluation and validation of such tools as related to clinical applications that rely on morphometric quantifications for treatment planning. To address this lack of validation, we systematically assess the outcome of widely used off-the-shelf SSM tools, namely ShapeWorks, SPHARM-PDM, and Deformetrica, in the context of designing closure devices for left atrium appendage (LAA) in atrial fibrillation (AF) patients to prevent stroke, where an incomplete LAA closure may be worse than no closure. This study is motivated by the potential role of SSM in the geometric design of closure devices, which could be informed by population-level statistics, and patient-specific device selection, which is driven by anatomical measurements that could be automated by relating patient-level anatomy to population-level morphometrics. Hence, understanding the consequences of different SSM tools for the final analysis is critical for the careful choice of the tool to be deployed in real clinical scenarios. Results demonstrate that estimated measurements from ShapeWorks model are more consistent compared to models from Deformetrica and SPHARM-PDM. Furthermore, ShapeWorks and Deformetrica shape models capture clinically relevant population-level variability compared to SPHARM-PDM models.



### Weighted Sigmoid Gate Unit for an Activation Function of Deep Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1810.01829v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.01829v1)
- **Published**: 2018-10-03 16:26:24+00:00
- **Updated**: 2018-10-03 16:26:24+00:00
- **Authors**: Masayuki Tanaka
- **Comment**: None
- **Journal**: None
- **Summary**: An activation function has crucial role in a deep neural network.   A simple rectified linear unit (ReLU) are widely used for the activation function.   In this paper, a weighted sigmoid gate unit (WiG) is proposed as the activation function.   The proposed WiG consists of a multiplication of inputs and the weighted sigmoid gate.   It is shown that the WiG includes the ReLU and same activation functions as a special case.   Many activation functions have been proposed to overcome the performance of the ReLU.   In the literature, the performance is mainly evaluated with an object recognition task.   The proposed WiG is evaluated with the object recognition task and the image restoration task.   Then, the expeirmental comparisons demonstrate the proposed WiG overcomes the existing activation functions including the ReLU.



### An Effective Single-Image Super-Resolution Model Using Squeeze-and-Excitation Networks
- **Arxiv ID**: http://arxiv.org/abs/1810.01831v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.01831v1)
- **Published**: 2018-10-03 16:29:37+00:00
- **Updated**: 2018-10-03 16:29:37+00:00
- **Authors**: Kangfu Mei, Aiwen Jiang, Juncheng Li, Jihua Ye, Mingwen Wang
- **Comment**: 12 pages, accepted by ICONIP2018
- **Journal**: None
- **Summary**: Recent works on single-image super-resolution are concentrated on improving performance through enhancing spatial encoding between convolutional layers. In this paper, we focus on modeling the correlations between channels of convolutional features. We present an effective deep residual network based on squeeze-and-excitation blocks (SEBlock) to reconstruct high-resolution (HR) image from low-resolution (LR) image. SEBlock is used to adaptively recalibrate channel-wise feature mappings. Further, short connections between each SEBlock are used to remedy information loss. Extensive experiments show that our model can achieve the state-of-the-art performance and get finer texture details.



### Human-Centered Autonomous Vehicle Systems: Principles of Effective Shared Autonomy
- **Arxiv ID**: http://arxiv.org/abs/1810.01835v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.HC, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1810.01835v1)
- **Published**: 2018-10-03 16:36:22+00:00
- **Updated**: 2018-10-03 16:36:22+00:00
- **Authors**: Lex Fridman
- **Comment**: None
- **Journal**: None
- **Summary**: Building effective, enjoyable, and safe autonomous vehicles is a lot harder than has historically been considered. The reason is that, simply put, an autonomous vehicle must interact with human beings. This interaction is not a robotics problem nor a machine learning problem nor a psychology problem nor an economics problem nor a policy problem. It is all of these problems put into one. It challenges our assumptions about the limitations of human beings at their worst and the capabilities of artificial intelligence systems at their best. This work proposes a set of principles for designing and building autonomous vehicles in a human-centered way that does not run away from the complexity of human nature but instead embraces it. We describe our development of the Human-Centered Autonomous Vehicle (HCAV) as an illustrative case study of implementing these principles in practice.



### Task-Oriented Hand Motion Retargeting for Dexterous Manipulation Imitation
- **Arxiv ID**: http://arxiv.org/abs/1810.01845v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.01845v1)
- **Published**: 2018-10-03 17:14:05+00:00
- **Updated**: 2018-10-03 17:14:05+00:00
- **Authors**: Dafni Antotsiou, Guillermo Garcia-Hernando, Tae-Kyun Kim
- **Comment**: ECCV 2018 workshop paper
- **Journal**: None
- **Summary**: Human hand actions are quite complex, especially when they involve object manipulation, mainly due to the high dimensionality of the hand and the vast action space that entails. Imitating those actions with dexterous hand models involves different important and challenging steps: acquiring human hand information, retargeting it to a hand model, and learning a policy from acquired data. In this work, we capture the hand information by using a state-of-the-art hand pose estimator. We tackle the retargeting problem from the hand pose to a 29 DoF hand model by combining inverse kinematics and PSO with a task objective optimisation. This objective encourages the virtual hand to accomplish the manipulation task, relieving the effect of the estimator's noise and the domain gap. Our approach leads to a better success rate in the grasping task compared to our inverse kinematics baseline, allowing us to record successful human demonstrations. Furthermore, we used these demonstrations to learn a policy network using generative adversarial imitation learning (GAIL) that is able to autonomously grasp an object in the virtual space.



### SuperDepth: Self-Supervised, Super-Resolved Monocular Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/1810.01849v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1810.01849v1)
- **Published**: 2018-10-03 17:24:06+00:00
- **Updated**: 2018-10-03 17:24:06+00:00
- **Authors**: Sudeep Pillai, Rares Ambrus, Adrien Gaidon
- **Comment**: 6 pages, 5 figures, 2 tables, ICRA 2019 Submission
- **Journal**: None
- **Summary**: Recent techniques in self-supervised monocular depth estimation are approaching the performance of supervised methods, but operate in low resolution only. We show that high resolution is key towards high-fidelity self-supervised monocular depth prediction. Inspired by recent deep learning methods for Single-Image Super-Resolution, we propose a sub-pixel convolutional layer extension for depth super-resolution that accurately synthesizes high-resolution disparities from their corresponding low-resolution convolutional features. In addition, we introduce a differentiable flip-augmentation layer that accurately fuses predictions from the image and its horizontally flipped version, reducing the effect of left and right shadow regions generated in the disparity map due to occlusions. Both contributions provide significant performance gains over the state-of-the-art in self-supervised depth and pose estimation on the public KITTI benchmark. A video of our approach can be found at https://youtu.be/jKNgBeBMx0I.



### PADDIT: Probabilistic Augmentation of Data using Diffeomorphic Image Transformation
- **Arxiv ID**: http://arxiv.org/abs/1810.01928v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.01928v2)
- **Published**: 2018-10-03 19:39:29+00:00
- **Updated**: 2020-03-09 13:47:47+00:00
- **Authors**: Mauricio Orbes Arteaga, Lauge Sørensen, M. Jorge Cardoso, Marc Modat, Sebastien Ourselin, Stefan Sommer, Mads Nielsen, Christian Igel, Akshay Pai
- **Comment**: None
- **Journal**: None
- **Summary**: For proper generalization performance of convolutional neural networks (CNNs) in medical image segmentation, the learnt features should be invariant under particular non-linear shape variations of the input. To induce invariance in CNNs to such transformations, we propose Probabilistic Augmentation of Data using Diffeomorphic Image Transformation (PADDIT) -- a systematic framework for generating realistic transformations that can be used to augment data for training CNNs. We show that CNNs trained with PADDIT outperforms CNNs trained without augmentation and with generic augmentation in segmenting white matter hyperintensities from T1 and FLAIR brain MRI scans.



### Transfer Learning via Unsupervised Task Discovery for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1810.02358v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.02358v2)
- **Published**: 2018-10-03 19:48:38+00:00
- **Updated**: 2019-04-07 11:50:11+00:00
- **Authors**: Hyeonwoo Noh, Taehoon Kim, Jonghwan Mun, Bohyung Han
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: We study how to leverage off-the-shelf visual and linguistic data to cope with out-of-vocabulary answers in visual question answering task. Existing large-scale visual datasets with annotations such as image class labels, bounding boxes and region descriptions are good sources for learning rich and diverse visual concepts. However, it is not straightforward how the visual concepts can be captured and transferred to visual question answering models due to missing link between question dependent answering models and visual data without question. We tackle this problem in two steps: 1) learning a task conditional visual classifier, which is capable of solving diverse question-specific visual recognition tasks, based on unsupervised task discovery and 2) transferring the task conditional visual classifier to visual question answering models. Specifically, we employ linguistic knowledge sources such as structured lexical database (e.g. WordNet) and visual descriptions for unsupervised task discovery, and transfer a learned task conditional visual classifier as an answering unit in a visual question answering model. We empirically show that the proposed algorithm generalizes to out-of-vocabulary answers successfully using the knowledge transferred from the visual dataset.



### CoverBLIP: accelerated and scalable iterative matched-filtering for Magnetic Resonance Fingerprint reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1810.01967v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.01967v1)
- **Published**: 2018-10-03 20:52:49+00:00
- **Updated**: 2018-10-03 20:52:49+00:00
- **Authors**: Mohammad Golbabaee, Zhouye Chen, Yves Wiaux, Mike Davies
- **Comment**: None
- **Journal**: None
- **Summary**: Current popular methods for Magnetic Resonance Fingerprint (MRF) recovery are bottlenecked by the heavy computations of a matched-filtering step due to the growing size and complexity of the fingerprint dictionaries in multi-parametric quantitative MRI applications. We address this shortcoming by arranging dictionary atoms in the form of cover tree structures and adopt the corresponding fast approximate nearest neighbour searches to accelerate matched-filtering. For datasets belonging to smooth low-dimensional manifolds cover trees offer search complexities logarithmic in terms of data population. With this motivation we propose an iterative reconstruction algorithm, named CoverBLIP, to address large-size MRF problems where the fingerprint dictionary i.e. discrete manifold of Bloch responses, encodes several intrinsic NMR parameters. We study different forms of convergence for this algorithm and we show that provided with a notion of embedding, the inexact and non-convex iterations of CoverBLIP linearly convergence toward a near-global solution with the same order of accuracy as using exact brute-force searches. Our further examinations on both synthetic and real-world datasets and using different sampling strategies, indicates between 2 to 3 orders of magnitude reduction in total search computations. Cover trees are robust against the curse-of-dimensionality and therefore CoverBLIP provides a notion of scalability -- a consistent gain in time-accuracy performance-- for searching high-dimensional atoms which may not be easily preprocessed (i.e. for dimensionality reduction) due to the increasing degrees of non-linearities appearing in the emerging multi-parametric MRF dictionaries.



### The Blackbird Dataset: A large-scale dataset for UAV perception in aggressive flight
- **Arxiv ID**: http://arxiv.org/abs/1810.01987v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1810.01987v1)
- **Published**: 2018-10-03 21:54:11+00:00
- **Updated**: 2018-10-03 21:54:11+00:00
- **Authors**: Amado Antonini, Winter Guerra, Varun Murali, Thomas Sayre-McCord, Sertac Karaman
- **Comment**: Accepted to appear at ISER 2018
- **Journal**: None
- **Summary**: The Blackbird unmanned aerial vehicle (UAV) dataset is a large-scale, aggressive indoor flight dataset collected using a custom-built quadrotor platform for use in evaluation of agile perception.Inspired by the potential of future high-speed fully-autonomous drone racing, the Blackbird dataset contains over 10 hours of flight data from 168 flights over 17 flight trajectories and 5 environments at velocities up to $7.0ms^-1$. Each flight includes sensor data from 120Hz stereo and downward-facing photorealistic virtual cameras, 100Hz IMU, $\sim190Hz$ motor speed sensors, and 360Hz millimeter-accurate motion capture ground truth. Camera images for each flight were photorealistically rendered using FlightGoggles across a variety of environments to facilitate easy experimentation of high performance perception algorithms. The dataset is available for download at http://blackbird-dataset.mit.edu/



### Image and Encoded Text Fusion for Multi-Modal Classification
- **Arxiv ID**: http://arxiv.org/abs/1810.02001v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.02001v1)
- **Published**: 2018-10-03 23:11:39+00:00
- **Updated**: 2018-10-03 23:11:39+00:00
- **Authors**: Ignazio Gallo, Alessandro Calefati, Shah Nawaz, Muhammad Kamran Janjua
- **Comment**: Accepted to DICTA 2018
- **Journal**: None
- **Summary**: Multi-modal approaches employ data from multiple input streams such as textual and visual domains. Deep neural networks have been successfully employed for these approaches. In this paper, we present a novel multi-modal approach that fuses images and text descriptions to improve multi-modal classification performance in real-world scenarios. The proposed approach embeds an encoded text onto an image to obtain an information-enriched image. To learn feature representations of resulting images, standard Convolutional Neural Networks (CNNs) are employed for the classification task. We demonstrate how a CNN based pipeline can be used to learn representations of the novel fusion approach. We compare our approach with individual sources on two large-scale multi-modal classification datasets while obtaining encouraging results. Furthermore, we evaluate our approach against two famous multi-modal strategies namely early fusion and late fusion.



