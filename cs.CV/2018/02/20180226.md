# Arxiv Papers in cs.CV on 2018-02-26
### Multi-Evidence Filtering and Fusion for Multi-Label Classification, Object Detection and Semantic Segmentation Based on Weakly Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/1802.09129v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1802.09129v1)
- **Published**: 2018-02-26 02:07:19+00:00
- **Updated**: 2018-02-26 02:07:19+00:00
- **Authors**: Weifeng Ge, Sibei Yang, Yizhou Yu
- **Comment**: accepted by IEEE International Conference on Computer Vision and
  Pattern Recognition (CVPR) 2018
- **Journal**: None
- **Summary**: Supervised object detection and semantic segmentation require object or even pixel level annotations. When there exist image level labels only, it is challenging for weakly supervised algorithms to achieve accurate predictions. The accuracy achieved by top weakly supervised algorithms is still significantly lower than their fully supervised counterparts. In this paper, we propose a novel weakly supervised curriculum learning pipeline for multi-label object recognition, detection and semantic segmentation. In this pipeline, we first obtain intermediate object localization and pixel labeling results for the training images, and then use such results to train task-specific deep networks in a fully supervised manner. The entire process consists of four stages, including object localization in the training images, filtering and fusing object instances, pixel labeling for the training images, and task-specific network training. To obtain clean object instances in the training images, we propose a novel algorithm for filtering, fusing and classifying object instances collected from multiple solution mechanisms. In this algorithm, we incorporate both metric learning and density-based clustering to filter detected object instances. Experiments show that our weakly supervised pipeline achieves state-of-the-art results in multi-label image classification as well as weakly supervised object detection and very competitive results in weakly supervised semantic segmentation on MS-COCO, PASCAL VOC 2007 and PASCAL VOC 2012.



### PBGen: Partial Binarization of Deconvolution-Based Generators for Edge Intelligence
- **Arxiv ID**: http://arxiv.org/abs/1802.09153v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.09153v3)
- **Published**: 2018-02-26 03:50:09+00:00
- **Updated**: 2020-01-27 04:15:55+00:00
- **Authors**: Jinglan Liu, Jiaxin Zhang, Yukun Ding, Xiaowei Xu, Meng Jiang, Yiyu Shi
- **Comment**: 17 pages, paper re-organized
- **Journal**: None
- **Summary**: This work explores the binarization of the deconvolution-based generator in a GAN for memory saving and speedup of image construction. Our study suggests that different from convolutional neural networks (including the discriminator) where all layers can be binarized, only some of the layers in the generator can be binarized without significant performance loss. Supported by theoretical analysis and verified by experiments, a direct metric based on the dimension of deconvolution operations is established, which can be used to quickly decide which layers in the generator can be binarized. Our results also indicate that both the generator and the discriminator should be binarized simultaneously for balanced competition and better performance. Experimental results based on CelebA suggest that directly applying state-of-the-art binarization techniques to all the layers of the generator will lead to 2.83$\times$ performance loss measured by sliced Wasserstein distance compared with the original generator, while applying them to selected layers only can yield up to 25.81$\times$ saving in memory consumption, and 1.96$\times$ and 1.32$\times$ speedup in inference and training respectively with little performance loss.



### Photographic Text-to-Image Synthesis with a Hierarchically-nested Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/1802.09178v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.09178v2)
- **Published**: 2018-02-26 06:33:32+00:00
- **Updated**: 2018-04-06 21:05:39+00:00
- **Authors**: Zizhao Zhang, Yuanpu Xie, Lin Yang
- **Comment**: CVPR2018 Spotlight
- **Journal**: None
- **Summary**: This paper presents a novel method to deal with the challenging task of generating photographic images conditioned on semantic image descriptions. Our method introduces accompanying hierarchical-nested adversarial objectives inside the network hierarchies, which regularize mid-level representations and assist generator training to capture the complex image statistics. We present an extensile single-stream generator architecture to better adapt the jointed discriminators and push generated images up to high resolutions. We adopt a multi-purpose adversarial loss to encourage more effective image and text information usage in order to improve the semantic consistency and image fidelity simultaneously. Furthermore, we introduce a new visual-semantic similarity measure to evaluate the semantic consistency of generated images. With extensive experimental validation on three public datasets, our method significantly improves previous state of the arts on all datasets over different evaluation metrics.



### Demystifying Parallel and Distributed Deep Learning: An In-Depth Concurrency Analysis
- **Arxiv ID**: http://arxiv.org/abs/1802.09941v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.DC, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1802.09941v2)
- **Published**: 2018-02-26 08:47:34+00:00
- **Updated**: 2018-09-15 08:36:28+00:00
- **Authors**: Tal Ben-Nun, Torsten Hoefler
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Neural Networks (DNNs) are becoming an important tool in modern computing applications. Accelerating their training is a major challenge and techniques range from distributed algorithms to low-level circuit design. In this survey, we describe the problem from a theoretical perspective, followed by approaches for its parallelization. We present trends in DNN architectures and the resulting implications on parallelization strategies. We then review and model the different types of concurrency in DNNs: from the single operator, through parallelism in network inference and training, to distributed deep learning. We discuss asynchronous stochastic optimization, distributed system architectures, communication schemes, and neural architecture search. Based on those approaches, we extrapolate potential directions for parallelism in deep learning.



### Depth Masked Discriminative Correlation Filter
- **Arxiv ID**: http://arxiv.org/abs/1802.09227v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.09227v2)
- **Published**: 2018-02-26 10:00:14+00:00
- **Updated**: 2018-10-10 11:04:03+00:00
- **Authors**: Uğur Kart, Joni-Kristian Kämäräinen, Jiří Matas, Lixin Fan, Francesco Cricri
- **Comment**: 6 pages, accepted to ICPR 2018. \copyright 2018 IEEE
- **Journal**: None
- **Summary**: Depth information provides a strong cue for occlusion detection and handling, but has been largely omitted in generic object tracking until recently due to lack of suitable benchmark datasets and applications. In this work, we propose a Depth Masked Discriminative Correlation Filter (DM-DCF) which adopts novel depth segmentation based occlusion detection that stops correlation filter updating and depth masking which adaptively adjusts the spatial support for correlation filter. In Princeton RGBD Tracking Benchmark, our DM-DCF is among the state-of-the-art in overall ranking and the winner on multiple categories. Moreover, since it is based on DCF, ``DM-DCF`` runs an order of magnitude faster than its competitors making it suitable for time constrained applications.



### 2D/3D Pose Estimation and Action Recognition using Multitask Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1802.09232v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.09232v2)
- **Published**: 2018-02-26 10:16:48+00:00
- **Updated**: 2018-03-21 13:39:45+00:00
- **Authors**: Diogo C. Luvizon, David Picard, Hedi Tabia
- **Comment**: To appear in CVPR 2018
- **Journal**: None
- **Summary**: Action recognition and human pose estimation are closely related but both problems are generally handled as distinct tasks in the literature. In this work, we propose a multitask framework for jointly 2D and 3D pose estimation from still images and human action recognition from video sequences. We show that a single architecture can be used to solve the two problems in an efficient way and still achieves state-of-the-art results. Additionally, we demonstrate that optimization from end-to-end leads to significantly higher accuracy than separated learning. The proposed architecture can be trained with data from different categories simultaneously in a seamlessly way. The reported results on four datasets (MPII, Human3.6M, Penn Action and NTU) demonstrate the effectiveness of our method on the targeted tasks.



### HBST: A Hamming Distance embedding Binary Search Tree for Visual Place Recognition
- **Arxiv ID**: http://arxiv.org/abs/1802.09261v2
- **DOI**: 10.1109/LRA.2018.2856542
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1802.09261v2)
- **Published**: 2018-02-26 11:56:11+00:00
- **Updated**: 2018-02-28 22:37:05+00:00
- **Authors**: Dominik Schlegel, Giorgio Grisetti
- **Comment**: Submitted to IEEE Robotics and Automation Letters (RA-L) 2018 with
  International Conference on Intelligent Robots and Systems (IROS) 2018
  option, 8 pages, 10 figures
- **Journal**: IEEE Robotics and Automation Letters (Volume: 3, Issue: 4, Oct.
  2018, Pages: 3741 - 3748)
- **Summary**: Reliable and efficient Visual Place Recognition is a major building block of modern SLAM systems. Leveraging on our prior work, in this paper we present a Hamming Distance embedding Binary Search Tree (HBST) approach for binary Descriptor Matching and Image Retrieval. HBST allows for descriptor Search and Insertion in logarithmic time by exploiting particular properties of binary Feature descriptors. We support the idea behind our search structure with a thorough analysis on the exploited descriptor properties and their effects on completeness and complexity of search and insertion. To validate our claims we conducted comparative experiments for HBST and several state-of-the-art methods on a broad range of publicly available datasets. HBST is available as a compact open-source C++ header-only library.



### VR-SGD: A Simple Stochastic Variance Reduction Method for Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/1802.09932v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, math.OC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1802.09932v2)
- **Published**: 2018-02-26 12:56:07+00:00
- **Updated**: 2018-10-28 15:03:47+00:00
- **Authors**: Fanhua Shang, Kaiwen Zhou, Hongying Liu, James Cheng, Ivor W. Tsang, Lijun Zhang, Dacheng Tao, Licheng Jiao
- **Comment**: 46 pages, 25 figures. IEEE Transactions on Knowledge and Data
  Engineering, accepted in October, 2018. arXiv admin note: substantial text
  overlap with arXiv:1704.04966
- **Journal**: None
- **Summary**: In this paper, we propose a simple variant of the original SVRG, called variance reduced stochastic gradient descent (VR-SGD). Unlike the choices of snapshot and starting points in SVRG and its proximal variant, Prox-SVRG, the two vectors of VR-SGD are set to the average and last iterate of the previous epoch, respectively. The settings allow us to use much larger learning rates, and also make our convergence analysis more challenging. We also design two different update rules for smooth and non-smooth objective functions, respectively, which means that VR-SGD can tackle non-smooth and/or non-strongly convex problems directly without any reduction techniques. Moreover, we analyze the convergence properties of VR-SGD for strongly convex problems, which show that VR-SGD attains linear convergence. Different from its counterparts that have no convergence guarantees for non-strongly convex problems, we also provide the convergence guarantees of VR-SGD for this case, and empirically verify that VR-SGD with varying learning rates achieves similar performance to its momentum accelerated variant that has the optimal convergence rate $\mathcal{O}(1/T^2)$. Finally, we apply VR-SGD to solve various machine learning problems, such as convex and non-convex empirical risk minimization, and leading eigenvalue computation. Experimental results show that VR-SGD converges significantly faster than SVRG and Prox-SVRG, and usually outperforms state-of-the-art accelerated methods, e.g., Katyusha.



### Constructing Category-Specific Models for Monocular Object-SLAM
- **Arxiv ID**: http://arxiv.org/abs/1802.09292v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1802.09292v1)
- **Published**: 2018-02-26 13:42:40+00:00
- **Updated**: 2018-02-26 13:42:40+00:00
- **Authors**: Parv Parkhiya, Rishabh Khawad, J. Krishna Murthy, Brojeshwar Bhowmick, K. Madhava Krishna
- **Comment**: Accepted to ICRA 2018
- **Journal**: None
- **Summary**: We present a new paradigm for real-time object-oriented SLAM with a monocular camera. Contrary to previous approaches, that rely on object-level models, we construct category-level models from CAD collections which are now widely available. To alleviate the need for huge amounts of labeled data, we develop a rendering pipeline that enables synthesis of large datasets from a limited amount of manually labeled data. Using data thus synthesized, we learn category-level models for object deformations in 3D, as well as discriminative object features in 2D. These category models are instance-independent and aid in the design of object landmark observations that can be incorporated into a generic monocular SLAM framework. Where typical object-SLAM approaches usually solve only for object and camera poses, we also estimate object shape on-the-fly, allowing for a wide range of objects from the category to be present in the scene. Moreover, since our 2D object features are learned discriminatively, the proposed object-SLAM system succeeds in several scenarios where sparse feature-based monocular SLAM fails due to insufficient features or parallax. Also, the proposed category-models help in object instance retrieval, useful for Augmented Reality (AR) applications. We evaluate the proposed framework on multiple challenging real-world scenes and show --- to the best of our knowledge --- first results of an instance-independent monocular object-SLAM system and the benefits it enjoys over feature-based SLAM methods.



### Beyond Pixels: Leveraging Geometry and Shape Cues for Online Multi-Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/1802.09298v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1802.09298v2)
- **Published**: 2018-02-26 13:53:16+00:00
- **Updated**: 2018-07-27 10:43:55+00:00
- **Authors**: Sarthak Sharma, Junaid Ahmed Ansari, J. Krishna Murthy, K. Madhava Krishna
- **Comment**: ICRA 2018 paper. Code available at
  https://github.com/JunaidCS032/MOTBeyondPixels
- **Journal**: None
- **Summary**: This paper introduces geometry and object shape and pose costs for multi-object tracking in urban driving scenarios. Using images from a monocular camera alone, we devise pairwise costs for object tracks, based on several 3D cues such as object pose, shape, and motion. The proposed costs are agnostic to the data association method and can be incorporated into any optimization framework to output the pairwise data associations. These costs are easy to implement, can be computed in real-time, and complement each other to account for possible errors in a tracking-by-detection framework. We perform an extensive analysis of the designed costs and empirically demonstrate consistent improvement over the state-of-the-art under varying conditions that employ a range of object detectors, exhibit a variety in camera and object motions, and, more importantly, are not reliant on the choice of the association framework. We also show that, by using the simplest of associations frameworks (two-frame Hungarian assignment), we surpass the state-of-the-art in multi-object-tracking on road scenes. More qualitative and quantitative results can be found at the following URL: https://junaidcs032.github.io/Geometry_ObjectShape_MOT/.



### Yedrouj-Net: An efficient CNN for spatial steganalysis
- **Arxiv ID**: http://arxiv.org/abs/1803.00407v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/1803.00407v1)
- **Published**: 2018-02-26 14:09:14+00:00
- **Updated**: 2018-02-26 14:09:14+00:00
- **Authors**: Mehdi Yedroudj, Frederic Comby, Marc Chaumont
- **Comment**: ICASSP'2018, 15-20 April 2018, Calgary, Alberta, Canada, 5 pages
- **Journal**: IEEE International Conference on Acoustics, Speech and Signal
  Processing, ICASSP'2018, 15-20 April 2018, Calgary, Alberta, Canada, 5 pages
- **Summary**: For about 10 years, detecting the presence of a secret message hidden in an image was performed with an Ensemble Classifier trained with Rich features. In recent years, studies such as Xu et al. have indicated that well-designed convolutional Neural Networks (CNN) can achieve comparable performance to the two-step machine learning approaches.   In this paper, we propose a CNN that outperforms the state-ofthe-art in terms of error probability. The proposition is in the continuity of what has been recently proposed and it is a clever fusion of important bricks used in various papers. Among the essential parts of the CNN, one can cite the use of a pre-processing filterbank and a Truncation activation function, five convolutional layers with a Batch Normalization associated with a Scale Layer, as well as the use of a sufficiently sized fully connected section. An augmented database has also been used to improve the training of the CNN.   Our CNN was experimentally evaluated against S-UNIWARD and WOW embedding algorithms and its performances were compared with those of three other methods: an Ensemble Classifier plus a Rich Model, and two other CNN steganalyzers.



### DropLasso: A robust variant of Lasso for single cell RNA-seq data
- **Arxiv ID**: http://arxiv.org/abs/1802.09381v1
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV, q-bio.GN, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1802.09381v1)
- **Published**: 2018-02-26 15:10:44+00:00
- **Updated**: 2018-02-26 15:10:44+00:00
- **Authors**: Beyrem Khalfaoui, Jean-Philippe Vert
- **Comment**: None
- **Journal**: None
- **Summary**: Single-cell RNA sequencing (scRNA-seq) is a fast growing approach to measure the genome-wide transcriptome of many individual cells in parallel, but results in noisy data with many dropout events. Existing methods to learn molecular signatures from bulk transcriptomic data may therefore not be adapted to scRNA-seq data, in order to automatically classify individual cells into predefined classes. We propose a new method called DropLasso to learn a molecular signature from scRNA-seq data. DropLasso extends the dropout regularisation technique, popular in neural network training, to esti- mate sparse linear models. It is well adapted to data corrupted by dropout noise, such as scRNA-seq data, and we clarify how it relates to elastic net regularisation. We provide promising results on simulated and real scRNA-seq data, suggesting that DropLasso may be better adapted than standard regularisa- tions to infer molecular signatures from scRNA-seq data.



### Using Curvilinear Features in Focus for Registering a Single Image to a 3D Object
- **Arxiv ID**: http://arxiv.org/abs/1802.09384v1
- **DOI**: 10.1109/TIP.2019.2911484
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.09384v1)
- **Published**: 2018-02-26 15:13:54+00:00
- **Updated**: 2018-02-26 15:13:54+00:00
- **Authors**: Hatem A. Rashwan, Sylvie Chambon, Pierre Gurdjos, Géraldine Morin, Vincent Charvillat
- **Comment**: None
- **Journal**: None
- **Summary**: In the context of 2D/3D registration, this paper introduces an approach that allows to match features detected in two different modalities: photographs and 3D models, by using a common 2D reprensentation. More precisely, 2D images are matched with a set of depth images, representing the 3D model. After introducing the concept of curvilinear saliency, related to curvature estimation, we propose a new ridge and valley detector for depth images rendered from 3D model. A variant of this detector is adapted to photographs, in particular by applying it in multi-scale and by combining this feature detector with the principle of focus curves. Finally, a registration algorithm for determining the correct viewpoint of the 3D model and thus the pose is proposed. It is based on using histogram of gradients features adapted to the features manipulated in 2D and in 3D, and the introduction of repeatability scores. The results presented highlight the quality of the features detected, in term of repeatability, and also the interest of the approach for registration and pose estimation.



### Classification of breast cancer histology images using transfer learning
- **Arxiv ID**: http://arxiv.org/abs/1802.09424v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.09424v1)
- **Published**: 2018-02-26 16:10:40+00:00
- **Updated**: 2018-02-26 16:10:40+00:00
- **Authors**: Sulaiman Vesal, Nishant Ravikumar, AmirAbbas Davari, Stephan Ellmann, Andreas Maier
- **Comment**: 8 pages, Submitted to 15th International Conference on Image Analysis
  and Recognition (ICAIR 2018)
- **Journal**: None
- **Summary**: Breast cancer is one of the leading causes of mortality in women. Early detection and treatment are imperative for improving survival rates, which have steadily increased in recent years as a result of more sophisticated computer-aided-diagnosis (CAD) systems. A critical component of breast cancer diagnosis relies on histopathology, a laborious and highly subjective process. Consequently, CAD systems are essential to reduce inter-rater variability and supplement the analyses conducted by specialists. In this paper, a transfer-learning based approach is proposed, for the task of breast histology image classification into four tissue sub-types, namely, normal, benign, \textit{in situ} carcinoma and invasive carcinoma. The histology images, provided as part of the BACH 2018 grand challenge, were first normalized to correct for color variations resulting from inconsistencies during slide preparation. Subsequently, image patches were extracted and used to fine-tune Google`s Inception-V3 and ResNet50 convolutional neural networks (CNNs), both pre-trained on the ImageNet database, enabling them to learn domain-specific features, necessary to classify the histology images. The ResNet50 network (based on residual learning) achieved a test classification accuracy of 97.50% for four classes, outperforming the Inception-V3 network which achieved an accuracy of 91.25%.



### Self Super-Resolution for Magnetic Resonance Images using Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/1802.09431v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1802.09431v1)
- **Published**: 2018-02-26 16:17:18+00:00
- **Updated**: 2018-02-26 16:17:18+00:00
- **Authors**: Can Zhao, Aaron Carass, Blake E. Dewey, Jerry L. Prince
- **Comment**: Accepted by IEEE International Symposium on Biomedical Imaging (ISBI)
  2018
- **Journal**: None
- **Summary**: High resolution magnetic resonance~(MR) imaging~(MRI) is desirable in many clinical applications, however, there is a trade-off between resolution, speed of acquisition, and noise. It is common for MR images to have worse through-plane resolution~(slice thickness) than in-plane resolution. In these MRI images, high frequency information in the through-plane direction is not acquired, and cannot be resolved through interpolation. To address this issue, super-resolution methods have been developed to enhance spatial resolution. As an ill-posed problem, state-of-the-art super-resolution methods rely on the presence of external/training atlases to learn the transform from low resolution~(LR) images to high resolution~(HR) images. For several reasons, such HR atlas images are often not available for MRI sequences. This paper presents a self super-resolution~(SSR) algorithm, which does not use any external atlas images, yet can still resolve HR images only reliant on the acquired LR image. We use a blurred version of the input image to create training data for a state-of-the-art super-resolution deep network. The trained network is applied to the original input image to estimate the HR image. Our SSR result shows a significant improvement on through-plane resolution compared to competing SSR methods.



### A Robust Real-Time Automatic License Plate Recognition Based on the YOLO Detector
- **Arxiv ID**: http://arxiv.org/abs/1802.09567v6
- **DOI**: 10.1109/IJCNN.2018.8489629
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.09567v6)
- **Published**: 2018-02-26 19:31:56+00:00
- **Updated**: 2018-04-28 12:54:38+00:00
- **Authors**: Rayson Laroca, Evair Severo, Luiz A. Zanlorensi, Luiz S. Oliveira, Gabriel Resende Gonçalves, William Robson Schwartz, David Menotti
- **Comment**: Accepted for presentation at the International Joint Conference on
  Neural Networks (IJCNN) 2018
- **Journal**: None
- **Summary**: Automatic License Plate Recognition (ALPR) has been a frequent topic of research due to many practical applications. However, many of the current solutions are still not robust in real-world situations, commonly depending on many constraints. This paper presents a robust and efficient ALPR system based on the state-of-the-art YOLO object detector. The Convolutional Neural Networks (CNNs) are trained and fine-tuned for each ALPR stage so that they are robust under different conditions (e.g., variations in camera, lighting, and background). Specially for character segmentation and recognition, we design a two-stage approach employing simple data augmentation tricks such as inverted License Plates (LPs) and flipped characters. The resulting ALPR approach achieved impressive results in two datasets. First, in the SSIG dataset, composed of 2,000 frames from 101 vehicle videos, our system achieved a recognition rate of 93.53% and 47 Frames Per Second (FPS), performing better than both Sighthound and OpenALPR commercial systems (89.80% and 93.03%, respectively) and considerably outperforming previous results (81.80%). Second, targeting a more realistic scenario, we introduce a larger public dataset, called UFPR-ALPR dataset, designed to ALPR. This dataset contains 150 videos and 4,500 frames captured when both camera and vehicles are moving and also contains different types of vehicles (cars, motorcycles, buses and trucks). In our proposed dataset, the trial versions of commercial systems achieved recognition rates below 70%. On the other hand, our system performed better, with recognition rate of 78.33% and 35 FPS.



### i3PosNet: Instrument Pose Estimation from X-Ray in temporal bone surgery
- **Arxiv ID**: http://arxiv.org/abs/1802.09575v2
- **DOI**: 10.1007/s11548-020-02157-4
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1802.09575v2)
- **Published**: 2018-02-26 20:00:40+00:00
- **Updated**: 2020-03-10 18:51:15+00:00
- **Authors**: David Kügler, Jannik Sehring, Andrei Stefanov, Igor Stenin, Julia Kristin, Thomas Klenzner, Jörg Schipper, Anirban Mukhopadhyay
- **Comment**: Accepted at International journal of computer assisted radiology and
  surgery pending publication
- **Journal**: None
- **Summary**: Purpose: Accurate estimation of the position and orientation (pose) of surgical instruments is crucial for delicate minimally invasive temporal bone surgery. Current techniques lack in accuracy and/or line-of-sight constraints (conventional tracking systems) or expose the patient to prohibitive ionizing radiation (intra-operative CT). A possible solution is to capture the instrument with a c-arm at irregular intervals and recover the pose from the image.   Methods: i3PosNet infers the position and orientation of instruments from images using a pose estimation network. Said framework considers localized patches and outputs pseudo-landmarks. The pose is reconstructed from pseudo-landmarks by geometric considerations.   Results: We show i3PosNet reaches errors less than 0.05mm. It outperforms conventional image registration-based approaches reducing average and maximum errors by at least two thirds. i3PosNet trained on synthetic images generalizes to real x-rays without any further adaptation.   Conclusion: The translation of Deep Learning based methods to surgical applications is difficult, because large representative datasets for training and testing are not available. This work empirically shows sub-millimeter pose estimation trained solely based on synthetic training data.



