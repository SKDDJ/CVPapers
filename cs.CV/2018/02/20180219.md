# Arxiv Papers in cs.CV on 2018-02-19
### A Closed-form Solution to Photorealistic Image Stylization
- **Arxiv ID**: http://arxiv.org/abs/1802.06474v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.06474v5)
- **Published**: 2018-02-19 00:34:18+00:00
- **Updated**: 2018-07-27 01:14:23+00:00
- **Authors**: Yijun Li, Ming-Yu Liu, Xueting Li, Ming-Hsuan Yang, Jan Kautz
- **Comment**: Accepted by ECCV 2018
- **Journal**: None
- **Summary**: Photorealistic image stylization concerns transferring style of a reference photo to a content photo with the constraint that the stylized photo should remain photorealistic. While several photorealistic image stylization methods exist, they tend to generate spatially inconsistent stylizations with noticeable artifacts. In this paper, we propose a method to address these issues. The proposed method consists of a stylization step and a smoothing step. While the stylization step transfers the style of the reference photo to the content photo, the smoothing step ensures spatially consistent stylizations. Each of the steps has a closed-form solution and can be computed efficiently. We conduct extensive experimental validations. The results show that the proposed method generates photorealistic stylization outputs that are more preferred by human subjects as compared to those by the competing methods while running much faster. Source code and additional results are available at https://github.com/NVIDIA/FastPhotoStyle .



### Tiny SSD: A Tiny Single-shot Detection Deep Convolutional Neural Network for Real-time Embedded Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1802.06488v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1802.06488v1)
- **Published**: 2018-02-19 01:57:46+00:00
- **Updated**: 2018-02-19 01:57:46+00:00
- **Authors**: Alexander Wong, Mohammad Javad Shafiee, Francis Li, Brendan Chwyl
- **Comment**: 7 pages
- **Journal**: None
- **Summary**: Object detection is a major challenge in computer vision, involving both object classification and object localization within a scene. While deep neural networks have been shown in recent years to yield very powerful techniques for tackling the challenge of object detection, one of the biggest challenges with enabling such object detection networks for widespread deployment on embedded devices is high computational and memory requirements. Recently, there has been an increasing focus in exploring small deep neural network architectures for object detection that are more suitable for embedded devices, such as Tiny YOLO and SqueezeDet. Inspired by the efficiency of the Fire microarchitecture introduced in SqueezeNet and the object detection performance of the single-shot detection macroarchitecture introduced in SSD, this paper introduces Tiny SSD, a single-shot detection deep convolutional neural network for real-time embedded object detection that is composed of a highly optimized, non-uniform Fire sub-network stack and a non-uniform sub-network stack of highly optimized SSD-based auxiliary convolutional feature layers designed specifically to minimize model size while maintaining object detection performance. The resulting Tiny SSD possess a model size of 2.3MB (~26X smaller than Tiny YOLO) while still achieving an mAP of 61.3% on VOC 2007 (~4.2% higher than Tiny YOLO). These experimental results show that very small deep neural network architectures can be designed for real-time object detection that are well-suited for embedded scenarios.



### Image Forensics: Detecting duplication of scientific images with manipulation-invariant image similarity
- **Arxiv ID**: http://arxiv.org/abs/1802.06515v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.06515v3)
- **Published**: 2018-02-19 04:41:24+00:00
- **Updated**: 2020-03-17 16:27:48+00:00
- **Authors**: M. Cicconet, H. Elliott, D. L. Richmond, D. Wainstock, M. Walsh
- **Comment**: 12 pages; 6 figures; keywords: siamese network, similarity metric,
  image forensics, image manipulation
- **Journal**: None
- **Summary**: Manipulation and re-use of images in scientific publications is a concerning problem that currently lacks a scalable solution. Current tools for detecting image duplication are mostly manual or semi-automated, despite the availability of an overwhelming target dataset for a learning-based approach. This paper addresses the problem of determining if, given two images, one is a manipulated version of the other by means of copy, rotation, translation, scale, perspective transform, histogram adjustment, or partial erasing. We propose a data-driven solution based on a 3-branch Siamese Convolutional Neural Network. The ConvNet model is trained to map images into a 128-dimensional space, where the Euclidean distance between duplicate images is smaller than or equal to 1, and the distance between unique images is greater than 1. Our results suggest that such an approach has the potential to improve surveillance of the published and in-peer-review literature for image manipulation.



### Salient Object Detection by Lossless Feature Reflection
- **Arxiv ID**: http://arxiv.org/abs/1802.06527v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.06527v2)
- **Published**: 2018-02-19 05:59:08+00:00
- **Updated**: 2018-04-17 03:19:49+00:00
- **Authors**: Pingping Zhang, Wei Liu, Huchuan Lu, Chunhua Shen
- **Comment**: Accepted by IJCAI-2018, 7 pages
- **Journal**: None
- **Summary**: Salient object detection, which aims to identify and locate the most salient pixels or regions in images, has been attracting more and more interest due to its various real-world applications. However, this vision task is quite challenging, especially under complex image scenes. Inspired by the intrinsic reflection of natural images, in this paper we propose a novel feature learning framework for large-scale salient object detection. Specifically, we design a symmetrical fully convolutional network (SFCN) to learn complementary saliency features under the guidance of lossless feature reflection. The location information, together with contextual and semantic information, of salient objects are jointly utilized to supervise the proposed network for more accurate saliency predictions. In addition, to overcome the blurry boundary problem, we propose a new structural loss function to learn clear object boundaries and spatially consistent saliency. The coarse prediction results are effectively refined by these structural information for performance improvements. Extensive experiments on seven saliency detection datasets demonstrate that our approach achieves consistently superior performance and outperforms the very recent state-of-the-art methods.



### A Neural Multi-sequence Alignment TeCHnique (NeuMATCH)
- **Arxiv ID**: http://arxiv.org/abs/1803.00057v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1803.00057v2)
- **Published**: 2018-02-19 06:51:01+00:00
- **Updated**: 2018-04-09 20:51:32+00:00
- **Authors**: Pelin Dogan, Boyang Li, Leonid Sigal, Markus Gross
- **Comment**: Accepted at CVPR 2018 (Spotlight). arXiv file includes the paper and
  the supplemental material
- **Journal**: None
- **Summary**: The alignment of heterogeneous sequential data (video to text) is an important and challenging problem. Standard techniques for this task, including Dynamic Time Warping (DTW) and Conditional Random Fields (CRFs), suffer from inherent drawbacks. Mainly, the Markov assumption implies that, given the immediate past, future alignment decisions are independent of further history. The separation between similarity computation and alignment decision also prevents end-to-end training. In this paper, we propose an end-to-end neural architecture where alignment actions are implemented as moving data between stacks of Long Short-term Memory (LSTM) blocks. This flexible architecture supports a large variety of alignment tasks, including one-to-one, one-to-many, skipping unmatched elements, and (with extensions) non-monotonic alignment. Extensive experiments on semi-synthetic and real datasets show that our algorithm outperforms state-of-the-art baselines.



### Satellite imagery analysis for operational damage assessment in Emergency situations
- **Arxiv ID**: http://arxiv.org/abs/1803.00397v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1803.00397v1)
- **Published**: 2018-02-19 08:04:25+00:00
- **Updated**: 2018-02-19 08:04:25+00:00
- **Authors**: Alexey Trekin, German Novikov, Georgy Potapov, Vladimir Ignatiev, Evgeny Burnaev
- **Comment**: 12 pages, 10 figures
- **Journal**: None
- **Summary**: When major disaster occurs the questions are raised how to estimate the damage in time to support the decision making process and relief efforts by local authorities or humanitarian teams. In this paper we consider the use of Machine Learning and Computer Vision on remote sensing imagery to improve time efficiency of assessment of damaged buildings in disaster affected area. We propose a general workflow that can be useful in various disaster management applications, and demonstrate the use of the proposed workflow for the assessment of the damage caused by the wildfires in California in 2017.



### Weighted Linear Discriminant Analysis based on Class Saliency Information
- **Arxiv ID**: http://arxiv.org/abs/1802.06547v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.06547v1)
- **Published**: 2018-02-19 08:34:41+00:00
- **Updated**: 2018-02-19 08:34:41+00:00
- **Authors**: Lei Xu, Alexandros Iosifidis, Moncef Gabbouj
- **Comment**: Submitted to ICIP 2018
- **Journal**: None
- **Summary**: In this paper, we propose a new variant of Linear Discriminant Analysis to overcome underlying drawbacks of traditional LDA and other LDA variants targeting problems involving imbalanced classes. Traditional LDA sets assumptions related to Gaussian class distribution and neglects influence of outlier classes, that might hurt in performance. We exploit intuitions coming from a probabilistic interpretation of visual saliency estimation in order to define saliency of a class in multi-class setting. Such information is then used to redefine the between-class and within-class scatters in a more robust manner. Compared to traditional LDA and other weight-based LDA variants, the proposed method has shown certain improvements on facial image classification problems in publicly available datasets.



### Deep Residual Network for Joint Demosaicing and Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1802.06573v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.06573v1)
- **Published**: 2018-02-19 09:52:59+00:00
- **Updated**: 2018-02-19 09:52:59+00:00
- **Authors**: Ruofan Zhou, Radhakrishna Achanta, Sabine Süsstrunk
- **Comment**: None
- **Journal**: None
- **Summary**: In digital photography, two image restoration tasks have been studied extensively and resolved independently: demosaicing and super-resolution. Both these tasks are related to resolution limitations of the camera. Performing super-resolution on a demosaiced images simply exacerbates the artifacts introduced by demosaicing. In this paper, we show that such accumulation of errors can be easily averted by jointly performing demosaicing and super-resolution. To this end, we propose a deep residual network for learning an end-to-end mapping between Bayer images and high-resolution images. By training on high-quality samples, our deep residual demosaicing and super-resolution network is able to recover high-quality super-resolved images from low-resolution Bayer mosaics in a single step without producing the artifacts common to such processing when the two operations are done separately. We perform extensive experiments to show that our deep residual network achieves demosaiced and super-resolved images that are superior to the state-of-the-art both qualitatively and in terms of PSNR and SSIM metrics.



### Ensemble computation approach to the Hough transform
- **Arxiv ID**: http://arxiv.org/abs/1802.06619v1
- **DOI**: None
- **Categories**: **cs.CC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1802.06619v1)
- **Published**: 2018-02-19 13:28:18+00:00
- **Updated**: 2018-02-19 13:28:18+00:00
- **Authors**: Timur M. Khanipov
- **Comment**: 22 pages, 8 TikZ figures
- **Journal**: None
- **Summary**: It is demonstrated that the classical Hough transform with shift-elevation parametrization of digital straight lines has additive complexity of at most $\mathcal{O}(n^3 / \log n)$ on a $n\times n$ image. The proof is constructive and uses ensemble computation approach to build summation circuits. The proposed method has similarities with the fast Hough transform (FHT) and may be considered a form of the "divide and conquer" technique. It is based on the fact that lines with close slopes can be decomposed into common components, allowing generalization for other pattern families. When applied to FHT patterns, the algorithm yields exactly the $\Theta(n^2\log n)$ FHT asymptotics which might suggest that the actual classical Hough transform circuits could smaller size than $\Theta(n^3/ \log n)$.



### Osteoarthritis Disease Detection System using Self Organizing Maps Method based on Ossa Manus X-Ray
- **Arxiv ID**: http://arxiv.org/abs/1802.06624v1
- **DOI**: 10.5120/ijca2017915278
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.06624v1)
- **Published**: 2018-02-19 13:43:05+00:00
- **Updated**: 2018-02-19 13:43:05+00:00
- **Authors**: Putri Kurniasih, Dian Pratiwi
- **Comment**: 6 pages, 12 figures, 1 table
- **Journal**: International Journal of Computer Applications, Foundation of
  Computer Science (FCS), NY, USA. Volume 173 - Number 3, 2017
- **Summary**: Osteoarthritis is a disease found in the world, including in Indonesia. The purpose of this study was to detect the disease Osteoarthritis using Self Organizing mapping (SOM), and to know the procedure of artificial intelligence on the methods of Self Organizing Mapping (SOM). In this system, there are several stages to preserve to detect disease Osteoarthritis using Self Organizing maps is the result of photographic images rontgen Ossa Manus normal and sick with the resolution (150 x 200 pixels) do the repair phase contrast, the Gray scale, thresholding process, Histogram of process , and do the last process, where the process of doing training (Training) and testing on images that have kept the shape data (.text). the conclusion is the result of testing by using a data image, where 42 of data have 12 Normal image data and image data 30 sick. On the results of the process of training data there are 8 X-ray image revealed normal right and 19 data x-ray image of pain expressed is correct. Then the accuracy on the process of training was 96.42%, and in the process of testing normal true image 4 obtained revealed Normal, 9 data pain stated true pain and 1 data imagery hurts stated incorrectly, then the accuracy gained from the results of testing are 92,8%.



### Robustness of Rotation-Equivariant Networks to Adversarial Perturbations
- **Arxiv ID**: http://arxiv.org/abs/1802.06627v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1802.06627v2)
- **Published**: 2018-02-19 13:49:15+00:00
- **Updated**: 2018-05-17 15:24:31+00:00
- **Authors**: Beranger Dumont, Simona Maggio, Pablo Montalvo
- **Comment**: 4 pages + references; public implementation of Spatially Transformed
  Adversarial Examples can be found at https://github.com/rakutentech/stAdv
- **Journal**: None
- **Summary**: Deep neural networks have been shown to be vulnerable to adversarial examples: very small perturbations of the input having a dramatic impact on the predictions. A wealth of adversarial attacks and distance metrics to quantify the similarity between natural and adversarial images have been proposed, recently enlarging the scope of adversarial examples with geometric transformations beyond pixel-wise attacks. In this context, we investigate the robustness to adversarial attacks of new Convolutional Neural Network architectures providing equivariance to rotations. We found that rotation-equivariant networks are significantly less vulnerable to geometric-based attacks than regular networks on the MNIST, CIFAR-10, and ImageNet datasets.



### Simultaneous Compression and Quantization: A Joint Approach for Efficient Unsupervised Hashing
- **Arxiv ID**: http://arxiv.org/abs/1802.06645v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.06645v3)
- **Published**: 2018-02-19 14:36:06+00:00
- **Updated**: 2019-11-13 03:35:06+00:00
- **Authors**: Tuan Hoang, Thanh-Toan Do, Huu Le, Dang-Khoa Le-Tan, Ngai-Man Cheung
- **Comment**: None
- **Journal**: None
- **Summary**: For unsupervised data-dependent hashing, the two most important requirements are to preserve similarity in the low-dimensional feature space and to minimize the binary quantization loss. A well-established hashing approach is Iterative Quantization (ITQ), which addresses these two requirements in separate steps. In this paper, we revisit the ITQ approach and propose novel formulations and algorithms to the problem. Specifically, we propose a novel approach, named Simultaneous Compression and Quantization (SCQ), to jointly learn to compress (reduce dimensionality) and binarize input data in a single formulation under strict orthogonal constraint. With this approach, we introduce a loss function and its relaxed version, termed Orthonormal Encoder (OnE) and Orthogonal Encoder (OgE) respectively, which involve challenging binary and orthogonal constraints. We propose to attack the optimization using novel algorithms based on recent advances in cyclic coordinate descent approach. Comprehensive experiments on unsupervised image retrieval demonstrate that our proposed methods consistently outperform other state-of-the-art hashing methods. Notably, our proposed methods outperform recent deep neural networks and GAN based hashing in accuracy, while being very computationally-efficient.



### Multi-task, multi-label and multi-domain learning with residual convolutional networks for emotion recognition
- **Arxiv ID**: http://arxiv.org/abs/1802.06664v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.06664v1)
- **Published**: 2018-02-19 15:16:36+00:00
- **Updated**: 2018-02-19 15:16:36+00:00
- **Authors**: Gerard Pons, David Masip
- **Comment**: Preprint submitted to IJCV
- **Journal**: None
- **Summary**: Automated emotion recognition in the wild from facial images remains a challenging problem. Although recent advances in Deep Learning have supposed a significant breakthrough in this topic, strong changes in pose, orientation and point of view severely harm current approaches. In addition, the acquisition of labeled datasets is costly, and current state-of-the-art deep learning algorithms cannot model all the aforementioned difficulties. In this paper, we propose to apply a multi-task learning loss function to share a common feature representation with other related tasks. Particularly we show that emotion recognition benefits from jointly learning a model with a detector of facial Action Units (collective muscle movements). The proposed loss function addresses the problem of learning multiple tasks with heterogeneously labeled data, improving previous multi-task approaches. We validate the proposal using two datasets acquired in non controlled environments, and an application to predict compound facial emotion expressions.



### Disentangling 3D Pose in A Dendritic CNN for Unconstrained 2D Face Alignment
- **Arxiv ID**: http://arxiv.org/abs/1802.06713v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.06713v3)
- **Published**: 2018-02-19 17:15:06+00:00
- **Updated**: 2018-03-28 16:38:55+00:00
- **Authors**: Amit Kumar, Rama Chellappa
- **Comment**: CVPR'18
- **Journal**: None
- **Summary**: Heatmap regression has been used for landmark localization for quite a while now. Most of the methods use a very deep stack of bottleneck modules for heatmap classification stage, followed by heatmap regression to extract the keypoints. In this paper, we present a single dendritic CNN, termed as Pose Conditioned Dendritic Convolution Neural Network (PCD-CNN), where a classification network is followed by a second and modular classification network, trained in an end to end fashion to obtain accurate landmark points. Following a Bayesian formulation, we disentangle the 3D pose of a face image explicitly by conditioning the landmark estimation on pose, making it different from multi-tasking approaches. Extensive experimentation shows that conditioning on pose reduces the localization error by making it agnostic to face pose. The proposed model can be extended to yield variable number of landmark points and hence broadening its applicability to other datasets. Instead of increasing depth or width of the network, we train the CNN efficiently with Mask-Softmax Loss and hard sample mining to achieve upto $15\%$ reduction in error compared to state-of-the-art methods for extreme and medium pose face images from challenging datasets including AFLW, AFW, COFW and IBUG.



### Learning Representative Temporal Features for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1802.06724v4
- **DOI**: 10.1007/s11042-021-11022-8
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.06724v4)
- **Published**: 2018-02-19 17:36:24+00:00
- **Updated**: 2021-05-04 18:34:32+00:00
- **Authors**: Ali Javidani, Ahmad Mahmoudi-Aznaveh
- **Comment**: 15 pages
- **Journal**: None
- **Summary**: In this paper, a novel video classification method is presented that aims to recognize different categories of third-person videos efficiently. Our motivation is to achieve a light model that could be trained with insufficient training data. With this intuition, the processing of the 3-dimensional video input is broken to 1D in temporal dimension on top of the 2D in spatial. The processes related to 2D spatial frames are being done by utilizing pre-trained networks with no training phase. The only step which involves training is to classify the 1D time series resulted from the description of the 2D signals. As a matter of fact, optical flow images are first calculated from consecutive frames and described by pre-trained CNN networks. Their dimension is then reduced using PCA. By stacking the description vectors beside each other, a multi-channel time series is created for each video. Each channel of the time series represents a specific feature and follows it over time. The main focus of the proposed method is to classify the obtained time series effectively. Towards this, the idea is to let the machine learn temporal features. This is done by training a multi-channel one dimensional Convolutional Neural Network (1D-CNN). The 1D-CNN learns the features along the only temporal dimension. Hence, the number of training parameters decreases significantly which would result in the trainability of the method on even smaller datasets. It is illustrated that the proposed method could reach the state-of-the-art results on two public datasets UCF11, jHMDB and competitive results on HMDB51.



### Divide, Denoise, and Defend against Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/1802.06806v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1802.06806v2)
- **Published**: 2018-02-19 19:01:56+00:00
- **Updated**: 2019-04-25 22:32:22+00:00
- **Authors**: Seyed-Mohsen Moosavi-Dezfooli, Ashish Shrivastava, Oncel Tuzel
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks, although shown to be a successful class of machine learning algorithms, are known to be extremely unstable to adversarial perturbations. Improving the robustness of neural networks against these attacks is important, especially for security-critical applications. To defend against such attacks, we propose dividing the input image into multiple patches, denoising each patch independently, and reconstructing the image, without losing significant image content. We call our method D3. This proposed defense mechanism is non-differentiable which makes it non-trivial for an adversary to apply gradient-based attacks. Moreover, we do not fine-tune the network with adversarial examples, making it more robust against unknown attacks. We present an analysis of the tradeoff between accuracy and robustness against adversarial attacks. We evaluate our method under black-box, grey-box, and white-box settings. On the ImageNet dataset, our method outperforms the state-of-the-art by 19.7% under grey-box setting, and performs comparably under black-box setting. For the white-box setting, the proposed method achieves 34.4% accuracy compared to the 0% reported in the recent works.



### Shield: Fast, Practical Defense and Vaccination for Deep Learning using JPEG Compression
- **Arxiv ID**: http://arxiv.org/abs/1802.06816v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/1802.06816v1)
- **Published**: 2018-02-19 19:13:42+00:00
- **Updated**: 2018-02-19 19:13:42+00:00
- **Authors**: Nilaksh Das, Madhuri Shanbhogue, Shang-Tse Chen, Fred Hohman, Siwei Li, Li Chen, Michael E. Kounavis, Duen Horng Chau
- **Comment**: under review at KDD'18
- **Journal**: None
- **Summary**: The rapidly growing body of research in adversarial machine learning has demonstrated that deep neural networks (DNNs) are highly vulnerable to adversarially generated images. This underscores the urgent need for practical defense that can be readily deployed to combat attacks in real-time. Observing that many attack strategies aim to perturb image pixels in ways that are visually imperceptible, we place JPEG compression at the core of our proposed Shield defense framework, utilizing its capability to effectively "compress away" such pixel manipulation. To immunize a DNN model from artifacts introduced by compression, Shield "vaccinates" a model by re-training it with compressed images, where different compression levels are applied to generate multiple vaccinated models that are ultimately used together in an ensemble defense. On top of that, Shield adds an additional layer of protection by employing randomization at test time that compresses different regions of an image using random compression levels, making it harder for an adversary to estimate the transformation performed. This novel combination of vaccination, ensembling, and randomization makes Shield a fortified multi-pronged protection. We conducted extensive, large-scale experiments using the ImageNet dataset, and show that our approaches eliminate up to 94% of black-box attacks and 98% of gray-box attacks delivered by the recent, strongest attacks, such as Carlini-Wagner's L2 and DeepFool. Our approaches are fast and work without requiring knowledge about the model.



### Online Detection of Action Start in Untrimmed, Streaming Videos
- **Arxiv ID**: http://arxiv.org/abs/1802.06822v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.06822v3)
- **Published**: 2018-02-19 19:39:05+00:00
- **Updated**: 2018-07-23 05:15:15+00:00
- **Authors**: Zheng Shou, Junting Pan, Jonathan Chan, Kazuyuki Miyazawa, Hassan Mansour, Anthony Vetro, Xavier Giro-i-Nieto, Shih-Fu Chang
- **Comment**: Accepted by ECCV'18
- **Journal**: None
- **Summary**: We aim to tackle a novel task in action detection - Online Detection of Action Start (ODAS) in untrimmed, streaming videos. The goal of ODAS is to detect the start of an action instance, with high categorization accuracy and low detection latency. ODAS is important in many applications such as early alert generation to allow timely security or emergency response. We propose three novel methods to specifically address the challenges in training ODAS models: (1) hard negative samples generation based on Generative Adversarial Network (GAN) to distinguish ambiguous background, (2) explicitly modeling the temporal consistency between data around action start and data succeeding action start, and (3) adaptive sampling strategy to handle the scarcity of training data. We conduct extensive experiments using THUMOS'14 and ActivityNet. We show that our proposed methods lead to significant performance gains and improve the state-of-the-art methods. An ablation study confirms the effectiveness of each proposed method.



### Global Pose Estimation with an Attention-based Recurrent Network
- **Arxiv ID**: http://arxiv.org/abs/1802.06857v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1802.06857v1)
- **Published**: 2018-02-19 21:17:10+00:00
- **Updated**: 2018-02-19 21:17:10+00:00
- **Authors**: Emilio Parisotto, Devendra Singh Chaplot, Jian Zhang, Ruslan Salakhutdinov
- **Comment**: First two authors contributed equally
- **Journal**: None
- **Summary**: The ability for an agent to localize itself within an environment is crucial for many real-world applications. For unknown environments, Simultaneous Localization and Mapping (SLAM) enables incremental and concurrent building of and localizing within a map. We present a new, differentiable architecture, Neural Graph Optimizer, progressing towards a complete neural network solution for SLAM by designing a system composed of a local pose estimation model, a novel pose selection module, and a novel graph optimization process. The entire architecture is trained in an end-to-end fashion, enabling the network to automatically learn domain-specific features relevant to the visual odometry and avoid the involved process of feature engineering. We demonstrate the effectiveness of our system on a simulated 2D maze and the 3D ViZ-Doom environment.



### Automated soft tissue lesion detection and segmentation in digital mammography using a u-net deep learning network
- **Arxiv ID**: http://arxiv.org/abs/1802.06865v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.06865v2)
- **Published**: 2018-02-19 21:39:49+00:00
- **Updated**: 2018-03-08 09:21:11+00:00
- **Authors**: Timothy de Moor, Alejandro Rodriguez-Ruiz, Albert Gubern Mérida, Ritse Mann, Jonas Teuwen
- **Comment**: To appear in IWBI 2018
- **Journal**: None
- **Summary**: Computer-aided detection or decision support systems aim to improve breast cancer screening programs by helping radiologists to evaluate digital mammography (DM) exams. Commonly such methods proceed in two steps: selection of candidate regions for malignancy, and later classification as either malignant or not. In this study, we present a candidate detection method based on deep learning to automatically detect and additionally segment soft tissue lesions in DM. A database of DM exams (mostly bilateral and two views) was collected from our institutional archive. In total, 7196 DM exams (28294 DM images) acquired with systems from three different vendors (General Electric, Siemens, Hologic) were collected, of which 2883 contained malignant lesions verified with histopathology. Data was randomly split on an exam level into training (50\%), validation (10\%) and testing (40\%) of deep neural network with u-net architecture. The u-net classifies the image but also provides lesion segmentation. Free receiver operating characteristic (FROC) analysis was used to evaluate the model, on an image and on an exam level. On an image level, a maximum sensitivity of 0.94 at 7.93 false positives (FP) per image was achieved. Similarly, per exam a maximum sensitivity of 0.98 at 7.81 FP per image was achieved. In conclusion, the method could be used as a candidate selection model with high accuracy and with the additional information of lesion segmentation.



### Machine Learning Methods for Data Association in Multi-Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/1802.06897v2
- **DOI**: 10.1145/3394659
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.06897v2)
- **Published**: 2018-02-19 22:42:56+00:00
- **Updated**: 2020-08-25 22:42:17+00:00
- **Authors**: Patrick Emami, Panos M. Pardalos, Lily Elefteriadou, Sanjay Ranka
- **Comment**: Accepted for publication in ACM Computing Surveys
- **Journal**: Volume 53, Issue 4 (August 2020)
- **Summary**: Data association is a key step within the multi-object tracking pipeline that is notoriously challenging due to its combinatorial nature. A popular and general way to formulate data association is as the NP-hard multidimensional assignment problem (MDAP). Over the last few years, data-driven approaches to assignment have become increasingly prevalent as these techniques have started to mature. We focus this survey solely on learning algorithms for the assignment step of multi-object tracking, and we attempt to unify various methods by highlighting their connections to linear assignment as well as to the MDAP. First, we review probabilistic and end-to-end optimization approaches to data association, followed by methods that learn association affinities from data. We then compare the performance of the methods presented in this survey, and conclude by discussing future research directions.



### EV-FlowNet: Self-Supervised Optical Flow Estimation for Event-based Cameras
- **Arxiv ID**: http://arxiv.org/abs/1802.06898v4
- **DOI**: 10.15607/RSS.2018.XIV.062
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1802.06898v4)
- **Published**: 2018-02-19 22:47:52+00:00
- **Updated**: 2018-08-13 15:32:01+00:00
- **Authors**: Alex Zihao Zhu, Liangzhe Yuan, Kenneth Chaney, Kostas Daniilidis
- **Comment**: 9 pages, 5 figures, 1 table. Accompanying video:
  https://youtu.be/eMHZBSoq0sE. Dataset:
  https://daniilidis-group.github.io/mvsec/, Robotics: Science and Systems 2018
- **Journal**: None
- **Summary**: Event-based cameras have shown great promise in a variety of situations where frame based cameras suffer, such as high speed motions and high dynamic range scenes. However, developing algorithms for event measurements requires a new class of hand crafted algorithms. Deep learning has shown great success in providing model free solutions to many problems in the vision community, but existing networks have been developed with frame based images in mind, and there does not exist the wealth of labeled data for events as there does for images for supervised training. To these points, we present EV-FlowNet, a novel self-supervised deep learning pipeline for optical flow estimation for event based cameras. In particular, we introduce an image based representation of a given event stream, which is fed into a self-supervised neural network as the sole input. The corresponding grayscale images captured from the same camera at the same time as the events are then used as a supervisory signal to provide a loss function at training time, given the estimated flow from the network. We show that the resulting network is able to accurately predict optical flow from events only in a variety of different scenes, with performance competitive to image based networks. This method not only allows for accurate estimation of dense optical flow, but also provides a framework for the transfer of other self-supervised methods to the event-based domain.



