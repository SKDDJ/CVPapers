# Arxiv Papers in cs.CV on 2018-02-17
### Semi-supervised multi-task learning for lung cancer diagnosis
- **Arxiv ID**: http://arxiv.org/abs/1802.06181v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1802.06181v2)
- **Published**: 2018-02-17 03:49:32+00:00
- **Updated**: 2018-05-04 19:08:13+00:00
- **Authors**: Naji Khosravan, Ulas Bagci
- **Comment**: Accepted for publication at IEEE EMBC (40th International Engineering
  in Medicine and Biology Conference)
- **Journal**: None
- **Summary**: Early detection of lung nodules is of great importance in lung cancer screening. Existing research recognizes the critical role played by CAD systems in early detection and diagnosis of lung nodules. However, many CAD systems, which are used as cancer detection tools, produce a lot of false positives (FP) and require a further FP reduction step. Furthermore, guidelines for early diagnosis and treatment of lung cancer are consist of different shape and volume measurements of abnormalities. Segmentation is at the heart of our understanding of nodules morphology making it a major area of interest within the field of computer aided diagnosis systems. This study set out to test the hypothesis that joint learning of false positive (FP) nodule reduction and nodule segmentation can improve the computer aided diagnosis (CAD) systems' performance on both tasks. To support this hypothesis we propose a 3D deep multi-task CNN to tackle these two problems jointly. We tested our system on LUNA16 dataset and achieved an average dice similarity coefficient (DSC) of 91% as segmentation accuracy and a score of nearly 92% for FP reduction. As a proof of our hypothesis, we showed improvements of segmentation and FP reduction tasks over two baselines. Our results support that joint training of these two tasks through a multi-task learning approach improves system performance on both. We also showed that a semi-supervised approach can be used to overcome the limitation of lack of labeled data for the 3D segmentation task.



### HWNet v2: An Efficient Word Image Representation for Handwritten Documents
- **Arxiv ID**: http://arxiv.org/abs/1802.06194v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.06194v2)
- **Published**: 2018-02-17 05:12:03+00:00
- **Updated**: 2019-03-19 09:07:02+00:00
- **Authors**: Praveen Krishnan, C. V. Jawahar
- **Comment**: 20 pages, 14 figures
- **Journal**: None
- **Summary**: We present a framework for learning an efficient holistic representation for handwritten word images. The proposed method uses a deep convolutional neural network with traditional classification loss. The major strengths of our work lie in: (i) the efficient usage of synthetic data to pre-train a deep network, (ii) an adapted version of the ResNet-34 architecture with the region of interest pooling (referred to as HWNet v2) which learns discriminative features for variable sized word images, and (iii) a realistic augmentation of training data with multiple scales and distortions which mimics the natural process of handwriting. We further investigate the process of transfer learning to reduce the domain gap between synthetic and real domain, and also analyze the invariances learned at different layers of the network using visualization techniques proposed in the literature.   Our representation leads to a state-of-the-art word spotting performance on standard handwritten datasets and historical manuscripts in different languages with minimal representation size. On the challenging IAM dataset, our method is first to report an mAP of around 0.90 for word spotting with a representation size of just 32 dimensions. Furthermore, we also present results on printed document datasets in English and Indic scripts which validates the generic nature of the proposed framework for learning word image representation.



### Towards Principled Design of Deep Convolutional Networks: Introducing SimpNet
- **Arxiv ID**: http://arxiv.org/abs/1802.06205v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.06205v1)
- **Published**: 2018-02-17 07:53:58+00:00
- **Updated**: 2018-02-17 07:53:58+00:00
- **Authors**: Seyyed Hossein Hasanpour, Mohammad Rouhani, Mohsen Fayyaz, Mohammad Sabokrou, Ehsan Adeli
- **Comment**: The Submitted version to the IEEE TIP on December 2017, replaced high
  resolution images with low-res counterparts due to arXiv size limitation, 19
  pages
- **Journal**: None
- **Summary**: Major winning Convolutional Neural Networks (CNNs), such as VGGNet, ResNet, DenseNet, \etc, include tens to hundreds of millions of parameters, which impose considerable computation and memory overheads. This limits their practical usage in training and optimizing for real-world applications. On the contrary, light-weight architectures, such as SqueezeNet, are being proposed to address this issue. However, they mainly suffer from low accuracy, as they have compromised between the processing power and efficiency. These inefficiencies mostly stem from following an ad-hoc designing procedure. In this work, we discuss and propose several crucial design principles for an efficient architecture design and elaborate intuitions concerning different aspects of the design procedure. Furthermore, we introduce a new layer called {\it SAF-pooling} to improve the generalization power of the network while keeping it simple by choosing best features. Based on such principles, we propose a simple architecture called {\it SimpNet}. We empirically show that SimpNet provides a good trade-off between the computation/memory efficiency and the accuracy solely based on these primitive but crucial principles. SimpNet outperforms the deeper and more complex architectures such as VGGNet, ResNet, WideResidualNet \etc, on several well-known benchmarks, while having 2 to 25 times fewer number of parameters and operations. We obtain state-of-the-art results (in terms of a balance between the accuracy and the number of involved parameters) on standard datasets, such as CIFAR10, CIFAR100, MNIST and SVHN. The implementations are available at \href{url}{https://github.com/Coderx7/SimpNet}.



### A New De-blurring Technique for License Plate Images with Robust Length Estimation
- **Arxiv ID**: http://arxiv.org/abs/1802.06214v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.06214v1)
- **Published**: 2018-02-17 08:59:03+00:00
- **Updated**: 2018-02-17 08:59:03+00:00
- **Authors**: P. S. Prashanth Rao, Rajesh Kumar Muthu
- **Comment**: Accepted and Published in 2017 IEEE International Conference on
  Intelligent Computing and Control (I2C2),23 Jun - 24 Jun 2017, India
- **Journal**: None
- **Summary**: Recognizing a license plate clearly while seeing a surveillance camera snapshot is often important in cases where the troublemaker vehicle(s) have to be identified. In many real world situations, these images are blurred due to fast motion of the vehicle and cannot be recognized by the human eye. For this kind of blurring, the kernel involved can be said to be a linear uniform convolution described by its angle and length. We propose a new de-blurring technique in this paper to parametrically estimate the kernel as accurately as possible with emphasis on the length estimation process. We use a technique which employs Hough transform in estimating the kernel angle. To accurately estimate the kernel length, a novel approach using the cepstral transform is introduced. We compare the de-blurred results obtained using our scheme with those of other recently introduced blind de-blurring techniques. The comparisons corroborate that our scheme can remove a large blur from the image captured by the camera to recover vital semantic information about the license plate.



### Connectivity-Driven Parcellation Methods for the Human Cerebral Cortex
- **Arxiv ID**: http://arxiv.org/abs/1802.06772v1
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1802.06772v1)
- **Published**: 2018-02-17 13:00:27+00:00
- **Updated**: 2018-02-17 13:00:27+00:00
- **Authors**: Salim Arslan
- **Comment**: Abstract is summarised to satisfy the character limit imposed by
  Arxiv. Please refer to the pdf for the full text. Forked from
  https://spiral.imperial.ac.uk/handle/10044/1/54760
- **Journal**: None
- **Summary**: In this thesis, we present robust and fully-automated methods for the subdivision of the entire human cerebral cortex based on connectivity information. Our contributions are four-fold: First, we propose a clustering approach to delineate a cortical parcellation that provides a reliable abstraction of the brain's functional organisation. Second, we cast the parcellation problem as a feature reduction problem and make use of manifold learning and image segmentation techniques to identify cortical regions with distinct structural connectivity patterns. Third, we present a multi-layer graphical model that combines within- and between-subject connectivity, which is then decomposed into a cortical parcellation that can represent the whole population, while accounting for the variability across subjects. Finally, we conduct a large-scale, systematic comparison of existing parcellation methods, with a focus on providing some insight into the reliability of brain parcellations in terms of reflecting the underlying connectivity, as well as, revealing their impact on network analysis.   We evaluate the proposed parcellation methods on publicly available data from the Human Connectome Project and a plethora of quantitative and qualitative evaluation techniques investigated in the literature. Experiments across multiple resolutions demonstrate the accuracy of the presented methods at both subject and group levels with regards to reproducibility and fidelity to the data. The neuro-biological interpretation of the proposed parcellations is also investigated by comparing parcel boundaries with well-structured properties of the cerebral cortex. Results show the advantage of connectivity-driven parcellations over traditional approaches in terms of better fitting the underlying connectivity.



### Exact and Consistent Interpretation for Piecewise Linear Neural Networks: A Closed Form Solution
- **Arxiv ID**: http://arxiv.org/abs/1802.06259v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1802.06259v2)
- **Published**: 2018-02-17 16:47:32+00:00
- **Updated**: 2019-09-12 17:21:14+00:00
- **Authors**: Lingyang Chu, Xia Hu, Juhua Hu, Lanjun Wang, Jian Pei
- **Comment**: KDD 2018
- **Journal**: None
- **Summary**: Strong intelligent machines powered by deep neural networks are increasingly deployed as black boxes to make decisions in risk-sensitive domains, such as finance and medical. To reduce potential risk and build trust with users, it is critical to interpret how such machines make their decisions. Existing works interpret a pre-trained neural network by analyzing hidden neurons, mimicking pre-trained models or approximating local predictions. However, these methods do not provide a guarantee on the exactness and consistency of their interpretation. In this paper, we propose an elegant closed form solution named $OpenBox$ to compute exact and consistent interpretations for the family of Piecewise Linear Neural Networks (PLNN). The major idea is to first transform a PLNN into a mathematically equivalent set of linear classifiers, then interpret each linear classifier by the features that dominate its prediction. We further apply $OpenBox$ to demonstrate the effectiveness of non-negative and sparse constraints on improving the interpretability of PLNNs. The extensive experiments on both synthetic and real world data sets clearly demonstrate the exactness and consistency of our interpretation.



### A Collaborative Computer Aided Diagnosis (C-CAD) System with Eye-Tracking, Sparse Attentional Model, and Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1802.06260v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1802.06260v2)
- **Published**: 2018-02-17 17:20:50+00:00
- **Updated**: 2018-04-28 15:06:18+00:00
- **Authors**: Naji Khosravan, Haydar Celik, Baris Turkbey, Elizabeth Jones, Bradford Wood, Ulas Bagci
- **Comment**: Submitted to Medical Image Analysis Journal (MedIA)
- **Journal**: None
- **Summary**: There are at least two categories of errors in radiology screening that can lead to suboptimal diagnostic decisions and interventions:(i)human fallibility and (ii)complexity of visual search. Computer aided diagnostic (CAD) tools are developed to help radiologists to compensate for some of these errors. However, despite their significant improvements over conventional screening strategies, most CAD systems do not go beyond their use as second opinion tools due to producing a high number of false positives, which human interpreters need to correct. In parallel with efforts in computerized analysis of radiology scans, several researchers have examined behaviors of radiologists while screening medical images to better understand how and why they miss tumors, how they interact with the information in an image, and how they search for unknown pathology in the images. Eye-tracking tools have been instrumental in exploring answers to these fundamental questions. In this paper, we aim to develop a paradigm shift CAD system, called collaborative CAD (C-CAD), that unifies both of the above mentioned research lines: CAD and eye-tracking. We design an eye-tracking interface providing radiologists with a real radiology reading room experience. Then, we propose a novel algorithm that unifies eye-tracking data and a CAD system. Specifically, we present a new graph based clustering and sparsification algorithm to transform eye-tracking data (gaze) into a signal model to interpret gaze patterns quantitatively and qualitatively. The proposed C-CAD collaborates with radiologists via eye-tracking technology and helps them to improve diagnostic decisions. The C-CAD learns radiologists' search efficiency by processing their gaze patterns. To do this, the C-CAD uses a deep learning algorithm in a newly designed multi-task learning platform to segment and diagnose cancers simultaneously.



