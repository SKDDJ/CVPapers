# Arxiv Papers in cs.CV on 2018-02-06
### Toward Marker-free 3D Pose Estimation in Lifting: A Deep Multi-view Solution
- **Arxiv ID**: http://arxiv.org/abs/1802.01741v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.01741v1)
- **Published**: 2018-02-06 00:28:44+00:00
- **Updated**: 2018-02-06 00:28:44+00:00
- **Authors**: Rahil Mehrizi, Xi Peng, Zhiqiang Tang, Xu Xu, Dimitris Metaxas, Kang Li
- **Comment**: FG2018, accepted as a long paper
- **Journal**: None
- **Summary**: Lifting is a common manual material handling task performed in the workplaces. It is considered as one of the main risk factors for Work-related Musculoskeletal Disorders. To improve work place safety, it is necessary to assess musculoskeletal and biomechanical risk exposures associated with these tasks, which requires very accurate 3D pose. Existing approaches mainly utilize marker-based sensors to collect 3D information. However, these methods are usually expensive to setup, time-consuming in process, and sensitive to the surrounding environment. In this study, we propose a multi-view based deep perceptron approach to address aforementioned limitations. Our approach consists of two modules: a "view-specific perceptron" network extracts rich information independently from the image of view, which includes both 2D shape and hierarchical texture information; while a "multi-view integration" network synthesizes information from all available views to predict accurate 3D pose. To fully evaluate our approach, we carried out comprehensive experiments to compare different variants of our design. The results prove that our approach achieves comparable performance with former marker-based methods, i.e. an average error of $14.72 \pm 2.96$ mm on the lifting dataset. The results are also compared with state-of-the-art methods on HumanEva-I dataset, which demonstrates the superior performance of our approach.



### Highly accurate model for prediction of lung nodule malignancy with CT scans
- **Arxiv ID**: http://arxiv.org/abs/1802.01756v1
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.QM, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1802.01756v1)
- **Published**: 2018-02-06 01:42:21+00:00
- **Updated**: 2018-02-06 01:42:21+00:00
- **Authors**: Jason Causey, Junyu Zhang, Shiqian Ma, Bo Jiang, Jake Qualls, David G. Politte, Fred Prior, Shuzhong Zhang, Xiuzhen Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Computed tomography (CT) examinations are commonly used to predict lung nodule malignancy in patients, which are shown to improve noninvasive early diagnosis of lung cancer. It remains challenging for computational approaches to achieve performance comparable to experienced radiologists. Here we present NoduleX, a systematic approach to predict lung nodule malignancy from CT data, based on deep learning convolutional neural networks (CNN). For training and validation, we analyze >1000 lung nodules in images from the LIDC/IDRI cohort. All nodules were identified and classified by four experienced thoracic radiologists who participated in the LIDC project. NoduleX achieves high accuracy for nodule malignancy classification, with an AUC of ~0.99. This is commensurate with the analysis of the dataset by experienced radiologists. Our approach, NoduleX, provides an effective framework for highly accurate nodule malignancy prediction with the model trained on a large patient population. Our results are replicable with software available at http://bioinformatics.astate.edu/NoduleX.



### Scale-recurrent Network for Deep Image Deblurring
- **Arxiv ID**: http://arxiv.org/abs/1802.01770v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.01770v1)
- **Published**: 2018-02-06 03:00:40+00:00
- **Updated**: 2018-02-06 03:00:40+00:00
- **Authors**: Xin Tao, Hongyun Gao, Yi Wang, Xiaoyong Shen, Jue Wang, Jiaya Jia
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: In single image deblurring, the "coarse-to-fine" scheme, i.e. gradually restoring the sharp image on different resolutions in a pyramid, is very successful in both traditional optimization-based methods and recent neural-network-based approaches. In this paper, we investigate this strategy and propose a Scale-recurrent Network (SRN-DeblurNet) for this deblurring task. Compared with the many recent learning-based approaches in [25], it has a simpler network structure, a smaller number of parameters and is easier to train. We evaluate our method on large-scale deblurring datasets with complex motion. Results show that our method can produce better quality results than state-of-the-arts, both quantitatively and qualitatively.



### Brute-Force Facial Landmark Analysis With A 140,000-Way Classifier
- **Arxiv ID**: http://arxiv.org/abs/1802.01777v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.01777v2)
- **Published**: 2018-02-06 03:20:41+00:00
- **Updated**: 2018-02-14 18:26:16+00:00
- **Authors**: Mengtian Li, Laszlo Jeni, Deva Ramanan
- **Comment**: In AAAI 2018, code can be find at https://github.com/mtli/BFFL
- **Journal**: None
- **Summary**: We propose a simple approach to visual alignment, focusing on the illustrative task of facial landmark estimation. While most prior work treats this as a regression problem, we instead formulate it as a discrete $K$-way classification task, where a classifier is trained to return one of $K$ discrete alignments. One crucial benefit of a classifier is the ability to report back a (softmax) distribution over putative alignments. We demonstrate that this distribution is a rich representation that can be marginalized (to generate uncertainty estimates over groups of landmarks) and conditioned on (to incorporate top-down context, provided by temporal constraints in a video stream or an interactive human user). Such capabilities are difficult to integrate into classic regression-based approaches. We study performance as a function of the number of classes $K$, including the extreme "exemplar class" setting where $K$ is equal to the number of training examples (140K in our setting). Perhaps surprisingly, we show that classifiers can still be learned in this setting. When compared to prior work in classification, our $K$ is unprecedentedly large, including many "fine-grained" classes that are very similar. We address these issues by using a multi-label loss function that allows for training examples to be non-uniformly shared across discrete classes. We perform a comprehensive experimental analysis of our method on standard benchmarks, demonstrating state-of-the-art results for facial alignment in videos.



### Face Detection Using Improved Faster RCNN
- **Arxiv ID**: http://arxiv.org/abs/1802.02142v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.02142v1)
- **Published**: 2018-02-06 05:25:02+00:00
- **Updated**: 2018-02-06 05:25:02+00:00
- **Authors**: Changzheng Zhang, Xiang Xu, Dandan Tu
- **Comment**: None
- **Journal**: None
- **Summary**: Faster RCNN has achieved great success for generic object detection including PASCAL object detection and MS COCO object detection. In this report, we propose a detailed designed Faster RCNN method named FDNet1.0 for face detection. Several techniques were employed including multi-scale training, multi-scale testing, light-designed RCNN, some tricks for inference and a vote-based ensemble method. Our method achieves two 1th places and one 2nd place in three tasks over WIDER FACE validation dataset (easy set, medium set, hard set).



### Digital Watermarking for Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1802.02601v1
- **DOI**: 10.1007/s13735-018-0147-1
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.02601v1)
- **Published**: 2018-02-06 05:32:36+00:00
- **Updated**: 2018-02-06 05:32:36+00:00
- **Authors**: Yuki Nagai, Yusuke Uchida, Shigeyuki Sakazawa, Shin'ichi Satoh
- **Comment**: This is a pre-print of an article published in International Journal
  of Multimedia Information Retrieval. The final authenticated version is
  available online at: https://doi.org/10.1007/s13735-018-0147-1 . arXiv admin
  note: substantial text overlap with arXiv:1701.04082
- **Journal**: None
- **Summary**: Although deep neural networks have made tremendous progress in the area of multimedia representation, training neural models requires a large amount of data and time. It is well-known that utilizing trained models as initial weights often achieves lower training error than neural networks that are not pre-trained. A fine-tuning step helps to reduce both the computational cost and improve performance. Therefore, sharing trained models has been very important for the rapid progress of research and development. In addition, trained models could be important assets for the owner(s) who trained them, hence we regard trained models as intellectual property. In this paper, we propose a digital watermarking technology for ownership authorization of deep neural networks. First, we formulate a new problem: embedding watermarks into deep neural networks. We also define requirements, embedding situations, and attack types on watermarking in deep neural networks. Second, we propose a general framework for embedding a watermark in model parameters, using a parameter regularizer. Our approach does not impair the performance of networks into which a watermark is placed because the watermark is embedded while training the host network. Finally, we perform comprehensive experiments to reveal the potential of watermarking deep neural networks as the basis of this new research effort. We show that our framework can embed a watermark during the training of a deep neural network from scratch, and during fine-tuning and distilling, without impairing its performance. The embedded watermark does not disappear even after fine-tuning or parameter pruning; the watermark remains complete even after 65% of parameters are pruned.



### Rollable Latent Space for Azimuth Invariant SAR Target Recognition
- **Arxiv ID**: http://arxiv.org/abs/1802.01821v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.01821v2)
- **Published**: 2018-02-06 06:59:05+00:00
- **Updated**: 2018-04-20 01:17:20+00:00
- **Authors**: Kazutoshi Sagi, Takahiro Toizumi, Yuzo Senda
- **Comment**: 4 pages, Accepted in International Geoscience and Remote Sensing
  Symposium (IGARSS) 2018
- **Journal**: None
- **Summary**: This paper proposes rollable latent space (RLS) for an azimuth invariant synthetic aperture radar (SAR) target recognition. Scarce labeled data and limited viewing direction are critical issues in SAR target recognition.The RLS is a designed space in which rolling of latent features corresponds to 3D rotation of an object. Thus latent features of an arbitrary view can be inferred using those of different views. This characteristic further enables us to augment data from limited viewing in RLS. RLS-based classifiers with and without data augmentation and a conventional classifier trained with target front shots are evaluated over untrained target back shots. Results show that the RLS-based classifier with augmentation improves an accuracy by 30% compared to the conventional classifier.



### Geometry-Contrastive GAN for Facial Expression Transfer
- **Arxiv ID**: http://arxiv.org/abs/1802.01822v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.01822v2)
- **Published**: 2018-02-06 07:09:13+00:00
- **Updated**: 2018-10-22 08:56:09+00:00
- **Authors**: Fengchun Qiao, Naiming Yao, Zirui Jiao, Zhihao Li, Hui Chen, Hongan Wang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a Geometry-Contrastive Generative Adversarial Network (GC-GAN) for transferring continuous emotions across different subjects. Given an input face with certain emotion and a target facial expression from another subject, GC-GAN can generate an identity-preserving face with the target expression. Geometry information is introduced into cGANs as continuous conditions to guide the generation of facial expressions. In order to handle the misalignment across different subjects or emotions, contrastive learning is used to transform geometry manifold into an embedded semantic manifold of facial expressions. Therefore, the embedded geometry is injected into the latent space of GANs and control the emotion generation effectively. Experimental results demonstrate that our proposed method can be applied in facial expression transfer even there exist big differences in facial shapes and expressions between different subjects.



### Fast Piecewise-Affine Motion Estimation Without Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1802.01872v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.01872v1)
- **Published**: 2018-02-06 10:15:30+00:00
- **Updated**: 2018-02-06 10:15:30+00:00
- **Authors**: Denis Fortun, Martin Storath, Dennis Rickert, Andreas Weinmann, Michael Unser
- **Comment**: None
- **Journal**: None
- **Summary**: Current algorithmic approaches for piecewise affine motion estimation are based on alternating motion segmentation and estimation. We propose a new method to estimate piecewise affine motion fields directly without intermediate segmentation. To this end, we reformulate the problem by imposing piecewise constancy of the parameter field, and derive a specific proximal splitting optimization scheme. A key component of our framework is an efficient one-dimensional piecewise-affine estimator for vector-valued signals. The first advantage of our approach over segmentation-based methods is its absence of initialization. The second advantage is its lower computational cost which is independent of the complexity of the motion field. In addition to these features, we demonstrate competitive accuracy with other piecewise-parametric methods on standard evaluation benchmarks. Our new regularization scheme also outperforms the more standard use of total variation and total generalized variation.



### Every Smile is Unique: Landmark-Guided Diverse Smile Generation
- **Arxiv ID**: http://arxiv.org/abs/1802.01873v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.01873v3)
- **Published**: 2018-02-06 10:15:39+00:00
- **Updated**: 2018-03-28 11:44:33+00:00
- **Authors**: Wei Wang, Xavier Alameda-Pineda, Dan Xu, Pascal Fua, Elisa Ricci, Nicu Sebe
- **Comment**: Accepted as a poster in Conference on Computer Vision and Pattern
  Recognition (CVPR), 2018
- **Journal**: None
- **Summary**: Each smile is unique: one person surely smiles in different ways (e.g., closing/opening the eyes or mouth). Given one input image of a neutral face, can we generate multiple smile videos with distinctive characteristics? To tackle this one-to-many video generation problem, we propose a novel deep learning architecture named Conditional Multi-Mode Network (CMM-Net). To better encode the dynamics of facial expressions, CMM-Net explicitly exploits facial landmarks for generating smile sequences. Specifically, a variational auto-encoder is used to learn a facial landmark embedding. This single embedding is then exploited by a conditional recurrent network which generates a landmark embedding sequence conditioned on a specific expression (e.g., spontaneous smile). Next, the generated landmark embeddings are fed into a multi-mode recurrent landmark generator, producing a set of landmark sequences still associated to the given smile class but clearly distinct from each other. Finally, these landmark sequences are translated into face videos. Our experimental results demonstrate the effectiveness of our CMM-Net in generating realistic videos of multiple smile expressions.



### Learning Image Representations by Completing Damaged Jigsaw Puzzles
- **Arxiv ID**: http://arxiv.org/abs/1802.01880v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.01880v1)
- **Published**: 2018-02-06 10:42:28+00:00
- **Updated**: 2018-02-06 10:42:28+00:00
- **Authors**: Dahun Kim, Donghyeon Cho, Donggeun Yoo, In So Kweon
- **Comment**: accepted at WACV 2018
- **Journal**: None
- **Summary**: In this paper, we explore methods of complicating self-supervised tasks for representation learning. That is, we do severe damage to data and encourage a network to recover them. First, we complicate each of three powerful self-supervised task candidates: jigsaw puzzle, inpainting, and colorization. In addition, we introduce a novel complicated self-supervised task called "Completing damaged jigsaw puzzles" which is puzzles with one piece missing and the other pieces without color. We train a convolutional neural network not only to solve the puzzles, but also generate the missing content and colorize the puzzles. The recovery of the aforementioned damage pushes the network to obtain robust and general-purpose representations. We demonstrate that complicating the self-supervised tasks improves their original versions and that our final task learns more robust and transferable representations compared to the previous methods, as well as the simple combination of our candidate tasks. Our approach achieves state-of-the-art performance in transfer learning on PASCAL classification and semantic segmentation.



### The steerable graph Laplacian and its application to filtering image data-sets
- **Arxiv ID**: http://arxiv.org/abs/1802.01894v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1802.01894v2)
- **Published**: 2018-02-06 11:56:25+00:00
- **Updated**: 2018-08-07 19:00:04+00:00
- **Authors**: Boris Landa, Yoel Shkolnisky
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, improvements in various image acquisition techniques gave rise to the need for adaptive processing methods, aimed particularly for large datasets corrupted by noise and deformations. In this work, we consider datasets of images sampled from a low-dimensional manifold (i.e. an image-valued manifold), where the images can assume arbitrary planar rotations. To derive an adaptive and rotation-invariant framework for processing such datasets, we introduce a graph Laplacian (GL)-like operator over the dataset, termed ${\textit{steerable graph Laplacian}}$. Essentially, the steerable GL extends the standard GL by accounting for all (infinitely-many) planar rotations of all images. As it turns out, similarly to the standard GL, a properly normalized steerable GL converges to the Laplace-Beltrami operator on the low-dimensional manifold. However, the steerable GL admits an improved convergence rate compared to the GL, where the improved convergence behaves as if the intrinsic dimension of the underlying manifold is lower by one. Moreover, it is shown that the steerable GL admits eigenfunctions of the form of Fourier modes (along the orbits of the images' rotations) multiplied by eigenvectors of certain matrices, which can be computed efficiently by the FFT. For image datasets corrupted by noise, we employ a subset of these eigenfunctions to "filter" the dataset via a Fourier-like filtering scheme, essentially using all images and their rotations simultaneously. We demonstrate our filtering framework by de-noising simulated single-particle cryo-EM image datasets.



### Deep Inference of Personality Traits by Integrating Image and Word Use in Social Networks
- **Arxiv ID**: http://arxiv.org/abs/1802.06757v1
- **DOI**: None
- **Categories**: **cs.CY**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1802.06757v1)
- **Published**: 2018-02-06 11:58:58+00:00
- **Updated**: 2018-02-06 11:58:58+00:00
- **Authors**: Guillem Cucurull, Pau Rodríguez, V. Oguz Yazici, Josep M. Gonfaus, F. Xavier Roca, Jordi Gonzàlez
- **Comment**: None
- **Journal**: None
- **Summary**: Social media, as a major platform for communication and information exchange, is a rich repository of the opinions and sentiments of 2.3 billion users about a vast spectrum of topics. To sense the whys of certain social user's demands and cultural-driven interests, however, the knowledge embedded in the 1.8 billion pictures which are uploaded daily in public profiles has just started to be exploited since this process has been typically been text-based. Following this trend on visual-based social analysis, we present a novel methodology based on Deep Learning to build a combined image-and-text based personality trait model, trained with images posted together with words found highly correlated to specific personality traits. So the key contribution here is to explore whether OCEAN personality trait modeling can be addressed based on images, here called \emph{Mind{P}ics}, appearing with certain tags with psychological insights. We found that there is a correlation between those posted images and their accompanying texts, which can be successfully modeled using deep neural networks for personality estimation. The experimental results are consistent with previous cyber-psychology results based on texts or images. In addition, classification results on some traits show that some patterns emerge in the set of images corresponding to a specific text, in essence to those representing an abstract concept. These results open new avenues of research for further refining the proposed personality model under the supervision of psychology experts.



### Attribute-Guided Network for Cross-Modal Zero-Shot Hashing
- **Arxiv ID**: http://arxiv.org/abs/1802.01943v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.01943v1)
- **Published**: 2018-02-06 13:43:06+00:00
- **Updated**: 2018-02-06 13:43:06+00:00
- **Authors**: Zhong Ji, Yuxin Sun, Yunlong Yu, Yanwei Pang, Jungong Han
- **Comment**: 9 pages, 8 figures
- **Journal**: None
- **Summary**: Zero-Shot Hashing aims at learning a hashing model that is trained only by instances from seen categories but can generate well to those of unseen categories. Typically, it is achieved by utilizing a semantic embedding space to transfer knowledge from seen domain to unseen domain. Existing efforts mainly focus on single-modal retrieval task, especially Image-Based Image Retrieval (IBIR). However, as a highlighted research topic in the field of hashing, cross-modal retrieval is more common in real world applications. To address the Cross-Modal Zero-Shot Hashing (CMZSH) retrieval task, we propose a novel Attribute-Guided Network (AgNet), which can perform not only IBIR, but also Text-Based Image Retrieval (TBIR). In particular, AgNet aligns different modal data into a semantically rich attribute space, which bridges the gap caused by modality heterogeneity and zero-shot setting. We also design an effective strategy that exploits the attribute to guide the generation of hash codes for image and text within the same network. Extensive experimental results on three benchmark datasets (AwA, SUN, and ImageNet) demonstrate the superiority of AgNet on both cross-modal and single-modal zero-shot image retrieval tasks.



### Multimodal Image Captioning for Marketing Analysis
- **Arxiv ID**: http://arxiv.org/abs/1802.01958v2
- **DOI**: 10.1109/MIPR.2018.00035
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.01958v2)
- **Published**: 2018-02-06 14:23:32+00:00
- **Updated**: 2019-05-06 10:35:53+00:00
- **Authors**: Philipp Harzig, Stephan Brehm, Rainer Lienhart, Carolin Kaiser, René Schallner
- **Comment**: 4 pages, 1 figure, accepted at MIPR2018
- **Journal**: None
- **Summary**: Automatically captioning images with natural language sentences is an important research topic. State of the art models are able to produce human-like sentences. These models typically describe the depicted scene as a whole and do not target specific objects of interest or emotional relationships between these objects in the image. However, marketing companies require to describe these important attributes of a given scene. In our case, objects of interest are consumer goods, which are usually identifiable by a product logo and are associated with certain brands. From a marketing point of view, it is desirable to also evaluate the emotional context of a trademarked product, i.e., whether it appears in a positive or a negative connotation. We address the problem of finding brands in images and deriving corresponding captions by introducing a modified image captioning network. We also add a third output modality, which simultaneously produces real-valued image ratings. Our network is trained using a classification-aware loss function in order to stimulate the generation of sentences with an emphasis on words identifying the brand of a product. We evaluate our model on a dataset of images depicting interactions between humans and branded products. The introduced network improves mean class accuracy by 24.5 percent. Thanks to adding the third output modality, it also considerably improves the quality of generated captions for images depicting branded products.



### Orthogonally Regularized Deep Networks For Image Super-resolution
- **Arxiv ID**: http://arxiv.org/abs/1802.02018v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.02018v1)
- **Published**: 2018-02-06 15:57:49+00:00
- **Updated**: 2018-02-06 15:57:49+00:00
- **Authors**: Tiantong Guo, Hojjat S. Mousavi, Vishal Monga
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning methods, in particular trained Convolutional Neural Networks (CNNs) have recently been shown to produce compelling state-of-the-art results for single image Super-Resolution (SR). Invariably, a CNN is learned to map the low resolution (LR) image to its corresponding high resolution (HR) version in the spatial domain. Aiming for faster inference and more efficient solutions than solving the SR problem in the spatial domain, we propose a novel network structure for learning the SR mapping function in an image transform domain, specifically the Discrete Cosine Transform (DCT). As a first contribution, we show that DCT can be integrated into the network structure as a Convolutional DCT (CDCT) layer. We further extend the network to allow the CDCT layer to become trainable (i.e. optimizable). Because this layer represents an image transform, we enforce pairwise orthogonality constraints on the individual basis functions/filters. This Orthogonally Regularized Deep SR network (ORDSR) simplifies the SR task by taking advantage of image transform domain while adapting the design of transform basis to the training image set.



### DeepTravel: a Neural Network Based Travel Time Estimation Model with Auxiliary Supervision
- **Arxiv ID**: http://arxiv.org/abs/1802.02147v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1802.02147v1)
- **Published**: 2018-02-06 16:08:06+00:00
- **Updated**: 2018-02-06 16:08:06+00:00
- **Authors**: Hanyuan Zhang, Hao Wu, Weiwei Sun, Baihua Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: Estimating the travel time of a path is of great importance to smart urban mobility. Existing approaches are either based on estimating the time cost of each road segment which are not able to capture many cross-segment complex factors, or designed heuristically in a non-learning-based way which fail to utilize the existing abundant temporal labels of the data, i.e., the time stamp of each trajectory point. In this paper, we leverage on new development of deep neural networks and propose a novel auxiliary supervision model, namely DeepTravel, that can automatically and effectively extract different features, as well as make full use of the temporal labels of the trajectory data. We have conducted comprehensive experiments on real datasets to demonstrate the out-performance of DeepTravel over existing approaches.



### Multispectral Compressive Imaging Strategies using Fabry-Pérot Filtered Sensors
- **Arxiv ID**: http://arxiv.org/abs/1802.02040v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1802.02040v1)
- **Published**: 2018-02-06 16:23:42+00:00
- **Updated**: 2018-02-06 16:23:42+00:00
- **Authors**: Kévin Degraux, Valerio Cambareri, Bert Geelen, Laurent Jacques, Gauthier Lafruit
- **Comment**: 19 pages, 7 figures
- **Journal**: None
- **Summary**: This paper introduces two acquisition device architectures for multispectral compressive imaging. Unlike most existing methods, the proposed computational imaging techniques do not include any dispersive element, as they use a dedicated sensor which integrates narrowband Fabry-P\'erot spectral filters at the pixel level. The first scheme leverages joint inpainting and super-resolution to fill in those voxels that are missing due to the device's limited pixel count. The second scheme, in link with compressed sensing, introduces spatial random convolutions, but is more complex and may be affected by diffraction. In both cases we solve the associated inverse problems by using the same signal prior. Specifically, we propose a redundant analysis signal prior in a convex formulation. Through numerical simulations, we explore different realistic setups. Our objective is also to highlight some practical guidelines and discuss their complexity trade-offs to integrate these schemes into actual computational imaging systems. Our conclusion is that the second technique performs best at high compression levels, in a properly sized and calibrated setup. Otherwise, the first, simpler technique should be favored.



### Multi-Temporal Land Cover Classification with Sequential Recurrent Encoders
- **Arxiv ID**: http://arxiv.org/abs/1802.02080v4
- **DOI**: 10.3390/ijgi7040129
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.02080v4)
- **Published**: 2018-02-06 17:13:05+00:00
- **Updated**: 2018-04-07 18:20:04+00:00
- **Authors**: Marc Rußwurm, Marco Körner
- **Comment**: None
- **Journal**: ISPRS Int. J. Geo-Inf. 2018, 7, 129
- **Summary**: Earth observation (EO) sensors deliver data with daily or weekly temporal resolution. Most land use and land cover (LULC) approaches, however, expect cloud-free and mono-temporal observations. The increasing temporal capabilities of today's sensors enables the use of temporal, along with spectral and spatial features. Domains, such as speech recognition or neural machine translation, work with inherently temporal data and, today, achieve impressive results using sequential encoder-decoder structures. Inspired by these sequence-to-sequence models, we adapt an encoder structure with convolutional recurrent layers in order to approximate a phenological model for vegetation classes based on a temporal sequence of Sentinel 2 (S2) images. In our experiments, we visualize internal activations over a sequence of cloudy and non-cloudy images and find several recurrent cells, which reduce the input activity for cloudy observations. Hence, we assume that our network has learned cloud-filtering schemes solely from input data, which could alleviate the need for tedious cloud-filtering as a preprocessing step for many EO approaches. Moreover, using unfiltered temporal series of top-of-atmosphere (TOA) reflectance data, we achieved in our experiments state-of-the-art classification accuracies on a large number of crop classes with minimal preprocessing compared to other classification approaches.



### A Log-Euclidean and Total Variation based Variational Framework for Computational Sonography
- **Arxiv ID**: http://arxiv.org/abs/1802.02088v1
- **DOI**: 10.1117/12.2292501
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.02088v1)
- **Published**: 2018-02-06 17:42:07+00:00
- **Updated**: 2018-02-06 17:42:07+00:00
- **Authors**: Jyotirmoy Banerjee, Premal A. Patel, Fred Ushakov, Donald Peebles, Jan Deprest, Sebastien Ourselin, David Hawkes, Tom Vercauteren
- **Comment**: SPIE Medical Imaging 2018
- **Journal**: None
- **Summary**: We propose a spatial compounding technique and variational framework to improve 3D ultrasound image quality by compositing multiple ultrasound volumes acquired from different probe orientations. In the composite volume, instead of intensity values, we estimate a tensor at every voxel. The resultant tensor image encapsulates the directional information of the underlying imaging data and can be used to generate ultrasound volumes from arbitrary, potentially unseen, probe positions. Extending the work of Hennersperger et al., we introduce a log-Euclidean framework to ensure that the tensors are positive-definite, eventually ensuring non-negative images. Additionally, we regularise the underpinning ill-posed variational problem while preserving edge information by relying on a total variation penalisation of the tensor field in the log domain. We present results on in vivo human data to show the efficacy of the approach.



### Structural Recurrent Neural Network (SRNN) for Group Activity Analysis
- **Arxiv ID**: http://arxiv.org/abs/1802.02091v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.02091v1)
- **Published**: 2018-02-06 17:46:32+00:00
- **Updated**: 2018-02-06 17:46:32+00:00
- **Authors**: Sovan Biswas, Juergen Gall
- **Comment**: Accepted in WACV 2018
- **Journal**: None
- **Summary**: A group of persons can be analyzed at various semantic levels such as individual actions, their interactions, and the activity of the entire group. In this paper, we propose a structural recurrent neural network (SRNN) that uses a series of interconnected RNNs to jointly capture the actions of individuals, their interactions, as well as the group activity. While previous structural recurrent neural networks assumed that the number of nodes and edges is constant, we use a grid pooling layer to address the fact that the number of individuals in a group can vary. We evaluate two variants of the structural recurrent neural network on the Volleyball Dataset.



### Efficient Large-Scale Multi-Modal Classification
- **Arxiv ID**: http://arxiv.org/abs/1802.02892v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1802.02892v1)
- **Published**: 2018-02-06 20:30:59+00:00
- **Updated**: 2018-02-06 20:30:59+00:00
- **Authors**: D. Kiela, E. Grave, A. Joulin, T. Mikolov
- **Comment**: Published at AAAI-18, 7 pages
- **Journal**: None
- **Summary**: While the incipient internet was largely text-based, the modern digital world is becoming increasingly multi-modal. Here, we examine multi-modal classification where one modality is discrete, e.g. text, and the other is continuous, e.g. visual representations transferred from a convolutional neural network. In particular, we focus on scenarios where we have to be able to classify large quantities of data quickly. We investigate various methods for performing multi-modal fusion and analyze their trade-offs in terms of classification accuracy and computational efficiency. Our findings indicate that the inclusion of continuous information improves performance over text-only on a range of multi-modal classification tasks, even with simple fusion methods. In addition, we experiment with discretizing the continuous features in order to speed up and simplify the fusion process even further. Our results show that fusion with discretized features outperforms text-only classification, at a fraction of the computational cost of full multi-modal fusion, with the additional benefit of improved interpretability.



