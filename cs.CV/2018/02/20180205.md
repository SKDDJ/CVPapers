# Arxiv Papers in cs.CV on 2018-02-05
### Image Synthesis in Multi-Contrast MRI with Conditional Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1802.01221v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.01221v1)
- **Published**: 2018-02-05 00:10:12+00:00
- **Updated**: 2018-02-05 00:10:12+00:00
- **Authors**: Salman Ul Hassan Dar, Mahmut Yurt, Levent Karacan, Aykut Erdem, Erkut Erdem, Tolga Ã‡ukur
- **Comment**: None
- **Journal**: None
- **Summary**: Acquiring images of the same anatomy with multiple different contrasts increases the diversity of diagnostic information available in an MR exam. Yet, scan time limitations may prohibit acquisition of certain contrasts, and images for some contrast may be corrupted by noise and artifacts. In such cases, the ability to synthesize unacquired or corrupted contrasts from remaining contrasts can improve diagnostic utility. For multi-contrast synthesis, current methods learn a nonlinear intensity transformation between the source and target images, either via nonlinear regression or deterministic neural networks. These methods can in turn suffer from loss of high-spatial-frequency information in synthesized images. Here we propose a new approach for multi-contrast MRI synthesis based on conditional generative adversarial networks. The proposed approach preserves high-frequency details via an adversarial loss; and it offers enhanced synthesis performance via a pixel-wise loss for registered multi-contrast images and a cycle-consistency loss for unregistered images. Information from neighboring cross-sections are utilized to further improved synthesis quality. Demonstrations on T1- and T2-weighted images from healthy subjects and patients clearly indicate the superior performance of the proposed approach compared to previous state-of-the-art methods. Our synthesis approach can help improve quality and versatility of multi-contrast MRI exams without the need for prolonged examinations.



### Tracking Multiple Moving Objects Using Unscented Kalman Filtering Techniques
- **Arxiv ID**: http://arxiv.org/abs/1802.01235v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.01235v1)
- **Published**: 2018-02-05 02:27:56+00:00
- **Updated**: 2018-02-05 02:27:56+00:00
- **Authors**: Xi Chen, Xiao Wang, Jianhua Xuan
- **Comment**: 2012 International Conference on Engineering and Applied Science
  (ICEAS 2012)
- **Journal**: None
- **Summary**: It is an important task to reliably detect and track multiple moving objects for video surveillance and monitoring. However, when occlusion occurs in nonlinear motion scenarios, many existing methods often fail to continuously track multiple moving objects of interest. In this paper we propose an effective approach for detection and tracking of multiple moving objects with occlusion. Moving targets are initially detected using a simple yet efficient block matching technique, providing rough location information for multiple object tracking. More accurate location information is then estimated for each moving object by a nonlinear tracking algorithm. Considering the ambiguity caused by the occlusion among multiple moving objects, we apply an unscented Kalman filtering (UKF) technique for reliable object detection and tracking. Different from conventional Kalman filtering (KF), which cannot achieve the optimal estimation in nonlinear tracking scenarios, UKF can be used to track both linear and nonlinear motions due to the unscented transform. Further, it estimates the velocity information for each object to assist to the object detection algorithm, effectively delineating multiple moving objects of occlusion. The experimental results demonstrate that the proposed method can correctly detect and track multiple moving objects with nonlinear motion patterns and occlusions.



### Face Destylization
- **Arxiv ID**: http://arxiv.org/abs/1802.01237v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.01237v1)
- **Published**: 2018-02-05 02:30:46+00:00
- **Updated**: 2018-02-05 02:30:46+00:00
- **Authors**: Fatemeh Shiri, Xin Yu, Fatih Porikli, Piotr Koniusz
- **Comment**: None
- **Journal**: None
- **Summary**: Numerous style transfer methods which produce artistic styles of portraits have been proposed to date. However, the inverse problem of converting the stylized portraits back into realistic faces is yet to be investigated thoroughly. Reverting an artistic portrait to its original photo-realistic face image has potential to facilitate human perception and identity analysis. In this paper, we propose a novel Face Destylization Neural Network (FDNN) to restore the latent photo-realistic faces from the stylized ones. We develop a Style Removal Network composed of convolutional, fully-connected and deconvolutional layers. The convolutional layers are designed to extract facial components from stylized face images. Consecutively, the fully-connected layer transfers the extracted feature maps of stylized images into the corresponding feature maps of real faces and the deconvolutional layers generate real faces from the transferred feature maps. To enforce the destylized faces to be similar to authentic face images, we employ a discriminative network, which consists of convolutional and fully connected layers. We demonstrate the effectiveness of our network by conducting experiments on an extensive set of synthetic images. Furthermore, we illustrate our network can recover faces from stylized portraits and real paintings for which the stylized data was unavailable during the training phase.



### Enhancing Multi-Class Classification of Random Forest using Random Vector Functional Neural Network and Oblique Decision Surfaces
- **Arxiv ID**: http://arxiv.org/abs/1802.01240v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1802.01240v1)
- **Published**: 2018-02-05 02:42:39+00:00
- **Updated**: 2018-02-05 02:42:39+00:00
- **Authors**: Rakesh Katuwal, P. N. Suganthan
- **Comment**: 8 pages, 5 figures
- **Journal**: None
- **Summary**: Both neural networks and decision trees are popular machine learning methods and are widely used to solve problems from diverse domains. These two classifiers are commonly used base classifiers in an ensemble framework. In this paper, we first present a new variant of oblique decision tree based on a linear classifier, then construct an ensemble classifier based on the fusion of a fast neural network, random vector functional link network and oblique decision trees. Random Vector Functional Link Network has an elegant closed form solution with extremely short training time. The neural network partitions each training bag (obtained using bagging) at the root level into C subsets where C is the number of classes in the dataset and subsequently, C oblique decision trees are trained on such partitions. The proposed method provides a rich insight into the data by grouping the confusing or hard to classify samples for each class and thus, provides an opportunity to employ fine-grained classification rule over the data. The performance of the ensemble classifier is evaluated on several multi-class datasets where it demonstrates a superior performance compared to other state-of- the-art classifiers.



### A Multiresolution Convolutional Neural Network with Partial Label Training for Annotating Reflectance Confocal Microscopy Images of Skin
- **Arxiv ID**: http://arxiv.org/abs/1802.02213v2
- **DOI**: None
- **Categories**: **cs.CV**, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/1802.02213v2)
- **Published**: 2018-02-05 04:20:45+00:00
- **Updated**: 2018-08-23 01:09:52+00:00
- **Authors**: Alican Bozkurt, Kivanc Kose, Christi Alessi-Fox, Melissa Gill, Dana H. Brooks, Jennifer G. Dy, Milind Rajadhyaksha
- **Comment**: This paper is accepted to MICCAI'18 conference. This is an extended
  version of the abstract presented at to "The Optical Society Biophotonics
  Congress: Biomedical Optics 2018" conference (c.f. previous ARXIV version)
- **Journal**: None
- **Summary**: We describe a new multiresolution "nested encoder-decoder" convolutional network architecture and use it to annotate morphological patterns in reflectance confocal microscopy (RCM) images of human skin for aiding cancer diagnosis. Skin cancers are the most common types of cancers, melanoma being the deadliest among them. RCM is an effective, non-invasive pre-screening tool for skin cancer diagnosis, with the required cellular resolution. However, images are complex, low-contrast, and highly variable, so that clinicians require months to years of expert-level training to be able to make accurate assessments. In this paper, we address classifying 4 key clinically important structural/textural patterns in RCM images. The occurrence and morphology of these patterns are used by clinicians for diagnosis of melanomas. The large size of RCM images, the large variance of pattern size, the large-scale range over which patterns appear, the class imbalance in collected images, and the lack of fully-labeled images all make this a challenging problem to address, even with automated machine learning tools. We designed a novel nested U-net architecture to cope with these challenges, and a selective loss function to handle partial labeling. Trained and tested on 56 melanoma-suspicious, partially labeled, 12k x 12k pixel images, our network automatically annotated diagnostic patterns with high sensitivity and specificity, providing consistent labels for unlabeled sections of the test images. Providing such annotation will aid clinicians in achieving diagnostic accuracy, and perhaps more important, dramatically facilitate clinical training, thus enabling much more rapid adoption of RCM into widespread clinical use process. In addition, our adaptation of U-net architecture provides an intrinsically multiresolution deep network that may be useful in other challenging biomedical image analysis applications.



### ClassSim: Similarity between Classes Defined by Misclassification Ratios of Trained Classifiers
- **Arxiv ID**: http://arxiv.org/abs/1802.01267v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1802.01267v1)
- **Published**: 2018-02-05 05:00:35+00:00
- **Updated**: 2018-02-05 05:00:35+00:00
- **Authors**: Kazuma Arino, Yohei Kikuta
- **Comment**: 15 pages, 2 figures, 11 tables
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) have achieved exceptional performances in many tasks, particularly, in supervised classification tasks. However, achievements with supervised classification tasks are based on large datasets with well-separated classes. Typically, real-world applications involve wild datasets that include similar classes; thus, evaluating similarities between classes and understanding relations among classes are important. To address this issue, a similarity metric, ClassSim, based on the misclassification ratios of trained DNNs is proposed herein. We conducted image recognition experiments to demonstrate that the proposed method provides better similarities compared with existing methods and is useful for classification problems. Source code including all experimental results is available at https://github.com/karino2/ClassSim/.



### ASMCNN: An Efficient Brain Extraction Using Active Shape Model and Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1802.01268v3
- **DOI**: 10.1016/j.ins.2022.01.011
- **Categories**: **cs.CV**, 68T10
- **Links**: [PDF](http://arxiv.org/pdf/1802.01268v3)
- **Published**: 2018-02-05 05:01:25+00:00
- **Updated**: 2022-01-28 03:09:48+00:00
- **Authors**: Duy H. M. Nguyen, Duy M. Nguyen, Mai T. N. Truong, Thu Nguyen, Khanh T. Tran, Nguyen A. Triet, Pham T. Bao, Binh T. Nguyen
- **Comment**: 47 pages, 20 figures
- **Journal**: None
- **Summary**: Brain extraction (skull stripping) is a challenging problem in neuroimaging. It is due to the variability in conditions from data acquisition or abnormalities in images, making brain morphology and intensity characteristics changeable and complicated. In this paper, we propose an algorithm for skull stripping in Magnetic Resonance Imaging (MRI) scans, namely ASMCNN, by combining the Active Shape Model (ASM) and Convolutional Neural Network (CNN) for taking full of their advantages to achieve remarkable results. Instead of working with 3D structures, we process 2D image sequences in the sagittal plane. First, we divide images into different groups such that, in each group, shapes and structures of brain boundaries have similar appearances. Second, a modified version of ASM is used to detect brain boundaries by utilizing prior knowledge of each group. Finally, CNN and post-processing methods, including Conditional Random Field (CRF), Gaussian processes, and several special rules are applied to refine the segmentation contours. Experimental results show that our proposed method outperforms current state-of-the-art algorithms by a significant margin in all experiments.



### Face recognition for monitoring operator shift in railways
- **Arxiv ID**: http://arxiv.org/abs/1802.01273v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.01273v2)
- **Published**: 2018-02-05 05:52:51+00:00
- **Updated**: 2018-05-21 04:31:05+00:00
- **Authors**: S Ritika, Dattaraj Rao
- **Comment**: None
- **Journal**: None
- **Summary**: Train Pilot is a very tedious and stressful job. Pilots must be vigilant at all times and its easy for them to lose track of time of shift. In countries like USA the pilots are mandated by law to adhere to 8 hour shifts. If they exceed 8 hours of shift the railroads may be penalized for over-tiring their drivers. The problem happens when the 8 hour shift may end in middle of a journey. In such case, the new drivers must be moved to the location locomotive is operating for shift change. Hence accurate monitoring of drivers during their shift and making sure the shifts are scheduled correctly is very important for railroads. Here we propose an automated camera system that uses camera mounted inside Locomotive cabs to continuously record video feeds. These feeds are analyzed in real time to detect the face of driver and recognize the driver using state of the art deep Learning techniques. The outcome is an increased safety of train pilots. Cameras continuously capture video from inside the cab which is stored on an on board data acquisition device. Using advanced computer vision and deep learning techniques the videos are analyzed at regular intervals to detect presence of the pilot and identify the pilot. Using a time based analysis, it is identified for how long that shift has been active. If this time exceeds allocated shift time an alert is sent to the dispatch to adjust shift hours.



### Dream Formulations and Deep Neural Networks: Humanistic Themes in the Iconology of the Machine-Learned Image
- **Arxiv ID**: http://arxiv.org/abs/1802.01274v1
- **DOI**: None
- **Categories**: **cs.CY**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1802.01274v1)
- **Published**: 2018-02-05 05:57:40+00:00
- **Updated**: 2018-02-05 05:57:40+00:00
- **Authors**: Emily L. Spratt
- **Comment**: 29 pages, 8 Figures, This paper was originally presented as Dream
  Formulations and Image Recognition: Algorithms for the Study of Renaissance
  Art, at Critical Approaches to Digital Art History, The Villa I Tatti, The
  Harvard University Center for Italian Renaissance Studies and The Newberry
  Center for Renaissance Studies, Renaissance Society of America Annual
  Meeting, Chicago, 31 March 2017
- **Journal**: None
- **Summary**: This paper addresses the interpretability of deep learning-enabled image recognition processes in computer vision science in relation to theories in art history and cognitive psychology on the vision-related perceptual capabilities of humans. Examination of what is determinable about the machine-learned image in comparison to humanistic theories of visual perception, particularly in regard to art historian Erwin Panofsky's methodology for image analysis and psychologist Eleanor Rosch's theory of graded categorization according to prototypes, finds that there are surprising similarities between the two that suggest that researchers in the arts and the sciences would have much to benefit from closer collaborations. Utilizing the examples of Google's DeepDream and the Machine Learning and Perception Lab at Georgia Tech's Grad-CAM: Gradient-weighted Class Activation Mapping programs, this study suggests that a revival of art historical research in iconography and formalism in the age of AI is essential for shaping the future navigation and interpretation of all machine-learned images, given the rapid developments in image recognition technologies.



### Zero-Shot Kernel Learning
- **Arxiv ID**: http://arxiv.org/abs/1802.01279v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.01279v2)
- **Published**: 2018-02-05 06:30:44+00:00
- **Updated**: 2018-06-24 03:05:25+00:00
- **Authors**: Hongguang Zhang, Piotr Koniusz
- **Comment**: IEEE Conference on Computer Vision and Pattern Recognition 2018
- **Journal**: None
- **Summary**: In this paper, we address an open problem of zero-shot learning. Its principle is based on learning a mapping that associates feature vectors extracted from i.e. images and attribute vectors that describe objects and/or scenes of interest. In turns, this allows classifying unseen object classes and/or scenes by matching feature vectors via mapping to a newly defined attribute vector describing a new class. Due to importance of such a learning task, there exist many methods that learn semantic, probabilistic, linear or piece-wise linear mappings. In contrast, we apply well-established kernel methods to learn a non-linear mapping between the feature and attribute spaces. We propose an easy learning objective inspired by the Linear Discriminant Analysis, Kernel-Target Alignment and Kernel Polarization methods that promotes incoherence. We evaluate performance of our algorithm on the Polynomial as well as shift-invariant Gaussian and Cauchy kernels. Despite simplicity of our approach, we obtain state-of-the-art results on several zero-shot learning datasets and benchmarks including a recent AWA2 dataset.



### Data Augmentation of Railway Images for Track Inspection
- **Arxiv ID**: http://arxiv.org/abs/1802.01286v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.01286v1)
- **Published**: 2018-02-05 07:07:14+00:00
- **Updated**: 2018-02-05 07:07:14+00:00
- **Authors**: S Ritika, Dattaraj Rao
- **Comment**: None
- **Journal**: None
- **Summary**: Regular maintenance of all the assets is pivotal for proper functioning of railway. Manual maintenance can be very cumbersome and leave room for errors. Track anomalies like vegetation overgrowth, sun kinks affect the track construct and result in unequal load transfer, imbalanced lateral forces on tracks which causes further deterioration of tracks and can ultimately result in derailment of locomotive. Hence there is a need to continuously monitor rail track health. Track anomalies are rare with the skew as high as one anomaly in millions of good images. We propose a method to build training data that will make our algorithms more robust and help us detect real world track issues. The data augmentation will have a direct effect in making us detect better anomalies and hence improve time for railroads that is spent in manual inspection. This paper talks about a real world use case of detecting railway track defects from a camera mounted on a moving locomotive and tracking their locations. The camera is engineered to withstand the environment factors on a moving train and provide a consistent steady image at around 30 frames per second. An image simulation pipeline of track detection, region of interest selection, augmenting image for anomalies is implemented. Training images are simulated for sun kink and vegetation overgrowth. Inception V3 model pretrained on Imagenet dataset is finetuned for a 2 class classification. For the case of vegetation overgrowth, the model generalizes well on actual vegetation images, though it was trained and validated solely on simulated images which might have different distribution than the actual vegetation. Sun kink classifier can classify professionally simulated sun kink videos with a precision of 97.5%.



### First-order Adversarial Vulnerability of Neural Networks and Input Dimension
- **Arxiv ID**: http://arxiv.org/abs/1802.01421v4
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG, 68T45, I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/1802.01421v4)
- **Published**: 2018-02-05 14:36:44+00:00
- **Updated**: 2019-06-16 20:55:06+00:00
- **Authors**: Carl-Johann Simon-Gabriel, Yann Ollivier, LÃ©on Bottou, Bernhard SchÃ¶lkopf, David Lopez-Paz
- **Comment**: Paper previously called: "Adversarial Vulnerability of Neural
  Networks Increases with Input Dimension". 9 pages main text and references,
  11 pages appendix, 14 figures
- **Journal**: Proceedings of ICML 2019
- **Summary**: Over the past few years, neural networks were proven vulnerable to adversarial images: targeted but imperceptible image perturbations lead to drastically different predictions. We show that adversarial vulnerability increases with the gradients of the training objective when viewed as a function of the inputs. Surprisingly, vulnerability does not depend on network topology: for many standard network architectures, we prove that at initialization, the $\ell_1$-norm of these gradients grows as the square root of the input dimension, leaving the networks increasingly vulnerable with growing image size. We empirically show that this dimension dependence persists after either usual or robust training, but gets attenuated with higher regularization.



### A Method for Restoring the Training Set Distribution in an Image Classifier
- **Arxiv ID**: http://arxiv.org/abs/1802.01435v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1802.01435v1)
- **Published**: 2018-02-05 14:49:06+00:00
- **Updated**: 2018-02-05 14:49:06+00:00
- **Authors**: Alexey Chaplygin, Joshua Chacksfield
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Convolutional Neural Networks are a well-known staple of modern image classification. However, it can be difficult to assess the quality and robustness of such models. Deep models are known to perform well on a given training and estimation set, but can easily be fooled by data that is specifically generated for the purpose. It has been shown that one can produce an artificial example that does not represent the desired class, but activates the network in the desired way. This paper describes a new way of reconstructing a sample from the training set distribution of an image classifier without deep knowledge about the underlying distribution. This enables access to the elements of images that most influence the decision of a convolutional network and to extract meaningful information about the training distribution.



### Road Segmentation in SAR Satellite Images with Deep Fully-Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1802.01445v2
- **DOI**: 10.1109/LGRS.2018.2864342
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.01445v2)
- **Published**: 2018-02-05 14:59:39+00:00
- **Updated**: 2018-08-16 09:45:37+00:00
- **Authors**: Corentin Henry, Seyed Majid Azimi, Nina Merkle
- **Comment**: 5 pages, accepted for publication in IEEE Geoscience and Remote
  Sensing Letters
- **Journal**: None
- **Summary**: Remote sensing is extensively used in cartography. As transportation networks grow and change, extracting roads automatically from satellite images is crucial to keep maps up-to-date. Synthetic Aperture Radar satellites can provide high resolution topographical maps. However roads are difficult to identify in these data as they look visually similar to targets such as rivers and railways. Most road extraction methods on Synthetic Aperture Radar images still rely on a prior segmentation performed by classical computer vision algorithms. Few works study the potential of deep learning techniques, despite their successful applications to optical imagery. This letter presents an evaluation of Fully-Convolutional Neural Networks for road segmentation in SAR images. We study the relative performance of early and state-of-the-art networks after carefully enhancing their sensitivity towards thin objects by adding spatial tolerance rules. Our models shows promising results, successfully extracting most of the roads in our test dataset. This shows that, although Fully-Convolutional Neural Networks natively lack efficiency for road segmentation, they are capable of good results if properly tuned. As the segmentation quality does not scale well with the increasing depth of the networks, the design of specialized architectures for roads extraction should yield better performances.



### Image denoising with generalized Gaussian mixture model patch priors
- **Arxiv ID**: http://arxiv.org/abs/1802.01458v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, math.ST, stat.ML, stat.TH
- **Links**: [PDF](http://arxiv.org/pdf/1802.01458v2)
- **Published**: 2018-02-05 15:18:21+00:00
- **Updated**: 2018-06-11 13:00:13+00:00
- **Authors**: Charles-Alban Deledalle, Shibin Parameswaran, Truong Q. Nguyen
- **Comment**: None
- **Journal**: None
- **Summary**: Patch priors have become an important component of image restoration. A powerful approach in this category of restoration algorithms is the popular Expected Patch Log-Likelihood (EPLL) algorithm. EPLL uses a Gaussian mixture model (GMM) prior learned on clean image patches as a way to regularize degraded patches. In this paper, we show that a generalized Gaussian mixture model (GGMM) captures the underlying distribution of patches better than a GMM. Even though GGMM is a powerful prior to combine with EPLL, the non-Gaussianity of its components presents major challenges to be applied to a computationally intensive process of image restoration. Specifically, each patch has to undergo a patch classification step and a shrinkage step. These two steps can be efficiently solved with a GMM prior but are computationally impractical when using a GGMM prior. In this paper, we provide approximations and computational recipes for fast evaluation of these two steps, so that EPLL can embed a GGMM prior on an image with more than tens of thousands of patches. Our main contribution is to analyze the accuracy of our approximations based on thorough theoretical analysis. Our evaluations indicate that the GGMM prior is consistently a better fit formodeling image patch distribution and performs better on average in image denoising task.



### Exploring Spatial Context for 3D Semantic Segmentation of Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1802.01500v2
- **DOI**: 10.1109/ICCVW.2017.90
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.01500v2)
- **Published**: 2018-02-05 16:17:58+00:00
- **Updated**: 2019-12-18 22:28:53+00:00
- **Authors**: Francis Engelmann, Theodora Kontogianni, Alexander Hermans, Bastian Leibe
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning approaches have made tremendous progress in the field of semantic segmentation over the past few years. However, most current approaches operate in the 2D image space. Direct semantic segmentation of unstructured 3D point clouds is still an open research problem. The recently proposed PointNet architecture presents an interesting step ahead in that it can operate on unstructured point clouds, achieving encouraging segmentation results. However, it subdivides the input points into a grid of blocks and processes each such block individually. In this paper, we investigate the question how such an architecture can be extended to incorporate larger-scale spatial context. We build upon PointNet and propose two extensions that enlarge the receptive field over the 3D scene. We evaluate the proposed strategies on challenging indoor and outdoor datasets and show improved results in both scenarios.



### 3D non-rigid registration using color: Color Coherent Point Drift
- **Arxiv ID**: http://arxiv.org/abs/1802.01516v1
- **DOI**: 10.1016/j.cviu.2018.01.008
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.01516v1)
- **Published**: 2018-02-05 17:14:09+00:00
- **Updated**: 2018-02-05 17:14:09+00:00
- **Authors**: Marcelo Saval-Calvo, Jorge Azorin-Lopez, Andres Fuster-Guillo, Victor Villena-Martinez, Robert B. Fisher
- **Comment**: Published in Computer Vision and Image Understanding
- **Journal**: None
- **Summary**: Research into object deformations using computer vision techniques has been under intense study in recent years. A widely used technique is 3D non-rigid registration to estimate the transformation between two instances of a deforming structure. Despite many previous developments on this topic, it remains a challenging problem. In this paper we propose a novel approach to non-rigid registration combining two data spaces in order to robustly calculate the correspondences and transformation between two data sets. In particular, we use point color as well as 3D location as these are the common outputs of RGB-D cameras. We have propose the Color Coherent Point Drift (CCPD) algorithm (an extension of the CPD method [1]). Evaluation is performed using synthetic and real data. The synthetic data includes easy shapes that allow evaluation of the effect of noise, outliers and missing data. Moreover, an evaluation of realistic figures obtained using Blensor is carried out. Real data acquired using a general purpose Primesense Carmine sensor is used to validate the CCPD for real shapes. For all tests, the proposed method is compared to the original CPD showing better results in registration accuracy in most cases.



### Background subtraction using the factored 3-way restricted Boltzmann machines
- **Arxiv ID**: http://arxiv.org/abs/1802.01522v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1802.01522v1)
- **Published**: 2018-02-05 17:29:40+00:00
- **Updated**: 2018-02-05 17:29:40+00:00
- **Authors**: Soonam Lee, Daekeun Kim
- **Comment**: EECS545 (2011 Winter) class project report at the University of
  Michigan. This is for archiving purpose
- **Journal**: None
- **Summary**: In this paper, we proposed a method for reconstructing the 3D model based on continuous sensory input. The robot can draw on extremely large data from the real world using various sensors. However, the sensory inputs are usually too noisy and high-dimensional data. It is very difficult and time consuming for robot to process using such raw data when the robot tries to construct 3D model. Hence, there needs to be a method that can extract useful information from such sensory inputs. To address this problem our method utilizes the concept of Object Semantic Hierarchy (OSH). Different from the previous work that used this hierarchy framework, we extract the motion information using the Deep Belief Network technique instead of applying classical computer vision approaches. We have trained on two large sets of random dot images (10,000) which are translated and rotated, respectively, and have successfully extracted several bases that explain the translation and rotation motion. Based on this translation and rotation bases, background subtraction have become possible using Object Semantic Hierarchy.



### Real-time Prediction of Intermediate-Horizon Automotive Collision Risk
- **Arxiv ID**: http://arxiv.org/abs/1802.01532v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1802.01532v1)
- **Published**: 2018-02-05 17:47:58+00:00
- **Updated**: 2018-02-05 17:47:58+00:00
- **Authors**: Blake Wulfe, Sunil Chintakindi, Sou-Cheng T. Choi, Rory Hartong-Redden, Anuradha Kodali, Mykel J. Kochenderfer
- **Comment**: None
- **Journal**: None
- **Summary**: Advanced collision avoidance and driver hand-off systems can benefit from the ability to accurately predict, in real time, the probability a vehicle will be involved in a collision within an intermediate horizon of 10 to 20 seconds. The rarity of collisions in real-world data poses a significant challenge to developing this capability because, as we demonstrate empirically, intermediate-horizon risk prediction depends heavily on high-dimensional driver behavioral features. As a result, a large amount of data is required to fit an effective predictive model. In this paper, we assess whether simulated data can help alleviate this issue. Focusing on highway driving, we present a three-step approach for generating data and fitting a predictive model capable of real-time prediction. First, high-risk automotive scenes are generated using importance sampling on a learned Bayesian network scene model. Second, collision risk is estimated through Monte Carlo simulation. Third, a neural network domain adaptation model is trained on real and simulated data to address discrepancies between the two domains. Experiments indicate that simulated data can mitigate issues resulting from collision rarity, thereby improving risk prediction in real-world data.



### Regularized Evolution for Image Classifier Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/1802.01548v7
- **DOI**: None
- **Categories**: **cs.NE**, cs.AI, cs.CV, cs.DC, I.2.6; I.5.1; I.5.2
- **Links**: [PDF](http://arxiv.org/pdf/1802.01548v7)
- **Published**: 2018-02-05 18:20:52+00:00
- **Updated**: 2019-02-16 23:28:16+00:00
- **Authors**: Esteban Real, Alok Aggarwal, Yanping Huang, Quoc V Le
- **Comment**: Accepted for publication at AAAI 2019, the Thirty-Third AAAI
  Conference on Artificial Intelligence
- **Journal**: None
- **Summary**: The effort devoted to hand-crafting neural network image classifiers has motivated the use of architecture search to discover them automatically. Although evolutionary algorithms have been repeatedly applied to neural network topologies, the image classifiers thus discovered have remained inferior to human-crafted ones. Here, we evolve an image classifier---AmoebaNet-A---that surpasses hand-designs for the first time. To do this, we modify the tournament selection evolutionary algorithm by introducing an age property to favor the younger genotypes. Matching size, AmoebaNet-A has comparable accuracy to current state-of-the-art ImageNet models discovered with more complex architecture-search methods. Scaled to larger size, AmoebaNet-A sets a new state-of-the-art 83.9% / 96.6% top-5 ImageNet accuracy. In a controlled comparison against a well known reinforcement learning algorithm, we give evidence that evolution can obtain results faster with the same hardware, especially at the earlier stages of the search. This is relevant when fewer compute resources are available. Evolution is, thus, a simple method to effectively discover high-quality architectures.



### One-Shot Imitation from Observing Humans via Domain-Adaptive Meta-Learning
- **Arxiv ID**: http://arxiv.org/abs/1802.01557v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1802.01557v1)
- **Published**: 2018-02-05 18:36:19+00:00
- **Updated**: 2018-02-05 18:36:19+00:00
- **Authors**: Tianhe Yu, Chelsea Finn, Annie Xie, Sudeep Dasari, Tianhao Zhang, Pieter Abbeel, Sergey Levine
- **Comment**: First two authors contributed equally. Video available at
  https://sites.google.com/view/daml
- **Journal**: None
- **Summary**: Humans and animals are capable of learning a new behavior by observing others perform the skill just once. We consider the problem of allowing a robot to do the same -- learning from a raw video pixels of a human, even when there is substantial domain shift in the perspective, environment, and embodiment between the robot and the observed human. Prior approaches to this problem have hand-specified how human and robot actions correspond and often relied on explicit human pose detection systems. In this work, we present an approach for one-shot learning from a video of a human by using human and robot demonstration data from a variety of previous tasks to build up prior knowledge through meta-learning. Then, combining this prior knowledge and only a single video demonstration from a human, the robot can perform the task that the human demonstrated. We show experiments on both a PR2 arm and a Sawyer arm, demonstrating that after meta-learning, the robot can learn to place, push, and pick-and-place new objects using just one video of a human performing the manipulation.



### An Occluded Stacked Hourglass Approach to Facial Landmark Localization and Occlusion Estimation
- **Arxiv ID**: http://arxiv.org/abs/1802.02137v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1802.02137v1)
- **Published**: 2018-02-05 19:17:52+00:00
- **Updated**: 2018-02-05 19:17:52+00:00
- **Authors**: Kevan Yuen, Mohan M. Trivedi
- **Comment**: None
- **Journal**: None
- **Summary**: A key step to driver safety is to observe the driver's activities with the face being a key step in this process to extracting information such as head pose, blink rate, yawns, talking to passenger which can then help derive higher level information such as distraction, drowsiness, intent, and where they are looking. In the context of driving safety, it is important for the system perform robust estimation under harsh lighting and occlusion but also be able to detect when the occlusion occurs so that information predicted from occluded parts of the face can be taken into account properly. This paper introduces the Occluded Stacked Hourglass, based on the work of original Stacked Hourglass network for body pose joint estimation, which is retrained to process a detected face window and output 68 occlusion heat maps, each corresponding to a facial landmark. Landmark location, occlusion levels and a refined face detection score, to reject false positives, are extracted from these heat maps. Using the facial landmark locations, features such as head pose and eye/mouth openness can be extracted to derive driver attention and activity. The system is evaluated for face detection, head pose, and occlusion estimation on various datasets in the wild, both quantitatively and qualitatively, and shows state-of-the-art results.



### Musical Chair: Efficient Real-Time Recognition Using Collaborative IoT Devices
- **Arxiv ID**: http://arxiv.org/abs/1802.02138v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AR
- **Links**: [PDF](http://arxiv.org/pdf/1802.02138v3)
- **Published**: 2018-02-05 19:32:35+00:00
- **Updated**: 2018-03-21 16:21:24+00:00
- **Authors**: Ramyad Hadidi, Jiashen Cao, Matthew Woodward, Michael S. Ryoo, Hyesoon Kim
- **Comment**: None
- **Journal**: None
- **Summary**: The prevalence of Internet of things (IoT) devices and abundance of sensor data has created an increase in real-time data processing such as recognition of speech, image, and video. While currently such processes are offloaded to the computationally powerful cloud system, a localized and distributed approach is desirable because (i) it preserves the privacy of users and (ii) it omits the dependency on cloud services. However, IoT networks are usually composed of resource-constrained devices, and a single device is not powerful enough to process real-time data. To overcome this challenge, we examine data and model parallelism for such devices in the context of deep neural networks. We propose Musical Chair to enable efficient, localized, and dynamic real-time recognition by harvesting the aggregated computational power from the resource-constrained devices in the same IoT network as input sensors. Musical chair adapts to the availability of computing devices at runtime and adjusts to the inherit dynamics of IoT networks. To demonstrate Musical Chair, on a network of Raspberry PIs (up to 12) each connected to a camera, we implement a state-of-the-art action recognition model for videos and two recognition models for images. Compared to the Tegra TX2, an embedded low-power platform with a six-core CPU and a GPU, our distributed action recognition system achieves not only similar energy consumption but also twice the performance of the TX2. Furthermore, in image recognition, Musical Chair achieves similar performance and saves dynamic energy.



### On the Feasibility of Generic Deep Disaggregation for Single-Load Extraction
- **Arxiv ID**: http://arxiv.org/abs/1802.02139v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1802.02139v1)
- **Published**: 2018-02-05 20:59:36+00:00
- **Updated**: 2018-02-05 20:59:36+00:00
- **Authors**: Karim Said Barsim, Bin Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, and with the growing development of big energy datasets, data-driven learning techniques began to represent a potential solution to the energy disaggregation problem outperforming engineered and hand-crafted models. However, most proposed deep disaggregation models are load-dependent in the sense that either expert knowledge or a hyper-parameter optimization stage is required prior to training and deployment (normally for each load category) even upon acquisition and cleansing of aggregate and sub-metered data. In this paper, we present a feasibility study on the development of a generic disaggregation model based on data-driven learning. Specifically, we present a generic deep disaggregation model capable of achieving state-of-art performance in load monitoring for a variety of load categories. The developed model is evaluated on the publicly available UK-DALE dataset with a moderately low sampling frequency and various domestic loads.



### Adviser Networks: Learning What Question to Ask for Human-In-The-Loop Viewpoint Estimation
- **Arxiv ID**: http://arxiv.org/abs/1802.01666v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.01666v3)
- **Published**: 2018-02-05 21:01:10+00:00
- **Updated**: 2018-10-25 14:30:13+00:00
- **Authors**: Mohamed El Banani, Jason J. Corso
- **Comment**: 15 pages, 3 figures. Updated Acknowledgment
- **Journal**: None
- **Summary**: Humans have an unparalleled visual intelligence and can overcome visual ambiguities that machines currently cannot. Recent works have shown that incorporating guidance from humans during inference for monocular viewpoint-estimation can help overcome difficult cases in which the computer-alone would have otherwise failed. These hybrid intelligence approaches are hence gaining traction. However, deciding what question to ask the human at inference time remains an unknown for these problems.   We address this question by formulating it as an Adviser Problem: can we learn a mapping from the input to a specific question to ask the human to maximize the expected positive impact to the overall task? We formulate a solution to the adviser problem for viewpoint estimation using a deep network where the question asks for the location of a keypoint in the input image. We show that by using the Adviser Network's recommendations, the model and the human outperforms the previous hybrid-intelligence state-of-the-art by 3.7%, and the computer-only state-of-the-art by 5.28% absolute.



### Compressive Light Field Reconstructions using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1802.01722v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.01722v1)
- **Published**: 2018-02-05 23:00:47+00:00
- **Updated**: 2018-02-05 23:00:47+00:00
- **Authors**: Mayank Gupta, Arjun Jauhari, Kuldeep Kulkarni, Suren Jayasuriya, Alyosha Molnar, Pavan Turaga
- **Comment**: Published at CCD 2017 workshop held in conjunction with CVPR 2017
- **Journal**: None
- **Summary**: Light field imaging is limited in its computational processing demands of high sampling for both spatial and angular dimensions. Single-shot light field cameras sacrifice spatial resolution to sample angular viewpoints, typically by multiplexing incoming rays onto a 2D sensor array. While this resolution can be recovered using compressive sensing, these iterative solutions are slow in processing a light field. We present a deep learning approach using a new, two branch network architecture, consisting jointly of an autoencoder and a 4D CNN, to recover a high resolution 4D light field from a single coded 2D image. This network decreases reconstruction time significantly while achieving average PSNR values of 26-32 dB on a variety of light fields. In particular, reconstruction time is decreased from 35 minutes to 6.7 minutes as compared to the dictionary method for equivalent visual quality. These reconstructions are performed at small sampling/compression ratios as low as 8%, allowing for cheaper coded light field cameras. We test our network reconstructions on synthetic light fields, simulated coded measurements of real light fields captured from a Lytro Illum camera, and real coded images from a custom CMOS diffractive light field camera. The combination of compressive light field capture with deep learning allows the potential for real-time light field video acquisition systems in the future.



