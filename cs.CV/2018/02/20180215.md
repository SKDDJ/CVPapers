# Arxiv Papers in cs.CV on 2018-02-15
### AtlasNet: A Papier-Mâché Approach to Learning 3D Surface Generation
- **Arxiv ID**: http://arxiv.org/abs/1802.05384v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.05384v3)
- **Published**: 2018-02-15 02:07:30+00:00
- **Updated**: 2018-07-20 16:00:34+00:00
- **Authors**: Thibault Groueix, Matthew Fisher, Vladimir G. Kim, Bryan C. Russell, Mathieu Aubry
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a method for learning to generate the surface of 3D shapes. Our approach represents a 3D shape as a collection of parametric surface elements and, in contrast to methods generating voxel grids or point clouds, naturally infers a surface representation of the shape. Beyond its novelty, our new shape generation framework, AtlasNet, comes with significant advantages, such as improved precision and generalization capabilities, and the possibility to generate a shape of arbitrary resolution without memory issues. We demonstrate these benefits and compare to strong baselines on the ShapeNet benchmark for two applications: (i) auto-encoding shapes, and (ii) single-view reconstruction from a still image. We also provide results showing its potential for other applications, such as morphing, parametrization, super-resolution, matching, and co-segmentation.



### Fooling OCR Systems with Adversarial Text Images
- **Arxiv ID**: http://arxiv.org/abs/1802.05385v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1802.05385v1)
- **Published**: 2018-02-15 02:08:19+00:00
- **Updated**: 2018-02-15 02:08:19+00:00
- **Authors**: Congzheng Song, Vitaly Shmatikov
- **Comment**: None
- **Journal**: None
- **Summary**: We demonstrate that state-of-the-art optical character recognition (OCR) based on deep learning is vulnerable to adversarial images. Minor modifications to images of printed text, which do not change the meaning of the text to a human reader, cause the OCR system to "recognize" a different text where certain words chosen by the adversary are replaced by their semantic opposites. This completely changes the meaning of the output produced by the OCR system and by the NLP applications that use OCR for preprocessing their inputs.



### Teaching Machines to Code: Neural Markup Generation with Visual Attention
- **Arxiv ID**: http://arxiv.org/abs/1802.05415v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1802.05415v2)
- **Published**: 2018-02-15 06:17:51+00:00
- **Updated**: 2018-06-15 21:36:10+00:00
- **Authors**: Sumeet S. Singh
- **Comment**: For datasets, visualizations and ancillary material see:
  https://untrix.github.io/i2l . For source code go to:
  https://github.com/untrix/im2latex
- **Journal**: None
- **Summary**: We present a neural transducer model with visual attention that learns to generate LaTeX markup of a real-world math formula given its image. Applying sequence modeling and transduction techniques that have been very successful across modalities such as natural language, image, handwriting, speech and audio; we construct an image-to-markup model that learns to produce syntactically and semantically correct LaTeX markup code over 150 words long and achieves a BLEU score of 89%; improving upon the previous state-of-art for the Im2Latex problem. We also demonstrate with heat-map visualization how attention helps in interpreting the model and can pinpoint (detect and localize) symbols on the image accurately despite having been trained without any bounding box data.



### Mapping Images to Scene Graphs with Permutation-Invariant Structured Prediction
- **Arxiv ID**: http://arxiv.org/abs/1802.05451v4
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1802.05451v4)
- **Published**: 2018-02-15 09:50:15+00:00
- **Updated**: 2018-11-01 21:48:19+00:00
- **Authors**: Roei Herzig, Moshiko Raboh, Gal Chechik, Jonathan Berant, Amir Globerson
- **Comment**: Paper is accepted for NIPS 2018 conference
- **Journal**: None
- **Summary**: Machine understanding of complex images is a key goal of artificial intelligence. One challenge underlying this task is that visual scenes contain multiple inter-related objects, and that global context plays an important role in interpreting the scene. A natural modeling framework for capturing such effects is structured prediction, which optimizes over complex labels, while modeling within-label interactions. However, it is unclear what principles should guide the design of a structured prediction model that utilizes the power of deep learning components. Here we propose a design principle for such architectures that follows from a natural requirement of permutation invariance. We prove a necessary and sufficient characterization for architectures that follow this invariance, and discuss its implication on model design. Finally, we show that the resulting model achieves new state of the art results on the Visual Genome scene graph labeling benchmark, outperforming all recent approaches.



### Learning from a Handful Volumes: MRI Resolution Enhancement with Volumetric Super-Resolution Forests
- **Arxiv ID**: http://arxiv.org/abs/1802.05518v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.05518v1)
- **Published**: 2018-02-15 13:23:31+00:00
- **Updated**: 2018-02-15 13:23:31+00:00
- **Authors**: Aline Sindel, Katharina Breininger, Johannes Käßer, Andreas Hess, Andreas Maier, Thomas Köhler
- **Comment**: Preprint submitted to ICIP 2018
- **Journal**: None
- **Summary**: Magnetic resonance imaging (MRI) enables 3-D imaging of anatomical structures. However, the acquisition of MR volumes with high spatial resolution leads to long scan times. To this end, we propose volumetric super-resolution forests (VSRF) to enhance MRI resolution retrospectively. Our method learns a locally linear mapping between low-resolution and high-resolution volumetric image patches by employing random forest regression. We customize features suitable for volumetric MRI to train the random forest and propose a median tree ensemble for robust regression. VSRF outperforms state-of-the-art example-based super-resolution in term of image quality and efficiency for model training and inference in different MRI datasets. It is also superior to unsupervised methods with just a handful or even a single volume to assemble training data.



### Deep Learning for Lip Reading using Audio-Visual Information for Urdu Language
- **Arxiv ID**: http://arxiv.org/abs/1802.05521v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1802.05521v1)
- **Published**: 2018-02-15 13:28:19+00:00
- **Updated**: 2018-02-15 13:28:19+00:00
- **Authors**: M Faisal, Sanaullah Manzoor
- **Comment**: None
- **Journal**: None
- **Summary**: Human lip-reading is a challenging task. It requires not only knowledge of underlying language but also visual clues to predict spoken words. Experts need certain level of experience and understanding of visual expressions learning to decode spoken words. Now-a-days, with the help of deep learning it is possible to translate lip sequences into meaningful words. The speech recognition in the noisy environments can be increased with the visual information [1]. To demonstrate this, in this project, we have tried to train two different deep-learning models for lip-reading: first one for video sequences using spatiotemporal convolution neural network, Bi-gated recurrent neural network and Connectionist Temporal Classification Loss, and second for audio that inputs the MFCC features to a layer of LSTM cells and output the sequence. We have also collected a small audio-visual dataset to train and test our model. Our target is to integrate our both models to improve the speech recognition in the noisy environment



### Unsupervised Learning of Depth and Ego-Motion from Monocular Video Using 3D Geometric Constraints
- **Arxiv ID**: http://arxiv.org/abs/1802.05522v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.05522v2)
- **Published**: 2018-02-15 13:33:21+00:00
- **Updated**: 2018-06-09 00:31:55+00:00
- **Authors**: Reza Mahjourian, Martin Wicke, Anelia Angelova
- **Comment**: Upload CVPR camera-ready
- **Journal**: None
- **Summary**: We present a novel approach for unsupervised learning of depth and ego-motion from monocular video. Unsupervised learning removes the need for separate supervisory signals (depth or ego-motion ground truth, or multi-view video). Prior work in unsupervised depth learning uses pixel-wise or gradient-based losses, which only consider pixels in small local neighborhoods. Our main contribution is to explicitly consider the inferred 3D geometry of the scene, enforcing consistency of the estimated 3D point clouds and ego-motion across consecutive frames. This is a challenging task and is solved by a novel (approximate) backpropagation algorithm for aligning 3D structures.   We combine this novel 3D-based loss with 2D losses based on photometric quality of frame reconstructions using estimated depth and ego-motion from adjacent frames. We also incorporate validity masks to avoid penalizing areas in which no useful information exists.   We test our algorithm on the KITTI dataset and on a video dataset captured on an uncalibrated mobile phone camera. Our proposed approach consistently improves depth estimates on both datasets, and outperforms the state-of-the-art for both depth and ego-motion. Because we only require a simple video, learning depth and ego-motion on large and varied datasets becomes possible. We demonstrate this by training on the low quality uncalibrated video dataset and evaluating on KITTI, ranking among top performing prior methods which are trained on KITTI itself.



### Natural data structure extracted from neighborhood-similarity graphs
- **Arxiv ID**: http://arxiv.org/abs/1803.00500v1
- **DOI**: 10.1016/j.chaos.2018.12.033
- **Categories**: **stat.ML**, cond-mat.dis-nn, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1803.00500v1)
- **Published**: 2018-02-15 14:06:46+00:00
- **Updated**: 2018-02-15 14:06:46+00:00
- **Authors**: Tom Lorimer, Karlis Kanders, Ruedi Stoop
- **Comment**: None
- **Journal**: None
- **Summary**: 'Big' high-dimensional data are commonly analyzed in low-dimensions, after performing a dimensionality-reduction step that inherently distorts the data structure. For the same purpose, clustering methods are also often used. These methods also introduce a bias, either by starting from the assumption of a particular geometric form of the clusters, or by using iterative schemes to enhance cluster contours, with uncontrollable consequences. The goal of data analysis should, however, be to encode and detect structural data features at all scales and densities simultaneously, without assuming a parametric form of data point distances, or modifying them. We propose a novel approach that directly encodes data point neighborhood similarities as a sparse graph. Our non-iterative framework permits a transparent interpretation of data, without altering the original data dimension and metric. Several natural and synthetic data applications demonstrate the efficacy of our novel approach.



### Convolutional Analysis Operator Learning: Acceleration and Convergence
- **Arxiv ID**: http://arxiv.org/abs/1802.05584v7
- **DOI**: 10.1109/TIP.2019.2937734
- **Categories**: **eess.IV**, cs.CV, cs.LG, math.OC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1802.05584v7)
- **Published**: 2018-02-15 14:51:38+00:00
- **Updated**: 2019-09-11 10:03:54+00:00
- **Authors**: Il Yong Chun, Jeffrey A. Fessler
- **Comment**: 22 pages, 11 figures, fixed incorrect math theorem numbers in fig. 3
- **Journal**: IEEE Trans. Image Process., 29:2108-2122, 2020
- **Summary**: Convolutional operator learning is gaining attention in many signal processing and computer vision applications. Learning kernels has mostly relied on so-called patch-domain approaches that extract and store many overlapping patches across training signals. Due to memory demands, patch-domain methods have limitations when learning kernels from large datasets -- particularly with multi-layered structures, e.g., convolutional neural networks -- or when applying the learned kernels to high-dimensional signal recovery problems. The so-called convolution approach does not store many overlapping patches, and thus overcomes the memory problems particularly with careful algorithmic designs; it has been studied within the "synthesis" signal model, e.g., convolutional dictionary learning. This paper proposes a new convolutional analysis operator learning (CAOL) framework that learns an analysis sparsifying regularizer with the convolution perspective, and develops a new convergent Block Proximal Extrapolated Gradient method using a Majorizer (BPEG-M) to solve the corresponding block multi-nonconvex problems. To learn diverse filters within the CAOL framework, this paper introduces an orthogonality constraint that enforces a tight-frame filter condition, and a regularizer that promotes diversity between filters. Numerical experiments show that, with sharp majorizers, BPEG-M significantly accelerates the CAOL convergence rate compared to the state-of-the-art block proximal gradient (BPG) method. Numerical experiments for sparse-view computational tomography show that a convolutional sparsifying regularizer learned via CAOL significantly improves reconstruction quality compared to a conventional edge-preserving regularizer. Using more and wider kernels in a learned regularizer better preserves edges in reconstructed images.



### Towards End-to-End Lane Detection: an Instance Segmentation Approach
- **Arxiv ID**: http://arxiv.org/abs/1802.05591v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.05591v1)
- **Published**: 2018-02-15 15:09:19+00:00
- **Updated**: 2018-02-15 15:09:19+00:00
- **Authors**: Davy Neven, Bert De Brabandere, Stamatios Georgoulis, Marc Proesmans, Luc Van Gool
- **Comment**: None
- **Journal**: None
- **Summary**: Modern cars are incorporating an increasing number of driver assist features, among which automatic lane keeping. The latter allows the car to properly position itself within the road lanes, which is also crucial for any subsequent lane departure or trajectory planning decision in fully autonomous cars. Traditional lane detection methods rely on a combination of highly-specialized, hand-crafted features and heuristics, usually followed by post-processing techniques, that are computationally expensive and prone to scalability due to road scene variations. More recent approaches leverage deep learning models, trained for pixel-wise lane segmentation, even when no markings are present in the image due to their big receptive field. Despite their advantages, these methods are limited to detecting a pre-defined, fixed number of lanes, e.g. ego-lanes, and can not cope with lane changes. In this paper, we go beyond the aforementioned limitations and propose to cast the lane detection problem as an instance segmentation problem - in which each lane forms its own instance - that can be trained end-to-end. To parametrize the segmented lane instances before fitting the lane, we further propose to apply a learned perspective transformation, conditioned on the image, in contrast to a fixed "bird's-eye view" transformation. By doing so, we ensure a lane fitting which is robust against road plane changes, unlike existing approaches that rely on a fixed, pre-defined transformation. In summary, we propose a fast lane detection algorithm, running at 50 fps, which can handle a variable number of lanes and cope with lane changes. We verify our method on the tuSimple dataset and achieve competitive results.



### Conditioning of three-dimensional generative adversarial networks for pore and reservoir-scale models
- **Arxiv ID**: http://arxiv.org/abs/1802.05622v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, physics.geo-ph
- **Links**: [PDF](http://arxiv.org/pdf/1802.05622v1)
- **Published**: 2018-02-15 15:34:23+00:00
- **Updated**: 2018-02-15 15:34:23+00:00
- **Authors**: Lukas Mosser, Olivier Dubrule, Martin J. Blunt
- **Comment**: 5 pages, 2 figures
- **Journal**: None
- **Summary**: Geostatistical modeling of petrophysical properties is a key step in modern integrated oil and gas reservoir studies. Recently, generative adversarial networks (GAN) have been shown to be a successful method for generating unconditional simulations of pore- and reservoir-scale models. This contribution leverages the differentiable nature of neural networks to extend GANs to the conditional simulation of three-dimensional pore- and reservoir-scale models. Based on the previous work of Yeh et al. (2016), we use a content loss to constrain to the conditioning data and a perceptual loss obtained from the evaluation of the GAN discriminator network. The technique is tested on the generation of three-dimensional micro-CT images of a Ketton limestone constrained by two-dimensional cross-sections, and on the simulation of the Maules Creek alluvial aquifer constrained by one-dimensional sections. Our results show that GANs represent a powerful method for sampling conditioned pore and reservoir samples for stochastic reservoir evaluation workflows.



### cGANs with Projection Discriminator
- **Arxiv ID**: http://arxiv.org/abs/1802.05637v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1802.05637v2)
- **Published**: 2018-02-15 16:19:21+00:00
- **Updated**: 2018-08-15 00:02:46+00:00
- **Authors**: Takeru Miyato, Masanori Koyama
- **Comment**: Published as a conference paper at ICLR 2018
- **Journal**: None
- **Summary**: We propose a novel, projection based way to incorporate the conditional information into the discriminator of GANs that respects the role of the conditional information in the underlining probabilistic model. This approach is in contrast with most frameworks of conditional GANs used in application today, which use the conditional information by concatenating the (embedded) conditional vector to the feature vectors. With this modification, we were able to significantly improve the quality of the class conditional image generation on ILSVRC2012 (ImageNet) 1000-class image dataset from the current state-of-the-art result, and we achieved this with a single pair of a discriminator and a generator. We were also able to extend the application to super-resolution and succeeded in producing highly discriminative super-resolution images. This new structure also enabled high quality category transformation based on parametric functional transformation of conditional batch normalization layers in the generator.



### 3D Convolutional Encoder-Decoder Network for Low-Dose CT via Transfer Learning from a 2D Trained Network
- **Arxiv ID**: http://arxiv.org/abs/1802.05656v2
- **DOI**: 10.1109/TMI.2018.2832217
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.05656v2)
- **Published**: 2018-02-15 16:51:16+00:00
- **Updated**: 2018-04-29 21:28:25+00:00
- **Authors**: Hongming Shan, Yi Zhang, Qingsong Yang, Uwe Kruger, Mannudeep K. Kalra, Ling Sun, Wenxiang Cong, Ge Wang
- **Comment**: To be published in the IEEE TMI
- **Journal**: IEEE Transactions on Medical Imaging 37(6) (2018) 1522-1534
- **Summary**: Low-dose computed tomography (CT) has attracted a major attention in the medical imaging field, since CT-associated x-ray radiation carries health risks for patients. The reduction of CT radiation dose, however, compromises the signal-to-noise ratio, and may compromise the image quality and the diagnostic performance. Recently, deep-learning-based algorithms have achieved promising results in low-dose CT denoising, especially convolutional neural network (CNN) and generative adversarial network (GAN). This article introduces a Contracting Path-based Convolutional Encoder-decoder (CPCE) network in 2D and 3D configurations within the GAN framework for low-dose CT denoising. A novel feature of our approach is that an initial 3D CPCE denoising model can be directly obtained by extending a trained 2D CNN and then fine-tuned to incorporate 3D spatial information from adjacent slices. Based on the transfer learning from 2D to 3D, the 3D network converges faster and achieves a better denoising performance than that trained from scratch. By comparing the CPCE with recently published methods based on the simulated Mayo dataset and the real MGH dataset, we demonstrate that the 3D CPCE denoising model has a better performance, suppressing image noise and preserving subtle structures.



### Inverting The Generator Of A Generative Adversarial Network (II)
- **Arxiv ID**: http://arxiv.org/abs/1802.05701v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.05701v1)
- **Published**: 2018-02-15 18:50:20+00:00
- **Updated**: 2018-02-15 18:50:20+00:00
- **Authors**: Antonia Creswell, Anil A Bharath
- **Comment**: Under review at IEEE TNNLS
- **Journal**: None
- **Summary**: Generative adversarial networks (GANs) learn a deep generative model that is able to synthesise novel, high-dimensional data samples. New data samples are synthesised by passing latent samples, drawn from a chosen prior distribution, through the generative model. Once trained, the latent space exhibits interesting properties, that may be useful for down stream tasks such as classification or retrieval. Unfortunately, GANs do not offer an "inverse model", a mapping from data space back to latent space, making it difficult to infer a latent representation for a given data sample. In this paper, we introduce a technique, inversion, to project data samples, specifically images, to the latent space using a pre-trained GAN. Using our proposed inversion technique, we are able to identify which attributes of a dataset a trained GAN is able to model and quantify GAN performance, based on a reconstruction loss. We demonstrate how our proposed inversion technique may be used to quantitatively compare performance of various GAN models trained on three image datasets. We provide code for all of our experiments, https://github.com/ToniCreswell/InvertingGAN.



### Multimodal Explanations: Justifying Decisions and Pointing to the Evidence
- **Arxiv ID**: http://arxiv.org/abs/1802.08129v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1802.08129v1)
- **Published**: 2018-02-15 19:12:03+00:00
- **Updated**: 2018-02-15 19:12:03+00:00
- **Authors**: Dong Huk Park, Lisa Anne Hendricks, Zeynep Akata, Anna Rohrbach, Bernt Schiele, Trevor Darrell, Marcus Rohrbach
- **Comment**: arXiv admin note: text overlap with arXiv:1612.04757
- **Journal**: None
- **Summary**: Deep models that are both effective and explainable are desirable in many settings; prior explainable models have been unimodal, offering either image-based visualization of attention weights or text-based generation of post-hoc justifications. We propose a multimodal approach to explanation, and argue that the two modalities provide complementary explanatory strengths. We collect two new datasets to define and evaluate this task, and propose a novel model which can provide joint textual rationale generation and attention visualization. Our datasets define visual and textual justifications of a classification decision for activity recognition tasks (ACT-X) and for visual question answering tasks (VQA-X). We quantitatively show that training with the textual explanations not only yields better textual justification models, but also better localizes the evidence that supports the decision. We also qualitatively show cases where visual explanation is more insightful than textual explanation, and vice versa, supporting our thesis that multimodal explanation models offer significant benefits over unimodal approaches.



### Systematic Weight Pruning of DNNs using Alternating Direction Method of Multipliers
- **Arxiv ID**: http://arxiv.org/abs/1802.05747v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1802.05747v2)
- **Published**: 2018-02-15 20:22:42+00:00
- **Updated**: 2018-04-22 02:53:53+00:00
- **Authors**: Tianyun Zhang, Shaokai Ye, Yipeng Zhang, Yanzhi Wang, Makan Fardad
- **Comment**: None
- **Journal**: None
- **Summary**: We present a systematic weight pruning framework of deep neural networks (DNNs) using the alternating direction method of multipliers (ADMM). We first formulate the weight pruning problem of DNNs as a constrained nonconvex optimization problem, and then adopt the ADMM framework for systematic weight pruning. We show that ADMM is highly suitable for weight pruning due to the computational efficiency it offers. We achieve a much higher compression ratio compared with prior work while maintaining the same test accuracy, together with a faster convergence rate. Our models are released at https://github.com/KaiqiZhang/admm-pruning



### Image Dataset for Visual Objects Classification in 3D Printing
- **Arxiv ID**: http://arxiv.org/abs/1803.00391v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.00391v2)
- **Published**: 2018-02-15 20:23:52+00:00
- **Updated**: 2018-03-22 16:30:04+00:00
- **Authors**: Hongjia Li, Xiaolong Ma, Aditya Singh Rathore, Zhe Li, Qiyuan An, Chen Song, Wenyao Xu, Yanzhi Wang
- **Comment**: It is not accepted and the work needed major reversion and
  improvement
- **Journal**: None
- **Summary**: The rapid development in additive manufacturing (AM), also known as 3D printing, has brought about potential risk and security issues along with significant benefits. In order to enhance the security level of the 3D printing process, the present research aims to detect and recognize illegal components using deep learning. In this work, we collected a dataset of 61,340 2D images (28x28 for each image) of 10 classes including guns and other non-gun objects, corresponding to the projection results of the original 3D models. To validate the dataset, we train a convolutional neural network (CNN) model for gun classification which can achieve 98.16% classification accuracy.



### Image Transformer
- **Arxiv ID**: http://arxiv.org/abs/1802.05751v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.05751v3)
- **Published**: 2018-02-15 20:37:15+00:00
- **Updated**: 2018-06-15 23:27:07+00:00
- **Authors**: Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Łukasz Kaiser, Noam Shazeer, Alexander Ku, Dustin Tran
- **Comment**: Appears in International Conference on Machine Learning, 2018. Code
  available at https://github.com/tensorflow/tensor2tensor
- **Journal**: None
- **Summary**: Image generation has been successfully cast as an autoregressive sequence generation or transformation problem. Recent work has shown that self-attention is an effective way of modeling textual sequences. In this work, we generalize a recently proposed model architecture based on self-attention, the Transformer, to a sequence modeling formulation of image generation with a tractable likelihood. By restricting the self-attention mechanism to attend to local neighborhoods we significantly increase the size of images the model can process in practice, despite maintaining significantly larger receptive fields per layer than typical convolutional neural networks. While conceptually simple, our generative models significantly outperform the current state of the art in image generation on ImageNet, improving the best published negative log-likelihood on ImageNet from 3.83 to 3.77. We also present results on image super-resolution with a large magnification ratio, applying an encoder-decoder configuration of our architecture. In a human evaluation study, we find that images generated by our super-resolution model fool human observers three times more often than the previous state of the art.



### ASP:A Fast Adversarial Attack Example Generation Framework based on Adversarial Saliency Prediction
- **Arxiv ID**: http://arxiv.org/abs/1802.05763v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1802.05763v3)
- **Published**: 2018-02-15 21:07:05+00:00
- **Updated**: 2018-06-12 22:32:05+00:00
- **Authors**: Fuxun Yu, Qide Dong, Xiang Chen
- **Comment**: None
- **Journal**: None
- **Summary**: With the excellent accuracy and feasibility, the Neural Networks have been widely applied into the novel intelligent applications and systems. However, with the appearance of the Adversarial Attack, the NN based system performance becomes extremely vulnerable:the image classification results can be arbitrarily misled by the adversarial examples, which are crafted images with human unperceivable pixel-level perturbation. As this raised a significant system security issue, we implemented a series of investigations on the adversarial attack in this work: We first identify an image's pixel vulnerability to the adversarial attack based on the adversarial saliency analysis. By comparing the analyzed saliency map and the adversarial perturbation distribution, we proposed a new evaluation scheme to comprehensively assess the adversarial attack precision and efficiency. Then, with a novel adversarial saliency prediction method, a fast adversarial example generation framework, namely "ASP", is proposed with significant attack efficiency improvement and dramatic computation cost reduction. Compared to the previous methods, experiments show that ASP has at most 12 times speed-up for adversarial example generation, 2 times lower perturbation rate, and high attack success rate of 87% on both MNIST and Cifar10. ASP can be also well utilized to support the data-hungry NN adversarial training. By reducing the attack success rate as much as 90%, ASP can quickly and effectively enhance the defense capability of NN based system to the adversarial attacks.



### Learning to Count Objects in Natural Images for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1802.05766v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1802.05766v1)
- **Published**: 2018-02-15 21:16:59+00:00
- **Updated**: 2018-02-15 21:16:59+00:00
- **Authors**: Yan Zhang, Jonathon Hare, Adam Prügel-Bennett
- **Comment**: Published in ICLR 2018
- **Journal**: None
- **Summary**: Visual Question Answering (VQA) models have struggled with counting objects in natural images so far. We identify a fundamental problem due to soft attention in these models as a cause. To circumvent this problem, we propose a neural network component that allows robust counting from object proposals. Experiments on a toy task show the effectiveness of this component and we obtain state-of-the-art accuracy on the number category of the VQA v2 dataset without negatively affecting other categories, even outperforming ensemble models with our single model. On a difficult balanced pair metric, the component gives a substantial improvement in counting over a strong baseline by 6.6%.



### Detecting Anomalous Faces with 'No Peeking' Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/1802.05798v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.05798v1)
- **Published**: 2018-02-15 23:35:37+00:00
- **Updated**: 2018-02-15 23:35:37+00:00
- **Authors**: Anand Bhattad, Jason Rock, David Forsyth
- **Comment**: None
- **Journal**: None
- **Summary**: Detecting anomalous faces has important applications. For example, a system might tell when a train driver is incapacitated by a medical event, and assist in adopting a safe recovery strategy. These applications are demanding, because they require accurate detection of rare anomalies that may be seen only at runtime. Such a setting causes supervised methods to perform poorly. We describe a method for detecting an anomalous face image that meets these requirements. We construct a feature vector that reliably has large entries for anomalous images, then use various simple unsupervised methods to score the image based on the feature. Obvious constructions (autoencoder codes; autoencoder residuals) are defeated by a 'peeking' behavior in autoencoders. Our feature construction removes rectangular patches from the image, predicts the likely content of the patch conditioned on the rest of the image using a specially trained autoencoder, then compares the result to the image. High scores suggest that the patch was difficult for an autoencoder to predict, and so is likely anomalous. We demonstrate that our method can identify real anomalous face images in pools of typical images, taken from celeb-A, that is much larger than usual in state-of-the-art experiments. A control experiment based on our method with another set of normal celebrity images - a 'typical set', but nonceleb-A are not identified as anomalous; confirms this is not due to special properties of celeb-A.



### Tree-CNN: A Hierarchical Deep Convolutional Neural Network for Incremental Learning
- **Arxiv ID**: http://arxiv.org/abs/1802.05800v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1802.05800v3)
- **Published**: 2018-02-15 23:36:56+00:00
- **Updated**: 2019-09-08 15:58:05+00:00
- **Authors**: Deboleena Roy, Priyadarshini Panda, Kaushik Roy
- **Comment**: 8 pages, 6 figures, 7 tables Accepted in Neural Networks, 2019
- **Journal**: None
- **Summary**: Over the past decade, Deep Convolutional Neural Networks (DCNNs) have shown remarkable performance in most computer vision tasks. These tasks traditionally use a fixed dataset, and the model, once trained, is deployed as is. Adding new information to such a model presents a challenge due to complex training issues, such as "catastrophic forgetting", and sensitivity to hyper-parameter tuning. However, in this modern world, data is constantly evolving, and our deep learning models are required to adapt to these changes. In this paper, we propose an adaptive hierarchical network structure composed of DCNNs that can grow and learn as new data becomes available. The network grows in a tree-like fashion to accommodate new classes of data, while preserving the ability to distinguish the previously trained classes. The network organizes the incrementally available data into feature-driven super-classes and improves upon existing hierarchical CNN models by adding the capability of self-growth. The proposed hierarchical model, when compared against fine-tuning a deep network, achieves significant reduction of training effort, while maintaining competitive accuracy on CIFAR-10 and CIFAR-100.



