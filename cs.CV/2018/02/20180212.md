# Arxiv Papers in cs.CV on 2018-02-12
### Answerer in Questioner's Mind: Information Theoretic Approach to Goal-Oriented Visual Dialog
- **Arxiv ID**: http://arxiv.org/abs/1802.03881v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1802.03881v3)
- **Published**: 2018-02-12 04:08:06+00:00
- **Updated**: 2018-11-28 05:07:23+00:00
- **Authors**: Sang-Woo Lee, Yu-Jung Heo, Byoung-Tak Zhang
- **Comment**: Selected for a spotlight presentation at NIPS, 2018. Camera ready
  version
- **Journal**: None
- **Summary**: Goal-oriented dialog has been given attention due to its numerous applications in artificial intelligence. Goal-oriented dialogue tasks occur when a questioner asks an action-oriented question and an answerer responds with the intent of letting the questioner know a correct action to take. To ask the adequate question, deep learning and reinforcement learning have been recently applied. However, these approaches struggle to find a competent recurrent neural questioner, owing to the complexity of learning a series of sentences. Motivated by theory of mind, we propose "Answerer in Questioner's Mind" (AQM), a novel information theoretic algorithm for goal-oriented dialog. With AQM, a questioner asks and infers based on an approximated probabilistic model of the answerer. The questioner figures out the answerer's intention via selecting a plausible question by explicitly calculating the information gain of the candidate intentions and possible answers to each question. We test our framework on two goal-oriented visual dialog tasks: "MNIST Counting Dialog" and "GuessWhat?!". In our experiments, AQM outperforms comparative algorithms by a large margin.



### Deep feature compression for collaborative object detection
- **Arxiv ID**: http://arxiv.org/abs/1802.03931v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.03931v1)
- **Published**: 2018-02-12 08:26:07+00:00
- **Updated**: 2018-02-12 08:26:07+00:00
- **Authors**: Hyomin Choi, Ivan V. Bajic
- **Comment**: None
- **Journal**: None
- **Summary**: Recent studies have shown that the efficiency of deep neural networks in mobile applications can be significantly improved by distributing the computational workload between the mobile device and the cloud. This paradigm, termed collaborative intelligence, involves communicating feature data between the mobile and the cloud. The efficiency of such approach can be further improved by lossy compression of feature data, which has not been examined to date. In this work we focus on collaborative object detection and study the impact of both near-lossless and lossy compression of feature data on its accuracy. We also propose a strategy for improving the accuracy under lossy feature compression. Experiments indicate that using this strategy, the communication overhead can be reduced by up to 70% without sacrificing accuracy.



### Object Detection with Mask-based Feature Encoding
- **Arxiv ID**: http://arxiv.org/abs/1802.03934v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.03934v1)
- **Published**: 2018-02-12 08:44:39+00:00
- **Updated**: 2018-02-12 08:44:39+00:00
- **Authors**: Xiaochuan Fan, Hao Guo, Kang Zheng, Wei Feng, Song Wang
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: Region-based Convolutional Neural Networks (R-CNNs) have achieved great success in the field of object detection. The existing R-CNNs usually divide a Region-of-Interest (ROI) into grids, and then localize objects by utilizing the spatial information reflected by the relative position of each grid in the ROI. In this paper, we propose a novel feature-encoding approach, where spatial information is represented through the spatial distributions of visual patterns. In particular, we design a Mask Weight Network (MWN) to learn a set of masks and then apply channel-wise masking operations to ROI feature map, followed by a global pooling and a cheap fully-connected layer. We integrate the newly designed feature encoder into the Faster R-CNN architecture. The resulting new Faster R-CNNs can preserve the object-detection accuracy of the standard Faster R-CNNs by using substantially fewer parameters. Compared to R-FCNs using state-of-art PS ROI pooling and deformable PS ROI pooling, the new Faster R-CNNs can produce higher object-detection accuracy with good run-time efficiency. We also show that a specifically designed and learned MWN can capture global contextual information and further improve the object-detection accuracy. Validation experiments are conducted on both PASCAL VOC and MS COCO datasets.



### Temporal and volumetric denoising via quantile sparse image prior
- **Arxiv ID**: http://arxiv.org/abs/1802.03943v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.03943v3)
- **Published**: 2018-02-12 09:09:55+00:00
- **Updated**: 2019-06-17 07:04:18+00:00
- **Authors**: Franziska Schirrmacher, Thomas Köhler, Tobias Lindenberger, Lennart Husvogt, Jürgen Endres, James G. Fujimoto, Joachim Hornegger, Arnd Dörfler, Philip Hoelter, Andreas K. Maier
- **Comment**: Accepted for MICCAI2017 special issue
- **Journal**: None
- **Summary**: This paper introduces an universal and structure-preserving regularization term, called quantile sparse image (QuaSI) prior. The prior is suitable for denoising images from various medical imaging modalities. We demonstrate its effectiveness on volumetric optical coherence tomography (OCT) and computed tomography (CT) data, which show different noise and image characteristics. OCT offers high-resolution scans of the human retina but is inherently impaired by speckle noise. CT on the other hand has a lower resolution and shows high-frequency noise. For the purpose of denoising, we propose a variational framework based on the QuaSI prior and a Huber data fidelity model that can handle 3-D and 3-D+t data. Efficient optimization is facilitated through the use of an alternating direction method of multipliers (ADMM) scheme and the linearization of the quantile filter. Experiments on multiple datasets emphasize the excellent performance of the proposed method.



### Integration of Absolute Orientation Measurements in the KinectFusion Reconstruction pipeline
- **Arxiv ID**: http://arxiv.org/abs/1802.03980v2
- **DOI**: 10.1109/CVPRW.2018.00198
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.03980v2)
- **Published**: 2018-02-12 11:18:40+00:00
- **Updated**: 2018-04-22 13:13:29+00:00
- **Authors**: Silvio Giancola, Jens Schneider, Peter Wonka, Bernard S. Ghanem
- **Comment**: CVPR Workshop on Visual Odometry and Computer Vision Applications
  Based on Location Clues 2018
- **Journal**: None
- **Summary**: In this paper, we show how absolute orientation measurements provided by low-cost but high-fidelity IMU sensors can be integrated into the KinectFusion pipeline. We show that integration improves both runtime, robustness and quality of the 3D reconstruction. In particular, we use this orientation data to seed and regularize the ICP registration technique. We also present a technique to filter the pairs of 3D matched points based on the distribution of their distances. This filter is implemented efficiently on the GPU. Estimating the distribution of the distances helps control the number of iterations necessary for the convergence of the ICP algorithm. Finally, we show experimental results that highlight improvements in robustness, a speed-up of almost 12%, and a gain in tracking quality of 53% for the ATE metric on the Freiburg benchmark.



### Subspace Support Vector Data Description
- **Arxiv ID**: http://arxiv.org/abs/1802.03989v3
- **DOI**: 10.1109/ICPR.2018.8545819
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.03989v3)
- **Published**: 2018-02-12 11:46:23+00:00
- **Updated**: 2018-04-29 21:02:02+00:00
- **Authors**: Fahad Sohrab, Jenni Raitoharju, Moncef Gabbouj, Alexandros Iosifidis
- **Comment**: 6 pages, submitted/accepted, ICPR 2018
- **Journal**: None
- **Summary**: This paper proposes a novel method for solving one-class classification problems. The proposed approach, namely Subspace Support Vector Data Description, maps the data to a subspace that is optimized for one-class classification. In that feature space, the optimal hypersphere enclosing the target class is then determined. The method iteratively optimizes the data mapping along with data description in order to define a compact class representation in a low-dimensional feature space. We provide both linear and non-linear mappings for the proposed method. Experiments on 14 publicly available datasets indicate that the proposed Subspace Support Vector Data Description provides better performance compared to baselines and other recently proposed one-class classification methods.



### Context-Aware Learning using Transferable Features for Classification of Breast Cancer Histology Images
- **Arxiv ID**: http://arxiv.org/abs/1803.00386v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.00386v2)
- **Published**: 2018-02-12 12:42:27+00:00
- **Updated**: 2018-03-06 07:23:56+00:00
- **Authors**: Ruqayya Awan, Navid Alemi Koohbanani, Muhammad Shaban, Anna Lisowska, Nasir Rajpoot
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have been recently used for a variety of histology image analysis. However, availability of a large dataset is a major prerequisite for training a CNN which limits its use by the computational pathology community. In previous studies, CNNs have demonstrated their potential in terms of feature generalizability and transferability accompanied with better performance. Considering these traits of CNN, we propose a simple yet effective method which leverages the strengths of CNN combined with the advantages of including contextual information, particularly designed for a small dataset. Our method consists of two main steps: first it uses the activation features of CNN trained for a patch-based classification and then it trains a separate classifier using features of overlapping patches to perform image-based classification using the contextual information. The proposed framework outperformed the state-of-the-art method for breast cancer classification.



### Lipschitz-Margin Training: Scalable Certification of Perturbation Invariance for Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1802.04034v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1802.04034v3)
- **Published**: 2018-02-12 13:37:09+00:00
- **Updated**: 2018-10-31 19:02:36+00:00
- **Authors**: Yusuke Tsuzuku, Issei Sato, Masashi Sugiyama
- **Comment**: To appear in NIPS2018
- **Journal**: None
- **Summary**: High sensitivity of neural networks against malicious perturbations on inputs causes security concerns. To take a steady step towards robust classifiers, we aim to create neural network models provably defended from perturbations. Prior certification work requires strong assumptions on network structures and massive computational costs, and thus the range of their applications was limited. From the relationship between the Lipschitz constants and prediction margins, we present a computationally efficient calculation technique to lower-bound the size of adversarial perturbations that can deceive networks, and that is widely applicable to various complicated networks. Moreover, we propose an efficient training procedure that robustifies networks and significantly improves the provably guarded areas around data points. In experimental evaluations, our method showed its ability to provide a non-trivial guarantee and enhance robustness for even large networks.



### Blind Image Deconvolution using Deep Generative Priors
- **Arxiv ID**: http://arxiv.org/abs/1802.04073v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.04073v4)
- **Published**: 2018-02-12 14:39:04+00:00
- **Updated**: 2019-02-26 20:14:35+00:00
- **Authors**: Muhammad Asim, Fahad Shamshad, Ali Ahmed
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a novel approach to regularize the \textit{ill-posed} and \textit{non-linear} blind image deconvolution (blind deblurring) using deep generative networks as priors. We employ two separate generative models --- one trained to produce sharp images while the other trained to generate blur kernels from lower-dimensional parameters. To deblur, we propose an alternating gradient descent scheme operating in the latent lower-dimensional space of each of the pretrained generative models. Our experiments show promising deblurring results on images even under large blurs, and heavy noise. To address the shortcomings of generative models such as mode collapse, we augment our generative priors with classical image priors and report improved performance on complex image datasets. The deblurring performance depends on how well the range of the generator spans the image class. Interestingly, our experiments show that even an untrained structured (convolutional) generative networks acts as an image prior in the image deblurring context allowing us to extend our results to more diverse natural image datasets.



### Deep learning based supervised semantic segmentation of Electron Cryo-Subtomograms
- **Arxiv ID**: http://arxiv.org/abs/1802.04087v1
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1802.04087v1)
- **Published**: 2018-02-12 14:54:49+00:00
- **Updated**: 2018-02-12 14:54:49+00:00
- **Authors**: Chang Liu, Xiangrui Zeng, Ruogu Lin, Xiaodan Liang, Zachary Freyberg, Eric Xing, Min Xu
- **Comment**: 9 pages
- **Journal**: IEEE International Conference on Image Processing (ICIP) 2018
- **Summary**: Cellular Electron Cryo-Tomography (CECT) is a powerful imaging technique for the 3D visualization of cellular structure and organization at submolecular resolution. It enables analyzing the native structures of macromolecular complexes and their spatial organization inside single cells. However, due to the high degree of structural complexity and practical imaging limitations, systematic macromolecular structural recovery inside CECT images remains challenging. Particularly, the recovery of a macromolecule is likely to be biased by its neighbor structures due to the high molecular crowding. To reduce the bias, here we introduce a novel 3D convolutional neural network inspired by Fully Convolutional Network and Encoder-Decoder Architecture for the supervised segmentation of macromolecules of interest in subtomograms. The tests of our models on realistically simulated CECT data demonstrate that our new approach has significantly improved segmentation performance compared to our baseline approach. Also, we demonstrate that the proposed model has generalization ability to segment new structures that do not exist in training data.



### A General Pipeline for 3D Detection of Vehicles
- **Arxiv ID**: http://arxiv.org/abs/1803.00387v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1803.00387v1)
- **Published**: 2018-02-12 15:32:23+00:00
- **Updated**: 2018-02-12 15:32:23+00:00
- **Authors**: Xinxin Du, Marcelo H. Ang Jr., Sertac Karaman, Daniela Rus
- **Comment**: Accepted at ICRA 2018
- **Journal**: None
- **Summary**: Autonomous driving requires 3D perception of vehicles and other objects in the in environment. Much of the current methods support 2D vehicle detection. This paper proposes a flexible pipeline to adopt any 2D detection network and fuse it with a 3D point cloud to generate 3D information with minimum changes of the 2D detection networks. To identify the 3D box, an effective model fitting algorithm is developed based on generalised car models and score maps. A two-stage convolutional neural network (CNN) is proposed to refine the detected 3D box. This pipeline is tested on the KITTI dataset using two different 2D detection networks. The 3D detection results based on these two networks are similar, demonstrating the flexibility of the proposed pipeline. The results rank second among the 3D detection algorithms, indicating its competencies in 3D detection.



### DCFNet: Deep Neural Network with Decomposed Convolutional Filters
- **Arxiv ID**: http://arxiv.org/abs/1802.04145v3
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1802.04145v3)
- **Published**: 2018-02-12 15:58:54+00:00
- **Updated**: 2018-07-27 23:58:20+00:00
- **Authors**: Qiang Qiu, Xiuyuan Cheng, Robert Calderbank, Guillermo Sapiro
- **Comment**: Published at ICML 2018
- **Journal**: None
- **Summary**: Filters in a Convolutional Neural Network (CNN) contain model parameters learned from enormous amounts of data. In this paper, we suggest to decompose convolutional filters in CNN as a truncated expansion with pre-fixed bases, namely the Decomposed Convolutional Filters network (DCFNet), where the expansion coefficients remain learned from data. Such a structure not only reduces the number of trainable parameters and computation, but also imposes filter regularity by bases truncation. Through extensive experiments, we consistently observe that DCFNet maintains accuracy for image classification tasks with a significant reduction of model parameters, particularly with Fourier-Bessel (FB) bases, and even with random bases. Theoretically, we analyze the representation stability of DCFNet with respect to input variations, and prove representation stability under generic assumptions on the expansion coefficients. The analysis is consistent with the empirical observations.



### Image-based Synthesis for Deep 3D Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1802.04216v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.04216v1)
- **Published**: 2018-02-12 17:59:47+00:00
- **Updated**: 2018-02-12 17:59:47+00:00
- **Authors**: Grégory Rogez, Cordelia Schmid
- **Comment**: accepted to appear in IJCV (with minor revisions). Follow-up to NIPS
  2016 arXiv:1607.02046
- **Journal**: None
- **Summary**: This paper addresses the problem of 3D human pose estimation in the wild. A significant challenge is the lack of training data, i.e., 2D images of humans annotated with 3D poses. Such data is necessary to train state-of-the-art CNN architectures. Here, we propose a solution to generate a large set of photorealistic synthetic images of humans with 3D pose annotations. We introduce an image-based synthesis engine that artificially augments a dataset of real images with 2D human pose annotations using 3D motion capture data. Given a candidate 3D pose, our algorithm selects for each joint an image whose 2D pose locally matches the projected 3D pose. The selected images are then combined to generate a new synthetic image by stitching local image patches in a kinematically constrained manner. The resulting images are used to train an end-to-end CNN for full-body 3D pose estimation. We cluster the training data into a large number of pose classes and tackle pose estimation as a $K$-way classification problem. Such an approach is viable only with large training sets such as ours. Our method outperforms most of the published works in terms of 3D pose estimation in controlled environments (Human3.6M) and shows promising results for real-world images (LSP). This demonstrates that CNNs trained on artificial images generalize well to real images. Compared to data generated from more classical rendering engines, our synthetic images do not require any domain adaptation or fine-tuning stage.



### Image Retargetability
- **Arxiv ID**: http://arxiv.org/abs/1802.04392v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.04392v2)
- **Published**: 2018-02-12 23:23:50+00:00
- **Updated**: 2019-08-07 10:49:25+00:00
- **Authors**: Fan Tang, Weiming Dong, Yiping Meng, Chongyang Ma, Fuzhang Wu, Xinrui Li, Tong-Yee Lee
- **Comment**: 11 pages, 13 figures
- **Journal**: None
- **Summary**: Real-world applications could benefit from the ability to automatically retarget an image to different aspect ratios and resolutions, while preserving its visually and semantically important content. However, not all images can be equally well processed that way. In this work, we introduce the notion of image retargetability to describe how well a particular image can be handled by content-aware image retargeting. We propose to learn a deep convolutional neural network to rank photo retargetability in which the relative ranking of photo retargetability is directly modeled in the loss function. Our model incorporates joint learning of meaningful photographic attributes and image content information which can help regularize the complicated retargetability rating problem. To train and analyze this model, we have collected a database which contains retargetability scores and meaningful image attributes assigned by six expert raters. Experiments demonstrate that our unified model can generate retargetability rankings that are highly consistent with human labels. To further validate our model, we show applications of image retargetability in retargeting method selection, retargeting method assessment and photo collage generation.



