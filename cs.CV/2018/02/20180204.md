# Arxiv Papers in cs.CV on 2018-02-04
### Museum Exhibit Identification Challenge for Domain Adaptation and Beyond
- **Arxiv ID**: http://arxiv.org/abs/1802.01093v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.01093v1)
- **Published**: 2018-02-04 09:16:44+00:00
- **Updated**: 2018-02-04 09:16:44+00:00
- **Authors**: Piotr Koniusz, Yusuf Tas, Hongguang Zhang, Mehrtash Harandi, Fatih Porikli, Rui Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we approach an open problem of artwork identification and propose a new dataset dubbed Open Museum Identification Challenge (Open MIC). It contains photos of exhibits captured in 10 distinct exhibition spaces of several museums which showcase paintings, timepieces, sculptures, glassware, relics, science exhibits, natural history pieces, ceramics, pottery, tools and indigenous crafts. The goal of Open MIC is to stimulate research in domain adaptation, egocentric recognition and few-shot learning by providing a testbed complementary to the famous Office dataset which reaches 90% accuracy. To form our dataset, we captured a number of images per art piece with a mobile phone and wearable cameras to form the source and target data splits, respectively. To achieve robust baselines, we build on a recent approach that aligns per-class scatter matrices of the source and target CNN streams [15]. Moreover, we exploit the positive definite nature of such representations by using end-to-end Bregman divergences and the Riemannian metric. We present baselines such as training/evaluation per exhibition and training/evaluation on the combined set covering 866 exhibit identities. As each exhibition poses distinct challenges e.g., quality of lighting, motion blur, occlusions, clutter, viewpoint and scale variations, rotations, glares, transparency, non-planarity, clipping, we break down results w.r.t. these factors.



### End2You -- The Imperial Toolkit for Multimodal Profiling by End-to-End Learning
- **Arxiv ID**: http://arxiv.org/abs/1802.01115v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.01115v1)
- **Published**: 2018-02-04 12:30:00+00:00
- **Updated**: 2018-02-04 12:30:00+00:00
- **Authors**: Panagiotis Tzirakis, Stefanos Zafeiriou, Bjorn W. Schuller
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce End2You -- the Imperial College London toolkit for multimodal profiling by end-to-end deep learning. End2You is an open-source toolkit implemented in Python and is based on Tensorflow. It provides capabilities to train and evaluate models in an end-to-end manner, i.e., using raw input. It supports input from raw audio, visual, physiological or other types of information or combination of those, and the output can be of an arbitrary representation, for either classification or regression tasks. To our knowledge, this is the first toolkit that provides generic end-to-end learning for profiling capabilities in either unimodal or multimodal cases. To test our toolkit, we utilise the RECOLA database as was used in the AVEC 2016 challenge. Experimental results indicate that End2You can provide comparable results to state-of-the-art methods despite no need of expert-alike feature representations, but self-learning these from the data "end to end".



### Object Sorting Using a Global Texture-Shape 3D Feature Descriptor
- **Arxiv ID**: http://arxiv.org/abs/1802.01116v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1802.01116v3)
- **Published**: 2018-02-04 12:35:49+00:00
- **Updated**: 2019-06-19 06:59:07+00:00
- **Authors**: Zhun Fan, Zhongxing Li, Benzhang Qiu, Wenji Li, Jianye Hu, Alex Noel Josephraj, Heping Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Object recognition and grasping plays a key role in robotic systems, especially for the autonomous robots to implement object sorting tasks in a warehouse. In this paper, we present a global texture-shape 3D feature descriptor which can be utilized in a system of object recognition and grasping, and can perform object sorting tasks well. Our proposed descriptor stems from the clustered viewpoint feature histogram (CVFH), which relies on the geometrical information of the whole 3D object surface only, and can not perform well in recognizing the objects with similar geometrical information. Therefore, we extend the CVFH descriptor with texture and color information to generate a new global 3D feature descriptor. The proposed descriptor is evaluated in tasks of recognizing and classifying 3D objects by applying multi-class support vector machines (SVM) in both public 3D image dataset and real scenes. The results of evaluation show that the proposed descriptor achieves a significant better performance for object recognition compared with the original CVFH. Then, the proposed descriptor is applied in our object recognition and grasping system, showing that the proposed descriptor helps the system implement the object recognition, object grasping and object sorting tasks well.



### Searching for Representative Modes on Hypergraphs for Robust Geometric Model Fitting
- **Arxiv ID**: http://arxiv.org/abs/1802.01129v1
- **DOI**: 10.1109/TPAMI.2018.2803173
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.01129v1)
- **Published**: 2018-02-04 14:18:19+00:00
- **Updated**: 2018-02-04 14:18:19+00:00
- **Authors**: Hanzi Wang, Guobao Xiao, Yan Yan, David Suter
- **Comment**: None
- **Journal**: IEEE Transactions on Pattern Analysis and Machine
  Intelligence,2018
- **Summary**: In this paper, we propose a simple and effective {geometric} model fitting method to fit and segment multi-structure data even in the presence of severe outliers. We cast the task of geometric model fitting as a representative mode-seeking problem on hypergraphs. Specifically, a hypergraph is firstly constructed, where the vertices represent model hypotheses and the hyperedges denote data points. The hypergraph involves higher-order similarities (instead of pairwise similarities used on a simple graph), and it can characterize complex relationships between model hypotheses and data points. {In addition, we develop a hypergraph reduction technique to remove "insignificant" vertices while retaining as many "significant" vertices as possible in the hypergraph}. Based on the {simplified hypergraph, we then propose a novel mode-seeking algorithm to search for representative modes within reasonable time. Finally, the} proposed mode-seeking algorithm detects modes according to two key elements, i.e., the weighting scores of vertices and the similarity analysis between vertices. Overall, the proposed fitting method is able to efficiently and effectively estimate the number and the parameters of model instances in the data simultaneously. Experimental results demonstrate that the proposed method achieves significant superiority over {several} state-of-the-art model fitting methods on both synthetic data and real images.



### Human Action Adverb Recognition: ADHA Dataset and A Three-Stream Hybrid Model
- **Arxiv ID**: http://arxiv.org/abs/1802.01144v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.01144v2)
- **Published**: 2018-02-04 15:25:52+00:00
- **Updated**: 2018-02-12 06:49:38+00:00
- **Authors**: Bo Pang, Kaiwen Zha, Cewu Lu
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce the first benchmark for a new problem --- recognizing human action adverbs (HAA): "Adverbs Describing Human Actions" (ADHA). This is the first step for computer vision to change over from pattern recognition to real AI. We demonstrate some key features of ADHA: a semantically complete set of adverbs describing human actions, a set of common, describable human actions, and an exhaustive labeling of simultaneously emerging actions in each video. We commit an in-depth analysis on the implementation of current effective models in action recognition and image captioning on adverb recognition, and the results show that such methods are unsatisfactory. Moreover, we propose a novel three-stream hybrid model to deal the HAA problem, which achieves a better result.



### Personalized Machine Learning for Robot Perception of Affect and Engagement in Autism Therapy
- **Arxiv ID**: http://arxiv.org/abs/1802.01186v2
- **DOI**: 10.1126/scirobotics.aao6760
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1802.01186v2)
- **Published**: 2018-02-04 20:05:26+00:00
- **Updated**: 2018-06-19 01:21:12+00:00
- **Authors**: Ognjen Rudovic, Jaeryoung Lee, Miles Dai, Bjorn Schuller, Rosalind Picard
- **Comment**: The paper has undergone a major revision and its content is outdated
- **Journal**: None
- **Summary**: Robots have great potential to facilitate future therapies for children on the autism spectrum. However, existing robots lack the ability to automatically perceive and respond to human affect, which is necessary for establishing and maintaining engaging interactions. Moreover, their inference challenge is made harder by the fact that many individuals with autism have atypical and unusually diverse styles of expressing their affective-cognitive states. To tackle the heterogeneity in behavioral cues of children with autism, we use the latest advances in deep learning to formulate a personalized machine learning (ML) framework for automatic perception of the childrens affective states and engagement during robot-assisted autism therapy. The key to our approach is a novel shift from the traditional ML paradigm - instead of using 'one-size-fits-all' ML models, our personalized ML framework is optimized for each child by leveraging relevant contextual information (demographics and behavioral assessment scores) and individual characteristics of each child. We designed and evaluated this framework using a dataset of multi-modal audio, video and autonomic physiology data of 35 children with autism (age 3-13) and from 2 cultures (Asia and Europe), participating in a 25-minute child-robot interaction (~500k datapoints). Our experiments confirm the feasibility of the robot perception of affect and engagement, showing clear improvements due to the model personalization. The proposed approach has potential to improve existing therapies for autism by offering more efficient monitoring and summarization of the therapy progress.



### Efficient Video Object Segmentation via Network Modulation
- **Arxiv ID**: http://arxiv.org/abs/1802.01218v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.01218v1)
- **Published**: 2018-02-04 23:53:58+00:00
- **Updated**: 2018-02-04 23:53:58+00:00
- **Authors**: Linjie Yang, Yanran Wang, Xuehan Xiong, Jianchao Yang, Aggelos K. Katsaggelos
- **Comment**: Submitted to CVPR 2018
- **Journal**: None
- **Summary**: Video object segmentation targets at segmenting a specific object throughout a video sequence, given only an annotated first frame. Recent deep learning based approaches find it effective by fine-tuning a general-purpose segmentation model on the annotated frame using hundreds of iterations of gradient descent. Despite the high accuracy these methods achieve, the fine-tuning process is inefficient and fail to meet the requirements of real world applications. We propose a novel approach that uses a single forward pass to adapt the segmentation model to the appearance of a specific object. Specifically, a second meta neural network named modulator is learned to manipulate the intermediate layers of the segmentation network given limited visual and spatial information of the target object. The experiments show that our approach is 70times faster than fine-tuning approaches while achieving similar accuracy.



