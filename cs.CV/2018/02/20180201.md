# Arxiv Papers in cs.CV on 2018-02-01
### Interpreting CNNs via Decision Trees
- **Arxiv ID**: http://arxiv.org/abs/1802.00121v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.00121v2)
- **Published**: 2018-02-01 01:47:15+00:00
- **Updated**: 2019-02-25 10:12:11+00:00
- **Authors**: Quanshi Zhang, Yu Yang, Haotian Ma, Ying Nian Wu
- **Comment**: None
- **Journal**: None
- **Summary**: This paper aims to quantitatively explain rationales of each prediction that is made by a pre-trained convolutional neural network (CNN). We propose to learn a decision tree, which clarifies the specific reason for each prediction made by the CNN at the semantic level. I.e., the decision tree decomposes feature representations in high conv-layers of the CNN into elementary concepts of object parts. In this way, the decision tree tells people which object parts activate which filters for the prediction and how much they contribute to the prediction score. Such semantic and quantitative explanations for CNN predictions have specific values beyond the traditional pixel-level analysis of CNNs. More specifically, our method mines all potential decision modes of the CNN, where each mode represents a common case of how the CNN uses object parts for prediction. The decision tree organizes all potential decision modes in a coarse-to-fine manner to explain CNN predictions at different fine-grained levels. Experiments have demonstrated the effectiveness of the proposed method.



### Semantic White Balance: Semantic Color Constancy Using Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1802.00153v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.00153v5)
- **Published**: 2018-02-01 04:29:56+00:00
- **Updated**: 2019-05-31 04:23:06+00:00
- **Authors**: Mahmoud Afifi
- **Comment**: Deep Learning and Reinforcement Learning Summer School (DLR), CIFAR -
  Vector Institute, (Poster sessions), 2018
- **Journal**: None
- **Summary**: The goal of computational color constancy is to preserve the perceptive colors of objects under different lighting conditions by removing the effect of color casts caused by the scene's illumination. With the rapid development of deep learning based techniques, significant progress has been made in image semantic segmentation. In this work, we exploit the semantic information together with the color and spatial information of the input image in order to remove color casts. We train a convolutional neural network (CNN) model that learns to estimate the illuminant color and gamma correction parameters based on the semantic information of the given image. Experimental results show that feeding the CNN with the semantic information leads to a significant improvement in the results by reducing the error by more than 40%.



### Deep Neural Nets with Interpolating Function as Output Activation
- **Arxiv ID**: http://arxiv.org/abs/1802.00168v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML, 68Txx
- **Links**: [PDF](http://arxiv.org/pdf/1802.00168v3)
- **Published**: 2018-02-01 06:25:39+00:00
- **Updated**: 2018-06-17 00:56:50+00:00
- **Authors**: Bao Wang, Xiyang Luo, Zhen Li, Wei Zhu, Zuoqiang Shi, Stanley J. Osher
- **Comment**: 11 pages, 4 figures
- **Journal**: None
- **Summary**: We replace the output layer of deep neural nets, typically the softmax function, by a novel interpolating function. And we propose end-to-end training and testing algorithms for this new architecture. Compared to classical neural nets with softmax function as output activation, the surrogate with interpolating function as output activation combines advantages of both deep and manifold learning. The new framework demonstrates the following major advantages: First, it is better applicable to the case with insufficient training data. Second, it significantly improves the generalization accuracy on a wide variety of networks. The algorithm is implemented in PyTorch, and code will be made publicly available.



### Perceptual Compressive Sensing
- **Arxiv ID**: http://arxiv.org/abs/1802.00176v2
- **DOI**: 10.1007/978-3-030-03338-5_23
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.00176v2)
- **Published**: 2018-02-01 07:19:18+00:00
- **Updated**: 2018-08-27 03:08:39+00:00
- **Authors**: Jiang Du, Xuemei Xie, Chenye Wang, Guangming Shi
- **Comment**: Accepted by The First Chinese Conference on Pattern Recognition and
  Computer Vision (PRCV 2018). This is a pre-print version (not final version)
- **Journal**: Chinese Conference on Pattern Recognition and Computer Vision
  (PRCV) 2018
- **Summary**: Compressive sensing (CS) works to acquire measurements at sub-Nyquist rate and recover the scene images. Existing CS methods always recover the scene images in pixel level. This causes the smoothness of recovered images and lack of structure information, especially at a low measurement rate. To overcome this drawback, in this paper, we propose perceptual CS to obtain high-level structured recovery. Our task no longer focuses on pixel level. Instead, we work to make a better visual effect. In detail, we employ perceptual loss, defined on feature level, to enhance the structure information of the recovered images. Experiments show that our method achieves better visual results with stronger structure information than existing CS methods at the same measurement rate.



### Full Image Recover for Block-Based Compressive Sensing
- **Arxiv ID**: http://arxiv.org/abs/1802.00179v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.00179v1)
- **Published**: 2018-02-01 07:23:34+00:00
- **Updated**: 2018-02-01 07:23:34+00:00
- **Authors**: Xuemei Xie, Chenye Wang, Jiang Du, Guangming Shi
- **Comment**: ICME 2018 submission # 1536
- **Journal**: None
- **Summary**: Recent years, compressive sensing (CS) has improved greatly for the application of deep learning technology. For convenience, the input image is usually measured and reconstructed block by block. This usually causes block effect in reconstructed images. In this paper, we present a novel CNN-based network to solve this problem. In measurement part, the input image is adaptively measured block by block to acquire a group of measurements. While in reconstruction part, all the measurements from one image are used to reconstruct the full image at the same time. Different from previous method recovering block by block, the structure information destroyed in measurement part is recovered in our framework. Block effect is removed accordingly. We train the proposed framework by mean square error (MSE) loss function. Experiments show that there is no block effect at all in the proposed method. And our results outperform 1.8 dB compared with existing methods.



### Automatic Pavement Crack Detection Based on Structured Prediction with the Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1802.02208v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.02208v1)
- **Published**: 2018-02-01 09:07:55+00:00
- **Updated**: 2018-02-01 09:07:55+00:00
- **Authors**: Zhun Fan, Yuming Wu, Jiewei Lu, Wenji Li
- **Comment**: None
- **Journal**: None
- **Summary**: Automated pavement crack detection is a challenging task that has been researched for decades due to the complicated pavement conditions in real world. In this paper, a supervised method based on deep learning is proposed, which has the capability of dealing with different pavement conditions. Specifically, a convolutional neural network (CNN) is used to learn the structure of the cracks from raw images, without any preprocessing. Small patches are extracted from crack images as inputs to generate a large training database, a CNN is trained and crack detection is modeled as a multi-label classification problem. Typically, crack pixels are much fewer than non-crack pixels. To deal with the problem with severely imbalanced data, a strategy with modifying the ratio of positive to negative samples is proposed. The method is tested on two public databases and compared with five existing methods. Experimental results show that it outperforms the other methods.



### Dual Recurrent Attention Units for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1802.00209v3
- **DOI**: None
- **Categories**: **cs.AI**, cs.CL, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1802.00209v3)
- **Published**: 2018-02-01 09:35:33+00:00
- **Updated**: 2019-03-26 13:41:21+00:00
- **Authors**: Ahmed Osman, Wojciech Samek
- **Comment**: 8 pages, 5 figures
- **Journal**: None
- **Summary**: Visual Question Answering (VQA) requires AI models to comprehend data in two domains, vision and text. Current state-of-the-art models use learned attention mechanisms to extract relevant information from the input domains to answer a certain question. Thus, robust attention mechanisms are essential for powerful VQA models. In this paper, we propose a recurrent attention mechanism and show its benefits compared to the traditional convolutional approach. We perform two ablation studies to evaluate recurrent attention. First, we introduce a baseline VQA model with visual attention and test the performance difference between convolutional and recurrent attention on the VQA 2.0 dataset. Secondly, we design an architecture for VQA which utilizes dual (textual and visual) Recurrent Attention Units (RAUs). Using this model, we show the effect of all possible combinations of recurrent and convolutional dual attention. Our single model outperforms the first place winner on the VQA 2016 challenge and to the best of our knowledge, it is the second best performing single model on the VQA 1.0 dataset. Furthermore, our model noticeably improves upon the winner of the VQA 2017 challenge. Moreover, we experiment replacing attention mechanisms in state-of-the-art models with our RAUs and show increased performance.



### Face Aging with Contextual Generative Adversarial Nets
- **Arxiv ID**: http://arxiv.org/abs/1802.00237v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.00237v1)
- **Published**: 2018-02-01 10:52:21+00:00
- **Updated**: 2018-02-01 10:52:21+00:00
- **Authors**: Si Liu, Yao Sun, Defa Zhu, Renda Bao, Wei Wang, Xiangbo Shu, Shuicheng Yan
- **Comment**: accepted at ACM Multimedia 2017
- **Journal**: None
- **Summary**: Face aging, which renders aging faces for an input face, has attracted extensive attention in the multimedia research. Recently, several conditional Generative Adversarial Nets (GANs) based methods have achieved great success. They can generate images fitting the real face distributions conditioned on each individual age group. However, these methods fail to capture the transition patterns, e.g., the gradual shape and texture changes between adjacent age groups. In this paper, we propose a novel Contextual Generative Adversarial Nets (C-GANs) to specifically take it into consideration. The C-GANs consists of a conditional transformation network and two discriminative networks. The conditional transformation network imitates the aging procedure with several specially designed residual blocks. The age discriminative network guides the synthesized face to fit the real conditional distribution. The transition pattern discriminative network is novel, aiming to distinguish the real transition patterns with the fake ones. It serves as an extra regularization term for the conditional transformation network, ensuring the generated image pairs to fit the corresponding real transition pattern distribution. Experimental results demonstrate the proposed framework produces appealing results by comparing with the state-of-the-art and ground truth. We also observe performance gain for cross-age face verification.



### VR-Goggles for Robots: Real-to-sim Domain Adaptation for Visual Control
- **Arxiv ID**: http://arxiv.org/abs/1802.00265v4
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1802.00265v4)
- **Published**: 2018-02-01 12:42:02+00:00
- **Updated**: 2019-01-16 09:04:06+00:00
- **Authors**: Jingwei Zhang, Lei Tai, Peng Yun, Yufeng Xiong, Ming Liu, Joschka Boedecker, Wolfram Burgard
- **Comment**: IEEE RA-L 2019 to appear. The first two authors contributed equally.
  Video and supplement file are available on the project
  page(https://goo.gl/KcvmRm)
- **Journal**: None
- **Summary**: In this paper, we deal with the reality gap from a novel perspective, targeting transferring Deep Reinforcement Learning (DRL) policies learned in simulated environments to the real-world domain for visual control tasks. Instead of adopting the common solutions to the problem by increasing the visual fidelity of synthetic images output from simulators during the training phase, we seek to tackle the problem by translating the real-world image streams back to the synthetic domain during the deployment phase, to make the robot feel at home. We propose this as a lightweight, flexible, and efficient solution for visual control, as 1) no extra transfer steps are required during the expensive training of DRL agents in simulation; 2) the trained DRL agents will not be constrained to being deployable in only one specific real-world environment; 3) the policy training and the transfer operations are decoupled, and can be conducted in parallel. Besides this, we propose a simple yet effective shift loss that is agnostic to the downstream task, to constrain the consistency between subsequent frames which is important for consistent policy outputs. We validate the shift loss for artistic style transfer for videos and domain adaptation, and validate our visual control approach in indoor and outdoor robotics experiments.



### HoloFace: Augmenting Human-to-Human Interactions on HoloLens
- **Arxiv ID**: http://arxiv.org/abs/1802.00278v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.00278v1)
- **Published**: 2018-02-01 13:33:04+00:00
- **Updated**: 2018-02-01 13:33:04+00:00
- **Authors**: Marek Kowalski, Zbigniew Nasarzewski, Grzegorz Galinski, Piotr Garbat
- **Comment**: 9 pages, 7 figures, 2018 IEEE Winter Conference on Applications of
  Computer Vision (WACV 2018), YouTube video:
  https://www.youtube.com/watch?v=Zexjx9VWkSU
- **Journal**: None
- **Summary**: We present HoloFace, an open-source framework for face alignment, head pose estimation and facial attribute retrieval for Microsoft HoloLens. HoloFace implements two state-of-the-art face alignment methods which can be used interchangeably: one running locally and one running on a remote backend. Head pose estimation is accomplished by fitting a deformable 3D model to the landmarks localized using face alignment. The head pose provides both the rotation of the head and a position in the world space. The parameters of the fitted 3D face model provide estimates of facial attributes such as mouth opening or smile. Together the above information can be used to augment the faces of people seen by the HoloLens user, and thus their interaction. Potential usage scenarios include facial recognition, emotion recognition, eye gaze tracking and many others. We demonstrate the capabilities of our framework by augmenting the faces of people seen through the HoloLens with various objects and animations.



### Virtual-to-Real: Learning to Control in Visual Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1802.00285v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, cs.SY
- **Links**: [PDF](http://arxiv.org/pdf/1802.00285v4)
- **Published**: 2018-02-01 13:52:38+00:00
- **Updated**: 2018-10-28 04:56:58+00:00
- **Authors**: Zhang-Wei Hong, Chen Yu-Ming, Shih-Yang Su, Tzu-Yun Shann, Yi-Hsiang Chang, Hsuan-Kung Yang, Brian Hsi-Lin Ho, Chih-Chieh Tu, Yueh-Chuan Chang, Tsu-Ching Hsiao, Hsin-Wei Hsiao, Sih-Pin Lai, Chun-Yi Lee
- **Comment**: 7 pages, accepted by IJCAI-18
- **Journal**: None
- **Summary**: Collecting training data from the physical world is usually time-consuming and even dangerous for fragile robots, and thus, recent advances in robot learning advocate the use of simulators as the training platform. Unfortunately, the reality gap between synthetic and real visual data prohibits direct migration of the models trained in virtual worlds to the real world. This paper proposes a modular architecture for tackling the virtual-to-real problem. The proposed architecture separates the learning model into a perception module and a control policy module, and uses semantic image segmentation as the meta representation for relating these two modules. The perception module translates the perceived RGB image to semantic image segmentation. The control policy module is implemented as a deep reinforcement learning agent, which performs actions based on the translated image segmentation. Our architecture is evaluated in an obstacle avoidance task and a target following task. Experimental results show that our architecture significantly outperforms all of the baseline methods in both virtual and real environments, and demonstrates a faster learning curve than them. We also present a detailed analysis for a variety of variant configurations, and validate the transferability of our modular architecture.



### Classification and Disease Localization in Histopathology Using Only Global Labels: A Weakly-Supervised Approach
- **Arxiv ID**: http://arxiv.org/abs/1802.02212v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1802.02212v2)
- **Published**: 2018-02-01 15:21:14+00:00
- **Updated**: 2020-02-20 16:25:47+00:00
- **Authors**: Pierre Courtiol, Eric W. Tramel, Marc Sanselme, Gilles Wainrib
- **Comment**: None
- **Journal**: None
- **Summary**: Analysis of histopathology slides is a critical step for many diagnoses, and in particular in oncology where it defines the gold standard. In the case of digital histopathological analysis, highly trained pathologists must review vast whole-slide-images of extreme digital resolution ($100,000^2$ pixels) across multiple zoom levels in order to locate abnormal regions of cells, or in some cases single cells, out of millions. The application of deep learning to this problem is hampered not only by small sample sizes, as typical datasets contain only a few hundred samples, but also by the generation of ground-truth localized annotations for training interpretable classification and segmentation models. We propose a method for disease localization in the context of weakly supervised learning, where only image-level labels are available during training. Even without pixel-level annotations, we are able to demonstrate performance comparable with models trained with strong annotations on the Camelyon-16 lymph node metastases detection challenge. We accomplish this through the use of pre-trained deep convolutional networks, feature embedding, as well as learning via top instances and negative evidence, a multiple instance learning technique from the field of semantic segmentation and object detection.



### Annotation-Free and One-Shot Learning for Instance Segmentation of Homogeneous Object Clusters
- **Arxiv ID**: http://arxiv.org/abs/1802.00383v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.00383v2)
- **Published**: 2018-02-01 16:46:49+00:00
- **Updated**: 2018-02-07 07:17:20+00:00
- **Authors**: Zheng Wu, Ruiheng Chang, Jiaxu Ma, Cewu Lu, Chi-Keung Tang
- **Comment**: 7 pages, 8 figures, submission to IJCAI 2018
- **Journal**: None
- **Summary**: We propose a novel approach for instance segmen- tation given an image of homogeneous object clus- ter (HOC). Our learning approach is one-shot be- cause a single video of an object instance is cap- tured and it requires no human annotation. Our in- tuition is that images of homogeneous objects can be effectively synthesized based on structure and illumination priors derived from real images. A novel solver is proposed that iteratively maximizes our structured likelihood to generate realistic im- ages of HOC. Illumination transformation scheme is applied to make the real and synthetic images share the same illumination condition. Extensive experiments and comparisons are performed to ver- ify our method. We build a dataset consisting of pixel-level annotated images of HOC. The dataset and code will be published with the paper.



### Dense 3D Object Reconstruction from a Single Depth View
- **Arxiv ID**: http://arxiv.org/abs/1802.00411v2
- **DOI**: 10.1109/TPAMI.2018.2868195
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1802.00411v2)
- **Published**: 2018-02-01 17:39:15+00:00
- **Updated**: 2018-08-23 23:54:05+00:00
- **Authors**: Bo Yang, Stefano Rosa, Andrew Markham, Niki Trigoni, Hongkai Wen
- **Comment**: TPAMI 2018. Code and data are available at:
  https://github.com/Yang7879/3D-RecGAN-extended. This article extends from
  arXiv:1708.07969
- **Journal**: None
- **Summary**: In this paper, we propose a novel approach, 3D-RecGAN++, which reconstructs the complete 3D structure of a given object from a single arbitrary depth view using generative adversarial networks. Unlike existing work which typically requires multiple views of the same object or class labels to recover the full 3D geometry, the proposed 3D-RecGAN++ only takes the voxel grid representation of a depth view of the object as input, and is able to generate the complete 3D occupancy grid with a high resolution of 256^3 by recovering the occluded/missing regions. The key idea is to combine the generative capabilities of autoencoders and the conditional Generative Adversarial Networks (GAN) framework, to infer accurate and fine-grained 3D structures of objects in high-dimensional voxel space. Extensive experiments on large synthetic datasets and real-world Kinect datasets show that the proposed 3D-RecGAN++ significantly outperforms the state of the art in single view 3D object reconstruction, and is able to reconstruct unseen types of objects.



### Deep-Temporal LSTM for Daily Living Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1802.00421v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.00421v2)
- **Published**: 2018-02-01 18:25:59+00:00
- **Updated**: 2018-06-15 13:42:27+00:00
- **Authors**: Srijan Das, Michal Koperski, Francois Bremond, Gianpiero Francesca
- **Comment**: Submitted in conference
- **Journal**: None
- **Summary**: In this paper, we propose to improve the traditional use of RNNs by employing a many to many model for video classification. We analyze the importance of modeling spatial layout and temporal encoding for daily living action recognition. Many RGB methods focus only on short term temporal information obtained from optical flow. Skeleton based methods on the other hand show that modeling long term skeleton evolution improves action recognition accuracy. In this work, we propose a deep-temporal LSTM architecture which extends standard LSTM and allows better encoding of temporal information. In addition, we propose to fuse 3D skeleton geometry with deep static appearance. We validate our approach on public available CAD60, MSRDailyActivity3D and NTU-RGB+D, achieving competitive performance as compared to the state-of-the art.



### DensePose: Dense Human Pose Estimation In The Wild
- **Arxiv ID**: http://arxiv.org/abs/1802.00434v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.00434v1)
- **Published**: 2018-02-01 18:53:26+00:00
- **Updated**: 2018-02-01 18:53:26+00:00
- **Authors**: Rıza Alp Güler, Natalia Neverova, Iasonas Kokkinos
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we establish dense correspondences between RGB image and a surface-based representation of the human body, a task we refer to as dense human pose estimation. We first gather dense correspondences for 50K persons appearing in the COCO dataset by introducing an efficient annotation pipeline. We then use our dataset to train CNN-based systems that deliver dense correspondence 'in the wild', namely in the presence of background, occlusions and scale variations. We improve our training set's effectiveness by training an 'inpainting' network that can fill in missing groundtruth values and report clear improvements with respect to the best results that would be achievable in the past. We experiment with fully-convolutional networks and region-based models and observe a superiority of the latter; we further improve accuracy through cascading, obtaining a system that delivers highly0accurate results in real time. Supplementary materials and videos are provided on the project page http://densepose.org



### APPLE Picker: Automatic Particle Picking, a Low-Effort Cryo-EM Framework
- **Arxiv ID**: http://arxiv.org/abs/1802.00469v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.00469v2)
- **Published**: 2018-02-01 19:37:51+00:00
- **Updated**: 2018-06-14 13:51:12+00:00
- **Authors**: Ayelet Heimowitz, Joakim Andén, Amit Singer
- **Comment**: 18 pages, 14 figures
- **Journal**: None
- **Summary**: Particle picking is a crucial first step in the computational pipeline of single-particle cryo-electron microscopy (cryo-EM). Selecting particles from the micrographs is difficult especially for small particles with low contrast. As high-resolution reconstruction typically requires hundreds of thousands of particles, manually picking that many particles is often too time-consuming. While semi-automated particle picking is currently a popular approach, it may suffer from introducing manual bias into the selection process. In addition, semi-automated particle picking is still somewhat time-consuming. This paper presents the APPLE (Automatic Particle Picking with Low user Effort) picker, a simple and novel approach for fast, accurate, and fully automatic particle picking. While our approach was inspired by template matching, it is completely template-free. This approach is evaluated on publicly available datasets containing micrographs of $\beta$-galactosidase and keyhole limpet hemocyanin projections.



### Learning random-walk label propagation for weakly-supervised semantic segmentation
- **Arxiv ID**: http://arxiv.org/abs/1802.00470v1
- **DOI**: 10.1109/CVPR.2017.315
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.00470v1)
- **Published**: 2018-02-01 19:44:45+00:00
- **Updated**: 2018-02-01 19:44:45+00:00
- **Authors**: Paul Vernaza, Manmohan Chandraker
- **Comment**: This is a revised version of a paper presented at CVPR 2017 that
  corrects some equations. See footnotes
- **Journal**: CVPR 2017
- **Summary**: Large-scale training for semantic segmentation is challenging due to the expense of obtaining training data for this task relative to other vision tasks. We propose a novel training approach to address this difficulty. Given cheaply-obtained sparse image labelings, we propagate the sparse labels to produce guessed dense labelings. A standard CNN-based segmentation network is trained to mimic these labelings. The label-propagation process is defined via random-walk hitting probabilities, which leads to a differentiable parameterization with uncertainty estimates that are incorporated into our loss. We show that by learning the label-propagator jointly with the segmentation predictor, we are able to effectively learn semantic edges given no direct edge supervision. Experiments also show that training a segmentation network in this way outperforms the naive approach.



### A New Registration Approach for Dynamic Analysis of Calcium Signals in Organs
- **Arxiv ID**: http://arxiv.org/abs/1802.00491v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.00491v1)
- **Published**: 2018-02-01 21:24:47+00:00
- **Updated**: 2018-02-01 21:24:47+00:00
- **Authors**: Peixian Liang, Jianxu Chen, Pavel A. Brodskiy, Qinfeng Wu, Yejia Zhang, Yizhe Zhang, Lin Yang, Jeremiah J. Zartman, Danny Z. Chen
- **Comment**: Accepted at ISBI 2018
- **Journal**: None
- **Summary**: Wing disc pouches of fruit flies are a powerful genetic model for studying physiological intercellular calcium ($Ca^{2+}$) signals for dynamic analysis of cell signaling in organ development and disease studies. A key to analyzing spatial-temporal patterns of $Ca^{2+}$ signal waves is to accurately align the pouches across image sequences. However, pouches in different image frames may exhibit extensive intensity oscillations due to $Ca^{2+}$ signaling dynamics, and commonly used multimodal non-rigid registration methods may fail to achieve satisfactory results. In this paper, we develop a new two-phase non-rigid registration approach to register pouches in image sequences. First, we conduct segmentation of the region of interest. (i.e., pouches) using a deep neural network model. Second, we obtain an optimal transformation and align pouches across the image sequences. Evaluated using both synthetic data and real pouch data, our method considerably outperforms the state-of-the-art non-rigid registration methods.



### Learning Semantic Segmentation with Diverse Supervision
- **Arxiv ID**: http://arxiv.org/abs/1802.00509v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.00509v1)
- **Published**: 2018-02-01 22:26:24+00:00
- **Updated**: 2018-02-01 22:26:24+00:00
- **Authors**: Linwei Ye, Zhi Liu, Yang Wang
- **Comment**: 2018 IEEE Winter Conference on Applications of Computer Vision (WACV)
- **Journal**: None
- **Summary**: Models based on deep convolutional neural networks (CNN) have significantly improved the performance of semantic segmentation. However, learning these models requires a large amount of training images with pixel-level labels, which are very costly and time-consuming to collect. In this paper, we propose a method for learning CNN-based semantic segmentation models from images with several types of annotations that are available for various computer vision tasks, including image-level labels for classification, box-level labels for object detection and pixel-level labels for semantic segmentation. The proposed method is flexible and can be used together with any existing CNN-based semantic segmentation networks. Experimental evaluation on the challenging PASCAL VOC 2012 and SIFT-flow benchmarks demonstrate that the proposed method can effectively make use of diverse training data to improve the performance of the learned models.



