# Arxiv Papers in cs.CV on 2018-02-20
### Teaching Categories to Human Learners with Visual Explanations
- **Arxiv ID**: http://arxiv.org/abs/1802.06924v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1802.06924v1)
- **Published**: 2018-02-20 00:49:38+00:00
- **Updated**: 2018-02-20 00:49:38+00:00
- **Authors**: Oisin Mac Aodha, Shihan Su, Yuxin Chen, Pietro Perona, Yisong Yue
- **Comment**: None
- **Journal**: None
- **Summary**: We study the problem of computer-assisted teaching with explanations. Conventional approaches for machine teaching typically only provide feedback at the instance level e.g., the category or label of the instance. However, it is intuitive that clear explanations from a knowledgeable teacher can significantly improve a student's ability to learn a new concept. To address these existing limitations, we propose a teaching framework that provides interpretable explanations as feedback and models how the learner incorporates this additional information. In the case of images, we show that we can automatically generate explanations that highlight the parts of the image that are responsible for the class label. Experiments on human learners illustrate that, on average, participants achieve better test set performance on challenging categorization tasks when taught with our interpretable approach compared to existing methods.



### Scale Optimization for Full-Image-CNN Vehicle Detection
- **Arxiv ID**: http://arxiv.org/abs/1802.06926v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1802.06926v1)
- **Published**: 2018-02-20 00:54:15+00:00
- **Updated**: 2018-02-20 00:54:15+00:00
- **Authors**: Yang Gao, Shouyan Guo, Kaimin Huang, Jiaxin Chen, Qian Gong, Yang Zou, Tong Bai, Gary Overett
- **Comment**: Accepted by 2017 IEEE Intelligent Vehicles Symposium (IV). Link:
  http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7995812
- **Journal**: None
- **Summary**: Many state-of-the-art general object detection methods make use of shared full-image convolutional features (as in Faster R-CNN). This achieves a reasonable test-phase computation time while enjoys the discriminative power provided by large Convolutional Neural Network (CNN) models. Such designs excel on benchmarks which contain natural images but which have very unnatural distributions, i.e. they have an unnaturally high-frequency of the target classes and a bias towards a "friendly" or "dominant" object scale. In this paper we present further study of the use and adaptation of the Faster R-CNN object detection method for datasets presenting natural scale distribution and unbiased real-world object frequency. In particular, we show that better alignment of the detector scale sensitivity to the extant distribution improves vehicle detection performance. We do this by modifying both the selection of Region Proposals, and through using more scale-appropriate full-image convolution features within the CNN model. By selecting better scales in the region proposal input and by combining feature maps through careful design of the convolutional neural network, we improve performance on smaller objects. We significantly increase detection AP for the KITTI dataset car class from 76.3% on our baseline Faster R-CNN detector to 83.6% in our improved detector.



### On Lyapunov exponents and adversarial perturbation
- **Arxiv ID**: http://arxiv.org/abs/1802.06927v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1802.06927v1)
- **Published**: 2018-02-20 01:09:22+00:00
- **Updated**: 2018-02-20 01:09:22+00:00
- **Authors**: Vinay Uday Prabhu, Nishant Desai, John Whaley
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we would like to disseminate a serendipitous discovery involving Lyapunov exponents of a 1-D time series and their use in serving as a filtering defense tool against a specific kind of deep adversarial perturbation. To this end, we use the state-of-the-art CleverHans library to generate adversarial perturbations against a standard Convolutional Neural Network (CNN) architecture trained on the MNIST as well as the Fashion-MNIST datasets. We empirically demonstrate how the Lyapunov exponents computed on the flattened 1-D vector representations of the images served as highly discriminative features that could be to pre-classify images as adversarial or legitimate before feeding the image into the CNN for classification. We also explore the issue of possible false-alarms when the input images are noisy in a non-adversarial sense.



### Non-Local Graph-Based Prediction For Reversible Data Hiding In Images
- **Arxiv ID**: http://arxiv.org/abs/1802.06935v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1802.06935v1)
- **Published**: 2018-02-20 02:28:51+00:00
- **Updated**: 2018-02-20 02:28:51+00:00
- **Authors**: Qi Chang, Gene Cheung, Yao Zhao, Xiaolong Li, Rongrong Ni
- **Comment**: None
- **Journal**: None
- **Summary**: Reversible data hiding (RDH) is desirable in applications where both the hidden message and the cover medium need to be recovered without loss. Among many RDH approaches is prediction-error expansion (PEE), containing two steps: i) prediction of a target pixel value, and ii) embedding according to the value of prediction-error. In general, higher prediction performance leads to larger embedding capacity and/or lower signal distortion. Leveraging on recent advances in graph signal processing (GSP), we pose pixel prediction as a graph-signal restoration problem, where the appropriate edge weights of the underlying graph are computed using a similar patch searched in a semi-local neighborhood. Specifically, for each candidate patch, we first examine eigenvalues of its structure tensor to estimate its local smoothness. If sufficiently smooth, we pose a maximum a posteriori (MAP) problem using either a quadratic Laplacian regularizer or a graph total variation (GTV) term as signal prior. While the MAP problem using the first prior has a closed-form solution, we design an efficient algorithm for the second prior using alternating direction method of multipliers (ADMM) with nested proximal gradient descent. Experimental results show that with better quality GSP-based prediction, at low capacity the visual quality of the embedded image exceeds state-of-the-art methods noticeably.



### Fast and robust misalignment correction of Fourier ptychographic microscopy
- **Arxiv ID**: http://arxiv.org/abs/1803.00395v1
- **DOI**: 10.1364/OE.26.023661
- **Categories**: **cs.CV**, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/1803.00395v1)
- **Published**: 2018-02-20 03:32:29+00:00
- **Updated**: 2018-02-20 03:32:29+00:00
- **Authors**: Ao Zhou, Wei Wang, Ni Chen, Edmund Y. Lam, Byoungho Lee, Guohai Situ
- **Comment**: None
- **Journal**: None
- **Summary**: Fourier ptychographi cmicroscopy(FPM) is a newly developed computational imaging technique that can provide gigapixel images with both high resolution (HR) and wide field of view (FOV). However, the positional misalignment of the LED array induces a degradation of the reconstruction, especially in the regions away from the optical axis. In this paper, we propose a robust and fast method to correct the LED misalignment of FPM, termed as misalignment correction for FPM (mcFPM). Although different regions in the FOV have different sensitivity to the LED misalignment, the experimental results show that mcFPM is robust to eliminate the degradation in each region. Compared with the state-of-the-art methods, mcFPM is much faster.



### Recurrent Residual Convolutional Neural Network based on U-Net (R2U-Net) for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1802.06955v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.06955v5)
- **Published**: 2018-02-20 03:59:39+00:00
- **Updated**: 2018-05-29 17:57:31+00:00
- **Authors**: Md Zahangir Alom, Mahmudul Hasan, Chris Yakopcic, Tarek M. Taha, Vijayan K. Asari
- **Comment**: 12 pages, 21 figures, 3 Tables
- **Journal**: None
- **Summary**: Deep learning (DL) based semantic segmentation methods have been providing state-of-the-art performance in the last few years. More specifically, these techniques have been successfully applied to medical image classification, segmentation, and detection tasks. One deep learning technique, U-Net, has become one of the most popular for these applications. In this paper, we propose a Recurrent Convolutional Neural Network (RCNN) based on U-Net as well as a Recurrent Residual Convolutional Neural Network (RRCNN) based on U-Net models, which are named RU-Net and R2U-Net respectively. The proposed models utilize the power of U-Net, Residual Network, as well as RCNN. There are several advantages of these proposed architectures for segmentation tasks. First, a residual unit helps when training deep architecture. Second, feature accumulation with recurrent residual convolutional layers ensures better feature representation for segmentation tasks. Third, it allows us to design better U-Net architecture with same number of network parameters with better performance for medical image segmentation. The proposed models are tested on three benchmark datasets such as blood vessel segmentation in retina images, skin cancer segmentation, and lung lesion segmentation. The experimental results show superior performance on segmentation tasks compared to equivalent models including U-Net and residual U-Net (ResU-Net).



### Agile Amulet: Real-Time Salient Object Detection with Contextual Attention
- **Arxiv ID**: http://arxiv.org/abs/1802.06960v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.06960v1)
- **Published**: 2018-02-20 04:14:08+00:00
- **Updated**: 2018-02-20 04:14:08+00:00
- **Authors**: Pingping Zhang, Luyao Wang, Dong Wang, Huchuan Lu, Chunhua Shen
- **Comment**: 10 pages, 4 figures and 3 tables
- **Journal**: None
- **Summary**: This paper proposes an Agile Aggregating Multi-Level feaTure framework (Agile Amulet) for salient object detection. The Agile Amulet builds on previous works to predict saliency maps using multi-level convolutional features. Compared to previous works, Agile Amulet employs some key innovations to improve training and testing speed while also increase prediction accuracy. More specifically, we first introduce a contextual attention module that can rapidly highlight most salient objects or regions with contextual pyramids. Thus, it effectively guides the learning of low-layer convolutional features and tells the backbone network where to look. The contextual attention module is a fully convolutional mechanism that simultaneously learns complementary features and predicts saliency scores at each pixel. In addition, we propose a novel method to aggregate multi-level deep convolutional features. As a result, we are able to use the integrated side-output features of pre-trained convolutional networks alone, which significantly reduces the model parameters leading to a model size of 67 MB, about half of Amulet. Compared to other deep learning based saliency methods, Agile Amulet is of much lighter-weight, runs faster (30 fps in real-time) and achieves higher performance on seven public benchmarks in terms of both quantitative and qualitative evaluation.



### Co-occurrence matrix analysis-based semi-supervised training for object detection
- **Arxiv ID**: http://arxiv.org/abs/1802.06964v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.06964v1)
- **Published**: 2018-02-20 04:39:49+00:00
- **Updated**: 2018-02-20 04:39:49+00:00
- **Authors**: Min-Kook Choi, Jaehyeong Park, Jihun Jung, Heechul Jung, Jin-Hee Lee, Woong Jae Won, Woo Young Jung, Jincheol Kim, Soon Kwon
- **Comment**: Submitted to International Conference on Image Processing (ICIP) 2018
- **Journal**: None
- **Summary**: One of the most important factors in training object recognition networks using convolutional neural networks (CNNs) is the provision of annotated data accompanying human judgment. Particularly, in object detection or semantic segmentation, the annotation process requires considerable human effort. In this paper, we propose a semi-supervised learning (SSL)-based training methodology for object detection, which makes use of automatic labeling of un-annotated data by applying a network previously trained from an annotated dataset. Because an inferred label by the trained network is dependent on the learned parameters, it is often meaningless for re-training the network. To transfer a valuable inferred label to the unlabeled data, we propose a re-alignment method based on co-occurrence matrix analysis that takes into account one-hot-vector encoding of the estimated label and the correlation between the objects in the image. We used an MS-COCO detection dataset to verify the performance of the proposed SSL method and deformable neural networks (D-ConvNets) as an object detector for basic training. The performance of the existing state-of-the-art detectors (DConvNets, YOLO v2, and single shot multi-box detector (SSD)) can be improved by the proposed SSL method without using the additional model parameter or modifying the network architecture.



### A survey on trajectory clustering analysis
- **Arxiv ID**: http://arxiv.org/abs/1802.06971v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.06971v1)
- **Published**: 2018-02-20 05:28:51+00:00
- **Updated**: 2018-02-20 05:28:51+00:00
- **Authors**: Jiang Bian, Dayong Tian, Yuanyan Tang, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: This paper comprehensively surveys the development of trajectory clustering. Considering the critical role of trajectory data mining in modern intelligent systems for surveillance security, abnormal behavior detection, crowd behavior analysis, and traffic control, trajectory clustering has attracted growing attention. Existing trajectory clustering methods can be grouped into three categories: unsupervised, supervised and semi-supervised algorithms. In spite of achieving a certain level of development, trajectory clustering is limited in its success by complex conditions such as application scenarios and data dimensions. This paper provides a holistic understanding and deep insight into trajectory clustering, and presents a comprehensive analysis of representative methods and promising future directions.



### Robust positioning of drones for land use monitoring in strong terrain relief using vision-based navigation
- **Arxiv ID**: http://arxiv.org/abs/1803.00398v1
- **DOI**: 10.21660/2018.45.7322
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.00398v1)
- **Published**: 2018-02-20 06:51:09+00:00
- **Updated**: 2018-02-20 06:51:09+00:00
- **Authors**: Oleg Kupervasser, Vitalii Sarychev, Alexander Rubinstein, Roman Yavich
- **Comment**: 7 pages, 7 figures
- **Journal**: International Journal of GEOMATE, May, 2018 Vol.14, Issue 45,
  pp.10-16; Proceeding of the 7 Conference on GEOMATE 2017, November 2017, Tsu
  City, Japan
- **Summary**: For land use monitoring, the main problems are robust positioning in urban canyons and strong terrain reliefs with the use of GPS system only. Indeed, satellite signal reflection and shielding in urban canyons and strong terrain relief results in problems with correct positioning. Using GNSS-RTK does not solve the problem completely because in some complex situations the whole satellite's system works incorrectly. We transform the weakness (urban canyons and strong terrain relief) to an advantage. It is a vision-based navigation using a map of the terrain relief. We investigate and demonstrate the effectiveness of this technology in Chinese region Xiaoshan. The accuracy of the vision-based navigation system corresponds to the expected for these conditions. . It was concluded that the maximum position error based on vision-based navigation is 20 m and the maximum angle Euler error based on vision-based navigation is 0.83 degree. In case of camera movement, the maximum position error based on vision-based navigation is 30m and the maximum Euler angle error based on vision-based navigation is 2.2 degrees.



### Unsupervised Band Selection of Hyperspectral Images via Multi-dictionary Sparse Representation
- **Arxiv ID**: http://arxiv.org/abs/1802.06983v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.06983v1)
- **Published**: 2018-02-20 07:04:09+00:00
- **Updated**: 2018-02-20 07:04:09+00:00
- **Authors**: Fei Li, Pingping Zhang, Huchuan Lu
- **Comment**: Submitted to IEEE Transactions on Circuits and Systems for Video
  Technology, including 13 figures and 2 tables
- **Journal**: None
- **Summary**: Hyperspectral images have far more spectral bands than ordinary multispectral images. Rich band information provides more favorable conditions for the tremendous applications. However, significant increase in the dimensionality of spectral bands may lead to the curse of dimensionality, especially for classification applications. Furthermore, there are a large amount of redundant information among the raw image cubes due to water absorptions, sensor noises and other influence factors. Band selection is a direct and effective method to remove redundant information and reduce the spectral dimension for decreasing computational complexity and avoiding the curse of dimensionality. In this paper, we present a novel learning framework for band selection based on the idea of sparse representation. More specifically, first each band is approximately represented by the linear combination of other bands, then the original band image can be represented by a multi-dictionary learning mechanism. As a result, a group of weights can be obtained by sparse optimization for all bands. Finally, the specific bands will be selected, if they get higher weights than other bands in the representation of the original image. Experimental results on three widely used hyperspectral datasets show that our proposed algorithm achieves better performance in hyperspectral image classification, when compared with other state-of-art band selection methods.



### Segmentation hiérarchique faiblement supervisée
- **Arxiv ID**: http://arxiv.org/abs/1802.07008v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1802.07008v1)
- **Published**: 2018-02-20 08:42:05+00:00
- **Updated**: 2018-02-20 08:42:05+00:00
- **Authors**: Amin Fehri, Santiago Velasco-Forero, Fernand Meyer
- **Comment**: in French
- **Journal**: 26e colloque GRETSI, Sep 2017, Juan-les-Pins, France
- **Summary**: Image segmentation is the process of partitioning an image into a set of meaningful regions according to some criteria. Hierarchical segmentation has emerged as a major trend in this regard as it favors the emergence of important regions at different scales. On the other hand, many methods allow us to have prior information on the position of structures of interest in the images. In this paper, we present a versatile hierarchical segmentation method that takes into account any prior spatial information and outputs a hierarchical segmentation that emphasizes the contours or regions of interest while preserving the important structures in the image. An application of this method to the weakly-supervised segmentation problem is presented.



### Fusing Video and Inertial Sensor Data for Walking Person Identification
- **Arxiv ID**: http://arxiv.org/abs/1802.07021v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.07021v1)
- **Published**: 2018-02-20 09:16:21+00:00
- **Updated**: 2018-02-20 09:16:21+00:00
- **Authors**: Yuehong Huang, Yu-Chee Tseng
- **Comment**: None
- **Journal**: None
- **Summary**: An autonomous computer system (such as a robot) typically needs to identify, locate, and track persons appearing in its sight. However, most solutions have their limitations regarding efficiency, practicability, or environmental constraints. In this paper, we propose an effective and practical system which combines video and inertial sensors for person identification (PID). Persons who do different activities are easy to identify. To show the robustness and potential of our system, we propose a walking person identification (WPID) method to identify persons walking at the same time. By comparing features derived from both video and inertial sensor data, we can associate sensors in smartphones with human objects in videos. Results show that the correctly identified rate of our WPID method can up to 76% in 2 seconds.



### Do deep nets really need weight decay and dropout?
- **Arxiv ID**: http://arxiv.org/abs/1802.07042v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.07042v3)
- **Published**: 2018-02-20 10:12:23+00:00
- **Updated**: 2018-07-12 16:37:49+00:00
- **Authors**: Alex Hernández-García, Peter König
- **Comment**: Minor changes: more explicit legend of the figure, clearer second
  paragraph of the introduction, details of weight decay and dropout in the
  appendix
- **Journal**: None
- **Summary**: The impressive success of modern deep neural networks on computer vision tasks has been achieved through models of very large capacity compared to the number of available training examples. This overparameterization is often said to be controlled with the help of different regularization techniques, mainly weight decay and dropout. However, since these techniques reduce the effective capacity of the model, typically even deeper and wider architectures are required to compensate for the reduced capacity. Therefore, there seems to be a waste of capacity in this practice. In this paper we build upon recent research that suggests that explicit regularization may not be as important as widely believed and carry out an ablation study that concludes that weight decay and dropout may not be necessary for object recognition if enough data augmentation is introduced.



### Latent RANSAC
- **Arxiv ID**: http://arxiv.org/abs/1802.07045v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.07045v2)
- **Published**: 2018-02-20 10:16:11+00:00
- **Updated**: 2018-06-03 14:41:36+00:00
- **Authors**: Simon Korman, Roee Litman
- **Comment**: presented in CVPR 2018
- **Journal**: None
- **Summary**: We present a method that can evaluate a RANSAC hypothesis in constant time, i.e. independent of the size of the data. A key observation here is that correct hypotheses are tightly clustered together in the latent parameter domain. In a manner similar to the generalized Hough transform we seek to find this cluster, only that we need as few as two votes for a successful detection. Rapidly locating such pairs of similar hypotheses is made possible by adapting the recent "Random Grids" range-search technique. We only perform the usual (costly) hypothesis verification stage upon the discovery of a close pair of hypotheses. We show that this event rarely happens for incorrect hypotheses, enabling a significant speedup of the RANSAC pipeline. The suggested approach is applied and tested on three robust estimation problems: camera localization, 3D rigid alignment and 2D-homography estimation. We perform rigorous testing on both synthetic and real datasets, demonstrating an improvement in efficiency without a compromise in accuracy. Furthermore, we achieve state-of-the-art 3D alignment results on the challenging "Redwood" loop-closure challenge.



### Novel View Synthesis for Large-scale Scene using Adversarial Loss
- **Arxiv ID**: http://arxiv.org/abs/1802.07064v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.07064v1)
- **Published**: 2018-02-20 11:21:11+00:00
- **Updated**: 2018-02-20 11:21:11+00:00
- **Authors**: Xiaochuan Yin, Henglai Wei, Penghong lin, Xiangwei Wang, Qijun Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Novel view synthesis aims to synthesize new images from different viewpoints of given images. Most of previous works focus on generating novel views of certain objects with a fixed background. However, for some applications, such as virtual reality or robotic manipulations, large changes in background may occur due to the egomotion of the camera. Generated images of a large-scale environment from novel views may be distorted if the structure of the environment is not considered. In this work, we propose a novel fully convolutional network, that can take advantage of the structural information explicitly by incorporating the inverse depth features. The inverse depth features are obtained from CNNs trained with sparse labeled depth values. This framework can easily fuse multiple images from different viewpoints. To fill the missing textures in the generated image, adversarial loss is applied, which can also improve the overall image quality. Our method is evaluated on the KITTI dataset. The results show that our method can generate novel views of large-scale scene without distortion. The effectiveness of our approach is demonstrated through qualitative and quantitative evaluation.



### Composite Optimization by Nonconvex Majorization-Minimization
- **Arxiv ID**: http://arxiv.org/abs/1802.07072v2
- **DOI**: None
- **Categories**: **math.OC**, cs.CV, math.NA, 90C26, 90C06, 68U10, 32B20, 65K10, 47J06
- **Links**: [PDF](http://arxiv.org/pdf/1802.07072v2)
- **Published**: 2018-02-20 11:59:33+00:00
- **Updated**: 2018-09-03 10:33:44+00:00
- **Authors**: Jonas Geiping, Michael Moeller
- **Comment**: 38 pages, 12 figures, accepted for publication in SIIMS
- **Journal**: None
- **Summary**: The minimization of a nonconvex composite function can model a variety of imaging tasks. A popular class of algorithms for solving such problems are majorization-minimization techniques which iteratively approximate the composite nonconvex function by a majorizing function that is easy to minimize. Most techniques, e.g. gradient descent, utilize convex majorizers in order to guarantee that the majorizer is easy to minimize. In our work we consider a natural class of nonconvex majorizers for these functions, and show that these majorizers are still sufficient for a globally convergent optimization scheme. Numerical results illustrate that by applying this scheme, one can often obtain superior local optima compared to previous majorization-minimization methods, when the nonconvex majorizers are solved to global optimality. Finally, we illustrate the behavior of our algorithm for depth super-resolution from raw time-of-flight data.



### Correlation Flow: Robust Optical Flow Using Kernel Cross-Correlators
- **Arxiv ID**: http://arxiv.org/abs/1802.07078v2
- **DOI**: 10.1109/ICRA.2018.8460569
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1802.07078v2)
- **Published**: 2018-02-20 12:19:03+00:00
- **Updated**: 2018-02-25 06:29:18+00:00
- **Authors**: Chen Wang, Tete Ji, Thien-Minh Nguyen, Lihua Xie
- **Comment**: 2018 International Conference on Robotics and Automation (ICRA 2018)
- **Journal**: 2018 IEEE International Conference on Robotics and Automation
  (ICRA)
- **Summary**: Robust velocity and position estimation is crucial for autonomous robot navigation. The optical flow based methods for autonomous navigation have been receiving increasing attentions in tandem with the development of micro unmanned aerial vehicles. This paper proposes a kernel cross-correlator (KCC) based algorithm to determine optical flow using a monocular camera, which is named as correlation flow (CF). Correlation flow is able to provide reliable and accurate velocity estimation and is robust to motion blur. In addition, it can also estimate the altitude velocity and yaw rate, which are not available by traditional methods. Autonomous flight tests on a quadcopter show that correlation flow can provide robust trajectory estimation with very low processing power. The source codes are released based on the ROS framework.



### i-RevNet: Deep Invertible Networks
- **Arxiv ID**: http://arxiv.org/abs/1802.07088v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1802.07088v1)
- **Published**: 2018-02-20 12:38:49+00:00
- **Updated**: 2018-02-20 12:38:49+00:00
- **Authors**: Jörn-Henrik Jacobsen, Arnold Smeulders, Edouard Oyallon
- **Comment**: None
- **Journal**: ICLR 2018 - International Conference on Learning Representations,
  Apr 2018, Vancouver, Canada. 2018, https://iclr.cc/
- **Summary**: It is widely believed that the success of deep convolutional networks is based on progressively discarding uninformative variability about the input with respect to the problem at hand. This is supported empirically by the difficulty of recovering images from their hidden representations, in most commonly used network architectures. In this paper we show via a one-to-one mapping that this loss of information is not a necessary condition to learn representations that generalize well on complicated problems, such as ImageNet. Via a cascade of homeomorphic layers, we build the i-RevNet, a network that can be fully inverted up to the final projection onto the classes, i.e. no information is discarded. Building an invertible architecture is difficult, for one, because the local inversion is ill-conditioned, we overcome this by providing an explicit inverse. An analysis of i-RevNets learned representations suggests an alternative explanation for the success of deep networks by a progressive contraction and linear separation with depth. To shed light on the nature of the model learned by the i-RevNet we reconstruct linear interpolations between natural image representations.



### Camera-based vehicle velocity estimation from monocular video
- **Arxiv ID**: http://arxiv.org/abs/1802.07094v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.07094v1)
- **Published**: 2018-02-20 12:54:39+00:00
- **Updated**: 2018-02-20 12:54:39+00:00
- **Authors**: Moritz Kampelmühler, Michael G. Müller, Christoph Feichtenhofer
- **Comment**: 8 pages, 5 figures, in CVWW2018
- **Journal**: None
- **Summary**: This paper documents the winning entry at the CVPR2017 vehicle velocity estimation challenge. Velocity estimation is an emerging task in autonomous driving which has not yet been thoroughly explored. The goal is to estimate the relative velocity of a specific vehicle from a sequence of images. In this paper, we present a light-weight approach for directly regressing vehicle velocities from their trajectories using a multilayer perceptron. Another contribution is an explorative study of features for monocular vehicle velocity estimation. We find that light-weight trajectory based features outperform depth and motion cues extracted from deep ConvNets, especially for far-distance predictions where current disparity and optical flow estimators are challenged significantly. Our light-weight approach is real-time capable on a single CPU and outperforms all competing entries in the velocity estimation challenge. On the test set, we report an average error of 1.12 m/s which is comparable to a (ground-truth) system that combines LiDAR and radar techniques to achieve an error of around 0.71 m/s.



### Uncertainty Estimates and Multi-Hypotheses Networks for Optical Flow
- **Arxiv ID**: http://arxiv.org/abs/1802.07095v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.07095v4)
- **Published**: 2018-02-20 12:56:39+00:00
- **Updated**: 2018-12-20 15:17:23+00:00
- **Authors**: Eddy Ilg, Özgün Çiçek, Silvio Galesso, Aaron Klein, Osama Makansi, Frank Hutter, Thomas Brox
- **Comment**: Accepted to ECCV 2018 as poster. See Video at:
  https://youtu.be/HvyovWSo8uE
- **Journal**: None
- **Summary**: Optical flow estimation can be formulated as an end-to-end supervised learning problem, which yields estimates with a superior accuracy-runtime tradeoff compared to alternative methodology. In this paper, we make such networks estimate their local uncertainty about the correctness of their prediction, which is vital information when building decisions on top of the estimations. For the first time we compare several strategies and techniques to estimate uncertainty in a large-scale computer vision task like optical flow estimation. Moreover, we introduce a new network architecture utilizing the Winner-Takes-All loss and show that this can provide complementary hypotheses and uncertainty estimates efficiently with a single forward pass and without the need for sampling or ensembles. Finally, we demonstrate the quality of the different uncertainty estimates, which is clearly above previous confidence measures on optical flow and allows for interactive frame rates.



### Stroke Controllable Fast Style Transfer with Adaptive Receptive Fields
- **Arxiv ID**: http://arxiv.org/abs/1802.07101v4
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1802.07101v4)
- **Published**: 2018-02-20 13:21:53+00:00
- **Updated**: 2018-10-19 02:26:47+00:00
- **Authors**: Yongcheng Jing, Yang Liu, Yezhou Yang, Zunlei Feng, Yizhou Yu, Dacheng Tao, Mingli Song
- **Comment**: Accepted by ECCV2018. Supplementary material:
  https://yongchengjing.com/pdf/strokeControllable_supp.pdf
- **Journal**: None
- **Summary**: The Fast Style Transfer methods have been recently proposed to transfer a photograph to an artistic style in real-time. This task involves controlling the stroke size in the stylized results, which remains an open challenge. In this paper, we present a stroke controllable style transfer network that can achieve continuous and spatial stroke size control. By analyzing the factors that influence the stroke size, we propose to explicitly account for the receptive field and the style image scales. We propose a StrokePyramid module to endow the network with adaptive receptive fields, and two training strategies to achieve faster convergence and augment new stroke sizes upon a trained model respectively. By combining the proposed runtime control strategies, our network can achieve continuous changes in stroke sizes and produce distinct stroke sizes in different spatial regions within the same output image.



### Deep BCD-Net Using Identical Encoding-Decoding CNN Structures for Iterative Image Recovery
- **Arxiv ID**: http://arxiv.org/abs/1802.07129v2
- **DOI**: 10.1109/IVMSPW.2018.8448694
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1802.07129v2)
- **Published**: 2018-02-20 14:37:30+00:00
- **Updated**: 2018-04-28 22:16:11+00:00
- **Authors**: Il Yong Chun, Jeffrey A. Fessler
- **Comment**: 5 pages, 3 figures
- **Journal**: Proc. IEEE Image, Video, and Multidim. Signal Process. (IVMSP)
  Workshop, pp. 1-5, Apr. 2018
- **Summary**: In "extreme" computational imaging that collects extremely undersampled or noisy measurements, obtaining an accurate image within a reasonable computing time is challenging. Incorporating image mapping convolutional neural networks (CNN) into iterative image recovery has great potential to resolve this issue. This paper 1) incorporates image mapping CNN using identical convolutional kernels in both encoders and decoders into a block coordinate descent (BCD) signal recovery method and 2) applies alternating direction method of multipliers to train the aforementioned image mapping CNN. We refer to the proposed recurrent network as BCD-Net using identical encoding-decoding CNN structures. Numerical experiments show that, for a) denoising low signal-to-noise-ratio images and b) extremely undersampled magnetic resonance imaging, the proposed BCD-Net achieves significantly more accurate image recovery, compared to BCD-Net using distinct encoding-decoding structures and/or the conventional image recovery model using both wavelets and total variation.



### Transport-Based Pattern Theory: A Signal Transformation Approach
- **Arxiv ID**: http://arxiv.org/abs/1802.07163v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.07163v2)
- **Published**: 2018-02-20 15:56:44+00:00
- **Updated**: 2018-04-17 13:29:53+00:00
- **Authors**: Liam Cattell, Gustavo K. Rohde
- **Comment**: None
- **Journal**: None
- **Summary**: In many scientific fields imaging is used to relate a certain physical quantity to other dependent variables. Therefore, images can be considered as a map from a real-world coordinate system to the non-negative measurements being acquired. In this work we describe an approach for simultaneous modeling and inference of such data, using the mathematics of optimal transport. To achieve this, we describe a numerical implementation of the linear optimal transport transform, based on the solution of the Monge-Ampere equation, which uses Brenier's theorem to characterize the solution of the Monge functional as the derivative of a convex potential function. We use our implementation of the transform to compute a curl-free mapping between two images, and show that it is able to match images with lower error that existing methods. Moreover, we provide theoretical justification for properties of the linear optimal transport framework observed in the literature, including a theorem for the linear separation of data classes. Finally, we use our optimal transport method to empirically demonstrate that the linear separability theorem holds, by rendering non-linearly separable data as linearly separable following transform to transport space.



### Real-Time Dense Stereo Matching With ELAS on FPGA Accelerated Embedded Devices
- **Arxiv ID**: http://arxiv.org/abs/1802.07210v1
- **DOI**: 10.1109/LRA.2018.2800786
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1802.07210v1)
- **Published**: 2018-02-20 17:24:28+00:00
- **Updated**: 2018-02-20 17:24:28+00:00
- **Authors**: Oscar Rahnama, Duncan Frost, Ondrej Miksik, Philip H. S. Torr
- **Comment**: 8 pages, 7 figures, 2 tables
- **Journal**: IEEE Robotics and Automation Letters, vol. 3, no. 3, pp.
  2008-2015, July 2018
- **Summary**: For many applications in low-power real-time robotics, stereo cameras are the sensors of choice for depth perception as they are typically cheaper and more versatile than their active counterparts. Their biggest drawback, however, is that they do not directly sense depth maps; instead, these must be estimated through data-intensive processes. Therefore, appropriate algorithm selection plays an important role in achieving the desired performance characteristics.   Motivated by applications in space and mobile robotics, we implement and evaluate a FPGA-accelerated adaptation of the ELAS algorithm. Despite offering one of the best trade-offs between efficiency and accuracy, ELAS has only been shown to run at 1.5-3 fps on a high-end CPU. Our system preserves all intriguing properties of the original algorithm, such as the slanted plane priors, but can achieve a frame rate of 47fps whilst consuming under 4W of power. Unlike previous FPGA based designs, we take advantage of both components on the CPU/FPGA System-on-Chip to showcase the strategy necessary to accelerate more complex and computationally diverse algorithms for such low power, real-time systems.



### MoNet: Moments Embedding Network
- **Arxiv ID**: http://arxiv.org/abs/1802.07303v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.07303v2)
- **Published**: 2018-02-20 19:54:58+00:00
- **Updated**: 2018-03-29 19:34:39+00:00
- **Authors**: Mengran Gou, Fei Xiong, Octavia Camps, Mario Sznaier
- **Comment**: Accepted in CVPR 2018
- **Journal**: None
- **Summary**: Bilinear pooling has been recently proposed as a feature encoding layer, which can be used after the convolutional layers of a deep network, to improve performance in multiple vision tasks. Different from conventional global average pooling or fully connected layer, bilinear pooling gathers 2nd order information in a translation invariant fashion. However, a serious drawback of this family of pooling layers is their dimensionality explosion. Approximate pooling methods with compact properties have been explored towards resolving this weakness. Additionally, recent results have shown that significant performance gains can be achieved by adding 1st order information and applying matrix normalization to regularize unstable higher order information. However, combining compact pooling with matrix normalization and other order information has not been explored until now. In this paper, we unify bilinear pooling and the global Gaussian embedding layers through the empirical moment matrix. In addition, we propose a novel sub-matrix square-root layer, which can be used to normalize the output of the convolution layer directly and mitigate the dimensionality problem with off-the-shelf compact pooling methods. Our experiments on three widely used fine-grained classification datasets illustrate that our proposed architecture, MoNet, can achieve similar or better performance than with the state-of-art G2DeNet. Furthermore, when combined with compact pooling technique, MoNet obtains comparable performance with encoded features with 96% less dimensions.



### Devon: Deformable Volume Network for Learning Optical Flow
- **Arxiv ID**: http://arxiv.org/abs/1802.07351v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.07351v2)
- **Published**: 2018-02-20 21:53:19+00:00
- **Updated**: 2019-03-04 07:35:11+00:00
- **Authors**: Yao Lu, Jack Valmadre, Heng Wang, Juho Kannala, Mehrtash Harandi, Philip H. S. Torr
- **Comment**: None
- **Journal**: None
- **Summary**: State-of-the-art neural network models estimate large displacement optical flow in multi-resolution and use warping to propagate the estimation between two resolutions. Despite their impressive results, it is known that there are two problems with the approach. First, the multi-resolution estimation of optical flow fails in situations where small objects move fast. Second, warping creates artifacts when occlusion or dis-occlusion happens. In this paper, we propose a new neural network module, Deformable Cost Volume, which alleviates the two problems. Based on this module, we designed the Deformable Volume Network (Devon) which can estimate multi-scale optical flow in a single high resolution. Experiments show Devon is more suitable in handling small objects moving fast and achieves comparable results to the state-of-the-art methods in public benchmarks.



### Calcium Removal From Cardiac CT Images Using Deep Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1803.00399v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1803.00399v1)
- **Published**: 2018-02-20 23:10:34+00:00
- **Updated**: 2018-02-20 23:10:34+00:00
- **Authors**: Siming Yan, Feng Shi, Yuhua Chen, Damini Dey, Sang-Eun Lee, Hyuk-Jae Chang, Debiao Li, Yibin Xie
- **Comment**: Accepted by ISBI 2018
- **Journal**: None
- **Summary**: Coronary calcium causes beam hardening and blooming artifacts on cardiac computed tomography angiography (CTA) images, which lead to overestimation of lumen stenosis and reduction of diagnostic specificity. To properly remove coronary calcification and restore arterial lumen precisely, we propose a machine learning-based method with a multi-step inpainting process. We developed a new network configuration, Dense-Unet, to achieve optimal performance with low computational cost. Results after the calcium removal process were validated by comparing with gold-standard X-ray angiography. Our results demonstrated that removing coronary calcification from images with the proposed approach was feasible, and may potentially improve the diagnostic accuracy of CTA.



