# Arxiv Papers in cs.CV on 2018-02-10
### Invertible Autoencoder for domain adaptation
- **Arxiv ID**: http://arxiv.org/abs/1802.06869v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1802.06869v1)
- **Published**: 2018-02-10 01:00:32+00:00
- **Updated**: 2018-02-10 01:00:32+00:00
- **Authors**: Yunfei Teng, Anna Choromanska, Mariusz Bojarski
- **Comment**: None
- **Journal**: None
- **Summary**: The unsupervised image-to-image translation aims at finding a mapping between the source ($A$) and target ($B$) image domains, where in many applications aligned image pairs are not available at training. This is an ill-posed learning problem since it requires inferring the joint probability distribution from marginals. Joint learning of coupled mappings $F_{AB}: A \rightarrow B$ and $F_{BA}: B \rightarrow A$ is commonly used by the state-of-the-art methods, like CycleGAN [Zhu et al., 2017], to learn this translation by introducing cycle consistency requirement to the learning problem, i.e. $F_{AB}(F_{BA}(B)) \approx B$ and $F_{BA}(F_{AB}(A)) \approx A$. Cycle consistency enforces the preservation of the mutual information between input and translated images. However, it does not explicitly enforce $F_{BA}$ to be an inverse operation to $F_{AB}$. We propose a new deep architecture that we call invertible autoencoder (InvAuto) to explicitly enforce this relation. This is done by forcing an encoder to be an inverted version of the decoder, where corresponding layers perform opposite mappings and share parameters. The mappings are constrained to be orthonormal. The resulting architecture leads to the reduction of the number of trainable parameters (up to $2$ times). We present image translation results on benchmark data sets and demonstrate state-of-the art performance of our approach. Finally, we test the proposed domain adaptation method on the task of road video conversion. We demonstrate that the videos converted with InvAuto have high quality and show that the NVIDIA neural-network-based end-to-end learning system for autonomous driving, known as PilotNet, trained on real road videos performs well when tested on the converted ones.



### MAGAN: Aligning Biological Manifolds
- **Arxiv ID**: http://arxiv.org/abs/1803.00385v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.00385v1)
- **Published**: 2018-02-10 01:11:34+00:00
- **Updated**: 2018-02-10 01:11:34+00:00
- **Authors**: Matthew Amodio, Smita Krishnaswamy
- **Comment**: None
- **Journal**: Proceedings of the 35th International Conference on Machine
  Learning, PMLR 80:215-223, 2018
- **Summary**: It is increasingly common in many types of natural and physical systems (especially biological systems) to have different types of measurements performed on the same underlying system. In such settings, it is important to align the manifolds arising from each measurement in order to integrate such data and gain an improved picture of the system. We tackle this problem using generative adversarial networks (GANs). Recently, GANs have been utilized to try to find correspondences between sets of samples. However, these GANs are not explicitly designed for proper alignment of manifolds. We present a new GAN called the Manifold-Aligning GAN (MAGAN) that aligns two manifolds such that related points in each measurement space are aligned together. We demonstrate applications of MAGAN in single-cell biology in integrating two different measurement types together. In our demonstrated examples, cells from the same tissue are measured with both genomic (single-cell RNA-sequencing) and proteomic (mass cytometry) technologies. We show that the MAGAN successfully aligns them such that known correlations between measured markers are improved compared to other recently proposed models.



### AMC: AutoML for Model Compression and Acceleration on Mobile Devices
- **Arxiv ID**: http://arxiv.org/abs/1802.03494v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.03494v4)
- **Published**: 2018-02-10 01:32:44+00:00
- **Updated**: 2019-01-16 03:25:50+00:00
- **Authors**: Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, Song Han
- **Comment**: None
- **Journal**: None
- **Summary**: Model compression is a critical technique to efficiently deploy neural network models on mobile devices which have limited computation resources and tight power budgets. Conventional model compression techniques rely on hand-crafted heuristics and rule-based policies that require domain experts to explore the large design space trading off among model size, speed, and accuracy, which is usually sub-optimal and time-consuming. In this paper, we propose AutoML for Model Compression (AMC) which leverage reinforcement learning to provide the model compression policy. This learning-based compression policy outperforms conventional rule-based compression policy by having higher compression ratio, better preserving the accuracy and freeing human labor. Under 4x FLOPs reduction, we achieved 2.7% better accuracy than the handcrafted model compression policy for VGG-16 on ImageNet. We applied this automated, push-the-button compression pipeline to MobileNet and achieved 1.81x speedup of measured inference latency on an Android phone and 1.43x speedup on the Titan XP GPU, with only 0.1% loss of ImageNet Top-1 accuracy.



### Generative Adversarial Networks and Probabilistic Graph Models for Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1802.03495v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1802.03495v1)
- **Published**: 2018-02-10 01:33:52+00:00
- **Updated**: 2018-02-10 01:33:52+00:00
- **Authors**: Zilong Zhong, Jonathan Li
- **Comment**: Accepted by AAAI-18
- **Journal**: None
- **Summary**: High spectral dimensionality and the shortage of annotations make hyperspectral image (HSI) classification a challenging problem. Recent studies suggest that convolutional neural networks can learn discriminative spatial features, which play a paramount role in HSI interpretation. However, most of these methods ignore the distinctive spectral-spatial characteristic of hyperspectral data. In addition, a large amount of unlabeled data remains an unexploited gold mine for efficient data use. Therefore, we proposed an integration of generative adversarial networks (GANs) and probabilistic graphical models for HSI classification. Specifically, we used a spectral-spatial generator and a discriminator to identify land cover categories of hyperspectral cubes. Moreover, to take advantage of a large amount of unlabeled data, we adopted a conditional random field to refine the preliminary classification results generated by GANs. Experimental results obtained using two commonly studied datasets demonstrate that the proposed framework achieved encouraging classification accuracy using a small number of data for training.



### Local Contrast Learning
- **Arxiv ID**: http://arxiv.org/abs/1802.03499v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1802.03499v1)
- **Published**: 2018-02-10 01:54:44+00:00
- **Updated**: 2018-02-10 01:54:44+00:00
- **Authors**: Chuanyun Xu, Yang Zhang, Xin Feng, YongXing Ge, Yihao Zhang, Jianwu Long
- **Comment**: 10 pages, 4 figures
- **Journal**: None
- **Summary**: Learning a deep model from small data is yet an opening and challenging problem. We focus on one-shot classification by deep learning approach based on a small quantity of training samples. We proposed a novel deep learning approach named Local Contrast Learning (LCL) based on the key insight about a human cognitive behavior that human recognizes the objects in a specific context by contrasting the objects in the context or in her/his memory. LCL is used to train a deep model that can contrast the recognizing sample with a couple of contrastive samples randomly drawn and shuffled. On one-shot classification task on Omniglot, the deep model based LCL with 122 layers and 1.94 millions of parameters, which was trained on a tiny dataset with only 60 classes and 20 samples per class, achieved the accuracy 97.99% that outperforms human and state-of-the-art established by Bayesian Program Learning (BPL) trained on 964 classes. LCL is a fundamental idea which can be applied to alleviate parametric model's overfitting resulted by lack of training samples.



### Coulomb Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/1802.03505v6
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1802.03505v6)
- **Published**: 2018-02-10 02:37:31+00:00
- **Updated**: 2019-11-26 10:25:20+00:00
- **Authors**: Emanuele Sansone, Hafiz Tiomoko Ali, Sun Jiacheng
- **Comment**: None
- **Journal**: None
- **Summary**: Learning the true density in high-dimensional feature spaces is a well-known problem in machine learning. In this work, we consider generative autoencoders based on maximum-mean discrepancy (MMD) and provide theoretical insights. In particular, (i) we prove that MMD coupled with Coulomb kernels has optimal convergence properties, which are similar to convex functionals, thus improving the training of autoencoders, and (ii) we provide a probabilistic bound on the generalization performance, highlighting some fundamental conditions to achieve better generalization. We validate the theory on synthetic examples and on the popular dataset of celebrities' faces, showing that our model, called Coulomb autoencoders, outperform the state-of-the-art.



### Multiparametric Deep Learning Tissue Signatures for a Radiological Biomarker of Breast Cancer: Preliminary Results
- **Arxiv ID**: http://arxiv.org/abs/1802.08200v1
- **DOI**: 10.1002/mp.13849
- **Categories**: **physics.med-ph**, cs.AI, cs.CV, q-bio.QM, 68T05, 92C55, I.2.1, I.2.5, I.6.5, J.3, H.1.1
- **Links**: [PDF](http://arxiv.org/pdf/1802.08200v1)
- **Published**: 2018-02-10 02:51:59+00:00
- **Updated**: 2018-02-10 02:51:59+00:00
- **Authors**: Vishwa S. Parekh, Katarzyna J. Macura, Susan Harvey, Ihab Kamel, Riham EI-Khouli, David A. Bluemke, Michael A. Jacobs
- **Comment**: Deep Learning, Machine learning, Magnetic resonance imaging,
  multiparametric MRI, Breast, Cancer, Diffusion, tissue biomarkers
- **Journal**: Medical physics 2020 47 (1), 75-88
- **Summary**: A new paradigm is beginning to emerge in Radiology with the advent of increased computational capabilities and algorithms. This has led to the ability of real time learning by computer systems of different lesion types to help the radiologist in defining disease. For example, using a deep learning network, we developed and tested a multiparametric deep learning (MPDL) network for segmentation and classification using multiparametric magnetic resonance imaging (mpMRI) radiological images. The MPDL network was constructed from stacked sparse autoencoders with inputs from mpMRI. Evaluation of MPDL consisted of cross-validation, sensitivity, and specificity. Dice similarity between MPDL and post-DCE lesions were evaluated. We demonstrate high sensitivity and specificity for differentiation of malignant from benign lesions of 90% and 85% respectively with an AUC of 0.93. The Integrated MPDL method accurately segmented and classified different breast tissue from multiparametric breast MRI using deep leaning tissue signatures.



### On-device Scalable Image-based Localization via Prioritized Cascade Search and Fast One-Many RANSAC
- **Arxiv ID**: http://arxiv.org/abs/1802.03510v2
- **DOI**: 10.1109/TIP.2018.2881829
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.03510v2)
- **Published**: 2018-02-10 03:23:07+00:00
- **Updated**: 2018-11-15 01:56:59+00:00
- **Authors**: Ngoc-Trung Tran, Dang-Khoa Le Tan, Anh-Dzung Doan, Thanh-Toan Do, Tuan-Anh Bui, Mengxuan Tan, Ngai-Man Cheung
- **Comment**: None
- **Journal**: None
- **Summary**: We present the design of an entire on-device system for large-scale urban localization using images. The proposed design integrates compact image retrieval and 2D-3D correspondence search to estimate the location in extensive city regions. Our design is GPS agnostic and does not require network connection. In order to overcome the resource constraints of mobile devices, we propose a system design that leverages the scalability advantage of image retrieval and accuracy of 3D model-based localization. Furthermore, we propose a new hashing-based cascade search for fast computation of 2D-3D correspondences. In addition, we propose a new one-many RANSAC for accurate pose estimation. The new one-many RANSAC addresses the challenge of repetitive building structures (e.g. windows, balconies) in urban localization. Extensive experiments demonstrate that our 2D-3D correspondence search achieves state-of-the-art localization accuracy on multiple benchmark datasets. Furthermore, our experiments on a large Google Street View (GSV) image dataset show the potential of large-scale localization entirely on a typical mobile device.



### Vehicle Pose and Shape Estimation through Multiple Monocular Vision
- **Arxiv ID**: http://arxiv.org/abs/1802.03515v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1802.03515v5)
- **Published**: 2018-02-10 03:56:19+00:00
- **Updated**: 2018-11-11 04:53:47+00:00
- **Authors**: Wenhao Ding, Shuaijun Li, Guilin Zhang, Xiangyu Lei, Huihuan Qian
- **Comment**: 8 pages, 8 figures, published in ROBIO 2018
- **Journal**: None
- **Summary**: In this paper, we present an accurate approach to estimate vehicles' pose and shape from off-board multiview images. The images are taken by monocular cameras and have small overlaps. We utilize state-of-the-art convolutional neural networks (CNNs) to extract vehicles' semantic keypoints and introduce a Cross Projection Optimization (CPO) method to estimate the 3D pose. During the iterative CPO process, an adaptive shape adjustment method named Hierarchical Wireframe Constraint (HWC) is implemented to estimate the shape. Our approach is evaluated under both simulated and real-world scenes for performance verification. It's shown that our algorithm outperforms other existing monocular and stereo methods for vehicles' pose and shape estimation. This approach provides a new and robust solution for off-board visual vehicle localization and tracking, which can be applied to massive surveillance camera networks for intelligent transportation.



### Deep learning in radiology: an overview of the concepts and a survey of the state of the art
- **Arxiv ID**: http://arxiv.org/abs/1802.08717v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.AP, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1802.08717v1)
- **Published**: 2018-02-10 04:00:55+00:00
- **Updated**: 2018-02-10 04:00:55+00:00
- **Authors**: Maciej A. Mazurowski, Mateusz Buda, Ashirbani Saha, Mustafa R. Bashir
- **Comment**: 27 pages, 4 figures
- **Journal**: None
- **Summary**: Deep learning is a branch of artificial intelligence where networks of simple interconnected units are used to extract patterns from data in order to solve complex problems. Deep learning algorithms have shown groundbreaking performance in a variety of sophisticated tasks, especially those related to images. They have often matched or exceeded human performance. Since the medical field of radiology mostly relies on extracting useful information from images, it is a very natural application area for deep learning, and research in this area has rapidly grown in recent years. In this article, we review the clinical reality of radiology and discuss the opportunities for application of deep learning algorithms. We also introduce basic concepts of deep learning including convolutional neural networks. Then, we present a survey of the research in deep learning applied to radiology. We organize the studies by the types of specific tasks that they attempt to solve and review the broad range of utilized deep learning algorithms. Finally, we briefly discuss opportunities and challenges for incorporating deep learning in the radiology practice of the future.



### Hydra: an Ensemble of Convolutional Neural Networks for Geospatial Land Classification
- **Arxiv ID**: http://arxiv.org/abs/1802.03518v2
- **DOI**: 10.1109/TGRS.2019.2906883
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.03518v2)
- **Published**: 2018-02-10 04:16:36+00:00
- **Updated**: 2019-03-20 18:03:28+00:00
- **Authors**: Rodrigo Minetto, Mauricio Pamplona Segundo, Sudeep Sarkar
- **Comment**: 12 pages, 14 figures, 5 tables
- **Journal**: None
- **Summary**: We describe in this paper Hydra, an ensemble of convolutional neural networks (CNN) for geospatial land classification. The idea behind Hydra is to create an initial CNN that is coarsely optimized but provides a good starting pointing for further optimization, which will serve as the Hydra's body. Then, the obtained weights are fine-tuned multiple times with different augmentation techniques, crop styles, and classes weights to form an ensemble of CNNs that represent the Hydra's heads. By doing so, we prompt convergence to different endpoints, which is a desirable aspect for ensembles. With this framework, we were able to reduce the training time while maintaining the classification performance of the ensemble. We created ensembles for our experiments using two state-of-the-art CNN architectures, ResNet and DenseNet. We have demonstrated the application of our Hydra framework in two datasets, FMOW and NWPU-RESISC45, achieving results comparable to the state-of-the-art for the former and the best reported performance so far for the latter. Code and CNN models are available at https://github.com/maups/hydra-fmow



### Coverless information hiding based on Generative Model
- **Arxiv ID**: http://arxiv.org/abs/1802.03528v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.03528v1)
- **Published**: 2018-02-10 06:16:33+00:00
- **Updated**: 2018-02-10 06:16:33+00:00
- **Authors**: Xintao Duan, Haoxian Song
- **Comment**: 4 pages,7 figures
- **Journal**: None
- **Summary**: A new coverless image information hiding method based on generative model is proposed, we feed the secret image to the generative model database, and generate a meaning-normal and independent image different from the secret image, then, the generated image is transmitted to the receiver and is fed to the generative model database to generate another image visually the same as the secret image. So we only need to transmit the meaning-normal image which is not related to the secret image, and we can achieve the same effect as the transmission of the secret image. This is the first time to propose the coverless image information hiding method based on generative model, compared with the traditional image steganography, the transmitted image does not embed any information of the secret image in this method, therefore, can effectively resist steganalysis tools. Experimental results show that our method has high capacity, safety and reliability.



### Collaborative Learning for Weakly Supervised Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1802.03531v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.03531v1)
- **Published**: 2018-02-10 06:36:52+00:00
- **Updated**: 2018-02-10 06:36:52+00:00
- **Authors**: Jiajie Wang, Jiangchao Yao, Ya Zhang, Rui Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Weakly supervised object detection has recently received much attention, since it only requires image-level labels instead of the bounding-box labels consumed in strongly supervised learning. Nevertheless, the save in labeling expense is usually at the cost of model accuracy. In this paper, we propose a simple but effective weakly supervised collaborative learning framework to resolve this problem, which trains a weakly supervised learner and a strongly supervised learner jointly by enforcing partial feature sharing and prediction consistency. For object detection, taking WSDDN-like architecture as weakly supervised detector sub-network and Faster-RCNN-like architecture as strongly supervised detector sub-network, we propose an end-to-end Weakly Supervised Collaborative Detection Network. As there is no strong supervision available to train the Faster-RCNN-like sub-network, a new prediction consistency loss is defined to enforce consistency of predictions between the two sub-networks as well as within the Faster-RCNN-like sub-networks. At the same time, the two detectors are designed to partially share features to further guarantee the model consistency at perceptual level. Extensive experiments on PASCAL VOC 2007 and 2012 data sets have demonstrated the effectiveness of the proposed framework.



### Tubule segmentation of fluorescence microscopy images based on convolutional neural networks with inhomogeneity correction
- **Arxiv ID**: http://arxiv.org/abs/1802.03542v1
- **DOI**: 10.2352/ISSN.2470-1173.2018.15.COIMG-199
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1802.03542v1)
- **Published**: 2018-02-10 08:03:22+00:00
- **Updated**: 2018-02-10 08:03:22+00:00
- **Authors**: Soonam Lee, Chichen Fu, Paul Salama, Kenneth W. Dunn, Edward J. Delp
- **Comment**: IS&T International Symposium on Electronic Imaging 2018
- **Journal**: None
- **Summary**: Fluorescence microscopy has become a widely used tool for studying various biological structures of in vivo tissue or cells. However, quantitative analysis of these biological structures remains a challenge due to their complexity which is exacerbated by distortions caused by lens aberrations and light scattering. Moreover, manual quantification of such image volumes is an intractable and error-prone process, making the need for automated image analysis methods crucial. This paper describes a segmentation method for tubular structures in fluorescence microscopy images using convolutional neural networks with data augmentation and inhomogeneity correction. The segmentation results of the proposed method are visually and numerically compared with other microscopy segmentation methods. Experimental results indicate that the proposed method has better performance with correctly segmenting and identifying multiple tubular structures compared to other methods.



### 2-gram-based Phonetic Feature Generation for Convolutional Neural Network in Assessment of Trademark Similarity
- **Arxiv ID**: http://arxiv.org/abs/1802.03581v1
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1802.03581v1)
- **Published**: 2018-02-10 12:50:34+00:00
- **Updated**: 2018-02-10 12:50:34+00:00
- **Authors**: Kyung Pyo Ko, Kwang Hee Lee, Mi So Jang, Gun Hong Park
- **Comment**: 10 pages, 6 figures, 10 tables
- **Journal**: None
- **Summary**: A trademark is a mark used to identify various commodities. If same or similar trademark is registered for the same or similar commodity, the purchaser of the goods may be confused. Therefore, in the process of trademark registration examination, the examiner judges whether the trademark is the same or similar to the other applied or registered trademarks. The confusion in trademarks is based on the visual, phonetic or conceptual similarity of the marks. In this paper, we focus specifically on the phonetic similarity between trademarks. We propose a method to generate 2D phonetic feature for convolutional neural network in assessment of trademark similarity. This proposed algorithm is tested with 12,553 trademark phonetic similar pairs and 34,020 trademark phonetic non-similar pairs from 2010 to 2016. As a result, we have obtained approximately 92% judgment accuracy.



### Joint Learning for Pulmonary Nodule Segmentation, Attributes and Malignancy Prediction
- **Arxiv ID**: http://arxiv.org/abs/1802.03584v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.03584v1)
- **Published**: 2018-02-10 13:11:55+00:00
- **Updated**: 2018-02-10 13:11:55+00:00
- **Authors**: Botong Wu, Zhen Zhou, Jianwei Wang, Yizhou Wang
- **Comment**: 5 papers, accepted for publication in IEEE International Symposium on
  Biomedical Imaging (ISBI) 2018
- **Journal**: None
- **Summary**: Refer to the literature of lung nodule classification, many studies adopt Convolutional Neural Networks (CNN) to directly predict the malignancy of lung nodules with original thoracic Computed Tomography (CT) and nodule location. However, these studies cannot tell how the CNN works in terms of predicting the malignancy of the given nodule, e.g., it's hard to conclude that whether the region within the nodule or the contextual information matters according to the output of the CNN. In this paper, we propose an interpretable and multi-task learning CNN -- Joint learning for \textbf{P}ulmonary \textbf{N}odule \textbf{S}egmentation \textbf{A}ttributes and \textbf{M}alignancy \textbf{P}rediction (PN-SAMP). It is able to not only accurately predict the malignancy of lung nodules, but also provide semantic high-level attributes as well as the areas of detected nodules. Moreover, the combination of nodule segmentation, attributes and malignancy prediction is helpful to improve the performance of each single task. In addition, inspired by the fact that radiologists often change window widths and window centers to help to make decision on uncertain nodules, PN-SAMP mixes multiple WW/WC together to gain information for the raw CT input images. To verify the effectiveness of the proposed method, the evaluation is implemented on the public LIDC-IDRI dataset, which is one of the largest dataset for lung nodule malignancy prediction. Experiments indicate that the proposed PN-SAMP achieves significant improvement with respect to lung nodule classification, and promising performance on lung nodule segmentation and attribute learning, compared with the-state-of-the-art methods.



### Deep Visual Domain Adaptation: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1802.03601v4
- **DOI**: 10.1016/j.neucom.2018.05.083
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.03601v4)
- **Published**: 2018-02-10 14:35:27+00:00
- **Updated**: 2018-05-25 02:36:16+00:00
- **Authors**: Mei Wang, Weihong Deng
- **Comment**: Manuscript accepted by Neurocomputing 2018
- **Journal**: Neurocomputing, 2018, 312: 135-153
- **Summary**: Deep domain adaption has emerged as a new learning technique to address the lack of massive amounts of labeled data. Compared to conventional methods, which learn shared feature subspaces or reuse important source instances with shallow representations, deep domain adaption methods leverage deep networks to learn more transferable representations by embedding domain adaptation in the pipeline of deep learning. There have been comprehensive surveys for shallow domain adaption, but few timely reviews the emerging deep learning based methods. In this paper, we provide a comprehensive survey of deep domain adaptation methods for computer vision applications with four major contributions. First, we present a taxonomy of different deep domain adaption scenarios according to the properties of data that define how two domains are diverged. Second, we summarize deep domain adaption approaches into several categories based on training loss, and analyze and compare briefly the state-of-the-art methods under these categories. Third, we overview the computer vision applications that go beyond image classification, such as face recognition, semantic segmentation and object detection. Fourth, some potential deficiencies of current methods and several future directions are highlighted.



### Combinets: Creativity via Recombination of Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1802.03605v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1802.03605v4)
- **Published**: 2018-02-10 14:54:10+00:00
- **Updated**: 2018-09-06 21:44:51+00:00
- **Authors**: Matthew Guzdial, Mark O. Riedl
- **Comment**: 9 pages, 1 figure
- **Journal**: None
- **Summary**: One of the defining characteristics of human creativity is the ability to make conceptual leaps, creating something surprising from typical knowledge. In comparison, deep neural networks often struggle to handle cases outside of their training data, which is especially problematic for problems with limited training data. Approaches exist to transfer knowledge from problems with sufficient data to those with insufficient data, but they tend to require additional training or a domain-specific method of transfer. We present a new approach, conceptual expansion, that serves as a general representation for reusing existing trained models to derive new models without backpropagation. We evaluate our approach on few-shot variations of two tasks: image classification and image generation, and outperform standard transfer learning approaches.



### Optimize transfer learning for lung diseases in bronchoscopy using a new concept: sequential fine-tuning
- **Arxiv ID**: http://arxiv.org/abs/1802.03617v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.03617v1)
- **Published**: 2018-02-10 16:43:53+00:00
- **Updated**: 2018-02-10 16:43:53+00:00
- **Authors**: Tao Tan, Zhang Li, Haixia Liu, Ping Liu, Wenfang Tang, Hui Li, Yue Sun, Yusheng Yan, Keyu Li, Tao Xu, Shanshan Wan, Ke Lou, Jun Xu, Huiming Ying, Quchang Ouyang, Yuling Tang, Zheyu Hu, Qiang Li
- **Comment**: None
- **Journal**: None
- **Summary**: Bronchoscopy inspection as a follow-up procedure from the radiological imaging plays a key role in lung disease diagnosis and determining treatment plans for the patients. Doctors needs to make a decision whether to biopsy the patients timely when performing bronchoscopy. However, the doctors also needs to be very selective with biopsies as biopsies may cause uncontrollable bleeding of the lung tissue which is life-threaten. To help doctors to be more selective on biopsies and provide a second opinion on diagnosis, in this work, we propose a computer-aided diagnosis (CAD) system for lung diseases including cancers and tuberculosis (TB). The system is developed based on transfer learning. We propose a novel transfer learning method: sentential fine-tuning . Compared to traditional fine-tuning methods, our methods achieves the best performance. We obtained a overall accuracy of 77.0% a dataset of 81 normal cases, 76 tuberculosis cases and 277 lung cancer cases while the other traditional transfer learning methods achieve an accuracy of 73% and 68%. . The detection accuracy of our method for cancers, TB and normal cases are 87%, 54% and 91% respectively. This indicates that the CAD system has potential to improve lung disease diagnosis accuracy in bronchoscopy and it also might be used to be more selective with biopsies.



### On the Universal Approximability and Complexity Bounds of Quantized ReLU Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1802.03646v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1802.03646v4)
- **Published**: 2018-02-10 19:43:42+00:00
- **Updated**: 2019-01-12 21:54:14+00:00
- **Authors**: Yukun Ding, Jinglan Liu, Jinjun Xiong, Yiyu Shi
- **Comment**: Published in ICLR 2019
- **Journal**: None
- **Summary**: Compression is a key step to deploy large neural networks on resource-constrained platforms. As a popular compression technique, quantization constrains the number of distinct weight values and thus reducing the number of bits required to represent and store each weight. In this paper, we study the representation power of quantized neural networks. First, we prove the universal approximability of quantized ReLU networks on a wide class of functions. Then we provide upper bounds on the number of weights and the memory size for a given approximation error bound and the bit-width of weights for function-independent and function-dependent structures. Our results reveal that, to attain an approximation error bound of $\epsilon$, the number of weights needed by a quantized network is no more than $\mathcal{O}\left(\log^5(1/\epsilon)\right)$ times that of an unquantized network. This overhead is of much lower order than the lower bound of the number of weights needed for the error bound, supporting the empirical success of various quantization techniques. To the best of our knowledge, this is the first in-depth study on the complexity bounds of quantized neural networks.



