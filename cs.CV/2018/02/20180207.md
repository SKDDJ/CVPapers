# Arxiv Papers in cs.CV on 2018-02-07
### Universal Deep Neural Network Compression
- **Arxiv ID**: http://arxiv.org/abs/1802.02271v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1802.02271v2)
- **Published**: 2018-02-07 00:39:56+00:00
- **Updated**: 2019-02-21 01:33:44+00:00
- **Authors**: Yoojin Choi, Mostafa El-Khamy, Jungwon Lee
- **Comment**: NeurIPS 2018 Workshop on Compact Deep Neural Network Representation
  with Industrial Applications (CDNNRIA)
- **Journal**: None
- **Summary**: In this paper, we investigate lossy compression of deep neural networks (DNNs) by weight quantization and lossless source coding for memory-efficient deployment. Whereas the previous work addressed non-universal scalar quantization and entropy coding of DNN weights, we for the first time introduce universal DNN compression by universal vector quantization and universal source coding. In particular, we examine universal randomized lattice quantization of DNNs, which randomizes DNN weights by uniform random dithering before lattice quantization and can perform near-optimally on any source without relying on knowledge of its probability distribution. Moreover, we present a method of fine-tuning vector quantized DNNs to recover the performance loss after quantization. Our experimental results show that the proposed universal DNN compression scheme compresses the 32-layer ResNet (trained on CIFAR-10) and the AlexNet (trained on ImageNet) with compression ratios of $47.1$ and $42.5$, respectively.



### Spectral Image Visualization Using Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1802.02290v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1802.02290v1)
- **Published**: 2018-02-07 02:23:47+00:00
- **Updated**: 2018-02-07 02:23:47+00:00
- **Authors**: Siyu Chen, Danping Liao, Yuntao Qian
- **Comment**: 6 pages, 7 figures
- **Journal**: None
- **Summary**: Spectral images captured by satellites and radio-telescopes are analyzed to obtain information about geological compositions distributions, distant asters as well as undersea terrain. Spectral images usually contain tens to hundreds of continuous narrow spectral bands and are widely used in various fields. But the vast majority of those image signals are beyond the visible range, which calls for special visualization technique. The visualizations of spectral images shall convey as much information as possible from the original signal and facilitate image interpretation. However, most of the existing visualizatio methods display spectral images in false colors, which contradict with human's experience and expectation. In this paper, we present a novel visualization generative adversarial network (GAN) to display spectral images in natural colors. To achieve our goal, we propose a loss function which consists of an adversarial loss and a structure loss. The adversarial loss pushes our solution to the natural image distribution using a discriminator network that is trained to differentiate between false-color images and natural-color images. We also use a cycle loss as the structure constraint to guarantee structure consistency. Experimental results show that our method is able to generate structure-preserved and natural-looking visualizations.



### 3D Point Cloud Descriptors in Hand-crafted and Deep Learning Age: State-of-the-Art
- **Arxiv ID**: http://arxiv.org/abs/1802.02297v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.02297v2)
- **Published**: 2018-02-07 03:39:08+00:00
- **Updated**: 2020-07-27 00:42:27+00:00
- **Authors**: Xian-Feng Han, Shi-Jie Sun, Xiang-Yu Song, Guo-Qiang Xiao
- **Comment**: None
- **Journal**: None
- **Summary**: The introduction of inexpensive 3D data acquisition devices has promisingly facilitated the wide availability and popularity of 3D point cloud, which attracts more attention to the effective extraction of novel 3D point cloud descriptors for accuracy of the efficiency of 3D computer vision tasks in recent years. However, how to develop discriminative and robust feature descriptors from 3D point cloud remains a challenging task due to their intrinsic characteristics. In this paper, we give a comprehensively insightful investigation of the existing 3D point cloud descriptors. These methods can principally be divided into two categories according to the advancement of descriptors: hand-crafted based and deep learning-based apporaches, which will be further discussed from the perspective of elaborate classification, their advantages, and limitations. Finally, we present the future research direction of the extraction of 3D point cloud descriptors.



### Self-Supervised Video Hashing with Hierarchical Binary Auto-encoder
- **Arxiv ID**: http://arxiv.org/abs/1802.02305v1
- **DOI**: 10.1109/TIP.2018.2814344
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.02305v1)
- **Published**: 2018-02-07 04:31:19+00:00
- **Updated**: 2018-02-07 04:31:19+00:00
- **Authors**: Jingkuan Song, Hanwang Zhang, Xiangpeng Li, Lianli Gao, Meng Wang, Richang Hong
- **Comment**: None
- **Journal**: None
- **Summary**: Existing video hash functions are built on three isolated stages: frame pooling, relaxed learning, and binarization, which have not adequately explored the temporal order of video frames in a joint binary optimization model, resulting in severe information loss. In this paper, we propose a novel unsupervised video hashing framework dubbed Self-Supervised Video Hashing (SSVH), that is able to capture the temporal nature of videos in an end-to-end learning-to-hash fashion. We specifically address two central problems: 1) how to design an encoder-decoder architecture to generate binary codes for videos; and 2) how to equip the binary codes with the ability of accurate video retrieval. We design a hierarchical binary autoencoder to model the temporal dependencies in videos with multiple granularities, and embed the videos into binary codes with less computations than the stacked architecture. Then, we encourage the binary codes to simultaneously reconstruct the visual content and neighborhood structure of the videos. Experiments on two real-world datasets (FCVID and YFCC) show that our SSVH method can significantly outperform the state-of-the-art methods and achieve the currently best performance on the task of unsupervised video retrieval.



### Machine Learning-Based Prototyping of Graphical User Interfaces for Mobile Apps
- **Arxiv ID**: http://arxiv.org/abs/1802.02312v2
- **DOI**: None
- **Categories**: **cs.SE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1802.02312v2)
- **Published**: 2018-02-07 05:32:59+00:00
- **Updated**: 2018-06-05 03:12:06+00:00
- **Authors**: Kevin Moran, Carlos Bernal-CÃ¡rdenas, Michael Curcio, Richard Bonett, Denys Poshyvanyk
- **Comment**: Accepted to IEEE Transactions on Software Engineering
- **Journal**: None
- **Summary**: It is common practice for developers of user-facing software to transform a mock-up of a graphical user interface (GUI) into code. This process takes place both at an application's inception and in an evolutionary context as GUI changes keep pace with evolving features. Unfortunately, this practice is challenging and time-consuming. In this paper, we present an approach that automates this process by enabling accurate prototyping of GUIs via three tasks: detection, classification, and assembly. First, logical components of a GUI are detected from a mock-up artifact using either computer vision techniques or mock-up metadata. Then, software repository mining, automated dynamic analysis, and deep convolutional neural networks are utilized to accurately classify GUI-components into domain-specific types (e.g., toggle-button). Finally, a data-driven, K-nearest-neighbors algorithm generates a suitable hierarchical GUI structure from which a prototype application can be automatically assembled. We implemented this approach for Android in a system called ReDraw. Our evaluation illustrates that ReDraw achieves an average GUI-component classification accuracy of 91% and assembles prototype applications that closely mirror target mock-ups in terms of visual affinity while exhibiting reasonable code structure. Interviews with industrial practitioners illustrate ReDraw's potential to improve real development workflows.



### Bitewing Radiography Semantic Segmentation Base on Conditional Generative Adversarial Nets
- **Arxiv ID**: http://arxiv.org/abs/1802.02571v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.02571v1)
- **Published**: 2018-02-07 05:44:48+00:00
- **Updated**: 2018-02-07 05:44:48+00:00
- **Authors**: Jiang Yun, Tan Ning, Zhang Hai, Peng Tingting
- **Comment**: 12pages, in Chinese
- **Journal**: None
- **Summary**: Currently, Segmentation of bitewing radiograpy images is a very challenging task. The focus of the study is to segment it into caries, enamel, dentin, pulp, crowns, restoration and root canal treatments. The main method of semantic segmentation of bitewing radiograpy images at this stage is the U-shaped deep convolution neural network, but its accuracy is low. in order to improve the accuracy of semantic segmentation of bitewing radiograpy images, this paper proposes the use of Conditional Generative Adversarial network (cGAN) combined with U-shaped network structure (U-Net) approach to semantic segmentation of bitewing radiograpy images. The experimental results show that the accuracy of cGAN combined with U-Net is 69.7%, which is 13.3% higher than the accuracy of u-shaped deep convolution neural network of 56.4%.



### A Novel Co-design Peta-scale Heterogeneous Cluster for Deep Learning Training
- **Arxiv ID**: http://arxiv.org/abs/1802.02326v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.02326v3)
- **Published**: 2018-02-07 07:20:42+00:00
- **Updated**: 2018-05-18 23:01:23+00:00
- **Authors**: Xin Chen, Hua Zhou, Yuxiang Gao, Yu Zhu
- **Comment**: 23 pages, 4 figures, 1 table
- **Journal**: None
- **Summary**: Large scale deep Convolution Neural Networks (CNNs) increasingly demands the computing power. It is key for researchers to own a great powerful computing platform to leverage deep learning (DL) advancing.On the other hand, as the commonly-used accelerator, the commodity GPUs cards of new generations are more and more expensive. Consequently, it is of importance to design an affordable distributed heterogeneous system that provides powerful computational capacity and develop a well-suited software that efficiently utilizes its computational capacity. In this paper, we present our co-design distributed system including a peta-scale GPU cluster, called "Manoa". Based on properties and topology of Manoa, we first propose job server framework and implement it, named "MiMatrix". The central node of MiMatrix, referred to as the job server, undertakes all of controlling, scheduling and monitoring, and I/O tasks without weight data transfer for AllReduce processing in each iteration. Therefore, MiMatrix intrinsically solves the bandwidth bottleneck of central node in parameter server framework that is widely used in distributed DL tasks. Meanwhile, we also propose a new AllReduce algorithm, GPUDirect RDMA-Aware AllReduce~(GDRAA), in which both computation and handshake message are O(1) and the number of synchronization is two in each iteration that is a theoretical minimum number. Owe to the dedicated co-design distributed system, MiMatrix efficiently makes use of the Manoa's computational capacity and bandwidth. We benchmark Manoa Resnet50 and Resenet101 on Imagenet-1K dataset. Some of results have demonstrated state-of-the-art.



### Outlier Detection for Robust Multi-dimensional Scaling
- **Arxiv ID**: http://arxiv.org/abs/1802.02341v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.02341v1)
- **Published**: 2018-02-07 08:16:56+00:00
- **Updated**: 2018-02-07 08:16:56+00:00
- **Authors**: Leonid Blouvshtein, Daniel Cohen-Or
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-dimensional scaling (MDS) plays a central role in data-exploration, dimensionality reduction and visualization. State-of-the-art MDS algorithms are not robust to outliers, yielding significant errors in the embedding even when only a handful of outliers are present. In this paper, we introduce a technique to detect and filter outliers based on geometric reasoning. We test the validity of triangles formed by three points, and mark a triangle as broken if its triangle inequality does not hold. The premise of our work is that unlike inliers, outlier distances tend to break many triangles. Our method is tested and its performance is evaluated on various datasets and distributions of outliers. We demonstrate that for a reasonable amount of outliers, e.g., under $20\%$, our method is effective, and leads to a high embedding quality.



### SlideRunner - A Tool for Massive Cell Annotations in Whole Slide Images
- **Arxiv ID**: http://arxiv.org/abs/1802.02347v1
- **DOI**: 10.1007/978-3-662-56537-7_81
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.02347v1)
- **Published**: 2018-02-07 08:29:17+00:00
- **Updated**: 2018-02-07 08:29:17+00:00
- **Authors**: Marc Aubreville, Christof Bertram, Robert Klopfleisch, Andreas Maier
- **Comment**: 6 pages, submitted to Bildverarbeitung in der Medizin 2018
- **Journal**: Bildverarbeitung f\"ur die Medizin 2018
- **Summary**: Large-scale image data such as digital whole-slide histology images pose a challenging task at annotation software solutions. Today, a number of good solutions with varying scopes exist. For cell annotation, however, we find that many do not match the prerequisites for fast annotations. Especially in the field of mitosis detection, it is assumed that detection accuracy could significantly benefit from larger annotation databases that are currently however very troublesome to produce. Further, multiple independent (blind) expert labels are a big asset for such databases, yet there is currently no tool for this kind of annotation available.   To ease this tedious process of expert annotation and grading, we introduce SlideRunner, an open source annotation and visualization tool for digital histopathology, developed in close cooperation with two pathologists. SlideRunner is capable of setting annotations like object centers (for e.g. cells) as well as object boundaries (e.g. for tumor outlines). It provides single-click annotations as well as a blind mode for multi-annotations, where the expert is directly shown the microscopy image containing the cells that he has not yet rated.



### ShakeDrop Regularization for Deep Residual Learning
- **Arxiv ID**: http://arxiv.org/abs/1802.02375v3
- **DOI**: 10.1109/ACCESS.2019.2960566
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.02375v3)
- **Published**: 2018-02-07 10:23:54+00:00
- **Updated**: 2020-01-06 07:14:51+00:00
- **Authors**: Yoshihiro Yamada, Masakazu Iwamura, Takuya Akiba, Koichi Kise
- **Comment**: None
- **Journal**: IEEE Access, 7, 1, pp.186126-186136 (2019)
- **Summary**: Overfitting is a crucial problem in deep neural networks, even in the latest network architectures. In this paper, to relieve the overfitting effect of ResNet and its improvements (i.e., Wide ResNet, PyramidNet, and ResNeXt), we propose a new regularization method called ShakeDrop regularization. ShakeDrop is inspired by Shake-Shake, which is an effective regularization method, but can be applied to ResNeXt only. ShakeDrop is more effective than Shake-Shake and can be applied not only to ResNeXt but also ResNet, Wide ResNet, and PyramidNet. An important key is to achieve stability of training. Because effective regularization often causes unstable training, we introduce a training stabilizer, which is an unusual use of an existing regularizer. Through experiments under various conditions, we demonstrate the conditions under which ShakeDrop works well.



### From Selective Deep Convolutional Features to Compact Binary Representations for Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1802.02899v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.02899v3)
- **Published**: 2018-02-07 10:45:14+00:00
- **Updated**: 2019-03-05 23:24:22+00:00
- **Authors**: Thanh-Toan Do, Tuan Hoang, Dang-Khoa Le Tan, Huu Le, Tam V. Nguyen, Ngai-Man Cheung
- **Comment**: Accepted to Transactions on Multimedia Computing Communications and
  Applications (TOMM)
- **Journal**: None
- **Summary**: In the large-scale image retrieval task, the two most important requirements are the discriminability of image representations and the efficiency in computation and storage of representations. Regarding the former requirement, Convolutional Neural Network (CNN) is proven to be a very powerful tool to extract highly discriminative local descriptors for effective image search. Additionally, in order to further improve the discriminative power of the descriptors, recent works adopt fine-tuned strategies. In this paper, taking a different approach, we propose a novel, computationally efficient, and competitive framework. Specifically, we firstly propose various strategies to compute masks, namely SIFT-mask, SUM-mask, and MAX-mask, to select a representative subset of local convolutional features and eliminate redundant features. Our in-depth analyses demonstrate that proposed masking schemes are effective to address the burstiness drawback and improve retrieval accuracy. Secondly, we propose to employ recent embedding and aggregating methods which can significantly boost the feature discriminability. Regarding the computation and storage efficiency, we include a hashing module to produce very compact binary image representations. Extensive experiments on six image retrieval benchmarks demonstrate that our proposed framework achieves the state-of-the-art retrieval performances.



### Super-resolution of spatiotemporal event-stream image captured by the asynchronous temporal contrast vision sensor
- **Arxiv ID**: http://arxiv.org/abs/1802.02398v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.02398v2)
- **Published**: 2018-02-07 12:14:56+00:00
- **Updated**: 2018-03-16 10:39:03+00:00
- **Authors**: Hongmin Li, Guoqi Li, Hanchao Liu, Luping Shi
- **Comment**: None
- **Journal**: None
- **Summary**: Super-resolution (SR) is a useful technology to generate a high-resolution (HR) visual output from the low-resolution (LR) visual inputs overcoming the physical limitations of the cameras. However, SR has not been applied to enhance the resolution of spatiotemporal event-stream images captured by the frame-free dynamic vision sensors (DVSs). SR of event-stream image is fundamentally different from existing frame-based schemes since basically each pixel value of DVS images is an event sequence. In this work, a two-stage scheme is proposed to solve the SR problem of the spatiotemporal event-stream image. We use a nonhomogeneous Poisson point process to model the event sequence, and sample the events of each pixel by simulating a nonhomogeneous Poisson process according to the specified event number and rate function. Firstly, the event number of each pixel of the HR DVS image is determined with a sparse signal representation based method to obtain the HR event-count map from that of the LR DVS recording. The rate function over time line of the point process of each HR pixel is computed using a spatiotemporal filter on the corresponding LR neighbor pixels. Secondly, the event sequence of each new pixel is generated with a thinning based event sampling algorithm. Two metrics are proposed to assess the event-stream SR results. The proposed method is demonstrated through obtaining HR event-stream images from a series of DVS recordings with the proposed method. Results show that the upscaled HR event streams has perceptually higher spatial texture detail than the LR DVS images. Besides, the temporal properties of the upscaled HR event streams match that of the original input very well. This work enables many potential applications of event-based vision.



### Revisiting the Inverted Indices for Billion-Scale Approximate Nearest Neighbors
- **Arxiv ID**: http://arxiv.org/abs/1802.02422v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.02422v2)
- **Published**: 2018-02-07 14:01:04+00:00
- **Updated**: 2018-07-23 11:52:39+00:00
- **Authors**: Dmitry Baranchuk, Artem Babenko, Yury Malkov
- **Comment**: Paper accepted to ECCV 18
- **Journal**: None
- **Summary**: This work addresses the problem of billion-scale nearest neighbor search. The state-of-the-art retrieval systems for billion-scale databases are currently based on the inverted multi-index, the recently proposed generalization of the inverted index structure. The multi-index provides a very fine-grained partition of the feature space that allows extracting concise and accurate short-lists of candidates for the search queries. In this paper, we argue that the potential of the simple inverted index was not fully exploited in previous works and advocate its usage both for the highly-entangled deep descriptors and relatively disentangled SIFT descriptors. We introduce a new retrieval system that is based on the inverted index and outperforms the multi-index by a large margin for the same memory consumption and construction complexity. For example, our system achieves the state-of-the-art recall rates several times faster on the dataset of one billion deep descriptors compared to the efficient implementation of the inverted multi-index from the FAISS library.



### Stochastic Deconvolutional Neural Network Ensemble Training on Generative Pseudo-Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1802.02436v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1802.02436v1)
- **Published**: 2018-02-07 14:36:15+00:00
- **Updated**: 2018-02-07 14:36:15+00:00
- **Authors**: Alexey Chaplygin, Joshua Chacksfield
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: The training of Generative Adversarial Networks is a difficult task mainly due to the nature of the networks. One such issue is when the generator and discriminator start oscillating, rather than converging to a fixed point. Another case can be when one agent becomes more adept than the other which results in the decrease of the other agent's ability to learn, reducing the learning capacity of the system as a whole. Additionally, there exists the problem of Mode Collapse which involves the generators output collapsing to a single sample or a small set of similar samples. To train GANs a careful selection of the architecture that is used along with a variety of other methods to improve training. Even when applying these methods there is low stability of training in relation to the parameters that are chosen. Stochastic ensembling is suggested as a method for improving the stability while training GANs.



### Pixel-Level Alignment of Facial Images for High Accuracy Recognition Using Ensemble of Patches
- **Arxiv ID**: http://arxiv.org/abs/1802.02438v1
- **DOI**: 10.1364/JOSAA.35.001149
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.02438v1)
- **Published**: 2018-02-07 14:38:37+00:00
- **Updated**: 2018-02-07 14:38:37+00:00
- **Authors**: Hoda Mohammadzade, Amirhossein Sayyafan, Benyamin Ghojogh
- **Comment**: 11 pages, 16 figures, 1 table, key-words: face recognition, pixel
  alignment, geometrical transformation, pose and expression variation,
  ensemble of patches, fusion of texture and geometry
- **Journal**: None
- **Summary**: The variation of pose, illumination and expression makes face recognition still a challenging problem. As a pre-processing in holistic approaches, faces are usually aligned by eyes. The proposed method tries to perform a pixel alignment rather than eye-alignment by mapping the geometry of faces to a reference face while keeping their own textures. The proposed geometry alignment not only creates a meaningful correspondence among every pixel of all faces, but also removes expression and pose variations effectively. The geometry alignment is performed pixel-wise, i.e., every pixel of the face is corresponded to a pixel of the reference face. In the proposed method, the information of intensity and geometry of faces are separated properly, trained by separate classifiers, and finally fused together to recognize human faces. Experimental results show a great improvement using the proposed method in comparison to eye-aligned recognition. For instance, at the false acceptance rate of 0.001, the recognition rates are respectively improved by 24% and 33% in Yale and AT&T datasets. In LFW dataset, which is a challenging big dataset, improvement is 20% at FAR of 0.1.



### SCH-GAN: Semi-supervised Cross-modal Hashing by Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/1802.02488v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.02488v1)
- **Published**: 2018-02-07 15:45:12+00:00
- **Updated**: 2018-02-07 15:45:12+00:00
- **Authors**: Jian Zhang, Yuxin Peng, Mingkuan Yuan
- **Comment**: 12 pages, submitted to IEEE Transactions on Cybernetics
- **Journal**: None
- **Summary**: Cross-modal hashing aims to map heterogeneous multimedia data into a common Hamming space, which can realize fast and flexible retrieval across different modalities. Supervised cross-modal hashing methods have achieved considerable progress by incorporating semantic side information. However, they mainly have two limitations: (1) Heavily rely on large-scale labeled cross-modal training data which are labor intensive and hard to obtain. (2) Ignore the rich information contained in the large amount of unlabeled data across different modalities, especially the margin examples that are easily to be incorrectly retrieved, which can help to model the correlations. To address these problems, in this paper we propose a novel Semi-supervised Cross-Modal Hashing approach by Generative Adversarial Network (SCH-GAN). We aim to take advantage of GAN's ability for modeling data distributions to promote cross-modal hashing learning in an adversarial way. The main contributions can be summarized as follows: (1) We propose a novel generative adversarial network for cross-modal hashing. In our proposed SCH-GAN, the generative model tries to select margin examples of one modality from unlabeled data when giving a query of another modality. While the discriminative model tries to distinguish the selected examples and true positive examples of the query. These two models play a minimax game so that the generative model can promote the hashing performance of discriminative model. (2) We propose a reinforcement learning based algorithm to drive the training of proposed SCH-GAN. The generative model takes the correlation score predicted by discriminative model as a reward, and tries to select the examples close to the margin to promote discriminative model by maximizing the margin between positive and negative data. Experiments on 3 widely-used datasets verify the effectiveness of our proposed approach.



### Deep Reinforcement Learning for Image Hashing
- **Arxiv ID**: http://arxiv.org/abs/1802.02904v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.02904v2)
- **Published**: 2018-02-07 15:50:48+00:00
- **Updated**: 2019-02-24 12:53:12+00:00
- **Authors**: Yuxin Peng, Jian Zhang, Zhaoda Ye
- **Comment**: 12 pages, submitted to IEEE Transactions on Multimedia
- **Journal**: None
- **Summary**: Deep hashing methods have received much attention recently, which achieve promising results by taking advantage of the strong representation power of deep networks. However, most existing deep hashing methods learn a whole set of hashing functions independently, while ignore the correlations between different hashing functions that can promote the retrieval accuracy greatly. Inspired by the sequential decision ability of deep reinforcement learning, we propose a new Deep Reinforcement Learning approach for Image Hashing (DRLIH). Our proposed DRLIH approach models the hashing learning problem as a sequential decision process, which learns each hashing function by correcting the errors imposed by previous ones and promotes retrieval accuracy. To the best of our knowledge, this is the first work to address hashing problem from deep reinforcement learning perspective. The main contributions of our proposed DRLIH approach can be summarized as follows: (1) We propose a deep reinforcement learning hashing network. In the proposed network, we utilize recurrent neural network (RNN) as agents to model the hashing functions, which take actions of projecting images into binary codes sequentially, so that the current hashing function learning can take previous hashing functions' error into account. (2) We propose a sequential learning strategy based on proposed DRLIH. We define the state as a tuple of internal features of RNN's hidden layers and image features, which can reflect history decisions made by the agents. We also propose an action group method to enhance the correlation of hash functions in the same group. Experiments on three widely-used datasets demonstrate the effectiveness of our proposed DRLIH approach.



### Joint Attention in Driver-Pedestrian Interaction: from Theory to Practice
- **Arxiv ID**: http://arxiv.org/abs/1802.02522v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1802.02522v2)
- **Published**: 2018-02-07 17:03:11+00:00
- **Updated**: 2018-03-27 13:39:37+00:00
- **Authors**: Amir Rasouli, John K. Tsotsos
- **Comment**: None
- **Journal**: None
- **Summary**: Today, one of the major challenges that autonomous vehicles are facing is the ability to drive in urban environments. Such a task requires communication between autonomous vehicles and other road users in order to resolve various traffic ambiguities. The interaction between road users is a form of negotiation in which the parties involved have to share their attention regarding a common objective or a goal (e.g. crossing an intersection), and coordinate their actions in order to accomplish it. In this literature review we aim to address the interaction problem between pedestrians and drivers (or vehicles) from joint attention point of view. More specifically, we will discuss the theoretical background behind joint attention, its application to traffic interaction and practical approaches to implementing joint attention for autonomous vehicles.



### Fair comparison of skin detection approaches on publicly available datasets
- **Arxiv ID**: http://arxiv.org/abs/1802.02531v6
- **DOI**: 10.1016/j.eswa.2020.113677
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.02531v6)
- **Published**: 2018-02-07 17:16:46+00:00
- **Updated**: 2020-07-10 21:31:26+00:00
- **Authors**: Alessandra Lumini, Loris Nanni
- **Comment**: None
- **Journal**: Expert Systems with Applications Volume 160, 1 December 2020,
  113677
- **Summary**: Skin detection is the process of discriminating skin and non-skin regions in a digital image and it is widely used in several applications ranging from hand gesture analysis to track body parts and face detection. Skin detection is a challenging problem which has drawn extensive attention from the research community, nevertheless a fair comparison among approaches is very difficult due to the lack of a common benchmark and a unified testing protocol. In this work, we investigate the most recent researches in this field and we propose a fair comparison among approaches using several different datasets. The major contributions of this work are an exhaustive literature review of skin color detection approaches, a framework to evaluate and combine different skin detector approaches, whose source code is made freely available for future research, and an extensive experimental comparison among several recent methods which have also been used to define an ensemble that works well in many different problems. Experiments are carried out in 10 different datasets including more than 10000 labelled images: experimental results confirm that the best method here proposed obtains a very good performance with respect to other stand-alone approaches, without requiring ad hoc parameter tuning. A MATLAB version of the framework for testing and of the methods proposed in this paper will be freely available from https://github.com/LorisNanni



### A Spatial Mapping Algorithm with Applications in Deep Learning-Based Structure Classification
- **Arxiv ID**: http://arxiv.org/abs/1802.02532v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1802.02532v2)
- **Published**: 2018-02-07 17:18:33+00:00
- **Updated**: 2018-02-22 20:38:14+00:00
- **Authors**: Thomas Corcoran, Rafael Zamora-Resendiz, Xinlian Liu, Silvia Crivelli
- **Comment**: 17 Pages, 4 figures, 7 tables. The first two authors contributed
  equally to this work
- **Journal**: None
- **Summary**: Convolutional Neural Network (CNN)-based machine learning systems have made breakthroughs in feature extraction and image recognition tasks in two dimensions (2D). Although there is significant ongoing work to apply CNN technology to domains involving complex 3D data, the success of such efforts has been constrained, in part, by limitations in data representation techniques. Most current approaches rely upon low-resolution 3D models, strategic limitation of scope in the 3D space, or the application of lossy projection techniques to allow for the use of 2D CNNs. To address this issue, we present a mapping algorithm that converts 3D structures to 2D and 1D data grids by mapping a traversal of a 3D space-filling curve to the traversal of corresponding 2D and 1D curves. We explore the performance of 2D and 1D CNNs trained on data encoded with our method versus comparable volumetric CNNs operating upon raw 3D data from a popular benchmarking dataset. Our experiments demonstrate that both 2D and 1D representations of 3D data generated via our method preserve a significant proportion of the 3D data's features in forms learnable by CNNs. Furthermore, we demonstrate that our method of encoding 3D data into lower-dimensional representations allows for decreased CNN training time cost, increased original 3D model rendering resolutions, and supports increased numbers of data channels when compared to purely volumetric approaches. This demonstration is accomplished in the context of a structural biology classification task wherein we train 3D, 2D, and 1D CNNs on examples of two homologous branches within the Ras protein family. The essential contribution of this paper is the introduction of a dimensionality-reduction method that may ease the application of powerful deep learning tools to domains characterized by complex structural data.



### FixaTons: A collection of Human Fixations Datasets and Metrics for Scanpath Similarity
- **Arxiv ID**: http://arxiv.org/abs/1802.02534v3
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1802.02534v3)
- **Published**: 2018-02-07 17:20:31+00:00
- **Updated**: 2018-10-03 15:20:59+00:00
- **Authors**: Dario Zanca, Valeria Serchi, Pietro Piu, Francesca Rosini, Alessandra Rufa
- **Comment**: None
- **Journal**: None
- **Summary**: In the last three decades, human visual attention has been a topic of great interest in various disciplines. In computer vision, many models have been proposed to predict the distribution of human fixations on a visual stimulus. Recently, thanks to the creation of large collections of data, machine learning algorithms have obtained state-of-the-art performance on the task of saliency map estimation. On the other hand, computational models of scanpath are much less studied. Works are often only descriptive or task specific. This is due to the fact that the scanpath is harder to model because it must include the description of a dynamic. General purpose computational models are present in the literature, but are then evaluated in tasks of saliency prediction, losing therefore information about the dynamics and the behaviour. In addition, two technical reasons have limited the research. The first reason is the lack of robust and uniformly used set of metrics to compare the similarity between scanpath. The second reason is the lack of sufficiently large and varied scanpath datasets. In this report we want to help in both directions. We present FixaTons, a large collection of datasets human scanpaths (temporally ordered sequences of fixations) and saliency maps. It comes along with a software library for easy data usage, statistics calculation and implementation of metrics for scanpath and saliency prediction evaluation.



### VISER: Visual Self-Regularization
- **Arxiv ID**: http://arxiv.org/abs/1802.02568v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1802.02568v1)
- **Published**: 2018-02-07 18:55:01+00:00
- **Updated**: 2018-02-07 18:55:01+00:00
- **Authors**: Hamid Izadinia, Pierre Garrigues
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we propose the use of large set of unlabeled images as a source of regularization data for learning robust visual representation. Given a visual model trained by a labeled dataset in a supervised fashion, we augment our training samples by incorporating large number of unlabeled data and train a semi-supervised model. We demonstrate that our proposed learning approach leverages an abundance of unlabeled images and boosts the visual recognition performance which alleviates the need to rely on large labeled datasets for learning robust representation. To increment the number of image instances needed to learn robust visual models in our approach, each labeled image propagates its label to its nearest unlabeled image instances. These retrieved unlabeled images serve as local perturbations of each labeled image to perform Visual Self-Regularization (VISER). To retrieve such visual self regularizers, we compute the cosine similarity in a semantic space defined by the penultimate layer in a fully convolutional neural network. We use the publicly available Yahoo Flickr Creative Commons 100M dataset as the source of our unlabeled image set and propose a distributed approximate nearest neighbor algorithm to make retrieval practical at that scale. Using the labeled instances and their regularizer samples we show that we significantly improve object categorization and localization performance on the MS COCO and Visual Genome datasets where objects appear in context.



### Unsupervised Typography Transfer
- **Arxiv ID**: http://arxiv.org/abs/1802.02595v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.02595v1)
- **Published**: 2018-02-07 19:03:42+00:00
- **Updated**: 2018-02-07 19:03:42+00:00
- **Authors**: Hanfei Sun, Yiming Luo, Ziang Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Traditional methods in Chinese typography synthesis view characters as an assembly of radicals and strokes, but they rely on manual definition of the key points, which is still time-costing. Some recent work on computer vision proposes a brand new approach: to treat every Chinese character as an independent and inseparable image, so the pre-processing and post-processing of each character can be avoided. Then with a combination of a transfer network and a discriminating network, one typography can be well transferred to another. Despite the quite satisfying performance of the model, the training process requires to be supervised, which means in the training data each character in the source domain and the target domain needs to be perfectly paired. Sometimes the pairing is time-costing, and sometimes there is no perfect pairing, such as the pairing between traditional Chinese and simplified Chinese characters. In this paper, we proposed an unsupervised typography transfer method which doesn't need pairing.



### Generating Triples with Adversarial Networks for Scene Graph Construction
- **Arxiv ID**: http://arxiv.org/abs/1802.02598v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.02598v1)
- **Published**: 2018-02-07 19:12:31+00:00
- **Updated**: 2018-02-07 19:12:31+00:00
- **Authors**: Matthew Klawonn, Eric Heim
- **Comment**: Accepted to AAAI 2018
- **Journal**: None
- **Summary**: Driven by successes in deep learning, computer vision research has begun to move beyond object detection and image classification to more sophisticated tasks like image captioning or visual question answering. Motivating such endeavors is the desire for models to capture not only objects present in an image, but more fine-grained aspects of a scene such as relationships between objects and their attributes. Scene graphs provide a formal construct for capturing these aspects of an image. Despite this, there have been only a few recent efforts to generate scene graphs from imagery. Previous works limit themselves to settings where bounding box information is available at train time and do not attempt to generate scene graphs with attributes. In this paper we propose a method, based on recent advancements in Generative Adversarial Networks, to overcome these deficiencies. We take the approach of first generating small subgraphs, each describing a single statement about a scene from a specific region of the input image chosen using an attention mechanism. By doing so, our method is able to produce portions of the scene graphs with attribute information without the need for bounding box labels. Then, the complete scene graph is constructed from these subgraphs. We show that our model improves upon prior work in scene graph generation on state-of-the-art data sets and accepted metrics. Further, we demonstrate that our model is capable of handling a larger vocabulary size than prior work has attempted.



### An Unsupervised Learning Model for Deformable Medical Image Registration
- **Arxiv ID**: http://arxiv.org/abs/1802.02604v3
- **DOI**: 10.1109/CVPR.2018.00964
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.02604v3)
- **Published**: 2018-02-07 19:23:57+00:00
- **Updated**: 2018-04-20 21:31:35+00:00
- **Authors**: Guha Balakrishnan, Amy Zhao, Mert R. Sabuncu, John Guttag, Adrian V. Dalca
- **Comment**: 9 pages, in CVPR 2018
- **Journal**: None
- **Summary**: We present a fast learning-based algorithm for deformable, pairwise 3D medical image registration. Current registration methods optimize an objective function independently for each pair of images, which can be time-consuming for large data. We define registration as a parametric function, and optimize its parameters given a set of images from a collection of interest. Given a new pair of scans, we can quickly compute a registration field by directly evaluating the function using the learned parameters. We model this function using a convolutional neural network (CNN), and use a spatial transform layer to reconstruct one image from another while imposing smoothness constraints on the registration field. The proposed method does not require supervised information such as ground truth registration fields or anatomical landmarks. We demonstrate registration accuracy comparable to state-of-the-art 3D image registration, while operating orders of magnitude faster in practice. Our method promises to significantly speed up medical image analysis and processing pipelines, while facilitating novel directions in learning-based registration and its applications. Our code is available at https://github.com/balakg/voxelmorph .



### Deep Versus Wide Convolutional Neural Networks for Object Recognition on Neuromorphic System
- **Arxiv ID**: http://arxiv.org/abs/1802.02608v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.02608v1)
- **Published**: 2018-02-07 19:32:40+00:00
- **Updated**: 2018-02-07 19:32:40+00:00
- **Authors**: Md Zahangir Alom, Theodore Josue, Md Nayim Rahman, Will Mitchell, Chris Yakopcic, Tarek M. Taha
- **Comment**: 8 pages, 14 figures. Submitted to International Joint Conference on
  Neural Networks (IJCNN) 2018
- **Journal**: None
- **Summary**: In the last decade, special purpose computing systems, such as Neuromorphic computing, have become very popular in the field of computer vision and machine learning for classification tasks. In 2015, IBM's released the TrueNorth Neuromorphic system, kick-starting a new era of Neuromorphic computing. Alternatively, Deep Learning approaches such as Deep Convolutional Neural Networks (DCNN) show almost human-level accuracies for detection and classification tasks. IBM's 2016 release of a deep learning framework for DCNNs, called Energy Efficient Deep Neuromorphic Networks (Eedn). Eedn shows promise for delivering high accuracies across a number of different benchmarks, while consuming very low power, using IBM's TrueNorth chip. However, there are many things that remained undiscovered using the Eedn framework for classification tasks on a Neuromorphic system. In this paper, we have empirically evaluated the performance of different DCNN architectures implemented within the Eedn framework. The goal of this work was discover the most efficient way to implement DCNN models for object classification tasks using the TrueNorth system. We performed our experiments using benchmark data sets such as MNIST, COIL 20, and COIL 100. The experimental results show very promising classification accuracies with very low power consumption on IBM's NS1e Neurosynaptic system. The results show that for datasets with large numbers of classes, wider networks perform better when compared to deep networks comprised of nearly the same core complexity on IBM's TrueNorth system.



### Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1802.02611v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.02611v3)
- **Published**: 2018-02-07 19:37:11+00:00
- **Updated**: 2018-08-22 20:41:10+00:00
- **Authors**: Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, Hartwig Adam
- **Comment**: ECCV 2018 camera ready
- **Journal**: None
- **Summary**: Spatial pyramid pooling module or encode-decoder structure are used in deep neural networks for semantic segmentation task. The former networks are able to encode multi-scale contextual information by probing the incoming features with filters or pooling operations at multiple rates and multiple effective fields-of-view, while the latter networks can capture sharper object boundaries by gradually recovering the spatial information. In this work, we propose to combine the advantages from both methods. Specifically, our proposed model, DeepLabv3+, extends DeepLabv3 by adding a simple yet effective decoder module to refine the segmentation results especially along object boundaries. We further explore the Xception model and apply the depthwise separable convolution to both Atrous Spatial Pyramid Pooling and decoder modules, resulting in a faster and stronger encoder-decoder network. We demonstrate the effectiveness of the proposed model on PASCAL VOC 2012 and Cityscapes datasets, achieving the test set performance of 89.0\% and 82.1\% without any post-processing. Our paper is accompanied with a publicly available reference implementation of the proposed models in Tensorflow at \url{https://github.com/tensorflow/models/tree/master/research/deeplab}.



### Effective Quantization Approaches for Recurrent Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1802.02615v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.02615v1)
- **Published**: 2018-02-07 19:43:01+00:00
- **Updated**: 2018-02-07 19:43:01+00:00
- **Authors**: Md Zahangir Alom, Adam T Moody, Naoya Maruyama, Brian C Van Essen, Tarek M. Taha
- **Comment**: 8 pages, 23 figures,Submitted to International Joint Conference on
  Neural Networks (IJCNN) 2018
- **Journal**: None
- **Summary**: Deep learning, and in particular Recurrent Neural Networks (RNN) have shown superior accuracy in a large variety of tasks including machine translation, language understanding, and movie frame generation. However, these deep learning approaches are very expensive in terms of computation. In most cases, Graphic Processing Units (GPUs) are in used for large scale implementations. Meanwhile, energy efficient RNN approaches are proposed for deploying solutions on special purpose hardware including Field Programming Gate Arrays (FPGAs) and mobile platforms. In this paper, we propose an effective quantization approach for Recurrent Neural Networks (RNN) techniques including Long Short Term Memory (LSTM), Gated Recurrent Units (GRU), and Convolutional Long Short Term Memory (ConvLSTM). We have implemented different quantization methods including Binary Connect {-1, 1}, Ternary Connect {-1, 0, 1}, and Quaternary Connect {-1, -0.5, 0.5, 1}. These proposed approaches are evaluated on different datasets for sentiment analysis on IMDB and video frame predictions on the moving MNIST dataset. The experimental results are compared against the full precision versions of the LSTM, GRU, and ConvLSTM. They show promising results for both sentiment analysis and video frame prediction.



### Going Deeper in Spiking Neural Networks: VGG and Residual Architectures
- **Arxiv ID**: http://arxiv.org/abs/1802.02627v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.02627v4)
- **Published**: 2018-02-07 20:58:10+00:00
- **Updated**: 2019-02-19 18:31:03+00:00
- **Authors**: Abhronil Sengupta, Yuting Ye, Robert Wang, Chiao Liu, Kaushik Roy
- **Comment**: None
- **Journal**: None
- **Summary**: Over the past few years, Spiking Neural Networks (SNNs) have become popular as a possible pathway to enable low-power event-driven neuromorphic hardware. However, their application in machine learning have largely been limited to very shallow neural network architectures for simple problems. In this paper, we propose a novel algorithmic technique for generating an SNN with a deep architecture, and demonstrate its effectiveness on complex visual recognition problems such as CIFAR-10 and ImageNet. Our technique applies to both VGG and Residual network architectures, with significantly better accuracy than the state-of-the-art. Finally, we present analysis of the sparse event-driven computations to demonstrate reduced hardware overhead when operating in the spiking domain.



### Spatially adaptive image compression using a tiled deep network
- **Arxiv ID**: http://arxiv.org/abs/1802.02629v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.02629v1)
- **Published**: 2018-02-07 20:59:39+00:00
- **Updated**: 2018-02-07 20:59:39+00:00
- **Authors**: David Minnen, George Toderici, Michele Covell, Troy Chinen, Nick Johnston, Joel Shor, Sung Jin Hwang, Damien Vincent, Saurabh Singh
- **Comment**: None
- **Journal**: International Conference on Image Processing 2017
- **Summary**: Deep neural networks represent a powerful class of function approximators that can learn to compress and reconstruct images. Existing image compression algorithms based on neural networks learn quantized representations with a constant spatial bit rate across each image. While entropy coding introduces some spatial variation, traditional codecs have benefited significantly by explicitly adapting the bit rate based on local image complexity and visual saliency. This paper introduces an algorithm that combines deep neural networks with quality-sensitive bit rate adaptation using a tiled network. We demonstrate the importance of spatial context prediction and show improved quantitative (PSNR) and qualitative (subjective rater assessment) results compared to a non-adaptive baseline and a recently published image compression model based on fully-convolutional neural networks.



### SCK: A sparse coding based key-point detector
- **Arxiv ID**: http://arxiv.org/abs/1802.02647v5
- **DOI**: 10.1109/ICIP.2018.8451829
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.02647v5)
- **Published**: 2018-02-07 21:39:22+00:00
- **Updated**: 2018-12-07 22:30:41+00:00
- **Authors**: Thanh Hong-Phuoc, Yifeng He, Ling Guan
- **Comment**: Manuscript accepted for presentation at 2018 IEEE International
  Conference on Image Processing, October 7-10, 2018, Athens, Greece. Patent
  applied. If you use any techniques, claims, images in this manuscript, please
  cite the corresponding paper
- **Journal**: 2018 25th IEEE International Conference on Image Processing
  (ICIP), Athens, Greece, 2018, pp. 3768-3772
- **Summary**: All current popular hand-crafted key-point detectors such as Harris corner, MSER, SIFT, SURF... rely on some specific pre-designed structures for the detection of corners, blobs, or junctions in an image. In this paper, a novel sparse coding based key-point detector which requires no particular pre-designed structures is presented. The key-point detector is based on measuring the complexity level of each block in an image to decide where a key-point should be. The complexity level of a block is defined as the total number of non-zero components of a sparse representation of that block. Generally, a block constructed with more components is more complex and has greater potential to be a good key-point. Experimental results on Webcam and EF datasets [1, 2] show that the proposed detector achieves significantly high repeatability compared to hand-crafted features, and even outperforms the matching scores of the state-of-the-art learning based detector.



### Fine-Grained Land Use Classification at the City Scale Using Ground-Level Images
- **Arxiv ID**: http://arxiv.org/abs/1802.02668v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1802.02668v1)
- **Published**: 2018-02-07 23:01:13+00:00
- **Updated**: 2018-02-07 23:01:13+00:00
- **Authors**: Yi Zhu, Xueqing Deng, Shawn Newsam
- **Comment**: None
- **Journal**: None
- **Summary**: We perform fine-grained land use mapping at the city scale using ground-level images. Mapping land use is considerably more difficult than mapping land cover and is generally not possible using overhead imagery as it requires close-up views and seeing inside buildings. We postulate that the growing collections of georeferenced, ground-level images suggest an alternate approach to this geographic knowledge discovery problem. We develop a general framework that uses Flickr images to map 45 different land-use classes for the City of San Francisco. Individual images are classified using a novel convolutional neural network containing two streams, one for recognizing objects and another for recognizing scenes. This network is trained in an end-to-end manner directly on the labeled training images. We propose several strategies to overcome the noisiness of our user-generated data including search-based training set augmentation and online adaptive training. We derive a ground truth map of San Francisco in order to evaluate our method. We demonstrate the effectiveness of our approach through geo-visualization and quantitative analysis. Our framework achieves over 29% recall at the individual land parcel level which represents a strong baseline for the challenging 45-way land use classification problem especially given the noisiness of the image data.



### PPFNet: Global Context Aware Local Features for Robust 3D Point Matching
- **Arxiv ID**: http://arxiv.org/abs/1802.02669v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1802.02669v2)
- **Published**: 2018-02-07 23:01:52+00:00
- **Updated**: 2018-03-01 20:26:25+00:00
- **Authors**: Haowen Deng, Tolga Birdal, Slobodan Ilic
- **Comment**: Accepted for publication at CVPR 2018
- **Journal**: None
- **Summary**: We present PPFNet - Point Pair Feature NETwork for deeply learning a globally informed 3D local feature descriptor to find correspondences in unorganized point clouds. PPFNet learns local descriptors on pure geometry and is highly aware of the global context, an important cue in deep learning. Our 3D representation is computed as a collection of point-pair-features combined with the points and normals within a local vicinity. Our permutation invariant network design is inspired by PointNet and sets PPFNet to be ordering-free. As opposed to voxelization, our method is able to consume raw point clouds to exploit the full sparsity. PPFNet uses a novel $\textit{N-tuple}$ loss and architecture injecting the global information naturally into the local descriptor. It shows that context awareness also boosts the local feature representation. Qualitative and quantitative evaluations of our network suggest increased recall, improved robustness and invariance as well as a vital step in the 3D descriptor extraction performance.



