# Arxiv Papers in cs.CV on 2018-02-27
### On the Suitability of $L_p$-norms for Creating and Preventing Adversarial Examples
- **Arxiv ID**: http://arxiv.org/abs/1802.09653v3
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1802.09653v3)
- **Published**: 2018-02-27 00:04:12+00:00
- **Updated**: 2018-07-27 13:48:36+00:00
- **Authors**: Mahmood Sharif, Lujo Bauer, Michael K. Reiter
- **Comment**: Appeared in CV-COPS/CVPRW 2018
- **Journal**: None
- **Summary**: Much research effort has been devoted to better understanding adversarial examples, which are specially crafted inputs to machine-learning models that are perceptually similar to benign inputs, but are classified differently (i.e., misclassified). Both algorithms that create adversarial examples and strategies for defending against them typically use $L_p$-norms to measure the perceptual similarity between an adversarial input and its benign original. Prior work has already shown, however, that two images need not be close to each other as measured by an $L_p$-norm to be perceptually similar. In this work, we show that nearness according to an $L_p$-norm is not just unnecessary for perceptual similarity, but is also insufficient. Specifically, focusing on datasets (CIFAR10 and MNIST), $L_p$-norms, and thresholds used in prior work, we show through online user studies that "adversarial examples" that are closer to their benign counterparts than required by commonly used $L_p$-norm thresholds can nevertheless be perceptually different to humans from the corresponding benign examples. Namely, the perceptual distance between two images that are "near" each other according to an $L_p$-norm can be high enough that participants frequently classify the two images as representing different objects or digits. Combined with prior work, we thus demonstrate that nearness of inputs as measured by $L_p$-norms is neither necessary nor sufficient for perceptual similarity, which has implications for both creating and defending against adversarial examples. We propose and discuss alternative similarity metrics to stimulate future research in the area.



### Translating and Segmenting Multimodal Medical Volumes with Cycle- and Shape-Consistency Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/1802.09655v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.09655v2)
- **Published**: 2018-02-27 00:10:13+00:00
- **Updated**: 2019-03-16 18:19:45+00:00
- **Authors**: Zizhao Zhang, Lin Yang, Yefeng Zheng
- **Comment**: CVPR2018 (a small revision in v2)
- **Journal**: None
- **Summary**: Synthesized medical images have several important applications, e.g., as an intermedium in cross-modality image registration and as supplementary training samples to boost the generalization capability of a classifier. Especially, synthesized computed tomography (CT) data can provide X-ray attenuation map for radiation therapy planning. In this work, we propose a generic cross-modality synthesis approach with the following targets: 1) synthesizing realistic looking 3D images using unpaired training data, 2) ensuring consistent anatomical structures, which could be changed by geometric distortion in cross-modality synthesis and 3) improving volume segmentation by using synthetic data for modalities with limited training samples. We show that these goals can be achieved with an end-to-end 3D convolutional neural network (CNN) composed of mutually-beneficial generators and segmentors for image synthesis and segmentation tasks. The generators are trained with an adversarial loss, a cycle-consistency loss, and also a shape-consistency loss, which is supervised by segmentors, to reduce the geometric distortion. From the segmentation view, the segmentors are boosted by synthetic data from generators in an online manner. Generators and segmentors prompt each other alternatively in an end-to-end training fashion. With extensive experiments on a dataset including a total of 4,496 CT and magnetic resonance imaging (MRI) cardiovascular volumes, we show both tasks are beneficial to each other and coupling these two tasks results in better performance than solving them exclusively.



### Semantic segmentation of trajectories with agent models
- **Arxiv ID**: http://arxiv.org/abs/1802.09659v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.09659v1)
- **Published**: 2018-02-27 00:39:13+00:00
- **Updated**: 2018-02-27 00:39:13+00:00
- **Authors**: Daisuke Ogawa, Toru Tamaki, Bisser Raytchev, Kazufumi Kaneda
- **Comment**: in Proc of FCV2018, 21/Feb/2018
- **Journal**: None
- **Summary**: In many cases, such as trajectories clustering and classification, we often divide a trajectory into segments as preprocessing. In this paper, we propose a trajectory semantic segmentation method based on learned behavior models. In the proposed method, we learn some behavior models from video sequences. Next, using learned behavior models and a hidden Markov model, we segment a trajectory into semantic segments. Comparing with the Ramer-Douglas-Peucker algorithm, we show the effectiveness of the proposed method.



### Directional Statistics-based Deep Metric Learning for Image Classification and Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1802.09662v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.09662v2)
- **Published**: 2018-02-27 00:54:19+00:00
- **Updated**: 2018-03-28 02:55:27+00:00
- **Authors**: Xuefei Zhe, Shifeng Chen, Hong Yan
- **Comment**: codes will come soon
- **Journal**: None
- **Summary**: Deep distance metric learning (DDML), which is proposed to learn image similarity metrics in an end-to-end manner based on the convolution neural network, has achieved encouraging results in many computer vision tasks.$L2$-normalization in the embedding space has been used to improve the performance of several DDML methods. However, the commonly used Euclidean distance is no longer an accurate metric for $L2$-normalized embedding space, i.e., a hyper-sphere. Another challenge of current DDML methods is that their loss functions are usually based on rigid data formats, such as the triplet tuple. Thus, an extra process is needed to prepare data in specific formats. In addition, their losses are obtained from a limited number of samples, which leads to a lack of the global view of the embedding space. In this paper, we replace the Euclidean distance with the cosine similarity to better utilize the $L2$-normalization, which is able to attenuate the curse of dimensionality. More specifically, a novel loss function based on the von Mises-Fisher distribution is proposed to learn a compact hyper-spherical embedding space. Moreover, a new efficient learning algorithm is developed to better capture the global structure of the embedding space. Experiments for both classification and retrieval tasks on several standard datasets show that our method achieves state-of-the-art performance with a simpler training procedure. Furthermore, we demonstrate that, even with a small number of convolutional layers, our model can still obtain significantly better classification performance than the widely used softmax loss.



### Single-View Food Portion Estimation: Learning Image-to-Energy Mappings Using Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1802.09670v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.09670v2)
- **Published**: 2018-02-27 01:25:03+00:00
- **Updated**: 2018-05-23 07:09:12+00:00
- **Authors**: Shaobo Fang, Zeman Shao, Runyu Mao, Chichen Fu, Deborah A. Kerr, Carol J. Boushey, Edward J. Delp, Fengqing Zhu
- **Comment**: 2018 IEEE International Conference on Image Processing
- **Journal**: None
- **Summary**: Due to the growing concern of chronic diseases and other health problems related to diet, there is a need to develop accurate methods to estimate an individual's food and energy intake. Measuring accurate dietary intake is an open research problem. In particular, accurate food portion estimation is challenging since the process of food preparation and consumption impose large variations on food shapes and appearances. In this paper, we present a food portion estimation method to estimate food energy (kilocalories) from food images using Generative Adversarial Networks (GAN). We introduce the concept of an "energy distribution" for each food image. To train the GAN, we design a food image dataset based on ground truth food labels and segmentation masks for each food image as well as energy information associated with the food image. Our goal is to learn the mapping of the food image to the food energy. We can then estimate food energy based on the energy distribution. We show that an average energy estimation error rate of 10.89% can be obtained by learning the image-to-energy mapping.



### Recurrent Residual Module for Fast Inference in Videos
- **Arxiv ID**: http://arxiv.org/abs/1802.09723v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.09723v1)
- **Published**: 2018-02-27 05:25:20+00:00
- **Updated**: 2018-02-27 05:25:20+00:00
- **Authors**: Bowen Pan, Wuwei Lin, Xiaolin Fang, Chaoqin Huang, Bolei Zhou, Cewu Lu
- **Comment**: To appear in CVPR 2018
- **Journal**: None
- **Summary**: Deep convolutional neural networks (CNNs) have made impressive progress in many video recognition tasks such as video pose estimation and video object detection. However, CNN inference on video is computationally expensive due to processing dense frames individually. In this work, we propose a framework called Recurrent Residual Module (RRM) to accelerate the CNN inference for video recognition tasks. This framework has a novel design of using the similarity of the intermediate feature maps of two consecutive frames, to largely reduce the redundant computation. One unique property of the proposed method compared to previous work is that feature maps of each frame are precisely computed. The experiments show that, while maintaining the similar recognition performance, our RRM yields averagely 2x acceleration on the commonly used CNNs such as AlexNet, ResNet, deep compression model (thus 8-12x faster than the original dense models using the efficient inference engine), and impressively 9x acceleration on some binary networks such as XNOR-Nets (thus 500x faster than the original model). We further verify the effectiveness of the RRM on speeding up CNNs for video pose estimation and video object detection.



### ReHAR: Robust and Efficient Human Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/1802.09745v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.09745v1)
- **Published**: 2018-02-27 06:58:01+00:00
- **Updated**: 2018-02-27 06:58:01+00:00
- **Authors**: Xin Li, Mooi Choo Chuah
- **Comment**: Accepted by WACV 2018
- **Journal**: None
- **Summary**: Designing a scheme that can achieve a good performance in predicting single person activities and group activities is a challenging task. In this paper, we propose a novel robust and efficient human activity recognition scheme called ReHAR, which can be used to handle single person activities and group activities prediction. First, we generate an optical flow image for each video frame. Then, both video frames and their corresponding optical flow images are fed into a Single Frame Representation Model to generate representations. Finally, an LSTM is used to pre- dict the final activities based on the generated representations. The whole model is trained end-to-end to allow meaningful representations to be generated for the final activity recognition. We evaluate ReHAR using two well-known datasets: the NCAA Basketball Dataset and the UCFSports Action Dataset. The experimental results show that the pro- posed ReHAR achieves a higher activity recognition accuracy with an order of magnitude shorter computation time compared to the state-of-the-art methods.



### Learning Representations for Neural Network-Based Classification Using the Information Bottleneck Principle
- **Arxiv ID**: http://arxiv.org/abs/1802.09766v6
- **DOI**: 10.1109/TPAMI.2019.2909031
- **Categories**: **cs.LG**, cs.CV, cs.IT, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/1802.09766v6)
- **Published**: 2018-02-27 08:24:19+00:00
- **Updated**: 2019-04-11 11:20:27+00:00
- **Authors**: Rana Ali Amjad, Bernhard C. Geiger
- **Comment**: 16 pages, to appear in IEEE Trans. Pattern Analysis and Machine
  Intelligence
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence
  42(9):2225-2239, 2020. (c) IEEE
- **Summary**: In this theory paper, we investigate training deep neural networks (DNNs) for classification via minimizing the information bottleneck (IB) functional. We show that the resulting optimization problem suffers from two severe issues: First, for deterministic DNNs, either the IB functional is infinite for almost all values of network parameters, making the optimization problem ill-posed, or it is piecewise constant, hence not admitting gradient-based optimization methods. Second, the invariance of the IB functional under bijections prevents it from capturing properties of the learned representation that are desirable for classification, such as robustness and simplicity. We argue that these issues are partly resolved for stochastic DNNs, DNNs that include a (hard or soft) decision rule, or by replacing the IB functional with related, but more well-behaved cost functions. We conclude that recent successes reported about training DNNs using the IB framework must be attributed to such solutions. As a side effect, our results indicate limitations of the IB framework for the analysis of DNNs. We also note that rather than trying to repair the inherent problems in the IB functional, a better approach may be to design regularizers on latent representation enforcing the desired properties directly.



### Mixed Supervised Object Detection with Robust Objectness Transfer
- **Arxiv ID**: http://arxiv.org/abs/1802.09778v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.09778v3)
- **Published**: 2018-02-27 09:02:38+00:00
- **Updated**: 2019-09-26 03:24:49+00:00
- **Authors**: Yan Li, Junge Zhang, Kaiqi Huang, Jianguo Zhang
- **Comment**: IEEE Transactions on Pattern Analysis and Machine Intelligence
  (2019). Together with Supplementary Materials. Note: The author list in
  Google Scholar is INCORRECT. The right author list is 1) Yan Li, 2) Junge
  Zhang, 3) Kaiqi Huang and 4) Jianguo Zhang. The official published version
  can be found in https://ieeexplore.ieee.org/abstract/document/8304628
- **Journal**: None
- **Summary**: In this paper, we consider the problem of leveraging existing fully labeled categories to improve the weakly supervised detection (WSD) of new object categories, which we refer to as mixed supervised detection (MSD). Different from previous MSD methods that directly transfer the pre-trained object detectors from existing categories to new categories, we propose a more reasonable and robust objectness transfer approach for MSD. In our framework, we first learn domain-invariant objectness knowledge from the existing fully labeled categories. The knowledge is modeled based on invariant features that are robust to the distribution discrepancy between the existing categories and new categories; therefore the resulting knowledge would generalize well to new categories and could assist detection models to reject distractors (e.g., object parts) in weakly labeled images of new categories. Under the guidance of learned objectness knowledge, we utilize multiple instance learning (MIL) to model the concepts of both objects and distractors and to further improve the ability of rejecting distractors in weakly labeled images. Our robust objectness transfer approach outperforms the existing MSD methods, and achieves state-of-the-art results on the challenging ILSVRC2013 detection dataset and the PASCAL VOC datasets.



### Coarse to fine non-rigid registration: a chain of scale-specific neural networks for multimodal image alignment with application to remote sensing
- **Arxiv ID**: http://arxiv.org/abs/1802.09816v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1802.09816v1)
- **Published**: 2018-02-27 10:47:06+00:00
- **Updated**: 2018-02-27 10:47:06+00:00
- **Authors**: Armand Zampieri, Guillaume Charpiat, Yuliya Tarabalka
- **Comment**: None
- **Journal**: None
- **Summary**: We tackle here the problem of multimodal image non-rigid registration, which is of prime importance in remote sensing and medical imaging. The difficulties encountered by classical registration approaches include feature design and slow optimization by gradient descent. By analyzing these methods, we note the significance of the notion of scale. We design easy-to-train, fully-convolutional neural networks able to learn scale-specific features. Once chained appropriately, they perform global registration in linear time, getting rid of gradient descent schemes by predicting directly the deformation.We show their performance in terms of quality and speed through various tasks of remote sensing multimodal image alignment. In particular, we are able to register correctly cadastral maps of buildings as well as road polylines onto RGB images, and outperform current keypoint matching methods.



### Spatio-Temporal Graph Convolution for Skeleton Based Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1802.09834v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.09834v1)
- **Published**: 2018-02-27 11:39:46+00:00
- **Updated**: 2018-02-27 11:39:46+00:00
- **Authors**: Chaolong Li, Zhen Cui, Wenming Zheng, Chunyan Xu, Jian Yang
- **Comment**: Accepted by AAAI 2018
- **Journal**: None
- **Summary**: Variations of human body skeletons may be considered as dynamic graphs, which are generic data representation for numerous real-world applications. In this paper, we propose a spatio-temporal graph convolution (STGC) approach for assembling the successes of local convolutional filtering and sequence learning ability of autoregressive moving average. To encode dynamic graphs, the constructed multi-scale local graph convolution filters, consisting of matrices of local receptive fields and signal mappings, are recursively performed on structured graph data of temporal and spatial domain. The proposed model is generic and principled as it can be generalized into other dynamic models. We theoretically prove the stability of STGC and provide an upper-bound of the signal transformation to be learnt. Further, the proposed recursive model can be stacked into a multi-layer architecture. To evaluate our model, we conduct extensive experiments on four benchmark skeleton-based action datasets, including the large-scale challenging NTU RGB+D. The experimental results demonstrate the effectiveness of our proposed model and the improvement over the state-of-the-art.



### Adversarial Active Learning for Deep Networks: a Margin Based Approach
- **Arxiv ID**: http://arxiv.org/abs/1802.09841v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1802.09841v1)
- **Published**: 2018-02-27 12:02:33+00:00
- **Updated**: 2018-02-27 12:02:33+00:00
- **Authors**: Melanie Ducoffe, Frederic Precioso
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new active learning strategy designed for deep neural networks. The goal is to minimize the number of data annotation queried from an oracle during training. Previous active learning strategies scalable for deep networks were mostly based on uncertain sample selection. In this work, we focus on examples lying close to the decision boundary. Based on theoretical works on margin theory for active learning, we know that such examples may help to considerably decrease the number of annotations. While measuring the exact distance to the decision boundaries is intractable, we propose to rely on adversarial examples. We do not consider anymore them as a threat instead we exploit the information they provide on the distribution of the input space in order to approximate the distance to decision boundaries. We demonstrate empirically that adversarial active queries yield faster convergence of CNNs trained on MNIST, the Shoe-Bag and the Quick-Draw datasets.



### Graph Laplacian for Image Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/1802.09843v6
- **DOI**: 10.1007/s00138-020-01059-4
- **Categories**: **eess.IV**, cs.CV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/1802.09843v6)
- **Published**: 2018-02-27 12:08:06+00:00
- **Updated**: 2020-02-10 12:55:14+00:00
- **Authors**: Francesco Verdoja, Marco Grangetto
- **Comment**: Published in Machine Vision and Applications (Springer)
- **Journal**: Machine Vision and Applications, vol. 31, no. 1, Feb. 2020
- **Summary**: Reed-Xiaoli detector (RXD) is recognized as the benchmark algorithm for image anomaly detection; however, it presents known limitations, namely the dependence over the image following a multivariate Gaussian model, the estimation and inversion of a high-dimensional covariance matrix, and the inability to effectively include spatial awareness in its evaluation. In this work, a novel graph-based solution to the image anomaly detection problem is proposed; leveraging the graph Fourier transform, we are able to overcome some of RXD's limitations while reducing computational cost at the same time. Tests over both hyperspectral and medical images, using both synthetic and real anomalies, prove the proposed technique is able to obtain significant gains over performance by other algorithms in the state of the art.



### Solving Inverse Computational Imaging Problems using Deep Pixel-level Prior
- **Arxiv ID**: http://arxiv.org/abs/1802.09850v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1802.09850v2)
- **Published**: 2018-02-27 12:23:27+00:00
- **Updated**: 2018-04-24 00:37:58+00:00
- **Authors**: Akshat Dave, Anil Kumar Vadathya, Ramana Subramanyam, Rahul Baburajan, Kaushik Mitra
- **Comment**: None
- **Journal**: None
- **Summary**: Signal reconstruction is a challenging aspect of computational imaging as it often involves solving ill-posed inverse problems. Recently, deep feed-forward neural networks have led to state-of-the-art results in solving various inverse imaging problems. However, being task specific, these networks have to be learned for each inverse problem. On the other hand, a more flexible approach would be to learn a deep generative model once and then use it as a signal prior for solving various inverse problems. We show that among the various state of the art deep generative models, autoregressive models are especially suitable for our purpose for the following reasons. First, they explicitly model the pixel level dependencies and hence are capable of reconstructing low-level details such as texture patterns and edges better. Second, they provide an explicit expression for the image prior which can then be used for MAP based inference along with the forward model. Third, they can model long range dependencies in images which make them ideal for handling global multiplexing as encountered in various compressive imaging systems. We demonstrate the efficacy of our proposed approach in solving three computational imaging problems: Single Pixel Camera (SPC), LiSens and FlatCam. For both real and simulated cases, we obtain better reconstructions than the state-of-the-art methods in terms of perceptual and quantitative metrics.



### Extraction of V2V Encountering Scenarios from Naturalistic Driving Database
- **Arxiv ID**: http://arxiv.org/abs/1802.09917v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, 68Q32
- **Links**: [PDF](http://arxiv.org/pdf/1802.09917v2)
- **Published**: 2018-02-27 14:40:04+00:00
- **Updated**: 2018-05-23 16:22:25+00:00
- **Authors**: Zhaobin Mo, Sisi Li, Diange Yang, Ding Zhao
- **Comment**: 6 pages; 11 figures; Submitted to International Symposium on Advanced
  Vehicle Control, Beijing, China, July 2018
- **Journal**: None
- **Summary**: It is necessary to thoroughly evaluate the effectiveness and safety of Connected Vehicles (CVs) algorithm before their release and deployment. Current evaluation approach mainly relies on simulation platform with the single-vehicle driving model. The main drawback of it is the lack of network realism. To overcome this problem, we extract naturalistic V2V encounters data from the database, and then separate the primary vehicle encounter category by clustering. A fast mining algorithm is proposed that can be applied to parallel query for further process acceleration. 4,500 encounters are mined from a 275 GB database collected in the Safety Pilot Model Program in Ann Arbor Michigan, USA. K-means and Dynamic Time Warping (DTW) are used in clustering. Results show this method can quickly mine and cluster primary driving scenarios from a large database. Our results separate the car-following, intersection and by-passing, which are the primary category of the vehicle encounter. We anticipate the work in the essay can become a general method to effectively extract vehicle encounters from any existing database that contains vehicular GPS information. What's more, the naturalistic data of different vehicle encounters can be applied in Connected Vehicles evaluation.



### Real-World Repetition Estimation by Div, Grad and Curl
- **Arxiv ID**: http://arxiv.org/abs/1802.09971v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.09971v1)
- **Published**: 2018-02-27 15:42:34+00:00
- **Updated**: 2018-02-27 15:42:34+00:00
- **Authors**: Tom F. H. Runia, Cees G. M. Snoek, Arnold W. M. Smeulders
- **Comment**: None
- **Journal**: None
- **Summary**: We consider the problem of estimating repetition in video, such as performing push-ups, cutting a melon or playing violin. Existing work shows good results under the assumption of static and stationary periodicity. As realistic video is rarely perfectly static and stationary, the often preferred Fourier-based measurements is inapt. Instead, we adopt the wavelet transform to better handle non-static and non-stationary video dynamics. From the flow field and its differentials, we derive three fundamental motion types and three motion continuities of intrinsic periodicity in 3D. On top of this, the 2D perception of 3D periodicity considers two extreme viewpoints. What follows are 18 fundamental cases of recurrent perception in 2D. In practice, to deal with the variety of repetitive appearance, our theory implies measuring time-varying flow and its differentials (gradient, divergence and curl) over segmented foreground motion. For experiments, we introduce the new QUVA Repetition dataset, reflecting reality by including non-static and non-stationary videos. On the task of counting repetitions in video, we obtain favorable results compared to a deep learning alternative.



### Fusion of Multispectral Data Through Illumination-aware Deep Neural Networks for Pedestrian Detection
- **Arxiv ID**: http://arxiv.org/abs/1802.09972v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.09972v1)
- **Published**: 2018-02-27 15:42:40+00:00
- **Updated**: 2018-02-27 15:42:40+00:00
- **Authors**: Dayan Guan, Yanpeng Cao, Jun Liang, Yanlong Cao, Michael Ying Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Multispectral pedestrian detection has received extensive attention in recent years as a promising solution to facilitate robust human target detection for around-the-clock applications (e.g. security surveillance and autonomous driving). In this paper, we demonstrate illumination information encoded in multispectral images can be utilized to significantly boost performance of pedestrian detection. A novel illumination-aware weighting mechanism is present to accurately depict illumination condition of a scene. Such illumination information is incorporated into two-stream deep convolutional neural networks to learn multispectral human-related features under different illumination conditions (daytime and nighttime). Moreover, we utilized illumination information together with multispectral data to generate more accurate semantic segmentation which are used to boost pedestrian detection accuracy. Putting all of the pieces together, we present a powerful framework for multispectral pedestrian detection based on multi-task learning of illumination-aware pedestrian detection and semantic segmentation. Our proposed method is trained end-to-end using a well-designed multi-task loss function and outperforms state-of-the-art approaches on KAIST multispectral pedestrian dataset.



### Mono-Camera 3D Multi-Object Tracking Using Deep Learning Detections and PMBM Filtering
- **Arxiv ID**: http://arxiv.org/abs/1802.09975v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/1802.09975v1)
- **Published**: 2018-02-27 15:46:15+00:00
- **Updated**: 2018-02-27 15:46:15+00:00
- **Authors**: Samuel Scheidegger, Joachim Benjaminsson, Emil Rosenberg, Amrit Krishnan, Karl Granstrom
- **Comment**: 8 pages, 2 figures, for associated videos, see https://goo.gl/AoydgW
- **Journal**: None
- **Summary**: Monocular cameras are one of the most commonly used sensors in the automotive industry for autonomous vehicles. One major drawback using a monocular camera is that it only makes observations in the two dimensional image plane and can not directly measure the distance to objects. In this paper, we aim at filling this gap by developing a multi-object tracking algorithm that takes an image as input and produces trajectories of detected objects in a world coordinate system. We solve this by using a deep neural network trained to detect and estimate the distance to objects from a single input image. The detections from a sequence of images are fed in to a state-of-the art Poisson multi-Bernoulli mixture tracking filter. The combination of the learned detector and the PMBM filter results in an algorithm that achieves 3D tracking using only mono-camera images as input. The performance of the algorithm is evaluated both in 3D world coordinates, and 2D image coordinates, using the publicly available KITTI object tracking dataset. The algorithm shows the ability to accurately track objects, correctly handle data associations, even when there is a big overlap of the objects in the image, and is one of the top performing algorithms on the KITTI object tracking benchmark. Furthermore, the algorithm is efficient, running on average close to 20 frames per second.



### Neural Stereoscopic Image Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/1802.09985v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.09985v4)
- **Published**: 2018-02-27 16:02:17+00:00
- **Updated**: 2018-07-27 15:51:44+00:00
- **Authors**: Xinyu Gong, Haozhi Huang, Lin Ma, Fumin Shen, Wei Liu, Tong Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Neural style transfer is an emerging technique which is able to endow daily-life images with attractive artistic styles. Previous work has succeeded in applying convolutional neural networks (CNNs) to style transfer for monocular images or videos. However, style transfer for stereoscopic images is still a missing piece. Different from processing a monocular image, the two views of a stylized stereoscopic pair are required to be consistent to provide observers a comfortable visual experience. In this paper, we propose a novel dual path network for view-consistent style transfer on stereoscopic images. While each view of the stereoscopic pair is processed in an individual path, a novel feature aggregation strategy is proposed to effectively share information between the two paths. Besides a traditional perceptual loss being used for controlling the style transfer quality in each view, a multi-layer view loss is leveraged to enforce the network to coordinate the learning of both the paths to generate view-consistent stylized results. Extensive experiments show that, compared against previous methods, our proposed model can produce stylized stereoscopic images which achieve decent view consistency.



### Multi-View Silhouette and Depth Decomposition for High Resolution 3D Object Representation
- **Arxiv ID**: http://arxiv.org/abs/1802.09987v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.09987v3)
- **Published**: 2018-02-27 16:09:28+00:00
- **Updated**: 2018-11-14 21:40:12+00:00
- **Authors**: Edward Smith, Scott Fujimoto, David Meger
- **Comment**: 22 pages, NIPS 2018
- **Journal**: None
- **Summary**: We consider the problem of scaling deep generative shape models to high-resolution. Drawing motivation from the canonical view representation of objects, we introduce a novel method for the fast up-sampling of 3D objects in voxel space through networks that perform super-resolution on the six orthographic depth projections. This allows us to generate high-resolution objects with more efficient scaling than methods which work directly in 3D. We decompose the problem of 2D depth super-resolution into silhouette and depth prediction to capture both structure and fine detail. This allows our method to generate sharp edges more easily than an individual network. We evaluate our work on multiple experiments concerning high-resolution 3D objects, and show our system is capable of accurately predicting novel objects at resolutions as large as 512$\mathbf{\times}$512$\mathbf{\times}$512 -- the highest resolution reported for this task. We achieve state-of-the-art performance on 3D object reconstruction from RGB images on the ShapeNet dataset, and further demonstrate the first effective 3D super-resolution method.



### Deep Learning Architectures for Face Recognition in Video Surveillance
- **Arxiv ID**: http://arxiv.org/abs/1802.09990v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.09990v2)
- **Published**: 2018-02-27 16:10:15+00:00
- **Updated**: 2018-06-27 14:00:33+00:00
- **Authors**: Saman Bashbaghi, Eric Granger, Robert Sabourin, Mostafa Parchami
- **Comment**: None
- **Journal**: None
- **Summary**: Face recognition (FR) systems for video surveillance (VS) applications attempt to accurately detect the presence of target individuals over a distributed network of cameras. In video-based FR systems, facial models of target individuals are designed a priori during enrollment using a limited number of reference still images or video data. These facial models are not typically representative of faces being observed during operations due to large variations in illumination, pose, scale, occlusion, blur, and to camera inter-operability. Specifically, in still-to-video FR application, a single high-quality reference still image captured with still camera under controlled conditions is employed to generate a facial model to be matched later against lower-quality faces captured with video cameras under uncontrolled conditions. Current video-based FR systems can perform well on controlled scenarios, while their performance is not satisfactory in uncontrolled scenarios mainly because of the differences between the source (enrollment) and the target (operational) domains. Most of the efforts in this area have been toward the design of robust video-based FR systems in unconstrained surveillance environments. This chapter presents an overview of recent advances in still-to-video FR scenario through deep convolutional neural networks (CNNs). In particular, deep learning architectures proposed in the literature based on triplet-loss function (e.g., cross-correlation matching CNN, trunk-branch ensemble CNN and HaarNet) and supervised autoencoders (e.g., canonical face representation CNN) are reviewed and compared in terms of accuracy and computational complexity.



### Simultaneous Traffic Sign Detection and Boundary Estimation using Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1802.10019v1
- **DOI**: 10.1109/TITS.2018.2801560
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.10019v1)
- **Published**: 2018-02-27 16:51:04+00:00
- **Updated**: 2018-02-27 16:51:04+00:00
- **Authors**: Hee Seok Lee, Kang Kim
- **Comment**: Accepted for publication in IEEE Transactions on Intelligent
  Transportation Systems
- **Journal**: None
- **Summary**: We propose a novel traffic sign detection system that simultaneously estimates the location and precise boundary of traffic signs using convolutional neural network (CNN). Estimating the precise boundary of traffic signs is important in navigation systems for intelligent vehicles where traffic signs can be used as 3D landmarks for road environment. Previous traffic sign detection systems, including recent methods based on CNN, only provide bounding boxes of traffic signs as output, and thus requires additional processes such as contour estimation or image segmentation to obtain the precise sign boundary. In this work, the boundary estimation of traffic signs is formulated as a 2D pose and shape class prediction problem, and this is effectively solved by a single CNN. With the predicted 2D pose and the shape class of a target traffic sign in an input image, we estimate the actual boundary of the target sign by projecting the boundary of a corresponding template sign image into the input image plane. By formulating the boundary estimation problem as a CNN-based pose and shape prediction task, our method is end-to-end trainable, and more robust to occlusion and small targets than other boundary estimation methods that rely on contour estimation or image segmentation. The proposed method with architectural optimization provides an accurate traffic sign boundary estimation which is also efficient in compute, showing a detection frame rate higher than 7 frames per second on low-power mobile platforms.



### Genaue modellbasierte Identifikation von gynäkologischen Katheterpfaden für die MRT-bildgestützte Brachytherapie
- **Arxiv ID**: http://arxiv.org/abs/1803.00492v2
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1803.00492v2)
- **Published**: 2018-02-27 17:05:56+00:00
- **Updated**: 2018-03-10 10:22:32+00:00
- **Authors**: Andre Mastmeyer
- **Comment**: German text: 5 pages, 3 figures, Management im Krankenhaus 11/2017
- **Journal**: None
- **Summary**: German text, english abstract: Mortality in gynecologic cancers, including cervical, ovarian, vaginal and vulvar cancers, is more than 6% internationally [1]. In many countries external radiotherapy is supplemented by brachytherapy with high locally administered doses as standard. The superior ability of magnetic resonance imaging (MRI) to differentiate soft tissue has led to an increasing use of this imaging technique in the intraoperative planning and implementation of brachytherapy. A technical challenge associated with the use of MRI imaging for brachytherapy - in contrast to computed tomography (CT) imaging - is the dark-diffuse appearance and thus difficult identification of the catheter paths in the resulting images. This problem is addressed by the precise method described herein of tracing the catheters from the catheter tip. The average identification time for a single catheter path was three seconds on a standard PC. Segmentation time, accuracy and precision are promising indicators of the value of this method for the clinical application of image-guided gynecological brachytherapy. After surgery (OP), the healthy surrounding tissue of the tumor is usually irradiated. This reduces the risk of leaving behind residual cells that would likely cause a recurrence of the cancer or the formation of metastases - secondary tumors elsewhere in the body. In the case of a tumor on the cervix or prostate, the operation is minimally invasive, ie. the removal of the cancer and the irradiation are performed cost-effectively and risk-avoiding by keyhole surgery instead of open surgery.



### Improving OCR Accuracy on Early Printed Books using Deep Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1802.10033v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DL
- **Links**: [PDF](http://arxiv.org/pdf/1802.10033v1)
- **Published**: 2018-02-27 17:17:50+00:00
- **Updated**: 2018-02-27 17:17:50+00:00
- **Authors**: Christoph Wick, Christian Reul, Frank Puppe
- **Comment**: 16 pages, 4 figures, 8 tables, submitted to JLCL Volume 33 (2018),
  Issue 1
- **Journal**: None
- **Summary**: This paper proposes a combination of a convolutional and a LSTM network to improve the accuracy of OCR on early printed books. While the standard model of line based OCR uses a single LSTM layer, we utilize a CNN- and Pooling-Layer combination in advance of an LSTM layer. Due to the higher amount of trainable parameters the performance of the network relies on a high amount of training examples to unleash its power. Hereby, the error is reduced by a factor of up to 44%, yielding a CER of 1% and below. To further improve the results we use a voting mechanism to achieve character error rates (CER) below $0.5%$. The runtime of the deep model for training and prediction of a book behaves very similar to a shallow network.



### Generating High Quality Visible Images from SAR Images Using CNNs
- **Arxiv ID**: http://arxiv.org/abs/1802.10036v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.10036v1)
- **Published**: 2018-02-27 17:31:21+00:00
- **Updated**: 2018-02-27 17:31:21+00:00
- **Authors**: Puyang Wang, Vishal M. Patel
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel approach for generating high quality visible-like images from Synthetic Aperture Radar (SAR) images using Deep Convolutional Generative Adversarial Network (GAN) architectures. The proposed approach is based on a cascaded network of convolutional neural nets (CNNs) for despeckling and image colorization. The cascaded structure results in faster convergence during training and produces high quality visible images from the corresponding SAR images. Experimental results on both simulated and real SAR images show that the proposed method can produce visible-like images better compared to the recent state-of-the-art deep learning-based methods.



### Improving OCR Accuracy on Early Printed Books by combining Pretraining, Voting, and Active Learning
- **Arxiv ID**: http://arxiv.org/abs/1802.10038v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.10038v2)
- **Published**: 2018-02-27 17:35:36+00:00
- **Updated**: 2018-02-28 08:54:49+00:00
- **Authors**: Christian Reul, Uwe Springmann, Christoph Wick, Frank Puppe
- **Comment**: Submitted to JLCL Volume 33 (2018), Issue 1: Special Issue on
  Automatic Text and Layout Recognition
- **Journal**: None
- **Summary**: We combine three methods which significantly improve the OCR accuracy of OCR models trained on early printed books: (1) The pretraining method utilizes the information stored in already existing models trained on a variety of typesets (mixed models) instead of starting the training from scratch. (2) Performing cross fold training on a single set of ground truth data (line images and their transcriptions) with a single OCR engine (OCRopus) produces a committee whose members then vote for the best outcome by also taking the top-N alternatives and their intrinsic confidence values into account. (3) Following the principle of maximal disagreement we select additional training lines which the voters disagree most on, expecting them to offer the highest information gain for a subsequent training (active learning). Evaluations on six early printed books yielded the following results: On average the combination of pretraining and voting improved the character accuracy by 46% when training five folds starting from the same mixed model. This number rose to 53% when using different models for pretraining, underlining the importance of diverse voters. Incorporating active learning improved the obtained results by another 16% on average (evaluated on three of the six books). Overall, the proposed methods lead to an average error rate of 2.5% when training on only 60 lines. Using a substantial ground truth pool of 1,000 lines brought the error rate down even further to less than 1% on average.



### A Mathematical Framework for Deep Learning in Elastic Source Imaging
- **Arxiv ID**: http://arxiv.org/abs/1802.10055v3
- **DOI**: None
- **Categories**: **math.OC**, cs.CV, cs.LG, 35R30, 74D99, 92C55
- **Links**: [PDF](http://arxiv.org/pdf/1802.10055v3)
- **Published**: 2018-02-27 18:14:00+00:00
- **Updated**: 2018-05-25 23:17:48+00:00
- **Authors**: Jaejun Yoo, Abdul Wahab, Jong Chul Ye
- **Comment**: 28 pages, 14 figures
- **Journal**: None
- **Summary**: An inverse elastic source problem with sparse measurements is of concern. A generic mathematical framework is proposed which incorporates a low- dimensional manifold regularization in the conventional source reconstruction algorithms thereby enhancing their performance with sparse datasets. It is rigorously established that the proposed framework is equivalent to the so-called \emph{deep convolutional framelet expansion} in machine learning literature for inverse problems. Apposite numerical examples are furnished to substantiate the efficacy of the proposed framework.



### CSRNet: Dilated Convolutional Neural Networks for Understanding the Highly Congested Scenes
- **Arxiv ID**: http://arxiv.org/abs/1802.10062v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.10062v4)
- **Published**: 2018-02-27 18:39:31+00:00
- **Updated**: 2018-04-11 04:20:08+00:00
- **Authors**: Yuhong Li, Xiaofan Zhang, Deming Chen
- **Comment**: Accepted by CVPR 2018
- **Journal**: None
- **Summary**: We propose a network for Congested Scene Recognition called CSRNet to provide a data-driven and deep learning method that can understand highly congested scenes and perform accurate count estimation as well as present high-quality density maps. The proposed CSRNet is composed of two major components: a convolutional neural network (CNN) as the front-end for 2D feature extraction and a dilated CNN for the back-end, which uses dilated kernels to deliver larger reception fields and to replace pooling operations. CSRNet is an easy-trained model because of its pure convolutional structure. We demonstrate CSRNet on four datasets (ShanghaiTech dataset, the UCF_CC_50 dataset, the WorldEXPO'10 dataset, and the UCSD dataset) and we deliver the state-of-the-art performance. In the ShanghaiTech Part_B dataset, CSRNet achieves 47.3% lower Mean Absolute Error (MAE) than the previous state-of-the-art method. We extend the targeted applications for counting other objects, such as the vehicle in TRANCOS dataset. Results show that CSRNet significantly improves the output quality with 15.4% lower MAE than the previous state-of-the-art approach.



### Reconstruction of partially sampled multi-band images - Application to STEM-EELS imaging
- **Arxiv ID**: http://arxiv.org/abs/1802.10066v1
- **DOI**: None
- **Categories**: **eess.IV**, cond-mat.mtrl-sci, cs.CV, physics.data-an
- **Links**: [PDF](http://arxiv.org/pdf/1802.10066v1)
- **Published**: 2018-02-27 18:46:27+00:00
- **Updated**: 2018-02-27 18:46:27+00:00
- **Authors**: Étienne Monier, Thomas Oberlin, Nathalie Brun, Marcel Tencé, Marta de Frutos, Nicolas Dobigeon
- **Comment**: None
- **Journal**: None
- **Summary**: Electron microscopy has shown to be a very powerful tool to map the chemical nature of samples at various scales down to atomic resolution. However, many samples can not be analyzed with an acceptable signal-to-noise ratio because of the radiation damage induced by the electron beam. This is particularly crucial for electron energy loss spectroscopy (EELS) which acquires spectral-spatial data and requires high beam intensity. Since scanning transmission electron microscopes (STEM) are able to acquire data cubes by scanning the electron probe over the sample and recording a spectrum for each spatial position, it is possible to design the scan pattern and to sample only specific pixels. As a consequence, partial acquisition schemes are now conceivable, provided a reconstruction of the full data cube is conducted as a post-processing step. This paper proposes two reconstruction algorithms for multi-band images acquired by STEM-EELS which exploits the spectral structure and the spatial smoothness of the image. The performance of the proposed schemes is illustrated thanks to experiments conducted on a realistic phantom dataset as well as real EELS spectrum-images.



### Tell Me Where to Look: Guided Attention Inference Network
- **Arxiv ID**: http://arxiv.org/abs/1802.10171v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1802.10171v1)
- **Published**: 2018-02-27 21:17:30+00:00
- **Updated**: 2018-02-27 21:17:30+00:00
- **Authors**: Kunpeng Li, Ziyan Wu, Kuan-Chuan Peng, Jan Ernst, Yun Fu
- **Comment**: Accepted in CVPR2018
- **Journal**: None
- **Summary**: Weakly supervised learning with only coarse labels can obtain visual explanations of deep neural network such as attention maps by back-propagating gradients. These attention maps are then available as priors for tasks such as object localization and semantic segmentation. In one common framework we address three shortcomings of previous approaches in modeling such attention maps: We (1) first time make attention maps an explicit and natural component of the end-to-end training, (2) provide self-guidance directly on these maps by exploring supervision form the network itself to improve them, and (3) seamlessly bridge the gap between using weak and extra supervision if available. Despite its simplicity, experiments on the semantic segmentation task demonstrate the effectiveness of our methods. We clearly surpass the state-of-the-art on Pascal VOC 2012 val. and test set. Besides, the proposed framework provides a way not only explaining the focus of the learner but also feeding back with direct guidance towards specific tasks. Under mild assumptions our method can also be understood as a plug-in to existing weakly supervised learners to improve their generalization performance.



### Brain Tumor Type Classification via Capsule Networks
- **Arxiv ID**: http://arxiv.org/abs/1802.10200v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.10200v2)
- **Published**: 2018-02-27 22:55:54+00:00
- **Updated**: 2018-03-01 22:19:02+00:00
- **Authors**: Parnian Afshar, Arash Mohammadi, Konstantinos N. Plataniotis
- **Comment**: None
- **Journal**: None
- **Summary**: Brain tumor is considered as one of the deadliest and most common form of cancer both in children and in adults. Consequently, determining the correct type of brain tumor in early stages is of significant importance to devise a precise treatment plan and predict patient's response to the adopted treatment. In this regard, there has been a recent surge of interest in designing Convolutional Neural Networks (CNNs) for the problem of brain tumor type classification. However, CNNs typically require large amount of training data and can not properly handle input transformations. Capsule networks (referred to as CapsNets) are brand new machine learning architectures proposed very recently to overcome these shortcomings of CNNs, and posed to revolutionize deep learning solutions. Of particular interest to this work is that Capsule networks are robust to rotation and affine transformation, and require far less training data, which is the case for processing medical image datasets including brain Magnetic Resonance Imaging (MRI) images. In this paper, we focus to achieve the following four objectives: (i) Adopt and incorporate CapsNets for the problem of brain tumor classification to design an improved architecture which maximizes the accuracy of the classification problem at hand; (ii) Investigate the over-fitting problem of CapsNets based on a real set of MRI images; (iii) Explore whether or not CapsNets are capable of providing better fit for the whole brain images or just the segmented tumor, and; (iv) Develop a visualization paradigm for the output of the CapsNet to better explain the learned features. Our results show that the proposed approach can successfully overcome CNNs for the brain tumor classification problem.



### Improved Explainability of Capsule Networks: Relevance Path by Agreement
- **Arxiv ID**: http://arxiv.org/abs/1802.10204v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.10204v1)
- **Published**: 2018-02-27 23:08:17+00:00
- **Updated**: 2018-02-27 23:08:17+00:00
- **Authors**: Atefeh Shahroudnejad, Arash Mohammadi, Konstantinos N. Plataniotis
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advancements in signal processing and machine learning domains have resulted in an extensive surge of interest in deep learning models due to their unprecedented performance and high accuracy for different and challenging problems of significant engineering importance. However, when such deep learning architectures are utilized for making critical decisions such as the ones that involve human lives (e.g., in medical applications), it is of paramount importance to understand, trust, and in one word "explain" the rational behind deep models' decisions. Currently, deep learning models are typically considered as black-box systems, which do not provide any clue on their internal processing actions. Although some recent efforts have been initiated to explain behavior and decisions of deep networks, explainable artificial intelligence (XAI) domain is still in its infancy. In this regard, we consider capsule networks (referred to as CapsNets), which are novel deep structures; recently proposed as an alternative counterpart to convolutional neural networks (CNNs), and posed to change the future of machine intelligence. In this paper, we investigate and analyze structures and behaviors of the CapsNets and illustrate potential explainability properties of such networks. Furthermore, we show possibility of transforming deep learning architectures in to transparent networks via incorporation of capsules in different layers instead of convolution layers of the CNNs.



