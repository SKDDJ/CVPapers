# Arxiv Papers in cs.CV on 2018-02-18
### Efficient Sparse-Winograd Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1802.06367v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1802.06367v1)
- **Published**: 2018-02-18 12:29:05+00:00
- **Updated**: 2018-02-18 12:29:05+00:00
- **Authors**: Xingyu Liu, Jeff Pool, Song Han, William J. Dally
- **Comment**: Published as a conference paper at ICLR 2018
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) are computationally intensive, which limits their application on mobile devices. Their energy is dominated by the number of multiplies needed to perform the convolutions. Winograd's minimal filtering algorithm (Lavin, 2015) and network pruning (Han et al., 2015) can reduce the operation count, but these two methods cannot be directly combined $-$ applying the Winograd transform fills in the sparsity in both the weights and the activations. We propose two modifications to Winograd-based CNNs to enable these methods to exploit sparsity. First, we move the ReLU operation into the Winograd domain to increase the sparsity of the transformed activations. Second, we prune the weights in the Winograd domain to exploit static weight sparsity. For models on CIFAR-10, CIFAR-100 and ImageNet datasets, our method reduces the number of multiplications by $10.4\times$, $6.8\times$ and $10.8\times$ respectively with loss of accuracy less than $0.1\%$, outperforming previous baselines by $2.0\times$-$3.0\times$. We also show that moving ReLU to the Winograd domain allows more aggressive pruning.



### Visual-Only Recognition of Normal, Whispered and Silent Speech
- **Arxiv ID**: http://arxiv.org/abs/1802.06399v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.06399v1)
- **Published**: 2018-02-18 16:40:46+00:00
- **Updated**: 2018-02-18 16:40:46+00:00
- **Authors**: Stavros Petridis, Jie Shen, Doruk Cetin, Maja Pantic
- **Comment**: Accepted to ICASSP 2018
- **Journal**: None
- **Summary**: Silent speech interfaces have been recently proposed as a way to enable communication when the acoustic signal is not available. This introduces the need to build visual speech recognition systems for silent and whispered speech. However, almost all the recently proposed systems have been trained on vocalised data only. This is in contrast with evidence in the literature which suggests that lip movements change depending on the speech mode. In this work, we introduce a new audiovisual database which is publicly available and contains normal, whispered and silent speech. To the best of our knowledge, this is the first study which investigates the differences between the three speech modes using the visual modality only. We show that an absolute decrease in classification rate of up to 3.7% is observed when training and testing on normal and whispered, respectively, and vice versa. An even higher decrease of up to 8.5% is reported when the models are tested on silent speech. This reveals that there are indeed visual differences between the 3 speech modes and the common assumption that vocalized training data can be used directly to train a silent speech recognition system may not be true.



### Using 3D Hahn Moments as A Computational Representation of ATS Drugs Molecular Structure
- **Arxiv ID**: http://arxiv.org/abs/1802.06404v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.06404v2)
- **Published**: 2018-02-18 17:03:22+00:00
- **Updated**: 2018-03-17 14:44:33+00:00
- **Authors**: Satrya Fajri Pratama, Azah Kamilah Muda, Yun-Huoy Choo, Ramon Carb√≥-Dorca, Ajith Abraham
- **Comment**: None
- **Journal**: None
- **Summary**: The campaign against drug abuse is fought by all countries, most notably on ATS drugs. The technical limitations of the current test kits to detect new brand of ATS drugs present a challenge to law enforcement authorities and forensic laboratories. Meanwhile, new molecular imaging devices which allowed mankind to characterize the physical 3D molecular structure have been recently introduced, and it can be used to remedy the limitations of existing drug test kits. Thus, a new type of 3D molecular structure representation technique should be developed to cater the 3D molecular structure acquired physically using these molecular imaging devices. One of the applications of image processing methods to represent a 3D image is 3D moments, and this study formulates a new 3D moments technique, namely 3D Hahn moments, to represent the 3D molecular structure of ATS drugs. The performance of the proposed technique was analysed using drug chemical structures obtained from UNODC for the ATS drugs, while non-ATS drugs are obtained randomly from ChemSpider database. The evaluation shows the technique is qualified to be further explored in the future works to be fully compatible with ATS drug identification domain.



### End-to-end Audiovisual Speech Recognition
- **Arxiv ID**: http://arxiv.org/abs/1802.06424v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.06424v2)
- **Published**: 2018-02-18 19:07:31+00:00
- **Updated**: 2018-02-22 11:58:14+00:00
- **Authors**: Stavros Petridis, Themos Stafylakis, Pingchuan Ma, Feipeng Cai, Georgios Tzimiropoulos, Maja Pantic
- **Comment**: Accepted to ICASSP 2018
- **Journal**: None
- **Summary**: Several end-to-end deep learning approaches have been recently presented which extract either audio or visual features from the input images or audio signals and perform speech recognition. However, research on end-to-end audiovisual models is very limited. In this work, we present an end-to-end audiovisual model based on residual networks and Bidirectional Gated Recurrent Units (BGRUs). To the best of our knowledge, this is the first audiovisual fusion model which simultaneously learns to extract features directly from the image pixels and audio waveforms and performs within-context word recognition on a large publicly available dataset (LRW). The model consists of two streams, one for each modality, which extract features directly from mouth regions and raw waveforms. The temporal dynamics in each stream/modality are modeled by a 2-layer BGRU and the fusion of multiple streams/modalities takes place via another 2-layer BGRU. A slight improvement in the classification rate over an end-to-end audio-only and MFCC-based model is reported in clean audio conditions and low levels of noise. In presence of high levels of noise, the end-to-end audiovisual model significantly outperforms both audio-only models.



### DARTS: Deceiving Autonomous Cars with Toxic Signs
- **Arxiv ID**: http://arxiv.org/abs/1802.06430v3
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1802.06430v3)
- **Published**: 2018-02-18 19:39:28+00:00
- **Updated**: 2018-05-31 20:05:27+00:00
- **Authors**: Chawin Sitawarin, Arjun Nitin Bhagoji, Arsalan Mosenia, Mung Chiang, Prateek Mittal
- **Comment**: Submitted to ACM CCS 2018; Extended version of [1801.02780] Rogue
  Signs: Deceiving Traffic Sign Recognition with Malicious Ads and Logos
- **Journal**: None
- **Summary**: Sign recognition is an integral part of autonomous cars. Any misclassification of traffic signs can potentially lead to a multitude of disastrous consequences, ranging from a life-threatening accident to even a large-scale interruption of transportation services relying on autonomous cars. In this paper, we propose and examine security attacks against sign recognition systems for Deceiving Autonomous caRs with Toxic Signs (we call the proposed attacks DARTS). In particular, we introduce two novel methods to create these toxic signs. First, we propose Out-of-Distribution attacks, which expand the scope of adversarial examples by enabling the adversary to generate these starting from an arbitrary point in the image space compared to prior attacks which are restricted to existing training/test data (In-Distribution). Second, we present the Lenticular Printing attack, which relies on an optical phenomenon to deceive the traffic sign recognition system. We extensively evaluate the effectiveness of the proposed attacks in both virtual and real-world settings and consider both white-box and black-box threat models. Our results demonstrate that the proposed attacks are successful under both settings and threat models. We further show that Out-of-Distribution attacks can outperform In-Distribution attacks on classifiers defended using the adversarial training defense, exposing a new attack vector for these defenses.



### Fast 5DOF Needle Tracking in iOCT
- **Arxiv ID**: http://arxiv.org/abs/1802.06446v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.06446v1)
- **Published**: 2018-02-18 21:15:54+00:00
- **Updated**: 2018-02-18 21:15:54+00:00
- **Authors**: Jakob Weiss, Nicola Rieke, Mohammad Ali Nasseri, Mathias Maier, Abouzar Eslami, Nassir Navab
- **Comment**: None
- **Journal**: None
- **Summary**: Purpose. Intraoperative Optical Coherence Tomography (iOCT) is an increasingly available imaging technique for ophthalmic microsurgery that provides high-resolution cross-sectional information of the surgical scene. We propose to build on its desirable qualities and present a method for tracking the orientation and location of a surgical needle. Thereby, we enable direct analysis of instrument-tissue interaction directly in OCT space without complex multimodal calibration that would be required with traditional instrument tracking methods. Method. The intersection of the needle with the iOCT scan is detected by a peculiar multi-step ellipse fitting that takes advantage of the directionality of the modality. The geometric modelling allows us to use the ellipse parameters and provide them into a latency aware estimator to infer the 5DOF pose during needle movement. Results. Experiments on phantom data and ex-vivo porcine eyes indicate that the algorithm retains angular precision especially during lateral needle movement and provides a more robust and consistent estimation than baseline methods. Conclusion. Using solely crosssectional iOCT information, we are able to successfully and robustly estimate a 5DOF pose of the instrument in less than 5.5 ms on a CPU.



### DA-GAN: Instance-level Image Translation by Deep Attention Generative Adversarial Networks (with Supplementary Materials)
- **Arxiv ID**: http://arxiv.org/abs/1802.06454v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.06454v1)
- **Published**: 2018-02-18 22:16:19+00:00
- **Updated**: 2018-02-18 22:16:19+00:00
- **Authors**: Shuang Ma, Jianlong Fu, Chang Wen Chen, Tao Mei
- **Comment**: Computer Vision and Pattern Recognition
- **Journal**: None
- **Summary**: Unsupervised image translation, which aims in translating two independent sets of images, is challenging in discovering the correct correspondences without paired data. Existing works build upon Generative Adversarial Network (GAN) such that the distribution of the translated images are indistinguishable from the distribution of the target set. However, such set-level constraints cannot learn the instance-level correspondences (e.g. aligned semantic parts in object configuration task). This limitation often results in false positives (e.g. geometric or semantic artifacts), and further leads to mode collapse problem. To address the above issues, we propose a novel framework for instance-level image translation by Deep Attention GAN (DA-GAN). Such a design enables DA-GAN to decompose the task of translating samples from two sets into translating instances in a highly-structured latent space. Specifically, we jointly learn a deep attention encoder, and the instancelevel correspondences could be consequently discovered through attending on the learned instance pairs. Therefore, the constraints could be exploited on both set-level and instance-level. Comparisons against several state-ofthe- arts demonstrate the superiority of our approach, and the broad application capability, e.g, pose morphing, data augmentation, etc., pushes the margin of domain translation problem.



### Structured Label Inference for Visual Understanding
- **Arxiv ID**: http://arxiv.org/abs/1802.06459v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.06459v1)
- **Published**: 2018-02-18 22:37:32+00:00
- **Updated**: 2018-02-18 22:37:32+00:00
- **Authors**: Nelson Nauata, Hexiang Hu, Guang-Tong Zhou, Zhiwei Deng, Zicheng Liao, Greg Mori
- **Comment**: None
- **Journal**: None
- **Summary**: Visual data such as images and videos contain a rich source of structured semantic labels as well as a wide range of interacting components. Visual content could be assigned with fine-grained labels describing major components, coarse-grained labels depicting high level abstractions, or a set of labels revealing attributes. Such categorization over different, interacting layers of labels evinces the potential for a graph-based encoding of label information. In this paper, we exploit this rich structure for performing graph-based inference in label space for a number of tasks: multi-label image and video classification and action detection in untrimmed videos. We consider the use of the Bidirectional Inference Neural Network (BINN) and Structured Inference Neural Network (SINN) for performing graph-based inference in label space and propose a Long Short-Term Memory (LSTM) based extension for exploiting activity progression on untrimmed videos. The methods were evaluated on (i) the Animal with Attributes (AwA), Scene Understanding (SUN) and NUS-WIDE datasets for multi-label image classification, (ii) the first two releases of the YouTube-8M large scale dataset for multi-label video classification, and (iii) the THUMOS'14 and MultiTHUMOS video datasets for action detection. Our results demonstrate the effectiveness of structured label inference in these challenging tasks, achieving significant improvements against baselines.



### Robust Fitting in Computer Vision: Easy or Hard?
- **Arxiv ID**: http://arxiv.org/abs/1802.06464v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CC
- **Links**: [PDF](http://arxiv.org/pdf/1802.06464v3)
- **Published**: 2018-02-18 22:54:50+00:00
- **Updated**: 2018-07-23 05:42:28+00:00
- **Authors**: Tat-Jun Chin, Zhipeng Cai, Frank Neumann
- **Comment**: None
- **Journal**: European Conference on Computer Vision (ECCV), 2018
- **Summary**: Robust model fitting plays a vital role in computer vision, and research into algorithms for robust fitting continues to be active. Arguably the most popular paradigm for robust fitting in computer vision is consensus maximisation, which strives to find the model parameters that maximise the number of inliers. Despite the significant developments in algorithms for consensus maximisation, there has been a lack of fundamental analysis of the problem in the computer vision literature. In particular, whether consensus maximisation is "tractable" remains a question that has not been rigorously dealt with, thus making it difficult to assess and compare the performance of proposed algorithms, relative to what is theoretically achievable. To shed light on these issues, we present several computational hardness results for consensus maximisation. Our results underline the fundamental intractability of the problem, and resolve several ambiguities existing in the literature.



