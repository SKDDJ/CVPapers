# Arxiv Papers in cs.CV on 2018-02-21
### Density-aware Single Image De-raining using a Multi-stream Dense Network
- **Arxiv ID**: http://arxiv.org/abs/1802.07412v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.07412v1)
- **Published**: 2018-02-21 03:16:31+00:00
- **Updated**: 2018-02-21 03:16:31+00:00
- **Authors**: He Zhang, Vishal M. Patel
- **Comment**: Accepted in CVPR'18
- **Journal**: None
- **Summary**: Single image rain streak removal is an extremely challenging problem due to the presence of non-uniform rain densities in images. We present a novel density-aware multi-stream densely connected convolutional neural network-based algorithm, called DID-MDN, for joint rain density estimation and de-raining. The proposed method enables the network itself to automatically determine the rain-density information and then efficiently remove the corresponding rain-streaks guided by the estimated rain-density label. To better characterize rain-streaks with different scales and shapes, a multi-stream densely connected de-raining network is proposed which efficiently leverages features from different scales. Furthermore, a new dataset containing images with rain-density labels is created and used to train the proposed density-aware network. Extensive experiments on synthetic and real datasets demonstrate that the proposed method achieves significant improvements over the recent state-of-the-art methods. In addition, an ablation study is performed to demonstrate the improvements obtained by different modules in the proposed method. Code can be found at: https://github.com/hezhangsprinter



### Angle constrained path to cluster multiple manifolds
- **Arxiv ID**: http://arxiv.org/abs/1802.07416v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.07416v1)
- **Published**: 2018-02-21 03:51:48+00:00
- **Updated**: 2018-02-21 03:51:48+00:00
- **Authors**: Amir Babaeian
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a method to cluster multiple intersected manifolds. The algorithm chooses several landmark nodes randomly and then checks whether there is an angle constrained path between each landmark node and every other node in the neighborhood graph. When the points lie on different manifolds with intersection they should not be connected using a smooth path, thus the angle constraint is used to prevent connecting points from one cluster to another one. The resulting algorithm is implemented as a simple variation of Dijkstras algorithm used in Isomap. However, Isomap was specifically designed for dimensionality reduction in the single-manifold setting, and in particular, can-not handle intersections. Our method is simpler than the previous proposals in the literature and performs comparably to the best methods, both on simulated and some real datasets.



### Conditional Adversarial Synthesis of 3D Facial Action Units
- **Arxiv ID**: http://arxiv.org/abs/1802.07421v2
- **DOI**: 10.1016/j.neucom.2019.05.003
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.07421v2)
- **Published**: 2018-02-21 04:31:51+00:00
- **Updated**: 2018-03-15 02:03:44+00:00
- **Authors**: Zhilei Liu, Guoxian Song, Jianfei Cai, Tat-Jen Cham, Juyong Zhang
- **Comment**: None
- **Journal**: NeuroComputing 355 (2019) 200-208
- **Summary**: Employing deep learning-based approaches for fine-grained facial expression analysis, such as those involving the estimation of Action Unit (AU) intensities, is difficult due to the lack of a large-scale dataset of real faces with sufficiently diverse AU labels for training. In this paper, we consider how AU-level facial image synthesis can be used to substantially augment such a dataset. We propose an AU synthesis framework that combines the well-known 3D Morphable Model (3DMM), which intrinsically disentangles expression parameters from other face attributes, with models that adversarially generate 3DMM expression parameters conditioned on given target AU labels, in contrast to the more conventional approach of generating facial images directly. In this way, we are able to synthesize new combinations of expression parameters and facial images from desired AU labels. Extensive quantitative and qualitative results on the benchmark DISFA dataset demonstrate the effectiveness of our method on 3DMM facial expression parameter synthesis and data augmentation for deep learning-based AU intensity estimation.



### Binary Constrained Deep Hashing Network for Image Retrieval without Manual Annotation
- **Arxiv ID**: http://arxiv.org/abs/1802.07437v7
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.07437v7)
- **Published**: 2018-02-21 06:20:59+00:00
- **Updated**: 2018-12-18 00:17:41+00:00
- **Authors**: Thanh-Toan Do, Tuan Hoang, Dang-Khoa Le Tan, Trung Pham, Huu Le, Ngai-Man Cheung, Ian Reid
- **Comment**: Accepted to WACV 2019
- **Journal**: None
- **Summary**: Learning compact binary codes for image retrieval task using deep neural networks has attracted increasing attention recently. However, training deep hashing networks for the task is challenging due to the binary constraints on the hash codes, the similarity preserving property, and the requirement for a vast amount of labelled images. To the best of our knowledge, none of the existing methods has tackled all of these challenges completely in a unified framework. In this work, we propose a novel end-to-end deep learning approach for the task, in which the network is trained to produce binary codes directly from image pixels without the need of manual annotation. In particular, to deal with the non-smoothness of binary constraints, we propose a novel pairwise constrained loss function, which simultaneously encodes the distances between pairs of hash codes, and the binary quantization error. In order to train the network with the proposed loss function, we propose an efficient parameter learning algorithm. In addition, to provide similar / dissimilar training images to train the network, we exploit 3D models reconstructed from unlabelled images for automatic generation of enormous training image pairs. The extensive experiments on image retrieval benchmark datasets demonstrate the improvements of the proposed method over the state-of-the-art compact representation methods on the image retrieval problem.



### Learning to Play with Intrinsically-Motivated Self-Aware Agents
- **Arxiv ID**: http://arxiv.org/abs/1802.07442v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML, 68
- **Links**: [PDF](http://arxiv.org/pdf/1802.07442v2)
- **Published**: 2018-02-21 07:01:43+00:00
- **Updated**: 2018-10-30 20:08:46+00:00
- **Authors**: Nick Haber, Damian Mrowca, Li Fei-Fei, Daniel L. K. Yamins
- **Comment**: In NIPS 2018. 10 pages, 5 figures
- **Journal**: None
- **Summary**: Infants are experts at playing, with an amazing ability to generate novel structured behaviors in unstructured environments that lack clear extrinsic reward signals. We seek to mathematically formalize these abilities using a neural network that implements curiosity-driven intrinsic motivation. Using a simple but ecologically naturalistic simulated environment in which an agent can move and interact with objects it sees, we propose a "world-model" network that learns to predict the dynamic consequences of the agent's actions. Simultaneously, we train a separate explicit "self-model" that allows the agent to track the error map of its own world-model, and then uses the self-model to adversarially challenge the developing world-model. We demonstrate that this policy causes the agent to explore novel and informative interactions with its environment, leading to the generation of a spectrum of complex behaviors, including ego-motion prediction, object attention, and object gathering. Moreover, the world-model that the agent learns supports improved performance on object dynamics prediction, detection, localization and recognition tasks. Taken together, our results are initial steps toward creating flexible autonomous agents that self-supervise in complex novel physical environments.



### Load Balanced GANs for Multi-view Face Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1802.07447v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.07447v2)
- **Published**: 2018-02-21 07:10:36+00:00
- **Updated**: 2018-03-04 05:02:30+00:00
- **Authors**: Jie Cao, Yibo Hu, Bing Yu, Ran He, Zhenan Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-view face synthesis from a single image is an ill-posed problem and often suffers from serious appearance distortion. Producing photo-realistic and identity preserving multi-view results is still a not well defined synthesis problem. This paper proposes Load Balanced Generative Adversarial Networks (LB-GAN) to precisely rotate the yaw angle of an input face image to any specified angle. LB-GAN decomposes the challenging synthesis problem into two well constrained subtasks that correspond to a face normalizer and a face editor respectively. The normalizer first frontalizes an input image, and then the editor rotates the frontalized image to a desired pose guided by a remote code. In order to generate photo-realistic local details, the normalizer and the editor are trained in a two-stage manner and regulated by a conditional self-cycle loss and an attention based L2 loss. Exhaustive experiments on controlled and uncontrolled environments demonstrate that the proposed method not only improves the visual realism of multi-view synthetic images, but also preserves identity information well.



### Spatial Morphing Kernel Regression For Feature Interpolation
- **Arxiv ID**: http://arxiv.org/abs/1802.07452v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.07452v2)
- **Published**: 2018-02-21 07:30:51+00:00
- **Updated**: 2018-05-04 17:05:23+00:00
- **Authors**: Xueqing Deng, Yi Zhu, Shawn Newsam
- **Comment**: accepted by ICIP 2018
- **Journal**: None
- **Summary**: In recent years, geotagged social media has become popular as a novel source for geographic knowledge discovery. Ground-level images and videos provide a different perspective than overhead imagery and can be applied to a range of applications such as land use mapping, activity detection, pollution mapping, etc. The sparse and uneven distribution of this data presents a problem, however, for generating dense maps. We therefore investigate the problem of spatially interpolating the high-dimensional features extracted from sparse social media to enable dense labeling using standard classifiers. Further, we show how prior knowledge about region boundaries can be used to improve the interpolation through spatial morphing kernel regression. We show that an interpolate-then-classify framework can produce dense maps from sparse observations but that care must be taken in choosing the interpolation method. We also show that the spatial morphing kernel improves the results.



### Learning Image Conditioned Label Space for Multilabel Classification
- **Arxiv ID**: http://arxiv.org/abs/1802.07460v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.07460v1)
- **Published**: 2018-02-21 08:12:23+00:00
- **Updated**: 2018-02-21 08:12:23+00:00
- **Authors**: Yi-Nan Li, Mei-Chen Yeh
- **Comment**: None
- **Journal**: None
- **Summary**: This work addresses the task of multilabel image classification. Inspired by the great success from deep convolutional neural networks (CNNs) for single-label visual-semantic embedding, we exploit extending these models for multilabel images. Specifically, we propose an image-dependent ranking model, which returns a ranked list of labels according to its relevance to the input image. In contrast to conventional CNN models that learn an image representation (i.e. the image embedding vector), the developed model learns a mapping (i.e. a transformation matrix) from an image in an attempt to differentiate between its relevant and irrelevant labels. Despite the conceptual simplicity of our approach, experimental results on a public benchmark dataset demonstrate that the proposed model achieves state-of-the-art performance while using fewer training images than other multilabel classification methods.



### Emergence of Structured Behaviors from Curiosity-Based Intrinsic Motivation
- **Arxiv ID**: http://arxiv.org/abs/1802.07461v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML, 68
- **Links**: [PDF](http://arxiv.org/pdf/1802.07461v1)
- **Published**: 2018-02-21 08:13:12+00:00
- **Updated**: 2018-02-21 08:13:12+00:00
- **Authors**: Nick Haber, Damian Mrowca, Li Fei-Fei, Daniel L. K. Yamins
- **Comment**: 6 pages, 5 figures
- **Journal**: None
- **Summary**: Infants are experts at playing, with an amazing ability to generate novel structured behaviors in unstructured environments that lack clear extrinsic reward signals. We seek to replicate some of these abilities with a neural network that implements curiosity-driven intrinsic motivation. Using a simple but ecologically naturalistic simulated environment in which the agent can move and interact with objects it sees, the agent learns a world model predicting the dynamic consequences of its actions. Simultaneously, the agent learns to take actions that adversarially challenge the developing world model, pushing the agent to explore novel and informative interactions with its environment. We demonstrate that this policy leads to the self-supervised emergence of a spectrum of complex behaviors, including ego motion prediction, object attention, and object gathering. Moreover, the world model that the agent learns supports improved performance on object dynamics prediction and localization tasks. Our results are a proof-of-principle that computational models of intrinsic motivation might account for key features of developmental visuomotor learning in infants.



### Multiclass Weighted Loss for Instance Segmentation of Cluttered Cells
- **Arxiv ID**: http://arxiv.org/abs/1802.07465v1
- **DOI**: 10.1109/ICIP.2018.8451187
- **Categories**: **cs.CV**, I.4.6; I.2.10; J.3
- **Links**: [PDF](http://arxiv.org/pdf/1802.07465v1)
- **Published**: 2018-02-21 08:41:48+00:00
- **Updated**: 2018-02-21 08:41:48+00:00
- **Authors**: Fidel A. Guerrero-Pena, Pedro D. Marrero Fernandez, Tsang Ing Ren, Mary Yui, Ellen Rothenberg, Alexandre Cunha
- **Comment**: Submitted to IEEE ICIP 2018
- **Journal**: None
- **Summary**: We propose a new multiclass weighted loss function for instance segmentation of cluttered cells. We are primarily motivated by the need of developmental biologists to quantify and model the behavior of blood T-cells which might help us in understanding their regulation mechanisms and ultimately help researchers in their quest for developing an effective immuno-therapy cancer treatment. Segmenting individual touching cells in cluttered regions is challenging as the feature distribution on shared borders and cell foreground are similar thus difficulting discriminating pixels into proper classes. We present two novel weight maps applied to the weighted cross entropy loss function which take into account both class imbalance and cell geometry. Binary ground truth training data is augmented so the learning model can handle not only foreground and background but also a third touching class. This framework allows training using U-Net. Experiments with our formulations have shown superior results when compared to other similar schemes, outperforming binary class models with significant improvement of boundary adequacy and instance detection. We validate our results on manually annotated microscope images of T-cells.



### ViTac: Feature Sharing between Vision and Tactile Sensing for Cloth Texture Recognition
- **Arxiv ID**: http://arxiv.org/abs/1802.07490v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1802.07490v2)
- **Published**: 2018-02-21 10:06:14+00:00
- **Updated**: 2018-03-13 14:57:30+00:00
- **Authors**: Shan Luo, Wenzhen Yuan, Edward Adelson, Anthony G. Cohn, Raul Fuentes
- **Comment**: 6 pages, 5 figures, Accepted for 2018 IEEE International Conference
  on Robotics and Automation
- **Journal**: None
- **Summary**: Vision and touch are two of the important sensing modalities for humans and they offer complementary information for sensing the environment. Robots could also benefit from such multi-modal sensing ability. In this paper, addressing for the first time (to the best of our knowledge) texture recognition from tactile images and vision, we propose a new fusion method named Deep Maximum Covariance Analysis (DMCA) to learn a joint latent space for sharing features through vision and tactile sensing. The features of camera images and tactile data acquired from a GelSight sensor are learned by deep neural networks. But the learned features are of a high dimensionality and are redundant due to the differences between the two sensing modalities, which deteriorates the perception performance. To address this, the learned features are paired using maximum covariance analysis. Results of the algorithm on a newly collected dataset of paired visual and tactile data relating to cloth textures show that a good recognition performance of greater than 90\% can be achieved by using the proposed DMCA framework. In addition, we find that the perception performance of either vision or tactile sensing can be improved by employing the shared representation space, compared to learning from unimodal data.



### Density Weighted Connectivity of Grass Pixels in Image Frames for Biomass Estimation
- **Arxiv ID**: http://arxiv.org/abs/1802.07512v1
- **DOI**: 10.1016/j.eswa.2018.01.055
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1802.07512v1)
- **Published**: 2018-02-21 11:07:05+00:00
- **Updated**: 2018-02-21 11:07:05+00:00
- **Authors**: Ligang Zhang, Brijesh Verma, David Stockwell, Sujan Chowdhury
- **Comment**: 28 pages, accepted manuscript, Expert Systems with Applications
- **Journal**: None
- **Summary**: Accurate estimation of the biomass of roadside grasses plays a significant role in applications such as fire-prone region identification. Current solutions heavily depend on field surveys, remote sensing measurements and image processing using reference markers, which often demand big investments of time, effort and cost. This paper proposes Density Weighted Connectivity of Grass Pixels (DWCGP) to automatically estimate grass biomass from roadside image data. The DWCGP calculates the length of continuously connected grass pixels along a vertical orientation in each image column, and then weights the length by the grass density in a surrounding region of the column. Grass pixels are classified using feedforward artificial neural networks and the dominant texture orientation at every pixel is computed using multi-orientation Gabor wavelet filter vote. Evaluations on a field survey dataset show that the DWCGP reduces Root-Mean-Square Error from 5.84 to 5.52 by additionally considering grass density on top of grass height. The DWCGP shows robustness to non-vertical grass stems and to changes of both Gabor filter parameters and surrounding region widths. It also has performance close to human observation and higher than eight baseline approaches, as well as promising results for classifying low vs. high fire risk and identifying fire-prone road regions.



### Discriminative Label Consistent Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1802.08077v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.08077v1)
- **Published**: 2018-02-21 13:30:52+00:00
- **Updated**: 2018-02-21 13:30:52+00:00
- **Authors**: Lingkun Luo, Liming Chen, Ying lu, Shiqiang Hu
- **Comment**: 12 pages, 7 figures. arXiv admin note: text overlap with
  arXiv:1712.10042
- **Journal**: None
- **Summary**: Domain adaptation (DA) is transfer learning which aims to learn an effective predictor on target data from source data despite data distribution mismatch between source and target. We present in this paper a novel unsupervised DA method for cross-domain visual recognition which simultaneously optimizes the three terms of a theoretically established error bound. Specifically, the proposed DA method iteratively searches a latent shared feature subspace where not only the divergence of data distributions between the source domain and the target domain is decreased as most state-of-the-art DA methods do, but also the inter-class distances are increased to facilitate discriminative learning. Moreover, the proposed DA method sparsely regresses class labels from the features achieved in the shared subspace while minimizing the prediction errors on the source data and ensuring label consistency between source and target. Data outliers are also accounted for to further avoid negative knowledge transfer. Comprehensive experiments and in-depth analysis verify the effectiveness of the proposed DA method which consistently outperforms the state-of-the-art DA methods on standard DA benchmarks, i.e., 12 cross-domain image classification tasks.



### DeepASL: Enabling Ubiquitous and Non-Intrusive Word and Sentence-Level Sign Language Translation
- **Arxiv ID**: http://arxiv.org/abs/1802.07584v3
- **DOI**: 10.1145/3131672.3131693
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.07584v3)
- **Published**: 2018-02-21 14:29:36+00:00
- **Updated**: 2018-10-18 00:21:43+00:00
- **Authors**: Biyi Fang, Jillian Co, Mi Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: There is an undeniable communication barrier between deaf people and people with normal hearing ability. Although innovations in sign language translation technology aim to tear down this communication barrier, the majority of existing sign language translation systems are either intrusive or constrained by resolution or ambient lighting conditions. Moreover, these existing systems can only perform single-sign ASL translation rather than sentence-level translation, making them much less useful in daily-life communication scenarios. In this work, we fill this critical gap by presenting DeepASL, a transformative deep learning-based sign language translation technology that enables ubiquitous and non-intrusive American Sign Language (ASL) translation at both word and sentence levels. DeepASL uses infrared light as its sensing mechanism to non-intrusively capture the ASL signs. It incorporates a novel hierarchical bidirectional deep recurrent neural network (HB-RNN) and a probabilistic framework based on Connectionist Temporal Classification (CTC) for word-level and sentence-level ASL translation respectively. To evaluate its performance, we have collected 7,306 samples from 11 participants, covering 56 commonly used ASL words and 100 ASL sentences. DeepASL achieves an average 94.5% word-level translation accuracy and an average 8.2% word error rate on translating unseen ASL sentences. Given its promising performance, we believe DeepASL represents a significant step towards breaking the communication barrier between deaf people and hearing majority, and thus has the significant potential to fundamentally change deaf people's lives.



### Collaboratively Weighting Deep and Classic Representation via L2 Regularization for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1802.07589v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.07589v2)
- **Published**: 2018-02-21 14:46:48+00:00
- **Updated**: 2018-11-17 10:18:00+00:00
- **Authors**: Shaoning Zeng, Bob Zhang, Yanghao Zhang, Jianping Gou
- **Comment**: ACML 2018
- **Journal**: PMLR Volume 95: Asian Conference on Machine Learning, 14-16
  November 2018
- **Summary**: Deep convolutional neural networks provide a powerful feature learning capability for image classification. The deep image features can be utilized to deal with many image understanding tasks like image classification and object recognition. However, the robustness obtained in one dataset can be hardly reproduced in the other domain, which leads to inefficient models far from state-of-the-art. We propose a deep collaborative weight-based classification (DeepCWC) method to resolve this problem, by providing a novel option to fully take advantage of deep features in classic machine learning. It firstly performs the L2-norm based collaborative representation on the original images, as well as the deep features extracted by deep CNN models. Then, two distance vectors, obtained based on the pair of linear representations, are fused together via a novel collaborative weight. This collaborative weight enables deep and classic representations to weigh each other. We observed the complementarity between two representations in a series of experiments on 10 facial and object datasets. The proposed DeepCWC produces very promising classification results, and outperforms many other benchmark methods, especially the ones claimed for Fashion-MNIST. The code is going to be published in our public repository.



### Batch Normalization and the impact of batch structure on the behavior of deep convolution networks
- **Arxiv ID**: http://arxiv.org/abs/1802.07590v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.07590v1)
- **Published**: 2018-02-21 14:47:18+00:00
- **Updated**: 2018-02-21 14:47:18+00:00
- **Authors**: Mohamed Hajaj, Duncan Gillies
- **Comment**: None
- **Journal**: None
- **Summary**: Batch normalization was introduced in 2015 to speed up training of deep convolution networks by normalizing the activations across the current batch to have zero mean and unity variance. The results presented here show an interesting aspect of batch normalization, where controlling the shape of the training batches can influence what the network will learn. If training batches are structured as balanced batches (one image per class), and inference is also carried out on balanced test batches, using the batch's own means and variances, then the conditional results will improve considerably. The network uses the strong information about easy images in a balanced batch, and propagates it through the shared means and variances to help decide the identity of harder images on the same batch. Balancing the test batches requires the labels of the test images, which are not available in practice, however further investigation can be done using batch structures that are less strict and might not require the test image labels. The conditional results show the error rate almost reduced to zero for nontrivial datasets with small number of classes such as the CIFAR10.



### Explanations based on the Missing: Towards Contrastive Explanations with Pertinent Negatives
- **Arxiv ID**: http://arxiv.org/abs/1802.07623v2
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1802.07623v2)
- **Published**: 2018-02-21 15:51:38+00:00
- **Updated**: 2018-10-29 16:08:36+00:00
- **Authors**: Amit Dhurandhar, Pin-Yu Chen, Ronny Luss, Chun-Chen Tu, Paishun Ting, Karthikeyan Shanmugam, Payel Das
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we propose a novel method that provides contrastive explanations justifying the classification of an input by a black box classifier such as a deep neural network. Given an input we find what should be %necessarily and minimally and sufficiently present (viz. important object pixels in an image) to justify its classification and analogously what should be minimally and necessarily \emph{absent} (viz. certain background pixels). We argue that such explanations are natural for humans and are used commonly in domains such as health care and criminology. What is minimally but critically \emph{absent} is an important part of an explanation, which to the best of our knowledge, has not been explicitly identified by current explanation methods that explain predictions of neural networks. We validate our approach on three real datasets obtained from diverse domains; namely, a handwritten digits dataset MNIST, a large procurement fraud dataset and a brain activity strength dataset. In all three cases, we witness the power of our approach in generating precise explanations that are also easy for human experts to understand and evaluate.



### Scalable and Robust Sparse Subspace Clustering Using Randomized Clustering and Multilayer Graphs
- **Arxiv ID**: http://arxiv.org/abs/1802.07648v2
- **DOI**: 10.1016/j.sigpro.2019.05.017
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1802.07648v2)
- **Published**: 2018-02-21 16:21:42+00:00
- **Updated**: 2018-02-23 06:59:12+00:00
- **Authors**: Maryam Abdolali, Nicolas Gillis, Mohammad Rahmati
- **Comment**: 25 pages, v2: typos corrected
- **Journal**: Signal Processing 163, pp. 166-180, 2019
- **Summary**: Sparse subspace clustering (SSC) is one of the current state-of-the-art methods for partitioning data points into the union of subspaces, with strong theoretical guarantees. However, it is not practical for large data sets as it requires solving a LASSO problem for each data point, where the number of variables in each LASSO problem is the number of data points. To improve the scalability of SSC, we propose to select a few sets of anchor points using a randomized hierarchical clustering method, and, for each set of anchor points, solve the LASSO problems for each data point allowing only anchor points to have a non-zero weight (this reduces drastically the number of variables). This generates a multilayer graph where each layer corresponds to a different set of anchor points. Using the Grassmann manifold of orthogonal matrices, the shared connectivity among the layers is summarized within a single subspace. Finally, we use $k$-means clustering within that subspace to cluster the data points, similarly as done by spectral clustering in SSC. We show on both synthetic and real-world data sets that the proposed method not only allows SSC to scale to large-scale data sets, but that it is also much more robust as it performs significantly better on noisy data and on data with close susbspaces and outliers, while it is not prone to oversegmentation.



### Building Efficient ConvNets using Redundant Feature Pruning
- **Arxiv ID**: http://arxiv.org/abs/1802.07653v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.07653v1)
- **Published**: 2018-02-21 16:31:28+00:00
- **Updated**: 2018-02-21 16:31:28+00:00
- **Authors**: Babajide O. Ayinde, Jacek M. Zurada
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents an efficient technique to prune deep and/or wide convolutional neural network models by eliminating redundant features (or filters). Previous studies have shown that over-sized deep neural network models tend to produce a lot of redundant features that are either shifted version of one another or are very similar and show little or no variations; thus resulting in filtering redundancy. We propose to prune these redundant features along with their connecting feature maps according to their differentiation and based on their relative cosine distances in the feature space, thus yielding smaller network size with reduced inference costs and competitive performance. We empirically show on select models and CIFAR-10 dataset that inference costs can be reduced by 40% for VGG-16, 27% for ResNet-56, and 39% for ResNet-110.



### Learning Multiple Categories on Deep Convolution Networks
- **Arxiv ID**: http://arxiv.org/abs/1802.07672v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.07672v1)
- **Published**: 2018-02-21 17:11:31+00:00
- **Updated**: 2018-02-21 17:11:31+00:00
- **Authors**: Mohamed Hajaj, Duncan Gillies
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolution networks have proved very successful with big datasets such as the 1000-classes ImageNet. Results show that the error rate increases slowly as the size of the dataset increases. Experiments presented here may explain why these networks are very effective in solving big recognition problems. If the big task is made up of multiple smaller tasks, then the results show the ability of deep convolution networks to decompose the complex task into a number of smaller tasks and to learn them simultaneously. The results show that the performance of solving the big task on a single network is very close to the average performance of solving each of the smaller tasks on a separate network. Experiments also show the advantage of using task specific or category labels in combination with class labels.



### Stochastic Video Generation with a Learned Prior
- **Arxiv ID**: http://arxiv.org/abs/1802.07687v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, stat.ML, I.2.6; I.4
- **Links**: [PDF](http://arxiv.org/pdf/1802.07687v2)
- **Published**: 2018-02-21 17:36:27+00:00
- **Updated**: 2018-03-02 17:39:23+00:00
- **Authors**: Emily Denton, Rob Fergus
- **Comment**: None
- **Journal**: None
- **Summary**: Generating video frames that accurately predict future world states is challenging. Existing approaches either fail to capture the full distribution of outcomes, or yield blurry generations, or both. In this paper we introduce an unsupervised video generation model that learns a prior model of uncertainty in a given environment. Video frames are generated by drawing samples from this prior and combining them with a deterministic estimate of the future frame. The approach is simple and easily trained end-to-end on a variety of datasets. Sample generations are both varied and sharp, even many frames into the future, and compare favorably to those from existing approaches.



### Lossless Compression of Angiogram Foreground with Visual Quality Preservation of Background
- **Arxiv ID**: http://arxiv.org/abs/1802.07769v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.07769v1)
- **Published**: 2018-02-21 19:42:50+00:00
- **Updated**: 2018-02-21 19:42:50+00:00
- **Authors**: Mahdi Ahmadi, Ali Emami, Mohsen Hajabdollahi, S. M. Reza Soroushmehr, Nader Karimi, Shadrokh Samavi, Kayvan Najarian
- **Comment**: 4 pages , 7 figures
- **Journal**: None
- **Summary**: By increasing the volume of telemedicine information, the need for medical image compression has become more important. In angiographic images, a small ratio of the entire image usually belongs to the vasculature that provides crucial information for diagnosis. Other parts of the image are diagnostically less important and can be compressed with higher compression ratio. However, the quality of those parts affect the visual perception of the image as well. Existing methods compress foreground and background of angiographic images using different techniques. In this paper we first utilize convolutional neural network to segment vessels and then represent a hierarchical block processing algorithm capable of both eliminating the background redundancies and preserving the overall visual quality of angiograms.



### Generalizable Adversarial Examples Detection Based on Bi-model Decision Mismatch
- **Arxiv ID**: http://arxiv.org/abs/1802.07770v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.07770v3)
- **Published**: 2018-02-21 19:43:08+00:00
- **Updated**: 2019-04-22 20:29:57+00:00
- **Authors**: João Monteiro, Isabela Albuquerque, Zahid Akhtar, Tiago H. Falk
- **Comment**: None
- **Journal**: None
- **Summary**: Modern applications of artificial neural networks have yielded remarkable performance gains in a wide range of tasks. However, recent studies have discovered that such modelling strategy is vulnerable to Adversarial Examples, i.e. examples with subtle perturbations often too small and imperceptible to humans, but that can easily fool neural networks. Defense techniques against adversarial examples have been proposed, but ensuring robust performance against varying or novel types of attacks remains an open problem. In this work, we focus on the detection setting, in which case attackers become identifiable while models remain vulnerable. Particularly, we employ the decision layer of independently trained models as features for posterior detection. The proposed framework does not require any prior knowledge of adversarial examples generation techniques, and can be directly employed along with unmodified off-the-shelf models. Experiments on the standard MNIST and CIFAR10 datasets deliver empirical evidence that such detection approach generalizes well across not only different adversarial examples generation methods but also quality degradation attacks. Non-linear binary classifiers trained on top of our proposed features can achieve a high detection rate (>90%) in a set of white-box attacks and maintain such performance when tested against unseen attacks.



### ViS-HuD: Using Visual Saliency to Improve Human Detection with Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1803.01687v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.01687v3)
- **Published**: 2018-02-21 19:57:37+00:00
- **Updated**: 2018-04-18 06:42:46+00:00
- **Authors**: Vandit Gajjar, Yash Khandhediya, Ayesha Gurnani, Viraj Mavani, Mehul S. Raval
- **Comment**: 9 Pages, 10 Figures, 2 Tables; Accepted to MBCC Workshop in
  Conjunction with CVPR-2018
- **Journal**: None
- **Summary**: The paper presents a technique to improve human detection in still images using deep learning. Our novel method, ViS-HuD, computes visual saliency map from the image. Then the input image is multiplied by the map and product is fed to the Convolutional Neural Network (CNN) which detects humans in the image. A visual saliency map is generated using ML-Net and human detection is carried out using DetectNet. ML-Net is pre-trained on SALICON while, DetectNet is pre-trained on ImageNet database for visual saliency detection and image classification respectively. The CNNs of ViS-HuD were trained on two challenging databases - Penn Fudan and TUD-Brussels Benchmark. Experimental results demonstrate that the proposed method achieves state-of-the-art performance on Penn Fudan Dataset with 91.4% human detection accuracy and it achieves average miss-rate of 53% on the TUDBrussels benchmark.



### Left Ventricle Segmentation in Cardiac MR Images Using Fully Convolutional Network
- **Arxiv ID**: http://arxiv.org/abs/1802.07778v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.07778v1)
- **Published**: 2018-02-21 20:01:35+00:00
- **Updated**: 2018-02-21 20:01:35+00:00
- **Authors**: Mina Nasr-Esfahani, Majid Mohrekesh, Mojtaba Akbari, S. M. Reza Soroushmehr, Ebrahim Nasr-Esfahani, Nader Karimi, Shadrokh Samavi, Kayvan Najarian
- **Comment**: 4 pages, 3 figures
- **Journal**: None
- **Summary**: Medical image analysis, especially segmenting a specific organ, has an important role in developing clinical decision support systems. In cardiac magnetic resonance (MR) imaging, segmenting the left and right ventricles helps physicians diagnose different heart abnormalities. There are challenges for this task, including the intensity and shape similarity between left ventricle and other organs, inaccurate boundaries and presence of noise in most of the images. In this paper we propose an automated method for segmenting the left ventricle in cardiac MR images. We first automatically extract the region of interest, and then employ it as an input of a fully convolutional network. We train the network accurately despite the small number of left ventricle pixels in comparison with the whole image. Thresholding on the output map of the fully convolutional network and selection of regions based on their roundness are performed in our proposed post-processing phase. The Dice score of our method reaches 87.24% by applying this algorithm on the York dataset of heart images.



### Lossless Image Compression Algorithm for Wireless Capsule Endoscopy by Content-Based Classification of Image Blocks
- **Arxiv ID**: http://arxiv.org/abs/1802.07781v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.07781v1)
- **Published**: 2018-02-21 20:09:46+00:00
- **Updated**: 2018-02-21 20:09:46+00:00
- **Authors**: Atefe Rajaeefar, Ali Emami, S. M. Reza Soroushmehr, Nader Karimi, Shadrokh Samavi, Kayvan Najarian
- **Comment**: 4 pages, 5 figures
- **Journal**: None
- **Summary**: Recent advances in capsule endoscopy systems have introduced new methods and capabilities. The capsule endoscopy system, by observing the entire digestive tract, has significantly improved diagnosing gastrointestinal disorders and diseases. The system has challenges such as the need to enhance the quality of the transmitted images, low frame rates of transmission, and battery lifetime that need to be addressed. One of the important parts of a capsule endoscopy system is the image compression unit. Better compression of images increases the frame rate and hence improves the diagnosis process. In this paper a high precision compression algorithm with high compression ratio is proposed. In this algorithm we use the similarity between frames to compress the data more efficiently.



### Reversible Image Watermarking for Health Informatics Systems Using Distortion Compensation in Wavelet Domain
- **Arxiv ID**: http://arxiv.org/abs/1802.07786v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.07786v1)
- **Published**: 2018-02-21 20:17:39+00:00
- **Updated**: 2018-02-21 20:17:39+00:00
- **Authors**: Hamidreza Zarrabi, Mohsen Hajabdollahi, S. M. Reza Soroushmehr, Nader Karimi, Shadrokh Samavi, Kayvan Najarian
- **Comment**: 4 pages, 5 figures
- **Journal**: None
- **Summary**: Reversible image watermarking guaranties restoration of both original cover and watermark logo from the watermarked image. Capacity and distortion of the image under reversible watermarking are two important parameters. In this study a reversible watermarking is investigated with focusing on increasing the embedding capacity and reducing the distortion in medical images. Integer wavelet transform is used for embedding where in each iteration, one watermark bit is embedded in one transform coefficient. We devise a novel approach that when a coefficient is modified in an iteration, the produced distortion is compensated in the next iteration. This distortion compensation method would result in low distortion rate. The proposed method is tested on four types of medical images including MRI of brain, cardiac MRI, MRI of breast, and intestinal polyp images. Using a one-level wavelet transform, maximum capacity of 1.5 BPP is obtained. Experimental results demonstrate that the proposed method is superior to the state-of-the-art works in terms of capacity and distortion.



### Segmentation of Bleeding Regions in Wireless Capsule Endoscopy Images an Approach for inside Capsule Video Summarization
- **Arxiv ID**: http://arxiv.org/abs/1802.07788v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.07788v1)
- **Published**: 2018-02-21 20:27:21+00:00
- **Updated**: 2018-02-21 20:27:21+00:00
- **Authors**: Mohsen Hajabdollahi, Reza Esfandiarpoor, S. M. Reza Soroushmehr, Nader Karimi, Shadrokh Samavi, Kayvan Najarian
- **Comment**: 4 pages, 3 figures
- **Journal**: None
- **Summary**: Wireless capsule endoscopy (WCE) is an effective means of diagnosis of gastrointestinal disorders. Detection of informative scenes by WCE could reduce the length of transmitted videos and can help with the diagnosis. In this paper we propose a simple and efficient method for segmentation of the bleeding regions in WCE captured images. Suitable color channels are selected and classified by a multi-layer perceptron (MLP) structure. The MLP structure is quantized such that the implementation does not require multiplications. The proposed method is tested by simulation on WCE bleeding image dataset. The proposed structure is designed considering hardware resource constrains that exist in WCE systems.



### Semantic Segmentation Refinement by Monte Carlo Region Growing of High Confidence Detections
- **Arxiv ID**: http://arxiv.org/abs/1802.07789v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.07789v1)
- **Published**: 2018-02-21 20:29:12+00:00
- **Updated**: 2018-02-21 20:29:12+00:00
- **Authors**: Philipe A. Dias, Henry Medeiros
- **Comment**: None
- **Journal**: None
- **Summary**: Despite recent improvements using fully convolutional networks, in general, the segmentation produced by most state-of-the-art semantic segmentation methods does not show satisfactory adherence to the object boundaries. We propose a method to refine the segmentation results generated by such deep learning models. Our method takes as input the confidence scores generated by a pixel-dense segmentation network and re-labels pixels with low confidence levels. The re-labeling approach employs a region growing mechanism that aggregates these pixels to neighboring areas with high confidence scores and similar appearance. In order to correct the labels of pixels that were incorrectly classified with high confidence level by the semantic segmentation algorithm, we generate multiple region growing steps through a Monte Carlo sampling of the seeds of the regions. Our method improves the accuracy of a state-of-the-art fully convolutional semantic segmentation approach on the publicly available COCO and PASCAL datasets, and it shows significantly better results on selected sequences of the finely-annotated DAVIS dataset.



### Liver Segmentation in Abdominal CT Images by Adaptive 3D Region Growing
- **Arxiv ID**: http://arxiv.org/abs/1802.07794v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.07794v2)
- **Published**: 2018-02-21 20:35:18+00:00
- **Updated**: 2019-02-05 18:32:18+00:00
- **Authors**: Shima Rafiei, Nader Karimi, Behzad Mirmahboub, S. M. Reza Soroushmehr, Banafsheh Felfelian, Shadrokh Samavi, Kayvan Najarian
- **Comment**: Table 1 of the paper contains comparisons and results that are not
  correct
- **Journal**: None
- **Summary**: Automatic liver segmentation plays an important role in computer-aided diagnosis and treatment. Manual segmentation of organs is a difficult and tedious task and so prone to human errors. In this paper, we propose an adaptive 3D region growing with subject-specific conditions. For this aim we use the intensity distribution of most probable voxels in prior map along with location prior. We also incorporate the boundary of target organs to restrict the region growing. In order to obtain strong edges and high contrast, we propose an effective contrast enhancement algorithm to facilitate more accurate segmentation. In this paper, 92.56% Dice score is achieved. We compare our method with the method of hard thresholding on Deeds prior map and also with the majority voting on Deeds registration with 13 organs.



### Continuous Relaxation of MAP Inference: A Nonconvex Perspective
- **Arxiv ID**: http://arxiv.org/abs/1802.07796v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1802.07796v2)
- **Published**: 2018-02-21 20:42:58+00:00
- **Updated**: 2018-02-25 22:12:55+00:00
- **Authors**: D. Khuê Lê-Huu, Nikos Paragios
- **Comment**: Accepted for publication at the 2018 IEEE Conference on Computer
  Vision and Pattern Recognition (CVPR)
- **Journal**: None
- **Summary**: In this paper, we study a nonconvex continuous relaxation of MAP inference in discrete Markov random fields (MRFs). We show that for arbitrary MRFs, this relaxation is tight, and a discrete stationary point of it can be easily reached by a simple block coordinate descent algorithm. In addition, we study the resolution of this relaxation using popular gradient methods, and further propose a more effective solution using a multilinear decomposition framework based on the alternating direction method of multipliers (ADMM). Experiments on many real-world problems demonstrate that the proposed ADMM significantly outperforms other nonconvex relaxation based methods, and compares favorably with state of the art MRF optimization algorithms in different settings.



### Liver segmentation in CT images using three dimensional to two dimensional fully convolutional network
- **Arxiv ID**: http://arxiv.org/abs/1802.07800v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.07800v2)
- **Published**: 2018-02-21 20:50:48+00:00
- **Updated**: 2018-03-03 15:55:53+00:00
- **Authors**: Shima Rafiei, Ebrahim Nasr-Esfahani, S. M. Reza Soroushmehr, Nader Karimi, Shadrokh Samavi, Kayvan Najarian
- **Comment**: 5 pages, 2 figures
- **Journal**: None
- **Summary**: The need for CT scan analysis is growing for pre-diagnosis and therapy of abdominal organs. Automatic organ segmentation of abdominal CT scan can help radiologists analyze the scans faster and segment organ images with fewer errors. However, existing methods are not efficient enough to perform the segmentation process for victims of accidents and emergencies situations. In this paper we propose an efficient liver segmentation with our 3D to 2D fully connected network (3D-2D-FCN). The segmented mask is enhanced by means of conditional random field on the organ's border. Consequently, we segment a target liver in less than a minute with Dice score of 93.52.



### Low complexity convolutional neural network for vessel segmentation in portable retinal diagnostic devices
- **Arxiv ID**: http://arxiv.org/abs/1802.07804v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.07804v1)
- **Published**: 2018-02-21 21:01:22+00:00
- **Updated**: 2018-02-21 21:01:22+00:00
- **Authors**: M. Hajabdollahi, R. Esfandiarpoor, S. M. R. Soroushmehr, N. Karimi, S. Samavi, K. Najarian
- **Comment**: 5 pages, 6 figures
- **Journal**: None
- **Summary**: Retinal vessel information is helpful in retinal disease screening and diagnosis. Retinal vessel segmentation provides useful information about vessels and can be used by physicians during intraocular surgery and retinal diagnostic operations. Convolutional neural networks (CNNs) are powerful tools for classification and segmentation of medical images. Complexity of CNNs makes it difficult to implement them in portable devices such as binocular indirect ophthalmoscopes. In this paper a simplification approach is proposed for CNNs based on combination of quantization and pruning. Fully connected layers are quantized and convolutional layers are pruned to have a simple and efficient network structure. Experiments on images of the STARE dataset show that our simplified network is able to segment retinal vessels with acceptable accuracy and low complexity.



### Detecting Small, Densely Distributed Objects with Filter-Amplifier Networks and Loss Boosting
- **Arxiv ID**: http://arxiv.org/abs/1802.07845v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.07845v2)
- **Published**: 2018-02-21 23:17:36+00:00
- **Updated**: 2018-05-07 06:17:11+00:00
- **Authors**: Zhenhua Chen, David Crandall, Robert Templeman
- **Comment**: rejected by a conference
- **Journal**: None
- **Summary**: Detecting small, densely distributed objects is a significant challenge: small objects often contain less distinctive information compared to larger ones, and finer-grained precision of bounding box boundaries are required. In this paper, we propose two techniques for addressing this problem. First, we estimate the likelihood that each pixel belongs to an object boundary rather than predicting coordinates of bounding boxes (as YOLO, Faster-RCNN and SSD do), by proposing a new architecture called Filter-Amplifier Networks (FANs). Second, we introduce a technique called Loss Boosting (LB) which attempts to soften the loss imbalance problem on each image. We test our algorithm on the problem of detecting electrical components on a new, realistic, diverse dataset of printed circuit boards (PCBs), as well as the problem of detecting vehicles in the Vehicle Detection in Aerial Imagery (VEDAI) dataset.   Experiments show that our method works significantly better than current state-of-the-art algorithms with respect to accuracy, recall and average IoU.



### Cross-Modality Synthesis from CT to PET using FCN and GAN Networks for Improved Automated Lesion Detection
- **Arxiv ID**: http://arxiv.org/abs/1802.07846v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1802.07846v2)
- **Published**: 2018-02-21 23:25:19+00:00
- **Updated**: 2018-07-23 09:14:59+00:00
- **Authors**: Avi Ben-Cohen, Eyal Klang, Stephen P. Raskin, Shelly Soffer, Simona Ben-Haim, Eli Konen, Michal Marianne Amitai, Hayit Greenspan
- **Comment**: Preprint submitted to Engineering applications of artificial
  intelligence
- **Journal**: None
- **Summary**: In this work we present a novel system for generation of virtual PET images using CT scans. We combine a fully convolutional network (FCN) with a conditional generative adversarial network (GAN) to generate simulated PET data from given input CT data. The synthesized PET can be used for false-positive reduction in lesion detection solutions. Clinically, such solutions may enable lesion detection and drug treatment evaluation in a CT-only environment, thus reducing the need for the more expensive and radioactive PET/CT scan. Our dataset includes 60 PET/CT scans from Sheba Medical center. We used 23 scans for training and 37 for testing. Different schemes to achieve the synthesized output were qualitatively compared. Quantitative evaluation was conducted using an existing lesion detection software, combining the synthesized PET as a false positive reduction layer for the detection of malignant lesions in the liver. Current results look promising showing a 28% reduction in the average false positive per case from 2.9 to 2.1. The suggested solution is comprehensive and can be expanded to additional body organs, and different modalities.



