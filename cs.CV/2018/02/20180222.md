# Arxiv Papers in cs.CV on 2018-02-22
### Driver Hand Localization and Grasp Analysis: A Vision-based Real-time Approach
- **Arxiv ID**: http://arxiv.org/abs/1802.07854v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.07854v1)
- **Published**: 2018-02-22 00:14:49+00:00
- **Updated**: 2018-02-22 00:14:49+00:00
- **Authors**: Siddharth, Akshay Rangesh, Eshed Ohn-Bar, Mohan M. Trivedi
- **Comment**: None
- **Journal**: None
- **Summary**: Extracting hand regions and their grasp information from images robustly in real-time is critical for occupants' safety and in-vehicular infotainment applications. It must however, be noted that naturalistic driving scenes suffer from rapidly changing illumination and occlusion. This is aggravated by the fact that hands are highly deformable objects, and change in appearance frequently. This work addresses the task of accurately localizing driver hands and classifying the grasp state of each hand. We use a fast ConvNet to first detect likely hand regions. Next, a pixel-based skin classifier that takes into account the global illumination changes is used to refine the hand detections and remove false positives. This step generates a pixel-level mask for each hand. Finally, we study each such masked regions and detect if the driver is grasping the wheel, or in some cases a mobile phone. Through evaluation we demonstrate that our method can outperform state-of-the-art pixel based hand detectors, while running faster (at 35 fps) than other deep ConvNet based frameworks even for grasp analysis. Hand mask cues are shown to be crucial when analyzing a set of driver hand gestures (wheel/mobile phone grasp and no-grasp) in naturalistic driving settings. The proposed detection and localization pipeline hence can act as a general framework for real-time hand detection and gesture classification.



### xView: Objects in Context in Overhead Imagery
- **Arxiv ID**: http://arxiv.org/abs/1802.07856v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.07856v1)
- **Published**: 2018-02-22 00:26:46+00:00
- **Updated**: 2018-02-22 00:26:46+00:00
- **Authors**: Darius Lam, Richard Kuzma, Kevin McGee, Samuel Dooley, Michael Laielli, Matthew Klaric, Yaroslav Bulatov, Brendan McCord
- **Comment**: Initial submission
- **Journal**: None
- **Summary**: We introduce a new large-scale dataset for the advancement of object detection techniques and overhead object detection research. This satellite imagery dataset enables research progress pertaining to four key computer vision frontiers. We utilize a novel process for geospatial category detection and bounding box annotation with three stages of quality control. Our data is collected from WorldView-3 satellites at 0.3m ground sample distance, providing higher resolution imagery than most public satellite imagery datasets. We compare xView to other object detection datasets in both natural and overhead imagery domains and then provide a baseline analysis using the Single Shot MultiBox Detector. xView is one of the largest and most diverse publicly available object-detection datasets to date, with over 1 million objects across 60 classes in over 1,400 km^2 of imagery.



### Multi-Sensor Integration for Indoor 3D Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1802.07866v1
- **DOI**: 10.13140/RG.2.2.10534.42566
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1802.07866v1)
- **Published**: 2018-02-22 01:04:28+00:00
- **Updated**: 2018-02-22 01:04:28+00:00
- **Authors**: Jacky C. K. Chow
- **Comment**: PhD Thesis, 2014, University of Calgary (Canada),
  http://hdl.handle.net/11023/1484
- **Journal**: None
- **Summary**: Outdoor maps and navigation information delivered by modern services and technologies like Google Maps and Garmin navigators have revolutionized the lifestyle of many people. Motivated by the desire for similar navigation systems for indoor usage from consumers, advertisers, emergency rescuers/responders, etc., many indoor environments such as shopping malls, museums, casinos, airports, transit stations, offices, and schools need to be mapped. Typically, the environment is first reconstructed by capturing many point clouds from various stations and defining their spatial relationships. Currently, there is a lack of an accurate, rigorous, and speedy method for relating point clouds in indoor, urban, satellite-denied environments. This thesis presents a novel and automatic way for fusing calibrated point clouds obtained using a terrestrial laser scanner and the Microsoft Kinect by integrating them with a low-cost inertial measurement unit. The developed system, titled the Scannect, is the first joint static-kinematic indoor 3D mapper.



### End-to-end learning of keypoint detector and descriptor for pose invariant 3D matching
- **Arxiv ID**: http://arxiv.org/abs/1802.07869v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.07869v2)
- **Published**: 2018-02-22 01:58:43+00:00
- **Updated**: 2018-05-09 15:00:23+00:00
- **Authors**: Georgios Georgakis, Srikrishna Karanam, Ziyan Wu, Jan Ernst, Jana Kosecka
- **Comment**: 9 pages, 9 figures, 3 tables, CVPR 2018
- **Journal**: None
- **Summary**: Finding correspondences between images or 3D scans is at the heart of many computer vision and image retrieval applications and is often enabled by matching local keypoint descriptors. Various learning approaches have been applied in the past to different stages of the matching pipeline, considering detector, descriptor, or metric learning objectives. These objectives were typically addressed separately and most previous work has focused on image data. This paper proposes an end-to-end learning framework for keypoint detection and its representation (descriptor) for 3D depth maps or 3D scans, where the two can be jointly optimized towards task-specific objectives without a need for separate annotations. We employ a Siamese architecture augmented by a sampling layer and a novel score loss function which in turn affects the selection of region proposals. The positive and negative examples are obtained automatically by sampling corresponding region proposals based on their consistency with known 3D pose labels. Matching experiments with depth data on multiple benchmark datasets demonstrate the efficacy of the proposed approach, showing significant improvements over state-of-the-art methods.



### Improved Techniques For Weakly-Supervised Object Localization
- **Arxiv ID**: http://arxiv.org/abs/1802.07888v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.07888v2)
- **Published**: 2018-02-22 02:53:28+00:00
- **Updated**: 2018-05-10 03:49:05+00:00
- **Authors**: Junsuk Choe, Joo Hyun Park, Hyunjung Shim
- **Comment**: Submitted to BMVC 2018
- **Journal**: None
- **Summary**: We propose an improved technique for weakly-supervised object localization. Conventional methods have a limitation that they focus only on most discriminative parts of the target objects. The recent study addressed this issue and resolved this limitation by augmenting the training data for less discriminative parts. To this end, we employ an effective data augmentation for improving the accuracy of the object localization. In addition, we introduce improved learning techniques by optimizing Convolutional Neural Networks (CNN) based on the state-of-the-art model. Based on extensive experiments, we evaluate the effectiveness of the proposed approach both qualitatively and quantitatively. Especially, we observe that our method improves the Top-1 localization accuracy by 21.4 - 37.3% depending on configurations, compared to the current state-of-the-art technique of the weakly-supervised object localization.



### Glimpse Clouds: Human Activity Recognition from Unstructured Feature Points
- **Arxiv ID**: http://arxiv.org/abs/1802.07898v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.07898v4)
- **Published**: 2018-02-22 04:09:30+00:00
- **Updated**: 2018-08-21 08:21:02+00:00
- **Authors**: Fabien Baradel, Christian Wolf, Julien Mille, Graham W. Taylor
- **Comment**: CVPR 2018 - project page:
  https://fabienbaradel.github.io/cvpr18_glimpseclouds/
- **Journal**: CVPR 2018
- **Summary**: We propose a method for human activity recognition from RGB data that does not rely on any pose information during test time and does not explicitly calculate pose information internally. Instead, a visual attention module learns to predict glimpse sequences in each frame. These glimpses correspond to interest points in the scene that are relevant to the classified activities. No spatial coherence is forced on the glimpse locations, which gives the module liberty to explore different points at each frame and better optimize the process of scrutinizing visual information. Tracking and sequentially integrating this kind of unstructured data is a challenge, which we address by separating the set of glimpses from a set of recurrent tracking/recognition workers. These workers receive glimpses, jointly performing subsequent motion tracking and activity prediction. The glimpses are soft-assigned to the workers, optimizing coherence of the assignments in space, time and feature space using an external memory module. No hard decisions are taken, i.e. each glimpse point is assigned to all existing workers, albeit with different importance. Our methods outperform state-of-the-art methods on the largest human activity recognition dataset available to-date; NTU RGB+D Dataset, and on a smaller human action recognition dataset Northwestern-UCLA Multiview Action 3D Dataset. Our code is publicly available at https://github.com/fabienbaradel/glimpse_clouds.



### Deep Unsupervised Learning of Visual Similarities
- **Arxiv ID**: http://arxiv.org/abs/1802.08562v1
- **DOI**: 10.1016/j.patcog.2018.01.036
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.08562v1)
- **Published**: 2018-02-22 04:11:59+00:00
- **Updated**: 2018-02-22 04:11:59+00:00
- **Authors**: Artsiom Sanakoyeu, Miguel A. Bautista, Björn Ommer
- **Comment**: arXiv admin note: text overlap with arXiv:1608.08792
- **Journal**: Pattern Recognition Volume 78, June 2018, Pages 331-343
- **Summary**: Exemplar learning of visual similarities in an unsupervised manner is a problem of paramount importance to Computer Vision. In this context, however, the recent breakthrough in deep learning could not yet unfold its full potential. With only a single positive sample, a great imbalance between one positive and many negatives, and unreliable relationships between most samples, training of Convolutional Neural networks is impaired. In this paper we use weak estimates of local similarities and propose a single optimization problem to extract batches of samples with mutually consistent relations. Conflicting relations are distributed over different batches and similar samples are grouped into compact groups. Learning visual similarities is then framed as a sequence of categorization tasks. The CNN then consolidates transitivity relations within and between groups and learns a single representation for all samples without the need for labels. The proposed unsupervised approach has shown competitive performance on detailed posture analysis and object classification.



### Video Person Re-identification by Temporal Residual Learning
- **Arxiv ID**: http://arxiv.org/abs/1802.07918v1
- **DOI**: 10.1109/TIP.2018.2878505
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.07918v1)
- **Published**: 2018-02-22 07:13:53+00:00
- **Updated**: 2018-02-22 07:13:53+00:00
- **Authors**: Ju Dai, Pingping Zhang, Huchuan Lu, Hongyu Wang
- **Comment**: Submitted to IEEE Transactions on Image Processing, including 5
  figures and 4 tables. The first two authors contribute equally to this work
- **Journal**: None
- **Summary**: In this paper, we propose a novel feature learning framework for video person re-identification (re-ID). The proposed framework largely aims to exploit the adequate temporal information of video sequences and tackle the poor spatial alignment of moving pedestrians. More specifically, for exploiting the temporal information, we design a temporal residual learning (TRL) module to simultaneously extract the generic and specific features of consecutive frames. The TRL module is equipped with two bi-directional LSTM (BiLSTM), which are respectively responsible to describe a moving person in different aspects, providing complementary information for better feature representations. To deal with the poor spatial alignment in video re-ID datasets, we propose a spatial-temporal transformer network (ST^2N) module. Transformation parameters in the ST^2N module are learned by leveraging the high-level semantic information of the current frame as well as the temporal context knowledge from other frames. The proposed ST^2N module with less learnable parameters allows effective person alignments under significant appearance changes. Extensive experimental results on the large-scale MARS, PRID2011, ILIDS-VID and SDU-VID datasets demonstrate that the proposed method achieves consistently superior performance and outperforms most of the very recent state-of-the-art methods.



### Graph-Based Blind Image Deblurring From a Single Photograph
- **Arxiv ID**: http://arxiv.org/abs/1802.07929v1
- **DOI**: 10.1109/TIP.2018.2874290
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.07929v1)
- **Published**: 2018-02-22 07:48:18+00:00
- **Updated**: 2018-02-22 07:48:18+00:00
- **Authors**: Yuanchao Bai, Gene Cheung, Xianming Liu, Wen Gao
- **Comment**: None
- **Journal**: None
- **Summary**: Blind image deblurring, i.e., deblurring without knowledge of the blur kernel, is a highly ill-posed problem. The problem can be solved in two parts: i) estimate a blur kernel from the blurry image, and ii) given estimated blur kernel, de-convolve blurry input to restore the target image. In this paper, we propose a graph-based blind image deblurring algorithm by interpreting an image patch as a signal on a weighted graph. Specifically, we first argue that a skeleton image---a proxy that retains the strong gradients of the target but smooths out the details---can be used to accurately estimate the blur kernel and has a unique bi-modal edge weight distribution. Then, we design a reweighted graph total variation (RGTV) prior that can efficiently promote a bi-modal edge weight distribution given a blurry patch. Further, to analyze RGTV in the graph frequency domain, we introduce a new weight function to represent RGTV as a graph $l_1$-Laplacian regularizer. This leads to a graph spectral filtering interpretation of the prior with desirable properties, including robustness to noise and blur, strong piecewise smooth (PWS) filtering and sharpness promotion. Minimizing a blind image deblurring objective with RGTV results in a non-convex non-differentiable optimization problem. We leverage the new graph spectral interpretation for RGTV to design an efficient algorithm that solves for the skeleton image and the blur kernel alternately. Specifically for Gaussian blur, we propose a further speedup strategy for blind Gaussian deblurring using accelerated graph spectral filtering. Finally, with the computed blur kernel, recent non-blind image deblurring algorithms can be applied to restore the target image. Experimental results demonstrate that our algorithm successfully restores latent sharp images and outperforms state-of-the-art methods quantitatively and qualitatively.



### Where's YOUR focus: Personalized Attention
- **Arxiv ID**: http://arxiv.org/abs/1802.07931v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.07931v1)
- **Published**: 2018-02-22 07:58:18+00:00
- **Updated**: 2018-02-22 07:58:18+00:00
- **Authors**: Sikun Lin, Pan Hui
- **Comment**: None
- **Journal**: None
- **Summary**: Human visual attention is subjective and biased according to the personal preference of the viewer, however, current works of saliency detection are general and objective, without counting the factor of the observer. This will make the attention prediction for a particular person not accurate enough. In this work, we present the novel idea of personalized attention prediction and develop Personalized Attention Network (PANet), a convolutional network that predicts saliency in images with personal preference. The model consists of two streams which share common feature extraction layers, and one stream is responsible for saliency prediction, while the other is adapted from the detection model and used to fit user preference. We automatically collect user preference from their albums and leaves them freedom to define what and how many categories their preferences are divided into. To train PANet, we dynamically generate ground truth saliency maps upon existing detection labels and saliency labels, and the generation parameters are based upon our collected datasets consists of 1k images. We evaluate the model with saliency prediction metrics and test the trained model on different preference vectors. The results have shown that our system is much better than general models in personalized saliency prediction and is efficient to use for different preferences.



### Unravelling Robustness of Deep Learning based Face Recognition Against Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/1803.00401v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.00401v1)
- **Published**: 2018-02-22 08:03:26+00:00
- **Updated**: 2018-02-22 08:03:26+00:00
- **Authors**: Gaurav Goswami, Nalini Ratha, Akshay Agarwal, Richa Singh, Mayank Vatsa
- **Comment**: Accepted in AAAI 2018 (8 pages, 5 figures, 5 tables)
- **Journal**: None
- **Summary**: Deep neural network (DNN) architecture based models have high expressive power and learning capacity. However, they are essentially a black box method since it is not easy to mathematically formulate the functions that are learned within its many layers of representation. Realizing this, many researchers have started to design methods to exploit the drawbacks of deep learning based algorithms questioning their robustness and exposing their singularities. In this paper, we attempt to unravel three aspects related to the robustness of DNNs for face recognition: (i) assessing the impact of deep architectures for face recognition in terms of vulnerabilities to attacks inspired by commonly observed distortions in the real world that are well handled by shallow learning methods along with learning based adversaries; (ii) detecting the singularities by characterizing abnormal filter response behavior in the hidden layers of deep networks; and (iii) making corrections to the processing pipeline to alleviate the problem. Our experimental evaluation using multiple open-source DNN-based face recognition networks, including OpenFace and VGG-Face, and two publicly available databases (MEDS and PaSC) demonstrates that the performance of deep learning based face recognition algorithms can suffer greatly in the presence of such distortions. The proposed method is also compared with existing detection algorithms and the results show that it is able to detect the attacks with very high accuracy by suitably designing a classifier using the response of the hidden layers in the network. Finally, we present several effective countermeasures to mitigate the impact of adversarial attacks and improve the overall robustness of DNN-based face recognition.



### Adversarial Learning for Semi-Supervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1802.07934v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.07934v2)
- **Published**: 2018-02-22 08:13:20+00:00
- **Updated**: 2018-07-24 22:56:44+00:00
- **Authors**: Wei-Chih Hung, Yi-Hsuan Tsai, Yan-Ting Liou, Yen-Yu Lin, Ming-Hsuan Yang
- **Comment**: Accepted in BMVC 2018. Code and models available at
  https://github.com/hfslyc/AdvSemiSeg
- **Journal**: None
- **Summary**: We propose a method for semi-supervised semantic segmentation using an adversarial network. While most existing discriminators are trained to classify input images as real or fake on the image level, we design a discriminator in a fully convolutional manner to differentiate the predicted probability maps from the ground truth segmentation distribution with the consideration of the spatial resolution. We show that the proposed discriminator can be used to improve semantic segmentation accuracy by coupling the adversarial loss with the standard cross entropy loss of the proposed model. In addition, the fully convolutional discriminator enables semi-supervised learning through discovering the trustworthy regions in predicted results of unlabeled images, thereby providing additional supervisory signals. In contrast to existing methods that utilize weakly-labeled images, our method leverages unlabeled images to enhance the segmentation model. Experimental results on the PASCAL VOC 2012 and Cityscapes datasets demonstrate the effectiveness of the proposed algorithm.



### Stereo obstacle detection for unmanned surface vehicles by IMU-assisted semantic segmentation
- **Arxiv ID**: http://arxiv.org/abs/1802.07956v1
- **DOI**: 10.1016/j.robot.2018.02.017
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1802.07956v1)
- **Published**: 2018-02-22 09:52:43+00:00
- **Updated**: 2018-02-22 09:52:43+00:00
- **Authors**: Borja Bovcon, Rok Mandeljc, Janez Perš, Matej Kristan
- **Comment**: 14 pages, 18 figures, new publicly available multi-modal obstacle
  detection dataset
- **Journal**: None
- **Summary**: A new obstacle detection algorithm for unmanned surface vehicles (USVs) is presented. A state-of-the-art graphical model for semantic segmentation is extended to incorporate boat pitch and roll measurements from the on-board inertial measurement unit (IMU), and a stereo verification algorithm that consolidates tentative detections obtained from the segmentation is proposed. The IMU readings are used to estimate the location of horizon line in the image, which automatically adjusts the priors in the probabilistic semantic segmentation model. We derive the equations for projecting the horizon into images, propose an efficient optimization algorithm for the extended graphical model, and offer a practical IMU-camera-USV calibration procedure. Using an USV equipped with multiple synchronized sensors, we captured a new challenging multi-modal dataset, and annotated its images with water edge and obstacles. Experimental results show that the proposed algorithm significantly outperforms the state of the art, with nearly 30% improvement in water-edge detection accuracy, an over 21% reduction of false positive rate, an almost 60% reduction of false negative rate, and an over 65% increase of true positive rate, while its Matlab implementation runs in real-time.



### Non-rigid Object Tracking via Deep Multi-scale Spatial-temporal Discriminative Saliency Maps
- **Arxiv ID**: http://arxiv.org/abs/1802.07957v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.07957v2)
- **Published**: 2018-02-22 09:55:29+00:00
- **Updated**: 2019-02-28 02:18:06+00:00
- **Authors**: Pingping Zhang, Wei Liu, Dong Wang, Yinjie Lei, Hongyu Wang, Chunhua Shen, Huchuan Lu
- **Comment**: 12 pages, 9 figures and 1 table
- **Journal**: None
- **Summary**: In this paper, we propose a novel effective non-rigid object tracking framework based on the spatial-temporal consistent saliency detection. In contrast to most existing trackers that utilize a bounding box to specify the tracked target, the proposed framework can extract accurate regions of the target as tracking outputs. It achieves a better description of the non-rigid objects and reduces the background pollution for the tracking model. Furthermore, our model has several unique features. First, a tailored fully convolutional neural network (TFCN) is developed to model the local saliency prior for a given image region, which not only provides the pixel-wise outputs but also integrates the semantic information. Second, a novel multi-scale multi-region mechanism is proposed to generate local saliency maps that effectively consider visual perceptions with different spatial layouts and scale variations. Subsequently, local saliency maps are fused via a weighted entropy method, resulting in a final discriminative saliency map. Finally, we present a non-rigid object tracking algorithm based on the predicted saliency maps. By utilizing a spatial-temporal consistent saliency map (STCSM), we conduct target-background classification and use a simple fine-tuning scheme for online updating. Extensive experiments demonstrate that the proposed algorithm achieves competitive performance in both saliency detection and visual tracking, especially outperforming other related trackers on the non-rigid object tracking datasets.



### Robustness of classifiers to uniform $\ell\_p$ and Gaussian noise
- **Arxiv ID**: http://arxiv.org/abs/1802.07971v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1802.07971v1)
- **Published**: 2018-02-22 10:31:21+00:00
- **Updated**: 2018-02-22 10:31:21+00:00
- **Authors**: Jean-Yves Franceschi, Alhussein Fawzi, Omar Fawzi
- **Comment**: None
- **Journal**: 21st International Conference on Artificial Intelligence and
  Statistics (AISTATS) 2018, Apr 2018, Playa Blanca, Spain. 2018,
  http://www.aistats.org/
- **Summary**: We study the robustness of classifiers to various kinds of random noise models. In particular, we consider noise drawn uniformly from the $\ell\_p$ ball for $p \in [1, \infty]$ and Gaussian noise with an arbitrary covariance matrix. We characterize this robustness to random noise in terms of the distance to the decision boundary of the classifier. This analysis applies to linear classifiers as well as classifiers with locally approximately flat decision boundaries, a condition which is satisfied by state-of-the-art deep neural networks. The predicted robustness is verified experimentally.



### MagnifyMe: Aiding Cross Resolution Face Recognition via Identity Aware Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1802.08057v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.08057v1)
- **Published**: 2018-02-22 14:26:53+00:00
- **Updated**: 2018-02-22 14:26:53+00:00
- **Authors**: Maneet Singh, Shruti Nagpal, Richa Singh, Mayank Vatsa, Angshul Majumdar
- **Comment**: None
- **Journal**: None
- **Summary**: Enhancing low resolution images via super-resolution or image synthesis for cross-resolution face recognition has been well studied. Several image processing and machine learning paradigms have been explored for addressing the same. In this research, we propose Synthesis via Deep Sparse Representation algorithm for synthesizing a high resolution face image from a low resolution input image. The proposed algorithm learns multi-level sparse representation for both high and low resolution gallery images, along with an identity aware dictionary and a transformation function between the two representations for face identification scenarios. With low resolution test data as input, the high resolution test image is synthesized using the identity aware dictionary and transformation which is then used for face recognition. The performance of the proposed SDSR algorithm is evaluated on four databases, including one real world dataset. Experimental results and comparison with existing seven algorithms demonstrate the efficacy of the proposed algorithm in terms of both face identification and image quality measures.



### Classification of Breast Cancer Histology using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1802.08080v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.08080v2)
- **Published**: 2018-02-22 14:56:38+00:00
- **Updated**: 2018-07-25 07:58:20+00:00
- **Authors**: Aditya Golatkar, Deepak Anand, Amit Sethi
- **Comment**: 8 pages. Published at ICIAR 2018, Portugal
- **Journal**: None
- **Summary**: Breast Cancer is a major cause of death worldwide among women. Hematoxylin and Eosin (H&E) stained breast tissue samples from biopsies are observed under microscopes for the primary diagnosis of breast cancer. In this paper, we propose a deep learning-based method for classification of H&E stained breast tissue images released for BACH challenge 2018 by fine-tuning Inception-v3 convolutional neural network (CNN) proposed by Szegedy et al. These images are to be classified into four classes namely, i) normal tissue, ii) benign tumor, iii) in-situ carcinoma and iv) invasive carcinoma. Our strategy is to extract patches based on nuclei density instead of random or grid sampling, along with rejection of patches that are not rich in nuclei (non-epithelial) regions for training and testing. Every patch (nuclei-dense region) in an image is classified in one of the four above mentioned categories. The class of the entire image is determined using majority voting over the nuclear classes. We obtained an average four class accuracy of 85% and an average two class (non-cancer vs. carcinoma) accuracy of 93%, which improves upon a previous benchmark by Araujo et al.



### Harmonious Attention Network for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1802.08122v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.08122v1)
- **Published**: 2018-02-22 16:04:55+00:00
- **Updated**: 2018-02-22 16:04:55+00:00
- **Authors**: Wei Li, Xiatian Zhu, Shaogang Gong
- **Comment**: Accepted in CVPR 2018
- **Journal**: None
- **Summary**: Existing person re-identification (re-id) methods either assume the availability of well-aligned person bounding box images as model input or rely on constrained attention selection mechanisms to calibrate misaligned images. They are therefore sub-optimal for re-id matching in arbitrarily aligned person images potentially with large human pose variations and unconstrained auto-detection errors. In this work, we show the advantages of jointly learning attention selection and feature representation in a Convolutional Neural Network (CNN) by maximising the complementary information of different levels of visual attention subject to re-id discriminative learning constraints. Specifically, we formulate a novel Harmonious Attention CNN (HA-CNN) model for joint learning of soft pixel attention and hard regional attention along with simultaneous optimisation of feature representations, dedicated to optimise person re-id in uncontrolled (misaligned) images. Extensive comparative evaluations validate the superiority of this new HA-CNN model for person re-id over a wide variety of state-of-the-art methods on three large-scale benchmarks including CUHK03, Market-1501, and DukeMTMC-ReID.



### Adversarial Examples that Fool both Computer Vision and Time-Limited Humans
- **Arxiv ID**: http://arxiv.org/abs/1802.08195v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, q-bio.NC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1802.08195v3)
- **Published**: 2018-02-22 17:40:51+00:00
- **Updated**: 2018-05-22 03:02:41+00:00
- **Authors**: Gamaleldin F. Elsayed, Shreya Shankar, Brian Cheung, Nicolas Papernot, Alex Kurakin, Ian Goodfellow, Jascha Sohl-Dickstein
- **Comment**: None
- **Journal**: Advances in Neural Information Processing Systems, 2018
- **Summary**: Machine learning models are vulnerable to adversarial examples: small changes to images can cause computer vision models to make mistakes such as identifying a school bus as an ostrich. However, it is still an open question whether humans are prone to similar mistakes. Here, we address this question by leveraging recent techniques that transfer adversarial examples from computer vision models with known parameters and architecture to other models with unknown parameters and architecture, and by matching the initial processing of the human visual system. We find that adversarial examples that strongly transfer across computer vision models influence the classifications made by time-limited human observers.



### ChatPainter: Improving Text to Image Generation using Dialogue
- **Arxiv ID**: http://arxiv.org/abs/1802.08216v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.08216v1)
- **Published**: 2018-02-22 18:15:40+00:00
- **Updated**: 2018-02-22 18:15:40+00:00
- **Authors**: Shikhar Sharma, Dendi Suhubdy, Vincent Michalski, Samira Ebrahimi Kahou, Yoshua Bengio
- **Comment**: None
- **Journal**: None
- **Summary**: Synthesizing realistic images from text descriptions on a dataset like Microsoft Common Objects in Context (MS COCO), where each image can contain several objects, is a challenging task. Prior work has used text captions to generate images. However, captions might not be informative enough to capture the entire image and insufficient for the model to be able to understand which objects in the images correspond to which words in the captions. We show that adding a dialogue that further describes the scene leads to significant improvement in the inception score and in the quality of generated images on the MS COCO dataset.



### VizWiz Grand Challenge: Answering Visual Questions from Blind People
- **Arxiv ID**: http://arxiv.org/abs/1802.08218v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1802.08218v4)
- **Published**: 2018-02-22 18:16:53+00:00
- **Updated**: 2018-05-09 17:26:40+00:00
- **Authors**: Danna Gurari, Qing Li, Abigale J. Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, Jeffrey P. Bigham
- **Comment**: None
- **Journal**: None
- **Summary**: The study of algorithms to automatically answer visual questions currently is motivated by visual question answering (VQA) datasets constructed in artificial VQA settings. We propose VizWiz, the first goal-oriented VQA dataset arising from a natural VQA setting. VizWiz consists of over 31,000 visual questions originating from blind people who each took a picture using a mobile phone and recorded a spoken question about it, together with 10 crowdsourced answers per visual question. VizWiz differs from the many existing VQA datasets because (1) images are captured by blind photographers and so are often poor quality, (2) questions are spoken and so are more conversational, and (3) often visual questions cannot be answered. Evaluation of modern algorithms for answering visual questions and deciding if a visual question is answerable reveals that VizWiz is a challenging dataset. We introduce this dataset to encourage a larger community to develop more generalized algorithms that can assist blind people.



### Tensor field networks: Rotation- and translation-equivariant neural networks for 3D point clouds
- **Arxiv ID**: http://arxiv.org/abs/1802.08219v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1802.08219v3)
- **Published**: 2018-02-22 18:17:31+00:00
- **Updated**: 2018-05-18 20:09:34+00:00
- **Authors**: Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, Patrick Riley
- **Comment**: changes for NIPS submission
- **Journal**: None
- **Summary**: We introduce tensor field neural networks, which are locally equivariant to 3D rotations, translations, and permutations of points at every layer. 3D rotation equivariance removes the need for data augmentation to identify features in arbitrary orientations. Our network uses filters built from spherical harmonics; due to the mathematical consequences of this filter choice, each layer accepts as input (and guarantees as output) scalars, vectors, and higher-order tensors, in the geometric sense of these terms. We demonstrate the capabilities of tensor field networks with tasks in geometry, physics, and chemistry.



### Hessian-based Analysis of Large Batch Training and Robustness to Adversaries
- **Arxiv ID**: http://arxiv.org/abs/1802.08241v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1802.08241v4)
- **Published**: 2018-02-22 18:55:00+00:00
- **Updated**: 2018-12-02 19:58:30+00:00
- **Authors**: Zhewei Yao, Amir Gholami, Qi Lei, Kurt Keutzer, Michael W. Mahoney
- **Comment**: Presented in NeurIPS'18 conference
- **Journal**: NeurIPS 2018
- **Summary**: Large batch size training of Neural Networks has been shown to incur accuracy loss when trained with the current methods. The exact underlying reasons for this are still not completely understood. Here, we study large batch size training through the lens of the Hessian operator and robust optimization. In particular, we perform a Hessian based study to analyze exactly how the landscape of the loss function changes when training with large batch size. We compute the true Hessian spectrum, without approximation, by back-propagating the second derivative. Extensive experiments on multiple networks show that saddle-points are not the cause for generalization gap of large batch size training, and the results consistently show that large batch converges to points with noticeably higher Hessian spectrum. Furthermore, we show that robust training allows one to favor flat areas, as points with large Hessian spectrum show poor robustness to adversarial perturbation. We further study this relationship, and provide empirical and theoretical proof that the inner loop for robust training is a saddle-free optimization problem \textit{almost everywhere}. We present detailed experiments with five different network architectures, including a residual network, tested on MNIST, CIFAR-10, and CIFAR-100 datasets. We have open sourced our method which can be accessed at [1].



### SPLATNet: Sparse Lattice Networks for Point Cloud Processing
- **Arxiv ID**: http://arxiv.org/abs/1802.08275v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1802.08275v4)
- **Published**: 2018-02-22 19:30:09+00:00
- **Updated**: 2018-05-09 14:22:41+00:00
- **Authors**: Hang Su, Varun Jampani, Deqing Sun, Subhransu Maji, Evangelos Kalogerakis, Ming-Hsuan Yang, Jan Kautz
- **Comment**: Camera-ready, accepted to CVPR 2018 (oral); project website:
  http://vis-www.cs.umass.edu/splatnet/
- **Journal**: None
- **Summary**: We present a network architecture for processing point clouds that directly operates on a collection of points represented as a sparse set of samples in a high-dimensional lattice. Naively applying convolutions on this lattice scales poorly, both in terms of memory and computational cost, as the size of the lattice increases. Instead, our network uses sparse bilateral convolutional layers as building blocks. These layers maintain efficiency by using indexing structures to apply convolutions only on occupied parts of the lattice, and allow flexible specifications of the lattice structure enabling hierarchical and spatially-aware feature learning, as well as joint 2D-3D reasoning. Both point-based and image-based representations can be easily incorporated in a network with such layers and the resulting model can be trained in an end-to-end manner. We present results on 3D segmentation tasks where our approach outperforms existing state-of-the-art techniques.



### Sleep-deprived Fatigue Pattern Analysis using Large-Scale Selfies from Social Med
- **Arxiv ID**: http://arxiv.org/abs/1802.08310v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.08310v1)
- **Published**: 2018-02-22 21:31:31+00:00
- **Updated**: 2018-02-22 21:31:31+00:00
- **Authors**: Xuefeng Peng, Jiebo Luo, Catherine Glenn, Li-Kai Chi, Jingyao Zhan
- **Comment**: None
- **Journal**: Special Session on Intelligent Data Mining, IEEE Big Data
  Conference, Boston, MA, December 2017
- **Summary**: The complexities of fatigue have drawn much attention from researchers across various disciplines. Short-term fatigue may cause safety issue while driving; thus, dynamic systems were designed to track driver fatigue. Long-term fatigue could lead to chronic syndromes, and eventually affect individuals physical and psychological health. Traditional methodologies of evaluating fatigue not only require sophisticated equipment but also consume enormous time. In this paper, we attempt to develop a novel and efficient method to predict individual's fatigue rate by scrutinizing human facial cues. Our goal is to predict fatigue rate based on a selfie. To associate the fatigue rate with user behaviors, we have collected nearly 1-million timeline posts from 10,480 users on Instagram. We first detect all the faces and identify their demographics using automatic algorithms. Next, we investigate the fatigue distribution by weekday over different age, gender, and ethnic groups. This work represents a promising way to assess sleep-deprived fatigue, and our study provides a viable and efficient computational framework for user fatigue modeling in large-scale via social media.



