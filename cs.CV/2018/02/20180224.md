# Arxiv Papers in cs.CV on 2018-02-24
### Tool Detection and Operative Skill Assessment in Surgical Videos Using Region-Based Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1802.08774v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.08774v2)
- **Published**: 2018-02-24 00:55:34+00:00
- **Updated**: 2018-07-22 00:54:59+00:00
- **Authors**: Amy Jin, Serena Yeung, Jeffrey Jopling, Jonathan Krause, Dan Azagury, Arnold Milstein, Li Fei-Fei
- **Comment**: arXiv admin note: text overlap with arXiv:1806.02031 by other authors
- **Journal**: None
- **Summary**: Five billion people in the world lack access to quality surgical care. Surgeon skill varies dramatically, and many surgical patients suffer complications and avoidable harm. Improving surgical training and feedback would help to reduce the rate of complications, half of which have been shown to be preventable. To do this, it is essential to assess operative skill, a process that currently requires experts and is manual, time consuming, and subjective. In this work, we introduce an approach to automatically assess surgeon performance by tracking and analyzing tool movements in surgical videos, leveraging region-based convolutional neural networks. In order to study this problem, we also introduce a new dataset, m2cai16-tool-locations, which extends the m2cai16-tool dataset with spatial bounds of tools. While previous methods have addressed tool presence detection, ours is the first to not only detect presence but also spatially localize surgical tools in real-world laparoscopic surgical videos. We show that our method both effectively detects the spatial bounds of tools as well as significantly outperforms existing methods on tool presence detection. We further demonstrate the ability of our method to assess surgical quality through analysis of tool usage patterns, movement range, and economy of motion.



### Superpixel based Class-Semantic Texton Occurrences for Natural Roadside Vegetation Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1802.08781v1
- **DOI**: 10.1007/s00138-017-0833-7
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.08781v1)
- **Published**: 2018-02-24 01:51:41+00:00
- **Updated**: 2018-02-24 01:51:41+00:00
- **Authors**: Ligang Zhang, Brijesh Verma
- **Comment**: This is a pre-print of an article published in Machine Vision and
  Applications. The final authenticated version is available online at:
  https://doi.org/10.1007/s00138-017-0833-7
- **Journal**: Machine Vision and Applications (2017) 28: 293
- **Summary**: Vegetation segmentation from roadside data is a field that has received relatively little attention in present studies, but can be of great potentials in a wide range of real-world applications, such as road safety assessment and vegetation condition monitoring. In this paper, we present a novel approach that generates class-semantic color-texture textons and aggregates superpixel based texton occurrences for vegetation segmentation in natural roadside images. Pixel-level class-semantic textons are first learnt by generating two individual sets of bag-of-word visual dictionaries from color and filter-bank texture features separately for each object class using manually cropped training data. For a testing image, it is first oversegmented into a set of homogeneous superpixels. The color and texture features of all pixels in each superpixel are extracted and further mapped to one of the learnt textons using the nearest distance metric, resulting in a color and a texture texton occurrence matrix. The color and texture texton occurrences are aggregated using a linear mixing method over each superpixel and the segmentation is finally achieved using a simple yet effective majority voting strategy. Evaluations on two public image datasets from videos collected by the Department of Transport and Main Roads (DTMR), Queensland, Australia, and a public roadside grass dataset show high accuracy of the proposed approach. We also demonstrate the effectiveness of the approach for vegetation segmentation in real-world scenarios.



### Facial Expression Analysis under Partial Occlusion: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1802.08784v1
- **DOI**: 10.1145/3158369
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.08784v1)
- **Published**: 2018-02-24 02:01:22+00:00
- **Updated**: 2018-02-24 02:01:22+00:00
- **Authors**: Ligang Zhang, Brijesh Verma, Dian Tjondronegoro, Vinod Chandran
- **Comment**: Authors pre-print of the article accepted for publication in ACM
  Computing Surveys (accepted on 02-Nov-2017)
- **Journal**: ACM Computing Surveys 51, 2, Article 25 (February 2018)
- **Summary**: Automatic machine-based Facial Expression Analysis (FEA) has made substantial progress in the past few decades driven by its importance for applications in psychology, security, health, entertainment and human computer interaction. The vast majority of completed FEA studies are based on non-occluded faces collected in a controlled laboratory environment. Automatic expression recognition tolerant to partial occlusion remains less understood, particularly in real-world scenarios. In recent years, efforts investigating techniques to handle partial occlusion for FEA have seen an increase. The context is right for a comprehensive perspective of these developments and the state of the art from this perspective. This survey provides such a comprehensive review of recent advances in dataset creation, algorithm development, and investigations of the effects of occlusion critical for robust performance in FEA systems. It outlines existing challenges in overcoming partial occlusion and discusses possible opportunities in advancing the technology. To the best of our knowledge, it is the first FEA survey dedicated to occlusion and aimed at promoting better informed and benchmarked future work.



### Spatially Constrained Location Prior for Scene Parsing
- **Arxiv ID**: http://arxiv.org/abs/1802.08790v1
- **DOI**: 10.1109/IJCNN.2016.7727373
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.08790v1)
- **Published**: 2018-02-24 03:21:01+00:00
- **Updated**: 2018-02-24 03:21:01+00:00
- **Authors**: Ligang Zhang, Brijesh Verma, David Stockwell, Sujan Chowdhury
- **Comment**: authors' pre-print version of a article published in IJCNN 2016
- **Journal**: International Joint Conference on Neural Networks (IJCNN), 2016,
  pp. 1480-1486
- **Summary**: Semantic context is an important and useful cue for scene parsing in complicated natural images with a substantial amount of variations in objects and the environment. This paper proposes Spatially Constrained Location Prior (SCLP) for effective modelling of global and local semantic context in the scene in terms of inter-class spatial relationships. Unlike existing studies focusing on either relative or absolute location prior of objects, the SCLP effectively incorporates both relative and absolute location priors by calculating object co-occurrence frequencies in spatially constrained image blocks. The SCLP is general and can be used in conjunction with various visual feature-based prediction models, such as Artificial Neural Networks and Support Vector Machine (SVM), to enforce spatial contextual constraints on class labels. Using SVM classifiers and a linear regression model, we demonstrate that the incorporation of SCLP achieves superior performance compared to the state-of-the-art methods on the Stanford background and SIFT Flow datasets.



### Multispectral Image Intrinsic Decomposition via Low Rank Constraint
- **Arxiv ID**: http://arxiv.org/abs/1802.08793v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.08793v1)
- **Published**: 2018-02-24 04:04:39+00:00
- **Updated**: 2018-02-24 04:04:39+00:00
- **Authors**: Qian Huang, Weixin Zhu, Yang Zhao, Linsen Chen, Yao Wang, Tao Yue, Xun Cao
- **Comment**: Proceedings of the IEEE International Conference on Computer Vision
  and Pattern Recognition (CVPR)
- **Journal**: None
- **Summary**: Multispectral images contain many clues of surface characteristics of the objects, thus can be widely used in many computer vision tasks, e.g., recolorization and segmentation. However, due to the complex illumination and the geometry structure of natural scenes, the spectra curves of a same surface can look very different. In this paper, a Low Rank Multispectral Image Intrinsic Decomposition model (LRIID) is presented to decompose the shading and reflectance from a single multispectral image. We extend the Retinex model, which is proposed for RGB image intrinsic decomposition, for multispectral domain. Based on this, a low rank constraint is proposed to reduce the ill-posedness of the problem and make the algorithm solvable. A dataset of 12 images is given with the ground truth of shadings and reflectance, so that the objective evaluations can be conducted. The experiments demonstrate the effectiveness of proposed method.



### Constrained Image Generation Using Binarized Neural Networks with Decision Procedures
- **Arxiv ID**: http://arxiv.org/abs/1802.08795v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.08795v1)
- **Published**: 2018-02-24 04:12:30+00:00
- **Updated**: 2018-02-24 04:12:30+00:00
- **Authors**: Svyatoslav Korneev, Nina Narodytska, Luca Pulina, Armando Tacchella, Nikolaj Bjorner, Mooly Sagiv
- **Comment**: None
- **Journal**: None
- **Summary**: We consider the problem of binary image generation with given properties. This problem arises in a number of practical applications, including generation of artificial porous medium for an electrode of lithium-ion batteries, for composed materials, etc. A generated image represents a porous medium and, as such, it is subject to two sets of constraints: topological constraints on the structure and process constraints on the physical process over this structure. To perform image generation we need to define a mapping from a porous medium to its physical process parameters. For a given geometry of a porous medium, this mapping can be done by solving a partial differential equation (PDE). However, embedding a PDE solver into the search procedure is computationally expensive. We use a binarized neural network to approximate a PDE solver. This allows us to encode the entire problem as a logical formula. Our main contribution is that, for the first time, we show that this problem can be tackled using decision procedures. Our experiments show that our model is able to produce random constrained images that satisfy both topological and process constraints.



### Residual Dense Network for Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1802.08797v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.08797v2)
- **Published**: 2018-02-24 04:40:06+00:00
- **Updated**: 2018-03-27 02:53:58+00:00
- **Authors**: Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, Yun Fu
- **Comment**: To appear in CVPR 2018 as spotlight
- **Journal**: None
- **Summary**: A very deep convolutional neural network (CNN) has recently achieved great success for image super-resolution (SR) and offered hierarchical features as well. However, most deep CNN based SR models do not make full use of the hierarchical features from the original low-resolution (LR) images, thereby achieving relatively-low performance. In this paper, we propose a novel residual dense network (RDN) to address this problem in image SR. We fully exploit the hierarchical features from all the convolutional layers. Specifically, we propose residual dense block (RDB) to extract abundant local features via dense connected convolutional layers. RDB further allows direct connections from the state of preceding RDB to all the layers of current RDB, leading to a contiguous memory (CM) mechanism. Local feature fusion in RDB is then used to adaptively learn more effective features from preceding and current local features and stabilizes the training of wider network. After fully obtaining dense local features, we use global feature fusion to jointly and adaptively learn global hierarchical features in a holistic way. Extensive experiments on benchmark datasets with different degradation models show that our RDN achieves favorable performance against state-of-the-art methods.



### Single Image Super-Resolution via Cascaded Multi-Scale Cross Network
- **Arxiv ID**: http://arxiv.org/abs/1802.08808v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.08808v1)
- **Published**: 2018-02-24 06:16:34+00:00
- **Updated**: 2018-02-24 06:16:34+00:00
- **Authors**: Yanting Hu, Xinbo Gao, Jie Li, Yuanfei Huang, Hanzi Wang
- **Comment**: 12 pages,11 figures
- **Journal**: None
- **Summary**: The deep convolutional neural networks have achieved significant improvements in accuracy and speed for single image super-resolution. However, as the depth of network grows, the information flow is weakened and the training becomes harder and harder. On the other hand, most of the models adopt a single-stream structure with which integrating complementary contextual information under different receptive fields is difficult. To improve information flow and to capture sufficient knowledge for reconstructing the high-frequency details, we propose a cascaded multi-scale cross network (CMSC) in which a sequence of subnetworks is cascaded to infer high resolution features in a coarse-to-fine manner. In each cascaded subnetwork, we stack multiple multi-scale cross (MSC) modules to fuse complementary multi-scale information in an efficient way as well as to improve information flow across the layers. Meanwhile, by introducing residual-features learning in each stage, the relative information between high-resolution and low-resolution features is fully utilized to further boost reconstruction performance. We train the proposed network with cascaded-supervision and then assemble the intermediate predictions of the cascade to achieve high quality image reconstruction. Extensive quantitative and qualitative evaluations on benchmark datasets illustrate the superiority of our proposed method over state-of-the-art super-resolution methods.



### A Twofold Siamese Network for Real-Time Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/1802.08817v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.08817v1)
- **Published**: 2018-02-24 07:57:52+00:00
- **Updated**: 2018-02-24 07:57:52+00:00
- **Authors**: Anfeng He, Chong Luo, Xinmei Tian, Wenjun Zeng
- **Comment**: Accepted by CVPR'18
- **Journal**: None
- **Summary**: Observing that Semantic features learned in an image classification task and Appearance features learned in a similarity matching task complement each other, we build a twofold Siamese network, named SA-Siam, for real-time object tracking. SA-Siam is composed of a semantic branch and an appearance branch. Each branch is a similarity-learning Siamese network. An important design choice in SA-Siam is to separately train the two branches to keep the heterogeneity of the two types of features. In addition, we propose a channel attention mechanism for the semantic branch. Channel-wise weights are computed according to the channel activations around the target position. While the inherited architecture from SiamFC \cite{SiamFC} allows our tracker to operate beyond real-time, the twofold design and the attention mechanism significantly improve the tracking performance. The proposed SA-Siam outperforms all other real-time trackers by a large margin on OTB-2013/50/100 benchmarks.



### Convolutional Neural Networks combined with Runge-Kutta Methods
- **Arxiv ID**: http://arxiv.org/abs/1802.08831v7
- **DOI**: 10.1007/s00521-022-07785-2
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1802.08831v7)
- **Published**: 2018-02-24 10:31:24+00:00
- **Updated**: 2022-09-09 13:56:00+00:00
- **Authors**: Mai Zhu, Bo Chang, Chong Fu
- **Comment**: 26 pages, 2 figures
- **Journal**: Neural Computing and Applications (2022)
- **Summary**: A convolutional neural network can be constructed using numerical methods for solving dynamical systems, since the forward pass of the network can be regarded as a trajectory of a dynamical system. However, existing models based on numerical solvers cannot avoid the iterations of implicit methods, which makes the models inefficient at inference time. In this paper, we reinterpret the pre-activation Residual Networks (ResNets) and their variants from the dynamical systems view. We consider that the iterations of implicit Runge-Kutta methods are fused into the training of these models. Moreover, we propose a novel approach to constructing network models based on high-order Runge-Kutta methods in order to achieve higher efficiency. Our proposed models are referred to as the Runge-Kutta Convolutional Neural Networks (RKCNNs). The RKCNNs are evaluated on multiple benchmark datasets. The experimental results show that RKCNNs are vastly superior to other dynamical system network models: they achieve higher accuracy with much fewer resources. They also expand the family of network models based on numerical methods for dynamical systems.



### Adaptive Deep Learning through Visual Domain Localization
- **Arxiv ID**: http://arxiv.org/abs/1802.08833v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1802.08833v1)
- **Published**: 2018-02-24 10:36:51+00:00
- **Updated**: 2018-02-24 10:36:51+00:00
- **Authors**: Gabriele Angeletti, Barbara Caputo, Tatiana Tommasi
- **Comment**: Accepted at ICRA 2018
- **Journal**: None
- **Summary**: A commercial robot, trained by its manufacturer to recognize a predefined number and type of objects, might be used in many settings, that will in general differ in their illumination conditions, background, type and degree of clutter, and so on. Recent computer vision works tackle this generalization issue through domain adaptation methods, assuming as source the visual domain where the system is trained and as target the domain of deployment. All approaches assume to have access to images from all classes of the target during training, an unrealistic condition in robotics applications. We address this issue proposing an algorithm that takes into account the specific needs of robot vision. Our intuition is that the nature of the domain shift experienced mostly in robotics is local. We exploit this through the learning of maps that spatially ground the domain and quantify the degree of shift, embedded into an end-to-end deep domain adaptation architecture. By explicitly localizing the roots of the domain shift we significantly reduce the number of parameters of the architecture to tune, we gain the flexibility necessary to deal with subset of categories in the target domain at training time, and we provide a clear feedback on the rationale behind any classification decision, which can be exploited in human-robot interactions. Experiments on two different settings of the iCub World database confirm the suitability of our method for robot vision.



### Deep learning for conifer/deciduous classification of airborne LiDAR 3D point clouds representing individual trees
- **Arxiv ID**: http://arxiv.org/abs/1802.08872v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1802.08872v1)
- **Published**: 2018-02-24 16:10:39+00:00
- **Updated**: 2018-02-24 16:10:39+00:00
- **Authors**: Hamid Hamraz, Nathan B. Jacobs, Marco A. Contreras, Chase H. Clark
- **Comment**: Under review as of the date of submission
- **Journal**: None
- **Summary**: The purpose of this study was to investigate the use of deep learning for coniferous/deciduous classification of individual trees from airborne LiDAR data. To enable efficient processing by a deep convolutional neural network (CNN), we designed two discrete representations using leaf-off and leaf-on LiDAR data: a digital surface model with four channels (DSMx4) and a set of four 2D views (4x2D). A training dataset of labeled tree crowns was generated via segmentation of tree crowns, followed by co-registration with field data. Potential mislabels due to GPS error or tree leaning were corrected using a statistical ensemble filtering procedure. Because the training data was heavily unbalanced (~8% conifers), we trained an ensemble of CNNs on random balanced sub-samples of augmented data (180 rotational variations per instance). The 4x2D representation yielded similar classification accuracies to the DSMx4 representation (~82% coniferous and ~90% deciduous) while converging faster. The data augmentation improved the classification accuracies, but more real training instances (especially coniferous) likely results in much stronger improvements. Leaf-off LiDAR data were the primary source of useful information, which is likely due to the perennial nature of coniferous foliage. LiDAR intensity values also proved to be useful, but normalization yielded no significant improvements. Lastly, the classification accuracies of overstory trees (~90%) were more balanced than those of understory trees (~90% deciduous and ~65% coniferous), which is likely due to the incomplete capture of understory tree crowns via airborne LiDAR. Automatic derivation of optimal features via deep learning provide the opportunity for remarkable improvements in prediction tasks where captured data are not friendly to human visual system - likely yielding sub-optimal human-designed features.



### Improving Recall of In Situ Sequencing by Self-Learned Features and a Graphical Model
- **Arxiv ID**: http://arxiv.org/abs/1802.08894v1
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1802.08894v1)
- **Published**: 2018-02-24 18:53:56+00:00
- **Updated**: 2018-02-24 18:53:56+00:00
- **Authors**: Gabriele Partel, Giorgia Milli, Carolina Wählby
- **Comment**: 4 pages, 3 figures
- **Journal**: None
- **Summary**: Image-based sequencing of mRNA makes it possible to see where in a tissue sample a given gene is active, and thus discern large numbers of different cell types in parallel. This is crucial for gaining a better understanding of tissue development and disease such as cancer. Signals are collected over multiple staining and imaging cycles, and signal density together with noise makes signal decoding challenging. Previous approaches have led to low signal recall in efforts to maintain high sensitivity. We propose an approach where signal candidates are generously included, and true-signal probability at the cycle level is self-learned using a convolutional neural network. Signal candidates and probability predictions are thereafter fed into a graphical model searching for signal candidates across sequencing cycles. The graphical model combines intensity, probability and spatial distance to find optimal paths representing decoded signal sequences. We evaluate our approach in relation to state-of-the-art, and show that we increase recall by $27\%$ at maintained sensitivity. Furthermore, visual examination shows that most of the now correctly resolved signals were previously lost due to high signal density. Thus, the proposed approach has the potential to significantly improve further analysis of spatial statistics in in situ sequencing experiments.



### Free-breathing cardiac MRI using bandlimited manifold modelling
- **Arxiv ID**: http://arxiv.org/abs/1802.08909v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.08909v1)
- **Published**: 2018-02-24 20:43:23+00:00
- **Updated**: 2018-02-24 20:43:23+00:00
- **Authors**: Sunrita Poddar, Yasir Mohsin, Deidra Ansah, Bijoy Thattaliyath, Ravi Ashwath, Mathews Jacob
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a novel bandlimited manifold framework and an algorithm to recover freebreathing and ungated cardiac MR images from highly undersampled measurements. The image frames in the free breathing and ungated dataset are assumed to be points on a bandlimited manifold. We introduce a novel kernel low-rank algorithm to estimate the manifold structure (Laplacian) from a navigator-based acquisition scheme. The structure of the manifold is then used to recover the images from highly undersampled measurements. A computationally efficient algorithm, which relies on the bandlimited approximation of the Laplacian matrix, is used to recover the images. The proposed scheme is demonstrated on several patients with different breathing patterns and cardiac rates, without requiring the need for manually tuning the reconstruction parameters in each case. The proposed scheme enabled the recovery of free-breathing and ungated data, providing reconstructions that are qualitatively similar to breath-held scans performed on the same patients. This shows the potential of the technique as a clinical protocol for free-breathing cardiac scans.



### Generating retinal flow maps from structural optical coherence tomography with artificial intelligence
- **Arxiv ID**: http://arxiv.org/abs/1802.08925v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1802.08925v1)
- **Published**: 2018-02-24 22:51:43+00:00
- **Updated**: 2018-02-24 22:51:43+00:00
- **Authors**: Cecilia S. Lee, Ariel J. Tyring, Yue Wu, Sa Xiao, Ariel S. Rokem, Nicolaas P. Deruyter, Qinqin Zhang, Adnan Tufail, Ruikang K. Wang, Aaron Y. Lee
- **Comment**: Under revision at Nature Communications. Submitted on June 5th 2017
- **Journal**: None
- **Summary**: Despite significant advances in artificial intelligence (AI) for computer vision, its application in medical imaging has been limited by the burden and limits of expert-generated labels. We used images from optical coherence tomography angiography (OCTA), a relatively new imaging modality that measures perfusion of the retinal vasculature, to train an AI algorithm to generate vasculature maps from standard structural optical coherence tomography (OCT) images of the same retinae, both exceeding the ability and bypassing the need for expert labeling. Deep learning was able to infer perfusion of microvasculature from structural OCT images with similar fidelity to OCTA and significantly better than expert clinicians (P < 0.00001). OCTA suffers from need of specialized hardware, laborious acquisition protocols, and motion artifacts; whereas our model works directly from standard OCT which are ubiquitous and quick to obtain, and allows unlocking of large volumes of previously collected standard OCT data both in existing clinical trials and clinical practice. This finding demonstrates a novel application of AI to medical imaging, whereby subtle regularities between different modalities are used to image the same body part and AI is used to generate detailed and accurate inferences of tissue function from structure imaging.



