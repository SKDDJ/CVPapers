# Arxiv Papers in cs.CV on 2018-02-25
### A Dataset To Evaluate The Representations Learned By Video Prediction Models
- **Arxiv ID**: http://arxiv.org/abs/1802.08936v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.08936v3)
- **Published**: 2018-02-25 01:01:17+00:00
- **Updated**: 2018-03-22 00:45:02+00:00
- **Authors**: Ryan Szeto, Simon Stent, German Ros, Jason J. Corso
- **Comment**: Accepted to ICLR 2018 Workshop Track. Fixed Figure 2
- **Journal**: None
- **Summary**: We present a parameterized synthetic dataset called Moving Symbols to support the objective study of video prediction networks. Using several instantiations of the dataset in which variation is explicitly controlled, we highlight issues in an existing state-of-the-art approach and propose the use of a performance metric with greater semantic meaning to improve experimental interpretability. Our dataset provides canonical test cases that will help the community better understand, and eventually improve, the representations learned by such networks in the future. Code is available at https://github.com/rszeto/moving-symbols .



### Detecting Comma-shaped Clouds for Severe Weather Forecasting using Shape and Motion
- **Arxiv ID**: http://arxiv.org/abs/1802.08937v3
- **DOI**: 10.1109/TGRS.2018.2887206
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.08937v3)
- **Published**: 2018-02-25 01:15:47+00:00
- **Updated**: 2018-12-13 18:27:28+00:00
- **Authors**: Xinye Zheng, Jianbo Ye, Yukun Chen, Stephen Wistar, Jia Li, Jose A. Piedra-Fernández, Michael A. Steinberg, James Z. Wang
- **Comment**: Under submission
- **Journal**: None
- **Summary**: Meteorologists use shapes and movements of clouds in satellite images as indicators of several major types of severe storms. Satellite imaginary data are in increasingly higher resolution, both spatially and temporally, making it impossible for humans to fully leverage the data in their forecast. Automatic satellite imagery analysis methods that can find storm-related cloud patterns as soon as they are detectable are in demand. We propose a machine learning and pattern recognition based approach to detect "comma-shaped" clouds in satellite images, which are specific cloud distribution patterns strongly associated with the cyclone formulation. In order to detect regions with the targeted movement patterns, our method is trained on manually annotated cloud examples represented by both shape and motion-sensitive features. Sliding windows in different scales are used to ensure that dense clouds will be captured, and we implement effective selection rules to shrink the region of interest among these sliding windows. Finally, we evaluate the method on a hold-out annotated comma-shaped cloud dataset and cross-match the results with recorded storm events in the severe weather database. The validated utility and accuracy of our method suggest a high potential for assisting meteorologists in weather forecasting.



### Multi-Oriented Scene Text Detection via Corner Localization and Region Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1802.08948v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.08948v2)
- **Published**: 2018-02-25 03:34:12+00:00
- **Updated**: 2018-02-27 03:50:15+00:00
- **Authors**: Pengyuan Lyu, Cong Yao, Wenhao Wu, Shuicheng Yan, Xiang Bai
- **Comment**: To appear in CVPR2018
- **Journal**: None
- **Summary**: Previous deep learning based state-of-the-art scene text detection methods can be roughly classified into two categories. The first category treats scene text as a type of general objects and follows general object detection paradigm to localize scene text by regressing the text box locations, but troubled by the arbitrary-orientation and large aspect ratios of scene text. The second one segments text regions directly, but mostly needs complex post processing. In this paper, we present a method that combines the ideas of the two types of methods while avoiding their shortcomings. We propose to detect scene text by localizing corner points of text bounding boxes and segmenting text regions in relative positions. In inference stage, candidate boxes are generated by sampling and grouping corner points, which are further scored by segmentation maps and suppressed by NMS. Compared with previous methods, our method can handle long oriented text naturally and doesn't need complex post processing. The experiments on ICDAR2013, ICDAR2015, MSRA-TD500, MLT and COCO-Text demonstrate that the proposed algorithm achieves better or comparable results in both accuracy and efficiency. Based on VGG16, it achieves an F-measure of 84.3% on ICDAR2015 and 81.5% on MSRA-TD500.



### Bonnet: An Open-Source Training and Deployment Framework for Semantic Segmentation in Robotics using CNNs
- **Arxiv ID**: http://arxiv.org/abs/1802.08960v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1802.08960v2)
- **Published**: 2018-02-25 06:47:30+00:00
- **Updated**: 2019-02-01 17:08:34+00:00
- **Authors**: Andres Milioto, Cyrill Stachniss
- **Comment**: To be published in to IEEE International Conference on Robotics and
  Automation 2019
- **Journal**: None
- **Summary**: The ability to interpret a scene is an important capability for a robot that is supposed to interact with its environment. The knowledge of what is in front of the robot is, for example, relevant for navigation, manipulation, or planning. Semantic segmentation labels each pixel of an image with a class label and thus provides a detailed semantic annotation of the surroundings to the robot. Convolutional neural networks (CNNs) are popular methods for addressing this type of problem. The available software for training and the integration of CNNs for real robots, however, is quite fragmented and often difficult to use for non-experts, despite the availability of several high-quality open-source frameworks for neural network implementation and training. In this paper, we propose a tool called Bonnet, which addresses this fragmentation problem by building a higher abstraction that is specific for the semantic segmentation task. It provides a modular approach to simplify the training of a semantic segmentation CNN independently of the used dataset and the intended task. Furthermore, we also address the deployment on a real robotic platform. Thus, we do not propose a new CNN approach in this paper. Instead, we provide a stable and easy-to-use tool to make this technology more approachable in the context of autonomous systems. In this sense, we aim at closing a gap between computer vision research and its use in robotics research. We provide an open-source codebase for training and deployment. The training interface is implemented in Python using TensorFlow and the deployment interface provides a C++ library that can be easily integrated in an existing robotics codebase, a ROS node, and two standalone applications for label prediction in images and videos.



### Building Instance Classification Using Street View Images
- **Arxiv ID**: http://arxiv.org/abs/1802.09026v1
- **DOI**: 10.1016/j.isprsjprs.2018.02.006
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1802.09026v1)
- **Published**: 2018-02-25 16:04:34+00:00
- **Updated**: 2018-02-25 16:04:34+00:00
- **Authors**: Jian Kang, Marco Körner, Yuanyuan Wang, Hannes Taubenböck, Xiao Xiang Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Land-use classification based on spaceborne or aerial remote sensing images has been extensively studied over the past decades. Such classification is usually a patch-wise or pixel-wise labeling over the whole image. But for many applications, such as urban population density mapping or urban utility planning, a classification map based on individual buildings is much more informative. However, such semantic classification still poses some fundamental challenges, for example, how to retrieve fine boundaries of individual buildings. In this paper, we proposed a general framework for classifying the functionality of individual buildings. The proposed method is based on Convolutional Neural Networks (CNNs) which classify facade structures from street view images, such as Google StreetView, in addition to remote sensing images which usually only show roof structures. Geographic information was utilized to mask out individual buildings, and to associate the corresponding street view images. We created a benchmark dataset which was used for training and evaluating CNNs. In addition, the method was applied to generate building classification maps on both region and city scales of several cities in Canada and the US. Keywords: CNN, Building instance classification, Street view images, OpenStreetMap



### Wide Compression: Tensor Ring Nets
- **Arxiv ID**: http://arxiv.org/abs/1802.09052v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1802.09052v1)
- **Published**: 2018-02-25 18:09:04+00:00
- **Updated**: 2018-02-25 18:09:04+00:00
- **Authors**: Wenqi Wang, Yifan Sun, Brian Eriksson, Wenlin Wang, Vaneet Aggarwal
- **Comment**: Accepted to CVPR 2018
- **Journal**: None
- **Summary**: Deep neural networks have demonstrated state-of-the-art performance in a variety of real-world applications. In order to obtain performance gains, these networks have grown larger and deeper, containing millions or even billions of parameters and over a thousand layers. The trade-off is that these large architectures require an enormous amount of memory, storage, and computation, thus limiting their usability. Inspired by the recent tensor ring factorization, we introduce Tensor Ring Networks (TR-Nets), which significantly compress both the fully connected layers and the convolutional layers of deep neural networks. Our results show that our TR-Nets approach {is able to compress LeNet-5 by $11\times$ without losing accuracy}, and can compress the state-of-the-art Wide ResNet by $243\times$ with only 2.3\% degradation in {Cifar10 image classification}. Overall, this compression scheme shows promise in scientific computing and deep learning, especially for emerging resource-constrained devices such as smartphones, wearables, and IoT devices.



### Seeing Small Faces from Robust Anchor's Perspective
- **Arxiv ID**: http://arxiv.org/abs/1802.09058v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.09058v1)
- **Published**: 2018-02-25 18:48:53+00:00
- **Updated**: 2018-02-25 18:48:53+00:00
- **Authors**: Chenchen Zhu, Ran Tao, Khoa Luu, Marios Savvides
- **Comment**: CVPR 2018
- **Journal**: None
- **Summary**: This paper introduces a novel anchor design to support anchor-based face detection for superior scale-invariant performance, especially on tiny faces. To achieve this, we explicitly address the problem that anchor-based detectors drop performance drastically on faces with tiny sizes, e.g. less than 16x16 pixels. In this paper, we investigate why this is the case. We discover that current anchor design cannot guarantee high overlaps between tiny faces and anchor boxes, which increases the difficulty of training. The new Expected Max Overlapping (EMO) score is proposed which can theoretically explain the low overlapping issue and inspire several effective strategies of new anchor design leading to higher face overlaps, including anchor stride reduction with new network architectures, extra shifted anchors, and stochastic face shifting. Comprehensive experiments show that our proposed method significantly outperforms the baseline anchor-based detector, while consistently achieving state-of-the-art results on challenging face detection datasets with competitive runtime speed.



### Attention-Aware Generative Adversarial Networks (ATA-GANs)
- **Arxiv ID**: http://arxiv.org/abs/1802.09070v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.09070v1)
- **Published**: 2018-02-25 19:40:30+00:00
- **Updated**: 2018-02-25 19:40:30+00:00
- **Authors**: Dimitris Kastaniotis, Ioanna Ntinou, Dimitrios Tsourounis, George Economou, Spiros Fotopoulos
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we present a novel approach for training Generative Adversarial Networks (GANs). Using the attention maps produced by a Teacher- Network we are able to improve the quality of the generated images as well as perform weakly object localization on the generated images. To this end, we generate images of HEp-2 cells captured with Indirect Imunofluoresence (IIF) and study the ability of our network to perform a weakly localization of the cell. Firstly, we demonstrate that whilst GANs can learn the mapping between the input domain and the target distribution efficiently, the discriminator network is not able to detect the regions of interest. Secondly, we present a novel attention transfer mechanism which allows us to enforce the discriminator to put emphasis on the regions of interest via transfer learning. Thirdly, we show that this leads to more realistic images, as the discriminator learns to put emphasis on the area of interest. Fourthly, the proposed method allows one to generate both images as well as attention maps which can be useful for data annotation e.g in object detection.



### Adversarially Learned One-Class Classifier for Novelty Detection
- **Arxiv ID**: http://arxiv.org/abs/1802.09088v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.09088v2)
- **Published**: 2018-02-25 21:34:27+00:00
- **Updated**: 2018-05-24 06:41:55+00:00
- **Authors**: Mohammad Sabokrou, Mohammad Khalooei, Mahmood Fathy, Ehsan Adeli
- **Comment**: CVPR 2018 Paper
- **Journal**: None
- **Summary**: Novelty detection is the process of identifying the observation(s) that differ in some respect from the training observations (the target class). In reality, the novelty class is often absent during training, poorly sampled or not well defined. Therefore, one-class classifiers can efficiently model such problems. However, due to the unavailability of data from the novelty class, training an end-to-end deep network is a cumbersome task. In this paper, inspired by the success of generative adversarial networks for training deep models in unsupervised and semi-supervised settings, we propose an end-to-end architecture for one-class classification. Our architecture is composed of two deep networks, each of which trained by competing with each other while collaborating to understand the underlying concept in the target class, and then classify the testing samples. One network works as the novelty detector, while the other supports it by enhancing the inlier samples and distorting the outliers. The intuition is that the separability of the enhanced inliers and distorted outliers is much better than deciding on the original samples. The proposed framework applies to different related applications of anomaly and outlier detection in images and videos. The results on MNIST and Caltech-256 image datasets, along with the challenging UCSD Ped2 dataset for video anomaly detection illustrate that our proposed method learns the target class effectively and is superior to the baseline and state-of-the-art methods.



