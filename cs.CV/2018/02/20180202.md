# Arxiv Papers in cs.CV on 2018-02-02
### Complex Network Classification with Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1802.00539v2
- **DOI**: 10.26599/TST.2019.9010055
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.00539v2)
- **Published**: 2018-02-02 02:12:33+00:00
- **Updated**: 2018-04-08 13:31:41+00:00
- **Authors**: Ruyue Xin, Jiang Zhang, Yitong Shao
- **Comment**: None
- **Journal**: None
- **Summary**: Classifying large scale networks into several categories and distinguishing them according to their fine structures is of great importance with several applications in real life. However, most studies of complex networks focus on properties of a single network but seldom on classification, clustering, and comparison between different networks, in which the network is treated as a whole. Due to the non-Euclidean properties of the data, conventional methods can hardly be applied on networks directly. In this paper, we propose a novel framework of complex network classifier (CNC) by integrating network embedding and convolutional neural network to tackle the problem of network classification. By training the classifiers on synthetic complex network data and real international trade network data, we show CNC can not only classify networks in a high accuracy and robustness, it can also extract the features of the networks automatically.



### ExpNet: Landmark-Free, Deep, 3D Facial Expressions
- **Arxiv ID**: http://arxiv.org/abs/1802.00542v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.00542v1)
- **Published**: 2018-02-02 02:42:44+00:00
- **Updated**: 2018-02-02 02:42:44+00:00
- **Authors**: Feng-Ju Chang, Anh Tuan Tran, Tal Hassner, Iacopo Masi, Ram Nevatia, Gerard Medioni
- **Comment**: Accepted to the IEEE International Conference on Automatic Face and
  Gesture Recognition, 2018
- **Journal**: None
- **Summary**: We describe a deep learning based method for estimating 3D facial expression coefficients. Unlike previous work, our process does not relay on facial landmark detection methods as a proxy step. Recent methods have shown that a CNN can be trained to regress accurate and discriminative 3D morphable model (3DMM) representations, directly from image intensities. By foregoing facial landmark detection, these methods were able to estimate shapes for occluded faces appearing in unprecedented in-the-wild viewing conditions. We build on those methods by showing that facial expressions can also be estimated by a robust, deep, landmark-free approach. Our ExpNet CNN is applied directly to the intensities of a face image and regresses a 29D vector of 3D expression coefficients. We propose a unique method for collecting data to train this network, leveraging on the robustness of deep networks to training label noise. We further offer a novel means of evaluating the accuracy of estimated expression coefficients: by measuring how well they capture facial emotions on the CK+ and EmotiW-17 emotion recognition benchmarks. We show that our ExpNet produces expression coefficients which better discriminate between facial emotions than those obtained using state of the art, facial landmark detection techniques. Moreover, this advantage grows as image scales drop, demonstrating that our ExpNet is more robust to scale changes than landmark detection methods. Finally, at the same level of accuracy, our ExpNet is orders of magnitude faster than its alternatives.



### Detecting Zones and Threat on 3D Body for Security in Airports using Deep Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/1802.00565v2
- **DOI**: 10.5281/zenodo.1189345
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.00565v2)
- **Published**: 2018-02-02 05:45:21+00:00
- **Updated**: 2018-02-09 17:14:09+00:00
- **Authors**: Abel Ag Rb Guimaraes, Ghassem Tofighi
- **Comment**: 7 pages, 17 figures, This article was accepted from the Star
  Conference, Data Science and Big Data Analyses MAY 24-25, 2018 | Toronto,
  Canada
- **Journal**: None
- **Summary**: In this research, it was used a segmentation and classification method to identify threat recognition in human scanner images of airport security. The Department of Homeland Security's (DHS) in USA has a higher false alarm, produced from theirs algorithms using today's scanners at the airports. To repair this problem they started a new competition at Kaggle site asking the science community to improve their detection with new algorithms. The dataset used in this research comes from DHS at https://www.kaggle.com/c/passenger-screening-algorithm-challenge/data According to DHS: "This dataset contains a large number of body scans acquired by a new generation of millimeter wave scanner called the High Definition-Advanced Imaging Technology (HD-AIT) system. They are comprised of volunteers wearing different clothing types (from light summer clothes to heavy winter clothes), different body mass indices, different genders, different numbers of threats, and different types of threats". Using Python as a principal language, the preprocessed of the dataset images extracted features from 200 bodies using: intensity, intensity differences and local neighbourhood to detect, to produce segmentation regions and label those regions to be used as a truth in a training and test dataset. The regions are subsequently give to a CNN deep learning classifier to predict 17 classes (that represents the body zones): zone1, zone2, ... zone17 and zones with threat in a total of 34 zones. The analysis showed the results of the classifier an accuracy of 98.2863% and a loss of 0.091319, as well as an average of 100% for recall and precision.



### Mixed-Resolution Image Representation and Compression with Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1802.01447v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.01447v2)
- **Published**: 2018-02-02 08:57:44+00:00
- **Updated**: 2018-08-01 03:26:44+00:00
- **Authors**: Lijun Zhao, Huihui Bai, Feng Li, Anhong Wang, Yao Zhao
- **Comment**: 5 pages, and 2 figures. arXiv admin note: substantial text overlap
  with arXiv:1712.05969
- **Journal**: None
- **Summary**: In this paper, we propose an end-to-end mixed-resolution image compression framework with convolutional neural networks. Firstly, given one input image, feature description neural network (FDNN) is used to generate a new representation of this image, so that this image representation can be more efficiently compressed by standard codec, as compared to the input image. Furthermore, we use post-processing neural network (PPNN) to remove the coding artifacts caused by quantization of codec. Secondly, low-resolution image representation is adopted for high efficiency compression in terms of most of bit spent by image's structures under low bit-rate. However, more bits should be assigned to image details in the high-resolution, when most of structures have been kept after compression at the high bit-rate. This comes from a fact that the low-resolution image representation can't burden more information than high-resolution representation beyond a certain bit-rate. Finally, to resolve the problem of error back-propagation from the PPNN network to the FDNN network, we introduce to learn a virtual codec neural network to imitate two continuous procedures of standard compression and post-processing. The objective experimental results have demonstrated the proposed method has a large margin improvement, when comparing with several state-of-the-art approaches.



### Visual Interpretability for Deep Learning: a Survey
- **Arxiv ID**: http://arxiv.org/abs/1802.00614v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.00614v2)
- **Published**: 2018-02-02 09:39:40+00:00
- **Updated**: 2018-02-07 08:02:59+00:00
- **Authors**: Quanshi Zhang, Song-Chun Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: This paper reviews recent studies in understanding neural-network representations and learning neural networks with interpretable/disentangled middle-layer representations. Although deep neural networks have exhibited superior performance in various tasks, the interpretability is always the Achilles' heel of deep neural networks. At present, deep neural networks obtain high discrimination power at the cost of low interpretability of their black-box representations. We believe that high model interpretability may help people to break several bottlenecks of deep learning, e.g., learning from very few annotations, learning via human-computer communications at the semantic level, and semantically debugging network representations. We focus on convolutional neural networks (CNNs), and we revisit the visualization of CNN representations, methods of diagnosing representations of pre-trained CNNs, approaches for disentangling pre-trained CNN representations, learning of CNNs with disentangled representations, and middle-to-end learning based on model interpretability. Finally, we discuss prospective trends in explainable artificial intelligence.



### When can $l_p$-norm objective functions be minimized via graph cuts?
- **Arxiv ID**: http://arxiv.org/abs/1802.00624v2
- **DOI**: None
- **Categories**: **cs.DS**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1802.00624v2)
- **Published**: 2018-02-02 10:09:22+00:00
- **Updated**: 2019-09-09 08:10:05+00:00
- **Authors**: Filip Malmberg, Robin Strand
- **Comment**: In proceedings of the 19th international workshop on combinatorial
  image analysis (IWCIA), 2018
- **Journal**: None
- **Summary**: Techniques based on minimal graph cuts have become a standard tool for solving combinatorial optimization problems arising in image processing and computer vision applications. These techniques can be used to minimize objective functions written as the sum of a set of unary and pairwise terms, provided that the objective function is submodular. This can be interpreted as minimizing the $l_1$-norm of the vector containing all pairwise and unary terms. By raising each term to a power $p$, the same technique can also be used to minimize the $l_p$-norm of the vector. Unfortunately, the submodularity of an $l_1$-norm objective function does not guarantee the submodularity of the corresponding $l_p$-norm objective function. The contribution of this paper is to provide useful conditions under which an $l_p$-norm objective function is submodular for all $p\geq 1$, thereby identifying a large class of $l_p$-norm objective functions that can be minimized via minimal graph cuts.



### Activity-conditioned continuous human pose estimation for performance analysis of athletes using the example of swimming
- **Arxiv ID**: http://arxiv.org/abs/1802.00634v1
- **DOI**: 10.1109/wacv.2018.00055
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.00634v1)
- **Published**: 2018-02-02 10:56:41+00:00
- **Updated**: 2018-02-02 10:56:41+00:00
- **Authors**: Moritz Einfalt, Dan Zecha, Rainer Lienhart
- **Comment**: 10 pages, 9 figures, accepted at WACV 2018
- **Journal**: None
- **Summary**: In this paper we consider the problem of human pose estimation in real-world videos of swimmers. Swimming channels allow filming swimmers simultaneously above and below the water surface with a single stationary camera. These recordings can be used to quantitatively assess the athletes' performance. The quantitative evaluation, so far, requires manual annotations of body parts in each video frame. We therefore apply the concept of CNNs in order to automatically infer the required pose information. Starting with an off-the-shelf architecture, we develop extensions to leverage activity information - in our case the swimming style of an athlete - and the continuous nature of the video recordings. Our main contributions are threefold: (a) We apply and evaluate a fine-tuned Convolutional Pose Machine architecture as a baseline in our very challenging aquatic environment and discuss its error modes, (b) we propose an extension to input swimming style information into the fully convolutional architecture and (c) modify the architecture for continuous pose estimation in videos. With these additions we achieve reliable pose estimates with up to +16% more correct body joint detections compared to the baseline architecture.



### Convolutional neural network-based regression for depth prediction in digital holography
- **Arxiv ID**: http://arxiv.org/abs/1802.00664v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1802.00664v1)
- **Published**: 2018-02-02 12:41:08+00:00
- **Updated**: 2018-02-02 12:41:08+00:00
- **Authors**: Tomoyoshi Shimobaba, Takashi Kakue, Tomoyoshi Ito
- **Comment**: None
- **Journal**: None
- **Summary**: Digital holography enables us to reconstruct objects in three-dimensional space from holograms captured by an imaging device. For the reconstruction, we need to know the depth position of the recoded object in advance. In this study, we propose depth prediction using convolutional neural network (CNN)-based regression. In the previous researches, the depth of an object was estimated through reconstructed images at different depth positions from a hologram using a certain metric that indicates the most focused depth position; however, such a depth search is time-consuming. The CNN of the proposed method can directly predict the depth position with millimeter precision from holograms.



### Handwritten Isolated Bangla Compound Character Recognition: a new benchmark using a novel deep learning approach
- **Arxiv ID**: http://arxiv.org/abs/1802.00671v1
- **DOI**: 10.1016/j.patrec.2017.03.004
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.00671v1)
- **Published**: 2018-02-02 13:06:43+00:00
- **Updated**: 2018-02-02 13:06:43+00:00
- **Authors**: Saikat Roy, Nibaran Das, Mahantapas Kundu, Mita Nasipuri
- **Comment**: None
- **Journal**: Pattern Recognition Letters, Elsevier, Vol. 90, Pages 15-21, 2017
- **Summary**: In this work, a novel deep learning technique for the recognition of handwritten Bangla isolated compound character is presented and a new benchmark of recognition accuracy on the CMATERdb 3.1.3.3 dataset is reported. Greedy layer wise training of Deep Neural Network has helped to make significant strides in various pattern recognition problems. We employ layerwise training to Deep Convolutional Neural Networks (DCNN) in a supervised fashion and augment the training process with the RMSProp algorithm to achieve faster convergence. We compare results with those obtained from standard shallow learning methods with predefined features, as well as standard DCNNs. Supervised layerwise trained DCNNs are found to outperform standard shallow learning models such as Support Vector Machines as well as regular DCNNs of similar architecture by achieving error rate of 9.67% thereby setting a new benchmark on the CMATERdb 3.1.3.3 with recognition accuracy of 90.33%, representing an improvement of nearly 10%.



### Explaining First Impressions: Modeling, Recognizing, and Explaining Apparent Personality from Videos
- **Arxiv ID**: http://arxiv.org/abs/1802.00745v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.00745v3)
- **Published**: 2018-02-02 16:02:55+00:00
- **Updated**: 2019-09-29 03:24:32+00:00
- **Authors**: Hugo Jair Escalante, Heysem Kaya, Albert Ali Salah, Sergio Escalera, Yagmur Gucluturk, Umut Guclu, Xavier Baro, Isabelle Guyon, Julio Jacques Junior, Meysam Madadi, Stephane Ayache, Evelyne Viegas, Furkan Gurpinar, Achmadnoer Sukma Wicaksana, Cynthia C. S. Liem, Marcel A. J. van Gerven, Rob van Lier
- **Comment**: Preprint submitted to TAC
- **Journal**: None
- **Summary**: Explainability and interpretability are two critical aspects of decision support systems. Within computer vision, they are critical in certain tasks related to human behavior analysis such as in health care applications. Despite their importance, it is only recently that researchers are starting to explore these aspects. This paper provides an introduction to explainability and interpretability in the context of computer vision with an emphasis on looking at people tasks. Specifically, we review and study those mechanisms in the context of first impressions analysis. To the best of our knowledge, this is the first effort in this direction. Additionally, we describe a challenge we organized on explainability in first impressions analysis from video. We analyze in detail the newly introduced data set, the evaluation protocol, and summarize the results of the challenge. Finally, derived from our study, we outline research opportunities that we foresee will be decisive in the near future for the development of the explainable computer vision field.



### Deep Convolutional Neural Networks for Breast Cancer Histology Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/1802.00752v2
- **DOI**: 10.1007/978-3-319-93000-8_83
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.00752v2)
- **Published**: 2018-02-02 16:20:58+00:00
- **Updated**: 2018-04-03 13:59:59+00:00
- **Authors**: Alexander Rakhlin, Alexey Shvets, Vladimir Iglovikov, Alexandr A. Kalinin
- **Comment**: 8 pages, 4 figures
- **Journal**: None
- **Summary**: Breast cancer is one of the main causes of cancer death worldwide. Early diagnostics significantly increases the chances of correct treatment and survival, but this process is tedious and often leads to a disagreement between pathologists. Computer-aided diagnosis systems showed potential for improving the diagnostic accuracy. In this work, we develop the computational approach based on deep convolution neural networks for breast cancer histology image classification. Hematoxylin and eosin stained breast histology microscopy image dataset is provided as a part of the ICIAR 2018 Grand Challenge on Breast Cancer Histology Images. Our approach utilizes several deep neural network architectures and gradient boosted trees classifier. For 4-class classification task, we report 87.2% accuracy. For 2-class classification task to detect carcinomas we report 93.8% accuracy, AUC 97.3%, and sensitivity/specificity 96.5/88.0% at the high-sensitivity operating point. To our knowledge, this approach outperforms other common methods in automated histopathological image classification. The source code for our approach is made publicly available at https://github.com/alexander-rakhlin/ICIAR2018



### Learning Attribute Representation for Human Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/1802.00761v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.00761v1)
- **Published**: 2018-02-02 16:37:30+00:00
- **Updated**: 2018-02-02 16:37:30+00:00
- **Authors**: Fernando Moya Rueda, Gernot A. Fink
- **Comment**: 6 pages, submitted to ICPR 2018
- **Journal**: None
- **Summary**: Attribute representations became relevant in image recognition and word spotting, providing support under the presence of unbalance and disjoint datasets. However, for human activity recognition using sequential data from on-body sensors, human-labeled attributes are lacking. This paper introduces a search for attributes that represent favorably signal segments for recognizing human activities. It presents three deep architectures, including temporal-convolutions and an IMU centered design, for predicting attributes. An empiric evaluation of random and learned attribute representations, and as well as the networks is carried out on two datasets, outperforming the state-of-the art.



### No Modes left behind: Capturing the data distribution effectively using GANs
- **Arxiv ID**: http://arxiv.org/abs/1802.00771v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.00771v1)
- **Published**: 2018-02-02 17:10:55+00:00
- **Updated**: 2018-02-02 17:10:55+00:00
- **Authors**: Shashank Sharma, Vinay P. Namboodiri
- **Comment**: accepted to AAAI 2018 conference
- **Journal**: None
- **Summary**: Generative adversarial networks (GANs) while being very versatile in realistic image synthesis, still are sensitive to the input distribution. Given a set of data that has an imbalance in the distribution, the networks are susceptible to missing modes and not capturing the data distribution. While various methods have been tried to improve training of GANs, these have not addressed the challenges of covering the full data distribution. Specifically, a generator is not penalized for missing a mode. We show that these are therefore still susceptible to not capturing the full data distribution.   In this paper, we propose a simple approach that combines an encoder based objective with novel loss functions for generator and discriminator that improves the solution in terms of capturing missing modes. We validate that the proposed method results in substantial improvements through its detailed analysis on toy and real datasets. The quantitative and qualitative results demonstrate that the proposed method improves the solution for the problem of missing modes and improves training of GANs.



### Green Stability Assumption: Unsupervised Learning for Statistics-Based Illumination Estimation
- **Arxiv ID**: http://arxiv.org/abs/1802.00776v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.00776v1)
- **Published**: 2018-02-02 17:19:40+00:00
- **Updated**: 2018-02-02 17:19:40+00:00
- **Authors**: Nikola Banić, Sven Lončarić
- **Comment**: 5 pages, 3 figures
- **Journal**: None
- **Summary**: In the image processing pipeline of almost every digital camera there is a part dedicated to computational color constancy i.e. to removing the influence of illumination on the colors of the image scene. Some of the best known illumination estimation methods are the so called statistics-based methods. They are less accurate than the learning-based illumination estimation methods, but they are faster and simpler to implement in embedded systems, which is one of the reasons for their widespread usage. Although in the relevant literature it often appears as if they require no training, this is not true because they have parameter values that need to be fine-tuned in order to be more accurate. In this paper it is first shown that the accuracy of statistics-based methods reported in most papers was not obtained by means of the necessary cross-validation, but by using the whole benchmark datasets for both training and testing. After that the corrected results are given for the best known benchmark datasets. Finally, the so called green stability assumption is proposed that can be used to fine-tune the values of the parameters of the statistics-based methods by using only non-calibrated images without known ground-truth illumination. The obtained accuracy is practically the same as when using calibrated training images, but the whole process is much faster. The experimental results are presented and discussed. The source code is available at http://www.fer.unizg.hr/ipg/resources/color_constancy/.



### Intriguing Properties of Randomly Weighted Networks: Generalizing While Learning Next to Nothing
- **Arxiv ID**: http://arxiv.org/abs/1802.00844v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1802.00844v1)
- **Published**: 2018-02-02 20:53:31+00:00
- **Updated**: 2018-02-02 20:53:31+00:00
- **Authors**: Amir Rosenfeld, John K. Tsotsos
- **Comment**: None
- **Journal**: None
- **Summary**: Training deep neural networks results in strong learned representations that show good generalization capabilities. In most cases, training involves iterative modification of all weights inside the network via back-propagation. In Extreme Learning Machines, it has been suggested to set the first layer of a network to fixed random values instead of learning it. In this paper, we propose to take this approach a step further and fix almost all layers of a deep convolutional neural network, allowing only a small portion of the weights to be learned. As our experiments show, fixing even the majority of the parameters of the network often results in performance which is on par with the performance of learning all of them. The implications of this intriguing property of deep neural networks are discussed and we suggest ways to harness it to create more robust representations.



### Incremental Classifier Learning with Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1802.00853v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.00853v1)
- **Published**: 2018-02-02 21:35:45+00:00
- **Updated**: 2018-02-02 21:35:45+00:00
- **Authors**: Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye, Zicheng Liu, Yandong Guo, Zhengyou Zhang, Yun Fu
- **Comment**: 10 pages, 6 figures
- **Journal**: None
- **Summary**: In this paper, we address the incremental classifier learning problem, which suffers from catastrophic forgetting. The main reason for catastrophic forgetting is that the past data are not available during learning. Typical approaches keep some exemplars for the past classes and use distillation regularization to retain the classification capability on the past classes and balance the past and new classes. However, there are four main problems with these approaches. First, the loss function is not efficient for classification. Second, there is unbalance problem between the past and new classes. Third, the size of pre-decided exemplars is usually limited and they might not be distinguishable from unseen new classes. Forth, the exemplars may not be allowed to be kept for a long time due to privacy regulations. To address these problems, we propose (a) a new loss function to combine the cross-entropy loss and distillation loss, (b) a simple way to estimate and remove the unbalance between the old and new classes , and (c) using Generative Adversarial Networks (GANs) to generate historical data and select representative exemplars during generation. We believe that the data generated by GANs have much less privacy issues than real images because GANs do not directly copy any real image patches. We evaluate the proposed method on CIFAR-100, Flower-102, and MS-Celeb-1M-Base datasets and extensive experiments demonstrate the effectiveness of our method.



