# Arxiv Papers in cs.CV on 2018-02-28
### Neural Aesthetic Image Reviewer
- **Arxiv ID**: http://arxiv.org/abs/1802.10240v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.10240v1)
- **Published**: 2018-02-28 02:58:30+00:00
- **Updated**: 2018-02-28 02:58:30+00:00
- **Authors**: Wenshan Wang, Su Yang, Weishan Zhang, Jiulong Zhang
- **Comment**: 8 pages, 13 figures
- **Journal**: None
- **Summary**: Recently, there is a rising interest in perceiving image aesthetics. The existing works deal with image aesthetics as a classification or regression problem. To extend the cognition from rating to reasoning, a deeper understanding of aesthetics should be based on revealing why a high- or low-aesthetic score should be assigned to an image. From such a point of view, we propose a model referred to as Neural Aesthetic Image Reviewer, which can not only give an aesthetic score for an image, but also generate a textual description explaining why the image leads to a plausible rating score. Specifically, we propose two multi-task architectures based on shared aesthetically semantic layers and task-specific embedding layers at a high level for performance improvement on different tasks. To facilitate researches on this problem, we collect the AVA-Reviews dataset, which contains 52,118 images and 312,708 comments in total. Through multi-task learning, the proposed models can rate aesthetic images as well as produce comments in an end-to-end manner. It is confirmed that the proposed models outperform the baselines according to the performance evaluation on the AVA-Reviews dataset. Moreover, we demonstrate experimentally that our model can generate textual reviews related to aesthetics, which are consistent with human perception.



### IM2HEIGHT: Height Estimation from Single Monocular Imagery via Fully Residual Convolutional-Deconvolutional Network
- **Arxiv ID**: http://arxiv.org/abs/1802.10249v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.10249v1)
- **Published**: 2018-02-28 03:32:36+00:00
- **Updated**: 2018-02-28 03:32:36+00:00
- **Authors**: Lichao Mou, Xiao Xiang Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we tackle a very novel problem, namely height estimation from a single monocular remote sensing image, which is inherently ambiguous, and a technically ill-posed problem, with a large source of uncertainty coming from the overall scale. We propose a fully convolutional-deconvolutional network architecture being trained end-to-end, encompassing residual learning, to model the ambiguous mapping between monocular remote sensing images and height maps. Specifically, it is composed of two parts, i.e., convolutional sub-network and deconvolutional sub-network. The former corresponds to feature extractor that transforms the input remote sensing image to high-level multidimensional feature representation, whereas the latter plays the role of a height generator that produces height map from the feature extracted from the convolutional sub-network. Moreover, to preserve fine edge details of estimated height maps, we introduce a skip connection to the network, which is able to shuttle low-level visual information, e.g., object boundaries and edges, directly across the network. To demonstrate the usefulness of single-view height prediction, we show a practical example of instance segmentation of buildings using estimated height map. This paper, for the first time in the remote sensing community, attempts to estimate height from monocular vision. The proposed network is validated using a large-scale high resolution aerial image data set covered an area of Berlin. Both visual and quantitative analysis of the experimental results demonstrate the effectiveness of our approach.



### Joint Event Detection and Description in Continuous Video Streams
- **Arxiv ID**: http://arxiv.org/abs/1802.10250v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.10250v3)
- **Published**: 2018-02-28 03:40:05+00:00
- **Updated**: 2018-12-25 05:58:28+00:00
- **Authors**: Huijuan Xu, Boyang Li, Vasili Ramanishka, Leonid Sigal, Kate Saenko
- **Comment**: WACV2019
- **Journal**: None
- **Summary**: Dense video captioning is a fine-grained video understanding task that involves two sub-problems: localizing distinct events in a long video stream, and generating captions for the localized events. We propose the Joint Event Detection and Description Network (JEDDi-Net), which solves the dense video captioning task in an end-to-end fashion. Our model continuously encodes the input video stream with three-dimensional convolutional layers, proposes variable-length temporal events based on pooled features, and generates their captions. Proposal features are extracted within each proposal segment through 3D Segment-of-Interest pooling from shared video feature encoding. In order to explicitly model temporal relationships between visual events and their captions in a single video, we also propose a two-level hierarchical captioning module that keeps track of context. On the large-scale ActivityNet Captions dataset, JEDDi-Net demonstrates improved results as measured by standard metrics. We also present the first dense captioning results on the TACoS-MultiLevel dataset.



### Frank-Wolfe Network: An Interpretable Deep Structure for Non-Sparse Coding
- **Arxiv ID**: http://arxiv.org/abs/1802.10252v4
- **DOI**: 10.1109/TCSVT.2019.2936135
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.10252v4)
- **Published**: 2018-02-28 03:49:08+00:00
- **Updated**: 2019-08-16 01:28:00+00:00
- **Authors**: Dong Liu, Ke Sun, Zhangyang Wang, Runsheng Liu, Zheng-Jun Zha
- **Comment**: Accepted to IEEE Transactions on Circuits and Systems for Video
  Technology. Code and pretrained models: https://github.com/sunke123/FW-Net
- **Journal**: None
- **Summary**: The problem of $L_p$-norm constrained coding is to convert signal into code that lies inside an $L_p$-ball and most faithfully reconstructs the signal. Previous works under the name of sparse coding considered the cases of $L_0$ and $L_1$ norms. The cases with $p>1$ values, i.e. non-sparse coding studied in this paper, remain a difficulty. We propose an interpretable deep structure namely Frank-Wolfe Network (F-W Net), whose architecture is inspired by unrolling and truncating the Frank-Wolfe algorithm for solving an $L_p$-norm constrained problem with $p\geq 1$. We show that the Frank-Wolfe solver for the $L_p$-norm constraint leads to a novel closed-form nonlinear unit, which is parameterized by $p$ and termed $pool_p$. The $pool_p$ unit links the conventional pooling, activation, and normalization operations, making F-W Net distinct from existing deep networks either heuristically designed or converted from projected gradient descent algorithms. We further show that the hyper-parameter $p$ can be made learnable instead of pre-chosen in F-W Net, which gracefully solves the non-sparse coding problem even with unknown $p$. We evaluate the performance of F-W Net on an extensive range of simulations as well as the task of handwritten digit recognition, where F-W Net exhibits strong learning capability. We then propose a convolutional version of F-W Net, and apply the convolutional F-W Net into image denoising and super-resolution tasks, where F-W Net all demonstrates impressive effectiveness, flexibility, and robustness.



### Escoin: Efficient Sparse Convolutional Neural Network Inference on GPUs
- **Arxiv ID**: http://arxiv.org/abs/1802.10280v2
- **DOI**: None
- **Categories**: **cs.DC**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1802.10280v2)
- **Published**: 2018-02-28 06:31:45+00:00
- **Updated**: 2019-04-03 21:11:27+00:00
- **Authors**: Xuhao Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks have achieved remarkable accuracy in many artificial intelligence applications, e.g. computer vision, at the cost of a large number of parameters and high computational complexity. Weight pruning can compress DNN models by removing redundant parameters in the networks, but it brings sparsity in the weight matrix, and therefore makes the computation inefficient on GPUs. Although pruning can remove more than 80% of the weights, it actually hurts inference performance (speed) when running models on GPUs.   Two major problems cause this unsatisfactory performance on GPUs. First, lowering convolution onto matrix multiplication reduces data reuse opportunities and wastes memory bandwidth. Second, the sparsity brought by pruning makes the computation irregular, which leads to inefficiency when running on massively parallel GPUs. To overcome these two limitations, we propose Escort, an efficient sparse convolutional neural networks on GPUs. Instead of using the lowering method, we choose to compute the sparse convolutions directly. We then orchestrate the parallelism and locality for the direct sparse convolution kernel, and apply customized optimization techniques to further improve performance. Evaluation on NVIDIA GPUs show that Escort can improve sparse convolution speed by 2.63x and 3.07x, and inference speed by 1.43x and 1.69x, compared to CUBLAS and CUSPARSE respectively.



### Speeding Up the Bilateral Filter: A Joint Acceleration Way
- **Arxiv ID**: http://arxiv.org/abs/1803.00004v1
- **DOI**: 10.1109/TIP.2016.2549701
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.00004v1)
- **Published**: 2018-02-28 07:14:59+00:00
- **Updated**: 2018-02-28 07:14:59+00:00
- **Authors**: Longquan Dai, Mengke Yuan, Xiaopeng Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Computational complexity of the brute-force implementation of the bilateral filter (BF) depends on its filter kernel size. To achieve the constant-time BF whose complexity is irrelevant to the kernel size, many techniques have been proposed, such as 2D box filtering, dimension promotion, and shiftability property. Although each of the above techniques suffers from accuracy and efficiency problems, previous algorithm designers were used to take only one of them to assemble fast implementations due to the hardness of combining them together. Hence, no joint exploitation of these techniques has been proposed to construct a new cutting edge implementation that solves these problems. Jointly employing five techniques: kernel truncation, best N -term approximation as well as previous 2D box filtering, dimension promotion, and shiftability property, we propose a unified framework to transform BF with arbitrary spatial and range kernels into a set of 3D box filters that can be computed in linear time. To the best of our knowledge, our algorithm is the first method that can integrate all these acceleration techniques and, therefore, can draw upon one another's strong point to overcome deficiencies. The strength of our method has been corroborated by several carefully designed experiments. In particular, the filtering accuracy is significantly improved without sacrificing the efficiency at running time.



### Hardware-Efficient Guided Image Filtering For Multi-Label Problem
- **Arxiv ID**: http://arxiv.org/abs/1803.00005v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.00005v1)
- **Published**: 2018-02-28 07:27:43+00:00
- **Updated**: 2018-02-28 07:27:43+00:00
- **Authors**: Longquan Dai, Mengke Yuan, Zechao Li, Xiaopeng Zhang, Jinhui Tang
- **Comment**: None
- **Journal**: None
- **Summary**: The Guided Filter (GF) is well-known for its linear complexity. However, when filtering an image with an n-channel guidance, GF needs to invert an n x n matrix for each pixel. To the best of our knowledge existing matrix inverse algorithms are inefficient on current hardwares. This shortcoming limits applications of multichannel guidance in computation intensive system such as multi-label system. We need a new GF-like filter that can perform fast multichannel image guided filtering. Since the optimal linear complexity of GF cannot be minimized further, the only way thus is to bring all potentialities of current parallel computing hardwares into full play. In this paper we propose a hardware-efficient Guided Filter (HGF), which solves the efficiency problem of multichannel guided image filtering and yields competent results when applying it to multi-label problems with synthesized polynomial multichannel guidance. Specifically, in order to boost the filtering performance, HGF takes a new matrix inverse algorithm which only involves two hardware-efficient operations: element-wise arithmetic calculations and box filtering. In order to break the linear model restriction, HGF synthesizes a polynomial multichannel guidance to introduce nonlinearity. Benefiting from our polynomial guidance and hardware-efficient matrix inverse algorithm, HGF not only is more sensitive to the underlying structure of guidance but also achieves the fastest computing speed. Due to these merits, HGF obtains state-of-the-art results in terms of accuracy and efficiency in the computation intensive multi-label



### A Model for Medical Diagnosis Based on Plantar Pressure
- **Arxiv ID**: http://arxiv.org/abs/1802.10316v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.10316v1)
- **Published**: 2018-02-28 09:12:46+00:00
- **Updated**: 2018-02-28 09:12:46+00:00
- **Authors**: Guoxiong Xu, Zhengfei Wang, Hongshi Huang, Wenxin Li, Can Liu, Shilei Liu
- **Comment**: None
- **Journal**: None
- **Summary**: The process of determining which disease or condition explains a person's symptoms and signs can be very complicated and may be inaccurate in some cases. The general belief is that diagnosing diseases relies on doctors' keen intuition, rich experience and professional equipment. In this work, we employ ideas from recent advances in plantar pressure research and from the powerful capacity of the convolutional neural network for learning representations. Here, we propose a model using convolutional neural network based on plantar pressure for medical diagnosis. Our model learns a network that maps plantar pressure data to its corresponding medical diagnostic label. We then apply our model to make the medical diagnosis on datasets we collected from cooperative hospital and achieve an accuracy of 98.36%. We demonstrate that the model base on the convolutional neural network is competitive in medical diagnosis.



### Neural Inverse Rendering for General Reflectance Photometric Stereo
- **Arxiv ID**: http://arxiv.org/abs/1802.10328v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1802.10328v2)
- **Published**: 2018-02-28 09:47:20+00:00
- **Updated**: 2018-05-29 09:15:37+00:00
- **Authors**: Tatsunori Taniai, Takanori Maehara
- **Comment**: To appear in International Conference on Machine Learning 2018 (ICML
  2018). 10 pages + 20 pages (appendices)
- **Journal**: None
- **Summary**: We present a novel convolutional neural network architecture for photometric stereo (Woodham, 1980), a problem of recovering 3D object surface normals from multiple images observed under varying illuminations. Despite its long history in computer vision, the problem still shows fundamental challenges for surfaces with unknown general reflectance properties (BRDFs). Leveraging deep neural networks to learn complicated reflectance models is promising, but studies in this direction are very limited due to difficulties in acquiring accurate ground truth for training and also in designing networks invariant to permutation of input images. In order to address these challenges, we propose a physics based unsupervised learning framework where surface normals and BRDFs are predicted by the network and fed into the rendering equation to synthesize observed images. The network weights are optimized during testing by minimizing reconstruction loss between observed and synthesized images. Thus, our learning process does not require ground truth normals or even pre-training on external images. Our method is shown to achieve the state-of-the-art performance on a challenging real-world scene benchmark.



### Learning to Adapt Structured Output Space for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1802.10349v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.10349v3)
- **Published**: 2018-02-28 10:39:26+00:00
- **Updated**: 2020-07-06 06:45:05+00:00
- **Authors**: Yi-Hsuan Tsai, Wei-Chih Hung, Samuel Schulter, Kihyuk Sohn, Ming-Hsuan Yang, Manmohan Chandraker
- **Comment**: Accepted in CVPR'18. Code and model are available at
  https://github.com/wasidennis/AdaptSegNet
- **Journal**: None
- **Summary**: Convolutional neural network-based approaches for semantic segmentation rely on supervision with pixel-level ground truth, but may not generalize well to unseen image domains. As the labeling process is tedious and labor intensive, developing algorithms that can adapt source ground truth labels to the target domain is of great interest. In this paper, we propose an adversarial learning method for domain adaptation in the context of semantic segmentation. Considering semantic segmentations as structured outputs that contain spatial similarities between the source and target domains, we adopt adversarial learning in the output space. To further enhance the adapted model, we construct a multi-level adversarial network to effectively perform output space domain adaptation at different feature levels. Extensive experiments and ablation study are conducted under various domain adaptation settings, including synthetic-to-real and cross-city scenarios. We show that the proposed method performs favorably against the state-of-the-art methods in terms of accuracy and visual quality.



### Deep-6DPose: Recovering 6D Object Pose from a Single RGB Image
- **Arxiv ID**: http://arxiv.org/abs/1802.10367v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1802.10367v1)
- **Published**: 2018-02-28 11:27:41+00:00
- **Updated**: 2018-02-28 11:27:41+00:00
- **Authors**: Thanh-Toan Do, Ming Cai, Trung Pham, Ian Reid
- **Comment**: None
- **Journal**: None
- **Summary**: Detecting objects and their 6D poses from only RGB images is an important task for many robotic applications. While deep learning methods have made significant progress in visual object detection and segmentation, the object pose estimation task is still challenging. In this paper, we introduce an end-toend deep learning framework, named Deep-6DPose, that jointly detects, segments, and most importantly recovers 6D poses of object instances from a single RGB image. In particular, we extend the recent state-of-the-art instance segmentation network Mask R-CNN with a novel pose estimation branch to directly regress 6D object poses without any post-refinements. Our key technical contribution is the decoupling of pose parameters into translation and rotation so that the rotation can be regressed via a Lie algebra representation. The resulting pose regression loss is differential and unconstrained, making the training tractable. The experiments on two standard pose benchmarking datasets show that our proposed approach compares favorably with the state-of-the-art RGB-based multi-stage pose estimation methods. Importantly, due to the end-to-end architecture, Deep-6DPose is considerably faster than competing multi-stage methods, offers an inference speed of 10 fps that is well suited for robotic applications.



### Graph Kernels based on High Order Graphlet Parsing and Hashing
- **Arxiv ID**: http://arxiv.org/abs/1803.00425v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.00425v1)
- **Published**: 2018-02-28 12:37:41+00:00
- **Updated**: 2018-02-28 12:37:41+00:00
- **Authors**: Anjan Dutta, Hichem Sahbi
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1702.00156
- **Journal**: None
- **Summary**: Graph-based methods are known to be successful in many machine learning and pattern classification tasks. These methods consider semi-structured data as graphs where nodes correspond to primitives (parts, interest points, segments, etc.) and edges characterize the relationships between these primitives. However, these non-vectorial graph data cannot be straightforwardly plugged into off-the-shelf machine learning algorithms without a preliminary step of -- explicit/implicit -- graph vectorization and embedding. This embedding process should be resilient to intra-class graph variations while being highly discriminant. In this paper, we propose a novel high-order stochastic graphlet embedding (SGE) that maps graphs into vector spaces. Our main contribution includes a new stochastic search procedure that efficiently parses a given graph and extracts/samples unlimitedly high-order graphlets. We consider these graphlets, with increasing orders, to model local primitives as well as their increasingly complex interactions. In order to build our graph representation, we measure the distribution of these graphlets into a given graph, using particular hash functions that efficiently assign sampled graphlets into isomorphic sets with a very low probability of collision. When combined with maximum margin classifiers, these graphlet-based representations have positive impact on the performance of pattern comparison and recognition as corroborated through extensive experiments using standard benchmark databases.



### Compressing Neural Networks using the Variational Information Bottleneck
- **Arxiv ID**: http://arxiv.org/abs/1802.10399v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.10399v3)
- **Published**: 2018-02-28 13:26:46+00:00
- **Updated**: 2018-04-19 13:31:16+00:00
- **Authors**: Bin Dai, Chen Zhu, David Wipf
- **Comment**: None
- **Journal**: None
- **Summary**: Neural networks can be compressed to reduce memory and computational requirements, or to increase accuracy by facilitating the use of a larger base architecture. In this paper we focus on pruning individual neurons, which can simultaneously trim model size, FLOPs, and run-time memory. To improve upon the performance of existing compression algorithms we utilize the information bottleneck principle instantiated via a tractable variational bound. Minimization of this information theoretic bound reduces the redundancy between adjacent layers by aggregating useful information into a subset of neurons that can be preserved. In contrast, the activations of disposable neurons are shut off via an attractive form of sparse regularization that emerges naturally from this framework, providing tangible advantages over traditional sparsity penalties without contributing additional tuning parameters to the energy landscape. We demonstrate state-of-the-art compression rates across an array of datasets and network architectures.



### Convolutional Neural Networks with Alternately Updated Clique
- **Arxiv ID**: http://arxiv.org/abs/1802.10419v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.10419v3)
- **Published**: 2018-02-28 14:04:38+00:00
- **Updated**: 2018-04-03 07:58:04+00:00
- **Authors**: Yibo Yang, Zhisheng Zhong, Tiancheng Shen, Zhouchen Lin
- **Comment**: Accepted in CVPR 2018 as Oral presentation. Code available at
  https://github.com/iboing/CliqueNet
- **Journal**: None
- **Summary**: Improving information flow in deep networks helps to ease the training difficulties and utilize parameters more efficiently. Here we propose a new convolutional neural network architecture with alternately updated clique (CliqueNet). In contrast to prior networks, there are both forward and backward connections between any two layers in the same block. The layers are constructed as a loop and are updated alternately. The CliqueNet has some unique properties. For each layer, it is both the input and output of any other layer in the same block, so that the information flow among layers is maximized. During propagation, the newly updated layers are concatenated to re-update previously updated layer, and parameters are reused for multiple times. This recurrent feedback structure is able to bring higher level visual information back to refine low-level filters and achieve spatial attention. We analyze the features generated at different stages and observe that using refined features leads to a better result. We adopt a multi-scale feature strategy that effectively avoids the progressive growth of parameters. Experiments on image recognition datasets including CIFAR-10, CIFAR-100, SVHN and ImageNet show that our proposed models achieve the state-of-the-art performance with fewer parameters.



### Fine-grained wound tissue analysis using deep neural network
- **Arxiv ID**: http://arxiv.org/abs/1802.10426v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.10426v1)
- **Published**: 2018-02-28 14:18:36+00:00
- **Updated**: 2018-02-28 14:18:36+00:00
- **Authors**: Hossein Nejati, Hamed Alizadeh Ghazijahani, Milad Abdollahzadeh, Tooba Malekzadeh, Ngai-Man Cheung, Kheng Hock Lee, Lian Leng Low
- **Comment**: None
- **Journal**: None
- **Summary**: Tissue assessment for chronic wounds is the basis of wound grading and selection of treatment approaches. While several image processing approaches have been proposed for automatic wound tissue analysis, there has been a shortcoming in these approaches for clinical practices. In particular, seemingly, all previous approaches have assumed only 3 tissue types in the chronic wounds, while these wounds commonly exhibit 7 distinct tissue types that presence of each one changes the treatment procedure. In this paper, for the first time, we investigate the classification of 7 wound issue types. We work with wound professionals to build a new database of 7 types of wound tissue. We propose to use pre-trained deep neural networks for feature extraction and classification at the patch-level. We perform experiments to demonstrate that our approach outperforms other state-of-the-art. We will make our database publicly available to facilitate research in wound assessment.



### A Simple Method to improve Initialization Robustness for Active Contours driven by Local Region Fitting Energy
- **Arxiv ID**: http://arxiv.org/abs/1802.10437v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.10437v2)
- **Published**: 2018-02-28 14:43:00+00:00
- **Updated**: 2018-03-17 02:36:40+00:00
- **Authors**: Keyan Ding, Linfang Xiao
- **Comment**: None
- **Journal**: None
- **Summary**: Active contour models based on local region fitting energy can segment images with intensity inhomogeneity effectively, but their segmentation results are easy to error if the initial contour is inappropriate. In this paper, we present a simple and universal method of improving the robustness of initial contour for these local fitting-based models. The core idea of proposed method is exchanging the fitting values on the two sides of contour, so that the fitting values inside the contour are always larger (or smaller) than the values outside the contour in the process of curve evolution. In this way, the whole curve will evolve along the inner (or outer) boundaries of object, and less likely to be stuck in the object or background. Experimental results have proved that using the proposed method can enhance the robustness of initial contour and meanwhile keep the original advantages in the local fitting-based models.



### HSI-CNN: A Novel Convolution Neural Network for Hyperspectral Image
- **Arxiv ID**: http://arxiv.org/abs/1802.10478v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.10478v1)
- **Published**: 2018-02-28 15:31:20+00:00
- **Updated**: 2018-02-28 15:31:20+00:00
- **Authors**: Yanan Luo, Jie Zou, Chengfei Yao, Tao Li, Gang Bai
- **Comment**: 6 pages, 8 figures, Under review as a conference paper at
  International Conference on Pattern Recognition 2018
- **Journal**: None
- **Summary**: With the development of deep learning, the performance of hyperspectral image (HSI) classification has been greatly improved in recent years. The shortage of training samples has become a bottleneck for further improvement of performance. In this paper, we propose a novel convolutional neural network framework for the characteristics of hyperspectral image data, called HSI-CNN. Firstly, the spectral-spatial feature is extracted from a target pixel and its neighbors. Then, a number of one-dimensional feature maps, obtained by convolution operation on spectral-spatial features, are stacked into a two-dimensional matrix. Finally, the two-dimensional matrix considered as an image is fed into standard CNN. This is why we call it HSI-CNN. In addition, we also implements two depth network classification models, called HSI-CNN+XGBoost and HSI-CapsNet, in order to compare the performance of our framework. Experiments show that the performance of hyperspectral image classification is improved efficiently with HSI-CNN framework. We evaluate the model's performance using four popular HSI datasets, which are the Kennedy Space Center (KSC), Indian Pines (IP), Pavia University scene (PU) and Salinas scene (SA). As far as we concerned, HSI-CNN has got the state-of-art accuracy among all methods we have known on these datasets of 99.28%, 99.09%, 99.42%, 98.95% separately.



### LDOP: Local Directional Order Pattern for Robust Face Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1803.07441v3
- **DOI**: 10.1007/s11042-019-08370-x
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.07441v3)
- **Published**: 2018-02-28 16:04:08+00:00
- **Updated**: 2019-12-24 05:33:56+00:00
- **Authors**: Shiv Ram Dubey, Snehasis Mukherjee
- **Comment**: Published in Multimedia Tools and Applications, Springer
- **Journal**: Multimedia Tools and Applications, Springer, 2019
- **Summary**: The local descriptors have gained wide range of attention due to their enhanced discriminative abilities. It has been proved that the consideration of multi-scale local neighborhood improves the performance of the descriptor, though at the cost of increased dimension. This paper proposes a novel method to construct a local descriptor using multi-scale neighborhood by finding the local directional order among the intensity values at different scales in a particular direction. Local directional order is the multi-radius relationship factor in a particular direction. The proposed local directional order pattern (LDOP) for a particular pixel is computed by finding the relationship between the center pixel and local directional order indexes. It is required to transform the center value into the range of neighboring orders. Finally, the histogram of LDOP is computed over whole image to construct the descriptor. In contrast to the state-of-the-art descriptors, the dimension of the proposed descriptor does not depend upon the number of neighbors involved to compute the order; it only depends upon the number of directions. The introduced descriptor is evaluated over the image retrieval framework and compared with the state-of-the-art descriptors over challenging face databases such as PaSC, LFW, PubFig, FERET, AR, AT&T, and ExtendedYale. The experimental results confirm the superiority and robustness of the LDOP descriptor.



### Brain Tumor Segmentation and Radiomics Survival Prediction: Contribution to the BRATS 2017 Challenge
- **Arxiv ID**: http://arxiv.org/abs/1802.10508v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.10508v1)
- **Published**: 2018-02-28 16:19:45+00:00
- **Updated**: 2018-02-28 16:19:45+00:00
- **Authors**: Fabian Isensee, Philipp Kickingereder, Wolfgang Wick, Martin Bendszus, Klaus H. Maier-Hein
- **Comment**: None
- **Journal**: None
- **Summary**: Quantitative analysis of brain tumors is critical for clinical decision making. While manual segmentation is tedious, time consuming and subjective, this task is at the same time very challenging to solve for automatic segmentation methods. In this paper we present our most recent effort on developing a robust segmentation algorithm in the form of a convolutional neural network. Our network architecture was inspired by the popular U-Net and has been carefully modified to maximize brain tumor segmentation performance. We use a dice loss function to cope with class imbalances and use extensive data augmentation to successfully prevent overfitting. Our method beats the current state of the art on BraTS 2015, is one of the leading methods on the BraTS 2017 validation set (dice scores of 0.896, 0.797 and 0.732 for whole tumor, tumor core and enhancing tumor, respectively) and achieves very good Dice scores on the test set (0.858 for whole, 0.775 for core and 0.647 for enhancing tumor). We furthermore take part in the survival prediction subchallenge by training an ensemble of a random forest regressor and multilayer perceptrons on shape features describing the tumor subregions. Our approach achieves 52.6% accuracy, a Spearman correlation coefficient of 0.496 and a mean square error of 209607 on the test set.



### Using Deep Learning for Segmentation and Counting within Microscopy Data
- **Arxiv ID**: http://arxiv.org/abs/1802.10548v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1802.10548v1)
- **Published**: 2018-02-28 17:31:16+00:00
- **Updated**: 2018-02-28 17:31:16+00:00
- **Authors**: Carlos X. Hernández, Mohammad M. Sultan, Vijay S. Pande
- **Comment**: None
- **Journal**: None
- **Summary**: Cell counting is a ubiquitous, yet tedious task that would greatly benefit from automation. From basic biological questions to clinical trials, cell counts provide key quantitative feedback that drive research. Unfortunately, cell counting is most commonly a manual task and can be time-intensive. The task is made even more difficult due to overlapping cells, existence of multiple focal planes, and poor imaging quality, among other factors. Here, we describe a convolutional neural network approach, using a recently described feature pyramid network combined with a VGG-style neural network, for segmenting and subsequent counting of cells in a given microscopy image.



### Retrieval and Registration of Long-Range Overlapping Frames for Scalable Mosaicking of In Vivo Fetoscopy
- **Arxiv ID**: http://arxiv.org/abs/1802.10554v1
- **DOI**: 10.1007/s11548-018-1728-4
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.10554v1)
- **Published**: 2018-02-28 17:44:02+00:00
- **Updated**: 2018-02-28 17:44:02+00:00
- **Authors**: Loïc Peter, Marcel Tella-Amo, Dzhoshkun Ismail Shakir, George Attilakos, Ruwan Wimalasundera, Jan Deprest, Sébastien Ourselin, Tom Vercauteren
- **Comment**: Accepted for publication in International Journal of Computer
  Assisted Radiology and Surgery (IJCARS)
- **Journal**: None
- **Summary**: Purpose: The standard clinical treatment of Twin-to-Twin Transfusion Syndrome consists in the photo-coagulation of undesired anastomoses located on the placenta which are responsible to a blood transfer between the two twins. While being the standard of care procedure, fetoscopy suffers from a limited field-of-view of the placenta resulting in missed anastomoses. To facilitate the task of the clinician, building a global map of the placenta providing a larger overview of the vascular network is highly desired. Methods: To overcome the challenging visual conditions inherent to in vivo sequences (low contrast, obstructions or presence of artifacts, among others), we propose the following contributions: (i) robust pairwise registration is achieved by aligning the orientation of the image gradients, and (ii) difficulties regarding long-range consistency (e.g. due to the presence of outliers) is tackled via a bag-of-word strategy, which identifies overlapping frames of the sequence to be registered regardless of their respective location in time. Results: In addition to visual difficulties, in vivo sequences are characterised by the intrinsic absence of gold standard. We present mosaics motivating qualitatively our methodological choices and demonstrating their promising aspect. We also demonstrate semi-quantitatively, via visual inspection of registration results, the efficacy of our registration approach in comparison to two standard baselines. Conclusion: This paper proposes the first approach for the construction of mosaics of placenta in in vivo fetoscopy sequences. Robustness to visual challenges during registration and long-range temporal consistency are proposed, offering first positive results on in vivo data for which standard mosaicking techniques are not applicable.



### Novelty Detection with GAN
- **Arxiv ID**: http://arxiv.org/abs/1802.10560v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.10560v1)
- **Published**: 2018-02-28 17:50:19+00:00
- **Updated**: 2018-02-28 17:50:19+00:00
- **Authors**: Mark Kliger, Shachar Fleishman
- **Comment**: None
- **Journal**: None
- **Summary**: The ability of a classifier to recognize unknown inputs is important for many classification-based systems. We discuss the problem of simultaneous classification and novelty detection, i.e. determining whether an input is from the known set of classes and from which specific class, or from an unknown domain and does not belong to any of the known classes. We propose a method based on the Generative Adversarial Networks (GAN) framework. We show that a multi-class discriminator trained with a generator that generates samples from a mixture of nominal and novel data distributions is the optimal novelty detector. We approximate that generator with a mixture generator trained with the Feature Matching loss and empirically show that the proposed method outperforms conventional methods for novelty detection. Our findings demonstrate a simple, yet powerful new application of the GAN framework for the task of novelty detection.



### Stereoscopic Neural Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/1802.10591v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.10591v2)
- **Published**: 2018-02-28 18:58:10+00:00
- **Updated**: 2018-05-20 22:47:39+00:00
- **Authors**: Dongdong Chen, Lu Yuan, Jing Liao, Nenghai Yu, Gang Hua
- **Comment**: Accepted by CVPR2018
- **Journal**: None
- **Summary**: This paper presents the first attempt at stereoscopic neural style transfer, which responds to the emerging demand for 3D movies or AR/VR. We start with a careful examination of applying existing monocular style transfer methods to left and right views of stereoscopic images separately. This reveals that the original disparity consistency cannot be well preserved in the final stylization results, which causes 3D fatigue to the viewers. To address this issue, we incorporate a new disparity loss into the widely adopted style loss function by enforcing the bidirectional disparity constraint in non-occluded regions. For a practical real-time solution, we propose the first feed-forward network by jointly training a stylization sub-network and a disparity sub-network, and integrate them in a feature level middle domain. Our disparity sub-network is also the first end-to-end network for simultaneous bidirectional disparity and occlusion mask estimation. Finally, our network is effectively extended to stereoscopic videos, by considering both temporal coherence and disparity consistency. We will show that the proposed method clearly outperforms the baseline algorithms both quantitatively and qualitatively.



### A Feature Clustering Approach Based on Histogram of Oriented Optical Flow and Superpixels
- **Arxiv ID**: http://arxiv.org/abs/1803.00031v1
- **DOI**: 10.1109/ICIINFS.2015.7399059
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.00031v1)
- **Published**: 2018-02-28 19:09:18+00:00
- **Updated**: 2018-02-28 19:09:18+00:00
- **Authors**: A. M. R. R. Bandara, L. Ranathunga, N. A. Abdullah
- **Comment**: None
- **Journal**: 2015 IEEE 10th International Conference on Industrial and
  Information Systems (ICIIS), Peradeniya, 2015, pp. 480-484
- **Summary**: Visual feature clustering is one of the cost-effective approaches to segment objects in videos. However, the assumptions made for developing the existing algorithms prevent them from being used in situations like segmenting an unknown number of static and moving objects under heavy camera movements. This paper addresses the problem by introducing a clustering approach based on superpixels and short-term Histogram of Oriented Optical Flow (HOOF). Salient Dither Pattern Feature (SDPF) is used as the visual feature to track the flow and Simple Linear Iterative Clustering (SLIC) is used for obtaining the superpixels. This new clustering approach is based on merging superpixels by comparing short term local HOOF and a color cue to form high-level semantic segments. The new approach was compared with one of the latest feature clustering approaches based on K-Means in eight-dimensional space and the results revealed that the new approach is better by means of consistency, completeness, and spatial accuracy. Further, the new approach completely solved the problem of not knowing the number of objects in a scene.



### A Retinal Image Enhancement Technique for Blood Vessel Segmentation Algorithm
- **Arxiv ID**: http://arxiv.org/abs/1803.00036v1
- **DOI**: 10.1109/ICIINFS.2017.8300426
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.00036v1)
- **Published**: 2018-02-28 19:15:33+00:00
- **Updated**: 2018-02-28 19:15:33+00:00
- **Authors**: A. M. R. R. Bandara, P. W. G. R. M. P. B. Giragama
- **Comment**: None
- **Journal**: 2017 IEEE International Conference on Industrial and Information
  Systems (ICIIS), Peradeniya, Sri Lanka, 2017, pp. 1-5
- **Summary**: The morphology of blood vessels in retinal fundus images is an important indicator of diseases like glaucoma, hypertension and diabetic retinopathy. The accuracy of retinal blood vessels segmentation affects the quality of retinal image analysis which is used in diagnosis methods in modern ophthalmology. Contrast enhancement is one of the crucial steps in any of retinal blood vessel segmentation approaches. The reliability of the segmentation depends on the consistency of the contrast over the image. This paper presents an assessment of the suitability of a recently invented spatially adaptive contrast enhancement technique for enhancing retinal fundus images for blood vessel segmentation. The enhancement technique was integrated with a variant of Tyler Coye algorithm, which has been improved with Hough line transformation based vessel reconstruction method. The proposed approach was evaluated on two public datasets STARE and DRIVE. The assessment was done by comparing the segmentation performance with five widely used contrast enhancement techniques based on wavelet transform, contrast limited histogram equalization, local normalization, linear un-sharp masking and contourlet transform. The results revealed that the assessed enhancement technique is well suited for the application and also outperforms all compared techniques.



### Invariant properties of a locally salient dither pattern with a spatial-chromatic histogram
- **Arxiv ID**: http://arxiv.org/abs/1803.00037v1
- **DOI**: 10.1109/ICIInfS.2013.6732000
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.00037v1)
- **Published**: 2018-02-28 19:18:34+00:00
- **Updated**: 2018-02-28 19:18:34+00:00
- **Authors**: A. M. R. R. Bandara, L. Ranathunga, N. A. Abdullah
- **Comment**: None
- **Journal**: 2013 IEEE 8th International Conference on Industrial and
  Information Systems, Peradeniya, 2013, pp. 304-308
- **Summary**: Compacted Dither Pattern Code (CDPC) is a recently found feature which is successful in irregular shapes based visual depiction. Locally salient dither pattern feature is an attempt to expand the capability of CDPC for both regular and irregular shape based visual depiction. This paper presents an analysis of rotational and scale invariance property of locally salient dither pattern feature with a two dimensional spatialchromatic histogram, which expands the applicability of the visual feature. Experiments were conducted to exhibit rotational and scale invariance of the feature. These experiments were conducted by combining linear Support Vector Machine (SVM) classifier to the new feature. The experimental results revealed that the locally salient dither pattern feature with the spatialchromatic histogram is rotationally and scale invariant.



### Super-Efficient Spatially Adaptive Contrast Enhancement Algorithm for Superficial Vein Imaging
- **Arxiv ID**: http://arxiv.org/abs/1803.00039v1
- **DOI**: 10.1109/ICIINFS.2017.8300427
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.00039v1)
- **Published**: 2018-02-28 19:21:50+00:00
- **Updated**: 2018-02-28 19:21:50+00:00
- **Authors**: A. M. R. R. Bandara, K. A. S. H. Kulathilake, P. W. G. R. M. P. B. Giragama
- **Comment**: None
- **Journal**: 2017 IEEE International Conference on Industrial and Information
  Systems (ICIIS), Peradeniya, Sri Lanka, 2017, pp. 1-6
- **Summary**: This paper presents a super-efficient spatially adaptive contrast enhancement algorithm for enhancing infrared (IR) radiation based superficial vein images in real-time. The super-efficiency permits the algorithm to run in consumer-grade handheld devices, which ultimately reduces the cost of vein imaging equipment. The proposed method utilizes the response from the low-frequency range of the IR image signal to adjust the boundaries of the reference dynamic range in a linear contrast stretching process with a tunable contrast enhancement parameter, as opposed to traditional approaches which use costly adaptive histogram equalization based methods. The algorithm has been implemented and deployed in a consumer grade Android-based mobile device to evaluate the performance. The results revealed that the proposed algorithm can process IR images of veins in real-time on low-performance computers. It was compared with several well-performed traditional methods and the results revealed that the new algorithm stands out with several beneficial features, namely, the fastest processing, the ability to enhance the desired details, the excellent illumination normalization capability and the ability to enhance details where the traditional methods failed.



### PDE-constrained optimization in medical image analysis
- **Arxiv ID**: http://arxiv.org/abs/1803.00058v1
- **DOI**: 10.1007/s11081-018-9390-9
- **Categories**: **math.OC**, cs.CV, 49K20, 65Y05, 65M32, 65K10, 76D55, 68U10, 35M10
- **Links**: [PDF](http://arxiv.org/pdf/1803.00058v1)
- **Published**: 2018-02-28 20:04:23+00:00
- **Updated**: 2018-02-28 20:04:23+00:00
- **Authors**: Andreas Mang, Amir Gholami, Christos Davatzikos, George Biros
- **Comment**: None
- **Journal**: Optimization and Engineering 19, 765-812, 2018
- **Summary**: PDE-constrained optimization problems find many applications in medical image analysis, for example, neuroimaging, cardiovascular imaging, and oncological imaging. We review related literature and give examples on the formulation, discretization, and numerical solution of PDE-constrained optimization problems for medical imaging. We discuss three examples. The first one is image registration. The second one is data assimilation for brain tumor patients, and the third one data assimilation in cardiovascular imaging. The image registration problem is a classical task in medical image analysis and seeks to find pointwise correspondences between two or more images. The data assimilation problems use a PDE-constrained formulation to link a biophysical model to patient-specific data obtained from medical images. The associated optimality systems turn out to be sets of nonlinear, multicomponent PDEs that are challenging to solve in an efficient way.   The ultimate goal of our work is the design of inversion methods that integrate complementary data, and rigorously follow mathematical and physical principles, in an attempt to support clinical decision making. This requires reliable, high-fidelity algorithms with a short time-to-solution. This task is complicated by model and data uncertainties, and by the fact that PDE-constrained optimization problems are ill-posed in nature, and in general yield high-dimensional, severely ill-conditioned systems after discretization. These features make regularization, effective preconditioners, and iterative solvers that, in many cases, have to be implemented on distributed-memory architectures to be practical, a prerequisite. We showcase state-of-the-art techniques in scientific computing to tackle these challenges.



### Gotta Adapt 'Em All: Joint Pixel and Feature-Level Domain Adaptation for Recognition in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1803.00068v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.00068v2)
- **Published**: 2018-02-28 20:23:20+00:00
- **Updated**: 2019-05-28 19:50:33+00:00
- **Authors**: Luan Tran, Kihyuk Sohn, Xiang Yu, Xiaoming Liu, Manmohan Chandraker
- **Comment**: Accepted to CVPR 2019
- **Journal**: None
- **Summary**: Recent developments in deep domain adaptation have allowed knowledge transfer from a labeled source domain to an unlabeled target domain at the level of intermediate features or input pixels. We propose that advantages may be derived by combining them, in the form of different insights that lead to a novel design and complementary properties that result in better performance. At the feature level, inspired by insights from semi-supervised learning, we propose a classification-aware domain adversarial neural network that brings target examples into more classifiable regions of source domain. Next, we posit that computer vision insights are more amenable to injection at the pixel level. In particular, we use 3D geometry and image synthesis based on a generalized appearance flow to preserve identity across pose transformations, while using an attribute-conditioned CycleGAN to translate a single source into multiple target images that differ in lower-level properties such as lighting. Besides standard UDA benchmark, we validate on a novel and apt problem of car recognition in unlabeled surveillance images using labeled images from the web, handling explicitly specified, nameable factors of variation through pixel-level and implicit, unspecified factors through feature-level adaptation.



### Chinese Text in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1803.00085v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.00085v1)
- **Published**: 2018-02-28 21:03:58+00:00
- **Updated**: 2018-02-28 21:03:58+00:00
- **Authors**: Tai-Ling Yuan, Zhe Zhu, Kun Xu, Cheng-Jun Li, Shi-Min Hu
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce Chinese Text in the Wild, a very large dataset of Chinese text in street view images. While optical character recognition (OCR) in document images is well studied and many commercial tools are available, detection and recognition of text in natural images is still a challenging problem, especially for more complicated character sets such as Chinese text. Lack of training data has always been a problem, especially for deep learning methods which require massive training data.   In this paper we provide details of a newly created dataset of Chinese text with about 1 million Chinese characters annotated by experts in over 30 thousand street view images. This is a challenging dataset with good diversity. It contains planar text, raised text, text in cities, text in rural areas, text under poor illumination, distant text, partially occluded text, etc. For each character in the dataset, the annotation includes its underlying character, its bounding box, and 6 attributes. The attributes indicate whether it has complex background, whether it is raised, whether it is handwritten or printed, etc. The large size and diversity of this dataset make it suitable for training robust neural networks for various tasks, particularly detection and recognition. We give baseline results using several state-of-the-art networks, including AlexNet, OverFeat, Google Inception and ResNet for character recognition, and YOLOv2 for character detection in images. Overall Google Inception has the best performance on recognition with 80.5% top-1 accuracy, while YOLOv2 achieves an mAP of 71.0% on detection. Dataset, source code and trained models will all be publicly available on the website.



### Neural Networks Should Be Wide Enough to Learn Disconnected Decision Regions
- **Arxiv ID**: http://arxiv.org/abs/1803.00094v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1803.00094v3)
- **Published**: 2018-02-28 21:28:28+00:00
- **Updated**: 2018-06-08 09:14:57+00:00
- **Authors**: Quynh Nguyen, Mahesh Chandra Mukkamala, Matthias Hein
- **Comment**: Accepted at ICML 2018. Added discussion for non-pyramidal networks
  and ReLU activation function
- **Journal**: None
- **Summary**: In the recent literature the important role of depth in deep learning has been emphasized. In this paper we argue that sufficient width of a feedforward network is equally important by answering the simple question under which conditions the decision regions of a neural network are connected. It turns out that for a class of activation functions including leaky ReLU, neural networks having a pyramidal structure, that is no layer has more hidden units than the input dimension, produce necessarily connected decision regions. This implies that a sufficiently wide hidden layer is necessary to guarantee that the network can produce disconnected decision regions. We discuss the implications of this result for the construction of neural networks, in particular the relation to the problem of adversarial manipulation of classifiers.



### SalientDSO: Bringing Attention to Direct Sparse Odometry
- **Arxiv ID**: http://arxiv.org/abs/1803.00127v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1803.00127v1)
- **Published**: 2018-02-28 23:02:47+00:00
- **Updated**: 2018-02-28 23:02:47+00:00
- **Authors**: Huai-Jen Liang, Nitin J. Sanket, Cornelia Fermüller, Yiannis Aloimonos
- **Comment**: None
- **Journal**: None
- **Summary**: Although cluttered indoor scenes have a lot of useful high-level semantic information which can be used for mapping and localization, most Visual Odometry (VO) algorithms rely on the usage of geometric features such as points, lines and planes. Lately, driven by this idea, the joint optimization of semantic labels and obtaining odometry has gained popularity in the robotics community. The joint optimization is good for accurate results but is generally very slow. At the same time, in the vision community, direct and sparse approaches for VO have stricken the right balance between speed and accuracy.   We merge the successes of these two communities and present a way to incorporate semantic information in the form of visual saliency to Direct Sparse Odometry - a highly successful direct sparse VO algorithm. We also present a framework to filter the visual saliency based on scene parsing. Our framework, SalientDSO, relies on the widely successful deep learning based approaches for visual saliency and scene parsing which drives the feature selection for obtaining highly-accurate and robust VO even in the presence of as few as 40 point features per frame. We provide extensive quantitative evaluation of SalientDSO on the ICL-NUIM and TUM monoVO datasets and show that we outperform DSO and ORB-SLAM - two very popular state-of-the-art approaches in the literature. We also collect and publicly release a CVL-UMD dataset which contains two indoor cluttered sequences on which we show qualitative evaluations. To our knowledge this is the first paper to use visual saliency and scene parsing to drive the feature selection in direct VO.



### Ring loss: Convex Feature Normalization for Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1803.00130v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.00130v1)
- **Published**: 2018-02-28 23:13:07+00:00
- **Updated**: 2018-02-28 23:13:07+00:00
- **Authors**: Yutong Zheng, Dipan K. Pal, Marios Savvides
- **Comment**: Accepted at CVPR 2018
- **Journal**: None
- **Summary**: We motivate and present Ring loss, a simple and elegant feature normalization approach for deep networks designed to augment standard loss functions such as Softmax. We argue that deep feature normalization is an important aspect of supervised classification problems where we require the model to represent each class in a multi-class problem equally well. The direct approach to feature normalization through the hard normalization operation results in a non-convex formulation. Instead, Ring loss applies soft normalization, where it gradually learns to constrain the norm to the scaled unit circle while preserving convexity leading to more robust features. We apply Ring loss to large-scale face recognition problems and present results on LFW, the challenging protocols of IJB-A Janus, Janus CS3 (a superset of IJB-A Janus), Celebrity Frontal-Profile (CFP) and MegaFace with 1 million distractors. Ring loss outperforms strong baselines, matches state-of-the-art performance on IJB-A Janus and outperforms all other results on the challenging Janus CS3 thereby achieving state-of-the-art. We also outperform strong baselines in handling extremely low resolution face matching.



