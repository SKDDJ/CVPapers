# Arxiv Papers in cs.CV on 2018-02-08
### A Semi-Supervised Two-Stage Approach to Learning from Noisy Labels
- **Arxiv ID**: http://arxiv.org/abs/1802.02679v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.02679v3)
- **Published**: 2018-02-08 00:35:42+00:00
- **Updated**: 2018-03-21 22:50:33+00:00
- **Authors**: Yifan Ding, Liqiang Wang, Deliang Fan, Boqing Gong
- **Comment**: None
- **Journal**: IEEE Winter Conf. on Applications of Computer Vision 2018
- **Summary**: The recent success of deep neural networks is powered in part by large-scale well-labeled training data. However, it is a daunting task to laboriously annotate an ImageNet-like dateset. On the contrary, it is fairly convenient, fast, and cheap to collect training images from the Web along with their noisy labels. This signifies the need of alternative approaches to training deep neural networks using such noisy labels. Existing methods tackling this problem either try to identify and correct the wrong labels or reweigh the data terms in the loss function according to the inferred noisy rates. Both strategies inevitably incur errors for some of the data points. In this paper, we contend that it is actually better to ignore the labels of some of the data points than to keep them if the labels are incorrect, especially when the noisy rate is high. After all, the wrong labels could mislead a neural network to a bad local optimum. We suggest a two-stage framework for the learning from noisy labels. In the first stage, we identify a small portion of images from the noisy training set of which the labels are correct with a high probability. The noisy labels of the other images are ignored. In the second stage, we train a deep neural network in a semi-supervised manner. This framework effectively takes advantage of the whole training set and yet only a portion of its labels that are most likely correct. Experiments on three datasets verify the effectiveness of our approach especially when the noisy rate is high.



### Driver Gaze Zone Estimation using Convolutional Neural Networks: A General Framework and Ablative Analysis
- **Arxiv ID**: http://arxiv.org/abs/1802.02690v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.02690v2)
- **Published**: 2018-02-08 02:13:50+00:00
- **Updated**: 2018-04-25 00:45:58+00:00
- **Authors**: Sourabh Vora, Akshay Rangesh, Mohan M. Trivedi
- **Comment**: None
- **Journal**: None
- **Summary**: Driver gaze has been shown to be an excellent surrogate for driver attention in intelligent vehicles. With the recent surge of highly autonomous vehicles, driver gaze can be useful for determining the handoff time to a human driver. While there has been significant improvement in personalized driver gaze zone estimation systems, a generalized system which is invariant to different subjects, perspectives and scales is still lacking. We take a step towards this generalized system using Convolutional Neural Networks (CNNs). We finetune 4 popular CNN architectures for this task, and provide extensive comparisons of their outputs. We additionally experiment with different input image patches, and also examine how image size affects performance. For training and testing the networks, we collect a large naturalistic driving dataset comprising of 11 long drives, driven by 10 subjects in two different cars. Our best performing model achieves an accuracy of 95.18% during cross-subject testing, outperforming current state of the art techniques for this task. Finally, we evaluate our best performing model on the publicly available Columbia Gaze Dataset comprising of images from 56 subjects with varying head pose and gaze directions. Without any training, our model successfully encodes the different gaze directions on this diverse dataset, demonstrating good generalization capabilities.



### Deep Image Super Resolution via Natural Image Priors
- **Arxiv ID**: http://arxiv.org/abs/1802.02721v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.02721v1)
- **Published**: 2018-02-08 05:34:01+00:00
- **Updated**: 2018-02-08 05:34:01+00:00
- **Authors**: Hojjat S. Mousavi, Tiantong Guo, Vishal Monga
- **Comment**: None
- **Journal**: None
- **Summary**: Single image super-resolution (SR) via deep learning has recently gained significant attention in the literature. Convolutional neural networks (CNNs) are typically learned to represent the mapping between low-resolution (LR) and high-resolution (HR) images/patches with the help of training examples. Most existing deep networks for SR produce high quality results when training data is abundant. However, their performance degrades sharply when training is limited. We propose to regularize deep structures with prior knowledge about the images so that they can capture more structural information from the same limited data. In particular, we incorporate in a tractable fashion within the CNN framework, natural image priors which have shown to have much recent success in imaging and vision inverse problems. Experimental results show that the proposed deep network with natural image priors is particularly effective in training starved regimes.



### Topologically Controlled Lossy Compression
- **Arxiv ID**: http://arxiv.org/abs/1802.02731v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CG, cs.CV, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1802.02731v1)
- **Published**: 2018-02-08 07:35:43+00:00
- **Updated**: 2018-02-08 07:35:43+00:00
- **Authors**: Maxime Soler, Melanie Plainchault, Bruno Conche, Julien Tierny
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a new algorithm for the lossy compression of scalar data defined on 2D or 3D regular grids, with topological control. Certain techniques allow users to control the pointwise error induced by the compression. However, in many scenarios it is desirable to control in a similar way the preservation of higher-level notions, such as topological features , in order to provide guarantees on the outcome of post-hoc data analyses. This paper presents the first compression technique for scalar data which supports a strictly controlled loss of topological features. It provides users with specific guarantees both on the preservation of the important features and on the size of the smaller features destroyed during compression. In particular, we present a simple compression strategy based on a topologically adaptive quantization of the range. Our algorithm provides strong guarantees on the bottleneck distance between persistence diagrams of the input and decompressed data, specifically those associated with extrema. A simple extension of our strategy additionally enables a control on the pointwise error. We also show how to combine our approach with state-of-the-art compressors, to further improve the geometrical reconstruction. Extensive experiments, for comparable compression rates, demonstrate the superiority of our algorithm in terms of the preservation of topological features. We show the utility of our approach by illustrating the compatibility between the output of post-hoc topological data analysis pipelines, executed on the input and decompressed data, for simulated or acquired data sets. We also provide a lightweight VTK-based C++ implementation of our approach for reproduction purposes.



### From Hashing to CNNs: Training BinaryWeight Networks via Hashing
- **Arxiv ID**: http://arxiv.org/abs/1802.02733v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.02733v1)
- **Published**: 2018-02-08 07:51:41+00:00
- **Updated**: 2018-02-08 07:51:41+00:00
- **Authors**: Qinghao Hu, Peisong Wang, Jian Cheng
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolutional neural networks (CNNs) have shown appealing performance on various computer vision tasks in recent years. This motivates people to deploy CNNs to realworld applications. However, most of state-of-art CNNs require large memory and computational resources, which hinders the deployment on mobile devices. Recent studies show that low-bit weight representation can reduce much storage and memory demand, and also can achieve efficient network inference. To achieve this goal, we propose a novel approach named BWNH to train Binary Weight Networks via Hashing. In this paper, we first reveal the strong connection between inner-product preserving hashing and binary weight networks, and show that training binary weight networks can be intrinsically regarded as a hashing problem. Based on this perspective, we propose an alternating optimization method to learn the hash codes instead of directly learning binary weights. Extensive experiments on CIFAR10, CIFAR100 and ImageNet demonstrate that our proposed BWNH outperforms current state-of-art by a large margin.



### Learning Inductive Biases with Simple Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1802.02745v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1802.02745v2)
- **Published**: 2018-02-08 08:25:51+00:00
- **Updated**: 2018-06-13 18:01:21+00:00
- **Authors**: Reuben Feinman, Brenden M. Lake
- **Comment**: Published in Proceedings of the 40th Annual Meeting of the Cognitive
  Science Society, July 2018
- **Journal**: Feinman, R. and Lake, B.M. (2018). Learning inductive biases with
  simple neural networks. In Proceedings of the 40th Annual Meeting of the
  Cognitive Science Society
- **Summary**: People use rich prior knowledge about the world in order to efficiently learn new concepts. These priors - also known as "inductive biases" - pertain to the space of internal models considered by a learner, and they help the learner make inferences that go beyond the observed data. A recent study found that deep neural networks optimized for object recognition develop the shape bias (Ritter et al., 2017), an inductive bias possessed by children that plays an important role in early word learning. However, these networks use unrealistically large quantities of training data, and the conditions required for these biases to develop are not well understood. Moreover, it is unclear how the learning dynamics of these networks relate to developmental processes in childhood. We investigate the development and influence of the shape bias in neural networks using controlled datasets of abstract patterns and synthetic images, allowing us to systematically vary the quantity and form of the experience provided to the learning algorithms. We find that simple neural networks develop a shape bias after seeing as few as 3 examples of 4 object categories. The development of these biases predicts the onset of vocabulary acceleration in our networks, consistent with the developmental process in children.



### Learning to score the figure skating sports videos
- **Arxiv ID**: http://arxiv.org/abs/1802.02774v3
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1802.02774v3)
- **Published**: 2018-02-08 09:53:56+00:00
- **Updated**: 2018-07-28 08:31:18+00:00
- **Authors**: Chengming Xu, Yanwei Fu, Bing Zhang, Zitian Chen, Yu-Gang Jiang, Xiangyang Xue
- **Comment**: None
- **Journal**: None
- **Summary**: This paper targets at learning to score the figure skating sports videos. To address this task, we propose a deep architecture that includes two complementary components, i.e., Self-Attentive LSTM and Multi-scale Convolutional Skip LSTM. These two components can efficiently learn the local and global sequential information in each video. Furthermore, we present a large-scale figure skating sports video dataset -- FisV dataset. This dataset includes 500 figure skating videos with the average length of 2 minutes and 50 seconds. Each video is annotated by two scores of nine different referees, i.e., Total Element Score(TES) and Total Program Component Score (PCS). Our proposed model is validated on FisV and MIT-skate datasets. The experimental results show the effectiveness of our models in learning to score the figure skating videos.



### Saliency-Enhanced Robust Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/1802.02783v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.02783v1)
- **Published**: 2018-02-08 10:21:50+00:00
- **Updated**: 2018-02-08 10:21:50+00:00
- **Authors**: Caglar Aytekin, Francesco Cricri, Emre Aksu
- **Comment**: Submitted to ICIP 2018
- **Journal**: None
- **Summary**: Discrete correlation filter (DCF) based trackers have shown considerable success in visual object tracking. These trackers often make use of low to mid level features such as histogram of gradients (HoG) and mid-layer activations from convolution neural networks (CNNs). We argue that including semantically higher level information to the tracked features may provide further robustness to challenging cases such as viewpoint changes. Deep salient object detection is one example of such high level features, as it make use of semantic information to highlight the important regions in the given scene. In this work, we propose an improvement over DCF based trackers by combining saliency based and other features based filter responses. This combination is performed with an adaptive weight on the saliency based filter responses, which is automatically selected according to the temporal consistency of visual saliency. We show that our method consistently improves a baseline DCF based tracker especially in challenging cases and performs superior to the state-of-the-art. Our improved tracker operates at 9.3 fps, introducing a small computational burden over the baseline which operates at 11 fps.



### Peekaboo - Where are the Objects? Structure Adjusting Superpixels
- **Arxiv ID**: http://arxiv.org/abs/1802.02796v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.02796v2)
- **Published**: 2018-02-08 10:38:56+00:00
- **Updated**: 2018-05-29 09:34:26+00:00
- **Authors**: Georg Maierhofer, Daniel Heydecker, Angelica I. Aviles-Rivero, Samar M. Alsaleh, Carola-Bibiane Schönlieb
- **Comment**: 5 pages, 5 figures, IEEE International Conference on Image Processing
- **Journal**: None
- **Summary**: This paper addresses the search for a fast and meaningful image segmentation in the context of $k$-means clustering. The proposed method builds on a widely-used local version of Lloyd's algorithm, called Simple Linear Iterative Clustering (SLIC). We propose an algorithm which extends SLIC to dynamically adjust the local search, adopting superpixel resolution dynamically to structure existent in the image, and thus provides for more meaningful superpixels in the same linear runtime as standard SLIC. The proposed method is evaluated against state-of-the-art techniques and improved boundary adherence and undersegmentation error are observed, whilst still remaining among the fastest algorithms which are tested.



### Archetypal Analysis for Sparse Representation-based Hyperspectral Sub-pixel Quantification
- **Arxiv ID**: http://arxiv.org/abs/1802.02813v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.02813v1)
- **Published**: 2018-02-08 11:47:19+00:00
- **Updated**: 2018-02-08 11:47:19+00:00
- **Authors**: Lukas Drees, Ribana Roscher, Susanne Wenzel
- **Comment**: None
- **Journal**: Photogrammetric Engineering \& Remote Sensing, 2018
- **Summary**: The estimation of land cover fractions from remote sensing images is a frequently used indicator of the environmental quality. This paper focuses on the quantification of land cover fractions in an urban area of Berlin, Germany, using simulated hyperspectral EnMAP data with a spatial resolution of 30m$\times$30m. We use constrained sparse representation, where each pixel with unknown surface characteristics is expressed by a weighted linear combination of elementary spectra with known land cover class. We automatically determine the elementary spectra from image reference data using archetypal analysis by simplex volume maximization, and combine it with reversible jump Markov chain Monte Carlo method. In our experiments, the estimation of the automatically derived elementary spectra is compared to the estimation obtained by a manually designed spectral library by means of reconstruction error, mean absolute error of the fraction estimates, sum of fractions, $R^2$, and the number of used elementary spectra. The experiments show that a collection of archetypes can be an adequate and efficient alternative to the manually designed spectral library with respect to the mentioned criteria.



### A Deep Unsupervised Learning Approach Toward MTBI Identification Using Diffusion MRI
- **Arxiv ID**: http://arxiv.org/abs/1802.02925v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.02925v2)
- **Published**: 2018-02-08 15:37:10+00:00
- **Updated**: 2018-04-11 22:01:13+00:00
- **Authors**: Shervin Minaee, Yao Wang, Anna Choromanska, Sohae Chung, Xiuyuan Wang, Els Fieremans, Steven Flanagan, Joseph Rath, Yvonne W Lui
- **Comment**: arXiv admin note: text overlap with arXiv:1710.06824
- **Journal**: None
- **Summary**: Mild traumatic brain injury is a growing public health problem with an estimated incidence of over 1.7 million people annually in US. Diagnosis is based on clinical history and symptoms, and accurate, concrete measures of injury are lacking. This work aims to directly use diffusion MR images obtained within one month of trauma to detect injury, by incorporating deep learning techniques. To overcome the challenge due to limited training data, we describe each brain region using the bag of word representation, which specifies the distribution of representative patch patterns. We apply a convolutional auto-encoder to learn the patch-level features, from overlapping image patches extracted from the MR images, to learn features from diffusion MR images of brain using an unsupervised approach. Our experimental results show that the bag of word representation using patch level features learnt by the auto encoder provides similar performance as that using the raw patch patterns, both significantly outperform earlier work relying on the mean values of MR metrics in selected brain regions.



### Rotate your Networks: Better Weight Consolidation and Less Catastrophic Forgetting
- **Arxiv ID**: http://arxiv.org/abs/1802.02950v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.02950v4)
- **Published**: 2018-02-08 16:25:29+00:00
- **Updated**: 2018-12-12 16:17:25+00:00
- **Authors**: Xialei Liu, Marc Masana, Luis Herranz, Joost Van de Weijer, Antonio M. Lopez, Andrew D. Bagdanov
- **Comment**: Accepted at ICPR'18. First two authors contributed equally
- **Journal**: None
- **Summary**: In this paper we propose an approach to avoiding catastrophic forgetting in sequential task learning scenarios. Our technique is based on a network reparameterization that approximately diagonalizes the Fisher Information Matrix of the network parameters. This reparameterization takes the form of a factorized rotation of parameter space which, when used in conjunction with Elastic Weight Consolidation (which assumes a diagonal Fisher Information Matrix), leads to significantly better performance on lifelong learning of sequential tasks. Experimental results on the MNIST, CIFAR-100, CUB-200 and Stanford-40 datasets demonstrate that we significantly improve the results of standard elastic weight consolidation, and that we obtain competitive results when compared to other state-of-the-art in lifelong learning without forgetting.



### Practical Issues of Action-conditioned Next Image Prediction
- **Arxiv ID**: http://arxiv.org/abs/1802.02975v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.02975v1)
- **Published**: 2018-02-08 17:38:26+00:00
- **Updated**: 2018-02-08 17:38:26+00:00
- **Authors**: Donglai Zhu, Hao Chen, Hengshuai Yao, Masoud Nosrati, Peyman Yadmellat, Yunfei Zhang
- **Comment**: 12 pages; 7 figures
- **Journal**: None
- **Summary**: The problem of action-conditioned image prediction is to predict the expected next frame given the current camera frame the robot observes and an action selected by the robot. We provide the first comparison of two recent popular models, especially for image prediction on cars. Our major finding is that action tiling encoding is the most important factor leading to the remarkable performance of the CDNA model. We present a light-weight model by action tiling encoding which has a single-decoder feedforward architecture same as [action_video_prediction_honglak]. On a real driving dataset, the CDNA model achieves ${0.3986} \times 10^{-3}$ MSE and ${0.9846}$ Structure SIMilarity (SSIM) with a network size of about {\bfseries ${12.6}$ million} parameters. With a small network of fewer than {\bfseries ${1}$ million} parameters, our new model achieves a comparable performance to CDNA at ${0.3613} \times 10^{-3}$ MSE and ${0.9633}$ SSIM. Our model requires less memory, is more computationally efficient and is advantageous to be used inside self-driving vehicles.



### Texture Segmentation Based Video Compression Using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1802.02992v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.02992v1)
- **Published**: 2018-02-08 18:09:49+00:00
- **Updated**: 2018-02-08 18:09:49+00:00
- **Authors**: Chichen Fu, Di Chen, Edward J. Delp, Zoe Liu, Fengqing Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: There has been a growing interest in using different approaches to improve the coding efficiency of modern video codec in recent years as demand for web-based video consumption increases. In this paper, we propose a model-based approach that uses texture analysis/synthesis to reconstruct blocks in texture regions of a video to achieve potential coding gains using the AV1 codec developed by the Alliance for Open Media (AOM). The proposed method uses convolutional neural networks to extract texture regions in a frame, which are then reconstructed using a global motion model. Our preliminary results show an increase in coding efficiency while maintaining satisfactory visual quality.



### Hole Filling with Multiple Reference Views in DIBR View Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1802.03079v1
- **DOI**: 10.1109/TMM.2018.2791810
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.03079v1)
- **Published**: 2018-02-08 23:38:12+00:00
- **Updated**: 2018-02-08 23:38:12+00:00
- **Authors**: Shuai Li, Ce Zhu, Ming-Ting Sun
- **Comment**: 12 pages, accepted by IEEE Transactions on Multimedia
- **Journal**: None
- **Summary**: Depth-image-based rendering (DIBR) oriented view synthesis has been widely employed in the current depth-based 3D video systems by synthesizing a virtual view from an arbitrary viewpoint. However, holes may appear in the synthesized view due to disocclusion, thus significantly degrading the quality. Consequently, efforts have been made on developing effective and efficient hole filling algorithms. Current hole filling techniques generally extrapolate/interpolate the hole regions with the neighboring information based on an assumption that the texture pattern in the holes is similar to that of the neighboring background information. However, in many scenarios especially of complex texture, the assumption may not hold. In other words, hole filling techniques can only provide an estimation for a hole which may not be good enough or may even be erroneous considering a wide variety of complex scene of images. In this paper, we first examine the view interpolation with multiple reference views, demonstrating that the problem of emerging holes in a target virtual view can be greatly alleviated by making good use of other neighboring complementary views in addition to its two (commonly used) most neighboring primary views. The effects of using multiple views for view extrapolation in reducing holes are also investigated in this paper. In view of the 3D Video and ongoing free-viewpoint TV standardization, we propose a new view synthesis framework which employs multiple views to synthesize output virtual views. Furthermore, a scheme of selective warping of complementary views is developed by efficiently locating a small number of useful pixels in the complementary views for hole reduction, to avoid a full warping of additional complementary views thus lowering greatly the warping complexity.



