# Arxiv Papers in cs.CV on 2018-02-09
### Automatic segmenting teeth in X-ray images: Trends, a novel data set, benchmarking and future perspectives
- **Arxiv ID**: http://arxiv.org/abs/1802.03086v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.03086v1)
- **Published**: 2018-02-09 00:31:06+00:00
- **Updated**: 2018-02-09 00:31:06+00:00
- **Authors**: Gil Jader, Luciano Oliveira, Matheus Pithon
- **Comment**: None
- **Journal**: None
- **Summary**: This review presents an in-depth study of the literature on segmentation methods applied in dental imaging. Ten segmentation methods were studied and categorized according to the type of the segmentation method (region-based, threshold-based, cluster-based, boundary-based or watershed-based), type of X-ray images used (intra-oral or extra-oral) and characteristics of the dataset used to evaluate the methods in the state-of-the-art works. We found that the literature has primarily focused on threshold-based segmentation methods (54%). 80% of the reviewed papers have used intra-oral X-ray images in their experiments, demonstrating preference to perform segmentation on images of already isolated parts of the teeth, rather than using extra-oral X-rays, which show tooth structure of the mouth and bones of the face. To fill a scientific gap in the field, a novel data set based on extra-oral X-ray images are proposed here. A statistical comparison of the results found with the 10 image segmentation methods over our proposed data set comprised of 1,500 images is also carried out, providing a more comprehensive source of performance assessment. Discussion on limitations of the methods conceived over the past year as well as future perspectives on exploiting learning-based segmentation methods to improve performance are also provided.



### Tracking Noisy Targets: A Review of Recent Object Tracking Approaches
- **Arxiv ID**: http://arxiv.org/abs/1802.03098v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.03098v2)
- **Published**: 2018-02-09 01:48:27+00:00
- **Updated**: 2018-02-14 01:12:32+00:00
- **Authors**: Mustansar Fiaz, Arif Mahmood, Soon Ki Jung
- **Comment**: 26 pages, 10 figures, 3 tables
- **Journal**: None
- **Summary**: Visual object tracking is an important computer vision problem with numerous real-world applications including human-computer interaction, autonomous vehicles, robotics, motion-based recognition, video indexing, surveillance and security. In this paper, we aim to extensively review the latest trends and advances in the tracking algorithms and evaluate the robustness of trackers in the presence of noise. The first part of this work comprises a comprehensive survey of recently proposed tracking algorithms. We broadly categorize trackers into correlation filter based trackers and the others as non-correlation filter trackers. Each category is further classified into various types of trackers based on the architecture of the tracking mechanism. In the second part of this work, we experimentally evaluate tracking algorithms for robustness in the presence of additive white Gaussian noise. Multiple levels of additive noise are added to the Object Tracking Benchmark (OTB) 2015, and the precision and success rates of the tracking algorithms are evaluated. Some algorithms suffered more performance degradation than others, which brings to light a previously unexplored aspect of the tracking algorithms. The relative rank of the algorithms based on their performance on benchmark datasets may change in the presence of noise. Our study concludes that no single tracker is able to achieve the same efficiency in the presence of noise as under noise-free conditions; thus, there is a need to include a parameter for robustness to noise when evaluating newly proposed tracking algorithms.



### Convolutional Hashing for Automated Scene Matching
- **Arxiv ID**: http://arxiv.org/abs/1802.03101v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, stat.ML, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/1802.03101v1)
- **Published**: 2018-02-09 02:11:18+00:00
- **Updated**: 2018-02-09 02:11:18+00:00
- **Authors**: Martin Loncaric, Bowei Liu, Ryan Weber
- **Comment**: 9 pages, 4 figures, 4 tables
- **Journal**: None
- **Summary**: We present a powerful new loss function and training scheme for learning binary hash functions. In particular, we demonstrate our method by creating for the first time a neural network that outperforms state-of-the-art Haar wavelets and color layout descriptors at the task of automated scene matching. By accurately relating distance on the manifold of network outputs to distance in Hamming space, we achieve a 100-fold reduction in nontrivial false positive rate and significantly higher true positive rate. We expect our insights to provide large wins for hashing models applied to other information retrieval hashing tasks as well.



### Batch Kalman Normalization: Towards Training Deep Neural Networks with Micro-Batches
- **Arxiv ID**: http://arxiv.org/abs/1802.03133v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1802.03133v2)
- **Published**: 2018-02-09 05:19:16+00:00
- **Updated**: 2018-02-28 02:01:50+00:00
- **Authors**: Guangrun Wang, Jiefeng Peng, Ping Luo, Xinjiang Wang, Liang Lin
- **Comment**: We presented how to improve and accelerate the training of DNNs,
  particularly under the context of micro-batches. (Submitted to IJCAI 2018)
- **Journal**: None
- **Summary**: As an indispensable component, Batch Normalization (BN) has successfully improved the training of deep neural networks (DNNs) with mini-batches, by normalizing the distribution of the internal representation for each hidden layer. However, the effectiveness of BN would diminish with scenario of micro-batch (e.g., less than 10 samples in a mini-batch), since the estimated statistics in a mini-batch are not reliable with insufficient samples. In this paper, we present a novel normalization method, called Batch Kalman Normalization (BKN), for improving and accelerating the training of DNNs, particularly under the context of micro-batches. Specifically, unlike the existing solutions treating each hidden layer as an isolated system, BKN treats all the layers in a network as a whole system, and estimates the statistics of a certain layer by considering the distributions of all its preceding layers, mimicking the merits of Kalman Filtering. BKN has two appealing properties. First, it enables more stable training and faster convergence compared to previous works. Second, training DNNs using BKN performs substantially better than those using BN and its variants, especially when very small mini-batches are presented. On the image classification benchmark of ImageNet, using BKN powered networks we improve upon the best-published model-zoo results: reaching 74.0% top-1 val accuracy for InceptionV2. More importantly, using BKN achieves the comparable accuracy with extremely smaller batch size, such as 64 times smaller on CIFAR-10/100 and 8 times smaller on ImageNet.



### Deep Private-Feature Extraction
- **Arxiv ID**: http://arxiv.org/abs/1802.03151v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CR, cs.CV, cs.IT, cs.LG, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/1802.03151v2)
- **Published**: 2018-02-09 07:12:29+00:00
- **Updated**: 2018-02-28 16:32:47+00:00
- **Authors**: Seyed Ali Osia, Ali Taheri, Ali Shahin Shamsabadi, Kleomenis Katevas, Hamed Haddadi, Hamid R. Rabiee
- **Comment**: None
- **Journal**: None
- **Summary**: We present and evaluate Deep Private-Feature Extractor (DPFE), a deep model which is trained and evaluated based on information theoretic constraints. Using the selective exchange of information between a user's device and a service provider, DPFE enables the user to prevent certain sensitive information from being shared with a service provider, while allowing them to extract approved information using their model. We introduce and utilize the log-rank privacy, a novel measure to assess the effectiveness of DPFE in removing sensitive information and compare different models based on their accuracy-privacy tradeoff. We then implement and evaluate the performance of DPFE on smartphones to understand its complexity, resource demands, and efficiency tradeoffs. Our results on benchmark image datasets demonstrate that under moderate resource utilization, DPFE can achieve high accuracy for primary tasks while preserving the privacy of sensitive features.



### Boosting Image Forgery Detection using Resampling Features and Copy-move analysis
- **Arxiv ID**: http://arxiv.org/abs/1802.03154v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.03154v2)
- **Published**: 2018-02-09 07:22:19+00:00
- **Updated**: 2018-02-19 08:35:54+00:00
- **Authors**: Tajuddin Manhar Mohammed, Jason Bunk, Lakshmanan Nataraj, Jawadul H. Bappy, Arjuna Flenner, B. S. Manjunath, Shivkumar Chandrasekaran, Amit K. Roy-Chowdhury, Lawrence Peterson
- **Comment**: None
- **Journal**: None
- **Summary**: Realistic image forgeries involve a combination of splicing, resampling, cloning, region removal and other methods. While resampling detection algorithms are effective in detecting splicing and resampling, copy-move detection algorithms excel in detecting cloning and region removal. In this paper, we combine these complementary approaches in a way that boosts the overall accuracy of image manipulation detection. We use the copy-move detection method as a pre-filtering step and pass those images that are classified as untampered to a deep learning based resampling detection framework. Experimental results on various datasets including the 2017 NIST Nimble Challenge Evaluation dataset comprising nearly 10,000 pristine and tampered images shows that there is a consistent increase of 8%-10% in detection rates, when copy-move algorithm is combined with different resampling detection algorithms.



### Tracking all members of a honey bee colony over their lifetime
- **Arxiv ID**: http://arxiv.org/abs/1802.03192v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.03192v2)
- **Published**: 2018-02-09 10:29:11+00:00
- **Updated**: 2018-03-15 10:31:02+00:00
- **Authors**: Franziska Boenisch, Benjamin Rosemann, Benjamin Wild, Fernando Wario, David Dormagen, Tim Landgraf
- **Comment**: None
- **Journal**: None
- **Summary**: Computational approaches to the analysis of collective behavior in social insects increasingly rely on motion paths as an intermediate data layer from which one can infer individual behaviors or social interactions. Honey bees are a popular model for learning and memory. Previous experience has been shown to affect and modulate future social interactions. So far, no lifetime history observations have been reported for all bees of a colony. In a previous work we introduced a tracking system customized to track up to $4000$ bees over several weeks. In this contribution we present an in-depth description of the underlying multi-step algorithm which both produces the motion paths, and also improves the marker decoding accuracy significantly. We automatically tracked ${\sim}2000$ marked honey bees over 10 weeks with inexpensive recording hardware using markers without any error correction bits. We found that the proposed two-step tracking reduced incorrect ID decodings from initially ${\sim}13\%$ to around $2\%$ post-tracking. Alongside this paper, we publish the first trajectory dataset for all bees in a colony, extracted from ${\sim} 4$ million images. We invite researchers to join the collective scientific effort to investigate this intriguing animal system. All components of our system are open-source.



### Full-Frame Scene Coordinate Regression for Image-Based Localization
- **Arxiv ID**: http://arxiv.org/abs/1802.03237v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.03237v2)
- **Published**: 2018-02-09 12:55:30+00:00
- **Updated**: 2018-06-25 16:17:40+00:00
- **Authors**: Xiaotian Li, Juha Ylioinas, Juho Kannala
- **Comment**: RSS 2018
- **Journal**: None
- **Summary**: Image-based localization, or camera relocalization, is a fundamental problem in computer vision and robotics, and it refers to estimating camera pose from an image. Recent state-of-the-art approaches use learning based methods, such as Random Forests (RFs) and Convolutional Neural Networks (CNNs), to regress for each pixel in the image its corresponding position in the scene's world coordinate frame, and solve the final pose via a RANSAC-based optimization scheme using the predicted correspondences. In this paper, instead of in a patch-based manner, we propose to perform the scene coordinate regression in a full-frame manner to make the computation efficient at test time and, more importantly, to add more global context to the regression process to improve the robustness. To do so, we adopt a fully convolutional encoder-decoder neural network architecture which accepts a whole image as input and produces scene coordinate predictions for all pixels in the image. However, using more global context is prone to overfitting. To alleviate this issue, we propose to use data augmentation to generate more data for training. In addition to the data augmentation in 2D image space, we also augment the data in 3D space. We evaluate our approach on the publicly available 7-Scenes dataset, and experiments show that it has better scene coordinate predictions and achieves state-of-the-art results in localization with improved robustness on the hardest frames (e.g., frames with repeated structures).



### RSDNet: Learning to Predict Remaining Surgery Duration from Laparoscopic Videos Without Manual Annotations
- **Arxiv ID**: http://arxiv.org/abs/1802.03243v2
- **DOI**: 10.1109/TMI.2018.2878055
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.03243v2)
- **Published**: 2018-02-09 13:07:46+00:00
- **Updated**: 2018-12-03 14:52:45+00:00
- **Authors**: Andru Putra Twinanda, Gaurav Yengera, Didier Mutter, Jacques Marescaux, Nicolas Padoy
- **Comment**: 10 pages, IEEE Transactions on Medical Imaging, 2018
- **Journal**: None
- **Summary**: Accurate surgery duration estimation is necessary for optimal OR planning, which plays an important role in patient comfort and safety as well as resource optimization. It is, however, challenging to preoperatively predict surgery duration since it varies significantly depending on the patient condition, surgeon skills, and intraoperative situation. In this paper, we propose a deep learning pipeline, referred to as RSDNet, which automatically estimates the remaining surgery duration (RSD) intraoperatively by using only visual information from laparoscopic videos. Previous state-of-the-art approaches for RSD prediction are dependent on manual annotation, whose generation requires expensive expert knowledge and is time-consuming, especially considering the numerous types of surgeries performed in a hospital and the large number of laparoscopic videos available. A crucial feature of RSDNet is that it does not depend on any manual annotation during training, making it easily scalable to many kinds of surgeries. The generalizability of our approach is demonstrated by testing the pipeline on two large datasets containing different types of surgeries: 120 cholecystectomy and 170 gastric bypass videos. The experimental results also show that the proposed network significantly outperforms a traditional method of estimating RSD without utilizing manual annotation. Further, this work provides a deeper insight into the deep learning network through visualization and interpretation of the features that are automatically learned.



### Piecewise Flat Embedding for Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1802.03248v5
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1802.03248v5)
- **Published**: 2018-02-09 13:19:14+00:00
- **Updated**: 2018-05-20 07:22:43+00:00
- **Authors**: Chaowei Fang, Zicheng Liao, Yizhou Yu
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a new multi-dimensional nonlinear embedding -- Piecewise Flat Embedding (PFE) -- for image segmentation. Based on the theory of sparse signal recovery, piecewise flat embedding with diverse channels attempts to recover a piecewise constant image representation with sparse region boundaries and sparse cluster value scattering. The resultant piecewise flat embedding exhibits interesting properties such as suppressing slowly varying signals, and offers an image representation with higher region identifiability which is desirable for image segmentation or high-level semantic analysis tasks. We formulate our embedding as a variant of the Laplacian Eigenmap embedding with an $L_{1,p} (0<p\leq1)$ regularization term to promote sparse solutions. First, we devise a two-stage numerical algorithm based on Bregman iterations to compute $L_{1,1}$-regularized piecewise flat embeddings. We further generalize this algorithm through iterative reweighting to solve the general $L_{1,p}$-regularized problem. To demonstrate its efficacy, we integrate PFE into two existing image segmentation frameworks, segmentation based on clustering and hierarchical segmentation based on contour detection. Experiments on four major benchmark datasets, BSDS500, MSRC, Stanford Background Dataset, and PASCAL Context, show that segmentation algorithms incorporating our embedding achieve significantly improved results.



### Multiple Target Tracking by Learning Feature Representation and Distance Metric Jointly
- **Arxiv ID**: http://arxiv.org/abs/1802.03252v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.03252v1)
- **Published**: 2018-02-09 13:34:21+00:00
- **Updated**: 2018-02-09 13:34:21+00:00
- **Authors**: Jun Xiang, Guoshuai Zhang, Jianhua Hou, Nong Sang, Rui Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Designing a robust affinity model is the key issue in multiple target tracking (MTT). This paper proposes a novel affinity model by learning feature representation and distance metric jointly in a unified deep architecture. Specifically, we design a CNN network to obtain appearance cue tailored towards person Re-ID, and an LSTM network for motion cue to predict target position, respectively. Both cues are combined with a triplet loss function, which performs end-to-end learning of the fused features in a desired embedding space. Experiments in the challenging MOT benchmark demonstrate, that even by a simple Linear Assignment strategy fed with affinity scores of our method, very competitive results are achieved when compared with the most recent state-of-theart approaches.



### Triplet-based Deep Similarity Learning for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1802.03254v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.03254v1)
- **Published**: 2018-02-09 13:40:59+00:00
- **Updated**: 2018-02-09 13:40:59+00:00
- **Authors**: Wentong Liao, Michael Ying Yang, Ni Zhan, Bodo Rosenhahn
- **Comment**: ICCV Workshops 2017
- **Journal**: None
- **Summary**: In recent years, person re-identification (re-id) catches great attention in both computer vision community and industry. In this paper, we propose a new framework for person re-identification with a triplet-based deep similarity learning using convolutional neural networks (CNNs). The network is trained with triplet input: two of them have the same class labels and the other one is different. It aims to learn the deep feature representation, with which the distance within the same class is decreased, while the distance between the different classes is increased as much as possible. Moreover, we trained the model jointly on six different datasets, which differs from common practice - one model is just trained on one dataset and tested also on the same one. However, the enormous number of possible triplet data among the large number of training samples makes the training impossible. To address this challenge, a double-sampling scheme is proposed to generate triplets of images as effective as possible. The proposed framework is evaluated on several benchmark datasets. The experimental results show that, our method is effective for the task of person re-identification and it is comparable or even outperforms the state-of-the-art methods.



### Video Event Recognition and Anomaly Detection by Combining Gaussian Process and Hierarchical Dirichlet Process Models
- **Arxiv ID**: http://arxiv.org/abs/1802.03257v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.03257v1)
- **Published**: 2018-02-09 13:46:41+00:00
- **Updated**: 2018-02-09 13:46:41+00:00
- **Authors**: Michael Ying Yang, Wentong Liao, Yanpeng Cao, Bodo Rosenhahn
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present an unsupervised learning framework for analyzing activities and interactions in surveillance videos. In our framework, three levels of video events are connected by Hierarchical Dirichlet Process (HDP) model: low-level visual features, simple atomic activities, and multi-agent interactions. Atomic activities are represented as distribution of low-level features, while complicated interactions are represented as distribution of atomic activities. This learning process is unsupervised. Given a training video sequence, low-level visual features are extracted based on optic flow and then clustered into different atomic activities and video clips are clustered into different interactions. The HDP model automatically decide the number of clusters, i.e. the categories of atomic activities and interactions. Based on the learned atomic activities and interactions, a training dataset is generated to train the Gaussian Process (GP) classifier. Then the trained GP models work in newly captured video to classify interactions and detect abnormal events in real time. Furthermore, the temporal dependencies between video events learned by HDP-Hidden Markov Models (HMM) are effectively integrated into GP classifier to enhance the accuracy of the classification in newly captured videos. Our framework couples the benefits of the generative model (HDP) with the discriminant model (GP). We provide detailed experiments showing that our framework enjoys favorable performance in video event classification in real-time in a crowded traffic scene.



### Efficient Neural Architecture Search via Parameter Sharing
- **Arxiv ID**: http://arxiv.org/abs/1802.03268v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1802.03268v2)
- **Published**: 2018-02-09 14:14:37+00:00
- **Updated**: 2018-02-12 03:34:00+00:00
- **Authors**: Hieu Pham, Melody Y. Guan, Barret Zoph, Quoc V. Le, Jeff Dean
- **Comment**: None
- **Journal**: None
- **Summary**: We propose Efficient Neural Architecture Search (ENAS), a fast and inexpensive approach for automatic model design. In ENAS, a controller learns to discover neural network architectures by searching for an optimal subgraph within a large computational graph. The controller is trained with policy gradient to select a subgraph that maximizes the expected reward on the validation set. Meanwhile the model corresponding to the selected subgraph is trained to minimize a canonical cross entropy loss. Thanks to parameter sharing between child models, ENAS is fast: it delivers strong empirical performances using much fewer GPU-hours than all existing automatic model design approaches, and notably, 1000x less expensive than standard Neural Architecture Search. On the Penn Treebank dataset, ENAS discovers a novel architecture that achieves a test perplexity of 55.8, establishing a new state-of-the-art among all methods without post-training processing. On the CIFAR-10 dataset, ENAS designs novel architectures that achieve a test error of 2.89%, which is on par with NASNet (Zoph et al., 2018), whose test error is 2.65%.



### Unsupervised Deep Domain Adaptation for Pedestrian Detection
- **Arxiv ID**: http://arxiv.org/abs/1802.03269v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.03269v1)
- **Published**: 2018-02-09 14:15:35+00:00
- **Updated**: 2018-02-09 14:15:35+00:00
- **Authors**: Lihang Liu, Weiyao Lin, Lisheng Wu, Yong Yu, Michael Ying Yang
- **Comment**: ECCV Workshop 2016
- **Journal**: None
- **Summary**: This paper addresses the problem of unsupervised domain adaptation on the task of pedestrian detection in crowded scenes. First, we utilize an iterative algorithm to iteratively select and auto-annotate positive pedestrian samples with high confidence as the training samples for the target domain. Meanwhile, we also reuse negative samples from the source domain to compensate for the imbalance between the amount of positive samples and negative samples. Second, based on the deep network we also design an unsupervised regularizer to mitigate influence from data noise. More specifically, we transform the last fully connected layer into two sub-layers - an element-wise multiply layer and a sum layer, and add the unsupervised regularizer to further improve the domain adaptation accuracy. In experiments for pedestrian detection, the proposed method boosts the recall value by nearly 30% while the precision stays almost the same. Furthermore, we perform our method on standard domain adaptation benchmarks on both supervised and unsupervised settings and also achieve state-of-the-art results.



### Augmented Reality needle ablation guidance tool for Irreversible Electroporation in the pancreas
- **Arxiv ID**: http://arxiv.org/abs/1802.03274v1
- **DOI**: 10.1117/12.2293671
- **Categories**: **cs.CV**, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1802.03274v1)
- **Published**: 2018-02-09 14:25:56+00:00
- **Updated**: 2018-02-09 14:25:56+00:00
- **Authors**: Timur Kuzhagaliyev, Neil T. Clancy, Mirek Janatka, Kevin Tchaka, Francisco Vasconcelos, Matthew J. Clarkson, Kurinchi Gurusamy, David J. Hawkes, Brian Davidson, Danail Stoyanov
- **Comment**: 6 pages, 5 figures. Proc. SPIE 10576 (2018) Copyright 2018 Society of
  Photo Optical Instrumentation Engineers (SPIE). One print or electronic copy
  may be made for personal use only. Systematic reproduction and distribution,
  duplication of any material in this publication for a fee or for commercial
  purposes, or modification of the contents of the publication are prohibited
- **Journal**: None
- **Summary**: Irreversible electroporation (IRE) is a soft tissue ablation technique suitable for treatment of inoperable tumours in the pancreas. The process involves applying a high voltage electric field to the tissue containing the mass using needle electrodes, leaving cancerous cells irreversibly damaged and vulnerable to apoptosis. Efficacy of the treatment depends heavily on the accuracy of needle placement and requires a high degree of skill from the operator. In this paper, we describe an Augmented Reality (AR) system designed to overcome the challenges associated with planning and guiding the needle insertion process. Our solution, based on the HoloLens (Microsoft, USA) platform, tracks the position of the headset, needle electrodes and ultrasound (US) probe in space. The proof of concept implementation of the system uses this tracking data to render real-time holographic guides on the HoloLens, giving the user insight into the current progress of needle insertion and an indication of the target needle trajectory. The operator's field of view is augmented using visual guides and real-time US feed rendered on a holographic plane, eliminating the need to consult external monitors. Based on these early prototypes, we are aiming to develop a system that will lower the skill level required for IRE while increasing overall accuracy of needle insertion and, hence, the likelihood of successful treatment.



### Slice Sampling Particle Belief Propagation
- **Arxiv ID**: http://arxiv.org/abs/1802.03275v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1802.03275v1)
- **Published**: 2018-02-09 14:27:58+00:00
- **Updated**: 2018-02-09 14:27:58+00:00
- **Authors**: Oliver Mueller, Michael Ying Yang, Bodo Rosenhahn
- **Comment**: published in ICCV 2013
- **Journal**: None
- **Summary**: Inference in continuous label Markov random fields is a challenging task. We use particle belief propagation (PBP) for solving the inference problem in continuous label space. Sampling particles from the belief distribution is typically done by using Metropolis-Hastings Markov chain Monte Carlo methods which involves sampling from a proposal distribution. This proposal distribution has to be carefully designed depending on the particular model and input data to achieve fast convergence. We propose to avoid dependence on a proposal distribution by introducing a slice sampling based PBP algorithm. The proposed approach shows superior convergence performance on an image denoising toy example. Our findings are validated on a challenging relational 2D feature tracking application.



### Temporally Object-based Video Co-Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1802.03279v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.03279v1)
- **Published**: 2018-02-09 14:32:12+00:00
- **Updated**: 2018-02-09 14:32:12+00:00
- **Authors**: Michael Ying Yang, Matthias Reso, Jun Tang, Wentong Liao, Bodo Rosenhahn
- **Comment**: ISVC 2015 (Oral)
- **Journal**: None
- **Summary**: In this paper, we propose an unsupervised video object co-segmentation framework based on the primary object proposals to extract the common foreground object(s) from a given video set. In addition to the objectness attributes and motion coherence our framework exploits the temporal consistency of the object-like regions between adjacent frames to enrich the set of original object proposals. We call the enriched proposal sets temporal proposal streams, as they are composed of the most similar proposals from each frame augmented with predicted proposals using temporally consistent superpixel information. The temporal proposal streams represent all the possible region tubes of the objects. Therefore, we formulate a graphical model to select a proposal stream for each object in which the pairwise potentials consist of the appearance dissimilarity between different streams in the same video and also the similarity between the streams in different videos. This model is suitable for single (multiple) foreground objects in two (more) videos, which can be solved by any existing energy minimization method. We evaluate our proposed framework by comparing it to other video co-segmentation algorithms. Our method achieves improved performance on state-of-the-art benchmark datasets.



### Cognitive Deficit of Deep Learning in Numerosity
- **Arxiv ID**: http://arxiv.org/abs/1802.05160v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.05160v4)
- **Published**: 2018-02-09 15:01:52+00:00
- **Updated**: 2018-11-11 14:47:31+00:00
- **Authors**: Xiaolin Wu, Xi Zhang, Xiao Shu
- **Comment**: Accepted for presentation at the AAAI-19
- **Journal**: None
- **Summary**: Subitizing, or the sense of small natural numbers, is an innate cognitive function of humans and primates; it responds to visual stimuli prior to the development of any symbolic skills, language or arithmetic. Given successes of deep learning (DL) in tasks of visual intelligence and given the primitivity of number sense, a tantalizing question is whether DL can comprehend numbers and perform subitizing. But somewhat disappointingly, extensive experiments of the type of cognitive psychology demonstrate that the examples-driven black box DL cannot see through superficial variations in visual representations and distill the abstract notion of natural number, a task that children perform with high accuracy and confidence. The failure is apparently due to the learning method not the CNN computational machinery itself. A recurrent neural network capable of subitizing does exist, which we construct by encoding a mechanism of mathematical morphology into the CNN convolutional kernels. Also, we investigate, using subitizing as a test bed, the ways to aid the black box DL by cognitive priors derived from human insight. Our findings are mixed and interesting, pointing to both cognitive deficit of pure DL, and some measured successes of boosting DL by predetermined cognitive implements. This case study of DL in cognitive computing is meaningful for visual numerosity represents a minimum level of human intelligence.



### Nature vs. Nurture: The Role of Environmental Resources in Evolutionary Deep Intelligence
- **Arxiv ID**: http://arxiv.org/abs/1802.03318v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1802.03318v1)
- **Published**: 2018-02-09 15:58:58+00:00
- **Updated**: 2018-02-09 15:58:58+00:00
- **Authors**: Audrey G. Chung, Paul Fieguth, Alexander Wong
- **Comment**: None
- **Journal**: None
- **Summary**: Evolutionary deep intelligence synthesizes highly efficient deep neural networks architectures over successive generations. Inspired by the nature versus nurture debate, we propose a study to examine the role of external factors on the network synthesis process by varying the availability of simulated environmental resources. Experimental results were obtained for networks synthesized via asexual evolutionary synthesis (1-parent) and sexual evolutionary synthesis (2-parent, 3-parent, and 5-parent) using a 10% subset of the MNIST dataset. Results show that a lower environmental factor model resulted in a more gradual loss in performance accuracy and decrease in storage size. This potentially allows significantly reduced storage size with minimal to no drop in performance accuracy, and the best networks were synthesized using the lowest environmental factor models.



### Shapes Characterization on Address Event Representation Using Histograms of Oriented Events and an Extended LBP Approach
- **Arxiv ID**: http://arxiv.org/abs/1802.03327v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.03327v1)
- **Published**: 2018-02-09 16:23:21+00:00
- **Updated**: 2018-02-09 16:23:21+00:00
- **Authors**: Pablo Negri
- **Comment**: None
- **Journal**: None
- **Summary**: Address Event Representation is a thriving technology that could change digital image processing paradigm. This paper proposes a methodology to characterize the shape of objects using the streaming of asynchronous events. A new descriptor that enhances spikes connectivity is associated with two oriented histogram based representations. This paper uses these features to develop both a non-supervised and a supervised multi-classification framework to recognize poker symbols from the Poker-DVS public dataset. The aforementioned framework, which uses a very limited number of events and a simple class modeling, yields results that challenge more sophisticated methodologies proposed by the state of the art. A feature family based on context shapes is applied to the more challenging 2015 Poker-DVS dataset with a supervised classifier obtaining an accuracy of 98.5 %. The system is also applied to the MNIST-DVS dataset yielding an accuracy of 94.6 % and 96.3 % on digit recognition, for scales 4 and 8 respectively.



### A Two-Stage Method for Text Line Detection in Historical Documents
- **Arxiv ID**: http://arxiv.org/abs/1802.03345v2
- **DOI**: 10.1007/s10032-019-00332-1
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.03345v2)
- **Published**: 2018-02-09 16:52:17+00:00
- **Updated**: 2019-07-11 09:45:21+00:00
- **Authors**: Tobias Grüning, Gundram Leifert, Tobias Strauß, Johannes Michael, Roger Labahn
- **Comment**: to be published in IJDAR
- **Journal**: International Journal on Document Analysis and Recognition
  (IJDAR), (2019), 1-18
- **Summary**: This work presents a two-stage text line detection method for historical documents. Each detected text line is represented by its baseline. In a first stage, a deep neural network called ARU-Net labels pixels to belong to one of the three classes: baseline, separator or other. The separator class marks beginning and end of each text line. The ARU-Net is trainable from scratch with manageably few manually annotated example images (less than 50). This is achieved by utilizing data augmentation strategies. The network predictions are used as input for the second stage which performs a bottom-up clustering to build baselines. The developed method is capable of handling complex layouts as well as curved and arbitrarily oriented text lines. It substantially outperforms current state-of-the-art approaches. For example, for the complex track of the cBAD: ICDAR2017 Competition on Baseline Detection the F-value is increased from 0.859 to 0.922. The framework to train and run the ARU-Net is open source.



### Generative ScatterNet Hybrid Deep Learning (G-SHDL) Network with Structural Priors for Semantic Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1802.03374v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.03374v2)
- **Published**: 2018-02-09 18:19:51+00:00
- **Updated**: 2018-02-13 15:56:37+00:00
- **Authors**: Amarjot Singh, Nick Kingsbury
- **Comment**: Accepted at the IEEE International Conference on Acoustics, Speech
  and Signal Processing (ICASSP) 2018
- **Journal**: None
- **Summary**: This paper proposes a generative ScatterNet hybrid deep learning (G-SHDL) network for semantic image segmentation. The proposed generative architecture is able to train rapidly from relatively small labeled datasets using the introduced structural priors. In addition, the number of filters in each layer of the architecture is optimized resulting in a computationally efficient architecture. The G-SHDL network produces state-of-the-art classification performance against unsupervised and semi-supervised learning on two image datasets. Advantages of the G-SHDL network over supervised methods are demonstrated with experiments performed on training datasets of reduced size.



### Same-different problems strain convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/1802.03390v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1802.03390v3)
- **Published**: 2018-02-09 18:55:34+00:00
- **Updated**: 2018-05-25 17:00:23+00:00
- **Authors**: Matthew Ricci, Junkyung Kim, Thomas Serre
- **Comment**: 6 Pages, 4 Figures
- **Journal**: None
- **Summary**: The robust and efficient recognition of visual relations in images is a hallmark of biological vision. We argue that, despite recent progress in visual recognition, modern machine vision algorithms are severely limited in their ability to learn visual relations. Through controlled experiments, we demonstrate that visual-relation problems strain convolutional neural networks (CNNs). The networks eventually break altogether when rote memorization becomes impossible, as when intra-class variability exceeds network capacity. Motivated by the comparable success of biological vision, we argue that feedback mechanisms including attention and perceptual grouping may be the key computational components underlying abstract visual reasoning.\



### Fibres of Failure: Classifying errors in predictive processes
- **Arxiv ID**: http://arxiv.org/abs/1803.00384v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DM, math.AT
- **Links**: [PDF](http://arxiv.org/pdf/1803.00384v1)
- **Published**: 2018-02-09 20:36:06+00:00
- **Updated**: 2018-02-09 20:36:06+00:00
- **Authors**: Leo Carlsson, Gunnar Carlsson, Mikael Vejdemo-Johansson
- **Comment**: 10 pages, submitted to ICML 2018
- **Journal**: None
- **Summary**: We describe Fibres of Failure (FiFa), a method to classify failure modes of predictive processes using the Mapper algorithm from Topological Data Analysis.   Our method uses Mapper to build a graph model of input data stratified by prediction error.   Groupings found in high-error regions of the Mapper model then provide distinct failure modes of the predictive process.   We demonstrate FiFa on misclassifications of MNIST images with added noise, and demonstrate two ways to use the failure mode classification: either to produce a correction layer that adjusts predictions by similarity to the failure modes; or to inspect members of the failure modes to illustrate and investigate what characterizes each failure mode.



### Pros and Cons of GAN Evaluation Measures
- **Arxiv ID**: http://arxiv.org/abs/1802.03446v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.03446v5)
- **Published**: 2018-02-09 21:05:32+00:00
- **Updated**: 2018-10-24 02:20:59+00:00
- **Authors**: Ali Borji
- **Comment**: None
- **Journal**: None
- **Summary**: Generative models, in particular generative adversarial networks (GANs), have received significant attention recently. A number of GAN variants have been proposed and have been utilized in many applications. Despite large strides in terms of theoretical progress, evaluating and comparing GANs remains a daunting task. While several measures have been introduced, as of yet, there is no consensus as to which measure best captures strengths and limitations of models and should be used for fair model comparison. As in other areas of computer vision and machine learning, it is critical to settle on one or few good measures to steer the progress in this field. In this paper, I review and critically discuss more than 24 quantitative and 5 qualitative measures for evaluating generative models with a particular emphasis on GAN-derived models. I also provide a set of 7 desiderata followed by an evaluation of whether a given measure or a family of measures is compatible with them.



### Gaussian Process Landmarking on Manifolds
- **Arxiv ID**: http://arxiv.org/abs/1802.03479v4
- **DOI**: None
- **Categories**: **stat.ME**, cs.CV, 60G15, 62K05, 65D18, I.3.5; I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/1802.03479v4)
- **Published**: 2018-02-09 23:50:10+00:00
- **Updated**: 2019-01-08 20:07:46+00:00
- **Authors**: Tingran Gao, Shahar Z. Kovalsky, Ingrid Daubechies
- **Comment**: 30 pages, 3 figures
- **Journal**: None
- **Summary**: As a means of improving analysis of biological shapes, we propose an algorithm for sampling a Riemannian manifold by sequentially selecting points with maximum uncertainty under a Gaussian process model. This greedy strategy is known to be near-optimal in the experimental design literature, and appears to outperform the use of user-placed landmarks in representing the geometry of biological objects in our application. In the noiseless regime, we establish an upper bound for the mean squared prediction error (MSPE) in terms of the number of samples and geometric quantities of the manifold, demonstrating that the MSPE for our proposed sequential design decays at a rate comparable to the oracle rate achievable by any sequential or non-sequential optimal design; to our knowledge this is the first result of this type for sequential experimental design. The key is to link the greedy algorithm to reduced basis methods in the context of model reduction for partial differential equations. We expect this approach will find additional applications in other fields of research.



### GraphVAE: Towards Generation of Small Graphs Using Variational Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/1802.03480v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1802.03480v1)
- **Published**: 2018-02-09 23:57:46+00:00
- **Updated**: 2018-02-09 23:57:46+00:00
- **Authors**: Martin Simonovsky, Nikos Komodakis
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning on graphs has become a popular research topic with many applications. However, past work has concentrated on learning graph embedding tasks, which is in contrast with advances in generative models for images and text. Is it possible to transfer this progress to the domain of graphs? We propose to sidestep hurdles associated with linearization of such discrete structures by having a decoder output a probabilistic fully-connected graph of a predefined maximum size directly at once. Our method is formulated as a variational autoencoder. We evaluate on the challenging task of molecule generation.



