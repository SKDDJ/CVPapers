# Arxiv Papers in cs.CV on 2018-02-13
### Recurrent Slice Networks for 3D Segmentation of Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1802.04402v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.04402v2)
- **Published**: 2018-02-13 00:04:27+00:00
- **Updated**: 2018-03-29 19:22:24+00:00
- **Authors**: Qiangui Huang, Weiyue Wang, Ulrich Neumann
- **Comment**: camera ready version for cvpr 2018 spotlight. codes are available
  here https://github.com/qianguih/RSNet
- **Journal**: None
- **Summary**: Point clouds are an efficient data format for 3D data. However, existing 3D segmentation methods for point clouds either do not model local dependencies \cite{pointnet} or require added computations \cite{kd-net,pointnet2}. This work presents a novel 3D segmentation framework, RSNet\footnote{Codes are released here https://github.com/qianguih/RSNet}, to efficiently model local structures in point clouds. The key component of the RSNet is a lightweight local dependency module. It is a combination of a novel slice pooling layer, Recurrent Neural Network (RNN) layers, and a slice unpooling layer. The slice pooling layer is designed to project features of unordered points onto an ordered sequence of feature vectors so that traditional end-to-end learning algorithms (RNNs) can be applied. The performance of RSNet is validated by comprehensive experiments on the S3DIS\cite{stanford}, ScanNet\cite{scannet}, and ShapeNet \cite{shapenet} datasets. In its simplest form, RSNets surpass all previous state-of-the-art methods on these benchmarks. And comparisons against previous state-of-the-art methods \cite{pointnet, pointnet2} demonstrate the efficiency of RSNets.



### TVAE: Triplet-Based Variational Autoencoder using Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/1802.04403v3
- **DOI**: None
- **Categories**: **stat.ML**, cs.AI, cs.CV, cs.LG, 68T30 (Primary), 68T01 (Secondary)
- **Links**: [PDF](http://arxiv.org/pdf/1802.04403v3)
- **Published**: 2018-02-13 00:05:19+00:00
- **Updated**: 2023-02-08 15:25:57+00:00
- **Authors**: Haque Ishfaq, Assaf Hoogi, Daniel Rubin
- **Comment**: Old technical note
- **Journal**: None
- **Summary**: Deep metric learning has been demonstrated to be highly effective in learning semantic representation and encoding information that can be used to measure data similarity, by relying on the embedding learned from metric learning. At the same time, variational autoencoder (VAE) has widely been used to approximate inference and proved to have a good performance for directed probabilistic models. However, for traditional VAE, the data label or feature information are intractable. Similarly, traditional representation learning approaches fail to represent many salient aspects of the data. In this project, we propose a novel integrated framework to learn latent embedding in VAE by incorporating deep metric learning. The features are learned by optimizing a triplet loss on the mean vectors of VAE in conjunction with standard evidence lower bound (ELBO) of VAE. This approach, which we call Triplet based Variational Autoencoder (TVAE), allows us to capture more fine-grained information in the latent embedding. Our model is tested on MNIST data set and achieves a high triplet accuracy of 95.60% while the traditional VAE (Kingma & Welling, 2013) achieves triplet accuracy of 75.08%.



### Deep Learning Models Delineates Multiple Nuclear Phenotypes in H&E Stained Histology Sections
- **Arxiv ID**: http://arxiv.org/abs/1802.04427v2
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1802.04427v2)
- **Published**: 2018-02-13 01:50:38+00:00
- **Updated**: 2018-02-14 19:02:52+00:00
- **Authors**: Mina Khoshdeli, Bahram Parvin
- **Comment**: None
- **Journal**: None
- **Summary**: Nuclear segmentation is an important step for profiling aberrant regions of histology sections. However, segmentation is a complex problem as a result of variations in nuclear geometry (e.g., size, shape), nuclear type (e.g., epithelial, fibroblast), and nuclear phenotypes (e.g., vesicular, aneuploidy). The problem is further complicated as a result of variations in sample preparation. It is shown and validated that fusion of very deep convolutional networks overcomes (i) complexities associated with multiple nuclear phenotypes, and (ii) separation of overlapping nuclei. The fusion relies on integrating of networks that learn region- and boundary-based representations. The system has been validated on a diverse set of nuclear phenotypes that correspond to the breast and brain histology sections.



### Texture Classification in Extreme Scale Variations using GANet
- **Arxiv ID**: http://arxiv.org/abs/1802.04441v1
- **DOI**: 10.1109/TIP.2019.2903300
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.04441v1)
- **Published**: 2018-02-13 02:29:57+00:00
- **Updated**: 2018-02-13 02:29:57+00:00
- **Authors**: Li Liu, Jie Chen, Guoying Zhao, Paul Fieguth, Xilin Chen, Matti Pietik√§inen
- **Comment**: submitted to IEEE Transactions on Image Processing
- **Journal**: None
- **Summary**: Research in texture recognition often concentrates on recognizing textures with intraclass variations such as illumination, rotation, viewpoint and small scale changes. In contrast, in real-world applications a change in scale can have a dramatic impact on texture appearance, to the point of changing completely from one texture category to another. As a result, texture variations due to changes in scale are amongst the hardest to handle. In this work we conduct the first study of classifying textures with extreme variations in scale. To address this issue, we first propose and then reduce scale proposals on the basis of dominant texture patterns. Motivated by the challenges posed by this problem, we propose a new GANet network where we use a Genetic Algorithm to change the units in the hidden layers during network training, in order to promote the learning of more informative semantic texture patterns. Finally, we adopt a FVCNN (Fisher Vector pooling of a Convolutional Neural Network filter bank) feature encoder for global texture representation.   Because extreme scale variations are not necessarily present in most standard texture databases, to support the proposed extreme-scale aspects of texture understanding we are developing a new dataset, the Extreme Scale Variation Textures (ESVaT), to test the performance of our framework. It is demonstrated that the proposed framework significantly outperforms gold-standard texture features by more than 10% on ESVaT. We also test the performance of our proposed approach on the KTHTIPS2b and OS datasets and a further dataset synthetically derived from Forrest, showing superior performance compared to the state of the art.



### An Optimized Architecture for Unpaired Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/1802.04467v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.04467v1)
- **Published**: 2018-02-13 05:56:18+00:00
- **Updated**: 2018-02-13 05:56:18+00:00
- **Authors**: Mohan Nikam
- **Comment**: Accepted to be published in Springer Advances in Intelligent Systems
  and Computing (AISC) Series 11156. Accepted for presentation in Springer
  ICANI (International Conference on Advanced computing, Networking and
  Informatics)-2018
- **Journal**: None
- **Summary**: Unpaired Image-to-Image translation aims to convert the image from one domain (input domain A) to another domain (target domain B), without providing paired examples for the training. The state-of-the-art, Cycle-GAN demonstrated the power of Generative Adversarial Networks with Cycle-Consistency Loss. While its results are promising, there is scope for optimization in the training process. This paper introduces a new neural network architecture, which only learns the translation from domain A to B and eliminates the need for reverse mapping (B to A), by introducing a new Deviation-loss term. Furthermore, few other improvements to the Cycle-GAN are found and utilized in this new architecture, contributing to significantly lesser training duration.



### Query-Free Attacks on Industry-Grade Face Recognition Systems under Resource Constraints
- **Arxiv ID**: http://arxiv.org/abs/1802.09900v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1802.09900v2)
- **Published**: 2018-02-13 08:44:58+00:00
- **Updated**: 2018-08-22 13:19:33+00:00
- **Authors**: Di Tang, XiaoFeng Wang, Kehuan Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: To launch black-box attacks against a Deep Neural Network (DNN) based Face Recognition (FR) system, one needs to build \textit{substitute} models to simulate the target model, so the adversarial examples discovered from substitute models could also mislead the target model. Such \textit{transferability} is achieved in recent studies through querying the target model to obtain data for training the substitute models. A real-world target, likes the FR system of law enforcement, however, is less accessible to the adversary. To attack such a system, a substitute model with similar quality as the target model is needed to identify their common defects. This is hard since the adversary often does not have the enough resources to train such a powerful model (hundreds of millions of images and rooms of GPUs are needed to train a commercial FR system).   We found in our research, however, that a resource-constrained adversary could still effectively approximate the target model's capability to recognize \textit{specific} individuals, by training \textit{biased} substitute models on additional images of those victims whose identities the attacker want to cover or impersonate. This is made possible by a new property we discovered, called \textit{Nearly Local Linearity} (NLL), which models the observation that an ideal DNN model produces the image representations (embeddings) whose distances among themselves truthfully describe the human perception of the differences among the input images. By simulating this property around the victim's images, we significantly improve the transferability of black-box impersonation attacks by nearly 50\%. Particularly, we successfully attacked a commercial system trained over 20 million images, using 4 million images and 1/5 of the training time but achieving 62\% transferability in an impersonation attack and 89\% in a dodging attack.



### Robust Deformation Estimation in Wood-Composite Materials using Variational Optical Flow
- **Arxiv ID**: http://arxiv.org/abs/1802.04546v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.04546v1)
- **Published**: 2018-02-13 10:34:59+00:00
- **Updated**: 2018-02-13 10:34:59+00:00
- **Authors**: Markus Hofinger, Thomas Pock, Thomas Moosbrugger
- **Comment**: 8 pages, 8 figures, originally published in 23 rd Computer Vision
  Winter Workshop proceedings 2018
  http://cmp.felk.cvut.cz/cvww2018/papers/28.pdf
- **Journal**: 23rd Computer Vision Winter Workshop proceedings February 2018
  page 97-104
- **Summary**: Wood-composite materials are widely used today as they homogenize humidity related directional deformations. Quantification of these deformations as coefficients is important for construction and engineering and topic of current research but still a manual process.   This work introduces a novel computer vision approach that automatically extracts these properties directly from scans of the wooden specimens, taken at different humidity levels during the long lasting humidity conditioning process. These scans are used to compute a humidity dependent deformation field for each pixel, from which the desired coefficients can easily be calculated.   The overall method includes automated registration of the wooden blocks, numerical optimization to compute a variational optical flow field which is further used to calculate dense strain fields and finally the engineering coefficients and their variance throughout the wooden blocks. The methods regularization is fully parameterizable which allows to model and suppress artifacts due to surface appearance changes of the specimens from mold, cracks, etc. that typically arise in the conditioning process.



### Automatic localization and decoding of honeybee markers using deep convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/1802.04557v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.04557v2)
- **Published**: 2018-02-13 11:03:30+00:00
- **Updated**: 2018-02-14 14:27:21+00:00
- **Authors**: Benjamin Wild, Leon Sixt, Tim Landgraf
- **Comment**: None
- **Journal**: None
- **Summary**: The honeybee is a fascinating model animal to investigate how collective behavior emerges from (inter-)actions of thousands of individuals. Bees may acquire unique memories throughout their lives. These experiences affect social interactions even over large time frames. Tracking and identifying all bees in the colony over their lifetimes therefore may likely shed light on the interplay of individual differences and colony behavior. This paper proposes a software pipeline based on two deep convolutional neural networks for the localization and decoding of custom binary markers that honeybees carry from their first to the last day in their life. We show that this approach outperforms similar systems proposed in recent literature. By opening this software for the public, we hope that the resulting datasets will help advancing the understanding of honeybee collective intelligence.



### Modeling of Facial Aging and Kinship: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1802.04636v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.04636v2)
- **Published**: 2018-02-13 14:26:40+00:00
- **Updated**: 2018-12-01 18:05:13+00:00
- **Authors**: Markos Georgopoulos, Yannis Panagakis, Maja Pantic
- **Comment**: None
- **Journal**: None
- **Summary**: Computational facial models that capture properties of facial cues related to aging and kinship increasingly attract the attention of the research community, enabling the development of reliable methods for age progression, age estimation, age-invariant facial characterization, and kinship verification from visual data. In this paper, we review recent advances in modeling of facial aging and kinship. In particular, we provide an up-to date, complete list of available annotated datasets and an in-depth analysis of geometric, hand-crafted, and learned facial representations that are used for facial aging and kinship characterization. Moreover, evaluation protocols and metrics are reviewed and notable experimental results for each surveyed task are analyzed. This survey allows us to identify challenges and discuss future research directions for the development of robust facial models in real-world conditions.



### Single-Perspective Warps in Natural Image Stitching
- **Arxiv ID**: http://arxiv.org/abs/1802.04645v2
- **DOI**: 10.1109/TIP.2019.2934344
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.04645v2)
- **Published**: 2018-02-13 14:40:24+00:00
- **Updated**: 2018-03-07 10:47:37+00:00
- **Authors**: Tianli Liao, Nan Li
- **Comment**: 10 pages, 10 figures
- **Journal**: None
- **Summary**: Results of image stitching can be perceptually divided into single-perspective and multiple-perspective. Compared to the multiple-perspective result, the single-perspective result excels in perspective consistency but suffers from projective distortion. In this paper, we propose two single-perspective warps for natural image stitching. The first one is a parametric warp, which is a combination of the as-projective-as-possible warp and the quasi-homography warp via dual-feature. The second one is a mesh-based warp, which is determined by optimizing a total energy function that simultaneously emphasizes different characteristics of the single-perspective warp, including alignment, naturalness, distortion and saliency. A comprehensive evaluation demonstrates that the proposed warp outperforms some state-of-the-art warps, including homography, APAP, AutoStitch, SPHP and GSP.



### Weakly supervised collective feature learning from curated media
- **Arxiv ID**: http://arxiv.org/abs/1802.04668v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, cs.SI
- **Links**: [PDF](http://arxiv.org/pdf/1802.04668v1)
- **Published**: 2018-02-13 15:05:10+00:00
- **Updated**: 2018-02-13 15:05:10+00:00
- **Authors**: Yusuke Mukuta, Akisato Kimura, David B Adrian, Zoubin Ghahramani
- **Comment**: Published in the Proceedings of AAAI Conferenrence on Artificial
  Intelligence (AAAI2018)
- **Journal**: None
- **Summary**: The current state-of-the-art in feature learning relies on the supervised learning of large-scale datasets consisting of target content items and their respective category labels. However, constructing such large-scale fully-labeled datasets generally requires painstaking manual effort. One possible solution to this problem is to employ community contributed text tags as weak labels, however, the concepts underlying a single text tag strongly depends on the users. We instead present a new paradigm for learning discriminative features by making full use of the human curation process on social networking services (SNSs). During the process of content curation, SNS users collect content items manually from various sources and group them by context, all for their own benefit. Due to the nature of this process, we can assume that (1) content items in the same group share the same semantic concept and (2) groups sharing the same images might have related semantic concepts. Through these insights, we can define human curated groups as weak labels from which our proposed framework can learn discriminative features as a representation in the space of semantic concepts the users intended when creating the groups. We show that this feature learning can be formulated as a problem of link prediction for a bipartite graph whose nodes corresponds to content items and human curated groups, and propose a novel method for feature learning based on sparse coding or network fine-tuning.



### BIRNet: Brain Image Registration Using Dual-Supervised Fully Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1802.04692v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.04692v1)
- **Published**: 2018-02-13 15:49:34+00:00
- **Updated**: 2018-02-13 15:49:34+00:00
- **Authors**: Jingfan Fan, Xiaohuan Cao, Pew-Thian Yap, Dinggang Shen
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a deep learning approach for image registration by predicting deformation from image appearance. Since obtaining ground-truth deformation fields for training can be challenging, we design a fully convolutional network that is subject to dual-guidance: (1) Coarse guidance using deformation fields obtained by an existing registration method; and (2) Fine guidance using image similarity. The latter guidance helps avoid overly relying on the supervision from the training deformation fields, which could be inaccurate. For effective training, we further improve the deep convolutional network with gap filling, hierarchical loss, and multi-source strategies. Experiments on a variety of datasets show promising registration accuracy and efficiency compared with state-of-the-art methods.



### Joint Demosaicing and Denoising with Perceptual Optimization on a Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/1802.04723v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.04723v1)
- **Published**: 2018-02-13 16:40:12+00:00
- **Updated**: 2018-02-13 16:40:12+00:00
- **Authors**: Weishong Dong, Ming Yuan, Xin Li, Guangming Shi
- **Comment**: None
- **Journal**: None
- **Summary**: Image demosaicing - one of the most important early stages in digital camera pipelines - addressed the problem of reconstructing a full-resolution image from so-called color-filter-arrays. Despite tremendous progress made in the pase decade, a fundamental issue that remains to be addressed is how to assure the visual quality of reconstructed images especially in the presence of noise corruption. Inspired by recent advances in generative adversarial networks (GAN), we present a novel deep learning approach toward joint demosaicing and denoising (JDD) with perceptual optimization in order to ensure the visual quality of reconstructed images. The key contributions of this work include: 1) we have developed a GAN-based approach toward image demosacing in which a discriminator network with both perceptual and adversarial loss functions are used for quality assurance; 2) we propose to optimize the perceptual quality of reconstructed images by the proposed GAN in an end-to-end manner. Such end-to-end optimization of GAN is particularly effective for jointly exploiting the gain brought by each modular component (e.g., residue learning in the generative network and perceptual loss in the discriminator network). Our extensive experimental results have shown convincingly improved performance over existing state-of-the-art methods in terms of both subjective and objective quality metrics with a comparable computational cost.



### Semantic Scene Completion Combining Colour and Depth: preliminary experiments
- **Arxiv ID**: http://arxiv.org/abs/1802.04735v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.04735v1)
- **Published**: 2018-02-13 16:59:40+00:00
- **Updated**: 2018-02-13 16:59:40+00:00
- **Authors**: Andre Bernardes Soares Guedes, Teofilo Emidio de Campos, Adrian Hilton
- **Comment**: 5 pages, 2 figures
- **Journal**: None
- **Summary**: Semantic scene completion is the task of producing a complete 3D voxel representation of volumetric occupancy with semantic labels for a scene from a single-view observation. We built upon the recent work of Song et al. (CVPR 2017), who proposed SSCnet, a method that performs scene completion and semantic labelling in a single end-to-end 3D convolutional network. SSCnet uses only depth maps as input, even though depth maps are usually obtained from devices that also capture colour information, such as RGBD sensors and stereo cameras. In this work, we investigate the potential of the RGB colour channels to improve SSCnet.



### Joint 3D Reconstruction of a Static Scene and Moving Objects
- **Arxiv ID**: http://arxiv.org/abs/1802.04738v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.04738v1)
- **Published**: 2018-02-13 17:05:55+00:00
- **Updated**: 2018-02-13 17:05:55+00:00
- **Authors**: Sergio Caccamo, Esra Ataer-Cansizoglu, Yuichi Taguchi
- **Comment**: This paper has been accepted and presented in 3DV-2017 conference
  held at Qingdao, China. Video experiments: https://youtu.be/goflUxzG2VI
- **Journal**: Proceedings International Conference on 3D Vision 2017
- **Summary**: We present a technique for simultaneous 3D reconstruction of static regions and rigidly moving objects in a scene. An RGB-D frame is represented as a collection of features, which are points and planes. We classify the features into static and dynamic regions and grow separate maps, static and object maps, for each of them. To robustly classify the features in each frame, we fuse multiple RANSAC-based registration results obtained by registering different groups of the features to different maps, including (1) all the features to the static map, (2) all the features to each object map, and (3) subsets of the features, each forming a segment, to each object map. This multi-group registration approach is designed to overcome the following challenges: scenes can be dominated by static regions, making object tracking more difficult; and moving object might have larger pose variation between frames compared to the static regions. We show qualitative results from indoor scenes with objects in various shapes. The technique enables on-the-fly object model generation to be used for robotic manipulation.



### Deep Predictive Coding Network for Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/1802.04762v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.04762v2)
- **Published**: 2018-02-13 17:49:33+00:00
- **Updated**: 2018-07-29 12:55:24+00:00
- **Authors**: Haiguang Wen, Kuan Han, Junxing Shi, Yizhen Zhang, Eugenio Culurciello, Zhongming Liu
- **Comment**: 10 pages, 5 figures, 4 tables
- **Journal**: None
- **Summary**: Based on the predictive coding theory in neuroscience, we designed a bi-directional and recurrent neural net, namely deep predictive coding networks (PCN). It has feedforward, feedback, and recurrent connections. Feedback connections from a higher layer carry the prediction of its lower-layer representation; feedforward connections carry the prediction errors to its higher-layer. Given image input, PCN runs recursive cycles of bottom-up and top-down computation to update its internal representations and reduce the difference between bottom-up input and top-down prediction at every layer. After multiple cycles of recursive updating, the representation is used for image classification. With benchmark data (CIFAR-10/100, SVHN, and MNIST), PCN was found to always outperform its feedforward-only counterpart: a model without any mechanism for recurrent dynamics. Its performance tended to improve given more cycles of computation over time. In short, PCN reuses a single architecture to recursively run bottom-up and top-down processes. As a dynamical system, PCN can be unfolded to a feedforward model that becomes deeper and deeper over time, while refining it representation towards more accurate and definitive object recognition.



### Learning Filter Scale and Orientation In CNNs
- **Arxiv ID**: http://arxiv.org/abs/1803.00388v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.00388v1)
- **Published**: 2018-02-13 19:45:21+00:00
- **Updated**: 2018-02-13 19:45:21+00:00
- **Authors**: Ilker Cam, F. Boray Tek
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks have many hyperparameters such as the filter size, number of filters, and pooling size, which require manual tuning. Though deep stacked structures are able to create multi-scale and hierarchical representations, manually fixed filter sizes limit the scale of representations that can be learned in a single convolutional layer.   This paper introduces a new adaptive filter model that allows variable scale and orientation. The scale and orientation parameters of filters can be learned using back propagation. Therefore, in a single convolution layer, we can create filters of different scale and orientation that can adapt to small or large features and objects. The proposed model uses a relatively large base size (grid) for filters. In the grid, a differentiable function acts as an envelope for the filters. The envelope function guides effective filter scale and shape/orientation by masking the filter weights before the convolution. Therefore, only the weights in the envelope are updated during training.   In this work, we employed a multivariate (2D) Gaussian as the envelope function and showed that it can grow, shrink, or rotate by updating its covariance matrix during back propagation training . We tested the new filter model on MNIST, MNIST-cluttered, and CIFAR-10 and compared the results with the networks that used conventional convolution layers. The results demonstrate that the new model can effectively learn and produce filters of different scales and orientations in a single layer. Moreover, the experiments show that the adaptive convolution layers perform equally; or better, especially when data includes objects of varying scale and noisy backgrounds.



### Challenging Images For Minds and Machines
- **Arxiv ID**: http://arxiv.org/abs/1802.04834v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1802.04834v1)
- **Published**: 2018-02-13 19:50:41+00:00
- **Updated**: 2018-02-13 19:50:41+00:00
- **Authors**: Amir Rosenfeld, John K. Tsotsos
- **Comment**: None
- **Journal**: None
- **Summary**: There is no denying the tremendous leap in the performance of machine learning methods in the past half-decade. Some might even say that specific sub-fields in pattern recognition, such as machine-vision, are as good as solved, reaching human and super-human levels. Arguably, lack of training data and computation power are all that stand between us and solving the remaining ones. In this position paper we underline cases in vision which are challenging to machines and even to human observers. This is to show limitations of contemporary models that are hard to ameliorate by following the current trend to increase training data, network capacity or computational power. Moreover, we claim that attempting to do so is in principle a suboptimal approach. We provide a taster of such examples in hope to encourage and challenge the machine learning community to develop new directions to solve the said difficulties.



### Learning via social awareness: Improving a deep generative sketching model with facial feedback
- **Arxiv ID**: http://arxiv.org/abs/1802.04877v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1802.04877v2)
- **Published**: 2018-02-13 22:19:10+00:00
- **Updated**: 2018-08-27 18:45:48+00:00
- **Authors**: Natasha Jaques, Jennifer McCleary, Jesse Engel, David Ha, Fred Bertsch, Rosalind Picard, Douglas Eck
- **Comment**: None
- **Journal**: None
- **Summary**: In the quest towards general artificial intelligence (AI), researchers have explored developing loss functions that act as intrinsic motivators in the absence of external rewards. This paper argues that such research has overlooked an important and useful intrinsic motivator: social interaction. We posit that making an AI agent aware of implicit social feedback from humans can allow for faster learning of more generalizable and useful representations, and could potentially impact AI safety. We collect social feedback in the form of facial expression reactions to samples from Sketch RNN, an LSTM-based variational autoencoder (VAE) designed to produce sketch drawings. We use a Latent Constraints GAN (LC-GAN) to learn from the facial feedback of a small group of viewers, by optimizing the model to produce sketches that it predicts will lead to more positive facial expressions. We show in multiple independent evaluations that the model trained with facial feedback produced sketches that are more highly rated, and induce significantly more positive facial expressions. Thus, we establish that implicit social feedback can improve the output of a deep learning model.



### Satellite Image Forgery Detection and Localization Using GAN and One-Class Classifier
- **Arxiv ID**: http://arxiv.org/abs/1802.04881v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.04881v1)
- **Published**: 2018-02-13 22:28:58+00:00
- **Updated**: 2018-02-13 22:28:58+00:00
- **Authors**: Sri Kalyan Yarlagadda, David G√ºera, Paolo Bestagini, Fengqing Maggie Zhu, Stefano Tubaro, Edward J. Delp
- **Comment**: Presented at the IS&T International Symposium on Electronic Imaging
  (EI)
- **Journal**: None
- **Summary**: Current satellite imaging technology enables shooting high-resolution pictures of the ground. As any other kind of digital images, overhead pictures can also be easily forged. However, common image forensic techniques are often developed for consumer camera images, which strongly differ in their nature from satellite ones (e.g., compression schemes, post-processing, sensors, etc.). Therefore, many accurate state-of-the-art forensic algorithms are bound to fail if blindly applied to overhead image analysis. Development of novel forensic tools for satellite images is paramount to assess their authenticity and integrity. In this paper, we propose an algorithm for satellite image forgery detection and localization. Specifically, we consider the scenario in which pixels within a region of a satellite image are replaced to add or remove an object from the scene. Our algorithm works under the assumption that no forged images are available for training. Using a generative adversarial network (GAN), we learn a feature representation of pristine satellite images. A one-class support vector machine (SVM) is trained on these features to determine their distribution. Finally, image forgeries are detected as anomalies. The proposed algorithm is validated against different kinds of satellite images containing forgeries of different size and shape.



### Computer-Aided Knee Joint Magnetic Resonance Image Segmentation - A Survey
- **Arxiv ID**: http://arxiv.org/abs/1802.04894v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.04894v1)
- **Published**: 2018-02-13 23:26:01+00:00
- **Updated**: 2018-02-13 23:26:01+00:00
- **Authors**: Boyu Zhang, Yingtao Zhang, H. D. Cheng, Min Xian, Shan Gai, Olivia Cheng, Kuan Huang
- **Comment**: 10 pages, 6 tables
- **Journal**: None
- **Summary**: Osteoarthritis (OA) is one of the major health issues among the elderly population. MRI is the most popular technology to observe and evaluate the progress of OA course. However, the extreme labor cost of MRI analysis makes the process inefficient and expensive. Also, due to human error and subjective nature, the inter- and intra-observer variability is rather high. Computer-aided knee MRI segmentation is currently an active research field because it can alleviate doctors and radiologists from the time consuming and tedious job, and improve the diagnosis performance which has immense potential for both clinic and scientific research. In the past decades, researchers have investigated automatic/semi-automatic knee MRI segmentation methods extensively. However, to the best of our knowledge, there is no comprehensive survey paper in this field yet. In this survey paper, we classify the existing methods by their principles and discuss the current research status and point out the future research trend in-depth.



