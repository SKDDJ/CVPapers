# Arxiv Papers in cs.CV on 2018-11-09
### Learning Energy Based Inpainting for Optical Flow
- **Arxiv ID**: http://arxiv.org/abs/1811.03721v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.03721v1)
- **Published**: 2018-11-09 00:14:38+00:00
- **Updated**: 2018-11-09 00:14:38+00:00
- **Authors**: Christoph Vogel, Patrick Kn√∂belreiter, Thomas Pock
- **Comment**: None
- **Journal**: Proc. Asian Conf. on Computer Vision (ACCV), 2018
- **Summary**: Modern optical flow methods are often composed of a cascade of many independent steps or formulated as a black box neural network that is hard to interpret and analyze. In this work we seek for a plain, interpretable, but learnable solution. We propose a novel inpainting based algorithm that approaches the problem in three steps: feature selection and matching, selection of supporting points and energy based inpainting. To facilitate the inference we propose an optimization layer that allows to backpropagate through 10K iterations of a first-order method without any numerical or memory problems. Compared to recent state-of-the-art networks, our modular CNN is very lightweight and competitive with other, more involved, inpainting based methods.



### Semantic and Contrast-Aware Saliency
- **Arxiv ID**: http://arxiv.org/abs/1811.03736v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.03736v1)
- **Published**: 2018-11-09 02:03:01+00:00
- **Updated**: 2018-11-09 02:03:01+00:00
- **Authors**: Xiaoshuai Sun
- **Comment**: arXiv admin note: text overlap with arXiv:1710.04071 by other authors
- **Journal**: None
- **Summary**: In this paper, we proposed an integrated model of semantic-aware and contrast-aware saliency combining both bottom-up and top-down cues for effective saliency estimation and eye fixation prediction. The proposed model processes visual information using two pathways. The first pathway aims to capture the attractive semantic information in images, especially for the presence of meaningful objects and object parts such as human faces. The second pathway is based on multi-scale on-line feature learning and information maximization, which learns an adaptive sparse representation for the input and discovers the high contrast salient patterns within the image context. The two pathways characterize both long-term and short-term attention cues and are integrated dynamically using maxima normalization. We investigate two different implementations of the semantic pathway including an End-to-End deep neural network solution and a dynamic feature integration solution, resulting in the SCA and SCAFI model respectively. Experimental results on artificial images and 5 popular benchmark datasets demonstrate the superior performance and better plausibility of the proposed model over both classic approaches and recent deep models.



### Scene Parsing via Dense Recurrent Neural Networks with Attentional Selection
- **Arxiv ID**: http://arxiv.org/abs/1811.04778v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.04778v1)
- **Published**: 2018-11-09 02:46:05+00:00
- **Updated**: 2018-11-09 02:46:05+00:00
- **Authors**: Heng Fan, Peng Chu, Longin Jan Latecki, Haibin Ling
- **Comment**: 10 pages. arXiv admin note: substantial text overlap with
  arXiv:1801.06831
- **Journal**: None
- **Summary**: Recurrent neural networks (RNNs) have shown the ability to improve scene parsing through capturing long-range dependencies among image units. In this paper, we propose dense RNNs for scene labeling by exploring various long-range semantic dependencies among image units. Different from existing RNN based approaches, our dense RNNs are able to capture richer contextual dependencies for each image unit by enabling immediate connections between each pair of image units, which significantly enhances their discriminative power. Besides, to select relevant dependencies and meanwhile to restrain irrelevant ones for each unit from dense connections, we introduce an attention model into dense RNNs. The attention model allows automatically assigning more importance to helpful dependencies while less weight to unconcerned dependencies. Integrating with convolutional neural networks (CNNs), we develop an end-to-end scene labeling system. Extensive experiments on three large-scale benchmarks demonstrate that the proposed approach can improve the baselines by large margins and outperform other state-of-the-art algorithms.



### Typeface Completion with Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1811.03762v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.03762v2)
- **Published**: 2018-11-09 03:49:12+00:00
- **Updated**: 2018-12-13 13:01:12+00:00
- **Authors**: Yonggyu Park, Junhyun Lee, Yookyung Koh, Inyeop Lee, Jinhyuk Lee, Jaewoo Kang
- **Comment**: None
- **Journal**: None
- **Summary**: The mood of a text and the intention of the writer can be reflected in the typeface. However, in designing a typeface, it is difficult to keep the style of various characters consistent, especially for languages with lots of morphological variations such as Chinese. In this paper, we propose a Typeface Completion Network (TCN) which takes one character as an input, and automatically completes the entire set of characters in the same style as the input characters. Unlike existing models proposed for image-to-image translation, TCN embeds a character image into two separate vectors representing typeface and content. Combined with a reconstruction loss from the latent space, and with other various losses, TCN overcomes the inherent difficulty in designing a typeface. Also, compared to previous image-to-image translation models, TCN generates high quality character images of the same typeface with a much smaller number of model parameters. We validate our proposed model on the Chinese and English character datasets, which is paired data, and the CelebA dataset, which is unpaired data. In these datasets, TCN outperforms recently proposed state-of-the-art models for image-to-image translation. The source code of our model is available at https://github.com/yongqyu/TCN.



### M2M-GAN: Many-to-Many Generative Adversarial Transfer Learning for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1811.03768v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.03768v1)
- **Published**: 2018-11-09 04:14:52+00:00
- **Updated**: 2018-11-09 04:14:52+00:00
- **Authors**: Wenqi Liang, Guangcong Wang, Jianhuang Lai, Junyong Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Cross-domain transfer learning (CDTL) is an extremely challenging task for the person re-identification (ReID). Given a source domain with annotations and a target domain without annotations, CDTL seeks an effective method to transfer the knowledge from the source domain to the target domain. However, such a simple two-domain transfer learning method is unavailable for the person ReID in that the source/target domain consists of several sub-domains, e.g., camera-based sub-domains. To address this intractable problem, we propose a novel Many-to-Many Generative Adversarial Transfer Learning method (M2M-GAN) that takes multiple source sub-domains and multiple target sub-domains into consideration and performs each sub-domain transferring mapping from the source domain to the target domain in a unified optimization process. The proposed method first translates the image styles of source sub-domains into that of target sub-domains, and then performs the supervised learning by using the transferred images and the corresponding annotations in source domain. As the gap is reduced, M2M-GAN achieves a promising result for the cross-domain person ReID. Experimental results on three benchmark datasets Market-1501, DukeMTMC-reID and MSMT17 show the effectiveness of our M2M-GAN.



### A Fully Automated System for Sizing Nasal PAP Masks Using Facial Photographs
- **Arxiv ID**: http://arxiv.org/abs/1811.03773v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.03773v1)
- **Published**: 2018-11-09 04:50:53+00:00
- **Updated**: 2018-11-09 04:50:53+00:00
- **Authors**: Benjamin Johnston, Philip de Chazal
- **Comment**: IEEE EMBS 2018
- **Journal**: None
- **Summary**: We present a fully automated system for sizing nasal Positive Airway Pressure (PAP) masks. The system is comprised of a mix of HOG object detectors as well as multiple convolutional neural network stages for facial landmark detection. The models were trained using samples from the publicly available PUT and MUCT datasets while transfer learning was also employed to improve the performance of the models on facial photographs of actual PAP mask users. The fully automated system demonstrated an overall accuracy of 64.71% in correctly selecting the appropriate mask size and 86.1% accuracy sizing within 1 mask size.



### A Theoretically Guaranteed Deep Optimization Framework for Robust Compressive Sensing MRI
- **Arxiv ID**: http://arxiv.org/abs/1811.03782v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.03782v3)
- **Published**: 2018-11-09 05:35:50+00:00
- **Updated**: 2018-11-13 13:46:13+00:00
- **Authors**: Risheng Liu, Yuxi Zhang, Shichao Cheng, Xin Fan, Zhongxuan Luo
- **Comment**: None
- **Journal**: None
- **Summary**: Magnetic Resonance Imaging (MRI) is one of the most dynamic and safe imaging techniques available for clinical applications. However, the rather slow speed of MRI acquisitions limits the patient throughput and potential indi cations. Compressive Sensing (CS) has proven to be an efficient technique for accelerating MRI acquisition. The most widely used CS-MRI model, founded on the premise of reconstructing an image from an incompletely filled k-space, leads to an ill-posed inverse problem. In the past years, lots of efforts have been made to efficiently optimize the CS-MRI model. Inspired by deep learning techniques, some preliminary works have tried to incorporate deep architectures into CS-MRI process. Unfortunately, the convergence issues (due to the experience-based networks) and the robustness (i.e., lack real-world noise modeling) of these deeply trained optimization methods are still missing. In this work, we develop a new paradigm to integrate designed numerical solvers and the data-driven architectures for CS-MRI. By introducing an optimal condition checking mechanism, we can successfully prove the convergence of our established deep CS-MRI optimization scheme. Furthermore, we explicitly formulate the Rician noise distributions within our framework and obtain an extended CS-MRI network to handle the real-world nosies in the MRI process. Extensive experimental results verify that the proposed paradigm outperforms the existing state-of-the-art techniques both in reconstruction accuracy and efficiency as well as robustness to noises in real scene.



### Gradient Descent Finds Global Minima of Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1811.03804v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, math.OC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.03804v4)
- **Published**: 2018-11-09 07:39:59+00:00
- **Updated**: 2019-05-28 19:01:22+00:00
- **Authors**: Simon S. Du, Jason D. Lee, Haochuan Li, Liwei Wang, Xiyu Zhai
- **Comment**: ICML 2019
- **Journal**: None
- **Summary**: Gradient descent finds a global minimum in training deep neural networks despite the objective function being non-convex. The current paper proves gradient descent achieves zero training loss in polynomial time for a deep over-parameterized neural network with residual connections (ResNet). Our analysis relies on the particular structure of the Gram matrix induced by the neural network architecture. This structure allows us to show the Gram matrix is stable throughout the training process and this stability implies the global optimality of the gradient descent algorithm. We further extend our analysis to deep residual convolutional neural networks and obtain a similar convergence result.



### Neural Stain Normalization and Unsupervised Classification of Cell Nuclei in Histopathological Breast Cancer Images
- **Arxiv ID**: http://arxiv.org/abs/1811.03815v1
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1811.03815v1)
- **Published**: 2018-11-09 08:34:36+00:00
- **Updated**: 2018-11-09 08:34:36+00:00
- **Authors**: Edwin Yuan, Junkyo Suh
- **Comment**: 9 pages, 8 figures
- **Journal**: None
- **Summary**: In this paper, we develop a complete pipeline for stain normalization, segmentation, and classification of nuclei in hematoxylin and eosin (H&E) stained breast cancer histopathology images. In the first step, we use a CNN-based stain transfer technique to normalize the staining characteristics of (H&E) images. We then train a neural network to segment images of nuclei from the H&E images. Finally, we train an Information Maximizing Generative Adversarial Network (InfoGAN) to learn visual representations of different types of nuclei and classify them in an entirely unsupervised manner. The results show that our proposed CNN stain normalization yields improved visual similarity and cell segmentation performance compared to the conventional SVD-based stain normalization method. In the final step of our pipeline, we demonstrate the ability to perform fully unsupervised clustering of various breast histopathology cell types based on morphological and color attributes. In addition, we quantitatively evaluate our neural network - based techniques against various quantitative metrics to validate the effectiveness of our pipeline.



### RoarNet: A Robust 3D Object Detection based on RegiOn Approximation Refinement
- **Arxiv ID**: http://arxiv.org/abs/1811.03818v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.03818v1)
- **Published**: 2018-11-09 08:43:45+00:00
- **Updated**: 2018-11-09 08:43:45+00:00
- **Authors**: Kiwoo Shin, Youngwook Paul Kwon, Masayoshi Tomizuka
- **Comment**: 7 pages, 8 figures, 2 tables
- **Journal**: None
- **Summary**: We present RoarNet, a new approach for 3D object detection from a 2D image and 3D Lidar point clouds. Based on two-stage object detection framework with PointNet as our backbone network, we suggest several novel ideas to improve 3D object detection performance. The first part of our method, RoarNet_2D, estimates the 3D poses of objects from a monocular image, which approximates where to examine further, and derives multiple candidates that are geometrically feasible. This step significantly narrows down feasible 3D regions, which otherwise requires demanding processing of 3D point clouds in a huge search space. Then the second part, RoarNet_3D, takes the candidate regions and conducts in-depth inferences to conclude final poses in a recursive manner. Inspired by PointNet, RoarNet_3D processes 3D point clouds directly without any loss of data, leading to precise detection. We evaluate our method in KITTI, a 3D object detection benchmark. Our result shows that RoarNet has superior performance to state-of-the-art methods that are publicly available. Remarkably, RoarNet also outperforms state-of-the-art methods even in settings where Lidar and camera are not time synchronized, which is practically important for actual driving environments. RoarNet is implemented in Tensorflow and publicly available with pre-trained models.



### Changing the Image Memorability: From Basic Photo Editing to GANs
- **Arxiv ID**: http://arxiv.org/abs/1811.03825v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.03825v4)
- **Published**: 2018-11-09 09:18:42+00:00
- **Updated**: 2019-04-20 10:20:38+00:00
- **Authors**: Oleksii Sidorov
- **Comment**: Accepted to CVPR 2019 Workshop (MBCCV)
- **Journal**: None
- **Summary**: Memorability is considered to be an important characteristic of visual content, whereas for advertisement and educational purposes it is often crucial. Despite numerous studies on understanding and predicting image memorability, there are almost no achievements in memorability modification. In this work, we study two approaches to image editing - GAN and classical image processing - and show their impact on memorability. The visual features which influence memorability directly stay unknown till now, hence it is impossible to control it manually. As a solution, we let GAN learn it deeply using labeled data, and then use it for conditional generation of new images. By analogy with algorithms which edit facial attributes, we consider memorability as yet another attribute and operate with it in the same way. Obtained data is also interesting for analysis, simply because there are no real-world examples of successful change of image memorability while preserving its other attributes. We believe this may give many new answers to the question "what makes an image memorable?" Apart from that we also study the influence of conventional photo-editing tools (Photoshop, Instagram, etc.) used daily by a wide audience on memorability. In this case, we start from real practical methods and study it using statistics and recent advances in memorability prediction. Photographers, designers, and advertisers will benefit from the results of this study directly.



### Image-Level Attentional Context Modeling Using Nested-Graph Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1811.03830v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1811.03830v2)
- **Published**: 2018-11-09 09:33:31+00:00
- **Updated**: 2018-11-12 07:46:33+00:00
- **Authors**: Guillaume Jaume, Behzad Bozorgtabar, Hazim Kemal Ekenel, Jean-Philippe Thiran, Maria Gabrani
- **Comment**: NIPS 2018, Relational Representation Learning Workshop
- **Journal**: None
- **Summary**: We introduce a new scene graph generation method called image-level attentional context modeling (ILAC). Our model includes an attentional graph network that effectively propagates contextual information across the graph using image-level features. Whereas previous works use an object-centric context, we build an image-level context agent to encode the scene properties. The proposed method comprises a single-stream network that iteratively refines the scene graph with a nested graph neural network. We demonstrate that our approach achieves competitive performance with the state-of-the-art for scene graph generation on the Visual Genome dataset, while requiring fewer parameters than other methods. We also show that ILAC can improve regular object detectors by incorporating relational image-level information.



### An Average of the Human Ear Canal: Recovering Acoustical Properties via Shape Analysis
- **Arxiv ID**: http://arxiv.org/abs/1811.03848v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.03848v1)
- **Published**: 2018-11-09 10:19:25+00:00
- **Updated**: 2018-11-09 10:19:25+00:00
- **Authors**: Sune Darkner, Stefan Sommer, Andreas Schuhmacher, Henrik Ingerslev Anders O. Baandrup, Carsten Thomsen, S√∏ren J√∏nsson
- **Comment**: None
- **Journal**: None
- **Summary**: Humans are highly dependent on the ability to process audio in order to interact through conversation and navigate from sound. For this, the shape of the ear acts as a mechanical audio filter. The anatomy of the outer human ear canal to approximately 15-20 mm beyond the Tragus is well described because of its importance for customized hearing aid production. This is however not the case for the part of the ear canal that is embedded in the skull, until the typanic membrane. Due to the sensitivity of the outer ear, this part, referred to as the bony part, has only been described in a few population studies and only ex-vivo. We present a study of the entire ear canal including the bony part and the tympanic membrane. We form an average ear canal from a number of MRI scans using standard image registration methods. We show that the obtained representation is realistic in the sense that it has acoustical properties almost identical to a real ear.



### Multimodal One-Shot Learning of Speech and Images
- **Arxiv ID**: http://arxiv.org/abs/1811.03875v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1811.03875v2)
- **Published**: 2018-11-09 12:14:20+00:00
- **Updated**: 2019-04-15 15:08:03+00:00
- **Authors**: Ryan Eloff, Herman A. Engelbrecht, Herman Kamper
- **Comment**: 5 pages, 1 figure, 3 tables; accepted to ICASSP 2019
- **Journal**: None
- **Summary**: Imagine a robot is shown new concepts visually together with spoken tags, e.g. "milk", "eggs", "butter". After seeing one paired audio-visual example per class, it is shown a new set of unseen instances of these objects, and asked to pick the "milk". Without receiving any hard labels, could it learn to match the new continuous speech input to the correct visual instance? Although unimodal one-shot learning has been studied, where one labelled example in a single modality is given per class, this example motivates multimodal one-shot learning. Our main contribution is to formally define this task, and to propose several baseline and advanced models. We use a dataset of paired spoken and visual digits to specifically investigate recent advances in Siamese convolutional neural networks. Our best Siamese model achieves twice the accuracy of a nearest neighbour model using pixel-distance over images and dynamic time warping over speech in 11-way cross-modal matching.



### Cross and Learn: Cross-Modal Self-Supervision
- **Arxiv ID**: http://arxiv.org/abs/1811.03879v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.03879v3)
- **Published**: 2018-11-09 12:29:39+00:00
- **Updated**: 2019-04-29 17:00:34+00:00
- **Authors**: Nawid Sayed, Biagio Brattoli, Bj√∂rn Ommer
- **Comment**: GCPR 2018
- **Journal**: None
- **Summary**: In this paper we present a self-supervised method for representation learning utilizing two different modalities. Based on the observation that cross-modal information has a high semantic meaning we propose a method to effectively exploit this signal. For our approach we utilize video data since it is available on a large scale and provides easily accessible modalities given by RGB and optical flow. We demonstrate state-of-the-art performance on highly contested action recognition datasets in the context of self-supervised learning. We show that our feature representation also transfers to other tasks and conduct extensive ablation studies to validate our core contributions. Code and model can be found at https://github.com/nawidsayed/Cross-and-Learn.



### Feature Selection Convolutional Neural Networks for Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/1811.08564v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08564v1)
- **Published**: 2018-11-09 13:12:21+00:00
- **Updated**: 2018-11-09 13:12:21+00:00
- **Authors**: Zhiyan Cui, Na Lu
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1807.03132
- **Journal**: None
- **Summary**: Most of the existing tracking methods based on CNN(convolutional neural networks) are too slow for real-time application despite the excellent tracking precision compared with the traditional ones. Moreover, neural networks are memory intensive which will take up lots of hardware resources. In this paper, a feature selection visual tracking algorithm combining CNN based MDNet(Multi-Domain Network) and RoIAlign was developed. We find that there is a lot of redundancy in feature maps from convolutional layers. So valid feature maps are selected by mutual information and others are abandoned which can reduce the complexity and computation of the network and do not affect the precision. The major problem of MDNet also lies in the time efficiency. Considering the computational complexity of MDNet is mainly caused by the large amount of convolution operations and fine-tuning of the network during tracking, a RoIAlign layer which could conduct the convolution over the whole image instead of each RoI is added to accelerate the convolution and a new strategy of fine-tuning the fully-connected layers is used to accelerate the update. With RoIAlign employed, the computation speed has been increased and it shows greater precision than RoIPool. Because RoIAlign can process float number coordinates by bilinear interpolation. These strategies can accelerate the processing, reduce the complexity with very low impact on precision and it can run at around 10 fps(while the speed of MDNet is about 1 fps). The proposed algorithm has been evaluated on a benchmark: OTB100, on which high precision and speed have been obtained.



### Toward Autonomous Rotation-Aware Unmanned Aerial Grasping
- **Arxiv ID**: http://arxiv.org/abs/1811.03921v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1811.03921v1)
- **Published**: 2018-11-09 14:28:31+00:00
- **Updated**: 2018-11-09 14:28:31+00:00
- **Authors**: Shijie Lin, Jinwang Wang, Wen Yang, Guisong Xia
- **Comment**: 8 pages, 11 figures
- **Journal**: None
- **Summary**: Autonomous Unmanned Aerial Manipulators (UAMs) have shown promising potentials to transform passive sensing missions into active 3-dimension interactive missions, but they still suffer from some difficulties impeding their wide applications, such as target detection and stabilization. This letter presents a vision-based autonomous UAM with a 3DoF robotic arm for rotational grasping, with a compensation on displacement for center of gravity. First, the hardware, software architecture and state estimation methods are detailed. All the mechanical designs are fully provided as open-source hardware for the reuse by the community. Then, we analyze the flow distribution generated by rotors and plan the robotic arm's motion based on this analysis. Next, a novel detection approach called Rotation-SqueezeDet is proposed to enable rotation-aware grasping, which can give the target position and rotation angle in near real-time on Jetson TX2. Finally, the effectiveness of the proposed scheme is validated in multiple experimental trials, highlighting it's applicability of autonomous aerial grasping in GPS-denied environments.



### Matrix Recovery with Implicitly Low-Rank Data
- **Arxiv ID**: http://arxiv.org/abs/1811.03945v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.03945v1)
- **Published**: 2018-11-09 15:04:24+00:00
- **Updated**: 2018-11-09 15:04:24+00:00
- **Authors**: Xingyu Xie, Jianlong Wu, Guangcan Liu, Jun Wang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we study the problem of matrix recovery, which aims to restore a target matrix of authentic samples from grossly corrupted observations. Most of the existing methods, such as the well-known Robust Principal Component Analysis (RPCA), assume that the target matrix we wish to recover is low-rank. However, the underlying data structure is often non-linear in practice, therefore the low-rankness assumption could be violated. To tackle this issue, we propose a novel method for matrix recovery in this paper, which could well handle the case where the target matrix is low-rank in an implicit feature space but high-rank or even full-rank in its original form. Namely, our method pursues the low-rank structure of the target matrix in an implicit feature space. By making use of the specifics of an accelerated proximal gradient based optimization algorithm, the proposed method could recover the target matrix with non-linear structures from its corrupted version. Comprehensive experiments on both synthetic and real datasets demonstrate the superiority of our method.



### Identify, locate and separate: Audio-visual object extraction in large video collections using weak supervision
- **Arxiv ID**: http://arxiv.org/abs/1811.04000v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1811.04000v1)
- **Published**: 2018-11-09 16:19:41+00:00
- **Updated**: 2018-11-09 16:19:41+00:00
- **Authors**: Sanjeel Parekh, Alexey Ozerov, Slim Essid, Ngoc Duong, Patrick P√©rez, Ga√´l Richard
- **Comment**: None
- **Journal**: None
- **Summary**: We tackle the problem of audiovisual scene analysis for weakly-labeled data. To this end, we build upon our previous audiovisual representation learning framework to perform object classification in noisy acoustic environments and integrate audio source enhancement capability. This is made possible by a novel use of non-negative matrix factorization for the audio modality. Our approach is founded on the multiple instance learning paradigm. Its effectiveness is established through experiments over a challenging dataset of music instrument performance videos. We also show encouraging visual object localization results.



### Splenomegaly Segmentation on Multi-modal MRI using Deep Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1811.04045v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.04045v1)
- **Published**: 2018-11-09 17:59:57+00:00
- **Updated**: 2018-11-09 17:59:57+00:00
- **Authors**: Yuankai Huo, Zhoubing Xu, Shunxing Bao, Camilo Bermudez, Hyeonsoo Moon, Prasanna Parvathaneni, Tamara K. Moyo, Michael R. Savona, Albert Assad, Richard G. Abramson, Bennett A. Landman
- **Comment**: Accepted by IEEE Transactions on Medical Imaging
- **Journal**: None
- **Summary**: The findings of splenomegaly, abnormal enlargement of the spleen, is a non-invasive clinical biomarker for liver and spleen disease. Automated segmentation methods are essential to efficiently quantify splenomegaly from clinically acquired abdominal magnetic resonance imaging (MRI) scans. However, the task is challenging due to (1) large anatomical and spatial variations of splenomegaly, (2) large inter- and intra-scan intensity variations on multi-modal MRI, and (3) limited numbers of labeled splenomegaly scans. In this paper, we propose the Splenomegaly Segmentation Network (SS-Net) to introduce the deep convolutional neural network (DCNN) approaches in multi-modal MRI splenomegaly segmentation. Large convolutional kernel layers were used to address the spatial and anatomical variations, while the conditional generative adversarial networks (GAN) were employed to leverage the segmentation performance of SS-Net in an end-to-end manner. A clinically acquired cohort containing both T1-weighted (T1w) and T2-weighted (T2w) MRI splenomegaly scans was used to train and evaluate the performance of multi-atlas segmentation (MAS), 2D DCNN networks, and a 3D DCNN network. From the experimental results, the DCNN methods achieved superior performance to the state-of-the-art MAS method. The proposed SS-Net method achieved the highest median and mean Dice scores among investigated baseline DCNN methods.



### A Microprocessor implemented in 65nm CMOS with Configurable and Bit-scalable Accelerator for Programmable In-memory Computing
- **Arxiv ID**: http://arxiv.org/abs/1811.04047v1
- **DOI**: 10.1109/JSSC.2020.2987714
- **Categories**: **cs.AR**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.04047v1)
- **Published**: 2018-11-09 18:03:14+00:00
- **Updated**: 2018-11-09 18:03:14+00:00
- **Authors**: Hongyang Jia, Yinqi Tang, Hossein Valavi, Jintao Zhang, Naveen Verma
- **Comment**: None
- **Journal**: IEEE Journal of Solid-State Circuits, vol. 55, no. 9, pp.
  2609-2621, Sept. 2020
- **Summary**: This paper presents a programmable in-memory-computing processor, demonstrated in a 65nm CMOS technology. For data-centric workloads, such as deep neural networks, data movement often dominates when implemented with today's computing architectures. This has motivated spatial architectures, where the arrangement of data-storage and compute hardware is distributed and explicitly aligned to the computation dataflow, most notably for matrix-vector multiplication. In-memory computing is a spatial architecture where processing elements correspond to dense bit cells, providing local storage and compute, typically employing analog operation. Though this raises the potential for high energy efficiency and throughput, analog operation has significantly limited robustness, scale, and programmability. This paper describes a 590kb in-memory-computing accelerator integrated in a programmable processor architecture, by exploiting recent approaches to charge-domain in-memory computing. The architecture takes the approach of tight coupling with an embedded CPU, through accelerator interfaces enabling integration in the standard processor memory space. Additionally, a near-memory-computing datapath both enables diverse computations locally, to address operations required across applications, and enables bit-precision scalability for matrix/input-vector elements, through a bit-parallel/bit-serial (BP/BS) scheme. Chip measurements show an energy efficiency of 152/297 1b-TOPS/W and throughput of 4.7/1.9 1b-TOPS (scaling linearly with the matrix/input-vector element precisions) at VDD of 1.2/0.85V. Neural network demonstrations with 1-b/4-b weights and activations for CIFAR-10 classification consume 5.3/105.2 $\mu$J/image at 176/23 fps, with accuracy at the level of digital/software implementation (89.3/92.4 $\%$ accuracy).



### Multiple People Tracking Using Hierarchical Deep Tracklet Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1811.04091v2
- **DOI**: None
- **Categories**: **cs.CV**, 68T45, I.2.10; I.4.8; I.2.6; I.4.9; I.5.3
- **Links**: [PDF](http://arxiv.org/pdf/1811.04091v2)
- **Published**: 2018-11-09 19:03:10+00:00
- **Updated**: 2018-11-17 17:27:17+00:00
- **Authors**: Maryam Babaee, Ali Athar, Gerhard Rigoll
- **Comment**: 13 pages (8 main + 2 bibliography + 5 appendices)
- **Journal**: None
- **Summary**: The task of multiple people tracking in monocular videos is challenging because of the numerous difficulties involved: occlusions, varying environments, crowded scenes, camera parameters and motion. In the tracking-by-detection paradigm, most approaches adopt person re-identification techniques based on computing the pairwise similarity between detections. However, these techniques are less effective in handling long-term occlusions. By contrast, tracklet (a sequence of detections) re-identification can improve association accuracy since tracklets offer a richer set of visual appearance and spatio-temporal cues. In this paper, we propose a tracking framework that employs a hierarchical clustering mechanism for merging tracklets. To this end, tracklet re-identification is performed by utilizing a novel multi-stage deep network that can jointly reason about the visual appearance and spatio-temporal properties of a pair of tracklets, thereby providing a robust measure of affinity. Experimental results on the challenging MOT16 and MOT17 benchmarks show that our method significantly outperforms state-of-the-arts.



### Reducing Network Agnostophobia
- **Arxiv ID**: http://arxiv.org/abs/1811.04110v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.04110v2)
- **Published**: 2018-11-09 19:29:58+00:00
- **Updated**: 2018-12-23 02:58:58+00:00
- **Authors**: Akshay Raj Dhamija, Manuel G√ºnther, Terrance E. Boult
- **Comment**: Neural Information Processing Systems (NeurIPS) 2018
- **Journal**: None
- **Summary**: Agnostophobia, the fear of the unknown, can be experienced by deep learning engineers while applying their networks to real-world applications. Unfortunately, network behavior is not well defined for inputs far from a networks training set. In an uncontrolled environment, networks face many instances that are not of interest to them and have to be rejected in order to avoid a false positive. This problem has previously been tackled by researchers by either a) thresholding softmax, which by construction cannot return "none of the known classes", or b) using an additional background or garbage class. In this paper, we show that both of these approaches help, but are generally insufficient when previously unseen classes are encountered. We also introduce a new evaluation metric that focuses on comparing the performance of multiple approaches in scenarios where such unseen classes or unknowns are encountered. Our major contributions are simple yet effective Entropic Open-Set and Objectosphere losses that train networks using negative samples from some classes. These novel losses are designed to maximize entropy for unknown inputs while increasing separation in deep feature space by modifying magnitudes of known and unknown samples. Experiments on networks trained to classify classes from MNIST and CIFAR-10 show that our novel loss functions are significantly better at dealing with unknown inputs from datasets such as Devanagari, NotMNIST, CIFAR-100, and SVHN.



### STA: Spatial-Temporal Attention for Large-Scale Video-based Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1811.04129v1
- **DOI**: 10.1609/aaai.v33i01.33018287
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.04129v1)
- **Published**: 2018-11-09 20:43:31+00:00
- **Updated**: 2018-11-09 20:43:31+00:00
- **Authors**: Yang Fu, Xiaoyang Wang, Yunchao Wei, Thomas Huang
- **Comment**: Accepted as a conference paper at AAAI 2019
- **Journal**: Proceedings of the AAAI Conference on Artificial Intelligence,
  vol. 33, pp. 8287-8294. 2019
- **Summary**: In this work, we propose a novel Spatial-Temporal Attention (STA) approach to tackle the large-scale person re-identification task in videos. Different from the most existing methods, which simply compute representations of video clips using frame-level aggregation (e.g. average pooling), the proposed STA adopts a more effective way for producing robust clip-level feature representation. Concretely, our STA fully exploits those discriminative parts of one target person in both spatial and temporal dimensions, which results in a 2-D attention score matrix via inter-frame regularization to measure the importances of spatial parts across different frames. Thus, a more robust clip-level feature representation can be generated according to a weighted sum operation guided by the mined 2-D attention score matrix. In this way, the challenging cases for video-based person re-identification such as pose variation and partial occlusion can be well tackled by the STA. We conduct extensive experiments on two large-scale benchmarks, i.e. MARS and DukeMTMC-VideoReID. In particular, the mAP reaches 87.7% on MARS, which significantly outperforms the state-of-the-arts with a large margin of more than 11.6%.



### A torus model for optical flow
- **Arxiv ID**: http://arxiv.org/abs/1812.00875v2
- **DOI**: 10.1016/j.patrec.2019.11.029
- **Categories**: **cs.CV**, math.AT
- **Links**: [PDF](http://arxiv.org/pdf/1812.00875v2)
- **Published**: 2018-11-09 22:50:29+00:00
- **Updated**: 2019-11-24 23:58:01+00:00
- **Authors**: Henry Adams, Johnathan Bush, Brittany Carr, Lara Kassab, Joshua Mirth
- **Comment**: None
- **Journal**: Pattern Recognition Letters 129 (2020), 304-310
- **Summary**: We propose a torus model for high-contrast patches of optical flow. Our model is derived from a database of ground-truth optical flow from the computer-generated video \emph{Sintel}, collected by Butler et al.\ in \emph{A naturalistic open source movie for optical flow evaluation}. Using persistent homology and zigzag persistence, popular tools from the field of computational topology, we show that the high-contrast $3\times 3$ patches from this video are well-modeled by a \emph{torus}, a nonlinear 2-dimensional manifold. Furthermore, we show that the optical flow torus model is naturally equipped with the structure of a fiber bundle, related to the statistics of range image patches.



