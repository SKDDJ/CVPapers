# Arxiv Papers in cs.CV on 2018-11-11
### Photorealistic Facial Synthesis in the Dimensional Affect Space
- **Arxiv ID**: http://arxiv.org/abs/1811.08004v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.08004v1)
- **Published**: 2018-11-11 01:30:21+00:00
- **Updated**: 2018-11-11 01:30:21+00:00
- **Authors**: Dimitrios Kollias, Shiyang Cheng, Maja Pantic, Stefanos Zafeiriou
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1811.05027
- **Journal**: None
- **Summary**: This paper presents a novel approach for synthesizing facial affect, which is based on our annotating 600,000 frames of the 4DFAB database in terms of valence and arousal. The input of this approach is a pair of these emotional state descriptors and a neutral 2D image of a person to whom the corresponding affect will be synthesized. Given this target pair, a set of 3D facial meshes is selected, which is used to build a blendshape model and generate the new facial affect. To synthesize the affect on the 2D neutral image, 3DMM fitting is performed and the reconstructed face is deformed to generate the target facial expressions. Last, the new face is rendered into the original image. Both qualitative and quantitative experimental studies illustrate the generation of realistic images, when the neutral image is sampled from a variety of well known databases, such as the Aff-Wild, AFEW, Multi-PIE, AFEW-VA, BU-3DFE, Bosphorus.



### Aff-Wild2: Extending the Aff-Wild Database for Affect Recognition
- **Arxiv ID**: http://arxiv.org/abs/1811.07770v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.07770v2)
- **Published**: 2018-11-11 01:57:15+00:00
- **Updated**: 2019-12-13 23:44:20+00:00
- **Authors**: Dimitrios Kollias, Stefanos Zafeiriou
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic understanding of human affect using visual signals is a problem that has attracted significant interest over the past 20 years. However, human emotional states are quite complex. To appraise such states displayed in real-world settings, we need expressive emotional descriptors that are capable of capturing and describing this complexity. The circumplex model of affect, which is described in terms of valence (i.e., how positive or negative is an emotion) and arousal (i.e., power of the activation of the emotion), can be used for this purpose. Recent progress in the emotion recognition domain has been achieved through the development of deep neural architectures and the availability of very large training databases. To this end, Aff-Wild has been the first large-scale "in-the-wild" database, containing around 1,200,000 frames. In this paper, we build upon this database, extending it with 260 more subjects and 1,413,000 new video frames. We call the union of Aff-Wild with the additional data, Aff-Wild2. The videos are downloaded from Youtube and have large variations in pose, age, illumination conditions, ethnicity and profession. Both database-specific as well as cross-database experiments are performed in this paper, by utilizing the Aff-Wild2, along with the RECOLA database. The developed deep neural architectures are based on the joint training of state-of-the-art convolutional and recurrent neural networks with attention mechanism; thus exploiting both the invariant properties of convolutional features, while modeling temporal dynamics that arise in human behaviour via the recurrent layers. The obtained results show premise for utilization of the extended Aff-Wild, as well as of the developed deep neural architectures for visual analysis of human behaviour in terms of continuous emotion dimensions.



### Deep Face Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/1811.04346v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.04346v1)
- **Published**: 2018-11-11 04:17:10+00:00
- **Updated**: 2018-11-11 04:17:10+00:00
- **Authors**: Vishal Agarwal
- **Comment**: Course project report
- **Journal**: None
- **Summary**: Face image quality is an important factor in facial recognition systems as its verification and recognition accuracy is highly dependent on the quality of image presented. Rejecting low quality images can significantly increase the accuracy of any facial recognition system. In this project, a simple approach is presented to train a deep convolutional neural network to perform end-to-end face image quality assessment. The work is done in 2 stages : First, generation of quality score label and secondly, training a deep convolutional neural network in a supervised manner to predict quality score between 0 and 1. The generation of quality labels is done by comparing the face image with a template of best quality images and then evaluating the normalized score based on the similarity.



### Neural Generative Models for 3D Faces with Application in 3D Texture Free Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1811.04358v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.04358v1)
- **Published**: 2018-11-11 05:56:09+00:00
- **Updated**: 2018-11-11 05:56:09+00:00
- **Authors**: Ahmed ElSayed, Elif Kongar, Ausif Mahmood, Tarek Sobh, Terrance Boult
- **Comment**: None
- **Journal**: None
- **Summary**: Using heterogeneous depth cameras and 3D scanners in 3D face verification causes variations in the resolution of the 3D point clouds. To solve this issue, previous studies use 3D registration techniques. Out of these proposed techniques, detecting points of correspondence is proven to be an efficient method given that the data belongs to the same individual. However, if the data belongs to different persons, the registration algorithms can convert the 3D point cloud of one person to another, destroying the distinguishing features between the two point clouds. Another issue regarding the storage size of the point clouds. That is, if the captured depth image contains around 50 thousand points in the cloud for a single pose for one individual, then the storage size of the entire dataset will be in order of giga if not tera bytes. With these motivations, this work introduces a new technique for 3D point clouds generation using a neural modeling system to handle the differences caused by heterogeneous depth cameras, and to generate a new face canonical compact representation. The proposed system reduces the stored 3D dataset size, and if required, provides an accurate dataset regeneration. Furthermore, the system generates neural models for all gallery point clouds and stores these models to represent the faces in the recognition or verification processes. For the probe cloud to be verified, a new model is generated specifically for that particular cloud and is matched against pre-stored gallery model presentations to identify the query cloud. This work also introduces the utilization of Siamese deep neural network in 3D face verification using generated model representations as raw data for the deep network, and shows that the accuracy of the trained network is comparable all published results on Bosphorus dataset.



### Improved Visual Relocalization by Discovering Anchor Points
- **Arxiv ID**: http://arxiv.org/abs/1811.04370v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.04370v1)
- **Published**: 2018-11-11 08:39:57+00:00
- **Updated**: 2018-11-11 08:39:57+00:00
- **Authors**: Soham Saha, Girish Varma, C. V. Jawahar
- **Comment**: 10 Pages, 6 figures, BMVC 2018, Newcastle, UK
- **Journal**: None
- **Summary**: We address the visual relocalization problem of predicting the location and camera orientation or pose (6DOF) of the given input scene. We propose a method based on how humans determine their location using the visible landmarks. We define anchor points uniformly across the route map and propose a deep learning architecture which predicts the most relevant anchor point present in the scene as well as the relative offsets with respect to it. The relevant anchor point need not be the nearest anchor point to the ground truth location, as it might not be visible due to the pose. Hence we propose a multi task loss function, which discovers the relevant anchor point, without needing the ground truth for it. We validate the effectiveness of our approach by experimenting on CambridgeLandmarks (large scale outdoor scenes) as well as 7 Scenes (indoor scenes) using variousCNN feature extractors. Our method improves the median error in indoor as well as outdoor localization datasets compared to the previous best deep learning model known as PoseNet (with geometric re-projection loss) using the same feature extractor. We improve the median error in localization in the specific case of Street scene, by over 8m.



### Fashion and Apparel Classification using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1811.04374v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.04374v1)
- **Published**: 2018-11-11 09:18:32+00:00
- **Updated**: 2018-11-11 09:18:32+00:00
- **Authors**: Alexander Schindler, Thomas Lidy, Stephan Karner, Matthias Hecker
- **Comment**: Proceedings of the 10th Forum Media Technology and 3rd All Around
  Audio Symposium, St. Poelten, Austria, November 29-30, 2017
- **Journal**: None
- **Summary**: We present an empirical study of applying deep Convolutional Neural Networks (CNN) to the task of fashion and apparel image classification to improve meta-data enrichment of e-commerce applications. Five different CNN architectures were analyzed using clean and pre-trained models. The models were evaluated in three different tasks person detection, product and gender classification, on two small and large scale datasets.



### Integrating Multiple Receptive Fields through Grouped Active Convolution
- **Arxiv ID**: http://arxiv.org/abs/1811.04387v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.04387v2)
- **Published**: 2018-11-11 10:32:43+00:00
- **Updated**: 2019-02-11 03:59:18+00:00
- **Authors**: Yunho Jeon, Junmo Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional networks have achieved great success in various vision tasks. This is mainly due to a considerable amount of research on network structure. In this study, instead of focusing on architectures, we focused on the convolution unit itself. The existing convolution unit has a fixed shape and is limited to observing restricted receptive fields. In earlier work, we proposed the active convolution unit (ACU), which can freely define its shape and learn by itself. In this paper, we provide a detailed analysis of the previously proposed unit and show that it is an efficient representation of a sparse weight convolution. Furthermore, we extend an ACU to a grouped ACU, which can observe multiple receptive fields in one layer. We found that the performance of a naive grouped convolution is degraded by increasing the number of groups; however, the proposed unit retains the accuracy even though the number of parameters decreases. Based on this result, we suggest a depthwise ACU, and various experiments have shown that our unit is efficient and can replace the existing convolutions.



### HSD-CNN: Hierarchically self decomposing CNN architecture using class specific filter sensitivity analysis
- **Arxiv ID**: http://arxiv.org/abs/1811.04406v2
- **DOI**: 10.1145/3293353.3293383
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.04406v2)
- **Published**: 2018-11-11 12:20:18+00:00
- **Updated**: 2018-11-21 21:34:36+00:00
- **Authors**: K. Sai Ram, Jayanta Mukherjee, Amit Patra, Partha Pratim Das
- **Comment**: Accepted in ICVGIP,2018
- **Journal**: None
- **Summary**: Conventional Convolutional neural networks (CNN) are trained on large domain datasets and are hence typically over-represented and inefficient in limited class applications. An efficient way to convert such large many-class pre-trained networks into small few-class networks is through a hierarchical decomposition of its feature maps. To alleviate this issue, we propose an automated framework for such decomposition in Hierarchically Self Decomposing CNN (HSD-CNN), in four steps. HSD-CNN is derived automatically using a class-specific filter sensitivity analysis that quantifies the impact of specific features on a class prediction. The decomposed hierarchical network can be utilized and deployed directly to obtain sub-networks for a subset of classes, and it is shown to perform better without the requirement of retraining these sub-networks. Experimental results show that HSD-CNN generally does not degrade accuracy if the full set of classes are used. Interestingly, when operating on known subsets of classes, HSD-CNN has an improvement in accuracy with a much smaller model size, requiring much fewer operations. HSD-CNN flow is verified on the CIFAR10, CIFAR100 and CALTECH101 data sets. We report accuracies up to $85.6\%$ ( $94.75\%$ ) on scenarios with 13 ( 4 ) classes of CIFAR100, using a pre-trained VGG-16 network on the full data set. In this case, the proposed HSD-CNN requires $3.97 \times$ fewer parameters and has $71.22\%$ savings in operations, in comparison to baseline VGG-16 containing features for all 100 classes.



### An initial attempt of combining visual selective attention with deep reinforcement learning
- **Arxiv ID**: http://arxiv.org/abs/1811.04407v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.04407v3)
- **Published**: 2018-11-11 12:22:44+00:00
- **Updated**: 2020-06-18 17:48:44+00:00
- **Authors**: Liu Yuezhang, Ruohan Zhang, Dana H. Ballard
- **Comment**: 7 pages, 8 figures
- **Journal**: None
- **Summary**: Visual attention serves as a means of feature selection mechanism in the perceptual system. Motivated by Broadbent's leaky filter model of selective attention, we evaluate how such mechanism could be implemented and affect the learning process of deep reinforcement learning. We visualize and analyze the feature maps of DQN on a toy problem Catch, and propose an approach to combine visual selective attention with deep reinforcement learning. We experiment with optical flow-based attention and A2C on Atari games. Experiment results show that visual selective attention could lead to improvements in terms of sample efficiency on tested games. An intriguing relation between attention and batch normalization is also discovered.



### A Multi-Task Learning & Generation Framework: Valence-Arousal, Action Units & Primary Expressions
- **Arxiv ID**: http://arxiv.org/abs/1811.07771v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.07771v2)
- **Published**: 2018-11-11 15:40:23+00:00
- **Updated**: 2019-12-13 23:39:14+00:00
- **Authors**: Dimitrios Kollias, Stefanos Zafeiriou
- **Comment**: None
- **Journal**: None
- **Summary**: Over the past few years many research efforts have been devoted to the field of affect analysis. Various approaches have been proposed for: i) discrete emotion recognition in terms of the primary facial expressions; ii) emotion analysis in terms of facial Action Units (AUs), assuming a fixed expression intensity; iii) dimensional emotion analysis, in terms of valence and arousal (VA). These approaches can only be effective, if they are developed using large, appropriately annotated databases, showing behaviors of people in-the-wild, i.e., in uncontrolled environments. Aff-Wild has been the first, large-scale, in-the-wild database (including around 1,200,000 frames of 300 videos), annotated in terms of VA. In the vast majority of existing emotion databases, their annotation is limited to either primary expressions, or valence-arousal, or action units. In this paper, we first annotate a part (around $234,000$ frames) of the Aff-Wild database in terms of $8$ AUs and another part (around $288,000$ frames) in terms of the $7$ basic emotion categories, so that parts of this database are annotated in terms of VA, as well as AUs, or primary expressions. Then, we set up and tackle multi-task learning for emotion recognition, as well as for facial image generation. Multi-task learning is performed using: i) a deep neural network with shared hidden layers, which learns emotional attributes by exploiting their inter-dependencies; ii) a discriminator of a generative adversarial network (GAN). On the other hand, image generation is implemented through the generator of the GAN. For these two tasks, we carefully design loss functions that fit the examined set-up. Experiments are presented which illustrate the good performance of the proposed approach when applied to the new annotated parts of the Aff-Wild database.



### A Progressively-trained Scale-invariant and Boundary-aware Deep Neural Network for the Automatic 3D Segmentation of Lung Lesions
- **Arxiv ID**: http://arxiv.org/abs/1811.04437v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.04437v1)
- **Published**: 2018-11-11 17:32:38+00:00
- **Updated**: 2018-11-11 17:32:38+00:00
- **Authors**: Bo Zhou, Randolph Crawford, Belma Dogdas, Gregory Goldmacher, Antong Chen
- **Comment**: 10 pages, 7 figures, 3 tables, accepted by 2019 IEEE Winter
  Conference on Applications of Computer Vision (2019 WACV)
- **Journal**: None
- **Summary**: Volumetric segmentation of lesions on CT scans is important for many types of analysis, including lesion growth kinetic modeling in clinical trials and machine learning of radiomic features. Manual segmentation is laborious, and impractical for large-scale use. For routine clinical use, and in clinical trials that apply the Response Evaluation Criteria In Solid Tumors (RECIST), clinicians typically outline the boundaries of a lesion on a single slice to extract diameter measurements. In this work, we have collected a large-scale database, named LesionVis, with pixel-wise manual 2D lesion delineations on the RECIST-slices. To extend the 2D segmentations to 3D, we propose a volumetric progressive lesion segmentation (PLS) algorithm to automatically segment the 3D lesion volume from 2D delineations using a scale-invariant and boundary-aware deep convolutional network (SIBA-Net). The SIBA-Net copes with the size transition of a lesion when the PLS progresses from the RECIST-slice to the edge-slices, as well as when performing longitudinal assessment of lesions whose size change over multiple time points. The proposed PLS-SiBA-Net (P-SiBA) approach is assessed on the lung lesion cases from LesionVis. Our experimental results demonstrate that the P-SiBA approach achieves mean Dice similarity coefficients (DSC) of 0.81, which significantly improves 3D segmentation accuracy compared with the approaches proposed previously (highest mean DSC at 0.78 on LesionVis). In summary, by leveraging the limited 2D delineations on the RECIST-slices, P-SiBA is an effective semi-supervised approach to produce accurate lesion segmentations in 3D.



### Pedestrian Collision Avoidance System (PeCAS): a Deep Learning Framework
- **Arxiv ID**: http://arxiv.org/abs/1811.04453v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.04453v2)
- **Published**: 2018-11-11 19:00:12+00:00
- **Updated**: 2018-12-20 15:09:24+00:00
- **Authors**: Peetak Mitra
- **Comment**: arXiv admin note: text overlap with arXiv:1412.0069 by other authors
- **Journal**: None
- **Summary**: We propose a new deep learning based framework to identify pedestrians, and caution distracted drivers, in an effort to prevent the loss of life and property. This framework uses two Convolutional Neural Networks (CNN), one which detects pedestrians and the second which predicts the onset of drowsiness in a driver, is implemented on a Raspberry Pi 3 Model B+, shows great promise. The algorithm for implementing such a low-cost, low-compute model is presented and the results discussed.



### A 3-D Projection Model for X-ray Dark-field Imaging
- **Arxiv ID**: http://arxiv.org/abs/1811.04457v2
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/1811.04457v2)
- **Published**: 2018-11-11 19:09:48+00:00
- **Updated**: 2019-03-04 16:11:08+00:00
- **Authors**: Shiyang Hu, Lina Felsner, Andreas Maier, Veronika Ludwig, Gisela Anton, Christian Riess
- **Comment**: Shiyang Hu and Lina Felsner contributed equally to this work
- **Journal**: None
- **Summary**: Talbot-Lau X-ray phase-contrast imaging is a novel imaging modality, which provides not only an X-ray absorption image, but also additionally a differential phase image and a dark-field image. The dark-field image is related to small angle scattering and has an interesting property when canning oriented structures: the recorded signal depends on the relative orientation of the structure in the imaging system. Exactly this property allows to draw conclusions about the orientation and to reconstruct the structure. However, the reconstruction is a complex, non-trivial challenge. A lot of research was conducted towards this goal in the last years and several reconstruction algorithms were proposed. A key step of the reconstruction algorithm is the inversion of a forward projection model. Up until now, only 2-D projection models are available, with effectively limit the scanning trajectory to a 2-D plane. To obtain true 3-D information, this limitation requires to combine several 2-D scans, which leads to quite complex, impractical acquisitions schemes. Furthermore, it is not possible with these models to use 3-D trajectories that might allow simpler protocols, like for example a helical trajectory. To address these limitations, we propose in this work a very general 3-D projection model. Our projection model defines the dark-field signal dependent on an arbitrarily chosen ray and sensitivity direction. We derive the projection model under the assumption that the observed scatter distribution has a Gaussian shape. We theoretically show the consistency of our model with more constrained existing 2-D models. Furthermore, we experimentally show the compatibility of our model with dark-field measurements of two matchsticks. We believe that this 3-D projection model is an important step towards more flexible trajectories and imaging protocols that are much better applicable in practice.



### Machine Learning with Abstention for Automated Liver Disease Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/1811.04463v1
- **DOI**: 10.1109/FIT.2017.00070
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.04463v1)
- **Published**: 2018-11-11 19:37:40+00:00
- **Updated**: 2018-11-11 19:37:40+00:00
- **Authors**: Kanza Hamid, Amina Asif, Wajid Abbasi, Durre Sabih, Fayyaz Minhas
- **Comment**: Preprint version before submission for publication. complete version
  published in proc. 15th International Conference on Frontiers of Information
  Technology (FIT 2017), December 18-20, 2017, Islamabad, Pakistan.
  http://ieeexplore.ieee.org/document/8261064/
- **Journal**: 15th IEEE International Conference on Frontiers of Information
  Technology (FIT 2017), December 18-20, 2017, Islamabad, Pakistan
- **Summary**: This paper presents a novel approach for detection of liver abnormalities in an automated manner using ultrasound images. For this purpose, we have implemented a machine learning model that can not only generate labels (normal and abnormal) for a given ultrasound image but it can also detect when its prediction is likely to be incorrect. The proposed model abstains from generating the label of a test example if it is not confident about its prediction. Such behavior is commonly practiced by medical doctors who, when given insufficient information or a difficult case, can chose to carry out further clinical or diagnostic tests before generating a diagnosis. However, existing machine learning models are designed in a way to always generate a label for a given example even when the confidence of their prediction is low. We have proposed a novel stochastic gradient based solver for the learning with abstention paradigm and use it to make a practical, state of the art method for liver disease classification. The proposed method has been benchmarked on a data set of approximately 100 patients from MINAR, Multan, Pakistan and our results show that the proposed scheme offers state of the art classification performance.



### Multiple Subspace Alignment Improves Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1811.04491v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.04491v1)
- **Published**: 2018-11-11 22:02:16+00:00
- **Updated**: 2018-11-11 22:02:16+00:00
- **Authors**: Kowshik Thopalli, Rushil Anirudh, Jayaraman J. Thiagarajan, Pavan Turaga
- **Comment**: under review in ICASSP 2019
- **Journal**: None
- **Summary**: We present a novel unsupervised domain adaptation (DA) method for cross-domain visual recognition. Though subspace methods have found success in DA, their performance is often limited due to the assumption of approximating an entire dataset using a single low-dimensional subspace. Instead, we develop a method to effectively represent the source and target datasets via a collection of low-dimensional subspaces, and subsequently align them by exploiting the natural geometry of the space of subspaces, on the Grassmann manifold. We demonstrate the effectiveness of this approach, using empirical studies on two widely used benchmarks, with state of the art domain adaptation performance



### An Interpretable Generative Model for Handwritten Digit Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1811.04507v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.04507v1)
- **Published**: 2018-11-11 23:37:07+00:00
- **Updated**: 2018-11-11 23:37:07+00:00
- **Authors**: Yao Zhu, Saksham Suri, Pranav Kulkarni, Yueru Chen, Jiali Duan, C. -C. Jay Kuo
- **Comment**: None
- **Journal**: None
- **Summary**: An interpretable generative model for handwritten digits synthesis is proposed in this work. Modern image generative models, such as Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), are trained by backpropagation (BP). The training process is complex and the underlying mechanism is difficult to explain. We propose an interpretable multi-stage PCA method to achieve the same goal and use handwritten digit images synthesis as an illustrative example. First, we derive principal-component-analysis-based (PCA-based) transform kernels at each stage based on the covariance of its inputs. This results in a sequence of transforms that convert input images of correlated pixels to spectral vectors of uncorrelated components. In other words, it is a whitening process. Then, we can synthesize an image based on random vectors and multi-stage transform kernels through a coloring process. The generative model is a feedforward (FF) design since no BP is used in model parameter determination. Its design complexity is significantly lower, and the whole design process is explainable. Finally, we design an FF generative model using the MNIST dataset, compare synthesis results with those obtained by state-of-the-art GAN and VAE methods, and show that the proposed generative model achieves comparable performance.



