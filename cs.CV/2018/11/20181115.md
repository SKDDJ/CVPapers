# Arxiv Papers in cs.CV on 2018-11-15
### Depth Prediction Without the Sensors: Leveraging Structure for Unsupervised Learning from Monocular Videos
- **Arxiv ID**: http://arxiv.org/abs/1811.06152v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.06152v1)
- **Published**: 2018-11-15 02:59:38+00:00
- **Updated**: 2018-11-15 02:59:38+00:00
- **Authors**: Vincent Casser, Soeren Pirk, Reza Mahjourian, Anelia Angelova
- **Comment**: Thirty-Third AAAI Conference on Artificial Intelligence (AAAI'19)
- **Journal**: None
- **Summary**: Learning to predict scene depth from RGB inputs is a challenging task both for indoor and outdoor robot navigation. In this work we address unsupervised learning of scene depth and robot ego-motion where supervision is provided by monocular videos, as cameras are the cheapest, least restrictive and most ubiquitous sensor for robotics.   Previous work in unsupervised image-to-depth learning has established strong baselines in the domain. We propose a novel approach which produces higher quality results, is able to model moving objects and is shown to transfer across data domains, e.g. from outdoors to indoor scenes. The main idea is to introduce geometric structure in the learning process, by modeling the scene and the individual objects; camera ego-motion and object motions are learned from monocular videos as input. Furthermore an online refinement method is introduced to adapt learning on the fly to unknown domains.   The proposed approach outperforms all state-of-the-art approaches, including those that handle motion e.g. through learned flow. Our results are comparable in quality to the ones which used stereo as supervision and significantly improve depth prediction on scenes and datasets which contain a lot of object motion. The approach is of practical relevance, as it allows transfer across environments, by transferring models trained on data collected for robot navigation in urban scenes to indoor navigation settings. The code associated with this paper can be found at https://sites.google.com/view/struct2depth.



### Improving Skin Condition Classification with a Question Answering Model
- **Arxiv ID**: http://arxiv.org/abs/1811.06165v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.06165v1)
- **Published**: 2018-11-15 04:27:21+00:00
- **Updated**: 2018-11-15 04:27:21+00:00
- **Authors**: Mohamed Akrout, Amir-massoud Farahmand, Tory Jarmain
- **Comment**: None
- **Journal**: Medical Imaging meets NeurIPS Workshop (2018)
- **Summary**: We present a skin condition classification methodology based on a sequential pipeline of a pre-trained Convolutional Neural Network (CNN) and a Question Answering (QA) model. This method enables us to not only increase the classification confidence and accuracy of the deployed CNN system, but also enables the emulation of the conventional approach of doctors asking the relevant questions in refining the ultimate diagnosis and differential. By combining the CNN output in the form of classification probabilities as a prior to the QA model and the image textual description, we greedily ask the best symptom that maximizes the information gain over symptoms. We demonstrate that combining the QA model with the CNN increases the accuracy up to 10% as compared to the CNN alone, and more than 30% as compared to the QA model alone.



### GaitSet: Regarding Gait as a Set for Cross-View Gait Recognition
- **Arxiv ID**: http://arxiv.org/abs/1811.06186v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.06186v4)
- **Published**: 2018-11-15 05:23:14+00:00
- **Updated**: 2018-12-12 06:07:17+00:00
- **Authors**: Hanqing Chao, Yiwei He, Junping Zhang, Jianfeng Feng
- **Comment**: AAAI 2019, code is available at https://github.com/AbnerHqC/GaitSet
- **Journal**: None
- **Summary**: As a unique biometric feature that can be recognized at a distance, gait has broad applications in crime prevention, forensic identification and social security. To portray a gait, existing gait recognition methods utilize either a gait template, where temporal information is hard to preserve, or a gait sequence, which must keep unnecessary sequential constraints and thus loses the flexibility of gait recognition. In this paper we present a novel perspective, where a gait is regarded as a set consisting of independent frames. We propose a new network named GaitSet to learn identity information from the set. Based on the set perspective, our method is immune to permutation of frames, and can naturally integrate frames from different videos which have been filmed under different scenarios, such as diverse viewing angles, different clothes/carrying conditions. Experiments show that under normal walking conditions, our single-model method achieves an average rank-1 accuracy of 95.0% on the CASIA-B gait dataset and an 87.1% accuracy on the OU-MVLP gait dataset. These results represent new state-of-the-art recognition accuracy. On various complex scenarios, our model exhibits a significant level of robustness. It achieves accuracies of 87.2% and 70.4% on CASIA-B under bag-carrying and coat-wearing walking conditions, respectively. These outperform the existing best methods by a large margin. The method presented can also achieve a satisfactory accuracy with a small number of frames in a test sample, e.g., 82.5% on CASIA-B with only 7 frames. The source code has been released at https://github.com/AbnerHqC/GaitSet.



### From Videos to URLs: A Multi-Browser Guide To Extract User's Behavior with Optical Character Recognition
- **Arxiv ID**: http://arxiv.org/abs/1811.06193v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.MM, cs.RO, cs.SE
- **Links**: [PDF](http://arxiv.org/pdf/1811.06193v2)
- **Published**: 2018-11-15 05:59:05+00:00
- **Updated**: 2020-05-20 00:24:34+00:00
- **Authors**: Mojtaba Heidarysafa, James Reed, Kamran Kowsari, April Celeste R. Leviton, Janet I. Warren, Donald E. Brown
- **Comment**: None
- **Journal**: None
- **Summary**: Tracking users' activities on the World Wide Web (WWW) allows researchers to analyze each user's internet behavior as time passes and for the amount of time spent on a particular domain. This analysis can be used in research design, as researchers may access to their participant's behaviors while browsing the web. Web search behavior has been a subject of interest because of its real-world applications in marketing, digital advertisement, and identifying potential threats online. In this paper, we present an image-processing based method to extract domains which are visited by a participant over multiple browsers during a lab session. This method could provide another way to collect users' activities during an online session given that the session recorder collected the data. The method can also be used to collect the textual content of web-pages that an individual visits for later analysis



### Face Verification and Forgery Detection for Ophthalmic Surgery Images
- **Arxiv ID**: http://arxiv.org/abs/1811.06194v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.06194v1)
- **Published**: 2018-11-15 05:59:42+00:00
- **Updated**: 2018-11-15 05:59:42+00:00
- **Authors**: Kaushal Bhogale, Nishant Shankar, Adheesh Juvekar, Asutosh Padhi
- **Comment**: None
- **Journal**: None
- **Summary**: Although modern face verification systems are accessible and accurate, they are not always robust to pose variance and occlusions. Moreover, accurate models require a large amount of data to train. We structure our experiments to operate on small amounts of data obtained from an NGO that funds ophthalmic surgeries. We set up our face verification task as that of verifying pre-operation and post-operation images of a patient that undergoes ophthalmic surgery, and as such the post-operation images have occlusions like an eye patch. In this paper, we present a system that performs the face verification task using one-shot learning. To this end, our paper uses deep convolutional networks and compares different model architectures and loss functions. Our best model achieves 85% test accuracy. During inference time, we also attempt to detect image forgeries in addition to performing face verification. To achieve this, we use Error Level Analysis. Finally, we propose an inference pipeline that demonstrates how these techniques can be used to implement an automated face verification and forgery detection system.



### Image declipping with deep networks
- **Arxiv ID**: http://arxiv.org/abs/1811.06277v1
- **DOI**: None
- **Categories**: **cs.CV**, 68
- **Links**: [PDF](http://arxiv.org/pdf/1811.06277v1)
- **Published**: 2018-11-15 10:26:09+00:00
- **Updated**: 2018-11-15 10:26:09+00:00
- **Authors**: Shachar Honig, Michael Werman
- **Comment**: 5 pages
- **Journal**: 2018 25th IEEE International Conference on Image Processing (ICIP)
- **Summary**: We present a deep network to recover pixel values lost to clipping. The clipped area of the image is typically a uniform area of minimum or maximum brightness, losing image detail and color fidelity. The degree to which the clipping is visually noticeable depends on the amount by which values were clipped, and the extent of the clipped area. Clipping may occur in any (or all) of the pixel's color channels. Although clipped pixels are common and occur to some degree in almost every image we tested, current automatic solutions have only partial success in repairing clipped pixels and work only in limited cases such as only with overexposure (not under-exposure) and when some of the color channels are not clipped. Using neural networks and their ability to model natural images allows our neural network, DeclipNet, to reconstruct data in clipped regions producing state of the art results.



### Guiding the One-to-one Mapping in CycleGAN via Optimal Transport
- **Arxiv ID**: http://arxiv.org/abs/1811.06284v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.06284v1)
- **Published**: 2018-11-15 10:34:33+00:00
- **Updated**: 2018-11-15 10:34:33+00:00
- **Authors**: Guansong Lu, Zhiming Zhou, Yuxuan Song, Kan Ren, Yong Yu
- **Comment**: The Thirty-Third AAAI Conference on Artificial Intelligence (AAAI
  2019)
- **Journal**: None
- **Summary**: CycleGAN is capable of learning a one-to-one mapping between two data distributions without paired examples, achieving the task of unsupervised data translation. However, there is no theoretical guarantee on the property of the learned one-to-one mapping in CycleGAN. In this paper, we experimentally find that, under some circumstances, the one-to-one mapping learned by CycleGAN is just a random one within the large feasible solution space. Based on this observation, we explore to add extra constraints such that the one-to-one mapping is controllable and satisfies more properties related to specific tasks. We propose to solve an optimal transport mapping restrained by a task-specific cost function that reflects the desired properties, and use the barycenters of optimal transport mapping to serve as references for CycleGAN. Our experiments indicate that the proposed algorithm is capable of learning a one-to-one mapping with the desired properties.



### Sketch based Reduced Memory Hough Transform
- **Arxiv ID**: http://arxiv.org/abs/1811.06287v1
- **DOI**: None
- **Categories**: **cs.CV**, 1
- **Links**: [PDF](http://arxiv.org/pdf/1811.06287v1)
- **Published**: 2018-11-15 10:44:35+00:00
- **Updated**: 2018-11-15 10:44:35+00:00
- **Authors**: Levi Offen, Michael Werman
- **Comment**: 5 pages
- **Journal**: 2018 25th IEEE International Conference on Image Processing (ICIP)
- **Summary**: This paper proposes using sketch algorithms to represent the votes in Hough transforms. Replacing the accumulator array with a sketch (Sketch Hough Transform - SHT) significantly reduces the memory needed to compute a Hough transform. We also present a new sketch, Count Median Update, which works better than known sketch methods for replacing the accumulator array in the Hough Transform.



### Selective Feature Connection Mechanism: Concatenating Multi-layer CNN Features with a Feature Selector
- **Arxiv ID**: http://arxiv.org/abs/1811.06295v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.06295v3)
- **Published**: 2018-11-15 10:58:21+00:00
- **Updated**: 2019-04-21 07:44:13+00:00
- **Authors**: Chen Du, Chunheng Wang, Yanna Wang, Cunzhao Shi, Baihua Xiao
- **Comment**: The paper is under consideration at Pattern Recognition Letters
- **Journal**: None
- **Summary**: Different layers of deep convolutional neural networks(CNNs) can encode different-level information. High-layer features always contain more semantic information, and low-layer features contain more detail information. However, low-layer features suffer from the background clutter and semantic ambiguity. During visual recognition, the feature combination of the low-layer and high-level features plays an important role in context modulation. If directly combining the high-layer and low-layer features, the background clutter and semantic ambiguity may be caused due to the introduction of detailed information. In this paper, we propose a general network architecture to concatenate CNN features of different layers in a simple and effective way, called Selective Feature Connection Mechanism (SFCM). Low-level features are selectively linked to high-level features with a feature selector which is generated by high-level features. The proposed connection mechanism can effectively overcome the above-mentioned drawbacks. We demonstrate the effectiveness, superiority, and universal applicability of this method on multiple challenging computer vision tasks, including image classification, scene text detection, and image-to-image translation.



### A Neurodynamic model of Saliency prediction in V1
- **Arxiv ID**: http://arxiv.org/abs/1811.06308v8
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.06308v8)
- **Published**: 2018-11-15 12:11:24+00:00
- **Updated**: 2020-09-18 20:36:01+00:00
- **Authors**: David Berga, Xavier Otazu
- **Comment**: 17 pages, 17 figures, 6 tables
- **Journal**: None
- **Summary**: Lateral connections in the primary visual cortex (V1) have long been hypothesized to be responsible of several visual processing mechanisms such as brightness induction, chromatic induction, visual discomfort and bottom-up visual attention (also named saliency). Many computational models have been developed to independently predict these and other visual processes, but no computational model has been able to reproduce all of them simultaneously. In this work we show that a biologically plausible computational model of lateral interactions of V1 is able to simultaneously predict saliency and all the aforementioned visual processes. Our model's (NSWAM) architecture is based on Pennachio's neurodynamic model of lateral connections of V1. It is defined as a network of firing rate neurons, sensitive to visual features such as brightness, color, orientation and scale. We tested NSWAM saliency predictions using images from several eye tracking datasets. We show that accuracy of predictions, using shuffled metrics, obtained by our architecture is similar to other state-of-the-art computational methods, particularly with synthetic images (CAT2000-Pattern & SID4VAM) which mainly contain low level features. Moreover, we outperform other biologically-inspired saliency models that are specifically designed to exclusively reproduce saliency. Hence, we show that our biologically plausible model of lateral connections can simultaneously explain different visual proceses present in V1 (without applying any type of training or optimization and keeping the same parametrization for all the visual processes). This can be useful for the definition of a unified architecture of the primary visual cortex.



### ShuffleDet: Real-Time Vehicle Detection Network in On-board Embedded UAV Imagery
- **Arxiv ID**: http://arxiv.org/abs/1811.06318v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.06318v1)
- **Published**: 2018-11-15 12:42:03+00:00
- **Updated**: 2018-11-15 12:42:03+00:00
- **Authors**: Seyed Majid Azimi
- **Comment**: Accepted in ECCV 2018, UAVision 2018
- **Journal**: None
- **Summary**: On-board real-time vehicle detection is of great significance for UAVs and other embedded mobile platforms. We propose a computationally inexpensive detection network for vehicle detection in UAV imagery which we call ShuffleDet. In order to enhance the speed-wise performance, we construct our method primarily using channel shuffling and grouped convolutions. We apply inception modules and deformable modules to consider the size and geometric shape of the vehicles. ShuffleDet is evaluated on CARPK and PUCPR+ datasets and compared against the state-of-the-art real-time object detection networks. ShuffleDet achieves 3.8 GFLOPs while it provides competitive performance on test sets of both datasets. We show that our algorithm achieves real-time performance by running at the speed of 14 frames per second on NVIDIA Jetson TX2 showing high potential for this method for real-time processing in UAVs.



### Deep Template Matching for Offline Handwritten Chinese Character Recognition
- **Arxiv ID**: http://arxiv.org/abs/1811.06347v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.06347v1)
- **Published**: 2018-11-15 14:02:22+00:00
- **Updated**: 2018-11-15 14:02:22+00:00
- **Authors**: Zhiyuan Li, Min Jin, Qi Wu, Huaxiang Lu
- **Comment**: 5 pages, 2 tables
- **Journal**: None
- **Summary**: Just like its remarkable achievements in many computer vision tasks, the convolutional neural networks (CNN) provide an end-to-end solution in handwritten Chinese character recognition (HCCR) with great success. However, the process of learning discriminative features for image recognition is difficult in cases where little data is available. In this paper, we propose a novel method for learning siamese neural network which employ a special structure to predict the similarity between handwritten Chinese characters and template images. The optimization of siamese neural network can be treated as a simple binary classification problem. When the training process has been finished, the powerful discriminative features help us to generalize the predictive power not just to new data, but to entirely new classes that never appear in the training set. Experiments performed on the ICDAR-2013 offline HCCR datasets have shown that the proposed method has a very promising generalization ability to the new classes that never appear in the training set.



### Pairwise Relational Networks using Local Appearance Features for Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1811.06405v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.06405v1)
- **Published**: 2018-11-15 14:43:04+00:00
- **Updated**: 2018-11-15 14:43:04+00:00
- **Authors**: Bong-Nam Kang, Yonghyun Kim, Daijin Kim
- **Comment**: To be appear in R2L workshop at NIPS 2018. arXiv admin note:
  substantial text overlap with arXiv:1808.04976
- **Journal**: None
- **Summary**: We propose a new face recognition method, called a pairwise relational network (PRN), which takes local appearance features around landmark points on the feature map, and captures unique pairwise relations with the same identity and discriminative pairwise relations between different identities. The PRN aims to determine facial part-relational structure from local appearance feature pairs. Because meaningful pairwise relations should be identity dependent, we add a face identity state feature, which obtains from the long short-term memory (LSTM) units network with the sequential local appearance features. To further improve accuracy, we combined the global appearance features with the pairwise relational feature. Experimental results on the LFW show that the PRN achieved 99.76% accuracy. On the YTF, PRN achieved the state-of-the-art accuracy (96.3%). The PRN also achieved comparable results to the state-of-the-art for both face verification and face identification tasks on the IJB-A and IJB-B. This work is already published on ECCV 2018.



### LinkNet: Relational Embedding for Scene Graph
- **Arxiv ID**: http://arxiv.org/abs/1811.06410v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.06410v1)
- **Published**: 2018-11-15 14:54:14+00:00
- **Updated**: 2018-11-15 14:54:14+00:00
- **Authors**: Sanghyun Woo, Dahun Kim, Donghyeon Cho, In So Kweon
- **Comment**: Accepted to NIPS 2018
- **Journal**: None
- **Summary**: Objects and their relationships are critical contents for image understanding. A scene graph provides a structured description that captures these properties of an image. However, reasoning about the relationships between objects is very challenging and only a few recent works have attempted to solve the problem of generating a scene graph from an image. In this paper, we present a method that improves scene graph generation by explicitly modeling inter-dependency among the entire object instances. We design a simple and effective relational embedding module that enables our model to jointly represent connections among all related objects, rather than focus on an object in isolation. Our method significantly benefits the main part of the scene graph generation task: relationship classification. Using it on top of a basic Faster R-CNN, our model achieves state-of-the-art results on the Visual Genome benchmark. We further push the performance by introducing global context encoding module and geometrical layout encoding module. We validate our final model, LinkNet, through extensive ablation studies, demonstrating its efficacy in scene graph generation.



### Preliminary Studies on a Large Face Database
- **Arxiv ID**: http://arxiv.org/abs/1811.06446v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.06446v1)
- **Published**: 2018-11-15 16:07:49+00:00
- **Updated**: 2018-11-15 16:07:49+00:00
- **Authors**: Benjamin Yip, Garrett Bingham, Katherine Kempfert, Jonathan Fabish, Troy Kling, Cuixian Chen, Yishi Wang
- **Comment**: It has been accepted in the 5th National Symposium for NSF REU
  Research in Data Science, Systems, and Security. G. Bingham and K. Kempfert
  contributed equally
- **Journal**: None
- **Summary**: We perform preliminary studies on a large longitudinal face database MORPH-II, which is a benchmark dataset in the field of computer vision and pattern recognition. First, we summarize the inconsistencies in the dataset and introduce the steps and strategy taken for cleaning. The potential implications of these inconsistencies on prior research are introduced. Next, we propose a new automatic subsetting scheme for evaluation protocol. It is intended to overcome the unbalanced racial and gender distributions of MORPH-II, while ensuring independence between training and testing sets. Finally, we contribute a novel global framework for age estimation that utilizes posterior probabilities from the race classification step to compute a racecomposite age estimate. Preliminary experimental results on MORPH-II are presented.



### Psychophysical evaluation of individual low-level feature influences on visual attention
- **Arxiv ID**: http://arxiv.org/abs/1811.06458v1
- **DOI**: 10.1016/j.visres.2018.10.006
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.06458v1)
- **Published**: 2018-11-15 16:37:31+00:00
- **Updated**: 2018-11-15 16:37:31+00:00
- **Authors**: David Berga, Xosé Ramón Fdez-Vidal, Xavier Otazu, Víctor Leborán, Xosé M. Pardo
- **Comment**: 29 pages, 24 figures, 5 tables
- **Journal**: None
- **Summary**: In this study we provide the analysis of eye movement behavior elicited by low-level feature distinctiveness with a dataset of synthetically-generated image patterns. Design of visual stimuli was inspired by the ones used in previous psychophysical experiments, namely in free-viewing and visual searching tasks, to provide a total of 15 types of stimuli, divided according to the task and feature to be analyzed. Our interest is to analyze the influences of low-level feature contrast between a salient region and the rest of distractors, providing fixation localization characteristics and reaction time of landing inside the salient region. Eye-tracking data was collected from 34 participants during the viewing of a 230 images dataset. Results show that saliency is predominantly and distinctively influenced by: 1. feature type, 2. feature contrast, 3. temporality of fixations, 4. task difficulty and 5. center bias. This experimentation proposes a new psychophysical basis for saliency model evaluation using synthetic images.



### Exploring the Deep Feature Space of a Cell Classification Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1811.06488v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1811.06488v1)
- **Published**: 2018-11-15 17:26:17+00:00
- **Updated**: 2018-11-15 17:26:17+00:00
- **Authors**: Ezra Webb, Cheng Lei, Chun-Jung Huang, Hirofumi Kobayashi, Hideharu Mikami, Keisuke Goda
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present contemporary techniques for visualising the feature space of a deep learning image classification neural network. These techniques are viewed in the context of a feed-forward network trained to classify low resolution fluorescence images of white blood cells captured using optofluidic imaging. The model has two output classes corresponding to two different cell types, which are often difficult to distinguish by eye. This paper has two major sections. The first looks to develop the information space presented by dimension reduction techniques, such as t-SNE, used to embed high-dimensional pre-softmax layer activations into a two-dimensional plane. The second section looks at feature visualisation by optimisation to generate feature images representing the learned features of the network. Using and developing these techniques we visualise class separation and structures within the dataset at various depths using clustering algorithms and feature images; track the development of feature complexity as we ascend the network; and begin to extract the features the network has learnt by modulating single-channel feature images with up-scaled neuron activation maps to distinguish their most salient parts.



### Development and Validation of a Deep Learning Algorithm for Improving Gleason Scoring of Prostate Cancer
- **Arxiv ID**: http://arxiv.org/abs/1811.06497v1
- **DOI**: 10.1038/s41746-019-0112-2
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.06497v1)
- **Published**: 2018-11-15 17:49:50+00:00
- **Updated**: 2018-11-15 17:49:50+00:00
- **Authors**: Kunal Nagpal, Davis Foote, Yun Liu, Po-Hsuan, Chen, Ellery Wulczyn, Fraser Tan, Niels Olson, Jenny L. Smith, Arash Mohtashamian, James H. Wren, Greg S. Corrado, Robert MacDonald, Lily H. Peng, Mahul B. Amin, Andrew J. Evans, Ankur R. Sangoi, Craig H. Mermel, Jason D. Hipp, Martin C. Stumpe
- **Comment**: None
- **Journal**: Nature Partner Journal Digital Medicine (2019)
- **Summary**: For prostate cancer patients, the Gleason score is one of the most important prognostic factors, potentially determining treatment independent of the stage. However, Gleason scoring is based on subjective microscopic examination of tumor morphology and suffers from poor reproducibility. Here we present a deep learning system (DLS) for Gleason scoring whole-slide images of prostatectomies. Our system was developed using 112 million pathologist-annotated image patches from 1,226 slides, and evaluated on an independent validation dataset of 331 slides, where the reference standard was established by genitourinary specialist pathologists. On the validation dataset, the mean accuracy among 29 general pathologists was 0.61. The DLS achieved a significantly higher diagnostic accuracy of 0.70 (p=0.002) and trended towards better patient risk stratification in correlations to clinical follow-up data. Our approach could improve the accuracy of Gleason scoring and subsequent therapy decisions, particularly where specialist expertise is unavailable. The DLS also goes beyond the current Gleason system to more finely characterize and quantitate tumor morphology, providing opportunities for refinement of the Gleason system itself.



### Adjusting for Confounding in Unsupervised Latent Representations of Images
- **Arxiv ID**: http://arxiv.org/abs/1811.06498v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.06498v2)
- **Published**: 2018-11-15 17:53:21+00:00
- **Updated**: 2018-11-26 10:35:32+00:00
- **Authors**: Craig A. Glastonbury, Michael Ferlaino, Christoffer Nellåker, Cecilia M. Lindgren
- **Comment**: Machine Learning for Health (ML4H) Workshop at NeurIPS 2018
  arXiv:1811.07216
- **Journal**: None
- **Summary**: Biological imaging data are often partially confounded or contain unwanted variability. Examples of such phenomena include variable lighting across microscopy image captures, stain intensity variation in histological slides, and batch effects for high throughput drug screening assays. Therefore, to develop "fair" models which generalise well to unseen examples, it is crucial to learn data representations that are insensitive to nuisance factors of variation. In this paper, we present a strategy based on adversarial training, capable of learning unsupervised representations invariant to confounders. As an empirical validation of our method, we use deep convolutional autoencoders to learn unbiased cellular representations from microscopy imaging.



### On transfer learning using a MAC model variant
- **Arxiv ID**: http://arxiv.org/abs/1811.06529v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.06529v2)
- **Published**: 2018-11-15 18:52:06+00:00
- **Updated**: 2018-11-16 23:37:30+00:00
- **Authors**: Vincent Marois, T. S. Jayram, Vincent Albouy, Tomasz Kornuta, Younes Bouhadjar, Ahmet S. Ozcan
- **Comment**: Paper accepted for Visually Grounded Interaction and Language (ViGIL)
  Workshop, NIPS 2018, Montreeal, Canada
- **Journal**: None
- **Summary**: We introduce a variant of the MAC model (Hudson and Manning, ICLR 2018) with a simplified set of equations that achieves comparable accuracy, while training faster. We evaluate both models on CLEVR and CoGenT, and show that, transfer learning with fine-tuning results in a 15 point increase in accuracy, matching the state of the art. Finally, in contrast, we demonstrate that improper fine-tuning can actually reduce a model's accuracy as well.



### CAN: Composite Appearance Network for Person Tracking and How to Model Errors in a Tracking System
- **Arxiv ID**: http://arxiv.org/abs/1811.06582v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.06582v4)
- **Published**: 2018-11-15 20:23:46+00:00
- **Updated**: 2019-10-03 22:48:18+00:00
- **Authors**: Neeti Narayan, Nishant Sankaran, Srirangaraj Setlur, Venu Govindaraju
- **Comment**: None
- **Journal**: None
- **Summary**: Tracking multiple people across multiple cameras is an open problem. It is typically divided into two tasks: (i) single-camera tracking (SCT) - identify trajectories in the same scene, and (ii) inter-camera tracking (ICT) - identify trajectories across cameras for real surveillance scenes. Many methods cater to SCT, while ICT still remains a challenge. In this paper, we propose a tracking method which uses motion cues and a feature aggregation network for template-based person re-identification by incorporating metadata such as person bounding box and camera information. We present a feature aggregation architecture called Composite Appearance Network (CAN) to address the above problem. The key structure of this architecture is called EvalNet that pays attention to each feature vector and learns to weight them based on gradients it receives for the overall template for optimal re-identification performance. We demonstrate the efficiency of our approach with experiments on the challenging multi-camera tracking dataset, DukeMTMC. We also survey existing tracking measures and present an online error metric called "Inference Error" (IE) that provides a better estimate of tracking/re-identification error, by treating SCT and ICT errors uniformly.



### Conditional GANs for Multi-Illuminant Color Constancy: Revolution or Yet Another Approach?
- **Arxiv ID**: http://arxiv.org/abs/1811.06604v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.06604v2)
- **Published**: 2018-11-15 21:58:16+00:00
- **Updated**: 2019-04-20 10:32:35+00:00
- **Authors**: Oleksii Sidorov
- **Comment**: Accepted to CVPR 2019 Workshop (NTIRE)
- **Journal**: None
- **Summary**: Non-uniform and multi-illuminant color constancy are important tasks, the solution of which will allow to discard information about lighting conditions in the image. Non-uniform illumination and shadows distort colors of real-world objects and mostly do not contain valuable information. Thus, many computer vision and image processing techniques would benefit from automatic discarding of this information at the pre-processing step. In this work we propose novel view on this classical problem via generative end-to-end algorithm based on image conditioned Generative Adversarial Network. We also demonstrate the potential of the given approach for joint shadow detection and removal. Forced by the lack of training data, we render the largest existing shadow removal dataset and make it publicly available. It consists of approximately 6,000 pairs of wide field of view synthetic images with and without shadows.



