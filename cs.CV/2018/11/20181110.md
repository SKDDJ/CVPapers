# Arxiv Papers in cs.CV on 2018-11-10
### Power Normalizing Second-order Similarity Network for Few-shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1811.04167v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.04167v1)
- **Published**: 2018-11-10 00:50:06+00:00
- **Updated**: 2018-11-10 00:50:06+00:00
- **Authors**: Hongguang Zhang, Piotr Koniusz
- **Comment**: None
- **Journal**: None
- **Summary**: Second- and higher-order statistics of data points have played an important role in advancing the state of the art on several computer vision problems such as the fine-grained image and scene recognition. However, these statistics need to be passed via an appropriate pooling scheme to obtain the best performance. Power Normalizations are non-linear activation units which enjoy probability-inspired derivations and can be applied in CNNs. In this paper, we propose a similarity learning network leveraging second-order information and Power Normalizations. To this end, we propose several formulations capturing second-order statistics and derive a sigmoid-like Power Normalizing function to demonstrate its interpretability. Our model is trained end-to-end to learn the similarity between the support set and query images for the problem of one- and few-shot learning. The evaluations on Omniglot, miniImagenet and Open MIC datasets demonstrate that this network obtains state-of-the-art results on several few-shot learning protocols.



### Use of Neural Signals to Evaluate the Quality of Generative Adversarial Network Performance in Facial Image Generation
- **Arxiv ID**: http://arxiv.org/abs/1811.04172v3
- **DOI**: 10.1007/s12559-019-09670-y
- **Categories**: **cs.CV**, cs.AI, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1811.04172v3)
- **Published**: 2018-11-10 01:37:56+00:00
- **Updated**: 2019-09-13 15:28:39+00:00
- **Authors**: Zhengwei Wang, Graham Healy, Alan F. Smeaton, Tomas E. Ward
- **Comment**: None
- **Journal**: Cognitive Computation, August 2019
- **Summary**: There is a growing interest in using generative adversarial networks (GANs) to produce image content that is indistinguishable from real images as judged by a typical person. A number of GAN variants for this purpose have been proposed, however, evaluating GANs performance is inherently difficult because current methods for measuring the quality of their output are not always consistent with what a human perceives. We propose a novel approach that combines a brain-computer interface (BCI) with GANs to generate a measure we call Neuroscore, which closely mirrors the behavioral ground truth measured from participants tasked with discerning real from synthetic images. This technique we call a neuro-AI interface, as it provides an interface between a human's neural systems and an AI process. In this paper, we first compare the three most widely used metrics in the literature for evaluating GANs in terms of visual quality and compare their outputs with human judgments. Secondly we propose and demonstrate a novel approach using neural signals and rapid serial visual presentation (RSVP) that directly measures a human perceptual response to facial production quality, independent of a behavioral response measurement. The correlation between our proposed Neuroscore and human perceptual judgments has Pearson correlation statistics: $\mathrm{r}(48) = -0.767, \mathrm{p} = 2.089e-10$. We also present the bootstrap result for the correlation i.e., $\mathrm{p}\leq 0.0001$. Results show that our Neuroscore is more consistent with human judgment compared to the conventional metrics we evaluated. We conclude that neural signals have potential applications for high quality, rapid evaluation of GANs in the context of visual image synthesis.



### Mapping Navigation Instructions to Continuous Control Actions with Position-Visitation Prediction
- **Arxiv ID**: http://arxiv.org/abs/1811.04179v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CL, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.04179v2)
- **Published**: 2018-11-10 02:57:38+00:00
- **Updated**: 2018-12-10 18:37:30+00:00
- **Authors**: Valts Blukis, Dipendra Misra, Ross A. Knepper, Yoav Artzi
- **Comment**: Appeared in Conference on Robot Learning 2018
- **Journal**: In Conference on Robot Learning (pp. 505-518) (2018)
- **Summary**: We propose an approach for mapping natural language instructions and raw observations to continuous control of a quadcopter drone. Our model predicts interpretable position-visitation distributions indicating where the agent should go during execution and where it should stop, and uses the predicted distributions to select the actions to execute. This two-step model decomposition allows for simple and efficient training using a combination of supervised learning and imitation learning. We evaluate our approach with a realistic drone simulator, and demonstrate absolute task-completion accuracy improvements of 16.85% over two state-of-the-art instruction-following methods.



### CAPTAIN: Comprehensive Composition Assistance for Photo Taking
- **Arxiv ID**: http://arxiv.org/abs/1811.04184v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.04184v1)
- **Published**: 2018-11-10 03:43:05+00:00
- **Updated**: 2018-11-10 03:43:05+00:00
- **Authors**: Farshid Farhat, Mohammad Mahdi Kamani, James Z. Wang
- **Comment**: 30 pages, 21 figures, 4 tables, submitted to IJCV (International
  Journal of Computer Vision)
- **Journal**: None
- **Summary**: Many people are interested in taking astonishing photos and sharing with others. Emerging hightech hardware and software facilitate ubiquitousness and functionality of digital photography. Because composition matters in photography, researchers have leveraged some common composition techniques to assess the aesthetic quality of photos computationally. However, composition techniques developed by professionals are far more diverse than well-documented techniques can cover. We leverage the vast underexplored innovations in photography for computational composition assistance. We propose a comprehensive framework, named CAPTAIN (Composition Assistance for Photo Taking), containing integrated deep-learned semantic detectors, sub-genre categorization, artistic pose clustering, personalized aesthetics-based image retrieval, and style set matching. The framework is backed by a large dataset crawled from a photo-sharing Website with mostly photography enthusiasts and professionals. The work proposes a sequence of steps that have not been explored in the past by researchers. The work addresses personal preferences for composition through presenting a ranked-list of photographs to the user based on user-specified weights in the similarity measure. The matching algorithm recognizes the best shot among a sequence of shots with respect to the user's preferred style set. We have conducted a number of experiments on the newly proposed components and reported findings. A user study demonstrates that the work is useful to those taking photos.



### Fast On-the-fly Retraining-free Sparsification of Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1811.04199v3
- **DOI**: 10.1016/j.neucom.2019.08.063
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.04199v3)
- **Published**: 2018-11-10 05:43:36+00:00
- **Updated**: 2019-09-08 17:03:08+00:00
- **Authors**: Amir H. Ashouri, Tarek S. Abdelrahman, Alwyn Dos Remedios
- **Comment**: Extended Version of Our Accepted Paper in NIPS 2018, CDNNRIA
  Workshop: (https://nips.cc/Conferences/2018/Schedule?showEvent=10941)-
  Reviews are available at OpenReview
  (https://openreview.net/forum?id=rkz1YD0vjm)
- **Journal**: Elsevier Neurocomputing, 2019
- **Summary**: Modern Convolutional Neural Networks (CNNs) are complex, encompassing millions of parameters. Their deployment exerts computational, storage and energy demands, particularly on embedded platforms. Existing approaches to prune or sparsify CNNs require retraining to maintain inference accuracy. Such retraining is not feasible in some contexts. In this paper, we explore the sparsification of CNNs by proposing three model-independent methods. Our methods are applied on-the-fly and require no retraining. We show that the state-of-the-art models' weights can be reduced by up to 73% (compression factor of 3.7x) without incurring more than 5% loss in Top-5 accuracy. Additional fine-tuning gains only 8% in sparsity, which indicates that our fast on-the-fly methods are effective.



### Addressing the Invisible: Street Address Generation for Developing Countries with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1811.07769v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.07769v1)
- **Published**: 2018-11-10 07:34:04+00:00
- **Updated**: 2018-11-10 07:34:04+00:00
- **Authors**: Ilke Demir, Ramesh Raskar
- **Comment**: Presented at NIPS 2018 Workshop on Machine Learning for the
  Developing World
- **Journal**: None
- **Summary**: More than half of the world's roads lack adequate street addressing systems. Lack of addresses is even more visible in daily lives of people in developing countries. We would like to object to the assumption that having an address is a luxury, by proposing a generative address design that maps the world in accordance with streets. The addressing scheme is designed considering several traditional street addressing methodologies employed in the urban development scenarios around the world. Our algorithm applies deep learning to extract roads from satellite images, converts the road pixel confidences into a road network, partitions the road network to find neighborhoods, and labels the regions, roads, and address units using graph- and proximity-based algorithms. We present our results on a sample US city, and several developing cities, compare travel times of users using current ad hoc and new complete addresses, and contrast our addressing solution to current industrial and open geocoding alternatives.



### Image Cartoon-Texture Decomposition Using Isotropic Patch Recurrence
- **Arxiv ID**: http://arxiv.org/abs/1811.04208v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.04208v1)
- **Published**: 2018-11-10 07:39:55+00:00
- **Updated**: 2018-11-10 07:39:55+00:00
- **Authors**: Ruotao Xu, Yuhui Quan, Yong Xu
- **Comment**: 13 pages, 10 figures
- **Journal**: None
- **Summary**: Aiming at separating the cartoon and texture layers from an image, cartoon-texture decomposition approaches resort to image priors to model cartoon and texture respectively. In recent years, patch recurrence has emerged as a powerful prior for image recovery. However, the existing strategies of using patch recurrence are ineffective to cartoon-texture decomposition, as both cartoon contours and texture patterns exhibit strong patch recurrence in images. To address this issue, we introduce the isotropy prior of patch recurrence, that the spatial configuration of similar patches in texture exhibits the isotropic structure which is different from that in cartoon, to model the texture component. Based on the isotropic patch recurrence, we construct a nonlocal sparsification system which can effectively distinguish well-patterned features from contour edges. Incorporating the constructed nonlocal system into morphology component analysis, we develop an effective method to both noiseless and noisy cartoon-texture decomposition. The experimental results have demonstrated the superior performance of the proposed method to the existing ones, as well as the effectiveness of the isotropic patch recurrence prior.



### Innovative 3D Depth Map Generation From A Holoscopic 3D Image Based on Graph Cut Technique
- **Arxiv ID**: http://arxiv.org/abs/1811.04217v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.04217v1)
- **Published**: 2018-11-10 08:59:19+00:00
- **Updated**: 2018-11-10 08:59:19+00:00
- **Authors**: Bodor Almatrouk, Mohammad Rafiq Swash, Abdul Hamid Sadka
- **Comment**: 6 pages, 3 figures, international journal on recent and innovation
  trends in computing and communication, (2018)
- **Journal**: None
- **Summary**: Holoscopic 3D imaging is a promising technique for capturing full colour spatial 3D images using a single aperture holoscopic 3D camera. It mimics fly's eye technique with a microlens array, which views the scene at a slightly different angle to its adjacent lens that records three dimensional information onto a two dimensional surface. This paper proposes a method of depth map generation from a holoscopic 3D image based on graph cut technique. The principal objective of this study is to estimate the depth information presented in a holoscopic 3D image with high precision. As such, depth map extraction is measured from a single still holoscopic 3D image which consists of multiple viewpoint images. The viewpoints are extracted and utilised for disparity calculation via disparity space image technique and pixels displacement is measured with sub pixel accuracy to overcome the issue of the narrow baseline between the viewpoint images for stereo matching. In addition, cost aggregation is used to correlate the matching costs within a particular neighbouring region using sum of absolute difference SAD combined with gradient-based metric and winner takes all algorithm is employed to select the minimum elements in the array as optimal disparity value. Finally, the optimal depth map is obtained using graph cut technique. The proposed method extends the utilisation of holoscopic 3D imaging system and enables the expansion of the technology for various applications of autonomous robotics, medical, inspection, AR VR, security and entertainment where 3D depth sensing and measurement are a concern.



### StationPlot: A New Non-stationarity Quantification Tool for Detection of Epileptic Seizures
- **Arxiv ID**: http://arxiv.org/abs/1811.04230v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1811.04230v1)
- **Published**: 2018-11-10 10:34:16+00:00
- **Updated**: 2018-11-10 10:34:16+00:00
- **Authors**: Sawon Pratiher, Subhankar Chattoraj, Rajdeep Mukherjee
- **Comment**: This paper is accepted for presentation at IEEE Global Conference on
  Signal and Information Processing (IEEE GlobalSIP), California, USA, 2018
- **Journal**: None
- **Summary**: A novel non-stationarity visualization tool known as StationPlot is developed for deciphering the chaotic behavior of a dynamical time series. A family of analytic measures enumerating geometrical aspects of the non-stationarity & degree of variability is formulated by convex hull geometry (CHG) on StationPlot. In the Euclidean space, both trend-stationary (TS) & difference-stationary (DS) perturbations are comprehended by the asymmetric structure of StationPlot's region of interest (ROI). The proposed method is experimentally validated using EEG signals, where it comprehend the relative temporal evolution of neural dynamics & its non-stationary morphology, thereby exemplifying its diagnostic competence for seizure activity (SA) detection. Experimental results & analysis-of-Variance (ANOVA) on the extracted CHG features demonstrates better classification performances as compared to the existing shallow feature based state-of-the-art & validates its efficacy as geometry-rich discriminative descriptors for signal processing applications.



### Skeleton-Based Action Recognition with Synchronous Local and Non-local Spatio-temporal Learning and Frequency Attention
- **Arxiv ID**: http://arxiv.org/abs/1811.04237v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.04237v3)
- **Published**: 2018-11-10 11:33:08+00:00
- **Updated**: 2019-06-12 12:50:41+00:00
- **Authors**: Guyue Hu, Bo Cui, Shan Yu
- **Comment**: 6 pages,4 figures; accepted to ICME 2019
- **Journal**: None
- **Summary**: Benefiting from its succinctness and robustness, skeleton-based action recognition has recently attracted much attention. Most existing methods utilize local networks (e.g., recurrent, convolutional, and graph convolutional networks) to extract spatio-temporal dynamics hierarchically. As a consequence, the local and non-local dependencies, which contain more details and semantics respectively, are asynchronously captured in different level of layers. Moreover, existing methods are limited to the spatio-temporal domain and ignore information in the frequency domain. To better extract synchronous detailed and semantic information from multi-domains, we propose a residual frequency attention (rFA) block to focus on discriminative patterns in the frequency domain, and a synchronous local and non-local (SLnL) block to simultaneously capture the details and semantics in the spatio-temporal domain. Besides, a soft-margin focal loss (SMFL) is proposed to optimize the learning whole process, which automatically conducts data selection and encourages intrinsic margins in classifiers. Our approach significantly outperforms other state-of-the-art methods on several large-scale datasets.



### Near Real-Time Data Labeling Using a Depth Sensor for EMG Based Prosthetic Arms
- **Arxiv ID**: http://arxiv.org/abs/1811.04239v1
- **DOI**: 10.1007/978-3-030-01057-7_25
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.04239v1)
- **Published**: 2018-11-10 11:59:56+00:00
- **Updated**: 2018-11-10 11:59:56+00:00
- **Authors**: Geesara Prathap, Titus Nanda Kumara, Roshan Ragel
- **Comment**: None
- **Journal**: None
- **Summary**: Recognizing sEMG (Surface Electromyography) signals belonging to a particular action (e.g., lateral arm raise) automatically is a challenging task as EMG signals themselves have a lot of variation even for the same action due to several factors. To overcome this issue, there should be a proper separation which indicates similar patterns repetitively for a particular action in raw signals. A repetitive pattern is not always matched because the same action can be carried out with different time duration. Thus, a depth sensor (Kinect) was used for pattern identification where three joint angles were recording continuously which is clearly separable for a particular action while recording sEMG signals. To Segment out a repetitive pattern in angle data, MDTW (Moving Dynamic Time Warping) approach is introduced. This technique is allowed to retrieve suspected motion of interest from raw signals. MDTW based on DTW algorithm, but it will be moving through the whole dataset in a pre-defined manner which is capable of picking up almost all the suspected segments inside a given dataset an optimal way. Elevated bicep curl and lateral arm raise movements are taken as motions of interest to show how the proposed technique can be employed to achieve auto identification and labelling. The full implementation is available at https://github.com/GPrathap/OpenBCIPython



### Breast Cancer Classification from Histopathological Images with Inception Recurrent Residual Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1811.04241v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.04241v1)
- **Published**: 2018-11-10 12:10:14+00:00
- **Updated**: 2018-11-10 12:10:14+00:00
- **Authors**: Md Zahangir Alom, Chris Yakopcic, Tarek M. Taha, Vijayan K. Asari
- **Comment**: 15 pages, 9 figures, 9 tables
- **Journal**: None
- **Summary**: The Deep Convolutional Neural Network (DCNN) is one of the most powerful and successful deep learning approaches. DCNNs have already provided superior performance in different modalities of medical imaging including breast cancer classification, segmentation, and detection. Breast cancer is one of the most common and dangerous cancers impacting women worldwide. In this paper, we have proposed a method for breast cancer classification with the Inception Recurrent Residual Convolutional Neural Network (IRRCNN) model. The IRRCNN is a powerful DCNN model that combines the strength of the Inception Network (Inception-v4), the Residual Network (ResNet), and the Recurrent Convolutional Neural Network (RCNN). The IRRCNN shows superior performance against equivalent Inception Networks, Residual Networks, and RCNNs for object recognition tasks. In this paper, the IRRCNN approach is applied for breast cancer classification on two publicly available datasets including BreakHis and Breast Cancer Classification Challenge 2015. The experimental results are compared against the existing machine learning and deep learning-based approaches with respect to image-based, patch-based, image-level, and patient-level classification. The IRRCNN model provides superior classification performance in terms of sensitivity, Area Under the Curve (AUC), the ROC curve, and global accuracy compared to existing approaches for both datasets.



### Deep Learning Approach for Building Detection in Satellite Multispectral Imagery
- **Arxiv ID**: http://arxiv.org/abs/1811.04247v1
- **DOI**: 10.1109/IS.2018.8710471
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.04247v1)
- **Published**: 2018-11-10 12:53:37+00:00
- **Updated**: 2018-11-10 12:53:37+00:00
- **Authors**: Geesara Prathap, Ilya Afanasyev
- **Comment**: None
- **Journal**: None
- **Summary**: Building detection from satellite multispectral imagery data is being a fundamental but a challenging problem mainly because it requires correct recovery of building footprints from high-resolution images. In this work, we propose a deep learning approach for building detection by applying numerous enhancements throughout the process. Initial dataset is preprocessed by 2-sigma percentile normalization. Then data preparation includes ensemble modelling where 3 models were created while incorporating OpenStreetMap data. Binary Distance Transformation (BDT) is used for improving data labeling process and the U-Net (Convolutional Networks for Biomedical Image Segmentation) is modified by adding batch normalization wrappers. Afterwards, it is explained how each component of our approach is correlated with the final detection accuracy. Finally, we compare our results with winning solutions of SpaceNet 2 competition for real satellite multispectral images of Vegas, Paris, Shanghai and Khartoum, demonstrating the importance of our solution for achieving higher building detection accuracy.



### Detecting Work Zones in SHRP 2 NDS Videos Using Deep Learning Based Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/1811.04250v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.04250v1)
- **Published**: 2018-11-10 13:07:06+00:00
- **Updated**: 2018-11-10 13:07:06+00:00
- **Authors**: Franklin Abodo, Robert Rittmuller, Brian Sumner, Andrew Berthaume
- **Comment**: IEEE 17th International Conference on Machine Learning and
  Applications (ICMLA 2018), 3 figures, 1 table, 2 algorithms
- **Journal**: None
- **Summary**: Naturalistic driving studies seek to perform the observations of human driver behavior in the variety of environmental conditions necessary to analyze, understand and predict that behavior using statistical and physical models. The second Strategic Highway Research Program (SHRP 2) funds a number of transportation safety-related projects including its primary effort, the Naturalistic Driving Study (NDS), and an effort supplementary to the NDS, the Roadway Information Database (RID). This work seeks to expand the range of answerable research questions that researchers might pose to the NDS and RID databases. Specifically, we present the SHRP 2 NDS Video Analytics (SNVA) software application, which extracts information from NDS-instrumented vehicles' forward-facing camera footage and efficiently integrates that information into the RID, tying the video content to geolocations and other trip attributes. Of particular interest to researchers and other stakeholders is the integration of work zone, traffic signal state and weather information. The version of SNVA introduced in this paper focuses on work zone detection, the highest priority. The ability to automate the discovery and cataloging of this information, and to do so quickly, is especially important given the two petabyte (2PB) size of the NDS video data set.



### Scene Text Detection and Recognition: The Deep Learning Era
- **Arxiv ID**: http://arxiv.org/abs/1811.04256v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.04256v5)
- **Published**: 2018-11-10 13:56:31+00:00
- **Updated**: 2020-08-09 15:57:22+00:00
- **Authors**: Shangbang Long, Xin He, Cong Yao
- **Comment**: This is a pre-print of an article published in International Journal
  of Computer Vision. The final authenticated version will be available online
  at: https://doi.org/10.1007/s11263-020-01369-0
- **Journal**: None
- **Summary**: With the rise and development of deep learning, computer vision has been tremendously transformed and reshaped. As an important research area in computer vision, scene text detection and recognition has been inescapably influenced by this wave of revolution, consequentially entering the era of deep learning. In recent years, the community has witnessed substantial advancements in mindset, approach and performance. This survey is aimed at summarizing and analyzing the major changes and significant progresses of scene text detection and recognition in the deep learning era. Through this article, we devote to: (1) introduce new insights and ideas; (2) highlight recent techniques and benchmarks; (3) look ahead into future trends. Specifically, we will emphasize the dramatic differences brought by deep learning and the grand challenges still remained. We expect that this review paper would serve as a reference book for researchers in this field. Related resources are also collected and compiled in our Github repository: https://github.com/Jyouhou/SceneTextPapers.



### The Method of Multimodal MRI Brain Image Segmentation Based on Differential Geometric Features
- **Arxiv ID**: http://arxiv.org/abs/1811.04281v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.04281v5)
- **Published**: 2018-11-10 16:46:28+00:00
- **Updated**: 2019-03-06 15:39:27+00:00
- **Authors**: Yongpei Zhu, Zicong Zhou, Guojun Liao, Qianxi Yang, Kehong Yuan
- **Comment**: 18 pages,7 figures
- **Journal**: None
- **Summary**: Accurate segmentation of brain tissue in magnetic resonance images (MRI) is a diffcult task due to different types of brain abnormalities. Using information and features from multimodal MRI including T1, T1-weighted inversion recovery (T1-IR) and T2-FLAIR and differential geometric features including the Jacobian determinant(JD) and the curl vector(CV) derived from T1 modality can result in a more accurate analysis of brain images. In this paper, we use the differential geometric information including JD and CV as image characteristics to measure the differences between different MRI images, which represent local size changes and local rotations of the brain image, and we can use them as one CNN channel with other three modalities (T1-weighted, T1-IR and T2-FLAIR) to get more accurate results of brain segmentation. We test this method on two datasets including IBSR dataset and MRBrainS datasets based on the deep voxelwise residual network, namely VoxResNet, and obtain excellent improvement over single modality or three modalities and increases average DSC(Cerebrospinal Fluid (CSF), Gray Matter (GM) and White Matter (WM)) by about 1.5% on the well-known MRBrainS18 dataset and about 2.5% on the IBSR dataset. Moreover, we discuss that one modality combined with its JD or CV information can replace the segmentation effect of three modalities, which can provide medical conveniences for doctor to diagnose because only to extract T1-modality MRI image of patients. Finally, we also compare the segmentation performance of our method in two networks, VoxResNet and U-Net network. The results show VoxResNet has a better performance than U-Net network with our method in brain MRI segmentation. We believe the proposed method can advance the performance in brain segmentation and clinical diagnosis.



### Coronary Calcium Detection using 3D Attention Identical Dual Deep Network Based on Weakly Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/1811.04289v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.04289v1)
- **Published**: 2018-11-10 18:22:52+00:00
- **Updated**: 2018-11-10 18:22:52+00:00
- **Authors**: Yuankai Huo, James G. Terry, Jiachen Wang, Vishwesh Nath, Camilo Bermudez, Shunxing Bao, Prasanna Parvathaneni, J. Jeffery Carr, Bennett A. Landman
- **Comment**: Accepted by SPIE medical imaging 2019
- **Journal**: None
- **Summary**: Coronary artery calcium (CAC) is biomarker of advanced subclinical coronary artery disease and predicts myocardial infarction and death prior to age 60 years. The slice-wise manual delineation has been regarded as the gold standard of coronary calcium detection. However, manual efforts are time and resource consuming and even impracticable to be applied on large-scale cohorts. In this paper, we propose the attention identical dual network (AID-Net) to perform CAC detection using scan-rescan longitudinal non-contrast CT scans with weakly supervised attention by only using per scan level labels. To leverage the performance, 3D attention mechanisms were integrated into the AID-Net to provide complementary information for classification tasks. Moreover, the 3D Gradient-weighted Class Activation Mapping (Grad-CAM) was also proposed at the testing stage to interpret the behaviors of the deep neural network. 5075 non-contrast chest CT scans were used as training, validation and testing datasets. Baseline performance was assessed on the same cohort. From the results, the proposed AID-Net achieved the superior performance on classification accuracy (0.9272) and AUC (0.9627).



### PolyNeuron: Automatic Neuron Discovery via Learned Polyharmonic Spline Activations
- **Arxiv ID**: http://arxiv.org/abs/1811.04303v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1811.04303v1)
- **Published**: 2018-11-10 20:14:26+00:00
- **Updated**: 2018-11-10 20:14:26+00:00
- **Authors**: Andrew Hryniowski, Alexander Wong
- **Comment**: 5 pages
- **Journal**: None
- **Summary**: Automated deep neural network architecture design has received a significant amount of recent attention. However, this attention has not been equally shared by one of the fundamental building blocks of a deep neural network, the neurons. In this study, we propose PolyNeuron, a novel automatic neuron discovery approach based on learned polyharmonic spline activations. More specifically, PolyNeuron revolves around learning polyharmonic splines, characterized by a set of control points, that represent the activation functions of the neurons in a deep neural network. A relaxed variant of PolyNeuron, which we term PolyNeuron-R, loosens the constraints imposed by PolyNeuron to reduce the computational complexity for discovering the neuron activation functions in an automated manner. Experiments show both PolyNeuron and PolyNeuron-R lead to networks that have improved or comparable performance on multiple network architectures (LeNet-5 and ResNet-20) using different datasets (MNIST and CIFAR10). As such, automatic neuron discovery approaches such as PolyNeuron is a worthy direction to explore.



### Multi-label Object Attribute Classification using a Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1811.04309v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.04309v1)
- **Published**: 2018-11-10 20:27:59+00:00
- **Updated**: 2018-11-10 20:27:59+00:00
- **Authors**: Soubarna Banik, Mikko Lauri, Simone Frintrop
- **Comment**: 14 pages, 7 figures
- **Journal**: None
- **Summary**: Objects of different classes can be described using a limited number of attributes such as color, shape, pattern, and texture. Learning to detect object attributes instead of only detecting objects can be helpful in dealing with a priori unknown objects. With this inspiration, a deep convolutional neural network for low-level object attribute classification, called the Deep Attribute Network (DAN), is proposed. Since object features are implicitly learned by object recognition networks, one such existing network is modified and fine-tuned for developing DAN. The performance of DAN is evaluated on the ImageNet Attribute and a-Pascal datasets. Experiments show that in comparison with state-of-the-art methods, the proposed model achieves better results.



### Automatic Brain Structures Segmentation Using Deep Residual Dilated U-Net
- **Arxiv ID**: http://arxiv.org/abs/1811.04312v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.04312v1)
- **Published**: 2018-11-10 20:47:46+00:00
- **Updated**: 2018-11-10 20:47:46+00:00
- **Authors**: Hongwei Li, Andrii Zhygallo, Bjoern Menze
- **Comment**: to be appeared in MICCAI post-proceedings (BrainLes workshop)
- **Journal**: None
- **Summary**: Brain image segmentation is used for visualizing and quantifying anatomical structures of the brain. We present an automated ap-proach using 2D deep residual dilated networks which captures rich context information of different tissues for the segmentation of eight brain structures. The proposed system was evaluated in the MICCAI Brain Segmentation Challenge and ranked 9th out of 22 teams. We further compared the method with traditional U-Net using leave-one-subject-out cross-validation setting on the public dataset. Experimental results shows that the proposed method outperforms traditional U-Net (i.e. 80.9% vs 78.3% in averaged Dice score, 4.35mm vs 11.59mm in averaged robust Hausdorff distance) and is computationally efficient.



### Fully Convolutional Network with Multi-Step Reinforcement Learning for Image Processing
- **Arxiv ID**: http://arxiv.org/abs/1811.04323v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1811.04323v2)
- **Published**: 2018-11-10 22:59:44+00:00
- **Updated**: 2018-11-13 15:58:01+00:00
- **Authors**: Ryosuke Furuta, Naoto Inoue, Toshihiko Yamasaki
- **Comment**: Accepted to AAAI 2019
- **Journal**: None
- **Summary**: This paper tackles a new problem setting: reinforcement learning with pixel-wise rewards (pixelRL) for image processing. After the introduction of the deep Q-network, deep RL has been achieving great success. However, the applications of deep RL for image processing are still limited. Therefore, we extend deep RL to pixelRL for various image processing applications. In pixelRL, each pixel has an agent, and the agent changes the pixel value by taking an action. We also propose an effective learning method for pixelRL that significantly improves the performance by considering not only the future states of the own pixel but also those of the neighbor pixels. The proposed method can be applied to some image processing tasks that require pixel-wise manipulations, where deep RL has never been applied. We apply the proposed method to three image processing tasks: image denoising, image restoration, and local color enhancement. Our experimental results demonstrate that the proposed method achieves comparable or better performance, compared with the state-of-the-art methods based on supervised learning.



### Handwriting Recognition of Historical Documents with few labeled data
- **Arxiv ID**: http://arxiv.org/abs/1811.07768v1
- **DOI**: 10.1109/DAS.2018.15
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.07768v1)
- **Published**: 2018-11-10 23:21:12+00:00
- **Updated**: 2018-11-10 23:21:12+00:00
- **Authors**: Edgard Chammas, Chafic Mokbel, Laurence Likforman-Sulem
- **Comment**: None
- **Journal**: None
- **Summary**: Historical documents present many challenges for offline handwriting recognition systems, among them, the segmentation and labeling steps. Carefully annotated textlines are needed to train an HTR system. In some scenarios, transcripts are only available at the paragraph level with no text-line information. In this work, we demonstrate how to train an HTR system with few labeled data. Specifically, we train a deep convolutional recurrent neural network (CRNN) system on only 10% of manually labeled text-line data from a dataset and propose an incremental training procedure that covers the rest of the data. Performance is further increased by augmenting the training set with specially crafted multiscale data. We also propose a model-based normalization scheme which considers the variability in the writing scale at the recognition phase. We apply this approach to the publicly available READ dataset. Our system achieved the second best result during the ICDAR2017 competition.



