# Arxiv Papers in cs.CV on 2018-11-25
### Generating Realistic Training Images Based on Tonality-Alignment Generative Adversarial Networks for Hand Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1811.09916v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.09916v4)
- **Published**: 2018-11-25 01:18:13+00:00
- **Updated**: 2020-12-13 02:09:40+00:00
- **Authors**: Liangjian Chen, Shih-Yao Lin, Yusheng Xie, Hui Tang, Yufan Xue, Xiaohui Xie, Yen-Yu Lin, Wei Fan
- **Comment**: None
- **Journal**: 30th British Machine Vision Conference 2019, BMVC 2019
- **Summary**: Hand pose estimation from a monocular RGB image is an important but challenging task. The main factor affecting its performance is the lack of a sufficiently large training dataset with accurate hand-keypoint annotations. In this work, we circumvent this problem by proposing an effective method for generating realistic hand poses and show that state-of-the-art algorithms for hand pose estimation can be greatly improved by utilizing the generated hand poses as training data. Specifically, we first adopt an augmented reality (AR) simulator to synthesize hand poses with accurate hand-keypoint labels. Although the synthetic hand poses come with precise joint labels, eliminating the need of manual annotations, they look unnatural and are not the ideal training data. To produce more realistic hand poses, we propose to blend a synthetic hand pose with a real background, such as arms and sleeves. To this end, we develop tonality-alignment generative adversarial networks (TAGANs), which align the tonality and color distributions between synthetic hand poses and real backgrounds, and can generate high quality hand poses. We evaluate TAGAN on three benchmarks, including the RHP, STB, and CMU-PS hand pose datasets. With the aid of the synthesized poses, our method performs favorably against the state-of-the-arts in both 2D and 3D hand pose estimations.



### PCGAN: Partition-Controlled Human Image Generation
- **Arxiv ID**: http://arxiv.org/abs/1811.09928v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.09928v1)
- **Published**: 2018-11-25 02:07:50+00:00
- **Updated**: 2018-11-25 02:07:50+00:00
- **Authors**: Dong Liang, Rui Wang, Xiaowei Tian, Cong Zou
- **Comment**: AAAI 2019 version
- **Journal**: None
- **Summary**: Human image generation is a very challenging task since it is affected by many factors. Many human image generation methods focus on generating human images conditioned on a given pose, while the generated backgrounds are often blurred.In this paper,we propose a novel Partition-Controlled GAN to generate human images according to target pose and background. Firstly, human poses in the given images are extracted, and foreground/background are partitioned for further use. Secondly, we extract and fuse appearance features, pose features and background features to generate the desired images. Experiments on Market-1501 and DeepFashion datasets show that our model not only generates realistic human images but also produce the human pose and background as we want. Extensive experiments on COCO and LIP datasets indicate the potential of our method.



### Guided Feature Selection for Deep Visual Odometry
- **Arxiv ID**: http://arxiv.org/abs/1811.09935v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1811.09935v1)
- **Published**: 2018-11-25 03:20:37+00:00
- **Updated**: 2018-11-25 03:20:37+00:00
- **Authors**: Fei Xue, Qiuyuan Wang, Xin Wang, Wei Dong, Junqiu Wang, Hongbin Zha
- **Comment**: Accepted to ACCV 2018
- **Journal**: None
- **Summary**: We present a novel end-to-end visual odometry architecture with guided feature selection based on deep convolutional recurrent neural networks. Different from current monocular visual odometry methods, our approach is established on the intuition that features contribute discriminately to different motion patterns. Specifically, we propose a dual-branch recurrent network to learn the rotation and translation separately by leveraging current Convolutional Neural Network (CNN) for feature representation and Recurrent Neural Network (RNN) for image sequence reasoning. To enhance the ability of feature selection, we further introduce an effective context-aware guidance mechanism to force each branch to distill related information for specific motion pattern explicitly. Experiments demonstrate that on the prevalent KITTI and ICL_NUIM benchmarks, our method outperforms current state-of-the-art model- and learning-based methods for both decoupled and joint camera pose recovery.



### Loop Closure Detection with RGB-D Feature Pyramid Siamese Networks
- **Arxiv ID**: http://arxiv.org/abs/1811.09938v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.09938v1)
- **Published**: 2018-11-25 03:39:48+00:00
- **Updated**: 2018-11-25 03:39:48+00:00
- **Authors**: Zhang Qianhao, Alexander Mai, Joseph Menke, Allen Yang
- **Comment**: None
- **Journal**: None
- **Summary**: In visual Simultaneous Localization And Mapping (SLAM), detecting loop closures has been an important but difficult task. Currently, most solutions are based on the bag-of-words approach. Yet the possibility of deep neural network application to this task has not been fully explored due to the lack of appropriate architecture design and of sufficient training data. In this paper we demonstrate the applicability of deep neural networks by addressing both issues. Specifically we show that a feature pyramid Siamese neural network can achieve state-of-the-art performance on pairwise loop closure detection. The network is trained and tested on large-scale RGB-D datasets with a novel automatic loop closure labeling algorithm. Each image pair is labelled by how much the images overlap, allowing loop closure to be computed directly rather than by labor intensive manual labeling. We present an algorithm to adopt any large-scale generic RGB-D dataset for use in training deep loop-closure networks. We show for the first time that deep neural networks are capable of detecting loop closures, and we provide a method for generating large-scale datasets for use in evaluating and training loop closure detectors.



### Privacy-Preserving Action Recognition for Smart Hospitals using Low-Resolution Depth Images
- **Arxiv ID**: http://arxiv.org/abs/1811.09950v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.09950v1)
- **Published**: 2018-11-25 05:55:21+00:00
- **Updated**: 2018-11-25 05:55:21+00:00
- **Authors**: Edward Chou, Matthew Tan, Cherry Zou, Michelle Guo, Albert Haque, Arnold Milstein, Li Fei-Fei
- **Comment**: Machine Learning for Health (ML4H) Workshop at NeurIPS 2018
  arXiv:1811.07216
- **Journal**: None
- **Summary**: Computer-vision hospital systems can greatly assist healthcare workers and improve medical facility treatment, but often face patient resistance due to the perceived intrusiveness and violation of privacy associated with visual surveillance. We downsample video frames to extremely low resolutions to degrade private information from surveillance videos. We measure the amount of activity-recognition information retained in low resolution depth images, and also apply a privately-trained DCSCN super-resolution model to enhance the utility of our images. We implement our techniques with two actual healthcare-surveillance scenarios, hand-hygiene compliance and ICU activity-logging, and show that our privacy-preserving techniques preserve enough information for realistic healthcare tasks.



### Deep RNN Framework for Visual Sequential Applications
- **Arxiv ID**: http://arxiv.org/abs/1811.09961v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.09961v4)
- **Published**: 2018-11-25 06:34:29+00:00
- **Updated**: 2019-10-25 03:55:16+00:00
- **Authors**: Bo Pang, Kaiwen Zha, Hanwen Cao, Chen Shi, Cewu Lu
- **Comment**: 10 pages, 7 figures, CVPR 2019
- **Journal**: None
- **Summary**: Extracting temporal and representation features efficiently plays a pivotal role in understanding visual sequence information. To deal with this, we propose a new recurrent neural framework that can be stacked deep effectively. There are mainly two novel designs in our deep RNN framework: one is a new RNN module called Context Bridge Module (CBM) which splits the information flowing along the sequence (temporal direction) and along depth (spatial representation direction), making it easier to train when building deep by balancing these two directions; the other is the Overlap Coherence Training Scheme that reduces the training complexity for long visual sequential tasks on account of the limitation of computing resources.   We provide empirical evidence to show that our deep RNN framework is easy to optimize and can gain accuracy from the increased depth on several visual sequence problems. On these tasks, we evaluate our deep RNN framework with 15 layers, 7* than conventional RNN networks, but it is still easy to train. Our deep framework achieves more than 11% relative improvements over shallow RNN models on Kinetics, UCF-101, and HMDB-51 for video classification. For auxiliary annotation, after replacing the shallow RNN part of Polygon-RNN with our 15-layer deep CBM, the performance improves by 14.7%. For video future prediction, our deep RNN improves the state-of-the-art shallow model's performance by 2.4% on PSNR and SSIM. The code and trained models are published accompanied by this paper: https://github.com/BoPang1996/Deep-RNN-Framework.



### Practical optimal registration of terrestrial LiDAR scan pairs
- **Arxiv ID**: http://arxiv.org/abs/1811.09962v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.09962v3)
- **Published**: 2018-11-25 06:36:28+00:00
- **Updated**: 2018-11-30 04:45:42+00:00
- **Authors**: Zhipeng Cai, Tat-Jun Chin, Alvaro Parra Bustos, Konrad Schindler
- **Comment**: None
- **Journal**: ISPRS Journal of Photogrammetry and Remote Sensing, Volume 147,
  January 2019, Pages 118-131
- **Summary**: Point cloud registration is a fundamental problem in 3D scanning. In this paper, we address the frequent special case of registering terrestrial LiDAR scans (or, more generally, levelled point clouds). Many current solutions still rely on the Iterative Closest Point (ICP) method or other heuristic procedures, which require good initializations to succeed and/or provide no guarantees of success. On the other hand, exact or optimal registration algorithms can compute the best possible solution without requiring initializations; however, they are currently too slow to be practical in realistic applications.   Existing optimal approaches ignore the fact that in routine use the relative rotations between scans are constrained to the azimuth, via the built-in level compensation in LiDAR scanners. We propose a novel, optimal and computationally efficient registration method for this 4DOF scenario. Our approach operates on candidate 3D keypoint correspondences, and contains two main steps: (1) a deterministic selection scheme that significantly reduces the candidate correspondence set in a way that is guaranteed to preserve the optimal solution; and (2) a fast branch-and-bound (BnB) algorithm with a novel polynomial-time subroutine for 1D rotation search, that quickly finds the optimal alignment for the reduced set. We demonstrate the practicality of our method on realistic point clouds from multiple LiDAR surveys.



### Graph Learning-Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1811.09971v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.09971v1)
- **Published**: 2018-11-25 08:21:44+00:00
- **Updated**: 2018-11-25 08:21:44+00:00
- **Authors**: Bo Jiang, Ziyan Zhang, Doudou Lin, Jin Tang
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, graph Convolutional Neural Networks (graph CNNs) have been widely used for graph data representation and semi-supervised learning tasks. However, existing graph CNNs generally use a fixed graph which may be not optimal for semi-supervised learning tasks. In this paper, we propose a novel Graph Learning-Convolutional Network (GLCN) for graph data representation and semi-supervised learning. The aim of GLCN is to learn an optimal graph structure that best serves graph CNNs for semi-supervised learning by integrating both graph learning and graph convolution together in a unified network architecture. The main advantage is that in GLCN, both given labels and the estimated labels are incorporated and thus can provide useful 'weakly' supervised information to refine (or learn) the graph construction and also to facilitate the graph convolution operation in GLCN for unknown label estimation. Experimental results on seven benchmarks demonstrate that GLCN significantly outperforms state-of-the-art traditional fixed structure based graph CNNs.



### Temporal Bilinear Networks for Video Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1811.09974v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.09974v1)
- **Published**: 2018-11-25 09:16:37+00:00
- **Updated**: 2018-11-25 09:16:37+00:00
- **Authors**: Yanghao Li, Sijie Song, Yuqi Li, Jiaying Liu
- **Comment**: Accepted by AAAI 2019
- **Journal**: None
- **Summary**: Temporal modeling in videos is a fundamental yet challenging problem in computer vision. In this paper, we propose a novel Temporal Bilinear (TB) model to capture the temporal pairwise feature interactions between adjacent frames. Compared with some existing temporal methods which are limited in linear transformations, our TB model considers explicit quadratic bilinear transformations in the temporal domain for motion evolution and sequential relation modeling. We further leverage the factorized bilinear model in linear complexity and a bottleneck network design to build our TB blocks, which also constrains the parameters and computation cost. We consider two schemes in terms of the incorporation of TB blocks and the original 2D spatial convolutions, namely wide and deep Temporal Bilinear Networks (TBN). Finally, we perform experiments on several widely adopted datasets including Kinetics, UCF101 and HMDB51. The effectiveness of our TBNs is validated by comprehensive ablation analyses and comparisons with various state-of-the-art methods.



### Is Data Clustering in Adversarial Settings Secure?
- **Arxiv ID**: http://arxiv.org/abs/1811.09982v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.09982v1)
- **Published**: 2018-11-25 10:21:59+00:00
- **Updated**: 2018-11-25 10:21:59+00:00
- **Authors**: Battista Biggio, Ignazio Pillai, Samuel Rota Bulò, Davide Ariu, Marcello Pelillo, Fabio Roli
- **Comment**: None
- **Journal**: Proceedings of the 2013 ACM Workshop on Artificial Intelligence
  and Security, AISec '13, pages 87-98, New York, NY, USA, 2013. ACM
- **Summary**: Clustering algorithms have been increasingly adopted in security applications to spot dangerous or illicit activities. However, they have not been originally devised to deal with deliberate attack attempts that may aim to subvert the clustering process itself. Whether clustering can be safely adopted in such settings remains thus questionable. In this work we propose a general framework that allows one to identify potential attacks against clustering algorithms, and to evaluate their impact, by making specific assumptions on the adversary's goal, knowledge of the attacked system, and capabilities of manipulating the input data. We show that an attacker may significantly poison the whole clustering process by adding a relatively small percentage of attack samples to the input data, and that some attack samples may be obfuscated to be hidden within some existing clusters. We present a case study on single-linkage hierarchical clustering, and report experiments on clustering of malware samples and handwritten digits.



### Learning Conditional Random Fields with Augmented Observations for Partially Observed Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1811.09986v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.09986v3)
- **Published**: 2018-11-25 10:59:19+00:00
- **Updated**: 2018-12-05 09:35:07+00:00
- **Authors**: Shih-Yao Lin, Yen-Yu Lin, Chu-Song Chen, Yi-Ping Hung
- **Comment**: None
- **Journal**: None
- **Summary**: This paper aims at recognizing partially observed human actions in videos. Action videos acquired in uncontrolled environments often contain corrupt frames, which make actions partially observed. Furthermore, these frames can last for arbitrary lengths of time and appear irregularly. They are inconsistent with training data and degrade the performance of pre-trained action recognition systems. We present an approach to address this issue. For each training and testing actions, we divide it into segments and explore the mutual dependency between temporal segments. This property states that the similarity of two actions at one segment often implies their similarity at another. We augment each segment with extra alternatives retrieved from training data. The augmentation algorithm is designed in a way where a few alternatives are good enough to replace the original segment where corrupt frames occur. Our approach is developed upon hidden conditional random fields and leverages the flexibility of hidden variables for uncertainty handling. It turns out that our approach integrates corrupt segment detection and alternative selection into the process of prediction, and can recognize partially observed actions more accurately. It is evaluated on both fully observed actions and partially observed ones with either synthetic or real corrupt frames. The experimental results manifest its general applicability and superior performance, especially when corrupt frames are present in the action videos.



### Low-resolution Face Recognition in the Wild via Selective Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/1811.09998v2
- **DOI**: 10.1109/TIP.2018.2883743
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.09998v2)
- **Published**: 2018-11-25 12:46:21+00:00
- **Updated**: 2019-03-13 01:43:51+00:00
- **Authors**: Shiming Ge, Shengwei Zhao, Chenyu Li, Jia Li
- **Comment**: None
- **Journal**: None
- **Summary**: Typically, the deployment of face recognition models in the wild needs to identify low-resolution faces with extremely low computational cost. To address this problem, a feasible solution is compressing a complex face model to achieve higher speed and lower memory at the cost of minimal performance drop. Inspired by that, this paper proposes a learning approach to recognize low-resolution faces via selective knowledge distillation. In this approach, a two-stream convolutional neural network (CNN) is first initialized to recognize high-resolution faces and resolution-degraded faces with a teacher stream and a student stream, respectively. The teacher stream is represented by a complex CNN for high-accuracy recognition, and the student stream is represented by a much simpler CNN for low-complexity recognition. To avoid significant performance drop at the student stream, we then selectively distil the most informative facial features from the teacher stream by solving a sparse graph optimization problem, which are then used to regularize the fine-tuning process of the student stream. In this way, the student stream is actually trained by simultaneously handling two tasks with limited computational resources: approximating the most informative facial cues via feature regression, and recovering the missing facial cues via low-resolution face classification. Experimental results show that the student stream performs impressively in recognizing low-resolution faces and costs only 0.15MB memory and runs at 418 faces per second on CPU and 9,433 faces per second on GPU.



### Non-local RoI for Cross-Object Perception
- **Arxiv ID**: http://arxiv.org/abs/1811.10002v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.10002v1)
- **Published**: 2018-11-25 13:05:49+00:00
- **Updated**: 2018-11-25 13:05:49+00:00
- **Authors**: Shou-Yao Roy Tseng, Hwann-Tzong Chen, Shao-Heng Tai, Tyng-Luh Liu
- **Comment**: NIPS 2018 Workshop on Relational Representation Learning. arXiv admin
  note: substantial text overlap with arXiv:1807.05361
- **Journal**: None
- **Summary**: We present a generic and flexible module that encodes region proposals by both their intrinsic features and the extrinsic correlations to the others. The proposed non-local region of interest (NL-RoI) can be seamlessly adapted into different generalized R-CNN architectures to better address various perception tasks. Observe that existing techniques from R-CNN treat RoIs independently and perform the prediction solely based on image features within each region proposal. However, the pairwise relationships between proposals could further provide useful information for detection and segmentation. NL-RoI is thus formulated to enrich each RoI representation with the information from all other RoIs, and yield a simple, low-cost, yet effective module for region-based convolutional networks. Our experimental results show that NL-RoI can improve the performance of Faster/Mask R-CNN for object detection and instance segmentation.



### A pooling based scene text proposal technique for scene text reading in the wild
- **Arxiv ID**: http://arxiv.org/abs/1811.10003v1
- **DOI**: 10.1016/j.patcog.2018.10.012
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10003v1)
- **Published**: 2018-11-25 13:14:53+00:00
- **Updated**: 2018-11-25 13:14:53+00:00
- **Authors**: Dinh NguyenVan, Shijian Lu, Shangxuan Tian, Nizar Ouarti, Mounir Mokhtari
- **Comment**: The article has 34 pages with nine figures, six tables. It has been
  accepted to publish in Journal of Pattern Recognition
- **Journal**: Journal of Pattern Recognition, Volume 87, March 2019, Pages
  118-129, ISSN: 0031-3203
- **Summary**: Automatic reading texts in scenes has attracted increasing interest in recent years as texts often carry rich semantic information that is useful for scene understanding. In this paper, we propose a novel scene text proposal technique aiming for accurate reading texts in scenes. Inspired by the pooling layer in the deep neural network architecture, a pooling based scene text proposal technique is developed. A novel score function is designed which exploits the histogram of oriented gradients and is capable of ranking the proposals according to their probabilities of being text. An end-to-end scene text reading system has also been developed by incorporating the proposed scene text proposal technique where false alarms elimination and words recognition are performed simultaneously. Extensive experiments over several public datasets show that the proposed technique can handle multi-orientation and multi-language scene texts and obtains outstanding proposal performance. The developed end-to-end systems also achieve very competitive scene text spotting and reading performance.



### Visual Attention on the Sun: What Do Existing Models Actually Predict?
- **Arxiv ID**: http://arxiv.org/abs/1811.10004v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10004v1)
- **Published**: 2018-11-25 13:15:37+00:00
- **Updated**: 2018-11-25 13:15:37+00:00
- **Authors**: Jia Li, Daowei Li, Kui Fu, Long Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Visual attention prediction is a classic problem that seems to be well addressed in the deep learning era. One compelling concern, however, gradually arise along with the rapidly growing performance scores over existing visual attention datasets: do existing deep models really capture the inherent mechanism of human visual attention? To address this concern, this paper proposes a new dataset, named VASUN, that records the free-viewing human attention on solar images. Different from previous datasets, images in VASUN contain many irregular visual patterns that existing deep models have never seen. By benchmarking existing models on VASUN, we find the performances of many state-of-the-art deep models drop remarkably, while many classic shallow models perform impressively. From these results, we find that the significant performance advance of existing deep attention models may come from their capabilities of memorizing and predicting the occurrence of some specific visual patterns other than learning the inherent mechanism of human visual attention. In addition, we also train several baseline models on VASUN to demonstrate the feasibility and key issues of predicting visual attention on the sun. These baseline models, together with the proposed dataset, can be used to revisit the problem of visual attention prediction from a novel perspective that are complementary to existing ones.



### Describe and Attend to Track: Learning Natural Language guided Structural Representation and Visual Attention for Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/1811.10014v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10014v2)
- **Published**: 2018-11-25 14:00:05+00:00
- **Updated**: 2018-11-27 02:54:57+00:00
- **Authors**: Xiao Wang, Chenglong Li, Rui Yang, Tianzhu Zhang, Jin Tang, Bin Luo
- **Comment**: None
- **Journal**: None
- **Summary**: The tracking-by-detection framework requires a set of positive and negative training samples to learn robust tracking models for precise localization of target objects. However, existing tracking models mostly treat different samples independently while ignores the relationship information among them. In this paper, we propose a novel structure-aware deep neural network to overcome such limitations. In particular, we construct a graph to represent the pairwise relationships among training samples, and additionally take the natural language as the supervised information to learn both feature representations and classifiers robustly. To refine the states of the target and re-track the target when it is back to view from heavy occlusion and out of view, we elaborately design a novel subnetwork to learn the target-driven visual attentions from the guidance of both visual and natural language cues. Extensive experiments on five tracking benchmark datasets validated the effectiveness of our proposed method.



### Dissimilarity Coefficient based Weakly Supervised Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1811.10016v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10016v1)
- **Published**: 2018-11-25 14:09:57+00:00
- **Updated**: 2018-11-25 14:09:57+00:00
- **Authors**: Aditya Arun, C. V. Jawahar, M. Pawan Kumar
- **Comment**: Preprint
- **Journal**: None
- **Summary**: We consider the problem of weakly supervised object detection, where the training samples are annotated using only image-level labels that indicate the presence or absence of an object category. In order to model the uncertainty in the location of the objects, we employ a dissimilarity coefficient based probabilistic learning objective. The learning objective minimizes the difference between an annotation agnostic prediction distribution and an annotation aware conditional distribution. The main computational challenge is the complex nature of the conditional distribution, which consists of terms over hundreds or thousands of variables. The complexity of the conditional distribution rules out the possibility of explicitly modeling it. Instead, we exploit the fact that deep learning frameworks rely on stochastic optimization. This allows us to use a state of the art discrete generative model that can provide annotation consistent samples from the conditional distribution. Extensive experiments on PASCAL VOC 2007 and 2012 data sets demonstrate the efficacy of our proposed approach.



### Background Subtraction with Real-time Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1811.10020v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10020v2)
- **Published**: 2018-11-25 14:20:44+00:00
- **Updated**: 2018-12-12 15:20:04+00:00
- **Authors**: Dongdong Zeng, Xiang Chen, Ming Zhu, Michael Goesele, Arjan Kuijper
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate and fast foreground object extraction is very important for object tracking and recognition in video surveillance. Although many background subtraction (BGS) methods have been proposed in the recent past, it is still regarded as a tough problem due to the variety of challenging situations that occur in real-world scenarios. In this paper, we explore this problem from a new perspective and propose a novel background subtraction framework with real-time semantic segmentation (RTSS). Our proposed framework consists of two components, a traditional BGS segmenter $\mathcal{B}$ and a real-time semantic segmenter $\mathcal{S}$. The BGS segmenter $\mathcal{B}$ aims to construct background models and segments foreground objects. The real-time semantic segmenter $\mathcal{S}$ is used to refine the foreground segmentation outputs as feedbacks for improving the model updating accuracy. $\mathcal{B}$ and $\mathcal{S}$ work in parallel on two threads. For each input frame $I_t$, the BGS segmenter $\mathcal{B}$ computes a preliminary foreground/background (FG/BG) mask $B_t$. At the same time, the real-time semantic segmenter $\mathcal{S}$ extracts the object-level semantics ${S}_t$. Then, some specific rules are applied on ${B}_t$ and ${S}_t$ to generate the final detection ${D}_t$. Finally, the refined FG/BG mask ${D}_t$ is fed back to update the background model. Comprehensive experiments evaluated on the CDnet 2014 dataset demonstrate that our proposed method achieves state-of-the-art performance among all unsupervised background subtraction methods while operating at real-time, and even performs better than some deep learning based supervised algorithms. In addition, our proposed framework is very flexible and has the potential for generalization.



### Multi-view Point Cloud Registration with Adaptive Convergence Threshold and its Application on 3D Model Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1811.10026v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10026v2)
- **Published**: 2018-11-25 14:51:03+00:00
- **Updated**: 2018-12-25 02:05:58+00:00
- **Authors**: Yaochen Li, Ying Liu, Rui Sun, Rui Guo, Li Zhu, Yong Qi
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-view point cloud registration is a hot topic in the communities of multimedia technology and artificial intelligence (AI). In this paper, we propose a framework to reconstruct the 3D models by the multi-view point cloud registration algorithm with adaptive convergence threshold, and subsequently apply it to 3D model retrieval. The iterative closest point (ICP) algorithm is implemented combining with the motion average algorithm for the registration of multi-view point clouds. After the registration process, we design applications for 3D model retrieval. The geometric saliency map is computed based on the vertex curvature. The test facial triangle is then generated based on the saliency map, which is applied to compare with the standard facial triangle. The face and non-face models are then discriminated. The experiments and comparisons prove the effectiveness of the proposed framework.



### Joint Facade Registration and Segmentation for Urban Localization
- **Arxiv ID**: http://arxiv.org/abs/1811.10048v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10048v2)
- **Published**: 2018-11-25 16:24:00+00:00
- **Updated**: 2019-02-21 16:09:39+00:00
- **Authors**: Antoine Fond, Marie-Odile Berger, Gilles Simon
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents an efficient approach for solving jointly facade registration and semantic segmentation. Progress in facade detection and recognition enable good initialization for the registration of a reference facade to a newly acquired target image. We propose here to rely on semantic segmentation to improve the accuracy of that initial registration. Simultaneously we aim to improve the quality of the semantic segmentation through the registration. These two problems are jointly solved in a Expectation-Maximization framework. We especially introduce a bayesian model that use prior semantic segmentation as well as geometric structure of the facade reference modeled by $L_p$ Gaussian Mixtures. We show the advantages of our method in term of robustness to clutter and change of illumination on urban images from various database.



### An overview of deep learning in medical imaging focusing on MRI
- **Arxiv ID**: http://arxiv.org/abs/1811.10052v2
- **DOI**: 10.1016/j.zemedi.2018.11.002
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.10052v2)
- **Published**: 2018-11-25 16:40:42+00:00
- **Updated**: 2018-12-16 09:58:09+00:00
- **Authors**: Alexander Selvikvåg Lundervold, Arvid Lundervold
- **Comment**: Minor updates. Close to the version published in Zeitschrift f\"ur
  Medizinische Physik (Available online 13 December 2018)
- **Journal**: Zeitschrift f\"ur Medizinische Physik, Volume 29, Issue 2, May
  2019
- **Summary**: What has happened in machine learning lately, and what does it mean for the future of medical image analysis? Machine learning has witnessed a tremendous amount of attention over the last few years. The current boom started around 2009 when so-called deep artificial neural networks began outperforming other established models on a number of important benchmarks. Deep neural networks are now the state-of-the-art machine learning models across a variety of areas, from image analysis to natural language processing, and widely deployed in academia and industry. These developments have a huge potential for medical imaging technology, medical data analysis, medical diagnostics and healthcare in general, slowly being realized. We provide a short overview of recent advances and some associated challenges in machine learning applied to medical image processing and image analysis. As this has become a very broad and fast expanding field we will not survey the entire landscape of applications, but put particular focus on deep learning in MRI.   Our aim is threefold: (i) give a brief introduction to deep learning with pointers to core references; (ii) indicate how deep learning has been applied to the entire MRI processing chain, from acquisition to image retrieval, from segmentation to disease prediction; (iii) provide a starting point for people interested in experimenting and perhaps contributing to the field of machine learning for medical imaging by pointing out good educational resources, state-of-the-art open-source code, and interesting sources of data and problems related medical imaging.



### Predicting Gender from Iris Texture May Be Harder Than It Seems
- **Arxiv ID**: http://arxiv.org/abs/1811.10066v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10066v1)
- **Published**: 2018-11-25 18:23:21+00:00
- **Updated**: 2018-11-25 18:23:21+00:00
- **Authors**: Andrey Kuehlkamp, Kevin Bowyer
- **Comment**: Paper accepted for publication at the IEEE Winter Conference on
  Applications of Computer Vision - 2019
- **Journal**: None
- **Summary**: Predicting gender from iris images has been reported by several researchers as an application of machine learning in biometrics. Recent works on this topic have suggested that the preponderance of the gender cues is located in the periocular region rather than in the iris texture itself. This paper focuses on teasing out whether the information for gender prediction is in the texture of the iris stroma, the periocular region, or both. We present a larger dataset for gender from iris, and evaluate gender prediction accuracy using linear SVM and CNN, comparing hand-crafted and deep features. We use probabilistic occlusion masking to gain insight on the problem. Results suggest the discriminative power of the iris texture for gender is weaker than previously thought, and that the gender-related information is primarily in the periocular region.



### Ensemble of Multi-View Learning Classifiers for Cross-Domain Iris Presentation Attack Detection
- **Arxiv ID**: http://arxiv.org/abs/1811.10068v1
- **DOI**: 10.1109/TIFS.2018.2878542
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10068v1)
- **Published**: 2018-11-25 18:32:35+00:00
- **Updated**: 2018-11-25 18:32:35+00:00
- **Authors**: Andrey Kuehlkamp, Allan Pinto, Anderson Rocha, Kevin Bowyer, Adam Czajka
- **Comment**: IEEE Transactions on Information Forensics and Security (Early
  Access), 2018
- **Journal**: None
- **Summary**: The adoption of large-scale iris recognition systems around the world has brought to light the importance of detecting presentation attack images (textured contact lenses and printouts). This work presents a new approach in iris Presentation Attack Detection (PAD), by exploring combinations of Convolutional Neural Networks (CNNs) and transformed input spaces through binarized statistical image features (BSIF). Our method combines lightweight CNNs to classify multiple BSIF views of the input image. Following explorations on complementary input spaces leading to more discriminative features to detect presentation attacks, we also propose an algorithm to select the best (and most discriminative) predictors for the task at hand.An ensemble of predictors makes use of their expected individual performances to aggregate their results into a final prediction. Results show that this technique improves on the current state of the art in iris PAD, outperforming the winner of LivDet-Iris2017 competition both for intra- and cross-dataset scenarios, and illustrating the very difficult nature of the cross-dataset scenario.



### Learning to discover and localize visual objects with open vocabulary
- **Arxiv ID**: http://arxiv.org/abs/1811.10080v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10080v1)
- **Published**: 2018-11-25 19:55:33+00:00
- **Updated**: 2018-11-25 19:55:33+00:00
- **Authors**: Keren Ye, Mingda Zhang, Wei Li, Danfeng Qin, Adriana Kovashka, Jesse Berent
- **Comment**: None
- **Journal**: None
- **Summary**: To alleviate the cost of obtaining accurate bounding boxes for training today's state-of-the-art object detection models, recent weakly supervised detection work has proposed techniques to learn from image-level labels. However, requiring discrete image-level labels is both restrictive and suboptimal. Real-world "supervision" usually consists of more unstructured text, such as captions. In this work we learn association maps between images and captions. We then use a novel objectness criterion to rank the resulting candidate boxes, such that high-ranking boxes have strong gradients along all edges. Thus, we can detect objects beyond a fixed object category vocabulary, if those objects are frequent and distinctive enough. We show that our objectness criterion improves the proposed bounding boxes in relation to prior weakly supervised detection methods. Further, we show encouraging results on object detection from image-level captions only.



### Reinforced Cross-Modal Matching and Self-Supervised Imitation Learning for Vision-Language Navigation
- **Arxiv ID**: http://arxiv.org/abs/1811.10092v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1811.10092v2)
- **Published**: 2018-11-25 20:49:58+00:00
- **Updated**: 2019-04-06 05:43:50+00:00
- **Authors**: Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen, Yuan-Fang Wang, William Yang Wang, Lei Zhang
- **Comment**: CVPR 2019 Oral
- **Journal**: None
- **Summary**: Vision-language navigation (VLN) is the task of navigating an embodied agent to carry out natural language instructions inside real 3D environments. In this paper, we study how to address three critical challenges for this task: the cross-modal grounding, the ill-posed feedback, and the generalization problems. First, we propose a novel Reinforced Cross-Modal Matching (RCM) approach that enforces cross-modal grounding both locally and globally via reinforcement learning (RL). Particularly, a matching critic is used to provide an intrinsic reward to encourage global matching between instructions and trajectories, and a reasoning navigator is employed to perform cross-modal grounding in the local visual scene. Evaluation on a VLN benchmark dataset shows that our RCM model significantly outperforms previous methods by 10% on SPL and achieves the new state-of-the-art performance. To improve the generalizability of the learned policy, we further introduce a Self-Supervised Imitation Learning (SIL) method to explore unseen environments by imitating its own past, good decisions. We demonstrate that SIL can approximate a better and more efficient policy, which tremendously minimizes the success rate performance gap between seen and unseen environments (from 30.7% to 11.7%).



### WarpGAN: Automatic Caricature Generation
- **Arxiv ID**: http://arxiv.org/abs/1811.10100v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10100v3)
- **Published**: 2018-11-25 21:36:01+00:00
- **Updated**: 2019-04-16 07:10:36+00:00
- **Authors**: Yichun Shi, Debayan Deb, Anil K. Jain
- **Comment**: None
- **Journal**: None
- **Summary**: We propose, WarpGAN, a fully automatic network that can generate caricatures given an input face photo. Besides transferring rich texture styles, WarpGAN learns to automatically predict a set of control points that can warp the photo into a caricature, while preserving identity. We introduce an identity-preserving adversarial loss that aids the discriminator to distinguish between different subjects. Moreover, WarpGAN allows customization of the generated caricatures by controlling the exaggeration extent and the visual styles. Experimental results on a public domain dataset, WebCaricature, show that WarpGAN is capable of generating a diverse set of caricatures while preserving the identities. Five caricature experts suggest that caricatures generated by WarpGAN are visually similar to hand-drawn ones and only prominent facial features are exaggerated.



