# Arxiv Papers in cs.CV on 2018-11-29
### Variational Autoencoding the Lagrangian Trajectories of Particles in a Combustion System
- **Arxiv ID**: http://arxiv.org/abs/1811.11896v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG, physics.data-an, stat.CO
- **Links**: [PDF](http://arxiv.org/pdf/1811.11896v2)
- **Published**: 2018-11-29 00:44:58+00:00
- **Updated**: 2018-12-12 03:18:25+00:00
- **Authors**: Pai Liu, Jingwei Gan, Rajan K. Chakrabarty
- **Comment**: 2nd version: typo corrected, corresponding author changed 19 pages, 9
  figures
- **Journal**: None
- **Summary**: We introduce a deep learning method to simulate the motion of particles trapped in a chaotic recirculating flame. The Lagrangian trajectories of particles, captured using a high-speed camera and subsequently reconstructed in 3-dimensional space, were used to train a variational autoencoder (VAE) which comprises multiple layers of convolutional neural networks. We show that the trajectories, which are statistically representative of those determined in experiments, can be generated using the VAE network. The performance of our model is evaluated with respect to the accuracy and generalization of the outputs.



### Visual Question Answering as Reading Comprehension
- **Arxiv ID**: http://arxiv.org/abs/1811.11903v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.11903v1)
- **Published**: 2018-11-29 01:11:16+00:00
- **Updated**: 2018-11-29 01:11:16+00:00
- **Authors**: Hui Li, Peng Wang, Chunhua Shen, Anton van den Hengel
- **Comment**: None
- **Journal**: None
- **Summary**: Visual question answering (VQA) demands simultaneous comprehension of both the image visual content and natural language questions. In some cases, the reasoning needs the help of common sense or general knowledge which usually appear in the form of text. Current methods jointly embed both the visual information and the textual feature into the same space. However, how to model the complex interactions between the two different modalities is not an easy task. In contrast to struggling on multimodal feature fusion, in this paper, we propose to unify all the input information by natural language so as to convert VQA into a machine reading comprehension problem. With this transformation, our method not only can tackle VQA datasets that focus on observation based questions, but can also be naturally extended to handle knowledge-based VQA which requires to explore large-scale external knowledge base. It is a step towards being able to exploit large volumes of text and natural language processing techniques to address VQA problem. Two types of models are proposed to deal with open-ended VQA and multiple-choice VQA respectively. We evaluate our models on three VQA benchmarks. The comparable performance with the state-of-the-art demonstrates the effectiveness of the proposed method.



### Single-view Object Shape Reconstruction Using Deep Shape Prior and Silhouette
- **Arxiv ID**: http://arxiv.org/abs/1811.11921v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.11921v2)
- **Published**: 2018-11-29 02:03:52+00:00
- **Updated**: 2019-08-01 00:44:27+00:00
- **Authors**: Kejie Li, Ravi Garg, Ming Cai, Ian Reid
- **Comment**: None
- **Journal**: None
- **Summary**: 3D shape reconstruction from a single image is a highly ill-posed problem. Modern deep learning based systems try to solve this problem by learning an end-to-end mapping from image to shape via a deep network. In this paper, we aim to solve this problem via an online optimization framework inspired by traditional methods. Our framework employs a deep autoencoder to learn a set of latent codes of 3D object shapes, which are fitted by a probabilistic shape prior using Gaussian Mixture Model (GMM). At inference, the shape and pose are jointly optimized guided by both image cues and deep shape prior without relying on an initialization from any trained deep nets. Surprisingly, our method achieves comparable performance to state-of-the-art methods even without training an end-to-end network, which shows a promising step in this direction.



### Automatic Rendering of Building Floor Plan Images from Textual Descriptions in English
- **Arxiv ID**: http://arxiv.org/abs/1811.11938v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.11938v1)
- **Published**: 2018-11-29 02:55:25+00:00
- **Updated**: 2018-11-29 02:55:25+00:00
- **Authors**: Mahak Jain, Anurag Sanyal, Shreya Goyal, Chiranjoy Chattopadhyay, Gaurav Bhatnagar
- **Comment**: 8 pages, 9 Figures
- **Journal**: None
- **Summary**: Human beings understand natural language description and could able to imagine a corresponding visual for the same. For example, given a description of the interior of a house, we could imagine its structure and arrangements of furniture. Automatic synthesis of real-world images from text descriptions has been explored in the computer vision community. However, there is no such attempt in the area of document images, like floor plans. Floor plan synthesis from sketches, as well as data-driven models, were proposed earlier. Ours is the first attempt to render building floor plan images from textual description automatically. Here, the input is a natural language description of the internal structure and furniture arrangements within a house, and the output is the 2D floor plan image of the same. We have experimented on publicly available benchmark floor plan datasets. We were able to render realistic synthesized floor plan images from the description written in English.



### Network Uncertainty Informed Semantic Feature Selection for Visual SLAM
- **Arxiv ID**: http://arxiv.org/abs/1811.11946v2
- **DOI**: 10.1109/CRV.2019.00024
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1811.11946v2)
- **Published**: 2018-11-29 03:53:17+00:00
- **Updated**: 2019-08-26 15:00:41+00:00
- **Authors**: Pranav Ganti, Steven L. Waslander
- **Comment**: Published in: 2019 16th Conference on Computer and Robot Vision (CRV)
- **Journal**: None
- **Summary**: In order to facilitate long-term localization using a visual simultaneous localization and mapping (SLAM) algorithm, careful feature selection can help ensure that reference points persist over long durations and the runtime and storage complexity of the algorithm remain consistent. We present SIVO (Semantically Informed Visual Odometry and Mapping), a novel information-theoretic feature selection method for visual SLAM which incorporates semantic segmentation and neural network uncertainty into the feature selection pipeline. Our algorithm selects points which provide the highest reduction in Shannon entropy between the entropy of the current state and the joint entropy of the state, given the addition of the new feature with the classification entropy of the feature from a Bayesian neural network. Each selected feature significantly reduces the uncertainty of the vehicle state and has been detected to be a static object (building, traffic sign, etc.) repeatedly with a high confidence. This selection strategy generates a sparse map which can facilitate long-term localization. The KITTI odometry dataset is used to evaluate our method, and we also compare our results against ORB_SLAM2. Overall, SIVO performs comparably to the baseline method while reducing the map size by almost 70%.



### ADCrowdNet: An Attention-injective Deformable Convolutional Network for Crowd Understanding
- **Arxiv ID**: http://arxiv.org/abs/1811.11968v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.11968v5)
- **Published**: 2018-11-29 05:10:03+00:00
- **Updated**: 2019-04-11 06:27:08+00:00
- **Authors**: Ning Liu, Yongchao Long, Changqing Zou, Qun Niu, Li Pan, Hefeng Wu
- **Comment**: Accepted by CVPR 2019
- **Journal**: None
- **Summary**: We propose an attention-injective deformable convolutional network called ADCrowdNet for crowd understanding that can address the accuracy degradation problem of highly congested noisy scenes. ADCrowdNet contains two concatenated networks. An attention-aware network called Attention Map Generator (AMG) first detects crowd regions in images and computes the congestion degree of these regions. Based on detected crowd regions and congestion priors, a multi-scale deformable network called Density Map Estimator (DME) then generates high-quality density maps. With the attention-aware training scheme and multi-scale deformable convolutional scheme, the proposed ADCrowdNet achieves the capability of being more effective to capture the crowd features and more resistant to various noises. We have evaluated our method on four popular crowd counting datasets (ShanghaiTech, UCF_CC_50, WorldEXPO'10, and UCSD) and an extra vehicle counting dataset TRANCOS, and our approach beats existing state-of-the-art approaches on all of these datasets.



### Traffic Danger Recognition With Surveillance Cameras Without Training Data
- **Arxiv ID**: http://arxiv.org/abs/1811.11969v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1811.11969v1)
- **Published**: 2018-11-29 05:16:40+00:00
- **Updated**: 2018-11-29 05:16:40+00:00
- **Authors**: Lijun Yu, Dawei Zhang, Xiangqun Chen, Alexander Hauptmann
- **Comment**: To be published in proceedings of Advanced Video and Signal-based
  Surveillance (AVSS), 2018 15th IEEE International Conference on, pp. 378-383,
  IEEE
- **Journal**: None
- **Summary**: We propose a traffic danger recognition model that works with arbitrary traffic surveillance cameras to identify and predict car crashes. There are too many cameras to monitor manually. Therefore, we developed a model to predict and identify car crashes from surveillance cameras based on a 3D reconstruction of the road plane and prediction of trajectories. For normal traffic, it supports real-time proactive safety checks of speeds and distances between vehicles to provide insights about possible high-risk areas. We achieve good prediction and recognition of car crashes without using any labeled training data of crashes. Experiments on the BrnoCompSpeed dataset show that our model can accurately monitor the road, with mean errors of 1.80% for distance measurement, 2.77 km/h for speed measurement, 0.24 m for car position prediction, and 2.53 km/h for speed prediction.



### Simple stopping criteria for information theoretic feature selection
- **Arxiv ID**: http://arxiv.org/abs/1811.11971v2
- **DOI**: 10.3390/e21010099
- **Categories**: **cs.CV**, cs.IT, math.IT, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.11971v2)
- **Published**: 2018-11-29 05:24:28+00:00
- **Updated**: 2019-01-29 10:38:02+00:00
- **Authors**: Shujian Yu, Jose C. Principe
- **Comment**: Paper published in the journal of Entropy
- **Journal**: Entropy 2019, 21(1), 99
- **Summary**: Feature selection aims to select the smallest feature subset that yields the minimum generalization error. In the rich literature in feature selection, information theory-based approaches seek a subset of features such that the mutual information between the selected features and the class labels is maximized. Despite the simplicity of this objective, there still remain several open problems in optimization. These include, for example, the automatic determination of the optimal subset size (i.e., the number of features) or a stopping criterion if the greedy searching strategy is adopted. In this paper, we suggest two stopping criteria by just monitoring the conditional mutual information (CMI) among groups of variables. Using the recently developed multivariate matrix-based Renyi's \alpha-entropy functional, which can be directly estimated from data samples, we showed that the CMI among groups of variables can be easily computed without any decomposition or approximation, hence making our criteria easy to implement and seamlessly integrated into any existing information theoretic feature selection methods with a greedy search strategy.



### Efficient Online Multi-Person 2D Pose Tracking with Recurrent Spatio-Temporal Affinity Fields
- **Arxiv ID**: http://arxiv.org/abs/1811.11975v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.11975v3)
- **Published**: 2018-11-29 05:57:53+00:00
- **Updated**: 2019-06-12 18:43:26+00:00
- **Authors**: Yaadhav Raaj, Haroon Idrees, Gines Hidalgo, Yaser Sheikh
- **Comment**: None
- **Journal**: None
- **Summary**: We present an online approach to efficiently and simultaneously detect and track the 2D pose of multiple people in a video sequence. We build upon Part Affinity Field (PAF) representation designed for static images, and propose an architecture that can encode and predict Spatio-Temporal Affinity Fields (STAF) across a video sequence. In particular, we propose a novel temporal topology cross-linked across limbs which can consistently handle body motions of a wide range of magnitudes. Additionally, we make the overall approach recurrent in nature, where the network ingests STAF heatmaps from previous frames and estimates those for the current frame. Our approach uses only online inference and tracking, and is currently the fastest and the most accurate bottom-up approach that is runtime invariant to the number of people in the scene and accuracy invariant to input frame rate of camera. Running at $\sim$30 fps on a single GPU at single scale, it achieves highly competitive results on the PoseTrack benchmarks.



### DuLa-Net: A Dual-Projection Network for Estimating Room Layouts from a Single RGB Panorama
- **Arxiv ID**: http://arxiv.org/abs/1811.11977v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.11977v2)
- **Published**: 2018-11-29 06:06:52+00:00
- **Updated**: 2019-04-02 15:37:59+00:00
- **Authors**: Shang-Ta Yang, Fu-En Wang, Chi-Han Peng, Peter Wonka, Min Sun, Hung-Kuo Chu
- **Comment**: None
- **Journal**: None
- **Summary**: We present a deep learning framework, called DuLa-Net, to predict Manhattan-world 3D room layouts from a single RGB panorama. To achieve better prediction accuracy, our method leverages two projections of the panorama at once, namely the equirectangular panorama-view and the perspective ceiling-view, that each contains different clues about the room layouts. Our network architecture consists of two encoder-decoder branches for analyzing each of the two views. In addition, a novel feature fusion structure is proposed to connect the two branches, which are then jointly trained to predict the 2D floor plans and layout heights. To learn more complex room layouts, we introduce the Realtor360 dataset that contains panoramas of Manhattan-world room layouts with different numbers of corners. Experimental results show that our work outperforms recent state-of-the-art in prediction accuracy and performance, especially in the rooms with non-cuboid layouts.



### Unsupervised Image-to-Image Translation Using Domain-Specific Variational Information Bound
- **Arxiv ID**: http://arxiv.org/abs/1811.11979v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.11979v1)
- **Published**: 2018-11-29 06:25:58+00:00
- **Updated**: 2018-11-29 06:25:58+00:00
- **Authors**: Hadi Kazemi, Sobhan Soleymani, Fariborz Taherkhani, Seyed Mehdi Iranmanesh, Nasser M. Nasrabadi
- **Comment**: NIPS 2018
- **Journal**: None
- **Summary**: Unsupervised image-to-image translation is a class of computer vision problems which aims at modeling conditional distribution of images in the target domain, given a set of unpaired images in the source and target domains. An image in the source domain might have multiple representations in the target domain. Therefore, ambiguity in modeling of the conditional distribution arises, specially when the images in the source and target domains come from different modalities. Current approaches mostly rely on simplifying assumptions to map both domains into a shared-latent space. Consequently, they are only able to model the domain-invariant information between the two modalities. These approaches usually fail to model domain-specific information which has no representation in the target domain. In this work, we propose an unsupervised image-to-image translation framework which maximizes a domain-specific variational information bound and learns the target domain-invariant representation of the two domain. The proposed framework makes it possible to map a single source image into multiple images in the target domain, utilizing several target domain-specific codes sampled randomly from the prior distribution, or extracted from reference images.



### Weakly Supervised Silhouette-based Semantic Scene Change Detection
- **Arxiv ID**: http://arxiv.org/abs/1811.11985v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.11985v3)
- **Published**: 2018-11-29 06:54:46+00:00
- **Updated**: 2022-09-05 09:32:27+00:00
- **Authors**: Ken Sakurada, Mikiya Shibuya, Weimin Wang
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a novel semantic scene change detection scheme with only weak supervision. A straightforward approach for this task is to train a semantic change detection network directly from a large-scale dataset in an end-to-end manner. However, a specific dataset for this task, which is usually labor-intensive and time-consuming, becomes indispensable. To avoid this problem, we propose to train this kind of network from existing datasets by dividing this task into change detection and semantic extraction. On the other hand, the difference in camera viewpoints, for example, images of the same scene captured from a vehicle-mounted camera at different time points, usually brings a challenge to the change detection task. To address this challenge, we propose a new siamese network structure with the introduction of correlation layer. In addition, we collect and annotate a publicly available dataset for semantic change detection to evaluate the proposed method. The experimental results verified both the robustness to viewpoint difference in change detection task and the effectiveness for semantic change detection of the proposed networks. Our code and dataset are available at https://kensakurada.github.io/pscd.



### Deep learning for pedestrians: backpropagation in CNNs
- **Arxiv ID**: http://arxiv.org/abs/1811.11987v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.SC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.11987v1)
- **Published**: 2018-11-29 07:00:09+00:00
- **Updated**: 2018-11-29 07:00:09+00:00
- **Authors**: Laurent Boué
- **Comment**: None
- **Journal**: None
- **Summary**: The goal of this document is to provide a pedagogical introduction to the main concepts underpinning the training of deep neural networks using gradient descent; a process known as backpropagation. Although we focus on a very influential class of architectures called "convolutional neural networks" (CNNs) the approach is generic and useful to the machine learning community as a whole. Motivated by the observation that derivations of backpropagation are often obscured by clumsy index-heavy narratives that appear somewhat mathemagical, we aim to offer a conceptually clear, vectorized description that articulates well the higher level logic. Following the principle of "writing is nature's way of letting you know how sloppy your thinking is", we try to make the calculations meticulous, self-contained and yet as intuitive as possible. Taking nothing for granted, ample illustrations serve as visual guides and an extensive bibliography is provided for further explorations.   (For the sake of clarity, long mathematical derivations and visualizations have been broken up into short "summarized views" and longer "detailed views" encoded into the PDF as optional content groups. Some figures contain animations designed to illustrate important concepts in a more engaging style. For these reasons, we advise to download the document locally and open it using Adobe Acrobat Reader. Other viewers were not tested and may not render the detailed views, animations correctly.)



### Shape-conditioned Image Generation by Learning Latent Appearance Representation from Unpaired Data
- **Arxiv ID**: http://arxiv.org/abs/1811.11991v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.11991v1)
- **Published**: 2018-11-29 07:20:29+00:00
- **Updated**: 2018-11-29 07:20:29+00:00
- **Authors**: Yutaro Miyauchi, Yusuke Sugano, Yasuyuki Matsushita
- **Comment**: Accepted at ACCV 2018
- **Journal**: None
- **Summary**: Conditional image generation is effective for diverse tasks including training data synthesis for learning-based computer vision. However, despite the recent advances in generative adversarial networks (GANs), it is still a challenging task to generate images with detailed conditioning on object shapes. Existing methods for conditional image generation use category labels and/or keypoints and are only give limited control over object categories. In this work, we present SCGAN, an architecture to generate images with a desired shape specified by an input normal map. The shape-conditioned image generation task is achieved by explicitly modeling the image appearance via a latent appearance vector. The network is trained using unpaired training samples of real images and rendered normal maps. This approach enables us to generate images of arbitrary object categories with the target shape and diverse image appearances. We show the effectiveness of our method through both qualitative and quantitative evaluation on training data generation tasks.



### Effective, Fast, and Memory-Efficient Compressed Multi-function Convolutional Neural Networks for More Accurate Medical Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1811.11996v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.11996v1)
- **Published**: 2018-11-29 07:33:23+00:00
- **Updated**: 2018-11-29 07:33:23+00:00
- **Authors**: Luna M. Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) usually use the same activation function, such as RELU, for all convolutional layers. There are performance limitations of just using RELU. In order to achieve better classification performance, reduce training and testing times, and reduce power consumption and memory usage, a new "Compressed Multi-function CNN" is developed. Google's Inception-V4, for example, is a very deep CNN that consists of 4 Inception-A blocks, 7 Inception-B blocks, and 3 Inception-C blocks. RELU is used for all convolutional layers. A new "Compressed Multi-function Inception-V4" (CMI) that can use different activation functions is created with k Inception-A blocks, m Inception-B blocks, and n Inception-C blocks where k in {1, 2, 3, 4}, m in {1, 2, 3, 4, 5, 6, 7}, n in {1, 2, 3}, and (k+m+n)<14. For performance analysis, a dataset for classifying brain MRI images into one of the four stages of Alzheimer's disease is used to compare three CMI architectures with Inception-V4 in terms of F1-score, training and testing times (related to power consumption), and memory usage (model size). Overall, simulations show that the new CMI models can outperform both the commonly used Inception-V4 and Inception-V4 using different activation functions. In the future, other "Compressed Multi-function CNNs", such as "Compressed Multi-function ResNets and DenseNets" that have a reduced number of convolutional blocks using different activation functions, will be developed to further increase classification accuracy, reduce training and testing times, reduce computational power, and reduce memory usage (model size) for building more effective healthcare systems, such as implementing accurate and convenient disease diagnosis systems on mobile devices that have limited battery power and memory.



### Hand Gesture Detection and Conversion to Speech and Text
- **Arxiv ID**: http://arxiv.org/abs/1811.11997v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.11997v1)
- **Published**: 2018-11-29 07:37:07+00:00
- **Updated**: 2018-11-29 07:37:07+00:00
- **Authors**: K. Manikandan, Ayush Patidar, Pallav Walia, Aneek Barman Roy
- **Comment**: 5 pages, 5 figures, International Conference on Innovations and
  Discoveries in Science, Engineering and Technology(ICIDSET) 2018
- **Journal**: International Journal of Pure and Applied Mathematics, Volume 120
  No. 6 2018, 1347-1362, ISSN: 1314-3395 (on-line version)
- **Summary**: The hand gestures are one of the typical methods used in sign language. It is very difficult for the hearing-impaired people to communicate with the world. This project presents a solution that will not only automatically recognize the hand gestures but will also convert it into speech and text output so that impaired person can easily communicate with normal people. A camera attached to computer will capture images of hand and the contour feature extraction is used to recognize the hand gestures of the person. Based on the recognized gestures, the recorded soundtrack will be played.



### Real-time 2D Multi-Person Pose Estimation on CPU: Lightweight OpenPose
- **Arxiv ID**: http://arxiv.org/abs/1811.12004v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.12004v1)
- **Published**: 2018-11-29 08:05:05+00:00
- **Updated**: 2018-11-29 08:05:05+00:00
- **Authors**: Daniil Osokin
- **Comment**: None
- **Journal**: None
- **Summary**: In this work we adapt multi-person pose estimation architecture to use it on edge devices. We follow the bottom-up approach from OpenPose, the winner of COCO 2016 Keypoints Challenge, because of its decent quality and robustness to number of people inside the frame. With proposed network design and optimized post-processing code the full solution runs at 28 frames per second (fps) on Intel$\unicode{xAE}$ NUC 6i7KYB mini PC and 26 fps on Core$^{TM}$ i7-6850K CPU. The network model has 4.1M parameters and 9 billions floating-point operations (GFLOPs) complexity, which is just ~15% of the baseline 2-stage OpenPose with almost the same quality. The code and model are available as a part of Intel$\unicode{xAE}$ OpenVINO$^{TM}$ Toolkit.



### Global Second-order Pooling Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1811.12006v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.12006v2)
- **Published**: 2018-11-29 08:15:39+00:00
- **Updated**: 2018-11-30 02:55:13+00:00
- **Authors**: Zilin Gao, Jiangtao Xie, Qilong Wang, Peihua Li
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Convolutional Networks (ConvNets) are fundamental to, besides large-scale visual recognition, a lot of vision tasks. As the primary goal of the ConvNets is to characterize complex boundaries of thousands of classes in a high-dimensional space, it is critical to learn higher-order representations for enhancing non-linear modeling capability. Recently, Global Second-order Pooling (GSoP), plugged at the end of networks, has attracted increasing attentions, achieving much better performance than classical, first-order networks in a variety of vision tasks. However, how to effectively introduce higher-order representation in earlier layers for improving non-linear capability of ConvNets is still an open problem. In this paper, we propose a novel network model introducing GSoP across from lower to higher layers for exploiting holistic image information throughout a network. Given an input 3D tensor outputted by some previous convolutional layer, we perform GSoP to obtain a covariance matrix which, after nonlinear transformation, is used for tensor scaling along channel dimension. Similarly, we can perform GSoP along spatial dimension for tensor scaling as well. In this way, we can make full use of the second-order statistics of the holistic image throughout a network. The proposed networks are thoroughly evaluated on large-scale ImageNet-1K, and experiments have shown that they outperformed non-trivially the counterparts while achieving state-of-the-art results.



### Efficient Semantic Segmentation for Visual Bird's-eye View Interpretation
- **Arxiv ID**: http://arxiv.org/abs/1811.12008v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.12008v1)
- **Published**: 2018-11-29 08:21:18+00:00
- **Updated**: 2018-11-29 08:21:18+00:00
- **Authors**: Timo Sämann, Karl Amende, Stefan Milz, Christian Witt, Martin Simon, Johannes Petzold
- **Comment**: None
- **Journal**: Advances in Intelligent Systems and Computing 2018
- **Summary**: The ability to perform semantic segmentation in real-time capable applications with limited hardware is of great importance. One such application is the interpretation of the visual bird's-eye view, which requires the semantic segmentation of the four omnidirectional camera images. In this paper, we present an efficient semantic segmentation that sets new standards in terms of runtime and hardware requirements. Our two main contributions are the decrease of the runtime by parallelizing the ArgMax layer and the reduction of hardware requirements by applying the channel pruning method to the ENet model.



### Optimized Skeleton-based Action Recognition via Sparsified Graph Regression
- **Arxiv ID**: http://arxiv.org/abs/1811.12013v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.12013v2)
- **Published**: 2018-11-29 08:36:18+00:00
- **Updated**: 2019-04-15 05:33:09+00:00
- **Authors**: Xiang Gao, Wei Hu, Jiaxiang Tang, Jiaying Liu, Zongming Guo
- **Comment**: None
- **Journal**: None
- **Summary**: With the prevalence of accessible depth sensors, dynamic human body skeletons have attracted much attention as a robust modality for action recognition. Previous methods model skeletons based on RNN or CNN, which has limited expressive power for irregular skeleton joints. While graph convolutional networks (GCN) have been proposed to address irregular graph-structured data, the fundamental graph construction remains challenging. In this paper, we represent skeletons naturally on graphs, and propose a graph regression based GCN (GR-GCN) for skeleton-based action recognition, aiming to capture the spatio-temporal variation in the data. As the graph representation is crucial to graph convolution, we first propose graph regression to statistically learn the underlying graph from multiple observations. In particular, we provide spatio-temporal modeling of skeletons and pose an optimization problem on the graph structure over consecutive frames, which enforces the sparsity of the underlying graph for efficient representation. The optimized graph not only connects each joint to its neighboring joints in the same frame strongly or weakly, but also links with relevant joints in the previous and subsequent frames. We then feed the optimized graph into the GCN along with the coordinates of the skeleton sequence for feature learning, where we deploy high-order and fast Chebyshev approximation of spectral graph convolution. Further, we provide analysis of the variation characterization by the Chebyshev approximation. Experimental results validate the effectiveness of the proposed graph regression and show that the proposed GR-GCN achieves the state-of-the-art performance on the widely used NTU RGB+D, UT-Kinect and SYSU 3D datasets.



### 3D Shape Reconstruction from a Single 2D Image via 2D-3D Self-Consistency
- **Arxiv ID**: http://arxiv.org/abs/1811.12016v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.12016v1)
- **Published**: 2018-11-29 08:47:35+00:00
- **Updated**: 2018-11-29 08:47:35+00:00
- **Authors**: Yi-Lun Liao, Yao-Cheng Yang, Yu-Chiang Frank Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Aiming at inferring 3D shapes from 2D images, 3D shape reconstruction has drawn huge attention from researchers in computer vision and deep learning communities. However, it is not practical to assume that 2D input images and their associated ground truth 3D shapes are always available during training. In this paper, we propose a framework for semi-supervised 3D reconstruction. This is realized by our introduced 2D-3D self-consistency, which aligns the predicted 3D models and the projected 2D foreground segmentation masks. Moreover, our model not only enables recovering 3D shapes with the corresponding 2D masks, camera pose information can be jointly disentangled and predicted, even such supervision is never available during training. In the experiments, we qualitatively and quantitatively demonstrate the effectiveness of our model, which performs favorably against state-of-the-art approaches in either supervised or semi-supervised settings.



### Large-Scale Distributed Second-Order Optimization Using Kronecker-Factored Approximate Curvature for Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1811.12019v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.12019v5)
- **Published**: 2018-11-29 08:52:04+00:00
- **Updated**: 2019-03-30 04:24:57+00:00
- **Authors**: Kazuki Osawa, Yohei Tsuji, Yuichiro Ueno, Akira Naruse, Rio Yokota, Satoshi Matsuoka
- **Comment**: 10 pages, 7 figures. Accepted at CVPR 2019, Long Beach, CA
- **Journal**: None
- **Summary**: Large-scale distributed training of deep neural networks suffer from the generalization gap caused by the increase in the effective mini-batch size. Previous approaches try to solve this problem by varying the learning rate and batch size over epochs and layers, or some ad hoc modification of the batch normalization. We propose an alternative approach using a second-order optimization method that shows similar generalization capability to first-order methods, but converges faster and can handle larger mini-batches. To test our method on a benchmark where highly optimized first-order methods are available as references, we train ResNet-50 on ImageNet. We converged to 75% Top-1 validation accuracy in 35 epochs for mini-batch sizes under 16,384, and achieved 75% even with a mini-batch size of 131,072, which took only 978 iterations.



### Attacks on State-of-the-Art Face Recognition using Attentional Adversarial Attack Generative Network
- **Arxiv ID**: http://arxiv.org/abs/1811.12026v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.12026v2)
- **Published**: 2018-11-29 09:14:56+00:00
- **Updated**: 2018-11-30 02:03:16+00:00
- **Authors**: Qing Song, Yingqi Wu, Lu Yang
- **Comment**: None
- **Journal**: None
- **Summary**: With the broad use of face recognition, its weakness gradually emerges that it is able to be attacked. So, it is important to study how face recognition networks are subject to attacks. In this paper, we focus on a novel way to do attacks against face recognition network that misleads the network to identify someone as the target person not misclassify inconspicuously. Simultaneously, for this purpose, we introduce a specific attentional adversarial attack generative network to generate fake face images. For capturing the semantic information of the target person, this work adds a conditional variational autoencoder and attention modules to learn the instance-level correspondences between faces. Unlike traditional two-player GAN, this work introduces face recognition networks as the third player to participate in the competition between generator and discriminator which allows the attacker to impersonate the target person better. The generated faces which are hard to arouse the notice of onlookers can evade recognition by state-of-the-art networks and most of them are recognized as the target person.



### Grid R-CNN
- **Arxiv ID**: http://arxiv.org/abs/1811.12030v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.12030v1)
- **Published**: 2018-11-29 09:20:43+00:00
- **Updated**: 2018-11-29 09:20:43+00:00
- **Authors**: Xin Lu, Buyu Li, Yuxin Yue, Quanquan Li, Junjie Yan
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a novel object detection framework named Grid R-CNN, which adopts a grid guided localization mechanism for accurate object detection. Different from the traditional regression based methods, the Grid R-CNN captures the spatial information explicitly and enjoys the position sensitive property of fully convolutional architecture. Instead of using only two independent points, we design a multi-point supervision formulation to encode more clues in order to reduce the impact of inaccurate prediction of specific points. To take the full advantage of the correlation of points in a grid, we propose a two-stage information fusion strategy to fuse feature maps of neighbor grid points. The grid guided localization approach is easy to be extended to different state-of-the-art detection frameworks. Grid R-CNN leads to high quality object localization, and experiments demonstrate that it achieves a 4.1% AP gain at IoU=0.8 and a 10.0% AP gain at IoU=0.9 on COCO benchmark compared to Faster R-CNN with Res50 backbone and FPN architecture.



### Utilizing Complex-valued Network for Learning to Compare Image Patches
- **Arxiv ID**: http://arxiv.org/abs/1811.12035v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.12035v2)
- **Published**: 2018-11-29 09:31:09+00:00
- **Updated**: 2019-03-25 02:13:46+00:00
- **Authors**: Siwen Jiang, Wenxuan Wei, Shihao Guo, Hongguang Fu, Lei Huang
- **Comment**: None
- **Journal**: None
- **Summary**: At present, the great achievements of convolutional neural network(CNN) in feature and metric learning have attracted many researchers. However, the vast majority of deep network architectures have been used to represent based on real values. The research of complex-valued networks is seldom concerned due to the absence of effective models and suitable distance of complex-valued vector. Motived by recent works, complex vectors have been shown to have a richer representational capacity and efficient complex blocks have been reported, we propose a new approach for learning image descriptors with complex numbers to compare image patches. We also propose a new architecture to learn image similarity function directly based on complex-valued network. We show that our models can perform competitive results on benchmark datasets. We make the source code of our models publicly available.



### EV-SegNet: Semantic Segmentation for Event-based Cameras
- **Arxiv ID**: http://arxiv.org/abs/1811.12039v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.12039v1)
- **Published**: 2018-11-29 09:48:48+00:00
- **Updated**: 2018-11-29 09:48:48+00:00
- **Authors**: Iñigo Alonso, Ana C. Murillo
- **Comment**: None
- **Journal**: IEEE Conference on Computer Vision and Pattern Recognition
  Workshops (CVPRW), 2019
- **Summary**: Event cameras, or Dynamic Vision Sensor (DVS), are very promising sensors which have shown several advantages over frame based cameras. However, most recent work on real applications of these cameras is focused on 3D reconstruction and 6-DOF camera tracking. Deep learning based approaches, which are leading the state-of-the-art in visual recognition tasks, could potentially take advantage of the benefits of DVS, but some adaptations are needed still needed in order to effectively work on these cameras. This work introduces a first baseline for semantic segmentation with this kind of data. We build a semantic segmentation CNN based on state-of-the-art techniques which takes event information as the only input. Besides, we propose a novel representation for DVS data that outperforms previously used event representations for related tasks. Since there is no existing labeled dataset for this task, we propose how to automatically generate approximated semantic segmentation labels for some sequences of the DDD17 dataset, which we publish together with the model, and demonstrate they are valid to train a model for DVS data only. We compare our results on semantic segmentation from DVS data with results using corresponding grayscale images, demonstrating how they are complementary and worth combining.



### MAMNet: Multi-path Adaptive Modulation Network for Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1811.12043v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.12043v2)
- **Published**: 2018-11-29 09:59:31+00:00
- **Updated**: 2020-03-27 09:11:34+00:00
- **Authors**: Jun-Hyuk Kim, Jun-Ho Choi, Manri Cheon, Jong-Seok Lee
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, single image super-resolution (SR) methods based on deep convolutional neural networks (CNNs) have made significant progress. However, due to the non-adaptive nature of the convolution operation, they cannot adapt to various characteristics of images, which limits their representational capability and, consequently, results in unnecessarily large model sizes. To address this issue, we propose a novel multi-path adaptive modulation network (MAMNet). Specifically, we propose a multi-path adaptive modulation block (MAMB), which is a lightweight yet effective residual block that adaptively modulates residual feature responses by fully exploiting their information via three paths. The three paths model three types of information suitable for SR: 1) channel-specific information (CSI) using global variance pooling, 2) inter-channel dependencies (ICD) based on the CSI, 3) and channel-specific spatial dependencies (CSD) via depth-wise convolution. We demonstrate that the proposed MAMB is effective and parameter-efficient for image SR than other feature modulation methods. In addition, experimental results show that our MAMNet outperforms most of the state-of-the-art methods with a relatively small number of parameters.



### Generalized Coarse-to-Fine Visual Recognition with Progressive Training
- **Arxiv ID**: http://arxiv.org/abs/1811.12047v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.12047v2)
- **Published**: 2018-11-29 10:16:32+00:00
- **Updated**: 2019-04-16 03:43:16+00:00
- **Authors**: Xutong Ren, Lingxi Xie, Chen Wei, Siyuan Qiao, Chi Su, Jiaying Liu, Qi Tian, Elliot K. Fishman, Alan L. Yuille
- **Comment**: None
- **Journal**: None
- **Summary**: Computer vision is difficult, partly because the desired mathematical function connecting input and output data is often complex, fuzzy and thus hard to learn. Coarse-to-fine (C2F) learning is a promising direction, but it remains unclear how it is applied to a wide range of vision problems.   This paper presents a generalized C2F framework by making two technical contributions. First, we provide a unified way of C2F propagation, in which the coarse prediction (a class vector, a detected box, a segmentation mask, etc.) is encoded into a dense (pixel-level) matrix and concatenated to the original input, so that the fine model takes the same design of the coarse model but sees additional information. Second, we present a progressive training strategy which starts with feeding the ground-truth instead of the coarse output into the fine model, and gradually increases the fraction of coarse output, so that at the end of training the fine model is ready for testing. We also relate our approach to curriculum learning by showing that data difficulty keeps increasing during the training process. We apply our framework to three vision tasks including image classification, object localization and semantic segmentation, and demonstrate consistent accuracy gain compared to the baseline training strategy.



### Networks for Nonlinear Diffusion Problems in Imaging
- **Arxiv ID**: http://arxiv.org/abs/1811.12084v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, math.AP, math.NA, 58J65, 94A08, 35R30
- **Links**: [PDF](http://arxiv.org/pdf/1811.12084v1)
- **Published**: 2018-11-29 11:54:54+00:00
- **Updated**: 2018-11-29 11:54:54+00:00
- **Authors**: Simon Arridge, Andreas Hauptmann
- **Comment**: None
- **Journal**: None
- **Summary**: A multitude of imaging and vision tasks have seen recently a major transformation by deep learning methods and in particular by the application of convolutional neural networks. These methods achieve impressive results, even for applications where it is not apparent that convolutions are suited to capture the underlying physics.   In this work we develop a network architecture based on nonlinear diffusion processes, named DiffNet. By design, we obtain a nonlinear network architecture that is well suited for diffusion related problems in imaging. Furthermore, the performed updates are explicit, by which we obtain better interpretability and generalisability compared to classical convolutional neural network architectures. The performance of DiffNet tested on the inverse problem of nonlinear diffusion with the Perona-Malik filter on the STL-10 image dataset. We obtain competitive results to the established U-Net architecture, with a fraction of parameters and necessary training data.



### Generating Easy-to-Understand Referring Expressions for Target Identifications
- **Arxiv ID**: http://arxiv.org/abs/1811.12104v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.12104v4)
- **Published**: 2018-11-29 12:46:54+00:00
- **Updated**: 2019-08-29 04:23:10+00:00
- **Authors**: Mikihiro Tanaka, Takayuki Itamochi, Kenichi Narioka, Ikuro Sato, Yoshitaka Ushiku, Tatsuya Harada
- **Comment**: None
- **Journal**: None
- **Summary**: This paper addresses the generation of referring expressions that not only refer to objects correctly but also let humans find them quickly. As a target becomes relatively less salient, identifying referred objects itself becomes more difficult. However, the existing studies regarded all sentences that refer to objects correctly as equally good, ignoring whether they are easily understood by humans. If the target is not salient, humans utilize relationships with the salient contexts around it to help listeners to comprehend it better. To derive this information from human annotations, our model is designed to extract information from the target and from the environment. Moreover, we regard that sentences that are easily understood are those that are comprehended correctly and quickly by humans. We optimized this by using the time required to locate the referred objects by humans and their accuracies. To evaluate our system, we created a new referring expression dataset whose images were acquired from Grand Theft Auto V (GTA V), limiting targets to persons. Experimental results show the effectiveness of our approach. Our code and dataset are available at https://github.com/mikittt/easy-to-understand-REG.



### Bootstrapping Deep Neural Networks from Approximate Image Processing Pipelines
- **Arxiv ID**: http://arxiv.org/abs/1811.12108v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.12108v2)
- **Published**: 2018-11-29 12:54:51+00:00
- **Updated**: 2019-02-15 21:22:18+00:00
- **Authors**: Kilho Son, Jesse Hostetler, Sek Chai
- **Comment**: 6 pages, 5 figures
- **Journal**: None
- **Summary**: Complex image processing and computer vision systems often consist of a processing pipeline of functional modules. We intend to replace parts or all of a target pipeline with deep neural networks to achieve benefits such as increased accuracy or reduced computational requirement. To acquire a large amount of labeled data necessary to train the deep neural network, we propose a workflow that leverages the target pipeline to create a significantly larger labeled training set automatically, without prior domain knowledge of the target pipeline. We show experimentally that despite the noise introduced by automated labeling and only using a very small initially labeled data set, the trained deep neural networks can achieve similar or even better performance than the components they replace, while in some cases also reducing computational requirements.



### Two-level Attention with Two-stage Multi-task Learning for Facial Emotion Recognition
- **Arxiv ID**: http://arxiv.org/abs/1811.12139v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.12139v1)
- **Published**: 2018-11-29 13:47:01+00:00
- **Updated**: 2018-11-29 13:47:01+00:00
- **Authors**: Xiaohua Wang, Muzi Peng, Lijuan Pan, Min Hu, Chunhua Jin, Fuji Ren
- **Comment**: 10 pages, 4 figures
- **Journal**: None
- **Summary**: Compared with facial emotion recognition on categorical model, the dimensional emotion recognition can describe numerous emotions of the real world more accurately. Most prior works of dimensional emotion estimation only considered laboratory data and used video, speech or other multi-modal features. The effect of these methods applied on static images in the real world is unknown. In this paper, a two-level attention with two-stage multi-task learning (2Att-2Mt) framework is proposed for facial emotion estimation on only static images. Firstly, the features of corresponding region(position-level features) are extracted and enhanced automatically by first-level attention mechanism. In the following, we utilize Bi-directional Recurrent Neural Network(Bi-RNN) with self-attention(second-level attention) to make full use of the relationship features of different layers(layer-level features) adaptively. Owing to the inherent complexity of dimensional emotion recognition, we propose a two-stage multi-task learning structure to exploited categorical representations to ameliorate the dimensional representations and estimate valence and arousal simultaneously in view of the correlation of the two targets. The quantitative results conducted on AffectNet dataset show significant advancement on Concordance Correlation Coefficient(CCC) and Root Mean Square Error(RMSE), illustrating the superiority of the proposed framework. Besides, extensive comparative experiments have also fully demonstrated the effectiveness of different components.



### Parameter-Free Spatial Attention Network for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1811.12150v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.12150v1)
- **Published**: 2018-11-29 14:05:04+00:00
- **Updated**: 2018-11-29 14:05:04+00:00
- **Authors**: Haoran Wang, Yue Fan, Zexin Wang, Licheng Jiao, Bernt Schiele
- **Comment**: None
- **Journal**: None
- **Summary**: Global average pooling (GAP) allows to localize discriminative information for recognition [40]. While GAP helps the convolution neural network to attend to the most discriminative features of an object, it may suffer if that information is missing e.g. due to camera viewpoint changes. To circumvent this issue, we argue that it is advantageous to attend to the global configuration of the object by modeling spatial relations among high-level features. We propose a novel architecture for Person Re-Identification, based on a novel parameter-free spatial attention layer introducing spatial relations among the feature map activations back to the model. Our spatial attention layer consistently improves the performance over the model without it. Results on four benchmarks demonstrate a superiority of our model over the state-of-the-art achieving rank-1 accuracy of 94.7% on Market-1501, 89.0% on DukeMTMC-ReID, 74.9% on CUHK03-labeled and 69.7% on CUHK03-detected.



### Efficient Coarse-to-Fine Non-Local Module for the Detection of Small Objects
- **Arxiv ID**: http://arxiv.org/abs/1811.12152v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.12152v2)
- **Published**: 2018-11-29 14:09:45+00:00
- **Updated**: 2019-05-20 19:38:48+00:00
- **Authors**: Hila Levi, Shimon Ullman
- **Comment**: None
- **Journal**: None
- **Summary**: An image is not just a collection of objects, but rather a graph where each object is related to other objects through spatial and semantic relations. Using relational reasoning modules, such as the non-local module \cite{wang2017non}, can therefore improve object detection. Current schemes apply such dedicated modules either to a specific layer of the bottom-up stream, or between already-detected objects. We show that the relational process can be better modeled in a coarse-to-fine manner and present a novel framework, applying a non-local module sequentially to increasing resolution feature maps along the top-down stream. In this way, information can naturally passed from larger objects to smaller related ones. Applying the module to fine feature maps further allows the information to pass between the small objects themselves, exploiting repetitions of instances of the same class. In practice, due to the expensive memory utilization of the non-local module, it is infeasible to apply the module as currently used to high-resolution feature maps. We redesigned the non local module, improved it in terms of memory and number of operations, allowing it to be placed anywhere along the network. We further incorporated relative spatial information into the module, in a manner that can be incorporated into our efficient implementation. We show the effectiveness of our scheme by improving the results of detecting small objects on COCO by 1-2 AP points over Faster and Mask RCNN and by 1 AP over using non-local module on the bottom-up stream.



### Iterative Residual CNNs for Burst Photography Applications
- **Arxiv ID**: http://arxiv.org/abs/1811.12197v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.12197v2)
- **Published**: 2018-11-29 14:30:52+00:00
- **Updated**: 2019-03-29 12:53:16+00:00
- **Authors**: Filippos Kokkinos, Stamatios Lefkimmiatis
- **Comment**: To appear at CVPR 2019
- **Journal**: None
- **Summary**: Modern inexpensive imaging sensors suffer from inherent hardware constraints which often result in captured images of poor quality. Among the most common ways to deal with such limitations is to rely on burst photography, which nowadays acts as the backbone of all modern smartphone imaging applications. In this work, we focus on the fact that every frame of a burst sequence can be accurately described by a forward (physical) model. This in turn allows us to restore a single image of higher quality from a sequence of low quality images as the solution of an optimization problem. Inspired by an extension of the gradient descent method that can handle non-smooth functions, namely the proximal gradient descent, and modern deep learning techniques, we propose a convolutional iterative network with a transparent architecture. Our network, uses a burst of low quality image frames and is able to produce an output of higher image quality recovering fine details which are not distinguishable in any of the original burst frames. We focus both on the burst photography pipeline as a whole, i.e. burst demosaicking and denoising, as well as on the traditional Gaussian denoising task. The developed method demonstrates consistent state-of-the art performance across the two tasks and as opposed to other recent deep learning approaches does not have any inherent restrictions either to the number of frames or their ordering. Code can be found at https://fkokkinos.github.io/deep_burst/



### ApolloCar3D: A Large 3D Car Instance Understanding Benchmark for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1811.12222v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.12222v2)
- **Published**: 2018-11-29 14:56:58+00:00
- **Updated**: 2018-11-30 08:43:54+00:00
- **Authors**: Xibin Song, Peng Wang, Dingfu Zhou, Rui Zhu, Chenye Guan, Yuchao Dai, Hao Su, Hongdong Li, Ruigang Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Autonomous driving has attracted remarkable attention from both industry and academia. An important task is to estimate 3D properties(e.g.translation, rotation and shape) of a moving or parked vehicle on the road. This task, while critical, is still under-researched in the computer vision community - partially owing to the lack of large scale and fully-annotated 3D car database suitable for autonomous driving research. In this paper, we contribute the first large-scale database suitable for 3D car instance understanding - ApolloCar3D. The dataset contains 5,277 driving images and over 60K car instances, where each car is fitted with an industry-grade 3D CAD model with absolute model size and semantically labelled keypoints. This dataset is above 20 times larger than PASCAL3D+ and KITTI, the current state-of-the-art. To enable efficient labelling in 3D, we build a pipeline by considering 2D-3D keypoint correspondences for a single instance and 3D relationship among multiple instances. Equipped with such dataset, we build various baseline algorithms with the state-of-the-art deep convolutional neural networks. Specifically, we first segment each car with a pre-trained Mask R-CNN, and then regress towards its 3D pose and shape based on a deformable 3D car model with or without using semantic keypoints. We show that using keypoints significantly improves fitting performance. Finally, we develop a new 3D metric jointly considering 3D pose and 3D shape, allowing for comprehensive evaluation and ablation study. By comparing with human performance we suggest several future directions for further improvements.



### ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness
- **Arxiv ID**: http://arxiv.org/abs/1811.12231v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, q-bio.NC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.12231v3)
- **Published**: 2018-11-29 15:04:05+00:00
- **Updated**: 2022-11-09 23:15:15+00:00
- **Authors**: Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A. Wichmann, Wieland Brendel
- **Comment**: Accepted at ICLR 2019 (oral)
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) are commonly thought to recognise objects by learning increasingly complex representations of object shapes. Some recent studies suggest a more important role of image textures. We here put these conflicting hypotheses to a quantitative test by evaluating CNNs and human observers on images with a texture-shape cue conflict. We show that ImageNet-trained CNNs are strongly biased towards recognising textures rather than shapes, which is in stark contrast to human behavioural evidence and reveals fundamentally different classification strategies. We then demonstrate that the same standard architecture (ResNet-50) that learns a texture-based representation on ImageNet is able to learn a shape-based representation instead when trained on "Stylized-ImageNet", a stylized version of ImageNet. This provides a much better fit for human behavioural performance in our well-controlled psychophysical lab setting (nine experiments totalling 48,560 psychophysical trials across 97 observers) and comes with a number of unexpected emergent benefits such as improved object detection performance and previously unseen robustness towards a wide range of image distortions, highlighting advantages of a shape-based representation.



### Perceiving Physical Equation by Observing Visual Scenarios
- **Arxiv ID**: http://arxiv.org/abs/1811.12238v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1811.12238v1)
- **Published**: 2018-11-29 15:13:26+00:00
- **Updated**: 2018-11-29 15:13:26+00:00
- **Authors**: Siyu Huang, Zhi-Qi Cheng, Xi Li, Xiao Wu, Zhongfei Zhang, Alexander Hauptmann
- **Comment**: NIPS 2018 Workshop on Modeling the Physical World
- **Journal**: None
- **Summary**: Inferring universal laws of the environment is an important ability of human intelligence as well as a symbol of general AI. In this paper, we take a step toward this goal such that we introduce a new challenging problem of inferring invariant physical equation from visual scenarios. For instance, teaching a machine to automatically derive the gravitational acceleration formula by watching a free-falling object. To tackle this challenge, we present a novel pipeline comprised of an Observer Engine and a Physicist Engine by respectively imitating the actions of an observer and a physicist in the real world. Generally, the Observer Engine watches the visual scenarios and then extracting the physical properties of objects. The Physicist Engine analyses these data and then summarizing the inherent laws of object dynamics. Specifically, the learned laws are expressed by mathematical equations such that they are more interpretable than the results given by common probabilistic models. Experiments on synthetic videos have shown that our pipeline is able to discover physical equations on various physical worlds with different visual appearances.



### Discovering Spatio-Temporal Action Tubes
- **Arxiv ID**: http://arxiv.org/abs/1811.12248v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.12248v1)
- **Published**: 2018-11-29 15:29:43+00:00
- **Updated**: 2018-11-29 15:29:43+00:00
- **Authors**: Yuancheng Ye, Xiaodong Yang, Yingli Tian
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we address the challenging problem of spatial and temporal action detection in videos. We first develop an effective approach to localize frame-level action regions through integrating static and kinematic information by the early- and late-fusion detection scheme. With the intention of exploring important temporal connections among the detected action regions, we propose a tracking-by-point-matching algorithm to stitch the discrete action regions into a continuous spatio-temporal action tube. Recurrent 3D convolutional neural network is used to predict action categories and determine temporal boundaries of the generated tubes. We then introduce an action footprint map to refine the candidate tubes based on the action-specific spatial characteristics preserved in the convolutional layers of R3DCNN. In the extensive experiments, our method achieves superior detection results on the three public benchmark datasets: UCFSports, J-HMDB and UCF101.



### Face Detection in the Operating Room: Comparison of State-of-the-art Methods and a Self-supervised Approach
- **Arxiv ID**: http://arxiv.org/abs/1811.12296v2
- **DOI**: 10.1007/s11548-019-01944-y
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.12296v2)
- **Published**: 2018-11-29 16:38:16+00:00
- **Updated**: 2018-12-03 11:08:53+00:00
- **Authors**: Thibaut Issenhuth, Vinkle Srivastav, Afshin Gangi, Nicolas Padoy
- **Comment**: 13 pages
- **Journal**: None
- **Summary**: Purpose: Face detection is a needed component for the automatic analysis and assistance of human activities during surgical procedures. Efficient face detection algorithms can indeed help to detect and identify the persons present in the room, and also be used to automatically anonymize the data. However, current algorithms trained on natural images do not generalize well to the operating room (OR) images. In this work, we provide a comparison of state-of-the-art face detectors on OR data and also present an approach to train a face detector for the OR by exploiting non-annotated OR images. Methods: We propose a comparison of 6 state-of-the-art face detectors on clinical data using Multi-View Operating Room Faces (MVOR-Faces), a dataset of operating room images capturing real surgical activities. We then propose to use self-supervision, a domain adaptation method, for the task of face detection in the OR. The approach makes use of non-annotated images to fine-tune a state-of-the-art detector for the OR without using any human supervision. Results: The results show that the best model, namely the tiny face detector, yields an average precision of 0.536 at Intersection over Union (IoU) of 0.5. Our self-supervised model using non-annotated clinical data outperforms this result by 9.2%. Conclusion: We present the first comparison of state-of-the-art face detectors on operating room images and show that results can be significantly improved by using self-supervision on non-annotated data.



### Incremental Scene Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1811.12297v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.12297v4)
- **Published**: 2018-11-29 16:41:44+00:00
- **Updated**: 2019-11-13 19:50:54+00:00
- **Authors**: Benjamin Planche, Xuejian Rong, Ziyan Wu, Srikrishna Karanam, Harald Kosch, YingLi Tian, Jan Ernst, Andreas Hutter
- **Comment**: None
- **Journal**: 33rd Conference on Neural Information Processing Systems (NeurIPS
  2019)
- **Summary**: We present a method to incrementally generate complete 2D or 3D scenes with the following properties: (a) it is globally consistent at each step according to a learned scene prior, (b) real observations of a scene can be incorporated while observing global consistency, (c) unobserved regions can be hallucinated locally in consistence with previous observations, hallucinations and global priors, and (d) hallucinations are statistical in nature, i.e., different scenes can be generated from the same observations. To achieve this, we model the virtual scene, where an active agent at each step can either perceive an observed part of the scene or generate a local hallucination. The latter can be interpreted as the agent's expectation at this step through the scene and can be applied to autonomous navigation. In the limit of observing real data at each point, our method converges to solving the SLAM problem. It can otherwise sample entirely imagined scenes from prior distributions. Besides autonomous agents, applications include problems where large data is required for building robust real-world applications, but few samples are available. We demonstrate efficacy on various 2D as well as 3D data.



### Iterative Projection and Matching: Finding Structure-preserving Representatives and Its Application to Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/1811.12326v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.12326v1)
- **Published**: 2018-11-29 17:24:13+00:00
- **Updated**: 2018-11-29 17:24:13+00:00
- **Authors**: Mohsen Joneidi, Alireza Zaeemzadeh, Nazanin Rahnavard, Mubarak Shah
- **Comment**: 11 pages, 5 figures, 5 tables
- **Journal**: None
- **Summary**: The goal of data selection is to capture the most structural information from a set of data. This paper presents a fast and accurate data selection method, in which the selected samples are optimized to span the subspace of all data. We propose a new selection algorithm, referred to as iterative projection and matching (IPM), with linear complexity w.r.t. the number of data, and without any parameter to be tuned. In our algorithm, at each iteration, the maximum information from the structure of the data is captured by one selected sample, and the captured information is neglected in the next iterations by projection on the null-space of previously selected samples. The computational efficiency and the selection accuracy of our proposed algorithm outperform those of the conventional methods. Furthermore, the superiority of the proposed algorithm is shown on active learning for video action recognition dataset on UCF-101; learning using representatives on ImageNet; training a generative adversarial network (GAN) to generate multi-view images from a single-view input on CMU Multi-PIE dataset; and video summarization on UTE Egocentric dataset.



### InverseRenderNet: Learning single image inverse rendering
- **Arxiv ID**: http://arxiv.org/abs/1811.12328v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.12328v1)
- **Published**: 2018-11-29 17:27:03+00:00
- **Updated**: 2018-11-29 17:27:03+00:00
- **Authors**: Ye Yu, William A. P. Smith
- **Comment**: None
- **Journal**: None
- **Summary**: We show how to train a fully convolutional neural network to perform inverse rendering from a single, uncontrolled image. The network takes an RGB image as input, regresses albedo and normal maps from which we compute lighting coefficients. Our network is trained using large uncontrolled image collections without ground truth. By incorporating a differentiable renderer, our network can learn from self-supervision. Since the problem is ill-posed we introduce additional supervision: 1. We learn a statistical natural illumination prior, 2. Our key insight is to perform offline multiview stereo (MVS) on images containing rich illumination variation. From the MVS pose and depth maps, we can cross project between overlapping views such that Siamese training can be used to ensure consistent estimation of photometric invariants. MVS depth also provides direct coarse supervision for normal map estimation. We believe this is the first attempt to use MVS supervision for learning inverse rendering.



### Touchdown: Natural Language Navigation and Spatial Reasoning in Visual Street Environments
- **Arxiv ID**: http://arxiv.org/abs/1811.12354v7
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.12354v7)
- **Published**: 2018-11-29 18:06:22+00:00
- **Updated**: 2020-05-16 23:36:36+00:00
- **Authors**: Howard Chen, Alane Suhr, Dipendra Misra, Noah Snavely, Yoav Artzi
- **Comment**: arXiv admin note: text overlap with arXiv:1809.00786
- **Journal**: Published in CVPR 2019
- **Summary**: We study the problem of jointly reasoning about language and vision through a navigation and spatial reasoning task. We introduce the Touchdown task and dataset, where an agent must first follow navigation instructions in a real-life visual urban environment, and then identify a location described in natural language to find a hidden object at the goal position. The data contains 9,326 examples of English instructions and spatial descriptions paired with demonstrations. Empirical analysis shows the data presents an open challenge to existing methods, and qualitative linguistic analysis shows that the data displays richer use of spatial reasoning compared to related resources.



### Sym-parameterized Dynamic Inference for Mixed-Domain Image Translation
- **Arxiv ID**: http://arxiv.org/abs/1811.12362v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.12362v3)
- **Published**: 2018-11-29 18:14:16+00:00
- **Updated**: 2019-10-28 10:24:39+00:00
- **Authors**: Simyung Chang, SeongUk Park, John Yang, Nojun Kwak
- **Comment**: 16pages, This paper is accepted at ICCV 2019
- **Journal**: None
- **Summary**: Recent advances in image-to-image translation have led to some ways to generate multiple domain images through a single network. However, there is still a limit in creating an image of a target domain without a dataset on it. We propose a method that expands the concept of `multi-domain' from data to the loss area and learns the combined characteristics of each domain to dynamically infer translations of images in mixed domains. First, we introduce Sym-parameter and its learning method for variously mixed losses while synchronizing them with input conditions. Then, we propose Sym-parameterized Generative Network (SGN) which is empirically confirmed of learning mixed characteristics of various data and losses, and translating images to any mixed-domain without ground truths, such as 30% Van Gogh and 20% Monet and 40% snowy.



### Diverse Image Synthesis from Semantic Layouts via Conditional IMLE
- **Arxiv ID**: http://arxiv.org/abs/1811.12373v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.12373v2)
- **Published**: 2018-11-29 18:36:00+00:00
- **Updated**: 2019-08-29 17:54:53+00:00
- **Authors**: Ke Li, Tianhao Zhang, Jitendra Malik
- **Comment**: 18 pages, 16 figures; IEEE International Conference on Computer
  Vision (ICCV), 2019
- **Journal**: None
- **Summary**: Most existing methods for conditional image synthesis are only able to generate a single plausible image for any given input, or at best a fixed number of plausible images. In this paper, we focus on the problem of generating images from semantic segmentation maps and present a simple new method that can generate an arbitrary number of images with diverse appearance for the same semantic layout. Unlike most existing approaches which adopt the GAN framework, our method is based on the recently introduced Implicit Maximum Likelihood Estimation (IMLE) framework. Compared to the leading approach, our method is able to generate more diverse images while producing fewer artifacts despite using the same architecture. The learned latent space also has sensible structure despite the lack of supervision that encourages such behaviour. Videos and code are available at https://people.eecs.berkeley.edu/~ke.li/projects/imle/scene_layouts/.



### On the Implicit Assumptions of GANs
- **Arxiv ID**: http://arxiv.org/abs/1811.12402v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.12402v1)
- **Published**: 2018-11-29 18:59:55+00:00
- **Updated**: 2018-11-29 18:59:55+00:00
- **Authors**: Ke Li, Jitendra Malik
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: Generative adversarial nets (GANs) have generated a lot of excitement. Despite their popularity, they exhibit a number of well-documented issues in practice, which apparently contradict theoretical guarantees. A number of enlightening papers have pointed out that these issues arise from unjustified assumptions that are commonly made, but the message seems to have been lost amid the optimism of recent years. We believe the identified problems deserve more attention, and highlight the implications on both the properties of GANs and the trajectory of research on probabilistic models. We recently proposed an alternative method that sidesteps these problems.



### AdaFrame: Adaptive Frame Selection for Fast Video Recognition
- **Arxiv ID**: http://arxiv.org/abs/1811.12432v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.12432v2)
- **Published**: 2018-11-29 19:08:52+00:00
- **Updated**: 2019-04-10 14:54:13+00:00
- **Authors**: Zuxuan Wu, Caiming Xiong, Chih-Yao Ma, Richard Socher, Larry S. Davis
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: We present AdaFrame, a framework that adaptively selects relevant frames on a per-input basis for fast video recognition. AdaFrame contains a Long Short-Term Memory network augmented with a global memory that provides context information for searching which frames to use over time. Trained with policy gradient methods, AdaFrame generates a prediction, determines which frame to observe next, and computes the utility, i.e., expected future rewards, of seeing more frames at each time step. At testing time, AdaFrame exploits predicted utilities to achieve adaptive lookahead inference such that the overall computational costs are reduced without incurring a decrease in accuracy. Extensive experiments are conducted on two large-scale video benchmarks, FCVID and ActivityNet. AdaFrame matches the performance of using all frames with only 8.21 and 8.65 frames on FCVID and ActivityNet, respectively. We further qualitatively demonstrate learned frame usage can indicate the difficulty of making classification decisions; easier samples need fewer frames while harder ones require more, both at instance-level within the same class and at class-level among different categories.



### Fast and Flexible Indoor Scene Synthesis via Deep Convolutional Generative Models
- **Arxiv ID**: http://arxiv.org/abs/1811.12463v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1811.12463v1)
- **Published**: 2018-11-29 20:03:28+00:00
- **Updated**: 2018-11-29 20:03:28+00:00
- **Authors**: Daniel Ritchie, Kai Wang, Yu-an Lin
- **Comment**: None
- **Journal**: None
- **Summary**: We present a new, fast and flexible pipeline for indoor scene synthesis that is based on deep convolutional generative models. Our method operates on a top-down image-based representation, and inserts objects iteratively into the scene by predicting their category, location, orientation and size with separate neural network modules. Our pipeline naturally supports automatic completion of partial scenes, as well as synthesis of complete scenes. Our method is significantly faster than the previous image-based method and generates result that outperforms it and other state-of-the-art deep generative scene models in terms of faithfulness to training data and perceived visual quality.



### Learning to Separate Multiple Illuminants in a Single Image
- **Arxiv ID**: http://arxiv.org/abs/1811.12481v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.12481v2)
- **Published**: 2018-11-29 20:56:25+00:00
- **Updated**: 2019-04-22 22:50:49+00:00
- **Authors**: Zhuo Hui, Ayan Chakrabarti, Kalyan Sunkavalli, Aswin C. Sankaranarayanan
- **Comment**: None
- **Journal**: None
- **Summary**: We present a method to separate a single image captured under two illuminants, with different spectra, into the two images corresponding to the appearance of the scene under each individual illuminant. We do this by training a deep neural network to predict the per-pixel reflectance chromaticity of the scene, which we use in conjunction with a previous flash/no-flash image-based separation algorithm to produce the final two output images. We design our reflectance chromaticity network and loss functions by incorporating intuitions from the physics of image formation. We show that this leads to significantly better performance than other single image techniques and even approaches the quality of the two image separation method.



### Leveraging Deep Stein's Unbiased Risk Estimator for Unsupervised X-ray Denoising
- **Arxiv ID**: http://arxiv.org/abs/1811.12488v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.12488v1)
- **Published**: 2018-11-29 21:04:38+00:00
- **Updated**: 2018-11-29 21:04:38+00:00
- **Authors**: Fahad Shamshad, Muhammad Awais, Muhammad Asim, Zain ul Aabidin Lodhi, Muhammad Umair, Ali Ahmed
- **Comment**: Machine Learning for Health (ML4H) Workshop at NeurIPS 2018
  arXiv:1811.07216
- **Journal**: None
- **Summary**: Among the plethora of techniques devised to curb the prevalence of noise in medical images, deep learning based approaches have shown the most promise. However, one critical limitation of these deep learning based denoisers is the requirement of high-quality noiseless ground truth images that are difficult to obtain in many medical imaging applications such as X-rays. To circumvent this issue, we leverage recently proposed approach of [7] that incorporates Stein's Unbiased Risk Estimator (SURE) to train a deep convolutional neural network without requiring denoised ground truth X-ray data. Our experimental results demonstrate the effectiveness of SURE based approach for denoising X-ray images.



### Playing Soccer without Colors in the SPL: A Convolutional Neural Network Approach
- **Arxiv ID**: http://arxiv.org/abs/1811.12493v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1811.12493v1)
- **Published**: 2018-11-29 21:26:01+00:00
- **Updated**: 2018-11-29 21:26:01+00:00
- **Authors**: Francisco Leiva, Nicolás Cruz, Ignacio Bugueño, Javier Ruiz-del-Solar
- **Comment**: 12 pages, 6 figures. Presented in RoboCup Symposium 2018. Final
  version will appear in Springer
- **Journal**: None
- **Summary**: The goal of this paper is to propose a vision system for humanoid robotic soccer that does not use any color information. The main features of this system are: (i) real-time operation in the NAO robot, and (ii) the ability to detect the ball, the robots, their orientations, the lines and key field features robustly. Our ball detector, robot detector, and robot's orientation detector obtain the highest reported detection rates. The proposed vision system is tested in a SPL field with several NAO robots under realistic and highly demanding conditions. The obtained results are: robot detection rate of 94.90%, ball detection rate of 97.10%, and a completely perceived orientation rate of 99.88% when the observed robot is static, and 95.52% when the observed robot is moving.



### On Implicit Filter Level Sparsity in Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1811.12495v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.SP, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.12495v2)
- **Published**: 2018-11-29 21:29:31+00:00
- **Updated**: 2019-04-05 15:40:40+00:00
- **Authors**: Dushyant Mehta, Kwang In Kim, Christian Theobalt
- **Comment**: Accepted at CVPR 2019
- **Journal**: None
- **Summary**: We investigate filter level sparsity that emerges in convolutional neural networks (CNNs) which employ Batch Normalization and ReLU activation, and are trained with adaptive gradient descent techniques and L2 regularization or weight decay. We conduct an extensive experimental study casting our initial findings into hypotheses and conclusions about the mechanisms underlying the emergent filter level sparsity. This study allows new insight into the performance gap obeserved between adapative and non-adaptive gradient descent methods in practice. Further, analysis of the effect of training strategies and hyperparameters on the sparsity leads to practical suggestions in designing CNN training strategies enabling us to explore the tradeoffs between feature selectivity, network capacity, and generalization performance. Lastly, we show that the implicit sparsity can be harnessed for neural network speedup at par or better than explicit sparsification / pruning approaches, with no modifications to the typical training pipeline required.



### 3D Semi-Supervised Learning with Uncertainty-Aware Multi-View Co-Training
- **Arxiv ID**: http://arxiv.org/abs/1811.12506v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.12506v2)
- **Published**: 2018-11-29 21:58:53+00:00
- **Updated**: 2020-02-24 01:50:28+00:00
- **Authors**: Yingda Xia, Fengze Liu, Dong Yang, Jinzheng Cai, Lequan Yu, Zhuotun Zhu, Daguang Xu, Alan Yuille, Holger Roth
- **Comment**: Accepted to WACV 2020
- **Journal**: None
- **Summary**: While making a tremendous impact in various fields, deep neural networks usually require large amounts of labeled data for training which are expensive to collect in many applications, especially in the medical domain. Unlabeled data, on the other hand, is much more abundant. Semi-supervised learning techniques, such as co-training, could provide a powerful tool to leverage unlabeled data. In this paper, we propose a novel framework, uncertainty-aware multi-view co-training (UMCT), to address semi-supervised learning on 3D data, such as volumetric data from medical imaging. In our work, co-training is achieved by exploiting multi-viewpoint consistency of 3D data. We generate different views by rotating or permuting the 3D data and utilize asymmetrical 3D kernels to encourage diversified features in different sub-networks. In addition, we propose an uncertainty-weighted label fusion mechanism to estimate the reliability of each view's prediction with Bayesian deep learning. As one view requires the supervision from other views in co-training, our self-adaptive approach computes a confidence score for the prediction of each unlabeled sample in order to assign a reliable pseudo label. Thus, our approach can take advantage of unlabeled data during training. We show the effectiveness of our proposed semi-supervised method on several public datasets from medical image segmentation tasks (NIH pancreas & LiTS liver tumor dataset). Meanwhile, a fully-supervised method based on our approach achieved state-of-the-art performances on both the LiTS liver tumor segmentation and the Medical Segmentation Decathlon (MSD) challenge, demonstrating the robustness and value of our framework, even when fully supervised training is feasible.



