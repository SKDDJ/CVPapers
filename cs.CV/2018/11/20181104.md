# Arxiv Papers in cs.CV on 2018-11-04
### A dataset for benchmarking vision-based localization at intersections
- **Arxiv ID**: http://arxiv.org/abs/1811.01306v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.01306v1)
- **Published**: 2018-11-04 01:10:09+00:00
- **Updated**: 2018-11-04 01:10:09+00:00
- **Authors**: Augusto L. Ballardini, Daniele Cattaneo, Domenico G. Sorrenti
- **Comment**: 7 pages, 26 figures, report describing the work done to prepare a
  dataset of sequences of a vehicle approaching an intersection, using the
  sequences recorded in the KITTI dataset
- **Journal**: None
- **Summary**: In this report we present the work performed in order to build a dataset for benchmarking vision-based localization at intersections, i.e., a set of stereo video sequences taken from a road vehicle that is approaching an intersection, altogether with a reliable measure of the observer position. This report is meant to complement our paper "Vision-Based Localization at Intersections using Digital Maps" submitted to ICRA2019. It complements the paper because the paper uses the dataset, but it had no space for describing the work done to obtain it. Moreover, the dataset is of interest for all those tackling the task of online localization at intersections for road vehicles, e.g., for a quantitative comparison with the proposal in our submitted paper, and it is therefore appropriate to put the dataset description in a separate report. We considered all datasets from road vehicles that we could find as for the end of August 2018. After our evaluation, we kept only sub-sequences from the KITTI dataset. In the future we will increase the collection of sequences with data from our vehicle.



### RA-UNet: A hybrid deep attention-aware network to extract liver and tumor in CT scans
- **Arxiv ID**: http://arxiv.org/abs/1811.01328v1
- **DOI**: 10.3389/fbioe.2020.605132
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.01328v1)
- **Published**: 2018-11-04 06:18:14+00:00
- **Updated**: 2018-11-04 06:18:14+00:00
- **Authors**: Qiangguo Jin, Zhaopeng Meng, Changming Sun, Leyi Wei, Ran Su
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic extraction of liver and tumor from CT volumes is a challenging task due to their heterogeneous and diffusive shapes. Recently, 2D and 3D deep convolutional neural networks have become popular in medical image segmentation tasks because of the utilization of large labeled datasets to learn hierarchical features. However, 3D networks have some drawbacks due to their high cost on computational resources. In this paper, we propose a 3D hybrid residual attention-aware segmentation method, named RA-UNet, to precisely extract the liver volume of interests (VOI) and segment tumors from the liver VOI. The proposed network has a basic architecture as a 3D U-Net which extracts contextual information combining low-level feature maps with high-level ones. Attention modules are stacked so that the attention-aware features change adaptively as the network goes "very deep" and this is made possible by residual learning. This is the first work that an attention residual mechanism is used to process medical volumetric images. We evaluated our framework on the public MICCAI 2017 Liver Tumor Segmentation dataset and the 3DIRCADb dataset. The results show that our architecture outperforms other state-of-the-art methods. We also extend our RA-UNet to brain tumor segmentation on the BraTS2018 and BraTS2017 datasets, and the results indicate that RA-UNet achieves good performance on a brain tumor segmentation task as well.



### Improving GAN with neighbors embedding and gradient matching
- **Arxiv ID**: http://arxiv.org/abs/1811.01333v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.01333v1)
- **Published**: 2018-11-04 08:06:59+00:00
- **Updated**: 2018-11-04 08:06:59+00:00
- **Authors**: Ngoc-Trung Tran, Tuan-Anh Bui, Ngai-Man Cheung
- **Comment**: Published as a conference paper at AAAI 2019
- **Journal**: None
- **Summary**: We propose two new techniques for training Generative Adversarial Networks (GANs). Our objectives are to alleviate mode collapse in GAN and improve the quality of the generated samples. First, we propose neighbor embedding, a manifold learning-based regularization to explicitly retain local structures of latent samples in the generated samples. This prevents generator from producing nearly identical data samples from different latent samples, and reduces mode collapse. We propose an inverse t-SNE regularizer to achieve this. Second, we propose a new technique, gradient matching, to align the distributions of the generated samples and the real samples. As it is challenging to work with high-dimensional sample distributions, we propose to align these distributions through the scalar discriminator scores. We constrain the difference between the discriminator scores of the real samples and generated ones. We further constrain the difference between the gradients of these discriminator scores. We derive these constraints from Taylor approximations of the discriminator function. We perform experiments to demonstrate that our proposed techniques are computationally simple and easy to be incorporated in existing systems. When Gradient matching and Neighbour embedding are applied together, our GN-GAN achieves outstanding results on 1D/2D synthetic, CIFAR-10 and STL-10 datasets, e.g. FID score of $30.80$ for the STL-10 dataset. Our code is available at: https://github.com/tntrung/gan



### Bi-Real Net: Binarizing Deep Network Towards Real-Network Performance
- **Arxiv ID**: http://arxiv.org/abs/1811.01335v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.01335v2)
- **Published**: 2018-11-04 08:15:26+00:00
- **Updated**: 2019-09-04 03:23:31+00:00
- **Authors**: Zechun Liu, Wenhan Luo, Baoyuan Wu, Xin Yang, Wei Liu, Kwang-Ting Cheng
- **Comment**: To appear in IJCV. It is the journal extension of our ECCV paper:
  arXiv:1808.00278. Codes are available on:
  https://github.com/liuzechun/Bi-Real-net
- **Journal**: None
- **Summary**: In this paper, we study 1-bit convolutional neural networks (CNNs), of which both the weights and activations are binary. While efficient, the lacking of representational capability and the training difficulty impede 1-bit CNNs from performing as well as real-valued networks. We propose Bi-Real net with a novel training algorithm to tackle these two challenges. To enhance the representational capability, we propagate the real-valued activations generated by each 1-bit convolution via a parameter-free shortcut. To address the training difficulty, we propose a training algorithm using a tighter approximation to the derivative of the sign function, a magnitude-aware gradient for weight updating, a better initialization method, and a two-step scheme for training a deep network. Experiments on ImageNet show that an 18-layer Bi-Real net with the proposed training algorithm achieves 56.4% top-1 classification accuracy, which is 10% higher than the state-of-the-arts (e.g., XNOR-Net) with greater memory saving and lower computational cost. Bi-Real net is also the first to scale up 1-bit CNNs to an ultra-deep network with 152 layers, and achieves 64.5% top-1 accuracy on ImageNet. A 50-layer Bi-Real net shows comparable performance to a real-valued network on the depth estimation task with only a 0.3% accuracy gap.



### Underwater Single Image Color Restoration Using Haze-Lines and a New Quantitative Dataset
- **Arxiv ID**: http://arxiv.org/abs/1811.01343v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.01343v3)
- **Published**: 2018-11-04 09:16:38+00:00
- **Updated**: 2019-03-24 20:38:45+00:00
- **Authors**: Dana Berman, Deborah Levy, Shai Avidan, Tali Treibitz
- **Comment**: None
- **Journal**: None
- **Summary**: Underwater images suffer from color distortion and low contrast, because light is attenuated while it propagates through water. Attenuation under water varies with wavelength, unlike terrestrial images where attenuation is assumed to be spectrally uniform. The attenuation depends both on the water body and the 3D structure of the scene, making color restoration difficult.   Unlike existing single underwater image enhancement techniques, our method takes into account multiple spectral profiles of different water types. By estimating just two additional global parameters: the attenuation ratios of the blue-red and blue-green color channels, the problem is reduced to single image dehazing, where all color channels have the same attenuation coefficients. Since the water type is unknown, we evaluate different parameters out of an existing library of water types. Each type leads to a different restored image and the best result is automatically chosen based on color distribution.   We collected a dataset of images taken in different locations with varying water properties, showing color charts in the scenes. Moreover, to obtain ground truth, the 3D structure of the scene was calculated based on stereo imaging. This dataset enables a quantitative evaluation of restoration algorithms on natural images and shows the advantage of our method.



### A Deep One-Shot Network for Query-based Logo Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1811.01395v5
- **DOI**: 10.1016/j.patcog.2019.106965
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.01395v5)
- **Published**: 2018-11-04 16:16:45+00:00
- **Updated**: 2019-07-13 18:55:59+00:00
- **Authors**: Ayan Kumar Bhunia, Ankan Kumar Bhunia, Shuvozit Ghose, Abhirup Das, Partha Pratim Roy, Umapada Pal
- **Comment**: Accepted in Pattern Recognition, Elsevier(2019)
- **Journal**: None
- **Summary**: Logo detection in real-world scene images is an important problem with applications in advertisement and marketing. Existing general-purpose object detection methods require large training data with annotations for every logo class. These methods do not satisfy the incremental demand of logo classes necessary for practical deployment since it is practically impossible to have such annotated data for new unseen logo. In this work, we develop an easy-to-implement query-based logo detection and localization system by employing a one-shot learning technique. Given an image of a query logo, our model searches for it within a given target image and predicts the possible location of the logo by estimating a binary segmentation mask. The proposed model consists of a conditional branch and a segmentation branch. The former gives a conditional latent representation of the given query logo which is combined with feature maps of the segmentation branch at multiple scales in order to find the matching position of the query logo in a target image, should it be present. Feature matching between the latent query representation and multi-scale feature maps of segmentation branch using simple concatenation operation followed by 1x1 convolution layer makes our model scale-invariant. Despite its simplicity, our query-based logo retrieval framework achieved superior performance in FlickrLogos-32 and TopLogos-10 dataset over different existing baselines.



### Handwriting Recognition in Low-resource Scripts using Adversarial Learning
- **Arxiv ID**: http://arxiv.org/abs/1811.01396v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.01396v5)
- **Published**: 2018-11-04 16:24:09+00:00
- **Updated**: 2019-02-25 10:40:19+00:00
- **Authors**: Ayan Kumar Bhunia, Abhirup Das, Ankan Kumar Bhunia, Perla Sai Raj Kishore, Partha Pratim Roy
- **Comment**: IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2019
- **Journal**: None
- **Summary**: Handwritten Word Recognition and Spotting is a challenging field dealing with handwritten text possessing irregular and complex shapes. The design of deep neural network models makes it necessary to extend training datasets in order to introduce variations and increase the number of samples; word-retrieval is therefore very difficult in low-resource scripts. Much of the existing literature comprises preprocessing strategies which are seldom sufficient to cover all possible variations. We propose the Adversarial Feature Deformation Module (AFDM) that learns ways to elastically warp extracted features in a scalable manner. The AFDM is inserted between intermediate layers and trained alternatively with the original framework, boosting its capability to better learn highly informative features rather than trivial ones. We test our meta-framework, which is built on top of popular word-spotting and word-recognition frameworks and enhanced by the AFDM, not only on extensive Latin word datasets but also sparser Indic scripts. We record results for varying training data sizes, and observe that our enhanced network generalizes much better in the low-data regime; the overall word-error rates and mAP scores are observed to improve as well.



### Texture Synthesis Guided Deep Hashing for Texture Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1811.01401v5
- **DOI**: 10.1109/WACV.2019.00070
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.01401v5)
- **Published**: 2018-11-04 16:41:47+00:00
- **Updated**: 2019-06-05 13:45:14+00:00
- **Authors**: Ayan Kumar Bhunia, Perla Sai Raj Kishore, Pranay Mukherjee, Abhirup Das, Partha Pratim Roy
- **Comment**: IEEE Winter Conference on Applications of Computer Vision (WACV),
  2019 Video Presentation: https://www.youtube.com/watch?v=tXaXTGhzaJo
- **Journal**: None
- **Summary**: With the large-scale explosion of images and videos over the internet, efficient hashing methods have been developed to facilitate memory and time efficient retrieval of similar images. However, none of the existing works uses hashing to address texture image retrieval mostly because of the lack of sufficiently large texture image databases. Our work addresses this problem by developing a novel deep learning architecture that generates binary hash codes for input texture images. For this, we first pre-train a Texture Synthesis Network (TSN) which takes a texture patch as input and outputs an enlarged view of the texture by injecting newer texture content. Thus it signifies that the TSN encodes the learnt texture specific information in its intermediate layers. In the next stage, a second network gathers the multi-scale feature representations from the TSN's intermediate layers using channel-wise attention, combines them in a progressive manner to a dense continuous representation which is finally converted into a binary hash code with the help of individual and pairwise label information. The new enlarged texture patches also help in data augmentation to alleviate the problem of insufficient texture data and are used to train the second stage of the network. Experiments on three public texture image retrieval datasets indicate the superiority of our texture synthesis guided hashing approach over current state-of-the-art methods.



### DeepKey: Towards End-to-End Physical Key Replication From a Single Photograph
- **Arxiv ID**: http://arxiv.org/abs/1811.01405v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.01405v1)
- **Published**: 2018-11-04 17:25:23+00:00
- **Updated**: 2018-11-04 17:25:23+00:00
- **Authors**: Rory Smith, Tilo Burghardt
- **Comment**: 14 pages, 12 figures
- **Journal**: None
- **Summary**: This paper describes DeepKey, an end-to-end deep neural architecture capable of taking a digital RGB image of an 'everyday' scene containing a pin tumbler key (e.g. lying on a table or carpet) and fully automatically inferring a printable 3D key model. We report on the key detection performance and describe how candidates can be transformed into physical prints. We show an example opening a real-world lock. Our system is described in detail, providing a breakdown of all components including key detection, pose normalisation, bitting segmentation and 3D model inference. We provide an in-depth evaluation and conclude by reflecting on limitations, applications, potential security risks and societal impact. We contribute the DeepKey Datasets of 5, 300+ images covering a few test keys with bounding boxes, pose and unaligned mask data.



### False Positive Reduction in Lung Computed Tomography Images using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1811.01424v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.01424v1)
- **Published**: 2018-11-04 19:46:48+00:00
- **Updated**: 2018-11-04 19:46:48+00:00
- **Authors**: Gorkem Polat, Ugur Halici, Yesim Serinagaoglu Dogrusoz
- **Comment**: 4 pages
- **Journal**: None
- **Summary**: Recent studies have shown that lung cancer screening using annual low-dose computed tomography (CT) reduces lung cancer mortality by 20% compared to traditional chest radiography. Therefore, CT lung screening has started to be used widely all across the world. However, analyzing these images is a serious burden for radiologists. In this study, we propose a novel and simple framework that analyzes CT lung screenings using convolutional neural networks (CNNs) and reduces false positives. Our framework shows that even non-complex architectures are very powerful to classify 3D nodule data when compared to traditional methods. We also use different fusions in order to show their power and effect on the overall score. 3D CNNs are preferred over 2D CNNs because data are in 3D, and 2D convolutional operations may result in information loss. Mini-batch is used in order to overcome class-imbalance. Proposed framework has been validated according to the LUNA16 challenge evaluation and got score of 0.786, which is the average sensitivity values at seven predefined false positive (FP) points.



