# Arxiv Papers in cs.CV on 2018-11-16
### Detecting The Objects on The Road Using Modular Lightweight Network
- **Arxiv ID**: http://arxiv.org/abs/1811.06641v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.06641v1)
- **Published**: 2018-11-16 01:14:59+00:00
- **Updated**: 2018-11-16 01:14:59+00:00
- **Authors**: Sen Cao, Yazhou Liu, Pongsak Lasang, Shengmei Shen
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a modular lightweight network model for road objects detection, such as car, pedestrian and cyclist, especially when they are far away from the camera and their sizes are small. Great advances have been made for the deep networks, but small objects detection is still a challenging task. In order to solve this problem, majority of existing methods utilize complicated network or bigger image size, which generally leads to higher computation cost. The proposed network model is referred to as modular feature fusion detector (MFFD), using a fast and efficient network architecture for detecting small objects. The contribution lies in the following aspects: 1) Two base modules have been designed for efficient computation: Front module reduce the information loss from raw input images; Tinier module decrease model size and computation cost, while ensuring the detection accuracy. 2) By stacking the base modules, we design a context features fusion framework for multi-scale object detection. 3) The propose method is efficient in terms of model size and computation cost, which is applicable for resource limited devices, such as embedded systems for advanced driver assistance systems (ADAS). Comparisons with the state-of-the-arts on the challenging KITTI dataset reveal the superiority of the proposed method. Especially, 100 fps can be achieved on the embedded GPUs such as Jetson TX2.



### Optical Flow Based Background Subtraction with a Moving Camera: Application to Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1811.06660v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1811.06660v1)
- **Published**: 2018-11-16 02:36:41+00:00
- **Updated**: 2018-11-16 02:36:41+00:00
- **Authors**: Sotirios Diamantas, Kostas Alexis
- **Comment**: 5 pages, 4 figures, presubmission
- **Journal**: None
- **Summary**: In this research we present a novel algorithm for background subtraction using a moving camera. Our algorithm is based purely on visual information obtained from a camera mounted on an electric bus, operating in downtown Reno which automatically detects moving objects of interest with the view to provide a fully autonomous vehicle. In our approach we exploit the optical flow vectors generated by the motion of the camera while keeping parameter assumptions a minimum. At first, we estimate the Focus of Expansion, which is used to model and simulate 3D points given the intrinsic parameters of the camera, and perform multiple linear regression to estimate the regression equation parameters and implement on the real data set of every frame to identify moving objects. We validated our algorithm using data taken from a common bus route.



### Generative Model for Material Experiments Based on Prior Knowledge and Attention Mechanism
- **Arxiv ID**: http://arxiv.org/abs/1811.07982v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1811.07982v1)
- **Published**: 2018-11-16 02:40:00+00:00
- **Updated**: 2018-11-16 02:40:00+00:00
- **Authors**: Mincong Luo, Xinfu He, Li Liu
- **Comment**: Accepted by NIPS2018 MMLM workshop
- **Journal**: None
- **Summary**: Material irradiation experiment is dangerous and complex, thus it requires those with a vast advanced expertise to process the images and data manually. In this paper, we propose a generative adversarial model based on prior knowledge and attention mechanism to achieve the generation of irradiated material images (data-to-image model), and a prediction model for corresponding industrial performance (image-to-data model). With the proposed models, researchers can skip the dangerous and complex irradiation experiments and obtain the irradiation images and industrial performance parameters directly by inputing some experimental parameters only. We also introduce a new dataset ISMD which contains 22000 irradiated images with 22,143 sets of corresponding parameters. Our model achieved high quality results by compared with several baseline models. The evaluation and detailed analysis are also performed.



### Ground Plane Polling for 6DoF Pose Estimation of Objects on the Road
- **Arxiv ID**: http://arxiv.org/abs/1811.06666v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.06666v4)
- **Published**: 2018-11-16 03:23:12+00:00
- **Updated**: 2020-02-07 23:53:07+00:00
- **Authors**: Akshay Rangesh, Mohan M. Trivedi
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces an approach to produce accurate 3D detection boxes for objects on the ground using single monocular images. We do so by merging 2D visual cues, 3D object dimensions, and ground plane constraints to produce boxes that are robust against small errors and incorrect predictions. First, we train a single-shot convolutional neural network (CNN) that produces multiple visual and geometric cues of interest: 2D bounding boxes, 2D keypoints of interest, coarse object orientations and object dimensions. Subsets of these cues are then used to poll probable ground planes from a pre-computed database of ground planes, to identify the "best fit" plane with highest consensus. Once identified, the "best fit" plane provides enough constraints to successfully construct the desired 3D detection box, without directly predicting the 6DoF pose of the object. The entire ground plane polling (GPP) procedure is constructed as a non-parametrized layer of the CNN that outputs the desired "best fit" plane and the corresponding 3D keypoints, which together define the final 3D bounding box. Doing so allows us to poll thousands of different ground plane configurations without adding considerable overhead, while also creating a single CNN that directly produces the desired output without the need for post processing. We evaluate our method on the 2D detection and orientation estimation benchmark from the challenging KITTI dataset, and provide additional comparisons for 3D metrics of importance. This single-stage, single-pass CNN results in superior localization and orientation estimation compared to more complex and computationally expensive monocular approaches.



### Composite Binary Decomposition Networks
- **Arxiv ID**: http://arxiv.org/abs/1811.06668v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1811.06668v1)
- **Published**: 2018-11-16 03:29:34+00:00
- **Updated**: 2018-11-16 03:29:34+00:00
- **Authors**: You Qiaoben, Zheng Wang, Jianguo Li, Yinpeng Dong, Yu-Gang Jiang, Jun Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Binary neural networks have great resource and computing efficiency, while suffer from long training procedure and non-negligible accuracy drops, when comparing to the full-precision counterparts. In this paper, we propose the composite binary decomposition networks (CBDNet), which first compose real-valued tensor of each layer with a limited number of binary tensors, and then decompose some conditioned binary tensors into two low-rank binary tensors, so that the number of parameters and operations are greatly reduced comparing to the original ones. Experiments demonstrate the effectiveness of the proposed method, as CBDNet can approximate image classification network ResNet-18 using 5.25 bits, VGG-16 using 5.47 bits, DenseNet-121 using 5.72 bits, object detection networks SSD300 using 4.38 bits, and semantic segmentation networks SegNet using 5.18 bits, all with minor accuracy drops.



### HSCS: Hierarchical Sparsity Based Co-saliency Detection for RGBD Images
- **Arxiv ID**: http://arxiv.org/abs/1811.06679v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.06679v1)
- **Published**: 2018-11-16 05:19:24+00:00
- **Updated**: 2018-11-16 05:19:24+00:00
- **Authors**: Runmin Cong, Jianjun Lei, Huazhu Fu, Qingming Huang, Xiaochun Cao, Nam Ling
- **Comment**: 11 pages, 5 figures, Accepted by IEEE Transactions on Multimedia,
  https://rmcong.github.io/
- **Journal**: None
- **Summary**: Co-saliency detection aims to discover common and salient objects in an image group containing more than two relevant images. Moreover, depth information has been demonstrated to be effective for many computer vision tasks. In this paper, we propose a novel co-saliency detection method for RGBD images based on hierarchical sparsity reconstruction and energy function refinement. With the assistance of the intra saliency map, the inter-image correspondence is formulated as a hierarchical sparsity reconstruction framework. The global sparsity reconstruction model with a ranking scheme focuses on capturing the global characteristics among the whole image group through a common foreground dictionary. The pairwise sparsity reconstruction model aims to explore the corresponding relationship between pairwise images through a set of pairwise dictionaries. In order to improve the intra-image smoothness and inter-image consistency, an energy function refinement model is proposed, which includes the unary data term, spatial smooth term, and holistic consistency term. Experiments on two RGBD co-saliency detection benchmarks demonstrate that the proposed method outperforms the state-of-the-art algorithms both qualitatively and quantitatively.



### DeRPN: Taking a further step toward more general object detection
- **Arxiv ID**: http://arxiv.org/abs/1811.06700v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.06700v1)
- **Published**: 2018-11-16 08:25:52+00:00
- **Updated**: 2018-11-16 08:25:52+00:00
- **Authors**: Lele Xie, Yuliang Liu, Lianwen Jin, Zecheng Xie
- **Comment**: 8pages, 4 figures, 6 tables, accepted to appear in AAAI 2019
- **Journal**: None
- **Summary**: Most current detection methods have adopted anchor boxes as regression references. However, the detection performance is sensitive to the setting of the anchor boxes. A proper setting of anchor boxes may vary significantly across different datasets, which severely limits the universality of the detectors. To improve the adaptivity of the detectors, in this paper, we present a novel dimension-decomposition region proposal network (DeRPN) that can perfectly displace the traditional Region Proposal Network (RPN). DeRPN utilizes an anchor string mechanism to independently match object widths and heights, which is conducive to treating variant object shapes. In addition, a novel scale-sensitive loss is designed to address the imbalanced loss computations of different scaled objects, which can avoid the small objects being overwhelmed by larger ones. Comprehensive experiments conducted on both general object detection datasets (Pascal VOC 2007, 2012 and MS COCO) and scene text detection datasets (ICDAR 2013 and COCO-Text) all prove that our DeRPN can significantly outperform RPN. It is worth mentioning that the proposed DeRPN can be employed directly on different models, tasks, and datasets without any modifications of hyperparameters or specialized optimization, which further demonstrates its adaptivity. The code will be released at https://github.com/HCIILAB/DeRPN.



### DropFilter: A Novel Regularization Method for Learning Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1811.06783v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.06783v2)
- **Published**: 2018-11-16 12:40:39+00:00
- **Updated**: 2018-11-19 01:28:42+00:00
- **Authors**: Hengyue Pan, Hui Jiang, Xin Niu, Yong Dou
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: The past few years have witnessed the fast development of different regularization methods for deep learning models such as fully-connected deep neural networks (DNNs) and Convolutional Neural Networks (CNNs). Most of previous methods mainly consider to drop features from input data and hidden layers, such as Dropout, Cutout and DropBlocks. DropConnect select to drop connections between fully-connected layers. By randomly discard some features or connections, the above mentioned methods control the overfitting problem and improve the performance of neural networks. In this paper, we proposed two novel regularization methods, namely DropFilter and DropFilter-PLUS, for the learning of CNNs. Different from the previous methods, DropFilter and DropFilter-PLUS selects to modify the convolution filters. For DropFilter-PLUS, we find a suitable way to accelerate the learning process based on theoretical analysis. Experimental results on MNIST show that using DropFilter and DropFilter-PLUS may improve performance on image classification tasks.



### Evaluating Uncertainty Quantification in End-to-End Autonomous Driving Control
- **Arxiv ID**: http://arxiv.org/abs/1811.06817v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.06817v1)
- **Published**: 2018-11-16 14:30:30+00:00
- **Updated**: 2018-11-16 14:30:30+00:00
- **Authors**: Rhiannon Michelmore, Marta Kwiatkowska, Yarin Gal
- **Comment**: 7 pages, 6 figures
- **Journal**: None
- **Summary**: A rise in popularity of Deep Neural Networks (DNNs), attributed to more powerful GPUs and widely available datasets, has seen them being increasingly used within safety-critical domains. One such domain, self-driving, has benefited from significant performance improvements, with millions of miles having been driven with no human intervention. Despite this, crashes and erroneous behaviours still occur, in part due to the complexity of verifying the correctness of DNNs and a lack of safety guarantees.   In this paper, we demonstrate how quantitative measures of uncertainty can be extracted in real-time, and their quality evaluated in end-to-end controllers for self-driving cars. To this end we utilise a recent method for gathering approximate uncertainty information from DNNs without changing the network's architecture. We propose evaluation techniques for the uncertainty on two separate architectures which use the uncertainty to predict crashes up to five seconds in advance. We find that mutual information, a measure of uncertainty in classification networks, is a promising indicator of forthcoming crashes.



### Saliency Supervision: An Intuitive and Effective Approach for Pain Intensity Regression
- **Arxiv ID**: http://arxiv.org/abs/1811.07987v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1811.07987v1)
- **Published**: 2018-11-16 15:18:09+00:00
- **Updated**: 2018-11-16 15:18:09+00:00
- **Authors**: Conghui Li, Zhaocheng Zhu, Yuming Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Getting pain intensity from face images is an important problem in autonomous nursing systems. However, due to the limitation in data sources and the subjectiveness in pain intensity values, it is hard to adopt modern deep neural networks for this problem without domain-specific auxiliary design. Inspired by human vision priori, we propose a novel approach called saliency supervision, where we directly regularize deep networks to focus on facial area that is discriminative for pain regression. Through alternative training between saliency supervision and global loss, our method can learn sparse and robust features, which is proved helpful for pain intensity regression. We verified saliency supervision with face-verification network backbone on the widely-used dataset, and achieved state-of-art performance without bells and whistles. Our saliency supervision is intuitive in spirit, yet effective in performance. We believe such saliency supervision is essential in dealing with ill-posed datasets, and has potential in a wide range of vision tasks.



### Anomaly Detection using Deep Learning based Image Completion
- **Arxiv ID**: http://arxiv.org/abs/1811.06861v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/1811.06861v1)
- **Published**: 2018-11-16 15:36:28+00:00
- **Updated**: 2018-11-16 15:36:28+00:00
- **Authors**: Matthias Haselmann, Dieter P. Gruber, Paul Tabatabai
- **Comment**: 6 pages, 5 figures, Accepted for publication by IEEE, 17th
  International Conference on Machine Learning and Applications (ICMLA) 2018
- **Journal**: None
- **Summary**: Automated surface inspection is an important task in many manufacturing industries and often requires machine learning driven solutions. Supervised approaches, however, can be challenging, since it is often difficult to obtain large amounts of labeled training data. In this work, we instead perform one-class unsupervised learning on fault-free samples by training a deep convolutional neural network to complete images whose center regions are cut out. Since the network is trained exclusively on fault-free data, it completes the image patches with a fault-free version of the missing image region. The pixel-wise reconstruction error within the cut out region is an anomaly image which can be used for anomaly detection. Results on surface images of decorated plastic parts demonstrate that this approach is suitable for detection of visible anomalies and moreover surpasses all other tested methods.



### Cost-Aware Fine-Grained Recognition for IoTs Based on Sequential Fixations
- **Arxiv ID**: http://arxiv.org/abs/1811.06868v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.06868v2)
- **Published**: 2018-11-16 15:41:24+00:00
- **Updated**: 2019-08-08 10:52:54+00:00
- **Authors**: Hanxiao Wang, Venkatesh Saligrama, Stan Sclaroff, Vitaly Ablavsky
- **Comment**: None
- **Journal**: None
- **Summary**: We consider the problem of fine-grained classification on an edge camera device that has limited power. The edge device must sparingly interact with the cloud to minimize communication bits to conserve power, and the cloud upon receiving the edge inputs returns a classification label. To deal with fine-grained classification, we adopt the perspective of sequential fixation with a foveated field-of-view to model cloud-edge interactions. We propose a novel deep reinforcement learning-based foveation model, DRIFT, that sequentially generates and recognizes mixed-acuity images.Training of DRIFT requires only image-level category labels and encourages fixations to contain task-relevant information, while maintaining data efficiency. Specifically, wetrain a foveation actor network with a novel Deep Deterministic Policy Gradient by Conditioned Critic and Coaching (DDPGC3) algorithm. In addition, we propose to shape the reward to provide informative feedback after each fixation to better guide RL training. We demonstrate the effectiveness of DRIFT on this task by evaluating on five fine-grained classification benchmark datasets, and show that the proposed approach achieves state-of-the-art performance with over 3X reduction in transmitted pixels.



### Residual Convolutional Neural Network Revisited with Active Weighted Mapping
- **Arxiv ID**: http://arxiv.org/abs/1811.06878v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.06878v1)
- **Published**: 2018-11-16 15:51:20+00:00
- **Updated**: 2018-11-16 15:51:20+00:00
- **Authors**: Jung HyoungHo, Lee Ryong, Lee Sanghwan, Hwang Wonjun
- **Comment**: None
- **Journal**: None
- **Summary**: In visual recognition, the key to the performance improvement of ResNet is the success in establishing the stack of deep sequential convolutional layers using identical mapping by a shortcut connection. It results in multiple paths of data flow under a network and the paths are merged with the equal weights. However, it is questionable whether it is correct to use the fixed and predefined weights at the mapping units of all paths. In this paper, we introduce the active weighted mapping method which infers proper weight values based on the characteristic of input data on the fly. The weight values of each mapping unit are not fixed but changed as the input image is changed, and the most proper weight values for each mapping unit are derived according to the input image. For this purpose, channel-wise information is embedded from both the shortcut connection and convolutional block, and then the fully connected layers are used to estimate the weight values for the mapping units. We train the backbone network and the proposed module alternately for a more stable learning of the proposed method. Results of the extensive experiments show that the proposed method works successfully on the various backbone architectures from ResNet to DenseNet. We also verify the superiority and generality of the proposed method on various datasets in comparison with the baseline.



### The Perfect Match: 3D Point Cloud Matching with Smoothed Densities
- **Arxiv ID**: http://arxiv.org/abs/1811.06879v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.06879v3)
- **Published**: 2018-11-16 15:53:02+00:00
- **Updated**: 2019-12-02 12:08:09+00:00
- **Authors**: Zan Gojcic, Caifa Zhou, Jan D. Wegner, Andreas Wieser
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: We propose 3DSmoothNet, a full workflow to match 3D point clouds with a siamese deep learning architecture and fully convolutional layers using a voxelized smoothed density value (SDV) representation. The latter is computed per interest point and aligned to the local reference frame (LRF) to achieve rotation invariance. Our compact, learned, rotation invariant 3D point cloud descriptor achieves 94.9% average recall on the 3DMatch benchmark data set, outperforming the state-of-the-art by more than 20 percent points with only 32 output dimensions. This very low output dimension allows for near realtime correspondence search with 0.1 ms per feature point on a standard PC. Our approach is sensor- and sceneagnostic because of SDV, LRF and learning highly descriptive features with fully convolutional layers. We show that 3DSmoothNet trained only on RGB-D indoor scenes of buildings achieves 79.0% average recall on laser scans of outdoor vegetation, more than double the performance of our closest, learning-based competitors. Code, data and pre-trained models are available online at https://github.com/zgojcic/3DSmoothNet.



### Learned Video Compression
- **Arxiv ID**: http://arxiv.org/abs/1811.06981v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.06981v1)
- **Published**: 2018-11-16 17:29:51+00:00
- **Updated**: 2018-11-16 17:29:51+00:00
- **Authors**: Oren Rippel, Sanjay Nair, Carissa Lew, Steve Branson, Alexander G. Anderson, Lubomir Bourdev
- **Comment**: None
- **Journal**: None
- **Summary**: We present a new algorithm for video coding, learned end-to-end for the low-latency mode. In this setting, our approach outperforms all existing video codecs across nearly the entire bitrate range. To our knowledge, this is the first ML-based method to do so.   We evaluate our approach on standard video compression test sets of varying resolutions, and benchmark against all mainstream commercial codecs, in the low-latency mode. On standard-definition videos, relative to our algorithm, HEVC/H.265, AVC/H.264 and VP9 typically produce codes up to 60% larger. On high-definition 1080p videos, H.265 and VP9 typically produce codes up to 20% larger, and H.264 up to 35% larger. Furthermore, our approach does not suffer from blocking artifacts and pixelation, and thus produces videos that are more visually pleasing.   We propose two main contributions. The first is a novel architecture for video compression, which (1) generalizes motion estimation to perform any learned compensation beyond simple translations, (2) rather than strictly relying on previously transmitted reference frames, maintains a state of arbitrary information learned by the model, and (3) enables jointly compressing all transmitted signals (such as optical flow and residual).   Secondly, we present a framework for ML-based spatial rate control: namely, a mechanism for assigning variable bitrates across space for each frame. This is a critical component for video coding, which to our knowledge had not been developed within a machine learning setting.



### Image Pre-processing Using OpenCV Library on MORPH-II Face Database
- **Arxiv ID**: http://arxiv.org/abs/1811.06934v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.06934v1)
- **Published**: 2018-11-16 17:36:33+00:00
- **Updated**: 2018-11-16 17:36:33+00:00
- **Authors**: Benjamin Yip, Rachel Towner, Troy Kling, Cuixian Chen, Yishi Wang
- **Comment**: Project for NSF-REU site at UNCW for Summer 2017
- **Journal**: None
- **Summary**: This paper outlines the steps taken toward pre-processing the 55,134 images of the MORPH-II non-commercial dataset. Following the introduction, section two begins with an overview of each step in the pre-processing pipeline. Section three expands upon each stage of the process and includes details on all calculations made, by providing the OpenCV functionality paired with each step. The last portion of this paper discusses the potential improvements to this pre-processing pipeline that became apparent in retrospect.



### Mode Variational LSTM Robust to Unseen Modes of Variation: Application to Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/1811.06937v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.06937v1)
- **Published**: 2018-11-16 17:40:13+00:00
- **Updated**: 2018-11-16 17:40:13+00:00
- **Authors**: Wissam J. Baddar, Yong Man Ro
- **Comment**: Accepted in AAAI-19
- **Journal**: None
- **Summary**: Spatio-temporal feature encoding is essential for encoding the dynamics in video sequences. Recurrent neural networks, particularly long short-term memory (LSTM) units, have been popular as an efficient tool for encoding spatio-temporal features in sequences. In this work, we investigate the effect of mode variations on the encoded spatio-temporal features using LSTMs. We show that the LSTM retains information related to the mode variation in the sequence, which is irrelevant to the task at hand (e.g. classification facial expressions). Actually, the LSTM forget mechanism is not robust enough to mode variations and preserves information that could negatively affect the encoded spatio-temporal features. We propose the mode variational LSTM to encode spatio-temporal features robust to unseen modes of variation. The mode variational LSTM modifies the original LSTM structure by adding an additional cell state that focuses on encoding the mode variation in the input sequence. To efficiently regulate what features should be stored in the additional cell state, additional gating functionality is also introduced. The effectiveness of the proposed mode variational LSTM is verified using the facial expression recognition task. Comparative experiments on publicly available datasets verified that the proposed mode variational LSTM outperforms existing methods. Moreover, a new dynamic facial expression dataset with different modes of variation, including various modes like pose and illumination variations, was collected to comprehensively evaluate the proposed mode variational LSTM. Experimental results verified that the proposed mode variational LSTM encodes spatio-temporal features robust to unseen modes of variation.



### Automatic Paper Summary Generation from Visual and Textual Information
- **Arxiv ID**: http://arxiv.org/abs/1811.06943v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1811.06943v1)
- **Published**: 2018-11-16 17:52:25+00:00
- **Updated**: 2018-11-16 17:52:25+00:00
- **Authors**: Shintaro Yamamoto, Yoshihiro Fukuhara, Ryota Suzuki, Shigeo Morishima, Hirokatsu Kataoka
- **Comment**: International Conference on Machine Vision 2018, Munich, Germany
- **Journal**: None
- **Summary**: Due to the recent boom in artificial intelligence (AI) research, including computer vision (CV), it has become impossible for researchers in these fields to keep up with the exponentially increasing number of manuscripts. In response to this situation, this paper proposes the paper summary generation (PSG) task using a simple but effective method to automatically generate an academic paper summary from raw PDF data. We realized PSG by combination of vision-based supervised components detector and language-based unsupervised important sentence extractor, which is applicable for a trained format of manuscripts. We show the quantitative evaluation of ability of simple vision-based components extraction, and the qualitative evaluation that our system can extract both visual item and sentence that are helpful for understanding. After processing via our PSG, the 979 manuscripts accepted by the Conference on Computer Vision and Pattern Recognition (CVPR) 2018 are available. It is believed that the proposed method will provide a better way for researchers to stay caught with important academic papers.



### Grasp2Vec: Learning Object Representations from Self-Supervised Grasping
- **Arxiv ID**: http://arxiv.org/abs/1811.06964v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1811.06964v2)
- **Published**: 2018-11-16 18:42:02+00:00
- **Updated**: 2018-11-19 18:25:51+00:00
- **Authors**: Eric Jang, Coline Devin, Vincent Vanhoucke, Sergey Levine
- **Comment**: CoRL 2018. Eric Jang and Coline Devin contributed equally to this
  work
- **Journal**: Proceedings of The 2nd Conference on Robot Learning, in PMLR
  87:99-112 (2018)
- **Summary**: Well structured visual representations can make robot learning faster and can improve generalization. In this paper, we study how we can acquire effective object-centric representations for robotic manipulation tasks without human labeling by using autonomous robot interaction with the environment. Such representation learning methods can benefit from continuous refinement of the representation as the robot collects more experience, allowing them to scale effectively without human intervention. Our representation learning approach is based on object persistence: when a robot removes an object from a scene, the representation of that scene should change according to the features of the object that was removed. We formulate an arithmetic relationship between feature vectors from this observation, and use it to learn a representation of scenes and objects that can then be used to identify object instances, localize them in the scene, and perform goal-directed grasping tasks where the robot must retrieve commanded objects from a bin. The same grasping procedure can also be used to automatically collect training data for our method, by recording images of scenes, grasping and removing an object, and recording the outcome. Our experiments demonstrate that this self-supervised approach for tasked grasping substantially outperforms direct reinforcement learning from images and prior representation learning methods.



### GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism
- **Arxiv ID**: http://arxiv.org/abs/1811.06965v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.06965v5)
- **Published**: 2018-11-16 18:43:28+00:00
- **Updated**: 2019-07-25 21:42:58+00:00
- **Authors**: Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Mia Xu Chen, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, Yonghui Wu, Zhifeng Chen
- **Comment**: 11 pages. Work in progress. Copyright 2018 by the authors
- **Journal**: None
- **Summary**: Scaling up deep neural network capacity has been known as an effective approach to improving model quality for several different machine learning tasks. In many cases, increasing model capacity beyond the memory limit of a single accelerator has required developing special algorithms or infrastructure. These solutions are often architecture-specific and do not transfer to other tasks. To address the need for efficient and task-independent model parallelism, we introduce GPipe, a pipeline parallelism library that allows scaling any network that can be expressed as a sequence of layers. By pipelining different sub-sequences of layers on separate accelerators, GPipe provides the flexibility of scaling a variety of different networks to gigantic sizes efficiently. Moreover, GPipe utilizes a novel batch-splitting pipelining algorithm, resulting in almost linear speedup when a model is partitioned across multiple accelerators. We demonstrate the advantages of GPipe by training large-scale neural networks on two different tasks with distinct network architectures: (i) Image Classification: We train a 557-million-parameter AmoebaNet model and attain a top-1 accuracy of 84.4% on ImageNet-2012, (ii) Multilingual Neural Machine Translation: We train a single 6-billion-parameter, 128-layer Transformer model on a corpus spanning over 100 languages and achieve better quality than all bilingual models.



### DARCCC: Detecting Adversaries by Reconstruction from Class Conditional Capsules
- **Arxiv ID**: http://arxiv.org/abs/1811.06969v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.06969v1)
- **Published**: 2018-11-16 18:52:58+00:00
- **Updated**: 2018-11-16 18:52:58+00:00
- **Authors**: Nicholas Frosst, Sara Sabour, Geoffrey Hinton
- **Comment**: To be presented at NIPS 2018 Workshop on Security in Machine Learning
- **Journal**: None
- **Summary**: We present a simple technique that allows capsule models to detect adversarial images. In addition to being trained to classify images, the capsule model is trained to reconstruct the images from the pose parameters and identity of the correct top-level capsule. Adversarial images do not look like a typical member of the predicted class and they have much larger reconstruction errors when the reconstruction is produced from the top-level capsule for that class. We show that setting a threshold on the $l2$ distance between the input image and its reconstruction from the winning capsule is very effective at detecting adversarial images for three different datasets. The same technique works quite well for CNNs that have been trained to reconstruct the image from all or part of the last hidden layer before the softmax. We then explore a stronger, white-box attack that takes the reconstruction error into account. This attack is able to fool our detection technique but in order to make the model change its prediction to another class, the attack must typically make the "adversarial" image resemble images of the other class.



### Data-Efficient Graph Embedding Learning for PCB Component Detection
- **Arxiv ID**: http://arxiv.org/abs/1811.06994v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.06994v2)
- **Published**: 2018-11-16 19:07:38+00:00
- **Updated**: 2018-11-20 05:16:40+00:00
- **Authors**: Chia-Wen Kuo, Jacob Ashmore, David Huggins, Zsolt Kira
- **Comment**: Paper accepted in WACV 2019. See
  https://drive.google.com/open?id=1VkS8n1mKvAWjEPkiOA28XgY6VNfrsWYo for
  supplementary materials
- **Journal**: None
- **Summary**: This paper presents a challenging computer vision task, namely the detection of generic components on a PCB, and a novel set of deep-learning methods that are able to jointly leverage the appearance of individual components and the propagation of information across the structure of the board to accurately detect and identify various types of components on a PCB. Due to the expense of manual data labeling, a highly unbalanced distribution of component types, and significant domain shift across boards, most earlier attempts based on traditional image processing techniques fail to generalize well to PCB images with various quality, lighting conditions, etc. Newer object detection pipelines such as Faster R-CNN, on the other hand, require a large amount of labeled data, do not deal with domain shift, and do not leverage structure. To address these issues, we propose a three stage pipeline in which a class-agnostic region proposal network is followed by a low-shot similarity prediction classifier. In order to exploit the data dependency within a PCB, we design a novel Graph Network block to refine the component features conditioned on each PCB. To the best of our knowledge, this is one of the earliest attempts to train a deep learning based model for such tasks, and we demonstrate improvements over recent graph networks for this task. We also provide in-depth analysis and discussion for this challenging task, pointing to future research.



### Coupling weak and strong supervision for classification of prostate cancer histopathology images
- **Arxiv ID**: http://arxiv.org/abs/1811.07013v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07013v1)
- **Published**: 2018-11-16 20:05:47+00:00
- **Updated**: 2018-11-16 20:05:47+00:00
- **Authors**: Eirini Arvaniti, Manfred Claassen
- **Comment**: Accepted in Medical Imaging meets NIPS Workshop, NIPS 2018
- **Journal**: None
- **Summary**: Automated grading of prostate cancer histopathology images is a challenging task, with one key challenge being the scarcity of annotations down to the level of regions of interest (strong labels), as typically the prostate cancer Gleason score is known only for entire tissue slides (weak labels). In this study, we focus on automated Gleason score assignment of prostate cancer whole-slide images on the basis of a large weakly-labeled dataset and a smaller strongly-labeled one. We efficiently leverage information from both label sources by jointly training a classifier on the two datasets and by introducing a gradient update scheme that assigns different relative importances to each training example, as a means of self-controlling the weak supervision signal. Our approach achieves superior performance when compared with standard Gleason scoring methods.



### Topology-Aware Non-Rigid Point Cloud Registration
- **Arxiv ID**: http://arxiv.org/abs/1811.07014v3
- **DOI**: 10.1109/TPAMI.2019.2940655
- **Categories**: **cs.CV**, cs.GR, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1811.07014v3)
- **Published**: 2018-11-16 20:08:47+00:00
- **Updated**: 2019-11-03 21:19:00+00:00
- **Authors**: Konstantinos Zampogiannis, Cornelia Fermuller, Yiannis Aloimonos
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we introduce a non-rigid registration pipeline for pairs of unorganized point clouds that may be topologically different. Standard warp field estimation algorithms, even under robust, discontinuity-preserving regularization, tend to produce erratic motion estimates on boundaries associated with `close-to-open' topology changes. We overcome this limitation by exploiting backward motion: in the opposite motion direction, a `close-to-open' event becomes `open-to-close', which is by default handled correctly. At the core of our approach lies a general, topology-agnostic warp field estimation algorithm, similar to those employed in recently introduced dynamic reconstruction systems from RGB-D input. We improve motion estimation on boundaries associated with topology changes in an efficient post-processing phase. Based on both forward and (inverted) backward warp hypotheses, we explicitly detect regions of the deformed geometry that undergo topological changes by means of local deformation criteria and broadly classify them as `contacts' or `separations'. Subsequently, the two motion hypotheses are seamlessly blended on a local basis, according to the type and proximity of detected events. Our method achieves state-of-the-art motion estimation accuracy on the MPI Sintel dataset. Experiments on a custom dataset with topological event annotations demonstrate the effectiveness of our pipeline in estimating motion on event boundaries, as well as promising performance in explicit topological event detection.



### Improving Rotated Text Detection with Rotation Region Proposal Networks
- **Arxiv ID**: http://arxiv.org/abs/1811.07031v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07031v1)
- **Published**: 2018-11-16 20:57:37+00:00
- **Updated**: 2018-11-16 20:57:37+00:00
- **Authors**: Jing Huang, Viswanath Sivakumar, Mher Mnatsakanyan, Guan Pang
- **Comment**: None
- **Journal**: None
- **Summary**: A significant number of images shared on social media platforms such as Facebook and Instagram contain text in various forms. It's increasingly becoming commonplace for bad actors to share misinformation, hate speech or other kinds of harmful content as text overlaid on images on such platforms. A scene-text understanding system should hence be able to handle text in various orientations that the adversary might use. Moreover, such a system can be incorporated into screen readers used to aid the visually impaired. In this work, we extend the scene-text extraction system at Facebook, Rosetta, to efficiently handle text in various orientations. Specifically, we incorporate the Rotation Region Proposal Networks (RRPN) in our text extraction pipeline and offer practical suggestions for building and deploying a model for detecting and recognizing text in arbitrary orientations efficiently. Experimental results show a significant improvement on detecting rotated text.



### BLeSS: Bio-inspired Low-level Spatiochromatic Similarity Assisted Image Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/1811.07044v1
- **DOI**: 10.1109/ICME.2016.7552874
- **Categories**: **eess.IV**, cs.CV, eess.SP, I.4
- **Links**: [PDF](http://arxiv.org/pdf/1811.07044v1)
- **Published**: 2018-11-16 21:49:38+00:00
- **Updated**: 2018-11-16 21:49:38+00:00
- **Authors**: Dogancan Temel, Ghassan AlRegib
- **Comment**: 7 pages, 3 figures, 3 tables
- **Journal**: 2016 IEEE International Conference on Multimedia and Expo (ICME),
  Seattle, WA, 2016, pp. 1-6
- **Summary**: This paper proposes a biologically-inspired low-level spatiochromatic-model-based similarity method (BLeSS) to assist full-reference image-quality estimators that originally oversimplify color perception processes. More specifically, the spatiochromatic model is based on spatial frequency, spatial orientation, and surround contrast effects. The assistant similarity method is used to complement image-quality estimators based on phase congruency, gradient magnitude, and spectral residual. The effectiveness of BLeSS is validated using FSIM, FSIMc and SR-SIM methods on LIVE, Multiply Distorted LIVE, and TID 2013 databases. In terms of Spearman correlation, BLeSS enhances the performance of all quality estimators in color-based degradations and the enhancement is at 100% for both feature- and spectral residual-based similarity methods. Moreover, BleSS significantly enhances the performance of SR-SIM and FSIM in the full TID 2013 database.



### Domain Adaptive Transfer Learning with Specialist Models
- **Arxiv ID**: http://arxiv.org/abs/1811.07056v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.07056v2)
- **Published**: 2018-11-16 22:52:27+00:00
- **Updated**: 2018-12-11 22:09:00+00:00
- **Authors**: Jiquan Ngiam, Daiyi Peng, Vijay Vasudevan, Simon Kornblith, Quoc V. Le, Ruoming Pang
- **Comment**: None
- **Journal**: None
- **Summary**: Transfer learning is a widely used method to build high performing computer vision models. In this paper, we study the efficacy of transfer learning by examining how the choice of data impacts performance. We find that more pre-training data does not always help, and transfer performance depends on a judicious choice of pre-training data. These findings are important given the continued increase in dataset sizes. We further propose domain adaptive transfer learning, a simple and effective pre-training method using importance weights computed based on the target dataset. Our method to compute importance weights follow from ideas in domain adaptation, and we show a novel application to transfer learning. Our methods achieve state-of-the-art results on multiple fine-grained classification datasets and are well-suited for use in practice.



### Relational Long Short-Term Memory for Video Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1811.07059v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07059v2)
- **Published**: 2018-11-16 23:03:23+00:00
- **Updated**: 2020-04-24 21:55:13+00:00
- **Authors**: Zexi Chen, Bharathkumar Ramachandra, Tianfu Wu, Ranga Raju Vatsavai
- **Comment**: None
- **Journal**: None
- **Summary**: Spatial and temporal relationships, both short-range and long-range, between objects in videos, are key cues for recognizing actions. It is a challenging problem to model them jointly. In this paper, we first present a new variant of Long Short-Term Memory, namely Relational LSTM, to address the challenge of relation reasoning across space and time between objects. In our Relational LSTM module, we utilize a non-local operation similar in spirit to the recently proposed non-local network to substitute the fully connected operation in the vanilla LSTM. By doing this, our Relational LSTM is capable of capturing long and short-range spatio-temporal relations between objects in videos in a principled way. Then, we propose a two-branch neural architecture consisting of the Relational LSTM module as the non-local branch and a spatio-temporal pooling based local branch. The local branch is utilized for capturing local spatial appearance and/or short-term motion features. The two branches are concatenated to learn video-level features from snippet-level ones which are then used for classification. Experimental results on UCF-101 and HMDB-51 datasets show that our model achieves state-of-the-art results among LSTM-based methods, while obtaining comparable performance with other state-of-the-art methods (which use not directly comparable schema). Further, on the more complex large-scale Charades dataset, we obtain a large 3.2% gain over state-of-the-art methods, verifying the effectiveness of our method in complex understanding.



### Assessing four Neural Networks on Handwritten Digit Recognition Dataset (MNIST)
- **Arxiv ID**: http://arxiv.org/abs/1811.08278v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08278v2)
- **Published**: 2018-11-16 23:55:57+00:00
- **Updated**: 2019-07-20 03:24:24+00:00
- **Authors**: Feiyang Chen, Nan Chen, Hanyang Mao, Hanlin Hu
- **Comment**: TPW course essay. arXiv admin note: text overlap with
  arXiv:1709.04219 by other authors
- **Journal**: None
- **Summary**: Although the image recognition has been a research topic for many years, many researchers still have a keen interest in it[1]. In some papers[2][3][4], however, there is a tendency to compare models only on one or two datasets, either because of time restraints or because the model is tailored to a specific task. Accordingly, it is hard to understand how well a certain model generalizes across image recognition field[6]. In this paper, we compare four neural networks on MNIST dataset[5] with different division. Among them, three are Convolutional Neural Networks (CNN)[7], Deep Residual Network (ResNet)[2] and Dense Convolutional Network (DenseNet)[3] respectively, and the other is our improvement on CNN baseline through introducing Capsule Network (CapsNet)[1] to image recognition area. We show that the previous models despite do a quite good job in this area, our retrofitting can be applied to get a better performance. The result obtained by CapsNet is an accuracy rate of 99.75\%, and it is the best result published so far. Another inspiring result is that CapsNet only needs a small amount of data to get excellent performance. Finally, we will apply CapsNet's ability to generalize in other image recognition field in the future.



