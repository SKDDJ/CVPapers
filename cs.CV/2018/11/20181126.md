# Arxiv Papers in cs.CV on 2018-11-26
### Foreground Clustering for Joint Segmentation and Localization in Videos and Images
- **Arxiv ID**: http://arxiv.org/abs/1811.10121v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.10121v1)
- **Published**: 2018-11-26 00:00:20+00:00
- **Updated**: 2018-11-26 00:00:20+00:00
- **Authors**: Abhishek Sharma
- **Comment**: In Proceedings of NIPS 2018
- **Journal**: None
- **Summary**: This paper presents a novel framework in which video/image segmentation and localization are cast into a single optimization problem that integrates information from low level appearance cues with that of high level localization cues in a very weakly supervised manner. The proposed framework leverages two representations at different levels, exploits the spatial relationship between bounding boxes and superpixels as linear constraints and simultaneously discriminates between foreground and background at bounding box and superpixel level. Different from previous approaches that mainly rely on discriminative clustering, we incorporate a foreground model that minimizes the histogram difference of an object across all image frames. Exploiting the geometric relation between the superpixels and bounding boxes enables the transfer of segmentation cues to improve localization output and vice-versa. Inclusion of the foreground model generalizes our discriminative framework to video data where the background tends to be similar and thus, not discriminative. We demonstrate the effectiveness of our unified framework on the YouTube Object video dataset, Internet Object Discovery dataset and Pascal VOC 2007.



### Artificial Retina Using A Hybrid Neural Network With Spatial Transform Capability
- **Arxiv ID**: http://arxiv.org/abs/1811.10126v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.ET, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1811.10126v1)
- **Published**: 2018-11-26 00:32:53+00:00
- **Updated**: 2018-11-26 00:32:53+00:00
- **Authors**: Richard Wood, Alexander McGlashan, C. B. Moon, W. Y. Kim
- **Comment**: 16 pages, 20 figures
- **Journal**: None
- **Summary**: This paper covers the design and programming of a hybrid (digital/analog) neural network to function as an artificial retina with the ability to perform a spatial discrete cosine transform. We describe the structure of the circuit, which uses an analog cell that is interlinked using a programmable digital array. The paper is broken into three main parts. First, we present the results of a Matlab simulation. Then we show the circuit simulation in Spice. This is followed by a demonstration of the practical device. This system has intentionally separated components with the specialty analog circuits being separated from the readily available digital field programmable gate array (FPGA) components. Further development includes the use of rapid manufacture-able organic electronics used for the analog components. The planned uses for this platform include crowd development of software that uses the underlying pulse based processing. The development package will include simulators in the form of Matlab and Spice type software platforms.



### FilterReg: Robust and Efficient Probabilistic Point-Set Registration using Gaussian Filter and Twist Parameterization
- **Arxiv ID**: http://arxiv.org/abs/1811.10136v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10136v3)
- **Published**: 2018-11-26 01:23:47+00:00
- **Updated**: 2019-07-16 17:34:47+00:00
- **Authors**: Wei Gao, Russ Tedrake
- **Comment**: CVPR 2019. The video demo and source code are on
  https://sites.google.com/view/filterreg/home
- **Journal**: None
- **Summary**: Probabilistic point-set registration methods have been gaining more attention for their robustness to noise, outliers and occlusions. However, these methods tend to be much slower than the popular iterative closest point (ICP) algorithms, which severely limits their usability. In this paper, we contribute a novel probabilistic registration method that achieves state-of-the-art robustness as well as substantially faster computational performance than modern ICP implementations. This is achieved using a rigorous yet computationally-efficient probabilistic formulation. Point-set registration is cast as a maximum likelihood estimation and solved using the EM algorithm. We show that with a simple augmentation, the E step can be formulated as a filtering problem, allowing us to leverage advances in efficient Gaussian filtering methods. We also propose a customized permutohedral filter for improved efficiency while retaining sufficient accuracy for our task. Additionally, we present a simple and efficient twist parameterization that generalizes our method to the registration of articulated and deformable objects. For articulated objects, the complexity of our method is almost independent of the Degrees Of Freedom (DOFs), which makes it highly efficient even for high DOF systems. The results demonstrate the proposed method consistently outperforms many competitive baselines on a variety of registration tasks.



### A Consolidated Approach to Convolutional Neural Networks and the Kolmogorov Complexity
- **Arxiv ID**: http://arxiv.org/abs/1812.00888v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1812.00888v1)
- **Published**: 2018-11-26 01:58:39+00:00
- **Updated**: 2018-11-26 01:58:39+00:00
- **Authors**: D Yoan L. Mekontchou Yomba
- **Comment**: None
- **Journal**: None
- **Summary**: The ability to precisely quantify similarity between various entities has been a fundamental complication in various problem spaces specifically in the classification of cellular images. Contemporary similarity measures applied in the domain of image processing proposed by the scientific community are mainly pursued in supervised settings. In this work, we will explore the innovative algorithmic normalized compression distance metric based on the information theoretic concept of Kolmogorov Complexity. Additionally we will observe its possible implementation in Convolutional Neural Networks to facilitate and automate the classification of Retinal Pigment Epithelial cell cultures for use in Age Related Macular Degeneration Stem Cell therapy in an unsupervised setting.



### Self-similarity Grouping: A Simple Unsupervised Cross Domain Adaptation Approach for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1811.10144v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.10144v3)
- **Published**: 2018-11-26 02:17:17+00:00
- **Updated**: 2019-09-24 03:43:29+00:00
- **Authors**: Yang Fu, Yunchao Wei, Guanshuo Wang, Yuqian Zhou, Honghui Shi, Thomas Huang
- **Comment**: This work has been accepted as an Oral presentation at ICCV2019
- **Journal**: None
- **Summary**: Domain adaptation in person re-identification (re-ID) has always been a challenging task. In this work, we explore how to harness the natural similar characteristics existing in the samples from the target domain for learning to conduct person re-ID in an unsupervised manner. Concretely, we propose a Self-similarity Grouping (SSG) approach, which exploits the potential similarity (from global body to local parts) of unlabeled samples to automatically build multiple clusters from different views. These independent clusters are then assigned with labels, which serve as the pseudo identities to supervise the training process. We repeatedly and alternatively conduct such a grouping and training process until the model is stable. Despite the apparent simplify, our SSG outperforms the state-of-the-arts by more than 4.6% (DukeMTMC to Market1501) and 4.4% (Market1501 to DukeMTMC) in mAP, respectively. Upon our SSG, we further introduce a clustering-guided semisupervised approach named SSG ++ to conduct the one-shot domain adaption in an open set setting (i.e. the number of independent identities from the target domain is unknown). Without spending much effort on labeling, our SSG ++ can further promote the mAP upon SSG by 10.7% and 6.9%, respectively. Our Code is available at: https://github.com/OasisYang/SSG .



### Spatially Controllable Image Synthesis with Internal Representation Collaging
- **Arxiv ID**: http://arxiv.org/abs/1811.10153v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.10153v2)
- **Published**: 2018-11-26 03:00:08+00:00
- **Updated**: 2019-04-09 06:19:53+00:00
- **Authors**: Ryohei Suzuki, Masanori Koyama, Takeru Miyato, Taizan Yonetsuji, Huachun Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel CNN-based image editing strategy that allows the user to change the semantic information of an image over an arbitrary region by manipulating the feature-space representation of the image in a trained GAN model. We will present two variants of our strategy: (1) spatial conditional batch normalization (sCBN), a type of conditional batch normalization with user-specifiable spatial weight maps, and (2) feature-blending, a method of directly modifying the intermediate features. Our methods can be used to edit both artificial image and real image, and they both can be used together with any GAN with conditional normalization layers. We will demonstrate the power of our method through experiments on various types of GANs trained on different datasets. Code will be available at https://github.com/pfnet-research/neural-collage.



### Temporal Convolutional Neural Network for the Classification of Satellite Image Time Series
- **Arxiv ID**: http://arxiv.org/abs/1811.10166v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10166v2)
- **Published**: 2018-11-26 03:42:52+00:00
- **Updated**: 2019-01-31 01:00:29+00:00
- **Authors**: Charlotte Pelletier, Geoffrey I. Webb, Francois Petitjean
- **Comment**: None
- **Journal**: None
- **Summary**: New remote sensing sensors now acquire high spatial and spectral Satellite Image Time Series (SITS) of the world. These series of images are a key component of classification systems that aim at obtaining up-to-date and accurate land cover maps of the Earth's surfaces. More specifically, the combination of the temporal, spectral and spatial resolutions of new SITS makes possible to monitor vegetation dynamics. Although traditional classification algorithms, such as Random Forest (RF), have been successfully applied for SITS classification, these algorithms do not make the most of the temporal domain. Conversely, some approaches that take into account the temporal dimension have recently been tested, especially Recurrent Neural Networks (RNNs). This paper proposes an exhaustive study of another deep learning approaches, namely Temporal Convolutional Neural Networks (TempCNNs) where convolutions are applied in the temporal dimension. The goal is to quantitatively and qualitatively evaluate the contribution of TempCNNs for SITS classification. This paper proposes a set of experiments performed on one million time series extracted from 46 Formosat-2 images. The experimental results show that TempCNNs are more accurate than RF and RNNs, that are the current state of the art for SITS classification. We also highlight some differences with results obtained in computer vision, e.g. about pooling layers. Moreover, we provide some general guidelines on the network architecture, common regularization mechanisms, and hyper-parameter values such as batch size. Finally, we assess the visual quality of the land cover maps produced by TempCNNs.



### Incorporating Deep Features in the Analysis of Tissue Microarray Images
- **Arxiv ID**: http://arxiv.org/abs/1812.00887v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.AP, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.00887v1)
- **Published**: 2018-11-26 04:18:17+00:00
- **Updated**: 2018-11-26 04:18:17+00:00
- **Authors**: Donghui Yan, Timothy W. Randolph, Jian Zou, Peng Gong
- **Comment**: 23 pages, 6 figures
- **Journal**: None
- **Summary**: Tissue microarray (TMA) images have been used increasingly often in cancer studies and the validation of biomarkers. TACOMA---a cutting-edge automatic scoring algorithm for TMA images---is comparable to pathologists in terms of accuracy and repeatability. Here we consider how this algorithm may be further improved. Inspired by the recent success of deep learning, we propose to incorporate representations learnable through computation. We explore representations of a group nature through unsupervised learning, e.g., hierarchical clustering and recursive space partition. Information carried by clustering or spatial partitioning may be more concrete than the labels when the data are heterogeneous, or could help when the labels are noisy. The use of such information could be viewed as regularization in model fitting. It is motivated by major challenges in TMA image scoring---heterogeneity and label noise, and the cluster assumption in semi-supervised learning. Using this information on TMA images of breast cancer, we have reduced the error rate of TACOMA by about 6%. Further simulations on synthetic data provide insights on when such representations would likely help. Although we focus on TMAs, learnable representations of this type are expected to be applicable in other settings.



### Bringing a Blurry Frame Alive at High Frame-Rate with an Event Camera
- **Arxiv ID**: http://arxiv.org/abs/1811.10180v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10180v2)
- **Published**: 2018-11-26 05:19:57+00:00
- **Updated**: 2018-11-27 06:06:30+00:00
- **Authors**: Liyuan Pan, Cedric Scheerlinck, Xin Yu, Richard Hartley, Miaomiao Liu, Yuchao Dai
- **Comment**: 14 pages, 11 figures
- **Journal**: None
- **Summary**: Event-based cameras can measure intensity changes (called `{\it events}') with microsecond accuracy under high-speed motion and challenging lighting conditions. With the active pixel sensor (APS), the event camera allows simultaneous output of the intensity frames. However, the output images are captured at a relatively low frame-rate and often suffer from motion blur. A blurry image can be regarded as the integral of a sequence of latent images, while the events indicate the changes between the latent images. Therefore, we are able to model the blur-generation process by associating event data to a latent image. In this paper, we propose a simple and effective approach, the \textbf{Event-based Double Integral (EDI)} model, to reconstruct a high frame-rate, sharp video from a single blurry frame and its event data. The video generation is based on solving a simple non-convex optimization problem in a single scalar variable. Experimental results on both synthetic and real images demonstrate the superiority of our EDI model and optimization method in comparison to the state-of-the-art.



### Phase-only Image Based Kernel Estimation for Single-image Blind Deblurring
- **Arxiv ID**: http://arxiv.org/abs/1811.10185v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10185v3)
- **Published**: 2018-11-26 05:40:32+00:00
- **Updated**: 2019-04-17 07:34:52+00:00
- **Authors**: Liyuan Pan, Richard Hartley, Miaomiao Liu, Yuchao Dai
- **Comment**: CVPR2019
- **Journal**: None
- **Summary**: The image blurring process is generally modelled as the convolution of a blur kernel with a latent image. Therefore, the estimation of the blur kernel is essentially important for blind image deblurring. Unlike existing approaches which focus on approaching the problem by enforcing various priors on the blur kernel and the latent image, we are aiming at obtaining a high quality blur kernel directly by studying the problem in the frequency domain. We show that the auto-correlation of the absolute phase-only image can provide faithful information about the motion (e.g. the motion direction and magnitude, we call it the motion pattern in this paper.) that caused the blur, leading to a new and efficient blur kernel estimation approach. The blur kernel is then refined and the sharp image is estimated by solving an optimization problem by enforcing a regularization on the blur kernel and the latent image. We further extend our approach to handle non-uniform blur, which involves spatially varying blur kernels. Our approach is evaluated extensively on synthetic and real data and shows good results compared to the state-of-the-art deblurring approaches.



### Cross-domain Deep Feature Combination for Bird Species Classification with Audio-visual Data
- **Arxiv ID**: http://arxiv.org/abs/1811.10199v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10199v1)
- **Published**: 2018-11-26 06:28:44+00:00
- **Updated**: 2018-11-26 06:28:44+00:00
- **Authors**: Bold Naranchimeg, Chao Zhang, Takuya Akashi
- **Comment**: None
- **Journal**: None
- **Summary**: In recent decade, many state-of-the-art algorithms on image classification as well as audio classification have achieved noticeable successes with the development of deep convolutional neural network (CNN). However, most of the works only exploit single type of training data. In this paper, we present a study on classifying bird species by exploiting the combination of both visual (images) and audio (sounds) data using CNN, which has been sparsely treated so far. Specifically, we propose CNN-based multimodal learning models in three types of fusion strategies (early, middle, late) to settle the issues of combining training data cross domains. The advantage of our proposed method lies on the fact that We can utilize CNN not only to extract features from image and audio data (spectrogram) but also to combine the features across modalities. In the experiment, we train and evaluate the network structure on a comprehensive CUB-200-2011 standard data set combing our originally collected audio data set with respect to the data species. We observe that a model which utilizes the combination of both data outperforms models trained with only an either type of data. We also show that transfer learning can significantly increase the classification performance.



### IDD: A Dataset for Exploring Problems of Autonomous Navigation in Unconstrained Environments
- **Arxiv ID**: http://arxiv.org/abs/1811.10200v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1811.10200v1)
- **Published**: 2018-11-26 06:29:26+00:00
- **Updated**: 2018-11-26 06:29:26+00:00
- **Authors**: Girish Varma, Anbumani Subramanian, Anoop Namboodiri, Manmohan Chandraker, C V Jawahar
- **Comment**: WACV'19
- **Journal**: None
- **Summary**: While several datasets for autonomous navigation have become available in recent years, they tend to focus on structured driving environments. This usually corresponds to well-delineated infrastructure such as lanes, a small number of well-defined categories for traffic participants, low variation in object or background appearance and strict adherence to traffic rules. We propose IDD, a novel dataset for road scene understanding in unstructured environments where the above assumptions are largely not satisfied. It consists of 10,004 images, finely annotated with 34 classes collected from 182 drive sequences on Indian roads. The label set is expanded in comparison to popular benchmarks such as Cityscapes, to account for new classes. It also reflects label distributions of road scenes significantly different from existing datasets, with most classes displaying greater within-class diversity. Consistent with real driving behaviours, it also identifies new classes such as drivable areas besides the road. We propose a new four-level label hierarchy, which allows varying degrees of complexity and opens up possibilities for new training methods. Our empirical study provides an in-depth analysis of the label characteristics. State-of-the-art methods for semantic segmentation achieve much lower accuracies on our dataset, demonstrating its distinction compared to Cityscapes. Finally, we propose that our dataset is an ideal opportunity for new problems such as domain adaptation, few-shot learning and behaviour prediction in road scenes.



### InstaNAS: Instance-aware Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/1811.10201v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.10201v3)
- **Published**: 2018-11-26 06:29:39+00:00
- **Updated**: 2019-05-23 09:25:04+00:00
- **Authors**: An-Chieh Cheng, Chieh Hubert Lin, Da-Cheng Juan, Wei Wei, Min Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Conventional Neural Architecture Search (NAS) aims at finding a single architecture that achieves the best performance, which usually optimizes task related learning objectives such as accuracy. However, a single architecture may not be representative enough for the whole dataset with high diversity and variety. Intuitively, electing domain-expert architectures that are proficient in domain-specific features can further benefit architecture related objectives such as latency. In this paper, we propose InstaNAS---an instance-aware NAS framework---that employs a controller trained to search for a "distribution of architectures" instead of a single architecture; This allows the model to use sophisticated architectures for the difficult samples, which usually comes with large architecture related cost, and shallow architectures for those easy samples. During the inference phase, the controller assigns each of the unseen input samples with a domain expert architecture that can achieve high accuracy with customized inference costs. Experiments within a search space inspired by MobileNetV2 show InstaNAS can achieve up to 48.8% latency reduction without compromising accuracy on a series of datasets against MobileNetV2.



### 3D-LaneNet: End-to-End 3D Multiple Lane Detection
- **Arxiv ID**: http://arxiv.org/abs/1811.10203v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1811.10203v3)
- **Published**: 2018-11-26 06:34:28+00:00
- **Updated**: 2019-09-10 06:48:14+00:00
- **Authors**: Noa Garnett, Rafi Cohen, Tomer Pe'er, Roee Lahav, Dan Levi
- **Comment**: To be presented in ICCV 2019
- **Journal**: None
- **Summary**: We introduce a network that directly predicts the 3D layout of lanes in a road scene from a single image. This work marks a first attempt to address this task with on-board sensing without assuming a known constant lane width or relying on pre-mapped environments. Our network architecture, 3D-LaneNet, applies two new concepts: intra-network inverse-perspective mapping (IPM) and anchor-based lane representation. The intra-network IPM projection facilitates a dual-representation information flow in both regular image-view and top-view. An anchor-per-column output representation enables our end-to-end approach which replaces common heuristics such as clustering and outlier rejection, casting lane estimation as an object detection problem. In addition, our approach explicitly handles complex situations such as lane merges and splits. Results are shown on two new 3D lane datasets, a synthetic and a real one. For comparison with existing methods, we test our approach on the image-only tuSimple lane detection benchmark, achieving performance competitive with state-of-the-art.



### City-Scale Road Audit System using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1811.10210v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1811.10210v1)
- **Published**: 2018-11-26 06:49:11+00:00
- **Updated**: 2018-11-26 06:49:11+00:00
- **Authors**: Sudhir Yarram, Girish Varma, C. V. Jawahar
- **Comment**: IROS'18
- **Journal**: None
- **Summary**: Road networks in cities are massive and is a critical component of mobility. Fast response to defects, that can occur not only due to regular wear and tear but also because of extreme events like storms, is essential. Hence there is a need for an automated system that is quick, scalable and cost-effective for gathering information about defects. We propose a system for city-scale road audit, using some of the most recent developments in deep learning and semantic segmentation. For building and benchmarking the system, we curated a dataset which has annotations required for road defects. However, many of the labels required for road audit have high ambiguity which we overcome by proposing a label hierarchy. We also propose a multi-step deep learning model that segments the road, subdivide the road further into defects, tags the frame for each defect and finally localizes the defects on a map gathered using GPS. We analyze and evaluate the models on image tagging as well as segmentation at different levels of the label hierarchy.



### A Survey on Joint Object Detection and Pose Estimation using Monocular Vision
- **Arxiv ID**: http://arxiv.org/abs/1811.10216v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10216v1)
- **Published**: 2018-11-26 07:43:23+00:00
- **Updated**: 2018-11-26 07:43:23+00:00
- **Authors**: Aniruddha V Patil, Pankaj Rabha
- **Comment**: Accepted at the International Joint Conference on Computer Vision and
  Pattern Recognition (CCVPR) 2018
- **Journal**: None
- **Summary**: In this survey we present a complete landscape of joint object detection and pose estimation methods that use monocular vision. Descriptions of traditional approaches that involve descriptors or models and various estimation methods have been provided. These descriptors or models include chordiograms, shape-aware deformable parts model, bag of boundaries, distance transform templates, natural 3D markers and facet features whereas the estimation methods include iterative clustering estimation, probabilistic networks and iterative genetic matching. Hybrid approaches that use handcrafted feature extraction followed by estimation by deep learning methods have been outlined. We have investigated and compared, wherever possible, pure deep learning based approaches (single stage and multi stage) for this problem. Comprehensive details of the various accuracy measures and metrics have been illustrated. For the purpose of giving a clear overview, the characteristics of relevant datasets are discussed. The trends that prevailed from the infancy of this problem until now have also been highlighted.



### Attentioned Convolutional LSTM InpaintingNetwork for Anomaly Detection in Videos
- **Arxiv ID**: http://arxiv.org/abs/1811.10228v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10228v1)
- **Published**: 2018-11-26 08:25:14+00:00
- **Updated**: 2018-11-26 08:25:14+00:00
- **Authors**: Itamar Ben-Ari, Ravid Shwartz-Ziv
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a semi-supervised model for detecting anomalies in videos inspiredby the Video Pixel Network [van den Oord et al., 2016]. VPN is a probabilisticgenerative model based on a deep neural network that estimates the discrete jointdistribution of raw pixels in video frames. Our model extends the Convolutional-LSTM video encoder part of the VPN with a novel convolutional based attentionmechanism. We also modify the Pixel-CNN decoder part of the VPN to a frameinpainting task where a partially masked version of the frame to predict is given asinput. The frame reconstruction error is used as an anomaly indicator. We test ourmodel on a modified version of the moving mnist dataset [Srivastava et al., 2015]. Our model is shown to be effective in detecting anomalies in videos. This approachcould be a component in applications requiring visual common sense.



### Brain-inspired robust delineation operator
- **Arxiv ID**: http://arxiv.org/abs/1811.10240v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10240v1)
- **Published**: 2018-11-26 09:24:58+00:00
- **Updated**: 2018-11-26 09:24:58+00:00
- **Authors**: Nicola Strisciuglio, George Azzopardi, Nicolai Petkov
- **Comment**: Accepted at Brain-driven Computer Vision workshop at ECCV 2018
- **Journal**: None
- **Summary**: In this paper we present a novel filter, based on the existing COSFIRE filter, for the delineation of patterns of interest. It includes a mechanism of push-pull inhibition that improves robustness to noise in terms of spurious texture. Push-pull inhibition is a phenomenon that is observed in neurons in area V1 of the visual cortex, which suppresses the response of certain simple cells for stimuli of preferred orientation but of non-preferred contrast. This type of inhibition allows for sharper detection of the patterns of interest and improves the quality of delineation especially in images with spurious texture.   We performed experiments on images from different applications, namely the detection of rose stems for automatic gardening, the delineation of cracks in pavements and road surfaces, and the segmentation of blood vessels in retinal images. Push-pull inhibition helped to improve results considerably in all applications.



### MonoGRNet: A Geometric Reasoning Network for Monocular 3D Object Localization
- **Arxiv ID**: http://arxiv.org/abs/1811.10247v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10247v2)
- **Published**: 2018-11-26 09:36:40+00:00
- **Updated**: 2020-03-31 14:52:26+00:00
- **Authors**: Zengyi Qin, Jinglu Wang, Yan Lu
- **Comment**: 8 pages, accepted by AAAI 2019, oral
- **Journal**: None
- **Summary**: Detecting and localizing objects in the real 3D space, which plays a crucial role in scene understanding, is particularly challenging given only a single RGB image due to the geometric information loss during imagery projection. We propose MonoGRNet for the amodal 3D object detection from a monocular RGB image via geometric reasoning in both the observed 2D projection and the unobserved depth dimension. MonoGRNet is a single, unified network composed of four task-specific subnetworks, responsible for 2D object detection, instance depth estimation (IDE), 3D localization and local corner regression. Unlike the pixel-level depth estimation that needs per-pixel annotations, we propose a novel IDE method that directly predicts the depth of the targeting 3D bounding box's center using sparse supervision. The 3D localization is further achieved by estimating the position in the horizontal and vertical dimensions. Finally, MonoGRNet is jointly learned by optimizing the locations and poses of the 3D bounding boxes in the global context. We demonstrate that MonoGRNet achieves state-of-the-art performance on challenging datasets.



### Region Based Extensive Response Index Pattern for Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/1811.10261v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10261v1)
- **Published**: 2018-11-26 10:03:08+00:00
- **Updated**: 2018-11-26 10:03:08+00:00
- **Authors**: Monu Verma, Santosh. K. Vipparthi, Girdhari Singh
- **Comment**: Conference
- **Journal**: None
- **Summary**: This paper presents a novel descriptor named Region based Extensive Response Index Pattern (RETRaIN) for facial expression recognition. The RETRaIN encodes the relation among the reference and neighboring pixels of facial active regions. These relations are computed by using directional compass mask on an input image and extract the high edge responses in foremost directions. Further extreme edge index positions are selected and encoded into six-bit compact code to reduce feature dimensionality and distinguish between the uniform and non-uniform patterns in the facial features. The performance of the proposed descriptor is tested and evaluated on three benchmark datasets Extended Cohn Kanade, JAFFE, and MUG. The RETRaIN achieves superior recognition accuracy in comparison to state-of-the-art techniques.



### Multi-hierarchical Independent Correlation Filters for Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/1811.10302v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10302v2)
- **Published**: 2018-11-26 11:41:23+00:00
- **Updated**: 2018-11-27 16:39:58+00:00
- **Authors**: Shuai Bai, Zhiqun He, Ting-Bing Xu, Zheng Zhu, Yuan Dong, Hongliang Bai
- **Comment**: None
- **Journal**: None
- **Summary**: For visual tracking, most of the traditional correlation filters (CF) based methods suffer from the bottleneck of feature redundancy and lack of motion information. In this paper, we design a novel tracking framework, called multi-hierarchical independent correlation filters (MHIT). The framework consists of motion estimation module, hierarchical features selection, independent CF online learning, and adaptive multi-branch CF fusion. Specifically, the motion estimation module is introduced to capture motion information, which effectively alleviates the object partial occlusion in the temporal video. The multi-hierarchical deep features of CNN representing different semantic information can be fully excavated to track multi-scale objects. To better overcome the deep feature redundancy, each hierarchical features are independently fed into a single branch to implement the online learning of parameters. Finally, an adaptive weight scheme is integrated into the framework to fuse these independent multi-branch CFs for the better and more robust visual object tracking. Extensive experiments on OTB and VOT datasets show that the proposed MHIT tracker can significantly improve the tracking performance. Especially, it obtains a 20.1% relative performance gain compared to the top trackers on the VOT2017 challenge, and also achieves new state-of-the-art performance on the VOT2018 challenge.



### Universal Semi-Supervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1811.10323v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.10323v3)
- **Published**: 2018-11-26 12:36:03+00:00
- **Updated**: 2019-09-24 06:13:53+00:00
- **Authors**: Tarun Kalluri, Girish Varma, Manmohan Chandraker, C V Jawahar
- **Comment**: Accepted as poster presentation at ICCV 2019
- **Journal**: None
- **Summary**: In recent years, the need for semantic segmentation has arisen across several different applications and environments. However, the expense and redundancy of annotation often limits the quantity of labels available for training in any domain, while deployment is easier if a single model works well across domains. In this paper, we pose the novel problem of universal semi-supervised semantic segmentation and propose a solution framework, to meet the dual needs of lower annotation and deployment costs. In contrast to counterpoints such as fine tuning, joint training or unsupervised domain adaptation, universal semi-supervised segmentation ensures that across all domains: (i) a single model is deployed, (ii) unlabeled data is used, (iii) performance is improved, (iv) only a few labels are needed and (v) label spaces may differ. To address this, we minimize supervised as well as within and cross-domain unsupervised losses, introducing a novel feature alignment objective based on pixel-aware entropy regularization for the latter. We demonstrate quantitative advantages over other approaches on several combinations of segmentation datasets across different geographies (Germany, England, India) and environments (outdoors, indoors), as well as qualitative insights on the aligned representations.



### Matchable Image Retrieval by Learning from Surface Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1811.10343v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10343v2)
- **Published**: 2018-11-26 13:03:15+00:00
- **Updated**: 2018-12-10 17:26:18+00:00
- **Authors**: Tianwei Shen, Zixin Luo, Lei Zhou, Runze Zhang, Siyu Zhu, Tian Fang, Long Quan
- **Comment**: accepted by ACCV 2018
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) have achieved superior performance on object image retrieval, while Bag-of-Words (BoW) models with handcrafted local features still dominate the retrieval of overlapping images in 3D reconstruction. In this paper, we narrow down this gap by presenting an efficient CNN-based method to retrieve images with overlaps, which we refer to as the matchable image retrieval problem. Different from previous methods that generates training data based on sparse reconstruction, we create a large-scale image database with rich 3D geometrics and exploit information from surface reconstruction to obtain fine-grained training data. We propose a batched triplet-based loss function combined with mesh re-projection to effectively learn the CNN representation. The proposed method significantly accelerates the image retrieval process in 3D reconstruction and outperforms the state-of-the-art CNN-based and BoW methods for matchable image retrieval. The code and data are available at https://github.com/hlzz/mirror.



### EFANet: Exchangeable Feature Alignment Network for Arbitrary Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/1811.10352v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1811.10352v3)
- **Published**: 2018-11-26 13:15:23+00:00
- **Updated**: 2019-12-21 18:36:11+00:00
- **Authors**: Zhijie Wu, Chunjin Song, Yang Zhou, Minglun Gong, Hui Huang
- **Comment**: Accepted by AAAI 2020
- **Journal**: None
- **Summary**: Style transfer has been an important topic both in computer vision and graphics. Since the seminal work of Gatys et al. first demonstrates the power of stylization through optimization in the deep feature space, quite a few approaches have achieved real-time arbitrary style transfer with straightforward statistic matching techniques. In this work, our key observation is that only considering features in the input style image for the global deep feature statistic matching or local patch swap may not always ensure a satisfactory style transfer; see e.g., Figure 1. Instead, we propose a novel transfer framework, EFANet, that aims to jointly analyze and better align exchangeable features extracted from content and style image pair. In this way, the style features from the style image seek for the best compatibility with the content information in the content image, leading to more structured stylization results. In addition, a new whitening loss is developed for purifying the computed content features and better fusion with styles in feature space. Qualitative and quantitative experiments demonstrate the advantages of our approach.



### Unsupervised learning with sparse space-and-time autoencoders
- **Arxiv ID**: http://arxiv.org/abs/1811.10355v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1811.10355v1)
- **Published**: 2018-11-26 13:22:17+00:00
- **Updated**: 2018-11-26 13:22:17+00:00
- **Authors**: Benjamin Graham
- **Comment**: None
- **Journal**: None
- **Summary**: We use spatially-sparse two, three and four dimensional convolutional autoencoder networks to model sparse structures in 2D space, 3D space, and 3+1=4 dimensional space-time. We evaluate the resulting latent spaces by testing their usefulness for downstream tasks. Applications are to handwriting recognition in 2D, segmentation for parts in 3D objects, segmentation for objects in 3D scenes, and body-part segmentation for 4D wire-frame models generated from motion capture data.



### Automatic segmentation of the Foveal Avascular Zone in ophthalmological OCT-A images
- **Arxiv ID**: http://arxiv.org/abs/1811.10374v1
- **DOI**: 10.1371/journal.pone.0212364
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10374v1)
- **Published**: 2018-11-26 14:07:28+00:00
- **Updated**: 2018-11-26 14:07:28+00:00
- **Authors**: Macarena Díaz, Jorge Novo, Paula Cutrín, Francisco Gómez-Ulla, Manuel G. Penedo, Marcos Ortega
- **Comment**: None
- **Journal**: None
- **Summary**: Angiography by Optical Coherence Tomography is a non-invasive retinal imaging modality of recent appearance that allows the visualization of the vascular structure at predefined depths based on the detection of the blood movement. OCT-A images constitute a suitable scenario to analyse the retinal vascular properties of regions of interest, measuring the characteristics of the foveal vascular and avascular zones. Extracted parameters of this region can be used as prognostic factors that determine if the patient suffers from certain pathologies, indicating the associated pathological degree. The manual extraction of these biomedical parameters is a long, tedious and subjective process, introducing a significant intra and inter-expert variability, which penalizes the utility of the measurements. In addition, the absence of tools that automatically facilitate these calculations encourages the creation of computer-aided diagnosis frameworks that ease the doctor's work, increasing their productivity and making viable the use of this type of vascular biomarkers.   We propose a fully automatic system that identifies and precisely segments the region of the foveal avascular zone (FAZ) using a novel ophthalmological image modality as is OCT-A. The system combines different image processing techniques to firstly identify the region where the FAZ is contained and, secondly, proceed with the extraction of its precise contour. The system was validated using a representative set of 168 OCT-A images, providing accurate results with the best correlation with the manual measurements of two experts clinician of 0.93 as well as a Jaccard's index of 0.82 of the best experimental case. This tool provides an accurate FAZ measurement with the desired objectivity and reproducibility, being very useful for the analysis of relevant vascular diseases through the study of the retinal microcirculation.



### A Convolutional Neural Network based Live Object Recognition System as Blind Aid
- **Arxiv ID**: http://arxiv.org/abs/1811.10399v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.10399v1)
- **Published**: 2018-11-26 14:38:25+00:00
- **Updated**: 2018-11-26 14:38:25+00:00
- **Authors**: Kedar Potdar, Chinmay D. Pai, Sukrut Akolkar
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces a live object recognition system that serves as a blind aid. Visually impaired people heavily rely on their other senses such as touch and auditory signals for understanding the environment around them. The act of knowing what object is in front of the blind person without touching it (by hand or some other tool) is very difficult. In some cases, the physical contact between the person and object can be dangerous, and even lethal.   This project employs a Convolutional Neural Network for recognition of pre-trained objects on the ImageNet dataset. A camera, aligned with the system's predetermined orientation serves as input to the computer system, which has the object recognition Neural Network deployed to carry out real-time object detection. Output from the network can then be parsed to present to the visually impaired person either in the form of audio or Braille text.



### Towards Machine Learning Prediction of Deep Brain Stimulation (DBS) Intra-operative Efficacy Maps
- **Arxiv ID**: http://arxiv.org/abs/1811.10415v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10415v1)
- **Published**: 2018-11-26 14:50:06+00:00
- **Updated**: 2018-11-26 14:50:06+00:00
- **Authors**: Camilo Bermudez, William Rodriguez, Yuankai Huo, Allison E. Hainline, Rui Li, Robert Shults, Pierre D. DHaese, Peter E. Konrad, Benoit M. Dawant, Bennett A. Landman
- **Comment**: Accepted to SPIE: Medical Imaging 2019
- **Journal**: None
- **Summary**: Deep brain stimulation (DBS) has the potential to improve the quality of life of people with a variety of neurological diseases. A key challenge in DBS is in the placement of a stimulation electrode in the anatomical location that maximizes efficacy and minimizes side effects. Pre-operative localization of the optimal stimulation zone can reduce surgical times and morbidity. Current methods of producing efficacy probability maps follow an anatomical guidance on magnetic resonance imaging (MRI) to identify the areas with the highest efficacy in a population. In this work, we propose to revisit this problem as a classification problem, where each voxel in the MRI is a sample informed by the surrounding anatomy. We use a patch-based convolutional neural network to classify a stimulation coordinate as having a positive reduction in symptoms during surgery. We use a cohort of 187 patients with a total of 2,869 stimulation coordinates, upon which 3D patches were extracted and associated with an efficacy score. We compare our results with a registration-based method of surgical planning. We show an improvement in the classification of intraoperative stimulation coordinates as a positive response in reduction of symptoms with AUC of 0.670 compared to a baseline registration-based approach, which achieves an AUC of 0.627 (p < 0.01). Although additional validation is needed, the proposed classification framework and deep learning method appear well-suited for improving pre-surgical planning and personalize treatment strategies.



### Deep Laplacian Pyramid Network for Text Images Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1811.10449v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10449v1)
- **Published**: 2018-11-26 15:29:20+00:00
- **Updated**: 2018-11-26 15:29:20+00:00
- **Authors**: Hanh T. M. Tran, Tien Ho-Phuoc
- **Comment**: paper, 6 pages
- **Journal**: None
- **Summary**: Convolutional neural networks have recently demonstrated interesting results for single image super-resolution. However, these networks were trained to deal with super-resolution problem on natural images. In this paper, we adapt a deep network, which was proposed for natural images superresolution, to single text image super-resolution. To evaluate the network, we present our database for single text image super-resolution. Moreover, we propose to combine Gradient Difference Loss (GDL) with L1/L2 loss to enhance edges in super-resolution image. Quantitative and qualitative evaluations on our dataset show that adding the GDL improves the super-resolution results.



### Context-Aware Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/1811.10452v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10452v2)
- **Published**: 2018-11-26 15:31:22+00:00
- **Updated**: 2019-04-15 08:32:16+00:00
- **Authors**: Weizhe Liu, Mathieu Salzmann, Pascal Fua
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: State-of-the-art methods for counting people in crowded scenes rely on deep networks to estimate crowd density. They typically use the same filters over the whole image or over large image patches. Only then do they estimate local scale to compensate for perspective distortion. This is typically achieved by training an auxiliary classifier to select, for predefined image patches, the best kernel size among a limited set of choices. As such, these methods are not end-to-end trainable and restricted in the scope of context they can leverage.   In this paper, we introduce an end-to-end trainable deep architecture that combines features obtained using multiple receptive field sizes and learns the importance of each such feature at each image location. In other words, our approach adaptively encodes the scale of the contextual information required to accurately predict crowd density. This yields an algorithm that outperforms state-of-the-art crowd counting methods, especially when perspective effects are strong.



### Scan2Mesh: From Unstructured Range Scans to 3D Meshes
- **Arxiv ID**: http://arxiv.org/abs/1811.10464v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10464v2)
- **Published**: 2018-11-26 15:54:15+00:00
- **Updated**: 2019-04-02 15:51:51+00:00
- **Authors**: Angela Dai, Matthias Nießner
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce Scan2Mesh, a novel data-driven generative approach which transforms an unstructured and potentially incomplete range scan into a structured 3D mesh representation. The main contribution of this work is a generative neural network architecture whose input is a range scan of a 3D object and whose output is an indexed face set conditioned on the input scan. In order to generate a 3D mesh as a set of vertices and face indices, the generative model builds on a series of proxy losses for vertices, edges, and faces. At each stage, we realize a one-to-one discrete mapping between the predicted and ground truth data points with a combination of convolutional- and graph neural network architectures. This enables our algorithm to predict a compact mesh representation similar to those created through manual artist effort using 3D modeling software. Our generated mesh results thus produce sharper, cleaner meshes with a fundamentally different structure from those generated through implicit functions, a first step in bridging the gap towards artist-created CAD models.



### Robust Cross-View Gait Recognition with Evidence: A Discriminant Gait GAN (DiGGAN) Approach
- **Arxiv ID**: http://arxiv.org/abs/1811.10493v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10493v3)
- **Published**: 2018-11-26 16:37:29+00:00
- **Updated**: 2020-09-17 10:17:12+00:00
- **Authors**: BingZhang Hu, Yu Guan, Yan Gao, Yang Long, Nicholas Lane, Thomas Ploetz
- **Comment**: Submitted to ACM Transactions on Intelligent Systems and Technology
- **Journal**: None
- **Summary**: Gait as a biometric trait has attracted much attention in many security and privacy applications such as identity recognition and authentication, during the last few decades. Because of its nature as a long-distance biometric trait, gait can be easily collected and used to identify individuals non-intrusively through CCTV cameras. However, it is very difficult to develop robust automated gait recognition systems, since gait may be affected by many covariate factors such as clothing, walking speed, camera view angle etc. Out of them, large view angle changes has been deemed as the most challenging factor as it can alter the overall gait appearance substantially.   Existing works on gait recognition are far from enough to provide satisfying performances because of such view changes. Furthermore, very few works have considered evidences -- the demonstrable information revealing the reliabilities of decisions, which are regarded as important demands in machine learning-based recognition/authentication applications. To address these issues, in this paper we propose a Discriminant Gait Generative Adversarial Network, namely DiGGAN, which can effectively extract view-invariant features for cross-view gait recognition; and more importantly, to transfer gait images to different views -- serving as evidences and showing how the decisions have been made. Quantitative experiments have been conducted on the two most popular cross-view gait datasets, the OU-MVLP and CASIA-B, where the proposed DiGGAN has outperformed state-of-the-art methods. Qualitative analysis has also been provided and demonstrates the proposed DiGGAN's capability in providing evidences.



### ExpandNets: Linear Over-parameterization to Train Compact Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1811.10495v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.10495v5)
- **Published**: 2018-11-26 16:40:24+00:00
- **Updated**: 2021-04-14 11:55:22+00:00
- **Authors**: Shuxuan Guo, Jose M. Alvarez, Mathieu Salzmann
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce an approach to training a given compact network. To this end, we leverage over-parameterization, which typically improves both neural network optimization and generalization. Specifically, we propose to expand each linear layer of the compact network into multiple consecutive linear layers, without adding any nonlinearity. As such, the resulting expanded network, or ExpandNet, can be contracted back to the compact one algebraically at inference. In particular, we introduce two convolutional expansion strategies and demonstrate their benefits on several tasks, including image classification, object detection, and semantic segmentation. As evidenced by our experiments, our approach outperforms both training the compact network from scratch and performing knowledge distillation from a teacher. Furthermore, our linear over-parameterization empirically reduces gradient confusion during training and improves the network generalization.



### Tracing in 2D to Reduce the Annotation Effort for 3D Deep Delineation
- **Arxiv ID**: http://arxiv.org/abs/1811.10508v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10508v1)
- **Published**: 2018-11-26 17:01:29+00:00
- **Updated**: 2018-11-26 17:01:29+00:00
- **Authors**: Mateusz Koziński, Agata Mosinska, Mathieu Salzmann, Pascal Fua
- **Comment**: None
- **Journal**: None
- **Summary**: The difficulty of obtaining annotations to build training databases still slows down the adoption of recent deep learning approaches for biomedical image analysis. In this paper, we show that we can train a Deep Net to perform 3D volumetric delineation given only 2D annotations in Maximum Intensity Projections (MIP). As a consequence, we can decrease the amount of time spent annotating by a factor of two while maintaining similar performance.   Our approach is inspired by space carving, a classical technique of reconstructing complex 3D shapes from arbitrarily-positioned cameras. We will demonstrate its effectiveness on 3D light microscopy images of neurons and retinal blood vessels and on Magnetic Resonance Angiography (MRA) brain scans.



### Deep Network Interpolation for Continuous Imagery Effect Transition
- **Arxiv ID**: http://arxiv.org/abs/1811.10515v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10515v1)
- **Published**: 2018-11-26 17:14:27+00:00
- **Updated**: 2018-11-26 17:14:27+00:00
- **Authors**: Xintao Wang, Ke Yu, Chao Dong, Xiaoou Tang, Chen Change Loy
- **Comment**: Technical report
- **Journal**: None
- **Summary**: Deep convolutional neural network has demonstrated its capability of learning a deterministic mapping for the desired imagery effect. However, the large variety of user flavors motivates the possibility of continuous transition among different output effects. Unlike existing methods that require a specific design to achieve one particular transition (e.g., style transfer), we propose a simple yet universal approach to attain a smooth control of diverse imagery effects in many low-level vision tasks, including image restoration, image-to-image translation, and style transfer. Specifically, our method, namely Deep Network Interpolation (DNI), applies linear interpolation in the parameter space of two or more correlated networks. A smooth control of imagery effects can be achieved by tweaking the interpolation coefficients. In addition to DNI and its broad applications, we also investigate the mechanism of network interpolation from the perspective of learned filters.



### Unsupervised 3D Shape Learning from Image Collections in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1811.10519v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10519v2)
- **Published**: 2018-11-26 17:21:30+00:00
- **Updated**: 2018-11-27 01:50:01+00:00
- **Authors**: Attila Szabó, Paolo Favaro
- **Comment**: None
- **Journal**: None
- **Summary**: We present a method to learn the 3D surface of objects directly from a collection of images. Previous work achieved this capability by exploiting additional manual annotation, such as object pose, 3D surface templates, temporal continuity of videos, manually selected landmarks, and foreground/background masks. In contrast, our method does not make use of any such annotation. Rather, it builds a generative model, a convolutional neural network, which, given a noise vector sample, outputs the 3D surface and texture of an object and a background image. These 3 components combined with an additional random viewpoint vector are then fed to a differential renderer to produce a view of the sampled object and background. Our general principle is that if the output of the renderer, the generated image, is realistic, then its input, the generated 3D and texture, should also be realistic. To achieve realism, the generative model is trained adversarially against a discriminator that tries to distinguish between the output of the renderer and real images from the given data set. Moreover, our generative model can be paired with an encoder and trained as an autoencoder, to automatically extract the 3D shape, texture and pose of the object in an image. Our trained generative model and encoder show promising results both on real and synthetic data, which demonstrate for the first time that fully unsupervised 3D learning from image collections is possible.



### Predicting Language Recovery after Stroke with Convolutional Networks on Stitched MRI
- **Arxiv ID**: http://arxiv.org/abs/1811.10520v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10520v1)
- **Published**: 2018-11-26 17:23:28+00:00
- **Updated**: 2018-11-26 17:23:28+00:00
- **Authors**: Yusuf H. Roohani, Noor Sajid, Pranava Madhyastha, Cathy J. Price, Thomas M. H. Hope
- **Comment**: Machine Learning for Health (ML4H) Workshop at NeurIPS 2018
  arXiv:1811.07216
- **Journal**: None
- **Summary**: One third of stroke survivors have language difficulties. Emerging evidence suggests that their likelihood of recovery depends mainly on the damage to language centers. Thus previous research for predicting language recovery post-stroke has focused on identifying damaged regions of the brain. In this paper, we introduce a novel method where we only make use of stitched 2-dimensional cross-sections of raw MRI scans in a deep convolutional neural network setup to predict language recovery post-stroke. Our results show: a) the proposed model that only uses MRI scans has comparable performance to models that are dependent on lesion specific information; b) the features learned by our model are complementary to the lesion specific information and the combination of both appear to outperform previously reported results in similar settings. We further analyse the CNN model for understanding regions in brain that are responsible for arriving at these predictions using gradient based saliency maps. Our findings are in line with previous lesion studies.



### Scene Categorization from Contours: Medial Axis Based Salience Measures
- **Arxiv ID**: http://arxiv.org/abs/1811.10524v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10524v1)
- **Published**: 2018-11-26 17:27:50+00:00
- **Updated**: 2018-11-26 17:27:50+00:00
- **Authors**: Morteza Rezanejad, Gabriel Downs, John Wilder, Dirk B. Walther, Allan Jepson, Sven Dickinson, Kaleem Siddiqi
- **Comment**: None
- **Journal**: None
- **Summary**: The computer vision community has witnessed recent advances in scene categorization from images, with the state-of-the art systems now achieving impressive recognition rates on challenging benchmarks such as the Places365 dataset. Such systems have been trained on photographs which include color, texture and shading cues. The geometry of shapes and surfaces, as conveyed by scene contours, is not explicitly considered for this task. Remarkably, humans can accurately recognize natural scenes from line drawings, which consist solely of contour-based shape cues. Here we report the first computer vision study on scene categorization of line drawings derived from popular databases including an artist scene database, MIT67, and Places365. Specifically, we use off-the-shelf pre-trained CNNs to perform scene classification given only contour information as input and find performance levels well above chance. We also show that medial-axis based contour salience methods can be used to select more informative subsets of contour pixels and that the variation in CNN classification performance on various choices for these subsets is qualitatively similar to that observed in human performance. Moreover, when the salience measures are used to weight the contours, as opposed to pruning them, we find that these weights boost our CNN performance above that for unweighted contour input. That is, the medial axis based salience weights appear to add useful information that is not available when CNNs are trained to use contours alone.



### Higher-order Projected Power Iterations for Scalable Multi-Matching
- **Arxiv ID**: http://arxiv.org/abs/1811.10541v2
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.10541v2)
- **Published**: 2018-11-26 17:44:48+00:00
- **Updated**: 2019-03-14 09:11:59+00:00
- **Authors**: Florian Bernard, Johan Thunberg, Paul Swoboda, Christian Theobalt
- **Comment**: None
- **Journal**: None
- **Summary**: The matching of multiple objects (e.g. shapes or images) is a fundamental problem in vision and graphics. In order to robustly handle ambiguities, noise and repetitive patterns in challenging real-world settings, it is essential to take geometric consistency between points into account. Computationally, the multi-matching problem is difficult. It can be phrased as simultaneously solving multiple (NP-hard) quadratic assignment problems (QAPs) that are coupled via cycle-consistency constraints. The main limitations of existing multi-matching methods are that they either ignore geometric consistency and thus have limited robustness, or they are restricted to small-scale problems due to their (relatively) high computational cost. We address these shortcomings by introducing a Higher-order Projected Power Iteration method, which is (i) efficient and scales to tens of thousands of points, (ii) straightforward to implement, (iii) able to incorporate geometric consistency, (iv) guarantees cycle-consistent multi-matchings, and (iv) comes with theoretical convergence guarantees. Experimentally we show that our approach is superior to existing methods.



### Similarity-preserving Image-image Domain Adaptation for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1811.10551v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10551v2)
- **Published**: 2018-11-26 17:56:32+00:00
- **Updated**: 2020-01-05 12:11:01+00:00
- **Authors**: Weijian Deng, Liang Zheng, Qixiang Ye, Yi Yang, Jianbin Jiao
- **Comment**: 14 pages, 7 tables, 14 figures, this version is not fully edited and
  will be updated soon. arXiv admin note: text overlap with arXiv:1711.07027
- **Journal**: None
- **Summary**: This article studies the domain adaptation problem in person re-identification (re-ID) under a "learning via translation" framework, consisting of two components, 1) translating the labeled images from the source to the target domain in an unsupervised manner, 2) learning a re-ID model using the translated images. The objective is to preserve the underlying human identity information after image translation, so that translated images with labels are effective for feature learning on the target domain. To this end, we propose a similarity preserving generative adversarial network (SPGAN) and its end-to-end trainable version, eSPGAN. Both aiming at similarity preserving, SPGAN enforces this property by heuristic constraints, while eSPGAN does so by optimally facilitating the re-ID model learning. More specifically, SPGAN separately undertakes the two components in the "learning via translation" framework. It first preserves two types of unsupervised similarity, namely, self-similarity of an image before and after translation, and domain-dissimilarity of a translated source image and a target image. It then learns a re-ID model using existing networks. In comparison, eSPGAN seamlessly integrates image translation and re-ID model learning. During the end-to-end training of eSPGAN, re-ID learning guides image translation to preserve the underlying identity information of an image. Meanwhile, image translation improves re-ID learning by providing identity-preserving training samples of the target domain style. In the experiment, we show that identities of the fake images generated by SPGAN and eSPGAN are well preserved. Based on this, we report the new state-of-the-art domain adaptation results on two large-scale person re-ID datasets.



### A deep neural network to enhance prediction of 1-year mortality using echocardiographic videos of the heart
- **Arxiv ID**: http://arxiv.org/abs/1811.10553v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1811.10553v2)
- **Published**: 2018-11-26 17:58:57+00:00
- **Updated**: 2019-05-14 20:36:00+00:00
- **Authors**: Alvaro Ulloa, Linyuan Jing, Christopher W Good, David P vanMaanen, Sushravya Raghunath, Jonathan D Suever, Christopher D Nevius, Gregory J Wehner, Dustin Hartzel, Joseph B Leader, Amro Alsaid, Aalpen A Patel, H Lester Kirchner, Marios S Pattichis, Christopher M Haggerty, Brandon K Fornwalt
- **Comment**: We updated results with improved performance after dropout bug in
  tensorflow v1.12. We also added learning curves showing promise in video
  model with more samples
- **Journal**: None
- **Summary**: Predicting future clinical events helps physicians guide appropriate intervention. Machine learning has tremendous promise to assist physicians with predictions based on the discovery of complex patterns from historical data, such as large, longitudinal electronic health records (EHR). This study is a first attempt to demonstrate such capabilities using raw echocardiographic videos of the heart. We show that a large dataset of 723,754 clinically-acquired echocardiographic videos (~45 million images) linked to longitudinal follow-up data in 27,028 patients can be used to train a deep neural network to predict 1-year mortality with good accuracy (area under the curve (AUC) in an independent test set = 0.839). Prediction accuracy was further improved by adding EHR data (AUC = 0.858). Finally, we demonstrate that the trained neural network was more accurate in mortality prediction than two expert cardiologists. These results highlight the potential of neural networks to add new power to clinical predictions.



### Leveraging Filter Correlations for Deep Model Compression
- **Arxiv ID**: http://arxiv.org/abs/1811.10559v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.10559v2)
- **Published**: 2018-11-26 18:05:18+00:00
- **Updated**: 2020-01-15 20:16:50+00:00
- **Authors**: Pravendra Singh, Vinay Kumar Verma, Piyush Rai, Vinay P. Namboodiri
- **Comment**: IEEE Winter Conference on Applications of Computer Vision (WACV),
  2020
- **Journal**: None
- **Summary**: We present a filter correlation based model compression approach for deep convolutional neural networks. Our approach iteratively identifies pairs of filters with the largest pairwise correlations and drops one of the filters from each such pair. However, instead of discarding one of the filters from each such pair na\"{i}vely, the model is re-optimized to make the filters in these pairs maximally correlated, so that discarding one of the filters from the pair results in minimal information loss. Moreover, after discarding the filters in each round, we further finetune the model to recover from the potential small loss incurred by the compression. We evaluate our proposed approach using a comprehensive set of experiments and ablation studies. Our compression method yields state-of-the-art FLOPs compression rates on various benchmarks, such as LeNet-5, VGG-16, and ResNet-50,56, while still achieving excellent predictive performance for tasks such as object detection on benchmark datasets.



### Low-Dose CT via Deep CNN with Skip Connection and Network in Network
- **Arxiv ID**: http://arxiv.org/abs/1811.10564v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.10564v2)
- **Published**: 2018-11-26 18:08:44+00:00
- **Updated**: 2019-08-03 02:53:26+00:00
- **Authors**: Chenyu You, Linfeng Yang, Yi Zhang, Ge Wang
- **Comment**: None
- **Journal**: None
- **Summary**: A major challenge in computed tomography (CT) is how to minimize patient radiation exposure without compromising image quality and diagnostic performance. The use of deep convolutional (Conv) neural networks for noise reduction in Low-Dose CT (LDCT) images has recently shown a great potential in this important application. In this paper, we present a highly efficient and effective neural network model for LDCT image noise reduction. Specifically, to capture local anatomical features we integrate Deep Convolutional Neural Networks (CNNs) and Skip connection layers for feature extraction. Also, we introduce parallelized $1\times 1$ CNN, called Network in Network, to lower the dimensionality of the output from the previous layer, achieving faster computational speed at less feature loss. To optimize the performance of the network, we adopt a Wasserstein generative adversarial network (WGAN) framework. Quantitative and qualitative comparisons demonstrate that our proposed network model can produce images with lower noise and more structural details than state-of-the-art noise-reduction methods.



### Convolutional Neural Networks Deceived by Visual Illusions
- **Arxiv ID**: http://arxiv.org/abs/1811.10565v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10565v1)
- **Published**: 2018-11-26 18:09:33+00:00
- **Updated**: 2018-11-26 18:09:33+00:00
- **Authors**: Alexander Gomez-Villa, Adrián Martín, Javier Vazquez-Corral, Marcelo Bertalmío
- **Comment**: None
- **Journal**: None
- **Summary**: Visual illusions teach us that what we see is not always what it is represented in the physical world. Its special nature make them a fascinating tool to test and validate any new vision model proposed. In general, current vision models are based on the concatenation of linear convolutions and non-linear operations. In this paper we get inspiration from the similarity of this structure with the operations present in Convolutional Neural Networks (CNNs). This motivated us to study if CNNs trained for low-level visual tasks are deceived by visual illusions. In particular, we show that CNNs trained for image denoising, image deblurring, and computational color constancy are able to replicate the human response to visual illusions, and that the extent of this replication varies with respect to variation in architecture and spatial pattern size. We believe that this CNNs behaviour appears as a by-product of the training for the low level vision tasks of denoising, color constancy or deblurring. Our work opens a new bridge between human perception and CNNs: in order to obtain CNNs that better replicate human behaviour, we may need to start aiming for them to better replicate visual illusions.



### Stacked Spatio-Temporal Graph Convolutional Networks for Action Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1811.10575v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10575v6)
- **Published**: 2018-11-26 18:28:24+00:00
- **Updated**: 2019-06-02 18:21:35+00:00
- **Authors**: Pallabi Ghosh, Yi Yao, Larry S. Davis, Ajay Divakaran
- **Comment**: None
- **Journal**: None
- **Summary**: We propose novel Stacked Spatio-Temporal Graph Convolutional Networks (Stacked-STGCN) for action segmentation, i.e., predicting and localizing a sequence of actions over long videos. We extend the Spatio-Temporal Graph Convolutional Network (STGCN) originally proposed for skeleton-based action recognition to enable nodes with different characteristics (e.g., scene, actor, object, action, etc.), feature descriptors with varied lengths, and arbitrary temporal edge connections to account for large graph deformation commonly associated with complex activities. We further introduce the stacked hourglass architecture to STGCN to leverage the advantages of an encoder-decoder design for improved generalization performance and localization accuracy. We explore various descriptors such as frame-level VGG, segment-level I3D, RCNN-based object, etc. as node descriptors to enable action segmentation based on joint inference over comprehensive contextual information. We show results on CAD120 (which provides pre-computed node features and edge weights for fair performance comparison across algorithms) as well as a more complex real-world activity dataset, Charades. Our Stacked-STGCN in general achieves 4.0% performance improvement over the best reported results in F1 score on CAD120 and 1.3% in mAP on Charades using VGG features.



### Visual Entailment Task for Visually-Grounded Language Learning
- **Arxiv ID**: http://arxiv.org/abs/1811.10582v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10582v2)
- **Published**: 2018-11-26 18:37:25+00:00
- **Updated**: 2019-01-21 03:07:54+00:00
- **Authors**: Ning Xie, Farley Lai, Derek Doran, Asim Kadav
- **Comment**: 4 pages, accepted by Visually Grounded Interaction and Language
  (ViGIL) workshop in NeurIPS 2018
- **Journal**: None
- **Summary**: We introduce a new inference task - Visual Entailment (VE) - which differs from traditional Textual Entailment (TE) tasks whereby a premise is defined by an image, rather than a natural language sentence as in TE tasks. A novel dataset SNLI-VE (publicly available at https://github.com/necla-ml/SNLI-VE) is proposed for VE tasks based on the Stanford Natural Language Inference corpus and Flickr30k. We introduce a differentiable architecture called the Explainable Visual Entailment model (EVE) to tackle the VE problem. EVE and several other state-of-the-art visual question answering (VQA) based models are evaluated on the SNLI-VE dataset, facilitating grounded language understanding and providing insights on how modern VQA based models perform.



### GAN Dissection: Visualizing and Understanding Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1811.10597v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.10597v2)
- **Published**: 2018-11-26 18:59:07+00:00
- **Updated**: 2018-12-08 22:56:10+00:00
- **Authors**: David Bau, Jun-Yan Zhu, Hendrik Strobelt, Bolei Zhou, Joshua B. Tenenbaum, William T. Freeman, Antonio Torralba
- **Comment**: 18 pages, 19 figures
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) have recently achieved impressive results for many real-world applications, and many GAN variants have emerged with improvements in sample quality and training stability. However, they have not been well visualized or understood. How does a GAN represent our visual world internally? What causes the artifacts in GAN results? How do architectural choices affect GAN learning? Answering such questions could enable us to develop new insights and better models.   In this work, we present an analytic framework to visualize and understand GANs at the unit-, object-, and scene-level. We first identify a group of interpretable units that are closely related to object concepts using a segmentation-based network dissection method. Then, we quantify the causal effect of interpretable units by measuring the ability of interventions to control objects in the output. We examine the contextual relationship between these units and their surroundings by inserting the discovered object concepts into new images. We show several practical applications enabled by our framework, from comparing internal representations across different layers, models, and datasets, to improving GANs by locating and removing artifact-causing units, to interactively manipulating objects in a scene. We provide open source interpretation tools to help researchers and practitioners better understand their GAN models.



### Evolving Space-Time Neural Architectures for Videos
- **Arxiv ID**: http://arxiv.org/abs/1811.10636v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1811.10636v2)
- **Published**: 2018-11-26 19:00:12+00:00
- **Updated**: 2019-08-20 18:17:46+00:00
- **Authors**: AJ Piergiovanni, Anelia Angelova, Alexander Toshev, Michael S. Ryoo
- **Comment**: None
- **Journal**: ICCV 2019
- **Summary**: We present a new method for finding video CNN architectures that capture rich spatio-temporal information in videos. Previous work, taking advantage of 3D convolutions, obtained promising results by manually designing video CNN architectures. We here develop a novel evolutionary search algorithm that automatically explores models with different types and combinations of layers to jointly learn interactions between spatial and temporal aspects of video representations. We demonstrate the generality of this algorithm by applying it to two meta-architectures, obtaining new architectures superior to manually designed architectures. Further, we propose a new component, the iTGM layer, which more efficiently utilizes its parameters to allow learning of space-time interactions over longer time horizons. The iTGM layer is often preferred by the evolutionary algorithm and allows building cost-efficient networks. The proposed approach discovers new and diverse video architectures that were previously unknown. More importantly they are both more accurate and faster than prior models, and outperform the state-of-the-art results on multiple datasets we test, including HMDB, Kinetics, and Moments in Time. We will open source the code and models, to encourage future model development.



### Understanding Image Quality and Trust in Peer-to-Peer Marketplaces
- **Arxiv ID**: http://arxiv.org/abs/1811.10648v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4; H.5; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/1811.10648v1)
- **Published**: 2018-11-26 19:10:34+00:00
- **Updated**: 2018-11-26 19:10:34+00:00
- **Authors**: Xiao Ma, Lina Mezghani, Kimberly Wilber, Hui Hong, Robinson Piramuthu, Mor Naaman, Serge Belongie
- **Comment**: WACV 2019
- **Journal**: None
- **Summary**: As any savvy online shopper knows, second-hand peer-to-peer marketplaces are filled with images of mixed quality. How does image quality impact marketplace outcomes, and can quality be automatically predicted? In this work, we conducted a large-scale study on the quality of user-generated images in peer-to-peer marketplaces. By gathering a dataset of common second-hand products (~75,000 images) and annotating a subset with human-labeled quality judgments, we were able to model and predict image quality with decent accuracy (~87%). We then conducted two studies focused on understanding the relationship between these image quality scores and two marketplace outcomes: sales and perceived trustworthiness. We show that image quality is associated with higher likelihood that an item will be sold, though other factors such as view count were better predictors of sales. Nonetheless, we show that high quality user-generated images selected by our models outperform stock imagery in eliciting perceptions of trust from users. Our findings can inform the design of future marketplaces and guide potential sellers to take better product images.



### Noisy Computations during Inference: Harmful or Helpful?
- **Arxiv ID**: http://arxiv.org/abs/1811.10649v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.10649v1)
- **Published**: 2018-11-26 19:18:18+00:00
- **Updated**: 2018-11-26 19:18:18+00:00
- **Authors**: Minghai Qin, Dejan Vucinic
- **Comment**: 20 pages, 11 figures, 11 tables
- **Journal**: None
- **Summary**: We study two aspects of noisy computations during inference. The first aspect is how to mitigate their side effects for naturally trained deep learning systems. One of the motivations for looking into this problem is to reduce the high power cost of conventional computing of neural networks through the use of analog neuromorphic circuits. Traditional GPU/CPU-centered deep learning architectures exhibit bottlenecks in power-restricted applications (e.g., embedded systems). The use of specialized neuromorphic circuits, where analog signals passed through memory-cell arrays are sensed to accomplish matrix-vector multiplications, promises large power savings and speed gains but brings with it the problems of limited precision of computations and unavoidable analog noise. We manage to improve inference accuracy from 21.1% to 99.5% for MNIST images, from 29.9% to 89.1% for CIFAR10, and from 15.5% to 89.6% for MNIST stroke sequences with the presence of strong noise (with signal-to-noise power ratio being 0 dB) by noise-injected training and a voting method. This observation promises neural networks that are insensitive to inference noise, which reduces the quality requirements on neuromorphic circuits and is crucial for their practical usage. The second aspect is how to utilize the noisy inference as a defensive architecture against black-box adversarial attacks. During inference, by injecting proper noise to signals in the neural networks, the robustness of adversarially-trained neural networks against black-box attacks has been further enhanced by 0.5% and 1.13% for two adversarially trained models for MNIST and CIFAR10, respectively.



### Show, Control and Tell: A Framework for Generating Controllable and Grounded Captions
- **Arxiv ID**: http://arxiv.org/abs/1811.10652v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1811.10652v3)
- **Published**: 2018-11-26 19:23:33+00:00
- **Updated**: 2019-05-09 07:58:20+00:00
- **Authors**: Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: Current captioning approaches can describe images using black-box architectures whose behavior is hardly controllable and explainable from the exterior. As an image can be described in infinite ways depending on the goal and the context at hand, a higher degree of controllability is needed to apply captioning algorithms in complex scenarios. In this paper, we introduce a novel framework for image captioning which can generate diverse descriptions by allowing both grounding and controllability. Given a control signal in the form of a sequence or set of image regions, we generate the corresponding caption through a recurrent architecture which predicts textual chunks explicitly grounded on regions, following the constraints of the given control. Experiments are conducted on Flickr30k Entities and on COCO Entities, an extended version of COCO in which we add grounding annotations collected in a semi-automatic manner. Results demonstrate that our method achieves state of the art performances on controllable image captioning, in terms of caption quality and diversity. Code and annotations are publicly available at: https://github.com/aimagelab/show-control-and-tell.



### Art2Real: Unfolding the Reality of Artworks via Semantically-Aware Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/1811.10666v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10666v3)
- **Published**: 2018-11-26 19:51:47+00:00
- **Updated**: 2019-05-17 09:14:40+00:00
- **Authors**: Matteo Tomei, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: The applicability of computer vision to real paintings and artworks has been rarely investigated, even though a vast heritage would greatly benefit from techniques which can understand and process data from the artistic domain. This is partially due to the small amount of annotated artistic data, which is not even comparable to that of natural images captured by cameras. In this paper, we propose a semantic-aware architecture which can translate artworks to photo-realistic visualizations, thus reducing the gap between visual features of artistic and realistic data. Our architecture can generate natural images by retrieving and learning details from real photos through a similarity matching strategy which leverages a weakly-supervised semantic understanding of the scene. Experimental results show that the proposed technique leads to increased realism and to a reduction in domain shift, which improves the performance of pre-trained architectures for classification, detection, and segmentation. Code is publicly available at: https://github.com/aimagelab/art2real.



### Deep Bayesian Self-Training
- **Arxiv ID**: http://arxiv.org/abs/1812.01681v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.01681v3)
- **Published**: 2018-11-26 19:59:06+00:00
- **Updated**: 2019-07-17 15:38:40+00:00
- **Authors**: Fabio De Sousa Ribeiro, Francesco Caliva, Mark Swainson, Kjartan Gudmundsson, Georgios Leontidis, Stefanos Kollias
- **Comment**: 16 pages, 10 figures, 6 tables
- **Journal**: None
- **Summary**: Supervised Deep Learning has been highly successful in recent years, achieving state-of-the-art results in most tasks. However, with the ongoing uptake of such methods in industrial applications, the requirement for large amounts of annotated data is often a challenge. In most real world problems, manual annotation is practically intractable due to time/labour constraints, thus the development of automated and adaptive data annotation systems is highly sought after. In this paper, we propose both a (i) Deep Bayesian Self-Training methodology for automatic data annotation, by leveraging predictive uncertainty estimates using variational inference and modern Neural Network architectures, as well as (ii) a practical adaptation procedure for handling high label variability between different dataset distributions through clustering of Neural Network latent variable representations. An experimental study on both public and private datasets is presented illustrating the superior performance of the proposed approach over standard Self-Training baselines, highlighting the importance of predictive uncertainty estimates in safety-critical domains.



### GANsfer Learning: Combining labelled and unlabelled data for GAN based data augmentation
- **Arxiv ID**: http://arxiv.org/abs/1811.10669v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10669v1)
- **Published**: 2018-11-26 20:08:44+00:00
- **Updated**: 2018-11-26 20:08:44+00:00
- **Authors**: Christopher Bowles, Roger Gunn, Alexander Hammers, Daniel Rueckert
- **Comment**: None
- **Journal**: None
- **Summary**: Medical imaging is a domain which suffers from a paucity of manually annotated data for the training of learning algorithms. Manually delineating pathological regions at a pixel level is a time consuming process, especially in 3D images, and often requires the time of a trained expert. As a result, supervised machine learning solutions must make do with small amounts of labelled data, despite there often being additional unlabelled data available. Whilst of less value than labelled images, these unlabelled images can contain potentially useful information. In this paper we propose combining both labelled and unlabelled data within a GAN framework, before using the resulting network to produce images for use when training a segmentation network. We explore the task of deep grey matter multi-class segmentation in an AD dataset and show that the proposed method leads to a significant improvement in segmentation results, particularly in cases where the amount of labelled data is restricted. We show that this improvement is largely driven by a greater ability to segment the structures known to be the most affected by AD, thereby demonstrating the benefits of exposing the system to more examples of pathological anatomical variation. We also show how a shift in domain of the training data from young and healthy towards older and more pathological examples leads to better segmentations of the latter cases, and that this leads to a significant improvement in the ability for the computed segmentations to stratify cases of AD.



### Adversarial Video Compression Guided by Soft Edge Detection
- **Arxiv ID**: http://arxiv.org/abs/1811.10673v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1811.10673v1)
- **Published**: 2018-11-26 20:22:04+00:00
- **Updated**: 2018-11-26 20:22:04+00:00
- **Authors**: Sungsoo Kim, Jin Soo Park, Christos G. Bampis, Jaeseong Lee, Mia K. Markey, Alexandros G. Dimakis, Alan C. Bovik
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a video compression framework using conditional Generative Adversarial Networks (GANs). We rely on two encoders: one that deploys a standard video codec and another which generates low-level maps via a pipeline of down-sampling, a newly devised soft edge detector, and a novel lossless compression scheme. For decoding, we use a standard video decoder as well as a neural network based one, which is trained using a conditional GAN. Recent "deep" approaches to video compression require multiple videos to pre-train generative networks to conduct interpolation. In contrast to this prior work, our scheme trains a generative decoder on pairs of a very limited number of key frames taken from a single video and corresponding low-level maps. The trained decoder produces reconstructed frames relying on a guidance of low-level maps, without any interpolation. Experiments on a diverse set of 131 videos demonstrate that our proposed GAN-based compression engine achieves much higher quality reconstructions at very low bitrates than prevailing standard codecs such as H.264 or HEVC.



### Matching Features without Descriptors: Implicitly Matched Interest Points
- **Arxiv ID**: http://arxiv.org/abs/1811.10681v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10681v2)
- **Published**: 2018-11-26 20:45:58+00:00
- **Updated**: 2019-08-05 13:17:11+00:00
- **Authors**: Titus Cieslewski, Michael Bloesch, Davide Scaramuzza
- **Comment**: 10 pages without references, accepted for publication at the British
  Machine Vision Conference (BMVC), Cardiff, 2019. v2 contains additional
  results, and a bug in the evaluation of LF-NET has been fixed
- **Journal**: British Machine Vision Conference (BMVC), Cardiff, 2019
- **Summary**: The extraction and matching of interest points is a prerequisite for many geometric computer vision problems. Traditionally, matching has been achieved by assigning descriptors to interest points and matching points that have similar descriptors. In this paper, we propose a method by which interest points are instead already implicitly matched at detection time. With this, descriptors do not need to be calculated, stored, communicated, or matched any more. This is achieved by a convolutional neural network with multiple output channels and can be thought of as a collection of a variety of detectors, each specialized to specific visual features. This paper describes how to design and train such a network in a way that results in successful relative pose estimation performance despite the limitation on interest point count. While the overall matching score is slightly lower than with traditional methods, the approach is descriptor free and thus enables localization systems with a significantly smaller memory footprint and multi-agent localization systems with lower bandwidth requirements. The network also outputs the confidence for a specific interest point resulting in a valid match. We evaluate performance relative to state-of-the-art alternatives.



### Attentive Relational Networks for Mapping Images to Scene Graphs
- **Arxiv ID**: http://arxiv.org/abs/1811.10696v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10696v2)
- **Published**: 2018-11-26 21:36:49+00:00
- **Updated**: 2019-04-06 13:11:09+00:00
- **Authors**: Mengshi Qi, Weijian Li, Zhengyuan Yang, Yunhong Wang, Jiebo Luo
- **Comment**: Accepted by CVPR 2019
- **Journal**: None
- **Summary**: Scene graph generation refers to the task of automatically mapping an image into a semantic structural graph, which requires correctly labeling each extracted object and their interaction relationships. Despite the recent success in object detection using deep learning techniques, inferring complex contextual relationships and structured graph representations from visual data remains a challenging topic. In this study, we propose a novel Attentive Relational Network that consists of two key modules with an object detection backbone to approach this problem. The first module is a semantic transformation module utilized to capture semantic embedded relation features, by translating visual features and linguistic features into a common semantic space. The other module is a graph self-attention module introduced to embed a joint graph representation through assigning various importance weights to neighboring nodes. Finally, accurate scene graphs are produced by the relation inference module to recognize all entities and the corresponding relations. We evaluate our proposed method on the widely-adopted Visual Genome Dataset, and the results demonstrate the effectiveness and superiority of our model.



### LSTA: Long Short-Term Attention for Egocentric Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1811.10698v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10698v3)
- **Published**: 2018-11-26 21:40:03+00:00
- **Updated**: 2019-04-12 09:39:00+00:00
- **Authors**: Swathikiran Sudhakaran, Sergio Escalera, Oswald Lanz
- **Comment**: Accepted to CVPR 2019
- **Journal**: None
- **Summary**: Egocentric activity recognition is one of the most challenging tasks in video analysis. It requires a fine-grained discrimination of small objects and their manipulation. While some methods base on strong supervision and attention mechanisms, they are either annotation consuming or do not take spatio-temporal patterns into account. In this paper we propose LSTA as a mechanism to focus on features from spatial relevant parts while attention is being tracked smoothly across the video sequence. We demonstrate the effectiveness of LSTA on egocentric activity recognition with an end-to-end trainable two-stream architecture, achieving state of the art performance on four standard benchmarks.



### Time-Aware and View-Aware Video Rendering for Unsupervised Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/1811.10699v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10699v2)
- **Published**: 2018-11-26 21:40:38+00:00
- **Updated**: 2018-11-29 15:24:39+00:00
- **Authors**: Shruti Vyas, Yogesh S Rawat, Mubarak Shah
- **Comment**: None
- **Journal**: None
- **Summary**: The recent success in deep learning has lead to various effective representation learning methods for videos. However, the current approaches for video representation require large amount of human labeled datasets for effective learning. We present an unsupervised representation learning framework to encode scene dynamics in videos captured from multiple viewpoints. The proposed framework has two main components: Representation Learning Network (RL-NET), which learns a representation with the help of Blending Network (BL-NET), and Video Rendering Network (VR-NET), which is used for video synthesis. The framework takes as input video clips from different viewpoints and time, learns an internal representation and uses this representation to render a video clip from an arbitrary given viewpoint and time. The ability of the proposed network to render video frames from arbitrary viewpoints and time enable it to learn a meaningful and robust representation of the scene dynamics. We demonstrate the effectiveness of the proposed method in rendering view-aware as well as time-aware video clips on two different real-world datasets including UCF-101 and NTU-RGB+D. To further validate the effectiveness of the learned representation, we use it for the task of view-invariant activity classification where we observe a significant improvement (~26%) in the performance on NTU-RGB+D dataset compared to the existing state-of-the art methods.



### Learning Robust Representations for Automatic Target Recognition
- **Arxiv ID**: http://arxiv.org/abs/1811.10714v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.10714v1)
- **Published**: 2018-11-26 22:08:21+00:00
- **Updated**: 2018-11-26 22:08:21+00:00
- **Authors**: Justin A. Goodwin, Olivia M. Brown, Taylor W. Killian, Sung-Hyun Son
- **Comment**: None
- **Journal**: None
- **Summary**: Radio frequency (RF) sensors are used alongside other sensing modalities to provide rich representations of the world. Given the high variability of complex-valued target responses, RF systems are susceptible to attacks masking true target characteristics from accurate identification. In this work, we evaluate different techniques for building robust classification architectures exploiting learned physical structure in received synthetic aperture radar signals of simulated 3D targets.



### Bilateral Adversarial Training: Towards Fast Training of More Robust Models Against Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/1811.10716v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10716v2)
- **Published**: 2018-11-26 22:15:44+00:00
- **Updated**: 2019-08-01 03:13:41+00:00
- **Authors**: Jianyu Wang, Haichao Zhang
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: In this paper, we study fast training of adversarially robust models. From the analyses of the state-of-the-art defense method, i.e., the multi-step adversarial training, we hypothesize that the gradient magnitude links to the model robustness. Motivated by this, we propose to perturb both the image and the label during training, which we call Bilateral Adversarial Training (BAT). To generate the adversarial label, we derive an closed-form heuristic solution. To generate the adversarial image, we use one-step targeted attack with the target label being the most confusing class. In the experiment, we first show that random start and the most confusing target attack effectively prevent the label leaking and gradient masking problem. Then coupled with the adversarial label part, our model significantly improves the state-of-the-art results. For example, against PGD100 white-box attack with cross-entropy loss, on CIFAR10, we achieve 63.7\% versus 47.2\%; on SVHN, we achieve 59.1\% versus 42.1\%. At last, the experiment on the very (computationally) challenging ImageNet dataset further demonstrates the effectiveness of our fast method.



### Learning View Priors for Single-view 3D Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1811.10719v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1811.10719v2)
- **Published**: 2018-11-26 22:23:44+00:00
- **Updated**: 2019-03-29 15:18:48+00:00
- **Authors**: Hiroharu Kato, Tatsuya Harada
- **Comment**: CVPR 2019. Project page:
  http://hiroharu-kato.com/projects_en/view_prior_learning.html
- **Journal**: None
- **Summary**: There is some ambiguity in the 3D shape of an object when the number of observed views is small. Because of this ambiguity, although a 3D object reconstructor can be trained using a single view or a few views per object, reconstructed shapes only fit the observed views and appear incorrect from the unobserved viewpoints. To reconstruct shapes that look reasonable from any viewpoint, we propose to train a discriminator that learns prior knowledge regarding possible views. The discriminator is trained to distinguish the reconstructed views of the observed viewpoints from those of the unobserved viewpoints. The reconstructor is trained to correct unobserved views by fooling the discriminator. Our method outperforms current state-of-the-art methods on both synthetic and natural image datasets; this validates the effectiveness of our method.



### IGNOR: Image-guided Neural Object Rendering
- **Arxiv ID**: http://arxiv.org/abs/1811.10720v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10720v2)
- **Published**: 2018-11-26 22:24:25+00:00
- **Updated**: 2020-01-15 15:30:46+00:00
- **Authors**: Justus Thies, Michael Zollhöfer, Christian Theobalt, Marc Stamminger, Matthias Nießner
- **Comment**: Video: https://youtu.be/s79HG9yn7QM
- **Journal**: None
- **Summary**: We propose a learned image-guided rendering technique that combines the benefits of image-based rendering and GAN-based image synthesis. The goal of our method is to generate photo-realistic re-renderings of reconstructed objects for virtual and augmented reality applications (e.g., virtual showrooms, virtual tours \& sightseeing, the digital inspection of historical artifacts). A core component of our work is the handling of view-dependent effects. Specifically, we directly train an object-specific deep neural network to synthesize the view-dependent appearance of an object. As input data we are using an RGB video of the object. This video is used to reconstruct a proxy geometry of the object via multi-view stereo. Based on this 3D proxy, the appearance of a captured view can be warped into a new target view as in classical image-based rendering. This warping assumes diffuse surfaces, in case of view-dependent effects, such as specular highlights, it leads to artifacts. To this end, we propose EffectsNet, a deep neural network that predicts view-dependent effects. Based on these estimations, we are able to convert observed images to diffuse images. These diffuse images can be projected into other views. In the target view, our pipeline reinserts the new view-dependent effects. To composite multiple reprojected images to a final output, we learn a composition network that outputs photo-realistic results. Using this image-guided approach, the network does not have to allocate capacity on ``remembering'' object appearance, instead it learns how to combine the appearance of captured images. We demonstrate the effectiveness of our approach both qualitatively and quantitatively on synthetic as well as on real data.



### MIST: Multiple Instance Spatial Transformer Network
- **Arxiv ID**: http://arxiv.org/abs/1811.10725v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10725v5)
- **Published**: 2018-11-26 22:49:20+00:00
- **Updated**: 2020-12-04 19:19:05+00:00
- **Authors**: Baptiste Angles, Yuhe Jin, Simon Kornblith, Andrea Tagliasacchi, Kwang Moo Yi
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a deep network that can be trained to tackle image reconstruction and classification problems that involve detection of multiple object instances, without any supervision regarding their whereabouts. The network learns to extract the most significant top-K patches, and feeds these patches to a task-specific network -- e.g., auto-encoder or classifier -- to solve a domain specific problem. The challenge in training such a network is the non-differentiable top-K selection process. To address this issue, we lift the training optimization problem by treating the result of top-K selection as a slack variable, resulting in a simple, yet effective, multi-stage training. Our method is able to learn to detect recurrent structures in the training dataset by learning to reconstruct images. It can also learn to localize structures when only knowledge on the occurrence of the object is provided, and in doing so it outperforms the state-of-the-art.



### GANtruth - an unpaired image-to-image translation method for driving scenarios
- **Arxiv ID**: http://arxiv.org/abs/1812.01710v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.01710v1)
- **Published**: 2018-11-26 23:19:43+00:00
- **Updated**: 2018-11-26 23:19:43+00:00
- **Authors**: Sebastian Bujwid, Miquel Martí, Hossein Azizpour, Alessandro Pieropan
- **Comment**: 32nd Conference on Neural Information Processing Systems (NeurIPS),
  Machine Learning for Intelligent Transportation Systems Workshop, Montr\'eal,
  Canada. 2018
- **Journal**: None
- **Summary**: Synthetic image translation has significant potentials in autonomous transportation systems. That is due to the expense of data collection and annotation as well as the unmanageable diversity of real-words situations. The main issue with unpaired image-to-image translation is the ill-posed nature of the problem. In this work, we propose a novel method for constraining the output space of unpaired image-to-image translation. We make the assumption that the environment of the source domain is known (e.g. synthetically generated), and we propose to explicitly enforce preservation of the ground-truth labels on the translated images.   We experiment on preserving ground-truth information such as semantic segmentation, disparity, and instance segmentation. We show significant evidence that our method achieves improved performance over the state-of-the-art model of UNIT for translating images from SYNTHIA to Cityscapes. The generated images are perceived as more realistic in human surveys and outperforms UNIT when used in a domain adaptation scenario for semantic segmentation.



### Joint Monocular 3D Vehicle Detection and Tracking
- **Arxiv ID**: http://arxiv.org/abs/1811.10742v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10742v3)
- **Published**: 2018-11-26 23:29:46+00:00
- **Updated**: 2019-09-12 08:50:53+00:00
- **Authors**: Hou-Ning Hu, Qi-Zhi Cai, Dequan Wang, Ji Lin, Min Sun, Philipp Krähenbühl, Trevor Darrell, Fisher Yu
- **Comment**: 18 pages, 12 figures. Add supplementary material. Accepted by ICCV
  2019. Website: https://eborboihuc.github.io/Mono-3DT Code:
  https://github.com/ucbdrive/3d-vehicle-tracking Video:
  https://youtu.be/EJAtOCKI31g
- **Journal**: None
- **Summary**: Vehicle 3D extents and trajectories are critical cues for predicting the future location of vehicles and planning future agent ego-motion based on those predictions. In this paper, we propose a novel online framework for 3D vehicle detection and tracking from monocular videos. The framework can not only associate detections of vehicles in motion over time, but also estimate their complete 3D bounding box information from a sequence of 2D images captured on a moving platform. Our method leverages 3D box depth-ordering matching for robust instance association and utilizes 3D trajectory prediction for re-identification of occluded vehicles. We also design a motion learning module based on an LSTM for more accurate long-term motion extrapolation. Our experiments on simulation, KITTI, and Argoverse datasets show that our 3D tracking pipeline offers robust data association and tracking. On Argoverse, our image-based method is significantly better for tracking 3D vehicles within 30 meters than the LiDAR-centric baseline methods.



