# Arxiv Papers in cs.CV on 2018-11-02
### SDCNet: Video Prediction Using Spatially-Displaced Convolution
- **Arxiv ID**: http://arxiv.org/abs/1811.00684v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.00684v2)
- **Published**: 2018-11-02 00:14:05+00:00
- **Updated**: 2021-03-28 00:13:51+00:00
- **Authors**: Fitsum A. Reda, Guilin Liu, Kevin J. Shih, Robert Kirby, Jon Barker, David Tarjan, Andrew Tao, Bryan Catanzaro
- **Comment**: Published in ECCV 2018. Codes available at
  https://github.com/NVIDIA/semantic-segmentation/tree/sdcnet/sdcnet. Project
  page available at https://nv-adlr.github.io/publication/2018-SDCNet
- **Journal**: None
- **Summary**: We present an approach for high-resolution video frame prediction by conditioning on both past frames and past optical flows. Previous approaches rely on resampling past frames, guided by a learned future optical flow, or on direct generation of pixels. Resampling based on flow is insufficient because it cannot deal with disocclusions. Generative models currently lead to blurry results. Recent approaches synthesis a pixel by convolving input patches with a predicted kernel. However, their memory requirement increases with kernel size. Here, we spatially-displaced convolution (SDC) module for video frame prediction. We learn a motion vector and a kernel for each pixel and synthesize a pixel by applying the kernel at a displaced location in the source image, defined by the predicted motion vector. Our approach inherits the merits of both vector-based and kernel-based approaches, while ameliorating their respective disadvantages. We train our model on 428K unlabelled 1080p video game frames. Our approach produces state-of-the-art results, achieving an SSIM score of 0.904 on high-definition YouTube-8M videos, 0.918 on Caltech Pedestrian videos. Our model handles large motion effectively and synthesizes crisp frames with consistent motion.



### Learning from Large-scale Noisy Web Data with Ubiquitous Reweighting for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1811.00700v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.00700v2)
- **Published**: 2018-11-02 01:26:28+00:00
- **Updated**: 2019-04-03 10:30:02+00:00
- **Authors**: Jia Li, Yafei Song, Jianfeng Zhu, Lele Cheng, Ying Su, Lin Ye, Pengcheng Yuan, Shumin Han
- **Comment**: None
- **Journal**: None
- **Summary**: Many advances of deep learning techniques originate from the efforts of addressing the image classification task on large-scale datasets. However, the construction of such clean datasets is costly and time-consuming since the Internet is overwhelmed by noisy images with inadequate and inaccurate tags. In this paper, we propose a Ubiquitous Reweighting Network (URNet) that learns an image classification model from large-scale noisy data. By observing the web data, we find that there are five key challenges, i.e., imbalanced class sizes, high intra-classes diversity and inter-class similarity, imprecise instances, insufficient representative instances, and ambiguous class labels. To alleviate these challenges, we assume that every training instance has the potential to contribute positively by alleviating the data bias and noise via reweighting the influence of each instance according to different class sizes, large instance clusters, its confidence, small instance bags and the labels. In this manner, the influence of bias and noise in the web data can be gradually alleviated, leading to the steadily improving performance of URNet. Experimental results in the WebVision 2018 challenge with 16 million noisy training images from 5000 classes show that our approach outperforms state-of-the-art models and ranks the first place in the image classification task.



### Similarity Learning with Higher-Order Graph Convolutions for Brain Network Analysis
- **Arxiv ID**: http://arxiv.org/abs/1811.02662v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.02662v5)
- **Published**: 2018-11-02 03:51:45+00:00
- **Updated**: 2019-05-01 21:10:27+00:00
- **Authors**: Guixiang Ma, Nesreen K. Ahmed, Ted Willke, Dipanjan Sengupta, Michael W. Cole, Nicholas B. Turk-Browne, Philip S. Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Learning a similarity metric has gained much attention recently, where the goal is to learn a function that maps input patterns to a target space while preserving the semantic distance in the input space. While most related work focused on images, we focus instead on learning a similarity metric for neuroimages, such as fMRI and DTI images. We propose an end-to-end similarity learning framework called Higher-order Siamese GCN for multi-subject fMRI data analysis. The proposed framework learns the brain network representations via a supervised metric-based approach with siamese neural networks using two graph convolutional networks as the twin networks. Our proposed framework performs higher-order convolutions by incorporating higher-order proximity in graph convolutional networks to characterize and learn the community structure in brain connectivity networks. To the best of our knowledge, this is the first community-preserving similarity learning framework for multi-subject brain network analysis. Experimental results on four real fMRI datasets demonstrate the potential use cases of the proposed framework for multi-subject brain analysis in health and neuropsychiatric disorders. Our proposed approach achieves an average AUC gain of 75% compared to PCA, an average AUC gain of 65.5% compared to Spectral Embedding, and an average AUC gain of 24.3% compared to S-GCN across the four datasets, indicating promising application in clinical investigation and brain disease diagnosis.



### Unique Identification of Macaques for Population Monitoring and Control
- **Arxiv ID**: http://arxiv.org/abs/1811.00743v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.00743v2)
- **Published**: 2018-11-02 05:32:36+00:00
- **Updated**: 2018-11-12 20:55:31+00:00
- **Authors**: Ankita Shukla, Gullal Singh Cheema, Saket Anand, Qamar Qureshi, Yadvendradev Jhala
- **Comment**: None
- **Journal**: None
- **Summary**: Despite loss of natural habitat due to development and urbanization, certain species like the Rhesus macaque have adapted well to the urban environment. With abundant food and no predators, macaque populations have increased substantially in urban areas, leading to frequent conflicts with humans. Overpopulated areas often witness macaques raiding crops, feeding on bird and snake eggs as well as destruction of nests, thus adversely affecting other species in the ecosystem. In order to mitigate these adverse effects, sterilization has emerged as a humane and effective way of population control of macaques. As sterilization requires physical capture of individuals or groups, their unique identification is integral to such control measures. In this work, we propose the Macaque Face Identification (MFID), an image based, non-invasive tool that relies on macaque facial recognition to identify individuals, and can be used to verify if they are sterilized. Our primary contribution is a robust facial recognition and verification module designed for Rhesus macaques, but extensible to other non-human primate species. We evaluate the performance of MFID on a dataset of 93 monkeys under closed set, open set and verification evaluation protocols. Finally, we also report state of the art results when evaluating our proposed model on endangered primate species.



### Show, Attend and Read: A Simple and Strong Baseline for Irregular Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/1811.00751v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.00751v2)
- **Published**: 2018-11-02 06:13:16+00:00
- **Updated**: 2019-03-16 05:58:16+00:00
- **Authors**: Hui Li, Peng Wang, Chunhua Shen, Guyu Zhang
- **Comment**: Accepted to Proc. AAAI Conference on Artificial Intelligence 2019
- **Journal**: None
- **Summary**: Recognizing irregular text in natural scene images is challenging due to the large variance in text appearance, such as curvature, orientation and distortion. Most existing approaches rely heavily on sophisticated model designs and/or extra fine-grained annotations, which, to some extent, increase the difficulty in algorithm implementation and data collection. In this work, we propose an easy-to-implement strong baseline for irregular scene text recognition, using off-the-shelf neural network components and only word-level annotations. It is composed of a $31$-layer ResNet, an LSTM-based encoder-decoder framework and a 2-dimensional attention module. Despite its simplicity, the proposed method is robust and achieves state-of-the-art performance on both regular and irregular scene text recognition benchmarks. Code is available at: https://tinyurl.com/ShowAttendRead



### Dealing with Ambiguity in Robotic Grasping via Multiple Predictions
- **Arxiv ID**: http://arxiv.org/abs/1811.00793v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.00793v1)
- **Published**: 2018-11-02 09:42:15+00:00
- **Updated**: 2018-11-02 09:42:15+00:00
- **Authors**: Ghazal Ghazaei, Iro Laina, Christian Rupprecht, Federico Tombari, Nassir Navab, Kianoush Nazarpour
- **Comment**: ACCV 2018
- **Journal**: None
- **Summary**: Humans excel in grasping and manipulating objects because of their life-long experience and knowledge about the 3D shape and weight distribution of objects. However, the lack of such intuition in robots makes robotic grasping an exceptionally challenging task. There are often several equally viable options of grasping an object. However, this ambiguity is not modeled in conventional systems that estimate a single, optimal grasp position. We propose to tackle this problem by simultaneously estimating multiple grasp poses from a single RGB image of the target object. Further, we reformulate the problem of robotic grasping by replacing conventional grasp rectangles with grasp belief maps, which hold more precise location information than a rectangle and account for the uncertainty inherent to the task. We augment a fully convolutional neural network with a multiple hypothesis prediction model that predicts a set of grasp hypotheses in under 60ms, which is critical for real-time robotic applications. The grasp detection accuracy reaches over 90% for unseen objects, outperforming the current state of the art on this task.



### Heterogeneity Aware Deep Embedding for Mobile Periocular Recognition
- **Arxiv ID**: http://arxiv.org/abs/1811.00846v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.00846v1)
- **Published**: 2018-11-02 13:25:38+00:00
- **Updated**: 2018-11-02 13:25:38+00:00
- **Authors**: Rishabh Garg, Yashasvi Baweja, Soumyadeep Ghosh, Mayank Vatsa, Richa Singh, Nalini Ratha
- **Comment**: None
- **Journal**: None
- **Summary**: Mobile biometric approaches provide the convenience of secure authentication with an omnipresent technology. However, this brings an additional challenge of recognizing biometric patterns in unconstrained environment including variations in mobile camera sensors, illumination conditions, and capture distance. To address the heterogeneous challenge, this research presents a novel heterogeneity aware loss function within a deep learning framework. The effectiveness of the proposed loss function is evaluated for periocular biometrics using the CSIP, IMP and VISOB mobile periocular databases. The results show that the proposed algorithm yields state-of-the-art results in a heterogeneous environment and improves generalizability for cross-database experiments.



### Classification of Findings with Localized Lesions in Fundoscopic Images using a Regionally Guided CNN
- **Arxiv ID**: http://arxiv.org/abs/1811.00871v1
- **DOI**: 10.1007/978-3-030-00949-6_21
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.00871v1)
- **Published**: 2018-11-02 14:15:14+00:00
- **Updated**: 2018-11-02 14:15:14+00:00
- **Authors**: Jaemin Son, Woong Bae, Sangkeun Kim, Sang Jun Park, Kyu-Hwan Jung
- **Comment**: 8 pages, Computational Pathology and Ophthalmic Medical Image
  Analysis, pp.176-184
- **Journal**: None
- **Summary**: Fundoscopic images are often investigated by ophthalmologists to spot abnormal lesions to make diagnoses. Recent successes of convolutional neural networks are confined to diagnoses of few diseases without proper localization of lesion. In this paper, we propose an efficient annotation method for localizing lesions and a CNN architecture that can classify an individual finding and localize the lesions at the same time. Also, we introduce a new loss function to guide the network to learn meaningful patterns with the guidance of the regional annotations. In experiments, we demonstrate that our network performed better than the widely used network and the guidance loss helps achieve higher AUROC up to 4.1% and superior localization capability.



### The Open Images Dataset V4: Unified image classification, object detection, and visual relationship detection at scale
- **Arxiv ID**: http://arxiv.org/abs/1811.00982v2
- **DOI**: 10.1007/s11263-020-01316-z
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.00982v2)
- **Published**: 2018-11-02 16:58:28+00:00
- **Updated**: 2020-02-21 15:15:33+00:00
- **Authors**: Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, Tom Duerig, Vittorio Ferrari
- **Comment**: Accepted to International Journal of Computer Vision, 2020
- **Journal**: None
- **Summary**: We present Open Images V4, a dataset of 9.2M images with unified annotations for image classification, object detection and visual relationship detection. The images have a Creative Commons Attribution license that allows to share and adapt the material, and they have been collected from Flickr without a predefined list of class names or tags, leading to natural class statistics and avoiding an initial design bias. Open Images V4 offers large scale across several dimensions: 30.1M image-level labels for 19.8k concepts, 15.4M bounding boxes for 600 object classes, and 375k visual relationship annotations involving 57 classes. For object detection in particular, we provide 15x more bounding boxes than the next largest datasets (15.4M boxes on 1.9M images). The images often show complex scenes with several objects (8 annotated objects per image on average). We annotated visual relationships between them, which support visual relationship detection, an emerging task that requires structured reasoning. We provide in-depth comprehensive statistics about the dataset, we validate the quality of the annotations, we study how the performance of several modern models evolves with increasing amounts of training data, and we demonstrate two applications made possible by having unified annotations of multiple types coexisting in the same images. We hope that the scale, quality, and variety of Open Images V4 will foster further research and innovation even beyond the areas of image classification, object detection, and visual relationship detection.



### Invertible Residual Networks
- **Arxiv ID**: http://arxiv.org/abs/1811.00995v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.00995v3)
- **Published**: 2018-11-02 17:17:55+00:00
- **Updated**: 2019-05-18 18:19:33+00:00
- **Authors**: Jens Behrmann, Will Grathwohl, Ricky T. Q. Chen, David Duvenaud, Jörn-Henrik Jacobsen
- **Comment**: None
- **Journal**: Proceedings of the International Conference on Machine Learning
  (ICML), 2019
- **Summary**: We show that standard ResNet architectures can be made invertible, allowing the same model to be used for classification, density estimation, and generation. Typically, enforcing invertibility requires partitioning dimensions or restricting network architectures. In contrast, our approach only requires adding a simple normalization step during training, already available in standard frameworks. Invertible ResNets define a generative model which can be trained by maximum likelihood on unlabeled data. To compute likelihoods, we introduce a tractable approximation to the Jacobian log-determinant of a residual block. Our empirical evaluation shows that invertible ResNets perform competitively with both state-of-the-art image classifiers and flow-based generative models, something that has not been previously achieved with a single architecture.



### Scalable Deep $k$-Subspace Clustering
- **Arxiv ID**: http://arxiv.org/abs/1811.01045v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.01045v1)
- **Published**: 2018-11-02 18:54:10+00:00
- **Updated**: 2018-11-02 18:54:10+00:00
- **Authors**: Tong Zhang, Pan Ji, Mehrtash Harandi, Richard Hartley, Ian Reid
- **Comment**: To appear in ACCV 2018
- **Journal**: None
- **Summary**: Subspace clustering algorithms are notorious for their scalability issues because building and processing large affinity matrices are demanding. In this paper, we introduce a method that simultaneously learns an embedding space along subspaces within it to minimize a notion of reconstruction error, thus addressing the problem of subspace clustering in an end-to-end learning paradigm. To achieve our goal, we propose a scheme to update subspaces within a deep neural network. This in turn frees us from the need of having an affinity matrix to perform clustering. Unlike previous attempts, our method can easily scale up to large datasets, making it unique in the context of unsupervised learning with deep architectures. Our experiments show that our method significantly improves the clustering accuracy while enjoying cheaper memory footprints.



### What evidence does deep learning model use to classify Skin Lesions?
- **Arxiv ID**: http://arxiv.org/abs/1811.01051v3
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1811.01051v3)
- **Published**: 2018-11-02 18:57:50+00:00
- **Updated**: 2019-02-13 13:54:47+00:00
- **Authors**: Xiaoxiao Li, Junyan Wu, Eric Z. Chen, Hongda Jiang
- **Comment**: 5 pages
- **Journal**: None
- **Summary**: Melanoma is a type of skin cancer with the most rapidly increasing incidence. Early detection of melanoma using dermoscopy images significantly increases patients' survival rate. However, accurately classifying skin lesions by eye, especially in the early stage of melanoma, is extremely challenging for the dermatologists. Hence, the discovery of reliable biomarkers will be meaningful for melanoma diagnosis. Recent years, the value of deep learning empowered computer-assisted diagnose has been shown in biomedical imaging based decision making. However, much research focuses on improving disease detection accuracy but not exploring the evidence of pathology. In this paper, we propose a method to interpret the deep learning classification findings. Firstly, we propose an accurate neural network architecture to classify skin lesions. Secondly, we utilize a prediction difference analysis method that examines each patch on the image through patch-wised corrupting to detect the biomarkers. Lastly, we validate that our biomarker findings are corresponding to the patterns in the literature. The findings can be significant and useful to guide clinical diagnosis.



### 3D Pick & Mix: Object Part Blending in Joint Shape and Image Manifolds
- **Arxiv ID**: http://arxiv.org/abs/1811.01068v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.01068v1)
- **Published**: 2018-11-02 19:56:55+00:00
- **Updated**: 2018-11-02 19:56:55+00:00
- **Authors**: Adrian Penate-Sanchez, Lourdes Agapito
- **Comment**: None
- **Journal**: None
- **Summary**: We present 3D Pick & Mix, a new 3D shape retrieval system that provides users with a new level of freedom to explore 3D shape and Internet image collections by introducing the ability to reason about objects at the level of their constituent parts. While classic retrieval systems can only formulate simple searches such as "find the 3D model that is most similar to the input image" our new approach can formulate advanced and semantically meaningful search queries such as: "find me the 3D model that best combines the design of the legs of the chair in image 1 but with no armrests, like the chair in image 2". Many applications could benefit from such rich queries, users could browse through catalogues of furniture and pick and mix parts, combining for example the legs of a chair from one shop and the armrests from another shop.



### Ischemic Stroke Lesion Segmentation in CT Perfusion Scans using Pyramid Pooling and Focal Loss
- **Arxiv ID**: http://arxiv.org/abs/1811.01085v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1811.01085v1)
- **Published**: 2018-11-02 20:55:20+00:00
- **Updated**: 2018-11-02 20:55:20+00:00
- **Authors**: S. Mazdak Abulnaga, Jonathan Rubin
- **Comment**: BrainLes 2018 MICCAI workshop
- **Journal**: None
- **Summary**: We present a fully convolutional neural network for segmenting ischemic stroke lesions in CT perfusion images for the ISLES 2018 challenge. Treatment of stroke is time sensitive and current standards for lesion identification require manual segmentation, a time consuming and challenging process. Automatic segmentation methods present the possibility of accurately identifying lesions and improving treatment planning. Our model is based on the PSPNet, a network architecture that makes use of pyramid pooling to provide global and local contextual information. To learn the varying shapes of the lesions, we train our network using focal loss, a loss function designed for the network to focus on learning the more difficult samples. We compare our model to networks trained using the U-Net and V-Net architectures. Our approach demonstrates effective performance in lesion segmentation and ranked among the top performers at the challenge conclusion.



### Low-Rank Tensor Modeling for Hyperspectral Unmixing Accounting for Spectral Variability
- **Arxiv ID**: http://arxiv.org/abs/1811.02413v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.02413v3)
- **Published**: 2018-11-02 21:09:58+00:00
- **Updated**: 2019-10-23 18:21:53+00:00
- **Authors**: Tales Imbiriba, Ricardo Augusto Borsoi, José Carlos Moreira Bermudez
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1803.06355
- **Journal**: None
- **Summary**: Traditional hyperspectral unmixing methods neglect the underlying variability of spectral signatures often observed in typical hyperspectral images (HI), propagating these missmodeling errors throughout the whole unmixing process. Attempts to model material spectra as members of sets or as random variables tend to lead to severely ill-posed unmixing problems. Although parametric models have been proposed to overcome this drawback by handling endmember variability through generalizations of the mixing model, the success of these techniques depend on employing appropriate regularization strategies. Moreover, the existing approaches fail to adequately explore the natural multidimensinal representation of HIs. Recently, tensor-based strategies considered low-rank decompositions of hyperspectral images as an alternative to impose low-dimensional structures on the solutions of standard and multitemporal unmixing problems. These strategies, however, present two main drawbacks: 1) they confine the solutions to low-rank tensors, which often cannot represent the complexity of real-world scenarios; and 2) they lack guarantees that endmembers and abundances will be correctly factorized in their respective tensors. In this work, we propose a more flexible approach, called ULTRA-V, that imposes low-rank structures through regularizations whose strictness is controlled by scalar parameters. Simulations attest the superior accuracy of the method when compared with state-of-the-art unmixing algorithms that account for spectral variability.



