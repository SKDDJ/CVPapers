# Arxiv Papers in cs.CV on 2018-11-19
### Pixel-Anchor: A Fast Oriented Scene Text Detector with Combined Networks
- **Arxiv ID**: http://arxiv.org/abs/1811.07432v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07432v1)
- **Published**: 2018-11-19 00:26:42+00:00
- **Updated**: 2018-11-19 00:26:42+00:00
- **Authors**: Yuan Li, Yuanjie Yu, Zefeng Li, Yangkun Lin, Meifang Xu, Jiwei Li, Xi Zhou
- **Comment**: 10 pages, 11 figures, 3 tables
- **Journal**: None
- **Summary**: Recently, semantic segmentation and general object detection frameworks have been widely adopted by scene text detecting tasks. However, both of them alone have obvious shortcomings in practice. In this paper, we propose a novel end-to-end trainable deep neural network framework, named Pixel-Anchor, which combines semantic segmentation and SSD in one network by feature sharing and anchor-level attention mechanism to detect oriented scene text. To deal with scene text which has large variances in size and aspect ratio, we combine FPN and ASPP operation as our encoder-decoder structure in the semantic segmentation part, and propose a novel Adaptive Predictor Layer in the SSD. Pixel-Anchor detects scene text in a single network forward pass, no complex post-processing other than an efficient fusion Non-Maximum Suppression is involved. We have benchmarked the proposed Pixel-Anchor on the public datasets. Pixel-Anchor outperforms the competing methods in terms of text localization accuracy and run speed, more specifically, on the ICDAR 2015 dataset, the proposed algorithm achieves an F-score of 0.8768 at 10 FPS for 960 x 1728 resolution images.



### CompoNet: Learning to Generate the Unseen by Part Synthesis and Composition
- **Arxiv ID**: http://arxiv.org/abs/1811.07441v4
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.07441v4)
- **Published**: 2018-11-19 00:45:17+00:00
- **Updated**: 2019-09-01 19:30:51+00:00
- **Authors**: Nadav Schor, Oren Katzir, Hao Zhang, Daniel Cohen-Or
- **Comment**: Accepted to ICCV 2019. Code: https://github.com/nschor/CompoNet
- **Journal**: None
- **Summary**: Data-driven generative modeling has made remarkable progress by leveraging the power of deep neural networks. A reoccurring challenge is how to enable a model to generate a rich variety of samples from the entire target distribution, rather than only from a distribution confined to the training data. In other words, we would like the generative model to go beyond the observed samples and learn to generate ``unseen'', yet still plausible, data. In our work, we present CompoNet, a generative neural network for 2D or 3D shapes that is based on a part-based prior, where the key idea is for the network to synthesize shapes by varying both the shape parts and their compositions. Treating a shape not as an unstructured whole, but as a (re-)composable set of deformable parts, adds a combinatorial dimension to the generative process to enrich the diversity of the output, encouraging the generator to venture more into the ``unseen''. We show that our part-based model generates richer variety of plausible shapes compared with baseline generative models. To this end, we introduce two quantitative metrics to evaluate the diversity of a generative model and assess how well the generated data covers both the training data and unseen data from the same target distribution. Code is available at https://github.com/nschor/CompoNet.



### Predictive and Semantic Layout Estimation for Robotic Applications in Manhattan Worlds
- **Arxiv ID**: http://arxiv.org/abs/1811.07442v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1811.07442v1)
- **Published**: 2018-11-19 00:49:54+00:00
- **Updated**: 2018-11-19 00:49:54+00:00
- **Authors**: Armon Shariati, Bernd Pfrommer, Camillo J. Taylor
- **Comment**: None
- **Journal**: None
- **Summary**: This paper describes an approach to automatically extracting floor plans from the kinds of incomplete measurements that could be acquired by an autonomous mobile robot. The approach proceeds by reasoning about extended structural layout surfaces which are automatically extracted from the available data. The scheme can be run in an online manner to build water tight representations of the environment. The system effectively speculates about room boundaries and free space regions which provides useful guidance to subsequent motion planning systems. Experimental results are presented on multiple data sets.



### Hybrid Feature Learning for Handwriting Verification
- **Arxiv ID**: http://arxiv.org/abs/1812.02621v1
- **DOI**: 10.1109/ICFHR-2018.2018.00041
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.02621v1)
- **Published**: 2018-11-19 02:02:28+00:00
- **Updated**: 2018-11-19 02:02:28+00:00
- **Authors**: Mohammad Abuzar Shaikh, Mihir Chauhan, Jun Chu, Sargur Srihari
- **Comment**: Accepted and presented in International Conference on Frontiers in
  Handwriting Recognition (ICFHR) 2018
- **Journal**: None
- **Summary**: We propose an effective Hybrid Deep Learning (HDL) architecture for the task of determining the probability that a questioned handwritten word has been written by a known writer. HDL is an amalgamation of Auto-Learned Features (ALF) and Human-Engineered Features (HEF). To extract auto-learned features we use two methods: First, Two Channel Convolutional Neural Network (TC-CNN); Second, Two Channel Autoencoder (TC-AE). Furthermore, human-engineered features are extracted by using two methods: First, Gradient Structural Concavity (GSC); Second, Scale Invariant Feature Transform (SIFT). Experiments are performed by complementing one of the HEF methods with one ALF method on 150000 pairs of samples of the word "AND" cropped from handwritten notes written by 1500 writers. Our results indicate that HDL architecture with AE-GSC achieves 99.7% accuracy on seen writer dataset and 92.16% accuracy on shuffled writer dataset which out performs CEDAR-FOX, as for unseen writer dataset, AE-SIFT performs comparable to this sophisticated handwriting comparison tool.



### Larger Norm More Transferable: An Adaptive Feature Norm Approach for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1811.07456v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07456v2)
- **Published**: 2018-11-19 02:04:58+00:00
- **Updated**: 2019-08-02 15:25:11+00:00
- **Authors**: Ruijia Xu, Guanbin Li, Jihan Yang, Liang Lin
- **Comment**: Accepted as an Oral presentation at ICCV2019
- **Journal**: None
- **Summary**: Domain adaptation enables the learner to safely generalize into novel environments by mitigating domain shifts across distributions. Previous works may not effectively uncover the underlying reasons that would lead to the drastic model degradation on the target task. In this paper, we empirically reveal that the erratic discrimination of the target domain mainly stems from its much smaller feature norms with respect to that of the source domain. To this end, we propose a novel parameter-free Adaptive Feature Norm approach. We demonstrate that progressively adapting the feature norms of the two domains to a large range of values can result in significant transfer gains, implying that those task-specific features with larger norms are more transferable. Our method successfully unifies the computation of both standard and partial domain adaptation with more robustness against the negative transfer issue. Without bells and whistles but a few lines of code, our method substantially lifts the performance on the target task and exceeds state-of-the-arts by a large margin (11.5% on Office-Home and 17.1% on VisDA2017). We hope our simple yet effective approach will shed some light on the future research of transfer learning. Code is available at https://github.com/jihanyang/AFN.



### Transfer Learning Using Classification Layer Features of CNN
- **Arxiv ID**: http://arxiv.org/abs/1811.07459v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07459v2)
- **Published**: 2018-11-19 02:11:08+00:00
- **Updated**: 2019-03-27 13:55:09+00:00
- **Authors**: Tasfia Shermin, Manzur Murshed, Guojun Lu, Shyh Wei Teng
- **Comment**: None
- **Journal**: None
- **Summary**: Although CNNs have gained the ability to transfer learned knowledge from source task to target task by virtue of large annotated datasets but consume huge processing time to fine-tune without GPU. In this paper, we propose a new computationally efficient transfer learning approach using classification layer features of pre-trained CNNs by appending layer after existing classification layer. We demonstrate that fine-tuning of the appended layer with existing classification layer for new task converges much faster than baseline and in average outperforms baseline classification accuracy. Furthermore, we execute thorough experiments to examine the influence of quantity, similarity, and dissimilarity of training sets in our classification outcomes to demonstrate transferability of classification layer features.



### Segregated Temporal Assembly Recurrent Networks for Weakly Supervised Multiple Action Detection
- **Arxiv ID**: http://arxiv.org/abs/1811.07460v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07460v1)
- **Published**: 2018-11-19 02:12:06+00:00
- **Updated**: 2018-11-19 02:12:06+00:00
- **Authors**: Yunlu Xu, Chengwei Zhang, Zhanzhan Cheng, Jianwen Xie, Yi Niu, Shiliang Pu, Fei Wu
- **Comment**: Accepted to Proc. AAAI Conference on Artificial Intelligence 2019
- **Journal**: None
- **Summary**: This paper proposes a segregated temporal assembly recurrent (STAR) network for weakly-supervised multiple action detection. The model learns from untrimmed videos with only supervision of video-level labels and makes prediction of intervals of multiple actions. Specifically, we first assemble video clips according to class labels by an attention mechanism that learns class-variable attention weights and thus helps the noise relieving from background or other actions. Secondly, we build temporal relationship between actions by feeding the assembled features into an enhanced recurrent neural network. Finally, we transform the output of recurrent neural network into the corresponding action distribution. In order to generate more precise temporal proposals, we design a score term called segregated temporal gradient-weighted class activation mapping (ST-GradCAM) fused with attention weights. Experiments on THUMOS'14 and ActivityNet1.3 datasets show that our approach outperforms the state-of-the-art weakly-supervised method, and performs at par with the fully-supervised counterparts.



### Indoor GeoNet: Weakly Supervised Hybrid Learning for Depth and Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1811.07461v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07461v1)
- **Published**: 2018-11-19 02:17:10+00:00
- **Updated**: 2018-11-19 02:17:10+00:00
- **Authors**: Amirreza Farnoosh, Sarah Ostadabbas
- **Comment**: None
- **Journal**: None
- **Summary**: Humans naturally perceive a 3D scene in front of them through accumulation of information obtained from multiple interconnected projections of the scene and by interpreting their correspondence. This phenomenon has inspired artificial intelligence models to extract the depth and view angle of the observed scene by modeling the correspondence between different views of that scene. Our paper is built upon previous works in the field of unsupervised depth and relative camera pose estimation from temporal consecutive video frames using deep learning (DL) models. Our approach uses a hybrid learning framework introduced in a recent work called GeoNet, which leverages geometric constraints in the 3D scenes to synthesize a novel view from intermediate DL-based predicted depth and relative pose. However, the state-of-the-art unsupervised depth and pose estimation DL models are exclusively trained/tested on a few available outdoor scene datasets and we have shown they are hardly transferable to new scenes, especially from indoor environments, in which estimation requires higher precision and dealing with probable occlusions. This paper introduces "Indoor GeoNet", a weakly supervised depth and camera pose estimation model targeted for indoor scenes. In Indoor GeoNet, we take advantage of the availability of indoor RGBD datasets collected by human or robot navigators, and added partial (i.e. weak) supervision in depth training into the model. Experimental results showed that our model effectively generalizes to new scenes from different buildings. Indoor GeoNet demonstrated significant depth and pose estimation error reduction when compared to the original GeoNet, while showing 3 times more reconstruction accuracy in synthesizing novel views in indoor environments.



### Bayesian Cycle-Consistent Generative Adversarial Networks via Marginalizing Latent Sampling
- **Arxiv ID**: http://arxiv.org/abs/1811.07465v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.07465v3)
- **Published**: 2018-11-19 02:22:49+00:00
- **Updated**: 2020-08-16 19:38:06+00:00
- **Authors**: Haoran You, Yu Cheng, Tianheng Cheng, Chunliang Li, Pan Zhou
- **Comment**: Accepted by IEEE TNNLS
- **Journal**: None
- **Summary**: Recent techniques built on Generative Adversarial Networks (GANs), such as Cycle-Consistent GANs, are able to learn mappings among different domains built from unpaired datasets, through min-max optimization games between generators and discriminators. However, it remains challenging to stabilize the training process and thus cyclic models fall into mode collapse accompanied by the success of discriminator. To address this problem, we propose an novel Bayesian cyclic model and an integrated cyclic framework for inter-domain mappings. The proposed method motivated by Bayesian GAN explores the full posteriors of cyclic model via sampling latent variables and optimizes the model with maximum a posteriori (MAP) estimation. Hence, we name it Bayesian CycleGAN. In addition, original CycleGAN cannot generate diversified results. But it is feasible for Bayesian framework to diversify generated images by replacing restricted latent variables in inference process. We evaluate the proposed Bayesian CycleGAN on multiple benchmark datasets, including Cityscapes, Maps, and Monet2photo. The proposed method improve the per-pixel accuracy by 15% for the Cityscapes semantic segmentation task within origin framework and improve 20% within the proposed integrated framework, showing better resilience to imbalance confrontation. The diversified results of Monet2Photo style transfer also demonstrate its superiority over original cyclic model. We provide codes for all of our experiments in https://github.com/ranery/Bayesian-CycleGAN.



### Multi-scale 3D Convolution Network for Video Based Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1811.07468v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07468v1)
- **Published**: 2018-11-19 02:40:32+00:00
- **Updated**: 2018-11-19 02:40:32+00:00
- **Authors**: Jianing Li, Shiliang Zhang, Tiejun Huang
- **Comment**: AAAI, 2019
- **Journal**: None
- **Summary**: This paper proposes a two-stream convolution network to extract spatial and temporal cues for video based person Re-Identification (ReID). A temporal stream in this network is constructed by inserting several Multi-scale 3D (M3D) convolution layers into a 2D CNN network. The resulting M3D convolution network introduces a fraction of parameters into the 2D CNN, but gains the ability of multi-scale temporal feature learning. With this compact architecture, M3D convolution network is also more efficient and easier to optimize than existing 3D convolution networks. The temporal stream further involves Residual Attention Layers (RAL) to refine the temporal features. By jointly learning spatial-temporal attention masks in a residual manner, RAL identifies the discriminative spatial regions and temporal cues. The other stream in our network is implemented with a 2D CNN for spatial feature extraction. The spatial and temporal features from two streams are finally fused for the video based person ReID. Evaluations on three widely used benchmarks datasets, i.e., MARS, PRID2011, and iLIDS-VID demonstrate the substantial advantages of our method over existing 3D convolution networks and state-of-art methods.



### Global and Local Sensitivity Guided Key Salient Object Re-augmentation for Video Saliency Detection
- **Arxiv ID**: http://arxiv.org/abs/1811.07480v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07480v1)
- **Published**: 2018-11-19 03:27:23+00:00
- **Updated**: 2018-11-19 03:27:23+00:00
- **Authors**: Ziqi Zhou, Zheng Wang, Huchuan Lu, Song Wang, Meijun Sun
- **Comment**: 6 figures, 10 pages
- **Journal**: None
- **Summary**: The existing still-static deep learning based saliency researches do not consider the weighting and highlighting of extracted features from different layers, all features contribute equally to the final saliency decision-making. Such methods always evenly detect all "potentially significant regions" and unable to highlight the key salient object, resulting in detection failure of dynamic scenes. In this paper, based on the fact that salient areas in videos are relatively small and concentrated, we propose a \textbf{key salient object re-augmentation method (KSORA) using top-down semantic knowledge and bottom-up feature guidance} to improve detection accuracy in video scenes. KSORA includes two sub-modules (WFE and KOS): WFE processes local salient feature selection using bottom-up strategy, while KOS ranks each object in global fashion by top-down statistical knowledge, and chooses the most critical object area for local enhancement. The proposed KSORA can not only strengthen the saliency value of the local key salient object but also ensure global saliency consistency. Results on three benchmark datasets suggest that our model has the capability of improving the detection accuracy on complex scenes. The significant performance of KSORA, with a speed of 17FPS on modern GPUs, has been verified by comparisons with other ten state-of-the-art algorithms.



### Show, Attend and Translate: Unpaired Multi-Domain Image-to-Image Translation with Visual Attention
- **Arxiv ID**: http://arxiv.org/abs/1811.07483v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07483v2)
- **Published**: 2018-11-19 03:37:52+00:00
- **Updated**: 2019-04-08 09:09:15+00:00
- **Authors**: Honglun Zhang, Wenqing Chen, Jidong Tian, Yongkun Wang, Yaohui Jin
- **Comment**: None
- **Journal**: None
- **Summary**: Recently unpaired multi-domain image-to-image translation has attracted great interests and obtained remarkable progress, where a label vector is utilized to indicate multi-domain information. In this paper, we propose SAT (Show, Attend and Translate), an unified and explainable generative adversarial network equipped with visual attention that can perform unpaired image-to-image translation for multiple domains. By introducing an action vector, we treat the original translation tasks as problems of arithmetic addition and subtraction. Visual attention is applied to guarantee that only the regions relevant to the target domains are translated. Extensive experiments on a facial attribute dataset demonstrate the superiority of our approach and the generated attention masks better explain what SAT attends when translating images.



### Sharpen Focus: Learning with Attention Separability and Consistency
- **Arxiv ID**: http://arxiv.org/abs/1811.07484v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.07484v3)
- **Published**: 2018-11-19 03:49:19+00:00
- **Updated**: 2019-08-07 21:10:26+00:00
- **Authors**: Lezi Wang, Ziyan Wu, Srikrishna Karanam, Kuan-Chuan Peng, Rajat Vikram Singh, Bo Liu, Dimitris N. Metaxas
- **Comment**: This paper is accepted to ICCV 2019. The supplementary material
  (appendix) can be found after the main paper
- **Journal**: None
- **Summary**: Recent developments in gradient-based attention modeling have seen attention maps emerge as a powerful tool for interpreting convolutional neural networks. Despite good localization for an individual class of interest, these techniques produce attention maps with substantially overlapping responses among different classes, leading to the problem of visual confusion and the need for discriminative attention. In this paper, we address this problem by means of a new framework that makes class-discriminative attention a principled part of the learning process. Our key innovations include new learning objectives for attention separability and cross-layer consistency, which result in improved attention discriminability and reduced visual confusion. Extensive experiments on image classification benchmarks show the effectiveness of our approach in terms of improved classification accuracy, including CIFAR-100 (+3.33%), Caltech-256 (+1.64%), ILSVRC2012 (+0.92%), CUB-200-2011 (+4.8%) and PASCAL VOC2012 (+5.73%).



### Visual-Texual Emotion Analysis with Deep Coupled Video and Danmu Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1811.07485v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1811.07485v1)
- **Published**: 2018-11-19 03:51:19+00:00
- **Updated**: 2018-11-19 03:51:19+00:00
- **Authors**: Chenchen Li, Jialin Wang, Hongwei Wang, Miao Zhao, Wenjie Li, Xiaotie Deng
- **Comment**: Draft, 25 pages
- **Journal**: None
- **Summary**: User emotion analysis toward videos is to automatically recognize the general emotional status of viewers from the multimedia content embedded in the online video stream. Existing works fall in two categories: 1) visual-based methods, which focus on visual content and extract a specific set of features of videos. However, it is generally hard to learn a mapping function from low-level video pixels to high-level emotion space due to great intra-class variance. 2) textual-based methods, which focus on the investigation of user-generated comments associated with videos. The learned word representations by traditional linguistic approaches typically lack emotion information and the global comments usually reflect viewers' high-level understandings rather than instantaneous emotions. To address these limitations, in this paper, we propose to jointly utilize video content and user-generated texts simultaneously for emotion analysis. In particular, we introduce exploiting a new type of user-generated texts, i.e., "danmu", which are real-time comments floating on the video and contain rich information to convey viewers' emotional opinions. To enhance the emotion discriminativeness of words in textual feature extraction, we propose Emotional Word Embedding (EWE) to learn text representations by jointly considering their semantics and emotions. Afterwards, we propose a novel visual-textual emotion analysis model with Deep Coupled Video and Danmu Neural networks (DCVDN), in which visual and textual features are synchronously extracted and fused to form a comprehensive representation by deep-canonically-correlated-autoencoder-based multi-view learning. Through extensive experiments on a self-crawled real-world video-danmu dataset, we prove that DCVDN significantly outperforms the state-of-the-art baselines.



### Re-Identification with Consistent Attentive Siamese Networks
- **Arxiv ID**: http://arxiv.org/abs/1811.07487v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.07487v4)
- **Published**: 2018-11-19 03:59:51+00:00
- **Updated**: 2019-04-11 14:25:28+00:00
- **Authors**: Meng Zheng, Srikrishna Karanam, Ziyan Wu, Richard J. Radke
- **Comment**: 10 pages, 8 figures, 3 tables, to appear in CVPR 2019
- **Journal**: None
- **Summary**: We propose a new deep architecture for person re-identification (re-id). While re-id has seen much recent progress, spatial localization and view-invariant representation learning for robust cross-view matching remain key, unsolved problems. We address these questions by means of a new attention-driven Siamese learning architecture, called the Consistent Attentive Siamese Network. Our key innovations compared to existing, competing methods include (a) a flexible framework design that produces attention with only identity labels as supervision, (b) explicit mechanisms to enforce attention consistency among images of the same person, and (c) a new Siamese framework that integrates attention and attention consistency, producing principled supervisory signals as well as the first mechanism that can explain the reasoning behind the Siamese framework's predictions. We conduct extensive evaluations on the CUHK03-NP, DukeMTMC-ReID, and Market-1501 datasets and report competitive performance.



### Quantifying Human Behavior on the Block Design Test Through Automated Multi-Level Analysis of Overhead Video
- **Arxiv ID**: http://arxiv.org/abs/1811.07488v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1811.07488v1)
- **Published**: 2018-11-19 04:03:03+00:00
- **Updated**: 2018-11-19 04:03:03+00:00
- **Authors**: Seunghwan Cha, James Ainooson, Maithilee Kunda
- **Comment**: None
- **Journal**: None
- **Summary**: The block design test is a standardized, widely used neuropsychological assessment of visuospatial reasoning that involves a person recreating a series of given designs out of a set of colored blocks. In current testing procedures, an expert neuropsychologist observes a person's accuracy and completion time as well as overall impressions of the person's problem-solving procedures, errors, etc., thus obtaining a holistic though subjective and often qualitative view of the person's cognitive processes. We propose a new framework that combines room sensors and AI techniques to augment the information available to neuropsychologists from block design and similar tabletop assessments. In particular, a ceiling-mounted camera captures an overhead view of the table surface. From this video, we demonstrate how automated classification using machine learning can produce a frame-level description of the state of the block task and the person's actions over the course of each test problem. We also show how a sequence-comparison algorithm can classify one individual's problem-solving strategy relative to a database of simulated strategies, and how these quantitative results can be visualized for use by neuropsychologists.



### A Self-Adaptive Network For Multiple Sclerosis Lesion Segmentation From Multi-Contrast MRI With Various Imaging Protocols
- **Arxiv ID**: http://arxiv.org/abs/1811.07491v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07491v1)
- **Published**: 2018-11-19 04:18:57+00:00
- **Updated**: 2018-11-19 04:18:57+00:00
- **Authors**: Yushan Feng, Huitong Pan, Craig Meyer, Xue Feng
- **Comment**: This paper is submitted to IEEE ISBI 2019
- **Journal**: None
- **Summary**: Deep neural networks (DNN) have shown promises in the lesion segmentation of multiple sclerosis (MS) from multicontrast MRI including T1, T2, proton density (PD) and FLAIR sequences. However, one challenge in deploying such networks into clinical practice is the variability of imaging protocols, which often differ from the training dataset as certain MRI sequences may be unavailable or unusable. Therefore, trained networks need to adapt to practical situations when imaging protocols are different in deployment. In this paper, we propose a DNN-based MS lesion segmentation framework with a novel technique called sequence dropout which can adapt to various combinations of input MRI sequences during deployment and achieve the maximal possible performance from the given input. In addition, with this framework, we studied the quantitative impact of each MRI sequence on the MS lesion segmentation task without training separate networks. Experiments were performed using the IEEE ISBI 2015 Longitudinal MS Lesion Challenge dataset and our method is currently ranked 2nd with a Dice similarity coefficient of 0.684. Furthermore, we showed our network achieved the maximal possible performance when one sequence is unavailable during deployment by comparing with separate networks trained on the corresponding input MRI sequences. In particular, we discovered T1 and PD have minor impact on segmentation performance while FLAIR is the predominant sequence. Experiments with multiple missing sequences were also performed and showed the robustness of our network.



### DeepSeeNet: A deep learning model for automated classification of patient-based age-related macular degeneration severity from color fundus photographs
- **Arxiv ID**: http://arxiv.org/abs/1811.07492v2
- **DOI**: 10.1016/j.ophtha.2018.11.015
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07492v2)
- **Published**: 2018-11-19 04:19:34+00:00
- **Updated**: 2019-01-26 17:34:07+00:00
- **Authors**: Yifan Peng, Shazia Dharssi, Qingyu Chen, Tiarnan D. Keenan, Elvira Agrón, Wai T. Wong, Emily Y. Chew, Zhiyong Lu
- **Comment**: Accepted for publication in Ophthalmology
- **Journal**: Ophthalmology. 2018 Nov 22. pii: S0161-6420(18)32185-7
- **Summary**: In assessing the severity of age-related macular degeneration (AMD), the Age-Related Eye Disease Study (AREDS) Simplified Severity Scale predicts the risk of progression to late AMD. However, its manual use requires the time-consuming participation of expert practitioners. Although several automated deep learning systems have been developed for classifying color fundus photographs (CFP) of individual eyes by AREDS severity score, none to date has used a patient-based scoring system that uses images from both eyes to assign a severity score. DeepSeeNet, a deep learning model, was developed to classify patients automatically by the AREDS Simplified Severity Scale (score 0-5) using bilateral CFP. DeepSeeNet was trained on 58,402 and tested on 900 images from the longitudinal follow-up of 4549 participants from AREDS. Gold standard labels were obtained using reading center grades. DeepSeeNet simulates the human grading process by first detecting individual AMD risk factors (drusen size, pigmentary abnormalities) for each eye and then calculating a patient-based AMD severity score using the AREDS Simplified Severity Scale. DeepSeeNet performed better on patient-based classification (accuracy = 0.671; kappa = 0.558) than retinal specialists (accuracy = 0.599; kappa = 0.467) with high AUC in the detection of large drusen (0.94), pigmentary abnormalities (0.93), and late AMD (0.97). DeepSeeNet demonstrated high accuracy with increased transparency in the automated assignment of individual patients to AMD risk categories based on the AREDS Simplified Severity Scale. These results highlight the potential of deep learning to assist and enhance clinical decision-making in patients with AMD, such as early AMD detection and risk prediction for developing late AMD. DeepSeeNet is publicly available on https://github.com/ncbi-nlp/DeepSeeNet.



### FotonNet: A HW-Efficient Object Detection System Using 3D-Depth Segmentation and 2D-DNN Classifier
- **Arxiv ID**: http://arxiv.org/abs/1811.07493v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07493v1)
- **Published**: 2018-11-19 04:31:29+00:00
- **Updated**: 2018-11-19 04:31:29+00:00
- **Authors**: Gurjeet Singh, Sun Miao, Shi Shi, Patrick Chiang
- **Comment**: 7 pages, 10 figures, 2 tables
- **Journal**: None
- **Summary**: Object detection and classification is one of the most important computer vision problems. Ever since the introduction of deep learning \cite{krizhevsky2012imagenet}, we have witnessed a dramatic increase in the accuracy of this object detection problem. However, most of these improvements have occurred using conventional 2D image processing. Recently, low-cost 3D-image sensors, such as the Microsoft Kinect (Time-of-Flight) or the Apple FaceID (Structured-Light), can provide 3D-depth or point cloud data that can be added to a convolutional neural network, acting as an extra set of dimensions. In our proposed approach, we introduce a new 2D + 3D system that takes the 3D-data to determine the object region followed by any conventional 2D-DNN, such as AlexNet. In this method, our approach can easily dissociate the information collection from the Point Cloud and 2D-Image data and combine both operations later. Hence, our system can use any existing trained 2D network on a large image dataset, and does not require a large 3D-depth dataset for new training. Experimental object detection results across 30 images show an accuracy of 0.67, versus 0.54 and 0.51 for RCNN and YOLO, respectively.



### Robust Visual Tracking using Multi-Frame Multi-Feature Joint Modeling
- **Arxiv ID**: http://arxiv.org/abs/1811.07498v1
- **DOI**: 10.1109/TCSVT.2018.2882339
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07498v1)
- **Published**: 2018-11-19 04:44:00+00:00
- **Updated**: 2018-11-19 04:44:00+00:00
- **Authors**: Peng Zhang, Shujian Yu, Jiamiao Xu, Xinge You, Xiubao Jiang, Xiao-Yuan Jing, Dacheng Tao
- **Comment**: This paper has been accepted by IEEE Transactions on Circuits and
  Systems for Video Technology. The MATLAB code of our method is available from
  our project homepage http://bmal.hust.edu.cn/project/KMF2JMTtracking.html
- **Journal**: None
- **Summary**: It remains a huge challenge to design effective and efficient trackers under complex scenarios, including occlusions, illumination changes and pose variations. To cope with this problem, a promising solution is to integrate the temporal consistency across consecutive frames and multiple feature cues in a unified model. Motivated by this idea, we propose a novel correlation filter-based tracker in this work, in which the temporal relatedness is reconciled under a multi-task learning framework and the multiple feature cues are modeled using a multi-view learning approach. We demonstrate the resulting regression model can be efficiently learned by exploiting the structure of blockwise diagonal matrix. A fast blockwise diagonal matrix inversion algorithm is developed thereafter for efficient online tracking. Meanwhile, we incorporate an adaptive scale estimation mechanism to strengthen the stability of scale variation tracking. We implement our tracker using two types of features and test it on two benchmark datasets. Experimental results demonstrate the superiority of our proposed approach when compared with other state-of-the-art trackers. project homepage http://bmal.hust.edu.cn/project/KMF2JMTtracking.html



### Fast Efficient Object Detection Using Selective Attention
- **Arxiv ID**: http://arxiv.org/abs/1811.07502v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07502v3)
- **Published**: 2018-11-19 05:07:50+00:00
- **Updated**: 2020-02-20 01:38:30+00:00
- **Authors**: Shivanthan Yohanandan, Andy Song, Adrian G. Dyer, Angela Faragasso, Subhrajit Roy, Dacheng Tao
- **Comment**: Retraction due to significant oversight
- **Journal**: None
- **Summary**: Retraction due to significant oversight



### Compressing Recurrent Neural Networks with Tensor Ring for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1811.07503v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.07503v1)
- **Published**: 2018-11-19 05:10:14+00:00
- **Updated**: 2018-11-19 05:10:14+00:00
- **Authors**: Yu Pan, Jing Xu, Maolin Wang, Jinmian Ye, Fei Wang, Kun Bai, Zenglin Xu
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Recurrent Neural Networks (RNNs) and their variants, such as Long-Short Term Memory (LSTM) networks, and Gated Recurrent Unit (GRU) networks, have achieved promising performance in sequential data modeling. The hidden layers in RNNs can be regarded as the memory units, which are helpful in storing information in sequential contexts. However, when dealing with high dimensional input data, such as video and text, the input-to-hidden linear transformation in RNNs brings high memory usage and huge computational cost. This makes the training of RNNs unscalable and difficult. To address this challenge, we propose a novel compact LSTM model, named as TR-LSTM, by utilizing the low-rank tensor ring decomposition (TRD) to reformulate the input-to-hidden transformation. Compared with other tensor decomposition methods, TR-LSTM is more stable. In addition, TR-LSTM can complete an end-to-end training and also provide a fundamental building block for RNNs in handling large input data. Experiments on real-world action recognition datasets have demonstrated the promising performance of the proposed TR-LSTM compared with the tensor train LSTM and other state-of-the-art competitors.



### Automatic Three-Dimensional Cephalometric Annotation System Using Three-Dimensional Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1811.07889v1
- **DOI**: 10.1080/21681163.2019.1674696
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07889v1)
- **Published**: 2018-11-19 05:47:34+00:00
- **Updated**: 2018-11-19 05:47:34+00:00
- **Authors**: Sung Ho Kang, Kiwan Jeon, Hak-Jin Kim, Jin Keun Seo, Sang-Hwy Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Background: Three-dimensional (3D) cephalometric analysis using computerized tomography data has been rapidly adopted for dysmorphosis and anthropometry. Several different approaches to automatic 3D annotation have been proposed to overcome the limitations of traditional cephalometry. The purpose of this study was to evaluate the accuracy of our newly-developed system using a deep learning algorithm for automatic 3D cephalometric annotation. Methods: To overcome current technical limitations, some measures were developed to directly annotate 3D human skull data. Our deep learning-based model system mainly consisted of a 3D convolutional neural network and image data resampling. Results: The discrepancies between the referenced and predicted coordinate values in three axes and in 3D distance were calculated to evaluate system accuracy. Our new model system yielded prediction errors of 3.26, 3.18, and 4.81 mm (for three axes) and 7.61 mm (for 3D). Moreover, there was no difference among the landmarks of the three groups, including the midsagittal plane, horizontal plane, and mandible (p>0.05). Conclusion: A new 3D convolutional neural network-based automatic annotation system for 3D cephalometry was developed. The strategies used to implement the system were detailed and measurement results were evaluated for accuracy. Further development of this system is planned for full clinical application of automatic 3D cephalometric annotation.



### Unsupervised Learning in Reservoir Computing for EEG-based Emotion Recognition
- **Arxiv ID**: http://arxiv.org/abs/1811.07516v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1811.07516v2)
- **Published**: 2018-11-19 06:07:33+00:00
- **Updated**: 2018-11-23 03:24:25+00:00
- **Authors**: Rahma Fourati, Boudour Ammar, Javier Sanchez-Medina, Adel M. Alimi
- **Comment**: None
- **Journal**: None
- **Summary**: In real-world applications such as emotion recognition from recorded brain activity, data are captured from electrodes over time. These signals constitute a multidimensional time series. In this paper, Echo State Network (ESN), a recurrent neural network with a great success in time series prediction and classification, is optimized with different neural plasticity rules for classification of emotions based on electroencephalogram (EEG) time series. Actually, the neural plasticity rules are a kind of unsupervised learning adapted for the reservoir, i.e. the hidden layer of ESN. More specifically, an investigation of Oja's rule, BCM rule and gaussian intrinsic plasticity rule was carried out in the context of EEG-based emotion recognition. The study, also, includes a comparison of the offline and online training of the ESN. When testing on the well-known affective benchmark "DEAP dataset" which contains EEG signals from 32 subjects, we find that pretraining ESN with gaussian intrinsic plasticity enhanced the classification accuracy and outperformed the results achieved with an ESN pretrained with synaptic plasticity. Four classification problems were conducted in which the system complexity is increased and the discrimination is more challenging, i.e. inter-subject emotion discrimination. Our proposed method achieves higher performance over the state of the art methods.



### Higher-order Network for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1811.07519v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07519v4)
- **Published**: 2018-11-19 06:22:50+00:00
- **Updated**: 2019-11-19 02:13:11+00:00
- **Authors**: Kai Hu, Bhiksha Raj
- **Comment**: None
- **Journal**: None
- **Summary**: Capturing spatiotemporal dynamics is an essential topic in video recognition. In this paper, we present learnable higher-order operations as a generic family of building blocks for capturing spatiotemporal dynamics from RGB input video space. Similar to higher-order functions, the weights of higher-order operations are themselves derived from the data with learnable parameters. Classical architectures such as residual learning and network-in-network are first-order operations where weights are directly learned from the data. Higher-order operations make it easier to capture context-sensitive patterns, such as motion. Self-attention models are also higher-order operations, but the attention weights are mostly computed from an affine operation or dot product. Learnable higher-order operations can be more generic and flexible. Experimentally, we show that on the task of video recognition, our higher-order models can achieve results on par with or better than the existing state-of-the-art methods on Something-Something (V1 and V2), Kinetics and Charades datasets.



### A Pretrained DenseNet Encoder for Brain Tumor Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1811.07542v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07542v1)
- **Published**: 2018-11-19 08:00:22+00:00
- **Updated**: 2018-11-19 08:00:22+00:00
- **Authors**: Jean Stawiaski
- **Comment**: arXiv admin note: text overlap with arXiv:1710.02316
- **Journal**: None
- **Summary**: This article presents a convolutional neural network for the automatic segmentation of brain tumors in multimodal 3D MR images based on a U-net architecture.We evaluate the use of a densely connected convolutional network encoder (DenseNet) which was pretrained on the ImageNet data set. We detail two network architectures that can take into account multiple 3D images as inputs. This work aims to identify if a generic pretrained network can be used for very specific medical applications where the target data differ both in the number of spatial dimensions as well as in the number of inputs channels. Moreover in order to regularize this transfer learning task we only train the decoder part of the U-net architecture. We evaluate the effectiveness of the proposed approach on the BRATS 2018 segmentation challenge where we obtained dice scores of 0.79, 0.90, 0.85 and 95/% Hausdorff distance of 2.9mm, 3.95mm, and 6.48mm for enhanced tumor core, whole tumor and tumor core respectively on the validation set. This scores degrades to 0.77, 0.88, 0.78 and 95 /% Hausdorff distance of 3.6mm, 5.72mm, and 5.83mm on the testing set.



### CA3Net: Contextual-Attentional Attribute-Appearance Network for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1811.07544v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07544v1)
- **Published**: 2018-11-19 08:05:07+00:00
- **Updated**: 2018-11-19 08:05:07+00:00
- **Authors**: Jiawei Liu, Zheng-Jun Zha, Hongtao Xie, Zhiwei Xiong, Yongdong Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Person re-identification aims to identify the same pedestrian across non-overlapping camera views. Deep learning techniques have been applied for person re-identification recently, towards learning representation of pedestrian appearance. This paper presents a novel Contextual-Attentional Attribute-Appearance Network (CA3Net) for person re-identification. The CA3Net simultaneously exploits the complementarity between semantic attributes and visual appearance, the semantic context among attributes, visual attention on attributes as well as spatial dependencies among body parts, leading to discriminative and robust pedestrian representation. Specifically, an attribute network within CA3Net is designed with an Attention-LSTM module. It concentrates the network on latent image regions related to each attribute as well as exploits the semantic context among attributes by a LSTM module. An appearance network is developed to learn appearance features from the full body, horizontal and vertical body parts of pedestrians with spatial dependencies among body parts. The CA3Net jointly learns the attribute and appearance features in a multi-task learning manner, generating comprehensive representation of pedestrians. Extensive experiments on two challenging benchmarks, i.e., Market-1501 and DukeMTMC-reID datasets, have demonstrated the effectiveness of the proposed approach.



### iQIYI-VID: A Large Dataset for Multi-modal Person Identification
- **Arxiv ID**: http://arxiv.org/abs/1811.07548v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07548v2)
- **Published**: 2018-11-19 08:16:42+00:00
- **Updated**: 2019-04-22 05:41:27+00:00
- **Authors**: Yuanliu Liu, Bo Peng, Peipei Shi, He Yan, Yong Zhou, Bing Han, Yi Zheng, Chao Lin, Jianbin Jiang, Yin Fan, Tingwei Gao, Ganwen Wang, Jian Liu, Xiangju Lu, Danming Xie
- **Comment**: None
- **Journal**: None
- **Summary**: Person identification in the wild is very challenging due to great variation in poses, face quality, clothes, makeup and so on. Traditional research, such as face recognition, person re-identification, and speaker recognition, often focuses on a single modal of information, which is inadequate to handle all the situations in practice. Multi-modal person identification is a more promising way that we can jointly utilize face, head, body, audio features, and so on. In this paper, we introduce iQIYI-VID, the largest video dataset for multi-modal person identification. It is composed of 600K video clips of 5,000 celebrities. These video clips are extracted from 400K hours of online videos of various types, ranging from movies, variety shows, TV series, to news broadcasting. All video clips pass through a careful human annotation process, and the error rate of labels is lower than 0.2\%. We evaluated the state-of-art models of face recognition, person re-identification, and speaker recognition on the iQIYI-VID dataset. Experimental results show that these models are still far from being perfect for the task of person identification in the wild. We proposed a Multi-modal Attention module to fuse multi-modal features that can improve person identification considerably. We have released the dataset online to promote multi-modal person identification research.



### Three Dimensional Convolutional Neural Network Pruning with Regularization-Based Method
- **Arxiv ID**: http://arxiv.org/abs/1811.07555v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.07555v2)
- **Published**: 2018-11-19 08:40:00+00:00
- **Updated**: 2019-05-20 03:48:09+00:00
- **Authors**: Yuxin Zhang, Huan Wang, Yang Luo, Lu Yu, Haoji Hu, Hangguan Shan, Tony Q. S. Quek
- **Comment**: ICIP 2019
- **Journal**: ICIP 2019
- **Summary**: Despite enjoying extensive applications in video analysis, three-dimensional convolutional neural networks (3D CNNs)are restricted by their massive computation and storage consumption. To solve this problem, we propose a threedimensional regularization-based neural network pruning method to assign different regularization parameters to different weight groups based on their importance to the network. Further we analyze the redundancy and computation cost for each layer to determine the different pruning ratios. Experiments show that pruning based on our method can lead to 2x theoretical speedup with only 0.41% accuracy loss for 3DResNet18 and 3.28% accuracy loss for C3D. The proposed method performs favorably against other popular methods for model compression and acceleration.



### Fine-grained Classification using Heterogeneous Web Data and Auxiliary Categories
- **Arxiv ID**: http://arxiv.org/abs/1811.07567v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07567v1)
- **Published**: 2018-11-19 09:28:15+00:00
- **Updated**: 2018-11-19 09:28:15+00:00
- **Authors**: Li Niu, Ashok Veeraraghavan, Ashu Sabharwal
- **Comment**: None
- **Journal**: None
- **Summary**: Fine-grained classification remains a very challenging problem, because of the absence of well-labeled training data caused by the high cost of annotating a large number of fine-grained categories. In the extreme case, given a set of test categories without any well-labeled training data, the majority of existing works can be grouped into the following two research directions: 1) crawl noisy labeled web data for the test categories as training data, which is dubbed as webly supervised learning; 2) transfer the knowledge from auxiliary categories with well-labeled training data to the test categories, which corresponds to zero-shot learning setting. Nevertheless, the above two research directions still have critical issues to be addressed. For the first direction, web data have noisy labels and considerably different data distribution from test data. For the second direction, zero-shot learning is struggling to achieve compelling results compared with conventional supervised learning. The issues of the above two directions motivate us to develop a novel approach which can jointly exploit both noisy web training data from test categories and well-labeled training data from auxiliary categories. In particular, on one hand, we crawl web data for test categories as noisy training data. On the other hand, we transfer the knowledge from auxiliary categories with well-labeled training data to test categories by virtue of free semantic information (e.g., word vector) of all categories. Moreover, given the fact that web data are generally associated with additional textual information (e.g., title and tag), we extend our method by using the surrounding textual information of web data as privileged information. Extensive experiments show the effectiveness of our proposed methods.



### Localisation via Deep Imagination: learn the features not the map
- **Arxiv ID**: http://arxiv.org/abs/1811.07583v1
- **DOI**: 10.1007/978-3-030-11021-5_44
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07583v1)
- **Published**: 2018-11-19 09:52:34+00:00
- **Updated**: 2018-11-19 09:52:34+00:00
- **Authors**: Jaime Spencer, Oscar Mendez, Richard Bowden, Simon Hadfield
- **Comment**: VNAD @ ECCV2018
- **Journal**: None
- **Summary**: How many times does a human have to drive through the same area to become familiar with it? To begin with, we might first build a mental model of our surroundings. Upon revisiting this area, we can use this model to extrapolate to new unseen locations and imagine their appearance. Based on this, we propose an approach where an agent is capable of modelling new environments after a single visitation. To this end, we introduce "Deep Imagination", a combination of classical Visual-based Monte Carlo Localisation and deep learning. By making use of a feature embedded 3D map, the system can "imagine" the view from any novel location. These "imagined" views are contrasted with the current observation in order to estimate the agent's current location. In order to build the embedded map, we train a deep Siamese Fully Convolutional U-Net to perform dense feature extraction. By training these features to be generic, no additional training or fine tuning is required to adapt to new environments. Our results demonstrate the generality and transfer capability of our learnt dense features by training and evaluating on multiple datasets. Additionally, we include several visualizations of the feature representations and resulting 3D maps, as well as their application to localisation.



### Self-Referenced Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1811.07598v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07598v1)
- **Published**: 2018-11-19 10:41:17+00:00
- **Updated**: 2018-11-19 10:41:17+00:00
- **Authors**: Xu Lan, Xiatian Zhu, Shaogang Gong
- **Comment**: To Appear in Asian Conference on Computer Vision 2018
- **Journal**: None
- **Summary**: Knowledge distillation is an effective approach to transferring knowledge from a teacher neural network to a student target network for satisfying the low-memory and fast running requirements in practice use. Whilst being able to create stronger target networks compared to the vanilla non-teacher based learning strategy, this scheme needs to train additionally a large teacher model with expensive computational cost. In this work, we present a Self-Referenced Deep Learning (SRDL) strategy. Unlike both vanilla optimisation and existing knowledge distillation, SRDL distils the knowledge discovered by the in-training target model back to itself to regularise the subsequent learning procedure therefore eliminating the need for training a large teacher model. SRDL improves the model generalisation performance compared to vanilla learning and conventional knowledge distillation approaches with negligible extra computational cost. Extensive evaluations show that a variety of deep networks benefit from SRDL resulting in enhanced deployment performance on both coarse-grained object categorisation tasks (CIFAR10, CIFAR100, Tiny ImageNet, and ImageNet) and fine-grained person instance identification tasks (Market-1501).



### Adversarial Autoencoders for Compact Representations of 3D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1811.07605v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.07605v3)
- **Published**: 2018-11-19 10:51:09+00:00
- **Updated**: 2019-05-01 19:22:36+00:00
- **Authors**: Maciej Zamorski, Maciej Zięba, Piotr Klukowski, Rafał Nowak, Karol Kurach, Wojciech Stokowiec, Tomasz Trzciński
- **Comment**: 10 pages, 8 figures
- **Journal**: None
- **Summary**: Deep generative architectures provide a way to model not only images but also complex, 3-dimensional objects, such as point clouds. In this work, we present a novel method to obtain meaningful representations of 3D shapes that can be used for challenging tasks including 3D points generation, reconstruction, compression, and clustering. Contrary to existing methods for 3D point cloud generation that train separate decoupled models for representation learning and generation, our approach is the first end-to-end solution that allows to simultaneously learn a latent space of representation and generate 3D shape out of it. Moreover, our model is capable of learning meaningful compact binary descriptors with adversarial training conducted on a latent space. To achieve this goal, we extend a deep Adversarial Autoencoder model (AAE) to accept 3D input and create 3D output. Thanks to our end-to-end training regime, the resulting method called 3D Adversarial Autoencoder (3dAAE) obtains either binary or continuous latent space that covers a much wider portion of training data distribution. Finally, our quantitative evaluation shows that 3dAAE provides state-of-the-art results for 3D points clustering and 3D object retrieval.



### Adversarial Soft-detection-based Aggregation Network for Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1811.07619v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07619v3)
- **Published**: 2018-11-19 11:22:37+00:00
- **Updated**: 2019-03-29 05:25:55+00:00
- **Authors**: Jian Xu, Chunheng Wang, Cunzhao Shi, Baihua Xiao
- **Comment**: 10 pages, 6 figures
- **Journal**: None
- **Summary**: In recent year, the compact representations based on activations of Convolutional Neural Network (CNN) achieve remarkable performance in image retrieval. However, retrieval of some interested object that only takes up a small part of the whole image is still a challenging problem. Therefore, it is significant to extract the discriminative representations that contain regional information of the pivotal small object. In this paper, we propose a novel adversarial soft-detection-based aggregation (ASDA) method free from bounding box annotations for image retrieval, based on adversarial detector and soft region proposal layer. Our trainable adversarial detector generates semantic maps based on adversarial erasing strategy to preserve more discriminative and detailed information. Computed based on semantic maps corresponding to various discriminative patterns and semantic contents, our soft region proposal is arbitrary shape rather than only rectangle and it reflects the significance of objects. The aggregation based on trainable soft region proposal highlights discriminative semantic contents and suppresses the noise of background.   We conduct comprehensive experiments on standard image retrieval datasets. Our weakly supervised ASDA method achieves state-of-the-art performance on most datasets. The results demonstrate that the proposed ASDA method is effective for image retrieval.



### Beyond Attributes: Adversarial Erasing Embedding Network for Zero-shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1811.07626v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07626v2)
- **Published**: 2018-11-19 11:39:02+00:00
- **Updated**: 2018-11-20 07:33:09+00:00
- **Authors**: Xiao-Bo Jin, Kai-Zhu Huang, Jianyu Miao
- **Comment**: 12 pages, 10 figures,working report
- **Journal**: None
- **Summary**: In this paper, an adversarial erasing embedding network with the guidance of high-order attributes (AEEN-HOA) is proposed for going further to solve the challenging ZSL/GZSL task. AEEN-HOA consists of two branches, i.e., the upper stream is capable of erasing some initially discovered regions, then the high-order attribute supervision is incorporated to characterize the relationship between the class attributes. Meanwhile, the bottom stream is trained by taking the current background regions to train the same attribute. As far as we know, it is the first time of introducing the erasing operations into the ZSL task. In addition, we first propose a class attribute activation map for the visualization of ZSL output, which shows the relationship between class attribute feature and attention map. Experiments on four standard benchmark datasets demonstrate the superiority of AEEN-HOA framework.



### ATOM: Accurate Tracking by Overlap Maximization
- **Arxiv ID**: http://arxiv.org/abs/1811.07628v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07628v2)
- **Published**: 2018-11-19 11:40:17+00:00
- **Updated**: 2019-04-11 17:56:18+00:00
- **Authors**: Martin Danelljan, Goutam Bhat, Fahad Shahbaz Khan, Michael Felsberg
- **Comment**: CVPR 2019 (Oral). Complete code and models are available at
  https://github.com/visionml/pytracking
- **Journal**: None
- **Summary**: While recent years have witnessed astonishing improvements in visual tracking robustness, the advancements in tracking accuracy have been limited. As the focus has been directed towards the development of powerful classifiers, the problem of accurate target state estimation has been largely overlooked. In fact, most trackers resort to a simple multi-scale search in order to estimate the target bounding box. We argue that this approach is fundamentally limited since target estimation is a complex task, requiring high-level knowledge about the object.   We address this problem by proposing a novel tracking architecture, consisting of dedicated target estimation and classification components. High level knowledge is incorporated into the target estimation through extensive offline learning. Our target estimation component is trained to predict the overlap between the target object and an estimated bounding box. By carefully integrating target-specific information, our approach achieves previously unseen bounding box accuracy. We further introduce a classification component that is trained online to guarantee high discriminative power in the presence of distractors. Our final tracking framework sets a new state-of-the-art on five challenging benchmarks. On the new large-scale TrackingNet dataset, our tracker ATOM achieves a relative gain of 15% over the previous best approach, while running at over 30 FPS. Code and models are available at https://github.com/visionml/pytracking.



### SEIGAN: Towards Compositional Image Generation by Simultaneously Learning to Segment, Enhance, and Inpaint
- **Arxiv ID**: http://arxiv.org/abs/1811.07630v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1811.07630v2)
- **Published**: 2018-11-19 11:50:20+00:00
- **Updated**: 2019-01-15 19:33:07+00:00
- **Authors**: Pavel Ostyakov, Roman Suvorov, Elizaveta Logacheva, Oleg Khomenko, Sergey I. Nikolenko
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel approach to image manipulation and understanding by simultaneously learning to segment object masks, paste objects to another background image, and remove them from original images. For this purpose, we develop a novel generative model for compositional image generation, SEIGAN (Segment-Enhance-Inpaint Generative Adversarial Network), which learns these three operations together in an adversarial architecture with additional cycle consistency losses. To train, SEIGAN needs only bounding box supervision and does not require pairing or ground truth masks. SEIGAN produces better generated images (evaluated by human assessors) than other approaches and produces high-quality segmentation masks, improving over other adversarially trained approaches and getting closer to the results of fully supervised training.



### Collaborative Dense SLAM
- **Arxiv ID**: http://arxiv.org/abs/1811.07632v2
- **DOI**: None
- **Categories**: **cs.CV**, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/1811.07632v2)
- **Published**: 2018-11-19 11:54:40+00:00
- **Updated**: 2018-11-21 12:12:59+00:00
- **Authors**: Louis Gallagher, John B. McDonald
- **Comment**: 10 pages, 3 figures
- **Journal**: None
- **Summary**: In this paper, we present a new system for live collaborative dense surface reconstruction. Cooperative robotics, multi participant augmented reality and human-robot interaction are all examples of situations where collaborative mapping can be leveraged for greater agent autonomy. Our system builds on ElasticFusion to allow a number of cameras starting with unknown initial relative positions to maintain local maps utilising the original algorithm. Carrying out visual place recognition across these local maps the system can identify when two maps overlap in space, providing an inter-map constraint from which the system can derive the relative poses of the two maps. Using these resulting pose constraints, our system performs map merging, allowing multiple cameras to fuse their measurements into a single shared reconstruction. The advantage of this approach is that it avoids replication of structures subsequent to loop closures, where multiple cameras traverse the same regions of the environment. Furthermore, it allows cameras to directly exploit and update regions of the environment previously mapped by other cameras within the system. We provide both quantitative and qualitative analyses using the synthetic ICL-NUIM dataset and the real-world Freiburg dataset including the impact of multi-camera mapping on surface reconstruction accuracy, camera pose estimation accuracy and overall processing time. We also include qualitative results in the form of sample reconstructions of room sized environments with up to 3 cameras undergoing intersecting and loopy trajectories.



### Watermark Retrieval from 3D Printed Objects via Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1811.07640v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07640v1)
- **Published**: 2018-11-19 12:20:52+00:00
- **Updated**: 2018-11-19 12:20:52+00:00
- **Authors**: Xin Zhang, Qian Wang, Toby Breckon, Ioannis Ivrissimtzis
- **Comment**: None
- **Journal**: None
- **Summary**: We present a method for reading digital data embedded in planar 3D printed surfaces. The data are organised in binary arrays and embedded as surface textures in a way inspired by QR codes. At the core of the retrieval method lies a Convolutional Neural Network, outputting a confidence map of the location of the surface textures encoding value 1 bits. Subsequently, the bit array is retrieved through a series of simple image processing and statistical operations applied on the confidence map. Extensive experimentation with images captured from various camera views, under various illumination conditions and from objects printed with various material colours, shows that the proposed method generalizes well and achieves the level of accuracy required in practical applications.



### Intention Oriented Image Captions with Guiding Objects
- **Arxiv ID**: http://arxiv.org/abs/1811.07662v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07662v2)
- **Published**: 2018-11-19 13:12:07+00:00
- **Updated**: 2019-04-11 08:36:21+00:00
- **Authors**: Yue Zheng, Yali Li, Shengjin Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Although existing image caption models can produce promising results using recurrent neural networks (RNNs), it is difficult to guarantee that an object we care about is contained in generated descriptions, for example in the case that the object is inconspicuous in the image. Problems become even harder when these objects did not appear in training stage. In this paper, we propose a novel approach for generating image captions with guiding objects (CGO). The CGO constrains the model to involve a human-concerned object when the object is in the image. CGO ensures that the object is in the generated description while maintaining fluency. Instead of generating the sequence from left to right, we start the description with a selected object and generate other parts of the sequence based on this object. To achieve this, we design a novel framework combining two LSTMs in opposite directions. We demonstrate the characteristics of our method on MSCOCO where we generate descriptions for each detected object in the images. With CGO, we can extend the ability of description to the objects being neglected in image caption labels and provide a set of more comprehensive and diverse descriptions for an image. CGO shows advantages when applied to the task of describing novel objects. We show experimental results on both MSCOCO and ImageNet datasets. Evaluations show that our method outperforms the state-of-the-art models in the task with average F1 75.8, leading to better descriptions in terms of both content accuracy and fluency.



### FD-GAN: Face-demorphing generative adversarial network for restoring accomplice's facial image
- **Arxiv ID**: http://arxiv.org/abs/1811.07665v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07665v2)
- **Published**: 2018-11-19 13:19:48+00:00
- **Updated**: 2019-03-22 02:07:21+00:00
- **Authors**: Fei Peng, Le-bing Zhang, Min Long
- **Comment**: 9 pages, 7 figures
- **Journal**: None
- **Summary**: Face morphing attack is proved to be a serious threat to the existing face recognition systems. Although a few face morphing detection methods have been put forward, the face morphing accomplice's facial restoration remains a challenging problem. In this paper, a face de-morphing generative adversarial network (FD-GAN) is proposed to restore the accomplice's facial image. It utilizes a symmetric dual network architecture and two levels of restoration losses to separate the identity feature of the morphing accomplice. By exploiting the captured facial image (containing the criminal's identity) from the face recognition system and the morphed image stored in the e-passport system (containing both criminal and accomplice's identities), the FD-GAN can effectively restore the accomplice's facial image. Experimental results and analysis demonstrate the effectiveness of the proposed scheme. It has great potential to be implemented for detecting the face morphing accomplice in a real identity verification scenario.



### IVD-Net: Intervertebral disc localization and segmentation in MRI with a multi-modal UNet
- **Arxiv ID**: http://arxiv.org/abs/1811.08305v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08305v1)
- **Published**: 2018-11-19 14:35:28+00:00
- **Updated**: 2018-11-19 14:35:28+00:00
- **Authors**: Jose Dolz, Christian Desrosiers, Ismail Ben Ayed
- **Comment**: Manuscript submitted to the Proceedings of the MICCAI 2018 IVD
  Challenge. arXiv admin note: text overlap with arXiv:1810.07003
- **Journal**: None
- **Summary**: Accurate localization and segmentation of intervertebral disc (IVD) is crucial for the assessment of spine disease diagnosis. Despite the technological advances in medical imaging, IVD localization and segmentation are still manually performed, which is time-consuming and prone to errors. If, in addition, multi-modal imaging is considered, the burden imposed on disease assessments increases substantially. In this paper, we propose an architecture for IVD localization and segmentation in multi-modal MRI, which extends the well-known UNet. Compared to single images, multi-modal data brings complementary information, contributing to better data representation and discriminative power. Our contributions are three-fold. First, how to effectively integrate and fully leverage multi-modal data remains almost unexplored. In this work, each MRI modality is processed in a different path to better exploit their unique information. Second, inspired by HyperDenseNet, the network is densely-connected both within each path and across different paths, granting the model the freedom to learn where and how the different modalities should be processed and combined. Third, we improved standard U-Net modules by extending inception modules with two dilated convolutions blocks of different scale, which helps handling multi-scale context. We report experiments over the data set of the public MICCAI 2018 Challenge on Automatic Intervertebral Disc Localization and Segmentation, with 13 multi-modal MRI images used for training and 3 for validation. We trained IVD-Net on an NVidia TITAN XP GPU with 16 GBs RAM, using ADAM as optimizer and a learning rate of 10e-5 during 200 epochs. Training took about 5 hours, and segmentation of a whole volume about 2-3 seconds, on average. Several baselines, with different multi-modal fusion strategies, were used to demonstrate the effectiveness of the proposed architecture.



### Do Normalization Layers in a Deep ConvNet Really Need to Be Distinct?
- **Arxiv ID**: http://arxiv.org/abs/1811.07727v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.07727v1)
- **Published**: 2018-11-19 14:36:25+00:00
- **Updated**: 2018-11-19 14:36:25+00:00
- **Authors**: Ping Luo, Zhanglin Peng, Jiamin Ren, Ruimao Zhang
- **Comment**: Preprint. Work in Progress. 14 pages, 13 figures
- **Journal**: None
- **Summary**: Yes, they do. This work investigates a perspective for deep learning: whether different normalization layers in a ConvNet require different normalizers. This is the first step towards understanding this phenomenon. We allow each convolutional layer to be stacked before a switchable normalization (SN) that learns to choose a normalizer from a pool of normalization methods. Through systematic experiments in ImageNet, COCO, Cityscapes, and ADE20K, we answer three questions: (a) Is it useful to allow each normalization layer to select its own normalizer? (b) What impacts the choices of normalizers? (c) Do different tasks and datasets prefer different normalizers? Our results suggest that (1) using distinct normalizers improves both learning and generalization of a ConvNet; (2) the choices of normalizers are more related to depth and batch size, but less relevant to parameter initialization, learning rate decay, and solver; (3) different tasks and datasets have different behaviors when learning to select normalizers.



### M2U-Net: Effective and Efficient Retinal Vessel Segmentation for Resource-Constrained Environments
- **Arxiv ID**: http://arxiv.org/abs/1811.07738v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07738v3)
- **Published**: 2018-11-19 14:51:56+00:00
- **Updated**: 2019-04-23 07:51:28+00:00
- **Authors**: Tim Laibacher, Tillman Weyde, Sepehr Jalali
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a novel neural network architecture for retinal vessel segmentation that improves over the state of the art on two benchmark datasets, is the first to run in real time on high resolution images, and its small memory and processing requirements make it deployable in mobile and embedded systems. The M2U-Net has a new encoder-decoder architecture that is inspired by the U-Net. It adds pretrained components of MobileNetV2 in the encoder part and novel contractive bottleneck blocks in the decoder part that, combined with bilinear upsampling, drastically reduce the parameter count to 0.55M compared to 31.03M in the original U-Net. We have evaluated its performance against a wide body of previously published results on three public datasets. On two of them, the M2U-Net achieves new state-of-the-art performance by a considerable margin. When implemented on a GPU, our method is the first to achieve real-time inference speeds on high-resolution fundus images. We also implemented our proposed network on an ARM-based embedded system where it segments images in between 0.6 and 15 sec, depending on the resolution. Thus, the M2U-Net enables a number of applications of retinal vessel structure extraction, such as early diagnosis of eye diseases, retinal biometric authentication systems, and robot assisted microsurgery.



### Past, Present, and Future Approaches Using Computer Vision for Animal Re-Identification from Camera Trap Data
- **Arxiv ID**: http://arxiv.org/abs/1811.07749v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07749v1)
- **Published**: 2018-11-19 15:30:06+00:00
- **Updated**: 2018-11-19 15:30:06+00:00
- **Authors**: Stefan Schneider, Graham W. Taylor, Stefan S. Linquist, Stefan C. Kremer
- **Comment**: 25 pages, 1 picture
- **Journal**: None
- **Summary**: The ability of a researcher to re-identify (re-ID) an individual animal upon re-encounter is fundamental for addressing a broad range of questions in the study of ecosystem function, community and population dynamics, and behavioural ecology. In this review, we describe a brief history of camera traps for re-ID, present a collection of computer vision feature engineering methodologies previously used for animal re-ID, provide an introduction to the underlying mechanisms of deep learning relevant to animal re-ID, highlight the success of deep learning methods for human re-ID, describe the few ecological studies currently utilizing deep learning for camera trap analyses, and our predictions for near future methodologies based on the rapid development of deep learning methods. By utilizing novel deep learning methods for object detection and similarity comparisons, ecologists can extract animals from an image/video data and train deep learning classifiers to re-ID animal individuals beyond the capabilities of a human observer. This methodology will allow ecologists with camera/video trap data to re-identify individuals that exit and re-enter the camera frame. Our expectation is that this is just the beginning of a major trend that could stand to revolutionize the analysis of camera trap data and, ultimately, our approach to animal ecology.



### Slum Segmentation and Change Detection : A Deep Learning Approach
- **Arxiv ID**: http://arxiv.org/abs/1811.07896v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.07896v1)
- **Published**: 2018-11-19 15:45:06+00:00
- **Updated**: 2018-11-19 15:45:06+00:00
- **Authors**: Shishira R Maiya, Sudharshan Chandra Babu
- **Comment**: Presented at NIPS 2018 Workshop on Machine Learning for the
  Developing World
- **Journal**: None
- **Summary**: More than one billion people live in slums around the world. In some developing countries, slum residents make up for more than half of the population and lack reliable sanitation services, clean water, electricity, other basic services. Thus, slum rehabilitation and improvement is an important global challenge, and a significant amount of effort and resources have been put into this endeavor. These initiatives rely heavily on slum mapping and monitoring, and it is essential to have robust and efficient methods for mapping and monitoring existing slum settlements. In this work, we introduce an approach to segment and map individual slums from satellite imagery, leveraging regional convolutional neural networks for instance segmentation using transfer learning. In addition, we also introduce a method to perform change detection and monitor slum change over time. We show that our approach effectively learns slum shape and appearance, and demonstrates strong quantitative results, resulting in a maximum AP of 80.0.



### Contextual Face Recognition with a Nested-Hierarchical Nonparametric Identity Model
- **Arxiv ID**: http://arxiv.org/abs/1811.07753v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07753v1)
- **Published**: 2018-11-19 15:45:59+00:00
- **Updated**: 2018-11-19 15:45:59+00:00
- **Authors**: Daniel C. Castro, Sebastian Nowozin
- **Comment**: NeurIPS 2018 Workshop on All of Bayesian Nonparametrics (BNP@NeurIPS
  2018). arXiv admin note: substantial text overlap with arXiv:1807.07872
- **Journal**: None
- **Summary**: Current face recognition systems typically operate via classification into known identities obtained from supervised identity annotations. There are two problems with this paradigm: (1) current systems are unable to benefit from often abundant unlabelled data; and (2) they equate successful recognition with labelling a given input image. Humans, on the other hand, regularly perform identification of individuals completely unsupervised, recognising the identity of someone they have seen before even without being able to name that individual. How can we go beyond the current classification paradigm towards a more human understanding of identities? In previous work, we proposed an integrated Bayesian model that coherently reasons about the observed images, identities, partial knowledge about names, and the situational context of each observation. Here, we propose extensions of the contextual component of this model, enabling unsupervised discovery of an unbounded number of contexts for improved face recognition.



### Injecting and removing malignant features in mammography with CycleGAN: Investigation of an automated adversarial attack using neural networks
- **Arxiv ID**: http://arxiv.org/abs/1811.07767v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/1811.07767v1)
- **Published**: 2018-11-19 16:08:11+00:00
- **Updated**: 2018-11-19 16:08:11+00:00
- **Authors**: Anton S. Becker, Lukas Jendele, Ondrej Skopek, Nicole Berger, Soleen Ghafoor, Magda Marcon, Ender Konukoglu
- **Comment**: To be presented at RSNA 2018
- **Journal**: None
- **Summary**: $\textbf{Purpose}$ To train a cycle-consistent generative adversarial network (CycleGAN) on mammographic data to inject or remove features of malignancy, and to determine whether these AI-mediated attacks can be detected by radiologists. $\textbf{Material and Methods}$ From the two publicly available datasets, BCDR and INbreast, we selected images from cancer patients and healthy controls. An internal dataset served as test data, withheld during training. We ran two experiments training CycleGAN on low and higher resolution images ($256 \times 256$ px and $512 \times 408$ px). Three radiologists read the images and rated the likelihood of malignancy on a scale from 1-5 and the likelihood of the image being manipulated. The readout was evaluated by ROC analysis (Area under the ROC curve = AUC). $\textbf{Results}$ At the lower resolution, only one radiologist exhibited markedly lower detection of cancer (AUC=0.85 vs 0.63, p=0.06), while the other two were unaffected (0.67 vs. 0.69 and 0.75 vs. 0.77, p=0.55). Only one radiologist could discriminate between original and modified images slightly better than guessing/chance (0.66, p=0.008). At the higher resolution, all radiologists showed significantly lower detection rate of cancer in the modified images (0.77-0.84 vs. 0.59-0.69, p=0.008), however, they were now able to reliably detect modified images due to better visibility of artifacts (0.92, 0.92 and 0.97). $\textbf{Conclusion}$ A CycleGAN can implicitly learn malignant features and inject or remove them so that a substantial proportion of small mammographic images would consequently be misdiagnosed. At higher resolutions, however, the method is currently limited and has a clear trade-off between manipulation of images and introduction of artifacts.



### Modeling Local Geometric Structure of 3D Point Clouds using Geo-CNN
- **Arxiv ID**: http://arxiv.org/abs/1811.07782v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07782v1)
- **Published**: 2018-11-19 16:30:20+00:00
- **Updated**: 2018-11-19 16:30:20+00:00
- **Authors**: Shiyi Lan, Ruichi Yu, Gang Yu, Larry S. Davis
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in deep convolutional neural networks (CNNs) have motivated researchers to adapt CNNs to directly model points in 3D point clouds. Modeling local structure has been proven to be important for the success of convolutional architectures, and researchers exploited the modeling of local point sets in the feature extraction hierarchy. However, limited attention has been paid to explicitly model the geometric structure amongst points in a local region. To address this problem, we propose Geo-CNN, which applies a generic convolution-like operation dubbed as GeoConv to each point and its local neighborhood. Local geometric relationships among points are captured when extracting edge features between the center and its neighboring points. We first decompose the edge feature extraction process onto three orthogonal bases, and then aggregate the extracted features based on the angles between the edge vector and the bases. This encourages the network to preserve the geometric structure in Euclidean space throughout the feature extraction hierarchy. GeoConv is a generic and efficient operation that can be easily integrated into 3D point cloud analysis pipelines for multiple applications. We evaluate Geo-CNN on ModelNet40 and KITTI and achieve state-of-the-art performance.



### Explicit Bias Discovery in Visual Question Answering Models
- **Arxiv ID**: http://arxiv.org/abs/1811.07789v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07789v1)
- **Published**: 2018-11-19 16:39:04+00:00
- **Updated**: 2018-11-19 16:39:04+00:00
- **Authors**: Varun Manjunatha, Nirat Saini, Larry S. Davis
- **Comment**: None
- **Journal**: None
- **Summary**: Researchers have observed that Visual Question Answering (VQA) models tend to answer questions by learning statistical biases in the data. For example, their answer to the question "What is the color of the grass?" is usually "Green", whereas a question like "What is the title of the book?" cannot be answered by inferring statistical biases. It is of interest to the community to explicitly discover such biases, both for understanding the behavior of such models, and towards debugging them. Our work address this problem. In a database, we store the words of the question, answer and visual words corresponding to regions of interest in attention maps. By running simple rule mining algorithms on this database, we discover human-interpretable rules which give us unique insight into the behavior of such models. Our results also show examples of unusual behaviors learned by models in attempting VQA tasks.



### Deep Shape-from-Template: Wide-Baseline, Dense and Fast Registration and Deformable Reconstruction from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/1811.07791v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR, cs.LG, cs.SE
- **Links**: [PDF](http://arxiv.org/pdf/1811.07791v3)
- **Published**: 2018-11-19 16:39:27+00:00
- **Updated**: 2021-02-28 03:12:50+00:00
- **Authors**: David Fuentes-Jimenez, David Casillas-Perez, Daniel Pizarro, Toby Collins, Adrien Bartoli
- **Comment**: None
- **Journal**: None
- **Summary**: We present Deep Shape-from-Template (DeepSfT), a novel Deep Neural Network (DNN) method for solving real-time automatic registration and 3D reconstruction of a deformable object viewed in a single monocular image.DeepSfT advances the state-of-the-art in various aspects. Compared to existing DNN SfT methods, it is the first fully convolutional real-time approach that handles an arbitrary object geometry, topology and surface representation. It also does not require ground truth registration with real data and scales well to very complex object models with large numbers of elements. Compared to previous non-DNN SfT methods, it does not involve numerical optimization at run-time, and is a dense, wide-baseline solution that does not demand, and does not suffer from, feature-based matching. It is able to process a single image with significant deformation and viewpoint changes, and handles well the core challenges of occlusions, weak texture and blur. DeepSfT is based on residual encoder-decoder structures and refining blocks. It is trained end-to-end with a novel combination of supervised learning from simulated renderings of the object model and semi-supervised automatic fine-tuning using real data captured with a standard RGB-D camera. The cameras used for fine-tuning and run-time can be different, making DeepSfT practical for real-world use. We show that DeepSfT significantly outperforms state-of-the-art wide-baseline approaches for non-trivial templates, with quantitative and qualitative evaluation.



### DeepIR: A Deep Semantics Driven Framework for Image Retargeting
- **Arxiv ID**: http://arxiv.org/abs/1811.07793v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07793v3)
- **Published**: 2018-11-19 16:43:28+00:00
- **Updated**: 2019-07-24 03:16:05+00:00
- **Authors**: Jianxin Lin, Tiankuang Zhou, Zhibo Chen
- **Comment**: 8 pages, 10 figures
- **Journal**: None
- **Summary**: We present \emph{Deep Image Retargeting} (\emph{DeepIR}), a coarse-to-fine framework for content-aware image retargeting. Our framework first constructs the semantic structure of input image with a deep convolutional neural network. Then a uniform re-sampling that suits for semantic structure preserving is devised to resize feature maps to target aspect ratio at each feature layer. The final retargeting result is generated by coarse-to-fine nearest neighbor field search and step-by-step nearest neighbor field fusion. We empirically demonstrate the effectiveness of our model with both qualitative and quantitative results on widely used RetargetMe dataset.



### Event-based Gesture Recognition with Dynamic Background Suppression using Smartphone Computational Capabilities
- **Arxiv ID**: http://arxiv.org/abs/1811.07802v3
- **DOI**: 10.3389/fnins.2020.00275
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07802v3)
- **Published**: 2018-11-19 17:03:01+00:00
- **Updated**: 2020-04-28 08:36:39+00:00
- **Authors**: Jean-Matthieu Maro, Ryad Benosman
- **Comment**: Draft version; final version published in Frontiers in Neuroscience
  (open access)
- **Journal**: Frontiers in Neuroscience 14 (2020) 275
- **Summary**: This paper introduces a framework of gesture recognition operating on the output of an event based camera using the computational resources of a mobile phone. We will introduce a new development around the concept of time-surfaces modified and adapted to run on the limited computational resources of a mobile platform. We also introduce a new method to remove dynamically backgrounds that makes full use of the high temporal resolution of event-based cameras. We assess the performances of the framework by operating on several dynamic scenarios in uncontrolled lighting conditions indoors and outdoors. We also introduce a new publicly available event-based dataset for gesture recognition selected through a clinical process to allow human-machine interactions for the visually-impaired and the elderly. We finally report comparisons with prior works that tackled event-based gesture recognition reporting comparable if not superior results if taking into account the limited computational and memory constraints of the used hardware.



### Deeper Interpretability of Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/1811.07807v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07807v2)
- **Published**: 2018-11-19 17:10:44+00:00
- **Updated**: 2018-11-20 09:43:21+00:00
- **Authors**: Tian Xu, Jiayu Zhan, Oliver G. B. Garrod, Philip H. S. Torr, Song-Chun Zhu, Robin A. A. Ince, Philippe G. Schyns
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Convolutional Neural Networks (CNNs) have been one of the most influential recent developments in computer vision, particularly for categorization. There is an increasing demand for explainable AI as these systems are deployed in the real world. However, understanding the information represented and processed in CNNs remains in most cases challenging. Within this paper, we explore the use of new information theoretic techniques developed in the field of neuroscience to enable novel understanding of how a CNN represents information. We trained a 10-layer ResNet architecture to identify 2,000 face identities from 26M images generated using a rigorously controlled 3D face rendering model that produced variations of intrinsic (i.e. face morphology, gender, age, expression and ethnicity) and extrinsic factors (i.e. 3D pose, illumination, scale and 2D translation). With our methodology, we demonstrate that unlike human's network overgeneralizes face identities even with extreme changes of face shape, but it is more sensitive to changes of texture. To understand the processing of information underlying these counterintuitive properties, we visualize the features of shape and texture that the network processes to identify faces. Then, we shed a light into the inner workings of the black box and reveal how hidden layers represent these features and whether the representations are invariant to pose. We hope that our methodology will provide an additional valuable tool for interpretability of CNNs.



### Event-Based Features Selection and Tracking from Intertwined Estimation of Velocity and Generative Contours
- **Arxiv ID**: http://arxiv.org/abs/1811.07839v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07839v1)
- **Published**: 2018-11-19 17:53:45+00:00
- **Updated**: 2018-11-19 17:53:45+00:00
- **Authors**: Laurent Dardelet, Sio-Hoi Ieng, Ryad Benosman
- **Comment**: 9 pages, 6 figures, 2 algorithms, 1 table
- **Journal**: None
- **Summary**: This paper presents a new event-based method for detecting and tracking features from the output of an event-based camera. Unlike many tracking algorithms from the computer vision community, this process does not aim for particular predefined shapes such as corners. It relies on a dual intertwined iterative continuous -- pure event-based -- estimation of the velocity vector and a bayesian description of the generative feature contours. By projecting along estimated speeds updated for each incoming event it is possible to identify and determine the spatial location and generative contour of the tracked feature while iteratively updating the estimation of the velocity vector. Results on several environments are shown taking into account large variations in terms of luminosity, speed, nature and size of the tracked features. The usage of speed instead of positions allows for a much faster feedback allowing for very fast convergence rates.



### OrthoSeg: A Deep Multimodal Convolutional Neural Network for Semantic Segmentation of Orthoimagery
- **Arxiv ID**: http://arxiv.org/abs/1811.07859v2
- **DOI**: 10.5194/isprs-archives-XLII-5-621-2018
- **Categories**: **cs.CV**, 68T45, I.4.6; I.5.1; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/1811.07859v2)
- **Published**: 2018-11-19 18:21:41+00:00
- **Updated**: 2018-11-20 16:45:21+00:00
- **Authors**: Pankaj Bodani, Kumar Shreshtha, Shashikant Sharma
- **Comment**: 8 pages, 9 figures, 3 tables
- **Journal**: Int. Arch. Photogramm. Remote Sens. Spatial Inf. Sci., XLII-5,
  621-628, 2018
- **Summary**: This paper addresses the task of semantic segmentation of orthoimagery using multimodal data e.g. optical RGB, infrared and digital surface model. We propose a deep convolutional neural network architecture termed OrthoSeg for semantic segmentation using multimodal, orthorectified and coregistered data. We also propose a training procedure for supervised training of OrthoSeg. The training procedure complements the inherent architectural characteristics of OrthoSeg for preventing complex co-adaptations of learned features, which may arise due to probable high dimensionality and spatial correlation in multimodal and/or multispectral coregistered data. OrthoSeg consists of parallel encoding networks for independent encoding of multimodal feature maps and a decoder designed for efficiently fusing independently encoded multimodal feature maps. A softmax layer at the end of the network uses the features generated by the decoder for pixel-wise classification. The decoder fuses feature maps from the parallel encoders locally as well as contextually at multiple scales to generate per-pixel feature maps for final pixel-wise classification resulting in segmented output. We experimentally show the merits of OrthoSeg by demonstrating state-of-the-art accuracy on the ISPRS Potsdam 2D Semantic Segmentation dataset. Adaptability is one of the key motivations behind OrthoSeg so that it serves as a useful architectural option for a wide range of problems involving the task of semantic segmentation of coregistered multimodal and/or multispectral imagery. Hence, OrthoSeg is designed to enable independent scaling of parallel encoder networks and decoder network to better match application requirements, such as the number of input channels, the effective field-of-view, and model capacity.



### Learning to synthesize: splitting and recombining low and high spatial frequencies for image recovery
- **Arxiv ID**: http://arxiv.org/abs/1811.07945v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07945v1)
- **Published**: 2018-11-19 19:29:20+00:00
- **Updated**: 2018-11-19 19:29:20+00:00
- **Authors**: Mo Deng, Shuai Li, George Barbastathis
- **Comment**: 10 pages, 10 figures. Supplement file can be provided upon reasonable
  request
- **Journal**: None
- **Summary**: Deep Neural Network (DNN)-based image reconstruction, despite many successes, often exhibits uneven fidelity between high and low spatial frequency bands. In this paper we propose the Learning Synthesis by DNN (LS-DNN) approach where two DNNs process the low and high spatial frequencies, respectively, and, improving over [30], the two DNNs are trained separately and a third DNN combines them into an image with high fidelity at all bands. We demonstrate LS-DNN in two canonical inverse problems: super-resolution (SR) in diffraction-limited imaging (DLI), and quantitative phase retrieval (QPR). Our results also show comparable or improved performance over perceptual-loss based SR [21], and can be generalized to a wider range of image recovery problems.



### Optimal Transport Classifier: Defending Against Adversarial Attacks by Regularized Deep Embedding
- **Arxiv ID**: http://arxiv.org/abs/1811.07950v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07950v2)
- **Published**: 2018-11-19 19:42:38+00:00
- **Updated**: 2018-12-09 17:30:27+00:00
- **Authors**: Yao Li, Martin Renqiang Min, Wenchao Yu, Cho-Jui Hsieh, Thomas C. M. Lee, Erik Kruus
- **Comment**: 9 pages
- **Journal**: Proceedings of the IEEE/CVF International Conference on Computer
  Vision (ICCV), 2021, pp. 7496-7505
- **Summary**: Recent studies have demonstrated the vulnerability of deep convolutional neural networks against adversarial examples. Inspired by the observation that the intrinsic dimension of image data is much smaller than its pixel space dimension and the vulnerability of neural networks grows with the input dimension, we propose to embed high-dimensional input images into a low-dimensional space to perform classification. However, arbitrarily projecting the input images to a low-dimensional space without regularization will not improve the robustness of deep neural networks. Leveraging optimal transport theory, we propose a new framework, Optimal Transport Classifier (OT-Classifier), and derive an objective that minimizes the discrepancy between the distribution of the true label and the distribution of the OT-Classifier output. Experimental results on several benchmark datasets show that, our proposed framework achieves state-of-the-art performance against strong adversarial attack methods.



### Tukey-Inspired Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1811.07958v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07958v2)
- **Published**: 2018-11-19 20:15:27+00:00
- **Updated**: 2018-11-30 02:37:11+00:00
- **Authors**: Brent A. Griffin, Jason J. Corso
- **Comment**: None
- **Journal**: None
- **Summary**: We investigate the problem of strictly unsupervised video object segmentation, i.e., the separation of a primary object from background in video without a user-provided object mask or any training on an annotated dataset. We find foreground objects in low-level vision data using a John Tukey-inspired measure of "outlierness". This Tukey-inspired measure also estimates the reliability of each data source as video characteristics change (e.g., a camera starts moving). The proposed method achieves state-of-the-art results for strictly unsupervised video object segmentation on the challenging DAVIS dataset. Finally, we use a variant of the Tukey-inspired measure to combine the output of multiple segmentation methods, including those using supervision during training, runtime, or both. This collectively more robust method of segmentation improves the Jaccard measure of its constituent methods by as much as 28%.



### Mitigating Architectural Mismatch During the Evolutionary Synthesis of Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1811.07966v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1811.07966v1)
- **Published**: 2018-11-19 20:36:16+00:00
- **Updated**: 2018-11-19 20:36:16+00:00
- **Authors**: Audrey Chung, Paul Fieguth, Alexander Wong
- **Comment**: 5 pages
- **Journal**: None
- **Summary**: Evolutionary deep intelligence has recently shown great promise for producing small, powerful deep neural network models via the organic synthesis of increasingly efficient architectures over successive generations. Existing evolutionary synthesis processes, however, have allowed the mating of parent networks independent of architectural alignment, resulting in a mismatch of network structures. We present a preliminary study into the effects of architectural alignment during evolutionary synthesis using a gene tagging system. Surprisingly, the network architectures synthesized using the gene tagging approach resulted in slower decreases in performance accuracy and storage size; however, the resultant networks were comparable in size and performance accuracy to the non-gene tagging networks. Furthermore, we speculate that there is a noticeable decrease in network variability for networks synthesized with gene tagging, indicating that enforcing a like-with-like mating policy potentially restricts the exploration of the search space of possible network architectures.



### Informed MCMC with Bayesian Neural Networks for Facial Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/1811.07969v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07969v2)
- **Published**: 2018-11-19 20:47:04+00:00
- **Updated**: 2018-11-29 15:05:50+00:00
- **Authors**: Adam Kortylewski, Mario Wieser, Andreas Morel-Forster, Aleksander Wieczorek, Sonali Parbhoo, Volker Roth, Thomas Vetter
- **Comment**: Accepted to the Bayesian Deep Learning Workshop at NeurIPS 2018
- **Journal**: None
- **Summary**: Computer vision tasks are difficult because of the large variability in the data that is induced by changes in light, background, partial occlusion as well as the varying pose, texture, and shape of objects. Generative approaches to computer vision allow us to overcome this difficulty by explicitly modeling the physical image formation process. Using generative object models, the analysis of an observed image is performed via Bayesian inference of the posterior distribution. This conceptually simple approach tends to fail in practice because of several difficulties stemming from sampling the posterior distribution: high-dimensionality and multi-modality of the posterior distribution as well as expensive simulation of the rendering process. The main difficulty of sampling approaches in a computer vision context is choosing the proposal distribution accurately so that maxima of the posterior are explored early and the algorithm quickly converges to a valid image interpretation. In this work, we propose to use a Bayesian Neural Network for estimating an image dependent proposal distribution. Compared to a standard Gaussian random walk proposal, this accelerates the sampler in finding regions of the posterior with high value. In this way, we can significantly reduce the number of samples needed to perform facial image analysis.



### Can Synthetic Faces Undo the Damage of Dataset Bias to Face Recognition and Facial Landmark Detection?
- **Arxiv ID**: http://arxiv.org/abs/1811.08565v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08565v2)
- **Published**: 2018-11-19 21:17:21+00:00
- **Updated**: 2019-06-23 00:26:34+00:00
- **Authors**: Adam Kortylewski, Bernhard Egger, Andreas Morel-Forster, Andreas Schneider, Thomas Gerig, Clemens Blumer, Corius Reyneke, Thomas Vetter
- **Comment**: Technical report
- **Journal**: None
- **Summary**: It is well known that deep learning approaches to face recognition and facial landmark detection suffer from biases in modern training datasets. In this work, we propose to use synthetic face images to reduce the negative effects of dataset biases on these tasks. Using a 3D morphable face model, we generate large amounts of synthetic face images with full control over facial shape and color, pose, illumination, and background. With a series of experiments, we extensively test the effects of priming deep nets by pre-training them with synthetic faces. We observe the following positive effects for face recognition and facial landmark detection tasks: 1) Priming with synthetic face images improves the performance consistently across all benchmarks because it reduces the negative effects of biases in the training data. 2) Traditional approaches for reducing the damage of dataset bias, such as data augmentation and transfer learning, are less effective than training with synthetic faces. 3) Using synthetic data, we can reduce the size of real-world datasets by 75% for face recognition and by 50% for facial landmark detection while maintaining performance. Thus, offering a means to focus the data collection process on less but higher quality data.



### Generalized Zero-Shot Recognition based on Visually Semantic Embedding
- **Arxiv ID**: http://arxiv.org/abs/1811.07993v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07993v2)
- **Published**: 2018-11-19 21:38:28+00:00
- **Updated**: 2019-04-09 17:48:27+00:00
- **Authors**: Pengkai Zhu, Hanxiao Wang, Venkatesh Saligrama
- **Comment**: Accepted in CVPR209. 9 pages, 3 figures, 6 tables
- **Journal**: None
- **Summary**: We propose a novel Generalized Zero-Shot learning (GZSL) method that is agnostic to both unseen images and unseen semantic vectors during training. Prior works in this context propose to map high-dimensional visual features to the semantic domain, we believe contributes to the semantic gap. To bridge the gap, we propose a novel low-dimensional embedding of visual instances that is "visually semantic." Analogous to semantic data that quantifies the existence of an attribute in the presented instance, components of our visual embedding quantifies existence of a prototypical part-type in the presented instance. In parallel, as a thought experiment, we quantify the impact of noisy semantic data by utilizing a novel visual oracle to visually supervise a learner. These factors, namely semantic noise, visual-semantic gap and label noise lead us to propose a new graphical model for inference with pairwise interactions between label, semantic data, and inputs. We tabulate results on a number of benchmark datasets demonstrating significant improvement in accuracy over state-of-the-art under both semantic and visual supervision.



### Synthetic Lung Nodule 3D Image Generation Using Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/1811.07999v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.07999v3)
- **Published**: 2018-11-19 21:51:38+00:00
- **Updated**: 2019-09-09 05:58:21+00:00
- **Authors**: Steve Kommrusch, Louis-Noël Pouchet
- **Comment**: 19 pages, 12 figures, full paper for work initially presented at
  IJCAI 2018
- **Journal**: None
- **Summary**: One of the challenges of using machine learning techniques with medical data is the frequent dearth of source image data on which to train. A representative example is automated lung cancer diagnosis, where nodule images need to be classified as suspicious or benign. In this work we propose an automatic synthetic lung nodule image generator. Our 3D shape generator is designed to augment the variety of 3D images. Our proposed system takes root in autoencoder techniques, and we provide extensive experimental characterization that demonstrates its ability to produce quality synthetic images.



### Scalable Logo Recognition using Proxies
- **Arxiv ID**: http://arxiv.org/abs/1811.08009v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.08009v1)
- **Published**: 2018-11-19 22:28:13+00:00
- **Updated**: 2018-11-19 22:28:13+00:00
- **Authors**: Istvan Fehervari, Srikar Appalaraju
- **Comment**: Accepted at IEEE WACV 2019, Hawaii USA
- **Journal**: None
- **Summary**: Logo recognition is the task of identifying and classifying logos. Logo recognition is a challenging problem as there is no clear definition of a logo and there are huge variations of logos, brands and re-training to cover every variation is impractical. In this paper, we formulate logo recognition as a few-shot object detection problem. The two main components in our pipeline are universal logo detector and few-shot logo recognizer. The universal logo detector is a class-agnostic deep object detector network which tries to learn the characteristics of what makes a logo. It predicts bounding boxes on likely logo regions. These logo regions are then classified by logo recognizer using nearest neighbor search, trained by triplet loss using proxies. We also annotated a first of its kind product logo dataset containing 2000 logos from 295K images collected from Amazon called PL2K. Our pipeline achieves 97% recall with 0.6 mAP on PL2K test dataset and state-of-the-art 0.565 mAP on the publicly available FlickrLogos-32 test set without fine-tuning.



### Explain to Fix: A Framework to Interpret and Correct DNN Object Detector Predictions
- **Arxiv ID**: http://arxiv.org/abs/1811.08011v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08011v1)
- **Published**: 2018-11-19 22:41:49+00:00
- **Updated**: 2018-11-19 22:41:49+00:00
- **Authors**: Denis Gudovskiy, Alec Hodgkinson, Takuya Yamaguchi, Yasunori Ishii, Sotaro Tsukizawa
- **Comment**: Systems for ML Workshop @ NIPS 2018
- **Journal**: None
- **Summary**: Explaining predictions of deep neural networks (DNNs) is an important and nontrivial task. In this paper, we propose a practical approach to interpret decisions made by a DNN object detector that has fidelity comparable to state-of-the-art methods and sufficient computational efficiency to process large datasets. Our method relies on recent theory and approximates Shapley feature importance values. We qualitatively and quantitatively show that the proposed explanation method can be used to find image features which cause failures in DNN object detection. The developed software tool combined into the "Explain to Fix" (E2X) framework has a factor of 10 higher computational efficiency than prior methods and can be used for cluster processing using graphics processing units (GPUs). Lastly, we propose a potential extension of the E2X framework where the discovered missing features can be added into training dataset to overcome failures after model retraining.



### A Comparative Study of Computational Aesthetics
- **Arxiv ID**: http://arxiv.org/abs/1811.08012v1
- **DOI**: 10.1109/ICIP.2014.7025118
- **Categories**: **eess.IV**, cs.CV, cs.MM, I.4
- **Links**: [PDF](http://arxiv.org/pdf/1811.08012v1)
- **Published**: 2018-11-19 22:46:12+00:00
- **Updated**: 2018-11-19 22:46:12+00:00
- **Authors**: Dogancan Temel, Ghassan AlRegib
- **Comment**: 6 pages, 5 figures, 1 table
- **Journal**: 2014 IEEE International Conference on Image Processing (ICIP),
  Paris, 2014, pp. 590-594
- **Summary**: Objective metrics model image quality by quantifying image degradations or estimating perceived image quality. However, image quality metrics do not model what makes an image more appealing or beautiful. In order to quantify the aesthetics of an image, we need to take it one step further and model the perception of aesthetics. In this paper, we examine computational aesthetics models that use hand-crafted, generic and hybrid descriptors. We show that generic descriptors can perform as well as state of the art hand-crafted aesthetics models that use global features. However, neither generic nor hand-crafted features is sufficient to model aesthetics when we only use global features without considering spatial composition or distribution. We also follow a visual dictionary approach similar to state of the art methods and show that it performs poorly without the spatial pyramid step.



### Visual Font Pairing
- **Arxiv ID**: http://arxiv.org/abs/1811.08015v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08015v1)
- **Published**: 2018-11-19 23:07:38+00:00
- **Updated**: 2018-11-19 23:07:38+00:00
- **Authors**: Shuhui Jiang, Zhaowen Wang, Aaron Hertzmann, Hailin Jin, Yun Fu
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces the problem of automatic font pairing. Font pairing is an important design task that is difficult for novices. Given a font selection for one part of a document (e.g., header), our goal is to recommend a font to be used in another part (e.g., body) such that the two fonts used together look visually pleasing. There are three main challenges in font pairing. First, this is a fine-grained problem, in which the subtle distinctions between fonts may be important. Second, rules and conventions of font pairing given by human experts are difficult to formalize. Third, font pairing is an asymmetric problem in that the roles played by header and body fonts are not interchangeable. To address these challenges, we propose automatic font pairing through learning visual relationships from large-scale human-generated font pairs. We introduce a new database for font pairing constructed from millions of PDF documents available on the Internet. We propose two font pairing algorithms: dual-space k-NN and asymmetric similarity metric learning (ASML). These two methods automatically learn fine-grained relationships from large-scale data. We also investigate several baseline methods based on the rules from professional designers. Experiments and user studies demonstrate the effectiveness of our proposed dataset and methods.



