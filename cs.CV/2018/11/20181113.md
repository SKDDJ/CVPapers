# Arxiv Papers in cs.CV on 2018-11-13
### Exploiting temporal and depth information for multi-frame face anti-spoofing
- **Arxiv ID**: http://arxiv.org/abs/1811.05118v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.05118v3)
- **Published**: 2018-11-13 05:59:34+00:00
- **Updated**: 2019-03-05 11:49:19+00:00
- **Authors**: Zezheng Wang, Chenxu Zhao, Yunxiao Qin, Qiusheng Zhou, Guojun Qi, Jun Wan, Zhen Lei
- **Comment**: None
- **Journal**: None
- **Summary**: Face anti-spoofing is significant to the security of face recognition systems. Previous works on depth supervised learning have proved the effectiveness for face anti-spoofing. Nevertheless, they only considered the depth as an auxiliary supervision in the single frame. Different from these methods, we develop a new method to estimate depth information from multiple RGB frames and propose a depth-supervised architecture which can efficiently encodes spatiotemporal information for presentation attack detection. It includes two novel modules: optical flow guided feature block (OFFB) and convolution gated recurrent units (ConvGRU) module, which are designed to extract short-term and long-term motion to discriminate living and spoofing faces. Extensive experiments demonstrate that the proposed approach achieves state-of-the-art results on four benchmark datasets, namely OULU-NPU, SiW, CASIA-MFSD, and Replay-Attack.



### Application of Faster R-CNN model on Human Running Pattern Recognition
- **Arxiv ID**: http://arxiv.org/abs/1811.05147v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.05147v1)
- **Published**: 2018-11-13 07:51:51+00:00
- **Updated**: 2018-11-13 07:51:51+00:00
- **Authors**: Kairan Yang, Feng Geng
- **Comment**: 10 pages, 2 figures, 4 tables
- **Journal**: None
- **Summary**: The advance algorithms like Faster Regional Convolutional Neural Network (Faster R-CNN) models are suitable to identify classified moving objects, due to the efficiency in learning the training dataset superior than ordinary CNN algorithms and the higher accuracy of labeling correct classes in the validation and testing dataset. This research examined and compared the three R-CNN type algorithms in object recognition to show the superior efficiency and accuracy of Faster R-CNN model on classifying human running patterns. Then it described the effect of Faster R-CNN in detecting different types of running patterns exhibited by a single individual or multiple individuals by conducting a dataset fitting experiment. In this study, the Faster R-CNN algorithm is implemented directly from the version released by Ross Girshick.



### Vehicle Re-identification Using Quadruple Directional Deep Learning Features
- **Arxiv ID**: http://arxiv.org/abs/1811.05163v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.05163v1)
- **Published**: 2018-11-13 08:44:19+00:00
- **Updated**: 2018-11-13 08:44:19+00:00
- **Authors**: Jianqing Zhu, Huanqiang Zeng, Jingchang Huang, Shengcai Liao, Zhen Lei, Canhui Cai, LiXin Zheng
- **Comment**: this paper has been submitted to IEEE Transactions on Intelligent
  Transportation Systems, under review
- **Journal**: None
- **Summary**: In order to resist the adverse effect of viewpoint variations for improving vehicle re-identification performance, we design quadruple directional deep learning networks to extract quadruple directional deep learning features (QD-DLF) of vehicle images. The quadruple directional deep learning networks are with similar overall architecture, including the same basic deep learning architecture but different directional feature pooling layers. Specifically, the same basic deep learning architecture is a shortly and densely connected convolutional neural network to extract basic feature maps of an input square vehicle image in the first stage. Then, the quadruple directional deep learning networks utilize different directional pooling layers, i.e., horizontal average pooling (HAP) layer, vertical average pooling (VAP) layer, diagonal average pooling (DAP) layer and anti-diagonal average pooling (AAP) layer, to compress the basic feature maps into horizontal, vertical, diagonal and anti-diagonal directional feature maps, respectively.   Finally, these directional feature maps are spatially normalized and concatenated together as a quadruple directional deep learning feature for vehicle re-identification. Extensive experiments on both VeRi and VehicleID databases show that the proposed QD-DLF approach outperforms multiple state-of-the-art vehicle re-identification methods.



### Child Gender Determination with Convolutional Neural Networks on Hand Radio-Graphs
- **Arxiv ID**: http://arxiv.org/abs/1811.05180v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.05180v1)
- **Published**: 2018-11-13 09:43:55+00:00
- **Updated**: 2018-11-13 09:43:55+00:00
- **Authors**: Mumtaz A. Kaloi, Kun He
- **Comment**: 8 pages, 8 figures
- **Journal**: None
- **Summary**: Motivation: In forensic or medico-legal investigation as well as in anthropology the gender determination of the subject (hit by a disastrous or any kind of traumatic situation) is mostly the first step. In state-of-the-art techniques the gender is determined by examining dimensions of the bones of skull and the pelvis area. In worse situations when there is only a small portion of the human remains to be investigated and the subject is a child, we need alternative techniques to determine the gender of the subject. In this work we propose a technique called GDCNN (Gender Determination with Convolutional Neural Networks), where the left hand radio-graphs of the children between a wide range of ages in 1 month to 18 years are examined to determine the gender. To our knowledge this technique is first of its kind. Further to identify the area of the attention we used Class Activation Mapping (CAM). Results: The results suggest the accuracy of the model is as high as 98%, which is very convincing by taking into account the incompletely grown skeleton of the children. The attention observed with CAM discovers that the lower part of the hand around carpals (wrist) is more important for child gender determination.



### Gradient Harmonized Single-stage Detector
- **Arxiv ID**: http://arxiv.org/abs/1811.05181v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.05181v1)
- **Published**: 2018-11-13 09:47:52+00:00
- **Updated**: 2018-11-13 09:47:52+00:00
- **Authors**: Buyu Li, Yu Liu, Xiaogang Wang
- **Comment**: To appear at AAAI 2019
- **Journal**: None
- **Summary**: Despite the great success of two-stage detectors, single-stage detector is still a more elegant and efficient way, yet suffers from the two well-known disharmonies during training, i.e. the huge difference in quantity between positive and negative examples as well as between easy and hard examples. In this work, we first point out that the essential effect of the two disharmonies can be summarized in term of the gradient. Further, we propose a novel gradient harmonizing mechanism (GHM) to be a hedging for the disharmonies. The philosophy behind GHM can be easily embedded into both classification loss function like cross-entropy (CE) and regression loss function like smooth-$L_1$ ($SL_1$) loss. To this end, two novel loss functions called GHM-C and GHM-R are designed to balancing the gradient flow for anchor classification and bounding box refinement, respectively. Ablation study on MS COCO demonstrates that without laborious hyper-parameter tuning, both GHM-C and GHM-R can bring substantial improvement for single-stage detector. Without any whistles and bells, our model achieves 41.6 mAP on COCO test-dev set which surpasses the state-of-the-art method, Focal Loss (FL) + $SL_1$, by 0.8.



### Massively Distributed SGD: ImageNet/ResNet-50 Training in a Flash
- **Arxiv ID**: http://arxiv.org/abs/1811.05233v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1811.05233v2)
- **Published**: 2018-11-13 11:52:04+00:00
- **Updated**: 2019-03-05 09:18:09+00:00
- **Authors**: Hiroaki Mikami, Hisahiro Suganuma, Pongsakorn U-chupala, Yoshiki Tanaka, Yuichi Kageyama
- **Comment**: None
- **Journal**: None
- **Summary**: Scaling the distributed deep learning to a massive GPU cluster level is challenging due to the instability of the large mini-batch training and the overhead of the gradient synchronization. We address the instability of the large mini-batch training with batch-size control and label smoothing. We address the overhead of the gradient synchronization with 2D-Torus all-reduce. Specifically, 2D-Torus all-reduce arranges GPUs in a logical 2D grid and performs a series of collective operation in different orientations. These two techniques are implemented with Neural Network Libraries (NNL). We have successfully trained ImageNet/ResNet-50 in 122 seconds without significant accuracy loss on ABCI cluster.



### BAN: Focusing on Boundary Context for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1811.05243v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.05243v1)
- **Published**: 2018-11-13 12:10:38+00:00
- **Updated**: 2018-11-13 12:10:38+00:00
- **Authors**: Yonghyun Kim, Taewook Kim, Bong-Nam Kang, Jieun Kim, Daijin Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Visual context is one of the important clue for object detection and the context information for boundaries of an object is especially valuable. We propose a boundary aware network (BAN) designed to exploit the visual contexts including boundary information and surroundings, named boundary context, and define three types of the boundary contexts: side, vertex and in/out-boundary context. Our BAN consists of 10 sub-networks for the area belonging to the boundary contexts. The detection head of BAN is defined as an ensemble of these sub-networks with different contributions depending on the sub-problem of detection. To verify our method, we visualize the activation of the sub-networks according to the boundary contexts and empirically show that the sub-networks contribute more to the related sub-problem in detection. We evaluate our method on PASCAL VOC detection benchmark and MS COCO dataset. The proposed method achieves the mean Average Precision (mAP) of 83.4% on PASCAL VOC and 36.9% on MS COCO. BAN allows the convolution network to provide an additional source of contexts for detection and selectively focus on the more important contexts, and it can be generally applied to many other detection methods as well to enhance the accuracy in detection.



### Modality Attention for End-to-End Audio-visual Speech Recognition
- **Arxiv ID**: http://arxiv.org/abs/1811.05250v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1811.05250v2)
- **Published**: 2018-11-13 12:28:03+00:00
- **Updated**: 2019-04-23 04:21:06+00:00
- **Authors**: Pan Zhou, Wenwen Yang, Wei Chen, Yanfeng Wang, Jia Jia
- **Comment**: accepted by ICASSP2019
- **Journal**: None
- **Summary**: Audio-visual speech recognition (AVSR) system is thought to be one of the most promising solutions for robust speech recognition, especially in noisy environment. In this paper, we propose a novel multimodal attention based method for audio-visual speech recognition which could automatically learn the fused representation from both modalities based on their importance. Our method is realized using state-of-the-art sequence-to-sequence (Seq2seq) architectures. Experimental results show that relative improvements from 2% up to 36% over the auditory modality alone are obtained depending on the different signal-to-noise-ratio (SNR). Compared to the traditional feature concatenation methods, our proposed approach can achieve better recognition performance under both clean and noisy conditions. We believe modality attention based end-to-end method can be easily generalized to other multimodal tasks with correlated information.



### Image Captioning Based on a Hierarchical Attention Mechanism and Policy Gradient Optimization
- **Arxiv ID**: http://arxiv.org/abs/1811.05253v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.05253v2)
- **Published**: 2018-11-13 12:31:26+00:00
- **Updated**: 2019-01-11 03:31:31+00:00
- **Authors**: Shiyang Yan, Yuan Xie, Fangyu Wu, Jeremy S. Smith, Wenjin Lu, Bailing Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Automatically generating the descriptions of an image, i.e., image captioning, is an important and fundamental topic in artificial intelligence, which bridges the gap between computer vision and natural language processing. Based on the successful deep learning models, especially the CNN model and Long Short-Term Memories (LSTMs) with attention mechanism, we propose a hierarchical attention model by utilizing both of the global CNN features and the local object features for more effective feature representation and reasoning in image captioning. The generative adversarial network (GAN), together with a reinforcement learning (RL) algorithm, is applied to solve the exposure bias problem in RNN-based supervised training for language problems. In addition, through the automatic measurement of the consistency between the generated caption and the image content by the discriminator in the GAN framework and RL optimization, we make the finally generated sentences more accurate and natural. Comprehensive experiments show the improved performance of the hierarchical attention mechanism and the effectiveness of our RL-based optimization method. Our model achieves state-of-the-art results on several important metrics in the MSCOCO dataset, using only greedy inference.



### Deep Neural Network Concepts for Background Subtraction: A Systematic Review and Comparative Evaluation
- **Arxiv ID**: http://arxiv.org/abs/1811.05255v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.05255v1)
- **Published**: 2018-11-13 12:35:19+00:00
- **Updated**: 2018-11-13 12:35:19+00:00
- **Authors**: Thierry Bouwmans, Sajid Javed, Maryam Sultana, Soon Ki Jung
- **Comment**: 46 pages, 4 figures, submitted to neural networks
- **Journal**: None
- **Summary**: Conventional neural networks show a powerful framework for background subtraction in video acquired by static cameras. Indeed, the well-known SOBS method and its variants based on neural networks were the leader methods on the largescale CDnet 2012 dataset during a long time. Recently, convolutional neural networks which belong to deep learning methods were employed with success for background initialization, foreground detection and deep learned features. Currently, the top current background subtraction methods in CDnet 2014 are based on deep neural networks with a large gap of performance in comparison on the conventional unsupervised approaches based on multi-features or multi-cues strategies. Furthermore, a huge amount of papers was published since 2016 when Braham and Van Droogenbroeck published their first work on CNN applied to background subtraction providing a regular gain of performance. In this context, we provide the first review of deep neural network concepts in background subtraction for novices and experts in order to analyze this success and to provide further directions. For this, we first surveyed the methods used background initialization, background subtraction and deep learned features. Then, we discuss the adequacy of deep neural networks for background subtraction. Finally, experimental results are presented on the CDnet 2014 dataset.



### Pose Invariant 3D Face Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1811.05295v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.05295v1)
- **Published**: 2018-11-13 14:00:48+00:00
- **Updated**: 2018-11-13 14:00:48+00:00
- **Authors**: Lei Jiang, XiaoJun Wu, Josef Kittler
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: 3D face reconstruction is an important task in the field of computer vision. Although 3D face reconstruction has being developing rapidly in recent years, it is still a challenge for face reconstruction under large pose. That is because much of the information about a face in a large pose will be unknowable. In order to address this issue, this paper proposes a novel 3D face reconstruction algorithm (PIFR) based on 3D Morphable Model (3DMM). After input a single face image, it generates a frontal image by normalizing the image. Then we set weighted sum of the 3D parameters of the two images. Our method solves the problem of face reconstruction of a single image of a traditional method in a large pose, works on arbitrary Pose and Expressions, greatly improves the accuracy of reconstruction. Experiments on the challenging AFW, LFPW and AFLW database show that our algorithm significantly improves the accuracy of 3D face reconstruction even under extreme poses .



### Self-Supervised Learning of Depth and Camera Motion from 360° Videos
- **Arxiv ID**: http://arxiv.org/abs/1811.05304v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.05304v1)
- **Published**: 2018-11-13 14:07:27+00:00
- **Updated**: 2018-11-13 14:07:27+00:00
- **Authors**: Fu-En Wang, Hou-Ning Hu, Hsien-Tzu Cheng, Juan-Ting Lin, Shang-Ta Yang, Meng-Li Shih, Hung-Kuo Chu, Min Sun
- **Comment**: ACCV 2018 Oral
- **Journal**: None
- **Summary**: As 360{\deg} cameras become prevalent in many autonomous systems (e.g., self-driving cars and drones), efficient 360{\deg} perception becomes more and more important. We propose a novel self-supervised learning approach for predicting the omnidirectional depth and camera motion from a 360{\deg} video. In particular, starting from the SfMLearner, which is designed for cameras with normal field-of-view, we introduce three key features to process 360{\deg} images efficiently. Firstly, we convert each image from equirectangular projection to cubic projection in order to avoid image distortion. In each network layer, we use Cube Padding (CP), which pads intermediate features from adjacent faces, to avoid image boundaries. Secondly, we propose a novel "spherical" photometric consistency constraint on the whole viewing sphere. In this way, no pixel will be projected outside the image boundary which typically happens in images with normal field-of-view. Finally, rather than naively estimating six independent camera motions (i.e., naively applying SfM-Learner to each face on a cube), we propose a novel camera pose consistency loss to ensure the estimated camera motions reaching consensus. To train and evaluate our approach, we collect a new PanoSUNCG dataset containing a large amount of 360{\deg} videos with groundtruth depth and camera motion. Our approach achieves state-of-the-art depth prediction and camera motion estimation on PanoSUNCG with faster inference speed comparing to equirectangular. In real-world indoor videos, our approach can also achieve qualitatively reasonable depth prediction by acquiring model pre-trained on PanoSUNCG.



### Improved Fourier Mellin Invariant for Robust Rotation Estimation with Omni-cameras
- **Arxiv ID**: http://arxiv.org/abs/1811.05306v4
- **DOI**: 10.1109/ICIP.2019.8802933
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1811.05306v4)
- **Published**: 2018-11-13 14:09:32+00:00
- **Updated**: 2019-10-14 02:16:49+00:00
- **Authors**: Qingwen Xu, Arturo Gomez Chavez, Heiko Bülow, Andreas Birk, Sören Schwertfeger
- **Comment**: 5 pages, 4 figures, 1 table
- **Journal**: None
- **Summary**: Spectral methods such as the improved Fourier Mellin Invariant (iFMI) transform have proved faster, more robust and accurate than feature based methods on image registration. However, iFMI is restricted to work only when the camera moves in 2D space and has not been applied on omni-cameras images so far. In this work, we extend the iFMI method and apply a motion model to estimate an omni-camera's pose when it moves in 3D space. This is particularly useful in field robotics applications to get a rapid and comprehensive view of unstructured environments, and to estimate robustly the robot pose. In the experiment section, we compared the extended iFMI method against ORB and AKAZE feature based approaches on three datasets showing different type of environments: office, lawn and urban scenery (MPI-omni dataset). The results show that our method boosts the accuracy of the robot pose estimation two to four times with respect to the feature registration techniques, while offering lower processing times. Furthermore, the iFMI approach presents the best performance against motion blur typically present in mobile robotics.



### Mammographic density: Comparison of visual assessment with fully automatic calculation on a multivendor dataset
- **Arxiv ID**: http://arxiv.org/abs/1811.05324v1
- **DOI**: 10.1007/s00330-015-3784-2
- **Categories**: **physics.med-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1811.05324v1)
- **Published**: 2018-11-13 14:31:36+00:00
- **Updated**: 2018-11-13 14:31:36+00:00
- **Authors**: Daniela Sacchetto, Lia Morra, Silvano Agliozzo, Daniela Bernardi, Tomas Bjorklund, Beniamino Brancato, Patrizia Bravetti, Luca A. Carbonaro, Loredana Correale, Carmen Fantò, Elisabetta Favettini, Laura Martincich, Luisella Milanesio, Sara Mombelloni, Francesco Monetti, Doralba Morrone, Marco Pellegrini, Barbara Pesce, Antonella Petrillo, Gianni Saguatti, Carmen Stevanin, Rubina M. Trimboli, Paola Tuttobene, Marvi Valentini, Vincenzo Marra, Alfonso Frigerio, Alberto Bert, Francesco Sardanelli
- **Comment**: None
- **Journal**: Sacchetto, Daniela et al. "Mammographic density: comparison of
  visual assessment with fully automatic calculation on a multivendor dataset."
  European radiology 26, no. 1 (2016): 175-183
- **Summary**: Objectives: To compare breast density (BD) assessment provided by an automated BD evaluator (ABDE) with that provided by a panel of experienced breast radiologists, on a multivendor dataset.   Methods: Twenty-one radiologists assessed 613 screening/diagnostic digital mammograms from 9 centers and 6 different vendors, using the BI-RADS a, b, c, and d density classification. The same mammograms were also evaluated by an ABDE providing the ratio between fibroglandular and total breast area on a continuous scale and, automatically, the BI-RADS score. Panel majority report (PMR) was used as reference standard. Agreement (k) and accuracy (proportion of cases correctly classified) were calculated for binary (BI-RADS a-b versus c-d) and 4-class classification.   Results: While the agreement of individual radiologists with PMR ranged from k=0.483 to k=0.885, the ABDE correctly classified 563/613 mammograms (92%). A substantial agreement for binary classification was found for individual reader pairs (k=0.620, standard deviation [SD]=0.140), individual versus PMR (k=0.736, SD=0.117), and individual versus ABDE (k=0.674, SD=0.095). Agreement between ABDE and PMR was almost perfect (k=0.831).   Conclusions: The ABDE showed an almost perfect agreement with a 21-radiologist panel in binary BD classification on a multivendor dataset, earning a chance as a reproducible alternative to visual evaluation.



### Detect or Track: Towards Cost-Effective Video Object Detection/Tracking
- **Arxiv ID**: http://arxiv.org/abs/1811.05340v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.05340v1)
- **Published**: 2018-11-13 14:44:10+00:00
- **Updated**: 2018-11-13 14:44:10+00:00
- **Authors**: Hao Luo, Wenxuan Xie, Xinggang Wang, Wenjun Zeng
- **Comment**: Accepted to AAAI 2019
- **Journal**: None
- **Summary**: State-of-the-art object detectors and trackers are developing fast. Trackers are in general more efficient than detectors but bear the risk of drifting. A question is hence raised -- how to improve the accuracy of video object detection/tracking by utilizing the existing detectors and trackers within a given time budget? A baseline is frame skipping -- detecting every N-th frames and tracking for the frames in between. This baseline, however, is suboptimal since the detection frequency should depend on the tracking quality. To this end, we propose a scheduler network, which determines to detect or track at a certain frame, as a generalization of Siamese trackers. Although being light-weight and simple in structure, the scheduler network is more effective than the frame skipping baselines and flow-based approaches, as validated on ImageNet VID dataset in video object detection/tracking.



### Hallucinating Point Cloud into 3D Sculptural Object
- **Arxiv ID**: http://arxiv.org/abs/1811.05389v3
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1811.05389v3)
- **Published**: 2018-11-13 16:28:32+00:00
- **Updated**: 2018-11-29 04:41:54+00:00
- **Authors**: Chun-Liang Li, Eunsu Kang, Songwei Ge, Lingyao Zhang, Austin Dill, Manzil Zaheer, Barnabas Poczos
- **Comment**: Accepted by Second Workshop on Machine Learning for Creativity and
  Design, NIPS 2018
- **Journal**: None
- **Summary**: Our team of artists and machine learning researchers designed a creative algorithm that can generate authentic sculptural artworks. These artworks do not mimic any given forms and cannot be easily categorized into the dataset categories. Our approach extends DeepDream from images to 3D point clouds. The proposed algorithm, Amalgamated DeepDream (ADD), leverages the properties of point clouds to create objects with better quality than the naive extension. ADD presents promise for the creativity of machines, the kind of creativity that pushes artists to explore novel methods or materials and to create new genres instead of creating variations of existing forms or styles within one genre. For example, from Realism to Abstract Expressionism, or to Minimalism. Lastly, we present the sculptures that are 3D printed based on the point clouds created by ADD.



### Deep Learning for Automated Classification of Tuberculosis-Related Chest X-Ray: Dataset Specificity Limits Diagnostic Performance Generalizability
- **Arxiv ID**: http://arxiv.org/abs/1811.07985v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML, I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/1811.07985v2)
- **Published**: 2018-11-13 16:32:42+00:00
- **Updated**: 2018-12-28 08:44:47+00:00
- **Authors**: Seelwan Sathitratanacheewin, Krit Pongpirul
- **Comment**: 6 pages, 3 figures
- **Journal**: None
- **Summary**: Machine learning has been an emerging tool for various aspects of infectious diseases including tuberculosis surveillance and detection. However, WHO provided no recommendations on using computer-aided tuberculosis detection software because of the small number of studies, methodological limitations, and limited generalizability of the findings. To quantify the generalizability of the machine-learning model, we developed a Deep Convolutional Neural Network (DCNN) model using a TB-specific CXR dataset of one population (National Library of Medicine Shenzhen No.3 Hospital) and tested it with non-TB-specific CXR dataset of another population (National Institute of Health Clinical Centers). The findings suggested that a supervised deep learning model developed by using the training dataset from one population may not have the same diagnostic performance in another population. Technical specification of CXR images, disease severity distribution, overfitting, and overdiagnosis should be examined before implementation in other settings.



### Home Activity Monitoring using Low Resolution Infrared Sensor
- **Arxiv ID**: http://arxiv.org/abs/1811.05416v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.05416v1)
- **Published**: 2018-11-13 17:15:11+00:00
- **Updated**: 2018-11-13 17:15:11+00:00
- **Authors**: Lili Tao, Timothy Volonakis, Bo Tan, Yanguo Jing, Kevin Chetty, Melvyn Smith
- **Comment**: None
- **Journal**: None
- **Summary**: Action monitoring in a home environment provides important information for health monitoring and may serve as input into a smart home environment. Visual analysis using cameras can recognise actions in a complex scene, such as someones living room. However, although there the huge potential benefits and importance, specifically for health, cameras are not widely accepted because of privacy concerns. This paper recognises human activities using a sensor that retains privacy. The sensor is not only different by being thermal, but it is also of low resolution: 8x8 pixels. The combination of the thermal imaging, and the low spatial resolution ensures the privacy of individuals. We present an approach to recognise daily activities using this sensor based on a discrete cosine transform. We evaluate the proposed method on a state-of-the-art dataset and experimentally confirm that our approach outperforms the baseline method. We also introduce a new dataset, and evaluate the method on it. Here we show that the sensor is considered better at detecting the occurrence of falls and Activities of Daily Living. Our method achieves an overall accuracy of 87.50% across 7 activities with a fall detection sensitivity of 100% and specificity of 99.21%.



### Fast Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1811.05419v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.05419v2)
- **Published**: 2018-11-13 17:18:14+00:00
- **Updated**: 2019-04-02 14:05:56+00:00
- **Authors**: Feng Zhang, Xiatian Zhu, Mao Ye
- **Comment**: Accepted by CVPR2019
- **Journal**: None
- **Summary**: Existing human pose estimation approaches often only consider how to improve the model generalisation performance, but putting aside the significant efficiency problem. This leads to the development of heavy models with poor scalability and cost-effectiveness in practical use. In this work, we investigate the under-studied but practically critical pose model efficiency problem. To this end, we present a new Fast Pose Distillation (FPD) model learning strategy. Specifically, the FPD trains a lightweight pose neural network architecture capable of executing rapidly with low computational cost. It is achieved by effectively transferring the pose structure knowledge of a strong teacher network. Extensive evaluations demonstrate the advantages of our FPD method over a broad range of state-of-the-art pose estimation approaches in terms of model cost-effectiveness on two standard benchmark datasets, MPII Human Pose and Leeds Sports Pose.



### Deep Object-Centric Policies for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1811.05432v2
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1811.05432v2)
- **Published**: 2018-11-13 17:44:24+00:00
- **Updated**: 2019-03-01 17:39:36+00:00
- **Authors**: Dequan Wang, Coline Devin, Qi-Zhi Cai, Fisher Yu, Trevor Darrell
- **Comment**: Accepted at ICRA 2019
- **Journal**: None
- **Summary**: While learning visuomotor skills in an end-to-end manner is appealing, deep neural networks are often uninterpretable and fail in surprising ways. For robotics tasks, such as autonomous driving, models that explicitly represent objects may be more robust to new scenes and provide intuitive visualizations. We describe a taxonomy of "object-centric" models which leverage both object instances and end-to-end learning. In the Grand Theft Auto V simulator, we show that object-centric models outperform object-agnostic methods in scenes with other vehicles and pedestrians, even with an imperfect detector. We also demonstrate that our architectures perform well on real-world environments by evaluating on the Berkeley DeepDrive Video dataset, where an object-centric model outperforms object-agnostic models in the low-data regimes.



### Interactive dimensionality reduction using similarity projections
- **Arxiv ID**: http://arxiv.org/abs/1811.05531v1
- **DOI**: 10.1016/j.knosys.2018.11.015
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.05531v1)
- **Published**: 2018-11-13 21:21:15+00:00
- **Updated**: 2018-11-13 21:21:15+00:00
- **Authors**: Dimitris Spathis, Nikolaos Passalis, Anastasios Tefas
- **Comment**: Accepted at Knowledge-Based Systems
- **Journal**: None
- **Summary**: Recent advances in machine learning allow us to analyze and describe the content of high-dimensional data like text, audio, images or other signals. In order to visualize that data in 2D or 3D, usually Dimensionality Reduction (DR) techniques are employed. Most of these techniques, e.g., PCA or t-SNE, produce static projections without taking into account corrections from humans or other data exploration scenarios. In this work, we propose the interactive Similarity Projection (iSP), a novel interactive DR framework based on similarity embeddings, where we form a differentiable objective based on the user interactions and perform learning using gradient descent, with an end-to-end trainable architecture. Two interaction scenarios are evaluated. First, a common methodology in multidimensional projection is to project a subset of data, arrange them in classes or clusters, and project the rest unseen dataset based on that manipulation, in a kind of semi-supervised interpolation. We report results that outperform competitive baselines in a wide range of metrics and datasets. Second, we explore the scenario of manipulating some classes, while enriching the optimization with high-dimensional neighbor information. Apart from improving classification precision and clustering on images and text documents, the new emerging structure of the projection unveils semantic manifolds. For example, on the Head Pose dataset, by just dragging the faces looking far left to the left and those looking far right to the right, all faces are re-arranged on a continuum even on the vertical axis (face up and down). This end-to-end framework can be used for fast, visual semi-supervised learning, manifold exploration, interactive domain adaptation of neural embeddings and transfer learning.



### Automated Pain Detection from Facial Expressions using FACS: A Review
- **Arxiv ID**: http://arxiv.org/abs/1811.07988v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.07988v1)
- **Published**: 2018-11-13 22:59:24+00:00
- **Updated**: 2018-11-13 22:59:24+00:00
- **Authors**: Zhanli Chen, Rashid Ansari, Diana Wilkie
- **Comment**: None
- **Journal**: None
- **Summary**: Facial pain expression is an important modality for assessing pain, especially when the patient's verbal ability to communicate is impaired. The facial muscle-based action units (AUs), which are defined by the Facial Action Coding System (FACS), have been widely studied and are highly reliable as a method for detecting facial expressions (FE) including valid detection of pain. Unfortunately, FACS coding by humans is a very time-consuming task that makes its clinical use prohibitive. Significant progress on automated facial expression recognition (AFER) has led to its numerous successful applications in FACS-based affective computing problems. However, only a handful of studies have been reported on automated pain detection (APD), and its application in clinical settings is still far from a reality. In this paper, we review the progress in research that has contributed to automated pain detection, with focus on 1) the framework-level similarity between spontaneous AFER and APD problems; 2) the evolution of system design including the recent development of deep learning methods; 3) the strategies and considerations in developing a FACS-based pain detection framework from existing research; and 4) introduction of the most relevant databases that are available for AFER and APD studies. We attempt to present key considerations in extending a general AFER framework to an APD framework in clinical settings. In addition, the performance metrics are also highlighted in evaluating an AFER or an APD system.



