# Arxiv Papers in cs.CV on 2018-11-22
### Polarity Loss for Zero-shot Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1811.08982v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08982v3)
- **Published**: 2018-11-22 01:01:19+00:00
- **Updated**: 2020-04-02 05:53:10+00:00
- **Authors**: Shafin Rahman, Salman Khan, Nick Barnes
- **Comment**: None
- **Journal**: None
- **Summary**: Conventional object detection models require large amounts of training data. In comparison, humans can recognize previously unseen objects by merely knowing their semantic description. To mimic similar behaviour, zero-shot object detection aims to recognize and localize 'unseen' object instances by using only their semantic information. The model is first trained to learn the relationships between visual and semantic domains for seen objects, later transferring the acquired knowledge to totally unseen objects. This setting gives rise to the need for correct alignment between visual and semantic concepts, so that the unseen objects can be identified using only their semantic attributes. In this paper, we propose a novel loss function called 'Polarity loss', that promotes correct visual-semantic alignment for an improved zero-shot object detection. On one hand, it refines the noisy semantic embeddings via metric learning on a 'Semantic vocabulary' of related concepts to establish a better synergy between visual and semantic domains. On the other hand, it explicitly maximizes the gap between positive and negative predictions to achieve better discrimination between seen, unseen and background objects. Our approach is inspired by embodiment theories in cognitive science, that claim human semantic understanding to be grounded in past experiences (seen objects), related linguistic concepts (word vocabulary) and visual perception (seen/unseen object images). We conduct extensive evaluations on MS-COCO and Pascal VOC datasets, showing significant improvements over state of the art.



### Supervised Fitting of Geometric Primitives to 3D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1811.08988v4
- **DOI**: 10.1109/CVPR.2019.00276
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08988v4)
- **Published**: 2018-11-22 01:45:54+00:00
- **Updated**: 2020-01-28 17:59:19+00:00
- **Authors**: Lingxiao Li, Minhyuk Sung, Anastasia Dubrovina, Li Yi, Leonidas Guibas
- **Comment**: CVPR 2019 (Oral)
- **Journal**: None
- **Summary**: Fitting geometric primitives to 3D point cloud data bridges a gap between low-level digitized 3D data and high-level structural information on the underlying 3D shapes. As such, it enables many downstream applications in 3D data processing. For a long time, RANSAC-based methods have been the gold standard for such primitive fitting problems, but they require careful per-input parameter tuning and thus do not scale well for large datasets with diverse shapes. In this work, we introduce Supervised Primitive Fitting Network (SPFN), an end-to-end neural network that can robustly detect a varying number of primitives at different scales without any user control. The network is supervised using ground truth primitive surfaces and primitive membership for the input points. Instead of directly predicting the primitives, our architecture first predicts per-point properties and then uses a differential model estimation module to compute the primitive type and parameters. We evaluate our approach on a novel benchmark of ANSI 3D mechanical component models and demonstrate a significant improvement over both the state-of-the-art RANSAC-based methods and the direct neural prediction.



### Multi-View Inpainting for RGB-D Sequence
- **Arxiv ID**: http://arxiv.org/abs/1811.09012v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.09012v1)
- **Published**: 2018-11-22 03:57:32+00:00
- **Updated**: 2018-11-22 03:57:32+00:00
- **Authors**: Feiran Li, Gustavo Alfonso Garcia Ricardez, Jun Takamatsu, Tsukasa Ogasawara
- **Comment**: 10 pages
- **Journal**: 3DV (2018) 464--47
- **Summary**: In this work we propose a novel approach to remove undesired objects from RGB-D sequences captured with freely moving cameras, which enables static 3D reconstruction. Our method jointly uses existing information from multiple frames as well as generates new one via inpainting techniques. We use balanced rules to select source frames; local homography based image warping method for alignment and Markov random field (MRF) based approach for combining existing information. For the left holes, we employ exemplar based multi-view inpainting method to deal with the color image and coherently use it as guidance to complete the depth correspondence. Experiments show that our approach is qualified for removing the undesired objects and inpainting the holes.



### Joint Face Hallucination and Deblurring via Structure Generation and Detail Enhancement
- **Arxiv ID**: http://arxiv.org/abs/1811.09019v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.09019v1)
- **Published**: 2018-11-22 04:33:29+00:00
- **Updated**: 2018-11-22 04:33:29+00:00
- **Authors**: Yibing Song, Jiawei Zhang, Lijun Gong, Shengfeng He, Linchao Bao, Jinshan Pan, Qingxiong Yang, Ming-Hsuan Yang
- **Comment**: In IJCV 2018
- **Journal**: None
- **Summary**: We address the problem of restoring a high-resolution face image from a blurry low-resolution input. This problem is difficult as super-resolution and deblurring need to be tackled simultaneously. Moreover, existing algorithms cannot handle face images well as low-resolution face images do not have much texture which is especially critical for deblurring. In this paper, we propose an effective algorithm by utilizing the domain-specific knowledge of human faces to recover high-quality faces. We first propose a facial component guided deep Convolutional Neural Network (CNN) to restore a coarse face image, which is denoted as the base image where the facial component is automatically generated from the input face image. However, the CNN based method cannot handle image details well. We further develop a novel exemplar-based detail enhancement algorithm via facial component matching. Extensive experiments show that the proposed method outperforms the state-of-the-art algorithms both quantitatively and qualitatively.



### Task-generalizable Adversarial Attack based on Perceptual Metric
- **Arxiv ID**: http://arxiv.org/abs/1811.09020v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.09020v3)
- **Published**: 2018-11-22 04:35:26+00:00
- **Updated**: 2019-03-26 06:09:30+00:00
- **Authors**: Muzammal Naseer, Salman H. Khan, Shafin Rahman, Fatih Porikli
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) can be easily fooled by adding human imperceptible perturbations to the images. These perturbed images are known as `adversarial examples' and pose a serious threat to security and safety critical systems. A litmus test for the strength of adversarial examples is their transferability across different DNN models in a black box setting (i.e. when the target model's architecture and parameters are not known to attacker). Current attack algorithms that seek to enhance adversarial transferability work on the decision level i.e. generate perturbations that alter the network decisions. This leads to two key limitations: (a) An attack is dependent on the task-specific loss function (e.g. softmax cross-entropy for object recognition) and therefore does not generalize beyond its original task. (b) The adversarial examples are specific to the network architecture and demonstrate poor transferability to other network architectures. We propose a novel approach to create adversarial examples that can broadly fool different networks on multiple tasks. Our approach is based on the following intuition: "Perpetual metrics based on neural network features are highly generalizable and show excellent performance in measuring and stabilizing input distortions. Therefore an ideal attack that creates maximum distortions in the network feature space should realize highly transferable examples". We report extensive experiments to show how adversarial examples generalize across multiple networks for classification, object detection and segmentation tasks.



### Three-dimensional Optical Coherence Tomography Image Denoising through Multi-input Fully-Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1811.09022v2
- **DOI**: 10.1016/j.compbiomed.2019.01.010
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.09022v2)
- **Published**: 2018-11-22 04:40:21+00:00
- **Updated**: 2019-01-26 10:57:29+00:00
- **Authors**: Ashkan Abbasi, Amirhassan Monadjemi, Leyuan Fang, Hossein Rabbani, Yi Zhang
- **Comment**: This is the last version of our paper published in Computers in
  Biology and Medicine, Elsevier
- **Journal**: None
- **Summary**: In recent years, there has been a growing interest in applying convolutional neural networks (CNNs) to low-level vision tasks such as denoising and super-resolution. Due to the coherent nature of the image formation process, optical coherence tomography (OCT) images are inevitably affected by noise. This paper proposes a new method named the multi-input fully-convolutional networks (MIFCN) for denoising of OCT images. In contrast to recently proposed natural image denoising CNNs, the proposed architecture allows the exploitation of high degrees of correlation and complementary information among neighboring OCT images through pixel by pixel fusion of multiple FCNs. The parameters of the proposed multi-input architecture are learned by considering the consistency between the overall output and the contribution of each input image. The proposed MIFCN method is compared with the state-of-the-art denoising methods adopted on OCT images of normal and age-related macular degeneration eyes in a quantitative and qualitative manner.



### Structured Binary Neural Networks for Accurate Image Classification and Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1811.10413v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10413v2)
- **Published**: 2018-11-22 05:24:17+00:00
- **Updated**: 2018-11-27 03:32:02+00:00
- **Authors**: Bohan Zhuang, Chunhua Shen, Mingkui Tan, Lingqiao Liu, Ian Reid
- **Comment**: arXiv admin note: text overlap with arXiv:1808.02631
- **Journal**: None
- **Summary**: In this paper, we propose to train convolutional neural networks (CNNs) with both binarized weights and activations, leading to quantized models specifically} for mobile devices with limited power capacity and computation resources. Previous works on quantizing CNNs seek to approximate the floating-point information using a set of discrete values, which we call value approximation, but typically assume the same architecture as the full-precision networks. In this paper, however, we take a novel 'structure approximation' view for quantization---it is very likely that a different architecture may be better for best performance. In particular, we propose a `network decomposition' strategy, named \textbf{Group-Net}, in which we divide the network into groups. In this way, each full-precision group can be effectively reconstructed by aggregating a set of homogeneous binary branches.   In addition, we learn effective connections among groups to improve the representational capability. Moreover, the proposed Group-Net shows strong generalization to other tasks. For instance, we extend Group-Net for highly accurate semantic segmentation by embedding rich context into the binary structure.   Experiments on both classification and semantic segmentation tasks demonstrate the superior performance of the proposed methods over various popular architectures. In particular, we outperform the previous best binary neural networks in terms of accuracy and major computation savings.



### Data Augmentation using Random Image Cropping and Patching for Deep CNNs
- **Arxiv ID**: http://arxiv.org/abs/1811.09030v2
- **DOI**: 10.1109/TCSVT.2019.2935128
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.09030v2)
- **Published**: 2018-11-22 06:07:40+00:00
- **Updated**: 2019-08-27 14:21:32+00:00
- **Authors**: Ryo Takahashi, Takashi Matsubara, Kuniaki Uehara
- **Comment**: accepted version, 16 pages
- **Journal**: IEEE Transactions on Circuits and Systems for Video Technology,
  2019
- **Summary**: Deep convolutional neural networks (CNNs) have achieved remarkable results in image processing tasks. However, their high expression ability risks overfitting. Consequently, data augmentation techniques have been proposed to prevent overfitting while enriching datasets. Recent CNN architectures with more parameters are rendering traditional data augmentation techniques insufficient. In this study, we propose a new data augmentation technique called random image cropping and patching (RICAP) which randomly crops four images and patches them to create a new training image. Moreover, RICAP mixes the class labels of the four images, resulting in an advantage similar to label smoothing. We evaluated RICAP with current state-of-the-art CNNs (e.g., the shake-shake regularization model) by comparison with competitive data augmentation techniques such as cutout and mixup. RICAP achieves a new state-of-the-art test error of $2.19\%$ on CIFAR-10. We also confirmed that deep CNNs with RICAP achieve better results on classification tasks using CIFAR-100 and ImageNet and an image-caption retrieval task using Microsoft COCO.



### KekuleScope: prediction of cancer cell line sensitivity and compound potency using convolutional neural networks trained on compound images
- **Arxiv ID**: http://arxiv.org/abs/1811.09036v2
- **DOI**: 10.1186/s13321-019-0364-5
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.09036v2)
- **Published**: 2018-11-22 06:45:22+00:00
- **Updated**: 2019-06-22 10:50:20+00:00
- **Authors**: Isidro Cortes Ciriano, Andreas Bender
- **Comment**: None
- **Journal**: None
- **Summary**: The application of convolutional neural networks (ConvNets) to harness high-content screening images or 2D compound representations is gaining increasing attention in drug discovery. However, existing applications often require large data sets for training, or sophisticated pretraining schemes. Here, we show using 33 IC50 data sets from ChEMBL 23 that the in vitro activity of compounds on cancer cell lines and protein targets can be accurately predicted on a continuous scale from their Kekule structure representations alone by extending existing architectures, which were pretrained on unrelated image data sets. We show that the predictive power of the generated models is comparable to that of Random Forest (RF) models and fully-connected Deep Neural Networks trained on circular (Morgan) fingerprints. Notably, including additional fully-connected layers further increases the predictive power of the ConvNets by up to 10%. Analysis of the predictions generated by RF models and ConvNets shows that by simply averaging the output of the RF models and ConvNets we obtain significantly lower errors in prediction for multiple data sets, although the effect size is small, than those obtained with either model alone, indicating that the features extracted by the convolutional layers of the ConvNets provide complementary predictive signal to Morgan fingerprints. Lastly, we show that multi-task ConvNets trained on compound images permit to model COX isoform selectivity on a continuous scale with errors in prediction comparable to the uncertainty of the data. Overall, in this work we present a set of ConvNet architectures for the prediction of compound activity from their Kekule structure representations with state-of-the-art performance, that require no generation of compound descriptors or use of sophisticated image processing techniques.



### Super Diffusion for Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1811.09038v1
- **DOI**: 10.1109/TIP.2019.2954209
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.09038v1)
- **Published**: 2018-11-22 06:50:28+00:00
- **Updated**: 2018-11-22 06:50:28+00:00
- **Authors**: Peng Jiang, Zhiyi Pan, Nuno Vasconcelos, Baoquan Chen, Jingliang Peng
- **Comment**: None
- **Journal**: IEEE Transactions on Image Processing, 11/25/2019
- **Summary**: One major branch of saliency object detection methods is diffusion-based which construct a graph model on a given image and diffuse seed saliency values to the whole graph by a diffusion matrix. While their performance is sensitive to specific feature spaces and scales used for the diffusion matrix definition, little work has been published to systematically promote the robustness and accuracy of salient object detection under the generic mechanism of diffusion. In this work, we firstly present a novel view of the working mechanism of the diffusion process based on mathematical analysis, which reveals that the diffusion process is actually computing the similarity of nodes with respect to the seeds based on diffusion maps. Following this analysis, we propose super diffusion, a novel inclusive learning-based framework for salient object detection, which makes the optimum and robust performance by integrating a large pool of feature spaces, scales and even features originally computed for non-diffusion-based salient object detection. A closed-form solution of the optimal parameters for the integration is determined through supervised learning. At the local level, we propose to promote each individual diffusion before the integration. Our mathematical analysis reveals the close relationship between saliency diffusion and spectral clustering. Based on this, we propose to re-synthesize each individual diffusion matrix from the most discriminative eigenvectors and the constant eigenvector (for saliency normalization). The proposed framework is implemented and experimented on prevalently used benchmark datasets, consistently leading to state-of-the-art performance.



### Detecting Adversarial Perturbations Through Spatial Behavior in Activation Spaces
- **Arxiv ID**: http://arxiv.org/abs/1811.09043v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1811.09043v2)
- **Published**: 2018-11-22 07:17:32+00:00
- **Updated**: 2018-12-04 10:33:27+00:00
- **Authors**: Ziv Katzir, Yuval Elovici
- **Comment**: None
- **Journal**: None
- **Summary**: Neural network based classifiers are still prone to manipulation through adversarial perturbations. State of the art attacks can overcome most of the defense or detection mechanisms suggested so far, and adversaries have the upper hand in this arms race. Adversarial examples are designed to resemble the normal input from which they were constructed, while triggering an incorrect classification. This basic design goal leads to a characteristic spatial behavior within the context of Activation Spaces, a term coined by the authors to refer to the hyperspaces formed by the activation values of the network's layers. Within the output of the first layers of the network, an adversarial example is likely to resemble normal instances of the source class, while in the final layers such examples will diverge towards the adversary's target class. The steps below enable us to leverage this inherent shift from one class to another in order to form a novel adversarial example detector. We construct Euclidian spaces out of the activation values of each of the deep neural network layers. Then, we induce a set of k-nearest neighbor classifiers (k-NN), one per activation space of each neural network layer, using the non-adversarial examples. We leverage those classifiers to produce a sequence of class labels for each nonperturbed input sample and estimate the a priori probability for a class label change between one activation space and another. During the detection phase we compute a sequence of classification labels for each input using the trained classifiers. We then estimate the likelihood of those classification sequences and show that adversarial sequences are far less likely than normal ones. We evaluated our detection method against the state of the art C&W attack method, using two image classification datasets (MNIST, CIFAR-10) reaching an AUC 0f 0.95 for the CIFAR-10 dataset.



### Mask R-CNN with Pyramid Attention Network for Scene Text Detection
- **Arxiv ID**: http://arxiv.org/abs/1811.09058v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.09058v1)
- **Published**: 2018-11-22 08:17:40+00:00
- **Updated**: 2018-11-22 08:17:40+00:00
- **Authors**: Zhida Huang, Zhuoyao Zhong, Lei Sun, Qiang Huo
- **Comment**: Accepted by WACV 2019
- **Journal**: None
- **Summary**: In this paper, we present a new Mask R-CNN based text detection approach which can robustly detect multi-oriented and curved text from natural scene images in a unified manner. To enhance the feature representation ability of Mask R-CNN for text detection tasks, we propose to use the Pyramid Attention Network (PAN) as a new backbone network of Mask R-CNN. Experiments demonstrate that PAN can suppress false alarms caused by text-like backgrounds more effectively. Our proposed approach has achieved superior performance on both multi-oriented (ICDAR-2015, ICDAR-2017 MLT) and curved (SCUT-CTW1500) text detection benchmark tasks by only using single-scale and single-model testing.



### Response monitoring of breast cancer on DCE-MRI using convolutional neural network-generated seed points and constrained volume growing
- **Arxiv ID**: http://arxiv.org/abs/1811.09063v1
- **DOI**: 10.1117/12.2508358
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.09063v1)
- **Published**: 2018-11-22 09:02:41+00:00
- **Updated**: 2018-11-22 09:02:41+00:00
- **Authors**: Bas H. M. van der Velden, Bob D. de Vos, Claudette E. Loo, Hugo J. Kuijf, Ivana Isgum, Kenneth G. A. Gilhuijs
- **Comment**: This work has been accepted for SPIE Medical Imaging 2019,
  Computer-Aided Diagnosis conference, Paper 10950-12
- **Journal**: Medical Imaging 2019: Computer-Aided Diagnosis (Vol. 10950, p.
  109500D). International Society for Optics and Photonics
- **Summary**: Response of breast cancer to neoadjuvant chemotherapy (NAC) can be monitored using the change in visible tumor on magnetic resonance imaging (MRI). In our current workflow, seed points are manually placed in areas of enhancement likely to contain cancer. A constrained volume growing method uses these manually placed seed points as input and generates a tumor segmentation. This method is rigorously validated using complete pathological embedding. In this study, we propose to exploit deep learning for fast and automatic seed point detection, replacing manual seed point placement in our existing and well-validated workflow. The seed point generator was developed in early breast cancer patients with pathology-proven segmentations (N=100), operated shortly after MRI. It consisted of an ensemble of three independently trained fully convolutional dilated neural networks that classified breast voxels as tumor or non-tumor. Subsequently, local maxima were used as seed points for volume growing in patients receiving NAC (N=10). The percentage of tumor volume change was evaluated against semi-automatic segmentations. The primary cancer was localized in 95% of the tumors at the cost of 0.9 false positive per patient. False positives included focally enhancing regions of unknown origin and parts of the intramammary blood vessels. Volume growing from the seed points showed a median tumor volume decrease of 70% (interquartile range: 50%-77%), comparable to the semi-automatic segmentations (median: 70%, interquartile range 23%-76%). To conclude, a fast and automatic seed point generator was developed, fully automating a well-validated semi-automatic workflow for response monitoring of breast cancer to neoadjuvant chemotherapy.



### Feature-based groupwise registration of historical aerial images to present-day ortho-photo maps
- **Arxiv ID**: http://arxiv.org/abs/1811.09081v1
- **DOI**: 10.1016/j.patcog.2019.01.024
- **Categories**: **cs.CV**, eess.IV, 68U10 (Primary), 65K10 (Secondary)
- **Links**: [PDF](http://arxiv.org/pdf/1811.09081v1)
- **Published**: 2018-11-22 10:04:58+00:00
- **Updated**: 2018-11-22 10:04:58+00:00
- **Authors**: Sebastian Zambanini
- **Comment**: Under review at Elsevier Pattern Recognition
- **Journal**: None
- **Summary**: In this paper, we address the registration of historical WWII images to present-day ortho-photo maps for the purpose of geolocalization. Due to the challenging nature of this problem, we propose to register the images jointly as a group rather than in a step-by-step manner. To this end, we exploit Hough Voting spaces as pairwise registration estimators and show how they can be integrated into a probabilistic groupwise registration framework that can be efficiently optimized. The feature-based nature of our registration framework allows to register images with a-priori unknown translational and rotational relations, and is also able to handle scale changes of up to 30% in our test data due to a final geometrically guided matching step. The superiority of the proposed method over existing pairwise and groupwise registration methods is demonstrated on eight highly challenging sets of historical images with corresponding ortho-photo maps.



### Multi-Task Generative Adversarial Network for Handling Imbalanced Clinical Data
- **Arxiv ID**: http://arxiv.org/abs/1811.10419v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10419v1)
- **Published**: 2018-11-22 10:19:22+00:00
- **Updated**: 2018-11-22 10:19:22+00:00
- **Authors**: Mina Rezaei, Haojin Yang, Christoph Meinel
- **Comment**: Machine Learning for Health (ML4H) Workshop at NeurIPS 2018
  arXiv:1811.07216. arXiv admin note: text overlap with arXiv:1810.03871
- **Journal**: None
- **Summary**: We propose a new generative adversarial architecture to mitigate imbalance data problem for the task of medical image semantic segmentation where the majority of pixels belong to a healthy region and few belong to lesion or non-health region. A model trained with imbalanced data tends to bias towards healthy data which is not desired in clinical applications. We design a new conditional GAN with two components: a generative model and a discriminative model to mitigate imbalanced data problem through selective weighted loss. While the generator is trained on sequential magnetic resonance images (MRI) to learn semantic segmentation and disease classification, the discriminator classifies whether a generated output is real or fake. The proposed architecture achieved state-of-the-art results on ACDC-2017 for cardiac segmentation and diseases classification. We have achieved competitive results on BraTS-2017 for brain tumor segmentation and brain diseases classification.



### Driver Behavior Recognition via Interwoven Deep Convolutional Neural Nets with Multi-stream Inputs
- **Arxiv ID**: http://arxiv.org/abs/1811.09128v2
- **DOI**: 10.1109/ACCESS.2020.3032344
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.09128v2)
- **Published**: 2018-11-22 12:05:23+00:00
- **Updated**: 2021-02-21 22:28:04+00:00
- **Authors**: Chaoyun Zhang, Rui Li, Woojin Kim, Daesub Yoon, Paul Patras
- **Comment**: 13 pages, 15 figures
- **Journal**: IEEE Access, vol. 8, pp. 191138-191151, 2020
- **Summary**: Understanding driver activity is vital for in-vehicle systems that aim to reduce the incidence of car accidents rooted in cognitive distraction. Automating real-time behavior recognition while ensuring actions classification with high accuracy is however challenging, given the multitude of circumstances surrounding drivers, the unique traits of individuals, and the computational constraints imposed by in-vehicle embedded platforms. Prior work fails to jointly meet these runtime/accuracy requirements and mostly rely on a single sensing modality, which in turn can be a single point of failure. In this paper, we harness the exceptional feature extraction abilities of deep learning and propose a dedicated Interwoven Deep Convolutional Neural Network (InterCNN) architecture to tackle the problem of accurate classification of driver behaviors in real-time. The proposed solution exploits information from multi-stream inputs, i.e., in-vehicle cameras with different fields of view and optical flows computed based on recorded images, and merges through multiple fusion layers abstract features that it extracts. This builds a tight ensembling system, which significantly improves the robustness of the model. In addition, we introduce a temporal voting scheme based on historical inference instances, to enhance the classification accuracy. Experiments conducted with a dataset that we collect in a mock-up car environment demonstrate that the proposed InterCNN with MobileNet convolutional blocks can classify 9 different behaviors with 73.97% accuracy, and 5 'aggregated' behaviors with 81.66% accuracy. We further show that our architecture is highly computationally efficient, as it performs inferences within 15ms, which satisfies the real-time constraints of intelligent cars. Nevertheless, our InterCNN is robust to lossy input, as the classification remains accurate when two input streams are occluded.



### Integral Geometric Dual Distributions of Multilinear Models
- **Arxiv ID**: http://arxiv.org/abs/1812.00882v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.00882v1)
- **Published**: 2018-11-22 12:12:51+00:00
- **Updated**: 2018-11-22 12:12:51+00:00
- **Authors**: Sami Sebastian Brandt
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an integral geometric approach for computing dual distributions for the parameter distributions of multilinear models. The dual distributions can be computed from, for example, the parameter distributions of conics, multiple view tensors, homographies, or as simple entities as points, lines, and planes. The dual distributions have analytical forms that follow from the asymptotic normality property of the maximum likelihood estimator and an application of integral transforms, fundamentally the generalised Radon transforms, on the probability density of the parameters. The approach allows us, for instance, to look at the uncertainty distributions in feature distributions, which are essentially tied to the distribution of training data, and helps us to derive conditional distributions for interesting variables and characterise confidence intervals of the estimates.



### BRDF Estimation of Complex Materials with Nested Learning
- **Arxiv ID**: http://arxiv.org/abs/1811.09131v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.09131v1)
- **Published**: 2018-11-22 12:15:30+00:00
- **Updated**: 2018-11-22 12:15:30+00:00
- **Authors**: Raquel Vidaurre, Dan Casas, Elena Garces, Jorge Lopez-Moreno
- **Comment**: Accepted to IEEE Winter Conference on Applications of Computer Vision
  2019 (WACV 2019)
- **Journal**: None
- **Summary**: The estimation of the optical properties of a material from RGB-images is an important but extremely ill-posed problem in Computer Graphics. While recent works have successfully approached this problem even from just a single photograph, significant simplifications of the material model are assumed, limiting the usability of such methods. The detection of complex material properties such as anisotropy or Fresnel effect remains an unsolved challenge. We propose a novel method that predicts the model parameters of an artist-friendly, physically-based BRDF, from only two low-resolution shots of the material. Thanks to a novel combination of deep neural networks in a nested architecture, we are able to handle the ambiguities given by the non-orthogonality and non-convexity of the parameter space. To train the network, we generate a novel dataset of physically-based synthetic images. We prove that our model can recover new properties like anisotropy, index of refraction and a second reflectance color, for materials that have tinted specular reflections or whose albedo changes at glancing angles.



### Uncalibrated Non-Rigid Factorisation by Independent Subspace Analysis
- **Arxiv ID**: http://arxiv.org/abs/1811.09132v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.09132v1)
- **Published**: 2018-11-22 12:17:21+00:00
- **Updated**: 2018-11-22 12:17:21+00:00
- **Authors**: Sami Sebastian Brandt, Hanno Ackermann, Stella Grasshof
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a general, prior-free approach for the uncalibrated non-rigid structure-from-motion problem for modelling and analysis of non-rigid objects such as human faces. The word general refers to an approach that recovers the non-rigid affine structure and motion from 2D point correspondences by assuming that (1) the non-rigid shapes are generated by a linear combination of rigid 3D basis shapes, (2) that the non-rigid shapes are affine in nature, i.e., they can be modelled as deviations from the mean, rigid shape, (3) and that the basis shapes are statistically independent. In contrast to the majority of existing works, no prior information is assumed for the structure and motion apart from the assumption the that underlying basis shapes are statistically independent. The independent 3D shape bases are recovered by independent subspace analysis (ISA). Likewise, in contrast to the most previous approaches, no calibration information is assumed for affine cameras; the reconstruction is solved up to a global affine ambiguity that makes our approach simple but efficient. In the experiments, we evaluated the method with several standard data sets including a real face expression data set of 7200 faces with 2D point correspondences and unknown 3D structure and motion for which we obtained promising results.



### IEGAN: Multi-purpose Perceptual Quality Image Enhancement Using Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/1811.09134v1
- **DOI**: 10.1109/WACV.2019.00070
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.09134v1)
- **Published**: 2018-11-22 12:24:42+00:00
- **Updated**: 2018-11-22 12:24:42+00:00
- **Authors**: Soumya Shubhra Ghosh, Yang Hua, Sankha Subhra Mukherjee, Neil Robertson
- **Comment**: Accepted at IEEE WACV 2019
- **Journal**: None
- **Summary**: Despite the breakthroughs in quality of image enhancement, an end-to-end solution for simultaneous recovery of the finer texture details and sharpness for degraded images with low resolution is still unsolved. Some existing approaches focus on minimizing the pixel-wise reconstruction error which results in a high peak signal-to-noise ratio. The enhanced images fail to provide high-frequency details and are perceptually unsatisfying, i.e., they fail to match the quality expected in a photo-realistic image. In this paper, we present Image Enhancement Generative Adversarial Network (IEGAN), a versatile framework capable of inferring photo-realistic natural images for both artifact removal and super-resolution simultaneously. Moreover, we propose a new loss function consisting of a combination of reconstruction loss, feature loss and an edge loss counterpart. The feature loss helps to push the output image to the natural image manifold and the edge loss preserves the sharpness of the output image. The reconstruction loss provides low-level semantic information to the generator regarding the quality of the generated images compared to the original. Our approach has been experimentally proven to recover photo-realistic textures from heavily compressed low-resolution images on public benchmarks and our proposed high-resolution World100 dataset.



### MGANet: A Robust Model for Quality Enhancement of Compressed Video
- **Arxiv ID**: http://arxiv.org/abs/1811.09150v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.09150v4)
- **Published**: 2018-11-22 12:58:44+00:00
- **Updated**: 2019-01-15 12:42:36+00:00
- **Authors**: Xiandong Meng, Xuan Deng, Shuyuan Zhu, Shuaicheng Liu, Chuan Wang, Chen Chen, Bing Zeng
- **Comment**: 12 pages, 12 figures
- **Journal**: None
- **Summary**: In video compression, most of the existing deep learning approaches concentrate on the visual quality of a single frame, while ignoring the useful priors as well as the temporal information of adjacent frames. In this paper, we propose a multi-frame guided attention network (MGANet) to enhance the quality of compressed videos. Our network is composed of a temporal encoder that discovers inter-frame relations, a guided encoder-decoder subnet that encodes and enhances the visual patterns of target frame, and a multi-supervised reconstruction component that aggregates information to predict details. We design a bidirectional residual convolutional LSTM unit to implicitly discover frames variations over time with respect to the target frame. Meanwhile, the guided map is proposed to guide our network to concentrate more on the block boundary. Our approach takes advantage of intra-frame prior information and inter-frame information to improve the quality of compressed video. Experimental results show the robustness and superior performance of the proposed method.Code is available at https://github.com/mengab/MGANet



### Generalized Range Moves
- **Arxiv ID**: http://arxiv.org/abs/1811.09171v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.09171v1)
- **Published**: 2018-11-22 13:42:33+00:00
- **Updated**: 2018-11-22 13:42:33+00:00
- **Authors**: Richard Hartley, Thalaiyasingam Ajanthan
- **Comment**: None
- **Journal**: None
- **Summary**: We consider move-making algorithms for energy minimization of multi-label Markov Random Fields (MRFs). Since this is not a tractable problem in general, a commonly used heuristic is to minimize over subsets of labels and variables in an iterative procedure. Such methods include {\alpha}-expansion, {\alpha}{\beta}-swap, and range-moves. In each iteration, a small subset of variables are active in the optimization, which diminishes their effectiveness, and increases the required number of iterations. In this paper, we present a method in which optimization can be carried out over all labels, and most, or all variables at once. Experiments show substantial improvement with respect to previous move-making algorithms.



### Dual Reweighted Lp-Norm Minimization for Salt-and-pepper Noise Removal
- **Arxiv ID**: http://arxiv.org/abs/1811.09173v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.09173v3)
- **Published**: 2018-11-22 13:50:30+00:00
- **Updated**: 2019-08-19 05:54:52+00:00
- **Authors**: Huiwen Dong, Jing Yu, Chuangbai Xiao
- **Comment**: None
- **Journal**: None
- **Summary**: The robust principal component analysis (RPCA), which aims to estimate underlying low-rank and sparse structures from the degraded observation data, has found wide applications in computer vision. It is usually replaced by the principal component pursuit (PCP) model in order to pursue the convex property, leading to the undesirable overshrink problem. In this paper, we propose a dual weighted lp-norm (DWLP) model with a more reasonable weighting rule and weaker powers, which greatly generalizes the previous work and provides a better approximation to the rank minimization problem for original matrix as well as the l0-norm minimization problem for sparse data. Moreover, an approximate closed-form solution is introduced to solve the lp-norm minimization, which has more stability in the nonconvex optimization and provides a more accurate estimation for the low-rank and sparse matrix recovery problem. We then apply the DWLP model to remove salt-and-pepper noise by exploiting the image nonlocal self-similarity. Both qualitative and quantitative experiments demonstrate that the proposed method outperforms other state-of-the-art methods. In terms of PSNR evaluation, our DWLP achieves about 7.188dB, 5.078dB, 3.854dB, 2.536dB and 0.158dB improvements over the current WSNM-RPCA under 10\% to 50\% salt-and-pepper noise with an interval 10\% respectively.



### Object-oriented Targets for Visual Navigation using Rich Semantic Representations
- **Arxiv ID**: http://arxiv.org/abs/1811.09178v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.09178v2)
- **Published**: 2018-11-22 14:07:16+00:00
- **Updated**: 2018-12-17 16:33:30+00:00
- **Authors**: Jean-Benoit Delbrouck, Stéphane Dupont
- **Comment**: Presented at NIPS workshop (ViGIL)
- **Journal**: None
- **Summary**: When searching for an object humans navigate through a scene using semantic information and spatial relationships. We look for an object using our knowledge of its attributes and relationships with other objects to infer the probable location. In this paper, we propose to tackle the visual navigation problem using rich semantic representations of the observed scene and object-oriented targets to train an agent. We show that both allows the agent to generalize to new targets and unseen scene in a short amount of training time.



### NeuroTreeNet: A New Method to Explore Horizontal Expansion Network
- **Arxiv ID**: http://arxiv.org/abs/1811.09618v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.09618v1)
- **Published**: 2018-11-22 14:16:04+00:00
- **Updated**: 2018-11-22 14:16:04+00:00
- **Authors**: Shenlong Lou, Yan Luo, Qiancong Fan, Feng Chen, Yiping Chen, Cheng Wang, Jonathan Li
- **Comment**: None
- **Journal**: None
- **Summary**: It is widely recognized that the deeper networks or networks with more feature maps have better performance. Existing studies mainly focus on extending the network depth and increasing the feature maps of networks. At the same time, horizontal expansion network (e.g. Inception Model) as an alternative way to improve network performance has not been fully investigated. Accordingly, we proposed NeuroTreeNet (NTN), as a new horizontal extension network through the combination of random forest and Inception Model. Based on the tree structure, in which each branch represents a network and the root node features are shared to child nodes, network parameters are effectively reduced. By combining all features of leaf nodes, even less feature maps achieved better performance. In addition, the relationship between tree structure and the performance of NTN was investigated in depth. Comparing to other networks (e.g. VDSR\_5) with equal magnitude parameters, our model showed preferable performance in super resolution reconstruction task.



### Self Paced Adversarial Training for Multimodal Few-shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1811.09192v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1811.09192v1)
- **Published**: 2018-11-22 14:29:45+00:00
- **Updated**: 2018-11-22 14:29:45+00:00
- **Authors**: Frederik Pahde, Oleksiy Ostapenko, Patrick Jähnichen, Tassilo Klein, Moin Nabi
- **Comment**: To appear at WACV 2019
- **Journal**: None
- **Summary**: State-of-the-art deep learning algorithms yield remarkable results in many visual recognition tasks. However, they still fail to provide satisfactory results in scarce data regimes. To a certain extent this lack of data can be compensated by multimodal information. Missing information in one modality of a single data point (e.g. an image) can be made up for in another modality (e.g. a textual description). Therefore, we design a few-shot learning task that is multimodal during training (i.e. image and text) and single-modal during test time (i.e. image). In this regard, we propose a self-paced class-discriminative generative adversarial network incorporating multimodality in the context of few-shot learning. The proposed approach builds upon the idea of cross-modal data generation in order to alleviate the data sparsity problem. We improve few-shot learning accuracies on the finegrained CUB and Oxford-102 datasets.



### Copy the Old or Paint Anew? An Adversarial Framework for (non-) Parametric Image Stylization
- **Arxiv ID**: http://arxiv.org/abs/1811.09236v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.09236v1)
- **Published**: 2018-11-22 16:54:12+00:00
- **Updated**: 2018-11-22 16:54:12+00:00
- **Authors**: Nikolay Jetchev, Urs Bergmann, Gokhan Yildirim
- **Comment**: Accepted at the NIPS 2018 workshop on Machine Learning for Creativity
  and Design
- **Journal**: None
- **Summary**: Parametric generative deep models are state-of-the-art for photo and non-photo realistic image stylization. However, learning complicated image representations requires compute-intense models parametrized by a huge number of weights, which in turn requires large datasets to make learning successful. Non-parametric exemplar-based generation is a technique that works well to reproduce style from small datasets, but is also compute-intensive. These aspects are a drawback for the practice of digital AI artists: typically one wants to use a small set of stylization images, and needs a fast flexible model in order to experiment with it. With this motivation, our work has these contributions: (i) a novel stylization method called Fully Adversarial Mosaics (FAMOS) that combines the strengths of both parametric and non-parametric approaches; (ii) multiple ablations and image examples that analyze the method and show its capabilities; (iii) source code that will empower artists and machine learning researchers to use and modify FAMOS.



### FAIM -- A ConvNet Method for Unsupervised 3D Medical Image Registration
- **Arxiv ID**: http://arxiv.org/abs/1811.09243v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.09243v2)
- **Published**: 2018-11-22 17:29:04+00:00
- **Updated**: 2019-06-28 20:43:34+00:00
- **Authors**: Dongyang Kuang, Tanya Schmah
- **Comment**: None
- **Journal**: None
- **Summary**: We present a new unsupervised learning algorithm, "FAIM", for 3D medical image registration. With a different architecture than the popular "U-net", the network takes a pair of full image volumes and predicts the displacement fields needed to register source to target. Compared with "U-net" based registration networks such as VoxelMorph, FAIM has fewer trainable parameters but can achieve higher registration accuracy as judged by Dice score on region labels in the Mindboggle-101 dataset. Moreover, with the proposed penalty loss on negative Jacobian determinants, FAIM produces deformations with many fewer "foldings", i.e. regions of non-invertibility where the surface folds over itself. In our experiment, we varied the strength of this penalty and investigated changes in registration accuracy and non-invertibility in terms of number of "folding" locations. We found that FAIM is able to maintain both the advantages of higher accuracy and fewer "folding" locations over VoxelMorph, over a range of hyper-parameters (with the same values used for both networks). Further, when trading off registration accuracy for better invertibility, FAIM required less sacrifice of registration accuracy. Codes for this paper will be released upon publication.



### Automatic L3 slice detection in 3D CT images using fully-convolutional networks
- **Arxiv ID**: http://arxiv.org/abs/1811.09244v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1811.09244v1)
- **Published**: 2018-11-22 17:31:18+00:00
- **Updated**: 2018-11-22 17:31:18+00:00
- **Authors**: Fahdi Kanavati, Shah Islam, Eric O. Aboagye, Andrea Rockall
- **Comment**: None
- **Journal**: None
- **Summary**: The analysis of single CT slices extracted at the third lumbar vertebra (L3) has garnered significant clinical interest in the past few years, in particular in regards to quantifying sarcopenia (muscle loss). In this paper, we propose an efficient method to automatically detect the L3 slice in 3D CT images. Our method works with images with a variety of fields of view, occlusions, and slice thicknesses. 3D CT images are first converted into 2D via Maximal Intensity Projection (MIP), reducing the dimensionality of the problem. The MIP images are then used as input to a 2D fully-convolutional network to predict the L3 slice locations in the form of 2D confidence maps. In addition we propose a variant architecture with less parameters allowing 1D confidence map prediction and slightly faster prediction time without loss of accuracy. Quantitative evaluation of our method on a dataset of 1006 3D CT images yields a median error of 1mm, similar to the inter-rater median error of 1mm obtained from two annotators, demonstrating the effectiveness of our method in efficiently and accurately detecting the L3 slice.



### Train Sparsely, Generate Densely: Memory-efficient Unsupervised Training of High-resolution Temporal GAN
- **Arxiv ID**: http://arxiv.org/abs/1811.09245v2
- **DOI**: 10.1007/s11263-020-01333-y
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.09245v2)
- **Published**: 2018-11-22 17:31:26+00:00
- **Updated**: 2020-06-01 11:47:45+00:00
- **Authors**: Masaki Saito, Shunta Saito, Masanori Koyama, Sosuke Kobayashi
- **Comment**: Accepted at International Journal of Computer Vision. The source code
  is available at https://github.com/pfnet-research/tgan2
- **Journal**: None
- **Summary**: Training of Generative Adversarial Network (GAN) on a video dataset is a challenge because of the sheer size of the dataset and the complexity of each observation. In general, the computational cost of training GAN scales exponentially with the resolution. In this study, we present a novel memory efficient method of unsupervised learning of high-resolution video dataset whose computational cost scales only linearly with the resolution. We achieve this by designing the generator model as a stack of small sub-generators and training the model in a specific way. We train each sub-generator with its own specific discriminator. At the time of the training, we introduce between each pair of consecutive sub-generators an auxiliary subsampling layer that reduces the frame-rate by a certain ratio. This procedure can allow each sub-generator to learn the distribution of the video at different levels of resolution. We also need only a few GPUs to train a highly complex generator that far outperforms the predecessor in terms of inception scores.



### Parametric Noise Injection: Trainable Randomness to Improve Deep Neural Network Robustness against Adversarial Attack
- **Arxiv ID**: http://arxiv.org/abs/1811.09310v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1811.09310v1)
- **Published**: 2018-11-22 21:10:52+00:00
- **Updated**: 2018-11-22 21:10:52+00:00
- **Authors**: Adnan Siraj Rakin, Zhezhi He, Deliang Fan
- **Comment**: None
- **Journal**: None
- **Summary**: Recent development in the field of Deep Learning have exposed the underlying vulnerability of Deep Neural Network (DNN) against adversarial examples. In image classification, an adversarial example is a carefully modified image that is visually imperceptible to the original image but can cause DNN model to misclassify it. Training the network with Gaussian noise is an effective technique to perform model regularization, thus improving model robustness against input variation. Inspired by this classical method, we explore to utilize the regularization characteristic of noise injection to improve DNN's robustness against adversarial attack. In this work, we propose Parametric-Noise-Injection (PNI) which involves trainable Gaussian noise injection at each layer on either activation or weights through solving the min-max optimization problem, embedded with adversarial training. These parameters are trained explicitly to achieve improved robustness. To the best of our knowledge, this is the first work that uses trainable noise injection to improve network robustness against adversarial attacks, rather than manually configuring the injected noise level through cross-validation. The extensive results show that our proposed PNI technique effectively improves the robustness against a variety of powerful white-box and black-box attacks such as PGD, C & W, FGSM, transferable attack and ZOO attack. Last but not the least, PNI method improves both clean- and perturbed-data accuracy in comparison to the state-of-the-art defense methods, which outperforms current unbroken PGD defense by 1.1 % and 6.8 % on clean test data and perturbed test data respectively using Resnet-20 architecture.



### MR-GAN: Manifold Regularized Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1811.10427v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.10427v1)
- **Published**: 2018-11-22 21:21:02+00:00
- **Updated**: 2018-11-22 21:21:02+00:00
- **Authors**: Qunwei Li, Bhavya Kailkhura, Rushil Anirudh, Yi Zhou, Yingbin Liang, Pramod Varshney
- **Comment**: arXiv admin note: text overlap with arXiv:1706.04156 by other authors
- **Journal**: None
- **Summary**: Despite the growing interest in generative adversarial networks (GANs), training GANs remains a challenging problem, both from a theoretical and a practical standpoint. To address this challenge, in this paper, we propose a novel way to exploit the unique geometry of the real data, especially the manifold information. More specifically, we design a method to regularize GAN training by adding an additional regularization term referred to as manifold regularizer. The manifold regularizer forces the generator to respect the unique geometry of the real data manifold and generate high quality data. Furthermore, we theoretically prove that the addition of this regularization term in any class of GANs including DCGAN and Wasserstein GAN leads to improved performance in terms of generalization, existence of equilibrium, and stability. Preliminary experiments show that the proposed manifold regularization helps in avoiding mode collapse and leads to stable training.



