# Arxiv Papers in cs.CV on 2018-11-01
### Pixel Level Data Augmentation for Semantic Image Segmentation using Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1811.00174v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.00174v4)
- **Published**: 2018-11-01 01:07:16+00:00
- **Updated**: 2019-11-26 05:49:32+00:00
- **Authors**: Shuangting Liu, Jiaqi Zhang, Yuxin Chen, Yifan Liu, Zengchang Qin, Tao Wan
- **Comment**: 5 pages
- **Journal**: None
- **Summary**: Semantic segmentation is one of the basic topics in computer vision, it aims to assign semantic labels to every pixel of an image. Unbalanced semantic label distribution could have a negative influence on segmentation accuracy. In this paper, we investigate using data augmentation approach to balance the semantic label distribution in order to improve segmentation performance. We propose using generative adversarial networks (GANs) to generate realistic images for improving the performance of semantic segmentation networks. Experimental results show that the proposed method can not only improve segmentation performance on those classes with low accuracy, but also obtain 1.3% to 2.1% increase in average segmentation accuracy. It shows that this augmentation method can boost accuracy and be easily applicable to any other segmentation models.



### A Bayesian Perspective of Convolutional Neural Networks through a Deconvolutional Generative Model
- **Arxiv ID**: http://arxiv.org/abs/1811.02657v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.02657v2)
- **Published**: 2018-11-01 01:27:37+00:00
- **Updated**: 2019-12-09 10:21:21+00:00
- **Authors**: Tan Nguyen, Nhat Ho, Ankit Patel, Anima Anandkumar, Michael I. Jordan, Richard G. Baraniuk
- **Comment**: Keywords: neural nets, generative models, semi-supervised learning,
  cross-entropy, statistical guarantees 80 pages, 7 figures, 8 tables
- **Journal**: None
- **Summary**: Inspired by the success of Convolutional Neural Networks (CNNs) for supervised prediction in images, we design the Deconvolutional Generative Model (DGM), a new probabilistic generative model whose inference calculations correspond to those in a given CNN architecture. The DGM uses a CNN to design the prior distribution in the probabilistic model. Furthermore, the DGM generates images from coarse to finer scales. It introduces a small set of latent variables at each scale, and enforces dependencies among all the latent variables via a conjugate prior distribution. This conjugate prior yields a new regularizer based on paths rendered in the generative model for training CNNs-the Rendering Path Normalization (RPN). We demonstrate that this regularizer improves generalization, both in theory and in practice. In addition, likelihood estimation in the DGM yields training losses for CNNs, and inspired by this, we design a new loss termed as the Max-Min cross entropy which outperforms the traditional cross-entropy loss for object classification. The Max-Min cross entropy suggests a new deep network architecture, namely the Max-Min network, which can learn from less labeled data while maintaining good prediction performance. Our experiments demonstrate that the DGM with the RPN and the Max-Min architecture exceeds or matches the-state-of-art on benchmarks including SVHN, CIFAR10, and CIFAR100 for semi-supervised and supervised learning tasks.



### Unauthorized AI cannot Recognize Me: Reversible Adversarial Example
- **Arxiv ID**: http://arxiv.org/abs/1811.00189v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.00189v3)
- **Published**: 2018-11-01 02:28:31+00:00
- **Updated**: 2021-10-08 17:42:59+00:00
- **Authors**: Jiayang Liu, Weiming Zhang, Kazuto Fukuchi, Youhei Akimoto, Jun Sakuma
- **Comment**: arXiv admin note: text overlap with arXiv:1806.09186
- **Journal**: None
- **Summary**: In this study, we propose a new methodology to control how user's data is recognized and used by AI via exploiting the properties of adversarial examples. For this purpose, we propose reversible adversarial example (RAE), a new type of adversarial example. A remarkable feature of RAE is that the image can be correctly recognized and used by the AI model specified by the user because the authorized AI can recover the original image from the RAE exactly by eliminating adversarial perturbation. On the other hand, other unauthorized AI models cannot recognize it correctly because it functions as an adversarial example. Moreover, RAE can be considered as one type of encryption to computer vision since reversibility guarantees the decryption. To realize RAE, we combine three technologies, adversarial example, reversible data hiding for exact recovery of adversarial perturbation, and encryption for selective control of AIs who can remove adversarial perturbation. Experimental results show that the proposed method can achieve comparable attack ability with the corresponding adversarial attack method and similar visual quality with the original image, including white-box attacks and black-box attacks.



### Cogni-Net: Cognitive Feature Learning through Deep Visual Perception
- **Arxiv ID**: http://arxiv.org/abs/1811.00201v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.00201v2)
- **Published**: 2018-11-01 03:14:18+00:00
- **Updated**: 2019-05-01 10:30:43+00:00
- **Authors**: Pranay Mukherjee, Abhirup Das, Ayan Kumar Bhunia, Partha Pratim Roy
- **Comment**: IEEE International Conference on Image Processing (ICIP), 2019
- **Journal**: None
- **Summary**: Can we ask computers to recognize what we see from brain signals alone? Our paper seeks to utilize the knowledge learnt in the visual domain by popular pre-trained vision models and use it to teach a recurrent model being trained on brain signals to learn a discriminative manifold of the human brain's cognition of different visual object categories in response to perceived visual cues. For this we make use of brain EEG signals triggered from visual stimuli like images and leverage the natural synchronization between images and their corresponding brain signals to learn a novel representation of the cognitive feature space. The concept of knowledge distillation has been used here for training the deep cognition model, CogniNet\footnote{The source code of the proposed system is publicly available at {https://www.github.com/53X/CogniNET}}, by employing a student-teacher learning technique in order to bridge the process of inter-modal knowledge transfer. The proposed novel architecture obtains state-of-the-art results, significantly surpassing other existing models. The experiments performed by us also suggest that if visual stimuli information like brain EEG signals can be gathered on a large scale, then that would help to obtain a better understanding of the largely unexplored domain of human brain cognition.



### Attention-Aware Generalized Mean Pooling for Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1811.00202v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.00202v2)
- **Published**: 2018-11-01 03:16:02+00:00
- **Updated**: 2019-01-28 06:11:13+00:00
- **Authors**: Yinzheng Gu, Chuanpeng Li, Jinbin Xie
- **Comment**: Shortened version for submission
- **Journal**: None
- **Summary**: It has been shown that image descriptors extracted by convolutional neural networks (CNNs) achieve remarkable results for retrieval problems. In this paper, we apply attention mechanism to CNN, which aims at enhancing more relevant features that correspond to important keypoints in the input image. The generated attention-aware features are then aggregated by the previous state-of-the-art generalized mean (GeM) pooling followed by normalization to produce a compact global descriptor, which can be efficiently compared to other image descriptors by the dot product. An extensive comparison of our proposed approach with state-of-the-art methods is performed on the new challenging ROxford5k and RParis6k retrieval benchmarks. Results indicate significant improvement over previous work. In particular, our attention-aware GeM (AGeM) descriptor outperforms state-of-the-art method on ROxford5k under the `Hard' evaluation protocal.



### Balanced Sparsity for Efficient DNN Inference on GPU
- **Arxiv ID**: http://arxiv.org/abs/1811.00206v4
- **DOI**: 10.1609/aaai.v33i01.33015676
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.00206v4)
- **Published**: 2018-11-01 03:30:51+00:00
- **Updated**: 2018-12-12 06:26:16+00:00
- **Authors**: Zhuliang Yao, Shijie Cao, Wencong Xiao, Chen Zhang, Lanshun Nie
- **Comment**: None
- **Journal**: None
- **Summary**: In trained deep neural networks, unstructured pruning can reduce redundant weights to lower storage cost. However, it requires the customization of hardwares to speed up practical inference. Another trend accelerates sparse model inference on general-purpose hardwares by adopting coarse-grained sparsity to prune or regularize consecutive weights for efficient computation. But this method often sacrifices model accuracy. In this paper, we propose a novel fine-grained sparsity approach, balanced sparsity, to achieve high model accuracy with commercial hardwares efficiently. Our approach adapts to high parallelism property of GPU, showing incredible potential for sparsity in the widely deployment of deep learning services. Experiment results show that balanced sparsity achieves up to 3.1x practical speedup for model inference on GPU, while retains the same high model accuracy as fine-grained sparsity.



### Tattoo Image Search at Scale: Joint Detection and Compact Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/1811.00218v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.00218v1)
- **Published**: 2018-11-01 04:20:31+00:00
- **Updated**: 2018-11-01 04:20:31+00:00
- **Authors**: Hu Han, Jie Li, Anil K. Jain, Shiguang Shan, Xilin Chen
- **Comment**: Technical Report (15 pages, 14 figures)
- **Journal**: None
- **Summary**: The explosive growth of digital images in video surveillance and social media has led to the significant need for efficient search of persons of interest in law enforcement and forensic applications. Despite tremendous progress in primary biometric traits (e.g., face and fingerprint) based person identification, a single biometric trait alone cannot meet the desired recognition accuracy in forensic scenarios. Tattoos, as one of the important soft biometric traits, have been found to be valuable for assisting in person identification. However, tattoo search in a large collection of unconstrained images remains a difficult problem, and existing tattoo search methods mainly focus on matching cropped tattoos, which is different from real application scenarios. To close the gap, we propose an efficient tattoo search approach that is able to learn tattoo detection and compact representation jointly in a single convolutional neural network (CNN) via multi-task learning. While the features in the backbone network are shared by both tattoo detection and compact representation learning, individual latent layers of each sub-network optimize the shared features toward the detection and feature learning tasks, respectively. We resolve the small batch size issue inside the joint tattoo detection and compact representation learning network via random image stitch and preceding feature buffering. We evaluate the proposed tattoo search system using multiple public-domain tattoo benchmarks, and a gallery set with about 300K distracter tattoo images compiled from these datasets and images from the Internet. In addition, we also introduce a tattoo sketch dataset containing 300 tattoos for sketch-based tattoo search. Experimental results show that the proposed approach has superior performance in tattoo detection and tattoo search at scale compared to several state-of-the-art tattoo retrieval algorithms.



### Consistent estimation of the max-flow problem: Towards unsupervised image segmentation
- **Arxiv ID**: http://arxiv.org/abs/1811.00220v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1811.00220v2)
- **Published**: 2018-11-01 04:35:01+00:00
- **Updated**: 2019-06-29 20:50:32+00:00
- **Authors**: Ashif Sikandar Iquebal, Satish Bukkapatnam
- **Comment**: None
- **Journal**: None
- **Summary**: Advances in the image-based diagnostics of complex biological and manufacturing processes have brought unsupervised image segmentation to the forefront of enabling automated, on the fly decision making. However, most existing unsupervised segmentation approaches are either computationally complex or require manual parameter selection (e.g., flow capacities in max-flow/min-cut segmentation). In this work, we present a fully unsupervised segmentation approach using a continuous max-flow formulation over the image domain while optimally estimating the flow parameters from the image characteristics. More specifically, we show that the maximum a posteriori estimate of the image labels can be formulated as a continuous max-flow problem given the flow capacities are known. The flow capacities are then iteratively obtained by employing a novel Markov random field prior over the image domain. We present theoretical results to establish the posterior consistency of the flow capacities. We compare the performance of our approach on two real-world case studies including brain tumor image segmentation and defect identification in additively manufactured components using electron microscopic images. Comparative results with several state-of-the-art supervised as well as unsupervised methods suggest that the present method performs statistically similar to the supervised methods, but results in more than 90% improvement in the Dice score when compared to the state-of-the-art unsupervised methods.



### CariGANs: Unpaired Photo-to-Caricature Translation
- **Arxiv ID**: http://arxiv.org/abs/1811.00222v2
- **DOI**: 10.1145/3272127.3275046
- **Categories**: **cs.CV**, cs.AI, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1811.00222v2)
- **Published**: 2018-11-01 04:39:20+00:00
- **Updated**: 2018-11-02 03:47:13+00:00
- **Authors**: Kaidi Cao, Jing Liao, Lu Yuan
- **Comment**: To appear at SIGGRAPH Asia 2018
- **Journal**: ACM Transactions on Graphics, Vol. 37, No. 6, Article 244.
  Publication date: November 2018
- **Summary**: Facial caricature is an art form of drawing faces in an exaggerated way to convey humor or sarcasm. In this paper, we propose the first Generative Adversarial Network (GAN) for unpaired photo-to-caricature translation, which we call "CariGANs". It explicitly models geometric exaggeration and appearance stylization using two components: CariGeoGAN, which only models the geometry-to-geometry transformation from face photos to caricatures, and CariStyGAN, which transfers the style appearance from caricatures to face photos without any geometry deformation. In this way, a difficult cross-domain translation problem is decoupled into two easier tasks. The perceptual study shows that caricatures generated by our CariGANs are closer to the hand-drawn ones, and at the same time better persevere the identity, compared to state-of-the-art methods. Moreover, our CariGANs allow users to control the shape exaggeration degree and change the color/texture style by tuning the parameters or giving an example caricature.



### A sequential guiding network with attention for image captioning
- **Arxiv ID**: http://arxiv.org/abs/1811.00228v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1811.00228v3)
- **Published**: 2018-11-01 05:03:26+00:00
- **Updated**: 2019-02-08 22:35:58+00:00
- **Authors**: Daouda Sow, Zengchang Qin, Mouhamed Niasse, Tao Wan
- **Comment**: 5 pages, 2 figures, 1 table, IEEE ICASSP 2019
- **Journal**: None
- **Summary**: The recent advances of deep learning in both computer vision (CV) and natural language processing (NLP) provide us a new way of understanding semantics, by which we can deal with more challenging tasks such as automatic description generation from natural images. In this challenge, the encoder-decoder framework has achieved promising performance when a convolutional neural network (CNN) is used as image encoder and a recurrent neural network (RNN) as decoder. In this paper, we introduce a sequential guiding network that guides the decoder during word generation. The new model is an extension of the encoder-decoder framework with attention that has an additional guiding long short-term memory (LSTM) and can be trained in an end-to-end manner by using image/descriptions pairs. We validate our approach by conducting extensive experiments on a benchmark dataset, i.e., MS COCO Captions. The proposed model achieves significant improvement comparing to the other state-of-the-art deep learning models.



### Survey on Vision-based Path Prediction
- **Arxiv ID**: http://arxiv.org/abs/1811.00233v1
- **DOI**: 10.1007/978-3-319-91131-1_4
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1811.00233v1)
- **Published**: 2018-11-01 05:12:01+00:00
- **Updated**: 2018-11-01 05:12:01+00:00
- **Authors**: Tsubasa Hirakawa, Takayoshi Yamashita, Toru Tamaki, Hironobu Fujiyoshi
- **Comment**: DAPI 2018
- **Journal**: None
- **Summary**: Path prediction is a fundamental task for estimating how pedestrians or vehicles are going to move in a scene. Because path prediction as a task of computer vision uses video as input, various information used for prediction, such as the environment surrounding the target and the internal state of the target, need to be estimated from the video in addition to predicting paths. Many prediction approaches that include understanding the environment and the internal state have been proposed. In this survey, we systematically summarize methods of path prediction that take video as input and and extract features from the video. Moreover, we introduce datasets used to evaluate path prediction methods quantitatively.



### Examining Performance of Sketch-to-Image Translation Models with Multiclass Automatically Generated Paired Training Data
- **Arxiv ID**: http://arxiv.org/abs/1811.00249v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.00249v1)
- **Published**: 2018-11-01 05:59:53+00:00
- **Updated**: 2018-11-01 05:59:53+00:00
- **Authors**: Dichao Hu
- **Comment**: 6 pages 3 figures
- **Journal**: None
- **Summary**: Image translation is a computer vision task that involves translating one representation of the scene into another. Various approaches have been proposed and achieved highly desirable results. Nevertheless, its accomplishment requires abundant paired training data which are expensive to acquire. Therefore, models for translation are usually trained on a set of paired training data which are carefully and laboriously designed. Our work is focused on learning through automatically generated paired data. We propose a method to generate fake sketches from images using an adversarial network and then pair the images with corresponding fake sketches to form large-scale multi-class paired training data for training a sketch-to-image translation model. Our model is an encoder-decoder architecture where the encoder generates fake sketches from images and the decoder performs sketch-to-image translation. Qualitative results show that the encoder can be used for generating large-scale multi-class paired data under low supervision. Our current dataset now contains 61255 image and (fake) sketch pairs from 256 different categories. These figures can be greatly increased in the future thanks to our weak reliance on manually labeled data.



### Filter Pruning via Geometric Median for Deep Convolutional Neural Networks Acceleration
- **Arxiv ID**: http://arxiv.org/abs/1811.00250v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.00250v3)
- **Published**: 2018-11-01 06:03:05+00:00
- **Updated**: 2019-07-14 10:31:03+00:00
- **Authors**: Yang He, Ping Liu, Ziwei Wang, Zhilan Hu, Yi Yang
- **Comment**: Accepted to CVPR 2019 (Oral)
- **Journal**: None
- **Summary**: Previous works utilized ''smaller-norm-less-important'' criterion to prune filters with smaller norm values in a convolutional neural network. In this paper, we analyze this norm-based criterion and point out that its effectiveness depends on two requirements that are not always met: (1) the norm deviation of the filters should be large; (2) the minimum norm of the filters should be small. To solve this problem, we propose a novel filter pruning method, namely Filter Pruning via Geometric Median (FPGM), to compress the model regardless of those two requirements. Unlike previous methods, FPGM compresses CNN models by pruning filters with redundancy, rather than those with ''relatively less'' importance. When applied to two image classification benchmarks, our method validates its usefulness and strengths. Notably, on CIFAR-10, FPGM reduces more than 52% FLOPs on ResNet-110 with even 2.69% relative accuracy improvement. Moreover, on ILSVRC-2012, FPGM reduces more than 42% FLOPs on ResNet-101 without top-5 accuracy drop, which has advanced the state-of-the-art. Code is publicly available on GitHub: https://github.com/he-y/filter-pruning-geometric-median



### Skeleton-based Activity Recognition with Local Order Preserving Match of Linear Patches
- **Arxiv ID**: http://arxiv.org/abs/1811.00256v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.00256v1)
- **Published**: 2018-11-01 06:46:33+00:00
- **Updated**: 2018-11-01 06:46:33+00:00
- **Authors**: Yaqiang Yao, Yan Liu, Huanhuan Chen
- **Comment**: 8 pages, 6 figures
- **Journal**: None
- **Summary**: Human activity recognition has drawn considerable attention recently in the field of computer vision due to the development of commodity depth cameras, by which the human activity is represented as a sequence of 3D skeleton postures. Assuming human body 3D joint locations of an activity lie on a manifold, the problem of recognizing human activity is formulated as the computation of activity manifold-manifold distance (AMMD). In this paper, we first design an efficient division method to decompose a manifold into ordered continuous maximal linear patches (CMLPs) that denote meaningful action snippets of the action sequence. Then the CMLP is represented by its position (average value of points) and the first principal component, which specify the major posture and main evolving direction of an action snippet, respectively. Finally, we compute the distance between CMLPs by taking both the posture and direction into consideration. Based on these preparations, an intuitive distance measure that preserves the local order of action snippets is proposed to compute AMMD. The performance on two benchmark datasets demonstrates the effectiveness of the proposed approach.



### Hierarchical Long Short-Term Concurrent Memory for Human Interaction Recognition
- **Arxiv ID**: http://arxiv.org/abs/1811.00270v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.00270v1)
- **Published**: 2018-11-01 07:36:28+00:00
- **Updated**: 2018-11-01 07:36:28+00:00
- **Authors**: Xiangbo Shu, Jinhui Tang, Guo-Jun Qi, Wei Liu, Jian Yang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we aim to address the problem of human interaction recognition in videos by exploring the long-term inter-related dynamics among multiple persons. Recently, Long Short-Term Memory (LSTM) has become a popular choice to model individual dynamic for single-person action recognition due to its ability of capturing the temporal motion information in a range. However, existing RNN models focus only on capturing the dynamics of human interaction by simply combining all dynamics of individuals or modeling them as a whole. Such models neglect the inter-related dynamics of how human interactions change over time. To this end, we propose a novel Hierarchical Long Short-Term Concurrent Memory (H-LSTCM) to model the long-term inter-related dynamics among a group of persons for recognizing the human interactions. Specifically, we first feed each person's static features into a Single-Person LSTM to learn the single-person dynamic. Subsequently, the outputs of all Single-Person LSTM units are fed into a novel Concurrent LSTM (Co-LSTM) unit, which mainly consists of multiple sub-memory units, a new cell gate and a new co-memory cell. In a Co-LSTM unit, each sub-memory unit stores individual motion information, while this Co-LSTM unit selectively integrates and stores inter-related motion information between multiple interacting persons from multiple sub-memory units via the cell gate and co-memory cell, respectively. Extensive experiments on four public datasets validate the effectiveness of the proposed H-LSTCM by comparing against baseline and state-of-the-art methods.



### Efficient Multi-Domain Dictionary Learning with GANs
- **Arxiv ID**: http://arxiv.org/abs/1811.00274v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.00274v1)
- **Published**: 2018-11-01 07:52:19+00:00
- **Updated**: 2018-11-01 07:52:19+00:00
- **Authors**: Cho Ying Wu, Ulrich Neumann
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose the multi-domain dictionary learning (MDDL) to make dictionary learning-based classification more robust to data representing in different domains. We use adversarial neural networks to generate data in different styles, and collect all the generated data into a miscellaneous dictionary. To tackle the dictionary learning with many samples, we compute the weighting matrix that compress the miscellaneous dictionary from multi-sample per class to single sample per class. We show that the time complexity solving the proposed MDDL with weighting matrix is the same as solving the dictionary with single sample per class. Moreover, since the weighting matrix could help the solver rely more on the training data, which possibly lie in the same domain with the testing data, the classification could be more accurate.



### Novel approach to locate region of interest in mammograms for Breast cancer
- **Arxiv ID**: http://arxiv.org/abs/1811.07818v1
- **DOI**: 10.18201/ijisae.2018644775
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07818v1)
- **Published**: 2018-11-01 11:01:40+00:00
- **Updated**: 2018-11-01 11:01:40+00:00
- **Authors**: BV Divyashree, Amarnath R, Naveen M, G Hemantha Kumar
- **Comment**: ROI, breast cancer, mammographic images, segmentation, entropy, quad
  tree
- **Journal**: International Journal of Intelligent Systems and Applications in
  Engineering.(ISSN:2147-6799) Vol 6, No 3 (2018)
- **Summary**: Locating region of interest for breast cancer masses in the mammographic image is a challenging problem in medical image processing. In this research work, the keen idea is to efficiently extract suspected mass region for further examination. In particular to this fact breast boundary segmentation on sliced rgb image using modified intensity based approach followed by quad tree based division to spot out suspicious area are proposed in the paper. To evaluate the performance DDSM standard dataset are experimented and achieved acceptable accuracy.



### A Local Block Coordinate Descent Algorithm for the Convolutional Sparse Coding Model
- **Arxiv ID**: http://arxiv.org/abs/1811.00312v1
- **DOI**: None
- **Categories**: **cs.CV**, 08
- **Links**: [PDF](http://arxiv.org/pdf/1811.00312v1)
- **Published**: 2018-11-01 11:02:38+00:00
- **Updated**: 2018-11-01 11:02:38+00:00
- **Authors**: Ev Zisselman, Jeremias Sulam, Michael Elad
- **Comment**: 13 pages, 10 figures
- **Journal**: None
- **Summary**: The Convolutional Sparse Coding (CSC) model has recently gained considerable traction in the signal and image processing communities. By providing a global, yet tractable, model that operates on the whole image, the CSC was shown to overcome several limitations of the patch-based sparse model while achieving superior performance in various applications. Contemporary methods for pursuit and learning the CSC dictionary often rely on the Alternating Direction Method of Multipliers (ADMM) in the Fourier domain for the computational convenience of convolutions, while ignoring the local characterizations of the image. A recent work by Papyan et al. suggested the SBDL algorithm for the CSC, while operating locally on image patches. SBDL demonstrates better performance compared to the Fourier-based methods, albeit still relying on the ADMM. In this work we maintain the localized strategy of the SBDL, while proposing a new and much simpler approach based on the Block Coordinate Descent algorithm - this method is termed Local Block Coordinate Descent (LoBCoD). Furthermore, we introduce a novel stochastic gradient descent version of LoBCoD for training the convolutional filters. The Stochastic-LoBCoD leverages the benefits of online learning, while being applicable to a single training image. We demonstrate the advantages of the proposed algorithms for image inpainting and multi-focus image fusion, achieving state-of-the-art results.



### Convolutional Recurrent Predictor: Implicit Representation for Multi-target Filtering and Tracking
- **Arxiv ID**: http://arxiv.org/abs/1811.00313v3
- **DOI**: 10.1109/TSP.2019.2931170
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.00313v3)
- **Published**: 2018-11-01 11:06:57+00:00
- **Updated**: 2019-07-28 14:29:53+00:00
- **Authors**: Mehryar Emambakhsh, Alessandro Bay, Eduard Vazquez
- **Comment**: None
- **Journal**: None
- **Summary**: Defining a multi-target motion model, which is an important step of tracking algorithms, can be very challenging. Using fixed models (as in several generative Bayesian algorithms, such as Kalman filters) can fail to accurately predict sophisticated target motions. On the other hand, sequential learning of the motion model (for example, using recurrent neural networks) can be computationally complex and difficult due to the variable unknown number of targets. In this paper, we propose a multi-target filtering and tracking algorithm which learns the motion model, simultaneously for all targets, from an implicitly represented state map and performs spatio-temporal data prediction. To this end, the multi-target state is modelled over a continuous hypothetical target space, using random finite sets and Gaussian mixture probability hypothesis density formulations. The prediction step is recursively performed using a deep convolutional recurrent neural network with a long short-term memory architecture, which is trained as a regression block, on the fly, over "probability density difference" maps. Our approach is evaluated over widely used pedestrian tracking benchmarks, remarkably outperforming state-of-the-art multi-target filtering algorithms, while giving competitive results when compared with other tracking approaches: The proposed approach generates an average 40.40 and 62.29 optimal sub-pattern assignment (OSPA) errors on MOT15 and MOT16/17 datasets, respectively, while producing 62.0%, 70.0% and 66.9% multi-object tracking accuracy (MOTA) on MOT16/17, PNNL Parking Lot and PETS09 pedestrian tracking datasets, respectively, when publicly available detectors are used.



### Asymmetric Bilateral Phase Correlation for Optical Flow Estimation in the Frequency Domain
- **Arxiv ID**: http://arxiv.org/abs/1811.00327v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.00327v1)
- **Published**: 2018-11-01 11:51:29+00:00
- **Updated**: 2018-11-01 11:51:29+00:00
- **Authors**: Vasileios Argyriou
- **Comment**: SITIS 2018
- **Journal**: None
- **Summary**: We address the problem of motion estimation in images operating in the frequency domain. A method is presented which extends phase correlation to handle multiple motions present in an area. Our scheme is based on a novel Bilateral-Phase Correlation (BLPC) technique that incorporates the concept and principles of Bilateral Filters retaining the motion boundaries by taking into account the difference both in value and distance in a manner very similar to Gaussian convolution. The optical flow is obtained by applying the proposed method at certain locations selected based on the present motion differences and then performing non-uniform interpolation in a multi-scale iterative framework. Experiments with several well-known datasets with and without ground-truth show that our scheme outperforms recently proposed state-of-the-art phase correlation based optical flow methods.



### Towards Highly Accurate and Stable Face Alignment for High-Resolution Videos
- **Arxiv ID**: http://arxiv.org/abs/1811.00342v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.00342v2)
- **Published**: 2018-11-01 12:35:09+00:00
- **Updated**: 2018-11-22 14:40:25+00:00
- **Authors**: Ying Tai, Yicong Liang, Xiaoming Liu, Lei Duan, Jilin Li, Chengjie Wang, Feiyue Huang, Yu Chen
- **Comment**: Accepted to AAAI 2019. 8 pages, 7 figures
- **Journal**: None
- **Summary**: In recent years, heatmap regression based models have shown their effectiveness in face alignment and pose estimation. However, Conventional Heatmap Regression (CHR) is not accurate nor stable when dealing with high-resolution facial videos, since it finds the maximum activated location in heatmaps which are generated from rounding coordinates, and thus leads to quantization errors when scaling back to the original high-resolution space. In this paper, we propose a Fractional Heatmap Regression (FHR) for high-resolution video-based face alignment. The proposed FHR can accurately estimate the fractional part according to the 2D Gaussian function by sampling three points in heatmaps. To further stabilize the landmarks among continuous video frames while maintaining the precise at the same time, we propose a novel stabilization loss that contains two terms to address time delay and non-smooth issues, respectively. Experiments on 300W, 300-VW and Talking Face datasets clearly demonstrate that the proposed method is more accurate and stable than the state-of-the-art models.



### Analyzing Perception-Distortion Tradeoff using Enhanced Perceptual Super-resolution Network
- **Arxiv ID**: http://arxiv.org/abs/1811.00344v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.00344v2)
- **Published**: 2018-11-01 12:45:25+00:00
- **Updated**: 2018-11-04 06:12:53+00:00
- **Authors**: Subeesh Vasu, Nimisha Thekke Madam, Rajagopalan A. N
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural network (CNN) based methods have recently achieved great success for image super-resolution (SR). However, most deep CNN based SR models attempt to improve distortion measures (e.g. PSNR, SSIM, IFC, VIF) while resulting in poor quantified perceptual quality (e.g. human opinion score, no-reference quality measures such as NIQE). Few works have attempted to improve the perceptual quality at the cost of performance reduction in distortion measures. A very recent study has revealed that distortion and perceptual quality are at odds with each other and there is always a trade-off between the two. Often the restoration algorithms that are superior in terms of perceptual quality, are inferior in terms of distortion measures. Our work attempts to analyze the trade-off between distortion and perceptual quality for the problem of single image SR. To this end, we use the well-known SR architecture-enhanced deep super-resolution (EDSR) network and show that it can be adapted to achieve better perceptual quality for a specific range of the distortion measure. While the original network of EDSR was trained to minimize the error defined based on per-pixel accuracy alone, we train our network using a generative adversarial network framework with EDSR as the generator module. Our proposed network, called enhanced perceptual super-resolution network (EPSR), is trained with a combination of mean squared error loss, perceptual loss, and adversarial loss. Our experiments reveal that EPSR achieves the state-of-the-art trade-off between distortion and perceptual quality while the existing methods perform well in either of these measures alone.



### Bi-GANs-ST for Perceptual Image Super-resolution
- **Arxiv ID**: http://arxiv.org/abs/1811.00367v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.00367v1)
- **Published**: 2018-11-01 13:27:56+00:00
- **Updated**: 2018-11-01 13:27:56+00:00
- **Authors**: Xiaotong Luo, Rong Chen, Yuan Xie, Yanyun Qu, Cuihua Li
- **Comment**: 15 pages, 7 figures
- **Journal**: None
- **Summary**: Image quality measurement is a critical problem for image super-resolution (SR) algorithms. Usually, they are evaluated by some well-known objective metrics, e.g., PSNR and SSIM, but these indices cannot provide suitable results in accordance with the perception of human being. Recently, a more reasonable perception measurement has been proposed in [1], which is also adopted by the PIRM-SR 2018 challenge. In this paper, motivated by [1], we aim to generate a high-quality SR result which balances between the two indices, i.e., the perception index and root-mean-square error (RMSE). To do so, we design a new deep SR framework, dubbed Bi-GANs-ST, by integrating two complementary generative adversarial networks (GAN) branches. One is memory residual SRGAN (MR-SRGAN), which emphasizes on improving the objective performance, such as reducing the RMSE. The other is weight perception SRGAN (WP-SRGAN), which obtains the result that favors better subjective perception via a two-stage adversarial training mechanism. Then, to produce final result with excellent perception scores and RMSE, we use soft-thresholding method to merge the results generated by the two GANs. Our method performs well on the perceptual image super-resolution task of the PIRM 2018 challenge. Experimental results on five benchmarks show that our proposal achieves highly competent performance compared with other state-of-the-art methods.



### Continuous-time Intensity Estimation Using Event Cameras
- **Arxiv ID**: http://arxiv.org/abs/1811.00386v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.00386v1)
- **Published**: 2018-11-01 13:56:26+00:00
- **Updated**: 2018-11-01 13:56:26+00:00
- **Authors**: Cedric Scheerlinck, Nick Barnes, Robert Mahony
- **Comment**: None
- **Journal**: None
- **Summary**: Event cameras provide asynchronous, data-driven measurements of local temporal contrast over a large dynamic range with extremely high temporal resolution. Conventional cameras capture low-frequency reference intensity information. These two sensor modalities provide complementary information. We propose a computationally efficient, asynchronous filter that continuously fuses image frames and events into a single high-temporal-resolution, high-dynamic-range image state. In absence of conventional image frames, the filter can be run on events only. We present experimental results on high-speed, high-dynamic-range sequences, as well as on new ground truth datasets we generate to demonstrate the proposed algorithm outperforms existing state-of-the-art methods.



### Excessive Invariance Causes Adversarial Vulnerability
- **Arxiv ID**: http://arxiv.org/abs/1811.00401v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.00401v4)
- **Published**: 2018-11-01 14:14:03+00:00
- **Updated**: 2020-07-12 07:26:06+00:00
- **Authors**: Jörn-Henrik Jacobsen, Jens Behrmann, Richard Zemel, Matthias Bethge
- **Comment**: None
- **Journal**: Proceedings of the 7th International Conference on Learning
  Representations (ICLR), 2019
- **Summary**: Despite their impressive performance, deep neural networks exhibit striking failures on out-of-distribution inputs. One core idea of adversarial example research is to reveal neural network errors under such distribution shifts. We decompose these errors into two complementary sources: sensitivity and invariance. We show deep networks are not only too sensitive to task-irrelevant changes of their input, as is well-known from epsilon-adversarial examples, but are also too invariant to a wide range of task-relevant changes, thus making vast regions in input space vulnerable to adversarial attacks. We show such excessive invariance occurs across various tasks and architecture types. On MNIST and ImageNet one can manipulate the class-specific content of almost any image without changing the hidden activations. We identify an insufficiency of the standard cross-entropy loss as a reason for these failures. Further, we extend this objective based on an information-theoretic analysis so it encourages the model to consider all task-dependent features in its decision. This provides the first approach tailored explicitly to overcome excessive invariance and resulting vulnerabilities.



### An Improved Learning Framework for Covariant Local Feature Detection
- **Arxiv ID**: http://arxiv.org/abs/1811.00438v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.00438v1)
- **Published**: 2018-11-01 15:34:00+00:00
- **Updated**: 2018-11-01 15:34:00+00:00
- **Authors**: Nehal Doiphode, Rahul Mitra, Shuaib Ahmed, Arjun Jain
- **Comment**: 15 pages
- **Journal**: ACCV 2018 Camera Ready
- **Summary**: Learning feature detection has been largely an unexplored area when compared to handcrafted feature detection. Recent learning formulations use the covariant constraint in their loss function to learn covariant detectors. However, just learning from covariant constraint can lead to detection of unstable features. To impart further, stability detectors are trained to extract pre-determined features obtained by hand-crafted detectors. However, in the process they lose the ability to detect novel features. In an attempt to overcome the above limitations, we propose an improved scheme by incorporating covariant constraints in form of triplets with addition to an affine covariant constraint. We show that using these additional constraints one can learn to detect novel and stable features without using pre-determined features for training. Extensive experiments show our model achieves state-of-the-art performance in repeatability score on the well known datasets such as Vgg-Affine, EF, and Webcam.



### CariGAN: Caricature Generation through Weakly Paired Adversarial Learning
- **Arxiv ID**: http://arxiv.org/abs/1811.00445v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.00445v2)
- **Published**: 2018-11-01 15:40:23+00:00
- **Updated**: 2018-11-20 21:22:31+00:00
- **Authors**: Wenbin Li, Wei Xiong, Haofu Liao, Jing Huo, Yang Gao, Jiebo Luo
- **Comment**: 12
- **Journal**: None
- **Summary**: Caricature generation is an interesting yet challenging task. The primary goal is to generate plausible caricatures with reasonable exaggerations given face images. Conventional caricature generation approaches mainly use low-level geometric transformations such as image warping to generate exaggerated images, which lack richness and diversity in terms of content and style. The recent progress in generative adversarial networks (GANs) makes it possible to learn an image-to-image transformation from data, so that richer contents and styles can be generated. However, directly applying the GAN-based models to this task leads to unsatisfactory results because there is a large variance in the caricature distribution. Moreover, some models require strictly paired training data which largely limits their usage scenarios. In this paper, we propose CariGAN overcome these problems. Instead of training on paired data, CariGAN learns transformations only from weakly paired images. Specifically, to enforce reasonable exaggeration and facial deformation, facial landmarks are adopted as an additional condition to constrain the generated image. Furthermore, an attention mechanism is introduced to encourage our model to focus on the key facial parts so that more vivid details in these regions can be generated. Finally, a Diversity Loss is proposed to encourage the model to produce diverse results to help alleviate the `mode collapse' problem of the conventional GAN-based models. Extensive experiments on a new large-scale `WebCaricature' dataset show that the proposed CariGAN can generate more plausible caricatures with larger diversity compared with the state-of-the-art models.



### Class-Agnostic Counting
- **Arxiv ID**: http://arxiv.org/abs/1811.00472v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.00472v1)
- **Published**: 2018-11-01 16:11:42+00:00
- **Updated**: 2018-11-01 16:11:42+00:00
- **Authors**: Erika Lu, Weidi Xie, Andrew Zisserman
- **Comment**: Asian Conference on Computer Vision (ACCV), 2018
- **Journal**: None
- **Summary**: Nearly all existing counting methods are designed for a specific object class. Our work, however, aims to create a counting model able to count any class of object. To achieve this goal, we formulate counting as a matching problem, enabling us to exploit the image self-similarity property that naturally exists in object counting problems. We make the following three contributions: first, a Generic Matching Network (GMN) architecture that can potentially count any object in a class-agnostic manner; second, by reformulating the counting problem as one of matching objects, we can take advantage of the abundance of video data labeled for tracking, which contains natural repetitions suitable for training a counting model. Such data enables us to train the GMN. Third, to customize the GMN to different user requirements, an adapter module is used to specialize the model with minimal effort, i.e. using a few labeled examples, and adapting only a small fraction of the trained parameters. This is a form of few-shot learning, which is practical for domains where labels are limited due to requiring expert knowledge (e.g. microbiology). We demonstrate the flexibility of our method on a diverse set of existing counting benchmarks: specifically cells, cars, and human crowds. The model achieves competitive performance on cell and crowd counting datasets, and surpasses the state-of-the-art on the car dataset using only three training images. When training on the entire dataset, the proposed method outperforms all previous methods by a large margin.



### Unsupervised representation learning using convolutional and stacked auto-encoders: a domain and cross-domain feature space analysis
- **Arxiv ID**: http://arxiv.org/abs/1811.00473v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.00473v1)
- **Published**: 2018-11-01 16:11:45+00:00
- **Updated**: 2018-11-01 16:11:45+00:00
- **Authors**: Gabriel B. Cavallari, Leonardo Sampaio Ferraz Ribeiro, Moacir Antonelli Ponti
- **Comment**: SIBGRAPI 2018 - Conference on Graphics, Patterns and Images
- **Journal**: None
- **Summary**: A feature learning task involves training models that are capable of inferring good representations (transformations of the original space) from input data alone. When working with limited or unlabelled data, and also when multiple visual domains are considered, methods that rely on large annotated datasets, such as Convolutional Neural Networks (CNNs), cannot be employed. In this paper we investigate different auto-encoder (AE) architectures, which require no labels, and explore training strategies to learn representations from images. The models are evaluated considering both the reconstruction error of the images and the feature spaces in terms of their discriminative power. We study the role of dense and convolutional layers on the results, as well as the depth and capacity of the networks, since those are shown to affect both the dimensionality reduction and the capability of generalising for different visual domains. Classification results with AE features were as discriminative as pre-trained CNN features. Our findings can be used as guidelines for the design of unsupervised representation learning methods within and across domains.



### Hybrid Pruning: Thinner Sparse Networks for Fast Inference on Edge Devices
- **Arxiv ID**: http://arxiv.org/abs/1811.00482v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1811.00482v1)
- **Published**: 2018-11-01 16:24:50+00:00
- **Updated**: 2018-11-01 16:24:50+00:00
- **Authors**: Xiaofan Xu, Mi Sun Park, Cormac Brick
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce hybrid pruning which combines both coarse-grained channel and fine-grained weight pruning to reduce model size, computation and power demands with no to little loss in accuracy for enabling modern networks deployment on resource-constrained devices, such as always-on security cameras and drones. Additionally, to effectively perform channel pruning, we propose a fast sensitivity test that helps us quickly identify the sensitivity of within and across layers of a network to the output accuracy for target multiplier accumulators (MACs) or accuracy tolerance. Our experiment shows significantly better results on ResNet50 on ImageNet compared to existing work, even with an additional constraint of channels be hardware-friendly number.



### A Corpus for Reasoning About Natural Language Grounded in Photographs
- **Arxiv ID**: http://arxiv.org/abs/1811.00491v3
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1811.00491v3)
- **Published**: 2018-11-01 16:47:44+00:00
- **Updated**: 2019-07-21 05:26:36+00:00
- **Authors**: Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, Yoav Artzi
- **Comment**: ACL 2019 Long Paper
- **Journal**: None
- **Summary**: We introduce a new dataset for joint reasoning about natural language and images, with a focus on semantic diversity, compositionality, and visual reasoning challenges. The data contains 107,292 examples of English sentences paired with web photographs. The task is to determine whether a natural language caption is true about a pair of photographs. We crowdsource the data using sets of visually rich images and a compare-and-contrast task to elicit linguistically diverse language. Qualitative analysis shows the data requires compositional joint reasoning, including about quantities, comparisons, and relations. Evaluation using state-of-the-art visual reasoning methods shows the data presents a strong challenge.



### Improving CNN Training using Disentanglement for Liver Lesion Classification in CT
- **Arxiv ID**: http://arxiv.org/abs/1811.00501v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.00501v1)
- **Published**: 2018-11-01 17:12:14+00:00
- **Updated**: 2018-11-01 17:12:14+00:00
- **Authors**: Avi Ben-Cohen, Roey Mechrez, Noa Yedidia, Hayit Greenspan
- **Comment**: None
- **Journal**: None
- **Summary**: Training data is the key component in designing algorithms for medical image analysis and in many cases it is the main bottleneck in achieving good results. Recent progress in image generation has enabled the training of neural network based solutions using synthetic data. A key factor in the generation of new samples is controlling the important appearance features and potentially being able to generate a new sample of a specific class with different variants. In this work we suggest the synthesis of new data by mixing the class specified and unspecified representation of different factors in the training data. Our experiments on liver lesion classification in CT show an average improvement of 7.4% in accuracy over the baseline training scheme.



### Navigation by Imitation in a Pedestrian-Rich Environment
- **Arxiv ID**: http://arxiv.org/abs/1811.00506v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.00506v1)
- **Published**: 2018-11-01 17:20:03+00:00
- **Updated**: 2018-11-01 17:20:03+00:00
- **Authors**: Jing Bi, Tianyou Xiao, Qiuyue Sun, Chenliang Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks trained on demonstrations of human actions give robot the ability to perform self-driving on the road. However, navigation in a pedestrian-rich environment, such as a campus setup, is still challenging---one needs to take frequent interventions to the robot and take control over the robot from early steps leading to a mistake. An arduous burden is, hence, placed on the learning framework design and data acquisition. In this paper, we propose a new learning-from-intervention Dataset Aggregation (DAgger) algorithm to overcome the limitations brought by applying imitation learning to navigation in the pedestrian-rich environment. Our new learning algorithm implements an error backtrack function that is able to effectively learn from expert interventions. Combining our new learning algorithm with deep convolutional neural networks and a hierarchically-nested policy-selection mechanism, we show that our robot is able to map pixels direct to control commands and navigate successfully in real world without explicitly modeling the pedestrian behaviors or the world model.



### Out of the Box: Reasoning with Graph Convolution Nets for Factual Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1811.00538v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.00538v1)
- **Published**: 2018-11-01 17:59:56+00:00
- **Updated**: 2018-11-01 17:59:56+00:00
- **Authors**: Medhini Narasimhan, Svetlana Lazebnik, Alexander G. Schwing
- **Comment**: Accepted to NIPS 2018
- **Journal**: None
- **Summary**: Accurately answering a question about a given image requires combining observations with general knowledge. While this is effortless for humans, reasoning with general knowledge remains an algorithmic challenge. To advance research in this direction a novel `fact-based' visual question answering (FVQA) task has been introduced recently along with a large set of curated facts which link two entities, i.e., two possible answers, via a relation. Given a question-image pair, deep network techniques have been employed to successively reduce the large set of facts until one of the two entities of the final remaining fact is predicted as the answer. We observe that a successive process which considers one fact at a time to form a local decision is sub-optimal. Instead, we develop an entity graph and use a graph convolutional network to `reason' about the correct answer by jointly considering all entities. We show on the challenging FVQA dataset that this leads to an improvement in accuracy of around 7% compared to the state of the art.



### Capsule Networks for Brain Tumor Classification based on MRI Images and Course Tumor Boundaries
- **Arxiv ID**: http://arxiv.org/abs/1811.00597v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.00597v1)
- **Published**: 2018-11-01 19:23:38+00:00
- **Updated**: 2018-11-01 19:23:38+00:00
- **Authors**: Parnian Afshar, Konstantinos N. Plataniotis, Arash Mohammadi
- **Comment**: None
- **Journal**: None
- **Summary**: According to official statistics, cancer is considered as the second leading cause of human fatalities. Among different types of cancer, brain tumor is seen as one of the deadliest forms due to its aggressive nature, heterogeneous characteristics, and low relative survival rate. Determining the type of brain tumor has significant impact on the treatment choice and patient's survival. Human-centered diagnosis is typically error-prone and unreliable resulting in a recent surge of interest to automatize this process using convolutional neural networks (CNNs). CNNs, however, fail to fully utilize spatial relations, which is particularly harmful for tumor classification, as the relation between the tumor and its surrounding tissue is a critical indicator of the tumor's type. In our recent work, we have incorporated newly developed CapsNets to overcome this shortcoming. CapsNets are, however, highly sensitive to the miscellaneous image background. The paper addresses this gap. The main contribution is to equip CapsNet with access to the tumor surrounding tissues, without distracting it from the main target. A modified CapsNet architecture is, therefore, proposed for brain tumor classification, which takes the tumor coarse boundaries as extra inputs within its pipeline to increase the CapsNet's focus. The proposed approach noticeably outperforms its counterparts.



### Prediction Error Meta Classification in Semantic Segmentation: Detection via Aggregated Dispersion Measures of Softmax Probabilities
- **Arxiv ID**: http://arxiv.org/abs/1811.00648v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML, 68T45, 62-07
- **Links**: [PDF](http://arxiv.org/pdf/1811.00648v2)
- **Published**: 2018-11-01 22:00:00+00:00
- **Updated**: 2019-10-02 14:38:24+00:00
- **Authors**: Matthias Rottmann, Pascal Colling, Thomas-Paul Hack, Robin Chan, Fabian Hüger, Peter Schlicht, Hanno Gottschalk
- **Comment**: None
- **Journal**: None
- **Summary**: We present a method that "meta" classifies whether seg-ments predicted by a semantic segmentation neural networkintersect with the ground truth. For this purpose, we employ measures of dispersion for predicted pixel-wise class probability distributions, like classification entropy, that yield heat maps of the input scene's size. We aggregate these dispersion measures segment-wise and derive metrics that are well-correlated with the segment-wise IoU of prediction and ground truth. This procedure yields an almost plug and play post-processing tool to rate the prediction quality of semantic segmentation networks on segment level. This is especially relevant for monitoring neural networks in online applications like automated driving or medical imaging where reliability is of utmost importance. In our tests, we use publicly available state-of-the-art networks trained on the Cityscapes dataset and the BraTS2017 dataset and analyze the predictive power of different metrics as well as different sets of metrics. To this end, we compute logistic LASSO regression fits for the task of classifying IoU=0 vs. IoU>0 per segment and obtain AUROC values of up to 91.55%. We complement these tests with linear regression fits to predict the segment-wise IoU and obtain prediction standard deviations of down to 0.130 as well as $R^2$ values of up to 84.15%. We show that these results clearly outperform standard approaches.



### Exposing DeepFake Videos By Detecting Face Warping Artifacts
- **Arxiv ID**: http://arxiv.org/abs/1811.00656v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.00656v3)
- **Published**: 2018-11-01 22:13:55+00:00
- **Updated**: 2019-05-22 15:06:39+00:00
- **Authors**: Yuezun Li, Siwei Lyu
- **Comment**: CVPRW 2019
- **Journal**: None
- **Summary**: In this work, we describe a new deep learning based method that can effectively distinguish AI-generated fake videos (referred to as {\em DeepFake} videos hereafter) from real videos. Our method is based on the observations that current DeepFake algorithm can only generate images of limited resolutions, which need to be further warped to match the original faces in the source video. Such transforms leave distinctive artifacts in the resulting DeepFake videos, and we show that they can be effectively captured by convolutional neural networks (CNNs). Compared to previous methods which use a large amount of real and DeepFake generated images to train CNN classifier, our method does not need DeepFake generated images as negative training examples since we target the artifacts in affine face warping as the distinctive feature to distinguish real and fake images. The advantages of our method are two-fold: (1) Such artifacts can be simulated directly using simple image processing operations on a image to make it as negative example. Since training a DeepFake model to generate negative examples is time-consuming and resource-demanding, our method saves a plenty of time and resources in training data collection; (2) Since such artifacts are general existed in DeepFake videos from different sources, our method is more robust compared to others. Our method is evaluated on two sets of DeepFake video datasets for its effectiveness in practice.



### Exposing Deep Fakes Using Inconsistent Head Poses
- **Arxiv ID**: http://arxiv.org/abs/1811.00661v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.00661v2)
- **Published**: 2018-11-01 22:27:20+00:00
- **Updated**: 2018-11-13 21:47:51+00:00
- **Authors**: Xin Yang, Yuezun Li, Siwei Lyu
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a new method to expose AI-generated fake face images or videos (commonly known as the Deep Fakes). Our method is based on the observations that Deep Fakes are created by splicing synthesized face region into the original image, and in doing so, introducing errors that can be revealed when 3D head poses are estimated from the face images. We perform experiments to demonstrate this phenomenon and further develop a classification method based on this cue. Using features based on this cue, an SVM classifier is evaluated using a set of real face images and Deep Fakes.



### Introduction to the 1st Place Winning Model of OpenImages Relationship Detection Challenge
- **Arxiv ID**: http://arxiv.org/abs/1811.00662v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.00662v2)
- **Published**: 2018-11-01 22:44:01+00:00
- **Updated**: 2018-11-07 19:42:31+00:00
- **Authors**: Ji Zhang, Kevin Shih, Andrew Tao, Bryan Catanzaro, Ahmed Elgammal
- **Comment**: None
- **Journal**: None
- **Summary**: This article describes the model we built that achieved 1st place in the OpenImage Visual Relationship Detection Challenge on Kaggle. Three key factors contribute the most to our success: 1) language bias is a powerful baseline for this task. We build the empirical distribution $P(predicate|subject,object)$ in the training set and directly use that in testing. This baseline achieved the 2nd place when submitted; 2) spatial features are as important as visual features, especially for spatial relationships such as "under" and "inside of"; 3) It is a very effective way to fuse different features by first building separate modules for each of them, then adding their output logits before the final softmax layer. We show in ablation study that each factor can improve the performance to a non-trivial extent, and the model reaches optimal when all of them are combined.



