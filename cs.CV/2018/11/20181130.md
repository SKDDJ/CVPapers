# Arxiv Papers in cs.CV on 2018-11-30
### Lightweight and Efficient Image Super-Resolution with Block State-based Recursive Network
- **Arxiv ID**: http://arxiv.org/abs/1811.12546v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.12546v1)
- **Published**: 2018-11-30 00:01:37+00:00
- **Updated**: 2018-11-30 00:01:37+00:00
- **Authors**: Jun-Ho Choi, Jun-Hyuk Kim, Manri Cheon, Jong-Seok Lee
- **Comment**: The code is available at https://github.com/idearibosome/tf-bsrn-sr
- **Journal**: None
- **Summary**: Recently, several deep learning-based image super-resolution methods have been developed by stacking massive numbers of layers. However, this leads too large model sizes and high computational complexities, thus some recursive parameter-sharing methods have been also proposed. Nevertheless, their designs do not properly utilize the potential of the recursive operation. In this paper, we propose a novel, lightweight, and efficient super-resolution method to maximize the usefulness of the recursive architecture, by introducing block state-based recursive network. By taking advantage of utilizing the block state, the recursive part of our model can easily track the status of the current image features. We show the benefits of the proposed method in terms of model size, speed, and efficiency. In addition, we show that our method outperforms the other state-of-the-art methods.



### Deep Multimodal Learning: An Effective Method for Video Classification
- **Arxiv ID**: http://arxiv.org/abs/1811.12563v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1811.12563v1)
- **Published**: 2018-11-30 01:05:41+00:00
- **Updated**: 2018-11-30 01:05:41+00:00
- **Authors**: Tianqi Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Videos have become ubiquitous on the Internet. And video analysis can provide lots of information for detecting and recognizing objects as well as help people understand human actions and interactions with the real world. However, facing data as huge as TB level, effective methods should be applied. Recurrent neural network (RNN) architecture has wildly been used on many sequential learning problems such as Language Model, Time-Series Analysis, etc. In this paper, we propose some variations of RNN such as stacked bidirectional LSTM/GRU network with attention mechanism to categorize large-scale video data. We also explore different multimodal fusion methods. Our model combines both visual and audio information on both video and frame level and received great result. Ensemble methods are also applied. Because of its multimodal characteristics, we decide to call this method Deep Multimodal Learning(DML). Our DML-based model was trained on Google Cloud and our own server and was tested in a well-known video classification competition on Kaggle held by Google.



### Are All Training Examples Created Equal? An Empirical Study
- **Arxiv ID**: http://arxiv.org/abs/1811.12569v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.12569v1)
- **Published**: 2018-11-30 01:16:42+00:00
- **Updated**: 2018-11-30 01:16:42+00:00
- **Authors**: Kailas Vodrahalli, Ke Li, Jitendra Malik
- **Comment**: 12 pages, 12 figures
- **Journal**: None
- **Summary**: Modern computer vision algorithms often rely on very large training datasets. However, it is conceivable that a carefully selected subsample of the dataset is sufficient for training. In this paper, we propose a gradient-based importance measure that we use to empirically analyze relative importance of training images in four datasets of varying complexity. We find that in some cases, a small subsample is indeed sufficient for training. For other datasets, however, the relative differences in importance are negligible. These results have important implications for active learning on deep networks. Additionally, our analysis method can be used as a general tool to better understand diversity of training examples in datasets.



### Parsing R-CNN for Instance-Level Human Analysis
- **Arxiv ID**: http://arxiv.org/abs/1811.12596v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.12596v1)
- **Published**: 2018-11-30 03:21:11+00:00
- **Updated**: 2018-11-30 03:21:11+00:00
- **Authors**: Lu Yang, Qing Song, Zhihui Wang, Ming Jiang
- **Comment**: COCO 2018 DensePose Challenge Winner
- **Journal**: None
- **Summary**: Instance-level human analysis is common in real-life scenarios and has multiple manifestations, such as human part segmentation, dense pose estimation, human-object interactions, etc. Models need to distinguish different human instances in the image panel and learn rich features to represent the details of each instance. In this paper, we present an end-to-end pipeline for solving the instance-level human analysis, named Parsing R-CNN. It processes a set of human instances simultaneously through comprehensive considering the characteristics of region-based approach and the appearance of a human, thus allowing representing the details of instances. Parsing R-CNN is very flexible and efficient, which is applicable to many issues in human instance analysis. Our approach outperforms all state-of-the-art methods on CIHP (Crowd Instance-level Human Parsing), MHP v2.0 (Multi-Human Parsing) and DensePose-COCO datasets. Based on the proposed Parsing R-CNN, we reach the 1st place in the COCO 2018 Challenge DensePose Estimation task. Code and models are public available.



### Learning Dynamics from Kinematics: Estimating 2D Foot Pressure Maps from Video Frames
- **Arxiv ID**: http://arxiv.org/abs/1811.12607v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.12607v4)
- **Published**: 2018-11-30 04:16:24+00:00
- **Updated**: 2019-05-28 13:33:23+00:00
- **Authors**: Christopher Funk, Savinay Nagendra, Jesse Scott, Bharadwaj Ravichandran, John H. Challis, Robert T. Collins, Yanxi Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Pose stability analysis is the key to understanding locomotion and control of body equilibrium, with applications in numerous fields such as kinesiology, medicine, and robotics. In biomechanics, Center of Pressure (CoP) is used in studies of human postural control and gait. We propose and validate a novel approach to learn CoP from pose of a human body to aid stability analysis. More specifically, we propose an end-to-end deep learning architecture to regress foot pressure heatmaps, and hence the CoP locations, from 2D human pose derived from video. We have collected a set of long (5min +) choreographed Taiji (Tai Chi) sequences of multiple subjects with synchronized foot pressure and video data. The derived human pose data and corresponding foot pressure maps are used jointly in training a convolutional neural network with residual architecture, named PressNET. Cross-subject validation results show promising performance of PressNET, significantly outperforming the baseline method of K-Nearest Neighbors. Furthermore, we demonstrate that our computation of center of pressure (CoP) from PressNET is not only significantly more accurate than those obtained from the baseline approach but also meets the expectations of corresponding lab-based measurements of stability studies in kinesiology.



### DeepFlux for Skeletons in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1811.12608v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.12608v1)
- **Published**: 2018-11-30 04:21:47+00:00
- **Updated**: 2018-11-30 04:21:47+00:00
- **Authors**: Yukang Wang, Yongchao Xu, Stavros Tsogkas, Xiang Bai, Sven Dickinson, Kaleem Siddiqi
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Computing object skeletons in natural images is challenging, owing to large variations in object appearance and scale, and the complexity of handling background clutter. Many recent methods frame object skeleton detection as a binary pixel classification problem, which is similar in spirit to learning-based edge detection, as well as to semantic segmentation methods. In the present article, we depart from this strategy by training a CNN to predict a two-dimensional vector field, which maps each scene point to a candidate skeleton pixel, in the spirit of flux-based skeletonization algorithms. This "image context flux" representation has two major advantages over previous approaches. First, it explicitly encodes the relative position of skeletal pixels to semantically meaningful entities, such as the image points in their spatial context, and hence also the implied object boundaries. Second, since the skeleton detection context is a region-based vector field, it is better able to cope with object parts of large width. We evaluate the proposed method on three benchmark datasets for skeleton detection and two for symmetry detection, achieving consistently superior performance over state-of-the-art methods.



### Virtual Class Enhanced Discriminative Embedding Learning
- **Arxiv ID**: http://arxiv.org/abs/1811.12611v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.12611v1)
- **Published**: 2018-11-30 04:43:20+00:00
- **Updated**: 2018-11-30 04:43:20+00:00
- **Authors**: Binghui Chen, Weihong Deng, Haifeng Shen
- **Comment**: NeurIPS 2018
- **Journal**: None
- **Summary**: Recently, learning discriminative features to improve the recognition performances gradually becomes the primary goal of deep learning, and numerous remarkable works have emerged. In this paper, we propose a novel yet extremely simple method \textbf{Virtual Softmax} to enhance the discriminative property of learned features by injecting a dynamic virtual negative class into the original softmax. Injecting virtual class aims to enlarge inter-class margin and compress intra-class distribution by strengthening the decision boundary constraint. Although it seems weird to optimize with this additional virtual class, we show that our method derives from an intuitive and clear motivation, and it indeed encourages the features to be more compact and separable. This paper empirically and experimentally demonstrates the superiority of Virtual Softmax, improving the performances on a variety of object classification and face verification tasks.



### Towards Robust Lung Segmentation in Chest Radiographs with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1811.12638v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.12638v1)
- **Published**: 2018-11-30 06:38:07+00:00
- **Updated**: 2018-11-30 06:38:07+00:00
- **Authors**: Jyoti Islam, Yanqing Zhang
- **Comment**: Machine Learning for Health (ML4H) Workshop at NeurIPS 2018
  arXiv:cs/0101200
- **Journal**: None
- **Summary**: Automated segmentation of Lungs plays a crucial role in the computer-aided diagnosis of chest X-Ray (CXR) images. Developing an efficient Lung segmentation model is challenging because of difficulties such as the presence of several edges at the rib cage and clavicle, inconsistent lung shape among different individuals, and the appearance of the lung apex. In this paper, we propose a robust model for Lung segmentation in Chest Radiographs. Our model learns to ignore the irrelevant regions in an input Chest Radiograph while highlighting regions useful for lung segmentation. The proposed model is evaluated on two public chest X-Ray datasets (Montgomery County, MD, USA, and Shenzhen No. 3 People's Hospital in China). The experimental result with a DICE score of 98.6% demonstrates the robustness of our proposed lung segmentation approach.



### Transferable Adversarial Attacks for Image and Video Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1811.12641v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.12641v5)
- **Published**: 2018-11-30 06:55:22+00:00
- **Updated**: 2019-05-13 07:22:16+00:00
- **Authors**: Xingxing Wei, Siyuan Liang, Ning Chen, Xiaochun Cao
- **Comment**: IJCAI2019 oral
- **Journal**: None
- **Summary**: Adversarial examples have been demonstrated to threaten many computer vision tasks including object detection. However, the existing attacking methods for object detection have two limitations: poor transferability, which denotes that the generated adversarial examples have low success rate to attack other kinds of detection methods, and high computation cost, which means that they need more time to generate an adversarial image, and therefore are difficult to deal with the video data. To address these issues, we utilize a generative mechanism to obtain the adversarial image and video. In this way, the processing time is reduced. To enhance the transferability, we destroy the feature maps extracted from the feature network, which usually constitutes the basis of object detectors. The proposed method is based on the Generative Adversarial Network (GAN) framework, where we combine the high-level class loss and low-level feature loss to jointly train the adversarial example generator. A series of experiments conducted on PASCAL VOC and ImageNet VID datasets show that our method can efficiently generate image and video adversarial examples, and more importantly, these adversarial examples have better transferability, and thus, are able to simultaneously attack two kinds of representative object detection models: proposal based models like Faster-RCNN, and regression based models like SSD.



### Classification is a Strong Baseline for Deep Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/1811.12649v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.12649v2)
- **Published**: 2018-11-30 07:21:25+00:00
- **Updated**: 2019-08-04 18:53:50+00:00
- **Authors**: Andrew Zhai, Hao-Yu Wu
- **Comment**: Accepted to BMVC 2019
- **Journal**: None
- **Summary**: Deep metric learning aims to learn a function mapping image pixels to embedding feature vectors that model the similarity between images. Two major applications of metric learning are content-based image retrieval and face verification. For the retrieval tasks, the majority of current state-of-the-art (SOTA) approaches are triplet-based non-parametric training. For the face verification tasks, however, recent SOTA approaches have adopted classification-based parametric training. In this paper, we look into the effectiveness of classification based approaches on image retrieval datasets. We evaluate on several standard retrieval datasets such as CAR-196, CUB-200-2011, Stanford Online Product, and In-Shop datasets for image retrieval and clustering, and establish that our classification-based approach is competitive across different feature dimensions and base feature networks. We further provide insights into the performance effects of subsampling classes for scalable classification-based training, and the effects of binarization, enabling efficient storage and computation for practical applications.



### FSNet: An Identity-Aware Generative Model for Image-based Face Swapping
- **Arxiv ID**: http://arxiv.org/abs/1811.12666v1
- **DOI**: 10.1007/978-3-030-20876-9_8
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.12666v1)
- **Published**: 2018-11-30 08:16:57+00:00
- **Updated**: 2018-11-30 08:16:57+00:00
- **Authors**: Ryota Natsume, Tatsuya Yatagawa, Shigeo Morishima
- **Comment**: 20pages, Asian Conference of Computer Vision 2018
- **Journal**: None
- **Summary**: This paper presents FSNet, a deep generative model for image-based face swapping. Traditionally, face-swapping methods are based on three-dimensional morphable models (3DMMs), and facial textures are replaced between the estimated three-dimensional (3D) geometries in two images of different individuals. However, the estimation of 3D geometries along with different lighting conditions using 3DMMs is still a difficult task. We herein represent the face region with a latent variable that is assigned with the proposed deep neural network (DNN) instead of facial textures. The proposed DNN synthesizes a face-swapped image using the latent variable of the face region and another image of the non-face region. The proposed method is not required to fit to the 3DMM; additionally, it performs face swapping only by feeding two face images to the proposed network. Consequently, our DNN-based face swapping performs better than previous approaches for challenging inputs with different face orientations and lighting conditions. Through several experiments, we demonstrated that the proposed method performs face swapping in a more stable manner than the state-of-the-art method, and that its results are compatible with the method thereof.



### Instance-level Facial Attributes Transfer with Geometry-Aware Flow
- **Arxiv ID**: http://arxiv.org/abs/1811.12670v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.12670v1)
- **Published**: 2018-11-30 08:43:00+00:00
- **Updated**: 2018-11-30 08:43:00+00:00
- **Authors**: Weidong Yin, Ziwei Liu, Chen Change Loy
- **Comment**: To appear in AAAI 2019. Code and models are available at:
  https://github.com/wdyin/GeoGAN
- **Journal**: None
- **Summary**: We address the problem of instance-level facial attribute transfer without paired training data, e.g. faithfully transferring the exact mustache from a source face to a target face. This is a more challenging task than the conventional semantic-level attribute transfer, which only preserves the generic attribute style instead of instance-level traits. We propose the use of geometry-aware flow, which serves as a well-suited representation for modeling the transformation between instance-level facial attributes. Specifically, we leverage the facial landmarks as the geometric guidance to learn the differentiable flows automatically, despite of the large pose gap existed. Geometry-aware flow is able to warp the source face attribute into the target face context and generate a warp-and-blend result. To compensate for the potential appearance gap between source and target faces, we propose a hallucination sub-network that produces an appearance residual to further refine the warp-and-blend result. Finally, a cycle-consistency framework consisting of both attribute transfer module and attribute removal module is designed, so that abundant unpaired face images can be used as training data. Extensive evaluations validate the capability of our approach in transferring instance-level facial attributes faithfully across large pose and appearance gaps. Thanks to the flow representation, our approach can readily be applied to generate realistic details on high-resolution images.



### ComDefend: An Efficient Image Compression Model to Defend Adversarial Examples
- **Arxiv ID**: http://arxiv.org/abs/1811.12673v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.12673v3)
- **Published**: 2018-11-30 08:56:30+00:00
- **Updated**: 2019-07-01 13:04:25+00:00
- **Authors**: Xiaojun Jia, Xingxing Wei, Xiaochun Cao, Hassan Foroosh
- **Comment**: None
- **Journal**: CVPR 2019
- **Summary**: Deep neural networks (DNNs) have been demonstrated to be vulnerable to adversarial examples. Specifically, adding imperceptible perturbations to clean images can fool the well trained deep neural networks. In this paper, we propose an end-to-end image compression model to defend adversarial examples: \textbf{ComDefend}. The proposed model consists of a compression convolutional neural network (ComCNN) and a reconstruction convolutional neural network (ResCNN). The ComCNN is used to maintain the structure information of the original image and purify adversarial perturbations. And the ResCNN is used to reconstruct the original image with high quality. In other words, ComDefend can transform the adversarial image to its clean version, which is then fed to the trained classifier. Our method is a pre-processing module, and does not modify the classifier's structure during the whole process. Therefore, it can be combined with other model-specific defense models to jointly improve the classifier's robustness. A series of experiments conducted on MNIST, CIFAR10 and ImageNet show that the proposed method outperforms the state-of-the-art defense methods, and is consistently effective to protect classifiers against adversarial attacks.



### Generating Material Maps to Map Informal Settlements
- **Arxiv ID**: http://arxiv.org/abs/1812.00786v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.00786v2)
- **Published**: 2018-11-30 09:09:41+00:00
- **Updated**: 2019-05-30 12:19:50+00:00
- **Authors**: Patrick Helber, Bradley Gram-Hansen, Indhu Varatharajan, Faiza Azam, Alejandro Coca-Castro, Veronika Kopackova, Piotr Bilinski
- **Comment**: Appeared at the 32nd Conference on Neural Information Processing
  Systems (NeurlPS 2018) Machine Learning for the Developing World (ML4DW)
  Workshop
- **Journal**: NeurlPS workshop on Machine Learning for the Developing World
  (ML4DW), 2018
- **Summary**: Detecting and mapping informal settlements encompasses several of the United Nations sustainable development goals. This is because informal settlements are home to the most socially and economically vulnerable people on the planet. Thus, understanding where these settlements are is of paramount importance to both government and non-government organizations (NGOs), such as the United Nations Children's Fund (UNICEF), who can use this information to deliver effective social and economic aid. We propose a method that detects and maps the locations of informal settlements using only freely available, Sentinel-2 low-resolution satellite spectral data and socio-economic data. This is in contrast to previous studies that only use costly very-high resolution (VHR) satellite and aerial imagery. We show how we can detect informal settlements by combining both domain knowledge and machine learning techniques, to build a classifier that looks for known roofing materials used in informal settlements. Please find additional material at https://frontierdevelopmentlab.github.io/informal-settlements/.



### Void Filling of Digital Elevation Models with Deep Generative Models
- **Arxiv ID**: http://arxiv.org/abs/1811.12693v2
- **DOI**: 10.1109/LGRS.2019.2902222
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.12693v2)
- **Published**: 2018-11-30 10:04:30+00:00
- **Updated**: 2019-02-26 13:24:22+00:00
- **Authors**: Konstantinos Gavriil, Georg Muntingh, Oliver J. D. Barrowclough
- **Comment**: 5 pages; 4 figures; corrected names in references; clarifications
  regarding the two generators in the paper; added reference (Borji 2018) on
  GAN evaluation measures; extended future work discussion; changed (Fig. 4.f)
  to show a failure case
- **Journal**: None
- **Summary**: In recent years, advances in machine learning algorithms, cheap computational resources, and the availability of big data have spurred the deep learning revolution in various application domains. In particular, supervised learning techniques in image analysis have led to superhuman performance in various tasks, such as classification, localization, and segmentation, while unsupervised learning techniques based on increasingly advanced generative models have been applied to generate high-resolution synthetic images indistinguishable from real images.   In this paper we consider a state-of-the-art machine learning model for image inpainting, namely a Wasserstein Generative Adversarial Network based on a fully convolutional architecture with a contextual attention mechanism. We show that this model can successfully be transferred to the setting of digital elevation models (DEMs) for the purpose of generating semantically plausible data for filling voids. Training, testing and experimentation is done on GeoTIFF data from various regions in Norway, made openly available by the Norwegian Mapping Authority.



### An Efficient Image Retrieval Based on Fusion of Low-Level Visual Features
- **Arxiv ID**: http://arxiv.org/abs/1811.12695v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.12695v1)
- **Published**: 2018-11-30 10:11:04+00:00
- **Updated**: 2018-11-30 10:11:04+00:00
- **Authors**: Atif Nazir, Kashif Nazir
- **Comment**: None
- **Journal**: None
- **Summary**: Due to an increase in the number of image achieves, Content-Based Image Retrieval (CBIR) has gained attention for research community of computer vision. The image visual contents are represented in a feature space in the form of numerical values that is considered as a feature vector of image. Images belonging to different classes may contain the common visuals and shapes that can result in the closeness of computed feature space of two different images belonging to separate classes. Due to this reason, feature extraction and image representation is selected with appropriate features as it directly affects the performance of image retrieval system. The commonly used visual features are image spatial layout, color, texture and shape. Image feature space is combined to achieve the discriminating ability that is not possible to achieve when the features are used separately. Due to this reason, in this paper, we aim to explore the low-level feature combination that are based on color and shape features. We selected color moments and color histogram to represent color while shape is represented by using invariant moments. We selected this combination, as these features are reported intuitive, compact and robust for image representation. We evaluated the performance of our proposed research by using the Corel, Coil and Ground Truth (GT) image datasets. We evaluated the proposed low-level feature fusion by calculating the precision, recall and time required for feature extraction. The precision, recall and feature extraction values obtained from the proposed low-level feature fusion outperforms the existing research of CBIR.



### Style Decomposition for Improved Neural Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/1811.12704v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.12704v1)
- **Published**: 2018-11-30 10:33:34+00:00
- **Updated**: 2018-11-30 10:33:34+00:00
- **Authors**: Paraskevas Pegios, Nikolaos Passalis, Anastasios Tefas
- **Comment**: None
- **Journal**: None
- **Summary**: Universal Neural Style Transfer (NST) methods are capable of performing style transfer of arbitrary styles in a style-agnostic manner via feature transforms in (almost) real-time. Even though their unimodal parametric style modeling approach has been proven adequate to transfer a single style from relatively simple images, they are usually not capable of effectively handling more complex styles, producing significant artifacts, as well as reducing the quality of the synthesized textures in the stylized image. To overcome these limitations, in this paper we propose a novel universal NST approach that separately models each sub-style that exists in a given style image (or a collection of style images). This allows for better modeling the subtle style differences within the same style image and then using the most appropriate sub-style (or mixtures of different sub-styles) to stylize the content image. The ability of the proposed approach to a) perform a wide range of different stylizations using the sub-styles that exist in one style image, while giving the ability to the user to appropriate mix the different sub-styles, b) automatically match the most appropriate sub-style to different semantic regions of the content image, improving existing state-of-the-art universal NST approaches, and c) detecting and transferring the sub-styles from collections of images are demonstrated through extensive experiments.



### Mapping Informal Settlements in Developing Countries with Multi-resolution, Multi-spectral Data
- **Arxiv ID**: http://arxiv.org/abs/1812.00812v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.00812v1)
- **Published**: 2018-11-30 10:38:37+00:00
- **Updated**: 2018-11-30 10:38:37+00:00
- **Authors**: Patrick Helber, Bradley Gram-Hansen, Indhu Varatharajan, Faiza Azam, Alejandro Coca-Castro, Veronika Kopackova, Piotr Bilinski
- **Comment**: arXiv admin note: text overlap with arXiv:1812.00786
- **Journal**: None
- **Summary**: Detecting and mapping informal settlements encompasses several of the United Nations sustainable development goals. This is because informal settlements are home to the most socially and economically vulnerable people on the planet. Thus, understanding where these settlements are is of paramount importance to both government and non-government organizations (NGOs), such as the United Nations Children's Fund (UNICEF), who can use this information to deliver effective social and economic aid. We propose two effective methods for detecting and mapping the locations of informal settlements. One uses only low-resolution (LR), freely available, Sentinel-2 multispectral satellite imagery with noisy annotations, whilst the other is a deep learning approach that uses only costly very-high-resolution (VHR) satellite imagery. To our knowledge, we are the first to map informal settlements successfully with low-resolution satellite imagery. We extensively evaluate and compare the proposed methods. Please find additional material at https://frontierdevelopmentlab.github.io/informal-settlements/.



### Evaluating Bayesian Deep Learning Methods for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1811.12709v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.12709v2)
- **Published**: 2018-11-30 10:41:31+00:00
- **Updated**: 2019-03-23 17:00:28+00:00
- **Authors**: Jishnu Mukhoti, Yarin Gal
- **Comment**: Updated baselines and numbers on concrete dropout
- **Journal**: None
- **Summary**: Deep learning has been revolutionary for computer vision and semantic segmentation in particular, with Bayesian Deep Learning (BDL) used to obtain uncertainty maps from deep models when predicting semantic classes. This information is critical when using semantic segmentation for autonomous driving for example. Standard semantic segmentation systems have well-established evaluation metrics. However, with BDL's rising popularity in computer vision we require new metrics to evaluate whether a BDL method produces better uncertainty estimates than another method. In this work we propose three such metrics to evaluate BDL models designed specifically for the task of semantic segmentation. We modify DeepLab-v3+, one of the state-of-the-art deep neural networks, and create its Bayesian counterpart using MC dropout and Concrete dropout as inference techniques. We then compare and test these two inference techniques on the well-known Cityscapes dataset using our suggested metrics. Our results provide new benchmarks for researchers to compare and evaluate their improved uncertainty quantification in pursuit of safer semantic segmentation.



### Improving Landmark Recognition using Saliency detection and Feature classification
- **Arxiv ID**: http://arxiv.org/abs/1811.12748v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.12748v1)
- **Published**: 2018-11-30 11:53:40+00:00
- **Updated**: 2018-11-30 11:53:40+00:00
- **Authors**: Akash Kumar, Sagnik Bhowmick, N. Jayanthi, S. Indu
- **Comment**: Pre-print of the paper to be published in Springer, accepted in the
  proceedings of the in 2nd Workshop on Digital Heritage at the 11th Indian
  Conference on Computer Vision, Graphics and Image Processing
- **Journal**: None
- **Summary**: Image Landmark Recognition has been one of the most sought-after classification challenges in the field of vision and perception. After so many years of generic classification of buildings and monuments from images, people are now focussing upon fine-grained problems - recognizing the category of each building or monument. We proposed an ensemble network for the purpose of classification of Indian Landmark Images. To this end, our method gives robust classification by ensembling the predictions from Graph-Based Visual Saliency (GBVS) network alongwith supervised feature-based classification algorithms such as kNN and Random Forest. The final architecture is an adaptive learning of all the mentioned networks. The proposed network produces a reliable score to eliminate false category cases. Evaluation of our model was done on a new dataset, which involves challenges such as landmark clutter, variable scaling, partial occlusion, etc.



### Domain-Invariant Adversarial Learning for Unsupervised Domain Adaption
- **Arxiv ID**: http://arxiv.org/abs/1811.12751v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.12751v1)
- **Published**: 2018-11-30 12:02:45+00:00
- **Updated**: 2018-11-30 12:02:45+00:00
- **Authors**: Yexun Zhang, Ya Zhang, Yanfeng Wang, Qi Tian
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised domain adaption aims to learn a powerful classifier for the target domain given a labeled source data set and an unlabeled target data set. To alleviate the effect of `domain shift', the major challenge in domain adaptation, studies have attempted to align the distributions of the two domains. Recent research has suggested that generative adversarial network (GAN) has the capability of implicitly capturing data distribution. In this paper, we thus propose a simple but effective model for unsupervised domain adaption leveraging adversarial learning. The same encoder is shared between the source and target domains which is expected to extract domain-invariant representations with the help of an adversarial discriminator. With the labeled source data, we introduce the center loss to increase the discriminative power of feature learned. We further align the conditional distribution of the two domains to enforce the discrimination of the features in the target domain. Unlike previous studies where the source features are extracted with a fixed pre-trained encoder, our method jointly learns feature representations of two domains. Moreover, by sharing the encoder, the model does not need to know the source of images during testing and hence is more widely applicable. We evaluate the proposed method on several unsupervised domain adaption benchmarks and achieve superior or comparable performance to state-of-the-art results.



### Projection Convolutional Neural Networks for 1-bit CNNs via Discrete Back Propagation
- **Arxiv ID**: http://arxiv.org/abs/1811.12755v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.12755v2)
- **Published**: 2018-11-30 12:10:21+00:00
- **Updated**: 2018-12-12 04:46:32+00:00
- **Authors**: Jiaxin Gu, Ce Li, Baochang Zhang, Jungong Han, Xianbin Cao, Jianzhuang Liu, David Doermann
- **Comment**: None
- **Journal**: None
- **Summary**: The advancement of deep convolutional neural networks (DCNNs) has driven significant improvement in the accuracy of recognition systems for many computer vision tasks. However, their practical applications are often restricted in resource-constrained environments. In this paper, we introduce projection convolutional neural networks (PCNNs) with a discrete back propagation via projection (DBPP) to improve the performance of binarized neural networks (BNNs). The contributions of our paper include: 1) for the first time, the projection function is exploited to efficiently solve the discrete back propagation problem, which leads to a new highly compressed CNNs (termed PCNNs); 2) by exploiting multiple projections, we learn a set of diverse quantized kernels that compress the full-precision kernels in a more efficient way than those proposed previously; 3) PCNNs achieve the best classification performance compared to other state-of-the-art BNNs on the ImageNet and CIFAR datasets.



### Multiview Based 3D Scene Understanding On Partial Point Sets
- **Arxiv ID**: http://arxiv.org/abs/1812.01712v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.01712v1)
- **Published**: 2018-11-30 12:23:59+00:00
- **Updated**: 2018-11-30 12:23:59+00:00
- **Authors**: Ye Zhu, Sven Ewan Shepstone, Pablo Martínez-Nuevo, Miklas Strøm Kristoffersen, Fabien Moutarde, Zhuang Fu
- **Comment**: This paper has been submitted to IEEE Transactions on Neural Networks
  and Learning Systems
- **Journal**: None
- **Summary**: Deep learning within the context of point clouds has gained much research interest in recent years mostly due to the promising results that have been achieved on a number of challenging benchmarks, such as 3D shape recognition and scene semantic segmentation. In many realistic settings however, snapshots of the environment are often taken from a single view, which only contains a partial set of the scene due to the field of view restriction of commodity cameras. 3D scene semantic understanding on partial point clouds is considered as a challenging task. In this work, we propose a processing approach for 3D point cloud data based on a multiview representation of the existing 360{\deg} point clouds. By fusing the original 360{\deg} point clouds and their corresponding 3D multiview representations as input data, a neural network is able to recognize partial point sets while improving the general performance on complete point sets, resulting in an overall increase of 31.9% and 4.3% in segmentation accuracy for partial and complete scene semantic understanding, respectively. This method can also be applied in a wider 3D recognition context such as 3D part segmentation.



### Non-Local Video Denoising by CNN
- **Arxiv ID**: http://arxiv.org/abs/1811.12758v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.12758v2)
- **Published**: 2018-11-30 12:24:27+00:00
- **Updated**: 2019-07-02 14:28:56+00:00
- **Authors**: Axel Davy, Thibaud Ehret, Jean-Michel Morel, Pablo Arias, Gabriele Facciolo
- **Comment**: A shorter version of this work has been accepted at ICIP 2019 (A
  NON-LOCAL CNN FOR VIDEO DENOISING). The results of v2 were improved compared
  to v1 and the code was updated accordingly. Code is available at:
  https://github.com/axeldavy/vnlnet
- **Journal**: None
- **Summary**: Non-local patch based methods were until recently state-of-the-art for image denoising but are now outperformed by CNNs. Yet they are still the state-of-the-art for video denoising, as video redundancy is a key factor to attain high denoising performance. The problem is that CNN architectures are hardly compatible with the search for self-similarities. In this work we propose a new and efficient way to feed video self-similarities to a CNN. The non-locality is incorporated into the network via a first non-trainable layer which finds for each patch in the input image its most similar patches in a search region. The central values of these patches are then gathered in a feature vector which is assigned to each image pixel. This information is presented to a CNN which is trained to predict the clean image. We apply the proposed architecture to image and video denoising. For the latter patches are searched for in a 3D spatio-temporal volume. The proposed architecture achieves state-of-the-art results. To the best of our knowledge, this is the first successful application of a CNN to video denoising.



### Model-blind Video Denoising Via Frame-to-frame Training
- **Arxiv ID**: http://arxiv.org/abs/1811.12766v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.12766v3)
- **Published**: 2018-11-30 12:44:50+00:00
- **Updated**: 2020-02-25 15:56:46+00:00
- **Authors**: Thibaud Ehret, Axel Davy, Jean-Michel Morel, Gabriele Facciolo, Pablo Arias
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: Modeling the processing chain that has produced a video is a difficult reverse engineering task, even when the camera is available. This makes model based video processing a still more complex task. In this paper we propose a fully blind video denoising method, with two versions off-line and on-line. This is achieved by fine-tuning a pre-trained AWGN denoising network to the video with a novel frame-to-frame training strategy. Our denoiser can be used without knowledge of the origin of the video or burst and the post processing steps applied from the camera sensor. The on-line process only requires a couple of frames before achieving visually-pleasing results for a wide range of perturbations. It nonetheless reaches state of the art performance for standard Gaussian noise, and can be used off-line with still better performance.



### From Known to the Unknown: Transferring Knowledge to Answer Questions about Novel Visual and Semantic Concepts
- **Arxiv ID**: http://arxiv.org/abs/1811.12772v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.12772v1)
- **Published**: 2018-11-30 13:00:37+00:00
- **Updated**: 2018-11-30 13:00:37+00:00
- **Authors**: Moshiur R Farazi, Salman H Khan, Nick Barnes
- **Comment**: None
- **Journal**: None
- **Summary**: Current Visual Question Answering (VQA) systems can answer intelligent questions about `Known' visual content. However, their performance drops significantly when questions about visually and linguistically `Unknown' concepts are presented during inference (`Open-world' scenario). A practical VQA system should be able to deal with novel concepts in real world settings. To address this problem, we propose an exemplar-based approach that transfers learning (i.e., knowledge) from previously `Known' concepts to answer questions about the `Unknown'. We learn a highly discriminative joint embedding space, where visual and semantic features are fused to give a unified representation. Once novel concepts are presented to the model, it looks for the closest match from an exemplar set in the joint embedding space. This auxiliary information is used alongside the given Image-Question pair to refine visual attention in a hierarchical fashion. Since handling the high dimensional exemplars on large datasets can be a significant challenge, we introduce an efficient matching scheme that uses a compact feature description for search and retrieval. To evaluate our model, we propose a new split for VQA, separating Unknown visual and semantic concepts from the training set. Our approach shows significant improvements over state-of-the-art VQA models on the proposed Open-World VQA dataset and standard VQA datasets.



### Cross-database non-frontal facial expression recognition based on transductive deep transfer learning
- **Arxiv ID**: http://arxiv.org/abs/1811.12774v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.12774v1)
- **Published**: 2018-11-30 13:09:36+00:00
- **Updated**: 2018-11-30 13:09:36+00:00
- **Authors**: Keyu Yan, Wenming Zheng, Tong Zhang, Yuan Zong, Zhen Cui
- **Comment**: None
- **Journal**: None
- **Summary**: Cross-database non-frontal expression recognition is a very meaningful but rather difficult subject in the fields of computer vision and affect computing. In this paper, we proposed a novel transductive deep transfer learning architecture based on widely used VGGface16-Net for this problem. In this framework, the VGGface16-Net is used to jointly learn an common optimal nonlinear discriminative features from the non-frontal facial expression samples between the source and target databases and then we design a novel transductive transfer layer to deal with the cross-database non-frontal facial expression classification task. In order to validate the performance of the proposed transductive deep transfer learning networks, we present extensive crossdatabase experiments on two famous available facial expression databases, namely the BU-3DEF and the Multi-PIE database. The final experimental results show that our transductive deep transfer network outperforms the state-of-the-art cross-database facial expression recognition methods.



### Efficient Neural Network Compression
- **Arxiv ID**: http://arxiv.org/abs/1811.12781v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.12781v3)
- **Published**: 2018-11-30 13:17:12+00:00
- **Updated**: 2019-04-12 11:53:58+00:00
- **Authors**: Hyeji Kim, Muhammad Umar Karim Khan, Chong-Min Kyung
- **Comment**: None
- **Journal**: None
- **Summary**: Network compression reduces the computational complexity and memory consumption of deep neural networks by reducing the number of parameters. In SVD-based network compression, the right rank needs to be decided for every layer of the network. In this paper, we propose an efficient method for obtaining the rank configuration of the whole network. Unlike previous methods which consider each layer separately, our method considers the whole network to choose the right rank configuration. We propose novel accuracy metrics to represent the accuracy and complexity relationship for a given neural network. We use these metrics in a non-iterative fashion to obtain the right rank configuration which satisfies the constraints on FLOPs and memory while maintaining sufficient accuracy. Experiments show that our method provides better compromise between accuracy and computational complexity/memory consumption while performing compression at much higher speed. For VGG-16 our network can reduce the FLOPs by 25% and improve accuracy by 0.7% compared to the baseline, while requiring only 3 minutes on a CPU to search for the right rank configuration. Previously, similar results were achieved in 4 hours with 8 GPUs. The proposed method can be used for lossless compression of a neural network as well. The better accuracy and complexity compromise, as well as the extremely fast speed of our method makes it suitable for neural network compression.



### The GAN that Warped: Semantic Attribute Editing with Unpaired Data
- **Arxiv ID**: http://arxiv.org/abs/1811.12784v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.12784v4)
- **Published**: 2018-11-30 13:26:38+00:00
- **Updated**: 2020-03-05 16:01:39+00:00
- **Authors**: Garoe Dorta, Sara Vicente, Neill D. F. Campbell, Ivor J. A. Simpson
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: Deep neural networks have recently been used to edit images with great success, in particular for faces. However, they are often limited to only being able to work at a restricted range of resolutions. Many methods are so flexible that face edits can often result in an unwanted loss of identity. This work proposes to learn how to perform semantic image edits through the application of smooth warp fields. Previous approaches that attempted to use warping for semantic edits required paired data, i.e. example images of the same subject with different semantic attributes. In contrast, we employ recent advances in Generative Adversarial Networks that allow our model to be trained with unpaired data. We demonstrate face editing at very high resolutions (4k images) with a single forward pass of a deep network at a lower resolution. We also show that our edits are substantially better at preserving the subject's identity. The robustness of our approach is demonstrated by showing plausible image editing results on the Cub200 birds dataset. To our knowledge this has not been previously accomplished, due the challenging nature of the dataset.



### TextMountain: Accurate Scene Text Detection via Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1811.12786v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.12786v1)
- **Published**: 2018-11-30 13:28:41+00:00
- **Updated**: 2018-11-30 13:28:41+00:00
- **Authors**: Yixing Zhu, Jun Du
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel scene text detection method named TextMountain. The key idea of TextMountain is making full use of border-center information. Different from previous works that treat center-border as a binary classification problem, we predict text center-border probability (TCBP) and text center-direction (TCD). The TCBP is just like a mountain whose top is text center and foot is text border. The mountaintop can separate text instances which cannot be easily achieved using semantic segmentation map and its rising direction can plan a road to top for each pixel on mountain foot at the group stage. The TCD helps TCBP learning better. Our label rules will not lead to the ambiguous problem with the transformation of angle, so the proposed method is robust to multi-oriented text and can also handle well with curved text. In inference stage, each pixel at the mountain foot needs to search the path to the mountaintop and this process can be efficiently completed in parallel, yielding the efficiency of our method compared with others. The experiments on MLT, ICDAR2015, RCTW-17 and SCUT-CTW1500 databases demonstrate that the proposed method achieves better or comparable performance in terms of both accuracy and efficiency. It is worth mentioning our method achieves an F-measure of 76.85% on MLT which outperforms the previous methods by a large margin. Code will be made available.



### iW-Net: an automatic and minimalistic interactive lung nodule segmentation deep network
- **Arxiv ID**: http://arxiv.org/abs/1811.12789v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.12789v1)
- **Published**: 2018-11-30 13:43:03+00:00
- **Updated**: 2018-11-30 13:43:03+00:00
- **Authors**: Guilherme Aresta, Colin Jacobs, Teresa Araújo, António Cunha, Isabel Ramos, Bram van Ginneken, Aurélio Campilho
- **Comment**: Pre-print submitted to IEEE Transactions on Biomedical Engineering
- **Journal**: None
- **Summary**: We propose iW-Net, a deep learning model that allows for both automatic and interactive segmentation of lung nodules in computed tomography images. iW-Net is composed of two blocks: the first one provides an automatic segmentation and the second one allows to correct it by analyzing 2 points introduced by the user in the nodule's boundary. For this purpose, a physics inspired weight map that takes the user input into account is proposed, which is used both as a feature map and in the system's loss function. Our approach is extensively evaluated on the public LIDC-IDRI dataset, where we achieve a state-of-the-art performance of 0.55 intersection over union vs the 0.59 inter-observer agreement. Also, we show that iW-Net allows to correct the segmentation of small nodules, essential for proper patient referral decision, as well as improve the segmentation of the challenging non-solid nodules and thus may be an important tool for increasing the early diagnosis of lung cancer.



### Structure and Motion from Multiframes
- **Arxiv ID**: http://arxiv.org/abs/1811.12797v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.12797v1)
- **Published**: 2018-11-30 14:05:37+00:00
- **Updated**: 2018-11-30 14:05:37+00:00
- **Authors**: Mieczysław A. Kłopotek
- **Comment**: 7 figures, 20 pages
- **Journal**: M.A. K{\l}opotek: Structure and Motion from Multiframes. Machine
  Graphics and Vision , Vol. 7, nos 1/2, 1998,pp. 383-396
- **Summary**: The paper gives an overview of the problems and methods of recovery of structure and motion parameters of rigid bodies from multiframes.



### Real Time Bangladeshi Sign Language Detection using Faster R-CNN
- **Arxiv ID**: http://arxiv.org/abs/1811.12813v1
- **DOI**: 10.1109/CIET.2018.8660780
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.12813v1)
- **Published**: 2018-11-30 14:25:04+00:00
- **Updated**: 2018-11-30 14:25:04+00:00
- **Authors**: Oishee Bintey Hoque, Mohammad Imrul Jubair, Md. Saiful Islam, Al-Farabi Akash, Alvin Sachie Paulson
- **Comment**: 6 pages, Accepted in International Conference on Innovation in
  Engineering and Technology (ICIET) 27-29 December, 2018, Dhaka, Bangladesh
- **Journal**: None
- **Summary**: Bangladeshi Sign Language (BdSL) is a commonly used medium of communication for the hearing-impaired people in Bangladesh. Developing a real time system to detect these signs from images is a great challenge. In this paper, we present a technique to detect BdSL from images that performs in real time. Our method uses Convolutional Neural Network based object detection technique to detect the presence of signs in the image region and to recognize its class. For this purpose, we adopted Faster Region-based Convolutional Network approach and developed a dataset $-$ BdSLImset $-$ to train our system. Previous research works in detecting BdSL generally depend on external devices while most of the other vision-based techniques do not perform efficiently in real time. Our approach, however, is free from such limitations and the experimental results demonstrate that the proposed method successfully identifies and recognizes Bangladeshi signs in real time.



### Graph-Based Global Reasoning Networks
- **Arxiv ID**: http://arxiv.org/abs/1811.12814v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.12814v1)
- **Published**: 2018-11-30 14:28:04+00:00
- **Updated**: 2018-11-30 14:28:04+00:00
- **Authors**: Yunpeng Chen, Marcus Rohrbach, Zhicheng Yan, Shuicheng Yan, Jiashi Feng, Yannis Kalantidis
- **Comment**: None
- **Journal**: None
- **Summary**: Globally modeling and reasoning over relations between regions can be beneficial for many computer vision tasks on both images and videos. Convolutional Neural Networks (CNNs) excel at modeling local relations by convolution operations, but they are typically inefficient at capturing global relations between distant regions and require stacking multiple convolution layers. In this work, we propose a new approach for reasoning globally in which a set of features are globally aggregated over the coordinate space and then projected to an interaction space where relational reasoning can be efficiently computed. After reasoning, relation-aware features are distributed back to the original coordinate space for down-stream tasks. We further present a highly efficient instantiation of the proposed approach and introduce the Global Reasoning unit (GloRe unit) that implements the coordinate-interaction space mapping by weighted global pooling and weighted broadcasting, and the relation reasoning via graph convolution on a small graph in interaction space. The proposed GloRe unit is lightweight, end-to-end trainable and can be easily plugged into existing CNNs for a wide range of tasks. Extensive experiments show our GloRe unit can consistently boost the performance of state-of-the-art backbone architectures, including ResNet, ResNeXt, SE-Net and DPN, for both 2D and 3D CNNs, on image classification, semantic segmentation and video action recognition task.



### Practical Full Resolution Learned Lossless Image Compression
- **Arxiv ID**: http://arxiv.org/abs/1811.12817v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.12817v3)
- **Published**: 2018-11-30 14:32:47+00:00
- **Updated**: 2020-03-06 15:57:56+00:00
- **Authors**: Fabian Mentzer, Eirikur Agustsson, Michael Tschannen, Radu Timofte, Luc Van Gool
- **Comment**: Updated preprocessing and Table 1, see A.1 in supplementary. Code and
  models: https://github.com/fab-jul/L3C-PyTorch
- **Journal**: None
- **Summary**: We propose the first practical learned lossless image compression system, L3C, and show that it outperforms the popular engineered codecs, PNG, WebP and JPEG 2000. At the core of our method is a fully parallelizable hierarchical probabilistic model for adaptive entropy coding which is optimized end-to-end for the compression task. In contrast to recent autoregressive discrete probabilistic models such as PixelCNN, our method i) models the image distribution jointly with learned auxiliary representations instead of exclusively modeling the image distribution in RGB space, and ii) only requires three forward-passes to predict all pixel probabilities instead of one for each pixel. As a result, L3C obtains over two orders of magnitude speedups when sampling compared to the fastest PixelCNN variant (Multiscale-PixelCNN). Furthermore, we find that learning the auxiliary representation is crucial and outperforms predefined auxiliary representations such as an RGB pyramid significantly.



### ADVENT: Adversarial Entropy Minimization for Domain Adaptation in Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1811.12833v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.12833v2)
- **Published**: 2018-11-30 15:00:29+00:00
- **Updated**: 2019-04-17 19:16:22+00:00
- **Authors**: Tuan-Hung Vu, Himalaya Jain, Maxime Bucher, Matthieu Cord, Patrick Pérez
- **Comment**: Accepted in CVPR'19. Code is available at
  https://github.com/valeoai/ADVENT
- **Journal**: None
- **Summary**: Semantic segmentation is a key problem for many computer vision tasks. While approaches based on convolutional neural networks constantly break new records on different benchmarks, generalizing well to diverse testing environments remains a major challenge. In numerous real world applications, there is indeed a large gap between data distributions in train and test domains, which results in severe performance loss at run-time. In this work, we address the task of unsupervised domain adaptation in semantic segmentation with losses based on the entropy of the pixel-wise predictions. To this end, we propose two novel, complementary methods using (i) entropy loss and (ii) adversarial loss respectively. We demonstrate state-of-the-art performance in semantic segmentation on two challenging "synthetic-2-real" set-ups and show that the approach can also be used for detection.



### Super-Resolution via Image-Adapted Denoising CNNs: Incorporating External and Internal Learning
- **Arxiv ID**: http://arxiv.org/abs/1811.12866v3
- **DOI**: 10.1109/LSP.2019.2920250
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.12866v3)
- **Published**: 2018-11-30 16:15:19+00:00
- **Updated**: 2019-05-29 10:50:58+00:00
- **Authors**: Tom Tirer, Raja Giryes
- **Comment**: Accepted to IEEE Signal Processing Letters (extended version)
- **Journal**: None
- **Summary**: While deep neural networks exhibit state-of-the-art results in the task of image super-resolution (SR) with a fixed known acquisition process (e.g., a bicubic downscaling kernel), they experience a huge performance loss when the real observation model mismatches the one used in training. Recently, two different techniques suggested to mitigate this deficiency, i.e., enjoy the advantages of deep learning without being restricted by the training phase. The first one follows the plug-and-play (P&P) approach that solves general inverse problems (e.g., SR) by using Gaussian denoisers for handling the prior term in model-based optimization schemes. The second builds on internal recurrence of information inside a single image, and trains a super-resolver network at test time on examples synthesized from the low-resolution image. Our work incorporates these two independent strategies, enjoying the impressive generalization capabilities of deep learning, captured by the first, and further improving it through internal learning at test time. First, we apply a recent P&P strategy to SR. Then, we show how it may become image-adaptive in test time. This technique outperforms the above two strategies on popular datasets and gives better results than other state-of-the-art methods in practical cases where the observation model is inexact or unknown in advance.



### Image-based model parameter optimization using Model-Assisted Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1812.00879v2
- **DOI**: 10.1109/TNNLS.2020.2969327
- **Categories**: **cs.CV**, cs.LG, hep-ex, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.00879v2)
- **Published**: 2018-11-30 17:27:53+00:00
- **Updated**: 2020-03-12 08:58:22+00:00
- **Authors**: Saúl Alonso-Monsalve, Leigh H. Whitehead
- **Comment**: None
- **Journal**: None
- **Summary**: We propose and demonstrate the use of a model-assisted generative adversarial network (GAN) to produce fake images that accurately match true images through the variation of the parameters of the model that describes the features of the images. The generator learns the model parameter values that produce fake images that best match the true images. Two case studies show excellent agreement between the generated best match parameters and the true parameters. The best match model parameter values can be used to retune the default simulation to minimize any bias when applying image recognition techniques to fake and true images. In the case of a real-world experiment, the true images are experimental data with unknown true model parameter values, and the fake images are produced by a simulation that takes the model parameters as input. The model-assisted GAN uses a convolutional neural network to emulate the simulation for all parameter values that, when trained, can be used as a conditional generator for fast fake-image production.



### Improving Traffic Safety Through Video Analysis in Jakarta, Indonesia
- **Arxiv ID**: http://arxiv.org/abs/1812.01106v1
- **DOI**: 10.1007/978-3-030-29513-4_48
- **Categories**: **cs.CY**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.01106v1)
- **Published**: 2018-11-30 18:51:16+00:00
- **Updated**: 2018-11-30 18:51:16+00:00
- **Authors**: João Caldeira, Alex Fout, Aniket Kesari, Raesetje Sefala, Joseph Walsh, Katy Dupre, Muhammad Rizal Khaefi, Setiaji, George Hodge, Zakiya Aryana Pramestri, Muhammad Adib Imtiyazi
- **Comment**: 6 pages; LaTeX; Presented at NeurIPS 2018 Workshop on Machine
  Learning for the Developing World; Presented at NeurIPS 2018 Workshop on AI
  for Social Good
- **Journal**: Proceedings of the 2019 Intelligent Systems Conference
  (IntelliSys) Volume 2, 642-649
- **Summary**: This project presents the results of a partnership between the Data Science for Social Good fellowship, Jakarta Smart City and Pulse Lab Jakarta to create a video analysis pipeline for the purpose of improving traffic safety in Jakarta. The pipeline transforms raw traffic video footage into databases that are ready to be used for traffic analysis. By analyzing these patterns, the city of Jakarta will better understand how human behavior and built infrastructure contribute to traffic challenges and safety risks. The results of this work should also be broadly applicable to smart city initiatives around the globe as they improve urban planning and sustainability through data science approaches.



### TextureNet: Consistent Local Parametrizations for Learning from High-Resolution Signals on Meshes
- **Arxiv ID**: http://arxiv.org/abs/1812.00020v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.00020v2)
- **Published**: 2018-11-30 19:01:09+00:00
- **Updated**: 2019-03-28 05:36:09+00:00
- **Authors**: Jingwei Huang, Haotian Zhang, Li Yi, Thomas Funkhouser, Matthias Nießner, Leonidas Guibas
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce, TextureNet, a neural network architecture designed to extract features from high-resolution signals associated with 3D surface meshes (e.g., color texture maps). The key idea is to utilize a 4-rotational symmetric (4-RoSy) field to define a domain for convolution on a surface. Though 4-RoSy fields have several properties favorable for convolution on surfaces (low distortion, few singularities, consistent parameterization, etc.), orientations are ambiguous up to 4-fold rotation at any sample point. So, we introduce a new convolutional operator invariant to the 4-RoSy ambiguity and use it in a network to extract features from high-resolution signals on geodesic neighborhoods of a surface. In comparison to alternatives, such as PointNet based methods which lack a notion of orientation, the coherent structure given by these neighborhoods results in significantly stronger features. As an example application, we demonstrate the benefits of our architecture for 3D semantic segmentation of textured 3D meshes. The results show that our method outperforms all existing methods on the basis of mean IoU by a significant margin in both geometry-only (6.4%) and RGB+Geometry (6.9-8.2%) settings.



### Learning from a tiny dataset of manual annotations: a teacher/student approach for surgical phase recognition
- **Arxiv ID**: http://arxiv.org/abs/1812.00033v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.00033v3)
- **Published**: 2018-11-30 19:50:05+00:00
- **Updated**: 2020-09-30 14:22:43+00:00
- **Authors**: Tong Yu, Didier Mutter, Jacques Marescaux, Nicolas Padoy
- **Comment**: Accepted at IPCAI 2019
- **Journal**: None
- **Summary**: Vision algorithms capable of interpreting scenes from a real-time video stream are necessary for computer-assisted surgery systems to achieve context-aware behavior. In laparoscopic procedures one particular algorithm needed for such systems is the identification of surgical phases, for which the current state of the art is a model based on a CNN-LSTM. A number of previous works using models of this kind have trained them in a fully supervised manner, requiring a fully annotated dataset. Instead, our work confronts the problem of learning surgical phase recognition in scenarios presenting scarce amounts of annotated data (under 25% of all available video recordings). We propose a teacher/student type of approach, where a strong predictor called the teacher, trained beforehand on a small dataset of ground truth-annotated videos, generates synthetic annotations for a larger dataset, which another model - the student - learns from. In our case, the teacher features a novel CNN-biLSTM-CRF architecture, designed for offline inference only. The student, on the other hand, is a CNN-LSTM capable of making real-time predictions. Results for various amounts of manually annotated videos demonstrate the superiority of the new CNN-biLSTM-CRF predictor as well as improved performance from the CNN-LSTM trained using synthetic labels generated for unannotated videos. For both offline and online surgical phase recognition with very few annotated recordings available, this new teacher/student strategy provides a valuable performance improvement by efficiently leveraging the unannotated data.



### Adversarial Defense by Stratified Convolutional Sparse Coding
- **Arxiv ID**: http://arxiv.org/abs/1812.00037v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.00037v2)
- **Published**: 2018-11-30 20:01:15+00:00
- **Updated**: 2019-06-21 16:06:41+00:00
- **Authors**: Bo Sun, Nian-hsuan Tsai, Fangchen Liu, Ronald Yu, Hao Su
- **Comment**: Published at CVPR 2019
- **Journal**: None
- **Summary**: We propose an adversarial defense method that achieves state-of-the-art performance among attack-agnostic adversarial defense methods while also maintaining robustness to input resolution, scale of adversarial perturbation, and scale of dataset size. Based on convolutional sparse coding, we construct a stratified low-dimensional quasi-natural image space that faithfully approximates the natural image space while also removing adversarial perturbations. We introduce a novel Sparse Transformation Layer (STL) in between the input image and the first layer of the neural network to efficiently project images into our quasi-natural image space. Our experiments show state-of-the-art performance of our method compared to other attack-agnostic adversarial defense methods in various adversarial settings.



### MAN: Moment Alignment Network for Natural Language Moment Retrieval via Iterative Graph Adjustment
- **Arxiv ID**: http://arxiv.org/abs/1812.00087v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.00087v2)
- **Published**: 2018-11-30 23:04:10+00:00
- **Updated**: 2019-05-17 23:15:57+00:00
- **Authors**: Da Zhang, Xiyang Dai, Xin Wang, Yuan-Fang Wang, Larry S. Davis
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: This research strives for natural language moment retrieval in long, untrimmed video streams. The problem is not trivial especially when a video contains multiple moments of interests and the language describes complex temporal dependencies, which often happens in real scenarios. We identify two crucial challenges: semantic misalignment and structural misalignment. However, existing approaches treat different moments separately and do not explicitly model complex moment-wise temporal relations. In this paper, we present Moment Alignment Network (MAN), a novel framework that unifies the candidate moment encoding and temporal structural reasoning in a single-shot feed-forward network. MAN naturally assigns candidate moment representations aligned with language semantics over different temporal locations and scales. Most importantly, we propose to explicitly model moment-wise temporal relations as a structured graph and devise an iterative graph adjustment network to jointly learn the best structure in an end-to-end manner. We evaluate the proposed approach on two challenging public benchmarks DiDeMo and Charades-STA, where our MAN significantly outperforms the state-of-the-art by a large margin.



### Mixed Precision Quantization of ConvNets via Differentiable Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/1812.00090v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.00090v1)
- **Published**: 2018-11-30 23:15:45+00:00
- **Updated**: 2018-11-30 23:15:45+00:00
- **Authors**: Bichen Wu, Yanghan Wang, Peizhao Zhang, Yuandong Tian, Peter Vajda, Kurt Keutzer
- **Comment**: None
- **Journal**: None
- **Summary**: Recent work in network quantization has substantially reduced the time and space complexity of neural network inference, enabling their deployment on embedded and mobile devices with limited computational and memory resources. However, existing quantization methods often represent all weights and activations with the same precision (bit-width). In this paper, we explore a new dimension of the design space: quantizing different layers with different bit-widths. We formulate this problem as a neural architecture search problem and propose a novel differentiable neural architecture search (DNAS) framework to efficiently explore its exponential search space with gradient-based optimization. Experiments show we surpass the state-of-the-art compression of ResNet on CIFAR-10 and ImageNet. Our quantized models with 21.1x smaller model size or 103.9x lower computational cost can still outperform baseline quantized or even full precision models.



### Understanding Unequal Gender Classification Accuracy from Face Images
- **Arxiv ID**: http://arxiv.org/abs/1812.00099v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.00099v1)
- **Published**: 2018-11-30 23:47:52+00:00
- **Updated**: 2018-11-30 23:47:52+00:00
- **Authors**: Vidya Muthukumar, Tejaswini Pedapati, Nalini Ratha, Prasanna Sattigeri, Chai-Wah Wu, Brian Kingsbury, Abhishek Kumar, Samuel Thomas, Aleksandra Mojsilovic, Kush R. Varshney
- **Comment**: None
- **Journal**: None
- **Summary**: Recent work shows unequal performance of commercial face classification services in the gender classification task across intersectional groups defined by skin type and gender. Accuracy on dark-skinned females is significantly worse than on any other group. In this paper, we conduct several analyses to try to uncover the reason for this gap. The main finding, perhaps surprisingly, is that skin type is not the driver. This conclusion is reached via stability experiments that vary an image's skin type via color-theoretic methods, namely luminance mode-shift and optimal transport. A second suspect, hair length, is also shown not to be the driver via experiments on face images cropped to exclude the hair. Finally, using contrastive post-hoc explanation techniques for neural networks, we bring forth evidence suggesting that differences in lip, eye and cheek structure across ethnicity lead to the differences. Further, lip and eye makeup are seen as strong predictors for a female face, which is a troubling propagation of a gender stereotype.



### DVC: An End-to-end Deep Video Compression Framework
- **Arxiv ID**: http://arxiv.org/abs/1812.00101v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1812.00101v3)
- **Published**: 2018-11-30 23:55:31+00:00
- **Updated**: 2019-04-07 05:41:45+00:00
- **Authors**: Guo Lu, Wanli Ouyang, Dong Xu, Xiaoyun Zhang, Chunlei Cai, Zhiyong Gao
- **Comment**: Accepted by CVPR 2019. Project page https://github.com/GuoLusjtu/DVC
- **Journal**: None
- **Summary**: Conventional video compression approaches use the predictive coding architecture and encode the corresponding motion information and residual information. In this paper, taking advantage of both classical architecture in the conventional video compression method and the powerful non-linear representation ability of neural networks, we propose the first end-to-end video compression deep model that jointly optimizes all the components for video compression. Specifically, learning based optical flow estimation is utilized to obtain the motion information and reconstruct the current frames. Then we employ two auto-encoder style neural networks to compress the corresponding motion and residual information. All the modules are jointly learned through a single loss function, in which they collaborate with each other by considering the trade-off between reducing the number of compression bits and improving quality of the decoded video. Experimental results show that the proposed approach can outperform the widely used video coding standard H.264 in terms of PSNR and be even on par with the latest standard H.265 in terms of MS-SSIM. Code is released at https://github.com/GuoLusjtu/DVC.



