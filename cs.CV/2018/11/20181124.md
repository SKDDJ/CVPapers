# Arxiv Papers in cs.CV on 2018-11-24
### Generate, Segment and Refine: Towards Generic Manipulation Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1811.09729v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.09729v3)
- **Published**: 2018-11-24 00:06:10+00:00
- **Updated**: 2019-09-11 18:29:06+00:00
- **Authors**: Peng Zhou, Bor-Chun Chen, Xintong Han, Mahyar Najibi, Abhinav Shrivastava, Ser Nam Lim, Larry S. Davis
- **Comment**: None
- **Journal**: AAAI-2020
- **Summary**: Detecting manipulated images has become a significant emerging challenge. The advent of image sharing platforms and the easy availability of advanced photo editing software have resulted in a large quantities of manipulated images being shared on the internet. While the intent behind such manipulations varies widely, concerns on the spread of fake news and misinformation is growing. Current state of the art methods for detecting these manipulated images suffers from the lack of training data due to the laborious labeling process. We address this problem in this paper, for which we introduce a manipulated image generation process that creates true positives using currently available datasets. Drawing from traditional work on image blending, we propose a novel generator for creating such examples. In addition, we also propose to further create examples that force the algorithm to focus on boundary artifacts during training. Strong experimental results validate our proposal.



### Divergence Prior and Vessel-tree Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1811.09745v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.09745v1)
- **Published**: 2018-11-24 02:14:43+00:00
- **Updated**: 2018-11-24 02:14:43+00:00
- **Authors**: Zhongwen Zhang, Egor Chesakov, Dmitrii Marin, Yuri Boykov
- **Comment**: 10 pages, 18 figures
- **Journal**: None
- **Summary**: We propose a new geometric regularization principle for reconstructing vector fields based on prior knowledge about their divergence. As one important example of this general idea, we focus on vector fields modelling blood flow pattern that should be divergent in arteries and convergent in veins. We show that this previously ignored regularization constraint can significantly improve the quality of vessel tree reconstruction particularly around bifurcations where non-zero divergence is concentrated. Our divergence prior is critical for resolving (binary) sign ambiguity in flow orientations produced by standard vessel filters, e.g. Frangi. Our vessel tree centerline reconstruction combines divergence constraints with robust curvature regularization. Our unsupervised method can reconstruct complete vessel trees with near-capillary details on synthetic and real 3D volumes.



### Automating Motion Correction in Multishot MRI Using Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1811.09750v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.09750v1)
- **Published**: 2018-11-24 03:13:27+00:00
- **Updated**: 2018-11-24 03:13:27+00:00
- **Authors**: Siddique Latif, Muhammad Asim, Muhammad Usman, Junaid Qadir, Rajib Rana
- **Comment**: None
- **Journal**: MED-NIPS 2018
- **Summary**: Multishot Magnetic Resonance Imaging (MRI) has recently gained popularity as it accelerates the MRI data acquisition process without compromising the quality of final MR image. However, it suffers from motion artifacts caused by patient movements which may lead to misdiagnosis. Modern state-of-the-art motion correction techniques are able to counter small degree motion, however, their adoption is hindered by their time complexity. This paper proposes a Generative Adversarial Network (GAN) for reconstructing motion free high-fidelity images while reducing the image reconstruction time by an impressive two orders of magnitude.



### Mean Local Group Average Precision (mLGAP): A New Performance Metric for Hashing-based Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1811.09763v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.09763v1)
- **Published**: 2018-11-24 04:31:41+00:00
- **Updated**: 2018-11-24 04:31:41+00:00
- **Authors**: Pak Lun Kevin Ding, Yikang Li, Baoxin Li
- **Comment**: None
- **Journal**: None
- **Summary**: The research on hashing techniques for visual data is gaining increased attention in recent years due to the need for compact representations supporting efficient search/retrieval in large-scale databases such as online images. Among many possibilities, Mean Average Precision(mAP) has emerged as the dominant performance metric for hashing-based retrieval. One glaring shortcoming of mAP is its inability in balancing retrieval accuracy and utilization of hash codes: pushing a system to attain higher mAP will inevitably lead to poorer utilization of the hash codes. Poor utilization of the hash codes hinders good retrieval because of increased collision of samples in the hash space. This means that a model giving a higher mAP values does not necessarily do a better job in retrieval. In this paper, we introduce a new metric named Mean Local Group Average Precision (mLGAP) for better evaluation of the performance of hashing-based retrieval. The new metric provides a retrieval performance measure that also reconciles the utilization of hash codes, leading to a more practically meaningful performance metric than conventional ones like mAP. To this end, we start by mathematical analysis of the deficiencies of mAP for hashing-based retrieval. We then propose mLGAP and show why it is more appropriate for hashing-based retrieval. Experiments on image retrieval are used to demonstrate the effectiveness of the proposed metric.



### A^2Net: Adjacent Aggregation Networks for Image Raindrop Removal
- **Arxiv ID**: http://arxiv.org/abs/1811.09780v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.09780v1)
- **Published**: 2018-11-24 07:08:31+00:00
- **Updated**: 2018-11-24 07:08:31+00:00
- **Authors**: Huangxing Lin, Xueyang Fu, Changxing Jing, Xinghao Ding, Yue Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Existing methods for single images raindrop removal either have poor robustness or suffer from parameter burdens. In this paper, we propose a new Adjacent Aggregation Network (A^2Net) with lightweight architectures to remove raindrops from single images. Instead of directly cascading convolutional layers, we design an adjacent aggregation architecture to better fuse features for rich representations generation, which can lead to high quality images reconstruction. To further simplify the learning process, we utilize a problem-specific knowledge to force the network focus on the luminance channel in the YUV color space instead of all RGB channels. By combining adjacent aggregating operation with color space transformation, the proposed A^2Net can achieve state-of-the-art performances on raindrop removal with significant parameters reduction.



### What and Where: A Context-based Recommendation System for Object Insertion
- **Arxiv ID**: http://arxiv.org/abs/1811.09783v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.09783v1)
- **Published**: 2018-11-24 07:48:35+00:00
- **Updated**: 2018-11-24 07:48:35+00:00
- **Authors**: Song-Hai Zhang, Zhengping Zhou, Bin Liu, Xin Dong, Dun Liang, Peter Hall, Shi-Min Hu
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we propose a novel topic consisting of two dual tasks: 1) given a scene, recommend objects to insert, 2) given an object category, retrieve suitable background scenes. A bounding box for the inserted object is predicted in both tasks, which helps downstream applications such as semi-automated advertising and video composition. The major challenge lies in the fact that the target object is neither present nor localized at test time, whereas available datasets only provide scenes with existing objects. To tackle this problem, we build an unsupervised algorithm based on object-level contexts, which explicitly models the joint probability distribution of object categories and bounding boxes with a Gaussian mixture model. Experiments on our newly annotated test set demonstrate that our system outperforms existing baselines on all subtasks, and do so under a unified framework. Our contribution promises future extensions and applications.



### Senti-Attend: Image Captioning using Sentiment and Attention
- **Arxiv ID**: http://arxiv.org/abs/1811.09789v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.09789v1)
- **Published**: 2018-11-24 08:47:16+00:00
- **Updated**: 2018-11-24 08:47:16+00:00
- **Authors**: Omid Mohamad Nezami, Mark Dras, Stephen Wan, Cecile Paris
- **Comment**: None
- **Journal**: None
- **Summary**: There has been much recent work on image captioning models that describe the factual aspects of an image. Recently, some models have incorporated non-factual aspects into the captions, such as sentiment or style. However, such models typically have difficulty in balancing the semantic aspects of the image and the non-factual dimensions of the caption; in addition, it can be observed that humans may focus on different aspects of an image depending on the chosen sentiment or style of the caption. To address this, we design an attention-based model to better add sentiment to image captions. The model embeds and learns sentiment with respect to image-caption data, and uses both high-level and word-level sentiment information during the learning process. The model outperforms the state-of-the-art work in image captioning with sentiment using standard evaluation metrics. An analysis of generated captions also shows that our model does this by a better selection of the sentiment-bearing adjectives and adjective-noun pairs.



### Spatio-Temporal Road Scene Reconstruction using Superpixel Markov Random Field
- **Arxiv ID**: http://arxiv.org/abs/1811.09790v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.09790v3)
- **Published**: 2018-11-24 08:48:39+00:00
- **Updated**: 2019-05-20 09:46:05+00:00
- **Authors**: Yaochen Li, Yuehu Liu, Jihua Zhu, Shiqi Ma, Zhenning Niu, Rui Guo
- **Comment**: None
- **Journal**: None
- **Summary**: Scene model construction based on image rendering is an indispensable but challenging technique in computer vision and intelligent transportation systems. In this paper, we propose a framework for constructing 3D corridor-based road scene models. This consists of two successive stages: road detection and scene construction. The road detection is realized by a new superpixel Markov random field (MRF) algorithm. The data fidelity term in the MRF's energy function is jointly computed according to the superpixel features of color, texture and location. The smoothness term is established on the basis of the interaction of spatio-temporally adjacent superpixels. In the subsequent scene construction, the foreground and background regions are modeled independently. Experiments for road detection demonstrate the proposed method outperforms the state-of-the-art in both accuracy and speed. The scene construction experiments confirm that the proposed scene models show better correctness ratios, and have the potential to support a range of applications.



### Discriminative Feature Learning for Unsupervised Video Summarization
- **Arxiv ID**: http://arxiv.org/abs/1811.09791v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.09791v1)
- **Published**: 2018-11-24 08:49:06+00:00
- **Updated**: 2018-11-24 08:49:06+00:00
- **Authors**: Yunjae Jung, Donghyeon Cho, Dahun Kim, Sanghyun Woo, In So Kweon
- **Comment**: Accepted to AAAI 2019 !!!
- **Journal**: None
- **Summary**: In this paper, we address the problem of unsupervised video summarization that automatically extracts key-shots from an input video. Specifically, we tackle two critical issues based on our empirical observations: (i) Ineffective feature learning due to flat distributions of output importance scores for each frame, and (ii) training difficulty when dealing with long-length video inputs. To alleviate the first problem, we propose a simple yet effective regularization loss term called variance loss. The proposed variance loss allows a network to predict output scores for each frame with high discrepancy which enables effective feature learning and significantly improves model performance. For the second problem, we design a novel two-stream network named Chunk and Stride Network (CSNet) that utilizes local (chunk) and global (stride) temporal view on the video features. Our CSNet gives better summarization results for long-length videos compared to the existing methods. In addition, we introduce an attention mechanism to handle the dynamic information in videos. We demonstrate the effectiveness of the proposed methods by conducting extensive ablation studies and show that our final model achieves new state-of-the-art results on two benchmark datasets.



### Self-Supervised Video Representation Learning with Space-Time Cubic Puzzles
- **Arxiv ID**: http://arxiv.org/abs/1811.09795v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.09795v1)
- **Published**: 2018-11-24 09:08:08+00:00
- **Updated**: 2018-11-24 09:08:08+00:00
- **Authors**: Dahun Kim, Donghyeon Cho, In So Kweon
- **Comment**: Accepted to AAAI 2019
- **Journal**: None
- **Summary**: Self-supervised tasks such as colorization, inpainting and zigsaw puzzle have been utilized for visual representation learning for still images, when the number of labeled images is limited or absent at all. Recently, this worthwhile stream of study extends to video domain where the cost of human labeling is even more expensive. However, the most of existing methods are still based on 2D CNN architectures that can not directly capture spatio-temporal information for video applications. In this paper, we introduce a new self-supervised task called as \textit{Space-Time Cubic Puzzles} to train 3D CNNs using large scale video dataset. This task requires a network to arrange permuted 3D spatio-temporal crops. By completing \textit{Space-Time Cubic Puzzles}, the network learns both spatial appearance and temporal relation of video frames, which is our final goal. In experiments, we demonstrate that our learned 3D representation is well transferred to action recognition tasks, and outperforms state-of-the-art 2D CNN-based competitors on UCF101 and HMDB51 datasets.



### A Novel Technique for Evidence based Conditional Inference in Deep Neural Networks via Latent Feature Perturbation
- **Arxiv ID**: http://arxiv.org/abs/1811.09796v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.09796v6)
- **Published**: 2018-11-24 09:17:57+00:00
- **Updated**: 2019-12-06 06:36:51+00:00
- **Authors**: Dinesh Khandelwal, Suyash Agrawal, Parag Singla, Chetan Arora
- **Comment**: None
- **Journal**: None
- **Summary**: Auxiliary information can be exploited in machine learning models using the paradigm of evidence based conditional inference. Multi-modal techniques in Deep Neural Networks (DNNs) can be seen as perturbing the latent feature representation for incorporating evidence from the auxiliary modality. However, they require training a specialized network which can map sparse evidence to a high dimensional latent space vector. Designing such a network, as well as collecting jointly labeled data for training is a non-trivial task. In this paper, we present a novel multi-task learning (MTL) based framework to perform evidence based conditional inference in DNNs which can overcome both these shortcomings. Our framework incorporates evidence as the output of secondary task(s), while modeling the original problem as the primary task of interest. During inference, we employ a novel Bayesian formulation to change the joint latent feature representation so as to maximize the probability of the observed evidence. Since our approach models evidence as prediction from a DNN, this can often be achieved using standard pre-trained backbones for popular tasks, eliminating the need for training altogether. Even when training is required, our MTL architecture ensures the same can be done without any need for jointly labeled data. Exploiting evidence using our framework, we show an improvement of 3.9% over the state-of-the-art, for predicting semantic segmentation given the image tags, and 2.8% for predicting instance segmentation given image captions.



### Bayesian QuickNAT: Model Uncertainty in Deep Whole-Brain Segmentation for Structure-wise Quality Control
- **Arxiv ID**: http://arxiv.org/abs/1811.09800v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.09800v1)
- **Published**: 2018-11-24 09:41:28+00:00
- **Updated**: 2018-11-24 09:41:28+00:00
- **Authors**: Abhijit Guha Roy, Sailesh Conjeti, Nassir Navab, Christian Wachinger
- **Comment**: Under Review in NeuroImage
- **Journal**: None
- **Summary**: We introduce Bayesian QuickNAT for the automated quality control of whole-brain segmentation on MRI T1 scans. Next to the Bayesian fully convolutional neural network, we also present inherent measures of segmentation uncertainty that allow for quality control per brain structure. For estimating model uncertainty, we follow a Bayesian approach, wherein, Monte Carlo (MC) samples from the posterior distribution are generated by keeping the dropout layers active at test time. Entropy over the MC samples provides a voxel-wise model uncertainty map, whereas expectation over the MC predictions provides the final segmentation. Next to voxel-wise uncertainty, we introduce four metrics to quantify structure-wise uncertainty in segmentation for quality control. We report experiments on four out-of-sample datasets comprising of diverse age range, pathology and imaging artifacts. The proposed structure-wise uncertainty metrics are highly correlated with the Dice score estimated with manual annotation and therefore present an inherent measure of segmentation quality. In particular, the intersection over union over all the MC samples is a suitable proxy for the Dice score. In addition to quality control at scan-level, we propose to incorporate the structure-wise uncertainty as a measure of confidence to do reliable group analysis on large data repositories. We envisage that the introduced uncertainty metrics would help assess the fidelity of automated deep learning based segmentation methods for large-scale population studies, as they enable automated quality control and group analyses in processing large data repositories.



### Object Detection based Deep Unsupervised Hashing
- **Arxiv ID**: http://arxiv.org/abs/1811.09822v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.09822v1)
- **Published**: 2018-11-24 12:30:55+00:00
- **Updated**: 2018-11-24 12:30:55+00:00
- **Authors**: Rong-Cheng Tu, Xian-Ling Mao, Bo-Si Feng, Bing-Bing Bian, Yu-shu Ying
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, similarity-preserving hashing methods have been extensively studied for large-scale image retrieval. Compared with unsupervised hashing, supervised hashing methods for labeled data have usually better performance by utilizing semantic label information. Intuitively, for unlabeled data, it will improve the performance of unsupervised hashing methods if we can first mine some supervised semantic 'label information' from unlabeled data and then incorporate the 'label information' into the training process. Thus, in this paper, we propose a novel Object Detection based Deep Unsupervised Hashing method (ODDUH). Specifically, a pre-trained object detection model is utilized to mining supervised 'label information', which is used to guide the learning process to generate high-quality hash codes.Extensive experiments on two public datasets demonstrate that the proposed method outperforms the state-of-the-art unsupervised hashing methods in the image retrieval task.



### Attention, Please! Adversarial Defense via Activation Rectification and Preservation
- **Arxiv ID**: http://arxiv.org/abs/1811.09831v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.09831v5)
- **Published**: 2018-11-24 13:14:08+00:00
- **Updated**: 2022-12-29 02:19:43+00:00
- **Authors**: Shangxi Wu, Jitao Sang, Kaiyuan Xu, Jiaming Zhang, Jian Yu
- **Comment**: None
- **Journal**: None
- **Summary**: This study provides a new understanding of the adversarial attack problem by examining the correlation between adversarial attack and visual attention change. In particular, we observed that: (1) images with incomplete attention regions are more vulnerable to adversarial attacks; and (2) successful adversarial attacks lead to deviated and scattered attention map. Accordingly, an attention-based adversarial defense framework is designed to simultaneously rectify the attention map for prediction and preserve the attention area between adversarial and original images. The problem of adding iteratively attacked samples is also discussed in the context of visual attention change. We hope the attention-related data analysis and defense solution in this study will shed some light on the mechanism behind the adversarial attack and also facilitate future adversarial defense/attack model design.



### Efficient Video Understanding via Layered Multi Frame-Rate Analysis
- **Arxiv ID**: http://arxiv.org/abs/1811.09834v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.09834v1)
- **Published**: 2018-11-24 13:43:34+00:00
- **Updated**: 2018-11-24 13:43:34+00:00
- **Authors**: Ziyao Tang, Yongxi Lu, Tara Javidi
- **Comment**: under review
- **Journal**: None
- **Summary**: One of the greatest challenges in the design of a real-time perception system for autonomous driving vehicles and drones is the conflicting requirement of safety (high prediction accuracy) and efficiency. Traditional approaches use a single frame rate for the entire system. Motivated by the observation that the lack of robustness against environmental factors is the major weakness of compact ConvNet architectures, we propose a dual frame-rate system that brings in the best of both worlds: A modulator stream that executes an expensive models robust to environmental factors at a low frame rate to extract slowly changing features describing the environment, and a prediction stream that executes a light-weight model at real-time to extract transient signals that describes particularities of the current frame. The advantage of our design is validated by our extensive empirical study, showing that our solution leads to consistent improvements using a variety of backbone architecture choice and input resolutions. These findings suggest multiple frame-rate systems as a promising direction in designing efficient perception for autonomous agents.



### Tell, Draw, and Repeat: Generating and Modifying Images Based on Continual Linguistic Instruction
- **Arxiv ID**: http://arxiv.org/abs/1811.09845v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.09845v3)
- **Published**: 2018-11-24 14:42:18+00:00
- **Updated**: 2019-09-23 15:14:05+00:00
- **Authors**: Alaaeldin El-Nouby, Shikhar Sharma, Hannes Schulz, Devon Hjelm, Layla El Asri, Samira Ebrahimi Kahou, Yoshua Bengio, Graham W. Taylor
- **Comment**: Accepted at ICCV 2019
- **Journal**: Proceedings of the 2019 IEEE International Conference on Computer
  Vision (ICCV)
- **Summary**: Conditional text-to-image generation is an active area of research, with many possible applications. Existing research has primarily focused on generating a single image from available conditioning information in one step. One practical extension beyond one-step generation is a system that generates an image iteratively, conditioned on ongoing linguistic input or feedback. This is significantly more challenging than one-step generation tasks, as such a system must understand the contents of its generated images with respect to the feedback history, the current feedback, as well as the interactions among concepts present in the feedback history. In this work, we present a recurrent image generation model which takes into account both the generated output up to the current step as well as all past instructions for generation. We show that our model is able to generate the background, add new objects, and apply simple transformations to existing objects. We believe our approach is an important step toward interactive generation. Code and data is available at: https://www.microsoft.com/en-us/research/project/generative-neural-visual-artist-geneva/ .



### Robust RGB-D Face Recognition Using Attribute-Aware Loss
- **Arxiv ID**: http://arxiv.org/abs/1811.09847v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.09847v2)
- **Published**: 2018-11-24 15:07:12+00:00
- **Updated**: 2019-05-17 15:13:03+00:00
- **Authors**: Luo Jiang, Juyong Zhang, Bailin Deng
- **Comment**: None
- **Journal**: None
- **Summary**: Existing convolutional neural network (CNN) based face recognition algorithms typically learn a discriminative feature mapping, using a loss function that enforces separation of features from different classes and/or aggregation of features within the same class. However, they may suffer from bias in the training data such as uneven sampling density, because they optimize the adjacency relationship of the learned features without considering the proximity of the underlying faces. Moreover, since they only use facial images for training, the learned feature mapping may not correctly indicate the relationship of other attributes such as gender and ethnicity, which can be important for some face recognition applications. In this paper, we propose a new CNN-based face recognition approach that incorporates such attributes into the training process. Using an attribute-aware loss function that regularizes the feature mapping using attribute proximity, our approach learns more discriminative features that are correlated with the attributes. We train our face recognition model on a large-scale RGB-D data set with over 100K identities captured under real application conditions. By comparing our approach with other methods on a variety of experiments, we demonstrate that depth channel and attribute-aware loss greatly improve the accuracy and robustness of face recognition.



### FANet: Quality-Aware Feature Aggregation Network for Robust RGB-T Tracking
- **Arxiv ID**: http://arxiv.org/abs/1811.09855v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.09855v2)
- **Published**: 2018-11-24 16:54:28+00:00
- **Updated**: 2019-10-14 07:37:57+00:00
- **Authors**: Yabin Zhu, Chenglong Li, Bin Luo, Jin Tang
- **Comment**: None
- **Journal**: None
- **Summary**: This paper investigates how to perform robust visual tracking in adverse and challenging conditions using complementary visual and thermal infrared data (RGBT tracking). We propose a novel deep network architecture called qualityaware Feature Aggregation Network (FANet) for robust RGBT tracking. Unlike existing RGBT trackers, our FANet aggregates hierarchical deep features within each modality to handle the challenge of significant appearance changes caused by deformation, low illumination, background clutter and occlusion. In particular, we employ the operations of max pooling to transform these hierarchical and multi-resolution features into uniform space with the same resolution, and use 1x1 convolution operation to compress feature dimensions to achieve more effective hierarchical feature aggregation. To model the interactions between RGB and thermal modalities, we elaborately design an adaptive aggregation subnetwork to integrate features from different modalities based on their reliabilities and thus are able to alleviate noise effects introduced by low-quality sources. The whole FANet is trained in an end-to-end manner. Extensive experiments on large-scale benchmark datasets demonstrate the high-accurate performance against other state-of-the-art RGBT tracking methods.



### On Periodic Functions as Regularizers for Quantization of Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1811.09862v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML, 68T05, I.2.6; I.5.0
- **Links**: [PDF](http://arxiv.org/pdf/1811.09862v1)
- **Published**: 2018-11-24 17:24:28+00:00
- **Updated**: 2018-11-24 17:24:28+00:00
- **Authors**: Maxim Naumov, Utku Diril, Jongsoo Park, Benjamin Ray, Jedrzej Jablonski, Andrew Tulloch
- **Comment**: 11 pages, 7 figures
- **Journal**: None
- **Summary**: Deep learning models have been successfully used in computer vision and many other fields. We propose an unorthodox algorithm for performing quantization of the model parameters. In contrast with popular quantization schemes based on thresholds, we use a novel technique based on periodic functions, such as continuous trigonometric sine or cosine as well as non-continuous hat functions. We apply these functions component-wise and add the sum over the model parameters as a regularizer to the model loss during training. The frequency and amplitude hyper-parameters of these functions can be adjusted during training. The regularization pushes the weights into discrete points that can be encoded as integers. We show that using this technique the resulting quantized models exhibit the same accuracy as the original ones on CIFAR-10 and ImageNet datasets.



### Forward Stability of ResNet and Its Variants
- **Arxiv ID**: http://arxiv.org/abs/1811.09885v1
- **DOI**: None
- **Categories**: **cs.CV**, math.DS
- **Links**: [PDF](http://arxiv.org/pdf/1811.09885v1)
- **Published**: 2018-11-24 19:43:22+00:00
- **Updated**: 2018-11-24 19:43:22+00:00
- **Authors**: Linan Zhang, Hayden Schaeffer
- **Comment**: 35 pages, 8 figures, 5 tables
- **Journal**: None
- **Summary**: The residual neural network (ResNet) is a popular deep network architecture which has the ability to obtain high-accuracy results on several image processing problems. In order to analyze the behavior and structure of ResNet, recent work has been on establishing connections between ResNets and continuous-time optimal control problems. In this work, we show that the post-activation ResNet is related to an optimal control problem with differential inclusions, and provide continuous-time stability results for the differential inclusion associated with ResNet. Motivated by the stability conditions, we show that alterations of either the architecture or the optimization problem can generate variants of ResNet which improve the theoretical stability bounds. In addition, we establish stability bounds for the full (discrete) network associated with two variants of ResNet, in particular, bounds on the growth of the features and a measure of the sensitivity of the features with respect to perturbations. These results also help to show the relationship between the depth, regularization, and stability of the feature space. Computational experiments on the proposed variants show that the accuracy of ResNet is preserved and that the accuracy seems to be monotone with respect to the depth and various corruptions.



### Matching Disparate Image Pairs Using Shape-Aware ConvNets
- **Arxiv ID**: http://arxiv.org/abs/1811.09889v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.09889v1)
- **Published**: 2018-11-24 20:09:15+00:00
- **Updated**: 2018-11-24 20:09:15+00:00
- **Authors**: Shefali Srivastava, Abhimanyu Chopra, Arun CS Kumar, Suchendra M. Bhandarkar, Deepak Sharma
- **Comment**: First two authors contributed equally, to Appear in the IEEE Winter
  Conference on Applications of Computer Vision (WACV) 2019
- **Journal**: None
- **Summary**: An end-to-end trainable ConvNet architecture, that learns to harness the power of shape representation for matching disparate image pairs, is proposed. Disparate image pairs are deemed those that exhibit strong affine variations in scale, viewpoint and projection parameters accompanied by the presence of partial or complete occlusion of objects and extreme variations in ambient illumination. Under these challenging conditions, neither local nor global feature-based image matching methods, when used in isolation, have been observed to be effective. The proposed correspondence determination scheme for matching disparate images exploits high-level shape cues that are derived from low-level local feature descriptors, thus combining the best of both worlds. A graph-based representation for the disparate image pair is generated by constructing an affinity matrix that embeds the distances between feature points in two images, thus modeling the correspondence determination problem as one of graph matching. The eigenspectrum of the affinity matrix, i.e., the learned global shape representation, is then used to further regress the transformation or homography that defines the correspondence between the source image and target image. The proposed scheme is shown to yield state-of-the-art results for both, coarse-level shape matching as well as fine point-wise correspondence determination.



### Conditional Recurrent Flow: Conditional Generation of Longitudinal Samples with Applications to Neuroimaging
- **Arxiv ID**: http://arxiv.org/abs/1811.09897v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.09897v2)
- **Published**: 2018-11-24 21:12:42+00:00
- **Updated**: 2018-12-11 04:40:49+00:00
- **Authors**: Seong Jae Hwang, Zirui Tao, Won Hwa Kim, Vikas Singh
- **Comment**: None
- **Journal**: None
- **Summary**: Generative models using neural network have opened a door to large-scale studies for various application domains, especially for studies that suffer from lack of real samples to obtain statistically robust inference. Typically, these generative models would train on existing data to learn the underlying distribution of the measurements (e.g., images) in latent spaces conditioned on covariates (e.g., image labels), and generate independent samples that are identically distributed in the latent space. Such models may work for cross-sectional studies, however, they are not suitable to generate data for longitudinal studies that focus on "progressive" behavior in a sequence of data. In practice, this is a quite common case in various neuroimaging studies whose goal is to characterize a trajectory of pathologies of a specific disease even from early stages. This may be too ambitious especially when the sample size is small (e.g., up to a few hundreds). Motivated from the setup above, we seek to develop a conditional generative model for longitudinal data generation by designing an invertable neural network. Inspired by recurrent nature of longitudinal data, we propose a novel neural network that incorporates recurrent subnetwork and context gating to include smooth transition in a sequence of generated data. Our model is validated on a video sequence dataset and a longitudinal AD dataset with various experimental settings for qualitative and quantitative evaluations of the generated samples. The results with the AD dataset captures AD specific group differences with sufficiently generated longitudinal samples that are consistent with existing literature, which implies a great potential to be applicable to other disease studies.



### RGB-D Based Action Recognition with Light-weight 3D Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1811.09908v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.09908v1)
- **Published**: 2018-11-24 23:35:08+00:00
- **Updated**: 2018-11-24 23:35:08+00:00
- **Authors**: Haokui Zhang, Ying Li, Peng Wang, Yu Liu, Chunhua Shen
- **Comment**: 14 pages
- **Journal**: None
- **Summary**: Different from RGB videos, depth data in RGB-D videos provide key complementary information for tristimulus visual data which potentially could achieve accuracy improvement for action recognition. However, most of the existing action recognition models solely using RGB videos limit the performance capacity. Additionally, the state-of-the-art action recognition models, namely 3D convolutional neural networks (3D-CNNs) contain tremendous parameters suffering from computational inefficiency. In this paper, we propose a series of 3D light-weight architectures for action recognition based on RGB-D data. Compared with conventional 3D-CNN models, the proposed light-weight 3D-CNNs have considerably less parameters involving lower computation cost, while it results in favorable recognition performance. Experimental results on two public benchmark datasets show that our models can approximate or outperform the state-of-the-art approaches. Specifically, on the RGB+D-NTU (NTU) dataset, we achieve 93.2% and 97.6% for cross-subject and cross-view measurement, and on the Northwestern-UCLA Multiview Action 3D (N-UCLA) dataset, we achieve 95.5% accuracy of cross-view.



### Steady-state Non-Line-of-Sight Imaging
- **Arxiv ID**: http://arxiv.org/abs/1811.09910v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.09910v2)
- **Published**: 2018-11-24 23:39:54+00:00
- **Updated**: 2019-04-07 02:09:07+00:00
- **Authors**: Wenzheng Chen, Simon Daneau, Fahim Mannan, Felix Heide
- **Comment**: None
- **Journal**: None
- **Summary**: Conventional intensity cameras recover objects in the direct line-of-sight of the camera, whereas occluded scene parts are considered lost in this process. Non-line-of-sight imaging (NLOS) aims at recovering these occluded objects by analyzing their indirect reflections on visible scene surfaces. Existing NLOS methods temporally probe the indirect light transport to unmix light paths based on their travel time, which mandates specialized instrumentation that suffers from low photon efficiency, high cost, and mechanical scanning. We depart from temporal probing and demonstrate steady-state NLOS imaging using conventional intensity sensors and continuous illumination. Instead of assuming perfectly isotropic scattering, the proposed method exploits directionality in the hidden surface reflectance, resulting in (small) spatial variation of their indirect reflections for varying illumination. To tackle the shape-dependence of these variations, we propose a trainable architecture which learns to map diffuse indirect reflections to scene reflectance using only synthetic training data. Relying on consumer color image sensors, with high fill factor, high quantum efficiency and low read-out noise, we demonstrate high-fidelity color NLOS imaging for scene configurations tackled before with picosecond time resolution.



