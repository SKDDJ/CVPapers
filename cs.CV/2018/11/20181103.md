# Arxiv Papers in cs.CV on 2018-11-03
### Pushing the boundaries of audiovisual word recognition using Residual Networks and LSTMs
- **Arxiv ID**: http://arxiv.org/abs/1811.01194v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.01194v1)
- **Published**: 2018-11-03 11:30:17+00:00
- **Updated**: 2018-11-03 11:30:17+00:00
- **Authors**: Themos Stafylakis, Muhammad Haris Khan, Georgios Tzimiropoulos
- **Comment**: Accepted to Computer Vision and Image Understanding (Elsevier)
- **Journal**: None
- **Summary**: Visual and audiovisual speech recognition are witnessing a renaissance which is largely due to the advent of deep learning methods. In this paper, we present a deep learning architecture for lipreading and audiovisual word recognition, which combines Residual Networks equipped with spatiotemporal input layers and Bidirectional LSTMs. The lipreading architecture attains 11.92% misclassification rate on the challenging Lipreading-In-The-Wild database, which is composed of excerpts from BBC-TV, each containing one of the 500 target words. Audiovisual experiments are performed using both intermediate and late integration, as well as several types and levels of environmental noise, and notable improvements over the audio-only network are reported, even in the case of clean speech. A further analysis on the utility of target word boundaries is provided, as well as on the capacity of the network in modeling the linguistic context of the target word. Finally, we examine difficult word pairs and discuss how visual information helps towards attaining higher recognition accuracy.



### DUNet: A deformable network for retinal vessel segmentation
- **Arxiv ID**: http://arxiv.org/abs/1811.01206v1
- **DOI**: 10.1016/j.knosys.2019.04.025
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.01206v1)
- **Published**: 2018-11-03 13:05:06+00:00
- **Updated**: 2018-11-03 13:05:06+00:00
- **Authors**: Qiangguo Jin, Zhaopeng Meng, Tuan D. Pham, Qi Chen, Leyi Wei, Ran Su
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic segmentation of retinal vessels in fundus images plays an important role in the diagnosis of some diseases such as diabetes and hypertension. In this paper, we propose Deformable U-Net (DUNet), which exploits the retinal vessels' local features with a U-shape architecture, in an end to end manner for retinal vessel segmentation. Inspired by the recently introduced deformable convolutional networks, we integrate the deformable convolution into the proposed network. The DUNet, with upsampling operators to increase the output resolution, is designed to extract context information and enable precise localization by combining low-level feature maps with high-level ones. Furthermore, DUNet captures the retinal vessels at various shapes and scales by adaptively adjusting the receptive fields according to vessels' scales and shapes. Three public datasets DRIVE, STARE and CHASE_DB1 are used to train and test our model. Detailed comparisons between the proposed network and the deformable neural network, U-Net are provided in our study. Results show that more detailed vessels are extracted by DUNet and it exhibits state-of-the-art performance for retinal vessel segmentation with a global accuracy of 0.9697/0.9722/0.9724 and AUC of 0.9856/0.9868/0.9863 on DRIVE, STARE and CHASE_DB1 respectively. Moreover, to show the generalization ability of the DUNet, we used another two retinal vessel data sets, one is named WIDE and the other is a synthetic data set with diverse styles, named SYNTHE, to qualitatively and quantitatively analyzed and compared with other methods. Results indicates that DUNet outperforms other state-of-the-arts.



### Deep Learning based Computer-Aided Diagnosis Systems for Diabetic Retinopathy: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1811.01238v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.01238v2)
- **Published**: 2018-11-03 15:58:57+00:00
- **Updated**: 2019-08-21 21:45:48+00:00
- **Authors**: Norah Asiri, Muhammad Hussain, Fadwa Al Adel, Nazih Alzaidi
- **Comment**: None
- **Journal**: None
- **Summary**: Diabetic retinopathy (DR) results in vision loss if not treated early. A computer-aided diagnosis (CAD) system based on retinal fundus images is an efficient and effective method for early DR diagnosis and assisting experts. A computer-aided diagnosis (CAD) system involves various stages like detection, segmentation and classification of lesions in fundus images. Many traditional machine-learning (ML) techniques based on hand-engineered features have been introduced. The recent emergence of deep learning (DL) and its decisive victory over traditional ML methods for various applications motivated the researchers to employ it for DR diagnosis, and many deep-learning-based methods have been introduced. In this paper, we review these methods, highlighting their pros and cons. In addition, we point out the challenges to be addressed in designing and learning about efficient, effective and robust deep-learning algorithms for various problems in DR diagnosis and draw attention to directions for future research.



### ReXCam: Resource-Efficient, Cross-Camera Video Analytics at Scale
- **Arxiv ID**: http://arxiv.org/abs/1811.01268v4
- **DOI**: None
- **Categories**: **cs.DC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1811.01268v4)
- **Published**: 2018-11-03 19:15:15+00:00
- **Updated**: 2019-12-04 03:17:48+00:00
- **Authors**: Samvit Jain, Xun Zhang, Yuhao Zhou, Ganesh Ananthanarayanan, Junchen Jiang, Yuanchao Shu, Joseph Gonzalez
- **Comment**: 15 pages
- **Journal**: None
- **Summary**: Enterprises are increasingly deploying large camera networks for video analytics. Many target applications entail a common problem template: searching for and tracking an object or activity of interest (e.g. a speeding vehicle, a break-in) through a large camera network in live video. Such cross-camera analytics is compute and data intensive, with cost growing with the number of cameras and time. To address this cost challenge, we present ReXCam, a new system for efficient cross-camera video analytics. ReXCam exploits spatial and temporal locality in the dynamics of real camera networks to guide its inference-time search for a query identity. In an offline profiling phase, ReXCam builds a cross-camera correlation model that encodes the locality observed in historical traffic patterns. At inference time, ReXCam applies this model to filter frames that are not spatially and temporally correlated with the query identity's current position. In the cases of occasional missed detections, ReXCam performs a fast-replay search on recently filtered video frames, enabling gracefully recovery. Together, these techniques allow ReXCam to reduce compute workload by 8.3x on an 8-camera dataset, and by 23x - 38x on a simulated 130-camera dataset. ReXCam has been implemented and deployed on a testbed of 5 AWS DeepLens cameras.



### Auto-ML Deep Learning for Rashi Scripts OCR
- **Arxiv ID**: http://arxiv.org/abs/1811.01290v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.01290v2)
- **Published**: 2018-11-03 21:53:47+00:00
- **Updated**: 2020-02-22 21:11:36+00:00
- **Authors**: Shahar Mahpod, Yosi Keller
- **Comment**: The paper is under consideration at Pattern Recognition Letters
- **Journal**: None
- **Summary**: In this work we propose an OCR scheme for manuscripts printed in Rashi font that is an ancient Hebrew font and corresponding dialect used in religious Jewish literature, for more than 600 years. The proposed scheme utilizes a convolution neural network (CNN) for visual inference and Long-Short Term Memory (LSTM) to learn the Rashi scripts dialect. In particular, we derive an AutoML scheme to optimize the CNN architecture, and a book-specific CNN training to improve the OCR accuracy. The proposed scheme achieved an accuracy of more than 99.8% using a dataset of more than 3M annotated letters from the Responsa Project dataset.



### Geometry-Aware Recurrent Neural Networks for Active Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/1811.01292v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.01292v2)
- **Published**: 2018-11-03 22:24:00+00:00
- **Updated**: 2018-11-14 04:07:09+00:00
- **Authors**: Ricson Cheng, Ziyan Wang, Katerina Fragkiadaki
- **Comment**: To appear in NIPS2018
- **Journal**: None
- **Summary**: We present recurrent geometry-aware neural networks that integrate visual information across multiple views of a scene into 3D latent feature tensors, while maintaining an one-to-one mapping between 3D physical locations in the world scene and latent feature locations. Object detection, object segmentation, and 3D reconstruction is then carried out directly using the constructed 3D feature memory, as opposed to any of the input 2D images. The proposed models are equipped with differentiable egomotion-aware feature warping and (learned) depth-aware unprojection operations to achieve geometrically consistent mapping between the features in the input frame and the constructed latent model of the scene. We empirically show the proposed model generalizes much better than geometryunaware LSTM/GRU networks, especially under the presence of multiple objects and cross-object occlusions. Combined with active view selection policies, our model learns to select informative viewpoints to integrate information from by "undoing" cross-object occlusions, seamlessly combining geometry with learning from experience.



