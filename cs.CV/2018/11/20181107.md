# Arxiv Papers in cs.CV on 2018-11-07
### Style Separation and Synthesis via Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1811.02740v1
- **DOI**: 10.1145/3240508.3240524
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.02740v1)
- **Published**: 2018-11-07 02:58:46+00:00
- **Updated**: 2018-11-07 02:58:46+00:00
- **Authors**: Rui Zhang, Sheng Tang, Yu Li, Junbo Guo, Yongdong Zhang, Jintao Li, Shuicheng Yan
- **Comment**: None
- **Journal**: The 26th ACM international conference on Multimedia (ACM MM),
  2018, pp. 183-191
- **Summary**: Style synthesis attracts great interests recently, while few works focus on its dual problem "style separation". In this paper, we propose the Style Separation and Synthesis Generative Adversarial Network (S3-GAN) to simultaneously implement style separation and style synthesis on object photographs of specific categories. Based on the assumption that the object photographs lie on a manifold, and the contents and styles are independent, we employ S3-GAN to build mappings between the manifold and a latent vector space for separating and synthesizing the contents and styles. The S3-GAN consists of an encoder network, a generator network, and an adversarial network. The encoder network performs style separation by mapping an object photograph to a latent vector. Two halves of the latent vector represent the content and style, respectively. The generator network performs style synthesis by taking a concatenated vector as input. The concatenated vector contains the style half vector of the style target image and the content half vector of the content target image. Once obtaining the images from the generator network, an adversarial network is imposed to generate more photo-realistic images. Experiments on CelebA and UT Zappos 50K datasets demonstrate that the S3-GAN has the capacity of style separation and synthesis simultaneously, and could capture various styles in a single model.



### View Inter-Prediction GAN: Unsupervised Representation Learning for 3D Shapes by Learning Global Shape Memories to Support Local View Predictions
- **Arxiv ID**: http://arxiv.org/abs/1811.02744v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.02744v1)
- **Published**: 2018-11-07 03:25:50+00:00
- **Updated**: 2018-11-07 03:25:50+00:00
- **Authors**: Zhizhong Han, Mingyang Shang, Yu-Shen Liu, Matthias Zwicker
- **Comment**: To be published at AAAI 2019
- **Journal**: None
- **Summary**: In this paper we present a novel unsupervised representation learning approach for 3D shapes, which is an important research challenge as it avoids the manual effort required for collecting supervised data. Our method trains an RNN-based neural network architecture to solve multiple view inter-prediction tasks for each shape. Given several nearby views of a shape, we define view inter-prediction as the task of predicting the center view between the input views, and reconstructing the input views in a low-level feature space. The key idea of our approach is to implement the shape representation as a shape-specific global memory that is shared between all local view inter-predictions for each shape. Intuitively, this memory enables the system to aggregate information that is useful to better solve the view inter-prediction tasks for each shape, and to leverage the memory as a view-independent shape representation. Our approach obtains the best results using a combination of L_2 and adversarial losses for the view inter-prediction task. We show that VIP-GAN outperforms state-of-the-art methods in unsupervised 3D feature learning on three large scale 3D shape benchmarks.



### Y^2Seq2Seq: Cross-Modal Representation Learning for 3D Shape and Text by Joint Reconstruction and Prediction of View and Word Sequences
- **Arxiv ID**: http://arxiv.org/abs/1811.02745v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.02745v1)
- **Published**: 2018-11-07 03:33:03+00:00
- **Updated**: 2018-11-07 03:33:03+00:00
- **Authors**: Zhizhong Han, Mingyang Shang, Xiyang Wang, Yu-Shen Liu, Matthias Zwicker
- **Comment**: To be pubilished at AAAI 2019
- **Journal**: None
- **Summary**: A recent method employs 3D voxels to represent 3D shapes, but this limits the approach to low resolutions due to the computational cost caused by the cubic complexity of 3D voxels. Hence the method suffers from a lack of detailed geometry. To resolve this issue, we propose Y^2Seq2Seq, a view-based model, to learn cross-modal representations by joint reconstruction and prediction of view and word sequences. Specifically, the network architecture of Y^2Seq2Seq bridges the semantic meaning embedded in the two modalities by two coupled `Y' like sequence-to-sequence (Seq2Seq) structures. In addition, our novel hierarchical constraints further increase the discriminability of the cross-modal representations by employing more detailed discriminative information. Experimental results on cross-modal retrieval and 3D shape captioning show that Y^2Seq2Seq outperforms the state-of-the-art methods.



### Component-based Attention for Large-scale Trademark Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1811.02746v2
- **DOI**: 10.1109/TIFS.2019.2959921
- **Categories**: **cs.CV**, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.02746v2)
- **Published**: 2018-11-07 03:33:28+00:00
- **Updated**: 2019-10-22 06:25:55+00:00
- **Authors**: Osman Tursun, Simon Denman, Sabesan Sivapalan, Sridha Sridharan, Clinton Fookes, Sandra Mau
- **Comment**: Fix typos related to authors' information
- **Journal**: None
- **Summary**: The demand for large-scale trademark retrieval (TR) systems has significantly increased to combat the rise in international trademark infringement. Unfortunately, the ranking accuracy of current approaches using either hand-crafted or pre-trained deep convolution neural network (DCNN) features is inadequate for large-scale deployments. We show in this paper that the ranking accuracy of TR systems can be significantly improved by incorporating hard and soft attention mechanisms, which direct attention to critical information such as figurative elements and reduce attention given to distracting and uninformative elements such as text and background. Our proposed approach achieves state-of-the-art results on a challenging large-scale trademark dataset.



### Learning to Steer by Mimicking Features from Heterogeneous Auxiliary Networks
- **Arxiv ID**: http://arxiv.org/abs/1811.02759v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.02759v1)
- **Published**: 2018-11-07 04:47:25+00:00
- **Updated**: 2018-11-07 04:47:25+00:00
- **Authors**: Yuenan Hou, Zheng Ma, Chunxiao Liu, Chen Change Loy
- **Comment**: 8 pages, 6 figures; Accepted by AAAI 2019; Our project page is
  available at https://cardwing.github.io/projects/FM-Net
- **Journal**: None
- **Summary**: The training of many existing end-to-end steering angle prediction models heavily relies on steering angles as the supervisory signal. Without learning from much richer contexts, these methods are susceptible to the presence of sharp road curves, challenging traffic conditions, strong shadows, and severe lighting changes. In this paper, we considerably improve the accuracy and robustness of predictions through heterogeneous auxiliary networks feature mimicking, a new and effective training method that provides us with much richer contextual signals apart from steering direction. Specifically, we train our steering angle predictive model by distilling multi-layer knowledge from multiple heterogeneous auxiliary networks that perform related but different tasks, e.g., image segmentation or optical flow estimation. As opposed to multi-task learning, our method does not require expensive annotations of related tasks on the target set. This is made possible by applying contemporary off-the-shelf networks on the target set and mimicking their features in different layers after transformation. The auxiliary networks are discarded after training without affecting the runtime efficiency of our model. Our approach achieves a new state-of-the-art on Udacity and Comma.ai, outperforming the previous best by a large margin of 12.8% and 52.1%, respectively. Encouraging results are also shown on Berkeley Deep Drive (BDD) dataset.



### Learning to Compose Topic-Aware Mixture of Experts for Zero-Shot Video Captioning
- **Arxiv ID**: http://arxiv.org/abs/1811.02765v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1811.02765v2)
- **Published**: 2018-11-07 05:33:07+00:00
- **Updated**: 2018-11-23 23:22:19+00:00
- **Authors**: Xin Wang, Jiawei Wu, Da Zhang, Yu Su, William Yang Wang
- **Comment**: Accepted to AAAI 2019
- **Journal**: None
- **Summary**: Although promising results have been achieved in video captioning, existing models are limited to the fixed inventory of activities in the training corpus, and do not generalize to open vocabulary scenarios. Here we introduce a novel task, zero-shot video captioning, that aims at describing out-of-domain videos of unseen activities. Videos of different activities usually require different captioning strategies in many aspects, i.e. word selection, semantic construction, and style expression etc, which poses a great challenge to depict novel activities without paired training data. But meanwhile, similar activities share some of those aspects in common. Therefore, We propose a principled Topic-Aware Mixture of Experts (TAMoE) model for zero-shot video captioning, which learns to compose different experts based on different topic embeddings, implicitly transferring the knowledge learned from seen activities to unseen ones. Besides, we leverage external topic-related text corpus to construct the topic embedding for each activity, which embodies the most relevant semantic vectors within the topic. Empirical results not only validate the effectiveness of our method in utilizing semantic knowledge for video captioning, but also show its strong generalization ability when describing novel activities.



### CAAD 2018: Iterative Ensemble Adversarial Attack
- **Arxiv ID**: http://arxiv.org/abs/1811.03456v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.03456v1)
- **Published**: 2018-11-07 07:36:55+00:00
- **Updated**: 2018-11-07 07:36:55+00:00
- **Authors**: Jiayang Liu, Weiming Zhang, Nenghai Yu
- **Comment**: arXiv admin note: text overlap with arXiv:1811.00189
- **Journal**: None
- **Summary**: Deep Neural Networks (DNNs) have recently led to significant improvements in many fields. However, DNNs are vulnerable to adversarial examples which are samples with imperceptible perturbations while dramatically misleading the DNNs. Adversarial attacks can be used to evaluate the robustness of deep learning models before they are deployed. Unfortunately, most of existing adversarial attacks can only fool a black-box model with a low success rate. To improve the success rates for black-box adversarial attacks, we proposed an iterated adversarial attack against an ensemble of image classifiers. With this method, we won the 5th place in CAAD 2018 Targeted Adversarial Attack competition.



### GeoSay: A Geometric Saliency for Extracting Buildings in Remote Sensing Images
- **Arxiv ID**: http://arxiv.org/abs/1811.02793v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.02793v1)
- **Published**: 2018-11-07 08:12:24+00:00
- **Updated**: 2018-11-07 08:12:24+00:00
- **Authors**: Gui-Song Xia, Jin Huang, Nan Xue, Qikai Lu, Xiaoxiang Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic extraction of buildings in remote sensing images is an important but challenging task and finds many applications in different fields such as urban planning, navigation and so on. This paper addresses the problem of buildings extraction in very high-spatial-resolution (VHSR) remote sensing (RS) images, whose spatial resolution is often up to half meters and provides rich information about buildings. Based on the observation that buildings in VHSR-RS images are always more distinguishable in geometry than in texture or spectral domain, this paper proposes a geometric building index (GBI) for accurate building extraction, by computing the geometric saliency from VHSR-RS images. More precisely, given an image, the geometric saliency is derived from a mid-level geometric representations based on meaningful junctions that can locally describe geometrical structures of images. The resulting GBI is finally measured by integrating the derived geometric saliency of buildings. Experiments on three public and commonly used datasets demonstrate that the proposed GBI achieves the state-of-the-art performance and shows impressive generalization capability. Additionally, GBI preserves both the exact position and accurate shape of single buildings compared to existing methods.



### Amalgamating Knowledge towards Comprehensive Classification
- **Arxiv ID**: http://arxiv.org/abs/1811.02796v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.02796v2)
- **Published**: 2018-11-07 08:21:39+00:00
- **Updated**: 2018-12-03 06:10:29+00:00
- **Authors**: Chengchao Shen, Xinchao Wang, Jie Song, Li Sun, Mingli Song
- **Comment**: Accepted to AAAI 2019
- **Journal**: None
- **Summary**: With the rapid development of deep learning, there have been an unprecedentedly large number of trained deep network models available online. Reusing such trained models can significantly reduce the cost of training the new models from scratch, if not infeasible at all as the annotations used for the training original networks are often unavailable to public. We propose in this paper to study a new model-reusing task, which we term as \emph{knowledge amalgamation}. Given multiple trained teacher networks, each of which specializes in a different classification problem, the goal of knowledge amalgamation is to learn a lightweight student model capable of handling the comprehensive classification. We assume no other annotations except the outputs from the teacher models are available, and thus focus on extracting and amalgamating knowledge from the multiple teachers. To this end, we propose a pilot two-step strategy to tackle the knowledge amalgamation task, by learning first the compact feature representations from teachers and then the network parameters in a layer-wise manner so as to build the student model. We apply this approach to four public datasets and obtain very encouraging results: even without any human annotation, the obtained student model is competent to handle the comprehensive classification task and in most cases outperforms the teachers in individual sub-tasks.



### Deep Neural Networks for ECG-free Cardiac Phase and End-Diastolic Frame Detection on Coronary Angiographies
- **Arxiv ID**: http://arxiv.org/abs/1811.02797v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.02797v1)
- **Published**: 2018-11-07 08:23:59+00:00
- **Updated**: 2018-11-07 08:23:59+00:00
- **Authors**: Costin Ciusdel, Alexandru Turcea, Andrei Puiu, Lucian Itu, Lucian Calmac, Emma Weiss, Cornelia Margineanu, Elisabeta Badila, Martin Berger, Thomas Redel, Tiziano Passerini, Mehmet Gulsun, Puneet Sharma
- **Comment**: 16 pages, 9 figures
- **Journal**: None
- **Summary**: Invasive coronary angiography (ICA) is the gold standard in Coronary Artery Disease (CAD) imaging. Detection of the end-diastolic frame (EDF) and, in general, cardiac phase detection on each temporal frame of a coronary angiography acquisition is of significant importance for the anatomical and non-invasive functional assessment of CAD. This task is generally performed via manual frame selection or semi-automated selection based on simultaneously acquired ECG signals - thus introducing the requirement of simultaneous ECG recordings. We evaluate the performance of a purely image based workflow based on deep neural networks for fully automated cardiac phase and EDF detection on coronary angiographies. A first deep neural network (DNN), trained to detect coronary arteries, is employed to preselect a subset of frames in which coronary arteries are well visible. A second DNN predicts cardiac phase labels for each frame. Only in the training and evaluation phases for the second DNN, ECG signals are used to provide ground truth labels for each angiographic frame. The networks were trained on 17800 coronary angiographies from 3900 patients and evaluated on 27900 coronary angiographies from 6250 patients. No exclusion criteria related to patient state, previous interventions, or pathology were formulated. Cardiac phase detection had an accuracy of 97.6%, a sensitivity of 97.6% and a specificity of 97.5% on the evaluation set. EDF prediction had a precision of 97.4% and a recall of 96.9%. Several sub-group analyses were performed, indicating that the cardiac phase detection performance is largely independent from acquisition angles and the heart rate of the patient. The average execution time of cardiac phase detection for one angiographic series was on average less than five seconds on a standard workstation.



### Image Smoothing via Unsupervised Learning
- **Arxiv ID**: http://arxiv.org/abs/1811.02804v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.02804v1)
- **Published**: 2018-11-07 09:08:05+00:00
- **Updated**: 2018-11-07 09:08:05+00:00
- **Authors**: Qingnan Fan, Jiaolong Yang, David Wipf, Baoquan Chen, Xin Tong
- **Comment**: Accepted in SIGGRAPH Asia 2018
- **Journal**: None
- **Summary**: Image smoothing represents a fundamental component of many disparate computer vision and graphics applications. In this paper, we present a unified unsupervised (label-free) learning framework that facilitates generating flexible and high-quality smoothing effects by directly learning from data using deep convolutional neural networks (CNNs). The heart of the design is the training signal as a novel energy function that includes an edge-preserving regularizer which helps maintain important yet potentially vulnerable image structures, and a spatially-adaptive Lp flattening criterion which imposes different forms of regularization onto different image regions for better smoothing quality. We implement a diverse set of image smoothing solutions employing the unified framework targeting various applications such as, image abstraction, pencil sketching, detail enhancement, texture removal and content-aware image manipulation, and obtain results comparable with or better than previous methods. Moreover, our method is extremely fast with a modern GPU (e.g, 200 fps for 1280x720 images). Our codes and model are released in https://github.com/fqnchina/ImageSmoothing.



### PaDNet: Pan-Density Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/1811.02805v3
- **DOI**: 10.1109/TIP.2019.2952083
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.02805v3)
- **Published**: 2018-11-07 09:10:47+00:00
- **Updated**: 2020-01-09 04:57:29+00:00
- **Authors**: Yukun Tian, Yiming Lei, Junping Zhang, James Z. Wang
- **Comment**: None
- **Journal**: None
- **Summary**: The problem of counting crowds in varying density scenes or in different density regions of the same scene, named as pan-density crowd counting, is highly challenging. Previous methods are designed for single density scenes or do not fully utilize pan-density information. We propose a novel framework, the Pan-Density Network (PaDNet), for pan-density crowd counting. In order to effectively capture pan-density information, PaDNet has a novel module, the Density-Aware Network (DAN), that contains multiple sub-networks pretrained on scenarios with different densities. Further, a module named the Feature Enhancement Layer (FEL) is proposed to aggregate the feature maps learned by DAN. It learns an enhancement rate or a weight for each feature map to boost these feature maps. Further, we propose two refined metrics, Patch MAE (PMAE) and Patch RMSE (PRMSE), for better evaluating the model performance on pan-density scenarios. Extensive experiments on four crowd counting benchmark datasets indicate that PaDNet achieves state-of-the-art recognition performance and high robustness in pan-density crowd counting.



### Neural Image Compression for Gigapixel Histopathology Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/1811.02840v2
- **DOI**: 10.1109/TPAMI.2019.2936841
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1811.02840v2)
- **Published**: 2018-11-07 11:29:34+00:00
- **Updated**: 2020-04-15 13:25:07+00:00
- **Authors**: David Tellez, Geert Litjens, Jeroen van der Laak, Francesco Ciompi
- **Comment**: Accepted in the IEEE Transactions on Pattern Analysis and Machine
  Intelligence journal
- **Journal**: None
- **Summary**: We propose Neural Image Compression (NIC), a two-step method to build convolutional neural networks for gigapixel image analysis solely using weak image-level labels. First, gigapixel images are compressed using a neural network trained in an unsupervised fashion, retaining high-level information while suppressing pixel-level noise. Second, a convolutional neural network (CNN) is trained on these compressed image representations to predict image-level labels, avoiding the need for fine-grained manual annotations. We compared several encoding strategies, namely reconstruction error minimization, contrastive training and adversarial feature learning, and evaluated NIC on a synthetic task and two public histopathology datasets. We found that NIC can exploit visual cues associated with image-level labels successfully, integrating both global and local visual information. Furthermore, we visualized the regions of the input gigapixel images where the CNN attended to, and confirmed that they overlapped with annotations from human experts.



### DOD-CNN: Doubly-injecting Object Information for Event Recognition
- **Arxiv ID**: http://arxiv.org/abs/1811.02910v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.02910v2)
- **Published**: 2018-11-07 14:44:17+00:00
- **Updated**: 2019-02-11 18:42:41+00:00
- **Authors**: Hyungtae Lee, Sungmin Eum, Heesung Kwon
- **Comment**: ICASSP 2019, 5 pages
- **Journal**: None
- **Summary**: Recognizing an event in an image can be enhanced by detecting relevant objects in two ways: 1) indirectly utilizing object detection information within the unified architecture or 2) directly making use of the object detection output results. We introduce a novel approach, referred to as Doubly-injected Object Detection CNN (DOD-CNN), exploiting the object information in both ways for the task of event recognition. The structure of this network is inspired by the Integrated Object Detection CNN (IOD-CNN) where object information is indirectly exploited by the event recognition module through the shared portion of the network. In the DOD-CNN architecture, the intermediate object detection outputs are directly injected into the event recognition network while keeping the indirect sharing structure inherited from the IOD-CNN, thus being `doubly-injected'. We also introduce a batch pooling layer which constructs one representative feature map from multiple object hypotheses. We have demonstrated the effectiveness of injecting the object detection information in two different ways in the task of malicious event recognition.



### Emerging Applications of Reversible Data Hiding
- **Arxiv ID**: http://arxiv.org/abs/1811.02928v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.02928v1)
- **Published**: 2018-11-07 15:18:50+00:00
- **Updated**: 2018-11-07 15:18:50+00:00
- **Authors**: Dongdong Hou, Weiming Zhang, Jiayang Liu, Siyan Zhou, Dongdong Chen, Nenghai Yu
- **Comment**: ICIGP 2019
- **Journal**: None
- **Summary**: Reversible data hiding (RDH) is one special type of information hiding, by which the host sequence as well as the embedded data can be both restored from the marked sequence without loss. Beside media annotation and integrity authentication, recently some scholars begin to apply RDH in many other fields innovatively. In this paper, we summarize these emerging applications, including steganography, adversarial example, visual transformation, image processing, and give out the general frameworks to make these operations reversible. As far as we are concerned, this is the first paper to summarize the extended applications of RDH.



### Multi-branch Convolutional Neural Network for Multiple Sclerosis Lesion Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1811.02942v4
- **DOI**: 10.1016/j.neuroimage.2019.03.068
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.02942v4)
- **Published**: 2018-11-07 15:42:57+00:00
- **Updated**: 2019-04-08 17:12:50+00:00
- **Authors**: Shahab Aslani, Michael Dayan, Loredana Storelli, Massimo Filippi, Vittorio Murino, Maria A Rocca, Diego Sona
- **Comment**: This paper has been accepted for publication in NeuroImage
- **Journal**: None
- **Summary**: In this paper, we present an automated approach for segmenting multiple sclerosis (MS) lesions from multi-modal brain magnetic resonance images. Our method is based on a deep end-to-end 2D convolutional neural network (CNN) for slice-based segmentation of 3D volumetric data. The proposed CNN includes a multi-branch downsampling path, which enables the network to encode information from multiple modalities separately. Multi-scale feature fusion blocks are proposed to combine feature maps from different modalities at different stages of the network. Then, multi-scale feature upsampling blocks are introduced to upsize combined feature maps to leverage information from lesion shape and location. We trained and tested the proposed model using orthogonal plane orientations of each 3D modality to exploit the contextual information in all directions. The proposed pipeline is evaluated on two different datasets: a private dataset including 37 MS patients and a publicly available dataset known as the ISBI 2015 longitudinal MS lesion segmentation challenge dataset, consisting of 14 MS patients. Considering the ISBI challenge, at the time of submission, our method was amongst the top performing solutions. On the private dataset, using the same array of performance metrics as in the ISBI challenge, the proposed approach shows high improvements in MS lesion segmentation compared with other publicly available tools.



### SurReal: enhancing Surgical simulation Realism using style transfer
- **Arxiv ID**: http://arxiv.org/abs/1811.02946v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.02946v1)
- **Published**: 2018-11-07 15:49:09+00:00
- **Updated**: 2018-11-07 15:49:09+00:00
- **Authors**: Imanol Luengo, Evangello Flouty, Petros Giataganas, Piyamate Wisanuvej, Jean Nehme, Danail Stoyanov
- **Comment**: None
- **Journal**: BMVC 2018
- **Summary**: Surgical simulation is an increasingly important element of surgical education. Using simulation can be a means to address some of the significant challenges in developing surgical skills with limited time and resources. The photo-realistic fidelity of simulations is a key feature that can improve the experience and transfer ratio of trainees. In this paper, we demonstrate how we can enhance the visual fidelity of existing surgical simulation by performing style transfer of multi-class labels from real surgical video onto synthetic content. We demonstrate our approach on simulations of cataract surgery using real data labels from an existing public dataset. Our results highlight the feasibility of the approach and also the powerful possibility to extend this technique to incorporate additional temporal constraints and to different applications.



### Instance Retrieval at Fine-grained Level Using Multi-Attribute Recognition
- **Arxiv ID**: http://arxiv.org/abs/1811.02949v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.02949v1)
- **Published**: 2018-11-07 15:54:23+00:00
- **Updated**: 2018-11-07 15:54:23+00:00
- **Authors**: Roshanak Zakizadeh, Yu Qian, Michele Sasdelli, Eduard Vazquez
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a method for instance ranking and retrieval at fine-grained level based on the global features extracted from a multi-attribute recognition model which is not dependent on landmarks information or part-based annotations. Further, we make this architecture suitable for mobile-device application by adopting the bilinear CNN to make the multi-attribute recognition model smaller (in terms of the number of parameters). The experiments run on the Dress category of DeepFashion In-Shop Clothes Retrieval and CUB200 datasets show that the results of instance retrieval at fine-grained level are promising for these datasets, specially in terms of texture and color.



### A Holistic Visual Place Recognition Approach using Lightweight CNNs for Significant ViewPoint and Appearance Changes
- **Arxiv ID**: http://arxiv.org/abs/1811.03032v4
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1811.03032v4)
- **Published**: 2018-11-07 17:33:52+00:00
- **Updated**: 2019-10-27 14:59:19+00:00
- **Authors**: Ahmad Khaliq, Shoaib Ehsan, Zetao Chen, Michael Milford, Klaus McDonald-Maier
- **Comment**: Conditionally Accepted as short paper at IEEE Transactions on
  Robotics (T-RO)
- **Journal**: None
- **Summary**: This paper presents a lightweight visual place recognition approach, capable of achieving high performance with low computational cost, and feasible for mobile robotics under significant viewpoint and appearance changes. Results on several benchmark datasets confirm an average boost of 13% in accuracy, and 12x average speedup relative to state-of-the-art methods.



### Adapting End-to-End Neural Speaker Verification to New Languages and Recording Conditions with Adversarial Training
- **Arxiv ID**: http://arxiv.org/abs/1811.03055v1
- **DOI**: None
- **Categories**: **eess.AS**, cs.CV, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/1811.03055v1)
- **Published**: 2018-11-07 18:15:34+00:00
- **Updated**: 2018-11-07 18:15:34+00:00
- **Authors**: Gautam Bhattacharya, Jahangir Alam, Patrick Kenny
- **Comment**: Submitted to ICASSP 2019
- **Journal**: None
- **Summary**: In this article we propose a novel approach for adapting speaker embeddings to new domains based on adversarial training of neural networks. We apply our embeddings to the task of text-independent speaker verification, a challenging, real-world problem in biometric security. We further the development of end-to-end speaker embedding models by combing a novel 1-dimensional, self-attentive residual network, an angular margin loss function and adversarial training strategy. Our model is able to learn extremely compact, 64-dimensional speaker embeddings that deliver competitive performance on a number of popular datasets using simple cosine distance scoring. One the NIST-SRE 2016 task we are able to beat a strong i-vector baseline, while on the Speakers in the Wild task our model was able to outperform both i-vector and x-vector baselines, showing an absolute improvement of 2.19% over the latter. Additionally, we show that the integration of adversarial training consistently leads to a significant improvement over an unadapted model.



### FLOPs as a Direct Optimization Objective for Learning Sparse Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1811.03060v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.03060v2)
- **Published**: 2018-11-07 18:21:25+00:00
- **Updated**: 2018-11-23 18:20:10+00:00
- **Authors**: Raphael Tang, Ashutosh Adhikari, Jimmy Lin
- **Comment**: 4 pages, accepted to the NIPS 2018 Workshop on Compact Deep Neural
  Networks with Industrial Applications (CDNNRIA)
- **Journal**: None
- **Summary**: There exists a plethora of techniques for inducing structured sparsity in parametric models during the optimization process, with the final goal of resource-efficient inference. However, few methods target a specific number of floating-point operations (FLOPs) as part of the optimization objective, despite many reporting FLOPs as part of the results. Furthermore, a one-size-fits-all approach ignores realistic system constraints, which differ significantly between, say, a GPU and a mobile phone -- FLOPs on the former incur less latency than on the latter; thus, it is important for practitioners to be able to specify a target number of FLOPs during model compression. In this work, we extend a state-of-the-art technique to directly incorporate FLOPs as part of the optimization objective and show that, given a desired FLOPs requirement, different neural networks can be successfully trained for image classification.



### Generative Adversarial Speaker Embedding Networks for Domain Robust End-to-End Speaker Verification
- **Arxiv ID**: http://arxiv.org/abs/1811.03063v1
- **DOI**: None
- **Categories**: **eess.AS**, cs.CV, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/1811.03063v1)
- **Published**: 2018-11-07 18:23:01+00:00
- **Updated**: 2018-11-07 18:23:01+00:00
- **Authors**: Gautam Bhattacharya, Joao Monteiro, Jahangir Alam, Patrick Kenny
- **Comment**: Submitted to ICASSP 2019
- **Journal**: None
- **Summary**: This article presents a novel approach for learning domain-invariant speaker embeddings using Generative Adversarial Networks. The main idea is to confuse a domain discriminator so that is can't tell if embeddings are from the source or target domains. We train several GAN variants using our proposed framework and apply them to the speaker verification task. On the challenging NIST-SRE 2016 dataset, we are able to match the performance of a strong baseline x-vector system. In contrast to the the baseline systems which are dependent on dimensionality reduction (LDA) and an external classifier (PLDA), our proposed speaker embeddings can be scored using simple cosine distance. This is achieved by optimizing our models end-to-end, using an angular margin loss function. Furthermore, we are able to significantly boost verification performance by averaging our different GAN models at the score level, achieving a relative improvement of 7.2% over the baseline.



### Prototypical Clustering Networks for Dermatological Disease Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/1811.03066v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.03066v1)
- **Published**: 2018-11-07 18:27:41+00:00
- **Updated**: 2018-11-07 18:27:41+00:00
- **Authors**: Viraj Prabhu, Anitha Kannan, Murali Ravuri, Manish Chablani, David Sontag, Xavier Amatriain
- **Comment**: None
- **Journal**: None
- **Summary**: We consider the problem of image classification for the purpose of aiding doctors in dermatological diagnosis. Dermatological diagnosis poses two major challenges for standard off-the-shelf techniques: First, the data distribution is typically extremely long tailed. Second, intra-class variability is often large. To address the first issue, we formulate the problem as low-shot learning, where once deployed, a base classifier must rapidly generalize to diagnose novel conditions given very few labeled examples. To model diverse classes effectively, we propose Prototypical Clustering Networks (PCN), an extension to Prototypical Networks that learns a mixture of prototypes for each class. Prototypes are initialized for each class via clustering and refined via an online update scheme. Classification is performed by measuring similarity to a weighted combination of prototypes within a class, where the weights are the inferred cluster responsibilities. We demonstrate the strengths of our approach in effective diagnosis on a realistic dataset of dermatological conditions.



### ColorUNet: A convolutional classification approach to colorization
- **Arxiv ID**: http://arxiv.org/abs/1811.03120v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.03120v1)
- **Published**: 2018-11-07 19:20:59+00:00
- **Updated**: 2018-11-07 19:20:59+00:00
- **Authors**: Vincent Billaut, Matthieu de Rochemonteix, Marc Thibault
- **Comment**: 9 pages, 10 figures Stanford University CS231n project
- **Journal**: None
- **Summary**: This paper tackles the challenge of colorizing grayscale images. We take a deep convolutional neural network approach, and choose to take the angle of classification, working on a finite set of possible colors. Similarly to a recent paper, we implement a loss and a prediction function that favor realistic, colorful images rather than "true" ones.   We show that a rather lightweight architecture inspired by the U-Net, and trained on a reasonable amount of pictures of landscapes, achieves satisfactory results on this specific subset of pictures. We show that data augmentation significantly improves the performance and robustness of the model, and provide visual analysis of the prediction confidence.   We show an application of our model, extending the task to video colorization. We suggest a way to smooth color predictions across frames, without the need to train a recurrent network designed for sequential inputs.



### DragonPaint: Rule based bootstrapping for small data with an application to cartoon coloring
- **Arxiv ID**: http://arxiv.org/abs/1811.03151v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.GR, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.03151v1)
- **Published**: 2018-11-07 21:23:31+00:00
- **Updated**: 2018-11-07 21:23:31+00:00
- **Authors**: K. Gretchen Greene
- **Comment**: None
- **Journal**: In Proceedings of the Fourth International Conference on
  Predictive Applications and APIs, 82, 1-9, Boston, MA, USA, 2018. PMLR
- **Summary**: In this paper, we confront the problem of deep learning's big labeled data requirements, offer a rule based strategy for extreme augmentation of small data sets and apply that strategy with the image to image translation model by Isola et al. (2016) to automate cel style cartoon coloring with very limited training data. While our experimental results using geometric rules and transformations demonstrate the performance of our methods on an image translation task with industry applications in art, design and animation, we also propose the use of rules on partial data sets as a generalizable small data strategy, potentially applicable across data types and domains.



### Forensic Discrimination between Traditional and Compressive Imaging Systems
- **Arxiv ID**: http://arxiv.org/abs/1811.03157v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.03157v1)
- **Published**: 2018-11-07 21:42:13+00:00
- **Updated**: 2018-11-07 21:42:13+00:00
- **Authors**: Ali Taimori, Farokh Marvasti
- **Comment**: None
- **Journal**: None
- **Summary**: Compressive sensing is a new technology for modern computational imaging systems. In comparison to widespread conventional image sensing, the compressive imaging paradigm requires specific forensic analysis techniques and tools. In this regards, one of basic scenarios in image forensics is to distinguish traditionally sensed images from sophisticated compressively sensed ones. To do this, we first mathematically and systematically model the imaging system based on compressive sensing technology. Afterwards, a simplified version of the whole model is presented, which is appropriate for forensic investigation applications. We estimate the nonlinear system of compressive sensing with a linear model. Then, we model the imaging pipeline as an inverse problem and demonstrate that different imagers have discriminative degradation kernels. Hence, blur kernels of various imaging systems have utilized as footprints for discriminating image acquisition sources. In order to accomplish the identification cycle, we have utilized the state-of-the-art Convolutional Neural Network (CNN) and Support Vector Machine (SVM) approaches to learn a classification system from estimated blur kernels. Numerical experiments show promising identification results. Simulation codes are available for research and development purposes.



### Automatic Thresholding of SIFT Descriptors
- **Arxiv ID**: http://arxiv.org/abs/1811.03173v1
- **DOI**: 10.1109/ICIP.2016.7532365
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.03173v1)
- **Published**: 2018-11-07 22:43:53+00:00
- **Updated**: 2018-11-07 22:43:53+00:00
- **Authors**: Matthew R. Kirchner
- **Comment**: In the proceedings of the 2016 IEEE International Conference on Image
  Processing (ICIP), pp. 291-295
- **Journal**: IEEE International Conference on Image Processing, pp. 291-295,
  2016
- **Summary**: We introduce a method to perform automatic thresholding of SIFT descriptors that improves matching performance by at least 15.9% on the Oxford image matching benchmark. The method uses a contrario methodology to determine a unique bin magnitude threshold. This is done by building a generative uniform background model for descriptors and determining when bin magnitudes have reached a sufficient level. The presented method, called meaningful clamping, contrasts from the current SIFT implementation by efficiently computing a clamping threshold that is unique for every descriptor.



### Solving Jigsaw Puzzles By the Graph Connection Laplacian
- **Arxiv ID**: http://arxiv.org/abs/1811.03188v5
- **DOI**: 10.1137/19M1290760
- **Categories**: **cs.CV**, cs.LG, cs.NA, math.NA, 90C20, 90C27, 90C35, 90C90
- **Links**: [PDF](http://arxiv.org/pdf/1811.03188v5)
- **Published**: 2018-11-07 23:45:03+00:00
- **Updated**: 2020-11-01 12:04:30+00:00
- **Authors**: Vahan Huroyan, Gilad Lerman, Hau-Tieng Wu
- **Comment**: None
- **Journal**: SIAM J. Imaging Sci. 13(4) (2020) 1717-1753
- **Summary**: We propose a novel mathematical framework to address the problem of automatically solving large jigsaw puzzles. This problem assumes a large image, which is cut into equal square pieces that are arbitrarily rotated and shuffled, and asks to recover the original image given the transformed pieces. The main contribution of this work is a method for recovering the rotations of the pieces when both shuffles and rotations are unknown. A major challenge of this procedure is estimating the graph connection Laplacian without the knowledge of shuffles. A careful combination of our proposed method for estimating rotations with any existing method for estimating shuffles results in a practical solution for the jigsaw puzzle problem. Our theory guarantees, in a clean setting, that our basic idea of recovering rotations is robust to some corruption of the connection graph. Numerical experiments demonstrate the competitive accuracy of this solution, its robustness to corruption and, its computational advantage for large puzzles.



