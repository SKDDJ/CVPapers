# Arxiv Papers in cs.CV on 2018-11-08
### Correlation Filter Selection for Visual Tracking Using Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/1811.03196v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.03196v1)
- **Published**: 2018-11-08 00:24:42+00:00
- **Updated**: 2018-11-08 00:24:42+00:00
- **Authors**: Yanchun Xie, Jimin Xiao, Kaizhu Huang, Jeyarajan Thiyagalingam, Yao Zhao
- **Comment**: 13 pages, 11 figures
- **Journal**: None
- **Summary**: Correlation filter has been proven to be an effective tool for a number of approaches in visual tracking, particularly for seeking a good balance between tracking accuracy and speed. However, correlation filter based models are susceptible to wrong updates stemming from inaccurate tracking results. To date, little effort has been devoted towards handling the correlation filter update problem. In this paper, we propose a novel approach to address the correlation filter update problem. In our approach, we update and maintain multiple correlation filter models in parallel, and we use deep reinforcement learning for the selection of an optimal correlation filter model among them. To facilitate the decision process in an efficient manner, we propose a decision-net to deal target appearance modeling, which is trained through hundreds of challenging videos using proximal policy optimization and a lightweight learning network. An exhaustive evaluation of the proposed approach on the OTB100 and OTB2013 benchmarks show that the approach is effective enough to achieve the average success rate of 62.3% and the average precision score of 81.2%, both exceeding the performance of traditional correlation filter based trackers.



### Deep Semantic Instance Segmentation of Tree-like Structures Using Synthetic Data
- **Arxiv ID**: http://arxiv.org/abs/1811.03208v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.03208v1)
- **Published**: 2018-11-08 01:23:45+00:00
- **Updated**: 2018-11-08 01:23:45+00:00
- **Authors**: Kerry Halupka, Rahil Garnavi, Stephen Moore
- **Comment**: Accepted to WACV 2019
- **Journal**: None
- **Summary**: Tree-like structures, such as blood vessels, often express complexity at very fine scales, requiring high-resolution grids to adequately describe their shape. Such sparse morphology can alternately be represented by locations of centreline points, but learning from this type of data with deep learning is challenging due to it being unordered, and permutation invariant. In this work, we propose a deep neural network that directly consumes unordered points along the centreline of a branching structure, to identify the topology of the represented structure in a single-shot. Key to our approach is the use of a novel multi-task loss function, enabling instance segmentation of arbitrarily complex branching structures. We train the network solely using synthetically generated data, utilizing domain randomization to facilitate the transfer to real 2D and 3D data. Results show that our network can reliably extract meaningful information about branch locations, bifurcations and endpoints, and sets a new benchmark for semantic instance segmentation in branching structures.



### An Infinite Parade of Giraffes: Expressive Augmentation and Complexity Layers for Cartoon Drawing
- **Arxiv ID**: http://arxiv.org/abs/1811.07023v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.07023v1)
- **Published**: 2018-11-08 01:28:12+00:00
- **Updated**: 2018-11-08 01:28:12+00:00
- **Authors**: K. G. Greene
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we explore creative image generation constrained by small data. To partially automate the creation of cartoon sketches consistent with a specific designer's style, where acquiring a very large original image set is impossible or cost prohibitive, we exploit domain specific knowledge for a huge reduction in original image requirements, creating an effectively infinite number of cartoon giraffes from just nine original drawings. We introduce "expressive augmentations" for cartoon sketches, mathematical transformations that create broad domain appropriate variation, far beyond the usual affine transformations, and we show that chained GANs models trained on the temporal stages of drawing or "complexity layers" can effectively add character appropriate details and finish new drawings in the designer's style.   We discuss the application of these tools in design processes for textiles, graphics, architectural elements and interior design.



### Satyam: Democratizing Groundtruth for Machine Vision
- **Arxiv ID**: http://arxiv.org/abs/1811.03621v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV, cs.LG, cs.SY, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.03621v1)
- **Published**: 2018-11-08 01:35:47+00:00
- **Updated**: 2018-11-08 01:35:47+00:00
- **Authors**: Hang Qiu, Krishna Chintalapudi, Ramesh Govindan
- **Comment**: None
- **Journal**: None
- **Summary**: The democratization of machine learning (ML) has led to ML-based machine vision systems for autonomous driving, traffic monitoring, and video surveillance. However, true democratization cannot be achieved without greatly simplifying the process of collecting groundtruth for training and testing these systems. This groundtruth collection is necessary to ensure good performance under varying conditions. In this paper, we present the design and evaluation of Satyam, a first-of-its-kind system that enables a layperson to launch groundtruth collection tasks for machine vision with minimal effort. Satyam leverages a crowdtasking platform, Amazon Mechanical Turk, and automates several challenging aspects of groundtruth collection: creating and launching of custom web-UI tasks for obtaining the desired groundtruth, controlling result quality in the face of spammers and untrained workers, adapting prices to match task complexity, filtering spammers and workers with poor performance, and processing worker payments. We validate Satyam using several popular benchmark vision datasets, and demonstrate that groundtruth obtained by Satyam is comparable to that obtained from trained experts and provides matching ML performance when used for training.



### Facial Landmark Detection for Manga Images
- **Arxiv ID**: http://arxiv.org/abs/1811.03214v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1811.03214v1)
- **Published**: 2018-11-08 01:36:51+00:00
- **Updated**: 2018-11-08 01:36:51+00:00
- **Authors**: Marco Stricker, Olivier Augereau, Koichi Kise, Motoi Iwata
- **Comment**: None
- **Journal**: None
- **Summary**: The topic of facial landmark detection has been widely covered for pictures of human faces, but it is still a challenge for drawings. Indeed, the proportions and symmetry of standard human faces are not always used for comics or mangas. The personal style of the author, the limitation of colors, etc. makes the landmark detection on faces in drawings a difficult task. Detecting the landmarks on manga images will be useful to provide new services for easily editing the character faces, estimating the character emotions, or generating automatically some animations such as lip or eye movements.   This paper contains two main contributions: 1) a new landmark annotation model for manga faces, and 2) a deep learning approach to detect these landmarks. We use the "Deep Alignment Network", a multi stage architecture where the first stage makes an initial estimation which gets refined in further stages. The first results show that the proposed method succeed to accurately find the landmarks in more than 80% of the cases.



### RGB-D SLAM in Dynamic Environments Using Point Correlations
- **Arxiv ID**: http://arxiv.org/abs/1811.03217v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1811.03217v2)
- **Published**: 2018-11-08 01:52:00+00:00
- **Updated**: 2020-07-20 08:55:15+00:00
- **Authors**: Weichen Dai, Yu Zhang, Ping Li, Zheng Fang, Sebastian Scherer
- **Comment**: 18 pages
- **Journal**: None
- **Summary**: In this paper, a simultaneous localization and mapping (SLAM) method that eliminates the influence of moving objects in dynamic environments is proposed. This method utilizes the correlation between map points to separate points that are part of the static scene and points that are part of different moving objects into different groups. A sparse graph is first created using Delaunay triangulation from all map points. In this graph, the vertices represent map points, and each edge represents the correlation between adjacent points. If the relative position between two points remains consistent over time, there is correlation between them, and they are considered to be moving together rigidly. If not, they are considered to have no correlation and to be in separate groups. After the edges between the uncorrelated points are removed during point-correlation optimization, the remaining graph separates the map points of the moving objects from the map points of the static scene. The largest group is assumed to be the group of reliable static map points. Finally, motion estimation is performed using only these points. The proposed method was implemented for RGB-D sensors, evaluated with a public RGB-D benchmark, and tested in several additional challenging environments. The experimental results demonstrate that robust and accurate performance can be achieved by the proposed SLAM method in both slightly and highly dynamic environments. Compared with other state-of-the-art methods, the proposed method can provide competitive accuracy with good real-time performance.



### Advanced machine learning informatics modeling using clinical and radiological imaging metrics for characterizing breast tumor characteristics with the OncotypeDX gene array
- **Arxiv ID**: http://arxiv.org/abs/1811.03218v1
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.AI, cs.CV, cs.LG, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1811.03218v1)
- **Published**: 2018-11-08 01:53:22+00:00
- **Updated**: 2018-11-08 01:53:22+00:00
- **Authors**: Michael A. Jacobs, Christopher Umbricht, Vishwa Parekh, Riham El Khouli, Leslie Cope, Katarzyna J. Macura, Susan Harvey, Antonio C. Wolff
- **Comment**: 32 pages, 6 figures, Abstract number SSQ01-04:Radiological Society of
  North America 2015 Scientific Assembly and Annual Meeting,Chicago IL
- **Journal**: None
- **Summary**: Purpose-Optimal use of established and imaging methods, such as multiparametric magnetic resonance imaging(mpMRI) can simultaneously identify key functional parameters and provide unique imaging phenotypes of breast cancer. Therefore, we have developed and implemented a new machine-learning informatic system that integrates clinical variables, derived from imaging and clinical health records, to compare with the 21-gene array assay, OncotypeDX. Materials and methods-We tested our informatics modeling in a subset of patients (n=81) who had ER+ disease and underwent OncotypeDX gene expression and breast mpMRI testing. The machine-learning informatic method is termed Integrated Radiomic Informatic System-IRIS was applied to the mpMRI, clinical and pathologic descriptors, as well as a gene array analysis. The IRIS method using an advanced graph theoretic model and quantitative metrics. Summary statistics (mean and standard deviations) for the quantitative imaging parameters were obtained. Sensitivity and specificity and Area Under the Curve were calculated for the classification of the patients. Results-The OncotypeDX classification by IRIS model had sensitivity of 95% and specificity of 89% with AUC of 0.92. The breast lesion size was larger for the high-risk groups and lower for both low risk and intermediate risk groups. There were significant differences in PK-DCE and ADC map values in each group. The ADC map values for high- and intermediate-risk groups were significantly lower than the low-risk group. Conclusion-These initial studies provide deeper understandings of imaging features and molecular gene array OncotypeDX score. This insight provides the foundation to relate these imaging features to the assessment of treatment response for improved personalized medicine.



### Knowledge Transfer via Distillation of Activation Boundaries Formed by Hidden Neurons
- **Arxiv ID**: http://arxiv.org/abs/1811.03233v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.03233v2)
- **Published**: 2018-11-08 02:47:56+00:00
- **Updated**: 2018-12-14 15:29:53+00:00
- **Authors**: Byeongho Heo, Minsik Lee, Sangdoo Yun, Jin Young Choi
- **Comment**: Accepted to AAAI 2019
- **Journal**: None
- **Summary**: An activation boundary for a neuron refers to a separating hyperplane that determines whether the neuron is activated or deactivated. It has been long considered in neural networks that the activations of neurons, rather than their exact output values, play the most important role in forming classification friendly partitions of the hidden feature space. However, as far as we know, this aspect of neural networks has not been considered in the literature of knowledge transfer. In this paper, we propose a knowledge transfer method via distillation of activation boundaries formed by hidden neurons. For the distillation, we propose an activation transfer loss that has the minimum value when the boundaries generated by the student coincide with those by the teacher. Since the activation transfer loss is not differentiable, we design a piecewise differentiable loss approximating the activation transfer loss. By the proposed method, the student learns a separating boundary between activation region and deactivation region formed by each neuron in the teacher. Through the experiments in various aspects of knowledge transfer, it is verified that the proposed method outperforms the current state-of-the-art.



### High Speed Tracking With A Fourier Domain Kernelized Correlation Filter
- **Arxiv ID**: http://arxiv.org/abs/1811.03236v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.03236v3)
- **Published**: 2018-11-08 02:59:31+00:00
- **Updated**: 2019-02-22 06:03:32+00:00
- **Authors**: Mingyang Guan, Zhengguo Li, Renjie He, Changyun Wen
- **Comment**: None
- **Journal**: None
- **Summary**: It is challenging to design a high speed tracking approach using l1-norm due to its non-differentiability. In this paper, a new kernelized correlation filter is introduced by leveraging the sparsity attribute of l1-norm based regularization to design a high speed tracker. We combine the l1-norm and l2-norm based regularizations in one Huber-type loss function, and then formulate an optimization problem in the Fourier Domain for fast computation, which enables the tracker to adaptively ignore the noisy features produced from occlusion and illumination variation, while keep the advantages of l2-norm based regression. This is achieved due to the attribute of Convolution Theorem that the correlation in spatial domain corresponds to an element-wise product in the Fourier domain, resulting in that the l1-norm optimization problem could be decomposed into multiple sub-optimization spaces in the Fourier domain. But the optimized variables in the Fourier domain are complex, which makes using the l1-norm impossible if the real and imaginary parts of the variables cannot be separated. However, our proposed optimization problem is formulated in such a way that their real part and imaginary parts are indeed well separated. As such, the proposed optimization problem can be solved efficiently to obtain their optimal values independently with closed-form solutions. Extensive experiments on two large benchmark datasets demonstrate that the proposed tracking algorithm significantly improves the tracking accuracy of the original kernelized correlation filter (KCF) while with little sacrifice on tracking speed. Moreover, it outperforms the state-of-the-art approaches in terms of accuracy, efficiency, and robustness.



### Model Selection for Generalized Zero-shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1811.03252v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.03252v1)
- **Published**: 2018-11-08 03:47:46+00:00
- **Updated**: 2018-11-08 03:47:46+00:00
- **Authors**: Hongguang Zhang, Piotr Koniusz
- **Comment**: None
- **Journal**: None
- **Summary**: In the problem of generalized zero-shot learning, the datapoints from unknown classes are not available during training. The main challenge for generalized zero-shot learning is the unbalanced data distribution which makes it hard for the classifier to distinguish if a given testing sample comes from a seen or unseen class. However, using Generative Adversarial Network (GAN) to generate auxiliary datapoints by the semantic embeddings of unseen classes alleviates the above problem. Current approaches combine the auxiliary datapoints and original training data to train the generalized zero-shot learning model and obtain state-of-the-art results. Inspired by such models, we propose to feed the generated data via a model selection mechanism. Specifically, we leverage two sources of datapoints (observed and auxiliary) to train some classifier to recognize which test datapoints come from seen and which from unseen classes. This way, generalized zero-shot learning can be divided into two disjoint classification tasks, thus reducing the negative influence of the unbalanced data distribution. Our evaluations on four publicly available datasets for generalized zero-shot learning show that our model obtains state-of-the-art results.



### Calibration Wizard: A Guidance System for Camera Calibration Based on Modelling Geometric and Corner Uncertainty
- **Arxiv ID**: http://arxiv.org/abs/1811.03264v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.03264v2)
- **Published**: 2018-11-08 04:40:09+00:00
- **Updated**: 2019-09-03 12:51:32+00:00
- **Authors**: Songyou Peng, Peter Sturm
- **Comment**: Oral presentation at ICCV 2019
- **Journal**: None
- **Summary**: It is well known that the accuracy of a calibration depends strongly on the choice of camera poses from which images of a calibration object are acquired. We present a system -- Calibration Wizard -- that interactively guides a user towards taking optimal calibration images. For each new image to be taken, the system computes, from all previously acquired images, the pose that leads to the globally maximum reduction of expected uncertainty on intrinsic parameters and then guides the user towards that pose. We also show how to incorporate uncertainty in corner point position in a novel principled manner, for both, calibration and computation of the next best pose. Synthetic and real-world experiments are performed to demonstrate the effectiveness of Calibration Wizard.



### Ordinal Regression using Noisy Pairwise Comparisons for Body Mass Index Range Estimation
- **Arxiv ID**: http://arxiv.org/abs/1811.03268v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.03268v1)
- **Published**: 2018-11-08 04:55:46+00:00
- **Updated**: 2018-11-08 04:55:46+00:00
- **Authors**: Luisa Polania, Dongning Wang, Glenn Fung
- **Comment**: Paper accepted for publication at the 2019 IEEE Winter Conference on
  Applications of Computer Vision (WACV 2019)
- **Journal**: None
- **Summary**: Ordinal regression aims to classify instances into ordinal categories. In this paper, body mass index (BMI) category estimation from facial images is cast as an ordinal regression problem. In particular, noisy binary search algorithms based on pairwise comparisons are employed to exploit the ordinal relationship among BMI categories. Comparisons are performed with Siamese architectures, one of which uses the Bradley-Terry model probabilities as target. The Bradley-Terry model is an approach to describe probabilities of the possible outcomes when elements of a set are repeatedly compared with one another in pairs. Experimental results show that our approach outperforms classification and regression-based methods at estimating BMI categories.



### A Retinex-based Image Enhancement Scheme with Noise Aware Shadow-up Function
- **Arxiv ID**: http://arxiv.org/abs/1811.03280v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.03280v1)
- **Published**: 2018-11-08 05:35:02+00:00
- **Updated**: 2018-11-08 05:35:02+00:00
- **Authors**: Chien Cheng Chien, Yuma Kinoshita, Sayaka Shiota, Hitoshi Kiya
- **Comment**: To appear in IWAIT-IFMIA 2019
- **Journal**: None
- **Summary**: This paper proposes a novel image contrast enhancement method based on both a noise aware shadow-up function and Retinex (retina and cortex) decomposition. Under low light conditions, images taken by digital cameras have low contrast in dark or bright regions. This is due to a limited dynamic range that imaging sensors have. For this reason, various contrast enhancement methods have been proposed. Our proposed method can enhance the contrast of images without not only over-enhancement but also noise amplification. In the proposed method, an image is decomposed into illumination layer and reflectance layer based on the retinex theory, and lightness information of the illumination layer is adjusted. A shadow-up function is used for preventing over-enhancement. The proposed mapping function, designed by using a noise aware histogram, allows not only to enhance contrast of dark region, but also to avoid amplifying noise, even under strong noise environments.



### Doc2Im: document to image conversion through self-attentive embedding
- **Arxiv ID**: http://arxiv.org/abs/1811.03291v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1811.03291v1)
- **Published**: 2018-11-08 06:51:46+00:00
- **Updated**: 2018-11-08 06:51:46+00:00
- **Authors**: Mithun Das Gupta
- **Comment**: None
- **Journal**: None
- **Summary**: Text classification is a fundamental task in NLP applications. Latest research in this field has largely been divided into two major sub-fields. Learning representations is one sub-field and learning deeper models, both sequential and convolutional, which again connects back to the representation is the other side. We posit the idea that the stronger the representation is, the simpler classifier models are needed to achieve higher performance. In this paper we propose a completely novel direction to text classification research, wherein we convert text to a representation very similar to images, such that any deep network able to handle images is equally able to handle text. We take a deeper look at the representation of documents as an image and subsequently utilize very simple convolution based models taken as is from computer vision domain. This image can be cropped, re-scaled, re-sampled and augmented just like any other image to work with most of the state-of-the-art large convolution based models which have been designed to handle large image datasets. We show impressive results with some of the latest benchmarks in the related fields. We perform transfer learning experiments, both from text to text domain and also from image to text domain. We believe this is a paradigm shift from the way document understanding and text classification has been traditionally done, and will drive numerous novel research ideas in the community.



### BAR: Bayesian Activity Recognition using variational inference
- **Arxiv ID**: http://arxiv.org/abs/1811.03305v2
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.03305v2)
- **Published**: 2018-11-08 08:04:09+00:00
- **Updated**: 2018-12-01 08:08:34+00:00
- **Authors**: Ranganath Krishnan, Mahesh Subedar, Omesh Tickoo
- **Comment**: None
- **Journal**: None
- **Summary**: Uncertainty estimation in deep neural networks is essential for designing reliable and robust AI systems. Applications such as video surveillance for identifying suspicious activities are designed with deep neural networks (DNNs), but DNNs do not provide uncertainty estimates. Capturing reliable uncertainty estimates in safety and security critical applications will help to establish trust in the AI system. Our contribution is to apply Bayesian deep learning framework to visual activity recognition application and quantify model uncertainty along with principled confidence. We utilize the stochastic variational inference technique while training the Bayesian DNNs to infer the approximate posterior distribution around model parameters and perform Monte Carlo sampling on the posterior of model parameters to obtain the predictive distribution. We show that the Bayesian inference applied to DNNs provide reliable confidence measures for visual activity recognition task as compared to conventional DNNs. We also show that our method improves the visual activity recognition precision-recall AUC by 6.2% compared to non-Bayesian baseline. We evaluate our models on Moments-In-Time (MiT) activity recognition dataset by selecting a subset of in- and out-of-distribution video samples.



### Improving Multi-Person Pose Estimation using Label Correction
- **Arxiv ID**: http://arxiv.org/abs/1811.03331v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.03331v1)
- **Published**: 2018-11-08 09:38:38+00:00
- **Updated**: 2018-11-08 09:38:38+00:00
- **Authors**: Naoki Kato, Tianqi Li, Kohei Nishino, Yusuke Uchida
- **Comment**: None
- **Journal**: None
- **Summary**: Significant attention is being paid to multi-person pose estimation methods recently, as there has been rapid progress in the field owing to convolutional neural networks. Especially, recent method which exploits part confidence maps and Part Affinity Fields (PAFs) has achieved accurate real-time prediction of multi-person keypoints. However, human annotated labels are sometimes inappropriate for learning models. For example, if there is a limb that extends outside an image, a keypoint for the limb may not have annotations because it exists outside of the image, and thus the labels for the limb can not be generated. If a model is trained with data including such missing labels, the output of the model for the location, even though it is correct, is penalized as a false positive, which is likely to cause negative effects on the performance of the model. In this paper, we point out the existence of some patterns of inappropriate labels, and propose a novel method for correcting such labels with a teacher model trained on such incomplete data. Experiments on the COCO dataset show that training with the corrected labels improves the performance of the model and also speeds up training.



### Repetitive Motion Estimation Network: Recover cardiac and respiratory signal from thoracic imaging
- **Arxiv ID**: http://arxiv.org/abs/1811.03343v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1811.03343v1)
- **Published**: 2018-11-08 10:30:10+00:00
- **Updated**: 2018-11-08 10:30:10+00:00
- **Authors**: Xiaoxiao Li, Vivek Singh, Yifan Wu, Klaus Kirchberg, James Duncan, Ankur Kapoor
- **Comment**: Accepted by NIPS workshop MED-NIPS 2018
- **Journal**: None
- **Summary**: Tracking organ motion is important in image-guided interventions, but motion annotations are not always easily available. Thus, we propose Repetitive Motion Estimation Network (RMEN) to recover cardiac and respiratory signals. It learns the spatio-temporal repetition patterns, embedding high dimensional motion manifolds to 1D vectors with partial motion phase boundary annotations. Compared with the best alternative models, our proposed RMEN significantly decreased the QRS peaks detection offsets by 59.3%. Results showed that RMEN could handle the irregular cardiac and respiratory motion cases. Repetitive motion patterns learned by RMEN were visualized and indicated in the feature maps.



### Activation Functions: Comparison of trends in Practice and Research for Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1811.03378v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1811.03378v1)
- **Published**: 2018-11-08 12:28:43+00:00
- **Updated**: 2018-11-08 12:28:43+00:00
- **Authors**: Chigozie Nwankpa, Winifred Ijomah, Anthony Gachagan, Stephen Marshall
- **Comment**: 20 pages, 6 figures, 2 tables
- **Journal**: None
- **Summary**: Deep neural networks have been successfully used in diverse emerging domains to solve real world complex problems with may more deep learning(DL) architectures, being developed to date. To achieve these state-of-the-art performances, the DL architectures use activation functions (AFs), to perform diverse computations between the hidden layers and the output layers of any given DL architecture. This paper presents a survey on the existing AFs used in deep learning applications and highlights the recent trends in the use of the activation functions for deep learning applications. The novelty of this paper is that it compiles majority of the AFs used in DL and outlines the current trends in the applications and usage of these functions in practical deep learning deployments against the state-of-the-art research results. This compilation will aid in making effective decisions in the choice of the most suitable and appropriate activation function for any given application, ready for deployment. This paper is timely because most research papers on AF highlights similar works and results while this paper will be the first, to compile the trends in AF applications in practice against the research results from literature, found in deep learning research to date.



### Active Learning using Deep Bayesian Networks for Surgical Workflow Analysis
- **Arxiv ID**: http://arxiv.org/abs/1811.03382v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.03382v2)
- **Published**: 2018-11-08 12:42:05+00:00
- **Updated**: 2019-04-02 15:25:35+00:00
- **Authors**: Sebastian Bodenstedt, Dominik Rivoir, Alexander Jenke, Martin Wagner, Michael Breucha, Beat Müller-Stich, Sören Torge Mees, Jürgen Weitz, Stefanie Speidel
- **Comment**: None
- **Journal**: None
- **Summary**: For many applications in the field of computer assisted surgery, such as providing the position of a tumor, specifying the most probable tool required next by the surgeon or determining the remaining duration of surgery, methods for surgical workflow analysis are a prerequisite. Often machine learning based approaches serve as basis for surgical workflow analysis. In general machine learning algorithms, such as convolutional neural networks (CNN), require large amounts of labeled data. While data is often available in abundance, many tasks in surgical workflow analysis need data annotated by domain experts, making it difficult to obtain a sufficient amount of annotations.   The aim of using active learning to train a machine learning model is to reduce the annotation effort. Active learning methods determine which unlabeled data points would provide the most information according to some metric, such as prediction uncertainty. Experts will then be asked to only annotate these data points. The model is then retrained with the new data and used to select further data for annotation. Recently, active learning has been applied to CNN by means of Deep Bayesian Networks (DBN). These networks make it possible to assign uncertainties to predictions.   In this paper, we present a DBN-based active learning approach adapted for image-based surgical workflow analysis task. Furthermore, by using a recurrent architecture, we extend this network to video-based surgical workflow analysis. We evaluate these approaches on the Cholec80 dataset by performing instrument presence detection and surgical phase segmentation. Here we are able to show that using a DBN-based active learning approach for selecting what data points to annotate next outperforms a baseline based on randomly selecting data points.



### Prediction of laparoscopic procedure duration using unlabeled, multimodal sensor data
- **Arxiv ID**: http://arxiv.org/abs/1811.03384v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.03384v2)
- **Published**: 2018-11-08 12:47:03+00:00
- **Updated**: 2019-04-03 15:00:42+00:00
- **Authors**: Sebastian Bodenstedt, Martin Wagner, Lars Mündermann, Hannes Kenngott, Beat Müller-Stich, Michael Breucha, Sören Torge Mees, Jürgen Weitz, Stefanie Speidel
- **Comment**: None
- **Journal**: None
- **Summary**: Purpose The course of surgical procedures is often unpredictable, making it difficult to estimate the duration of procedures beforehand. A context-aware method that analyses the workflow of an intervention online and automatically predicts the remaining duration would alleviate these problems. As basis for such an estimate, information regarding the current state of the intervention is required. Methods Today, the operating room contains a diverse range of sensors. During laparoscopic interventions, the endoscopic video stream is an ideal source of such information. Extracting quantitative information from the video is challenging though, due to its high dimensionality. Other surgical devices (e.g. insufflator, lights, etc.) provide data streams which are, in contrast to the video stream, more compact and easier to quantify. Though whether such streams offer sufficient information for estimating the duration of surgery is uncertain. Here, we propose and compare methods, based on convolutional neural networks, for continuously predicting the duration of laparoscopic interventions based on unlabeled data, such as from endoscopic images and surgical device streams. Results The methods are evaluated on 80 laparoscopic interventions of various types, for which surgical device data and the endoscopic video are available. Here the combined method performs best with an overall average error of 37% and an average halftime error of 28%. Conclusion In this paper, we present, to our knowledge, the first approach for online procedure duration prediction using unlabeled endoscopic video data and surgical device data in a laparoscopic setting. We also show that a method incorporating both vision and device data performs better than methods based only on vision, while methods only based on tool usage and surgical device data perform poorly, showing the importance of the visual channel.



### ExGate: Externally Controlled Gating for Feature-based Attention in Artificial Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1811.03403v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.03403v1)
- **Published**: 2018-11-08 13:39:49+00:00
- **Updated**: 2018-11-08 13:39:49+00:00
- **Authors**: Jarryd Son, Amit Mishra
- **Comment**: None
- **Journal**: None
- **Summary**: Perceptual capabilities of artificial systems have come a long way since the advent of deep learning. These methods have proven to be effective, however they are not as efficient as their biological counterparts. Visual attention is a set of mechanisms that are employed in biological visual systems to ease computational load by only processing pertinent parts of the stimuli. This paper addresses the implementation of top-down, feature-based attention in an artificial neural network by use of externally controlled neuron gating. Our results showed a 5% increase in classification accuracy on the CIFAR-10 dataset versus a non-gated version, while adding very few parameters. Our gated model also produces more reasonable errors in predictions by drastically reducing prediction of classes that belong to a different category to the true class.



### Explainable cardiac pathology classification on cine MRI with motion characterization by semi-supervised learning of apparent flow
- **Arxiv ID**: http://arxiv.org/abs/1811.03433v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.03433v2)
- **Published**: 2018-11-08 14:22:05+00:00
- **Updated**: 2019-03-27 20:52:47+00:00
- **Authors**: Qiao Zheng, Hervé Delingette, Nicholas Ayache
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a method to classify cardiac pathology based on a novel approach to extract image derived features to characterize the shape and motion of the heart. An original semi-supervised learning procedure, which makes efficient use of a large amount of non-segmented images and a small amount of images segmented manually by experts, is developed to generate pixel-wise apparent flow between two time points of a 2D+t cine MRI image sequence. Combining the apparent flow maps and cardiac segmentation masks, we obtain a local apparent flow corresponding to the 2D motion of myocardium and ventricular cavities. This leads to the generation of time series of the radius and thickness of myocardial segments to represent cardiac motion. These time series of motion features are reliable and explainable characteristics of pathological cardiac motion. Furthermore, they are combined with shape-related features to classify cardiac pathologies. Using only nine feature values as input, we propose an explainable, simple and flexible model for pathology classification. On ACDC training set and testing set, the model achieves 95% and 94% respectively as classification accuracy. Its performance is hence comparable to that of the state-of-the-art. Comparison with various other models is performed to outline some advantages of our model.



### Microscopic Nuclei Classification, Segmentation and Detection with improved Deep Convolutional Neural Network (DCNN) Approaches
- **Arxiv ID**: http://arxiv.org/abs/1811.03447v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.03447v1)
- **Published**: 2018-11-08 14:34:43+00:00
- **Updated**: 2018-11-08 14:34:43+00:00
- **Authors**: Md Zahangir Alom, Chris Yakopcic, Tarek M. Taha, Vijayan K. Asari
- **Comment**: 18 pages, 16 figures, 3 Tables
- **Journal**: None
- **Summary**: Due to cellular heterogeneity, cell nuclei classification, segmentation, and detection from pathological images are challenging tasks. In the last few years, Deep Convolutional Neural Networks (DCNN) approaches have been shown state-of-the-art (SOTA) performance on histopathological imaging in different studies. In this work, we have proposed different advanced DCNN models and evaluated for nuclei classification, segmentation, and detection. First, the Densely Connected Recurrent Convolutional Network (DCRN) model is used for nuclei classification. Second, Recurrent Residual U-Net (R2U-Net) is applied for nuclei segmentation. Third, the R2U-Net regression model which is named UD-Net is used for nuclei detection from pathological images. The experiments are conducted with different datasets including Routine Colon Cancer(RCC) classification and detection dataset, and Nuclei Segmentation Challenge 2018 dataset. The experimental results show that the proposed DCNN models provide superior performance compared to the existing approaches for nuclei classification, segmentation, and detection tasks. The results are evaluated with different performance metrics including precision, recall, Dice Coefficient (DC), Means Squared Errors (MSE), F1-score, and overall accuracy. We have achieved around 3.4% and 4.5% better F-1 score for nuclei classification and detection tasks compared to recently published DCNN based method. In addition, R2U-Net shows around 92.15% testing accuracy in term of DC. These improved methods will help for pathological practices for better quantitative analysis of nuclei in Whole Slide Images(WSI) which ultimately will help for better understanding of different types of cancer in clinical workflow.



### Multi-view Laplacian Eigenmaps Based on Bag-of-Neighbors For RGBD Human Emotion Recognition
- **Arxiv ID**: http://arxiv.org/abs/1811.03478v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.03478v1)
- **Published**: 2018-11-08 15:03:58+00:00
- **Updated**: 2018-11-08 15:03:58+00:00
- **Authors**: Shenglan Liu, Shuai Guo, Hong Qiao, Yang Wang, Bin Wang, Wenbo Luo, Mingming Zhang, Keye Zhang, Bixuan Du
- **Comment**: None
- **Journal**: None
- **Summary**: Human emotion recognition is an important direction in the field of biometric and information forensics. However, most existing human emotion research are based on the single RGB view. In this paper, we introduce a RGBD video-emotion dataset and a RGBD face-emotion dataset for research. To our best knowledge, this may be the first RGBD video-emotion dataset. We propose a new supervised nonlinear multi-view laplacian eigenmaps (MvLE) approach and a multihidden-layer out-of-sample network (MHON) for RGB-D humanemotion recognition. To get better representations of RGB view and depth view, MvLE is used to map the training set of both views from original space into the common subspace. As RGB view and depth view lie in different spaces, a new distance metric bag of neighbors (BON) used in MvLE can get the similar distributions of the two views. Finally, MHON is used to get the low-dimensional representations of test data and predict their labels. MvLE can deal with the cases that RGB view and depth view have different size of features, even different number of samples and classes. And our methods can be easily extended to more than two views. The experiment results indicate the effectiveness of our methods over some state-of-art methods.



### Triple consistency loss for pairing distributions in GAN-based face synthesis
- **Arxiv ID**: http://arxiv.org/abs/1811.03492v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.03492v1)
- **Published**: 2018-11-08 15:32:18+00:00
- **Updated**: 2018-11-08 15:32:18+00:00
- **Authors**: Enrique Sanchez, Michel Valstar
- **Comment**: Project site https://github.com/ESanchezLozano/GANnotation ,
  https://youtu.be/-8r7zexg4yg
- **Journal**: None
- **Summary**: Generative Adversarial Networks have shown impressive results for the task of object translation, including face-to-face translation. A key component behind the success of recent approaches is the self-consistency loss, which encourages a network to recover the original input image when the output generated for a desired attribute is itself passed through the same network, but with the target attribute inverted. While the self-consistency loss yields photo-realistic results, it can be shown that the input and target domains, supposed to be close, differ substantially. This is empirically found by observing that a network recovers the input image even if attributes other than the inversion of the original goal are set as target. This stops one combining networks for different tasks, or using a network to do progressive forward passes. In this paper, we show empirical evidence of this effect, and propose a new loss to bridge the gap between the distributions of the input and target domains. This "triple consistency loss", aims to minimise the distance between the outputs generated by the network for different routes to the target, independent of any intermediate steps. To show this is effective, we incorporate the triple consistency loss into the training of a new landmark-guided face to face synthesis, where, contrary to previous works, the generated images can simultaneously undergo a large transformation in both expression and pose. To the best of our knowledge, we are the first to tackle the problem of mismatching distributions in self-domain synthesis, and to propose "in-the-wild" landmark-guided synthesis. Code will be available at https://github.com/ESanchezLozano/GANnotation



### Memorable Maps: A Framework for Re-defining Places in Visual Place Recognition
- **Arxiv ID**: http://arxiv.org/abs/1811.03529v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1811.03529v2)
- **Published**: 2018-11-08 16:18:50+00:00
- **Updated**: 2019-03-21 16:21:49+00:00
- **Authors**: Mubariz Zaffar, Shoaib Ehsan, Michael Milford, Klaus Mcdonald Maier
- **Comment**: 13 pages, 25 figures, 1 table
- **Journal**: None
- **Summary**: This paper presents a cognition-inspired agnostic framework for building a map for Visual Place Recognition. This framework draws inspiration from human-memorability, utilizes the traditional image entropy concept and computes the static content in an image; thereby presenting a tri-folded criterion to assess the 'memorability' of an image for visual place recognition. A dataset namely 'ESSEX3IN1' is created, composed of highly confusing images from indoor, outdoor and natural scenes for analysis. When used in conjunction with state-of-the-art visual place recognition methods, the proposed framework provides significant performance boost to these techniques, as evidenced by results on ESSEX3IN1 and other public datasets.



### Learning Dense Stereo Matching for Digital Surface Models from Satellite Imagery
- **Arxiv ID**: http://arxiv.org/abs/1811.03535v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.03535v2)
- **Published**: 2018-11-08 16:31:23+00:00
- **Updated**: 2018-12-11 19:41:57+00:00
- **Authors**: Wayne Treible, Scott Sorensen, Andrew D. Gilliam, Chandra Kambhamettu, Joseph L. Mundy
- **Comment**: None
- **Journal**: None
- **Summary**: Digital Surface Model generation from satellite imagery is a difficult task that has been largely overlooked by the deep learning community. Stereo reconstruction techniques developed for terrestrial systems including self driving cars do not translate well to satellite imagery where image pairs vary considerably. In this work we present neural network tailored for Digital Surface Model generation, a ground truthing and training scheme which maximizes available hardware, and we present a comparison to existing methods. The resulting models are smooth, preserve boundaries, and enable further processing. This represents one of the first attempts at leveraging deep learning in this domain.



### Adaptive Semantic Segmentation with a Strategic Curriculum of Proxy Labels
- **Arxiv ID**: http://arxiv.org/abs/1811.03542v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.03542v1)
- **Published**: 2018-11-08 16:44:59+00:00
- **Updated**: 2018-11-08 16:44:59+00:00
- **Authors**: Kashyap Chitta, Jianwei Feng, Martial Hebert
- **Comment**: None
- **Journal**: None
- **Summary**: Training deep networks for semantic segmentation requires annotation of large amounts of data, which can be time-consuming and expensive. Unfortunately, these trained networks still generalize poorly when tested in domains not consistent with the training data. In this paper, we show that by carefully presenting a mixture of labeled source domain and proxy-labeled target domain data to a network, we can achieve state-of-the-art unsupervised domain adaptation results. With our design, the network progressively learns features specific to the target domain using annotation from only the source domain. We generate proxy labels for the target domain using the network's own predictions. Our architecture then allows selective mining of easy samples from this set of proxy labels, and hard samples from the annotated source domain. We conduct a series of experiments with the GTA5, Cityscapes and BDD100k datasets on synthetic-to-real domain adaptation and geographic domain adaptation, showing the advantages of our method over baselines and existing approaches.



### An End-to-end Approach to Semantic Segmentation with 3D CNN and Posterior-CRF in Medical Images
- **Arxiv ID**: http://arxiv.org/abs/1811.03549v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.03549v1)
- **Published**: 2018-11-08 17:00:05+00:00
- **Updated**: 2018-11-08 17:00:05+00:00
- **Authors**: Shuai Chen, Marleen de Bruijne
- **Comment**: Accepted in Medical Imaging meets NIPS Workshop, NIPS 2018
- **Journal**: None
- **Summary**: Fully-connected Conditional Random Field (CRF) is often used as post-processing to refine voxel classification results by encouraging spatial coherence. In this paper, we propose a new end-to-end training method called Posterior-CRF. In contrast with previous approaches which use the original image intensity in the CRF, our approach applies 3D, fully connected CRF to the posterior probabilities from a CNN and optimizes both CNN and CRF together. The experiments on white matter hyperintensities segmentation demonstrate that our method outperforms CNN, post-processing CRF and different end-to-end training CRF approaches.



### Biologically-plausible learning algorithms can scale to large datasets
- **Arxiv ID**: http://arxiv.org/abs/1811.03567v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.03567v3)
- **Published**: 2018-11-08 17:43:59+00:00
- **Updated**: 2018-12-21 02:03:52+00:00
- **Authors**: Will Xiao, Honglin Chen, Qianli Liao, Tomaso Poggio
- **Comment**: None
- **Journal**: None
- **Summary**: The backpropagation (BP) algorithm is often thought to be biologically implausible in the brain. One of the main reasons is that BP requires symmetric weight matrices in the feedforward and feedback pathways. To address this "weight transport problem" (Grossberg, 1987), two more biologically plausible algorithms, proposed by Liao et al. (2016) and Lillicrap et al. (2016), relax BP's weight symmetry requirements and demonstrate comparable learning capabilities to that of BP on small datasets. However, a recent study by Bartunov et al. (2018) evaluate variants of target-propagation (TP) and feedback alignment (FA) on MINIST, CIFAR, and ImageNet datasets, and find that although many of the proposed algorithms perform well on MNIST and CIFAR, they perform significantly worse than BP on ImageNet. Here, we additionally evaluate the sign-symmetry algorithm (Liao et al., 2016), which differs from both BP and FA in that the feedback and feedforward weights share signs but not magnitudes. We examine the performance of sign-symmetry and feedback alignment on ImageNet and MS COCO datasets using different network architectures (ResNet-18 and AlexNet for ImageNet, RetinaNet for MS COCO). Surprisingly, networks trained with sign-symmetry can attain classification performance approaching that of BP-trained networks. These results complement the study by Bartunov et al. (2018), and establish a new benchmark for future biologically plausible learning algorithms on more difficult datasets and more complex architectures.



### Large-Scale Visual Active Learning with Deep Probabilistic Ensembles
- **Arxiv ID**: http://arxiv.org/abs/1811.03575v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.03575v3)
- **Published**: 2018-11-08 17:56:43+00:00
- **Updated**: 2019-02-21 02:02:13+00:00
- **Authors**: Kashyap Chitta, Jose M. Alvarez, Adam Lesnikowski
- **Comment**: arXiv admin note: text overlap with arXiv:1811.02640
- **Journal**: None
- **Summary**: Annotating the right data for training deep neural networks is an important challenge. Active learning using uncertainty estimates from Bayesian Neural Networks (BNNs) could provide an effective solution to this. Despite being theoretically principled, BNNs require approximations to be applied to large-scale problems, where both performance and uncertainty estimation are crucial. In this paper, we introduce Deep Probabilistic Ensembles (DPEs), a scalable technique that uses a regularized ensemble to approximate a deep BNN. We conduct a series of large-scale visual active learning experiments to evaluate DPEs on classification with the CIFAR-10, CIFAR-100 and ImageNet datasets, and semantic segmentation with the BDD100k dataset. Our models require significantly less training data to achieve competitive performances, and steadily improve upon strong active learning baselines as the annotation budget is increased.



### Gender Effect on Face Recognition for a Large Longitudinal Database
- **Arxiv ID**: http://arxiv.org/abs/1811.03680v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.03680v1)
- **Published**: 2018-11-08 21:12:58+00:00
- **Updated**: 2018-11-08 21:12:58+00:00
- **Authors**: Caroline Werther, Morgan Ferguson, Kevin Park, Troy Kling, Cuixian Chen, Yishi Wang
- **Comment**: This paper has been accepted by IEEE International Workshop on
  Information Forensics and Security (2018 WIFS)
- **Journal**: None
- **Summary**: Aging or gender variation can affect the face recognition performance dramatically. While most of the face recognition studies are focused on the variation of pose, illumination and expression, it is important to consider the influence of gender effect and how to design an effective matching framework. In this paper, we address these problems on a very large longitudinal database MORPH-II which contains 55,134 face images of 13,617 individuals. First, we consider four comprehensive experiments with different combination of gender distribution and subset size, including: 1) equal gender distribution; 2) a large highly unbalanced gender distribution; 3) consider different gender combinations, such as male only, female only, or mixed gender; and 4) the effect of subset size in terms of number of individuals. Second, we consider eight nearest neighbor distance metrics and also Support Vector Machine (SVM) for classifiers and test the effect of different classifiers. Last, we consider different fusion techniques for an effective matching framework to improve the recognition performance.



### Can Deep Learning Outperform Modern Commercial CT Image Reconstruction Methods?
- **Arxiv ID**: http://arxiv.org/abs/1811.03691v1
- **DOI**: 10.1038/s42256-019-0057-9
- **Categories**: **cs.CV**, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1811.03691v1)
- **Published**: 2018-11-08 22:04:22+00:00
- **Updated**: 2018-11-08 22:04:22+00:00
- **Authors**: Hongming Shan, Atul Padole, Fatemeh Homayounieh, Uwe Kruger, Ruhani Doda Khera, Chayanin Nitiwarangkul, Mannudeep K. Kalra, Ge Wang
- **Comment**: 17 pages, 7 figures
- **Journal**: Nature Machine Intelligence, 1(6) (2019) 269-276
- **Summary**: Commercial iterative reconstruction techniques on modern CT scanners target radiation dose reduction but there are lingering concerns over their impact on image appearance and low contrast detectability. Recently, machine learning, especially deep learning, has been actively investigated for CT. Here we design a novel neural network architecture for low-dose CT (LDCT) and compare it with commercial iterative reconstruction methods used for standard of care CT. While popular neural networks are trained for end-to-end mapping, driven by big data, our novel neural network is intended for end-to-process mapping so that intermediate image targets are obtained with the associated search gradients along which the final image targets are gradually reached. This learned dynamic process allows to include radiologists in the training loop to optimize the LDCT denoising workflow in a task-specific fashion with the denoising depth as a key parameter. Our progressive denoising network was trained with the Mayo LDCT Challenge Dataset, and tested on images of the chest and abdominal regions scanned on the CT scanners made by three leading CT vendors. The best deep learning based reconstructions are systematically compared to the best iterative reconstructions in a double-blinded reader study. It is found that our deep learning approach performs either comparably or favorably in terms of noise suppression and structural fidelity, and runs orders of magnitude faster than the commercial iterative CT reconstruction algorithms.



### Mode matching in GANs through latent space learning and inversion
- **Arxiv ID**: http://arxiv.org/abs/1811.03692v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.03692v3)
- **Published**: 2018-11-08 22:08:12+00:00
- **Updated**: 2019-03-24 07:02:37+00:00
- **Authors**: Deepak Mishra, Prathosh A. P., Aravind Jayendran, Varun Srivastava, Santanu Chaudhury
- **Comment**: None
- **Journal**: None
- **Summary**: Generative adversarial networks (GANs) have shown remarkable success in generation of unstructured data, such as, natural images. However, discovery and separation of modes in the generated space, essential for several tasks beyond naive data generation, is still a challenge. In this paper, we address the problem of imposing desired modal properties on the generated space using a latent distribution, engineered in accordance with the modal properties of the true data distribution. This is achieved by training a latent space inversion network in tandem with the generative network using a divergence loss. The latent space is made to follow a continuous multimodal distribution generated by reparameterization of a pair of continuous and discrete random variables. In addition, the modal priors of the latent distribution are learned to match with the true data distribution using minimal-supervision with negligible increment in number of learnable parameters. We validate our method on multiple tasks such as mode separation, conditional generation, and attribute discovery on multiple real world image datasets and demonstrate its efficacy over other state-of-the-art methods.



### Deep Learning Predicts Hip Fracture using Confounding Patient and Healthcare Variables
- **Arxiv ID**: http://arxiv.org/abs/1811.03695v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.03695v1)
- **Published**: 2018-11-08 22:23:07+00:00
- **Updated**: 2018-11-08 22:23:07+00:00
- **Authors**: Marcus A. Badgeley, John R. Zech, Luke Oakden-Rayner, Benjamin S. Glicksberg, Manway Liu, William Gale, Michael V. McConnell, Beth Percha, Thomas M. Snyder, Joel T. Dudley
- **Comment**: None
- **Journal**: None
- **Summary**: Hip fractures are a leading cause of death and disability among older adults. Hip fractures are also the most commonly missed diagnosis on pelvic radiographs. Computer-Aided Diagnosis (CAD) algorithms have shown promise for helping radiologists detect fractures, but the image features underpinning their predictions are notoriously difficult to understand. In this study, we trained deep learning models on 17,587 radiographs to classify fracture, five patient traits, and 14 hospital process variables. All 20 variables could be predicted from a radiograph (p < 0.05), with the best performances on scanner model (AUC=1.00), scanner brand (AUC=0.98), and whether the order was marked "priority" (AUC=0.79). Fracture was predicted moderately well from the image (AUC=0.78) and better when combining image features with patient data (AUC=0.86, p=2e-9) or patient data plus hospital process features (AUC=0.91, p=1e-21). The model performance on a test set with matched patient variables was significantly lower than a random test set (AUC=0.67, p=0.003); and when the test set was matched on patient and image acquisition variables, the model performed randomly (AUC=0.52, 95% CI 0.46-0.58), indicating that these variables were the main source of the model's predictive ability overall. We also used Naive Bayes to combine evidence from image models with patient and hospital data and found their inclusion improved performance, but that this approach was nevertheless inferior to directly modeling all variables. If CAD algorithms are inexplicably leveraging patient and process variables in their predictions, it is unclear how radiologists should interpret their predictions in the context of other known patient data. Further research is needed to illuminate deep learning decision processes so that computers and clinicians can effectively cooperate.



### Validating Hyperspectral Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1811.03707v1
- **DOI**: 10.1109/LGRS.2019.2895697
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.03707v1)
- **Published**: 2018-11-08 22:59:40+00:00
- **Updated**: 2018-11-08 22:59:40+00:00
- **Authors**: Jakub Nalepa, Michal Myller, Michal Kawulok
- **Comment**: Submitted to IEEE Geoscience and Remote Sensing Letters
- **Journal**: None
- **Summary**: Hyperspectral satellite imaging attracts enormous research attention in the remote sensing community, hence automated approaches for precise segmentation of such imagery are being rapidly developed. In this letter, we share our observations on the strategy for validating hyperspectral image segmentation algorithms currently followed in the literature, and show that it can lead to over-optimistic experimental insights. We introduce a new routine for generating segmentation benchmarks, and use it to elaborate ready-to-use hyperspectral training-test data partitions. They can be utilized for fair validation of new and existing algorithms without any training-test data leakage.



