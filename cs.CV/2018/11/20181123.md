# Arxiv Papers in cs.CV on 2018-11-23
### Efficient Structured Pruning and Architecture Searching for Group Convolution
- **Arxiv ID**: http://arxiv.org/abs/1811.09341v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.09341v4)
- **Published**: 2018-11-23 01:45:44+00:00
- **Updated**: 2019-10-28 05:14:47+00:00
- **Authors**: Ruizhe Zhao, Wayne Luk
- **Comment**: Published as an ICCV'19 NEUARCH workshop paper
- **Journal**: None
- **Summary**: Efficient inference of Convolutional Neural Networks is a thriving topic recently. It is desirable to achieve the maximal test accuracy under given inference budget constraints when deploying a pre-trained model. Network pruning is a commonly used technique while it may produce irregular sparse models that can hardly gain actual speed-up. Group convolution is a promising pruning target due to its regular structure; however, incorporating such structure into the pruning procedure is challenging. It is because structural constraints are hard to describe and can make pruning intractable to solve. The need for configuring group convolution architecture, i.e., the number of groups, that maximises test accuracy also increases difficulty.   This paper presents an efficient method to address this challenge. We formulate group convolution pruning as finding the optimal channel permutation to impose structural constraints and solve it efficiently by heuristics. We also apply local search to exploring group configuration based on estimated pruning cost to maximise test accuracy. Compared to prior work, results show that our method produces competitive group convolution models for various tasks within a shorter pruning period and enables rapid group configuration exploration subject to inference budget constraints.



### A Simple Non-i.i.d. Sampling Approach for Efficient Training and Better Generalization
- **Arxiv ID**: http://arxiv.org/abs/1811.09347v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.09347v2)
- **Published**: 2018-11-23 02:49:47+00:00
- **Updated**: 2020-10-15 03:39:15+00:00
- **Authors**: Bowen Cheng, Yunchao Wei, Jiahui Yu, Shiyu Chang, Jinjun Xiong, Wen-Mei Hwu, Thomas S. Huang, Humphrey Shi
- **Comment**: Technical report
- **Journal**: None
- **Summary**: While training on samples drawn from independent and identical distribution has been a de facto paradigm for optimizing image classification networks, humans learn new concepts in an easy-to-hard manner and on the selected examples progressively. Driven by this fact, we investigate the training paradigms where the samples are not drawn from independent and identical distribution. We propose a data sampling strategy, named Drop-and-Refresh (DaR), motivated by the learning behaviors of humans that selectively drop easy samples and refresh them only periodically. We show in our experiments that the proposed DaR strategy can maintain (and in many cases improve) the predictive accuracy even when the training cost is reduced by 15% on various datasets (CIFAR 10, CIFAR 100 and ImageNet) and with different backbone architectures (ResNets, DenseNets and MobileNets). Furthermore and perhaps more importantly, we find the ImageNet pre-trained models using our DaR sampling strategy achieves better transferability for the downstream tasks including object detection (+0.3 AP), instance segmentation (+0.3 AP), scene parsing (+0.5 mIoU) and human pose estimation (+0.6 AP). Our investigation encourages people to rethink the connections between the sampling strategy for training and the transferability of its learned features for pre-training ImageNet models.



### A Sufficient Condition for Convergences of Adam and RMSProp
- **Arxiv ID**: http://arxiv.org/abs/1811.09358v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NA, math.OC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.09358v3)
- **Published**: 2018-11-23 04:26:47+00:00
- **Updated**: 2019-06-25 03:39:53+00:00
- **Authors**: Fangyu Zou, Li Shen, Zequn Jie, Weizhong Zhang, Wei Liu
- **Comment**: Accepted by CVPR2019 as an Oral presentation
- **Journal**: None
- **Summary**: Adam and RMSProp are two of the most influential adaptive stochastic algorithms for training deep neural networks, which have been pointed out to be divergent even in the convex setting via a few simple counterexamples. Many attempts, such as decreasing an adaptive learning rate, adopting a big batch size, incorporating a temporal decorrelation technique, seeking an analogous surrogate, etc., have been tried to promote Adam/RMSProp-type algorithms to converge. In contrast with existing approaches, we introduce an alternative easy-to-check sufficient condition, which merely depends on the parameters of the base learning rate and combinations of historical second-order moments, to guarantee the global convergence of generic Adam/RMSProp for solving large-scale non-convex stochastic optimization. Moreover, we show that the convergences of several variants of Adam, such as AdamNC, AdaEMA, etc., can be directly implied via the proposed sufficient condition in the non-convex setting. In addition, we illustrate that Adam is essentially a specifically weighted AdaGrad with exponential moving average momentum, which provides a novel perspective for understanding Adam and RMSProp. This observation coupled with this sufficient condition gives much deeper interpretations on their divergences. At last, we validate the sufficient condition by applying Adam and RMSProp to tackle a certain counterexample and train deep neural networks. Numerical results are exactly in accord with our theoretical analysis.



### Pointwise Rotation-Invariant Network with Adaptive Sampling and 3D Spherical Voxel Convolution
- **Arxiv ID**: http://arxiv.org/abs/1811.09361v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.09361v5)
- **Published**: 2018-11-23 05:11:14+00:00
- **Updated**: 2019-12-05 15:54:34+00:00
- **Authors**: Yang You, Yujing Lou, Qi Liu, Yu-Wing Tai, Lizhuang Ma, Cewu Lu, Weiming Wang
- **Comment**: 8 pages, to appear on AAAI 2020
- **Journal**: None
- **Summary**: Point cloud analysis without pose priors is very challenging in real applications, as the orientations of point clouds are often unknown. In this paper, we propose a brand new point-set learning framework PRIN, namely, Pointwise Rotation-Invariant Network, focusing on rotation-invariant feature extraction in point clouds analysis. We construct spherical signals by Density Aware Adaptive Sampling to deal with distorted point distributions in spherical space. In addition, we propose Spherical Voxel Convolution and Point Re-sampling to extract rotation-invariant features for each point. Our network can be applied to tasks ranging from object classification, part segmentation, to 3D feature matching and label alignment. We show that, on the dataset with randomly rotated point clouds, PRIN demonstrates better performance than state-of-the-art methods without any data augmentation. We also provide theoretical analysis for the rotation-invariance achieved by our methods.



### Learning Temporal Coherence via Self-Supervision for GAN-based Video Generation
- **Arxiv ID**: http://arxiv.org/abs/1811.09393v4
- **DOI**: 10.1145/3386569.3392457
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.09393v4)
- **Published**: 2018-11-23 09:16:22+00:00
- **Updated**: 2020-05-21 15:06:30+00:00
- **Authors**: Mengyu Chu, You Xie, Jonas Mayer, Laura Leal-Taix√©, Nils Thuerey
- **Comment**: Project page: https://ge.in.tum.de/publications/2019-tecogan-chu/,
  code link: https://github.com/thunil/TecoGAN
- **Journal**: None
- **Summary**: Our work explores temporal self-supervision for GAN-based video generation tasks. While adversarial training successfully yields generative models for a variety of areas, temporal relationships in the generated data are much less explored. Natural temporal changes are crucial for sequential generation tasks, e.g. video super-resolution and unpaired video translation. For the former, state-of-the-art methods often favor simpler norm losses such as $L^2$ over adversarial training. However, their averaging nature easily leads to temporally smooth results with an undesirable lack of spatial detail. For unpaired video translation, existing approaches modify the generator networks to form spatio-temporal cycle consistencies. In contrast, we focus on improving learning objectives and propose a temporally self-supervised algorithm. For both tasks, we show that temporal adversarial learning is key to achieving temporally coherent solutions without sacrificing spatial detail. We also propose a novel Ping-Pong loss to improve the long-term temporal consistency. It effectively prevents recurrent networks from accumulating artifacts temporally without depressing detailed features. Additionally, we propose a first set of metrics to quantitatively evaluate the accuracy as well as the perceptual quality of the temporal evolution. A series of user studies confirm the rankings computed with these metrics. Code, data, models, and results are provided at https://github.com/thunil/TecoGAN. The project page https://ge.in.tum.de/publications/2019-tecogan-chu/ contains supplemental materials.



### MVPNet: Multi-View Point Regression Networks for 3D Object Reconstruction from A Single Image
- **Arxiv ID**: http://arxiv.org/abs/1811.09410v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.09410v1)
- **Published**: 2018-11-23 09:56:48+00:00
- **Updated**: 2018-11-23 09:56:48+00:00
- **Authors**: Jinglu Wang, Bo Sun, Yan Lu
- **Comment**: 8 pages; accepted by AAAI 2019
- **Journal**: None
- **Summary**: In this paper, we address the problem of reconstructing an object's surface from a single image using generative networks. First, we represent a 3D surface with an aggregation of dense point clouds from multiple views. Each point cloud is embedded in a regular 2D grid aligned on an image plane of a viewpoint, making the point cloud convolution-favored and ordered so as to fit into deep network architectures. The point clouds can be easily triangulated by exploiting connectivities of the 2D grids to form mesh-based surfaces. Second, we propose an encoder-decoder network that generates such kind of multiple view-dependent point clouds from a single image by regressing their 3D coordinates and visibilities. We also introduce a novel geometric loss that is able to interpret discrepancy over 3D surfaces as opposed to 2D projective planes, resorting to the surface discretization on the constructed meshes. We demonstrate that the multi-view point regression network outperforms state-of-the-art methods with a significant improvement on challenging datasets.



### Joint Neural Architecture Search and Quantization
- **Arxiv ID**: http://arxiv.org/abs/1811.09426v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.09426v1)
- **Published**: 2018-11-23 10:58:46+00:00
- **Updated**: 2018-11-23 10:58:46+00:00
- **Authors**: Yukang Chen, Gaofeng Meng, Qian Zhang, Xinbang Zhang, Liangchen Song, Shiming Xiang, Chunhong Pan
- **Comment**: None
- **Journal**: None
- **Summary**: Designing neural architectures is a fundamental step in deep learning applications. As a partner technique, model compression on neural networks has been widely investigated to gear the needs that the deep learning algorithms could be run with the limited computation resources on mobile devices. Currently, both the tasks of architecture design and model compression require expertise tricks and tedious trials. In this paper, we integrate these two tasks into one unified framework, which enables the joint architecture search with quantization (compression) policies for neural networks. This method is named JASQ. Here our goal is to automatically find a compact neural network model with high performance that is suitable for mobile devices. Technically, a multi-objective evolutionary search algorithm is introduced to search the models under the balance between model size and performance accuracy. In experiments, we find that our approach outperforms the methods that search only for architectures or only for quantization policies. 1) Specifically, given existing networks, our approach can provide them with learning-based quantization policies, and outperforms their 2 bits, 4 bits, 8 bits, and 16 bits counterparts. It can yield higher accuracies than the float models, for example, over 1.02% higher accuracy on MobileNet-v1. 2) What is more, under the balance between model size and performance accuracy, two models are obtained with joint search of architectures and quantization policies: a high-accuracy model and a small model, JASQNet and JASQNet-Small that achieves 2.97% error rate with 0.9 MB on CIFAR-10.



### Fast Object Class Labelling via Speech
- **Arxiv ID**: http://arxiv.org/abs/1811.09461v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1811.09461v2)
- **Published**: 2018-11-23 13:08:39+00:00
- **Updated**: 2019-04-11 15:33:27+00:00
- **Authors**: Michael Gygli, Vittorio Ferrari
- **Comment**: to be published at CVPR 2019
- **Journal**: None
- **Summary**: Object class labelling is the task of annotating images with labels on the presence or absence of objects from a given class vocabulary. Simply asking one yes/no question per class, however, has a cost that is linear in the vocabulary size and is thus inefficient for large vocabularies. Modern approaches rely on a hierarchical organization of the vocabulary to reduce annotation time, but remain expensive (several minutes per image for the 200 classes in ILSVRC). Instead, we propose a new interface where classes are annotated via speech. Speaking is fast and allows for direct access to the class name, without searching through a list or hierarchy. As additional advantages, annotators can simultaneously speak and scan the image for objects, the interface can be kept extremely simple, and using it requires less mouse movement. As annotators using our interface should only say words from a given class vocabulary, we propose a dedicated task that trains them to do so. Through experiments on COCO and ILSVRC, we show our method yields high-quality annotations at 2.3x - 14.9x less annotation time than existing methods.



### Defect Detection from UAV Images based on Region-Based CNNs
- **Arxiv ID**: http://arxiv.org/abs/1811.09473v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.09473v1)
- **Published**: 2018-11-23 13:38:17+00:00
- **Updated**: 2018-11-23 13:38:17+00:00
- **Authors**: Meng Lan, Yipeng Zhang, Lefei Zhang, Bo Du
- **Comment**: received as best paper of The first International Workshop on
  Developmental Learning Workshop in ICDM 2018
- **Journal**: None
- **Summary**: With the wide applications of Unmanned Aerial Vehicle (UAV) in engineering such as the inspection of the electrical equipment from distance, the demands of efficient object detection algorithms for abundant images acquired by UAV have also been significantly increased in recent years. In this work, we study the performance of the region-based CNN for the electrical equipment defect detection by using the UAV images. In order to train the detection model, we collect a UAV images dataset composes of four classes of electrical equipment defects with thousands of annotated labels. Then, based on the region-based faster R-CNN model, we present a multi-class defects detection model for electrical equipment which is more efficient and accurate than traditional single class detection methods. Technically, we have replaced the RoI pooling layer with a similar operation in Tensorflow and promoted the mini-batch to 128 per image in the training procedure. These improvements have slightly increased the speed of detection without any accuracy loss. Therefore, the modified region-based CNN could simultaneously detect multi-class of defects of the electrical devices in nearly real time. Experimental results on the real word electrical equipment images demonstrate that the proposed method achieves better performance than the traditional object detection algorithms in defect detection.



### LSD$_2$ -- Joint Denoising and Deblurring of Short and Long Exposure Images with CNNs
- **Arxiv ID**: http://arxiv.org/abs/1811.09485v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.09485v3)
- **Published**: 2018-11-23 14:13:33+00:00
- **Updated**: 2020-09-01 20:24:46+00:00
- **Authors**: Janne Mustaniemi, Juho Kannala, Jiri Matas, Simo S√§rkk√§, Janne Heikkil√§
- **Comment**: None
- **Journal**: None
- **Summary**: The paper addresses the problem of acquiring high-quality photographs with handheld smartphone cameras in low-light imaging conditions. We propose an approach based on capturing pairs of short and long exposure images in rapid succession and fusing them into a single high-quality photograph. Unlike existing methods, we take advantage of both images simultaneously and perform a joint denoising and deblurring using a convolutional neural network. A novel approach is introduced to generate realistic short-long exposure image pairs. The method produces good images in extremely challenging conditions and outperforms existing denoising and deblurring methods. It also enables exposure fusion in the presence of motion blur.



### MURAUER: Mapping Unlabeled Real Data for Label AUstERity
- **Arxiv ID**: http://arxiv.org/abs/1811.09497v2
- **DOI**: None
- **Categories**: **cs.CV**, I.2.6; I.2.10; I.4.5; I.4.8; I.4.10; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/1811.09497v2)
- **Published**: 2018-11-23 14:40:07+00:00
- **Updated**: 2018-12-05 12:30:49+00:00
- **Authors**: Georg Poier, Michael Opitz, David Schinagl, Horst Bischof
- **Comment**: WACV 2019; Project page at https://poier.github.io/murauer
- **Journal**: None
- **Summary**: Data labeling for learning 3D hand pose estimation models is a huge effort. Readily available, accurately labeled synthetic data has the potential to reduce the effort. However, to successfully exploit synthetic data, current state-of-the-art methods still require a large amount of labeled real data. In this work, we remove this requirement by learning to map from the features of real data to the features of synthetic data mainly using a large amount of synthetic and unlabeled real data. We exploit unlabeled data using two auxiliary objectives, which enforce that (i) the mapped representation is pose specific and (ii) at the same time, the distributions of real and synthetic data are aligned. While pose specifity is enforced by a self-supervisory signal requiring that the representation is predictive for the appearance from different views, distributions are aligned by an adversarial term. In this way, we can significantly improve the results of the baseline system, which does not use unlabeled data and outperform many recent approaches already with about 1% of the labeled real data. This presents a step towards faster deployment of learning based hand pose estimation, making it accessible for a larger range of applications.



### Relation Networks for Optic Disc and Fovea Localization in Retinal Images
- **Arxiv ID**: http://arxiv.org/abs/1812.00883v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.00883v1)
- **Published**: 2018-11-23 14:51:55+00:00
- **Updated**: 2018-11-23 14:51:55+00:00
- **Authors**: Sudharshan Chandra Babu, Shishira R Maiya, Sivasankar Elango
- **Comment**: Machine Learning for Health (ML4H) Workshop at NeurIPS 2018
- **Journal**: None
- **Summary**: Diabetic Retinopathy is the leading cause of blindness in the world. At least 90\% of new cases can be reduced with proper treatment and monitoring of the eyes. However, scanning the entire population of patients is a difficult endeavor. Computer-aided diagnosis tools in retinal image analysis can make the process scalable and efficient. In this work, we focus on the problem of localizing the centers of the Optic disc and Fovea, a task crucial to the analysis of retinal scans. Current systems recognize the Optic disc and Fovea individually, without exploiting their relations during learning. We propose a novel approach to localizing the centers of the Optic disc and Fovea by simultaneously processing them and modeling their relative geometry and appearance. We show that our approach improves localization and recognition by incorporating object-object relations efficiently, and achieves highly competitive results.



### A Novel Learning-based Global Path Planning Algorithm for Planetary Rovers
- **Arxiv ID**: http://arxiv.org/abs/1811.10437v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10437v1)
- **Published**: 2018-11-23 15:20:03+00:00
- **Updated**: 2018-11-23 15:20:03+00:00
- **Authors**: Jiang Zhang, Yuanqing Xia, Ganghui Shen
- **Comment**: Submitted to Neurocomputing. arXiv admin note: text overlap with
  arXiv:1808.08395
- **Journal**: None
- **Summary**: Autonomous path planning algorithms are significant to planetary exploration rovers, since relying on commands from Earth will heavily reduce their efficiency of executing exploration missions. This paper proposes a novel learning-based algorithm to deal with global path planning problem for planetary exploration rovers. Specifically, a novel deep convolutional neural network with double branches (DB-CNN) is designed and trained, which can plan path directly from orbital images of planetary surfaces without implementing environment mapping. Moreover, the planning procedure requires no prior knowledge about planetary surface terrains. Finally, experimental results demonstrate that DB-CNN achieves better performance on global path planning and faster convergence during training compared with the existing Value Iteration Network (VIN).



### Complementary Segmentation of Primary Video Objects with Reversible Flows
- **Arxiv ID**: http://arxiv.org/abs/1811.09521v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.09521v1)
- **Published**: 2018-11-23 15:29:38+00:00
- **Updated**: 2018-11-23 15:29:38+00:00
- **Authors**: Jia Li, Junjie Wu, Anlin Zheng, Yafei Song, Yu Zhang, Xiaowu Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Segmenting primary objects in a video is an important yet challenging problem in computer vision, as it exhibits various levels of foreground/background ambiguities. To reduce such ambiguities, we propose a novel formulation via exploiting foreground and background context as well as their complementary constraint. Under this formulation, a unified objective function is further defined to encode each cue. For implementation, we design a Complementary Segmentation Network (CSNet) with two separate branches, which can simultaneously encode the foreground and background information along with joint spatial constraints. The CSNet is trained on massive images with manually annotated salient objects in an end-to-end manner. By applying CSNet on each video frame, the spatial foreground and background maps can be initialized. To enforce temporal consistency effectively and efficiently, we divide each frame into superpixels and construct neighborhood reversible flow that reflects the most reliable temporal correspondences between superpixels in far-away frames. With such flow, the initialized foregroundness and backgroundness can be propagated along the temporal dimension so that primary video objects gradually pop-out and distractors are well suppressed. Extensive experimental results on three video datasets show that the proposed approach achieves impressive performance in comparisons with 18 state-of-the-art models.



### An Adaptive Approach for Automated Grapevine Phenotyping using VGG-based Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1811.09561v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.09561v2)
- **Published**: 2018-11-23 17:05:31+00:00
- **Updated**: 2018-12-27 09:56:45+00:00
- **Authors**: Jonatan Grimm, Katja Herzog, Florian Rist, Anna Kicherer, Reinhard T√∂pfer, Volker Steinhage
- **Comment**: None
- **Journal**: None
- **Summary**: In (grapevine) breeding programs and research, periodic phenotyping and multi-year monitoring of different grapevine traits, like growth or yield, is needed especially in the field. This demand imply objective, precise and automated methods using sensors and adaptive software. This work presents a proof-of-concept analyzing RGB images of different growth stages of grapevines with the aim to detect and quantify promising plant organs which are related to yield. The input images are segmented by a Fully Convolutional Neural Network (FCN) into object and background pixels. The objects are plant organs like young shoots, pedicels, flower buds or grapes, which are principally suitable for yield estimation. In the ground truth of the training images, each object is separately annotated as a connected segment of object pixels, which enables end-to-end learning of the object features. Based on the CNN-based segmentation, the number of objects is determined by detecting and counting connected components of object pixels using region labeling. In an evaluation on six different data sets, the system achieves an IoU of up to 87.3% for the segmentation and an F1 score of up to 88.6% for the object detection.



### How does Lipschitz Regularization Influence GAN Training?
- **Arxiv ID**: http://arxiv.org/abs/1811.09567v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.09567v3)
- **Published**: 2018-11-23 17:18:00+00:00
- **Updated**: 2020-08-25 07:39:24+00:00
- **Authors**: Yipeng Qin, Niloy Mitra, Peter Wonka
- **Comment**: Accepted at ECCV 2020
- **Journal**: None
- **Summary**: Despite the success of Lipschitz regularization in stabilizing GAN training, the exact reason of its effectiveness remains poorly understood. The direct effect of $K$-Lipschitz regularization is to restrict the $L2$-norm of the neural network gradient to be smaller than a threshold $K$ (e.g., $K=1$) such that $\|\nabla f\| \leq K$. In this work, we uncover an even more important effect of Lipschitz regularization by examining its impact on the loss function: It degenerates GAN loss functions to almost linear ones by restricting their domain and interval of attainable gradient values. Our analysis shows that loss functions are only successful if they are degenerated to almost linear ones. We also show that loss functions perform poorly if they are not degenerated and that a wide range of functions can be used as loss function as long as they are sufficiently degenerated by regularization. Basically, Lipschitz regularization ensures that all loss functions effectively work in the same way. Empirically, we verify our proposition on the MNIST, CIFAR10 and CelebA datasets.



### Decoupling Direction and Norm for Efficient Gradient-Based L2 Adversarial Attacks and Defenses
- **Arxiv ID**: http://arxiv.org/abs/1811.09600v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.09600v3)
- **Published**: 2018-11-23 18:54:47+00:00
- **Updated**: 2019-04-03 21:11:11+00:00
- **Authors**: J√©r√¥me Rony, Luiz G. Hafemann, Luiz S. Oliveira, Ismail Ben Ayed, Robert Sabourin, Eric Granger
- **Comment**: Accepted as a conference paper to the 2019 IEEE/CVF Conference on
  Computer Vision and Pattern Recognition (CVPR oral presentation)
- **Journal**: None
- **Summary**: Research on adversarial examples in computer vision tasks has shown that small, often imperceptible changes to an image can induce misclassification, which has security implications for a wide range of image processing systems. Considering $L_2$ norm distortions, the Carlini and Wagner attack is presently the most effective white-box attack in the literature. However, this method is slow since it performs a line-search for one of the optimization terms, and often requires thousands of iterations. In this paper, an efficient approach is proposed to generate gradient-based attacks that induce misclassifications with low $L_2$ norm, by decoupling the direction and the norm of the adversarial perturbation that is added to the image. Experiments conducted on the MNIST, CIFAR-10 and ImageNet datasets indicate that our attack achieves comparable results to the state-of-the-art (in terms of $L_2$ norm) with considerably fewer iterations (as few as 100 iterations), which opens the possibility of using these attacks for adversarial training. Models trained with our attack achieve state-of-the-art robustness against white-box gradient-based $L_2$ attacks on the MNIST and CIFAR-10 datasets, outperforming the Madry defense when the attacks are limited to a maximum norm.



### A New Cervical Cytology Dataset for Nucleus Detection and Image Classification (Cervix93) and Methods for Cervical Nucleus Detection
- **Arxiv ID**: http://arxiv.org/abs/1811.09651v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.09651v1)
- **Published**: 2018-11-23 19:41:47+00:00
- **Updated**: 2018-11-23 19:41:47+00:00
- **Authors**: Hady Ahmady Phoulady, Peter R. Mouton
- **Comment**: 5 pages, 6 figures, conference
- **Journal**: None
- **Summary**: Analyzing Pap cytology slides is an important tasks in detecting and grading precancerous and cancerous cervical cancer stages. Processing cytology images usually involve segmenting nuclei and overlapping cells. We introduce a cervical cytology dataset that can be used to evaluate nucleus detection, as well as image classification methods in the cytology image processing area. This dataset contains 93 real image stacks with their grade labels and manually annotated nuclei within images. We also present two methods: a baseline method based on a previously proposed approach, and a deep learning method, and compare their results with other state-of-the-art methods. Both the baseline method and the deep learning method outperform other state-of-the-art methods by significant margins. Along with the dataset, we publicly make the evaluation code and the baseline method available to download for further benchmarking.



### Unsupervised brain lesion segmentation from MRI using a convolutional autoencoder
- **Arxiv ID**: http://arxiv.org/abs/1811.09655v1
- **DOI**: 10.1117/12.2512953
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1811.09655v1)
- **Published**: 2018-11-23 19:53:17+00:00
- **Updated**: 2018-11-23 19:53:17+00:00
- **Authors**: Hans E. Atlason, Askell Love, Sigurdur Sigurdsson, Vilmundur Gudnason, Lotta M. Ellingsen
- **Comment**: None
- **Journal**: None
- **Summary**: Lesions that appear hyperintense in both Fluid Attenuated Inversion Recovery (FLAIR) and T2-weighted magnetic resonance images (MRIs) of the human brain are common in the brains of the elderly population and may be caused by ischemia or demyelination. Lesions are biomarkers for various neurodegenerative diseases, making accurate quantification of them important for both disease diagnosis and progression. Automatic lesion detection using supervised learning requires manually annotated images, which can often be impractical to acquire. Unsupervised lesion detection, on the other hand, does not require any manual delineation; however, these methods can be challenging to construct due to the variability in lesion load, placement of lesions, and voxel intensities. Here we present a novel approach to address this problem using a convolutional autoencoder, which learns to segment brain lesions as well as the white matter, gray matter, and cerebrospinal fluid by reconstructing FLAIR images as conical combinations of softmax layer outputs generated from the corresponding T1, T2, and FLAIR images. Some of the advantages of this model are that it accurately learns to segment lesions regardless of lesion load, and it can be used to quickly and robustly segment new images that were not in the training set. Comparisons with state-of-the-art segmentation methods evaluated on ground truth manual labels indicate that the proposed method works well for generating accurate lesion segmentations without the need for manual annotations.



### Detailed Investigation of Deep Features with Sparse Representation and Dimensionality Reduction in CBIR: A Comparative Study
- **Arxiv ID**: http://arxiv.org/abs/1811.09681v1
- **DOI**: 10.3233/IDA-180895
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.09681v1)
- **Published**: 2018-11-23 20:53:13+00:00
- **Updated**: 2018-11-23 20:53:13+00:00
- **Authors**: Ahmad S. Tarawneh, Ceyhun Celik, Ahmad B. Hassanat, Dmitry Chetverikov
- **Comment**: None
- **Journal**: Intelligent Data Analysis, vol. 24, no. 1, 2020
- **Summary**: Research on content-based image retrieval (CBIR) has been under development for decades, and numerous methods have been competing to extract the most discriminative features for improved representation of the image content. Recently, deep learning methods have gained attention in computer vision, including CBIR. In this paper, we present a comparative investigation of different features, including low-level and high-level features, for CBIR. We compare the performance of CBIR systems using different deep features with state-of-the-art low-level features such as SIFT, SURF, HOG, LBP, and LTP, using different dictionaries and coefficient learning techniques. Furthermore, we conduct comparisons with a set of primitive and popular features that have been used in this field, including colour histograms and Gabor features. We also investigate the discriminative power of deep features using certain similarity measures under different validation approaches. Furthermore, we investigate the effects of the dimensionality reduction of deep features on the performance of CBIR systems using principal component analysis, discrete wavelet transform, and discrete cosine transform. Unprecedentedly, the experimental results demonstrate high (95\% and 93\%) mean average precisions when using the VGG-16 FC7 deep features of Corel-1000 and Coil-20 datasets with 10-D and 20-D K-SVD, respectively.



### Learning to attend in a brain-inspired deep neural network
- **Arxiv ID**: http://arxiv.org/abs/1811.09699v1
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1811.09699v1)
- **Published**: 2018-11-23 21:23:56+00:00
- **Updated**: 2018-11-23 21:23:56+00:00
- **Authors**: Hossein Adeli, Gregory Zelinsky
- **Comment**: 4 pages
- **Journal**: None
- **Summary**: Recent machine learning models have shown that including attention as a component results in improved model accuracy and interpretability, despite the concept of attention in these approaches only loosely approximating the brain's attention mechanism. Here we extend this work by building a more brain-inspired deep network model of the primate ATTention Network (ATTNet) that learns to shift its attention so as to maximize the reward. Using deep reinforcement learning, ATTNet learned to shift its attention to the visual features of a target category in the context of a search task. ATTNet's dorsal layers also learned to prioritize these shifts of attention so as to maximize success of the ventral pathway classification and receive greater reward. Model behavior was tested against the fixations made by subjects searching images for the same cued category. Both subjects and ATTNet showed evidence for attention being preferentially directed to target goals, behaviorally measured as oculomotor guidance to targets. More fundamentally, ATTNet learned to shift its attention to target like objects and spatially route its visual inputs to accomplish the task. This work makes a step toward a better understanding of the role of attention in the brain and other computational systems.



### Robustness via curvature regularization, and vice versa
- **Arxiv ID**: http://arxiv.org/abs/1811.09716v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.09716v1)
- **Published**: 2018-11-23 22:03:40+00:00
- **Updated**: 2018-11-23 22:03:40+00:00
- **Authors**: Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Jonathan Uesato, Pascal Frossard
- **Comment**: None
- **Journal**: None
- **Summary**: State-of-the-art classifiers have been shown to be largely vulnerable to adversarial perturbations. One of the most effective strategies to improve robustness is adversarial training. In this paper, we investigate the effect of adversarial training on the geometry of the classification landscape and decision boundaries. We show in particular that adversarial training leads to a significant decrease in the curvature of the loss surface with respect to inputs, leading to a drastically more "linear" behaviour of the network. Using a locally quadratic approximation, we provide theoretical evidence on the existence of a strong relation between large robustness and small curvature. To further show the importance of reduced curvature for improving the robustness, we propose a new regularizer that directly minimizes curvature of the loss surface, and leads to adversarial robustness that is on par with adversarial training. Besides being a more efficient and principled alternative to adversarial training, the proposed regularizer confirms our claims on the importance of exhibiting quasi-linear behavior in the vicinity of data points in order to achieve robustness.



### Automatic lesion boundary detection in dermoscopy
- **Arxiv ID**: http://arxiv.org/abs/1812.00877v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.00877v1)
- **Published**: 2018-11-23 22:36:36+00:00
- **Updated**: 2018-11-23 22:36:36+00:00
- **Authors**: Glib Kechyn
- **Comment**: None
- **Journal**: None
- **Summary**: This manuscript addresses the problem of the automatic lesion boundary detection in dermoscopy, using deep neural networks. An approach is based on the adaptation of the U-net convolutional neural network with skip connections for lesion boundary segmentation task. I hope this paper could serve, to some extent, as an experiment of using deep convolutional networks in biomedical segmentation task and as a guideline of the boundary detection benchmark, inspiring further attempts and researches.



