# Arxiv Papers in cs.CV on 2018-11-06
### TrafficPredict: Trajectory Prediction for Heterogeneous Traffic-Agents
- **Arxiv ID**: http://arxiv.org/abs/1811.02146v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1811.02146v5)
- **Published**: 2018-11-06 03:34:20+00:00
- **Updated**: 2019-04-09 07:08:02+00:00
- **Authors**: Yuexin Ma, Xinge Zhu, Sibo Zhang, Ruigang Yang, Wenping Wang, Dinesh Manocha
- **Comment**: Accepted by AAAI(Oral) 2019
- **Journal**: None
- **Summary**: To safely and efficiently navigate in complex urban traffic, autonomous vehicles must make responsible predictions in relation to surrounding traffic-agents (vehicles, bicycles, pedestrians, etc.). A challenging and critical task is to explore the movement patterns of different traffic-agents and predict their future trajectories accurately to help the autonomous vehicle make reasonable navigation decision. To solve this problem, we propose a long short-term memory-based (LSTM-based) realtime traffic prediction algorithm, TrafficPredict. Our approach uses an instance layer to learn instances' movements and interactions and has a category layer to learn the similarities of instances belonging to the same type to refine the prediction. In order to evaluate its performance, we collected trajectory datasets in a large city consisting of varying conditions and traffic densities. The dataset includes many challenging scenarios where vehicles, bicycles, and pedestrians move among one another. We evaluate the performance of TrafficPredict on our new dataset and highlight its higher accuracy for trajectory prediction by comparing with prior prediction methods.



### BLP -- Boundary Likelihood Pinpointing Networks for Accurate Temporal Action Localization
- **Arxiv ID**: http://arxiv.org/abs/1811.02189v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.02189v6)
- **Published**: 2018-11-06 06:54:58+00:00
- **Updated**: 2019-12-16 03:09:43+00:00
- **Authors**: Weijie Kong, Nannan Li, Shan Liu, Thomas Li, Ge Li
- **Comment**: Accepted to International Conference on Acoustics, Speech, and Signal
  Processing (ICASSP), 2019
- **Journal**: None
- **Summary**: Despite tremendous progress achieved in temporal action detection, state-of-the-art methods still suffer from the sharp performance deterioration when localizing the starting and ending temporal action boundaries. Although most methods apply boundary regression paradigm to tackle this problem, we argue that the direct regression lacks detailed enough information to yield accurate temporal boundaries. In this paper, we propose a novel Boundary Likelihood Pinpointing (BLP) network to alleviate this deficiency of boundary regression and improve the localization accuracy. Given a loosely localized search interval that contains an action instance, BLP casts the problem of localizing temporal boundaries as that of assigning probabilities on each equally divided unit of this interval. These generated probabilities provide useful information regarding the boundary location of the action inside this search interval. Based on these probabilities, we introduce a boundary pinpointing paradigm to pinpoint the accurate boundaries under a simple probabilistic framework. Compared with other C3D feature based detectors, extensive experiments demonstrate that BLP significantly improves the localization performance of recent state-of-the-art detectors, and achieves competitive detection mAP on both THUMOS' 14 and ActivityNet datasets, particularly when the evaluation tIoU is high.



### 3DCapsule: Extending the Capsule Architecture to Classify 3D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1811.02191v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.02191v1)
- **Published**: 2018-11-06 06:57:49+00:00
- **Updated**: 2018-11-06 06:57:49+00:00
- **Authors**: Ali Cheraghian, Lars Petersson
- **Comment**: 2019 IEEE Winter Conference on Applications of Computer Vision (WACV)
- **Journal**: None
- **Summary**: This paper introduces the 3DCapsule, which is a 3D extension of the recently introduced Capsule concept that makes it applicable to unordered point sets. The original Capsule relies on the existence of a spatial relationship between the elements in the feature map it is presented with, whereas in point permutation invariant formulations of 3D point set classification methods, such relationships are typically lost. Here, a new layer called ComposeCaps is introduced that, in lieu of a spatially relevant feature mapping, learns a new mapping that can be exploited by the 3DCapsule. Previous works in the 3D point set classification domain have focused on other parts of the architecture, whereas instead, the 3DCapsule is a drop-in replacement of the commonly used fully connected classifier. It is demonstrated via an ablation study, that when the 3DCapsule is applied to recent 3D point set classification architectures, it consistently shows an improvement, in particular when subjected to noisy data. Similarly, the ComposeCaps layer is evaluated and demonstrates an improvement over the baseline. In an apples-to-apples comparison against state-of-the-art methods, again, better performance is demonstrated by the 3DCapsule.



### In-the-wild Facial Expression Recognition in Extreme Poses
- **Arxiv ID**: http://arxiv.org/abs/1811.02194v1
- **DOI**: 10.1117/12.2302626
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.02194v1)
- **Published**: 2018-11-06 07:02:14+00:00
- **Updated**: 2018-11-06 07:02:14+00:00
- **Authors**: Fei Yang, Qian Zhang, Chi Zheng, Guoping Qiu
- **Comment**: Published on ICGIP2017
- **Journal**: None
- **Summary**: In the computer research area, facial expression recognition is a hot research problem. Recent years, the research has moved from the lab environment to in-the-wild circumstances. It is challenging, especially under extreme poses. But current expression detection systems are trying to avoid the pose effects and gain the general applicable ability. In this work, we solve the problem in the opposite approach. We consider the head poses and detect the expressions within special head poses. Our work includes two parts: detect the head pose and group it into one pre-defined head pose class; do facial expression recognize within each pose class. Our experiments show that the recognition results with pose class grouping are much better than that of direct recognition without considering poses. We combine the hand-crafted features, SIFT, LBP and geometric feature, with deep learning feature as the representation of the expressions. The handcrafted features are added into the deep learning framework along with the high level deep learning features. As a comparison, we implement SVM and random forest to as the prediction models. To train and test our methodology, we labeled the face dataset with 6 basic expressions.



### DSNet: Deep and Shallow Feature Learning for Efficient Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/1811.02208v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.02208v1)
- **Published**: 2018-11-06 07:55:42+00:00
- **Updated**: 2018-11-06 07:55:42+00:00
- **Authors**: Qiangqiang Wu, Yan Yan, Yanjie Liang, Yi Liu, Hanzi Wang
- **Comment**: To appear at ACCV 2018. 14 pages, 8 figures
- **Journal**: None
- **Summary**: In recent years, Discriminative Correlation Filter (DCF) based tracking methods have achieved great success in visual tracking. However, the multi-resolution convolutional feature maps trained from other tasks like image classification, cannot be naturally used in the conventional DCF formulation. Furthermore, these high-dimensional feature maps significantly increase the tracking complexity and thus limit the tracking speed. In this paper, we present a deep and shallow feature learning network, namely DSNet, to learn the multi-level same-resolution compressed (MSC) features for efficient online tracking, in an end-to-end offline manner. Specifically, the proposed DSNet compresses multi-level convolutional features to uniform spatial resolution features. The learned MSC features effectively encode both appearance and semantic information of objects in the same-resolution feature maps, thus enabling an elegant combination of the MSC features with any DCF-based methods. Additionally, a channel reliability measurement (CRM) method is presented to further refine the learned MSC features. We demonstrate the effectiveness of the MSC features learned from the proposed DSNet on two DCF tracking frameworks: the basic DCF framework and the continuous convolution operator framework. Extensive experiments show that the learned MSC features have the appealing advantage of allowing the equipped DCF-based tracking methods to perform favorably against the state-of-the-art methods while running at high frame rates.



### Weakly Supervised Scene Parsing with Point-based Distance Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/1811.02233v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.02233v1)
- **Published**: 2018-11-06 09:00:10+00:00
- **Updated**: 2018-11-06 09:00:10+00:00
- **Authors**: Rui Qian, Yunchao Wei, Honghui Shi, Jiachen Li, Jiaying Liu, Thomas Huang
- **Comment**: AAAI2019
- **Journal**: None
- **Summary**: Semantic scene parsing is suffering from the fact that pixel-level annotations are hard to be collected. To tackle this issue, we propose a Point-based Distance Metric Learning (PDML) in this paper. PDML does not require dense annotated masks and only leverages several labeled points that are much easier to obtain to guide the training process. Concretely, we leverage semantic relationship among the annotated points by encouraging the feature representations of the intra- and inter-category points to keep consistent, i.e. points within the same category should have more similar feature representations compared to those from different categories. We formulate such a characteristic into a simple distance metric loss, which collaborates with the point-wise cross-entropy loss to optimize the deep neural networks. Furthermore, to fully exploit the limited annotations, distance metric learning is conducted across different training images instead of simply adopting an image-dependent manner. We conduct extensive experiments on two challenging scene parsing benchmarks of PASCAL-Context and ADE 20K to validate the effectiveness of our PDML, and competitive mIoU scores are achieved.



### Semantic bottleneck for computer vision tasks
- **Arxiv ID**: http://arxiv.org/abs/1811.02234v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1811.02234v1)
- **Published**: 2018-11-06 09:01:02+00:00
- **Updated**: 2018-11-06 09:01:02+00:00
- **Authors**: Maxime Bucher, Stéphane Herbin, Frédéric Jurie
- **Comment**: None
- **Journal**: Asian Conference on Computer Vision (ACCV), Dec 2018, Perth,
  Australia
- **Summary**: This paper introduces a novel method for the representation of images that is semantic by nature, addressing the question of computation intelligibility in computer vision tasks. More specifically, our proposition is to introduce what we call a semantic bottleneck in the processing pipeline, which is a crossing point in which the representation of the image is entirely expressed with natural language , while retaining the efficiency of numerical representations. We show that our approach is able to generate semantic representations that give state-of-the-art results on semantic content-based image retrieval and also perform very well on image classification tasks. Intelligibility is evaluated through user centered experiments for failure detection.



### SparseFool: a few pixels make a big difference
- **Arxiv ID**: http://arxiv.org/abs/1811.02248v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.02248v4)
- **Published**: 2018-11-06 09:30:34+00:00
- **Updated**: 2019-05-27 16:33:41+00:00
- **Authors**: Apostolos Modas, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard
- **Comment**: In Proceedings of IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR), 2019
- **Journal**: None
- **Summary**: Deep Neural Networks have achieved extraordinary results on image classification tasks, but have been shown to be vulnerable to attacks with carefully crafted perturbations of the input data. Although most attacks usually change values of many image's pixels, it has been shown that deep networks are also vulnerable to sparse alterations of the input. However, no computationally efficient method has been proposed to compute sparse perturbations. In this paper, we exploit the low mean curvature of the decision boundary, and propose SparseFool, a geometry inspired sparse attack that controls the sparsity of the perturbations. Extensive evaluations show that our approach computes sparse perturbations very fast, and scales efficiently to high dimensional data. We further analyze the transferability and the visual effects of the perturbations, and show the existence of shared semantic information across the images and the networks. Finally, we show that adversarial training can only slightly improve the robustness against sparse additive perturbations computed with SparseFool.



### MDLatLRR: A novel decomposition method for infrared and visible image fusion
- **Arxiv ID**: http://arxiv.org/abs/1811.02291v5
- **DOI**: 10.1109/TIP.2020.2975984
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.02291v5)
- **Published**: 2018-11-06 11:21:53+00:00
- **Updated**: 2020-03-23 07:49:48+00:00
- **Authors**: Hui Li, Xiao-Jun Wu, Josef Kittler
- **Comment**: IEEE Trans. Image Processing 2020, 14 pages, 17 figures, 3 tables.
  arXiv admin note: text overlap with arXiv:1804.08992
- **Journal**: None
- **Summary**: Image decomposition is crucial for many image processing tasks, as it allows to extract salient features from source images. A good image decomposition method could lead to a better performance, especially in image fusion tasks. We propose a multi-level image decomposition method based on latent low-rank representation(LatLRR), which is called MDLatLRR. This decomposition method is applicable to many image processing fields. In this paper, we focus on the image fusion task. We develop a novel image fusion framework based on MDLatLRR, which is used to decompose source images into detail parts(salient features) and base parts. A nuclear-norm based fusion strategy is used to fuse the detail parts, and the base parts are fused by an averaging strategy. Compared with other state-of-the-art fusion methods, the proposed algorithm exhibits better fusion performance in both subjective and objective evaluation.



### Toward Driving Scene Understanding: A Dataset for Learning Driver Behavior and Causal Reasoning
- **Arxiv ID**: http://arxiv.org/abs/1811.02307v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.02307v1)
- **Published**: 2018-11-06 12:02:55+00:00
- **Updated**: 2018-11-06 12:02:55+00:00
- **Authors**: Vasili Ramanishka, Yi-Ting Chen, Teruhisa Misu, Kate Saenko
- **Comment**: The dataset is available at https://usa.honda-ri.com/hdd
- **Journal**: The IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR), 2018, pp. 7699-7707
- **Summary**: Driving Scene understanding is a key ingredient for intelligent transportation systems. To achieve systems that can operate in a complex physical and social environment, they need to understand and learn how humans drive and interact with traffic scenes. We present the Honda Research Institute Driving Dataset (HDD), a challenging dataset to enable research on learning driver behavior in real-life environments. The dataset includes 104 hours of real human driving in the San Francisco Bay Area collected using an instrumented vehicle equipped with different sensors. We provide a detailed analysis of HDD with a comparison to other driving datasets. A novel annotation methodology is introduced to enable research on driver behavior understanding from untrimmed data sequences. As the first step, baseline algorithms for driver behavior detection are trained and tested to demonstrate the feasibility of the proposed task.



### Fast Adaptive Bilateral Filtering
- **Arxiv ID**: http://arxiv.org/abs/1811.02308v1
- **DOI**: 10.1109/TIP.2018.2871597
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.02308v1)
- **Published**: 2018-11-06 12:04:44+00:00
- **Updated**: 2018-11-06 12:04:44+00:00
- **Authors**: Ruturaj G. Gavaskar, Kunal N. Chaudhury
- **Comment**: None
- **Journal**: R. G. Gavaskar and K. N. Chaudhury, "Fast Adaptive Bilateral
  Filtering," IEEE Transactions on Image Processing, vol. 28, no. 2, pp.
  779-790, Feb. 2019
- **Summary**: In the classical bilateral filter, a fixed Gaussian range kernel is used along with a spatial kernel for edge-preserving smoothing. We consider a generalization of this filter, the so-called adaptive bilateral filter, where the center and width of the Gaussian range kernel is allowed to change from pixel to pixel. Though this variant was originally proposed for sharpening and noise removal, it can also be used for other applications such as artifact removal and texture filtering. Similar to the bilateral filter, the brute-force implementation of its adaptive counterpart requires intense computations. While several fast algorithms have been proposed in the literature for bilateral filtering, most of them work only with a fixed range kernel. In this paper, we propose a fast algorithm for adaptive bilateral filtering, whose complexity does not scale with the spatial filter width. This is based on the observation that the concerned filtering can be performed purely in range space using an appropriately defined local histogram. We show that by replacing the histogram with a polynomial and the finite range-space sum with an integral, we can approximate the filter using analytic functions. In particular, an efficient algorithm is derived using the following innovations: the polynomial is fitted by matching its moments to those of the target histogram (this is done using fast convolutions), and the analytic functions are recursively computed using integration-by-parts. Our algorithm can accelerate the brute-force implementation by at least $20 \times$, without perceptible distortions in the visual quality. We demonstrate the effectiveness of our algorithm for sharpening, JPEG deblocking, and texture filtering.



### Super-Identity Convolutional Neural Network for Face Hallucination
- **Arxiv ID**: http://arxiv.org/abs/1811.02328v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.02328v1)
- **Published**: 2018-11-06 12:50:08+00:00
- **Updated**: 2018-11-06 12:50:08+00:00
- **Authors**: Kaipeng Zhang, Zhanpeng Zhang, Chia-Wen Cheng, Winston H. Hsu, Yu Qiao, Wei Liu, Tong Zhang
- **Comment**: Published in ECCV 2018
- **Journal**: None
- **Summary**: Face hallucination is a generative task to super-resolve the facial image with low resolution while human perception of face heavily relies on identity information. However, previous face hallucination approaches largely ignore facial identity recovery. This paper proposes Super-Identity Convolutional Neural Network (SICNN) to recover identity information for generating faces closed to the real identity. Specifically, we define a super-identity loss to measure the identity difference between a hallucinated face and its corresponding high-resolution face within the hypersphere identity metric space. However, directly using this loss will lead to a Dynamic Domain Divergence problem, which is caused by the large margin between the high-resolution domain and the hallucination domain. To overcome this challenge, we present a domain-integrated training approach by constructing a robust identity metric for faces from these two domains. Extensive experimental evaluations demonstrate that the proposed SICNN achieves superior visual quality over the state-of-the-art methods on a challenging task to super-resolve 12$\times$14 faces with an 8$\times$ upscaling factor. In addition, SICNN significantly improves the recognizability of ultra-low-resolution faces.



### Object 3D Reconstruction based on Photometric Stereo and Inverted Rendering
- **Arxiv ID**: http://arxiv.org/abs/1811.02357v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.02357v1)
- **Published**: 2018-11-06 14:07:35+00:00
- **Updated**: 2018-11-06 14:07:35+00:00
- **Authors**: Anish R. Khadka, Paolo Remagnino, Vasileios Argyriou
- **Comment**: 8 pages, 11 figure, SITIS conference
- **Journal**: None
- **Summary**: Methods for 3D reconstruction such as Photometric stereo recover the shape and reflectance properties using multiple images of an object taken with variable lighting conditions from a fixed viewpoint. Photometric stereo assumes that a scene is illuminated only directly by the illumination source. As result, indirect illumination effects due to inter-reflections introduce strong biases in the recovered shape. Our suggested approach is to recover scene properties in the presence of indirect illumination. To this end, we proposed an iterative PS method combined with a reverted Monte-Carlo ray tracing algorithm to overcome the inter-reflection effects aiming to separate the direct and indirect lighting. This approach iteratively reconstructs a surface considering both the environment around the object and its concavities. We demonstrate and evaluate our approach using three datasets and the overall results illustrate improvement over the classic PS approaches.



### Micro-Attention for Micro-Expression recognition
- **Arxiv ID**: http://arxiv.org/abs/1811.02360v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.02360v5)
- **Published**: 2018-11-06 14:13:01+00:00
- **Updated**: 2019-08-27 08:23:10+00:00
- **Authors**: Chongyang Wang, Min Peng, Tao Bi, Tong Chen
- **Comment**: 17 pages, 5 figures, 7 tables, Code is available at GitHub
- **Journal**: None
- **Summary**: Micro-expression, for its high objectivity in emotion detection, has emerged to be a promising modality in affective computing. Recently, deep learning methods have been successfully introduced into the micro-expression recognition area. Whilst the higher recognition accuracy achieved, substantial challenges in micro-expression recognition remain. The existence of micro expression in small-local areas on face and limited size of available databases still constrain the recognition accuracy on such emotional facial behavior. In this work, to tackle such challenges, we propose a novel attention mechanism called micro-attention cooperating with residual network. Micro-attention enables the network to learn to focus on facial areas of interest covering different action units. Moreover, coping with small datasets, the micro-attention is designed without adding noticeable parameters while a simple yet efficient transfer learning approach is together utilized to alleviate the overfitting risk. With extensive experimental evaluations on three benchmarks (CASMEII, SAMM and SMIC) and post-hoc feature visualizations, we demonstrate the effectiveness of the proposed micro-attention and push the boundary of automatic recognition of micro-expression.



### Fast High-Dimensional Bilateral and Nonlocal Means Filtering
- **Arxiv ID**: http://arxiv.org/abs/1811.02363v1
- **DOI**: 10.1109/TIP.2018.2878955
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.02363v1)
- **Published**: 2018-11-06 14:20:35+00:00
- **Updated**: 2018-11-06 14:20:35+00:00
- **Authors**: Pravin Nair, Kunal. N. Chaudhury
- **Comment**: This work is accepted in IEEE Transactions on Image Processing
- **Journal**: None
- **Summary**: Existing fast algorithms for bilateral and nonlocal means filtering mostly work with grayscale images. They cannot easily be extended to high-dimensional data such as color and hyperspectral images, patch-based data, flow-fields, etc. In this paper, we propose a fast algorithm for high-dimensional bilateral and nonlocal means filtering. Unlike existing approaches, where the focus is on approximating the data (using quantization) or the filter kernel (via analytic expansions), we locally approximate the kernel using weighted and shifted copies of a Gaussian, where the weights and shifts are inferred from the data. The algorithm emerging from the proposed approximation essentially involves clustering and fast convolutions, and is easy to implement. Moreover, a variant of our algorithm comes with a guarantee (bound) on the approximation error, which is not enjoyed by existing algorithms. We present some results for high-dimensional bilateral and nonlocal means filtering to demonstrate the speed and accuracy of our proposal. Moreover, we also show that our algorithm can outperform state-of-the-art fast approximations in terms of accuracy and timing.



### Identificação automática de pichação a partir de imagens urbanas
- **Arxiv ID**: http://arxiv.org/abs/1811.02372v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.02372v1)
- **Published**: 2018-11-06 14:41:57+00:00
- **Updated**: 2018-11-06 14:41:57+00:00
- **Authors**: Eric K. Tokuda, Claudio T. Silva, Roberto M. Cesar-Jr
- **Comment**: Presented at IEEE-SIBGRAPI WiP'18, in Portuguese
- **Journal**: None
- **Summary**: Graffiti tagging is a common issue in great cities an local authorities are on the move to combat it. The tagging map of a city can be a useful tool as it may help to clean-up highly saturated regions and discourage future acts in the neighbourhood and currently there is no way of getting a tagging map of a region in an automatic fashion and manual inspection or crowd participation are required. In this work, we describe a work in progress in creating an automatic way to get a tagging map of a city or region. It is based on the use of street view images and on the detection of graffiti tags in the images.



### Sets of autoencoders with shared latent spaces
- **Arxiv ID**: http://arxiv.org/abs/1811.02373v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.02373v1)
- **Published**: 2018-11-06 14:42:36+00:00
- **Updated**: 2018-11-06 14:42:36+00:00
- **Authors**: Vasily Morzhakov
- **Comment**: 13 pages,16 figures
- **Journal**: None
- **Summary**: Autoencoders receive latent models of input data. It was shown in recent works that they also estimate probability density functions of the input. This fact makes using the Bayesian decision theory possible. If we obtain latent models of input data for each class or for some points in the space of parameters in a parameter estimation task, we are able to estimate likelihood functions for those classes or points in parameter space. We show how the set of autoencoders solves the recognition problem. Each autoencoder describes its own model or context, a latent vector that presents input data in the latent space may be called treatment in its context. Sharing latent spaces of autoencoders gives a very important property that is the ability to separate treatment and context where the input information is treated through the set of autoencoders. There are two remarkable and most valuable results of this work: a mechanism that shows a possible way of forming abstract concepts and a way of reducing dataset's size during training. These results are confirmed by tests presented in the article.



### Fine-grained Apparel Classification and Retrieval without rich annotations
- **Arxiv ID**: http://arxiv.org/abs/1811.02385v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.02385v1)
- **Published**: 2018-11-06 14:55:33+00:00
- **Updated**: 2018-11-06 14:55:33+00:00
- **Authors**: Aniket Bhatnagar, Sanchit Aggarwal
- **Comment**: 14 pages, 6 figures, 3 tables, Submitted to Springer Journal of
  Applied Intelligence
- **Journal**: None
- **Summary**: The ability to correctly classify and retrieve apparel images has a variety of applications important to e-commerce, online advertising and internet search. In this work, we propose a robust framework for fine-grained apparel classification, in-shop and cross-domain retrieval which eliminates the requirement of rich annotations like bounding boxes and human-joints or clothing landmarks, and training of bounding box/ key-landmark detector for the same. Factors such as subtle appearance differences, variations in human poses, different shooting angles, apparel deformations, and self-occlusion add to the challenges in classification and retrieval of apparel items. Cross-domain retrieval is even harder due to the presence of large variation between online shopping images, usually taken in ideal lighting, pose, positive angle and clean background as compared with street photos captured by users in complicated conditions with poor lighting and cluttered scenes. Our framework uses compact bilinear CNN with tensor sketch algorithm to generate embeddings that capture local pairwise feature interactions in a translationally invariant manner. For apparel classification, we pass the feature embeddings through a softmax classifier, while, the in-shop and cross-domain retrieval pipelines use a triplet-loss based optimization approach, such that squared Euclidean distance between embeddings measures the dissimilarity between the images. Unlike previous works that relied on bounding box, key clothing landmarks or human joint detectors to assist the final deep classifier, proposed framework can be trained directly on the provided category labels or generated triplets for triplet loss optimization. Lastly, Experimental results on the DeepFashion fine-grained categorization, and in-shop and consumer-to-shop retrieval datasets provide a comparative analysis with previous work performed in the domain.



### Point2Sequence: Learning the Shape Representation of 3D Point Clouds with an Attention-based Sequence to Sequence Network
- **Arxiv ID**: http://arxiv.org/abs/1811.02565v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.02565v2)
- **Published**: 2018-11-06 15:07:03+00:00
- **Updated**: 2018-11-15 07:40:03+00:00
- **Authors**: Xinhai Liu, Zhizhong Han, Yu-Shen Liu, Matthias Zwicker
- **Comment**: To be published in AAAI 2019
- **Journal**: None
- **Summary**: Exploring contextual information in the local region is important for shape understanding and analysis. Existing studies often employ hand-crafted or explicit ways to encode contextual information of local regions. However, it is hard to capture fine-grained contextual information in hand-crafted or explicit manners, such as the correlation between different areas in a local region, which limits the discriminative ability of learned features. To resolve this issue, we propose a novel deep learning model for 3D point clouds, named Point2Sequence, to learn 3D shape features by capturing fine-grained contextual information in a novel implicit way. Point2Sequence employs a novel sequence learning model for point clouds to capture the correlations by aggregating multi-scale areas of each local region with attention. Specifically, Point2Sequence first learns the feature of each area scale in a local region. Then, it captures the correlation between area scales in the process of aggregating all area scales using a recurrent neural network (RNN) based encoder-decoder structure, where an attention mechanism is proposed to highlight the importance of different area scales. Experimental results show that Point2Sequence achieves state-of-the-art performance in shape classification and segmentation tasks.



### A Bit Too Much? High Speed Imaging from Sparse Photon Counts
- **Arxiv ID**: http://arxiv.org/abs/1811.02396v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.02396v3)
- **Published**: 2018-11-06 15:07:08+00:00
- **Updated**: 2019-05-11 14:30:00+00:00
- **Authors**: Paramanand Chandramouli, Samuel Burri, Claudio Bruschini, Edoardo Charbon, Andreas Kolb
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in photographic sensing technologies have made it possible to achieve light detection in terms of a single photon. Photon counting sensors are being increasingly used in many diverse applications. We address the problem of jointly recovering spatial and temporal scene radiance from very few photon counts. Our ConvNet-based scheme effectively combines spatial and temporal information present in measurements to reduce noise. We demonstrate that using our method one can acquire videos at a high frame rate and still achieve good quality signal-to-noise ratio. Experiments show that the proposed scheme performs quite well in different challenging scenarios while the existing approaches are unable to handle them.



### Synaptic Strength For Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1811.02454v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.02454v1)
- **Published**: 2018-11-06 16:06:49+00:00
- **Updated**: 2018-11-06 16:06:49+00:00
- **Authors**: Chen Lin, Zhao Zhong, Wei Wu, Junjie Yan
- **Comment**: Accepted by NIPS 2018
- **Journal**: None
- **Summary**: Convolutional Neural Networks(CNNs) are both computation and memory intensive which hindered their deployment in mobile devices. Inspired by the relevant concept in neural science literature, we propose Synaptic Pruning: a data-driven method to prune connections between input and output feature maps with a newly proposed class of parameters called Synaptic Strength. Synaptic Strength is designed to capture the importance of a connection based on the amount of information it transports. Experiment results show the effectiveness of our approach. On CIFAR-10, we prune connections for various CNN models with up to 96% , which results in significant size reduction and computation saving. Further evaluation on ImageNet demonstrates that synaptic pruning is able to discover efficient models which is competitive to state-of-the-art compact CNNs such as MobileNet-V2 and NasNet-Mobile. Our contribution is summarized as following: (1) We introduce Synaptic Strength, a new class of parameters for CNNs to indicate the importance of each connections. (2) Our approach can prune various CNNs with high compression without compromising accuracy. (3) Further investigation shows, the proposed Synaptic Strength is a better indicator for kernel pruning compared with the previous approach in both empirical result and theoretical analysis.



### Evolvement Constrained Adversarial Learning for Video Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/1811.02476v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.02476v1)
- **Published**: 2018-11-06 16:31:19+00:00
- **Updated**: 2018-11-06 16:31:19+00:00
- **Authors**: Wenbo Li, Longyin Wen, Xiao Bian, Siwei Lyu
- **Comment**: None
- **Journal**: None
- **Summary**: Video style transfer is a useful component for applications such as augmented reality, non-photorealistic rendering, and interactive games. Many existing methods use optical flow to preserve the temporal smoothness of the synthesized video. However, the estimation of optical flow is sensitive to occlusions and rapid motions. Thus, in this work, we introduce a novel evolve-sync loss computed by evolvements to replace optical flow. Using this evolve-sync loss, we build an adversarial learning framework, termed as Video Style Transfer Generative Adversarial Network (VST-GAN), which improves upon the MGAN method for image style transfer for more efficient video style transfer. We perform extensive experimental evaluations of our method and show quantitative and qualitative improvements over the state-of-the-art methods.



### Towards continual learning in medical imaging
- **Arxiv ID**: http://arxiv.org/abs/1811.02496v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.02496v1)
- **Published**: 2018-11-06 17:09:17+00:00
- **Updated**: 2018-11-06 17:09:17+00:00
- **Authors**: Chaitanya Baweja, Ben Glocker, Konstantinos Kamnitsas
- **Comment**: Accepted in Medical Imaging meets NIPS Workshop, NIPS 2018
- **Journal**: None
- **Summary**: This work investigates continual learning of two segmentation tasks in brain MRI with neural networks. To explore in this context the capabilities of current methods for countering catastrophic forgetting of the first task when a new one is learned, we investigate elastic weight consolidation, a recently proposed method based on Fisher information, originally evaluated on reinforcement learning of Atari games. We use it to sequentially learn segmentation of normal brain structures and then segmentation of white matter lesions. Our findings show this recent method reduces catastrophic forgetting, while large room for improvement exists in these challenging settings for continual learning.



### Deep feature transfer between localization and segmentation tasks
- **Arxiv ID**: http://arxiv.org/abs/1811.02539v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.02539v2)
- **Published**: 2018-11-06 18:28:56+00:00
- **Updated**: 2018-11-10 17:23:13+00:00
- **Authors**: Szu-Yeu Hu, Andrew Beers, Ken Chang, Kathi Höbel, J. Peter Campbell, Deniz Erdogumus, Stratis Ioannidis, Jennifer Dy, Michael F. Chiang, Jayashree Kalpathy-Cramer, James M. Brown
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a new pre-training scheme for U-net based image segmentation. We first train the encoding arm as a localization network to predict the center of the target, before extending it into a U-net architecture for segmentation. We apply our proposed method to the problem of segmenting the optic disc from fundus photographs. Our work shows that the features learned by encoding arm can be transferred to the segmentation network to reduce the annotation burden. We propose that an approach could have broad utility for medical image segmentation, and alleviate the burden of delineating complex structures by pre-training on annotations that are much easier to acquire.



### Hide-and-Seek: A Data Augmentation Technique for Weakly-Supervised Localization and Beyond
- **Arxiv ID**: http://arxiv.org/abs/1811.02545v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.02545v1)
- **Published**: 2018-11-06 18:35:16+00:00
- **Updated**: 2018-11-06 18:35:16+00:00
- **Authors**: Krishna Kumar Singh, Hao Yu, Aron Sarmasi, Gautam Pradeep, Yong Jae Lee
- **Comment**: TPAMI submission. This is a journal extension of our ICCV 2017 paper
  arXiv:1704.04232
- **Journal**: None
- **Summary**: We propose 'Hide-and-Seek' a general purpose data augmentation technique, which is complementary to existing data augmentation techniques and is beneficial for various visual recognition tasks. The key idea is to hide patches in a training image randomly, in order to force the network to seek other relevant content when the most discriminative content is hidden. Our approach only needs to modify the input image and can work with any network to improve its performance. During testing, it does not need to hide any patches. The main advantage of Hide-and-Seek over existing data augmentation techniques is its ability to improve object localization accuracy in the weakly-supervised setting, and we therefore use this task to motivate the approach. However, Hide-and-Seek is not tied only to the image localization task, and can generalize to other forms of visual input like videos, as well as other recognition tasks like image classification, temporal action localization, semantic segmentation, emotion recognition, age/gender estimation, and person re-identification. We perform extensive experiments to showcase the advantage of Hide-and-Seek on these various visual recognition problems.



### Embedded polarizing filters to separate diffuse and specular reflection
- **Arxiv ID**: http://arxiv.org/abs/1811.02608v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.02608v1)
- **Published**: 2018-11-06 19:43:45+00:00
- **Updated**: 2018-11-06 19:43:45+00:00
- **Authors**: Laurent Valentin Jospin, Gilles Baechler, Adam Scholefield
- **Comment**: ACCV 2018
- **Journal**: None
- **Summary**: Polarizing filters provide a powerful way to separate diffuse and specular reflection; however, traditional methods rely on several captures and require proper alignment of the filters. Recently, camera manufacturers have proposed to embed polarizing micro-filters in front of the sensor, creating a mosaic of pixels with different polarizations. In this paper, we investigate the advantages of such camera designs. In particular, we consider different design patterns for the filter arrays and propose an algorithm to demosaic an image generated by such cameras. This essentially allows us to separate the diffuse and specular components using a single image. The performance of our algorithm is compared with a color-based method using synthetic and real data. Finally, we demonstrate how we can recover the normals of a scene using the diffuse images estimated by our method.



### Automatic Assessment of Full Left Ventricular Coverage in Cardiac Cine Magnetic Resonance Imaging with Fisher-Discriminative 3D CNN
- **Arxiv ID**: http://arxiv.org/abs/1811.02688v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.02688v2)
- **Published**: 2018-11-06 22:07:09+00:00
- **Updated**: 2018-11-08 12:00:14+00:00
- **Authors**: Le Zhang, Ali Gooya, Marco Pereanez, Bo Dong, Stefan K. Piechnik, Stefan Neubauer, Steffen E. Petersen, Alejandro F. Frangi
- **Comment**: 12 pages, 5 figures, accepted by IEEE Transactions on Biomedical
  Engineering
- **Journal**: None
- **Summary**: Cardiac magnetic resonance (CMR) images play a growing role in the diagnostic imaging of cardiovascular diseases. Full coverage of the left ventricle (LV), from base to apex, is a basic criterion for CMR image quality and necessary for accurate measurement of cardiac volume and functional assessment. Incomplete coverage of the LV is identified through visual inspection, which is time-consuming and usually done retrospectively in the assessment of large imaging cohorts. This paper proposes a novel automatic method for determining LV coverage from CMR images by using Fisher-discriminative three-dimensional (FD3D) convolutional neural networks (CNNs). In contrast to our previous method employing 2D CNNs, this approach utilizes spatial contextual information in CMR volumes, extracts more representative high-level features and enhances the discriminative capacity of the baseline 2D CNN learning framework, thus achieving superior detection accuracy. A two-stage framework is proposed to identify missing basal and apical slices in measurements of CMR volume. First, the FD3D CNN extracts high-level features from the CMR stacks. These image representations are then used to detect the missing basal and apical slices. Compared to the traditional 3D CNN strategy, the proposed FD3D CNN minimizes within-class scatter and maximizes between-class scatter. We performed extensive experiments to validate the proposed method on more than 5,000 independent volumetric CMR scans from the UK Biobank study, achieving low error rates for missing basal/apical slice detection (4.9\%/4.6\%). The proposed method can also be adopted for assessing LV coverage for other types of CMR image data.



### Training Domain Specific Models for Energy-Efficient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1811.02689v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.02689v2)
- **Published**: 2018-11-06 22:07:54+00:00
- **Updated**: 2018-11-18 06:13:13+00:00
- **Authors**: Kentaro Yoshioka, Edward Lee, Mark Horowitz
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an end-to-end framework for training domain specific models (DSMs) to obtain both high accuracy and computational efficiency for object detection tasks. DSMs are trained with distillation \cite{hinton2015distilling} and focus on achieving high accuracy at a limited domain (e.g. fixed view of an intersection). We argue that DSMs can capture essential features well even with a small model size, enabling higher accuracy and efficiency than traditional techniques. In addition, we improve the training efficiency by reducing the dataset size by culling easy to classify images from the training set. For the limited domain, we observed that compact DSMs significantly surpass the accuracy of COCO trained models of the same size. By training on a compact dataset, we show that with an accuracy drop of only 3.6\%, the training time can be reduced by 93\%. The codes are uploaded in https://github.com/kentaroy47/training-domain-specific-models.



