# Arxiv Papers in cs.CV on 2018-11-21
### Seeing in the dark with recurrent convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/1811.08537v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.08537v1)
- **Published**: 2018-11-21 01:05:48+00:00
- **Updated**: 2018-11-21 01:05:48+00:00
- **Authors**: Till S. Hartmann
- **Comment**: 12 pages (with appendix), 6 figure (main text), 3 supplementary
  figures
- **Journal**: None
- **Summary**: Classical convolutional neural networks (cCNNs) are very good at categorizing objects in images. But, unlike human vision which is relatively robust to noise in images, the performance of cCNNs declines quickly as image quality worsens. Here we propose to use recurrent connections within the convolutional layers to make networks robust against pixel noise such as could arise from imaging at low light levels, and thereby significantly increase their performance when tested with simulated noisy video sequences. We show that cCNNs classify images with high signal to noise ratios (SNRs) well, but are easily outperformed when tested with low SNR images (high noise levels) by convolutional neural networks that have recurrency added to convolutional layers, henceforth referred to as gruCNNs. Addition of Bayes-optimal temporal integration to allow the cCNN to integrate multiple image frames still does not match gruCNN performance. Additionally, we show that at low SNRs, the probabilities predicted by the gruCNN (after calibration) have higher confidence than those predicted by the cCNN. We propose to consider recurrent connections in the early stages of neural networks as a solution to computer vision under imperfect lighting conditions and noisy environments; challenges faced during real-time video streams of autonomous driving at night, during rain or snow, and other non-ideal situations.



### CNN based dense underwater 3D scene reconstruction by transfer learning using bubble database
- **Arxiv ID**: http://arxiv.org/abs/1811.09675v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.09675v1)
- **Published**: 2018-11-21 01:08:51+00:00
- **Updated**: 2018-11-21 01:08:51+00:00
- **Authors**: Kazuto Ichimaru, Ryo Furukawa, Hiroshi Kawasaki
- **Comment**: IEEE Winter Conference on Applications of Computer Vision. arXiv
  admin note: text overlap with arXiv:1808.08348
- **Journal**: None
- **Summary**: Dense 3D shape acquisition of swimming human or live fish is an important research topic for sports, biological science and so on. For this purpose, active stereo sensor is usually used in the air, however it cannot be applied to the underwater environment because of refraction, strong light attenuation and severe interference of bubbles. Passive stereo is a simple solution for capturing dynamic scenes at underwater environment, however the shape with textureless surfaces or irregular reflections cannot be recovered. Recently, the stereo camera pair with a pattern projector for adding artificial textures on the objects is proposed. However, to use the system for underwater environment, several problems should be compensated, i.e., disturbance by fluctuation and bubbles. Simple solution is to use convolutional neural network for stereo to cancel the effects of bubbles and/or water fluctuation. Since it is not easy to train CNN with small size of database with large variation, we develop a special bubble generation device to efficiently create real bubble database of multiple size and density. In addition, we propose a transfer learning technique for multi-scale CNN to effectively remove bubbles and projected-patterns on the object. Further, we develop a real system and actually captured live swimming human, which has not been done before. Experiments are conducted to show the effectiveness of our method compared with the state of the art techniques.



### Adjustable Real-time Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/1811.08560v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08560v1)
- **Published**: 2018-11-21 02:20:05+00:00
- **Updated**: 2018-11-21 02:20:05+00:00
- **Authors**: Mohammad Babaeizadeh, Golnaz Ghiasi
- **Comment**: None
- **Journal**: None
- **Summary**: Artistic style transfer is the problem of synthesizing an image with content similar to a given image and style similar to another. Although recent feed-forward neural networks can generate stylized images in real-time, these models produce a single stylization given a pair of style/content images, and the user doesn't have control over the synthesized output. Moreover, the style transfer depends on the hyper-parameters of the model with varying "optimum" for different input images. Therefore, if the stylized output is not appealing to the user, she/he has to try multiple models or retrain one with different hyper-parameters to get a favorite stylization. In this paper, we address these issues by proposing a novel method which allows adjustment of crucial hyper-parameters, after the training and in real-time, through a set of manually adjustable parameters. These parameters enable the user to modify the synthesized outputs from the same pair of style/content images, in search of a favorite stylized image. Our quantitative and qualitative experiments indicate how adjusting these parameters is comparable to retraining the model with different hyper-parameters. We also demonstrate how these parameters can be randomized to generate results which are diverse but still very similar in style and content.



### Adaptive Re-ranking of Deep Feature for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1811.08561v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.08561v1)
- **Published**: 2018-11-21 02:22:03+00:00
- **Updated**: 2018-11-21 02:22:03+00:00
- **Authors**: Yong Liu, Lin Shang, Andy Song
- **Comment**: None
- **Journal**: None
- **Summary**: Typical person re-identification (re-ID) methods train a deep CNN to extract deep features and combine them with a distance metric for the final evaluation. In this work, we focus on exploiting the full information encoded in the deep feature to boost the re-ID performance. First, we propose a Deep Feature Fusion (DFF) method to exploit the diverse information embedded in a deep feature. DFF treats each sub-feature as an information carrier and employs a diffusion process to exchange their information. Second, we propose an Adaptive Re-Ranking (ARR) method to exploit the contextual information encoded in the features of neighbors. ARR utilizes the contextual information to re-rank the retrieval results in an iterative manner. Particularly, it adds more contextual information after each iteration automatically to consider more matches. Third, we propose a strategy that combines DFF and ARR to enhance the performance. Extensive comparative evaluations demonstrate the superiority of the proposed methods on three large benchmarks.



### Unsupervised Single Image Deraining with Self-supervised Constraints
- **Arxiv ID**: http://arxiv.org/abs/1811.08575v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08575v1)
- **Published**: 2018-11-21 02:50:06+00:00
- **Updated**: 2018-11-21 02:50:06+00:00
- **Authors**: Xin Jin, Zhibo Chen, Jianxin Lin, Zhikai Chen, Wei Zhou
- **Comment**: 10 pages, 8 figures
- **Journal**: None
- **Summary**: Most existing single image deraining methods require learning supervised models from a large set of paired synthetic training data, which limits their generality, scalability and practicality in real-world multimedia applications. Besides, due to lack of labeled-supervised constraints, directly applying existing unsupervised frameworks to the image deraining task will suffer from low-quality recovery. Therefore, we propose an Unsupervised Deraining Generative Adversarial Network (UD-GAN) to tackle above problems by introducing self-supervised constraints from the intrinsic statistics of unpaired rainy and clean images. Specifically, we firstly design two collaboratively optimized modules, namely Rain Guidance Module (RGM) and Background Guidance Module (BGM), to take full advantage of rainy image characteristics: The RGM is designed to discriminate real rainy images from fake rainy images which are created based on outputs of the generator with BGM. Simultaneously, the BGM exploits a hierarchical Gaussian-Blur gradient error to ensure background consistency between rainy input and de-rained output. Secondly, a novel luminance-adjusting adversarial loss is integrated into the clean image discriminator considering the built-in luminance difference between real clean images and derained images. Comprehensive experiment results on various benchmarking datasets and different training settings show that UD-GAN outperforms existing image deraining methods in both quantitative and qualitative comparisons.



### Progressive Feature Alignment for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1811.08585v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.08585v2)
- **Published**: 2018-11-21 03:32:31+00:00
- **Updated**: 2019-05-19 11:43:21+00:00
- **Authors**: Chaoqi Chen, Weiping Xie, Wenbing Huang, Yu Rong, Xinghao Ding, Yue Huang, Tingyang Xu, Junzhou Huang
- **Comment**: Accepted by CVPR 2019
- **Journal**: None
- **Summary**: Unsupervised domain adaptation (UDA) transfers knowledge from a label-rich source domain to a fully-unlabeled target domain. To tackle this task, recent approaches resort to discriminative domain transfer in virtue of pseudo-labels to enforce the class-level distribution alignment across the source and target domains. These methods, however, are vulnerable to the error accumulation and thus incapable of preserving cross-domain category consistency, as the pseudo-labeling accuracy is not guaranteed explicitly. In this paper, we propose the Progressive Feature Alignment Network (PFAN) to align the discriminative features across domains progressively and effectively, via exploiting the intra-class variation in the target domain. To be specific, we first develop an Easy-to-Hard Transfer Strategy (EHTS) and an Adaptive Prototype Alignment (APA) step to train our model iteratively and alternatively. Moreover, upon observing that a good domain adaptation usually requires a non-saturated source classifier, we consider a simple yet efficient way to retard the convergence speed of the source classification loss by further involving a temperature variate into the soft-max function. The extensive experimental results reveal that the proposed PFAN exceeds the state-of-the-art performance on three UDA datasets.



### Real-Time 6D Object Pose Estimation on CPU
- **Arxiv ID**: http://arxiv.org/abs/1811.08588v3
- **DOI**: 10.1109/IROS40897.2019.8967967
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1811.08588v3)
- **Published**: 2018-11-21 03:42:20+00:00
- **Updated**: 2019-08-26 05:45:04+00:00
- **Authors**: Yoshinori Konishi, Kosuke Hattori, Manabu Hashimoto
- **Comment**: accepted to IROS 2019
- **Journal**: IEEE/RSJ International Conference on Intelligent Robots and
  Systems (IROS), Macau, China, 2019, pp. 3451-3458
- **Summary**: We propose a fast and accurate 6D object pose estimation from a RGB-D image. Our proposed method is template matching based and consists of three main technical components, PCOF-MOD (multimodal PCOF), balanced pose tree (BPT) and optimum memory rearrangement for a coarse-to-fine search. Our model templates on densely sampled viewpoints and PCOF-MOD which explicitly handles a certain range of 3D object pose improve the robustness against background clutters. BPT which is an efficient tree-based data structures for a large number of templates and template matching on rearranged feature maps where nearby features are linearly aligned accelerate the pose estimation. The experimental evaluation on tabletop and bin-picking dataset showed that our method achieved higher accuracy and faster speed in comparison with state-of-the-art techniques including recent CNN based approaches. Moreover, our model templates can be trained only from 3D CAD in a few minutes and the pose estimation run in near real-time (23 fps) on CPU. These features are suitable for any real applications.



### Graph-Adaptive Pruning for Efficient Inference of Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1811.08589v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.08589v1)
- **Published**: 2018-11-21 03:43:38+00:00
- **Updated**: 2018-11-21 03:43:38+00:00
- **Authors**: Mengdi Wang, Qing Zhang, Jun Yang, Xiaoyuan Cui, Wei Lin
- **Comment**: 7 pages, 7 figures
- **Journal**: None
- **Summary**: In this work, we propose a graph-adaptive pruning (GAP) method for efficient inference of convolutional neural networks (CNNs). In this method, the network is viewed as a computational graph, in which the vertices denote the computation nodes and edges represent the information flow. Through topology analysis, GAP is capable of adapting to different network structures, especially the widely used cross connections and multi-path data flow in recent novel convolutional models. The models can be adaptively pruned at vertex-level as well as edge-level without any post-processing, thus GAP can directly get practical model compression and inference speed-up. Moreover, it does not need any customized computation library or hardware support. Finetuning is conducted after pruning to restore the model performance. In the finetuning step, we adopt a self-taught knowledge distillation (KD) strategy by utilizing information from the original model, through which, the performance of the optimized model can be sufficiently improved, without introduction of any other teacher model. Experimental results show the proposed GAP can achieve promising result to make inference more efficient, e.g., for ResNeXt-29 on CIFAR10, it can get 13X model compression and 4.3X practical speed-up with marginal loss of accuracy.



### Measuring Depression Symptom Severity from Spoken Language and 3D Facial Expressions
- **Arxiv ID**: http://arxiv.org/abs/1811.08592v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1811.08592v2)
- **Published**: 2018-11-21 03:52:31+00:00
- **Updated**: 2018-11-27 01:49:11+00:00
- **Authors**: Albert Haque, Michelle Guo, Adam S Miner, Li Fei-Fei
- **Comment**: Machine Learning for Health (ML4H) Workshop at NeurIPS 2018
  arXiv:1811.07216
- **Journal**: None
- **Summary**: With more than 300 million people depressed worldwide, depression is a global problem. Due to access barriers such as social stigma, cost, and treatment availability, 60% of mentally-ill adults do not receive any mental health services. Effective and efficient diagnosis relies on detecting clinical symptoms of depression. Automatic detection of depressive symptoms would potentially improve diagnostic accuracy and availability, leading to faster intervention. In this work, we present a machine learning method for measuring the severity of depressive symptoms. Our multi-modal method uses 3D facial expressions and spoken language, commonly available from modern cell phones. It demonstrates an average error of 3.67 points (15.3% relative) on the clinically-validated Patient Health Questionnaire (PHQ) scale. For detecting major depressive disorder, our model demonstrates 83.3% sensitivity and 82.6% specificity. Overall, this paper shows how speech recognition, computer vision, and natural language processing can be combined to assist mental health patients and practitioners. This technology could be deployed to cell phones worldwide and facilitate low-cost universal access to mental health care.



### Learning to Attend Relevant Regions in Videos from Eye Fixations
- **Arxiv ID**: http://arxiv.org/abs/1811.08594v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08594v3)
- **Published**: 2018-11-21 04:20:03+00:00
- **Updated**: 2020-12-04 11:38:46+00:00
- **Authors**: Thanh T. Nguyen, Dung Nguyen
- **Comment**: This is an incomplete work
- **Journal**: None
- **Summary**: Attentively important regions in video frames account for a majority part of the semantics in each frame. This information is helpful in many applications not only for entertainment (such as auto generating commentary and tourist guide) but also for robotic control which holds a larascope supported for laparoscopic surgery. However, it is not always straightforward to define and locate such semantic regions in videos. In this work, we attempt to address the problem of attending relevant regions in videos by leveraging the eye fixations labels with a RNN-based visual attention model. Our experimental results suggest that this approach holds a good potential to learn to attend semantic regions in videos while its performance also heavily relies on the quality of eye fixations labels.



### M2E-Try On Net: Fashion from Model to Everyone
- **Arxiv ID**: http://arxiv.org/abs/1811.08599v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08599v3)
- **Published**: 2018-11-21 05:11:42+00:00
- **Updated**: 2019-08-07 09:34:57+00:00
- **Authors**: Zhonghua Wu, Guosheng Lin, Qingyi Tao, Jianfei Cai
- **Comment**: None
- **Journal**: None
- **Summary**: Most existing virtual try-on applications require clean clothes images. Instead, we present a novel virtual Try-On network, M2E-Try On Net, which transfers the clothes from a model image to a person image without the need of any clean product images. To obtain a realistic image of person wearing the desired model clothes, we aim to solve the following challenges: 1) non-rigid nature of clothes - we need to align poses between the model and the user; 2) richness in textures of fashion items - preserving the fine details and characteristics of the clothes is critical for photo-realistic transfer; 3) variation of identity appearances - it is required to fit the desired model clothes to the person identity seamlessly. To tackle these challenges, we introduce three key components, including the pose alignment network (PAN), the texture refinement network (TRN) and the fitting network (FTN). Since it is unlikely to gather image pairs of input person image and desired output image (i.e. person wearing the desired clothes), our framework is trained in a self-supervised manner to gradually transfer the poses and textures of the model's clothes to the desired appearance. In the experiments, we verify on the Deep Fashion dataset and MVC dataset that our method can generate photo-realistic images for the person to try-on the model clothes. Furthermore, we explore the model capability for different fashion items, including both upper and lower garments.



### Scene Text Detection with Supervised Pyramid Context Network
- **Arxiv ID**: http://arxiv.org/abs/1811.08605v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08605v1)
- **Published**: 2018-11-21 06:13:03+00:00
- **Updated**: 2018-11-21 06:13:03+00:00
- **Authors**: Enze Xie, Yuhang Zang, Shuai Shao, Gang Yu, Cong Yao, Guangyao Li
- **Comment**: Accepted by AAAI 2019
- **Journal**: None
- **Summary**: Scene text detection methods based on deep learning have achieved remarkable results over the past years. However, due to the high diversity and complexity of natural scenes, previous state-of-the-art text detection methods may still produce a considerable amount of false positives, when applied to images captured in real-world environments. To tackle this issue, mainly inspired by Mask R-CNN, we propose in this paper an effective model for scene text detection, which is based on Feature Pyramid Network (FPN) and instance segmentation. We propose a supervised pyramid context network (SPCNET) to precisely locate text regions while suppressing false positives. Benefited from the guidance of semantic information and sharing FPN, SPCNET obtains significantly enhanced performance while introducing marginal extra computation. Experiments on standard datasets demonstrate that our SPCNET clearly outperforms start-of-the-art methods. Specifically, it achieves an F-measure of 92.1% on ICDAR2013, 87.2% on ICDAR2015, 74.1% on ICDAR2017 MLT and 82.9% on Total-Text.



### Joint Mapping and Calibration via Differentiable Sensor Fusion
- **Arxiv ID**: http://arxiv.org/abs/1812.00880v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.00880v2)
- **Published**: 2018-11-21 06:22:06+00:00
- **Updated**: 2018-12-20 04:58:10+00:00
- **Authors**: Jonathan P. Chen, Fritz Obermeyer, Vladimir Lyapunov, Lionel Gueguen, Noah D. Goodman
- **Comment**: None
- **Journal**: None
- **Summary**: We leverage automatic differentiation (AD) and probabilistic programming to develop an end-to-end optimization algorithm for batch triangulation of a large number of unknown objects. Given noisy detections extracted from noisily geo-located street level imagery without depth information, we jointly estimate the number and location of objects of different types, together with parameters for sensor noise characteristics and prior distribution of objects conditioned on side information. The entire algorithm is framed as nested stochastic variational inference. An inner loop solves a soft data association problem via loopy belief propagation; a middle loop performs soft EM clustering using a regularized Newton solver (leveraging an AD framework); an outer loop backpropagates through the inner loops to train global parameters. We place priors over sensor parameters for different traffic object types, and demonstrate improvements with richer priors incorporating knowledge of the environment.   We test our algorithm on detections of road signs observed by cars with mounted cameras, though in practice this technique can be used for any geo-tagged images. The detections were extracted by neural image detectors and classifiers, and we independently triangulate each type of sign (e.g. stop, traffic light). We find that our model is more robust to DNN misclassifications than current methods, generalizes across sign types, and can use geometric information to increase precision. Our algorithm outperforms our current production baseline based on k-means clustering. We show that variational inference training allows generalization by learning sign-specific parameters.



### A Novel Integrated Framework for Learning both Text Detection and Recognition
- **Arxiv ID**: http://arxiv.org/abs/1811.08611v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.08611v1)
- **Published**: 2018-11-21 07:14:34+00:00
- **Updated**: 2018-11-21 07:14:34+00:00
- **Authors**: Wanchen Sui, Qing Zhang, Jun Yang, Wei Chu
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel integrated framework for learning both text detection and recognition. For most of the existing methods, detection and recognition are treated as two isolated tasks and trained separately, since parameters of detection and recognition models are different and two models target to optimize their own loss functions during individual training processes. In contrast to those methods, by sharing model parameters, we merge the detection model and recognition model into a single end-to-end trainable model and train the joint model for two tasks simultaneously. The shared parameters not only help effectively reduce the computational load in inference process, but also improve the end-to-end text detection-recognition accuracy. In addition, we design a simpler and faster sequence learning method for the recognition network based on a succession of stacked convolutional layers without any recurrent structure, this is proved feasible and dramatically improves inference speed. Extensive experiments on different datasets demonstrate that the proposed method achieves very promising results.



### Neural Networks with Activation Networks
- **Arxiv ID**: http://arxiv.org/abs/1811.08618v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08618v1)
- **Published**: 2018-11-21 07:54:41+00:00
- **Updated**: 2018-11-21 07:54:41+00:00
- **Authors**: Jinhyeok Jang, Jaehong Kim, Jaeyeon Lee, Seungjoon Yang
- **Comment**: None
- **Journal**: None
- **Summary**: This work presents an adaptive activation method for neural networks that exploits the interdependency of features. Each pixel, node, and layer is assigned with a polynomial activation function, whose coefficients are provided by an auxiliary activation network. The activation of a feature depends on the features of neighboring pixels in a convolutional layer and other nodes in a dense layer. The dependency is learned from data by the activation networks. In our experiments, networks with activation networks provide significant performance improvement compared to the baseline networks on which they are built. The proposed method can be used to improve the network performance as an alternative to increasing the number of nodes and layers.



### Angular Triplet-Center Loss for Multi-view 3D Shape Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1811.08622v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08622v3)
- **Published**: 2018-11-21 08:07:04+00:00
- **Updated**: 2019-01-22 08:52:46+00:00
- **Authors**: Zhaoqun Li, Cheng Xu, Biao Leng
- **Comment**: Accepted by AAAI 2019
- **Journal**: None
- **Summary**: How to obtain the desirable representation of a 3D shape, which is discriminative across categories and polymerized within classes, is a significant challenge in 3D shape retrieval. Most existing 3D shape retrieval methods focus on capturing strong discriminative shape representation with softmax loss for the classification task, while the shape feature learning with metric loss is neglected for 3D shape retrieval. In this paper, we address this problem based on the intuition that the cosine distance of shape embeddings should be close enough within the same class and far away across categories. Since most of 3D shape retrieval tasks use cosine distance of shape features for measuring shape similarity, we propose a novel metric loss named angular triplet-center loss, which directly optimizes the cosine distances between the features. It inherits the triplet-center loss property to achieve larger inter-class distance and smaller intra-class distance simultaneously. Unlike previous metric loss utilized in 3D shape retrieval methods, where Euclidean distance is adopted and the margin design is difficult, the proposed method is more convenient to train feature embeddings and more suitable for 3D shape retrieval. Moreover, the angle margin is adopted to replace the cosine margin in order to provide more explicit discriminative constraints on an embedding space. Extensive experimental results on two popular 3D object retrieval benchmarks, ModelNet40 and ShapeNetCore 55, demonstrate the effectiveness of our proposed loss, and our method has achieved state-of-the-art results on various 3D shape datasets.



### A Deep Tree-Structured Fusion Model for Single Image Deraining
- **Arxiv ID**: http://arxiv.org/abs/1811.08632v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08632v1)
- **Published**: 2018-11-21 08:36:30+00:00
- **Updated**: 2018-11-21 08:36:30+00:00
- **Authors**: Xueyang Fu, Qi Qi, Yue Huang, Xinghao Ding, Feng Wu, John Paisley
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a simple yet effective deep tree-structured fusion model based on feature aggregation for the deraining problem. We argue that by effectively aggregating features, a relatively simple network can still handle tough image deraining problems well. First, to capture the spatial structure of rain we use dilated convolutions as our basic network block. We then design a tree-structured fusion architecture which is deployed within each block (spatial information) and across all blocks (content information). Our method is based on the assumption that adjacent features contain redundant information. This redundancy obstructs generation of new representations and can be reduced by hierarchically fusing adjacent features. Thus, the proposed model is more compact and can effectively use spatial and content information. Experiments on synthetic and real-world datasets show that our network achieves better deraining results with fewer parameters.



### Synetgy: Algorithm-hardware Co-design for ConvNet Accelerators on Embedded FPGAs
- **Arxiv ID**: http://arxiv.org/abs/1811.08634v4
- **DOI**: 10.1145/3289602.3293902
- **Categories**: **cs.CV**, cs.AR
- **Links**: [PDF](http://arxiv.org/pdf/1811.08634v4)
- **Published**: 2018-11-21 08:42:30+00:00
- **Updated**: 2020-05-11 01:45:12+00:00
- **Authors**: Yifan Yang, Qijing Huang, Bichen Wu, Tianjun Zhang, Liang Ma, Giulio Gambardella, Michaela Blott, Luciano Lavagno, Kees Vissers, John Wawrzynek, Kurt Keutzer
- **Comment**: Update to the latest results
- **Journal**: None
- **Summary**: Using FPGAs to accelerate ConvNets has attracted significant attention in recent years. However, FPGA accelerator design has not leveraged the latest progress of ConvNets. As a result, the key application characteristics such as frames-per-second (FPS) are ignored in favor of simply counting GOPs, and results on accuracy, which is critical to application success, are often not even reported. In this work, we adopt an algorithm-hardware co-design approach to develop a ConvNet accelerator called Synetgy and a novel ConvNet model called DiracDeltaNet$^{\dagger}$. Both the accelerator and ConvNet are tailored to FPGA requirements. DiracDeltaNet, as the name suggests, is a ConvNet with only $1\times 1$ convolutions while spatial convolutions are replaced by more efficient shift operations. DiracDeltaNet achieves competitive accuracy on ImageNet (88.7\% top-5), but with 42$\times$ fewer parameters and 48$\times$ fewer OPs than VGG16. We further quantize DiracDeltaNet's weights to 4-bit and activations to 4-bits, with less than 1\% accuracy loss. These quantizations exploit well the nature of FPGA hardware. In short, DiracDeltaNet's small model size, low computational OP count, low precision and simplified operators allow us to co-design a highly customized computing unit for an FPGA. We implement the computing units for DiracDeltaNet on an Ultra96 SoC system through high-level synthesis. Our accelerator's final top-5 accuracy of 88.1\% on ImageNet, is higher than all the previously reported embedded FPGA accelerators. In addition, the accelerator reaches an inference speed of 66.3 FPS on the ImageNet classification task, surpassing prior works with similar accuracy by at least 11.6$\times$.



### A Fingerprint Indexing Method Based on Minutia Descriptor and Clustering
- **Arxiv ID**: http://arxiv.org/abs/1811.08645v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08645v1)
- **Published**: 2018-11-21 09:19:54+00:00
- **Updated**: 2018-11-21 09:19:54+00:00
- **Authors**: Gwang-Il Ri, Chol-Gyun Ri, Su-Rim Ji
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we propose a novel fingerprint indexing approach for speeding up in the fingerprint recognition system. What kind of features are used for indexing and how to employ the extracted features for searching are crucial for the fingerprint indexing. In this paper, we select a minutia descriptor, which has been used to improve the accuracy of the fingerprint matching, as a local feature for indexing and construct a fixed-length feature vector which will be used for searching from the minutia descriptors of the fingerprint image using a clustering. And we propose a fingerprint searching approach that uses the Euclidean distance between two feature vectors as the similarity between two indexing features. Our indexing approach has several benefits. It reduces searching time significantly and is irrespective of the existence of singular points and robust even though the size of the fingerprint image is small or the quality is low. And the constructed indexing vector by this approach is independent of the features which are used for indexing based on the geometrical relations between the minutiae, like one based on the minutiae triplets. Thus, the proposed approach could be combined with other indexing approaches to gain a better indexing performance.



### PersEmoN: A Deep Network for Joint Analysis of Apparent Personality, Emotion and Their Relationship
- **Arxiv ID**: http://arxiv.org/abs/1811.08657v2
- **DOI**: 10.1109/TAFFC.2019.2951656
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08657v2)
- **Published**: 2018-11-21 10:01:46+00:00
- **Updated**: 2019-11-16 15:31:43+00:00
- **Authors**: Le Zhang, Songyou Peng, Stefan Winkler
- **Comment**: Accepted to IEEE Transactions on Affective Computing
- **Journal**: None
- **Summary**: Apparent personality and emotion analysis are both central to affective computing. Existing works solve them individually. In this paper we investigate if such high-level affect traits and their relationship can be jointly learned from face images in the wild. To this end, we introduce PersEmoN, an end-to-end trainable and deep Siamese-like network. It consists of two convolutional network branches, one for emotion and the other for apparent personality. Both networks share their bottom feature extraction module and are optimized within a multi-task learning framework. Emotion and personality networks are dedicated to their own annotated dataset. Furthermore, an adversarial-like loss function is employed to promote representation coherence among heterogeneous dataset sources. Based on this, we also explore the emotion-to-apparent-personality relationship. Extensive experiments demonstrate the effectiveness of PersEmoN.



### Retina U-Net: Embarrassingly Simple Exploitation of Segmentation Supervision for Medical Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1811.08661v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08661v1)
- **Published**: 2018-11-21 10:12:38+00:00
- **Updated**: 2018-11-21 10:12:38+00:00
- **Authors**: Paul F. Jaeger, Simon A. A. Kohl, Sebastian Bickelhaupt, Fabian Isensee, Tristan Anselm Kuder, Heinz-Peter Schlemmer, Klaus H. Maier-Hein
- **Comment**: None
- **Journal**: Neruips ML4H Workshop 2019 PLMR
- **Summary**: The task of localizing and categorizing objects in medical images often remains formulated as a semantic segmentation problem. This approach, however, only indirectly solves the coarse localization task by predicting pixel-level scores, requiring ad-hoc heuristics when mapping back to object-level scores. State-of-the-art object detectors on the other hand, allow for individual object scoring in an end-to-end fashion, while ironically trading in the ability to exploit the full pixel-wise supervision signal. This can be particularly disadvantageous in the setting of medical image analysis, where data sets are notoriously small. In this paper, we propose Retina U-Net, a simple architecture, which naturally fuses the Retina Net one-stage detector with the U-Net architecture widely used for semantic segmentation in medical images. The proposed architecture recaptures discarded supervision signals by complementing object detection with an auxiliary task in the form of semantic segmentation without introducing the additional complexity of previously proposed two-stage detectors. We evaluate the importance of full segmentation supervision on two medical data sets, provide an in-depth analysis on a series of toy experiments and show how the corresponding performance gain grows in the limit of small data sets. Retina U-Net yields strong detection performance only reached by its more complex two-staged counterparts. Our framework including all methods implemented for operation on 2D and 3D images is available at github.com/pfjaeger/medicaldetectiontoolkit.



### Computational Decomposition of Style for Controllable and Enhanced Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/1811.08668v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08668v2)
- **Published**: 2018-11-21 10:23:36+00:00
- **Updated**: 2018-11-24 09:34:48+00:00
- **Authors**: Minchao Li, Shikui Tu, Lei Xu
- **Comment**: 9 pages for main body and 11 pages for appendix. One picture is
  replaces compared to the last version. Some typos are corrected
- **Journal**: None
- **Summary**: Neural style transfer has been demonstrated to be powerful in creating artistic image with help of Convolutional Neural Networks (CNN). However, there is still lack of computational analysis of perceptual components of the artistic style. Different from some early attempts which studied the style by some pre-processing or post-processing techniques, we investigate the characteristics of the style systematically based on feature map produced by CNN. First, we computationally decompose the style into basic elements using not only spectrum based methods including Fast Fourier Transform (FFT), Discrete Cosine Transform (DCT) but also latent variable models such Principal Component Analysis (PCA), Independent Component Analysis (ICA). Then, the decomposition of style induces various ways of controlling the style elements which could be embedded as modules in state-of-the-art style transfer algorithms. Such decomposition of style brings several advantages. It enables the computational coding of different artistic styles by our style basis with similar styles clustering together, and thus it facilitates the mixing or intervention of styles based on the style basis from more than one styles so that compound style or new style could be generated to produce styled images. Experiments demonstrate the effectiveness of our method on not only painting style transfer but also sketch style transfer which indicates possible applications on picture-to-sketch problems.



### Graph Refinement based Airway Extraction using Mean-Field Networks and Graph Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1811.08674v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.08674v2)
- **Published**: 2018-11-21 10:50:31+00:00
- **Updated**: 2020-06-02 16:14:58+00:00
- **Authors**: Raghavendra Selvan, Thomas Kipf, Max Welling, Antonio Garcia-Uceda Juarez, Jesper H Pedersen, Jens Petersen, Marleen de Bruijne
- **Comment**: Accepted for publication at Medical Image Analysis. 14 pages
- **Journal**: None
- **Summary**: Graph refinement, or the task of obtaining subgraphs of interest from over-complete graphs, can have many varied applications. In this work, we extract trees or collection of sub-trees from image data by, first deriving a graph-based representation of the volumetric data and then, posing the tree extraction as a graph refinement task. We present two methods to perform graph refinement. First, we use mean-field approximation (MFA) to approximate the posterior density over the subgraphs from which the optimal subgraph of interest can be estimated. Mean field networks (MFNs) are used for inference based on the interpretation that iterations of MFA can be seen as feed-forward operations in a neural network. This allows us to learn the model parameters using gradient descent. Second, we present a supervised learning approach using graph neural networks (GNNs) which can be seen as generalisations of MFNs. Subgraphs are obtained by training a GNN-based graph refinement model to directly predict edge probabilities. We discuss connections between the two classes of methods and compare them for the task of extracting airways from 3D, low-dose, chest CT data. We show that both the MFN and GNN models show significant improvement when compared to one baseline method, that is similar to a top performing method in the EXACT'09 Challenge, and a 3D U-Net based airway segmentation model, in detecting more branches with fewer false positives.



### AttentionMask: Attentive, Efficient Object Proposal Generation Focusing on Small Objects
- **Arxiv ID**: http://arxiv.org/abs/1811.08728v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08728v1)
- **Published**: 2018-11-21 13:43:43+00:00
- **Updated**: 2018-11-21 13:43:43+00:00
- **Authors**: Christian Wilms, Simone Frintrop
- **Comment**: Accepted at ACCV 2018. Code is available at
  https://github.com/chwilms/AttentionMask
- **Journal**: None
- **Summary**: We propose a novel approach for class-agnostic object proposal generation, which is efficient and especially well-suited to detect small objects. Efficiency is achieved by scale-specific objectness attention maps which focus the processing on promising parts of the image and reduce the amount of sampled windows strongly. This leads to a system, which is $33\%$ faster than the state-of-the-art and clearly outperforming state-of-the-art in terms of average recall. Secondly, we add a module for detecting small objects, which are often missed by recent models. We show that this module improves the average recall for small objects by about $53\%$.



### SpotTune: Transfer Learning through Adaptive Fine-tuning
- **Arxiv ID**: http://arxiv.org/abs/1811.08737v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.08737v1)
- **Published**: 2018-11-21 14:02:03+00:00
- **Updated**: 2018-11-21 14:02:03+00:00
- **Authors**: Yunhui Guo, Honghui Shi, Abhishek Kumar, Kristen Grauman, Tajana Rosing, Rogerio Feris
- **Comment**: None
- **Journal**: None
- **Summary**: Transfer learning, which allows a source task to affect the inductive bias of the target task, is widely used in computer vision. The typical way of conducting transfer learning with deep neural networks is to fine-tune a model pre-trained on the source task using data from the target task. In this paper, we propose an adaptive fine-tuning approach, called SpotTune, which finds the optimal fine-tuning strategy per instance for the target data. In SpotTune, given an image from the target task, a policy network is used to make routing decisions on whether to pass the image through the fine-tuned layers or the pre-trained layers. We conduct extensive experiments to demonstrate the effectiveness of the proposed approach. Our method outperforms the traditional fine-tuning approach on 12 out of 14 standard datasets.We also compare SpotTune with other state-of-the-art fine-tuning strategies, showing superior performance. On the Visual Decathlon datasets, our method achieves the highest score across the board without bells and whistles.



### Semantic Stereo for Incidental Satellite Images
- **Arxiv ID**: http://arxiv.org/abs/1811.08739v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08739v1)
- **Published**: 2018-11-21 14:05:06+00:00
- **Updated**: 2018-11-21 14:05:06+00:00
- **Authors**: Marc Bosch, Kevin Foster, Gordon Christie, Sean Wang, Gregory D Hager, Myron Brown
- **Comment**: Accepted publication at WACV 2019
- **Journal**: None
- **Summary**: The increasingly common use of incidental satellite images for stereo reconstruction versus rigidly tasked binocular or trinocular coincident collection is helping to enable timely global-scale 3D mapping; however, reliable stereo correspondence from multi-date image pairs remains very challenging due to seasonal appearance differences and scene change. Promising recent work suggests that semantic scene segmentation can provide a robust regularizing prior for resolving ambiguities in stereo correspondence and reconstruction problems. To enable research for pairwise semantic stereo and multi-view semantic 3D reconstruction with incidental satellite images, we have established a large-scale public dataset including multi-view, multi-band satellite images and ground truth geometric and semantic labels for two large cities. To demonstrate the complementary nature of the stereo and segmentation tasks, we present lightweight public baselines adapted from recent state of the art convolutional neural network models and assess their performance.



### Gated Context Aggregation Network for Image Dehazing and Deraining
- **Arxiv ID**: http://arxiv.org/abs/1811.08747v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08747v2)
- **Published**: 2018-11-21 14:22:51+00:00
- **Updated**: 2018-12-15 13:39:41+00:00
- **Authors**: Dongdong Chen, Mingming He, Qingnan Fan, Jing Liao, Liheng Zhang, Dongdong Hou, Lu Yuan, Gang Hua
- **Comment**: Accepted by WACV 2019, Code released at
  "https://github.com/cddlyf/GCANet"
- **Journal**: None
- **Summary**: Image dehazing aims to recover the uncorrupted content from a hazy image. Instead of leveraging traditional low-level or handcrafted image priors as the restoration constraints, e.g., dark channels and increased contrast, we propose an end-to-end gated context aggregation network to directly restore the final haze-free image. In this network, we adopt the latest smoothed dilation technique to help remove the gridding artifacts caused by the widely-used dilated convolution with negligible extra parameters, and leverage a gated sub-network to fuse the features from different levels. Extensive experiments demonstrate that our method can surpass previous state-of-the-art methods by a large margin both quantitatively and qualitatively. In addition, to demonstrate the generality of the proposed method, we further apply it to the image deraining task, which also achieves the state-of-the-art performance. Code has been made available at https://github.com/cddlyf/GCANet.



### Chan-Vese Reformulation for Selective Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1811.08751v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.NA, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/1811.08751v2)
- **Published**: 2018-11-21 14:29:14+00:00
- **Updated**: 2019-07-05 18:44:26+00:00
- **Authors**: Michael Roberts, Jack Spencer
- **Comment**: To appear in the Journal of Mathematical Imaging and Vision 2019. (23
  pages, 19 figures)
- **Journal**: None
- **Summary**: Selective segmentation involves incorporating user input to partition an image into foreground and background, by discriminating between objects of a similar type. Typically, such methods involve introducing additional constraints to generic segmentation approaches. However, we show that this is often inconsistent with respect to common assumptions about the image. The proposed method introduces a new fitting term that is more useful in practice than the Chan-Vese framework. In particular, the idea is to define a term that allows for the background to consist of multiple regions of inhomogeneity. We provide comparitive experimental results to alternative approaches to demonstrate the advantages of the proposed method, broadening the possible application of these methods.



### Dynamic-Net: Tuning the Objective Without Re-training for Synthesis Tasks
- **Arxiv ID**: http://arxiv.org/abs/1811.08760v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08760v2)
- **Published**: 2018-11-21 14:49:34+00:00
- **Updated**: 2019-08-25 11:28:38+00:00
- **Authors**: Alon Shoshan, Roey Mechrez, Lihi Zelnik-Manor
- **Comment**: version update
- **Journal**: None
- **Summary**: One of the key ingredients for successful optimization of modern CNNs is identifying a suitable objective. To date, the objective is fixed a-priori at training time, and any variation to it requires re-training a new network. In this paper we present a first attempt at alleviating the need for re-training. Rather than fixing the network at training time, we train a "Dynamic-Net" that can be modified at inference time. Our approach considers an "objective-space" as the space of all linear combinations of two objectives, and the Dynamic-Net is emulating the traversing of this objective-space at test-time, without any further training. We show that this upgrades pre-trained networks by providing an out-of-learning extension, while maintaining the performance quality. The solution we propose is fast and allows a user to interactively modify the network, in real-time, in order to obtain the result he/she desires. We show the benefits of such an approach via several different applications.



### Learning Motion in Feature Space: Locally-Consistent Deformable Convolution Networks for Fine-Grained Action Detection
- **Arxiv ID**: http://arxiv.org/abs/1811.08815v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08815v5)
- **Published**: 2018-11-21 16:34:53+00:00
- **Updated**: 2019-11-06 21:37:45+00:00
- **Authors**: Khoi-Nguyen C. Mac, Dhiraj Joshi, Raymond A. Yeh, Jinjun Xiong, Rogerio S. Feris, Minh N. Do
- **Comment**: Accepted at ICCV 2019 as oral
- **Journal**: None
- **Summary**: Fine-grained action detection is an important task with numerous applications in robotics and human-computer interaction. Existing methods typically utilize a two-stage approach including extraction of local spatio-temporal features followed by temporal modeling to capture long-term dependencies. While most recent papers have focused on the latter (long-temporal modeling), here, we focus on producing features capable of modeling fine-grained motion more efficiently. We propose a novel locally-consistent deformable convolution, which utilizes the change in receptive fields and enforces a local coherency constraint to capture motion information effectively. Our model jointly learns spatio-temporal features (instead of using independent spatial and temporal streams). The temporal component is learned from the feature space instead of pixel space, e.g. optical flow. The produced features can be flexibly used in conjunction with other long-temporal modeling networks, e.g. ST-CNN, DilatedTCN, and ED-TCN. Overall, our proposed approach robustly outperforms the original long-temporal models on two fine-grained action datasets: 50 Salads and GTEA, achieving F1 scores of 80.22% and 75.39% respectively.



### Trajectory PHD and CPHD filters
- **Arxiv ID**: http://arxiv.org/abs/1811.08820v3
- **DOI**: 10.1109/TSP.2019.2943234
- **Categories**: **cs.CV**, stat.CO
- **Links**: [PDF](http://arxiv.org/pdf/1811.08820v3)
- **Published**: 2018-11-21 16:48:18+00:00
- **Updated**: 2019-10-25 12:43:38+00:00
- **Authors**: Ángel F. García-Fernández, Lennart Svensson
- **Comment**: MATLAB implementations are provided here:
  https://github.com/Agarciafernandez/MTT
- **Journal**: In IEEE Transactions on Signal Processing, vol. 67, no. 22, pp.
  5702-5714, Nov. 2019
- **Summary**: This paper presents the probability hypothesis density filter (PHD) and the cardinality PHD (CPHD) filter for sets of trajectories, which are referred to as the trajectory PHD (TPHD) and trajectory CPHD (TCPHD) filters. Contrary to the PHD/CPHD filters, the TPHD/TCPHD filters are able to produce trajectory estimates from first principles. The TPHD filter is derived by recursively obtaining the best Poisson multitrajectory density approximation to the posterior density over the alive trajectories by minimising the Kullback-Leibler divergence. The TCPHD is derived in the same way but propagating an independent identically distributed (IID) cluster multitrajectory density approximation. We also propose the Gaussian mixture implementations of the TPHD and TCPHD recursions, the Gaussian mixture TPHD (GMTPHD) and the Gaussian mixture TCPHD (GMTCPHD), and the L-scan computationally efficient implementations, which only update the density of the trajectory states of the last L time steps.



### Early Fusion for Goal Directed Robotic Vision
- **Arxiv ID**: http://arxiv.org/abs/1811.08824v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1811.08824v3)
- **Published**: 2018-11-21 16:55:17+00:00
- **Updated**: 2019-08-07 18:16:59+00:00
- **Authors**: Aaron Walsman, Yonatan Bisk, Saadia Gabriel, Dipendra Misra, Yoav Artzi, Yejin Choi, Dieter Fox
- **Comment**: None
- **Journal**: None
- **Summary**: Building perceptual systems for robotics which perform well under tight computational budgets requires novel architectures which rethink the traditional computer vision pipeline. Modern vision architectures require the agent to build a summary representation of the entire scene, even if most of the input is irrelevant to the agent's current goal. In this work, we flip this paradigm, by introducing EarlyFusion vision models that condition on a goal to build custom representations for downstream tasks. We show that these goal specific representations can be learned more quickly, are substantially more parameter efficient, and more robust than existing attention mechanisms in our domain. We demonstrate the effectiveness of these methods on a simulated robotic item retrieval problem that is trained in a fully end-to-end manner via imitation learning.



### Boosting in Image Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/1811.08429v1
- **DOI**: 10.1109/MMSP.2016.7813335
- **Categories**: **eess.IV**, cs.CV, cs.MM, eess.SP, I.4
- **Links**: [PDF](http://arxiv.org/pdf/1811.08429v1)
- **Published**: 2018-11-21 17:16:16+00:00
- **Updated**: 2018-11-21 17:16:16+00:00
- **Authors**: Dogancan Temel, Ghassan AlRegib
- **Comment**: Paper: 6 pages, 5 tables, 1 figure, Presentation: 16 slides
  [Ancillary files]
- **Journal**: D. Temel and G. AlRegib, "Boosting in image quality assessment,"
  2016 IEEE 18th International Workshop on Multimedia Signal Processing (MMSP),
  Montreal, QC, 2016, pp. 1-6
- **Summary**: In this paper, we analyze the effect of boosting in image quality assessment through multi-method fusion. Existing multi-method studies focus on proposing a single quality estimator. On the contrary, we investigate the generalizability of multi-method fusion as a framework. In addition to support vector machines that are commonly used in the multi-method fusion, we propose using neural networks in the boosting. To span different types of image quality assessment algorithms, we use quality estimators based on fidelity, perceptually-extended fidelity, structural similarity, spectral similarity, color, and learning. In the experiments, we perform k-fold cross validation using the LIVE, the multiply distorted LIVE, and the TID 2013 databases and the performance of image quality assessment algorithms are measured via accuracy-, linearity-, and ranking-based metrics. Based on the experiments, we show that boosting methods generally improve the performance of image quality assessment and the level of improvement depends on the type of the boosting algorithm. Our experimental results also indicate that boosting the worst performing quality estimator with two or more additional methods leads to statistically significant performance enhancements independent of the boosting technique and neural network-based boosting outperforms support vector machine-based boosting when two or more methods are fused.



### Recognizing Disguised Faces in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1811.08837v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08837v1)
- **Published**: 2018-11-21 17:28:35+00:00
- **Updated**: 2018-11-21 17:28:35+00:00
- **Authors**: Maneet Singh, Richa Singh, Mayank Vatsa, Nalini Ratha, Rama Chellappa
- **Comment**: None
- **Journal**: None
- **Summary**: Research in face recognition has seen tremendous growth over the past couple of decades. Beginning from algorithms capable of performing recognition in constrained environments, the current face recognition systems achieve very high accuracies on large-scale unconstrained face datasets. While upcoming algorithms continue to achieve improved performance, a majority of the face recognition systems are susceptible to failure under disguise variations, one of the most challenging covariate of face recognition. Most of the existing disguise datasets contain images with limited variations, often captured in controlled settings. This does not simulate a real world scenario, where both intentional and unintentional unconstrained disguises are encountered by a face recognition system. In this paper, a novel Disguised Faces in the Wild (DFW) dataset is proposed which contains over 11000 images of 1000 identities with different types of disguise accessories. The dataset is collected from the Internet, resulting in unconstrained face images similar to real world settings. This is the first-of-a-kind dataset with the availability of impersonator and genuine obfuscated face images for each subject. The proposed dataset has been analyzed in terms of three levels of difficulty: (i) easy, (ii) medium, and (iii) hard in order to showcase the challenging nature of the problem. It is our view that the research community can greatly benefit from the DFW dataset in terms of developing algorithms robust to such adversaries. The proposed dataset was released as part of the First International Workshop and Competition on Disguised Faces in the Wild at CVPR, 2018. This paper presents the DFW dataset in detail, including the evaluation protocols, baseline results, performance analysis of the submissions received as part of the competition, and three levels of difficulties of the DFW challenge dataset.



### fastMRI: An Open Dataset and Benchmarks for Accelerated MRI
- **Arxiv ID**: http://arxiv.org/abs/1811.08839v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.SP, physics.med-ph, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.08839v2)
- **Published**: 2018-11-21 17:32:14+00:00
- **Updated**: 2019-12-11 10:31:39+00:00
- **Authors**: Jure Zbontar, Florian Knoll, Anuroop Sriram, Tullie Murrell, Zhengnan Huang, Matthew J. Muckley, Aaron Defazio, Ruben Stern, Patricia Johnson, Mary Bruno, Marc Parente, Krzysztof J. Geras, Joe Katsnelson, Hersh Chandarana, Zizhao Zhang, Michal Drozdzal, Adriana Romero, Michael Rabbat, Pascal Vincent, Nafissa Yakubova, James Pinkerton, Duo Wang, Erich Owens, C. Lawrence Zitnick, Michael P. Recht, Daniel K. Sodickson, Yvonne W. Lui
- **Comment**: 35 pages, 10 figures
- **Journal**: None
- **Summary**: Accelerating Magnetic Resonance Imaging (MRI) by taking fewer measurements has the potential to reduce medical costs, minimize stress to patients and make MRI possible in applications where it is currently prohibitively slow or expensive. We introduce the fastMRI dataset, a large-scale collection of both raw MR measurements and clinical MR images, that can be used for training and evaluation of machine-learning approaches to MR image reconstruction. By introducing standardized evaluation criteria and a freely-accessible dataset, our goal is to help the community make rapid advances in the state of the art for MR image reconstruction. We also provide a self-contained introduction to MRI for machine learning researchers with no medical imaging background.



### An Interpretable Model for Scene Graph Generation
- **Arxiv ID**: http://arxiv.org/abs/1811.09543v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.09543v1)
- **Published**: 2018-11-21 17:51:01+00:00
- **Updated**: 2018-11-21 17:51:01+00:00
- **Authors**: Ji Zhang, Kevin Shih, Andrew Tao, Bryan Catanzaro, Ahmed Elgammal
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1811.00662
- **Journal**: None
- **Summary**: We propose an efficient and interpretable scene graph generator. We consider three types of features: visual, spatial and semantic, and we use a late fusion strategy such that each feature's contribution can be explicitly investigated. We study the key factors about these features that have the most impact on the performance, and also visualize the learned visual features for relationships and investigate the efficacy of our model. We won the champion of the OpenImages Visual Relationship Detection Challenge on Kaggle, where we outperform the 2nd place by 5\% (20\% relatively). We believe an accurate scene graph generator is a fundamental stepping stone for higher-level vision-language tasks such as image captioning and visual QA, since it provides a semantic, structured comprehension of an image that is beyond pixels and objects.



### Rethinking ImageNet Pre-training
- **Arxiv ID**: http://arxiv.org/abs/1811.08883v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08883v1)
- **Published**: 2018-11-21 18:55:58+00:00
- **Updated**: 2018-11-21 18:55:58+00:00
- **Authors**: Kaiming He, Ross Girshick, Piotr Dollár
- **Comment**: Technical report
- **Journal**: None
- **Summary**: We report competitive results on object detection and instance segmentation on the COCO dataset using standard models trained from random initialization. The results are no worse than their ImageNet pre-training counterparts even when using the hyper-parameters of the baseline system (Mask R-CNN) that were optimized for fine-tuning pre-trained models, with the sole exception of increasing the number of training iterations so the randomly initialized models may converge. Training from random initialization is surprisingly robust; our results hold even when: (i) using only 10% of the training data, (ii) for deeper and wider models, and (iii) for multiple tasks and metrics. Experiments show that ImageNet pre-training speeds up convergence early in training, but does not necessarily provide regularization or improve final target task accuracy. To push the envelope we demonstrate 50.9 AP on COCO object detection without using any external data---a result on par with the top COCO 2017 competition results that used ImageNet pre-training. These observations challenge the conventional wisdom of ImageNet pre-training for dependent tasks and we expect these discoveries will encourage people to rethink the current de facto paradigm of `pre-training and fine-tuning' in computer vision.



### HAQ: Hardware-Aware Automated Quantization with Mixed Precision
- **Arxiv ID**: http://arxiv.org/abs/1811.08886v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08886v3)
- **Published**: 2018-11-21 18:58:14+00:00
- **Updated**: 2019-04-06 20:35:54+00:00
- **Authors**: Kuan Wang, Zhijian Liu, Yujun Lin, Ji Lin, Song Han
- **Comment**: CVPR 2019. The first three authors contributed equally to this work.
  Project page: https://hanlab.mit.edu/projects/haq/
- **Journal**: None
- **Summary**: Model quantization is a widely used technique to compress and accelerate deep neural network (DNN) inference. Emergent DNN hardware accelerators begin to support mixed precision (1-8 bits) to further improve the computation efficiency, which raises a great challenge to find the optimal bitwidth for each layer: it requires domain experts to explore the vast design space trading off among accuracy, latency, energy, and model size, which is both time-consuming and sub-optimal. Conventional quantization algorithm ignores the different hardware architectures and quantizes all the layers in a uniform way. In this paper, we introduce the Hardware-Aware Automated Quantization (HAQ) framework which leverages the reinforcement learning to automatically determine the quantization policy, and we take the hardware accelerator's feedback in the design loop. Rather than relying on proxy signals such as FLOPs and model size, we employ a hardware simulator to generate direct feedback signals (latency and energy) to the RL agent. Compared with conventional methods, our framework is fully automated and can specialize the quantization policy for different neural network architectures and hardware architectures. Our framework effectively reduced the latency by 1.4-1.95x and the energy consumption by 1.9x with negligible loss of accuracy compared with the fixed bitwidth (8 bits) quantization. Our framework reveals that the optimal policies on different hardware architectures (i.e., edge and cloud architectures) under different resource constraints (i.e., latency, energy and model size) are drastically different. We interpreted the implication of different quantization policies, which offer insights for both neural network architecture design and hardware architecture design.



### A Comparative Study of Quality and Content-Based Spatial Pooling Strategies in Image Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/1811.08891v1
- **DOI**: 10.1109/GlobalSIP.2015.7418293
- **Categories**: **eess.IV**, cs.CV, cs.MM, eess.SP, I.4
- **Links**: [PDF](http://arxiv.org/pdf/1811.08891v1)
- **Published**: 2018-11-21 19:36:01+00:00
- **Updated**: 2018-11-21 19:36:01+00:00
- **Authors**: Dogancan Temel, Ghassan AlRegib
- **Comment**: Paper: 5 pages, 8 figures, Presentation: 21 slides [Ancillary files]
- **Journal**: 2015 IEEE GlobalSIP, Orlando, FL, 2015, pp. 732-736
- **Summary**: The process of quantifying image quality consists of engineering the quality features and pooling these features to obtain a value or a map. There has been a significant research interest in designing the quality features but pooling is usually overlooked compared to feature design. In this work, we compare the state of the art quality and content-based spatial pooling strategies and show that although features are the key in any image quality assessment, pooling also matters. We also propose a quality-based spatial pooling strategy that is based on linearly weighted percentile pooling (WPP). Pooling strategies are analyzed for squared error, SSIM and PerSIM in LIVE, multiply distorted LIVE and TID2013 image databases.



### MAC: Mining Activity Concepts for Language-based Temporal Localization
- **Arxiv ID**: http://arxiv.org/abs/1811.08925v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08925v1)
- **Published**: 2018-11-21 19:53:03+00:00
- **Updated**: 2018-11-21 19:53:03+00:00
- **Authors**: Runzhou Ge, Jiyang Gao, Kan Chen, Ram Nevatia
- **Comment**: WACV 2019
- **Journal**: None
- **Summary**: We address the problem of language-based temporal localization in untrimmed videos. Compared to temporal localization with fixed categories, this problem is more challenging as the language-based queries not only have no pre-defined activity list but also may contain complex descriptions. Previous methods address the problem by considering features from video sliding windows and language queries and learning a subspace to encode their correlation, which ignore rich semantic cues about activities in videos and queries. We propose to mine activity concepts from both video and language modalities by applying the actionness score enhanced Activity Concepts based Localizer (ACL). Specifically, the novel ACL encodes the semantic concepts from verb-obj pairs in language queries and leverages activity classifiers' prediction scores to encode visual concepts. Besides, ACL also has the capability to regress sliding windows as localization results. Experiments show that ACL significantly outperforms state-of-the-arts under the widely used metric, with more than 5% increase on both Charades-STA and TACoS datasets.



### Learning from Multiview Correlations in Open-Domain Videos
- **Arxiv ID**: http://arxiv.org/abs/1811.08890v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.08890v2)
- **Published**: 2018-11-21 19:57:11+00:00
- **Updated**: 2019-03-01 18:21:28+00:00
- **Authors**: Nils Holzenberger, Shruti Palaskar, Pranava Madhyastha, Florian Metze, Raman Arora
- **Comment**: None
- **Journal**: None
- **Summary**: An increasing number of datasets contain multiple views, such as video, sound and automatic captions. A basic challenge in representation learning is how to leverage multiple views to learn better representations. This is further complicated by the existence of a latent alignment between views, such as between speech and its transcription, and by the multitude of choices for the learning objective. We explore an advanced, correlation-based representation learning method on a 4-way parallel, multimodal dataset, and assess the quality of the learned representations on retrieval-based tasks. We show that the proposed approach produces rich representations that capture most of the information shared across views. Our best models for speech and textual modalities achieve retrieval rates from 70.7% to 96.9% on open-domain, user-generated instructional videos. This shows it is possible to learn reliable representations across disparate, unaligned and noisy modalities, and encourages using the proposed approach on larger datasets.



### Generating Adaptive and Robust Filter Sets Using an Unsupervised Learning Framework
- **Arxiv ID**: http://arxiv.org/abs/1811.08927v1
- **DOI**: 10.1109/ICIP.2017.8296841
- **Categories**: **eess.IV**, cs.CV, cs.MM, eess.SP, I.4
- **Links**: [PDF](http://arxiv.org/pdf/1811.08927v1)
- **Published**: 2018-11-21 20:02:33+00:00
- **Updated**: 2018-11-21 20:02:33+00:00
- **Authors**: Mohit Prabhushankar, Dogancan Temel, Ghassan AlRegib
- **Comment**: Paper:5 pages, 5 figures, 3 tables and Poster [Ancillary files]
- **Journal**: 2017 IEEE International Conference on Image Processing (ICIP),
  Beijing, 2017, pp. 3041-3045
- **Summary**: In this paper, we introduce an adaptive unsupervised learning framework, which utilizes natural images to train filter sets. The applicability of these filter sets is demonstrated by evaluating their performance in two contrasting applications - image quality assessment and texture retrieval. While assessing image quality, the filters need to capture perceptual differences based on dissimilarities between a reference image and its distorted version. In texture retrieval, the filters need to assess similarity between texture images to retrieve closest matching textures. Based on experiments, we show that the filter responses span a set in which a monotonicity-based metric can measure both the perceptual dissimilarity of natural images and the similarity of texture images. In addition, we corrupt the images in the test set and demonstrate that the proposed method leads to robust and reliable retrieval performance compared to existing methods.



### Pneumonia Detection in Chest Radiographs
- **Arxiv ID**: http://arxiv.org/abs/1811.08939v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08939v1)
- **Published**: 2018-11-21 20:33:40+00:00
- **Updated**: 2018-11-21 20:33:40+00:00
- **Authors**: The DeepRadiology Team
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we describe our approach to pneumonia classification and localization in chest radiographs. This method uses only \emph{open-source} deep learning object detection and is based on CoupleNet, a fully convolutional network which incorporates global and local features for object detection. Our approach achieves robustness through critical modifications of the training process and a novel ensembling algorithm which merges bounding boxes from several models. We tested our detection algorithm tested on a dataset of 3000 chest radiographs as part of the 2018 RSNA Pneumonia Challenge; our solution was recognized as a winning entry in a contest which attracted more than 1400 participants worldwide.



### MS-UNIQUE: Multi-model and Sharpness-weighted Unsupervised Image Quality Estimation
- **Arxiv ID**: http://arxiv.org/abs/1811.08947v1
- **DOI**: 10.2352/ISSN.2470-1173.2017.12.IQSP-223
- **Categories**: **eess.IV**, cs.CV, cs.MM, eess.SP, I.4
- **Links**: [PDF](http://arxiv.org/pdf/1811.08947v1)
- **Published**: 2018-11-21 20:55:56+00:00
- **Updated**: 2018-11-21 20:55:56+00:00
- **Authors**: Mohit Prabhushankar, Dogancan Temel, Ghassan AlRegib
- **Comment**: Paper: 6 pages, 6 figures, 2 tables and Presentation: 21 slides
  [Ancillary files]
- **Journal**: The Electronic Imaging, IQSP XIV, Burlingame, California, USA,
  Jan. 29 Feb. 2, 2017
- **Summary**: In this paper, we train independent linear decoder models to estimate the perceived quality of images. More specifically, we calculate the responses of individual non-overlapping image patches to each of the decoders and scale these responses based on the sharpness characteristics of filter set. We use multiple linear decoders to capture different abstraction levels of the image patches. Training each model is carried out on 100,000 image patches from the ImageNet database in an unsupervised fashion. Color space selection and ZCA Whitening are performed over these patches to enhance the descriptiveness of the data. The proposed quality estimator is tested on the LIVE and the TID 2013 image quality assessment databases. Performance of the proposed method is compared against eleven other state of the art methods in terms of accuracy, consistency, linearity, and monotonic behavior. Based on experimental results, the proposed method is generally among the top performing quality estimators in all categories.



### Microscope 2.0: An Augmented Reality Microscope with Real-time Artificial Intelligence Integration
- **Arxiv ID**: http://arxiv.org/abs/1812.00825v2
- **DOI**: 10.1038/s41591-019-0539-7
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.00825v2)
- **Published**: 2018-11-21 21:02:50+00:00
- **Updated**: 2018-12-04 05:36:36+00:00
- **Authors**: Po-Hsuan Cameron Chen, Krishna Gadepalli, Robert MacDonald, Yun Liu, Kunal Nagpal, Timo Kohlberger, Jeffrey Dean, Greg S. Corrado, Jason D. Hipp, Martin C. Stumpe
- **Comment**: None
- **Journal**: Nature Medicine (2019)
- **Summary**: The brightfield microscope is instrumental in the visual examination of both biological and physical samples at sub-millimeter scales. One key clinical application has been in cancer histopathology, where the microscopic assessment of the tissue samples is used for the diagnosis and staging of cancer and thus guides clinical therapy. However, the interpretation of these samples is inherently subjective, resulting in significant diagnostic variability. Moreover, in many regions of the world, access to pathologists is severely limited due to lack of trained personnel. In this regard, Artificial Intelligence (AI) based tools promise to improve the access and quality of healthcare. However, despite significant advances in AI research, integration of these tools into real-world cancer diagnosis workflows remains challenging because of the costs of image digitization and difficulties in deploying AI solutions. Here we propose a cost-effective solution to the integration of AI: the Augmented Reality Microscope (ARM). The ARM overlays AI-based information onto the current view of the sample through the optical pathway in real-time, enabling seamless integration of AI into the regular microscopy workflow. We demonstrate the utility of ARM in the detection of lymph node metastases in breast cancer and the identification of prostate cancer with a latency that supports real-time workflows. We anticipate that ARM will remove barriers towards the use of AI in microscopic analysis and thus improve the accuracy and efficiency of cancer diagnosis. This approach is applicable to other microscopy tasks and AI algorithms in the life sciences and beyond.



### Low-Resolution Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1811.08965v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08965v2)
- **Published**: 2018-11-21 22:14:24+00:00
- **Updated**: 2019-04-12 23:55:22+00:00
- **Authors**: Zhiyi Cheng, Xiatian Zhu, Shaogang Gong
- **Comment**: Accepted by 14th Asian Conference on Computer Vision
- **Journal**: None
- **Summary**: Whilst recent face-recognition (FR) techniques have made significant progress on recognising constrained high-resolution web images, the same cannot be said on natively unconstrained low-resolution images at large scales. In this work, we examine systematically this under-studied FR problem, and introduce a novel Complement Super-Resolution and Identity (CSRI) joint deep learning method with a unified end-to-end network architecture. We further construct a new large-scale dataset TinyFace of native unconstrained low-resolution face images from selected public datasets, because none benchmark of this nature exists in the literature. With extensive experiments we show there is a significant gap between the reported FR performances on popular benchmarks and the results on TinyFace, and the advantages of the proposed CSRI over a variety of state-of-the-art FR and super-resolution deep models on solving this largely ignored FR scenario. The TinyFace dataset is released publicly at: https://qmul-tinyface.github.io/.



