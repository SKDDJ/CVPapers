# Arxiv Papers in cs.CV on 2018-11-18
### Learning Local RGB-to-CAD Correspondences for Object Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1811.07249v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1811.07249v4)
- **Published**: 2018-11-18 00:32:58+00:00
- **Updated**: 2019-07-31 14:12:22+00:00
- **Authors**: Georgios Georgakis, Srikrishna Karanam, Ziyan Wu, Jana Kosecka
- **Comment**: 10 pages, 6 figures, 4 tables, ICCV 2019
- **Journal**: None
- **Summary**: We consider the problem of 3D object pose estimation. While much recent work has focused on the RGB domain, the reliance on accurately annotated images limits their generalizability and scalability. On the other hand, the easily available CAD models of objects are rich sources of data, providing a large number of synthetically rendered images. In this paper, we solve this key problem of existing methods requiring expensive 3D pose annotations by proposing a new method that matches RGB images to CAD models for object pose estimation. Our key innovations compared to existing work include removing the need for either real-world textures for CAD models or explicit 3D pose annotations for RGB images. We achieve this through a series of objectives that learn how to select keypoints and enforce viewpoint and modality invariance across RGB images and CAD model renderings. We conduct extensive experiments to demonstrate that the proposed method can reliably estimate object pose in RGB images, as well as generalize to object instances not seen during training.



### Iris Presentation Attack Detection Based on Photometric Stereo Features
- **Arxiv ID**: http://arxiv.org/abs/1811.07252v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07252v1)
- **Published**: 2018-11-18 01:21:22+00:00
- **Updated**: 2018-11-18 01:21:22+00:00
- **Authors**: Adam Czajka, Zhaoyuan Fang, Kevin W. Bowyer
- **Comment**: Patent Pending. Paper accepted for WACV 2019, Hawaii, USA
- **Journal**: None
- **Summary**: We propose a new iris presentation attack detection method using three-dimensional features of an observed iris region estimated by photometric stereo. Our implementation uses a pair of iris images acquired by a common commercial iris sensor (LG 4000). No hardware modifications of any kind are required. Our approach should be applicable to any iris sensor that can illuminate the eye from two different directions. Each iris image in the pair is captured under near-infrared illumination at a different angle relative to the eye. Photometric stereo is used to estimate surface normal vectors in the non-occluded portions of the iris region. The variability of the normal vectors is used as the presentation attack detection score. This score is larger for a texture that is irregularly opaque and printed on a convex contact lens, and is smaller for an authentic iris texture. Thus the problem is formulated as binary classification into (a) an eye wearing textured contact lens and (b) the texture of an actual iris surface (possibly seen through a clear contact lens). Experiments were carried out on a database of approx. 2,900 iris image pairs acquired from approx. 100 subjects. Our method was able to correctly classify over 95% of samples when tested on contact lens brands unseen in training, and over 98% of samples when the contact lens brand was seen during training. The source codes of the method are made available to other researchers.



### An Efficient Optical Flow Based Motion Detection Method for Non-stationary Scenes
- **Arxiv ID**: http://arxiv.org/abs/1811.08290v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08290v2)
- **Published**: 2018-11-18 01:57:44+00:00
- **Updated**: 2018-11-21 01:27:27+00:00
- **Authors**: Junjie Huang, Wei Zou, Zheng Zhu, Jiagang Zhu
- **Comment**: 6 pages. arXiv admin note: substantial text overlap with
  arXiv:1807.04890
- **Journal**: None
- **Summary**: Real-time motion detection in non-stationary scenes is a difficult task due to dynamic background, changing foreground appearance and limited computational resource. These challenges degrade the performance of the existing methods in practical applications. In this paper, an optical flow based framework is proposed to address this problem. By applying a novel strategy to utilize optical flow, we enable our method being free of model constructing, training or updating and can be performed efficiently. Besides, a dual judgment mechanism with adaptive intervals and adaptive thresholds is designed to heighten the system's adaptation to different situations. In experiment part, we quantitatively and qualitatively validate the effectiveness and feasibility of our method with videos in various scene conditions. The experimental results show that our method adapts itself to different situations and outperforms the state-of-the-art real-time methods, indicating the advantages of our optical flow based method.



### Optical Flow Based Online Moving Foreground Analysis
- **Arxiv ID**: http://arxiv.org/abs/1811.07256v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07256v1)
- **Published**: 2018-11-18 01:59:28+00:00
- **Updated**: 2018-11-18 01:59:28+00:00
- **Authors**: Junjie Huang, Wei Zou, Zheng Zhu, Jiagang Zhu
- **Comment**: 6pages
- **Journal**: None
- **Summary**: Obtained by moving object detection, the foreground mask result is unshaped and can not be directly used in most subsequent processes. In this paper, we focus on this problem and address it by constructing an optical flow based moving foreground analysis framework. During the processing procedure, the foreground masks are analyzed and segmented through two complementary clustering algorithms. As a result, we obtain the instance-level information like the number, location and size of moving objects. The experimental result show that our method adapts itself to the problem and performs well enough for practical applications.



### Exploit the Connectivity: Multi-Object Tracking with TrackletNet
- **Arxiv ID**: http://arxiv.org/abs/1811.07258v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07258v1)
- **Published**: 2018-11-18 02:23:27+00:00
- **Updated**: 2018-11-18 02:23:27+00:00
- **Authors**: Gaoang Wang, Yizhou Wang, Haotian Zhang, Renshu Gu, Jenq-Neng Hwang
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-object tracking (MOT) is an important and practical task related to both surveillance systems and moving camera applications, such as autonomous driving and robotic vision. However, due to unreliable detection, occlusion and fast camera motion, tracked targets can be easily lost, which makes MOT very challenging. Most recent works treat tracking as a re-identification (Re-ID) task, but how to combine appearance and temporal features is still not well addressed. In this paper, we propose an innovative and effective tracking method called TrackletNet Tracker (TNT) that combines temporal and appearance information together as a unified framework. First, we define a graph model which treats each tracklet as a vertex. The tracklets are generated by appearance similarity with CNN features and intersection-over-union (IOU) with epipolar constraints to compensate camera movement between adjacent frames. Then, for every pair of two tracklets, the similarity is measured by our designed multi-scale TrackletNet. Afterwards, the tracklets are clustered into groups which represent individual object IDs. Our proposed TNT has the ability to handle most of the challenges in MOT, and achieve promising results on MOT16 and MOT17 benchmark datasets compared with other state-of-the-art methods.



### GLStyleNet: Higher Quality Style Transfer Combining Global and Local Pyramid Features
- **Arxiv ID**: http://arxiv.org/abs/1811.07260v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1811.07260v1)
- **Published**: 2018-11-18 02:39:45+00:00
- **Updated**: 2018-11-18 02:39:45+00:00
- **Authors**: Zhizhong Wang, Lei Zhao, Wei Xing, Dongming Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Recent studies using deep neural networks have shown remarkable success in style transfer especially for artistic and photo-realistic images. However, the approaches using global feature correlations fail to capture small, intricate textures and maintain correct texture scales of the artworks, and the approaches based on local patches are defective on global effect. In this paper, we present a novel feature pyramid fusion neural network, dubbed GLStyleNet, which sufficiently takes into consideration multi-scale and multi-level pyramid features by best aggregating layers across a VGG network, and performs style transfer hierarchically with multiple losses of different scales. Our proposed method retains high-frequency pixel information and low frequency construct information of images from two aspects: loss function constraint and feature fusion. Our approach is not only flexible to adjust the trade-off between content and style, but also controllable between global and local. Compared to state-of-the-art methods, our method can transfer not just large-scale, obvious style cues but also subtle, exquisite ones, and dramatically improves the quality of style transfer. We demonstrate the effectiveness of our approach on portrait style transfer, artistic style transfer, photo-realistic style transfer and Chinese ancient painting style transfer tasks. Experimental results indicate that our unified approach improves image style transfer quality over previous state-of-the-art methods, while also accelerating the whole process in a certain extent. Our code is available at https://github.com/EndyWon/GLStyleNet.



### DeepConsensus: using the consensus of features from multiple layers to attain robust image classification
- **Arxiv ID**: http://arxiv.org/abs/1811.07266v3
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.07266v3)
- **Published**: 2018-11-18 03:37:52+00:00
- **Updated**: 2018-12-02 17:39:57+00:00
- **Authors**: Yuchen Li, Safwan Hossain, Kiarash Jamali, Frank Rudzicz
- **Comment**: None
- **Journal**: None
- **Summary**: We consider a classifier whose test set is exposed to various perturbations that are not present in the training set. These test samples still contain enough features to map them to the same class as their unperturbed counterpart. Current architectures exhibit rapid degradation of accuracy when trained on standard datasets but then used to classify perturbed samples of that data. To address this, we present a novel architecture named DeepConsensus that significantly improves generalization to these test-time perturbations. Our key insight is that deep neural networks should directly consider summaries of low and high level features when making classifications. Existing convolutional neural networks can be augmented with DeepConsensus, leading to improved resistance against large and small perturbations on MNIST, EMNIST, FashionMNIST, CIFAR10 and SVHN datasets.



### Deep Learning with Inaccurate Training Data for Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/1811.07268v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07268v1)
- **Published**: 2018-11-18 04:01:33+00:00
- **Updated**: 2018-11-18 04:01:33+00:00
- **Authors**: Bolin Liu, Xiao Shu, Xiaolin Wu
- **Comment**: None
- **Journal**: None
- **Summary**: In many applications of deep learning, particularly those in image restoration, it is either very difficult, prohibitively expensive, or outright impossible to obtain paired training data precisely as in the real world. In such cases, one is forced to use synthesized paired data to train the deep convolutional neural network (DCNN). However, due to the unavoidable generalization error in statistical learning, the synthetically trained DCNN often performs poorly on real world data. To overcome this problem, we propose a new general training method that can compensate for, to a large extent, the generalization errors of synthetically trained DCNNs.



### CIFAR10 to Compare Visual Recognition Performance between Deep Neural Networks and Humans
- **Arxiv ID**: http://arxiv.org/abs/1811.07270v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07270v2)
- **Published**: 2018-11-18 04:21:37+00:00
- **Updated**: 2019-08-15 13:52:53+00:00
- **Authors**: Tien Ho-Phuoc
- **Comment**: paper, 10 pages
- **Journal**: None
- **Summary**: Visual object recognition plays an essential role in human daily life. This ability is so efficient that we can recognize a face or an object seemingly without effort, though they may vary in position, scale, pose, and illumination. In the field of computer vision, a large number of studies have been carried out to build a human-like object recognition system. Recently, deep neural networks have shown impressive progress in object classification performance, and have been reported to surpass humans. Yet there is still lack of thorough and fair comparison between humans and artificial recognition systems. While some studies consider artificially degraded images, human recognition performance on dataset widely used for deep neural networks has not been fully evaluated. The present paper carries out an extensive experiment to evaluate human classification accuracy on CIFAR10, a well-known dataset of natural images. This then allows for a fair comparison with the state-of-the-art deep neural networks. Our CIFAR10-based evaluations show very efficient object recognition of recent CNNs but, at the same time, prove that they are still far from human-level capability of generalization. Moreover, a detailed investigation using multiple levels of difficulty reveals that easy images for humans may not be easy for deep neural networks. Such images form a subset of CIFAR10 that can be employed to evaluate and improve future neural networks.



### RePr: Improved Training of Convolutional Filters
- **Arxiv ID**: http://arxiv.org/abs/1811.07275v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.07275v3)
- **Published**: 2018-11-18 05:15:27+00:00
- **Updated**: 2019-02-25 06:04:16+00:00
- **Authors**: Aaditya Prakash, James Storer, Dinei Florencio, Cha Zhang
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: A well-trained Convolutional Neural Network can easily be pruned without significant loss of performance. This is because of unnecessary overlap in the features captured by the network's filters. Innovations in network architecture such as skip/dense connections and Inception units have mitigated this problem to some extent, but these improvements come with increased computation and memory requirements at run-time. We attempt to address this problem from another angle - not by changing the network structure but by altering the training method. We show that by temporarily pruning and then restoring a subset of the model's filters, and repeating this process cyclically, overlap in the learned features is reduced, producing improved generalization. We show that the existing model-pruning criteria are not optimal for selecting filters to prune in this context and introduce inter-filter orthogonality as the ranking criteria to determine under-expressive filters. Our method is applicable both to vanilla convolutional networks and more complex modern architectures, and improves the performance across a variety of tasks, especially when applied to smaller networks.



### Image-to-GPS Verification Through A Bottom-Up Pattern Matching Network
- **Arxiv ID**: http://arxiv.org/abs/1811.07288v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07288v1)
- **Published**: 2018-11-18 06:59:36+00:00
- **Updated**: 2018-11-18 06:59:36+00:00
- **Authors**: Jiaxin Cheng, Yue Wu, Wael Abd-Almageed, Prem Natarajan
- **Comment**: None
- **Journal**: None
- **Summary**: The image-to-GPS verification problem asks whether a given image is taken at a claimed GPS location. In this paper, we treat it as an image verification problem -- whether a query image is taken at the same place as a reference image retrieved at the claimed GPS location. We make three major contributions: 1) we propose a novel custom bottom-up pattern matching (BUPM) deep neural network solution; 2) we demonstrate that the verification can be directly done by cross-checking a perspective-looking query image and a panorama reference image, and 3) we collect and clean a dataset of 30K pairs query and reference. Our experimental results show that the proposed BUPM solution outperforms the state-of-the-art solutions in terms of both verification and localization.



### GAN-QP: A Novel GAN Framework without Gradient Vanishing and Lipschitz Constraint
- **Arxiv ID**: http://arxiv.org/abs/1811.07296v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.07296v4)
- **Published**: 2018-11-18 08:36:03+00:00
- **Updated**: 2018-12-15 11:30:28+00:00
- **Authors**: Jianlin Su
- **Comment**: simplify some proofs; add reconstruction
- **Journal**: None
- **Summary**: We know SGAN may have a risk of gradient vanishing. A significant improvement is WGAN, with the help of 1-Lipschitz constraint on discriminator to prevent from gradient vanishing. Is there any GAN having no gradient vanishing and no 1-Lipschitz constraint on discriminator? We do find one, called GAN-QP.   To construct a new framework of Generative Adversarial Network (GAN) usually includes three steps: 1. choose a probability divergence; 2. convert it into a dual form; 3. play a min-max game. In this articles, we demonstrate that the first step is not necessary. We can analyse the property of divergence and even construct new divergence in dual space directly. As a reward, we obtain a simpler alternative of WGAN: GAN-QP. We demonstrate that GAN-QP have a better performance than WGAN in theory and practice.



### A Variational Dirichlet Framework for Out-of-Distribution Detection
- **Arxiv ID**: http://arxiv.org/abs/1811.07308v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.07308v4)
- **Published**: 2018-11-18 10:24:58+00:00
- **Updated**: 2019-04-20 22:53:10+00:00
- **Authors**: Wenhu Chen, Yilin Shen, Hongxia Jin, William Wang
- **Comment**: Tech Report
- **Journal**: None
- **Summary**: With the recently rapid development in deep learning, deep neural networks have been widely adopted in many real-life applications. However, deep neural networks are also known to have very little control over its uncertainty for unseen examples, which potentially causes very harmful and annoying consequences in practical scenarios. In this paper, we are particularly interested in designing a higher-order uncertainty metric for deep neural networks and investigate its effectiveness under the out-of-distribution detection task proposed by~\cite{hendrycks2016baseline}. Our method first assumes there exists an underlying higher-order distribution $\mathbb{P}(z)$, which controls label-wise categorical distribution $\mathbb{P}(y)$ over classes on the K-dimension simplex, and then approximate such higher-order distribution via parameterized posterior function $p_{\theta}(z|x)$ under variational inference framework, finally we use the entropy of learned posterior distribution $p_{\theta}(z|x)$ as uncertainty measure to detect out-of-distribution examples. Further, we propose an auxiliary objective function to discriminate against synthesized adversarial examples to further increase the robustness of the proposed uncertainty measure. Through comprehensive experiments on various datasets, our proposed framework is demonstrated to consistently outperform competing algorithms.



### Regularized adversarial examples for model interpretability
- **Arxiv ID**: http://arxiv.org/abs/1811.07311v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.07311v2)
- **Published**: 2018-11-18 10:40:16+00:00
- **Updated**: 2018-11-21 07:29:32+00:00
- **Authors**: Yoel Shoshan, Vadim Ratner
- **Comment**: None
- **Journal**: None
- **Summary**: As machine learning algorithms continue to improve, there is an increasing need for explaining why a model produces a certain prediction for a certain input. In recent years, several methods for model interpretability have been developed, aiming to provide explanation of which subset regions of the model input is the main reason for the model prediction. In parallel, a significant research community effort is occurring in recent years for developing adversarial example generation methods for fooling models, while not altering the true label of the input,as it would have been classified by a human annotator. In this paper, we bridge the gap between adversarial example generation and model interpretability, and introduce a modification to the adversarial example generation process which encourages better interpretability. We analyze the proposed method on a public medical imaging dataset, both quantitatively and qualitatively, and show that it significantly outperforms the leading known alternative method. Our suggested method is simple to implement, and can be easily plugged into most common adversarial example generation frameworks. Additionally, we propose an explanation quality metric - $APE$ - "Adversarial Perturbative Explanation", which measures how well an explanation describes model decisions.



### Learning to infer: RL-based search for DNN primitive selection on Heterogeneous Embedded Systems
- **Arxiv ID**: http://arxiv.org/abs/1811.07315v1
- **DOI**: 10.23919/DATE.2019.8714959
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07315v1)
- **Published**: 2018-11-18 11:28:24+00:00
- **Updated**: 2018-11-18 11:28:24+00:00
- **Authors**: Miguel de Prado, Nuria Pazos, Luca Benini
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Learning is increasingly being adopted by industry for computer vision applications running on embedded devices. While Convolutional Neural Networks' accuracy has achieved a mature and remarkable state, inference latency and throughput are a major concern especially when targeting low-cost and low-power embedded platforms. CNNs' inference latency may become a bottleneck for Deep Learning adoption by industry, as it is a crucial specification for many real-time processes. Furthermore, deployment of CNNs across heterogeneous platforms presents major compatibility issues due to vendor-specific technology and acceleration libraries. In this work, we present QS-DNN, a fully automatic search based on Reinforcement Learning which, combined with an inference engine optimizer, efficiently explores through the design space and empirically finds the optimal combinations of libraries and primitives to speed up the inference of CNNs on heterogeneous embedded devices. We show that, an optimized combination can achieve 45x speedup in inference latency on CPU compared to a dependency-free baseline and 2x on average on GPGPU compared to the best vendor library. Further, we demonstrate that, the quality of results and time "to-solution" is much better than with Random Search and achieves up to 15x better results for a short-time search.



### On Matching Faces with Alterations due to Plastic Surgery and Disguise
- **Arxiv ID**: http://arxiv.org/abs/1811.07318v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07318v1)
- **Published**: 2018-11-18 12:05:54+00:00
- **Updated**: 2018-11-18 12:05:54+00:00
- **Authors**: Saksham Suri, Anush Sankaran, Mayank Vatsa, Richa Singh
- **Comment**: The 9th IEEE International Conference on Biometrics: Theory,
  Applications, and Systems (BTAS 2018)
- **Journal**: None
- **Summary**: Plastic surgery and disguise variations are two of the most challenging co-variates of face recognition. The state-of-art deep learning models are not sufficiently successful due to the availability of limited training samples. In this paper, a novel framework is proposed which transfers fundamental visual features learnt from a generic image dataset to supplement a supervised face recognition model. The proposed algorithm combines off-the-shelf supervised classifier and a generic, task independent network which encodes information related to basic visual cues such as color, shape, and texture. Experiments are performed on IIITD plastic surgery face dataset and Disguised Faces in the Wild (DFW) dataset. Results showcase that the proposed algorithm achieves state of the art results on both the datasets. Specifically on the DFW database, the proposed algorithm yields over 87% verification accuracy at 1% false accept rate which is 53.8% better than baseline results computed using VGGFace.



### Distribution Discrepancy Maximization for Image Privacy Preserving
- **Arxiv ID**: http://arxiv.org/abs/1811.07335v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1811.07335v1)
- **Published**: 2018-11-18 14:53:49+00:00
- **Updated**: 2018-11-18 14:53:49+00:00
- **Authors**: Sen Liu, Jianxin Lin, Zhibo Chen
- **Comment**: 8 pages, 6 figures
- **Journal**: None
- **Summary**: With the rapid increase in online photo sharing activities, image obfuscation algorithms become particularly important for protecting the sensitive information in the shared photos. However, existing image obfuscation methods based on hand-crafted principles are challenged by the dramatic development of deep learning techniques. To address this problem, we propose to maximize the distribution discrepancy between the original image domain and the encrypted image domain. Accordingly, we introduce a collaborative training scheme: a discriminator $D$ is trained to discriminate the reconstructed image from the encrypted image, and an encryption model $G_e$ is required to generate these two kinds of images to maximize the recognition rate of $D$, leading to the same training objective for both $D$ and $G_e$. We theoretically prove that such a training scheme maximizes two distributions' discrepancy. Compared with commonly-used image obfuscation methods, our model can produce satisfactory defense against the attack of deep recognition models indicated by significant accuracy decreases on FaceScrub, Casia-WebFace and LFW datasets.



### Implementation of Robust Face Recognition System Using Live Video Feed Based on CNN
- **Arxiv ID**: http://arxiv.org/abs/1811.07339v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07339v1)
- **Published**: 2018-11-18 15:31:08+00:00
- **Updated**: 2018-11-18 15:31:08+00:00
- **Authors**: Yang Li, Sangwhan Cha
- **Comment**: None
- **Journal**: None
- **Summary**: The way to accurately and effectively identify people has always been an interesting topic in research and industry. With the rapid development of artificial intelligence in recent years, facial recognition gains lots of attention due to prompting the development of emerging identification methods. Compared to traditional card recognition, fingerprint recognition and iris recognition, face recognition has many advantages including non-contact interface, high concurrency, and user-friendly usage. It has high potential to be used in government, public facilities, security, e-commerce, retailing, education and many other fields. With the development of deep learning and the introduction of deep convolutional neural networks, the accuracy and speed of face recognition have made great strides. However, the results from different networks and models are very different with different system architecture. Furthermore, it could take significant amount of data storage space and data processing time for the face recognition system with video feed, if the system stores images and features of human faces. In this paper, facial features are extracted by merging and comparing multiple models, and then a deep neural network is constructed to train and construct the combined features. In this way, the advantages of multiple models can be combined to mention the recognition accuracy. After getting a model with high accuracy, we build a product model. The model will take a human face image and extract it into a vector. Then the distance between vectors are compared to determine if two faces on different picture belongs to the same person. The proposed approach reduces data storage space and data processing time for the face recognition system with video feed scientifically with our proposed system architecture.



### Transfer Learning with Deep CNNs for Gender Recognition and Age Estimation
- **Arxiv ID**: http://arxiv.org/abs/1811.07344v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07344v1)
- **Published**: 2018-11-18 16:11:52+00:00
- **Updated**: 2018-11-18 16:11:52+00:00
- **Authors**: Philip Smith, Cuixian Chen
- **Comment**: It has been accepted in the 5th National Symposium for NSF REU
  Research in Data Science, Systems, and Security
- **Journal**: None
- **Summary**: In this project, competition-winning deep neural networks with pretrained weights are used for image-based gender recognition and age estimation. Transfer learning is explored using both VGG19 and VGGFace pretrained models by testing the effects of changes in various design schemes and training parameters in order to improve prediction accuracy. Training techniques such as input standardization, data augmentation, and label distribution age encoding are compared. Finally, a hierarchy of deep CNNs is tested that first classifies subjects by gender, and then uses separate male and female age models to predict age. A gender recognition accuracy of 98.7% and an MAE of 4.1 years is achieved. This paper shows that, with proper training techniques, good results can be obtained by retasking existing convolutional filters towards a new purpose.



### Deep Learning based Pedestrian Detection at Distance in Smart Cities
- **Arxiv ID**: http://arxiv.org/abs/1812.00876v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.00876v4)
- **Published**: 2018-11-18 17:15:24+00:00
- **Updated**: 2019-05-16 17:17:59+00:00
- **Authors**: Ranjith K Dinakaran, Philip Easom, Ahmed Bouridane, Li Zhang, Richard Jiang, Fozia Mehboob, Abdul Rauf
- **Comment**: Artificial Intelligence Conference 2019 | IntelliSys 2019 |
  https://saiconference.com/IntelliSys
- **Journal**: None
- **Summary**: Generative adversarial networks (GANs) have been promising for many computer vision problems due to their powerful capabilities to enhance the data for training and test. In this paper, we leveraged GANs and proposed a new architecture with a cascaded Single Shot Detector (SSD) for pedestrian detection at distance, which is yet a challenge due to the varied sizes of pedestrians in videos at distance. To overcome the low-resolution issues in pedestrian detection at distance, DCGAN is employed to improve the resolution first to reconstruct more discriminative features for a SSD to detect objects in images or videos. A crucial advantage of our method is that it learns a multi-scale metric to distinguish multiple objects at different distances under one image, while DCGAN serves as an encoder-decoder platform to generate parts of an image that contain better discriminative information. To measure the effectiveness of our proposed method, experiments were carried out on the Canadian Institute for Advanced Research (CIFAR) dataset, and it was demonstrated that the proposed new architecture achieved a much better detection rate, particularly on vehicles and pedestrians at distance, making it highly suitable for smart cities applications that need to discover key objects or pedestrians at distance.



### RGB-based 3D Hand Pose Estimation via Privileged Learning with Depth Images
- **Arxiv ID**: http://arxiv.org/abs/1811.07376v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07376v1)
- **Published**: 2018-11-18 18:52:08+00:00
- **Updated**: 2018-11-18 18:52:08+00:00
- **Authors**: Shanxin Yuan, Bjorn Stenger, Tae-Kyun Kim
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a method for hand pose estimation from RGB images that uses both external large-scale depth image datasets and paired depth and RGB images as privileged information at training time. We show that providing depth information during training significantly improves performance of pose estimation from RGB images during testing. We explore different ways of using this privileged information: (1) using depth data to initially train a depth-based network, (2) using the features from the depth-based network of the paired depth images to constrain mid-level RGB network weights, and (3) using the foreground mask, obtained from the depth data, to suppress the responses from the background area. By using paired RGB and depth images, we are able to supervise the RGB-based network to learn middle layer features that mimic that of the corresponding depth-based network, which is trained on large-scale, accurately annotated depth data. During testing, when only an RGB image is available, our method produces accurate 3D hand pose predictions. Our method is also tested on 2D hand pose estimation. Experiments on three public datasets show that the method outperforms the state-of-the-art methods for hand pose estimation using RGB image input.



### Deep Siamese Networks with Bayesian non-Parametrics for Video Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/1811.07386v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07386v1)
- **Published**: 2018-11-18 19:32:48+00:00
- **Updated**: 2018-11-18 19:32:48+00:00
- **Authors**: Anthony D. Rhodes, Manan Goel
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel algorithm utilizing a deep Siamese neural network as a general object similarity function in combination with a Bayesian optimization (BO) framework to encode spatio-temporal information for efficient object tracking in video. In particular, we treat the video tracking problem as a dynamic (i.e. temporally-evolving) optimization problem. Using Gaussian Process priors, we model a dynamic objective function representing the location of a tracked object in each frame. By exploiting temporal correlations, the proposed method queries the search space in a statistically principled and efficient way, offering several benefits over current state of the art video tracking methods.



### Temporal Recurrent Networks for Online Action Detection
- **Arxiv ID**: http://arxiv.org/abs/1811.07391v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07391v2)
- **Published**: 2018-11-18 20:03:55+00:00
- **Updated**: 2019-03-23 18:58:29+00:00
- **Authors**: Mingze Xu, Mingfei Gao, Yi-Ting Chen, Larry S. Davis, David J. Crandall
- **Comment**: None
- **Journal**: None
- **Summary**: Most work on temporal action detection is formulated as an offline problem, in which the start and end times of actions are determined after the entire video is fully observed. However, important real-time applications including surveillance and driver assistance systems require identifying actions as soon as each video frame arrives, based only on current and historical observations. In this paper, we propose a novel framework, Temporal Recurrent Network (TRN), to model greater temporal context of a video frame by simultaneously performing online action detection and anticipation of the immediate future. At each moment in time, our approach makes use of both accumulated historical evidence and predicted future information to better recognize the action that is currently occurring, and integrates both of these into a unified end-to-end architecture. We evaluate our approach on two popular online action detection datasets, HDD and TVSeries, as well as another widely used dataset, THUMOS'14. The results show that TRN significantly outperforms the state-of-the-art.



### Facial Expression and Peripheral Physiology Fusion to Decode Individualized Affective Experience
- **Arxiv ID**: http://arxiv.org/abs/1811.07392v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07392v1)
- **Published**: 2018-11-18 20:10:47+00:00
- **Updated**: 2018-11-18 20:10:47+00:00
- **Authors**: Yu Yin, Mohsen Nabian, Miolin Fan, ChunAn Chou, Maria Gendron, Sarah Ostadabbas
- **Comment**: 2nd IJCAI Workshop on Artificial Intelligence in Affective Computing
- **Journal**: None
- **Summary**: In this paper, we present a multimodal approach to simultaneously analyze facial movements and several peripheral physiological signals to decode individualized affective experiences under positive and negative emotional contexts, while considering their personalized resting dynamics. We propose a person-specific recurrence network to quantify the dynamics present in the person's facial movements and physiological data. Facial movement is represented using a robust head vs. 3D face landmark localization and tracking approach, and physiological data are processed by extracting known attributes related to the underlying affective experience. The dynamical coupling between different input modalities is then assessed through the extraction of several complex recurrent network metrics. Inference models are then trained using these metrics as features to predict individual's affective experience in a given context, after their resting dynamics are excluded from their response. We validated our approach using a multimodal dataset consists of (i) facial videos and (ii) several peripheral physiological signals, synchronously recorded from 12 participants while watching 4 emotion-eliciting video-based stimuli. The affective experience prediction results signified that our multimodal fusion method improves the prediction accuracy up to 19% when compared to the prediction using only one or a subset of the input modalities. Furthermore, we gained prediction improvement for affective experience by considering the effect of individualized resting dynamics.



### Multimodal Densenet
- **Arxiv ID**: http://arxiv.org/abs/1811.07407v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.07407v1)
- **Published**: 2018-11-18 21:31:22+00:00
- **Updated**: 2018-11-18 21:31:22+00:00
- **Authors**: Faisal Mahmood, Ziyun Yang, Thomas Ashley, Nicholas J. Durr
- **Comment**: 10 pages, 6 figures
- **Journal**: None
- **Summary**: Humans make accurate decisions by interpreting complex data from multiple sources. Medical diagnostics, in particular, often hinge on human interpretation of multi-modal information. In order for artificial intelligence to make progress in automated, objective, and accurate diagnosis and prognosis, methods to fuse information from multiple medical imaging modalities are required. However, combining information from multiple data sources has several challenges, as current deep learning architectures lack the ability to extract useful representations from multimodal information, and often simple concatenation is used to fuse such information. In this work, we propose Multimodal DenseNet, a novel architecture for fusing multimodal data. Instead of focusing on concatenation or early and late fusion, our proposed architectures fuses information over several layers and gives the model flexibility in how it combines information from multiple sources. We apply this architecture to the challenge of polyp characterization and landmark identification in endoscopy. Features from white light images are fused with features from narrow band imaging or depth maps. This study demonstrates that Multimodal DenseNet outperforms monomodal classification as well as other multimodal fusion techniques by a significant margin on two different datasets.



### PerSIM: Multi-resolution Image Quality Assessment in the Perceptually Uniform Color Domain
- **Arxiv ID**: http://arxiv.org/abs/1811.07417v1
- **DOI**: 10.1109/ICIP.2015.7351087
- **Categories**: **eess.IV**, cs.CV, cs.MM, eess.SP, I.4
- **Links**: [PDF](http://arxiv.org/pdf/1811.07417v1)
- **Published**: 2018-11-18 22:42:32+00:00
- **Updated**: 2018-11-18 22:42:32+00:00
- **Authors**: Dogancan Temel, Ghassan AlRegib
- **Comment**: 5 pages, 1 figure, 3 tables
- **Journal**: 2015 IEEE International Conference on Image Processing (ICIP),
  Quebec City, QC, 2015, pp. 1682-1686
- **Summary**: An average observer perceives the world in color instead of black and white. Moreover, the visual system focuses on structures and segments instead of individual pixels. Based on these observations, we propose a full reference objective image quality metric modeling visual system characteristics and chroma similarity in the perceptually uniform color domain (Lab). Laplacian of Gaussian features are obtained in the L channel to model the retinal ganglion cells in human visual system and color similarity is calculated over the a and b channels. In the proposed perceptual similarity index (PerSIM), a multi-resolution approach is followed to mimic the hierarchical nature of human visual system. LIVE and TID2013 databases are used in the validation and PerSIM outperforms all the compared metrics in the overall databases in terms of ranking, monotonic behavior and linearity.



