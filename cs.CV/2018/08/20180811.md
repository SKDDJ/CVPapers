# Arxiv Papers in cs.CV on 2018-08-11
### Video Logo Retrieval based on local Features
- **Arxiv ID**: http://arxiv.org/abs/1808.03735v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1808.03735v4)
- **Published**: 2018-08-11 01:27:40+00:00
- **Updated**: 2020-05-18 21:55:34+00:00
- **Authors**: Bochen Guan, Hanrong Ye, Hong Liu, William A. Sethares
- **Comment**: Accepted by ICIP 20. Contact author: Bochen Guan (gbochen@wisc.edu)
- **Journal**: None
- **Summary**: Estimation of the frequency and duration of logos in videos is important and challenging in the advertisement industry as a way of estimating the impact of ad purchases. Since logos occupy only a small area in the videos, the popular methods of image retrieval could fail. This paper develops an algorithm called Video Logo Retrieval (VLR), which is an image-to-video retrieval algorithm based on the spatial distribution of local image descriptors that measure the distance between the query image (the logo) and a collection of video images. VLR uses local features to overcome the weakness of global feature-based models such as convolutional neural networks (CNN). Meanwhile, VLR is flexible and does not require training after setting some hyper-parameters. The performance of VLR is evaluated on two challenging open benchmark tasks (SoccerNet and Standford I2V), and compared with other state-of-the-art logo retrieval or detection algorithms. Overall, VLR shows significantly higher accuracy compared with the existing methods.



### Neural Network Encapsulation
- **Arxiv ID**: http://arxiv.org/abs/1808.03749v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1808.03749v1)
- **Published**: 2018-08-11 04:36:53+00:00
- **Updated**: 2018-08-11 04:36:53+00:00
- **Authors**: Hongyang Li, Xiaoyang Guo, Bo Dai, Wanli Ouyang, Xiaogang Wang
- **Comment**: ECCV 2018
- **Journal**: None
- **Summary**: A capsule is a collection of neurons which represents different variants of a pattern in the network. The routing scheme ensures only certain capsules which resemble lower counterparts in the higher layer should be activated. However, the computational complexity becomes a bottleneck for scaling up to larger networks, as lower capsules need to correspond to each and every higher capsule. To resolve this limitation, we approximate the routing process with two branches: a master branch which collects primary information from its direct contact in the lower layer and an aide branch that replenishes master based on pattern variants encoded in other lower capsules. Compared with previous iterative and unsupervised routing scheme, these two branches are communicated in a fast, supervised and one-time pass fashion. The complexity and runtime of the model are therefore decreased by a large margin. Motivated by the routing to make higher capsule have agreement with lower capsule, we extend the mechanism as a compensation for the rapid loss of information in nearby layers. We devise a feedback agreement unit to send back higher capsules as feedback. It could be regarded as an additional regularization to the network. The feedback agreement is achieved by comparing the optimal transport divergence between two distributions (lower and higher capsules). Such an add-on witnesses a unanimous gain in both capsule and vanilla networks. Our proposed EncapNet performs favorably better against previous state-of-the-arts on CIFAR10/100, SVHN and a subset of ImageNet.



### The ActivityNet Large-Scale Activity Recognition Challenge 2018 Summary
- **Arxiv ID**: http://arxiv.org/abs/1808.03766v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.03766v2)
- **Published**: 2018-08-11 07:55:51+00:00
- **Updated**: 2018-08-23 04:28:38+00:00
- **Authors**: Bernard Ghanem, Juan Carlos Niebles, Cees Snoek, Fabian Caba Heilbron, Humam Alwassel, Victor Escorcia, Ranjay Krishna, Shyamal Buch, Cuong Duc Dao
- **Comment**: CVPR Workshop 2018 challenge summary
- **Journal**: None
- **Summary**: The 3rd annual installment of the ActivityNet Large- Scale Activity Recognition Challenge, held as a full-day workshop in CVPR 2018, focused on the recognition of daily life, high-level, goal-oriented activities from user-generated videos as those found in internet video portals. The 2018 challenge hosted six diverse tasks which aimed to push the limits of semantic visual understanding of videos as well as bridge visual content with human captions. Three out of the six tasks were based on the ActivityNet dataset, which was introduced in CVPR 2015 and organized hierarchically in a semantic taxonomy. These tasks focused on tracing evidence of activities in time in the form of proposals, class labels, and captions. In this installment of the challenge, we hosted three guest tasks to enrich the understanding of visual information in videos. The guest tasks focused on complementary aspects of the activity recognition problem at large scale and involved three challenging and recently compiled datasets: the Kinetics-600 dataset from Google DeepMind, the AVA dataset from Berkeley and Google, and the Moments in Time dataset from MIT and IBM Research.



### Learning Discriminative 3D Shape Representations by View Discerning Networks
- **Arxiv ID**: http://arxiv.org/abs/1808.03823v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1808.03823v2)
- **Published**: 2018-08-11 16:09:45+00:00
- **Updated**: 2018-08-20 21:22:36+00:00
- **Authors**: Biao Leng, Cheng Zhang, Xiaocheng Zhou, Cheng Xu, Kai Xu
- **Comment**: Accepted by IEEE Transactions on Visualization and Computer Graphics.
  Corresponding Author: Kai Xu (kevin.kai.xu@gmail.com)
- **Journal**: IEEE Transactions on Visualization and Computer Graphics, 2018
- **Summary**: In view-based 3D shape recognition, extracting discriminative visual representation of 3D shapes from projected images is considered the core problem. Projections with low discriminative ability can adversely influence the final 3D shape representation. Especially under the real situations with background clutter and object occlusion, the adverse effect is even more severe. To resolve this problem, we propose a novel deep neural network, View Discerning Network, which learns to judge the quality of views and adjust their contributions to the representation of shapes. In this network, a Score Generation Unit is devised to evaluate the quality of each projected image with score vectors. These score vectors are used to weight the image features and the weighted features perform much better than original features in 3D shape recognition task. In particular, we introduce two structures of Score Generation Unit, Channel-wise Score Unit and Part-wise Score Unit, to assess the quality of feature maps from different perspectives. Our network aggregates features and scores in an end-to-end framework, so that final shape descriptors are directly obtained from its output. Our experiments on ModelNet and ShapeNet Core55 show that View Discerning Network outperforms the state-of-the-arts in terms of the retrieval task, with excellent robustness against background clutter and object occlusion.



### Self-Supervised Model Adaptation for Multimodal Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1808.03833v3
- **DOI**: 10.1007/s11263-019-01188-y
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.03833v3)
- **Published**: 2018-08-11 16:36:38+00:00
- **Updated**: 2019-07-08 16:58:57+00:00
- **Authors**: Abhinav Valada, Rohit Mohan, Wolfram Burgard
- **Comment**: A Live demo is available at http://deepscene.cs.uni-freiburg.de and
  the code as well as the models are available at
  https://github.com/DeepSceneSeg
- **Journal**: International Journal of Computer Vision (IJCV), Special Issue:
  Deep Learning for Robotic Vision, vol. 128, no. 5, pp. 1239-1285, 2019
- **Summary**: Learning to reliably perceive and understand the scene is an integral enabler for robots to operate in the real-world. This problem is inherently challenging due to the multitude of object types as well as appearance changes caused by varying illumination and weather conditions. Leveraging complementary modalities can enable learning of semantically richer representations that are resilient to such perturbations. Despite the tremendous progress in recent years, most multimodal convolutional neural network approaches directly concatenate feature maps from individual modality streams rendering the model incapable of focusing only on relevant complementary information for fusion. To address this limitation, we propose a mutimodal semantic segmentation framework that dynamically adapts the fusion of modality-specific features while being sensitive to the object category, spatial location and scene context in a self-supervised manner. Specifically, we propose an architecture consisting of two modality-specific encoder streams that fuse intermediate encoder representations into a single decoder using our proposed self-supervised model adaptation fusion mechanism which optimally combines complementary features. As intermediate representations are not aligned across modalities, we introduce an attention scheme for better correlation. In addition, we propose a computationally efficient unimodal segmentation architecture termed AdapNet++ that incorporates a new encoder with multiscale residual units and an efficient atrous spatial pyramid pooling that has a larger effective receptive field with more than 10x fewer parameters, complemented with a strong decoder with a multi-resolution supervision scheme that recovers high-resolution details. Comprehensive empirical evaluations on several benchmarks demonstrate that both our unimodal and multimodal architectures achieve state-of-the-art performance.



### Fully-Automated Analysis of Body Composition from CT in Cancer Patients Using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1808.03844v1
- **DOI**: 10.1007/978-3-030-01201-4_22
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.03844v1)
- **Published**: 2018-08-11 18:10:22+00:00
- **Updated**: 2018-08-11 18:10:22+00:00
- **Authors**: Christopher P. Bridge, Michael Rosenthal, Bradley Wright, Gopal Kotecha, Florian Fintelmann, Fabian Troschel, Nityanand Miskin, Khanant Desai, William Wrobel, Ana Babic, Natalia Khalaf, Lauren Brais, Marisa Welch, Caitlin Zellers, Neil Tenenholtz, Mark Michalski, Brian Wolpin, Katherine Andriole
- **Comment**: None
- **Journal**: None
- **Summary**: The amounts of muscle and fat in a person's body, known as body composition, are correlated with cancer risks, cancer survival, and cardiovascular risk. The current gold standard for measuring body composition requires time-consuming manual segmentation of CT images by an expert reader. In this work, we describe a two-step process to fully automate the analysis of CT body composition using a DenseNet to select the CT slice and U-Net to perform segmentation. We train and test our methods on independent cohorts. Our results show Dice scores (0.95-0.98) and correlation coefficients (R=0.99) that are favorable compared to human readers. These results suggest that fully automated body composition analysis is feasible, which could enable both clinical use and large-scale population studies.



### A Domain Guided CNN Architecture for Predicting Age from Structural Brain Images
- **Arxiv ID**: http://arxiv.org/abs/1808.04362v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1808.04362v1)
- **Published**: 2018-08-11 19:43:22+00:00
- **Updated**: 2018-08-11 19:43:22+00:00
- **Authors**: Pascal Sturmfels, Saige Rutherford, Mike Angstadt, Mark Peterson, Chandra Sripada, Jenna Wiens
- **Comment**: Machine Learning for Healthcare 2018
- **Journal**: None
- **Summary**: Given the wide success of convolutional neural networks (CNNs) applied to natural images, researchers have begun to apply them to neuroimaging data. To date, however, exploration of novel CNN architectures tailored to neuroimaging data has been limited. Several recent works fail to leverage the 3D structure of the brain, instead treating the brain as a set of independent 2D slices. Approaches that do utilize 3D convolutions rely on architectures developed for object recognition tasks in natural 2D images. Such architectures make assumptions about the input that may not hold for neuroimaging. For example, existing architectures assume that patterns in the brain exhibit translation invariance. However, a pattern in the brain may have different meaning depending on where in the brain it is located. There is a need to explore novel architectures that are tailored to brain images. We present two simple modifications to existing CNN architectures based on brain image structure. Applied to the task of brain age prediction, our network achieves a mean absolute error (MAE) of 1.4 years and trains 30% faster than a CNN baseline that achieves a MAE of 1.6 years. Our results suggest that lessons learned from developing models on natural images may not directly transfer to neuroimaging tasks. Instead, there remains a large space of unexplored questions regarding model development in this area, whose answers may differ from conventional wisdom.



### Pixel Objectness: Learning to Segment Generic Objects Automatically in Images and Videos
- **Arxiv ID**: http://arxiv.org/abs/1808.04702v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.04702v2)
- **Published**: 2018-08-11 21:53:22+00:00
- **Updated**: 2018-12-18 02:46:57+00:00
- **Authors**: Bo Xiong, Suyog Dutt Jain, Kristen Grauman
- **Comment**: To appear in PAMI. arXiv admin note: text overlap with
  arXiv:1701.05349, arXiv:1701.05384
- **Journal**: None
- **Summary**: We propose an end-to-end learning framework for segmenting generic objects in both images and videos. Given a novel image or video, our approach produces a pixel-level mask for all "object-like" regions---even for object categories never seen during training. We formulate the task as a structured prediction problem of assigning an object/background label to each pixel, implemented using a deep fully convolutional network. When applied to a video, our model further incorporates a motion stream, and the network learns to combine both appearance and motion and attempts to extract all prominent objects whether they are moving or not. Beyond the core model, a second contribution of our approach is how it leverages varying strengths of training annotations. Pixel-level annotations are quite difficult to obtain, yet crucial for training a deep network approach for segmentation. Thus we propose ways to exploit weakly labeled data for learning dense foreground segmentation. For images, we show the value in mixing object category examples with image-level labels together with relatively few images with boundary-level annotations. For video, we show how to bootstrap weakly annotated videos together with the network trained for image segmentation. Through experiments on multiple challenging image and video segmentation benchmarks, our method offers consistently strong results and improves the state-of-the-art for fully automatic segmentation of generic (unseen) objects. In addition, we demonstrate how our approach benefits image retrieval and image retargeting, both of which flourish when given our high-quality foreground maps. Code, models, and videos are at:http://vision.cs.utexas.edu/projects/pixelobjectness/



