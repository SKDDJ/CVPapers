# Arxiv Papers in cs.CV on 2018-08-24
### Deep Feature Pyramid Reconfiguration for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1808.07993v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.07993v1)
- **Published**: 2018-08-24 03:17:17+00:00
- **Updated**: 2018-08-24 03:17:17+00:00
- **Authors**: Tao Kong, Fuchun Sun, Wenbing Huang, Huaping Liu
- **Comment**: To appear in ECCV 2018
- **Journal**: None
- **Summary**: State-of-the-art object detectors usually learn multi-scale representations to get better results by employing feature pyramids. However, the current designs for feature pyramids are still inefficient to integrate the semantic information over different scales. In this paper, we begin by investigating current feature pyramids solutions, and then reformulate the feature pyramid construction as the feature reconfiguration process. Finally, we propose a novel reconfiguration architecture to combine low-level representations with high-level semantic features in a highly-nonlinear yet efficient way. In particular, our architecture which consists of global attention and local reconfigurations, is able to gather task-oriented features across different spatial locations and scales, globally and locally. Both the global attention and local reconfiguration are lightweight, in-place, and end-to-end trainable. Using this method in the basic SSD system, our models achieve consistent and significant boosts compared with the original model and its other variations, without losing real-time processing speed.



### Decision fusion with multiple spatial supports by conditional random fields
- **Arxiv ID**: http://arxiv.org/abs/1808.08024v1
- **DOI**: 10.1109/TGRS.2018.2797316
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1808.08024v1)
- **Published**: 2018-08-24 06:52:50+00:00
- **Updated**: 2018-08-24 06:52:50+00:00
- **Authors**: Devis Tuia, Michele Volpi, Gabriele Moser
- **Comment**: None
- **Journal**: IEEE Transactions on Geoscience and Remote Sensing, 56(6),
  3277-3289, 2018
- **Summary**: Classification of remotely sensed images into land cover or land use is highly dependent on geographical information at least at two levels. First, land cover classes are observed in a spatially smooth domain separated by sharp region boundaries. Second, land classes and observation scale are also tightly intertwined: they tend to be consistent within areas of homogeneous appearance, or regions, in the sense that all pixels within a roof should be classified as roof, independently on the spatial support used for the classification. In this paper, we follow these two observations and encode them as priors in an energy minimization framework based on conditional random fields (CRFs), where classification results obtained at pixel and region levels are probabilistically fused. The aim is to enforce the final maps to be consistent not only in their own spatial supports (pixel and region) but also across supports, i.e., by getting the predictions on the pixel lattice and on the set of regions to agree. To this end, we define an energy function with three terms: 1) a data term for the individual elements in each support (support-specific nodes); 2) spatial regularization terms in a neighborhood for each of the supports (support-specific edges); and 3) a regularization term between individual pixels and the region containing each of them (intersupports edges). We utilize these priors in a unified energy minimization problem that can be optimized by standard solvers. The proposed 2LCRF model consists of a CRF defined over a bipartite graph, i.e., two interconnected layers within a single graph accounting for interlattice connections.



### Atherosclerotic carotid plaques on panoramic imaging: an automatic detection using deep learning with small dataset
- **Arxiv ID**: http://arxiv.org/abs/1808.08093v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.08093v1)
- **Published**: 2018-08-24 11:25:36+00:00
- **Updated**: 2018-08-24 11:25:36+00:00
- **Authors**: Lazar Kats, Marilena Vered, Ayelet Zlotogorski-Hurvitz, Itai Harpaz
- **Comment**: 11 pages, 5 figures
- **Journal**: None
- **Summary**: Stroke is the second most frequent cause of death worldwide with a considerable economic burden on the health systems. In about 15% of strokes, atherosclerotic carotid plaques (ACPs) constitute the main etiological factor. Early detection of ACPs may have a key-role for preventing strokes by managing the patient a-priory to the occurrence of the damage. ACPs can be detected on panoramic images. As these are one of the most common images performed for routine dental practice, they can be used as a source of available data for computerized methods of automatic detection in order to significantly increase timely diagnosis of ACPs. Recently, there has been a definite breakthrough in the field of analysis of medical images due to the use of deep learning based on neural networks. These methods, however have been barely used in dentistry. In this study we used the Faster Region-based Convolutional Network (Faster R-CNN) for deep learning. We aimed to assess the operation of the algorithm on a small database of 65 panoramic images. Due to a small amount of available training data, we had to use data augmentation by changing the brightness and randomly flipping and rotating cropped regions of interest in multiple angles. Receiver Operating Characteristic (ROC) analysis was performed to calculate the accuracy of detection. ACP was detected with a sensitivity of 75%, specificity of 80% and an accuracy of 83%. The ROC analysis showed a significant Area Under Curve (AUC) difference from 0.5. Our novelty lies in that we have showed the efficiency of the Faster R-CNN algorithm in detecting ACPs on routine panoramic images based on a small database. There is a need to further improve the application of the algorithm to the level of introducing this methodology in routine dental practice in order to enable us to prevent stroke events.



### MVOR: A Multi-view RGB-D Operating Room Dataset for 2D and 3D Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1808.08180v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.08180v3)
- **Published**: 2018-08-24 15:47:48+00:00
- **Updated**: 2021-08-20 10:39:50+00:00
- **Authors**: Vinkle Srivastav, Thibaut Issenhuth, Abdolrahim Kadkhodamohammadi, Michel de Mathelin, Afshin Gangi, Nicolas Padoy
- **Comment**: Dataset and code is available at
  https://github.com/camma-public/mvor. The paper was presented in the
  MICCAI-LABELS 2018
  (https://labels.tue-image.nl/previous-editions/labels-2018/)
- **Journal**: None
- **Summary**: Person detection and pose estimation is a key requirement to develop intelligent context-aware assistance systems. To foster the development of human pose estimation methods and their applications in the Operating Room (OR), we release the Multi-View Operating Room (MVOR) dataset, the first public dataset recorded during real clinical interventions. It consists of 732 synchronized multi-view frames recorded by three RGB-D cameras in a hybrid OR. It also includes the visual challenges present in such environments, such as occlusions and clutter. We provide camera calibration parameters, color and depth frames, human bounding boxes, and 2D/3D pose annotations. In this paper, we present the dataset, its annotations, as well as baseline results from several recent person detection and 2D/3D pose estimation methods. Since we need to blur some parts of the images to hide identity and nudity in the released dataset, we also present a comparative study of how the baselines have been impacted by the blurring. Results show a large margin for improvement and suggest that the MVOR dataset can be useful to compare the performance of the different methods.



### Automatic Foreground Extraction from Imperfect Backgrounds using Multi-Agent Consensus Equilibrium
- **Arxiv ID**: http://arxiv.org/abs/1808.08210v3
- **DOI**: 10.1016/j.jvcir.2020.102907
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.08210v3)
- **Published**: 2018-08-24 16:58:00+00:00
- **Updated**: 2020-05-31 20:01:58+00:00
- **Authors**: Xiran Wang, Jason Juang, Stanley H. Chan
- **Comment**: None
- **Journal**: None
- **Summary**: Extracting accurate foreground objects from a scene is an essential step for many video applications. Traditional background subtraction algorithms can generate coarse estimates, but generating high quality masks requires professional softwares with significant human interventions, e.g., providing trimaps or labeling key frames. We propose an automatic foreground extraction method in applications where a static but imperfect background is available. Examples include filming and surveillance where the background can be captured before the objects enter the scene or after they leave the scene. Our proposed method is very robust and produces significantly better estimates than state-of-the-art background subtraction, video segmentation and alpha matting methods. The key innovation of our method is a novel information fusion technique. The fusion framework allows us to integrate the individual strengths of alpha matting, background subtraction and image denoising to produce an overall better estimate. Such integration is particularly important when handling complex scenes with imperfect background. We show how the framework is developed, and how the individual components are built. Extensive experiments and ablation studies are conducted to evaluate the proposed method.



### GlymphVIS: Visualizing Glymphatic Transport Pathways Using Regularized Optimal Transport
- **Arxiv ID**: http://arxiv.org/abs/1808.08304v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.08304v2)
- **Published**: 2018-08-24 20:43:37+00:00
- **Updated**: 2018-09-01 16:27:39+00:00
- **Authors**: Rena Elkin, Saad Nadeem, Eldad Haber, Klara Steklova, Hedok Lee, Helene Benveniste, Allen Tannenbaum
- **Comment**: MICCAI 2018, 8 pages
- **Journal**: None
- **Summary**: The glymphatic system (GS) is a transit passage that facilitates brain metabolic waste removal and its dysfunction has been associated with neurodegenerative diseases such as Alzheimer's disease. The GS has been studied by acquiring temporal contrast enhanced magnetic resonance imaging (MRI) sequences of a rodent brain, and tracking the cerebrospinal fluid injected contrast agent as it flows through the GS. We present here a novel visualization framework, GlymphVIS, which uses regularized optimal transport (OT) to study the flow behavior between time points at which the images are taken. Using this regularized OT approach, we can incorporate diffusion, handle noise, and accurately capture and visualize the time varying dynamics in GS transport. Moreover, we are able to reduce the registration mean-squared and infinity-norm error across time points by up to a factor of 5 as compared to the current state-of-the-art method. Our visualization pipeline yields flow patterns that align well with experts' current findings of the glymphatic system.



### Reproducible and Interpretable Spiculation Quantification for Lung Cancer Screening
- **Arxiv ID**: http://arxiv.org/abs/1808.08307v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.08307v3)
- **Published**: 2018-08-24 20:59:56+00:00
- **Updated**: 2020-10-15 15:50:47+00:00
- **Authors**: Wookjin Choi, Saad Nadeem, Sadegh Riyahi, Joseph O. Deasy, Allen Tannenbaum, Wei Lu
- **Comment**: Computer Methods and Programs in Biomedicine (Journal Extension of
  MICCAI ShapeMI 2018 Workshop paper). **First two authors contributed equally
  to this work
- **Journal**: None
- **Summary**: Spiculations are important predictors of lung cancer malignancy, which are spikes on the surface of the pulmonary nodules. In this study, we proposed an interpretable and parameter-free technique to quantify the spiculation using area distortion metric obtained by the conformal (angle-preserving) spherical parameterization. We exploit the insight that for an angle-preserved spherical mapping of a given nodule, the corresponding negative area distortion precisely characterizes the spiculations on that nodule. We introduced novel spiculation scores based on the area distortion metric and spiculation measures. We also semi-automatically segment lung nodule (for reproducibility) as well as vessel and wall attachment to differentiate the real spiculations from lobulation and attachment. A simple pathological malignancy prediction model is also introduced. We used the publicly-available LIDC-IDRI dataset pathologists (strong-label) and radiologists (weak-label) ratings to train and test radiomics models containing this feature, and then externally validate the models. We achieved AUC$=$0.80 and 0.76, respectively, with the models trained on the 811 weakly-labeled LIDC datasets and tested on the 72 strongly-labeled LIDC and 73 LUNGx datasets; the previous best model for LUNGx had AUC$=$0.68. The number-of-spiculations feature was found to be highly correlated (Spearman's rank correlation coefficient $\rho = 0.44$) with the radiologists' spiculation score. We developed a reproducible and interpretable, parameter-free technique for quantifying spiculations on nodules. The spiculation quantification measures was then applied to the radiomics framework for pathological malignancy prediction with reproducible semi-automatic segmentation of nodule. Using our interpretable features (size, attachment, spiculation, lobulation), we were able to achieve higher performance than previous models.



### ParaNet - Using Dense Blocks for Early Inference
- **Arxiv ID**: http://arxiv.org/abs/1808.08308v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.08308v1)
- **Published**: 2018-08-24 21:01:17+00:00
- **Updated**: 2018-08-24 21:01:17+00:00
- **Authors**: Joseph Chuang, Eric Tsai, Kevin Huang, Jay Fetter
- **Comment**: None
- **Journal**: None
- **Summary**: DenseNets have been shown to be a competitive model among recent convolutional network architectures. These networks utilize Dense Blocks, which are groups of densely connected layers where the output of a hidden layer is fed in as the input of every other layer following it. In this paper, we aim to improve certain aspects of DenseNet, especially when it comes to practicality. We introduce ParaNet, a new architecture that constructs three pipelines which allow for early inference. We additionally introduce a cascading mechanism such that different pipelines are able to share parameters, as well as logit matching between the outputs of the pipelines. We separately evaluate each of the newly introduced mechanisms of ParaNet, then evaluate our proposed architecture on CIFAR-100.



### Quantification of Local Metabolic Tumor Volume Changes by Registering Blended PET-CT Images for Prediction of Pathologic Tumor Response
- **Arxiv ID**: http://arxiv.org/abs/1808.08312v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.08312v1)
- **Published**: 2018-08-24 21:17:11+00:00
- **Updated**: 2018-08-24 21:17:11+00:00
- **Authors**: Sadegh Riyahi, Wookjin Choi, Chia-Ju Liu, Saad Nadeem, Shan Tan, Hualiang Zhong, Wengen Chen, Abraham J. Wu, James G. Mechalakos, Joseph O. Deasy, Wei Lu
- **Comment**: MICCAI 2018: Workshop on Data-Driven Treatment Response Assessment
  (Oral Presentation), 11 pages
- **Journal**: None
- **Summary**: Quantification of local metabolic tumor volume (MTV) chan-ges after Chemo-radiotherapy would allow accurate tumor response evaluation. Currently, local MTV changes in esophageal (soft-tissue) cancer are measured by registering follow-up PET to baseline PET using the same transformation obtained by deformable registration of follow-up CT to baseline CT. Such approach is suboptimal because PET and CT capture fundamentally different properties (metabolic vs. anatomy) of a tumor. In this work we combined PET and CT images into a single blended PET-CT image and registered follow-up blended PET-CT image to baseline blended PET-CT image. B-spline regularized diffeomorphic registration was used to characterize the large MTV shrinkage. Jacobian of the resulting transformation was computed to measure the local MTV changes. Radiomic features (intensity and texture) were then extracted from the Jacobian map to predict pathologic tumor response. Local MTV changes calculated using blended PET-CT registration achieved the highest correlation with ground truth segmentation (R=0.88) compared to PET-PET (R=0.80) and CT-CT (R=0.67) registrations. Moreover, using blended PET-CT registration, the multivariate prediction model achieved the highest accuracy with only one Jacobian co-occurrence texture feature (accuracy=82.3%). This novel framework can replace the conventional approach that applies CT-CT transformation to the PET data for longitudinal evaluation of tumor response.



### BOP: Benchmark for 6D Object Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1808.08319v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1808.08319v1)
- **Published**: 2018-08-24 21:38:53+00:00
- **Updated**: 2018-08-24 21:38:53+00:00
- **Authors**: Tomas Hodan, Frank Michel, Eric Brachmann, Wadim Kehl, Anders Glent Buch, Dirk Kraft, Bertram Drost, Joel Vidal, Stephan Ihrke, Xenophon Zabulis, Caner Sahin, Fabian Manhardt, Federico Tombari, Tae-Kyun Kim, Jiri Matas, Carsten Rother
- **Comment**: ECCV 2018
- **Journal**: None
- **Summary**: We propose a benchmark for 6D pose estimation of a rigid object from a single RGB-D input image. The training data consists of a texture-mapped 3D object model or images of the object in known 6D poses. The benchmark comprises of: i) eight datasets in a unified format that cover different practical scenarios, including two new datasets focusing on varying lighting conditions, ii) an evaluation methodology with a pose-error function that deals with pose ambiguities, iii) a comprehensive evaluation of 15 diverse recent methods that captures the status quo of the field, and iv) an online evaluation system that is open for continuous submission of new results. The evaluation shows that methods based on point-pair features currently perform best, outperforming template matching methods, learning-based methods and methods based on 3D local features. The project website is available at bop.felk.cvut.cz.



