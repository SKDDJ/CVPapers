# Arxiv Papers in cs.CV on 2018-08-30
### Super-Resolution for Hyperspectral and Multispectral Image Fusion Accounting for Seasonal Spectral Variability
- **Arxiv ID**: http://arxiv.org/abs/1808.10072v2
- **DOI**: 10.1109/TIP.2019.2928895
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.10072v2)
- **Published**: 2018-08-30 00:32:37+00:00
- **Updated**: 2019-07-02 16:08:26+00:00
- **Authors**: Ricardo Augusto Borsoi, Tales Imbiriba, José Carlos Moreira Bermudez
- **Comment**: None
- **Journal**: None
- **Summary**: Image fusion combines data from different heterogeneous sources to obtain more precise information about an underlying scene. Hyperspectral-multispectral (HS-MS) image fusion is currently attracting great interest in remote sensing since it allows the generation of high spatial resolution HS images, circumventing the main limitation of this imaging modality. Existing HS-MS fusion algorithms, however, neglect the spectral variability often existing between images acquired at different time instants. This time difference causes variations in spectral signatures of the underlying constituent materials due to different acquisition and seasonal conditions. This paper introduces a novel HS-MS image fusion strategy that combines an unmixing-based formulation with an explicit parametric model for typical spectral variability between the two images. Simulations with synthetic and real data show that the proposed strategy leads to a significant performance improvement under spectral variability and state-of-the-art performance otherwise.



### Towards Effective Deep Embedding for Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1808.10075v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.10075v2)
- **Published**: 2018-08-30 00:51:13+00:00
- **Updated**: 2018-12-11 03:03:08+00:00
- **Authors**: Lei Zhang, Peng Wang, Lingqiao Liu, Chunhua Shen, Wei Wei, Yannning Zhang, Anton Van Den Hengel
- **Comment**: Working in progress
- **Journal**: None
- **Summary**: Zero-shot learning (ZSL) can be formulated as a cross-domain matching problem: after being projected into a joint embedding space, a visual sample will match against all candidate class-level semantic descriptions and be assigned to the nearest class. In this process, the embedding space underpins the success of such matching and is crucial for ZSL. In this paper, we conduct an in-depth study on the construction of embedding space for ZSL and posit that an ideal embedding space should satisfy two criteria: intra-class compactness and inter-class separability. While the former encourages the embeddings of visual samples of one class to distribute tightly close to the semantic description embedding of this class, the latter requires embeddings from different classes to be well separated from each other. Towards this goal, we present a simple but effective two-branch network to simultaneously map semantic descriptions and visual samples into a joint space, on which visual embeddings are forced to regress to their class-level semantic embeddings and the embeddings crossing classes are required to be distinguishable by a trainable classifier. Furthermore, we extend our method to a transductive setting to better handle the model bias problem in ZSL (i.e., samples from unseen classes tend to be categorized into seen classes) with minimal extra supervision. Specifically, we propose a pseudo labeling strategy to progressively incorporate the testing samples into the training process and thus balance the model between seen and unseen classes. Experimental results on five standard ZSL datasets show the superior performance of the proposed method and its transductive extension.



### Differential and integral invariants under Mobius transformation
- **Arxiv ID**: http://arxiv.org/abs/1808.10083v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV, math.DG
- **Links**: [PDF](http://arxiv.org/pdf/1808.10083v1)
- **Published**: 2018-08-30 01:45:57+00:00
- **Updated**: 2018-08-30 01:45:57+00:00
- **Authors**: He Zhang, Hanlin Mo, You Hao, Qi Li, Hua Li
- **Comment**: None
- **Journal**: None
- **Summary**: One of the most challenging problems in the domain of 2-D image or 3-D shape is to handle the non-rigid deformation. From the perspective of transformation groups, the conformal transformation is a key part of the diffeomorphism. According to the Liouville Theorem, an important part of the conformal transformation is the Mobius transformation, so we focus on Mobius transformation and propose two differential expressions that are invariable under 2-D and 3-D Mobius transformation respectively. Next, we analyze the absoluteness and relativity of invariance on them and their components. After that, we propose integral invariants under Mobius transformation based on the two differential expressions. Finally, we propose a conjecture about the structure of differential invariants under conformal transformation according to our observation on the composition of the above two differential invariants.



### Artifacts Detection and Error Block Analysis from Broadcasted Videos
- **Arxiv ID**: http://arxiv.org/abs/1808.10086v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1808.10086v1)
- **Published**: 2018-08-30 02:15:46+00:00
- **Updated**: 2018-08-30 02:15:46+00:00
- **Authors**: Md Mehedi Hasan, Tasneem Rahman, Kiok Ahn, Oksam Chae
- **Comment**: None
- **Journal**: None
- **Summary**: With the advancement of IPTV and HDTV technology, previous subtle errors in videos are now becoming more prominent because of the structure oriented and compression based artifacts. In this paper, we focus towards the development of a real-time video quality check system. Light weighted edge gradient magnitude information is incorporated to acquire the statistical information and the distorted frames are then estimated based on the characteristics of their surrounding frames. Then we apply the prominent texture patterns to classify them in different block errors and analyze them not only in video error detection application but also in error concealment, restoration and retrieval. Finally, evaluating the performance through experiments on prominent datasets and broadcasted videos show that the proposed algorithm is very much efficient to detect errors for video broadcast and surveillance applications in terms of computation time and analysis of distorted frames.



### CNN-PS: CNN-based Photometric Stereo for General Non-Convex Surfaces
- **Arxiv ID**: http://arxiv.org/abs/1808.10093v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.10093v1)
- **Published**: 2018-08-30 02:48:51+00:00
- **Updated**: 2018-08-30 02:48:51+00:00
- **Authors**: Satoshi Ikehata
- **Comment**: Accepted in ECCV 2018 (ECCV2018). Source code and supplementary are
  available at https://github.com/satoshi-ikehata/CNN-PS
- **Journal**: None
- **Summary**: Most conventional photometric stereo algorithms inversely solve a BRDF-based image formation model. However, the actual imaging process is often far more complex due to the global light transport on the non-convex surfaces. This paper presents a photometric stereo network that directly learns relationships between the photometric stereo input and surface normals of a scene. For handling unordered, arbitrary number of input images, we merge all the input data to the intermediate representation called {\it observation map} that has a fixed shape, is able to be fed into a CNN. To improve both training and prediction, we take into account the rotational pseudo-invariance of the observation map that is derived from the isotropic constraint. For training the network, we create a synthetic photometric stereo dataset that is generated by a physics-based renderer, therefore the global light transport is considered. Our experimental results on both synthetic and real datasets show that our method outperforms conventional BRDF-based photometric stereo algorithms especially when scenes are highly non-convex.



### Dense Scene Flow from Stereo Disparity and Optical Flow
- **Arxiv ID**: http://arxiv.org/abs/1808.10146v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.10146v1)
- **Published**: 2018-08-30 07:13:36+00:00
- **Updated**: 2018-08-30 07:13:36+00:00
- **Authors**: René Schuster, Oliver Wasenmüller, Didier Stricker
- **Comment**: None
- **Journal**: None
- **Summary**: Scene flow describes 3D motion in a 3D scene. It can either be modeled as a single task, or it can be reconstructed from the auxiliary tasks of stereo depth and optical flow estimation. While the second method can achieve real-time performance by using real-time auxiliary methods, it will typically produce non-dense results. In this representation of a basic combination approach for scene flow estimation, we will tackle the problem of non-density by interpolation.



### A Variational Feature Encoding Method of 3D Object for Probabilistic Semantic SLAM
- **Arxiv ID**: http://arxiv.org/abs/1808.10180v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1808.10180v1)
- **Published**: 2018-08-30 08:39:04+00:00
- **Updated**: 2018-08-30 08:39:04+00:00
- **Authors**: H. W. Yu, B. H. Lee
- **Comment**: to appear in the proceedings of IROS 2018
- **Journal**: None
- **Summary**: This paper presents a feature encoding method of complex 3D objects for high-level semantic features. Recent approaches to object recognition methods become important for semantic simultaneous localization and mapping (SLAM). However, there is a lack of consideration of the probabilistic observation model for 3D objects, as the shape of a 3D object basically follows a complex probability distribution. Furthermore, since the mobile robot equipped with a range sensor observes only a single view, much information of the object shape is discarded. These limitations are the major obstacles to semantic SLAM and view-independent loop closure using 3D object shapes as features. In order to enable the numerical analysis for the Bayesian inference, we approximate the true observation model of 3D objects to tractable distributions. Since the observation likelihood can be obtained from the generative model, we formulate the true generative model for 3D object with the Bayesian networks. To capture these complex distributions, we apply a variational auto-encoder. To analyze the approximated distributions and encoded features, we perform classification with maximum likelihood estimation and shape retrieval.



### Automated Scene Flow Data Generation for Training and Verification
- **Arxiv ID**: http://arxiv.org/abs/1808.10232v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.10232v2)
- **Published**: 2018-08-30 11:37:01+00:00
- **Updated**: 2018-08-31 11:48:46+00:00
- **Authors**: Oliver Wasenmüller, René Schuster, Didier Stricker, Karl Leiss, Jürger Pfister, Oleksandra Ganus, Julian Tatsch, Artem Savkin, Nikolas Brasch
- **Comment**: None
- **Journal**: None
- **Summary**: Scene flow describes the 3D position as well as the 3D motion of each pixel in an image. Such algorithms are the basis for many state-of-the-art autonomous or automated driving functions. For verification and training large amounts of ground truth data is required, which is not available for real data. In this paper, we demonstrate a technology to create synthetic data with dense and precise scene flow ground truth.



### Bayesian Outdoor Defect Detection
- **Arxiv ID**: http://arxiv.org/abs/1809.01000v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.01000v1)
- **Published**: 2018-08-30 12:36:36+00:00
- **Updated**: 2018-08-30 12:36:36+00:00
- **Authors**: Fei Jiang, Guosheng Yin
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a Bayesian defect detector to facilitate the defect detection on the motion blurred images on rough texture surfaces. To enhance the accuracy of Bayesian detection on removing non-defect pixels, we develop a class of reflected non-local prior distributions, which is constructed by using the mode of a distribution to subtract its density. The reflected non-local priors forces the Bayesian detector to approach 0 at the non-defect locations. We conduct experiments studies to demonstrate the superior performance of the Bayesian detector in eliminating the non-defect points. We implement the Bayesian detector in the motion blurred drone images, in which the detector successfully identifies the hail damages on the rough surface and substantially enhances the accuracy of the entire defect detection pipeline.



### PPF-FoldNet: Unsupervised Learning of Rotation Invariant 3D Local Descriptors
- **Arxiv ID**: http://arxiv.org/abs/1808.10322v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1808.10322v1)
- **Published**: 2018-08-30 14:45:26+00:00
- **Updated**: 2018-08-30 14:45:26+00:00
- **Authors**: Haowen Deng, Tolga Birdal, Slobodan Ilic
- **Comment**: Accepted for publication at ECCV 2018
- **Journal**: None
- **Summary**: We present PPF-FoldNet for unsupervised learning of 3D local descriptors on pure point cloud geometry. Based on the folding-based auto-encoding of well known point pair features, PPF-FoldNet offers many desirable properties: it necessitates neither supervision, nor a sensitive local reference frame, benefits from point-set sparsity, is end-to-end, fast, and can extract powerful rotation invariant descriptors. Thanks to a novel feature visualization, its evolution can be monitored to provide interpretable insights. Our extensive experiments demonstrate that despite having six degree-of-freedom invariance and lack of training labels, our network achieves state of the art results in standard benchmark datasets and outperforms its competitors when rotations and varying point densities are present. PPF-FoldNet achieves $9\%$ higher recall on standard benchmarks, $23\%$ higher recall when rotations are introduced into the same datasets and finally, a margin of $>35\%$ is attained when point density is significantly decreased.



### Gaussian Mixture Generative Adversarial Networks for Diverse Datasets, and the Unsupervised Clustering of Images
- **Arxiv ID**: http://arxiv.org/abs/1808.10356v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1808.10356v1)
- **Published**: 2018-08-30 15:32:40+00:00
- **Updated**: 2018-08-30 15:32:40+00:00
- **Authors**: Matan Ben-Yosef, Daphna Weinshall
- **Comment**: 20 pages, 8 figures
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) have been shown to produce realistically looking synthetic images with remarkable success, yet their performance seems less impressive when the training set is highly diverse. In order to provide a better fit to the target data distribution when the dataset includes many different classes, we propose a variant of the basic GAN model, called Gaussian Mixture GAN (GM-GAN), where the probability distribution over the latent space is a mixture of Gaussians. We also propose a supervised variant which is capable of conditional sample synthesis. In order to evaluate the model's performance, we propose a new scoring method which separately takes into account two (typically conflicting) measures - diversity vs. quality of the generated data. Through a series of empirical experiments, using both synthetic and real-world datasets, we quantitatively show that GM-GANs outperform baselines, both when evaluated using the commonly used Inception Score, and when evaluated using our own alternative scoring method. In addition, we qualitatively demonstrate how the \textit{unsupervised} variant of GM-GAN tends to map latent vectors sampled from different Gaussians in the latent space to samples of different classes in the data space. We show how this phenomenon can be exploited for the task of unsupervised clustering, and provide quantitative evaluation showing the superiority of our method for the unsupervised clustering of image datasets. Finally, we demonstrate a feature which further sets our model apart from other GAN models: the option to control the quality-diversity trade-off by altering, post-training, the probability distribution of the latent space. This allows one to sample higher quality and lower diversity samples, or vice versa, according to one's needs.



### Deep Chronnectome Learning via Full Bidirectional Long Short-Term Memory Networks for MCI Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/1808.10383v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.10383v1)
- **Published**: 2018-08-30 16:24:11+00:00
- **Updated**: 2018-08-30 16:24:11+00:00
- **Authors**: Weizheng Yan, Han Zhang, Jing Sui, Dinggang Shen
- **Comment**: The paper has been accepted by MICCAI2018
- **Journal**: None
- **Summary**: Brain functional connectivity (FC) extracted from resting-state fMRI (RS-fMRI) has become a popular approach for disease diagnosis, where discriminating subjects with mild cognitive impairment (MCI) from normal controls (NC) is still one of the most challenging problems. Dynamic functional connectivity (dFC), consisting of time-varying spatiotemporal dynamics, may characterize "chronnectome" diagnostic information for improving MCI classification. However, most of the current dFC studies are based on detecting discrete major brain status via spatial clustering, which ignores rich spatiotemporal dynamics contained in such chronnectome. We propose Deep Chronnectome Learning for exhaustively mining the comprehensive information, especially the hidden higher-level features, i.e., the dFC time series that may add critical diagnostic power for MCI classification. To this end, we devise a new Fully-connected Bidirectional Long Short-Term Memory Network (Full-BiLSTM) to effectively learn the periodic brain status changes using both past and future information for each brief time segment and then fuse them to form the final output. We have applied our method to a rigorously built large-scale multi-site database (i.e., with 164 data from NCs and 330 from MCIs, which can be further augmented by 25 folds). Our method outperforms other state-of-the-art approaches with an accuracy of 73.6% under solid cross-validations. We also made extensive comparisons among multiple variants of LSTM models. The results suggest high feasibility of our method with promising value also for other brain disorder diagnoses.



### DeepFall -- Non-invasive Fall Detection with Deep Spatio-Temporal Convolutional Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/1809.00977v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.00977v3)
- **Published**: 2018-08-30 16:41:58+00:00
- **Updated**: 2020-04-27 18:12:14+00:00
- **Authors**: Jacob Nogas, Shehroz S. Khan, Alex Mihailidis
- **Comment**: None
- **Journal**: None
- **Summary**: Human falls rarely occur; however, detecting falls is very important from the health and safety perspective. Due to the rarity of falls, it is difficult to employ supervised classification techniques to detect them. Moreover, in these highly skewed situations it is also difficult to extract domain specific features to identify falls. In this paper, we present a novel framework, \textit{DeepFall}, which formulates the fall detection problem as an anomaly detection problem. The \textit{DeepFall} framework presents the novel use of deep spatio-temporal convolutional autoencoders to learn spatial and temporal features from normal activities using non-invasive sensing modalities. We also present a new anomaly scoring method that combines the reconstruction score of frames across a temporal window to detect unseen falls. We tested the \textit{DeepFall} framework on three publicly available datasets collected through non-invasive sensing modalities, thermal camera and depth cameras and show superior results in comparison to traditional autoencoder methods to identify unseen falls.



### Learning End-to-end Autonomous Driving using Guided Auxiliary Supervision
- **Arxiv ID**: http://arxiv.org/abs/1808.10393v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1808.10393v1)
- **Published**: 2018-08-30 16:46:22+00:00
- **Updated**: 2018-08-30 16:46:22+00:00
- **Authors**: Ashish Mehta, Adithya Subramanian, Anbumani Subramanian
- **Comment**: 12 pages, 5 figures, 1 table
- **Journal**: None
- **Summary**: Learning to drive faithfully in highly stochastic urban settings remains an open problem. To that end, we propose a Multi-task Learning from Demonstration (MT-LfD) framework which uses supervised auxiliary task prediction to guide the main task of predicting the driving commands. Our framework involves an end-to-end trainable network for imitating the expert demonstrator's driving commands. The network intermediately predicts visual affordances and action primitives through direct supervision which provide the aforementioned auxiliary supervised guidance. We demonstrate that such joint learning and supervised guidance facilitates hierarchical task decomposition, assisting the agent to learn faster, achieve better driving performance and increases transparency of the otherwise black-box end-to-end network. We run our experiments to validate the MT-LfD framework in CARLA, an open-source urban driving simulator. We introduce multiple non-player agents in CARLA and induce temporal noise in them for realistic stochasticity.



### iCAN: Instance-Centric Attention Network for Human-Object Interaction Detection
- **Arxiv ID**: http://arxiv.org/abs/1808.10437v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.10437v1)
- **Published**: 2018-08-30 17:58:33+00:00
- **Updated**: 2018-08-30 17:58:33+00:00
- **Authors**: Chen Gao, Yuliang Zou, Jia-Bin Huang
- **Comment**: BMVC 2018. Project webpage: https://gaochen315.github.io/iCAN/ Code:
  https://github.com/vt-vl-lab/iCAN
- **Journal**: None
- **Summary**: Recent years have witnessed rapid progress in detecting and recognizing individual object instances. To understand the situation in a scene, however, computers need to recognize how humans interact with surrounding objects. In this paper, we tackle the challenging task of detecting human-object interactions (HOI). Our core idea is that the appearance of a person or an object instance contains informative cues on which relevant parts of an image to attend to for facilitating interaction prediction. To exploit these cues, we propose an instance-centric attention module that learns to dynamically highlight regions in an image conditioned on the appearance of each instance. Such an attention-based network allows us to selectively aggregate features relevant for recognizing HOIs. We validate the efficacy of the proposed network on the Verb in COCO and HICO-DET datasets and show that our approach compares favorably with the state-of-the-arts.



### Total Recall: Understanding Traffic Signs using Deep Hierarchical Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1808.10524v2
- **DOI**: 10.1109/ICCITECHN.2018.8631925
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.10524v2)
- **Published**: 2018-08-30 21:13:02+00:00
- **Updated**: 2018-10-26 12:47:06+00:00
- **Authors**: Sourajit Saha, Sharif Amit Kamran, Ali Shihab Sabbir
- **Comment**: None
- **Journal**: None
- **Summary**: Recognizing Traffic Signs using intelligent systems can drastically reduce the number of accidents happening world-wide. With the arrival of Self-driving cars it has become a staple challenge to solve the automatic recognition of Traffic and Hand-held signs in the major streets. Various machine learning techniques like Random Forest, SVM as well as deep learning models has been proposed for classifying traffic signs. Though they reach state-of-the-art performance on a particular data-set, but fall short of tackling multiple Traffic Sign Recognition benchmarks. In this paper, we propose a novel and one-for-all architecture that aces multiple benchmarks with better overall score than the state-of-the-art architectures. Our model is made of residual convolutional blocks with hierarchical dilated skip connections joined in steps. With this we score 99.33% Accuracy in German sign recognition benchmark and 99.17% Accuracy in Belgian traffic sign classification benchmark. Moreover, we propose a newly devised dilated residual learning representation technique which is very low in both memory and computational complexity.



### Hallucinating Dense Optical Flow from Sparse Lidar for Autonomous Vehicles
- **Arxiv ID**: http://arxiv.org/abs/1808.10542v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.10542v1)
- **Published**: 2018-08-30 22:54:44+00:00
- **Updated**: 2018-08-30 22:54:44+00:00
- **Authors**: Victor Vaquero, Alberto Sanfeliu, Francesc Moreno-Noguer
- **Comment**: Accepted in ICPR 2018. More information: www.victorvaquero.me
- **Journal**: None
- **Summary**: In this paper we propose a novel approach to estimate dense optical flow from sparse lidar data acquired on an autonomous vehicle. This is intended to be used as a drop-in replacement of any image-based optical flow system when images are not reliable due to e.g. adverse weather conditions or at night. In order to infer high resolution 2D flows from discrete range data we devise a three-block architecture of multiscale filters that combines multiple intermediate objectives, both in the lidar and image domain. To train this network we introduce a dataset with approximately 20K lidar samples of the Kitti dataset which we have augmented with a pseudo ground-truth image-based optical flow computed using FlowNet2. We demonstrate the effectiveness of our approach on Kitti, and show that despite using the low-resolution and sparse measurements of the lidar, we can regress dense optical flow maps which are at par with those estimated with image-based methods.



### LUCSS: Language-based User-customized Colourization of Scene Sketches
- **Arxiv ID**: http://arxiv.org/abs/1808.10544v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.10544v1)
- **Published**: 2018-08-30 23:01:55+00:00
- **Updated**: 2018-08-30 23:01:55+00:00
- **Authors**: Changqing Zou, Haoran Mo, Ruofei Du, Xing Wu, Chengying Gao, Hongbo Fu
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce LUCSS, a language-based system for interactive col- orization of scene sketches, based on their semantic understanding. LUCSS is built upon deep neural networks trained via a large-scale repository of scene sketches and cartoon-style color images with text descriptions. It con- sists of three sequential modules. First, given a scene sketch, the segmenta- tion module automatically partitions an input sketch into individual object instances. Next, the captioning module generates the text description with spatial relationships based on the instance-level segmentation results. Fi- nally, the interactive colorization module allows users to edit the caption and produce colored images based on the altered caption. Our experiments show the effectiveness of our approach and the desirability of its compo- nents to alternative choices.



