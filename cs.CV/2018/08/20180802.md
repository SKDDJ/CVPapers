# Arxiv Papers in cs.CV on 2018-08-02
### Physics-Based Generative Adversarial Models for Image Restoration and Beyond
- **Arxiv ID**: http://arxiv.org/abs/1808.00605v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.00605v2)
- **Published**: 2018-08-02 00:12:33+00:00
- **Updated**: 2020-03-29 15:40:03+00:00
- **Authors**: Jinshan Pan, Jiangxin Dong, Yang Liu, Jiawei Zhang, Jimmy Ren, Jinhui Tang, Yu-Wing Tai, Ming-Hsuan Yang
- **Comment**: IEEE TPAMI
- **Journal**: None
- **Summary**: We present an algorithm to directly solve numerous image restoration problems (e.g., image deblurring, image dehazing, image deraining, etc.). These problems are highly ill-posed, and the common assumptions for existing methods are usually based on heuristic image priors. In this paper, we find that these problems can be solved by generative models with adversarial learning. However, the basic formulation of generative adversarial networks (GANs) does not generate realistic images, and some structures of the estimated images are usually not preserved well. Motivated by an interesting observation that the estimated results should be consistent with the observed inputs under the physics models, we propose a physics model constrained learning algorithm so that it can guide the estimation of the specific task in the conventional GAN framework. The proposed algorithm is trained in an end-to-end fashion and can be applied to a variety of image restoration and related low-level vision problems. Extensive experiments demonstrate that our method performs favorably against the state-of-the-art algorithms.



### Object Localization and Size Estimation from RGB-D Images
- **Arxiv ID**: http://arxiv.org/abs/1808.00641v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1808.00641v1)
- **Published**: 2018-08-02 02:35:02+00:00
- **Updated**: 2018-08-02 02:35:02+00:00
- **Authors**: ShreeRanjani SrirangamSridharan, Oytun Ulutan, Shehzad Noor Taus Priyo, Swati Rallapalli, Mudhakar Srivatsa
- **Comment**: None
- **Journal**: None
- **Summary**: Depth sensing cameras (e.g., Kinect sensor, Tango phone) can acquire color and depth images that are registered to a common viewpoint. This opens the possibility of developing algorithms that exploit the advantages of both sensing modalities. Traditionally, cues from color images have been used for object localization (e.g., YOLO). However, the addition of a depth image can be further used to segment images that might otherwise have identical color information. Further, the depth image can be used for object size (height/width) estimation (in real-world measurements units, such as meters) as opposed to image based segmentation that would only support drawing bounding boxes around objects of interest. In this paper, we first collect color camera information along with depth information using a custom Android application on Tango Phab2 phone. Second, we perform timing and spatial alignment between the two data sources. Finally, we evaluate several ways of measuring the height of the object of interest within the captured images under a variety of settings.



### Two-Layer Lossless HDR Coding using Histogram Packing Technique with Backward Compatibility to JPEG
- **Arxiv ID**: http://arxiv.org/abs/1808.00956v1
- **DOI**: 10.1587/transfun.E101.A.1823
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.00956v1)
- **Published**: 2018-08-02 03:02:48+00:00
- **Updated**: 2018-08-02 03:02:48+00:00
- **Authors**: Osamu Watanabe, Hiroyuki Kobayashi, Hitoshi Kiya
- **Comment**: To appear in IEICE Trans. Fundamentals, vol.E101-A, no.11, November
  2018. arXiv admin note: substantial text overlap with arXiv:1806.10746
- **Journal**: None
- **Summary**: An efficient two-layer coding method using the histogram packing technique with the backward compatibility to the legacy JPEG is proposed in this paper. The JPEG XT, which is the international standard to compress HDR images, adopts two-layer coding scheme for backward compatibility to the legacy JPEG. However, this two-layer coding structure does not give better lossless performance than the other existing methods for HDR image compression with single-layer structure. Moreover, the lossless compression of the JPEG XT has a problem on determination of the coding parameters; The lossless performance is affected by the input images and/or the parameter values. That is, finding appropriate combination of the values is necessary to achieve good lossless performance. It is firstly pointed out that the histogram packing technique considering the histogram sparseness of HDR images is able to improve the performance of lossless compression. Then, a novel two-layer coding with the histogram packing technique and an additional lossless encoder is proposed. The experimental results demonstrate that not only the proposed method has a better lossless compression performance than that of the JPEG XT, but also there is no need to determine image-dependent parameter values for good compression performance without losing the backward compatibility to the well known legacy JPEG standard.



### Adaptive Temporal Encoding Network for Video Instance-level Human Parsing
- **Arxiv ID**: http://arxiv.org/abs/1808.00661v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.00661v2)
- **Published**: 2018-08-02 04:24:36+00:00
- **Updated**: 2018-08-10 09:46:46+00:00
- **Authors**: Qixian Zhou, Xiaodan Liang, Ke Gong, Liang Lin
- **Comment**: To appear in ACM MM 2018. Code link:
  https://github.com/HCPLab-SYSU/ATEN. Dataset link: http://sysu-hcp.net/lip
- **Journal**: None
- **Summary**: Beyond the existing single-person and multiple-person human parsing tasks in static images, this paper makes the first attempt to investigate a more realistic video instance-level human parsing that simultaneously segments out each person instance and parses each instance into more fine-grained parts (e.g., head, leg, dress). We introduce a novel Adaptive Temporal Encoding Network (ATEN) that alternatively performs temporal encoding among key frames and flow-guided feature propagation from other consecutive frames between two key frames. Specifically, ATEN first incorporates a Parsing-RCNN to produce the instance-level parsing result for each key frame, which integrates both the global human parsing and instance-level human segmentation into a unified model. To balance between accuracy and efficiency, the flow-guided feature propagation is used to directly parse consecutive frames according to their identified temporal consistency with key frames. On the other hand, ATEN leverages the convolution gated recurrent units (convGRU) to exploit temporal changes over a series of key frames, which are further used to facilitate the frame-level instance-level parsing. By alternatively performing direct feature propagation between consistent frames and temporal encoding network among key frames, our ATEN achieves a good balance between frame-level accuracy and time efficiency, which is a common crucial problem in video object segmentation research. To demonstrate the superiority of our ATEN, extensive experiments are conducted on the most popular video segmentation benchmark (DAVIS) and a newly collected Video Instance-level Parsing (VIP) dataset, which is the first video instance-level human parsing dataset comprised of 404 sequences and over 20k frames with instance-level and pixel-wise annotations.



### PCN: Point Completion Network
- **Arxiv ID**: http://arxiv.org/abs/1808.00671v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1808.00671v3)
- **Published**: 2018-08-02 05:20:21+00:00
- **Updated**: 2019-09-26 18:56:43+00:00
- **Authors**: Wentao Yuan, Tejas Khot, David Held, Christoph Mertz, Martial Hebert
- **Comment**: 3DV 2018 oral. Honorable mention for Best Paper award
- **Journal**: None
- **Summary**: Shape completion, the problem of estimating the complete geometry of objects from partial observations, lies at the core of many vision and robotics applications. In this work, we propose Point Completion Network (PCN), a novel learning-based approach for shape completion. Unlike existing shape completion methods, PCN directly operates on raw point clouds without any structural assumption (e.g. symmetry) or annotation (e.g. semantic class) about the underlying shape. It features a decoder design that enables the generation of fine-grained completions while maintaining a small number of parameters. Our experiments show that PCN produces dense, complete point clouds with realistic structures in the missing regions on inputs with various levels of incompleteness and noise, including cars from LiDAR scans in the KITTI dataset.



### Double Supervised Network with Attention Mechanism for Scene Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/1808.00677v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1808.00677v3)
- **Published**: 2018-08-02 06:01:52+00:00
- **Updated**: 2019-10-22 13:05:11+00:00
- **Authors**: Yuting Gao, Zheng Huang, Yuchen Dai, Cheng Xu, Kai Chen, Jie Tuo
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose Double Supervised Network with Attention Mechanism (DSAN), a novel end-to-end trainable framework for scene text recognition. It incorporates one text attention module during feature extraction which enforces the model to focus on text regions and the whole framework is supervised by two branches. One supervision branch comes from context-level modelling and another comes from one extra supervision enhancement branch which aims at tackling inexplicit semantic information at character level. These two supervisions can benefit each other and yield better performance. The proposed approach can recognize text in arbitrary length and does not need any predefined lexicon. Our method outperforms the current state-of-the-art methods on three text recognition benchmarks: IIIT5K, ICDAR2013 and SVT reaching accuracy 88.6%, 92.3% and 84.1% respectively which suggests the effectiveness of the proposed method.



### Online Temporal Calibration for Monocular Visual-Inertial Systems
- **Arxiv ID**: http://arxiv.org/abs/1808.00692v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.00692v1)
- **Published**: 2018-08-02 07:16:25+00:00
- **Updated**: 2018-08-02 07:16:25+00:00
- **Authors**: Tong Qin, Shaojie Shen
- **Comment**: IROS 2018
- **Journal**: None
- **Summary**: Accurate state estimation is a fundamental module for various intelligent applications, such as robot navigation, autonomous driving, virtual and augmented reality. Visual and inertial fusion is a popular technology for 6-DOF state estimation in recent years. Time instants at which different sensors' measurements are recorded are of crucial importance to the system's robustness and accuracy. In practice, timestamps of each sensor typically suffer from triggering and transmission delays, leading to temporal misalignment (time offsets) among different sensors. Such temporal offset dramatically influences the performance of sensor fusion. To this end, we propose an online approach for calibrating temporal offset between visual and inertial measurements. Our approach achieves temporal offset calibration by jointly optimizing time offset, camera and IMU states, as well as feature locations in a SLAM system. Furthermore, the approach is a general model, which can be easily employed in several feature-based optimization frameworks. Simulation and experimental results demonstrate the high accuracy of our calibration approach even compared with other state-of-art offline tools. The VIO comparison against other methods proves that the online temporal calibration significantly benefits visual-inertial systems. The source code of temporal calibration is integrated into our public project, VINS-Mono.



### Acoustic Scene Classification: A Competition Review
- **Arxiv ID**: http://arxiv.org/abs/1808.02357v1
- **DOI**: None
- **Categories**: **eess.AS**, cs.CV, cs.LG, cs.SD, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1808.02357v1)
- **Published**: 2018-08-02 07:40:17+00:00
- **Updated**: 2018-08-02 07:40:17+00:00
- **Authors**: Shayan Gharib, Honain Derrar, Daisuke Niizumi, Tuukka Senttula, Janne Tommola, Toni Heittola, Tuomas Virtanen, Heikki Huttunen
- **Comment**: This work has been accepted in IEEE International Workshop on Machine
  Learning for Signal Processing (MLSP 2018). Copyright may be transferred
  without notice, after which this version may no longer be accessible
- **Journal**: None
- **Summary**: In this paper we study the problem of acoustic scene classification, i.e., categorization of audio sequences into mutually exclusive classes based on their spectral content. We describe the methods and results discovered during a competition organized in the context of a graduate machine learning course; both by the students and external participants. We identify the most suitable methods and study the impact of each by performing an ablation study of the mixture of approaches. We also compare the results with a neural network baseline, and show the improvement over that. Finally, we discuss the impact of using a competition as a part of a university course, and justify its importance in the curriculum based on student feedback.



### Attributes' Importance for Zero-Shot Pose-Classification Based on Wearable Sensors
- **Arxiv ID**: http://arxiv.org/abs/1808.01358v1
- **DOI**: 10.3390/s18082485
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1808.01358v1)
- **Published**: 2018-08-02 07:51:01+00:00
- **Updated**: 2018-08-02 07:51:01+00:00
- **Authors**: Hiroki Ohashi, Mohammad Al-Naser, Sheraz Ahmed, Katsuyuki Nakamura, Takuto Sato, Andreas Dengel
- **Comment**: The paper was published at Sensors, an open access journal
  (http://www.mdpi.com/1424-8220/18/8/2485). This article belongs to the
  Special Issue Artificial Intelligence and Machine Learning in Sensors
  Networks
- **Journal**: Sensors 2018, 18, 2485
- **Summary**: This paper presents a simple yet effective method for improving the performance of zero-shot learning (ZSL). ZSL classifies instances of unseen classes, from which no training data is available, by utilizing the attributes of the classes. Conventional ZSL methods have equally dealt with all the available attributes, but this sometimes causes misclassification. This is because an attribute that is effective for classifying instances of one class is not always effective for another class. In this case, a metric of classifying the latter class can be undesirably influenced by the irrelevant attribute. This paper solves this problem by taking the importance of each attribute for each class into account when calculating the metric. In addition to the proposal of this new method, this paper also contributes by providing a dataset for pose classification based on wearable sensors, named HDPoseDS. It contains 22 classes of poses performed by 10 subjects with 31 IMU sensors across full body. To the best of our knowledge, it is the richest wearable-sensor dataset especially in terms of sensor density, and thus it is suitable for studying zero-shot pose/action recognition. The presented method was evaluated on HDPoseDS and outperformed relative improvement of 5.9% in comparison to the best baseline method.



### Evaluating the Readability of Force Directed Graph Layouts: A Deep Learning Approach
- **Arxiv ID**: http://arxiv.org/abs/1808.00703v2
- **DOI**: 10.1109/MCG.2018.2881501
- **Categories**: **cs.CV**, cs.GR, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1808.00703v2)
- **Published**: 2018-08-02 07:57:59+00:00
- **Updated**: 2018-11-14 23:20:45+00:00
- **Authors**: Hammad Haleem, Yong Wang, Abishek Puri, Sahil Wadhwa, Huamin Qu
- **Comment**: This work has been accepted at IEEE CG&A
- **Journal**: None
- **Summary**: Existing graph layout algorithms are usually not able to optimize all the aesthetic properties desired in a graph layout. To evaluate how well the desired visual features are reflected in a graph layout, many readability metrics have been proposed in the past decades. However, the calculation of these readability metrics often requires access to the node and edge coordinates and is usually computationally inefficient, especially for dense graphs. Importantly, when the node and edge coordinates are not accessible, it becomes impossible to evaluate the graph layouts quantitatively. In this paper, we present a novel deep learning-based approach to evaluate the readability of graph layouts by directly using graph images. A convolutional neural network architecture is proposed and trained on a benchmark dataset of graph images, which is composed of synthetically-generated graphs and graphs created by sampling from real large networks. Multiple representative readability metrics (including edge crossing, node spread, and group overlap) are considered in the proposed approach. We quantitatively compare our approach to traditional methods and qualitatively evaluate our approach using a case study and visualizing convolutional layers. This work is a first step towards using deep learning based methods to evaluate images from the visualization field quantitatively.



### Dynamic Adaptation on Non-Stationary Visual Domains
- **Arxiv ID**: http://arxiv.org/abs/1808.00736v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.00736v1)
- **Published**: 2018-08-02 09:49:32+00:00
- **Updated**: 2018-08-02 09:49:32+00:00
- **Authors**: Sindi Shkodrani, Michael Hofmann, Efstratios Gavves
- **Comment**: None
- **Journal**: None
- **Summary**: Domain adaptation aims to learn models on a supervised source domain that perform well on an unsupervised target. Prior work has examined domain adaptation in the context of stationary domain shifts, i.e. static data sets. However, with large-scale or dynamic data sources, data from a defined domain is not usually available all at once. For instance, in a streaming data scenario, dataset statistics effectively become a function of time. We introduce a framework for adaptation over non-stationary distribution shifts applicable to large-scale and streaming data scenarios. The model is adapted sequentially over incoming unsupervised streaming data batches. This enables improvements over several batches without the need for any additionally annotated data. To demonstrate the effectiveness of our proposed framework, we modify associative domain adaptation to work well on source and target data batches with unequal class distributions. We apply our method to several adaptation benchmark datasets for classification and show improved classifier accuracy not only for the currently adapted batch, but also when applied on future stream batches. Furthermore, we show the applicability of our associative learning modifications to semantic segmentation, where we achieve competitive results.



### Deeply Self-Supervised Contour Embedded Neural Network Applied to Liver Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1808.00739v5
- **DOI**: 10.1016/j.cmpb.2020.105447
- **Categories**: **cs.CV**, 68U10
- **Links**: [PDF](http://arxiv.org/pdf/1808.00739v5)
- **Published**: 2018-08-02 09:53:11+00:00
- **Updated**: 2019-10-17 10:30:37+00:00
- **Authors**: Minyoung Chung, Jingyu Lee, Minkyung Lee, Jeongjin Lee, Yeong-Gil Shin
- **Comment**: 10 pages, 9 figures
- **Journal**: None
- **Summary**: Objective: Herein, a neural network-based liver segmentation algorithm is proposed, and its performance was evaluated using abdominal computed tomography (CT) images. Methods: A fully convolutional network was developed to overcome the volumetric image segmentation problem. To guide a neural network to accurately delineate a target liver object, the network was deeply supervised by applying the adaptive self-supervision scheme to derive the essential contour, which acted as a complement with the global shape. The discriminative contour, shape, and deep features were internally merged for the segmentation results. Results and Conclusion: 160 abdominal CT images were used for training and validation. The quantitative evaluation of the proposed network was performed through an eight-fold cross-validation. The result showed that the method, which uses the contour feature, segmented the liver more accurately than the state-of-the-art with a 2.13% improvement in the dice score. Significance: In this study, a new framework was introduced to guide a neural network and learn complementary contour features. The proposed neural network demonstrates that the guided contour features can significantly improve the performance of the segmentation task.



### Robust Attentional Aggregation of Deep Feature Sets for Multi-view 3D Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1808.00758v2
- **DOI**: 10.1007/s11263-019-01217-w
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1808.00758v2)
- **Published**: 2018-08-02 11:09:13+00:00
- **Updated**: 2019-08-18 06:32:40+00:00
- **Authors**: Bo Yang, Sen Wang, Andrew Markham, Niki Trigoni
- **Comment**: IJCV 2019. Code and data are available at
  https://github.com/Yang7879/AttSets
- **Journal**: None
- **Summary**: We study the problem of recovering an underlying 3D shape from a set of images. Existing learning based approaches usually resort to recurrent neural nets, e.g., GRU, or intuitive pooling operations, e.g., max/mean poolings, to fuse multiple deep features encoded from input images. However, GRU based approaches are unable to consistently estimate 3D shapes given different permutations of the same set of input images as the recurrent unit is permutation variant. It is also unlikely to refine the 3D shape given more images due to the long-term memory loss of GRU. Commonly used pooling approaches are limited to capturing partial information, e.g., max/mean values, ignoring other valuable features. In this paper, we present a new feed-forward neural module, named AttSets, together with a dedicated training algorithm, named FASet, to attentively aggregate an arbitrarily sized deep feature set for multi-view 3D reconstruction. The AttSets module is permutation invariant, computationally efficient and flexible to implement, while the FASet algorithm enables the AttSets based network to be remarkably robust and generalize to an arbitrary number of input images. We thoroughly evaluate FASet and the properties of AttSets on multiple large public datasets. Extensive experiments show that AttSets together with FASet algorithm significantly outperforms existing aggregation approaches.



### Sparse and Dense Data with CNNs: Depth Completion and Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1808.00769v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.00769v2)
- **Published**: 2018-08-02 11:59:45+00:00
- **Updated**: 2018-08-31 10:13:45+00:00
- **Authors**: Maximilian Jaritz, Raoul de Charette, Emilie Wirbel, Xavier Perrotton, Fawzi Nashashibi
- **Comment**: 3DV 2018
- **Journal**: None
- **Summary**: Convolutional neural networks are designed for dense data, but vision data is often sparse (stereo depth, point clouds, pen stroke, etc.). We present a method to handle sparse depth data with optional dense RGB, and accomplish depth completion and semantic segmentation changing only the last layer. Our proposal efficiently learns sparse features without the need of an additional validity mask. We show how to ensure network robustness to varying input sparsities. Our method even works with densities as low as 0.8% (8 layer lidar), and outperforms all published state-of-the-art on the Kitti depth completion benchmark.



### The Quest for the Golden Activation Function
- **Arxiv ID**: http://arxiv.org/abs/1808.00783v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1808.00783v1)
- **Published**: 2018-08-02 12:44:09+00:00
- **Updated**: 2018-08-02 12:44:09+00:00
- **Authors**: Mina Basirat, Peter M. Roth
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Neural Networks have been shown to be beneficial for a variety of tasks, in particular allowing for end-to-end learning and reducing the requirement for manual design decisions. However, still many parameters have to be chosen in advance, also raising the need to optimize them. One important, but often ignored system parameter is the selection of a proper activation function. Thus, in this paper we target to demonstrate the importance of activation functions in general and show that for different tasks different activation functions might be meaningful. To avoid the manual design or selection of activation functions, we build on the idea of genetic algorithms to learn the best activation function for a given task. In addition, we introduce two new activation functions, ELiSH and HardELiSH, which can easily be incorporated in our framework. In this way, we demonstrate for three different image classification benchmarks that different activation functions are learned, also showing improved results compared to typically used baselines.



### Weakly Supervised Localisation for Fetal Ultrasound Images
- **Arxiv ID**: http://arxiv.org/abs/1808.00793v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.00793v1)
- **Published**: 2018-08-02 13:02:37+00:00
- **Updated**: 2018-08-02 13:02:37+00:00
- **Authors**: Nicolas Toussaint, Bishesh Khanal, Matthew Sinclair, Alberto Gomez, Emily Skelton, Jacqueline Matthew, Julia A. Schnabel
- **Comment**: 4th Workshop on Deep Learning for Medical Image Analysis, MICCAI
  2018, Granada, Spain
- **Journal**: None
- **Summary**: This paper addresses the task of detecting and localising fetal anatomical regions in 2D ultrasound images, where only image-level labels are present at training, i.e. without any localisation or segmentation information. We examine the use of convolutional neural network architectures coupled with soft proposal layers. The resulting network simultaneously performs anatomical region detection (classification) and localisation tasks. We generate a proposal map describing the attention of the network for a particular class. The network is trained on 85,500 2D fetal Ultrasound images and their associated labels. Labels correspond to six anatomical regions: head, spine, thorax, abdomen, limbs, and placenta. Detection achieves an average accuracy of 90\% on individual regions, and show that the proposal maps correlate well with relevant anatomical structures. This work presents itself as a powerful and essential step towards subsequent tasks such as fetal position and pose estimation, organ-specific segmentation, or image-guided navigation. Code and additional material is available at https://ntoussaint.github.io/fetalnav



### RGB Video Based Tennis Action Recognition Using a Deep Historical Long Short-Term Memory
- **Arxiv ID**: http://arxiv.org/abs/1808.00845v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1808.00845v2)
- **Published**: 2018-08-02 14:58:51+00:00
- **Updated**: 2018-09-25 14:28:06+00:00
- **Authors**: Jiaxin Cai, Xin Tang
- **Comment**: None
- **Journal**: None
- **Summary**: Action recognition has attracted increasing attention from RGB input in computer vision partially due to potential applications on somatic simulation and statistics of sport such as virtual tennis game and tennis techniques and tactics analysis by video. Recently, deep learning based methods have achieved promising performance for action recognition. In this paper, we propose weighted Long Short-Term Memory adopted with convolutional neural network representations for three dimensional tennis shots recognition. First, the local two-dimensional convolutional neural network spatial representations are extracted from each video frame individually using a pre-trained Inception network. Then, a weighted Long Short-Term Memory decoder is introduced to take the output state at time t and the historical embedding feature at time t-1 to generate feature vector using a score weighting scheme. Finally, we use the adopted CNN and weighted LSTM to map the original visual features into a vector space to generate the spatial-temporal semantical description of visual sequences and classify the action video content. Experiments on the benchmark demonstrate that our method using only simple raw RGB video can achieve better performance than the state-of-the-art baselines for tennis shot recognition.



### Geometry-Based Multiple Camera Head Detection in Dense Crowds
- **Arxiv ID**: http://arxiv.org/abs/1808.00856v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.00856v1)
- **Published**: 2018-08-02 15:23:47+00:00
- **Updated**: 2018-08-02 15:23:47+00:00
- **Authors**: Nicola Pellicanò, Emanuel Aldea, Sylvie Le Hégarat-Mascle
- **Comment**: Proceedings of the 28th British Machine Vision Conference (BMVC) -
  5th Activity Monitoring by Multiple Distributed Sensing Workshop, 2017
- **Journal**: None
- **Summary**: This paper addresses the problem of head detection in crowded environments. Our detection is based entirely on the geometric consistency across cameras with overlapping fields of view, and no additional learning process is required. We propose a fully unsupervised method for inferring scene and camera geometry, in contrast to existing algorithms which require specific calibration procedures. Moreover, we avoid relying on the presence of body parts other than heads or on background subtraction, which have limited effectiveness under heavy clutter. We cast the head detection problem as a stereo MRF-based optimization of a dense pedestrian height map, and we introduce a constraint which aligns the height gradient according to the vertical vanishing point direction. We validate the method in an outdoor setting with varying pedestrian density levels. With only three views, our approach is able to detect simultaneously tens of heavily occluded pedestrians across a large, homogeneous area.



### Supervised classification for object identification in urban areas using satellite imagery
- **Arxiv ID**: http://arxiv.org/abs/1808.00878v1
- **DOI**: 10.1109/ICOMET.2018.8346383
- **Categories**: **cs.LG**, cs.CV, eess.SP, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1808.00878v1)
- **Published**: 2018-08-02 16:00:32+00:00
- **Updated**: 2018-08-02 16:00:32+00:00
- **Authors**: Hazrat Ali, Adnan Ali Awan, Sanaullah Khan, Omer Shafique, Atiq ur Rahman, Shahid Khan
- **Comment**: 2018 International Conference on Computing, Mathematics and
  Engineering Technologies (iCoMET)
- **Journal**: H. Ali et al., 2018 International Conference on Computing,
  Mathematics and Engineering Technologies (iCoMET), Sukkur, 2018, pp. 1-4
- **Summary**: This paper presents a useful method to achieve classification in satellite imagery. The approach is based on pixel level study employing various features such as correlation, homogeneity, energy and contrast. In this study gray-scale images are used for training the classification model. For supervised classification, two classification techniques are employed namely the Support Vector Machine (SVM) and the Naive Bayes. With textural features used for gray-scale images, Naive Bayes performs better with an overall accuracy of 76% compared to 68% achieved by SVM. The computational time is evaluated while performing the experiment with two different window sizes i.e., 50x50 and 70x70. The required computational time on a single image is found to be 27 seconds for a window size of 70x70 and 45 seconds for a window size of 50x50.



### BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1808.00897v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.00897v1)
- **Published**: 2018-08-02 16:34:01+00:00
- **Updated**: 2018-08-02 16:34:01+00:00
- **Authors**: Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao, Gang Yu, Nong Sang
- **Comment**: Accepted to ECCV 2018. 17 pages, 4 figures, 9 tables
- **Journal**: None
- **Summary**: Semantic segmentation requires both rich spatial information and sizeable receptive field. However, modern approaches usually compromise spatial resolution to achieve real-time inference speed, which leads to poor performance. In this paper, we address this dilemma with a novel Bilateral Segmentation Network (BiSeNet). We first design a Spatial Path with a small stride to preserve the spatial information and generate high-resolution features. Meanwhile, a Context Path with a fast downsampling strategy is employed to obtain sufficient receptive field. On top of the two paths, we introduce a new Feature Fusion Module to combine features efficiently. The proposed architecture makes a right balance between the speed and segmentation performance on Cityscapes, CamVid, and COCO-Stuff datasets. Specifically, for a 2048x1024 input, we achieve 68.4% Mean IOU on the Cityscapes test dataset with speed of 105 FPS on one NVIDIA Titan XP card, which is significantly faster than the existing methods with comparable performance.



### Learning Actionable Representations from Visual Observations
- **Arxiv ID**: http://arxiv.org/abs/1808.00928v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1808.00928v3)
- **Published**: 2018-08-02 17:24:54+00:00
- **Updated**: 2019-02-02 23:09:02+00:00
- **Authors**: Debidatta Dwibedi, Jonathan Tompson, Corey Lynch, Pierre Sermanet
- **Comment**: This work is accepted in IROS 2018. Project website:
  https://sites.google.com/view/actionablerepresentations
- **Journal**: None
- **Summary**: In this work we explore a new approach for robots to teach themselves about the world simply by observing it. In particular we investigate the effectiveness of learning task-agnostic representations for continuous control tasks. We extend Time-Contrastive Networks (TCN) that learn from visual observations by embedding multiple frames jointly in the embedding space as opposed to a single frame. We show that by doing so, we are now able to encode both position and velocity attributes significantly more accurately. We test the usefulness of this self-supervised approach in a reinforcement learning setting. We show that the representations learned by agents observing themselves take random actions, or other agents perform tasks successfully, can enable the learning of continuous control policies using algorithms like Proximal Policy Optimization (PPO) using only the learned embeddings as input. We also demonstrate significant improvements on the real-world Pouring dataset with a relative error reduction of 39.4% for motion attributes and 11.1% for static attributes compared to the single-frame baseline. Video results are available at https://sites.google.com/view/actionablerepresentations .



### Diverse Image-to-Image Translation via Disentangled Representations
- **Arxiv ID**: http://arxiv.org/abs/1808.00948v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.00948v1)
- **Published**: 2018-08-02 17:54:27+00:00
- **Updated**: 2018-08-02 17:54:27+00:00
- **Authors**: Hsin-Ying Lee, Hung-Yu Tseng, Jia-Bin Huang, Maneesh Kumar Singh, Ming-Hsuan Yang
- **Comment**: ECCV 2018 (Oral). Project page: http://vllab.ucmerced.edu/hylee/DRIT/
  Code: https://github.com/HsinYingLee/DRIT/
- **Journal**: None
- **Summary**: Image-to-image translation aims to learn the mapping between two visual domains. There are two main challenges for many applications: 1) the lack of aligned training pairs and 2) multiple possible outputs from a single input image. In this work, we present an approach based on disentangled representation for producing diverse outputs without paired training images. To achieve diversity, we propose to embed images onto two spaces: a domain-invariant content space capturing shared information across domains and a domain-specific attribute space. Our model takes the encoded content features extracted from a given input and the attribute vectors sampled from the attribute space to produce diverse outputs at test time. To handle unpaired training data, we introduce a novel cross-cycle consistency loss based on disentangled representations. Qualitative results show that our model can generate diverse and realistic images on a wide range of tasks without paired training data. For quantitative comparisons, we measure realism with user study and diversity with a perceptual distance metric. We apply the proposed model to domain adaptation and show competitive performance when compared to the state-of-the-art on the MNIST-M and the LineMod datasets.



### What Goes Where: Predicting Object Distributions from Above
- **Arxiv ID**: http://arxiv.org/abs/1808.00995v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.00995v1)
- **Published**: 2018-08-02 19:20:30+00:00
- **Updated**: 2018-08-02 19:20:30+00:00
- **Authors**: Connor Greenwell, Scott Workman, Nathan Jacobs
- **Comment**: 4 pages, 5 figures, IGARSS 2018
- **Journal**: None
- **Summary**: In this work, we propose a cross-view learning approach, in which images captured from a ground-level view are used as weakly supervised annotations for interpreting overhead imagery. The outcome is a convolutional neural network for overhead imagery that is capable of predicting the type and count of objects that are likely to be seen from a ground-level perspective. We demonstrate our approach on a large dataset of geotagged ground-level and overhead imagery and find that our network captures semantically meaningful features, despite being trained without manual annotations.



### A Data Dependent Multiscale Model for Hyperspectral Unmixing With Spectral Variability
- **Arxiv ID**: http://arxiv.org/abs/1808.01047v4
- **DOI**: 10.1109/TIP.2020.2963959
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.01047v4)
- **Published**: 2018-08-02 23:28:54+00:00
- **Updated**: 2020-01-22 14:51:36+00:00
- **Authors**: Ricardo Augusto Borsoi, Tales Imbiriba, José Carlos Moreira Bermudez
- **Comment**: None
- **Journal**: None
- **Summary**: Spectral variability in hyperspectral images can result from factors including environmental, illumination, atmospheric and temporal changes. Its occurrence may lead to the propagation of significant estimation errors in the unmixing process. To address this issue, extended linear mixing models have been proposed which lead to large scale nonsmooth ill-posed inverse problems. Furthermore, the regularization strategies used to obtain meaningful results have introduced interdependencies among abundance solutions that further increase the complexity of the resulting optimization problem. In this paper we present a novel data dependent multiscale model for hyperspectral unmixing accounting for spectral variability. The new method incorporates spatial contextual information to the abundances in extended linear mixing models by using a multiscale transform based on superpixels. The proposed method results in a fast algorithm that solves the abundance estimation problem only once in each scale during each iteration. Simulation results using synthetic and real images compare the performances, both in accuracy and execution time, of the proposed algorithm and other state-of-the-art solutions.



### Composition Loss for Counting, Density Map Estimation and Localization in Dense Crowds
- **Arxiv ID**: http://arxiv.org/abs/1808.01050v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.01050v1)
- **Published**: 2018-08-02 23:38:48+00:00
- **Updated**: 2018-08-02 23:38:48+00:00
- **Authors**: Haroon Idrees, Muhmmad Tayyab, Kishan Athrey, Dong Zhang, Somaya Al-Maadeed, Nasir Rajpoot, Mubarak Shah
- **Comment**: None
- **Journal**: ECCV 2018
- **Summary**: With multiple crowd gatherings of millions of people every year in events ranging from pilgrimages to protests, concerts to marathons, and festivals to funerals; visual crowd analysis is emerging as a new frontier in computer vision. In particular, counting in highly dense crowds is a challenging problem with far-reaching applicability in crowd safety and management, as well as gauging political significance of protests and demonstrations. In this paper, we propose a novel approach that simultaneously solves the problems of counting, density map estimation and localization of people in a given dense crowd image. Our formulation is based on an important observation that the three problems are inherently related to each other making the loss function for optimizing a deep CNN decomposable. Since localization requires high-quality images and annotations, we introduce UCF-QNRF dataset that overcomes the shortcomings of previous datasets, and contains 1.25 million humans manually marked with dot annotations. Finally, we present evaluation measures and comparison with recent deep CNN networks, including those developed specifically for crowd counting. Our approach significantly outperforms state-of-the-art on the new dataset, which is the most challenging dataset with the largest number of crowd annotations in the most diverse set of scenes.



