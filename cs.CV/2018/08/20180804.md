# Arxiv Papers in cs.CV on 2018-08-04
### Teacher Guided Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/1808.01405v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.01405v3)
- **Published**: 2018-08-04 01:43:03+00:00
- **Updated**: 2019-09-06 13:09:37+00:00
- **Authors**: Pouya Bashivan, Mark Tensen, James J DiCarlo
- **Comment**: Accepted to ICCV 2019
- **Journal**: None
- **Summary**: Much of the recent improvement in neural networks for computer vision has resulted from discovery of new networks architectures. Most prior work has used the performance of candidate models following limited training to automatically guide the search in a feasible way. Could further gains in computational efficiency be achieved by guiding the search via measurements of a high performing network with unknown detailed architecture (e.g. the primate visual system)? As one step toward this goal, we use representational similarity analysis to evaluate the similarity of internal activations of candidate networks with those of a (fixed, high performing) teacher network. We show that adopting this evaluation metric could produce up to an order of magnitude in search efficiency over performance-guided methods. Our approach finds a convolutional cell structure with similar performance as was previously found using other methods but at a total computational cost that is two orders of magnitude lower than Neural Architecture Search (NAS) and more than four times lower than progressive neural architecture search (PNAS). We further show that measurements from only ~300 neurons from primate visual system provides enough signal to find a network with an Imagenet top-1 error that is significantly lower than that achieved by performance-guided architecture search alone. These results suggest that representational matching can be used to accelerate network architecture search in cases where one has access to some or all of the internal representations of a teacher network of interest, such as the brain's sensory processing networks.



### On Lipschitz Bounds of General Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1808.01415v1
- **DOI**: None
- **Categories**: **cs.IT**, cs.CV, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/1808.01415v1)
- **Published**: 2018-08-04 03:21:47+00:00
- **Updated**: 2018-08-04 03:21:47+00:00
- **Authors**: Dongmian Zou, Radu Balan, Maneesh Singh
- **Comment**: 26 pages, 20 figures
- **Journal**: None
- **Summary**: Many convolutional neural networks (CNNs) have a feed-forward structure. In this paper, a linear program that estimates the Lipschitz bound of such CNNs is proposed. Several CNNs, including the scattering networks, the AlexNet and the GoogleNet, are studied numerically and compared to the theoretical bounds. Next, concentration inequalities of the output distribution to a stationary random input signal expressed in terms of the Lipschitz bound are established. The Lipschitz bound is further used to establish a nonlinear discriminant analysis designed to measure the separation between features of different classes.



### Language Model Supervision for Handwriting Recognition Model Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1808.01423v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.01423v1)
- **Published**: 2018-08-04 04:27:05+00:00
- **Updated**: 2018-08-04 04:27:05+00:00
- **Authors**: Chris Tensmeyer, Curtis Wigington, Brian Davis, Seth Stewart, Tony Martinez, William Barrett
- **Comment**: None
- **Journal**: None
- **Summary**: Training state-of-the-art offline handwriting recognition (HWR) models requires large labeled datasets, but unfortunately such datasets are not available in all languages and domains due to the high cost of manual labeling.We address this problem by showing how high resource languages can be leveraged to help train models for low resource languages.We propose a transfer learning methodology where we adapt HWR models trained on a source language to a target language that uses the same writing script.This methodology only requires labeled data in the source language, unlabeled data in the target language, and a language model of the target language. The language model is used in a bootstrapping fashion to refine predictions in the target language for use as ground truth in training the model.Using this approach we demonstrate improved transferability among French, English, and Spanish languages using both historical and modern handwriting datasets. In the best case, transferring with the proposed methodology results in character error rates nearly as good as full supervised training.



### Learning to Align Images using Weak Geometric Supervision
- **Arxiv ID**: http://arxiv.org/abs/1808.01424v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.01424v1)
- **Published**: 2018-08-04 04:28:52+00:00
- **Updated**: 2018-08-04 04:28:52+00:00
- **Authors**: Jing Dong, Byron Boots, Frank Dellaert, Ranveer Chandra, Sudipta N. Sinha
- **Comment**: Accepted in 3DV 2018
- **Journal**: None
- **Summary**: Image alignment tasks require accurate pixel correspondences, which are usually recovered by matching local feature descriptors. Such descriptors are often derived using supervised learning on existing datasets with ground truth correspondences. However, the cost of creating such datasets is usually prohibitive. In this paper, we propose a new approach to align two images related by an unknown 2D homography where the local descriptor is learned from scratch from the images and the homography is estimated simultaneously. Our key insight is that a siamese convolutional neural network can be trained jointly while iteratively updating the homography parameters by optimizing a single loss function. Our method is currently weakly supervised because the input images need to be roughly aligned.   We have used this method to align images of different modalities such as RGB and near-infra-red (NIR) without using any prior labeled data. Images automatically aligned by our method were then used to train descriptors that generalize to new images. We also evaluated our method on RGB images. On the HPatches benchmark, our method achieves comparable accuracy to deep local descriptors that were trained offline in a supervised setting.



### Structure-Aware Shape Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1808.01427v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1808.01427v1)
- **Published**: 2018-08-04 05:15:49+00:00
- **Updated**: 2018-08-04 05:15:49+00:00
- **Authors**: Elena Balashova, Vivek Singh, Jiangping Wang, Brian Teixeira, Terrence Chen, Thomas Funkhouser
- **Comment**: Accepted to 3DV 2018
- **Journal**: None
- **Summary**: We propose a new procedure to guide training of a data-driven shape generative model using a structure-aware loss function. Complex 3D shapes often can be summarized using a coarsely defined structure which is consistent and robust across variety of observations. However, existing synthesis techniques do not account for structure during training, and thus often generate implausible and structurally unrealistic shapes. During training, we enforce structural constraints in order to enforce consistency and structure across the entire manifold. We propose a novel methodology for training 3D generative models that incorporates structural information into an end-to-end training pipeline.



### Traits & Transferability of Adversarial Examples against Instance Segmentation & Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1808.01452v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.01452v1)
- **Published**: 2018-08-04 08:57:58+00:00
- **Updated**: 2018-08-04 08:57:58+00:00
- **Authors**: Raghav Gurbaxani, Shivank Mishra
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: Despite the recent advancements in deploying neural networks for image classification, it has been found that adversarial examples are able to fool these models leading them to misclassify the images. Since these models are now being widely deployed, we provide an insight on the threat of these adversarial examples by evaluating their characteristics and transferability to more complex models that utilize Image Classification as a subtask.   We demonstrate the ineffectiveness of adversarial examples when applied to Instance Segmentation & Object Detection models. We show that this ineffectiveness arises from the inability of adversarial examples to withstand transformations such as scaling or a change in lighting conditions. Moreover, we show that there exists a small threshold below which the adversarial property is retained while applying these input transformations.   Additionally, these attacks demonstrate weak cross-network transferability across neural network architectures, e.g. VGG16 and ResNet50, however, the attack may fool both the networks if passed sequentially through networks during its formation.   The lack of scalability and transferability challenges the question of how adversarial images would be effective in the real world.



### T2Net: Synthetic-to-Realistic Translation for Solving Single-Image Depth Estimation Tasks
- **Arxiv ID**: http://arxiv.org/abs/1808.01454v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.01454v1)
- **Published**: 2018-08-04 09:10:14+00:00
- **Updated**: 2018-08-04 09:10:14+00:00
- **Authors**: Chuanxia Zheng, Tat-Jen Cham, Jianfei Cai
- **Comment**: 15 pages, 8 figures
- **Journal**: None
- **Summary**: Current methods for single-image depth estimation use training datasets with real image-depth pairs or stereo pairs, which are not easy to acquire. We propose a framework, trained on synthetic image-depth pairs and unpaired real images, that comprises an image translation network for enhancing realism of input images, followed by a depth prediction network. A key idea is having the first network act as a wide-spectrum input translator, taking in either synthetic or real images, and ideally producing minimally modified realistic images. This is done via a reconstruction loss when the training input is real, and GAN loss when synthetic, removing the need for heuristic self-regularization. The second network is trained on a task loss for synthetic image-depth pairs, with extra GAN loss to unify real and synthetic feature distributions. Importantly, the framework can be trained end-to-end, leading to good results, even surpassing early deep-learning methods that use real paired data.



### A survey on Deep Learning Advances on Different 3D Data Representations
- **Arxiv ID**: http://arxiv.org/abs/1808.01462v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.01462v2)
- **Published**: 2018-08-04 10:18:55+00:00
- **Updated**: 2019-04-06 18:20:17+00:00
- **Authors**: Eman Ahmed, Alexandre Saint, Abd El Rahman Shabayek, Kseniya Cherenkova, Rig Das, Gleb Gusev, Djamila Aouada, Bjorn Ottersten
- **Comment**: 35 pages
- **Journal**: None
- **Summary**: 3D data is a valuable asset the computer vision filed as it provides rich information about the full geometry of sensed objects and scenes. Recently, with the availability of both large 3D datasets and computational power, it is today possible to consider applying deep learning to learn specific tasks on 3D data such as segmentation, recognition and correspondence. Depending on the considered 3D data representation, different challenges may be foreseen in using existent deep learning architectures. In this work, we provide a comprehensive overview about various 3D data representations highlighting the difference between Euclidean and non-Euclidean ones. We also discuss how Deep Learning methods are applied on each representation, analyzing the challenges to overcome.



### Troy: Give Attention to Saliency and for Saliency
- **Arxiv ID**: http://arxiv.org/abs/1808.02373v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.02373v2)
- **Published**: 2018-08-04 11:57:27+00:00
- **Updated**: 2018-08-14 01:59:29+00:00
- **Authors**: Pingping Zhang, Huchuan Lu, Chunhua Shen
- **Comment**: All of authors agree to withdrawal this paper because we have noticed
  several important errors
- **Journal**: None
- **Summary**: In addition, our work has text overlap with arXiv:1804.06242, arXiv:1705.00938 by other authors. We want to rewrite this paper for avoiding this fact.



### Learning Multi-scale Features for Foreground Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1808.01477v1
- **DOI**: 10.1007/s10044-019-00845-9
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.01477v1)
- **Published**: 2018-08-04 12:55:25+00:00
- **Updated**: 2018-08-04 12:55:25+00:00
- **Authors**: Long Ang Lim, Hacer Yalim Keles
- **Comment**: None
- **Journal**: None
- **Summary**: Foreground segmentation algorithms aim segmenting moving objects from the background in a robust way under various challenging scenarios. Encoder-decoder type deep neural networks that are used in this domain recently perform impressive segmentation results. In this work, we propose a novel robust encoder-decoder structure neural network that can be trained end-to-end using only a few training examples. The proposed method extends the Feature Pooling Module (FPM) of FgSegNet by introducing features fusions inside this module, which is capable of extracting multi-scale features within images; resulting in a robust feature pooling against camera motion, which can alleviate the need of multi-scale inputs to the network. Our method outperforms all existing state-of-the-art methods in CDnet2014 dataset by an average overall F-Measure of 0.9847. We also evaluate the effectiveness of our method on SBI2015 and UCSD Background Subtraction datasets. The source code of the proposed method is made available at https://github.com/lim-anggun/FgSegNet_v2 .



### Non-locally Enhanced Encoder-Decoder Network for Single Image De-raining
- **Arxiv ID**: http://arxiv.org/abs/1808.01491v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.01491v1)
- **Published**: 2018-08-04 15:09:01+00:00
- **Updated**: 2018-08-04 15:09:01+00:00
- **Authors**: Guanbin Li, Xiang He, Wei Zhang, Huiyou Chang, Le Dong, Liang Lin
- **Comment**: Accepted to ACM Multimedia 2018
- **Journal**: None
- **Summary**: Single image rain streaks removal has recently witnessed substantial progress due to the development of deep convolutional neural networks. However, existing deep learning based methods either focus on the entrance and exit of the network by decomposing the input image into high and low frequency information and employing residual learning to reduce the mapping range, or focus on the introduction of cascaded learning scheme to decompose the task of rain streaks removal into multi-stages. These methods treat the convolutional neural network as an encapsulated end-to-end mapping module without deepening into the rationality and superiority of neural network design. In this paper, we delve into an effective end-to-end neural network structure for stronger feature expression and spatial correlation learning. Specifically, we propose a non-locally enhanced encoder-decoder network framework, which consists of a pooling indices embedded encoder-decoder network to efficiently learn increasingly abstract feature representation for more accurate rain streaks modeling while perfectly preserving the image detail. The proposed encoder-decoder framework is composed of a series of non-locally enhanced dense blocks that are designed to not only fully exploit hierarchical features from all the convolutional layers but also well capture the long-distance dependencies and structural information. Extensive experiments on synthetic and real datasets demonstrate that the proposed method can effectively remove rain-streaks on rainy image of various densities while well preserving the image details, which achieves significant improvements over the recent state-of-the-art methods.



### DELIMIT PyTorch - An extension for Deep Learning in Diffusion Imaging
- **Arxiv ID**: http://arxiv.org/abs/1808.01517v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1808.01517v1)
- **Published**: 2018-08-04 18:26:24+00:00
- **Updated**: 2018-08-04 18:26:24+00:00
- **Authors**: Simon Koppers, Dorit Merhof
- **Comment**: None
- **Journal**: None
- **Summary**: DELIMIT is a framework extension for deep learning in diffusion imaging, which extends the basic framework PyTorch towards spherical signals. Based on several novel layers, deep learning can be applied to spherical diffusion imaging data in a very convenient way. First, two spherical harmonic interpolation layers are added to the extension, which allow to transform the signal from spherical surface space into the spherical harmonic space, and vice versa. In addition, a local spherical convolution layer is introduced that adds the possibility to include gradient neighborhood information within the network. Furthermore, these extensions can also be utilized for the preprocessing of diffusion signals.



### Rethinking Pose in 3D: Multi-stage Refinement and Recovery for Markerless Motion Capture
- **Arxiv ID**: http://arxiv.org/abs/1808.01525v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.01525v1)
- **Published**: 2018-08-04 19:53:30+00:00
- **Updated**: 2018-08-04 19:53:30+00:00
- **Authors**: Denis Tome, Matteo Toso, Lourdes Agapito, Chris Russell
- **Comment**: International Conference on 3DVision (3dv)
- **Journal**: None
- **Summary**: We propose a CNN-based approach for multi-camera markerless motion capture of the human body. Unlike existing methods that first perform pose estimation on individual cameras and generate 3D models as post-processing, our approach makes use of 3D reasoning throughout a multi-stage approach. This novelty allows us to use provisional 3D models of human pose to rethink where the joints should be located in the image and to recover from past mistakes. Our principled refinement of 3D human poses lets us make use of image cues, even from images where we previously misdetected joints, to refine our estimates as part of an end-to-end approach. Finally, we demonstrate how the high-quality output of our multi-camera setup can be used as an additional training source to improve the accuracy of existing single camera models.



