# Arxiv Papers in cs.CV on 2018-08-12
### Semi-supervised Skin Lesion Segmentation via Transformation Consistent Self-ensembling Model
- **Arxiv ID**: http://arxiv.org/abs/1808.03887v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.03887v1)
- **Published**: 2018-08-12 03:57:29+00:00
- **Updated**: 2018-08-12 03:57:29+00:00
- **Authors**: Xiaomeng Li, Lequan Yu, Hao Chen, Chi-Wing Fu, Pheng-Ann Heng
- **Comment**: BMVC 2018
- **Journal**: None
- **Summary**: Automatic skin lesion segmentation on dermoscopic images is an essential component in computer-aided diagnosis of melanoma. Recently, many fully supervised deep learning based methods have been proposed for automatic skin lesion segmentation. However, these approaches require massive pixel-wise annotation from experienced dermatologists, which is very costly and time-consuming. In this paper, we present a novel semi-supervised method for skin lesion segmentation by leveraging both labeled and unlabeled data. The network is optimized by the weighted combination of a common supervised loss for labeled inputs only and a regularization loss for both labeled and unlabeled data. In this paper, we present a novel semi-supervised method for skin lesion segmentation, where the network is optimized by the weighted combination of a common supervised loss for labeled inputs only and a regularization loss for both labeled and unlabeled data. Our method encourages a consistent prediction for unlabeled images using the outputs of the network-in-training under different regularizations, so that it can utilize the unlabeled data. To utilize the unlabeled data, our method encourages the consistent predictions of the network-in-training for the same input under different regularizations. Aiming for the semi-supervised segmentation problem, we enhance the effect of regularization for pixel-level predictions by introducing a transformation, including rotation and flipping, consistent scheme in our self-ensembling model. With only 300 labeled training samples, our method sets a new record on the benchmark of the International Skin Imaging Collaboration (ISIC) 2017 skin lesion segmentation challenge. Such a result clearly surpasses fully-supervised state-of-the-arts that are trained with 2000 labeled data.



### Iterative Global Similarity Points : A robust coarse-to-fine integration solution for pairwise 3D point cloud registration
- **Arxiv ID**: http://arxiv.org/abs/1808.03899v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.03899v1)
- **Published**: 2018-08-12 06:19:37+00:00
- **Updated**: 2018-08-12 06:19:37+00:00
- **Authors**: Yue Pan, Bisheng Yang, Fuxun Liang, Zhen Dong
- **Comment**: Accepted to International Conference on 3DVision (3DV) 2018 [8 pages,
  6 figures and 3 tables]
- **Journal**: None
- **Summary**: In this paper, we propose a coarse-to-fine integration solution inspired by the classical ICP algorithm, to pairwise 3D point cloud registration with two improvements of hybrid metric spaces (eg, BSC feature and Euclidean geometry spaces) and globally optimal correspondences matching. First, we detect the keypoints of point clouds and use the Binary Shape Context (BSC) descriptor to encode their local features. Then, we formulate the correspondence matching task as an energy function, which models the global similarity of keypoints on the hybrid spaces of BSC feature and Euclidean geometry. Next, we estimate the globally optimal correspondences through optimizing the energy function by the Kuhn-Munkres algorithm and then calculate the transformation based on the correspondences. Finally,we iteratively refine the transformation between two point clouds by conducting optimal correspondences matching and transformation calculation in a mutually reinforcing manner, to achieve the coarse-to-fine registration under an unified framework.The proposed method is evaluated and compared to several state-of-the-art methods on selected challenging datasets with repetitive, symmetric and incomplete structures.Comprehensive experiments demonstrate that the proposed IGSP algorithm obtains good performance and outperforms the state-of-the-art methods in terms of both rotation and translation errors.



### Multimodal Local-Global Ranking Fusion for Emotion Recognition
- **Arxiv ID**: http://arxiv.org/abs/1809.04931v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1809.04931v1)
- **Published**: 2018-08-12 09:44:01+00:00
- **Updated**: 2018-08-12 09:44:01+00:00
- **Authors**: Paul Pu Liang, Amir Zadeh, Louis-Philippe Morency
- **Comment**: ACM International Conference on Multimodal Interaction (ICMI 2018)
- **Journal**: None
- **Summary**: Emotion recognition is a core research area at the intersection of artificial intelligence and human communication analysis. It is a significant technical challenge since humans display their emotions through complex idiosyncratic combinations of the language, visual and acoustic modalities. In contrast to traditional multimodal fusion techniques, we approach emotion recognition from both direct person-independent and relative person-dependent perspectives. The direct person-independent perspective follows the conventional emotion recognition approach which directly infers absolute emotion labels from observed multimodal features. The relative person-dependent perspective approaches emotion recognition in a relative manner by comparing partial video segments to determine if there was an increase or decrease in emotional intensity. Our proposed model integrates these direct and relative prediction perspectives by dividing the emotion recognition task into three easier subtasks. The first subtask involves a multimodal local ranking of relative emotion intensities between two short segments of a video. The second subtask uses local rankings to infer global relative emotion ranks with a Bayesian ranking algorithm. The third subtask incorporates both direct predictions from observed multimodal behaviors and relative emotion ranks from local-global rankings for final emotion prediction. Our approach displays excellent performance on an audio-visual emotion recognition benchmark and improves over other algorithms for multimodal fusion.



### Fine-grained visual recognition with salient feature detection
- **Arxiv ID**: http://arxiv.org/abs/1808.03935v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.03935v2)
- **Published**: 2018-08-12 13:05:52+00:00
- **Updated**: 2018-08-14 01:53:20+00:00
- **Authors**: Hui Feng, Shanshan Wang, Shuzhi Sam Ge
- **Comment**: 10 pages, 6 figures, 5 tables
- **Journal**: None
- **Summary**: Computer vision based fine-grained recognition has received great attention in recent years. Existing works focus on discriminative part localization and feature learning. In this paper, to improve the performance of fine-grained recognition, we try to precisely locate as many salient parts of object as possible at first. Then, we figure out the classification probability that can be obtained by using separate parts for object classification. Finally, through extracting efficient features from each part and combining them, then feeding to a classifier for recognition, an improved accuracy over state-of-art algorithms has been obtained on CUB200-2011 bird dataset.



### Denoising of 3-D Magnetic Resonance Images Using a Residual Encoder-Decoder Wasserstein Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/1808.03941v2
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1808.03941v2)
- **Published**: 2018-08-12 13:30:27+00:00
- **Updated**: 2019-05-05 03:42:19+00:00
- **Authors**: Maosong Ran, Jinrong Hu, Yang Chen, Hu Chen, Huaiqiang Sun, Jiliu Zhou, Yi Zhang
- **Comment**: To appear on Medical Image Analysis. 29 pages, 15 figures, 7 tables
- **Journal**: None
- **Summary**: Structure-preserved denoising of 3D magnetic resonance imaging (MRI) images is a critical step in medical image analysis. Over the past few years, many algorithms with impressive performances have been proposed. In this paper, inspired by the idea of deep learning, we introduce an MRI denoising method based on the residual encoder-decoder Wasserstein generative adversarial network (RED-WGAN). Specifically, to explore the structure similarity between neighboring slices, a 3D configuration is utilized as the basic processing unit. Residual autoencoders combined with deconvolution operations are introduced into the generator network. Furthermore, to alleviate the oversmoothing shortcoming of the traditional mean squared error (MSE) loss function, the perceptual similarity, which is implemented by calculating the distances in the feature space extracted by a pretrained VGG-19 network, is incorporated with the MSE and adversarial losses to form the new loss function. Extensive experiments are implemented to assess the performance of the proposed method. The experimental results show that the proposed RED-WGAN achieves performance superior to several state-of-the-art methods in both simulated and real clinical data. In particular, our method demonstrates powerful abilities in both noise suppression and structure preservation.



### Unsupervised learning for cross-domain medical image synthesis using deformation invariant cycle consistency networks
- **Arxiv ID**: http://arxiv.org/abs/1808.03944v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1808.03944v1)
- **Published**: 2018-08-12 13:49:19+00:00
- **Updated**: 2018-08-12 13:49:19+00:00
- **Authors**: Chengjia Wang, Gillian Macnaught, Giorgos Papanastasiou, Tom MacGillivray, David Newby
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, the cycle-consistent generative adversarial networks (CycleGAN) has been widely used for synthesis of multi-domain medical images. The domain-specific nonlinear deformations captured by CycleGAN make the synthesized images difficult to be used for some applications, for example, generating pseudo-CT for PET-MR attenuation correction. This paper presents a deformation-invariant CycleGAN (DicycleGAN) method using deformable convolutional layers and new cycle-consistency losses. Its robustness dealing with data that suffer from domain-specific nonlinear deformations has been evaluated through comparison experiments performed on a multi-sequence brain MR dataset and a multi-modality abdominal dataset. Our method has displayed its ability to generate synthesized data that is aligned with the source while maintaining a proper quality of signal compared to CycleGAN-generated data. The proposed model also obtained comparable performance with CycleGAN when data from the source and target domains are alignable through simple affine transformations.



### Open-World Stereo Video Matching with Deep RNN
- **Arxiv ID**: http://arxiv.org/abs/1808.03959v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.03959v1)
- **Published**: 2018-08-12 15:41:54+00:00
- **Updated**: 2018-08-12 15:41:54+00:00
- **Authors**: Yiran Zhong, Hongdong Li, Yuchao Dai
- **Comment**: Accepted by European Conference on Computer Vision (ECCV) 2018
- **Journal**: None
- **Summary**: Deep Learning based stereo matching methods have shown great successes and achieved top scores across different benchmarks. However, like most data-driven methods, existing deep stereo matching networks suffer from some well-known drawbacks such as requiring large amount of labeled training data, and that their performances are fundamentally limited by the generalization ability. In this paper, we propose a novel Recurrent Neural Network (RNN) that takes a continuous (possibly previously unseen) stereo video as input, and directly predicts a depth-map at each frame without a pre-training process, and without the need of ground-truth depth-maps as supervision. Thanks to the recurrent nature (provided by two convolutional-LSTM blocks), our network is able to memorize and learn from its past experiences, and modify its inner parameters (network weights) to adapt to previously unseen or unfamiliar environments. This suggests a remarkable generalization ability of the net, making it applicable in an {\em open world} setting. Our method works robustly with changes in scene content, image statistics, and lighting and season conditions {\em etc}. By extensive experiments, we demonstrate that the proposed method seamlessly adapts between different scenarios. Equally important, in terms of the stereo matching accuracy, it outperforms state-of-the-art deep stereo approaches on standard benchmark datasets such as KITTI and Middlebury stereo.



### Reconfigurable Inverted Index
- **Arxiv ID**: http://arxiv.org/abs/1808.03969v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1808.03969v1)
- **Published**: 2018-08-12 16:47:47+00:00
- **Updated**: 2018-08-12 16:47:47+00:00
- **Authors**: Yusuke Matsui, Ryota Hinami, Shin'ichi Satoh
- **Comment**: ACMMM 2018 (oral). Code: https://github.com/matsui528/rii
- **Journal**: None
- **Summary**: Existing approximate nearest neighbor search systems suffer from two fundamental problems that are of practical importance but have not received sufficient attention from the research community. First, although existing systems perform well for the whole database, it is difficult to run a search over a subset of the database. Second, there has been no discussion concerning the performance decrement after many items have been newly added to a system. We develop a reconfigurable inverted index (Rii) to resolve these two issues. Based on the standard IVFADC system, we design a data layout such that items are stored linearly. This enables us to efficiently run a subset search by switching the search method to a linear PQ scan if the size of a subset is small. Owing to the linear layout, the data structure can be dynamically adjusted after new items are added, maintaining the fast speed of the system. Extensive comparisons show that Rii achieves a comparable performance with state-of-the art systems such as Faiss.



### Multimodal Differential Network for Visual Question Generation
- **Arxiv ID**: http://arxiv.org/abs/1808.03986v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1808.03986v2)
- **Published**: 2018-08-12 18:56:56+00:00
- **Updated**: 2019-10-17 10:23:19+00:00
- **Authors**: Badri N. Patro, Sandeep Kumar, Vinod K. Kurmi, Vinay P. Namboodiri
- **Comment**: EMNLP 2018 (accepted)
- **Journal**: None
- **Summary**: Generating natural questions from an image is a semantic task that requires using visual and language modality to learn multimodal representations. Images can have multiple visual and language contexts that are relevant for generating questions namely places, captions, and tags. In this paper, we propose the use of exemplars for obtaining the relevant context. We obtain this by using a Multimodal Differential Network to produce natural and engaging questions. The generated questions show a remarkable similarity to the natural questions as validated by a human study. Further, we observe that the proposed approach substantially improves over state-of-the-art benchmarks on the quantitative metrics (BLEU, METEOR, ROUGE, and CIDEr).



### Language Guided Fashion Image Manipulation with Feature-wise Transformations
- **Arxiv ID**: http://arxiv.org/abs/1808.04000v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.04000v1)
- **Published**: 2018-08-12 20:18:41+00:00
- **Updated**: 2018-08-12 20:18:41+00:00
- **Authors**: Mehmet Günel, Erkut Erdem, Aykut Erdem
- **Comment**: Accepted to ECCV 2018, First Workshop on Computer Vision For Fashion,
  Art and Design (extended version)
- **Journal**: None
- **Summary**: Developing techniques for editing an outfit image through natural sentences and accordingly generating new outfits has promising applications for art, fashion and design. However, it is considered as a certainly challenging task since image manipulation should be carried out only on the relevant parts of the image while keeping the remaining sections untouched. Moreover, this manipulation process should generate an image that is as realistic as possible. In this work, we propose FiLMedGAN, which leverages feature-wise linear modulation (FiLM) to relate and transform visual features with natural language representations without using extra spatial information. Our experiments demonstrate that this approach, when combined with skip connections and total variation regularization, produces more plausible results than the baseline work, and has a better localization capability when generating new outfits consistent with the target description.



### Scene-LSTM: A Model for Human Trajectory Prediction
- **Arxiv ID**: http://arxiv.org/abs/1808.04018v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.04018v2)
- **Published**: 2018-08-12 23:19:36+00:00
- **Updated**: 2019-04-15 16:10:52+00:00
- **Authors**: Huynh Manh, Gita Alaghband
- **Comment**: 9 pages, 5 figures
- **Journal**: None
- **Summary**: We develop a human movement trajectory prediction system that incorporates the scene information (Scene-LSTM) as well as human movement trajectories (Pedestrian movement LSTM) in the prediction process within static crowded scenes. We superimpose a two-level grid structure (scene is divided into grid cells each modeled by a scene-LSTM, which are further divided into smaller sub-grids for finer spatial granularity) and explore common human trajectories occurring in the grid cell (e.g., making a right or left turn onto sidewalks coming out of an alley; or standing still at bus/train stops). Two coupled LSTM networks, Pedestrian movement LSTMs (one per target) and the corresponding Scene-LSTMs (one per grid-cell) are trained simultaneously to predict the next movements. We show that such common path information greatly influences prediction of future movement. We further design a scene data filter that holds important non-linear movement information. The scene data filter allows us to select the relevant parts of the information from the grid cell's memory relative to a target's state. We evaluate and compare two versions of our method with the Linear and several existing LSTM-based methods on five crowded video sequences from the UCY [1] and ETH [2] datasets. The results show that our method reduces the location displacement errors compared to related methods and specifically about 80% reduction compared to social interaction methods.



