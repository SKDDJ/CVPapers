# Arxiv Papers in cs.CV on 2018-08-09
### Controllable Image-to-Video Translation: A Case Study on Facial Expression Generation
- **Arxiv ID**: http://arxiv.org/abs/1808.02992v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.02992v1)
- **Published**: 2018-08-09 01:38:01+00:00
- **Updated**: 2018-08-09 01:38:01+00:00
- **Authors**: Lijie Fan, Wenbing Huang, Chuang Gan, Junzhou Huang, Boqing Gong
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: The recent advances in deep learning have made it possible to generate photo-realistic images by using neural networks and even to extrapolate video frames from an input video clip. In this paper, for the sake of both furthering this exploration and our own interest in a realistic application, we study image-to-video translation and particularly focus on the videos of facial expressions. This problem challenges the deep neural networks by another temporal dimension comparing to the image-to-image translation. Moreover, its single input image fails most existing video generation methods that rely on recurrent models. We propose a user-controllable approach so as to generate video clips of various lengths from a single face image. The lengths and types of the expressions are controlled by users. To this end, we design a novel neural network architecture that can incorporate the user input into its skip connections and propose several improvements to the adversarial training method for the neural network. Experiments and user studies verify the effectiveness of our approach. Especially, we would like to highlight that even for the face images in the wild (downloaded from the Web and the authors' own photos), our model can generate high-quality facial expression videos of which about 50\% are labeled as real by Amazon Mechanical Turk workers.



### Object Detection in Satellite Imagery using 2-Step Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1808.02996v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.02996v1)
- **Published**: 2018-08-09 02:12:43+00:00
- **Updated**: 2018-08-09 02:12:43+00:00
- **Authors**: Hiroki Miyamoto, Kazuki Uehara, Masahiro Murakawa, Hidenori Sakanashi, Hirokazu Nosato, Toru Kouyama, Ryosuke Nakamura
- **Comment**: 4 pages,5 figures
- **Journal**: None
- **Summary**: This paper presents an efficient object detection method from satellite imagery. Among a number of machine learning algorithms, we proposed a combination of two convolutional neural networks (CNN) aimed at high precision and high recall, respectively. We validated our models using golf courses as target objects. The proposed deep learning method demonstrated higher accuracy than previous object identification methods.



### An Iterative Boundary Random Walks Algorithm for Interactive Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1808.03002v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.03002v1)
- **Published**: 2018-08-09 02:50:59+00:00
- **Updated**: 2018-08-09 02:50:59+00:00
- **Authors**: Xiaofeng Xie, ZhuLiang Yu, Zhenghui Gu, Yuanqing Li
- **Comment**: 9 pages, 9 figures, 1 tabel. Project
  URL:https://xiaofengxie.github.io/project/my-project-name/
- **Journal**: None
- **Summary**: The interactive image segmentation algorithm can provide an intelligent ways to understand the intention of user input. Many interactive methods have the problem of that ask for large number of user input. To efficient produce intuitive segmentation under limited user input is important for industrial application. In this paper, we reveal a positive feedback system on image segmentation to show the pixels of self-learning. Two approaches, iterative random walks and boundary random walks, are proposed for segmentation potential, which is the key step in feedback system. Experiment results on image segmentation indicates that proposed algorithms can obtain more efficient input to random walks. And higher segmentation performance can be obtained by applying the iterative boundary random walks algorithm.



### Radon Inversion via Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1808.03015v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.03015v1)
- **Published**: 2018-08-09 04:19:08+00:00
- **Updated**: 2018-08-09 04:19:08+00:00
- **Authors**: Ji He, Jianhua Ma
- **Comment**: None
- **Journal**: None
- **Summary**: Radon transform is widely used in physical and life sciences and one of its major applications is the X-ray computed tomography (X-ray CT), which is significant in modern health examination. The Radon inversion or image reconstruction is challenging due to the potentially defective radon projections. Conventionally, the reconstruction process contains several ad hoc stages to approximate the corresponding Radon inversion. Each of the stages is highly dependent on the results of the previous stage. In this paper, we propose a novel unified framework for Radon inversion via deep learning (DL). The Radon inversion can be approximated by the proposed framework with an end-to-end fashion instead of processing step-by-step with multiple stages. For simplicity, the proposed framework is short as iRadonMap (inverse Radon transform approximation). Specifically, we implement the iRadonMap as an appropriative neural network, of which the architecture can be divided into two segments. In the first segment, a learnable fully-connected filtering layer is used to filter the radon projections along the view-angle direction, which is followed by a learnable sinusoidal back-projection layer to transfer the filtered radon projections into an image. The second segment is a common neural network architecture to further improve the reconstruction performance in the image domain. The iRadonMap is overall optimized by training a large number of generic images from ImageNet database. To evaluate the performance of the iRadonMap, clinical patient data is used. Qualitative results show promising reconstruction performance of the iRadonMap.



### Efficient Outlier Removal in Large Scale Global Structure-from-Motion
- **Arxiv ID**: http://arxiv.org/abs/1808.03041v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.03041v4)
- **Published**: 2018-08-09 07:05:18+00:00
- **Updated**: 2019-02-15 08:03:37+00:00
- **Authors**: Fei Wen, Danping Zou, Rendong Ying, Peilin Liu
- **Comment**: 7 pages
- **Journal**: None
- **Summary**: This work addresses the outlier removal problem in large-scale global structure-from-motion. In such applications, global outlier removal is very useful to mitigate the deterioration caused by mismatches in the feature point matching step. Unlike existing outlier removal methods, we exploit the structure in multiview geometry problems to propose a dimension reduced formulation, based on which two methods have been developed. The first method considers a convex relaxed $\ell_1$ minimization and is solved by a single linear programming (LP), whilst the second one approximately solves the ideal $\ell_0$ minimization by an iteratively reweighted method. The dimension reduction results in a significant speedup of the new algorithms. Further, the iteratively reweighted method can significantly reduce the possibility of removing true inliers. Realistic multiview reconstruction experiments demonstrated that, compared with state-of-the-art algorithms, the new algorithms are much more efficient and meanwhile can give improved solution. Matlab code for reproducing the results is available at \textit{https://github.com/FWen/OUTLR.git}.



### Optimal conditions for connectedness of discretized sets
- **Arxiv ID**: http://arxiv.org/abs/1808.03053v1
- **DOI**: None
- **Categories**: **cs.DM**, cs.CV, 52c99, 68c99, G.2.m; I.4.1
- **Links**: [PDF](http://arxiv.org/pdf/1808.03053v1)
- **Published**: 2018-08-09 08:17:46+00:00
- **Updated**: 2018-08-09 08:17:46+00:00
- **Authors**: Boris Brimkov, Valentin E. Brimkov
- **Comment**: 9 pages, 1 figure with 2 subfigures
- **Journal**: None
- **Summary**: Constructing a discretization of a given set is a major problem in various theoretical and applied disciplines. An offset discretization of a set $X$ is obtained by taking the integer points inside a closed neighborhood of $X$ of a certain radius. In this note we determine a minimum threshold for the offset radius, beyond which the discretization of a disconnected set is always connected. The results hold for a broad class of disconnected and unbounded subsets of $R^n$, and generalize several previous results. Algorithmic aspects and possible applications are briefly discussed.



### Paired 3D Model Generation with Conditional Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1808.03082v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.03082v2)
- **Published**: 2018-08-09 10:58:18+00:00
- **Updated**: 2019-03-15 12:37:59+00:00
- **Authors**: Cihan Öngün, Alptekin Temizel
- **Comment**: Published in ECCV 2018 Workshops, Springer, LNCS. Cite this paper as:
  Ongun C., Temizel A. (2019) Paired 3D Model Generation with Conditional
  Generative Adversarial Networks. In: Leal-Taixe L., Roth S. (eds) Computer
  Vision-ECCV 2018 Workshops. ECCV 2018. Lecture Notes in Computer Science, vol
  11129. Springer, Cham
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) are shown to be successful at generating new and realistic samples including 3D object models. Conditional GAN, a variant of GANs, allows generating samples in given conditions. However, objects generated for each condition are different and it does not allow generation of the same object in different conditions. In this paper, we first adapt conditional GAN, which is originally designed for 2D image generation, to the problem of generating 3D models in different rotations. We then propose a new approach to guide the network to generate the same 3D sample in different and controllable rotation angles (sample pairs). Unlike previous studies, the proposed method does not require modification of the standard conditional GAN architecture and it can be integrated into the training step of any conditional GAN. Experimental results and visual comparison of 3D models show that the proposed method is successful at generating model pairs in different conditions.



### Classifier-Guided Visual Correction of Noisy Labels for Image Classification Tasks
- **Arxiv ID**: http://arxiv.org/abs/1808.03114v4
- **DOI**: 10.1111/cgf.13973
- **Categories**: **cs.CV**, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1808.03114v4)
- **Published**: 2018-08-09 12:34:33+00:00
- **Updated**: 2020-04-06 13:55:12+00:00
- **Authors**: Alex Bäuerle, Heiko Neumann, Timo Ropinski
- **Comment**: None
- **Journal**: None
- **Summary**: Training data plays an essential role in modern applications of machine learning. However, gathering labeled training data is time-consuming. Therefore, labeling is often outsourced to less experienced users, or completely automated. This can introduce errors, which compromise valuable training data, and lead to suboptimal training results. We thus propose a novel approach that uses the power of pretrained classifiers to visually guide users to noisy labels, and let them interactively check error candidates, to iteratively improve the training data set. To systematically investigate training data, we propose a categorization of labeling errors into three different types, based on an analysis of potential pitfalls in label acquisition processes. For each of these types, we present approaches to detect, reason about, and resolve error candidates, as we propose measures and visual guidance techniques to support machine learning users. Our approach has been used to spot errors in well-known machine learning benchmark data sets, and we tested its usability during a user evaluation. While initially developed for images, the techniques presented in this paper are independent of the classification algorithm, and can also be extended to many other types of training data.



### Overcoming Missing and Incomplete Modalities with Generative Adversarial Networks for Building Footprint Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1808.03195v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.03195v1)
- **Published**: 2018-08-09 15:24:56+00:00
- **Updated**: 2018-08-09 15:24:56+00:00
- **Authors**: Benjamin Bischke, Patrick Helber, Florian König, Damian Borth, Andreas Dengel
- **Comment**: None
- **Journal**: None
- **Summary**: The integration of information acquired with different modalities, spatial resolution and spectral bands has shown to improve predictive accuracies. Data fusion is therefore one of the key challenges in remote sensing. Most prior work focusing on multi-modal fusion, assumes that modalities are always available during inference. This assumption limits the applications of multi-modal models since in practice the data collection process is likely to generate data with missing, incomplete or corrupted modalities. In this paper, we show that Generative Adversarial Networks can be effectively used to overcome the problems that arise when modalities are missing or incomplete. Focusing on semantic segmentation of building footprints with missing modalities, our approach achieves an improvement of about 2% on the Intersection over Union (IoU) against the same network that relies only on the available modality.



### Deep Video Color Propagation
- **Arxiv ID**: http://arxiv.org/abs/1808.03232v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.03232v1)
- **Published**: 2018-08-09 16:54:18+00:00
- **Updated**: 2018-08-09 16:54:18+00:00
- **Authors**: Simone Meyer, Victor Cornillère, Abdelaziz Djelouah, Christopher Schroers, Markus Gross
- **Comment**: BMVC 2018
- **Journal**: None
- **Summary**: Traditional approaches for color propagation in videos rely on some form of matching between consecutive video frames. Using appearance descriptors, colors are then propagated both spatially and temporally. These methods, however, are computationally expensive and do not take advantage of semantic information of the scene. In this work we propose a deep learning framework for color propagation that combines a local strategy, to propagate colors frame-by-frame ensuring temporal stability, and a global strategy, using semantics for color propagation within a longer range. Our evaluation shows the superiority of our strategy over existing video and image color propagation methods as well as neural photo-realistic style transfer approaches.



### User-Guided Deep Anime Line Art Colorization with Conditional Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1808.03240v2
- **DOI**: 10.1145/3240508.3240661
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.03240v2)
- **Published**: 2018-08-09 17:17:47+00:00
- **Updated**: 2018-08-10 05:25:00+00:00
- **Authors**: Yuanzheng Ci, Xinzhu Ma, Zhihui Wang, Haojie Li, Zhongxuan Luo
- **Comment**: Accepted for publication at the 2018 ACM Multimedia Conference (MM
  '18)
- **Journal**: None
- **Summary**: Scribble colors based line art colorization is a challenging computer vision problem since neither greyscale values nor semantic information is presented in line arts, and the lack of authentic illustration-line art training pairs also increases difficulty of model generalization. Recently, several Generative Adversarial Nets (GANs) based methods have achieved great success. They can generate colorized illustrations conditioned on given line art and color hints. However, these methods fail to capture the authentic illustration distributions and are hence perceptually unsatisfying in the sense that they often lack accurate shading. To address these challenges, we propose a novel deep conditional adversarial architecture for scribble based anime line art colorization. Specifically, we integrate the conditional framework with WGAN-GP criteria as well as the perceptual loss to enable us to robustly train a deep network that makes the synthesized images more natural and real. We also introduce a local features network that is independent of synthetic data. With GANs conditioned on features from such network, we notably increase the generalization capability over "in the wild" line arts. Furthermore, we collect two datasets that provide high-quality colorful illustrations and authentic line arts for training and benchmarking. With the proposed model trained on our illustration dataset, we demonstrate that images synthesized by the presented approach are considerably more realistic and precise than alternative approaches.



### 3D Shape Perception from Monocular Vision, Touch, and Shape Priors
- **Arxiv ID**: http://arxiv.org/abs/1808.03247v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1808.03247v1)
- **Published**: 2018-08-09 17:30:49+00:00
- **Updated**: 2018-08-09 17:30:49+00:00
- **Authors**: Shaoxiong Wang, Jiajun Wu, Xingyuan Sun, Wenzhen Yuan, William T. Freeman, Joshua B. Tenenbaum, Edward H. Adelson
- **Comment**: IROS 2018. The first two authors contributed equally to this work
- **Journal**: None
- **Summary**: Perceiving accurate 3D object shape is important for robots to interact with the physical world. Current research along this direction has been primarily relying on visual observations. Vision, however useful, has inherent limitations due to occlusions and the 2D-3D ambiguities, especially for perception with a monocular camera. In contrast, touch gets precise local shape information, though its efficiency for reconstructing the entire shape could be low. In this paper, we propose a novel paradigm that efficiently perceives accurate 3D object shape by incorporating visual and tactile observations, as well as prior knowledge of common object shapes learned from large-scale shape repositories. We use vision first, applying neural networks with learned shape priors to predict an object's 3D shape from a single-view color image. We then use tactile sensing to refine the shape; the robot actively touches the object regions where the visual prediction has high uncertainty. Our method efficiently builds the 3D shape of common objects from a color image and a small number of tactile explorations (around 10). Our setup is easy to apply and has potentials to help robots better perform grasping or manipulation tasks on real-world objects.



### Deep Morphing: Detecting bone structures in fluoroscopic X-ray images with prior knowledge
- **Arxiv ID**: http://arxiv.org/abs/1808.04441v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1808.04441v2)
- **Published**: 2018-08-09 17:55:31+00:00
- **Updated**: 2018-11-19 17:02:27+00:00
- **Authors**: Aaron Pries, Peter J. Schreier, Artur Lamm, Stefan Pede, Jürgen Schmidt
- **Comment**: None
- **Journal**: None
- **Summary**: We propose approaches based on deep learning to localize objects in images when only a small training dataset is available and the images have low quality. That applies to many problems in medical image processing, and in particular to the analysis of fluoroscopic (low-dose) X-ray images, where the images have low contrast. We solve the problem by incorporating high-level information about the objects, which could be a simple geometrical model, like a circular outline, or a more complex statistical model. A simple geometrical representation can sufficiently describe some objects and only requires minimal labeling. Statistical shape models can be used to represent more complex objects. We propose computationally efficient two-stage approaches, which we call deep morphing, for both representations by fitting the representation to the output of a deep segmentation network.



### The Elephant in the Room
- **Arxiv ID**: http://arxiv.org/abs/1808.03305v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1808.03305v1)
- **Published**: 2018-08-09 18:58:59+00:00
- **Updated**: 2018-08-09 18:58:59+00:00
- **Authors**: Amir Rosenfeld, Richard Zemel, John K. Tsotsos
- **Comment**: None
- **Journal**: None
- **Summary**: We showcase a family of common failures of state-of-the art object detectors. These are obtained by replacing image sub-regions by another sub-image that contains a trained object. We call this "object transplanting". Modifying an image in this manner is shown to have a non-local impact on object detection. Slight changes in object position can affect its identity according to an object detector as well as that of other objects in the image. We provide some analysis and suggest possible reasons for the reported phenomena.



### DeepMag: Source Specific Motion Magnification Using Gradient Ascent
- **Arxiv ID**: http://arxiv.org/abs/1808.03338v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1808.03338v1)
- **Published**: 2018-08-09 20:36:57+00:00
- **Updated**: 2018-08-09 20:36:57+00:00
- **Authors**: Weixuan Chen, Daniel McDuff
- **Comment**: 24 pages, 13 figures
- **Journal**: None
- **Summary**: Many important physical phenomena involve subtle signals that are difficult to observe with the unaided eye, yet visualizing them can be very informative. Current motion magnification techniques can reveal these small temporal variations in video, but require precise prior knowledge about the target signal, and cannot deal with interference motions at a similar frequency. We present DeepMag an end-to-end deep neural video-processing framework based on gradient ascent that enables automated magnification of subtle color and motion signals from a specific source, even in the presence of large motions of various velocities. While the approach is generalizable, the advantages of DeepMag are highlighted via the task of video-based physiological visualization. Through systematic quantitative and qualitative evaluation of the approach on videos with different levels of head motion, we compare the magnification of pulse and respiration to existing state-of-the-art methods. Our method produces magnified videos with substantially fewer artifacts and blurring whilst magnifying the physiological changes by a similar degree.



### Deep Learning for Single Image Super-Resolution: A Brief Review
- **Arxiv ID**: http://arxiv.org/abs/1808.03344v3
- **DOI**: 10.1109/TMM.2019.2919431
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.03344v3)
- **Published**: 2018-08-09 20:51:51+00:00
- **Updated**: 2019-07-12 02:48:41+00:00
- **Authors**: Wenming Yang, Xuechen Zhang, Yapeng Tian, Wei Wang, Jing-Hao Xue
- **Comment**: Accepted by IEEE Transactions on Multimedia (TMM)
- **Journal**: None
- **Summary**: Single image super-resolution (SISR) is a notoriously challenging ill-posed problem, which aims to obtain a high-resolution (HR) output from one of its low-resolution (LR) versions. To solve the SISR problem, recently powerful deep learning algorithms have been employed and achieved the state-of-the-art performance. In this survey, we review representative deep learning-based SISR methods, and group them into two categories according to their major contributions to two essential aspects of SISR: the exploration of efficient neural network architectures for SISR, and the development of effective optimization objectives for deep SISR learning. For each category, a baseline is firstly established and several critical limitations of the baseline are summarized. Then representative works on overcoming these limitations are presented based on their original contents as well as our critical understandings and analyses, and relevant comparisons are conducted from a variety of perspectives. Finally we conclude this review with some vital current challenges and future trends in SISR leveraging deep learning algorithms.



