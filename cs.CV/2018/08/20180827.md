# Arxiv Papers in cs.CV on 2018-08-27
### Approach for Video Classification with Multi-label on YouTube-8M Dataset
- **Arxiv ID**: http://arxiv.org/abs/1808.08671v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.08671v3)
- **Published**: 2018-08-27 02:56:56+00:00
- **Updated**: 2018-10-14 14:21:29+00:00
- **Authors**: Kwangsoo Shin, Junhyeong Jeon, Seungbin Lee, Boyoung Lim, Minsoo Jeong, Jongho Nang
- **Comment**: Accepted at The 2nd Workshop on YouTube-8M Large-Scale Video
  Understanding in ECCV 2018
- **Journal**: None
- **Summary**: Video traffic is increasing at a considerable rate due to the spread of personal media and advancements in media technology. Accordingly, there is a growing need for techniques to automatically classify moving images. This paper use NetVLAD and NetFV models and the Huber loss function for video classification problem and YouTube-8M dataset to verify the experiment. We tried various attempts according to the dataset and optimize hyperparameters, ultimately obtain a GAP score of 0.8668.



### Exploring the Applications of Faster R-CNN and Single-Shot Multi-box Detection in a Smart Nursery Domain
- **Arxiv ID**: http://arxiv.org/abs/1808.08675v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.08675v1)
- **Published**: 2018-08-27 03:16:32+00:00
- **Updated**: 2018-08-27 03:16:32+00:00
- **Authors**: Somnuk Phon-Amnuaisuk, Ken T. Murata, Praphan Pavarangkoon, Kazunori Yamamoto, Takamichi Mizuhara
- **Comment**: 11 pages, 4 figures
- **Journal**: None
- **Summary**: The ultimate goal of a baby detection task concerns detecting the presence of a baby and other objects in a sequence of 2D images, tracking them and understanding the semantic contents of the scene. Recent advances in deep learning and computer vision offer various powerful tools in general object detection and can be applied to a baby detection task. In this paper, the Faster Region-based Convolutional Neural Network and the Single-Shot Multi-Box Detection approaches are explored. They are the two state-of-the-art object detectors based on the region proposal tactic and the multi-box tactic. The presence of a baby in the scene obtained from these detectors, tested using different pre-trained models, are discussed. This study is important since the behaviors of these detectors in a baby detection task using different pre-trained models are still not well understood. This exploratory study reveals many useful insights into the applications of these object detectors in the smart nursery domain.



### HMS-Net: Hierarchical Multi-scale Sparsity-invariant Network for Sparse Depth Completion
- **Arxiv ID**: http://arxiv.org/abs/1808.08685v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.08685v2)
- **Published**: 2018-08-27 04:43:34+00:00
- **Updated**: 2020-02-20 18:19:25+00:00
- **Authors**: Zixuan Huang, Junming Fan, Shenggan Cheng, Shuai Yi, Xiaogang Wang, Hongsheng Li
- **Comment**: IEEE Trans. on Image Processing
- **Journal**: None
- **Summary**: Dense depth cues are important and have wide applications in various computer vision tasks. In autonomous driving, LIDAR sensors are adopted to acquire depth measurements around the vehicle to perceive the surrounding environments. However, depth maps obtained by LIDAR are generally sparse because of its hardware limitation. The task of depth completion attracts increasing attention, which aims at generating a dense depth map from an input sparse depth map. To effectively utilize multi-scale features, we propose three novel sparsity-invariant operations, based on which, a sparsity-invariant multi-scale encoder-decoder network (HMS-Net) for handling sparse inputs and sparse feature maps is also proposed. Additional RGB features could be incorporated to further improve the depth completion performance. Our extensive experiments and component analysis on two public benchmarks, KITTI depth completion benchmark and NYU-depth-v2 dataset, demonstrate the effectiveness of the proposed approach. As of Aug. 12th, 2018, on KITTI depth completion leaderboard, our proposed model without RGB guidance ranks first among all peer-reviewed methods without using RGB information, and our model with RGB guidance ranks second among all RGB-guided methods.



### Deeply Supervised Depth Map Super-Resolution as Novel View Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1808.08688v1
- **DOI**: 10.1109/TCSVT.2018.2866399
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.08688v1)
- **Published**: 2018-08-27 04:54:13+00:00
- **Updated**: 2018-08-27 04:54:13+00:00
- **Authors**: Xibin Song, Yuchao Dai, Xueying Qin
- **Comment**: Accepted by IEEE Transactions on Circuits and Systems for Video
  Technology (T-CSVT) 2018
- **Journal**: None
- **Summary**: Deep convolutional neural network (DCNN) has been successfully applied to depth map super-resolution and outperforms existing methods by a wide margin. However, there still exist two major issues with these DCNN based depth map super-resolution methods that hinder the performance: i) The low-resolution depth maps either need to be up-sampled before feeding into the network or substantial deconvolution has to be used; and ii) The supervision (high-resolution depth maps) is only applied at the end of the network, thus it is difficult to handle large up-sampling factors, such as $\times 8, \times 16$. In this paper, we propose a new framework to tackle the above problems. First, we propose to represent the task of depth map super-resolution as a series of novel view synthesis sub-tasks. The novel view synthesis sub-task aims at generating (synthesizing) a depth map from different camera pose, which could be learned in parallel. Second, to handle large up-sampling factors, we present a deeply supervised network structure to enforce strong supervision in each stage of the network. Third, a multi-scale fusion strategy is proposed to effectively exploit the feature maps at different scales and handle the blocking effect. In this way, our proposed framework could deal with challenging depth map super-resolution efficiently under large up-sampling factors (e.g. $\times 8, \times 16$). Our method only uses the low-resolution depth map as input, and the support of color image is not needed, which greatly reduces the restriction of our method. Extensive experiments on various benchmarking datasets demonstrate the superiority of our method over current state-of-the-art depth map super-resolution methods.



### Stereo Computation for a Single Mixture Image
- **Arxiv ID**: http://arxiv.org/abs/1808.08690v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.08690v1)
- **Published**: 2018-08-27 05:03:37+00:00
- **Updated**: 2018-08-27 05:03:37+00:00
- **Authors**: Yiran Zhong, Yuchao Dai, Hongdong Li
- **Comment**: Accepted by European Conference on Computer Vision (ECCV) 2018
- **Journal**: None
- **Summary**: This paper proposes an original problem of \emph{stereo computation from a single mixture image}-- a challenging problem that had not been researched before. The goal is to separate (\ie, unmix) a single mixture image into two constitute image layers, such that the two layers form a left-right stereo image pair, from which a valid disparity map can be recovered. This is a severely illposed problem, from one input image one effectively aims to recover three (\ie, left image, right image and a disparity map). In this work we give a novel deep-learning based solution, by jointly solving the two subtasks of image layer separation as well as stereo matching. Training our deep net is a simple task, as it does not need to have disparity maps. Extensive experiments demonstrate the efficacy of our method.



### Generalized Capsule Networks with Trainable Routing Procedure
- **Arxiv ID**: http://arxiv.org/abs/1808.08692v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.08692v1)
- **Published**: 2018-08-27 05:44:19+00:00
- **Updated**: 2018-08-27 05:44:19+00:00
- **Authors**: Zhenhua Chen, David Crandall
- **Comment**: None
- **Journal**: None
- **Summary**: CapsNet (Capsule Network) was first proposed by~\citet{capsule} and later another version of CapsNet was proposed by~\citet{emrouting}. CapsNet has been proved effective in modeling spatial features with much fewer parameters. However, the routing procedures in both papers are not well incorporated into the whole training process. The optimal number of routing procedure is misery which has to be found manually. To overcome this disadvantages of current routing procedures in CapsNet, we embed the routing procedure into the optimization procedure with all other parameters in neural networks, namely, make coupling coefficients in the routing procedure become completely trainable. We call it Generalized CapsNet (G-CapsNet). We implement both "full-connected" version of G-CapsNet and "convolutional" version of G-CapsNet. G-CapsNet achieves a similar performance in the dataset MNIST as in the original papers. We also test two capsule packing method (cross feature maps or with feature maps) from previous convolutional layers and see no evident difference. Besides, we also explored possibility of stacking multiple capsule layers. The code is shared on \hyperlink{https://github.com/chenzhenhua986/CAFFE-CapsNet}{CAFFE-CapsNet}.



### Wide Activation for Efficient and Accurate Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1808.08718v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1808.08718v2)
- **Published**: 2018-08-27 07:48:21+00:00
- **Updated**: 2018-12-21 02:42:59+00:00
- **Authors**: Jiahui Yu, Yuchen Fan, Jianchao Yang, Ning Xu, Zhaowen Wang, Xinchao Wang, Thomas Huang
- **Comment**: tech report and factsheet
- **Journal**: None
- **Summary**: In this report we demonstrate that with same parameters and computational budgets, models with wider features before ReLU activation have significantly better performance for single image super-resolution (SISR). The resulted SR residual network has a slim identity mapping pathway with wider (\(2\times\) to \(4\times\)) channels before activation in each residual block. To further widen activation (\(6\times\) to \(9\times\)) without computational overhead, we introduce linear low-rank convolution into SR networks and achieve even better accuracy-efficiency tradeoffs. In addition, compared with batch normalization or no normalization, we find training with weight normalization leads to better accuracy for deep super-resolution networks. Our proposed SR network \textit{WDSR} achieves better results on large-scale DIV2K image super-resolution benchmark in terms of PSNR with same or lower computational complexity. Based on WDSR, our method also won 1st places in NTIRE 2018 Challenge on Single Image Super-Resolution in all three realistic tracks. Experiments and ablation studies support the importance of wide activation for image super-resolution. Code is released at: https://github.com/JiahuiYu/wdsr_ntire2018



### Generalisation in humans and deep neural networks
- **Arxiv ID**: http://arxiv.org/abs/1808.08750v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, q-bio.NC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1808.08750v3)
- **Published**: 2018-08-27 09:17:57+00:00
- **Updated**: 2020-10-23 09:05:30+00:00
- **Authors**: Robert Geirhos, Carlos R. Medina Temme, Jonas Rauber, Heiko H. Schütt, Matthias Bethge, Felix A. Wichmann
- **Comment**: Added optimal probability aggregation method to appendix
- **Journal**: None
- **Summary**: We compare the robustness of humans and current convolutional deep neural networks (DNNs) on object recognition under twelve different types of image degradations. First, using three well known DNNs (ResNet-152, VGG-19, GoogLeNet) we find the human visual system to be more robust to nearly all of the tested image manipulations, and we observe progressively diverging classification error-patterns between humans and DNNs when the signal gets weaker. Secondly, we show that DNNs trained directly on distorted images consistently surpass human performance on the exact distortion types they were trained on, yet they display extremely poor generalisation abilities when tested on other distortion types. For example, training on salt-and-pepper noise does not imply robustness on uniform white noise and vice versa. Thus, changes in the noise distribution between training and testing constitutes a crucial challenge to deep learning vision systems that can be systematically addressed in a lifelong machine learning approach. Our new dataset consisting of 83K carefully measured human psychophysical trials provide a useful reference for lifelong robustness against image degradations set by the human visual system.



### What Makes Natural Scene Memorable?
- **Arxiv ID**: http://arxiv.org/abs/1808.08754v1
- **DOI**: 10.1145/3267799.3267802
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1808.08754v1)
- **Published**: 2018-08-27 09:38:16+00:00
- **Updated**: 2018-08-27 09:38:16+00:00
- **Authors**: Jiaxin Lu, Mai Xu, Ren Yang, Zulin Wang
- **Comment**: Accepted to ACM MM Workshops
- **Journal**: Proceedings of the 2018 Workshop on Understanding Subjective
  Attributes of Data, with the Focus on Evoked Emotions
- **Summary**: Recent studies on image memorability have shed light on the visual features that make generic images, object images or face photographs memorable. However, a clear understanding and reliable estimation of natural scene memorability remain elusive. In this paper, we provide an attempt to answer: "what exactly makes natural scene memorable". Specifically, we first build LNSIM, a large-scale natural scene image memorability database (containing 2,632 images and memorability annotations). Then, we mine our database to investigate how low-, middle- and high-level handcrafted features affect the memorability of natural scene. In particular, we find that high-level feature of scene category is rather correlated with natural scene memorability. Thus, we propose a deep neural network based natural scene memorability (DeepNSM) predictor, which takes advantage of scene category. Finally, the experimental results validate the effectiveness of DeepNSM.



### Stochastic Attraction-Repulsion Embedding for Large Scale Image Localization
- **Arxiv ID**: http://arxiv.org/abs/1808.08779v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.08779v2)
- **Published**: 2018-08-27 10:52:44+00:00
- **Updated**: 2019-08-06 08:00:58+00:00
- **Authors**: Liu Liu, Hongdong Li, Yuchao Dai
- **Comment**: ICCV
- **Journal**: None
- **Summary**: This paper tackles the problem of large-scale image-based localization (IBL) where the spatial location of a query image is determined by finding out the most similar reference images in a large database. For solving this problem, a critical task is to learn discriminative image representation that captures informative information relevant for localization. We propose a novel representation learning method having higher location-discriminating power. It provides the following contributions: 1) we represent a place (location) as a set of exemplar images depicting the same landmarks and aim to maximize similarities among intra-place images while minimizing similarities among inter-place images; 2) we model a similarity measure as a probability distribution on L_2-metric distances between intra-place and inter-place image representations; 3) we propose a new Stochastic Attraction and Repulsion Embedding (SARE) loss function minimizing the KL divergence between the learned and the actual probability distributions; 4) we give theoretical comparisons between SARE, triplet ranking and contrastive losses. It provides insights into why SARE is better by analyzing gradients. Our SARE loss is easy to implement and pluggable to any CNN. Experiments show that our proposed method improves the localization performance on standard benchmarks by a large margin. Demonstrating the broad applicability of our method, we obtained the third place out of 209 teams in the 2018 Google Landmark Retrieval Challenge. Our code and model are available at https://github.com/Liumouliu/deepIBL.



### A Deeper Insight into the UnDEMoN: Unsupervised Deep Network for Depth and Ego-Motion Estimation
- **Arxiv ID**: http://arxiv.org/abs/1809.00969v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1809.00969v3)
- **Published**: 2018-08-27 11:40:58+00:00
- **Updated**: 2018-11-27 09:21:17+00:00
- **Authors**: Madhu Babu V, Anima Majumder, Kaushik Das, Swagat Kumar
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents an unsupervised deep learning framework called UnDEMoN for estimating dense depth map and 6-DoF camera pose information directly from monocular images. The proposed network is trained using unlabeled monocular stereo image pairs and is shown to provide superior performance in depth and ego-motion estimation compared to the existing state-of-the-art. These improvements are achieved by introducing a new objective function that aims to minimize spatial as well as temporal reconstruction losses simultaneously. These losses are defined using bi-linear sampling kernel and penalized using the Charbonnier penalty function. The objective function, thus created, provides robustness to image gradient noises thereby improving the overall estimation accuracy without resorting to any coarse to fine strategies which are currently prevalent in the literature. Another novelty lies in the fact that we combine a disparity-based depth estimation network with a pose estimation network to obtain absolute scale-aware 6 DOF Camera pose and superior depth map. The effectiveness of the proposed approach is demonstrated through performance comparison with the existing supervised and unsupervised methods on the KITTI driving dataset.



### Task adapted reconstruction for inverse problems
- **Arxiv ID**: http://arxiv.org/abs/1809.00948v1
- **DOI**: 10.1088/1361-6420/ac28ec
- **Categories**: **cs.CV**, cs.LG, math.FA
- **Links**: [PDF](http://arxiv.org/pdf/1809.00948v1)
- **Published**: 2018-08-27 11:44:48+00:00
- **Updated**: 2018-08-27 11:44:48+00:00
- **Authors**: Jonas Adler, Sebastian Lunz, Olivier Verdier, Carola-Bibiane Schönlieb, Ozan Öktem
- **Comment**: None
- **Journal**: Inverse Problems, Vol. 38, Issue 7 (2022)
- **Summary**: The paper considers the problem of performing a task defined on a model parameter that is only observed indirectly through noisy data in an ill-posed inverse problem. A key aspect is to formalize the steps of reconstruction and task as appropriate estimators (non-randomized decision rules) in statistical estimation problems. The implementation makes use of (deep) neural networks to provide a differentiable parametrization of the family of estimators for both steps. These networks are combined and jointly trained against suitable supervised training data in order to minimize a joint differentiable loss function, resulting in an end-to-end task adapted reconstruction method. The suggested framework is generic, yet adaptable, with a plug-and-play structure for adjusting both the inverse problem and the task at hand. More precisely, the data model (forward operator and statistical model of the noise) associated with the inverse problem is exchangeable, e.g., by using neural network architecture given by a learned iterative method. Furthermore, any task that is encodable as a trainable neural network can be used. The approach is demonstrated on joint tomographic image reconstruction, classification and joint tomographic image reconstruction segmentation.



### Discriminative Representation Combinations for Accurate Face Spoofing Detection
- **Arxiv ID**: http://arxiv.org/abs/1808.08802v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.08802v2)
- **Published**: 2018-08-27 12:01:06+00:00
- **Updated**: 2018-08-28 01:17:57+00:00
- **Authors**: Xiao Song, Xu Zhao, Liangji Fang, Tianwei Lin
- **Comment**: To be published in Pattern Recognition
- **Journal**: None
- **Summary**: Three discriminative representations for face presentation attack detection are introduced in this paper. Firstly we design a descriptor called spatial pyramid coding micro-texture (SPMT) feature to characterize local appearance information. Secondly we utilize the SSD, which is a deep learning framework for detection, to excavate context cues and conduct end-to-end face presentation attack detection. Finally we design a descriptor called template face matched binocular depth (TFBD) feature to characterize stereo structures of real and fake faces. For accurate presentation attack detection, we also design two kinds of representation combinations. Firstly, we propose a decision-level cascade strategy to combine SPMT with SSD. Secondly, we use a simple score fusion strategy to combine face structure cues (TFBD) with local micro-texture features (SPMT). To demonstrate the effectiveness of our design, we evaluate the representation combination of SPMT and SSD on three public datasets, which outperforms all other state-of-the-art methods. In addition, we evaluate the representation combination of SPMT and TFBD on our dataset and excellent performance is also achieved.



### Attentive Sequence to Sequence Translation for Localizing Clips of Interest by Natural Language Descriptions
- **Arxiv ID**: http://arxiv.org/abs/1808.08803v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.08803v1)
- **Published**: 2018-08-27 12:01:26+00:00
- **Updated**: 2018-08-27 12:01:26+00:00
- **Authors**: Ke Ning, Linchao Zhu, Ming Cai, Yi Yang, Di Xie, Fei Wu
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel attentive sequence to sequence translator (ASST) for clip localization in videos by natural language descriptions. We make two contributions. First, we propose a bi-directional Recurrent Neural Network (RNN) with a finely calibrated vision-language attentive mechanism to comprehensively understand the free-formed natural language descriptions. The RNN parses natural language descriptions in two directions, and the attentive model attends every meaningful word or phrase to each frame, thereby resulting in a more detailed understanding of video content and description semantics. Second, we design a hierarchical architecture for the network to jointly model language descriptions and video content. Given a video-description pair, the network generates a matrix representation, i.e., a sequence of vectors. Each vector in the matrix represents a video frame conditioned by the description. The 2D representation not only preserves the temporal dependencies of frames but also provides an effective way to perform frame-level video-language matching. The hierarchical architecture exploits video content with multiple granularities, ranging from subtle details to global context. Integration of the multiple granularities yields a robust representation for multi-level video-language abstraction. We validate the effectiveness of our ASST on two large-scale datasets. Our ASST outperforms the state-of-the-art by $4.28\%$ in Rank$@1$ on the DiDeMo dataset. On the Charades-STA dataset, we significantly improve the state-of-the-art by $13.41\%$ in Rank$@1,IoU=0.5$.



### Real-Time MDNet
- **Arxiv ID**: http://arxiv.org/abs/1808.08834v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.08834v1)
- **Published**: 2018-08-27 13:13:14+00:00
- **Updated**: 2018-08-27 13:13:14+00:00
- **Authors**: Ilchae Jung, Jeany Son, Mooyeol Baek, Bohyung Han
- **Comment**: 16 pages, 8 figures, accepted at ECCV 2018
- **Journal**: None
- **Summary**: We present a fast and accurate visual tracking algorithm based on the multi-domain convolutional neural network (MDNet). The proposed approach accelerates feature extraction procedure and learns more discriminative models for instance classification; it enhances representation quality of target and background by maintaining a high resolution feature map with a large receptive field per activation. We also introduce a novel loss term to differentiate foreground instances across multiple domains and learn a more discriminative embedding of target objects with similar semantics. The proposed techniques are integrated into the pipeline of a well known CNN-based visual tracking algorithm, MDNet. We accomplish approximately 25 times speed-up with almost identical accuracy compared to MDNet. Our algorithm is evaluated in multiple popular tracking benchmark datasets including OTB2015, UAV123, and TempleColor, and outperforms the state-of-the-art real-time tracking methods consistently even without dataset-specific parameter tuning.



### Iterative multi-path tracking for video and volume segmentation with sparse point supervision
- **Arxiv ID**: http://arxiv.org/abs/1809.00970v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1809.00970v1)
- **Published**: 2018-08-27 13:38:50+00:00
- **Updated**: 2018-08-27 13:38:50+00:00
- **Authors**: Laurent Lejeune, Jan Grossrieder, Raphael Sznitman
- **Comment**: None
- **Journal**: None
- **Summary**: Recent machine learning strategies for segmentation tasks have shown great ability when trained on large pixel-wise annotated image datasets. It remains a major challenge however to aggregate such datasets, as the time and monetary cost associated with collecting extensive annotations is extremely high. This is particularly the case for generating precise pixel-wise annotations in video and volumetric image data. To this end, this work presents a novel framework to produce pixel-wise segmentations using minimal supervision. Our method relies on 2D point supervision, whereby a single 2D location within an object of interest is provided on each image of the data. Our method then estimates the object appearance in a semi-supervised fashion by learning object-image-specific features and by using these in a semi-supervised learning framework. Our object model is then used in a graph-based optimization problem that takes into account all provided locations and the image data in order to infer the complete pixel-wise segmentation. In practice, we solve this optimally as a tracking problem using a K-shortest path approach. Both the object model and segmentation are then refined iteratively to further improve the final segmentation. We show that by collecting 2D locations using a gaze tracker, our approach can provide state-of-the-art segmentations on a range of objects and image modalities (video and 3D volumes), and that these can then be used to train supervised machine learning classifiers.



### Targeted Nonlinear Adversarial Perturbations in Images and Videos
- **Arxiv ID**: http://arxiv.org/abs/1809.00958v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.00958v1)
- **Published**: 2018-08-27 14:09:18+00:00
- **Updated**: 2018-08-27 14:09:18+00:00
- **Authors**: Roberto Rey-de-Castro, Herschel Rabitz
- **Comment**: Code and data available at:
  https://github.com/roberto1648/adversarial-perturbations-on-images-and-videos
- **Journal**: None
- **Summary**: We introduce a method for learning adversarial perturbations targeted to individual images or videos. The learned perturbations are found to be sparse while at the same time containing a high level of feature detail. Thus, the extracted perturbations allow a form of object or action recognition and provide insights into what features the studied deep neural network models consider important when reaching their classification decisions. From an adversarial point of view, the sparse perturbations successfully confused the models into misclassifying, although the perturbed samples still belonged to the same original class by visual examination. This is discussed in terms of a prospective data augmentation scheme. The sparse yet high-quality perturbations may also be leveraged for image or video compression.



### Facial Information Recovery from Heavily Damaged Images using Generative Adversarial Network- PART 1
- **Arxiv ID**: http://arxiv.org/abs/1808.08867v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.08867v1)
- **Published**: 2018-08-27 14:45:08+00:00
- **Updated**: 2018-08-27 14:45:08+00:00
- **Authors**: Pushparaja Murugan
- **Comment**: 16 pages
- **Journal**: None
- **Summary**: Over the past decades, a large number of techniques have emerged in modern imaging systems to capture the exact information of the original scene regardless of shake, motion, lighting conditions and etc., These developments have progressively addressed the acquisition of images in high speed and high resolutions. However, the various ineradicable real-time factors cause the degradation of the information and the quality of the acquired images. The available techniques are not intelligent enough to generalize this complex phenomenon. Hence, it is necessary to develop an intellectual framework to recover the possible information presented in the original scene. In this article, we propose a kernel free framework based on conditional-GAN to recover the information from the heavily damaged images. The degradation of images is assumed to be occurred by the combination of a various blur. Learning parameter of the cGAN is optimized by multi-component loss function that includes improved wasserstein loss with regression loss function. The generator module of this network is developed by using U-Net architecture with local Residual connections and global skip connection. Local connections and a global skip connection are implemented for the utilization of all stages of features. Generated images show that the network has the potential to recover the probable information of blurred images from the learned features. This research work is carried out as a part of our IOP studio software 'Facial recognition module'.



### Stereo 3D Object Trajectory Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1808.09297v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.09297v1)
- **Published**: 2018-08-27 15:04:04+00:00
- **Updated**: 2018-08-27 15:04:04+00:00
- **Authors**: Sebastian Bullinger, Christoph Bodensteiner, Michael Arens, Rainer Stiefelhagen
- **Comment**: Under Review. arXiv admin note: text overlap with arXiv:1711.06136
- **Journal**: None
- **Summary**: We present a method to reconstruct the three-dimensional trajectory of a moving instance of a known object category using stereo video data. We track the two-dimensional shape of objects on pixel level exploiting instance-aware semantic segmentation techniques and optical flow cues. We apply Structure from Motion (SfM) techniques to object and background images to determine for each frame initial camera poses relative to object instances and background structures. We refine the initial SfM results by integrating stereo camera constraints exploiting factor graphs. We compute the object trajectory by combining object and background camera pose information. In contrast to stereo matching methods, our approach leverages temporal adjacent views for object point triangulation. As opposed to monocular trajectory reconstruction approaches, our method shows no degenerated cases. We evaluate our approach using publicly available video data of vehicles in urban scenes.



### Improved Breast Mass Segmentation in Mammograms with Conditional Residual U-net
- **Arxiv ID**: http://arxiv.org/abs/1808.08885v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.08885v1)
- **Published**: 2018-08-27 15:34:01+00:00
- **Updated**: 2018-08-27 15:34:01+00:00
- **Authors**: Heyi Li, Dongdong Chen, Bill Nailon, Mike Davies, Dave Laurenson
- **Comment**: To appear in MICCAI 2018, Breast Image Analysis Workshop
- **Journal**: None
- **Summary**: We explore the use of deep learning for breast mass segmentation in mammograms. By integrating the merits of residual learning and probabilistic graphical modelling with standard U-Net, we propose a new deep network, Conditional Residual U-Net (CRU-Net), to improve the U-Net segmentation performance. Benefiting from the advantage of probabilistic graphical modelling in the pixel-level labelling, and the structure insights of a deep residual network in the feature extraction, the CRU-Net provides excellent mass segmentation performance. Evaluations based on INbreast and DDSM-BCRP datasets demonstrate that the CRU-Net achieves the best mass segmentation performance compared to the state-of-art methodologies. Moreover, neither tedious pre-processing nor post-processing techniques are not required in our algorithm.



### Which Emoji Talks Best for My Picture?
- **Arxiv ID**: http://arxiv.org/abs/1808.08891v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.08891v1)
- **Published**: 2018-08-27 15:43:28+00:00
- **Updated**: 2018-08-27 15:43:28+00:00
- **Authors**: Anurag Illendula, Kv Manohar, Manish Reddy Yedulla
- **Comment**: Accepted at the 2018 IEEE/WIC/ACM International Conference on Web
  Intelligence (WI '18), December 3-6, 2018, Santiago de Chile
- **Journal**: None
- **Summary**: Emojis have evolved as complementary sources for expressing emotion in social-media platforms where posts are mostly composed of texts and images. In order to increase the expressiveness of the social media posts, users associate relevant emojis with their posts. Incorporating domain knowledge has improved machine understanding of text. In this paper, we investigate whether domain knowledge for emoji can improve the accuracy of emoji recommendation task in case of multimedia posts composed of image and text. Our emoji recommendation can suggest accurate emojis by exploiting both visual and textual content from social media posts as well as domain knowledge from Emojinet. Experimental results using pre-trained image classifiers and pre-trained word embedding models on Twitter dataset show that our results outperform the current state-of-the-art by 9.6\%. We also present a user study evaluation of our recommendation system on a set of images chosen from MSCOCO dataset.



### Smoothed Dilated Convolutions for Improved Dense Prediction
- **Arxiv ID**: http://arxiv.org/abs/1808.08931v2
- **DOI**: 10.1145/3219819.3219944
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1808.08931v2)
- **Published**: 2018-08-27 17:13:38+00:00
- **Updated**: 2019-05-01 23:04:40+00:00
- **Authors**: Zhengyang Wang, Shuiwang Ji
- **Comment**: The original version was accepted by KDD2018. Code is publicly
  available at https://github.com/divelab/dilated
- **Journal**: In Proceedings of the 24th ACM SIGKDD International Conference on
  Knowledge Discovery & Data Mining (pp. 2486-2495). 2018
- **Summary**: Dilated convolutions, also known as atrous convolutions, have been widely explored in deep convolutional neural networks (DCNNs) for various dense prediction tasks. However, dilated convolutions suffer from the gridding artifacts, which hampers the performance. In this work, we propose two simple yet effective degridding methods by studying a decomposition of dilated convolutions. Unlike existing models, which explore solutions by focusing on a block of cascaded dilated convolutional layers, our methods address the gridding artifacts by smoothing the dilated convolution itself. In addition, we point out that the two degridding approaches are intrinsically related and define separable and shared (SS) operations, which generalize the proposed methods. We further explore SS operations in view of operations on graphs and propose the SS output layer, which is able to smooth the entire DCNNs by only replacing the output layer. We evaluate our degridding methods and the SS output layer thoroughly, and visualize the smoothing effect through effective receptive field analysis. Results show that our methods degridding yield consistent improvements on the performance of dense prediction tasks, while adding negligible amounts of extra training parameters. And the SS output layer improves the performance significantly and is very efficient in terms of number of training parameters.



### Improving Information Extraction from Images with Learned Semantic Models
- **Arxiv ID**: http://arxiv.org/abs/1808.08941v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1808.08941v1)
- **Published**: 2018-08-27 17:39:56+00:00
- **Updated**: 2018-08-27 17:39:56+00:00
- **Authors**: Stephan Baier, Yunpu Ma, Volker Tresp
- **Comment**: None
- **Journal**: None
- **Summary**: Many applications require an understanding of an image that goes beyond the simple detection and classification of its objects. In particular, a great deal of semantic information is carried in the relationships between objects. We have previously shown that the combination of a visual model and a statistical semantic prior model can improve on the task of mapping images to their associated scene description. In this paper, we review the model and compare it to a novel conditional multi-way model for visual relationship detection, which does not include an explicitly trained visual prior model. We also discuss potential relationships between the proposed methods and memory models of the human brain.



### Open Set Chinese Character Recognition using Multi-typed Attributes
- **Arxiv ID**: http://arxiv.org/abs/1808.08993v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.08993v1)
- **Published**: 2018-08-27 18:53:31+00:00
- **Updated**: 2018-08-27 18:53:31+00:00
- **Authors**: Sheng He, Lambert Schomaker
- **Comment**: 29 pages, submitted to Pattern Recognition
- **Journal**: None
- **Summary**: Recognition of Off-line Chinese characters is still a challenging problem, especially in historical documents, not only in the number of classes extremely large in comparison to contemporary image retrieval methods, but also new unseen classes can be expected under open learning conditions (even for CNN). Chinese character recognition with zero or a few training samples is a difficult problem and has not been studied yet. In this paper, we propose a new Chinese character recognition method by multi-type attributes, which are based on pronunciation, structure and radicals of Chinese characters, applied to character recognition in historical books. This intermediate attribute code has a strong advantage over the common `one-hot' class representation because it allows for understanding complex and unseen patterns symbolically using attributes. First, each character is represented by four groups of attribute types to cover a wide range of character possibilities: Pinyin label, layout structure, number of strokes, three different input methods such as Cangjie, Zhengma and Wubi, as well as a four-corner encoding method. A convolutional neural network (CNN) is trained to learn these attributes. Subsequently, characters can be easily recognized by these attributes using a distance metric and a complete lexicon that is encoded in attribute space. We evaluate the proposed method on two open data sets: printed Chinese character recognition for zero-shot learning, historical characters for few-shot learning and a closed set: handwritten Chinese characters. Experimental results show a good general classification of seen classes but also a very promising generalization ability to unseen characters.



### COFGA: Classification Of Fine-Grained Features In Aerial Images
- **Arxiv ID**: http://arxiv.org/abs/1808.09001v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.09001v1)
- **Published**: 2018-08-27 19:09:51+00:00
- **Updated**: 2018-08-27 19:09:51+00:00
- **Authors**: Eran Dahan, Tzvi Diskin
- **Comment**: None
- **Journal**: None
- **Summary**: Classification between thousands of classes in high-resolution images is one of the heavily studied problems in deep learning over the last decade. However, the challenge of fine-grained multi-class classification of objects in aerial images, especially in low resource cases, is still challenging and an active area of research in the literature. Solving this problem can give rise to various applications in the field of scene understanding and classification and re-identification of specific objects from aerial images. In this paper, we provide a description of our dataset - COFGA of multi-class annotated objects in aerial images. We examine the results of existing state-of-the-art models and modified deep neural networks. Finally, we explain in detail the first published competition for solving this task.



### Review Helpfulness Assessment based on Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1808.09016v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1808.09016v1)
- **Published**: 2018-08-27 19:53:52+00:00
- **Updated**: 2018-08-27 19:53:52+00:00
- **Authors**: Xianshan Qu, Xiaopeng Li, John R. Rose
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we describe the implementation of a convolutional neural network (CNN) used to assess online review helpfulness. To our knowledge, this is the first use of this architecture to address this problem. We explore the impact of two related factors impacting CNN performance: different word embedding initializations and different input review lengths. We also propose an approach to combining rating star information with review text to further improve prediction accuracy. We demonstrate that this can improve the overall accuracy by 2%. Finally, we evaluate the method on a benchmark dataset and show an improvement in accuracy relative to published results for traditional methods of 2.5% for a model trained using only review text and 4.24% for a model trained on a combination of rating star information and review text.



### Real-time Pedestrian Detection Approach with an Efficient Data Communication Bandwidth Strategy
- **Arxiv ID**: http://arxiv.org/abs/1808.09023v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.09023v3)
- **Published**: 2018-08-27 20:13:47+00:00
- **Updated**: 2019-03-17 04:43:18+00:00
- **Authors**: Mizanur Rahman, Mhafuzul Islam, Jon Calhoun, Mashrur Chowdhury
- **Comment**: 20 pages, 6 figures, 2 tables. arXiv admin note: text overlap with
  arXiv:1506.02640 by other authors
- **Journal**: None
- **Summary**: Vehicle-to-Pedestrian (V2P) communication can significantly improve pedestrian safety at a signalized intersection. It is unlikely that pedestrians will carry a low latency communication enabled device and activate a pedestrian safety application in their hand-held device all the time. Because of this limitation, multiple traffic cameras at the signalized intersection can be used to accurately detect and locate pedestrians using deep learning and broadcast safety alerts related to pedestrians to warn connected and automated vehicles around a signalized intersection. However, unavailability of high-performance computing infrastructure at the roadside and limited network bandwidth between traffic cameras and the computing infrastructure limits the ability of real-time data streaming and processing for pedestrian detection. In this paper, we develop an edge computing based real-time pedestrian detection strategy combining pedestrian detection algorithm using deep learning and an efficient data communication approach to reduce bandwidth requirements while maintaining a high object detection accuracy. We utilize a lossy compression technique on traffic camera data to determine the tradeoff between the reduction of the communication bandwidth requirements and a defined object detection accuracy. The performance of the pedestrian-detection strategy is measured in terms of pedestrian classification accuracy with varying peak signal-to-noise ratios. The analyses reveal that we detect pedestrians by maintaining a defined detection accuracy with a peak signal-to-noise ratio (PSNR) 43 dB while reducing the communication bandwidth from 9.82 Mbits/sec to 0.31 Mbits/sec, a 31x reduction.



### Migrating Knowledge between Physical Scenarios based on Artificial Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1809.00972v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, physics.comp-ph
- **Links**: [PDF](http://arxiv.org/pdf/1809.00972v2)
- **Published**: 2018-08-27 20:46:50+00:00
- **Updated**: 2019-05-03 01:17:20+00:00
- **Authors**: Yurui Qu, Li Jing, Yichen Shen, Min Qiu, Marin Soljacic
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning is known to be data-hungry, which hinders its application in many areas of science when datasets are small. Here, we propose to use transfer learning methods to migrate knowledge between different physical scenarios and significantly improve the prediction accuracy of artificial neural networks trained on a small dataset. This method can help reduce the demand for expensive data by making use of additional inexpensive data. First, we demonstrate that in predicting the transmission from multilayer photonic film, the relative error rate is reduced by 46.8% (26.5%) when the source data comes from 10-layer (8-layer) films and the target data comes from 8-layer (10-layer) films. Second, we show that the relative error rate is decreased by 22% when knowledge is transferred between two very different physical scenarios: transmission from multilayer films and scattering from multilayer nanoparticles. Finally, we propose a multi-task learning method to improve the performance of different physical scenarios simultaneously in which each task only has a small dataset.



### Single Shot Scene Text Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1808.09044v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.09044v1)
- **Published**: 2018-08-27 21:59:26+00:00
- **Updated**: 2018-08-27 21:59:26+00:00
- **Authors**: Lluís Gómez, Andrés Mafla, Marçal Rusiñol, Dimosthenis Karatzas
- **Comment**: ECCV 2018
- **Journal**: None
- **Summary**: Textual information found in scene images provides high level semantic information about the image and its context and it can be leveraged for better scene understanding. In this paper we address the problem of scene text retrieval: given a text query, the system must return all images containing the queried text. The novelty of the proposed model consists in the usage of a single shot CNN architecture that predicts at the same time bounding boxes and a compact text representation of the words in them. In this way, the text based image retrieval task can be casted as a simple nearest neighbor search of the query text representation over the outputs of the CNN over the entire image database. Our experiments demonstrate that the proposed architecture outperforms previous state-of-the-art while it offers a significant increase in processing speed.



