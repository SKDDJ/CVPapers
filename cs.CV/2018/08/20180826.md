# Arxiv Papers in cs.CV on 2018-08-26
### Efficient Single Image Super Resolution using Enhanced Learned Group Convolutions
- **Arxiv ID**: http://arxiv.org/abs/1808.08509v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.08509v1)
- **Published**: 2018-08-26 06:16:29+00:00
- **Updated**: 2018-08-26 06:16:29+00:00
- **Authors**: Vandit Jain, Prakhar Bansal, Abhinav Kumar Singh, Rajeev Srivastava
- **Comment**: Accepted in International Conference on Neural Information Processing
  (ICONIP 2018)
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) have demonstrated great results for the single-image super-resolution (SISR) problem. Currently, most CNN algorithms promote deep and computationally expensive models to solve SISR. However, we propose a novel SISR method that uses relatively less number of computations. On training, we get group convolutions that have unused connections removed. We have refined this system specifically for the task at hand by removing unnecessary modules from original CondenseNet. Further, a reconstruction network consisting of deconvolutional layers has been used in order to upscale to high resolution. All these steps significantly reduce the number of computations required at testing time. Along with this, bicubic upsampled input is added to the network output for easier learning. Our model is named SRCondenseNet. We evaluate the method using various benchmark datasets and show that it performs favourably against the state-of-the-art methods in terms of both accuracy and number of computations required.



### Blind Ptychography by Douglas-Rachford Splitting
- **Arxiv ID**: http://arxiv.org/abs/1809.00962v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1809.00962v3)
- **Published**: 2018-08-26 07:51:43+00:00
- **Updated**: 2018-10-30 06:05:56+00:00
- **Authors**: A. Fannjiang, Z. Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Blind ptychography is the scanning version of coherent diffractive imaging which seeks to recover both the object and the probe simultaneously. Based on alternating minimization by Douglas-Rachford splitting, AMDRS is a blind ptychographic algorithm informed by the uniqueness theory, the Poisson noise model and the stability analysis. Enhanced by the initialization method and the use of a randomly phased mask, AMDRS converges globally and geometrically. Three boundary conditions are considered in the simulations: periodic, dark-field and bright-field boundary conditions. The dark-field boundary condition is suited for isolated objects while the bright-field boundary condition is for non-isolated objects. The periodic boundary condition is a mathematically convenient reference point. Depending on the avail- ability of the boundary prior the dark-field and the bright-field boundary conditions may or may not be enforced in the reconstruction. Not surprisingly, enforcing the boundary condition improves the rate of convergence, sometimes in a significant way. Enforcing the bright-field condition in the reconstruction can also remove the linear phase ambiguity.



### A MapReduce based Big-data Framework for Object Extraction from Mosaic Satellite Images
- **Arxiv ID**: http://arxiv.org/abs/1808.08528v1
- **DOI**: None
- **Categories**: **cs.DC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1808.08528v1)
- **Published**: 2018-08-26 10:32:40+00:00
- **Updated**: 2018-08-26 10:32:40+00:00
- **Authors**: Suleyman Eken, Ahmet Sayar
- **Comment**: Proceedings of Doctoral Consortium on Internet of Things At Roma,
  Italy 2016
- **Journal**: None
- **Summary**: We propose a framework stitching of vector representations of large scale raster mosaic images in distributed computing model. In this way, the negative effect of the lack of resources of the central system and scalability problem can be eliminated. The product obtained by this study can be used in applications requiring spatial and temporal analysis on big satellite map images. This study also shows that big data frameworks are not only used in applications of text-based data mining and machine learning algorithms, but also used in applications of algorithms in image processing. The effectiveness of the product realized with this project is also going to be proven by scalability and performance tests performed on real world LandSat-8 satellite images.



### DeepTracker: Visualizing the Training Process of Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1808.08531v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1808.08531v1)
- **Published**: 2018-08-26 11:09:44+00:00
- **Updated**: 2018-08-26 11:09:44+00:00
- **Authors**: Dongyu Liu, Weiwei Cui, Kai Jin, Yuxiao Guo, Huamin Qu
- **Comment**: Published at ACM Transactions on Intelligent Systems and Technology
  (in press)
- **Journal**: None
- **Summary**: Deep convolutional neural networks (CNNs) have achieved remarkable success in various fields. However, training an excellent CNN is practically a trial-and-error process that consumes a tremendous amount of time and computer resources. To accelerate the training process and reduce the number of trials, experts need to understand what has occurred in the training process and why the resulting CNN behaves as such. However, current popular training platforms, such as TensorFlow, only provide very little and general information, such as training/validation errors, which is far from enough to serve this purpose. To bridge this gap and help domain experts with their training tasks in a practical environment, we propose a visual analytics system, DeepTracker, to facilitate the exploration of the rich dynamics of CNN training processes and to identify the unusual patterns that are hidden behind the huge amount of training log. Specifically,we combine a hierarchical index mechanism and a set of hierarchical small multiples to help experts explore the entire training log from different levels of detail. We also introduce a novel cube-style visualization to reveal the complex correlations among multiple types of heterogeneous training data including neuron weights, validation images, and training iterations. Three case studies are conducted to demonstrate how DeepTracker provides its users with valuable knowledge in an industry-level CNN training process, namely in our case, training ResNet-50 on the ImageNet dataset. We show that our method can be easily applied to other state-of-the-art "very deep" CNN models.



### Scale Drift Correction of Camera Geo-Localization using Geo-Tagged Images
- **Arxiv ID**: http://arxiv.org/abs/1808.08544v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.08544v1)
- **Published**: 2018-08-26 12:43:34+00:00
- **Updated**: 2018-08-26 12:43:34+00:00
- **Authors**: Kazuya Iwami, Satoshi Ikehata, Kiyoharu Aizawa
- **Comment**: ECCV Workshop CVRSUAD
- **Journal**: None
- **Summary**: Camera geo-localization from a monocular video is a fundamental task for video analysis and autonomous navigation. Although 3D reconstruction is a key technique to obtain camera poses, monocular 3D reconstruction in a large environment tends to result in the accumulation of errors in rotation, translation, and especially in scale: a problem known as scale drift. To overcome these errors, we propose a novel framework that integrates incremental structure from motion (SfM) and a scale drift correction method utilizing geo-tagged images, such as those provided by Google Street View. Our correction method begins by obtaining sparse 6-DoF correspondences between the reconstructed 3D map coordinate system and the world coordinate system, by using geo-tagged images. Then, it corrects scale drift by applying pose graph optimization over Sim(3) constraints and bundle adjustment. Experimental evaluations on large-scale datasets show that the proposed framework not only sufficiently corrects scale drift, but also achieves accurate geo-localization in a kilometer-scale environment.



### Rain Streak Removal for Single Image via Kernel Guided CNN
- **Arxiv ID**: http://arxiv.org/abs/1808.08545v2
- **DOI**: 10.1109/TNNLS.2020.3015897
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.08545v2)
- **Published**: 2018-08-26 12:53:18+00:00
- **Updated**: 2018-08-28 09:11:42+00:00
- **Authors**: Ye-Tao Wang, Xi-Le Zhao, Tai-Xiang Jiang, Liang-Jian Deng, Yi Chang, Ting-Zhu Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Rain streak removal is an important issue and has recently been investigated extensively. Existing methods, especially the newly emerged deep learning methods, could remove the rain streaks well in many cases. However the essential factor in the generative procedure of the rain streaks, i.e., the motion blur, which leads to the line pattern appearances, were neglected by the deep learning rain streaks approaches and this resulted in over-derain or under-derain results. In this paper, we propose a novel rain streak removal approach using a kernel guided convolutional neural network (KGCNN), achieving the state-of-the-art performance with simple network architectures. We first model the rain streak interference with its motion blur mechanism. Then, our framework starts with learning the motion blur kernel, which is determined by two factors including angle and length, by a plain neural network, denoted as parameter net, from a patch of the texture component. Then, after a dimensionality stretching operation, the learned motion blur kernel is stretched into a degradation map with the same spatial size as the rainy patch. The stretched degradation map together with the texture patch is subsequently input into a derain convolutional network, which is a typical ResNet architecture and trained to output the rain streaks with the guidance of the learned motion blur kernel. Experiments conducted on extensive synthetic and real data demonstrate the effectiveness of the proposed method, which preserves the texture and the contrast while removing the rain streaks.



### Convolutional Neural Networks for Aerial Vehicle Detection and Recognition
- **Arxiv ID**: http://arxiv.org/abs/1808.08560v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.08560v1)
- **Published**: 2018-08-26 14:29:57+00:00
- **Updated**: 2018-08-26 14:29:57+00:00
- **Authors**: Amir Soleimani, Nasser M. Nasrabadi, Elias Griffith, Jason Ralph, Simon Maskell
- **Comment**: This paper has been accepted in the National Aerospace Electronics
  Conference (NAECON) 2018 and would be indexed in IEEE
- **Journal**: None
- **Summary**: This paper investigates the problem of aerial vehicle recognition using a text-guided deep convolutional neural network classifier. The network receives an aerial image and a desired class, and makes a yes or no output by matching the image and the textual description of the desired class. We train and test our model on a synthetic aerial dataset and our desired classes consist of the combination of the class types and colors of the vehicles. This strategy helps when considering more classes in testing than in training.



### Automatic 3D bi-ventricular segmentation of cardiac images by a shape-refined multi-task deep learning approach
- **Arxiv ID**: http://arxiv.org/abs/1808.08578v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1808.08578v3)
- **Published**: 2018-08-26 15:42:50+00:00
- **Updated**: 2019-07-13 19:39:39+00:00
- **Authors**: Jinming Duan, Ghalib Bello, Jo Schlemper, Wenjia Bai, Timothy J W Dawes, Carlo Biffi, Antonio de Marvao, Georgia Doumou, Declan P O'Regan, Daniel Rueckert
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning approaches have achieved state-of-the-art performance in cardiac magnetic resonance (CMR) image segmentation. However, most approaches have focused on learning image intensity features for segmentation, whereas the incorporation of anatomical shape priors has received less attention. In this paper, we combine a multi-task deep learning approach with atlas propagation to develop a shape-constrained bi-ventricular segmentation pipeline for short-axis CMR volumetric images. The pipeline first employs a fully convolutional network (FCN) that learns segmentation and landmark localisation tasks simultaneously. The architecture of the proposed FCN uses a 2.5D representation, thus combining the computational advantage of 2D FCNs networks and the capability of addressing 3D spatial consistency without compromising segmentation accuracy. Moreover, the refinement step is designed to explicitly enforce a shape constraint and improve segmentation quality. This step is effective for overcoming image artefacts (e.g. due to different breath-hold positions and large slice thickness), which preclude the creation of anatomically meaningful 3D cardiac shapes. The proposed pipeline is fully automated, due to network's ability to infer landmarks, which are then used downstream in the pipeline to initialise atlas propagation. We validate the pipeline on 1831 healthy subjects and 649 subjects with pulmonary hypertension. Extensive numerical experiments on the two datasets demonstrate that our proposed method is robust and capable of producing accurate, high-resolution and anatomically smooth bi-ventricular 3D models, despite the artefacts in input CMR volumes.



### CGIntrinsics: Better Intrinsic Image Decomposition through Physically-Based Rendering
- **Arxiv ID**: http://arxiv.org/abs/1808.08601v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.08601v3)
- **Published**: 2018-08-26 17:58:46+00:00
- **Updated**: 2018-12-05 22:34:49+00:00
- **Authors**: Zhengqi Li, Noah Snavely
- **Comment**: Paper for 'CGIntrinsics: Better Intrinsic Image Decomposition through
  Physically-Based Rendering' published in ECCV, 2018
- **Journal**: None
- **Summary**: Intrinsic image decomposition is a challenging, long-standing computer vision problem for which ground truth data is very difficult to acquire. We explore the use of synthetic data for training CNN-based intrinsic image decomposition models, then applying these learned models to real-world images. To that end, we present \ICG, a new, large-scale dataset of physically-based rendered images of scenes with full ground truth decompositions. The rendering process we use is carefully designed to yield high-quality, realistic images, which we find to be crucial for this problem domain. We also propose a new end-to-end training method that learns better decompositions by leveraging \ICG, and optionally IIW and SAW, two recent datasets of sparse annotations on real-world images. Surprisingly, we find that a decomposition network trained solely on our synthetic data outperforms the state-of-the-art on both IIW and SAW, and performance improves even further when IIW and SAW data is added during training. Our work demonstrates the suprising effectiveness of carefully-rendered synthetic data for the intrinsic images task.



### Label and Sample: Efficient Training of Vehicle Object Detector from Sparsely Labeled Data
- **Arxiv ID**: http://arxiv.org/abs/1808.08603v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.08603v1)
- **Published**: 2018-08-26 18:10:23+00:00
- **Updated**: 2018-08-26 18:10:23+00:00
- **Authors**: Xinlei Pan, Sung-Li Chiang, John Canny
- **Comment**: None
- **Journal**: None
- **Summary**: Self-driving vehicle vision systems must deal with an extremely broad and challenging set of scenes. They can potentially exploit an enormous amount of training data collected from vehicles in the field, but the volumes are too large to train offline naively. Not all training instances are equally valuable though, and importance sampling can be used to prioritize which training images to collect. This approach assumes that objects in images are labeled with high accuracy. To generate accurate labels in the field, we exploit the spatio-temporal coherence of vehicle video. We use a near-to-far labeling strategy by first labeling large, close objects in the video, and tracking them back in time to induce labels on small distant presentations of those objects. In this paper we demonstrate the feasibility of this approach in several steps. First, we note that an optimal subset (relative to all the objects encountered and labeled) of labeled objects in images can be obtained by importance sampling using gradients of the recognition network. Next we show that these gradients can be approximated with very low error using the loss function, which is already available when the CNN is running inference. Then, we generalize these results to objects in a larger scene using an object detection system. Finally, we describe a self-labeling scheme using object tracking. Objects are tracked back in time (near-to-far) and labels of near objects are used to check accuracy of those objects in the far field. We then evaluate the accuracy of models trained on importance sampled data vs models trained on complete data.



### Single Image Dehazing Based on Generic Regularity
- **Arxiv ID**: http://arxiv.org/abs/1808.08610v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.08610v1)
- **Published**: 2018-08-26 18:47:08+00:00
- **Updated**: 2018-08-26 18:47:08+00:00
- **Authors**: Kushal Borkar, Snehasis Mukherjee
- **Comment**: Submitted to journal
- **Journal**: This article is under consideration in the journal of Computer
  Vision and Image Understanding, 2019
- **Summary**: This paper proposes a novel technique for single image dehazing. Most of the state-of-the-art methods for single image dehazing relies either on Dark Channel Prior (DCP) or on Color line. The proposed method combines the two different approaches. We initially compute the dark channel prior and then apply a Nearest-Neighbor (NN) based regularization technique to obtain a smooth transmission map of the hazy image. We consider the effect of airlight on the image by using the color line model to assess the commitment of airlight in each patch of the image and interpolate at the local neighborhood where the estimate is unreliable. The NN based regularization of the DCP can remove the haze, whereas, the color line based interpolation of airlight effect makes the proposed system robust against the variation of haze within an image due to multiple light sources. The proposed method is tested on benchmark datasets and shows promising results compared to the state-of-the-art, including the deep learning based methods, which require a huge computational setup. Moreover, the proposed method outperforms the recent deep learning based methods when applied on images with sky regions.



### Online Human Activity Recognition using Low-Power Wearable Devices
- **Arxiv ID**: http://arxiv.org/abs/1808.08615v2
- **DOI**: 10.1145/3240765.3240833
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.08615v2)
- **Published**: 2018-08-26 19:49:20+00:00
- **Updated**: 2019-02-04 21:08:15+00:00
- **Authors**: Ganapati Bhat, Ranadeep Deb, Vatika Vardhan Chaurasia, Holly Shill, Umit Y. Ogras
- **Comment**: This is in proceedings of ICCAD 2018. The datasets are available at
  https://github.com/gmbhat/human-activity-recognition
- **Journal**: None
- **Summary**: Human activity recognition~(HAR) has attracted significant research interest due to its applications in health monitoring and patient rehabilitation. Recent research on HAR focuses on using smartphones due to their widespread use. However, this leads to inconvenient use, limited choice of sensors and inefficient use of resources, since smartphones are not designed for HAR. This paper presents the first HAR framework that can perform both online training and inference. The proposed framework starts with a novel technique that generates features using the fast Fourier and discrete wavelet transforms of a textile-based stretch sensor and accelerometer. Using these features, we design an artificial neural network classifier which is trained online using the policy gradient algorithm. Experiments on a low power IoT device (TI-CC2650 MCU) with nine users show 97.7% accuracy in identifying six activities and their transitions with less than 12.5 mW power consumption.



### Twin-GAN -- Unpaired Cross-Domain Image Translation with Weight-Sharing GANs
- **Arxiv ID**: http://arxiv.org/abs/1809.00946v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.00946v1)
- **Published**: 2018-08-26 23:09:03+00:00
- **Updated**: 2018-08-26 23:09:03+00:00
- **Authors**: Jerry Li
- **Comment**: None
- **Journal**: None
- **Summary**: We present a framework for translating unlabeled images from one domain into analog images in another domain. We employ a progressively growing skip-connected encoder-generator structure and train it with a GAN loss for realistic output, a cycle consistency loss for maintaining same-domain translation identity, and a semantic consistency loss that encourages the network to keep the input semantic features in the output. We apply our framework on the task of translating face images, and show that it is capable of learning semantic mappings for face images with no supervised one-to-one image mapping.



