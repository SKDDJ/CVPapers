# Arxiv Papers in cs.CV on 2018-08-01
### Toward Multimodal Interaction in Scalable Visual Digital Evidence Visualization Using Computer Vision Techniques and ISS
- **Arxiv ID**: http://arxiv.org/abs/1808.00118v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1808.00118v1)
- **Published**: 2018-08-01 00:28:32+00:00
- **Updated**: 2018-08-01 00:28:32+00:00
- **Authors**: Serguei A. Mokhov, Miao Song, Jashanjot Singh, Joey Paquet, Mourad Debbabi, Sudhir Mudur
- **Comment**: reformatted; ICPRAI 2018 conference proceedings, pp. 151-157,
  CENPARMI, Concordia University, Montreal
- **Journal**: None
- **Summary**: Visualization requirements in Forensic Lucid have to do with different levels of case knowledge abstraction, representation, aggregation, as well as the operational aspects as the final long-term goal of this proposal. It encompasses anything from the finer detailed representation of hierarchical contexts to Forensic Lucid programs, to the documented evidence and its management, its linkage to programs, to evaluation, and to the management of GIPSY software networks. This includes an ability to arbitrarily switch between those views combined with usable multimodal interaction. The purpose is to determine how the findings can be applied to Forensic Lucid and investigation case management. It is also natural to want a convenient and usable evidence visualization, its semantic linkage and the reasoning machinery for it. Thus, we propose a scalable management, visualization, and evaluation of digital evidence using the modified interactive 3D documentary system - Illimitable Space System - (ISS) to represent, semantically link, and provide a usable interface to digital investigators that is navigable via different multimodal interaction techniques using Computer Vision techniques including gestures, as well as eye-gaze and audio.



### FMCode: A 3D In-the-Air Finger Motion Based User Login Framework for Gesture Interface
- **Arxiv ID**: http://arxiv.org/abs/1808.00130v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/1808.00130v1)
- **Published**: 2018-08-01 01:24:22+00:00
- **Updated**: 2018-08-01 01:24:22+00:00
- **Authors**: Duo Lu, Dijiang Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Applications using gesture-based human-computer interface require a new user login method with gestures because it does not have a traditional input method to type a password. However, due to various challenges, existing gesture-based authentication systems are generally considered too weak to be useful in practice. In this paper, we propose a unified user login framework using 3D in-air-handwriting, called FMCode. We define new types of features critical to distinguish legitimate users from attackers and utilize Support Vector Machine (SVM) for user authentication. The features and data-driven models are specially designed to accommodate minor behavior variations that existing gesture authentication methods neglect. In addition, we use deep neural network approaches to efficiently identify the user based on his or her in-air-handwriting, which avoids expansive account database search methods employed by existing work. On a dataset collected by us with over 100 users, our prototype system achieves 0.1% and 0.5% best Equal Error Rate (EER) for user authentication, as well as 96.7% and 94.3% accuracy for user identification, using two types of gesture input devices. Compared to existing behavioral biometric systems using gesture and in-air-handwriting, our framework achieves the state-of-the-art performance. In addition, our experimental results show that FMCode is capable to defend against client-side spoofing attacks, and it performs persistently in the long run. These results and discoveries pave the way to practical usage of gesture-based user login over the gesture interface.



### Multi-modal Cycle-consistent Generalized Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1808.00136v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.00136v2)
- **Published**: 2018-08-01 01:47:55+00:00
- **Updated**: 2018-08-02 03:55:52+00:00
- **Authors**: Rafael Felix, B. G. Vijay Kumar, Ian Reid, Gustavo Carneiro
- **Comment**: Accepted at ECCV 2018, 15th European Conference on Computer Vision,
  September 8 to 14, 2018
- **Journal**: None
- **Summary**: In generalized zero shot learning (GZSL), the set of classes are split into seen and unseen classes, where training relies on the semantic features of the seen and unseen classes and the visual representations of only the seen classes, while testing uses the visual representations of the seen and unseen classes. Current methods address GZSL by learning a transformation from the visual to the semantic space, exploring the assumption that the distribution of classes in the semantic and visual spaces is relatively similar. Such methods tend to transform unseen testing visual representations into one of the seen classes' semantic features instead of the semantic features of the correct unseen class, resulting in low accuracy GZSL classification. Recently, generative adversarial networks (GAN) have been explored to synthesize visual representations of the unseen classes from their semantic features - the synthesized representations of the seen and unseen classes are then used to train the GZSL classifier. This approach has been shown to boost GZSL classification accuracy, however, there is no guarantee that synthetic visual representations can generate back their semantic feature in a multi-modal cycle-consistent manner. This constraint can result in synthetic visual representations that do not represent well their semantic features. In this paper, we propose the use of such constraint based on a new regularization for the GAN training that forces the generated visual features to reconstruct their original semantic features. Once our model is trained with this multi-modal cycle-consistent semantic compatibility, we can then synthesize more representative visual representations for the seen and, more importantly, for the unseen classes. Our proposed approach shows the best GZSL classification results in the field in several publicly available datasets.



### Action Anticipation By Predicting Future Dynamic Images
- **Arxiv ID**: http://arxiv.org/abs/1808.00141v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.00141v1)
- **Published**: 2018-08-01 02:10:50+00:00
- **Updated**: 2018-08-01 02:10:50+00:00
- **Authors**: Cristian Rodriguez, Basura Fernando, Hongdong Li
- **Comment**: 14 pages
- **Journal**: None
- **Summary**: Human action-anticipation methods predict what is the future action by observing only a few portion of an action in progress. This is critical for applications where computers have to react to human actions as early as possible such as autonomous driving, human-robotic interaction, assistive robotics among others. In this paper, we present a method for human action anticipation by predicting the most plausible future human motion. We represent human motion using Dynamic Images and make use of tailored loss functions to encourage a generative model to produce accurate future motion prediction. Our method outperforms the currently best performing action-anticipation methods by 4% on JHMDB-21, 5.2% on UT-Interaction and 5.1% on UCF 101-24 benchmarks.



### Depth Estimation via Affinity Learned with Convolutional Spatial Propagation Network
- **Arxiv ID**: http://arxiv.org/abs/1808.00150v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.00150v1)
- **Published**: 2018-08-01 03:23:06+00:00
- **Updated**: 2018-08-01 03:23:06+00:00
- **Authors**: Xinjing Cheng, Peng Wang, Ruigang Yang
- **Comment**: 14 pages, 8 figures, ECCV 2018
- **Journal**: None
- **Summary**: Depth estimation from a single image is a fundamental problem in computer vision. In this paper, we propose a simple yet effective convolutional spatial propagation network (CSPN) to learn the affinity matrix for depth prediction. Specifically, we adopt an efficient linear propagation model, where the propagation is performed with a manner of recurrent convolutional operation, and the affinity among neighboring pixels is learned through a deep convolutional neural network (CNN). We apply the designed CSPN to two depth estimation tasks given a single image: (1) To refine the depth output from state-of-the-art (SOTA) existing methods; and (2) to convert sparse depth samples to a dense depth map by embedding the depth samples within the propagation procedure. The second task is inspired by the availability of LIDARs that provides sparse but accurate depth measurements. We experimented the proposed CSPN over two popular benchmarks for depth estimation, i.e. NYU v2 and KITTI, where we show that our proposed approach improves in not only quality (e.g., 30% more reduction in depth error), but also speed (e.g., 2 to 5 times faster) than prior SOTA methods.



### Instance-level Human Parsing via Part Grouping Network
- **Arxiv ID**: http://arxiv.org/abs/1808.00157v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.00157v1)
- **Published**: 2018-08-01 03:51:59+00:00
- **Updated**: 2018-08-01 03:51:59+00:00
- **Authors**: Ke Gong, Xiaodan Liang, Yicheng Li, Yimin Chen, Ming Yang, Liang Lin
- **Comment**: Accepted by ECCV 2018 (Oral)
- **Journal**: None
- **Summary**: Instance-level human parsing towards real-world human analysis scenarios is still under-explored due to the absence of sufficient data resources and technical difficulty in parsing multiple instances in a single pass. Several related works all follow the "parsing-by-detection" pipeline that heavily relies on separately trained detection models to localize instances and then performs human parsing for each instance sequentially. Nonetheless, two discrepant optimization targets of detection and parsing lead to suboptimal representation learning and error accumulation for final results. In this work, we make the first attempt to explore a detection-free Part Grouping Network (PGN) for efficiently parsing multiple people in an image in a single pass. Our PGN reformulates instance-level human parsing as two twinned sub-tasks that can be jointly learned and mutually refined via a unified network: 1) semantic part segmentation for assigning each pixel as a human part (e.g., face, arms); 2) instance-aware edge detection to group semantic parts into distinct person instances. Thus the shared intermediate representation would be endowed with capabilities in both characterizing fine-grained parts and inferring instance belongings of each part. Finally, a simple instance partition process is employed to get final results during inference. We conducted experiments on PASCAL-Person-Part dataset and our PGN outperforms all state-of-the-art methods. Furthermore, we show its superiority on a newly collected multi-person parsing dataset (CIHP) including 38,280 diverse images, which is the largest dataset so far and can facilitate more advanced human analysis. The CIHP benchmark and our source code are available at http://sysu-hcp.net/lip/.



### Shuffle-Then-Assemble: Learning Object-Agnostic Visual Relationship Features
- **Arxiv ID**: http://arxiv.org/abs/1808.00171v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.00171v1)
- **Published**: 2018-08-01 05:20:28+00:00
- **Updated**: 2018-08-01 05:20:28+00:00
- **Authors**: Xu Yang, Hanwang Zhang, Jianfei Cai
- **Comment**: None
- **Journal**: None
- **Summary**: Due to the fact that it is prohibitively expensive to completely annotate visual relationships, i.e., the (obj1, rel, obj2) triplets, relationship models are inevitably biased to object classes of limited pairwise patterns, leading to poor generalization to rare or unseen object combinations. Therefore, we are interested in learning object-agnostic visual features for more generalizable relationship models. By "agnostic", we mean that the feature is less likely biased to the classes of paired objects. To alleviate the bias, we propose a novel \texttt{Shuffle-Then-Assemble} pre-training strategy. First, we discard all the triplet relationship annotations in an image, leaving two unpaired object domains without obj1-obj2 alignment. Then, our feature learning is to recover possible obj1-obj2 pairs. In particular, we design a cycle of residual transformations between the two domains, to capture shared but not object-specific visual patterns. Extensive experiments on two visual relationship benchmarks show that by using our pre-trained features, naive relationship models can be consistently improved and even outperform other state-of-the-art relationship models. Code has been made available at: \url{https://github.com/yangxuntu/vrd}.



### Real-time image-based instrument classification for laparoscopic surgery
- **Arxiv ID**: http://arxiv.org/abs/1808.00178v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.00178v1)
- **Published**: 2018-08-01 06:08:16+00:00
- **Updated**: 2018-08-01 06:08:16+00:00
- **Authors**: Sebastian Bodenstedt, Antonia Ohnemus, Darko Katic, Anna-Laura Wekerle, Martin Wagner, Hannes Kenngott, Beat Müller-Stich, Rüdiger Dillmann, Stefanie Speidel
- **Comment**: Workshop paper accepted and presented at Modeling and Monitoring of
  Computer Assisted Interventions (M2CAI) (2015)
- **Journal**: Modeling and Monitoring of Computer Assisted Interventions (M2CAI)
  (2015)
- **Summary**: During laparoscopic surgery, context-aware assistance systems aim to alleviate some of the difficulties the surgeon faces. To ensure that the right information is provided at the right time, the current phase of the intervention has to be known. Real-time locating and classification the surgical tools currently in use are key components of both an activity-based phase recognition and assistance generation.   In this paper, we present an image-based approach that detects and classifies tools during laparoscopic interventions in real-time. First, potential instrument bounding boxes are detected using a pixel-wise random forest segmentation. Each of these bounding boxes is then classified using a cascade of random forest. For this, multiple features, such as histograms over hue and saturation, gradients and SURF feature, are extracted from each detected bounding box.   We evaluated our approach on five different videos from two different types of procedures. We distinguished between the four most common classes of instruments (LigaSure, atraumatic grasper, aspirator, clip applier) and background. Our method succesfully located up to 86% of all instruments respectively. On manually provided bounding boxes, we achieve a instrument type recognition rate of up to 58% and on automatically detected bounding boxes up to 49%.   To our knowledge, this is the first approach that allows an image-based classification of surgical tools in a laparoscopic setting in real-time.



### From Thumbnails to Summaries - A single Deep Neural Network to Rule Them All
- **Arxiv ID**: http://arxiv.org/abs/1808.00184v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1808.00184v1)
- **Published**: 2018-08-01 06:24:39+00:00
- **Updated**: 2018-08-01 06:24:39+00:00
- **Authors**: Hongxiang Gu, Viswanathan Swaminathan
- **Comment**: 6 pages, 2 figures, IEEE International Conference on Multimedia and
  Expo (ICME) 2018
- **Journal**: None
- **Summary**: Video summaries come in many forms, from traditional single-image thumbnails, animated thumbnails, storyboards, to trailer-like video summaries. Content creators use the summaries to display the most attractive portion of their videos; the users use them to quickly evaluate if a video is worth watching. All forms of summaries are essential to video viewers, content creators, and advertisers. Often video content management systems have to generate multiple versions of summaries that vary in duration and presentational forms. We present a framework ReconstSum that utilizes LSTM-based autoencoder architecture to extract and select a sparse subset of video frames or keyshots that optimally represent the input video in an unsupervised manner. The encoder selects a subset from the input video while the decoder seeks to reconstruct the video from the selection. The goal is to minimize the difference between the original input video and the reconstructed video. Our method is easily extendable to generate a variety of applications including static video thumbnails, animated thumbnails, storyboards and "trailer-like" highlights. We specifically study and evaluate two most popular use cases: thumbnail generation and storyboard generation. We demonstrate that our methods generate better results than the state-of-the-art techniques in both use cases.



### Graph R-CNN for Scene Graph Generation
- **Arxiv ID**: http://arxiv.org/abs/1808.00191v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1808.00191v1)
- **Published**: 2018-08-01 06:50:19+00:00
- **Updated**: 2018-08-01 06:50:19+00:00
- **Authors**: Jianwei Yang, Jiasen Lu, Stefan Lee, Dhruv Batra, Devi Parikh
- **Comment**: 16 pages, ECCV 2018 camera ready
- **Journal**: None
- **Summary**: We propose a novel scene graph generation model called Graph R-CNN, that is both effective and efficient at detecting objects and their relations in images. Our model contains a Relation Proposal Network (RePN) that efficiently deals with the quadratic number of potential relations between objects in an image. We also propose an attentional Graph Convolutional Network (aGCN) that effectively captures contextual information between objects and relations. Finally, we introduce a new evaluation metric that is more holistic and realistic than existing metrics. We report state-of-the-art performance on scene graph generation as evaluated using both existing and our proposed metrics.



### Reinforced Evolutionary Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/1808.00193v3
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1808.00193v3)
- **Published**: 2018-08-01 06:53:53+00:00
- **Updated**: 2019-04-10 11:12:02+00:00
- **Authors**: Yukang Chen, Gaofeng Meng, Qian Zhang, Shiming Xiang, Chang Huang, Lisen Mu, Xinggang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Neural Architecture Search (NAS) is an important yet challenging task in network design due to its high computational consumption. To address this issue, we propose the Reinforced Evolutionary Neural Architecture Search (RE- NAS), which is an evolutionary method with the reinforced mutation for NAS. Our method integrates reinforced mutation into an evolution algorithm for neural architecture exploration, in which a mutation controller is introduced to learn the effects of slight modifications and make mutation actions. The reinforced mutation controller guides the model population to evolve efficiently. Furthermore, as child models can inherit parameters from their parents during evolution, our method requires very limited computational resources. In experiments, we conduct the proposed search method on CIFAR-10 and obtain a powerful network architecture, RENASNet. This architecture achieves a competitive result on CIFAR-10. The explored network architecture is transferable to ImageNet and achieves a new state-of-the-art accuracy, i.e., 75.7% top-1 accuracy with 5.36M parameters on mobile ImageNet. We further test its performance on semantic segmentation with DeepLabv3 on the PASCAL VOC. RENASNet outperforms MobileNet-v1, MobileNet-v2 and NASNet. It achieves 75.83% mIOU without being pre-trained on COCO.



### A Pseudo Multi-Exposure Fusion Method Using Single Image
- **Arxiv ID**: http://arxiv.org/abs/1808.00195v1
- **DOI**: 10.1587/transfun.E101.A.1806
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.00195v1)
- **Published**: 2018-08-01 07:03:11+00:00
- **Updated**: 2018-08-01 07:03:11+00:00
- **Authors**: Yuma Kinoshita, Sayaka Shiota, Hitoshi Kiya
- **Comment**: To appear in IEICE Trans. Fundamentals, vol.E101-A, no.11, November
  2018
- **Journal**: None
- **Summary**: This paper proposes a novel pseudo multi-exposure image fusion method based on a single image. Multi-exposure image fusion is used to produce images without saturation regions, by using photos with different exposures. However, it is difficult to take photos suited for the multi-exposure image fusion when we take a photo of dynamic scenes or record a video. In addition, the multi-exposure image fusion cannot be applied to existing images with a single exposure or videos. The proposed method enables us to produce pseudo multi-exposure images from a single image. To produce multi-exposure images, the proposed method utilizes the relationship between the exposure values and pixel values, which is obtained by assuming that a digital camera has a linear response function. Moreover, it is shown that the use of a local contrast enhancement method allows us to produce pseudo multi-exposure images with higher quality. Most of conventional multi-exposure image fusion methods are also applicable to the proposed multi-exposure images. Experimental results show the effectiveness of the proposed method by comparing the proposed one with conventional ones.



### Connecting Visual Experiences using Max-flow Network with Application to Visual Localization
- **Arxiv ID**: http://arxiv.org/abs/1808.00208v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.00208v1)
- **Published**: 2018-08-01 07:46:58+00:00
- **Updated**: 2018-08-01 07:46:58+00:00
- **Authors**: A. H. Abdul Hafez, Nakul Agarwal, C. V. Jawahar
- **Comment**: None
- **Journal**: None
- **Summary**: We are motivated by the fact that multiple representations of the environment are required to stand for the changes in appearance with time and for changes that appear in a cyclic manner. These changes are, for example, from day to night time, and from day to day across seasons. In such situations, the robot visits the same routes multiple times and collects different appearances of it. Multiple visual experiences usually find robotic vision applications like visual localization, mapping, place recognition, and autonomous navigation. The novelty in this paper is an algorithm that connects multiple visual experiences via aligning multiple image sequences. This problem is solved by finding the maximum flow in a directed graph flow-network, whose vertices represent the matches between frames in the test and reference sequences. Edges of the graph represent the cost of these matches. The problem of finding the best match is reduced to finding the minimum-cut surface, which is solved as a maximum flow network problem. Application to visual localization is considered in this paper to show the effectiveness of the proposed multiple image sequence alignment method, without loosing its generality. Experimental evaluations show that the precision of sequence matching is improved by considering multiple visual sequences for the same route, and that the method performs favorably against state-of-the-art single representation methods like SeqSLAM and ABLE-M.



### Tumor Delineation For Brain Radiosurgery by a ConvNet and Non-Uniform Patch Generation
- **Arxiv ID**: http://arxiv.org/abs/1808.00244v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.00244v1)
- **Published**: 2018-08-01 09:41:50+00:00
- **Updated**: 2018-08-01 09:41:50+00:00
- **Authors**: Egor Krivov, Valery Kostjuchenko, Alexandra Dalechina, Boris Shirokikh, Gleb karchuk, Alexander Denisenko, Andrey Golanov, Mikhail Belyaev
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning methods are actively used for brain lesion segmentation. One of the most popular models is DeepMedic, which was developed for segmentation of relatively large lesions like glioma and ischemic stroke. In our work, we consider segmentation of brain tumors appropriate to stereotactic radiosurgery which limits typical lesion sizes. These differences in target volumes lead to a large number of false negatives (especially for small lesions) as well as to an increased number of false positives for DeepMedic. We propose a new patch-sampling procedure to increase network performance for small lesions. We used a 6-year dataset from a stereotactic radiosurgery center. To evaluate our approach, we conducted experiments with the three most frequent brain tumors: metastasis, meningioma, schwannoma. In addition to cross-validation, we estimated quality on a hold-out test set which was collected several years later than the train one. The experimental results show solid improvements in both cases.



### Category-level 6D Object Pose Recovery in Depth Images
- **Arxiv ID**: http://arxiv.org/abs/1808.00255v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.00255v1)
- **Published**: 2018-08-01 10:36:21+00:00
- **Updated**: 2018-08-01 10:36:21+00:00
- **Authors**: Caner Sahin, Tae-Kyun Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Intra-class variations, distribution shifts among source and target domains are the major challenges of category-level tasks. In this study, we address category-level full 6D object pose estimation in the context of depth modality, introducing a novel part-based architecture that can tackle the above-mentioned challenges. Our architecture particularly adapts the distribution shifts arising from shape discrepancies, and naturally removes the variations of texture, illumination, pose, etc., so we call it as "Intrinsic Structure Adaptor (ISA)". We engineer ISA based on the followings: i) "Semantically Selected Centers (SSC)" are proposed in order to define the "6D pose" at the level of categories. ii) 3D skeleton structures, which we derive as shape-invariant features, are used to represent the parts extracted from the instances of given categories, and privileged one-class learning is employed based on these parts. iii) Graph matching is performed during training in such a way that the adaptation/generalization capability of the proposed architecture is improved across unseen instances. Experiments validate the promising performance of the proposed architecture on both synthetic and real datasets.



### Subitizing with Variational Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/1808.00257v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.00257v1)
- **Published**: 2018-08-01 10:45:52+00:00
- **Updated**: 2018-08-01 10:45:52+00:00
- **Authors**: Rijnder Wever, Tom F. H. Runia
- **Comment**: None
- **Journal**: European Conference on Computer Vision 2018 - Workshop on
  Brain-Driven Computer Vision
- **Summary**: Numerosity, the number of objects in a set, is a basic property of a given visual scene. Many animals develop the perceptual ability to subitize: the near-instantaneous identification of the numerosity in small sets of visual items. In computer vision, it has been shown that numerosity emerges as a statistical property in neural networks during unsupervised learning from simple synthetic images. In this work, we focus on more complex natural images using unsupervised hierarchical neural networks. Specifically, we show that variational autoencoders are able to spontaneously perform subitizing after training without supervision on a large amount images from the Salient Object Subitizing dataset. While our method is unable to outperform supervised convolutional networks for subitizing, we observe that the networks learn to encode numerosity as basic visual property. Moreover, we find that the learned representations are likely invariant to object area; an observation in alignment with studies on biological neural networks in cognitive neuroscience.



### Saliency for Fine-grained Object Recognition in Domains with Scarce Training Data
- **Arxiv ID**: http://arxiv.org/abs/1808.00262v3
- **DOI**: 10.1016/j.patcog.2019.05.002
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.00262v3)
- **Published**: 2018-08-01 10:55:14+00:00
- **Updated**: 2019-05-04 16:56:10+00:00
- **Authors**: Carola Figueroa Flores, Abel Gonzalez-García, Joost van de Weijer, Bogdan Raducanu
- **Comment**: Published in Pattern Recognition journal
- **Journal**: Pattern Recognition, 2019
- **Summary**: This paper investigates the role of saliency to improve the classification accuracy of a Convolutional Neural Network (CNN) for the case when scarce training data is available. Our approach consists in adding a saliency branch to an existing CNN architecture which is used to modulate the standard bottom-up visual features from the original image input, acting as an attentional mechanism that guides the feature extraction process. The main aim of the proposed approach is to enable the effective training of a fine-grained recognition model with limited training samples and to improve the performance on the task, thereby alleviating the need to annotate large dataset. % The vast majority of saliency methods are evaluated on their ability to generate saliency maps, and not on their functionality in a complete vision pipeline. Our proposed pipeline allows to evaluate saliency methods for the high-level task of object recognition. We perform extensive experiments on various fine-grained datasets (Flowers, Birds, Cars, and Dogs) under different conditions and show that saliency can considerably improve the network's performance, especially for the case of scarce training data. Furthermore, our experiments show that saliency methods that obtain improved saliency maps (as measured by traditional saliency benchmarks) also translate to saliency methods that yield improved performance gains when applied in an object recognition pipeline.



### Interpretable Visual Question Answering by Visual Grounding from Attention Supervision Mining
- **Arxiv ID**: http://arxiv.org/abs/1808.00265v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1808.00265v1)
- **Published**: 2018-08-01 11:06:08+00:00
- **Updated**: 2018-08-01 11:06:08+00:00
- **Authors**: Yundong Zhang, Juan Carlos Niebles, Alvaro Soto
- **Comment**: 8 pages, 4 figures
- **Journal**: None
- **Summary**: A key aspect of VQA models that are interpretable is their ability to ground their answers to relevant regions in the image. Current approaches with this capability rely on supervised learning and human annotated groundings to train attention mechanisms inside the VQA architecture. Unfortunately, obtaining human annotations specific for visual grounding is difficult and expensive. In this work, we demonstrate that we can effectively train a VQA architecture with grounding supervision that can be automatically obtained from available region descriptions and object annotations. We also show that our model trained with this mined supervision generates visual groundings that achieve a higher correlation with respect to manually-annotated groundings, meanwhile achieving state-of-the-art VQA accuracy.



### Recurrent neural networks for aortic image sequence segmentation with sparse annotations
- **Arxiv ID**: http://arxiv.org/abs/1808.00273v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.00273v1)
- **Published**: 2018-08-01 11:20:54+00:00
- **Updated**: 2018-08-01 11:20:54+00:00
- **Authors**: Wenjia Bai, Hideaki Suzuki, Chen Qin, Giacomo Tarroni, Ozan Oktay, Paul M. Matthews, Daniel Rueckert
- **Comment**: Accepted for publication by MICCAI 2018
- **Journal**: None
- **Summary**: Segmentation of image sequences is an important task in medical image analysis, which enables clinicians to assess the anatomy and function of moving organs. However, direct application of a segmentation algorithm to each time frame of a sequence may ignore the temporal continuity inherent in the sequence. In this work, we propose an image sequence segmentation algorithm by combining a fully convolutional network with a recurrent neural network, which incorporates both spatial and temporal information into the segmentation task. A key challenge in training this network is that the available manual annotations are temporally sparse, which forbids end-to-end training. We address this challenge by performing non-rigid label propagation on the annotations and introducing an exponentially weighted loss function for training. Experiments on aortic MR image sequences demonstrate that the proposed method significantly improves both accuracy and temporal smoothness of segmentation, compared to a baseline method that utilises spatial information only. It achieves an average Dice metric of 0.960 for the ascending aorta and 0.953 for the descending aorta.



### Bi-Real Net: Enhancing the Performance of 1-bit CNNs With Improved Representational Capability and Advanced Training Algorithm
- **Arxiv ID**: http://arxiv.org/abs/1808.00278v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.00278v5)
- **Published**: 2018-08-01 11:40:59+00:00
- **Updated**: 2018-09-29 03:34:59+00:00
- **Authors**: Zechun Liu, Baoyuan Wu, Wenhan Luo, Xin Yang, Wei Liu, Kwang-Ting Cheng
- **Comment**: Accepted to European Conference on Computer Vision (ECCV) 2018. Code
  is available on: https://github.com/liuzechun/Bi-Real-net
- **Journal**: None
- **Summary**: In this work, we study the 1-bit convolutional neural networks (CNNs), of which both the weights and activations are binary. While being efficient, the classification accuracy of the current 1-bit CNNs is much worse compared to their counterpart real-valued CNN models on the large-scale dataset, like ImageNet. To minimize the performance gap between the 1-bit and real-valued CNN models, we propose a novel model, dubbed Bi-Real net, which connects the real activations (after the 1-bit convolution and/or BatchNorm layer, before the sign function) to activations of the consecutive block, through an identity shortcut. Consequently, compared to the standard 1-bit CNN, the representational capability of the Bi-Real net is significantly enhanced and the additional cost on computation is negligible. Moreover, we develop a specific training algorithm including three technical novelties for 1- bit CNNs. Firstly, we derive a tight approximation to the derivative of the non-differentiable sign function with respect to activation. Secondly, we propose a magnitude-aware gradient with respect to the weight for updating the weight parameters. Thirdly, we pre-train the real-valued CNN model with a clip function, rather than the ReLU function, to better initialize the Bi-Real net. Experiments on ImageNet show that the Bi-Real net with the proposed training algorithm achieves 56.4% and 62.2% top-1 accuracy with 18 layers and 34 layers, respectively. Compared to the state-of-the-arts (e.g., XNOR Net), Bi-Real net achieves up to 10% higher top-1 accuracy with more memory saving and lower computational cost. Keywords: binary neural network, 1-bit CNNs, 1-layer-per-block



### Energy-based Tuning of Convolutional Neural Networks on Multi-GPUs
- **Arxiv ID**: http://arxiv.org/abs/1808.00286v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.00286v1)
- **Published**: 2018-08-01 12:08:02+00:00
- **Updated**: 2018-08-01 12:08:02+00:00
- **Authors**: Francisco M. Castro, Nicolás Guil, Manuel J. Marín-Jiménez, Jesús Pérez-Serrano, Manuel Ujaldón
- **Comment**: To appear in Concurrency and Computation: Practice and Experience
- **Journal**: None
- **Summary**: Deep Learning (DL) applications are gaining momentum in the realm of Artificial Intelligence, particularly after GPUs have demonstrated remarkable skills for accelerating their challenging computational requirements. Within this context, Convolutional Neural Network (CNN) models constitute a representative example of success on a wide set of complex applications, particularly on datasets where the target can be represented through a hierarchy of local features of increasing semantic complexity. In most of the real scenarios, the roadmap to improve results relies on CNN settings involving brute force computation, and researchers have lately proven Nvidia GPUs to be one of the best hardware counterparts for acceleration. Our work complements those findings with an energy study on critical parameters for the deployment of CNNs on flagship image and video applications: object recognition and people identification by gait, respectively. We evaluate energy consumption on four different networks based on the two most popular ones (ResNet/AlexNet): ResNet (167 layers), a 2D CNN (15 layers), a CaffeNet (25 layers) and a ResNetIm (94 layers) using batch sizes of 64, 128 and 256, and then correlate those with speed-up and accuracy to determine optimal settings. Experimental results on a multi-GPU server endowed with twin Maxwell and twin Pascal Titan X GPUs demonstrate that energy correlates with performance and that Pascal may have up to 40% gains versus Maxwell. Larger batch sizes extend performance gains and energy savings, but we have to keep an eye on accuracy, which sometimes shows a preference for small batches. We expect this work to provide a preliminary guidance for a wide set of CNN and DL applications in modern HPC times, where the GFLOPS/w ratio constitutes the primary goal.



### Attention-based Pyramid Aggregation Network for Visual Place Recognition
- **Arxiv ID**: http://arxiv.org/abs/1808.00288v1
- **DOI**: 10.1145/3240508.3240525
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.00288v1)
- **Published**: 2018-08-01 12:10:40+00:00
- **Updated**: 2018-08-01 12:10:40+00:00
- **Authors**: Yingying Zhu, Jiong Wang, Lingxi Xie, Liang Zheng
- **Comment**: Accepted to ACM Multimedia 2018
- **Journal**: None
- **Summary**: Visual place recognition is challenging in the urban environment and is usually viewed as a large scale image retrieval task. The intrinsic challenges in place recognition exist that the confusing objects such as cars and trees frequently occur in the complex urban scene, and buildings with repetitive structures may cause over-counting and the burstiness problem degrading the image representations. To address these problems, we present an Attention-based Pyramid Aggregation Network (APANet), which is trained in an end-to-end manner for place recognition. One main component of APANet, the spatial pyramid pooling, can effectively encode the multi-size buildings containing geo-information. The other one, the attention block, is adopted as a region evaluator for suppressing the confusing regional features while highlighting the discriminative ones. When testing, we further propose a simple yet effective PCA power whitening strategy, which significantly improves the widely used PCA whitening by reasonably limiting the impact of over-counting. Experimental evaluations demonstrate that the proposed APANet outperforms the state-of-the-art methods on two place recognition benchmarks, and generalizes well on standard image retrieval datasets.



### TraMNet - Transition Matrix Network for Efficient Action Tube Proposals
- **Arxiv ID**: http://arxiv.org/abs/1808.00297v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1808.00297v1)
- **Published**: 2018-08-01 12:33:57+00:00
- **Updated**: 2018-08-01 12:33:57+00:00
- **Authors**: Gurkirt Singh, Suman Saha, Fabio Cuzzolin
- **Comment**: 15 pages
- **Journal**: None
- **Summary**: Current state-of-the-art methods solve spatiotemporal action localisation by extending 2D anchors to 3D-cuboid proposals on stacks of frames, to generate sets of temporally connected bounding boxes called \textit{action micro-tubes}. However, they fail to consider that the underlying anchor proposal hypotheses should also move (transition) from frame to frame, as the actor or the camera does. Assuming we evaluate $n$ 2D anchors in each frame, then the number of possible transitions from each 2D anchor to the next, for a sequence of $f$ consecutive frames, is in the order of $O(n^f)$, expensive even for small values of $f$. To avoid this problem, we introduce a Transition-Matrix-based Network (TraMNet) which relies on computing transition probabilities between anchor proposals while maximising their overlap with ground truth bounding boxes across frames, and enforcing sparsity via a transition threshold. As the resulting transition matrix is sparse and stochastic, this reduces the proposal hypothesis search space from $O(n^f)$ to the cardinality of the thresholded matrix. At training time, transitions are specific to cell locations of the feature maps, so that a sparse (efficient) transition matrix is used to train the network. At test time, a denser transition matrix can be obtained either by decreasing the threshold or by adding to it all the relative transitions originating from any cell location, allowing the network to handle transitions in the test data that might not have been present in the training data, and making detection translation-invariant. Finally, we show that our network can handle sparse annotations such as those available in the DALY dataset. We report extensive experiments on the DALY, UCF101-24 and Transformed-UCF101-24 datasets to support our claims.



### Learning Visual Question Answering by Bootstrapping Hard Attention
- **Arxiv ID**: http://arxiv.org/abs/1808.00300v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1808.00300v1)
- **Published**: 2018-08-01 12:39:43+00:00
- **Updated**: 2018-08-01 12:39:43+00:00
- **Authors**: Mateusz Malinowski, Carl Doersch, Adam Santoro, Peter Battaglia
- **Comment**: ECCV 2018
- **Journal**: None
- **Summary**: Attention mechanisms in biological perception are thought to select subsets of perceptual information for more sophisticated processing which would be prohibitive to perform on all sensory inputs. In computer vision, however, there has been relatively little exploration of hard attention, where some information is selectively ignored, in spite of the success of soft attention, where information is re-weighted and aggregated, but never filtered out. Here, we introduce a new approach for hard attention and find it achieves very competitive performance on a recently-released visual question answering datasets, equalling and in some cases surpassing similar soft attention architectures while entirely ignoring some features. Even though the hard attention mechanism is thought to be non-differentiable, we found that the feature magnitudes correlate with semantic relevance, and provide a useful signal for our mechanism's attentional selection criterion. Because hard attention selects important features of the input information, it can also be more efficient than analogous soft attention mechanisms. This is especially important for recent approaches that use non-local pairwise operations, whereby computational and memory costs are quadratic in the size of the set of features.



### A Network Structure to Explicitly Reduce Confusion Errors in Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1808.00313v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.00313v1)
- **Published**: 2018-08-01 13:37:59+00:00
- **Updated**: 2018-08-01 13:37:59+00:00
- **Authors**: Qichuan Geng, Xinyu Huang, Zhong Zhou, Ruigang Yang
- **Comment**: 18 pages, 9 figures
- **Journal**: None
- **Summary**: Confusing classes that are ubiquitous in real world often degrade performance for many vision related applications like object detection, classification, and segmentation. The confusion errors are not only caused by similar visual patterns but also amplified by various factors during the training of our designed models, such as reduced feature resolution in the encoding process or imbalanced data distributions. A large amount of deep learning based network structures has been proposed in recent years to deal with these individual factors and improve network performance. However, to our knowledge, no existing work in semantic image segmentation is designed to tackle confusion errors explicitly. In this paper, we present a novel and general network structure that reduces confusion errors in more direct manner and apply the network for semantic segmentation. There are two major contributions in our network structure: 1) We ensemble subnets with heterogeneous output spaces based on the discriminative confusing groups. The training for each subnet can distinguish confusing classes within the group without affecting unrelated classes outside the group. 2) We propose an improved cross-entropy loss function that maximizes the probability assigned to the correct class and penalizes the probabilities assigned to the confusing classes at the same time. Our network structure is a general structure and can be easily adapted to any other networks to further reduce confusion errors. Without any changes in the feature encoder and post-processing steps, our experiments demonstrate consistent and significant improvements on different baseline models on Cityscapes and PASCAL VOC datasets (e.g., 3.05% over ResNet-101 and 1.30% over ResNet-38).



### Generative Adversarial Frontal View to Bird View Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1808.00327v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.00327v3)
- **Published**: 2018-08-01 14:09:02+00:00
- **Updated**: 2019-04-02 03:21:20+00:00
- **Authors**: Xinge Zhu, Zhichao Yin, Jianping Shi, Hongsheng Li, Dahua Lin
- **Comment**: Accepted to 3DV 2018; Codes are available at
  https://github.com/WERush/BridgeGAN
- **Journal**: None
- **Summary**: Environment perception is an important task with great practical value and bird view is an essential part for creating panoramas of surrounding environment. Due to the large gap and severe deformation between the frontal view and bird view, generating a bird view image from a single frontal view is challenging. To tackle this problem, we propose the BridgeGAN, i.e., a novel generative model for bird view synthesis. First, an intermediate view, i.e., homography view, is introduced to bridge the large gap. Next, conditioned on the three views (frontal view, homography view and bird view) in our task, a multi-GAN based model is proposed to learn the challenging cross-view translation. Extensive experiments conducted on a synthetic dataset have demonstrated that the images generated by our model are much better than those generated by existing methods, with more consistent global appearance and sharper details. Ablation studies and discussions show its reliability and robustness in some challenging cases.



### Structured Differential Learning for Automatic Threshold Setting
- **Arxiv ID**: http://arxiv.org/abs/1808.00361v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1808.00361v1)
- **Published**: 2018-08-01 15:13:28+00:00
- **Updated**: 2018-08-01 15:13:28+00:00
- **Authors**: Jonathan Connell, Benjamin Herta
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a technique that can automatically tune the parameters of a rule-based computer vision system comprised of thresholds, combinational logic, and time constants. This lets us retain the flexibility and perspicacity of a conventionally structured system while allowing us to perform approximate gradient descent using labeled data. While this is only a heuristic procedure, as far as we are aware there is no other efficient technique for tuning such systems. We describe the components of the system and the associated supervised learning mechanism. We also demonstrate the utility of the algorithm by comparing its performance versus hand tuning for an automotive headlight controller. Despite having over 100 parameters, the method is able to profitably adjust the system values given just the desired output for a number of videos.



### Deep Appearance Models for Face Rendering
- **Arxiv ID**: http://arxiv.org/abs/1808.00362v1
- **DOI**: 10.1145/3197517.3201401
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1808.00362v1)
- **Published**: 2018-08-01 15:13:48+00:00
- **Updated**: 2018-08-01 15:13:48+00:00
- **Authors**: Stephen Lombardi, Jason Saragih, Tomas Simon, Yaser Sheikh
- **Comment**: Accepted to SIGGRAPH 2018
- **Journal**: ACM Transactions on Graphics (SIGGRAPH 2018) 37, 4, Article 68
- **Summary**: We introduce a deep appearance model for rendering the human face. Inspired by Active Appearance Models, we develop a data-driven rendering pipeline that learns a joint representation of facial geometry and appearance from a multiview capture setup. Vertex positions and view-specific textures are modeled using a deep variational autoencoder that captures complex nonlinear effects while producing a smooth and compact latent representation. View-specific texture enables the modeling of view-dependent effects such as specularity. In addition, it can also correct for imperfect geometry stemming from biased or low resolution estimates. This is a significant departure from the traditional graphics pipeline, which requires highly accurate geometry as well as all elements of the shading model to achieve realism through physically-inspired light transport. Acquiring such a high level of accuracy is difficult in practice, especially for complex and intricate parts of the face, such as eyelashes and the oral cavity. These are handled naturally by our approach, which does not rely on precise estimates of geometry. Instead, the shading model accommodates deficiencies in geometry though the flexibility afforded by the neural network employed. At inference time, we condition the decoding network on the viewpoint of the camera in order to generate the appropriate texture for rendering. The resulting system can be implemented simply using existing rendering engines through dynamic textures with flat lighting. This representation, together with a novel unsupervised technique for mapping images to facial states, results in a system that is naturally suited to real-time interactive settings such as Virtual Reality (VR).



### Efficient Progressive Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/1808.00391v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.00391v1)
- **Published**: 2018-08-01 15:56:08+00:00
- **Updated**: 2018-08-01 15:56:08+00:00
- **Authors**: Juan-Manuel Perez-Rua, Moez Baccouche, Stephane Pateux
- **Comment**: Accepted for publication by the BMVA (BMVC 2018)
- **Journal**: None
- **Summary**: This paper addresses the difficult problem of finding an optimal neural architecture design for a given image classification task. We propose a method that aggregates two main results of the previous state-of-the-art in neural architecture search. These are, appealing to the strong sampling efficiency of a search scheme based on sequential model-based optimization (SMBO), and increasing training efficiency by sharing weights among sampled architectures. Sequential search has previously demonstrated its capabilities to find state-of-the-art neural architectures for image classification. However, its computational cost remains high, even unreachable under modest computational settings. Affording SMBO with weight-sharing alleviates this problem. On the other hand, progressive search with SMBO is inherently greedy, as it leverages a learned surrogate function to predict the validation error of neural architectures. This prediction is directly used to rank the sampled neural architectures. We propose to attenuate the greediness of the original SMBO method by relaxing the role of the surrogate function so it predicts architecture sampling probability instead. We demonstrate with experiments on the CIFAR-10 dataset that our method, denominated Efficient progressive neural architecture search (EPNAS), leads to increased search efficiency, while retaining competitiveness of found architectures.



### A Multi-channel Network with Image Retrieval for Accurate Brain Tissue Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1808.00457v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.00457v2)
- **Published**: 2018-08-01 16:27:00+00:00
- **Updated**: 2018-08-04 13:26:01+00:00
- **Authors**: Yao Sun, Yang Deng, Yue Xu, Shuo Zhang, Mingwang Zhu, Kehong Yuan
- **Comment**: 8 pages, 5 figures
- **Journal**: None
- **Summary**: Magnetic Resonance Imaging (MRI) is widely used in the pathological and functional studies of the brain, such as epilepsy, tumor diagnosis, etc. Automated accurate brain tissue segmentation like cerebro-spinal fluid (CSF), gray matter (GM), white matter (WM) is the basis of these studies and many researchers are seeking it to the best. Based on the truth that multi-channel segmentation network with its own ground truth achieves up to average dice ratio 0.98, we propose a novel method that we add a fourth channel with the ground truth of the most similar image's obtained by CBIR from the database. The results show that the method improves the segmentation performance, as measured by average dice ratio, by approximately 0.01 in the MRBrainS18 database. In addition, our method is concise and robust, which can be used to any network architecture that needs not be modified a lot.



### Global Norm-Aware Pooling for Pose-Robust Face Recognition at Low False Positive Rate
- **Arxiv ID**: http://arxiv.org/abs/1808.00435v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.00435v1)
- **Published**: 2018-08-01 17:32:31+00:00
- **Updated**: 2018-08-01 17:32:31+00:00
- **Authors**: Sheng Chen, Jia Guo, Yang Liu, Xiang Gao, Zhen Han
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel Global Norm-Aware Pooling (GNAP) block, which reweights local features in a convolutional neural network (CNN) adaptively according to their L2 norms and outputs a global feature vector with a global average pooling layer. Our GNAP block is designed to give dynamic weights to local features in different spatial positions without losing spatial symmetry. We use a GNAP block in a face feature embedding CNN to produce discriminative face feature vectors for pose-robust face recognition. The GNAP block is of very cheap computational cost, but it is very powerful for frontal-profile face recognition. Under the CFP frontal-profile protocol, the GNAP block can not only reduce EER dramatically but also boost TPR@FPR=0.1% (TPR i.e. True Positive Rate, FPR i.e. False Positive Rate) substantially. Our experiments show that the GNAP block greatly promotes pose-robust face recognition over the base model especially at low false positive rate.



### Towards a Semantic Perceptual Image Metric
- **Arxiv ID**: http://arxiv.org/abs/1808.00447v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.00447v1)
- **Published**: 2018-08-01 17:58:23+00:00
- **Updated**: 2018-08-01 17:58:23+00:00
- **Authors**: Troy Chinen, Johannes Ballé, Chunhui Gu, Sung Jin Hwang, Sergey Ioffe, Nick Johnston, Thomas Leung, David Minnen, Sean O'Malley, Charles Rosenberg, George Toderici
- **Comment**: None
- **Journal**: None
- **Summary**: We present a full reference, perceptual image metric based on VGG-16, an artificial neural network trained on object classification. We fit the metric to a new database based on 140k unique images annotated with ground truth by human raters who received minimal instruction. The resulting metric shows competitive performance on TID 2013, a database widely used to assess image quality assessments methods. More interestingly, it shows strong responses to objects potentially carrying semantic relevance such as faces and text, which we demonstrate using a visualization technique and ablation experiments. In effect, the metric appears to model a higher influence of semantic context on judgments, which we observe particularly in untrained raters. As the vast majority of users of image processing systems are unfamiliar with Image Quality Assessment (IQA) tasks, these findings may have significant impact on real-world applications of perceptual metrics.



### Learning Blind Video Temporal Consistency
- **Arxiv ID**: http://arxiv.org/abs/1808.00449v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.00449v1)
- **Published**: 2018-08-01 17:59:15+00:00
- **Updated**: 2018-08-01 17:59:15+00:00
- **Authors**: Wei-Sheng Lai, Jia-Bin Huang, Oliver Wang, Eli Shechtman, Ersin Yumer, Ming-Hsuan Yang
- **Comment**: This work is accepted in ECCV 2018. Project website:
  http://vllab.ucmerced.edu/wlai24/video_consistency/
- **Journal**: None
- **Summary**: Applying image processing algorithms independently to each frame of a video often leads to undesired inconsistent results over time. Developing temporally consistent video-based extensions, however, requires domain knowledge for individual tasks and is unable to generalize to other applications. In this paper, we present an efficient end-to-end approach based on deep recurrent network for enforcing temporal consistency in a video. Our method takes the original unprocessed and per-frame processed videos as inputs to produce a temporally consistent video. Consequently, our approach is agnostic to specific image processing algorithms applied on the original video. We train the proposed network by minimizing both short-term and long-term temporal losses as well as the perceptual loss to strike a balance between temporal stability and perceptual similarity with the processed frames. At test time, our model does not require computing optical flow and thus achieves real-time speed even for high-resolution videos. We show that our single model can handle multiple and unseen tasks, including but not limited to artistic style transfer, enhancement, colorization, image-to-image translation and intrinsic image decomposition. Extensive objective evaluation and subject study demonstrate that the proposed approach performs favorably against the state-of-the-art methods on various types of videos.



### Semantic Classification of 3D Point Clouds with Multiscale Spherical Neighborhoods
- **Arxiv ID**: http://arxiv.org/abs/1808.00495v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.00495v1)
- **Published**: 2018-08-01 18:24:02+00:00
- **Updated**: 2018-08-01 18:24:02+00:00
- **Authors**: Hugues Thomas, Jean-Emmanuel Deschaud, Beatriz Marcotegui, François Goulette, Yann Le Gall
- **Comment**: 3DV2018
- **Journal**: None
- **Summary**: This paper introduces a new definition of multiscale neighborhoods in 3D point clouds. This definition, based on spherical neighborhoods and proportional subsampling, allows the computation of features with a consistent geometrical meaning, which is not the case when using k-nearest neighbors. With an appropriate learning strategy, the proposed features can be used in a random forest to classify 3D points. In this semantic classification task, we show that our multiscale features outperform state-of-the-art features using the same experimental conditions. Furthermore, their classification power competes with more elaborate classification approaches including Deep Learning methods.



### Wavelet Sparse Regularization for Manifold-Valued Data
- **Arxiv ID**: http://arxiv.org/abs/1808.00505v1
- **DOI**: None
- **Categories**: **math.NA**, cs.CV, cs.NA, math.DG, 94A08, 90C90, 65T60, 53B99, 53C35, 65K10
- **Links**: [PDF](http://arxiv.org/pdf/1808.00505v1)
- **Published**: 2018-08-01 18:47:36+00:00
- **Updated**: 2018-08-01 18:47:36+00:00
- **Authors**: Martin Storath, Andreas Weinmann
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we consider the sparse regularization of manifold-valued data with respect to an interpolatory wavelet/multiscale transform. We propose and study variational models for this task and provide results on their well-posedness. We present algorithms for a numerical realization of these models in the manifold setup. Further, we provide experimental results to show the potential of the proposed schemes for applications.



### Direct Sparse Odometry with Rolling Shutter
- **Arxiv ID**: http://arxiv.org/abs/1808.00558v1
- **DOI**: 10.1007/978-3-030-01237-3_42
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.00558v1)
- **Published**: 2018-08-01 20:48:02+00:00
- **Updated**: 2018-08-01 20:48:02+00:00
- **Authors**: David Schubert, Nikolaus Demmel, Vladyslav Usenko, Jörg Stückler, Daniel Cremers
- **Comment**: None
- **Journal**: None
- **Summary**: Neglecting the effects of rolling-shutter cameras for visual odometry (VO) severely degrades accuracy and robustness. In this paper, we propose a novel direct monocular VO method that incorporates a rolling-shutter model. Our approach extends direct sparse odometry which performs direct bundle adjustment of a set of recent keyframe poses and the depths of a sparse set of image points. We estimate the velocity at each keyframe and impose a constant-velocity prior for the optimization. In this way, we obtain a near real-time, accurate direct VO method. Our approach achieves improved results on challenging rolling-shutter sequences over state-of-the-art global-shutter VO.



### Weather Classification: A new multi-class dataset, data augmentation approach and comprehensive evaluations of Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1808.00588v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.00588v1)
- **Published**: 2018-08-01 22:33:23+00:00
- **Updated**: 2018-08-01 22:33:23+00:00
- **Authors**: Jose Carlos Villarreal Guerra, Zeba Khanam, Shoaib Ehsan, Rustam Stolkin, Klaus McDonald-Maier
- **Comment**: None
- **Journal**: None
- **Summary**: Weather conditions often disrupt the proper functioning of transportation systems. Present systems either deploy an array of sensors or use an in-vehicle camera to predict weather conditions. These solutions have resulted in incremental cost and limited scope. To ensure smooth operation of all transportation services in all-weather conditions, a reliable detection system is necessary to classify weather in wild. The challenges involved in solving this problem is that weather conditions are diverse in nature and there is an absence of discriminate features among various weather conditions. The existing works to solve this problem have been scene specific and have targeted classification of two categories of weather. In this paper, we have created a new open source dataset consisting of images depicting three classes of weather i.e rain, snow and fog called RFS Dataset. A novel algorithm has also been proposed which has used super pixel delimiting masks as a form of data augmentation, leading to reasonable results with respect to ten Convolutional Neural Network architectures.



### Classification of Building Information Model (BIM) Structures with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1808.00601v1
- **DOI**: 10.1109/EUVIP.2018.8611701
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1808.00601v1)
- **Published**: 2018-08-01 23:56:28+00:00
- **Updated**: 2018-08-01 23:56:28+00:00
- **Authors**: Francesco Lomio, Ricardo Farinha, Mauri Laasonen, Heikki Huttunen
- **Comment**: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
- **Journal**: None
- **Summary**: In this work we study an application of machine learning to the construction industry and we use classical and modern machine learning methods to categorize images of building designs into three classes: Apartment building, Industrial building or Other. No real images are used, but only images extracted from Building Information Model (BIM) software, as these are used by the construction industry to store building designs. For this task, we compared four different methods: the first is based on classical machine learning, where Histogram of Oriented Gradients (HOG) was used for feature extraction and a Support Vector Machine (SVM) for classification; the other three methods are based on deep learning, covering common pre-trained networks as well as ones designed from scratch. To validate the accuracy of the models, a database of 240 images was used. The accuracy achieved is 57% for the HOG + SVM model, and above 89% for the neural networks.



