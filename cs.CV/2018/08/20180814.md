# Arxiv Papers in cs.CV on 2018-08-14
### Generative Invertible Networks (GIN): Pathophysiology-Interpretable Feature Mapping and Virtual Patient Generation
- **Arxiv ID**: http://arxiv.org/abs/1808.04495v1
- **DOI**: 10.1007/978-3-030-00928-1_61
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.04495v1)
- **Published**: 2018-08-14 00:18:33+00:00
- **Updated**: 2018-08-14 00:18:33+00:00
- **Authors**: Jialei Chen, Yujia Xie, Kan Wang, Zih Huei Wang, Geet Lahoti, Chuck Zhang, Mani A Vannan, Ben Wang, Zhen Qian
- **Comment**: None
- **Journal**: None
- **Summary**: Machine learning methods play increasingly important roles in pre-procedural planning for complex surgeries and interventions. Very often, however, researchers find the historical records of emerging surgical techniques, such as the transcatheter aortic valve replacement (TAVR), are highly scarce in quantity. In this paper, we address this challenge by proposing novel generative invertible networks (GIN) to select features and generate high-quality virtual patients that may potentially serve as an additional data source for machine learning. Combining a convolutional neural network (CNN) and generative adversarial networks (GAN), GIN discovers the pathophysiologic meaning of the feature space. Moreover, a test of predicting the surgical outcome directly using the selected features results in a high accuracy of 81.55%, which suggests little pathophysiologic information has been lost while conducting the feature selection. This demonstrates GIN can generate virtual patients not only visually authentic but also pathophysiologically interpretable.



### ScarGAN: Chained Generative Adversarial Networks to Simulate Pathological Tissue on Cardiovascular MR Scans
- **Arxiv ID**: http://arxiv.org/abs/1808.04500v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.04500v1)
- **Published**: 2018-08-14 01:10:00+00:00
- **Updated**: 2018-08-14 01:10:00+00:00
- **Authors**: Felix Lau, Tom Hendriks, Jesse Lieman-Sifry, Berk Norman, Sean Sall, Daniel Golden
- **Comment**: 12 pages, 5 figures. To appear in MICCAI DLMIA 2018
- **Journal**: None
- **Summary**: Medical images with specific pathologies are scarce, but a large amount of data is usually required for a deep convolutional neural network (DCNN) to achieve good accuracy. We consider the problem of segmenting the left ventricular (LV) myocardium on late gadolinium enhancement (LGE) cardiovascular magnetic resonance (CMR) scans of which only some of the scans have scar tissue. We propose ScarGAN to simulate scar tissue on healthy myocardium using chained generative adversarial networks (GAN). Our novel approach factorizes the simulation process into 3 steps: 1) a mask generator to simulate the shape of the scar tissue; 2) a domain-specific heuristic to produce the initial simulated scar tissue from the simulated shape; 3) a refining generator to add details to the simulated scar tissue. Unlike other approaches that generate samples from scratch, we simulate scar tissue on normal scans resulting in highly realistic samples. We show that experienced radiologists are unable to distinguish between real and simulated scar tissue. Training a U-Net with additional scans with scar tissue simulated by ScarGAN increases the percentage of scar pixels correctly included in LV myocardium prediction from 75.9% to 80.5%.



### Shared Multi-Task Imitation Learning for Indoor Self-Navigation
- **Arxiv ID**: http://arxiv.org/abs/1808.04503v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1808.04503v1)
- **Published**: 2018-08-14 01:38:47+00:00
- **Updated**: 2018-08-14 01:38:47+00:00
- **Authors**: Junhong Xu, Qiwei Liu, Hanqing Guo, Aaron Kageza, Saeed AlQarni, Shaoen Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Deep imitation learning enables robots to learn from expert demonstrations to perform tasks such as lane following or obstacle avoidance. However, in the traditional imitation learning framework, one model only learns one task, and thus it lacks of the capability to support a robot to perform various different navigation tasks with one model in indoor environments. This paper proposes a new framework, Shared Multi-headed Imitation Learning(SMIL), that allows a robot to perform multiple tasks with one model without switching among different models. We model each task as a sub-policy and design a multi-headed policy to learn the shared information among related tasks by summing up activations from all sub-policies. Compared to single or non-shared multi-headed policies, this framework is able to leverage correlated information among tasks to increase performance.We have implemented this framework using a robot based on NVIDIA TX2 and performed extensive experiments in indoor environments with different baseline solutions. The results demonstrate that SMIL has doubled the performance over nonshared multi-headed policy.



### Fine-Grained Representation Learning and Recognition by Exploiting Hierarchical Semantic Embedding
- **Arxiv ID**: http://arxiv.org/abs/1808.04505v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.04505v1)
- **Published**: 2018-08-14 02:09:34+00:00
- **Updated**: 2018-08-14 02:09:34+00:00
- **Authors**: Tianshui Chen, Wenxi Wu, Yuefang Gao, Le Dong, Xiaonan Luo, Liang Lin
- **Comment**: Accepted at ACM MM 2018 as oral presentation
- **Journal**: None
- **Summary**: Object categories inherently form a hierarchy with different levels of concept abstraction, especially for fine-grained categories. For example, birds (Aves) can be categorized according to a four-level hierarchy of order, family, genus, and species. This hierarchy encodes rich correlations among various categories across different levels, which can effectively regularize the semantic space and thus make prediction less ambiguous. However, previous studies of fine-grained image recognition primarily focus on categories of one certain level and usually overlook this correlation information. In this work, we investigate simultaneously predicting categories of different levels in the hierarchy and integrating this structured correlation information into the deep neural network by developing a novel Hierarchical Semantic Embedding (HSE) framework. Specifically, the HSE framework sequentially predicts the category score vector of each level in the hierarchy, from highest to lowest. At each level, it incorporates the predicted score vector of the higher level as prior knowledge to learn finer-grained feature representation. During training, the predicted score vector of the higher level is also employed to regularize label prediction by using it as soft targets of corresponding sub-categories. To evaluate the proposed framework, we organize the 200 bird species of the Caltech-UCSD birds dataset with the four-level category hierarchy and construct a large-scale butterfly dataset that also covers four level categories. Extensive experiments on these two and the newly-released VegFru datasets demonstrate the superiority of our HSE framework over the baseline methods and existing competitors.



### Low Rank Regularization: A Review
- **Arxiv ID**: http://arxiv.org/abs/1808.04521v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1808.04521v3)
- **Published**: 2018-08-14 04:38:58+00:00
- **Updated**: 2020-12-10 03:05:30+00:00
- **Authors**: Zhanxuan Hu, Feiping Nie, Rong Wang, Xuelong Li
- **Comment**: 16 pages,4 figures,4 tables
- **Journal**: None
- **Summary**: Low rank regularization, in essence, involves introducing a low rank or approximately low rank assumption for matrix we aim to learn, which has achieved great success in many fields including machine learning, data mining and computer version. Over the last decade, much progress has been made in theories and practical applications. Nevertheless, the intersection between them is very slight. In order to construct a bridge between practical applications and theoretical research, in this paper we provide a comprehensive survey for low rank regularization. We first review several traditional machine learning models using low rank regularization, and then show their (or their variants) applications in solving practical issues, such as non-rigid structure from motion and image denoising. Subsequently, we summarize the regularizers and optimization methods that achieve great success in traditional machine learning tasks but are rarely seen in solving practical issues. Finally, we provide a discussion and comparison for some representative regularizers including convex and non-convex relaxations. Extensive experimental results demonstrate that non-convex regularizers can provide a large advantage over the nuclear norm, the regularizer widely used in solving practical issues.



### Learning Linear Transformations for Fast Arbitrary Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/1808.04537v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.04537v1)
- **Published**: 2018-08-14 05:45:20+00:00
- **Updated**: 2018-08-14 05:45:20+00:00
- **Authors**: Xueting Li, Sifei Liu, Jan Kautz, Ming-Hsuan Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Given a random pair of images, an arbitrary style transfer method extracts the feel from the reference image to synthesize an output based on the look of the other content image. Recent arbitrary style transfer methods transfer second order statistics from reference image onto content image via a multiplication between content image features and a transformation matrix, which is computed from features with a pre-determined algorithm. These algorithms either require computationally expensive operations, or fail to model the feature covariance and produce artifacts in synthesized images. Generalized from these methods, in this work, we derive the form of transformation matrix theoretically and present an arbitrary style transfer approach that learns the transformation matrix with a feed-forward network. Our algorithm is highly efficient yet allows a flexible combination of multi-level styles while preserving content affinity during style transfer process. We demonstrate the effectiveness of our approach on four tasks: artistic style transfer, video and photo-realistic style transfer as well as domain adaptation, including comparisons with the state-of-the-art methods.



### Text-to-Image-to-Text Translation using Cycle Consistent Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1808.04538v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1808.04538v1)
- **Published**: 2018-08-14 05:45:25+00:00
- **Updated**: 2018-08-14 05:45:25+00:00
- **Authors**: Satya Krishna Gorti, Jeremy Ma
- **Comment**: None
- **Journal**: None
- **Summary**: Text-to-Image translation has been an active area of research in the recent past. The ability for a network to learn the meaning of a sentence and generate an accurate image that depicts the sentence shows ability of the model to think more like humans. Popular methods on text to image translation make use of Generative Adversarial Networks (GANs) to generate high quality images based on text input, but the generated images don't always reflect the meaning of the sentence given to the model as input. We address this issue by using a captioning network to caption on generated images and exploit the distance between ground truth captions and generated captions to improve the network further. We show extensive comparisons between our method and existing methods.



### MT-VAE: Learning Motion Transformations to Generate Multimodal Human Dynamics
- **Arxiv ID**: http://arxiv.org/abs/1808.04545v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.GR, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1808.04545v1)
- **Published**: 2018-08-14 06:21:03+00:00
- **Updated**: 2018-08-14 06:21:03+00:00
- **Authors**: Xinchen Yan, Akash Rastogi, Ruben Villegas, Kalyan Sunkavalli, Eli Shechtman, Sunil Hadap, Ersin Yumer, Honglak Lee
- **Comment**: Published at ECCV 2018
- **Journal**: None
- **Summary**: Long-term human motion can be represented as a series of motion modes---motion sequences that capture short-term temporal dynamics---with transitions between them. We leverage this structure and present a novel Motion Transformation Variational Auto-Encoders (MT-VAE) for learning motion sequence generation. Our model jointly learns a feature embedding for motion modes (that the motion sequence can be reconstructed from) and a feature transformation that represents the transition of one motion mode to the next motion mode. Our model is able to generate multiple diverse and plausible motion sequences in the future from the same input. We apply our approach to both facial and full body motion, and demonstrate applications like analogy-based motion transfer and video synthesis.



### Binary Image Features Proposed to Empower Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/1808.08275v1
- **DOI**: 10.1002/ima.22418
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.08275v1)
- **Published**: 2018-08-14 06:39:58+00:00
- **Updated**: 2018-08-14 06:39:58+00:00
- **Authors**: Soumi Ray, Vinod Kumar
- **Comment**: None
- **Journal**: None
- **Summary**: This literature has proposed three fast and easy computable image features to improve computer vision by offering more human-like vision power. These features are not based on image pixels absolute or relative intensity; neither based on shape or colour. So, no complex pixel by pixel calculation is required. For human eyes, pixel by pixel calculation is like seeing an image with maximum zoom which is done only when a higher level of details is required. Normally, first we look at an image to get an overall idea about it to know whether it deserves further investigation or not. This capacity of getting an idea at a glance is analysed and three basic features are proposed to empower computer vision. Potential of proposed features is tested and established through different medical dataset. Achieved accuracy in classification demonstrates possibilities and potential of the use of the proposed features in image processing.



### Moving Object Segmentation in Jittery Videos by Stabilizing Trajectories Modeled in Kendall's Shape Space
- **Arxiv ID**: http://arxiv.org/abs/1808.04551v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.04551v1)
- **Published**: 2018-08-14 06:41:21+00:00
- **Updated**: 2018-08-14 06:41:21+00:00
- **Authors**: Geethu Miriam Jacob, Sukhendu Das
- **Comment**: 13 pages, 3 figures, Published in British Machine Vision Conference
  2017 (BMVC-2017)
- **Journal**: None
- **Summary**: Moving Object Segmentation is a challenging task for jittery/wobbly videos. For jittery videos, the non-smooth camera motion makes discrimination between foreground objects and background layers hard to solve. While most recent works for moving video object segmentation fail in this scenario, our method generates an accurate segmentation of a single moving object. The proposed method performs a sparse segmentation, where frame-wise labels are assigned only to trajectory coordinates, followed by the pixel-wise labeling of frames. The sparse segmentation involving stabilization and clustering of trajectories in a 3-stage iterative process. At the 1st stage, the trajectories are clustered using pairwise Procrustes distance as a cue for creating an affinity matrix. The 2nd stage performs a block-wise Procrustes analysis of the trajectories and estimates Frechet means (in Kendall's shape space) of the clusters. The Frechet means represent the average trajectories of the motion clusters. An optimization function has been formulated to stabilize the Frechet means, yielding stabilized trajectories at the 3rd stage. The accuracy of the motion clusters are iteratively refined, producing distinct groups of stabilized trajectories. Next, the labels obtained from the sparse segmentation are propagated for pixel-wise labeling of the frames, using a GraphCut based energy formulation. Use of Procrustes analysis and energy minimization in Kendall's shape space for moving object segmentation in jittery videos, is the novelty of this work. Second contribution comes from experiments performed on a dataset formed of 20 real-world natural jittery videos, with manually annotated ground truth. Experiments are done with controlled levels of artificial jitter on videos of SegTrack2 dataset. Qualitative and quantitative results indicate the superiority of the proposed method.



### Deep Retinex Decomposition for Low-Light Enhancement
- **Arxiv ID**: http://arxiv.org/abs/1808.04560v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.04560v1)
- **Published**: 2018-08-14 07:20:55+00:00
- **Updated**: 2018-08-14 07:20:55+00:00
- **Authors**: Chen Wei, Wenjing Wang, Wenhan Yang, Jiaying Liu
- **Comment**: BMVC 2018(Oral). Dataset and Project page:
  https://daooshee.github.io/BMVC2018website/
- **Journal**: None
- **Summary**: Retinex model is an effective tool for low-light image enhancement. It assumes that observed images can be decomposed into the reflectance and illumination. Most existing Retinex-based methods have carefully designed hand-crafted constraints and parameters for this highly ill-posed decomposition, which may be limited by model capacity when applied in various scenes. In this paper, we collect a LOw-Light dataset (LOL) containing low/normal-light image pairs and propose a deep Retinex-Net learned on this dataset, including a Decom-Net for decomposition and an Enhance-Net for illumination adjustment. In the training process for Decom-Net, there is no ground truth of decomposed reflectance and illumination. The network is learned with only key constraints including the consistent reflectance shared by paired low/normal-light images, and the smoothness of illumination. Based on the decomposition, subsequent lightness enhancement is conducted on illumination by an enhancement network called Enhance-Net, and for joint denoising there is a denoising operation on reflectance. The Retinex-Net is end-to-end trainable, so that the learned decomposition is by nature good for lightness adjustment. Extensive experiments demonstrate that our method not only achieves visually pleasing quality for low-light enhancement but also provides a good representation of image decomposition.



### Learning A Shared Transform Model for Skull to Digital Face Image Matching
- **Arxiv ID**: http://arxiv.org/abs/1808.04571v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.04571v1)
- **Published**: 2018-08-14 07:58:42+00:00
- **Updated**: 2018-08-14 07:58:42+00:00
- **Authors**: Maneet Singh, Shruti Nagpal, Richa Singh, Mayank Vatsa, Afzel Noore
- **Comment**: Accepted in IEEE International Conference on Biometrics: Theory,
  Applications and Systems (BTAS), 2018
- **Journal**: None
- **Summary**: Human skull identification is an arduous task, traditionally requiring the expertise of forensic artists and anthropologists. This paper is an effort to automate the process of matching skull images to digital face images, thereby establishing an identity of the skeletal remains. In order to achieve this, a novel Shared Transform Model is proposed for learning discriminative representations. The model learns robust features while reducing the intra-class variations between skulls and digital face images. Such a model can assist law enforcement agencies by speeding up the process of skull identification, and reducing the manual load. Experimental evaluation performed on two pre-defined protocols of the publicly available IdentifyMe dataset demonstrates the efficacy of the proposed model.



### Automatic Airway Segmentation in chest CT using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1808.04576v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.04576v1)
- **Published**: 2018-08-14 08:13:03+00:00
- **Updated**: 2018-08-14 08:13:03+00:00
- **Authors**: A. Garcia-Uceda Juarez, H. A. W. M. Tiddens, M. de Bruijne
- **Comment**: None
- **Journal**: None
- **Summary**: Segmentation of the airway tree from chest computed tomography (CT) images is critical for quantitative assessment of airway diseases including bronchiectasis and chronic obstructive pulmonary disease (COPD). However, obtaining an accurate segmentation of airways from CT scans is difficult due to the high complexity of airway structures. Recently, deep convolutional neural networks (CNNs) have become the state-of-the-art for many segmentation tasks, and in particular the so-called Unet architecture for biomedical images. However, its application to the segmentation of airways still remains a challenging task. This work presents a simple but robust approach based on a 3D Unet to perform segmentation of airways from chest CTs. The method is trained on a dataset composed of 12 CTs, and tested on another 6 CTs. We evaluate the influence of different loss functions and data augmentation techniques, and reach an average dice coefficient of 0.8 between the ground-truth and our automated segmentations.



### DeepNeuro: an open-source deep learning toolbox for neuroimaging
- **Arxiv ID**: http://arxiv.org/abs/1808.04589v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.04589v1)
- **Published**: 2018-08-14 09:03:39+00:00
- **Updated**: 2018-08-14 09:03:39+00:00
- **Authors**: Andrew Beers, James Brown, Ken Chang, Katharina Hoebel, Elizabeth Gerstner, Bruce Rosen, Jayashree Kalpathy-Cramer
- **Comment**: None
- **Journal**: None
- **Summary**: Translating neural networks from theory to clinical practice has unique challenges, specifically in the field of neuroimaging. In this paper, we present DeepNeuro, a deep learning framework that is best-suited to putting deep learning algorithms for neuroimaging in practical usage with a minimum of friction. We show how this framework can be used to both design and train neural network architectures, as well as modify state-of-the-art architectures in a flexible and intuitive way. We display the pre- and postprocessing functions common in the medical imaging community that DeepNeuro offers to ensure consistent performance of networks across variable users, institutions, and scanners. And we show how pipelines created in DeepNeuro can be concisely packaged into shareable Docker containers and command-line interfaces using DeepNeuro's pipeline resources.



### Unsupervised learning of foreground object detection
- **Arxiv ID**: http://arxiv.org/abs/1808.04593v1
- **DOI**: 10.1007/s11263-019-01183-3
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.04593v1)
- **Published**: 2018-08-14 09:12:28+00:00
- **Updated**: 2018-08-14 09:12:28+00:00
- **Authors**: Ioana Croitoru, Simion-Vlad Bogolin, Marius Leordeanu
- **Comment**: International Journal of Computer Vision (IJCV), 2019
- **Journal**: None
- **Summary**: Unsupervised learning poses one of the most difficult challenges in computer vision today. The task has an immense practical value with many applications in artificial intelligence and emerging technologies, as large quantities of unlabeled videos can be collected at relatively low cost. In this paper, we address the unsupervised learning problem in the context of detecting the main foreground objects in single images. We train a student deep network to predict the output of a teacher pathway that performs unsupervised object discovery in videos or large image collections. Our approach is different from published methods on unsupervised object discovery. We move the unsupervised learning phase during training time, then at test time we apply the standard feed-forward processing along the student pathway. This strategy has the benefit of allowing increased generalization possibilities during training, while remaining fast at testing. Our unsupervised learning algorithm can run over several generations of student-teacher training. Thus, a group of student networks trained in the first generation collectively create the teacher at the next generation. In experiments our method achieves top results on three current datasets for object discovery in video, unsupervised image segmentation and saliency detection. At test time the proposed system is fast, being one to two orders of magnitude faster than published unsupervised methods.



### Looking Beyond a Clever Narrative: Visual Context and Attention are Primary Drivers of Affect in Video Advertisements
- **Arxiv ID**: http://arxiv.org/abs/1808.04610v1
- **DOI**: 10.1145/3242969.3242988
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.04610v1)
- **Published**: 2018-08-14 10:16:30+00:00
- **Updated**: 2018-08-14 10:16:30+00:00
- **Authors**: Abhinav Shukla, Harish Katti, Mohan Kankanhalli, Ramanathan Subramanian
- **Comment**: Accepted for publication in the Proceedings of 20th ACM International
  Conference on Multimodal Interaction, Boulder, CO, USA
- **Journal**: None
- **Summary**: Emotion evoked by an advertisement plays a key role in influencing brand recall and eventual consumer choices. Automatic ad affect recognition has several useful applications. However, the use of content-based feature representations does not give insights into how affect is modulated by aspects such as the ad scene setting, salient object attributes and their interactions. Neither do such approaches inform us on how humans prioritize visual information for ad understanding. Our work addresses these lacunae by decomposing video content into detected objects, coarse scene structure, object statistics and actively attended objects identified via eye-gaze. We measure the importance of each of these information channels by systematically incorporating related information into ad affect prediction models. Contrary to the popular notion that ad affect hinges on the narrative and the clever use of linguistic and social cues, we find that actively attended objects and the coarse scene structure better encode affective information as compared to individual scene objects or conspicuous background elements.



### Deep Learning Framework for Digital Breast Tomosynthesis Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1808.04640v1
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1808.04640v1)
- **Published**: 2018-08-14 11:41:32+00:00
- **Updated**: 2018-08-14 11:41:32+00:00
- **Authors**: Nikita Moriakov, Koen Michielsen, Jonas Adler, Ritse Mann, Ioannis Sechopoulos, Jonas Teuwen
- **Comment**: 4 pages, 2 figures, submitted to SPIE
- **Journal**: None
- **Summary**: Digital breast tomosynthesis is rapidly replacing digital mammography as the basic x-ray technique for evaluation of the breasts. However, the sparse sampling and limited angular range gives rise to different artifacts, which manufacturers try to solve in several ways. In this study we propose an extension of the Learned Primal-Dual algorithm for digital breast tomosynthesis. The Learned Primal-Dual algorithm is a deep neural network consisting of several `reconstruction blocks', which take in raw sinogram data as the initial input, perform a forward and a backward pass by taking projections and back-projections, and use a convolutional neural network to produce an intermediate reconstruction result which is then improved further by the successive reconstruction block. We extend the architecture by providing breast thickness measurements as a mask to the neural network and allow it to learn how to use this thickness mask. We have trained the algorithm on digital phantoms and the corresponding noise-free/noisy projections, and then tested the algorithm on digital phantoms for varying level of noise. Reconstruction performance of the algorithms was compared visually, using MSE loss and Structural Similarity Index. Results indicate that the proposed algorithm outperforms the baseline iterative reconstruction algorithm in terms of reconstruction quality for both breast edges and internal structures and is robust to noise.



### Buildings Detection in VHR SAR Images Using Fully Convolution Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1808.06155v1
- **DOI**: 10.1109/TGRS.2018.2864716
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1808.06155v1)
- **Published**: 2018-08-14 12:47:28+00:00
- **Updated**: 2018-08-14 12:47:28+00:00
- **Authors**: Muhammad Shahzad, Michael Maurer, Friedrich Fraundorfer, Yuanyuan Wang, Xiao Xiang Zhu
- **Comment**: Accepted publication in IEEE TGRS
- **Journal**: None
- **Summary**: This paper addresses the highly challenging problem of automatically detecting man-made structures especially buildings in very high resolution (VHR) synthetic aperture radar (SAR) images. In this context, the paper has two major contributions: Firstly, it presents a novel and generic workflow that initially classifies the spaceborne TomoSAR point clouds $ - $ generated by processing VHR SAR image stacks using advanced interferometric techniques known as SAR tomography (TomoSAR) $ - $ into buildings and non-buildings with the aid of auxiliary information (i.e., either using openly available 2-D building footprints or adopting an optical image classification scheme) and later back project the extracted building points onto the SAR imaging coordinates to produce automatic large-scale benchmark labelled (buildings/non-buildings) SAR datasets. Secondly, these labelled datasets (i.e., building masks) have been utilized to construct and train the state-of-the-art deep Fully Convolution Neural Networks with an additional Conditional Random Field represented as a Recurrent Neural Network to detect building regions in a single VHR SAR image. Such a cascaded formation has been successfully employed in computer vision and remote sensing fields for optical image classification but, to our knowledge, has not been applied to SAR images. The results of the building detection are illustrated and validated over a TerraSAR-X VHR spotlight SAR image covering approximately 39 km$ ^2 $ $ - $ almost the whole city of Berlin $ - $ with mean pixel accuracies of around 93.84%



### Improving Generalization via Scalable Neighborhood Component Analysis
- **Arxiv ID**: http://arxiv.org/abs/1808.04699v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1808.04699v1)
- **Published**: 2018-08-14 14:03:47+00:00
- **Updated**: 2018-08-14 14:03:47+00:00
- **Authors**: Zhirong Wu, Alexei A. Efros, Stella X. Yu
- **Comment**: To appear in ECCV 2018
- **Journal**: None
- **Summary**: Current major approaches to visual recognition follow an end-to-end formulation that classifies an input image into one of the pre-determined set of semantic categories. Parametric softmax classifiers are a common choice for such a closed world with fixed categories, especially when big labeled data is available during training. However, this becomes problematic for open-set scenarios where new categories are encountered with very few examples for learning a generalizable parametric classifier. We adopt a non-parametric approach for visual recognition by optimizing feature embeddings instead of parametric classifiers. We use a deep neural network to learn the visual feature that preserves the neighborhood structure in the semantic space, based on the Neighborhood Component Analysis (NCA) criterion. Limited by its computational bottlenecks, we devise a mechanism to use augmented memory to scale NCA for large datasets and very deep networks. Our experiments deliver not only remarkable performance on ImageNet classification for such a simple non-parametric method, but most importantly a more generalizable feature representation for sub-category discovery and few-shot recognition.



### Imagining the Unseen: Learning a Distribution over Incomplete Images with Dense Latent Trees
- **Arxiv ID**: http://arxiv.org/abs/1808.04745v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.04745v1)
- **Published**: 2018-08-14 15:24:39+00:00
- **Updated**: 2018-08-14 15:24:39+00:00
- **Authors**: Sebastian Kaltwang, Sina Samangooei, John Redford, Andrew Blake
- **Comment**: None
- **Journal**: None
- **Summary**: Images are composed as a hierarchy of object parts. We use this insight to create a generative graphical model that defines a hierarchical distribution over image parts. Typically, this leads to intractable inference due to loops in the graph. We propose an alternative model structure, the Dense Latent Tree (DLT), which avoids loops and allows for efficient exact inference, while maintaining a dense connectivity between parts of the hierarchy. The usefulness of DLTs is shown for the example task of image completion on partially observed MNIST and Fashion-MNIST data. We verify having successfully learned a hierarchical model of images by visualising its latent states.



### Treepedia 2.0: Applying Deep Learning for Large-scale Quantification of Urban Tree Cover
- **Arxiv ID**: http://arxiv.org/abs/1808.04754v1
- **DOI**: 10.1109/bigdatacongress.2018.00014
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.04754v1)
- **Published**: 2018-08-14 15:34:22+00:00
- **Updated**: 2018-08-14 15:34:22+00:00
- **Authors**: Bill Yang Cai, Xiaojiang Li, Ian Seiferling, Carlo Ratti
- **Comment**: Accepted and will appear in IEEE BigData Congress 2018 Conference
  Proceedings
- **Journal**: None
- **Summary**: Recent advances in deep learning have made it possible to quantify urban metrics at fine resolution, and over large extents using street-level images. Here, we focus on measuring urban tree cover using Google Street View (GSV) images. First, we provide a small-scale labelled validation dataset and propose standard metrics to compare the performance of automated estimations of street tree cover using GSV. We apply state-of-the-art deep learning models, and compare their performance to a previously established benchmark of an unsupervised method. Our training procedure for deep learning models is novel; we utilize the abundance of openly available and similarly labelled street-level image datasets to pre-train our model. We then perform additional training on a small training dataset consisting of GSV images. We find that deep learning models significantly outperform the unsupervised benchmark method. Our semantic segmentation model increased mean intersection-over-union (IoU) from 44.10% to 60.42% relative to the unsupervised method and our end-to-end model decreased Mean Absolute Error from 10.04% to 4.67%. We also employ a recently developed method called gradient-weighted class activation map (Grad-CAM) to interpret the features learned by the end-to-end model. This technique confirms that the end-to-end model has accurately learned to identify tree cover area as key features for predicting percentage tree cover. Our paper provides an example of applying advanced deep learning techniques on a large-scale, geo-tagged and image-based dataset to efficiently estimate important urban metrics. The results demonstrate that deep learning models are highly accurate, can be interpretable, and can also be efficient in terms of data-labelling effort and computational resources.



### Clumped Nuclei Segmentation with Adjacent Point Match and Local Shape based Intensity Analysis for Overlapped Nuclei in Fluorescence In-Situ Hybridization Images
- **Arxiv ID**: http://arxiv.org/abs/1808.04795v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.04795v1)
- **Published**: 2018-08-14 16:58:59+00:00
- **Updated**: 2018-08-14 16:58:59+00:00
- **Authors**: Xiaoyuan Guo, Hanyi Yu, Blair Rossetti, George Teodoro, Daniel Brat, Jun Kong
- **Comment**: 4 pages
- **Journal**: None
- **Summary**: Highly clumped nuclei clusters captured in fluorescence in situ hybridization microscopy images are common histology entities under investigations in a wide spectrum of tissue-related biomedical investigations. Due to their large scale in presence, computer based image analysis is used to facilitate such analysis with improved analysis efficiency and reproducibility. To ensure the quality of downstream biomedical analyses, it is essential to segment clustered nuclei with high quality. However, this presents a technical challenge commonly encountered in a large number of biomedical research, as nuclei are often overlapped due to a high cell density. In this paper, we propose an segmentation algorithm that identifies point pair connection candidates and evaluates adjacent point connections with a formulated ellipse fitting quality indicator. After connection relationships are determined, we recover the resulting dividing paths by following points with specific eigenvalues from Hessian in a constrained searching space. We validate our algorithm with 560 image patches from two classes of tumor regions of seven brain tumor patients. Both qualitative and quantitative experimental results suggest that our algorithm is promising for dividing overlapped nuclei in fluorescence in situ hybridization microscopy images widely used in various biomedical research.



### Hierarchical binary CNNs for landmark localization with limited resources
- **Arxiv ID**: http://arxiv.org/abs/1808.04803v1
- **DOI**: 10.1109/TPAMI.2018.2866051
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1808.04803v1)
- **Published**: 2018-08-14 17:32:29+00:00
- **Updated**: 2018-08-14 17:32:29+00:00
- **Authors**: Adrian Bulat, Georgios Tzimiropoulos
- **Comment**: Accepted to IEEE TPAMI18: Best of ICCV 2017 SI. Previously portions
  of this work appeared as arXiv:1703.00862, which was the conference version
- **Journal**: None
- **Summary**: Our goal is to design architectures that retain the groundbreaking performance of Convolutional Neural Networks (CNNs) for landmark localization and at the same time are lightweight, compact and suitable for applications with limited computational resources. To this end, we make the following contributions: (a) we are the first to study the effect of neural network binarization on localization tasks, namely human pose estimation and face alignment. We exhaustively evaluate various design choices, identify performance bottlenecks, and more importantly propose multiple orthogonal ways to boost performance. (b) Based on our analysis, we propose a novel hierarchical, parallel and multi-scale residual architecture that yields large performance improvement over the standard bottleneck block while having the same number of parameters, thus bridging the gap between the original network and its binarized counterpart. (c) We perform a large number of ablation studies that shed light on the properties and the performance of the proposed block. (d) We present results for experiments on the most challenging datasets for human pose estimation and face alignment, reporting in many cases state-of-the-art performance. (e) We further provide additional results for the problem of facial part segmentation. Code can be downloaded from https://www.adrianbulat.com/binary-cnn-landmark



### Multispectral Pedestrian Detection via Simultaneous Detection and Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1808.04818v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.04818v1)
- **Published**: 2018-08-14 17:59:12+00:00
- **Updated**: 2018-08-14 17:59:12+00:00
- **Authors**: Chengyang Li, Dan Song, Ruofeng Tong, Min Tang
- **Comment**: British Machine Vision Conference (BMVC) 2018
- **Journal**: None
- **Summary**: Multispectral pedestrian detection has attracted increasing attention from the research community due to its crucial competence for many around-the-clock applications (e.g., video surveillance and autonomous driving), especially under insufficient illumination conditions. We create a human baseline over the KAIST dataset and reveal that there is still a large gap between current top detectors and human performance. To narrow this gap, we propose a network fusion architecture, which consists of a multispectral proposal network to generate pedestrian proposals, and a subsequent multispectral classification network to distinguish pedestrian instances from hard negatives. The unified network is learned by jointly optimizing pedestrian detection and semantic segmentation tasks. The final detections are obtained by integrating the outputs from different modalities as well as the two stages. The approach significantly outperforms state-of-the-art methods on the KAIST dataset while remain fast. Additionally, we contribute a sanitized version of training annotations for the KAIST dataset, and examine the effects caused by different kinds of annotation errors. Future research of this problem will benefit from the sanitized version which eliminates the interference of annotation errors.



### URSA: A Neural Network for Unordered Point Clouds Using Constellations
- **Arxiv ID**: http://arxiv.org/abs/1808.04848v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.04848v2)
- **Published**: 2018-08-14 18:30:30+00:00
- **Updated**: 2018-10-23 18:08:13+00:00
- **Authors**: Mark B. Skouson, Brett J. Borghetti, Robert C. Leishman
- **Comment**: 11 pages, 6 Figures, 1 Table
- **Journal**: None
- **Summary**: This paper describes a neural network layer, named Ursa, that uses a constellation of points to learn classification information from point cloud data. Unlike other machine learning classification problems where the task is to classify an individual high-dimensional observation, in a point-cloud classification problem the goal is to classify a set of d-dimensional observations. Because a point cloud is a set, there is no ordering to the collection of points in a point-cloud classification problem. Thus, the challenge of classifying point clouds inputs is in building a classifier which is agnostic to the ordering of the observations, yet preserves the d-dimensional information of each point in the set. This research presents Ursa, a new layer type for an artificial neural network which achieves these two properties. Similar to new methods for this task, this architecture works directly on d-dimensional points rather than first converting the points to a d-dimensional volume. The Ursa layer is followed by a series of dense layers to classify 2D and 3D objects from point clouds. Experiments on ModelNet40 and MNIST data show classification results comparable with current methods, while reducing the training parameters by over 50 percent.



### GestureGAN for Hand Gesture-to-Gesture Translation in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1808.04859v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.04859v2)
- **Published**: 2018-08-14 18:57:22+00:00
- **Updated**: 2019-07-19 11:01:02+00:00
- **Authors**: Hao Tang, Wei Wang, Dan Xu, Yan Yan, Nicu Sebe
- **Comment**: 9 pages, 7 figures, accepted to ACM MM 2018 as an oral paper, fix
  typos
- **Journal**: None
- **Summary**: Hand gesture-to-gesture translation in the wild is a challenging task since hand gestures can have arbitrary poses, sizes, locations and self-occlusions. Therefore, this task requires a high-level understanding of the mapping between the input source gesture and the output target gesture. To tackle this problem, we propose a novel hand Gesture Generative Adversarial Network (GestureGAN). GestureGAN consists of a single generator $G$ and a discriminator $D$, which takes as input a conditional hand image and a target hand skeleton image. GestureGAN utilizes the hand skeleton information explicitly, and learns the gesture-to-gesture mapping through two novel losses, the color loss and the cycle-consistency loss. The proposed color loss handles the issue of "channel pollution" while back-propagating the gradients. In addition, we present the Fr\'echet ResNet Distance (FRD) to evaluate the quality of generated images. Extensive experiments on two widely used benchmark datasets demonstrate that the proposed GestureGAN achieves state-of-the-art performance on the unconstrained hand gesture-to-gesture translation task. Meanwhile, the generated images are in high-quality and are photo-realistic, allowing them to be used as data augmentation to improve the performance of a hand gesture classifier. Our model and code are available at https://github.com/Ha0Tang/GestureGAN.



### Latent Agents in Networks: Estimation and Targeting
- **Arxiv ID**: http://arxiv.org/abs/1808.04878v3
- **DOI**: None
- **Categories**: **cs.SI**, cs.CV, econ.TH, math.ST, stat.TH
- **Links**: [PDF](http://arxiv.org/pdf/1808.04878v3)
- **Published**: 2018-08-14 19:57:55+00:00
- **Updated**: 2022-01-26 20:54:34+00:00
- **Authors**: Baris Ata, Alexandre Belloni, Ozan Candogan
- **Comment**: 106 pages
- **Journal**: None
- **Summary**: We consider a network of agents. Associated with each agent are her covariate and outcome. Agents influence each other's outcomes according to a certain connection/influence structure. A subset of the agents participate on a platform, and hence, are observable to it. The rest are not observable to the platform and are called the latent agents. The platform does not know the influence structure of the observable or the latent parts of the network. It only observes the data on past covariates and decisions of the observable agents. Observable agents influence each other both directly and indirectly through the influence they exert on the latent agents.   We investigate how the platform can estimate the dependence of the observable agents' outcomes on their covariates, taking the latent agents into account. First, we show that this relationship can be succinctly captured by a matrix and provide an algorithm for estimating it under a suitable approximate sparsity condition using historical data of covariates and outcomes for the observable agents. We also obtain convergence rates for the proposed estimator despite the high dimensionality that allows more agents than observations. Second, we show that the approximate sparsity condition holds under the standard conditions used in the literature. Hence, our results apply to a large class of networks. Finally, we apply our results to two practical settings: targeted advertising and promotional pricing. We show that by using the available historical data with our estimator, it is possible to obtain asymptotically optimal advertising/pricing decisions, despite the presence of latent agents.



### Cross-view image synthesis using geometry-guided conditional GANs
- **Arxiv ID**: http://arxiv.org/abs/1808.05469v2
- **DOI**: 10.1016/j.cviu.2019.07.008
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.05469v2)
- **Published**: 2018-08-14 21:24:26+00:00
- **Updated**: 2019-07-18 05:34:36+00:00
- **Authors**: Krishna Regmi, Ali Borji
- **Comment**: Under review as a journal paper at CVIU. arXiv admin note:
  substantial text overlap with arXiv:1803.03396
- **Journal**: None
- **Summary**: We address the problem of generating images across two drastically different views, namely ground (street) and aerial (overhead) views. Image synthesis by itself is a very challenging computer vision task and is even more so when generation is conditioned on an image in another view. Due the difference in viewpoints, there is small overlapping field of view and little common content between these two views. Here, we try to preserve the pixel information between the views so that the generated image is a realistic representation of cross view input image. For this, we propose to use homography as a guide to map the images between the views based on the common field of view to preserve the details in the input image. We then use generative adversarial networks to inpaint the missing regions in the transformed image and add realism to it. Our exhaustive evaluation and model comparison demonstrate that utilizing geometry constraints adds fine details to the generated images and can be a better approach for cross view image synthesis than purely pixel based synthesis methods.



### Vendor-independent soft tissue lesion detection using weakly supervised and unsupervised adversarial domain adaptation
- **Arxiv ID**: http://arxiv.org/abs/1808.04909v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.04909v1)
- **Published**: 2018-08-14 21:58:28+00:00
- **Updated**: 2018-08-14 21:58:28+00:00
- **Authors**: Joris van Vugt, Elena Marchiori, Ritse Mann, Albert Gubern-Mérida, Nikita Moriakov, Jonas Teuwen
- **Comment**: Submitted to SPIE MI 2019
- **Journal**: None
- **Summary**: Computer-aided detection aims to improve breast cancer screening programs by helping radiologists to evaluate digital mammography (DM) exams. DM exams are generated by devices from different vendors, with diverse characteristics between and even within vendors. Physical properties of these devices and postprocessing of the images can greatly influence the resulting mammogram. This results in the fact that a deep learning model trained on data from one vendor cannot readily be applied to data from another vendor. This paper investigates the use of tailored transfer learning methods based on adversarial learning to tackle this problem. We consider a database of DM exams (mostly bilateral and two views) generated by Hologic and Siemens vendors. We analyze two transfer learning settings: 1) unsupervised transfer, where Hologic data with soft lesion annotation at pixel level and Siemens unlabelled data are used to annotate images in the latter data; 2) weak supervised transfer, where exam level labels for images from the Siemens mammograph are available. We propose tailored variants of recent state-of-the-art methods for transfer learning which take into account the class imbalance and incorporate knowledge provided by the annotations at exam level. Results of experiments indicate the beneficial effect of transfer learning in both transfer settings. Notably, at 0.02 false positives per image, we achieve a sensitivity of 0.37, compared to 0.30 of a baseline with no transfer. Results indicate that using exam level annotations gives an additional increase in sensitivity.



