# Arxiv Papers in cs.CV on 2018-08-06
### A Study of Deep Feature Fusion based Methods for Classifying Multi-lead ECG
- **Arxiv ID**: http://arxiv.org/abs/1808.01721v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.01721v1)
- **Published**: 2018-08-06 03:23:53+00:00
- **Updated**: 2018-08-06 03:23:53+00:00
- **Authors**: Bin Chen, Wei Guo, Bin Li, Rober K. F. Teng, Mingjun Dai, Jianping Luo, Hui Wang
- **Comment**: 6 pages, 5 figures
- **Journal**: None
- **Summary**: An automatic classification method has been studied to effectively detect and recognize Electrocardiogram (ECG). Based on the synchronizing and orthogonal relationships of multiple leads, we propose a Multi-branch Convolution and Residual Network (MBCRNet) with three kinds of feature fusion methods for automatic detection of normal and abnormal ECG signals. Experiments are conducted on the Chinese Cardiovascular Disease Database (CCDD). Through 10-fold cross-validation, we achieve an average accuracy of 87.04% and a sensitivity of 89.93%, which outperforms previous methods under the same database. It is also shown that the multi-lead feature fusion network can improve the classification accuracy over the network only with the single lead features.



### Liquid Pouring Monitoring via Rich Sensory Inputs
- **Arxiv ID**: http://arxiv.org/abs/1808.01725v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.01725v1)
- **Published**: 2018-08-06 03:59:00+00:00
- **Updated**: 2018-08-06 03:59:00+00:00
- **Authors**: Tz-Ying Wu, Juan-Ting Lin, Tsun-Hsuang Wang, Chan-Wei Hu, Juan Carlos Niebles, Min Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Humans have the amazing ability to perform very subtle manipulation task using a closed-loop control system with imprecise mechanics (i.e., our body parts) but rich sensory information (e.g., vision, tactile, etc.). In the closed-loop system, the ability to monitor the state of the task via rich sensory information is important but often less studied. In this work, we take liquid pouring as a concrete example and aim at learning to continuously monitor whether liquid pouring is successful (e.g., no spilling) or not via rich sensory inputs. We mimic humans' rich sensories using synchronized observation from a chest-mounted camera and a wrist-mounted IMU sensor. Given many success and failure demonstrations of liquid pouring, we train a hierarchical LSTM with late fusion for monitoring. To improve the robustness of the system, we propose two auxiliary tasks during training: inferring (1) the initial state of containers and (2) forecasting the one-step future 3D trajectory of the hand with an adversarial training procedure. These tasks encourage our method to learn representation sensitive to container states and how objects are manipulated in 3D. With these novel components, our method achieves ~8% and ~11% better monitoring accuracy than the baseline method without auxiliary tasks on unseen containers and unseen users respectively.



### Incorporating Scalability in Unsupervised Spatio-Temporal Feature Learning
- **Arxiv ID**: http://arxiv.org/abs/1808.01727v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.01727v2)
- **Published**: 2018-08-06 04:11:07+00:00
- **Updated**: 2018-08-15 00:44:53+00:00
- **Authors**: Sujoy Paul, Sourya Roy, Amit K. Roy-Chowdhury
- **Comment**: International Conference on Acoustics, Speech, and Signal Processing
  (ICASSP), 2018
- **Journal**: None
- **Summary**: Deep neural networks are efficient learning machines which leverage upon a large amount of manually labeled data for learning discriminative features. However, acquiring substantial amount of supervised data, especially for videos can be a tedious job across various computer vision tasks. This necessitates learning of visual features from videos in an unsupervised setting. In this paper, we propose a computationally simple, yet effective, framework to learn spatio-temporal feature embedding from unlabeled videos. We train a Convolutional 3D Siamese network using positive and negative pairs mined from videos under certain probabilistic assumptions. Experimental results on three datasets demonstrate that our proposed framework is able to learn weights which can be used for same as well as cross dataset and tasks.



### Deep Transfer Learning for EEG-based Brain Computer Interface
- **Arxiv ID**: http://arxiv.org/abs/1808.01752v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.01752v1)
- **Published**: 2018-08-06 07:23:34+00:00
- **Updated**: 2018-08-06 07:23:34+00:00
- **Authors**: Chuanqi Tan, Fuchun Sun, Wenchang Zhang
- **Comment**: In Proceedings of IEEE International Conference on Acoustics, Speech
  and Signal Processing (ICASSP) 2018, 15-20 April 2018, Alberta, Canada
- **Journal**: None
- **Summary**: The electroencephalography classifier is the most important component of brain-computer interface based systems. There are two major problems hindering the improvement of it. First, traditional methods do not fully exploit multimodal information. Second, large-scale annotated EEG datasets are almost impossible to acquire because biological data acquisition is challenging and quality annotation is costly. Herein, we propose a novel deep transfer learning approach to solve these two problems. First, we model cognitive events based on EEG data by characterizing the data using EEG optical flow, which is designed to preserve multimodal EEG information in a uniform representation. Second, we design a deep transfer learning framework which is suitable for transferring knowledge by joint training, which contains a adversarial network and a special loss function. The experiments demonstrate that our approach, when applied to EEG classification tasks, has many advantages, such as robustness and accuracy.



### Gray-box Adversarial Training
- **Arxiv ID**: http://arxiv.org/abs/1808.01753v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1808.01753v1)
- **Published**: 2018-08-06 07:26:44+00:00
- **Updated**: 2018-08-06 07:26:44+00:00
- **Authors**: Vivek B. S., Konda Reddy Mopuri, R. Venkatesh Babu
- **Comment**: Accepted to ECCV 2018
- **Journal**: None
- **Summary**: Adversarial samples are perturbed inputs crafted to mislead the machine learning systems. A training mechanism, called adversarial training, which presents adversarial samples along with clean samples has been introduced to learn robust models. In order to scale adversarial training for large datasets, these perturbations can only be crafted using fast and simple methods (e.g., gradient ascent). However, it is shown that adversarial training converges to a degenerate minimum, where the model appears to be robust by generating weaker adversaries. As a result, the models are vulnerable to simple black-box attacks. In this paper we, (i) demonstrate the shortcomings of existing evaluation policy, (ii) introduce novel variants of white-box and black-box attacks, dubbed gray-box adversarial attacks" based on which we propose novel evaluation method to assess the robustness of the learned models, and (iii) propose a novel variant of adversarial training, named Graybox Adversarial Training" that uses intermediate versions of the models to seed the adversaries. Experimental evaluation demonstrates that the models trained using our method exhibit better robustness compared to both undefended and adversarially trained model



### Defense Against Adversarial Attacks with Saak Transform
- **Arxiv ID**: http://arxiv.org/abs/1808.01785v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/1808.01785v1)
- **Published**: 2018-08-06 09:01:41+00:00
- **Updated**: 2018-08-06 09:01:41+00:00
- **Authors**: Sibo Song, Yueru Chen, Ngai-Man Cheung, C. -C. Jay Kuo
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) are known to be vulnerable to adversarial perturbations, which imposes a serious threat to DNN-based decision systems. In this paper, we propose to apply the lossy Saak transform to adversarially perturbed images as a preprocessing tool to defend against adversarial attacks. Saak transform is a recently-proposed state-of-the-art for computing the spatial-spectral representations of input images. Empirically, we observe that outputs of the Saak transform are very discriminative in differentiating adversarial examples from clean ones. Therefore, we propose a Saak transform based preprocessing method with three steps: 1) transforming an input image to a joint spatial-spectral representation via the forward Saak transform, 2) apply filtering to its high-frequency components, and, 3) reconstructing the image via the inverse Saak transform. The processed image is found to be robust against adversarial perturbations. We conduct extensive experiments to investigate various settings of the Saak transform and filtering functions. Without harming the decision performance on clean images, our method outperforms state-of-the-art adversarial defense methods by a substantial margin on both the CIFAR-10 and ImageNet datasets. Importantly, our results suggest that adversarial perturbations can be effectively and efficiently defended using state-of-the-art frequency analysis.



### FaceOff: Anonymizing Videos in the Operating Rooms
- **Arxiv ID**: http://arxiv.org/abs/1808.04440v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1808.04440v1)
- **Published**: 2018-08-06 09:36:08+00:00
- **Updated**: 2018-08-06 09:36:08+00:00
- **Authors**: Evangello Flouty, Odysseas Zisimopoulos, Danail Stoyanov
- **Comment**: MICCAI 2018: OR 2.0 Context-Aware Operating Theaters
- **Journal**: None
- **Summary**: Video capture in the surgical operating room (OR) is increasingly possible and has potential for use with computer assisted interventions (CAI), surgical data science and within smart OR integration. Captured video innately carries sensitive information that should not be completely visible in order to preserve the patient's and the clinical teams' identities. When surgical video streams are stored on a server, the videos must be anonymized prior to storage if taken outside of the hospital. In this article, we describe how a deep learning model, Faster R-CNN, can be used for this purpose and help to anonymize video data captured in the OR. The model detects and blurs faces in an effort to preserve anonymity. After testing an existing face detection trained model, a new dataset tailored to the surgical environment, with faces obstructed by surgical masks and caps, was collected for fine-tuning to achieve higher face-detection rates in the OR. We also propose a temporal regularisation kernel to improve recall rates. The fine-tuned model achieves a face detection recall of 88.05 % and 93.45 % before and after applying temporal-smoothing respectively.



### X-GANs: Image Reconstruction Made Easy for Extreme Cases
- **Arxiv ID**: http://arxiv.org/abs/1808.04432v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1808.04432v1)
- **Published**: 2018-08-06 10:36:53+00:00
- **Updated**: 2018-08-06 10:36:53+00:00
- **Authors**: Longfei Liu, Sheng Li, Yisong Chen, Guoping Wang
- **Comment**: 9 pages, 12 figures
- **Journal**: None
- **Summary**: Image reconstruction including image restoration and denoising is a challenging problem in the field of image computing. We present a new method, called X-GANs, for reconstruction of arbitrary corrupted resource based on a variant of conditional generative adversarial networks (conditional GANs). In our method, a novel generator and multi-scale discriminators are proposed, as well as the combined adversarial losses, which integrate a VGG perceptual loss, an adversarial perceptual loss, and an elaborate corresponding point loss together based on the analysis of image feature. Our conditional GANs have enabled a variety of applications in image reconstruction, including image denoising, image restoration from quite a sparse sampling, image inpainting, image recovery from the severely polluted block or even color-noise dominated images, which are extreme cases and haven't been addressed in the status quo. We have significantly improved the accuracy and quality of image reconstruction. Extensive perceptual experiments on datasets ranging from human faces to natural scenes demonstrate that images reconstructed by the presented approach are considerably more realistic than alternative work. Our method can also be extended to handle high-ratio image compression.



### Visual Question Generation for Class Acquisition of Unknown Objects
- **Arxiv ID**: http://arxiv.org/abs/1808.01821v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.01821v1)
- **Published**: 2018-08-06 11:14:35+00:00
- **Updated**: 2018-08-06 11:14:35+00:00
- **Authors**: Kohei Uehara, Antonio Tejero-De-Pablos, Yoshitaka Ushiku, Tatsuya Harada
- **Comment**: None
- **Journal**: None
- **Summary**: Traditional image recognition methods only consider objects belonging to already learned classes. However, since training a recognition model with every object class in the world is unfeasible, a way of getting information on unknown objects (i.e., objects whose class has not been learned) is necessary. A way for an image recognition system to learn new classes could be asking a human about objects that are unknown. In this paper, we propose a method for generating questions about unknown objects in an image, as means to get information about classes that have not been learned. Our method consists of a module for proposing objects, a module for identifying unknown objects, and a module for generating questions about unknown objects. The experimental results via human evaluation show that our method can successfully get information about unknown objects in an image dataset. Our code and dataset are available at https://github.com/mil-tokyo/vqg-unknown.



### Detailed Dense Inference with Convolutional Neural Networks via Discrete Wavelet Transform
- **Arxiv ID**: http://arxiv.org/abs/1808.01834v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.01834v1)
- **Published**: 2018-08-06 11:57:15+00:00
- **Updated**: 2018-08-06 11:57:15+00:00
- **Authors**: Lingni Ma, Jörg Stückler, Tao Wu, Daniel Cremers
- **Comment**: This work was first submitted to NIPS 2017, May 2017
- **Journal**: None
- **Summary**: Dense pixelwise prediction such as semantic segmentation is an up-to-date challenge for deep convolutional neural networks (CNNs). Many state-of-the-art approaches either tackle the loss of high-resolution information due to pooling in the encoder stage, or use dilated convolutions or high-resolution lanes to maintain detailed feature maps and predictions. Motivated by the structural analogy between multi-resolution wavelet analysis and the pooling/unpooling layers of CNNs, we introduce discrete wavelet transform (DWT) into the CNN encoder-decoder architecture and propose WCNN. The high-frequency wavelet coefficients are computed at encoder, which are later used at the decoder to unpooled jointly with coarse-resolution feature maps through the inverse DWT. The DWT/iDWT is further used to develop two wavelet pyramids to capture the global context, where the multi-resolution DWT is applied to successively reduce the spatial resolution and increase the receptive field. Experiment with the Cityscape dataset, the proposed WCNNs are computationally efficient and yield improvements the accuracy for high-resolution dense pixelwise prediction.



### Improving Temporal Interpolation of Head and Body Pose using Gaussian Process Regression in a Matrix Completion Setting
- **Arxiv ID**: http://arxiv.org/abs/1808.01837v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.01837v1)
- **Published**: 2018-08-06 12:05:53+00:00
- **Updated**: 2018-08-06 12:05:53+00:00
- **Authors**: Stephanie Tan, Hayley Hung
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a model for head and body pose estimation (HBPE) when labelled samples are highly sparse. The current state-of-the-art multimodal approach to HBPE utilizes the matrix completion method in a transductive setting to predict pose labels for unobserved samples. Based on this approach, the proposed method tackles HBPE when manually annotated ground truth labels are temporally sparse. We posit that the current state of the art approach oversimplifies the temporal sparsity assumption by using Laplacian smoothing. Our final solution uses: i) Gaussian process regression in place of Laplacian smoothing, ii) head and body coupling, and iii) nuclear norm minimization in the matrix completion setting. The model is applied to the challenging SALSA dataset for benchmark against the state-of-the-art method. Our presented formulation outperforms the state-of-the-art significantly in this particular setting, e.g. at 5% ground truth labels as training data, head pose accuracy and body pose accuracy is approximately 62% and 70%, respectively. As well as fitting a more flexible model to missing labels in time, we posit that our approach also loosens the head and body coupling constraint, allowing for a more expressive model of the head and body pose typically seen during conversational interaction in groups. This provides a new baseline to improve upon for future integration of multimodal sensor data for the purpose of HBPE.



### Occlusions, Motion and Depth Boundaries with a Generic Network for Disparity, Optical Flow or Scene Flow Estimation
- **Arxiv ID**: http://arxiv.org/abs/1808.01838v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.01838v2)
- **Published**: 2018-08-06 12:10:50+00:00
- **Updated**: 2018-08-08 09:26:30+00:00
- **Authors**: Eddy Ilg, Tonmoy Saikia, Margret Keuper, Thomas Brox
- **Comment**: Accepted to ECCV 2018 as poster. See video at:
  https://www.youtube.com/watch?v=SwOdSaBRysI
- **Journal**: None
- **Summary**: Occlusions play an important role in disparity and optical flow estimation, since matching costs are not available in occluded areas and occlusions indicate depth or motion boundaries. Moreover, occlusions are relevant for motion segmentation and scene flow estimation. In this paper, we present an efficient learning-based approach to estimate occlusion areas jointly with disparities or optical flow. The estimated occlusions and motion boundaries clearly improve over the state-of-the-art. Moreover, we present networks with state-of-the-art performance on the popular KITTI benchmark and good generic performance. Making use of the estimated occlusions, we also show improved results on motion segmentation and scene flow estimation.



### Metal Artifact Reduction in Cone-Beam X-Ray CT via Ray Profile Correction
- **Arxiv ID**: http://arxiv.org/abs/1808.01853v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.01853v1)
- **Published**: 2018-08-06 12:41:44+00:00
- **Updated**: 2018-08-06 12:41:44+00:00
- **Authors**: Sungsoo Ha, Klaus Mueller
- **Comment**: 10 pages, 8 figures
- **Journal**: None
- **Summary**: In computed tomography (CT), metal implants increase the inconsistencies between the measured data and the linear attenuation assumption made by analytic CT reconstruction algorithms. The inconsistencies give rise to dark and bright bands and streaks in the reconstructed image, collectively called metal artifacts. These artifacts make it difficult for radiologists to render correct diagnostic decisions. We describe a data-driven metal artifact reduction (MAR) algorithm for image-guided spine surgery that applies to scenarios in which a prior CT scan of the patient is available. We tested the proposed method with two clinical datasets that were both obtained during spine surgery. Using the proposed method, we were not only able to remove the dark and bright streaks caused by the implanted screws but we also recovered the anatomical structures hidden by these artifacts. This results in an improved capability of surgeons to confirm the correctness of the implanted pedicle screw placements.



### DeepTAM: Deep Tracking and Mapping
- **Arxiv ID**: http://arxiv.org/abs/1808.01900v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.01900v2)
- **Published**: 2018-08-06 13:43:31+00:00
- **Updated**: 2018-08-07 16:44:49+00:00
- **Authors**: Huizhong Zhou, Benjamin Ummenhofer, Thomas Brox
- **Comment**: Accepted to ECCV 2018 as oral. Project page:
  https://lmb.informatik.uni-freiburg.de/people/zhouh/deeptam/
- **Journal**: None
- **Summary**: We present a system for keyframe-based dense camera tracking and depth map estimation that is entirely learned. For tracking, we estimate small pose increments between the current camera image and a synthetic viewpoint. This significantly simplifies the learning problem and alleviates the dataset bias for camera motions. Further, we show that generating a large number of pose hypotheses leads to more accurate predictions. For mapping, we accumulate information in a cost volume centered at the current depth estimate. The mapping network then combines the cost volume and the keyframe image to update the depth prediction, thereby effectively making use of depth measurements and image-based priors. Our approach yields state-of-the-art results with few images and is robust with respect to noisy camera poses. We demonstrate that the performance of our 6 DOF tracking competes with RGB-D tracking algorithms. We compare favorably against strong classic and deep learning powered dense depth algorithms.



### Error Correction Maximization for Deep Image Hashing
- **Arxiv ID**: http://arxiv.org/abs/1808.01942v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.01942v1)
- **Published**: 2018-08-06 14:48:15+00:00
- **Updated**: 2018-08-06 14:48:15+00:00
- **Authors**: Xiang Xu, Xiaofang Wang, Kris M. Kitani
- **Comment**: None
- **Journal**: None
- **Summary**: We propose to use the concept of the Hamming bound to derive the optimal criteria for learning hash codes with a deep network. In particular, when the number of binary hash codes (typically the number of image categories) and code length are known, it is possible to derive an upper bound on the minimum Hamming distance between the hash codes. This upper bound can then be used to define the loss function for learning hash codes. By encouraging the margin (minimum Hamming distance) between the hash codes of different image categories to match the upper bound, we are able to learn theoretically optimal hash codes. Our experiments show that our method significantly outperforms competing deep learning-based approaches and obtains top performance on benchmark datasets.



### V-FCNN: Volumetric Fully Convolution Neural Network For Automatic Atrial Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1808.01944v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1808.01944v2)
- **Published**: 2018-08-06 14:51:33+00:00
- **Updated**: 2018-09-27 09:27:11+00:00
- **Authors**: Nicoló Savioli, Giovanni Montana, Pablo Lamata
- **Comment**: 9 pages, 4 figures, In Proceedings of MICCAI 2018 Atrial Segmentation
  Challenge
- **Journal**: None
- **Summary**: Atrial Fibrillation (AF) is a common electro-physiological cardiac disorder that causes changes in the anatomy of the atria. A better characterization of these changes is desirable for the definition of clinical biomarkers, furthermore, thus there is a need for its fully automatic segmentation from clinical images. In this work, we present an architecture based on 3D-convolution kernels, a Volumetric Fully Convolution Neural Network (V-FCNN), able to segment the entire volume in a one-shot, and consequently integrate the implicit spatial redundancy present in high-resolution images. A loss function based on the mixture of both Mean Square Error (MSE) and Dice Loss (DL) is used, in an attempt to combine the ability to capture the bulk shape as well as the reduction of local errors products by over-segmentation. Results demonstrate a reasonable performance in the middle region of the atria along with the impact of the challenges of capturing the variability of the pulmonary veins or the identification of the valve plane that separates the atria to the ventricle. A final dice of $92.5\%$ in $54$ patients ($4752$ atria test slices in total) is shown.



### Deep Shape Analysis on Abdominal Organs for Diabetes Prediction
- **Arxiv ID**: http://arxiv.org/abs/1808.01946v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.01946v1)
- **Published**: 2018-08-06 14:52:50+00:00
- **Updated**: 2018-08-06 14:52:50+00:00
- **Authors**: Benjamin Gutierrez-Becker, Sergios Gatidis, Daniel Gutmann, Annette Peters, Christopher Schlett Fabian Bamberg, Christian Wachinger
- **Comment**: Accepted for publication at the ShapeMI MICCAI Workshop 2018
- **Journal**: None
- **Summary**: Morphological analysis of organs based on images is a key task in medical imaging computing. Several approaches have been proposed for the quantitative assessment of morphological changes, and they have been widely used for the analysis of the effects of aging, disease and other factors in organ morphology. In this work, we propose a deep neural network for predicting diabetes on abdominal shapes. The network directly operates on raw point clouds without requiring mesh processing or shape alignment. Instead of relying on hand-crafted shape descriptors, an optimal representation is learned in the end-to-end training stage of the network. For comparison, we extend the state-of-the-art shape descriptor BrainPrint to the AbdomenPrint. Our results demonstrate that the network learns shape representations that better separates healthy and diabetic individuals than traditional representations.



### Adversarial Vision Challenge
- **Arxiv ID**: http://arxiv.org/abs/1808.01976v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1808.01976v2)
- **Published**: 2018-08-06 16:13:43+00:00
- **Updated**: 2018-12-06 18:21:49+00:00
- **Authors**: Wieland Brendel, Jonas Rauber, Alexey Kurakin, Nicolas Papernot, Behar Veliqi, Marcel Salathé, Sharada P. Mohanty, Matthias Bethge
- **Comment**: https://www.crowdai.org/challenges/adversarial-vision-challenge
- **Journal**: None
- **Summary**: The NIPS 2018 Adversarial Vision Challenge is a competition to facilitate measurable progress towards robust machine vision models and more generally applicable adversarial attacks. This document is an updated version of our competition proposal that was accepted in the competition track of 32nd Conference on Neural Information Processing Systems (NIPS 2018).



### Hashing with Binary Matrix Pursuit
- **Arxiv ID**: http://arxiv.org/abs/1808.01990v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1808.01990v1)
- **Published**: 2018-08-06 16:51:36+00:00
- **Updated**: 2018-08-06 16:51:36+00:00
- **Authors**: Fatih Cakir, Kun He, Stan Sclaroff
- **Comment**: 23 pages, 4 figures. In Proceedings of European Conference on
  Computer Vision (ECCV), 2018
- **Journal**: None
- **Summary**: We propose theoretical and empirical improvements for two-stage hashing methods. We first provide a theoretical analysis on the quality of the binary codes and show that, under mild assumptions, a residual learning scheme can construct binary codes that fit any neighborhood structure with arbitrary accuracy. Secondly, we show that with high-capacity hash functions such as CNNs, binary code inference can be greatly simplified for many standard neighborhood definitions, yielding smaller optimization problems and more robust codes. Incorporating our findings, we propose a novel two-stage hashing method that significantly outperforms previous hashing studies on widely used image retrieval benchmarks.



### Simultaneous Edge Alignment and Learning
- **Arxiv ID**: http://arxiv.org/abs/1808.01992v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1808.01992v3)
- **Published**: 2018-08-06 16:58:42+00:00
- **Updated**: 2018-10-26 05:36:51+00:00
- **Authors**: Zhiding Yu, Weiyang Liu, Yang Zou, Chen Feng, Srikumar Ramalingam, B. V. K. Vijaya Kumar, Jan Kautz
- **Comment**: Accepted to ECCV 2018
- **Journal**: None
- **Summary**: Edge detection is among the most fundamental vision problems for its role in perceptual grouping and its wide applications. Recent advances in representation learning have led to considerable improvements in this area. Many state of the art edge detection models are learned with fully convolutional networks (FCNs). However, FCN-based edge learning tends to be vulnerable to misaligned labels due to the delicate structure of edges. While such problem was considered in evaluation benchmarks, similar issue has not been explicitly addressed in general edge learning. In this paper, we show that label misalignment can cause considerably degraded edge learning quality, and address this issue by proposing a simultaneous edge alignment and learning framework. To this end, we formulate a probabilistic model where edge alignment is treated as latent variable optimization, and is learned end-to-end during network training. Experiments show several applications of this work, including improved edge detection with state of the art performance, and automatic refinement of noisy annotations.



### Multi-Estimator Full Left Ventricle Quantification through Ensemble Learning
- **Arxiv ID**: http://arxiv.org/abs/1808.02056v1
- **DOI**: 10.1007/978-3-030-12029-0_49
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.02056v1)
- **Published**: 2018-08-06 18:25:46+00:00
- **Updated**: 2018-08-06 18:25:46+00:00
- **Authors**: Jiasha Liu, Xiang Li, Hui Ren, Quanzheng Li
- **Comment**: Jiasha Liu, Xiang Li and Hui Ren contribute equally to this work
- **Journal**: None
- **Summary**: Cardiovascular disease accounts for 1 in every 4 deaths in United States. Accurate estimation of structural and functional cardiac parameters is crucial for both diagnosis and disease management. In this work, we develop an ensemble learning framework for more accurate and robust left ventricle (LV) quantification. The framework combines two 1st-level modules: direct estimation module and a segmentation module. The direct estimation module utilizes Convolutional Neural Network (CNN) to achieve end-to-end quantification. The CNN is trained by taking 2D cardiac images as input and cardiac parameters as output. The segmentation module utilizes a U-Net architecture for obtaining pixel-wise prediction of the epicardium and endocardium of LV from the background. The binary U-Net output is then analyzed by a separate CNN for estimating the cardiac parameters. We then employ linear regression between the 1st-level predictor and ground truth to learn a 2nd-level predictor that ensembles the results from 1st-level modules for the final estimation. Preliminary results by testing the proposed framework on the LVQuan18 dataset show superior performance of the ensemble learning model over the two base modules.



### Deep Generative Modeling for Scene Synthesis via Hybrid Representations
- **Arxiv ID**: http://arxiv.org/abs/1808.02084v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.02084v1)
- **Published**: 2018-08-06 19:42:24+00:00
- **Updated**: 2018-08-06 19:42:24+00:00
- **Authors**: Zaiwei Zhang, Zhenpei Yang, Chongyang Ma, Linjie Luo, Alexander Huth, Etienne Vouga, Qixing Huang
- **Comment**: None
- **Journal**: None
- **Summary**: We present a deep generative scene modeling technique for indoor environments. Our goal is to train a generative model using a feed-forward neural network that maps a prior distribution (e.g., a normal distribution) to the distribution of primary objects in indoor scenes. We introduce a 3D object arrangement representation that models the locations and orientations of objects, based on their size and shape attributes. Moreover, our scene representation is applicable for 3D objects with different multiplicities (repetition counts), selected from a database. We show a principled way to train this model by combining discriminator losses for both a 3D object arrangement representation and a 2D image-based representation. We demonstrate the effectiveness of our scene representation and the deep learning method on benchmark datasets. We also show the applications of this generative model in scene interpolation and scene completion.



### Inner Space Preserving Generative Pose Machine
- **Arxiv ID**: http://arxiv.org/abs/1808.02104v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.02104v1)
- **Published**: 2018-08-06 20:45:50+00:00
- **Updated**: 2018-08-06 20:45:50+00:00
- **Authors**: Shuangjun Liu, Sarah Ostadabbas
- **Comment**: http://www.northeastern.edu/ostadabbas/2018/07/23/inner-space-preserving-generative-pose-machine/
- **Journal**: European Conference on Computer Vision (ECCV2018)
- **Summary**: Image-based generative methods, such as generative adversarial networks (GANs) have already been able to generate realistic images with much context control, specially when they are conditioned. However, most successful frameworks share a common procedure which performs an image-to-image translation with pose of figures in the image untouched. When the objective is reposing a figure in an image while preserving the rest of the image, the state-of-the-art mainly assumes a single rigid body with simple background and limited pose shift, which can hardly be extended to the images under normal settings. In this paper, we introduce an image "inner space" preserving model that assigns an interpretable low-dimensional pose descriptor (LDPD) to an articulated figure in the image. Figure reposing is then generated by passing the LDPD and the original image through multi-stage augmented hourglass networks in a conditional GAN structure, called inner space preserving generative pose machine (ISP-GPM). We evaluated ISP-GPM on reposing human figures, which are highly articulated with versatile variations. Test of a state-of-the-art pose estimator on our reposed dataset gave an accuracy over 80% on PCK0.5 metric. The results also elucidated that our ISP-GPM is able to preserve the background with high accuracy while reasonably recovering the area blocked by the figure to be reposed.



### Non-Learning based Deep Parallel MRI Reconstruction (NLDpMRI)
- **Arxiv ID**: http://arxiv.org/abs/1808.02122v3
- **DOI**: 10.1117/12.2511653
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.02122v3)
- **Published**: 2018-08-06 21:26:06+00:00
- **Updated**: 2019-03-18 23:22:59+00:00
- **Authors**: Ali Pour Yazdanpanah, Onur Afacan, Simon K. Warfield
- **Comment**: None
- **Journal**: None
- **Summary**: Fast data acquisition in Magnetic Resonance Imaging (MRI) is vastly in demand and scan time directly depends on the number of acquired k-space samples. Recently, the deep learning-based MRI reconstruction techniques were suggested to accelerate MR image acquisition. The most common issues in any deep learning-based MRI reconstruction approaches are generalizability and transferability. For different MRI scanner configurations using these approaches, the network must be trained from scratch every time with new training dataset, acquired under new configurations, to be able to provide good reconstruction performance. Here, we propose a new generalized parallel imaging method based on deep neural networks called NLDpMRI to reduce any structured aliasing ambiguities related to the different k-space undersampling patterns for accelerated data acquisition. Two loss functions including non-regularized and regularized are proposed for parallel MRI reconstruction using deep network optimization and we reconstruct MR images by optimizing the proposed loss functions over the network parameters. Unlike any deep learning-based MRI reconstruction approaches, our method doesn't include any training step that the network learns from a large number of training samples and it only needs the single undersampled multi-coil k-space data for reconstruction. Also, the proposed method can handle k-space data with different undersampling patterns, and the different number of coils. Experimental results show that the proposed method outperforms the current state-of-the-art GRAPPA method and the deep learning-based variational network method.



### Attentive Semantic Alignment with Offset-Aware Correlation Kernels
- **Arxiv ID**: http://arxiv.org/abs/1808.02128v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.02128v2)
- **Published**: 2018-08-06 21:42:57+00:00
- **Updated**: 2018-10-26 07:55:18+00:00
- **Authors**: Paul Hongsuck Seo, Jongmin Lee, Deunsol Jung, Bohyung Han, Minsu Cho
- **Comment**: ECCV 2018 accepted paper
- **Journal**: None
- **Summary**: Semantic correspondence is the problem of establishing correspondences across images depicting different instances of the same object or scene class. One of recent approaches to this problem is to estimate parameters of a global transformation model that densely aligns one image to the other. Since an entire correlation map between all feature pairs across images is typically used to predict such a global transformation, noisy features from different backgrounds, clutter, and occlusion distract the predictor from correct estimation of the alignment. This is a challenging issue, in particular, in the problem of semantic correspondence where a large degree of image variations is often involved. In this paper, we introduce an attentive semantic alignment method that focuses on reliable correlations, filtering out distractors. For effective attention, we also propose an offset-aware correlation kernel that learns to capture translation-invariant local transformations in computing correlation values over spatial locations. Experiments demonstrate the effectiveness of the attentive model and offset-aware kernel, and the proposed model combining both techniques achieves the state-of-the-art performance.



### CPlaNet: Enhancing Image Geolocalization by Combinatorial Partitioning of Maps
- **Arxiv ID**: http://arxiv.org/abs/1808.02130v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.02130v1)
- **Published**: 2018-08-06 21:47:11+00:00
- **Updated**: 2018-08-06 21:47:11+00:00
- **Authors**: Paul Hongsuck Seo, Tobias Weyand, Jack Sim, Bohyung Han
- **Comment**: ECCV 2018 accepted paper
- **Journal**: None
- **Summary**: Image geolocalization is the task of identifying the location depicted in a photo based only on its visual information. This task is inherently challenging since many photos have only few, possibly ambiguous cues to their geolocation. Recent work has cast this task as a classification problem by partitioning the earth into a set of discrete cells that correspond to geographic regions. The granularity of this partitioning presents a critical trade-off; using fewer but larger cells results in lower location accuracy while using more but smaller cells reduces the number of training examples per class and increases model size, making the model prone to overfitting. To tackle this issue, we propose a simple but effective algorithm, combinatorial partitioning, which generates a large number of fine-grained output classes by intersecting multiple coarse-grained partitionings of the earth. Each classifier votes for the fine-grained classes that overlap with their respective coarse-grained ones. This technique allows us to predict locations at a fine scale while maintaining sufficient training examples per class. Our algorithm achieves the state-of-the-art performance in location recognition on multiple benchmark datasets.



### Weakly Supervised Bilinear Attention Network for Fine-Grained Visual Classification
- **Arxiv ID**: http://arxiv.org/abs/1808.02152v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.02152v2)
- **Published**: 2018-08-06 23:22:56+00:00
- **Updated**: 2019-02-28 09:59:30+00:00
- **Authors**: Tao Hu, Jizheng Xu, Cong Huang, Honggang Qi, Qingming Huang, Yan Lu
- **Comment**: None
- **Journal**: None
- **Summary**: For fine-grained visual classification, objects usually share similar geometric structure but present variant local appearance and different pose. Therefore, localizing and extracting discriminative local features play a crucial role in accurate category prediction. Existing works either pay attention to limited object parts or train isolated networks for locating and classification. In this paper, we propose Weakly Supervised Bilinear Attention Network (WS-BAN) to solve these issues. It jointly generates a set of attention maps (region-of-interest maps) to indicate the locations of object's parts and extracts sequential part features by Bilinear Attention Pooling (BAP). Besides, we propose attention regularization and attention dropout to weakly supervise the generating process of attention maps. WS-BAN can be trained end-to-end and achieves the state-of-the-art performance on multiple fine-grained classification datasets, including CUB-200-2011, Stanford Car and FGVC-Aircraft, which demonstrated its effectiveness.



### EOE: Expected Overlap Estimation over Unstructured Point Cloud Data
- **Arxiv ID**: http://arxiv.org/abs/1808.02155v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.02155v1)
- **Published**: 2018-08-06 23:39:27+00:00
- **Updated**: 2018-08-06 23:39:27+00:00
- **Authors**: Ben Eckart, Kihwan Kim, Jan Kautz
- **Comment**: The paper will be presented in 3DV 2018
- **Journal**: None
- **Summary**: We present an iterative overlap estimation technique to augment existing point cloud registration algorithms that can achieve high performance in difficult real-world situations where large pose displacement and non-overlapping geometry would otherwise cause traditional methods to fail. Our approach estimates overlapping regions through an iterative Expectation Maximization procedure that encodes the sensor field-of-view into the registration process. The proposed technique, Expected Overlap Estimation (EOE), is derived from the observation that differences in field-of-view violate the iid assumption implicitly held by all maximum likelihood based registration techniques. We demonstrate how our approach can augment many popular registration methods with minimal computational overhead. Through experimentation on both synthetic and real-world datasets, we find that adding an explicit overlap estimation step can aid robust outlier handling and increase the accuracy of both ICP-based and GMM-based registration methods, especially in large unstructured domains and where the amount of overlap between point clouds is very small.



