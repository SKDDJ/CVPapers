# Arxiv Papers in cs.CV on 2018-08-21
### Controlling Over-generalization and its Effect on Adversarial Examples Generation and Detection
- **Arxiv ID**: http://arxiv.org/abs/1808.08282v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1808.08282v2)
- **Published**: 2018-08-21 02:05:57+00:00
- **Updated**: 2018-10-03 22:43:58+00:00
- **Authors**: Mahdieh Abbasi, Arezoo Rajabi, Azadeh Sadat Mozafari, Rakesh B. Bobba, Christian Gagne
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) significantly improve the state-of-the-art for many applications, especially in computer vision. However, CNNs still suffer from a tendency to confidently classify out-distribution samples from unknown classes into pre-defined known classes. Further, they are also vulnerable to adversarial examples. We are relating these two issues through the tendency of CNNs to over-generalize for areas of the input space not covered well by the training set. We show that a CNN augmented with an extra output class can act as a simple yet effective end-to-end model for controlling over-generalization. As an appropriate training set for the extra class, we introduce two resources that are computationally efficient to obtain: a representative natural out-distribution set and interpolated in-distribution samples. To help select a representative natural out-distribution set among available ones, we propose a simple measurement to assess an out-distribution set's fitness. We also demonstrate that training such an augmented CNN with representative out-distribution natural datasets and some interpolated samples allows it to better handle a wide range of unseen out-distribution samples and black-box adversarial examples without training it on any adversaries. Finally, we show that generation of white-box adversarial attacks using our proposed augmented CNN can become harder, as the attack algorithms have to get around the rejection regions when generating actual adversaries.



### Constrained-size Tensorflow Models for YouTube-8M Video Understanding Challenge
- **Arxiv ID**: http://arxiv.org/abs/1808.06739v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.06739v3)
- **Published**: 2018-08-21 02:22:44+00:00
- **Updated**: 2018-11-09 15:09:56+00:00
- **Authors**: Tianqi Liu, Bo Liu
- **Comment**: Accepted paper at 2018 ECCV Youtube8M workshop:
  https://research.google.com/youtube8m/workshop2018/
- **Journal**: None
- **Summary**: This paper presents our 7th place solution to the second YouTube-8M video understanding competition which challenges participates to build a constrained-size model to classify millions of YouTube videos into thousands of classes. Our final model consists of four single models aggregated into one tensorflow graph. For each single model, we use the same network architecture as in the winning solution of the first YouTube-8M video understanding competition, namely Gated NetVLAD. We train the single models separately in tensorflow's default float32 precision, then replace weights with float16 precision and ensemble them in the evaluation and inference stages., achieving 48.5% compression rate without loss of precision. Our best model achieved 88.324% GAP on private leaderboard. The code is publicly available at https://github.com/boliu61/youtube-8m



### Abnormal Event Detection and Location for Dense Crowds using Repulsive Forces and Sparse Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1808.06749v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.06749v1)
- **Published**: 2018-08-21 03:24:00+00:00
- **Updated**: 2018-08-21 03:24:00+00:00
- **Authors**: Pei Lv, Shunhua Liu, Mingliang Xu, Bing Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a method based on repulsive forces and sparse reconstruction for the detection and location of abnormal events in crowded scenes. In order to avoid the challenging problem of accurately tracking each specific individual in a dense or complex scene, we divide each frame of the surveillance video into a fixed number of grids and select a single representative point in each grid as the individual to track. The repulsive force model, which can accurately reflect interactive behaviors of crowds, is used to calculate the interactive forces between grid particles in crowded scenes and to construct a force flow matrix using these discrete forces from a fixed number of continuous frames. The force flow matrix, which contains spatial and temporal information, is adopted to train a group of visual dictionaries by sparse coding. To further improve the detection efficiency and avoid concept drift, we propose a fully unsupervised global and local dynamic updating algorithm, based on sparse reconstruction and a group of word pools. For anomaly location, since our method is based on a fixed grid, we can judge whether anomalies occur in a region intuitively according to the reconstruction error of the corresponding visual words. We experimentally verify the proposed method using the UMN dataset, the UCSD dataset and the Web dataset separately. The results indicate that our method can not only detect abnormal events accurately, but can also pinpoint the location of anomalies.



### Estimating Metric Poses of Dynamic Objects Using Monocular Visual-Inertial Fusion
- **Arxiv ID**: http://arxiv.org/abs/1808.06753v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1808.06753v1)
- **Published**: 2018-08-21 04:04:20+00:00
- **Updated**: 2018-08-21 04:04:20+00:00
- **Authors**: Kejie Qiu, Tong Qin, Hongwen Xie, Shaojie Shen
- **Comment**: IROS 2018
- **Journal**: None
- **Summary**: A monocular 3D object tracking system generally has only up-to-scale pose estimation results without any prior knowledge of the tracked object. In this paper, we propose a novel idea to recover the metric scale of an arbitrary dynamic object by optimizing the trajectory of the objects in the world frame, without motion assumptions. By introducing an additional constraint in the time domain, our monocular visual-inertial tracking system can obtain continuous six degree of freedom (6-DoF) pose estimation without scale ambiguity. Our method requires neither fixed multi-camera nor depth sensor settings for scale observability, instead, the IMU inside the monocular sensing suite provides scale information for both camera itself and the tracked object. We build the proposed system on top of our monocular visual-inertial system (VINS) to obtain accurate state estimation of the monocular camera in the world frame. The whole system consists of a 2D object tracker, an object region-based visual bundle adjustment (BA), VINS and a correlation analysis-based metric scale estimator. Experimental comparisons with ground truth demonstrate the tracking accuracy of our 3D tracking performance while a mobile augmented reality (AR) demo shows the feasibility of potential applications.



### Automatic skin lesion segmentation on dermoscopic images by the means of superpixel merging
- **Arxiv ID**: http://arxiv.org/abs/1808.06759v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.06759v1)
- **Published**: 2018-08-21 04:18:37+00:00
- **Updated**: 2018-08-21 04:18:37+00:00
- **Authors**: Diego Patiño, Jonathan Avendaño, John Willian Branch
- **Comment**: MICCAI 2018
- **Journal**: None
- **Summary**: We present a superpixel-based strategy for segmenting skin lesion on dermoscopic images. The segmentation is carried out by over-segmenting the original image using the SLIC algorithm, and then merge the resulting superpixels into two regions: healthy skin and lesion. The mean RGB color of each superpixel was used as merging criterion. The presented method is capable of dealing with segmentation problems commonly found in dermoscopic images such as hair removal, oil bubbles, changes in illumination, and reflections images without any additional steps. The method was evaluated on the PH2 and ISIC 2017 dataset with results comparable to the state-of-art.



### Text-to-image Synthesis via Symmetrical Distillation Networks
- **Arxiv ID**: http://arxiv.org/abs/1808.06801v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.06801v1)
- **Published**: 2018-08-21 08:54:23+00:00
- **Updated**: 2018-08-21 08:54:23+00:00
- **Authors**: Mingkuan Yuan, Yuxin Peng
- **Comment**: 9 pages, accepted as an oral paper of ACM Multimedia 2018
- **Journal**: None
- **Summary**: Text-to-image synthesis aims to automatically generate images according to text descriptions given by users, which is a highly challenging task. The main issues of text-to-image synthesis lie in two gaps: the heterogeneous and homogeneous gaps. The heterogeneous gap is between the high-level concepts of text descriptions and the pixel-level contents of images, while the homogeneous gap exists between synthetic image distributions and real image distributions. For addressing these problems, we exploit the excellent capability of generic discriminative models (e.g. VGG19), which can guide the training process of a new generative model on multiple levels to bridge the two gaps. The high-level representations can teach the generative model to extract necessary visual information from text descriptions, which can bridge the heterogeneous gap. The mid-level and low-level representations can lead it to learn structures and details of images respectively, which relieves the homogeneous gap. Therefore, we propose Symmetrical Distillation Networks (SDN) composed of a source discriminative model as "teacher" and a target generative model as "student". The target generative model has a symmetrical structure with the source discriminative model, in order to transfer hierarchical knowledge accessibly. Moreover, we decompose the training process into two stages with different distillation paradigms for promoting the performance of the target generative model. Experiments on two widely-used datasets are conducted to verify the effectiveness of our proposed SDN.



### Fully-Convolutional Point Networks for Large-Scale Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1808.06840v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.06840v1)
- **Published**: 2018-08-21 10:58:57+00:00
- **Updated**: 2018-08-21 10:58:57+00:00
- **Authors**: Dario Rethage, Johanna Wald, Jürgen Sturm, Nassir Navab, Federico Tombari
- **Comment**: ECCV 2018
- **Journal**: None
- **Summary**: This work proposes a general-purpose, fully-convolutional network architecture for efficiently processing large-scale 3D data. One striking characteristic of our approach is its ability to process unorganized 3D representations such as point clouds as input, then transforming them internally to ordered structures to be processed via 3D convolutions. In contrast to conventional approaches that maintain either unorganized or organized representations, from input to output, our approach has the advantage of operating on memory efficient input data representations while at the same time exploiting the natural structure of convolutional operations to avoid the redundant computing and storing of spatial information in the network. The network eliminates the need to pre- or post process the raw sensor data. This, together with the fully-convolutional nature of the network, makes it an end-to-end method able to process point clouds of huge spaces or even entire rooms with up to 200k points at once. Another advantage is that our network can produce either an ordered output or map predictions directly onto the input cloud, thus making it suitable as a general-purpose point cloud descriptor applicable to many 3D tasks. We demonstrate our network's ability to effectively learn both low-level features as well as complex compositional relationships by evaluating it on benchmark datasets for semantic voxel segmentation, semantic part segmentation and 3D scene captioning.



### Deep Learned Full-3D Object Completion from Single View
- **Arxiv ID**: http://arxiv.org/abs/1808.06843v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.06843v1)
- **Published**: 2018-08-21 11:07:08+00:00
- **Updated**: 2018-08-21 11:07:08+00:00
- **Authors**: Dario Rethage, Federico Tombari, Felix Achilles, Nassir Navab
- **Comment**: CVPR 2016 - Scene Understanding Workshop (SUNw)
- **Journal**: None
- **Summary**: 3D geometry is a very informative cue when interacting with and navigating an environment. This writing proposes a new approach to 3D reconstruction and scene understanding, which implicitly learns 3D geometry from depth maps pairing a deep convolutional neural network architecture with an auto-encoder. A data set of synthetic depth views and voxelized 3D representations is built based on ModelNet, a large-scale collection of CAD models, to train networks. The proposed method offers a significant advantage over current, explicit reconstruction methods in that it learns key geometric features offline and makes use of those to predict the most probable reconstruction of an unseen object. The relatively small network, consisting of roughly 4 million weights, achieves a 92.9% reconstruction accuracy at a 30x30x30 resolution through the use of a pre-trained decompression layer. This is roughly 1/4 the weights of the current leading network. The fast execution time of the model makes it suitable for real-time applications.



### Deep Video-Based Performance Cloning
- **Arxiv ID**: http://arxiv.org/abs/1808.06847v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1808.06847v1)
- **Published**: 2018-08-21 11:20:49+00:00
- **Updated**: 2018-08-21 11:20:49+00:00
- **Authors**: Kfir Aberman, Mingyi Shi, Jing Liao, Dani Lischinski, Baoquan Chen, Daniel Cohen-Or
- **Comment**: None
- **Journal**: None
- **Summary**: We present a new video-based performance cloning technique. After training a deep generative network using a reference video capturing the appearance and dynamics of a target actor, we are able to generate videos where this actor reenacts other performances. All of the training data and the driving performances are provided as ordinary video segments, without motion capture or depth information. Our generative model is realized as a deep neural network with two branches, both of which train the same space-time conditional generator, using shared weights. One branch, responsible for learning to generate the appearance of the target actor in various poses, uses \emph{paired} training data, self-generated from the reference video. The second branch uses unpaired data to improve generation of temporally coherent video renditions of unseen pose sequences. We demonstrate a variety of promising results, where our method is able to generate temporally coherent videos, for challenging scenarios where the reference and driving videos consist of very different dance performances. Supplementary video: https://youtu.be/JpwsEeqNhhA.



### Soft Filter Pruning for Accelerating Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1808.06866v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.06866v1)
- **Published**: 2018-08-21 12:22:38+00:00
- **Updated**: 2018-08-21 12:22:38+00:00
- **Authors**: Yang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, Yi Yang
- **Comment**: Accepted to IJCAI 2018
- **Journal**: None
- **Summary**: This paper proposed a Soft Filter Pruning (SFP) method to accelerate the inference procedure of deep Convolutional Neural Networks (CNNs). Specifically, the proposed SFP enables the pruned filters to be updated when training the model after pruning. SFP has two advantages over previous works: (1) Larger model capacity. Updating previously pruned filters provides our approach with larger optimization space than fixing the filters to zero. Therefore, the network trained by our method has a larger model capacity to learn from the training data. (2) Less dependence on the pre-trained model. Large capacity enables SFP to train from scratch and prune the model simultaneously. In contrast, previous filter pruning methods should be conducted on the basis of the pre-trained model to guarantee their performance. Empirically, SFP from scratch outperforms the previous filter pruning methods. Moreover, our approach has been demonstrated effective for many advanced CNN architectures. Notably, on ILSCRC-2012, SFP reduces more than 42% FLOPs on ResNet-101 with even 0.2% top-5 accuracy improvement, which has advanced the state-of-the-art. Code is publicly available on GitHub: https://github.com/he-y/soft-filter-pruning



### Self-supervised learning of a facial attribute embedding from video
- **Arxiv ID**: http://arxiv.org/abs/1808.06882v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.06882v1)
- **Published**: 2018-08-21 13:01:46+00:00
- **Updated**: 2018-08-21 13:01:46+00:00
- **Authors**: Olivia Wiles, A. Sophia Koepke, Andrew Zisserman
- **Comment**: To appear in BMVC 2018. Supplementary material can be found at
  http://www.robots.ox.ac.uk/~vgg/research/unsup_learn_watch_faces/fabnet.html
- **Journal**: None
- **Summary**: We propose a self-supervised framework for learning facial attributes by simply watching videos of a human face speaking, laughing, and moving over time. To perform this task, we introduce a network, Facial Attributes-Net (FAb-Net), that is trained to embed multiple frames from the same video face-track into a common low-dimensional space. With this approach, we make three contributions: first, we show that the network can leverage information from multiple source frames by predicting confidence/attention masks for each frame; second, we demonstrate that using a curriculum learning regime improves the learned embedding; finally, we demonstrate that the network learns a meaningful face embedding that encodes information about head pose, facial landmarks and facial expression, i.e. facial attributes, without having been supervised with any labelled data. We are comparable or superior to state-of-the-art self-supervised methods on these tasks and approach the performance of supervised methods.



### Multimodal Interaction-aware Motion Prediction for Autonomous Street Crossing
- **Arxiv ID**: http://arxiv.org/abs/1808.06887v5
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1808.06887v5)
- **Published**: 2018-08-21 13:18:27+00:00
- **Updated**: 2020-08-03 16:41:58+00:00
- **Authors**: Noha Radwan, Wolfram Burgard, Abhinav Valada
- **Comment**: The International Journal of Robotics Research (2020)
- **Journal**: The International Journal of Robotics Research (IJRR), vol. 39,
  no. 13, pp. 1567-1598, 2020
- **Summary**: For mobile robots navigating on sidewalks, it is essential to be able to safely cross street intersections. Most existing approaches rely on the recognition of the traffic light signal to make an informed crossing decision. Although these approaches have been crucial enablers for urban navigation, the capabilities of robots employing such approaches are still limited to navigating only on streets containing signalized intersections. In this paper, we address this challenge and propose a multimodal convolutional neural network framework to predict the safety of a street intersection for crossing. Our architecture consists of two subnetworks; an interaction-aware trajectory estimation stream IA-TCNN, that predicts the future states of all observed traffic participants in the scene, and a traffic light recognition stream AtteNet. Our IA-TCNN utilizes dilated causal convolutions to model the behavior of the observable dynamic agents in the scene without explicitly assigning priorities to the interactions among them. While AtteNet utilizes Squeeze-Excitation blocks to learn a content-aware mechanism for selecting the relevant features from the data, thereby improving the noise robustness. Learned representations from the traffic light recognition stream are fused with the estimated trajectories from the motion prediction stream to learn the crossing decision. Furthermore, we extend our previously introduced Freiburg Street Crossing dataset with sequences captured at different types of intersections, demonstrating complex interactions among the traffic participants. Extensive experimental evaluations on public benchmark datasets and our proposed dataset demonstrate that our network achieves state-of-the-art performance for each of the subtasks, as well as for the crossing safety prediction.



### Isometric Transformation Invariant Graph-based Deep Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1808.07366v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.07366v1)
- **Published**: 2018-08-21 14:32:52+00:00
- **Updated**: 2018-08-21 14:32:52+00:00
- **Authors**: Renata Khasanova, Pascal Frossard
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1703.00356
- **Journal**: None
- **Summary**: Learning transformation invariant representations of visual data is an important problem in computer vision. Deep convolutional networks have demonstrated remarkable results for image and video classification tasks. However, they have achieved only limited success in the classification of images that undergo geometric transformations. In this work we present a novel Transformation Invariant Graph-based Network (TIGraNet), which learns graph-based features that are inherently invariant to isometric transformations such as rotation and translation of input images. In particular, images are represented as signals on graphs, which permits to replace classical convolution and pooling layers in deep networks with graph spectral convolution and dynamic graph pooling layers that together contribute to invariance to isometric transformation. Our experiments show high performance on rotated and translated images from the test set compared to classical architectures that are very sensitive to transformations in the data. The inherent invariance properties of our framework provide key advantages, such as increased resiliency to data variability and sustained performance with limited training sets. Our code is available online.



### Real Time Elbow Angle Estimation Using Single RGB Camera
- **Arxiv ID**: http://arxiv.org/abs/1808.07017v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1808.07017v1)
- **Published**: 2018-08-21 17:00:42+00:00
- **Updated**: 2018-08-21 17:00:42+00:00
- **Authors**: Muhammad Yahya, Jawad Ali Shah, Arif Warsi, Kushsairy Kadir, Sheroz Khan, M Izani
- **Comment**: 10 pages, 7 figures, 1 table, jounal
- **Journal**: None
- **Summary**: The use of motion capture has increased from last decade in a varied spectrum of applications like film special effects, controlling games and robots, rehabilitation system, animations etc. The current human motion capture techniques use markers, structured environment, and high resolution cameras in a dedicated environment. Because of rapid movement, elbow angle estimation is observed as the most difficult problem in human motion capture system. In this paper, we take elbow angle estimation as our research subject and propose a novel, markerless and cost-effective solution that uses RGB camera for estimating elbow angle in real time using part affinity field. We have recruited five (5) participants to perform cup to mouth movement and at the same time measured the angle by both RGB camera and Microsoft Kinect. The experimental results illustrate that markerless and cost-effective RGB camera has a median RMS errors of 3.06{\deg} and 0.95{\deg} in sagittal and coronal plane respectively as compared to Microsoft Kinect.



### Three Efficient, Low-Complexity Algorithms for Automatic Color Trapping
- **Arxiv ID**: http://arxiv.org/abs/1808.07096v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.07096v1)
- **Published**: 2018-08-21 19:27:39+00:00
- **Updated**: 2018-08-21 19:27:39+00:00
- **Authors**: Haiyin Wang, Mireille Boutin, Jeffrey Trask, Jan Allebach
- **Comment**: None
- **Journal**: None
- **Summary**: Color separations (most often cyan, magenta, yellow, and black) are commonly used in printing to reproduce multi-color images. For mechanical reasons, these color separations are generally not perfectly aligned with respect to each other when they are rendered by their respective imaging stations. This phenomenon, called color plane misregistration, causes gap and halo artifacts in the printed image. Color trapping is an image processing technique that aims to reduce these artifacts by modifying the susceptible edge boundaries to create small, unnoticeable overlaps between the color planes. We propose three low-complexity algorithms for automatic color trapping which hide the effects of small color plane mis-registrations. Our algorithms are designed for software or embedded firmware implementation. The trapping method they follow is based on a hardware-friendly technique proposed by J. Trask (JTHBCT03) which is too computationally expensive for software or firmware implementation. The first two algorithms are based on the use of look-up tables (LUTs). The first LUT-based algorithm corrects all registration errors of one pixel in extent and reduces several cases of misregistration errors of two pixels in extent using only 727 Kbytes of storage space. This algorithm is particularly attractive for implementation in the embedded firmware of low-cost formatter-based printers. The second LUT-based algorithm corrects all types of misregistration errors of up to two pixels in extent using 3.7 Mbytes of storage space. The third algorithm is a hybrid one that combines LUTs and feature extraction to minimize the storage requirements (724 Kbytes) while still correcting all misregistration errors of up to two pixels in extent. This algorithm is suitable for both embedded firmware implementation on low-cost formatter-based printers and software implementation on host-based printers.



### Improving Super-Resolution Methods via Incremental Residual Learning
- **Arxiv ID**: http://arxiv.org/abs/1808.07110v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.07110v2)
- **Published**: 2018-08-21 20:01:07+00:00
- **Updated**: 2019-05-16 07:21:59+00:00
- **Authors**: Muneeb Aadil, Rafia Rahim, Sibt ul Hussain
- **Comment**: Accepted at IEEE ICIP'19
- **Journal**: None
- **Summary**: Recently, Convolutional Neural Networks (CNNs) have shown promising performance in super-resolution (SR). However, these methods operate primarily on Low Resolution (LR) inputs for memory efficiency but this limits, as we demonstrate, their ability to (i) model high frequency information; and (ii) smoothly translate from LR to High Resolution (HR) space. To this end, we propose a novel Incremental Residual Learning (IRL) framework to address these mentioned issues. In IRL, first we select a typical SR pre-trained network as a master branch. Next we sequentially train and add residual branches to the main branch, where each residual branch is learned to model accumulated residuals of all previous branches. We plug state of the art methods in IRL framework and demonstrate consistent performance improvement on public benchmark datasets to set a new state of the art for SR at only approximately 20% increase in training time.



### Faster PET Reconstruction with Non-Smooth Priors by Randomization and Preconditioning
- **Arxiv ID**: http://arxiv.org/abs/1808.07150v5
- **DOI**: 10.1088/1361-6560/ab3d07
- **Categories**: **physics.med-ph**, cs.CV, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/1808.07150v5)
- **Published**: 2018-08-21 22:19:37+00:00
- **Updated**: 2019-08-02 21:20:52+00:00
- **Authors**: Matthias J. Ehrhardt, Pawel Markiewicz, Carola-Bibiane Schönlieb
- **Comment**: None
- **Journal**: None
- **Summary**: Uncompressed clinical data from modern positron emission tomography (PET) scanners are very large, exceeding 350 million data points (projection bins). The last decades have seen tremendous advancements in mathematical imaging tools many of which lead to non-smooth (i.e. non-differentiable) optimization problems which are much harder to solve than smooth optimization problems. Most of these tools have not been translated to clinical PET data, as the state-of-the-art algorithms for non-smooth problems do not scale well to large data. In this work, inspired by big data machine learning applications, we use advanced randomized optimization algorithms to solve the PET reconstruction problem for a very large class of non-smooth priors which includes for example total variation, total generalized variation, directional total variation and various different physical constraints. The proposed algorithm randomly uses subsets of the data and only updates the variables associated with these. While this idea often leads to divergent algorithms, we show that the proposed algorithm does indeed converge for any proper subset selection. Numerically, we show on real PET data (FDG and florbetapir) from a Siemens Biograph mMR that about ten projections and backprojections are sufficient to solve the MAP optimisation problem related to many popular non-smooth priors; thus showing that the proposed algorithm is fast enough to bring these models into routine clinical practice.



