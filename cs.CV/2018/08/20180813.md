# Arxiv Papers in cs.CV on 2018-08-13
### 3D Geometry-Aware Semantic Labeling of Outdoor Street Scenes
- **Arxiv ID**: http://arxiv.org/abs/1808.04028v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.04028v1)
- **Published**: 2018-08-13 00:13:19+00:00
- **Updated**: 2018-08-13 00:13:19+00:00
- **Authors**: Yiran Zhong, Yuchao Dai, Hongdong Li
- **Comment**: Accepted by ICPR 2018
- **Journal**: None
- **Summary**: This paper is concerned with the problem of how to better exploit 3D geometric information for dense semantic image labeling. Existing methods often treat the available 3D geometry information (e.g., 3D depth-map) simply as an additional image channel besides the R-G-B color channels, and apply the same technique for RGB image labeling. In this paper, we demonstrate that directly performing 3D convolution in the framework of a residual connected 3D voxel top-down modulation network can lead to superior results. Specifically, we propose a 3D semantic labeling method to label outdoor street scenes whenever a dense depth map is available. Experiments on the "Synthia" and "Cityscape" datasets show our method outperforms the state-of-the-art methods, suggesting such a simple 3D representation is effective in incorporating 3D geometric information.



### Time Perception Machine: Temporal Point Processes for the When, Where and What of Activity Prediction
- **Arxiv ID**: http://arxiv.org/abs/1808.04063v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.04063v2)
- **Published**: 2018-08-13 04:48:07+00:00
- **Updated**: 2018-08-14 06:36:27+00:00
- **Authors**: Yatao Zhong, Bicheng Xu, Guang-Tong Zhou, Luke Bornn, Greg Mori
- **Comment**: None
- **Journal**: None
- **Summary**: Numerous powerful point process models have been developed to understand temporal patterns in sequential data from fields such as health-care, electronic commerce, social networks, and natural disaster forecasting. In this paper, we develop novel models for learning the temporal distribution of human activities in streaming data (e.g., videos and person trajectories). We propose an integrated framework of neural networks and temporal point processes for predicting when the next activity will happen. Because point processes are limited to taking event frames as input, we propose a simple yet effective mechanism to extract features at frames of interest while also preserving the rich information in the remaining frames. We evaluate our model on two challenging datasets. The results show that our model outperforms traditional statistical point process approaches significantly, demonstrating its effectiveness in capturing the underlying temporal dynamics as well as the correlation within sequential activities. Furthermore, we also extend our model to a joint estimation framework for predicting the timing, spatial location, and category of the activity simultaneously, to answer the when, where, and what of activity prediction.



### A Transfer Learning based Feature-Weak-Relevant Method for Image Clustering
- **Arxiv ID**: http://arxiv.org/abs/1808.04068v2
- **DOI**: None
- **Categories**: **cs.CV**, 62H30
- **Links**: [PDF](http://arxiv.org/pdf/1808.04068v2)
- **Published**: 2018-08-13 05:39:28+00:00
- **Updated**: 2018-08-14 06:35:15+00:00
- **Authors**: Bo Dong, Xinnian Wang
- **Comment**: 17 pages,3 figures
- **Journal**: None
- **Summary**: Image clustering is to group a set of images into disjoint clusters in a way that images in the same cluster are more similar to each other than to those in other clusters, which is an unsupervised or semi-supervised learning process. It is a crucial and challenging task in machine learning and computer vision. The performances of existing image clustering methods have close relations with features used for clustering, even if unsupervised coding based methods have improved the performances a lot. To reduce the effect of clustering features, we propose a feature-weak-relevant method for image clustering. The proposed method converts an unsupervised clustering process into an alternative iterative process of unsupervised learning and transfer learning. The clustering process firstly starts up from handcrafted features based image clustering to estimate an initial label for every image, and secondly use a proposed sampling strategy to choose images with reliable labels to feed a transfer-learning model to learn representative features that can be used for next round of unsupervised learning. In this manner, image clustering is iteratively optimized. What's more, the handcrafted features are used to boot up the clustering process, and just have a little effect on the final performance; therefore, the proposed method is feature-weak-relevant. Experimental results on six kinds of public available datasets show that the proposed method outperforms state of the art methods and depends less on the employed features at the same time.



### Towards Audio to Scene Image Synthesis using Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/1808.04108v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1808.04108v1)
- **Published**: 2018-08-13 08:46:42+00:00
- **Updated**: 2018-08-13 08:46:42+00:00
- **Authors**: Chia-Hung Wan, Shun-Po Chuang, Hung-Yi Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Humans can imagine a scene from a sound. We want machines to do so by using conditional generative adversarial networks (GANs). By applying the techniques including spectral norm, projection discriminator and auxiliary classifier, compared with naive conditional GAN, the model can generate images with better quality in terms of both subjective and objective evaluations. Almost three-fourth of people agree that our model have the ability to generate images related to sounds. By inputting different volumes of the same sound, our model output different scales of changes based on the volumes, showing that our model truly knows the relationship between sounds and images to some extent.



### DenseRAN for Offline Handwritten Chinese Character Recognition
- **Arxiv ID**: http://arxiv.org/abs/1808.04134v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.04134v1)
- **Published**: 2018-08-13 10:16:08+00:00
- **Updated**: 2018-08-13 10:16:08+00:00
- **Authors**: Wenchao Wang, Jianshu Zhang, Jun Du, Zi-Rui Wang, Yixing Zhu
- **Comment**: Accepted by ICFHR2018
- **Journal**: None
- **Summary**: Recently, great success has been achieved in offline handwritten Chinese character recognition by using deep learning methods. Chinese characters are mainly logographic and consist of basic radicals, however, previous research mostly treated each Chinese character as a whole without explicitly considering its internal two-dimensional structure and radicals. In this study, we propose a novel radical analysis network with densely connected architecture (DenseRAN) to analyze Chinese character radicals and its two-dimensional structures simultaneously. DenseRAN first encodes input image to high-level visual features by employing DenseNet as an encoder. Then a decoder based on recurrent neural networks is employed, aiming at generating captions of Chinese characters by detecting radicals and two-dimensional structures through attention mechanism. The manner of treating a Chinese character as a composition of two-dimensional structures and radicals can reduce the size of vocabulary and enable DenseRAN to possess the capability of recognizing unseen Chinese character classes, only if the corresponding radicals have been seen in training set. Evaluated on ICDAR-2013 competition database, the proposed approach significantly outperforms whole-character modeling approach with a relative character error rate (CER) reduction of 18.54%. Meanwhile, for the case of recognizing 3277 unseen Chinese characters in CASIA-HWDB1.2 database, DenseRAN can achieve a character accuracy of about 41% while the traditional whole-character method has no capability to handle them.



### Parsimonious HMMs for Offline Handwritten Chinese Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/1808.04138v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.04138v1)
- **Published**: 2018-08-13 10:26:56+00:00
- **Updated**: 2018-08-13 10:26:56+00:00
- **Authors**: Wenchao Wang, Jun Du, Zi-Rui Wang
- **Comment**: Accepted by ICFHR2018
- **Journal**: None
- **Summary**: Recently, hidden Markov models (HMMs) have achieved promising results for offline handwritten Chinese text recognition. However, due to the large vocabulary of Chinese characters with each modeled by a uniform and fixed number of hidden states, a high demand of memory and computation is required. In this study, to address this issue, we present parsimonious HMMs via the state tying which can fully utilize the similarities among different Chinese characters. Two-step algorithm with the data-driven question-set is adopted to generate the tied-state pool using the likelihood measure. The proposed parsimonious HMMs with both Gaussian mixture models (GMMs) and deep neural networks (DNNs) as the emission distributions not only lead to a compact model but also improve the recognition accuracy via the data sharing for the tied states and the confusion decreasing among state classes. Tested on ICDAR-2013 competition database, in the best configured case, the new parsimonious DNN-HMM can yield a relative character error rate (CER) reduction of 6.2%, 25% reduction of model size and 60% reduction of decoding time over the conventional DNN-HMM. In the compact setting case of average 1-state HMM, our parsimonious DNN-HMM significantly outperforms the conventional DNN-HMM with a relative CER reduction of 35.5%.



### Incremental Non-Rigid Structure-from-Motion with Unknown Focal Length
- **Arxiv ID**: http://arxiv.org/abs/1808.04181v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.04181v1)
- **Published**: 2018-08-13 12:53:10+00:00
- **Updated**: 2018-08-13 12:53:10+00:00
- **Authors**: Thomas Probst, Danda Pani Paudel, Ajad Chhatkuli, Luc Van Gool
- **Comment**: ECCV 2018
- **Journal**: None
- **Summary**: The perspective camera and the isometric surface prior have recently gathered increased attention for Non-Rigid Structure-from-Motion (NRSfM). Despite the recent progress, several challenges remain, particularly the computational complexity and the unknown camera focal length. In this paper we present a method for incremental Non-Rigid Structure-from-Motion (NRSfM) with the perspective camera model and the isometric surface prior with unknown focal length. In the template-based case, we provide a method to estimate four parameters of the camera intrinsics. For the template-less scenario of NRSfM, we propose a method to upgrade reconstructions obtained for one focal length to another based on local rigidity and the so-called Maximum Depth Heuristics (MDH). On its basis we propose a method to simultaneously recover the focal length and the non-rigid shapes. We further solve the problem of incorporating a large number of points and adding more views in MDH-based NRSfM and efficiently solve them with Second-Order Cone Programming (SOCP). This does not require any shape initialization and produces results orders of times faster than many methods. We provide evaluations on standard sequences with ground-truth and qualitative reconstructions on challenging YouTube videos. These evaluations show that our method performs better in both speed and accuracy than the state of the art.



### Automatic Plaque Detection in IVOCT Pullbacks Using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1808.04187v2
- **DOI**: 10.1109/TMI.2018.2865659
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.04187v2)
- **Published**: 2018-08-13 13:01:14+00:00
- **Updated**: 2018-08-24 09:59:41+00:00
- **Authors**: Nils Gessert, Matthias Lutz, Markus Heyder, Sarah Latus, David M. Leistner, Youssef S. Abdelwahed, Alexander Schlaefer
- **Comment**: Accepted for Publication in IEEE Transactions on Medical Imaging
- **Journal**: None
- **Summary**: Coronary heart disease is a common cause of death despite being preventable. To treat the underlying plaque deposits in the arterial walls, intravascular optical coherence tomography can be used by experts to detect and characterize the lesions. In clinical routine, hundreds of images are acquired for each patient which requires automatic plaque detection for fast and accurate decision support. So far, automatic approaches rely on classic machine learning methods and deep learning solutions have rarely been studied. Given the success of deep learning methods with other imaging modalities, a thorough understanding of deep learning-based plaque detection for future clinical decision support systems is required. We address this issue with a new dataset consisting of in-vivo patient images labeled by three trained experts. Using this dataset, we employ state-of-the-art deep learning models that directly learn plaque classification from the images. For improved performance, we study different transfer learning approaches. Furthermore, we investigate the use of cartesian and polar image representations and employ data augmentation techniques tailored to each representation. We fuse both representations in a multi-path architecture for more effective feature exploitation. Last, we address the challenge of plaque differentiation in addition to detection. Overall, we find that our combined model performs best with an accuracy of 91.7%, a sensitivity of 90.9% and a specificity of 92.4%. Our results indicate that building a deep learning-based clinical decision support system for plaque detection is feasible.



### Fast Video Shot Transition Localization with Deep Structured Models
- **Arxiv ID**: http://arxiv.org/abs/1808.04234v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.04234v1)
- **Published**: 2018-08-13 14:04:03+00:00
- **Updated**: 2018-08-13 14:04:03+00:00
- **Authors**: Shitao Tang, Litong Feng, Zhangkui Kuang, Yimin Chen, Wei Zhang
- **Comment**: 16 pages, 3 figures, submitted to ACCV
- **Journal**: None
- **Summary**: Detection of video shot transition is a crucial pre-processing step in video analysis. Previous studies are restricted on detecting sudden content changes between frames through similarity measurement and multi-scale operations are widely utilized to deal with transitions of various lengths. However, localization of gradual transitions are still under-explored due to the high visual similarity between adjacent frames. Cut shot transitions are abrupt semantic breaks while gradual shot transitions contain low-level spatial-temporal patterns caused by video effects in addition to the gradual semantic breaks, e.g. dissolve. In order to address the problem, we propose a structured network which is able to detect these two shot transitions using targeted models separately. Considering speed performance trade-offs, we design a smart framework. With one TITAN GPU, the proposed method can achieve a 30\(\times\) real-time speed. Experiments on public TRECVID07 and RAI databases show that our method outperforms the state-of-the-art methods. In order to train a high-performance shot transition detector, we contribute a new database ClipShots, which contains 128636 cut transitions and 38120 gradual transitions from 4039 online videos. ClipShots intentionally collect short videos for more hard cases caused by hand-held camera vibrations, large object motions, and occlusion.



### BACH: Grand Challenge on Breast Cancer Histology Images
- **Arxiv ID**: http://arxiv.org/abs/1808.04277v2
- **DOI**: 10.1016/j.media.2019.05.010
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.04277v2)
- **Published**: 2018-08-13 14:48:46+00:00
- **Updated**: 2019-06-17 15:41:37+00:00
- **Authors**: Guilherme Aresta, Teresa Araújo, Scotty Kwok, Sai Saketh Chennamsetty, Mohammed Safwan, Varghese Alex, Bahram Marami, Marcel Prastawa, Monica Chan, Michael Donovan, Gerardo Fernandez, Jack Zeineh, Matthias Kohl, Christoph Walz, Florian Ludwig, Stefan Braunewell, Maximilian Baust, Quoc Dang Vu, Minh Nguyen Nhat To, Eal Kim, Jin Tae Kwak, Sameh Galal, Veronica Sanchez-Freire, Nadia Brancati, Maria Frucci, Daniel Riccio, Yaqi Wang, Lingling Sun, Kaiqiang Ma, Jiannan Fang, Ismael Kone, Lahsen Boulmane, Aurélio Campilho, Catarina Eloy, António Polónia, Paulo Aguiar
- **Comment**: Accepted for publication at Medical Image Analysis (Elsevier).
  Publication licensed under the Creative Commons CC-BY-NC-ND 4.0 license
  http://creativecommons.org/licenses/by-nc-nd/4.0/
- **Journal**: Medical Image Analysis, 2019
- **Summary**: Breast cancer is the most common invasive cancer in women, affecting more than 10% of women worldwide. Microscopic analysis of a biopsy remains one of the most important methods to diagnose the type of breast cancer. This requires specialized analysis by pathologists, in a task that i) is highly time- and cost-consuming and ii) often leads to nonconsensual results. The relevance and potential of automatic classification algorithms using hematoxylin-eosin stained histopathological images has already been demonstrated, but the reported results are still sub-optimal for clinical use. With the goal of advancing the state-of-the-art in automatic classification, the Grand Challenge on BreAst Cancer Histology images (BACH) was organized in conjunction with the 15th International Conference on Image Analysis and Recognition (ICIAR 2018). A large annotated dataset, composed of both microscopy and whole-slide images, was specifically compiled and made publicly available for the BACH challenge. Following a positive response from the scientific community, a total of 64 submissions, out of 677 registrations, effectively entered the competition. From the submitted algorithms it was possible to push forward the state-of-the-art in terms of accuracy (87%) in automatic classification of breast cancer with histopathological images. Convolutional neuronal networks were the most successful methodology in the BACH challenge. Detailed analysis of the collective results allowed the identification of remaining challenges in the field and recommendations for future developments. The BACH dataset remains publically available as to promote further improvements to the field of automatic classification in digital pathology.



### Unsupervised Hard Example Mining from Videos for Improved Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1808.04285v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.04285v1)
- **Published**: 2018-08-13 15:21:22+00:00
- **Updated**: 2018-08-13 15:21:22+00:00
- **Authors**: SouYoung Jin, Aruni RoyChowdhury, Huaizu Jiang, Ashish Singh, Aditya Prasad, Deep Chakraborty, Erik Learned-Miller
- **Comment**: 14 pages, 7 figures, accepted at ECCV 2018
- **Journal**: None
- **Summary**: Important gains have recently been obtained in object detection by using training objectives that focus on {\em hard negative} examples, i.e., negative examples that are currently rated as positive or ambiguous by the detector. These examples can strongly influence parameters when the network is trained to correct them. Unfortunately, they are often sparse in the training data, and are expensive to obtain. In this work, we show how large numbers of hard negatives can be obtained {\em automatically} by analyzing the output of a trained detector on video sequences. In particular, detections that are {\em isolated in time}, i.e., that have no associated preceding or following detections, are likely to be hard negatives. We describe simple procedures for mining large numbers of such hard negatives (and also hard {\em positives}) from unlabeled video data. Our experiments show that retraining detectors on these automatically obtained examples often significantly improves performance. We present experiments on multiple architectures and multiple data sets, including face detection, pedestrian detection and other object categories.



### Visual Sensor Network Reconfiguration with Deep Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/1808.04287v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML, 68T05
- **Links**: [PDF](http://arxiv.org/pdf/1808.04287v1)
- **Published**: 2018-08-13 15:24:01+00:00
- **Updated**: 2018-08-13 15:24:01+00:00
- **Authors**: Paul Jasek, Bernard Abayowa
- **Comment**: 6 pages, 5 figures
- **Journal**: None
- **Summary**: We present an approach for reconfiguration of dynamic visual sensor networks with deep reinforcement learning (RL). Our RL agent uses a modified asynchronous advantage actor-critic framework and the recently proposed Relational Network module at the foundation of its network architecture. To address the issue of sample inefficiency in current approaches to model-free reinforcement learning, we train our system in an abstract simulation environment that represents inputs from a dynamic scene. Our system is validated using inputs from a real-world scenario and preexisting object detection and tracking algorithms.



### Rank-1 Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1808.04303v1
- **DOI**: None
- **Categories**: **cs.CV**, 68 Computer Science
- **Links**: [PDF](http://arxiv.org/pdf/1808.04303v1)
- **Published**: 2018-08-13 15:55:41+00:00
- **Updated**: 2018-08-13 15:55:41+00:00
- **Authors**: Hyein Kim, Jungho Yoon, Byeongseon Jeong, Sukho Lee
- **Comment**: The paper is in 2 Column style 8 pages. It will be submitted to
  CVPR2019
- **Journal**: None
- **Summary**: In this paper, we propose a convolutional neural network(CNN) with 3-D rank-1 filters which are composed by the outer product of 1-D filters. After being trained, the 3-D rank-1 filters can be decomposed into 1-D filters in the test time for fast inference. The reason that we train 3-D rank-1 filters in the training stage instead of consecutive 1-D filters is that a better gradient flow can be obtained with this setting, which makes the training possible even in the case where the network with consecutive 1-D filters cannot be trained. The 3-D rank-1 filters are updated by both the gradient flow and the outer product of the 1-D filters in every epoch, where the gradient flow tries to obtain a solution which minimizes the loss function, while the outer product operation tries to make the parameters of the filter to live on a rank-1 sub-space. Furthermore, we show that the convolution with the rank-1 filters results in low rank outputs, constraining the final output of the CNN also to live on a low dimensional subspace.



### Improving Shape Deformation in Unsupervised Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/1808.04325v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.04325v2)
- **Published**: 2018-08-13 16:33:46+00:00
- **Updated**: 2019-01-17 21:24:25+00:00
- **Authors**: Aaron Gokaslan, Vivek Ramanujan, Daniel Ritchie, Kwang In Kim, James Tompkin
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised image-to-image translation techniques are able to map local texture between two domains, but they are typically unsuccessful when the domains require larger shape change. Inspired by semantic segmentation, we introduce a discriminator with dilated convolutions that is able to use information from across the entire image to train a more context-aware generator. This is coupled with a multi-scale perceptual loss that is better able to represent error in the underlying shape of objects. We demonstrate that this design is more capable of representing shape deformation in a challenging toy dataset, plus in complex mappings with significant dataset variation between humans, dolls, and anime faces, and between cats and dogs.



### Ghost imaging with the human eye
- **Arxiv ID**: http://arxiv.org/abs/1808.05137v1
- **DOI**: 10.1364/OE.27.009258
- **Categories**: **q-bio.NC**, cs.CV, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/1808.05137v1)
- **Published**: 2018-08-13 17:04:53+00:00
- **Updated**: 2018-08-13 17:04:53+00:00
- **Authors**: Alessandro Boccolini, Alessandro Fedrizzi, Daniele Faccio
- **Comment**: None
- **Journal**: None
- **Summary**: Computational ghost imaging relies on the decomposition of an image into patterns that are summed together with weights that measure the overlap of each pattern with the scene being imaged. These tasks rely on a computer. Here we demonstrate that the computational integration can be performed directly with the human eye. We use this human ghost imaging technique to evaluate the temporal response of the eye and establish the image persistence time to be around 20 ms followed by a further 20 ms exponential decay. These persistence times are in agreement with previous studies but can now potentially be extended to include a more precise characterisation of visual stimuli and provide a new experimental tool for the study of visual perception.



### Vision-Based Preharvest Yield Mapping for Apple Orchards
- **Arxiv ID**: http://arxiv.org/abs/1808.04336v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.04336v1)
- **Published**: 2018-08-13 17:23:16+00:00
- **Updated**: 2018-08-13 17:23:16+00:00
- **Authors**: Pravakar Roy, Abhijeet Kislay, Patrick A. Plonski, James Luby, Volkan Isler
- **Comment**: None
- **Journal**: None
- **Summary**: We present an end-to-end computer vision system for mapping yield in an apple orchard using images captured from a single camera. Our proposed system is platform independent and does not require any specific lighting conditions. Our main technical contributions are 1)~a semi-supervised clustering algorithm that utilizes colors to identify apples and 2)~an unsupervised clustering method that utilizes spatial properties to estimate fruit counts from apple clusters having arbitrarily complex geometry. Additionally, we utilize camera motion to merge the counts across multiple views. We verified the performance of our algorithms by conducting multiple field trials on three tree rows consisting of $252$ trees at the University of Minnesota Horticultural Research Center. Results indicate that the detection method achieves $F_1$-measure $.95 -.97$ for multiple color varieties and lighting conditions. The counting method achieves an accuracy of $89\%-98\%$. Additionally, we report merged fruit counts from both sides of the tree rows. Our yield estimation method achieves an overall accuracy of $91.98\% - 94.81\%$ across different datasets.



### Large-Scale Study of Curiosity-Driven Learning
- **Arxiv ID**: http://arxiv.org/abs/1808.04355v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1808.04355v1)
- **Published**: 2018-08-13 17:58:01+00:00
- **Updated**: 2018-08-13 17:58:01+00:00
- **Authors**: Yuri Burda, Harri Edwards, Deepak Pathak, Amos Storkey, Trevor Darrell, Alexei A. Efros
- **Comment**: First three authors contributed equally and ordered alphabetically.
  Website at https://pathak22.github.io/large-scale-curiosity/
- **Journal**: None
- **Summary**: Reinforcement learning algorithms rely on carefully engineering environment rewards that are extrinsic to the agent. However, annotating each environment with hand-designed, dense rewards is not scalable, motivating the need for developing reward functions that are intrinsic to the agent. Curiosity is a type of intrinsic reward function which uses prediction error as reward signal. In this paper: (a) We perform the first large-scale study of purely curiosity-driven learning, i.e. without any extrinsic rewards, across 54 standard benchmark environments, including the Atari game suite. Our results show surprisingly good performance, and a high degree of alignment between the intrinsic curiosity objective and the hand-designed extrinsic rewards of many game environments. (b) We investigate the effect of using different feature spaces for computing prediction error and show that random features are sufficient for many popular RL game benchmarks, but learned features appear to generalize better (e.g. to novel game levels in Super Mario Bros.). (c) We demonstrate limitations of the prediction-based rewards in stochastic setups. Game-play videos and code are at https://pathak22.github.io/large-scale-curiosity/



### Cross-Cultural and Cultural-Specific Production and Perception of Facial Expressions of Emotion in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1808.04399v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.04399v1)
- **Published**: 2018-08-13 18:43:59+00:00
- **Updated**: 2018-08-13 18:43:59+00:00
- **Authors**: Ramprakash Srinivasan, Aleix M. Martinez
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic recognition of emotion from facial expressions is an intense area of research, with a potentially long list of important application. Yet, the study of emotion requires knowing which facial expressions are used within and across cultures in the wild, not in controlled lab conditions; but such studies do not exist. Which and how many cross-cultural and cultural-specific facial expressions do people commonly use? And, what affect variables does each expression communicate to observers? If we are to design technology that understands the emotion of users, we need answers to these two fundamental questions. In this paper, we present the first large-scale study of the production and visual perception of facial expressions of emotion in the wild. We find that of the 16,384 possible facial configurations that people can theoretically produce, only 35 are successfully used to transmit emotive information across cultures, and only 8 within a smaller number of cultures. Crucially, we find that visual analysis of cross-cultural expressions yields consistent perception of emotion categories and valence, but not arousal. In contrast, visual analysis of cultural-specific expressions yields consistent perception of valence and arousal, but not of emotion categories. Additionally, we find that the number of expressions used to communicate each emotion is also different, e.g., 17 expressions transmit happiness, but only 1 is used to convey disgust.



### RedSync : Reducing Synchronization Traffic for Distributed Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1808.04357v3
- **DOI**: 10.1016/j.jpdc.2019.05.016
- **Categories**: **cs.DC**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1808.04357v3)
- **Published**: 2018-08-13 19:02:47+00:00
- **Updated**: 2019-07-22 09:48:26+00:00
- **Authors**: Jiarui Fang, Haohuan Fu, Guangwen Yang, Cho-Jui Hsieh
- **Comment**: 10 pages. Journal of Parallel and Distributed Computing, 2019
- **Journal**: None
- **Summary**: Data parallelism has become a dominant method to scale Deep Neural Network (DNN) training across multiple nodes. Since synchronizing a large number of gradients of the local model can be a bottleneck for large-scale distributed training, compressing communication data has gained widespread attention recently. Among several recent proposed compression algorithms, Residual Gradient Compression (RGC) is one of the most successful approaches---it can significantly compress the transmitting message size (0.1\% of the gradient size) of each node and still achieve correct accuracy and the same convergence speed. However, the literature on compressing deep networks focuses almost exclusively on achieving good theoretical compression rate, while the efficiency of RGC in real distributed implementation has been less investigated. In this paper, we develop an RGC-based system that is able to reduce the end-to-end training time on real-world multi-GPU systems. Our proposed design called RedSync, which introduces a set of optimizations to reduce communication bandwidth requirement while introducing limited overhead. We evaluate the performance of RedSync on two different multiple GPU platforms, including 128 GPUs of a supercomputer and an 8-GPU server. Our test cases include image classification tasks on Cifar10 and ImageNet, and language modeling tasks on Penn Treebank and Wiki2 datasets. For DNNs featured with high communication to computation ratio, which have long been considered with poor scalability, RedSync brings significant performance improvements.



### Multimodal Deep Neural Networks using Both Engineered and Learned Representations for Biodegradability Prediction
- **Arxiv ID**: http://arxiv.org/abs/1808.04456v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1808.04456v2)
- **Published**: 2018-08-13 20:36:08+00:00
- **Updated**: 2018-09-13 18:27:08+00:00
- **Authors**: Garrett B. Goh, Khushmeen Sakloth, Charles Siegel, Abhinav Vishnu, Jim Pfaendtner
- **Comment**: Submitted to a peer-reviewed ML conference
- **Journal**: None
- **Summary**: Deep learning algorithms excel at extracting patterns from raw data, and with large datasets, they have been very successful in computer vision and natural language applications. However, in other domains, large datasets on which to learn representations from may not exist. In this work, we develop a novel multimodal CNN-MLP neural network architecture that utilizes both domain-specific feature engineering as well as learned representations from raw data. We illustrate the effectiveness of such network designs in the chemical sciences, for predicting biodegradability. DeepBioD, a multimodal CNN-MLP network is more accurate than either standalone network designs, and achieves an error classification rate of 0.125 that is 27% lower than the current state-of-the-art. Thus, our work indicates that combining traditional feature engineering with representation learning can be effective, particularly in situations where labeled data is limited.



### Deep Randomized Ensembles for Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/1808.04469v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.04469v2)
- **Published**: 2018-08-13 21:11:56+00:00
- **Updated**: 2018-09-04 16:58:59+00:00
- **Authors**: Hong Xuan, Richard Souvenir, Robert Pless
- **Comment**: ECCV 2018
- **Journal**: None
- **Summary**: Learning embedding functions, which map semantically related inputs to nearby locations in a feature space supports a variety of classification and information retrieval tasks. In this work, we propose a novel, generalizable and fast method to define a family of embedding functions that can be used as an ensemble to give improved results. Each embedding function is learned by randomly bagging the training labels into small subsets. We show experimentally that these embedding ensembles create effective embedding functions. The ensemble output defines a metric space that improves state of the art performance for image retrieval on CUB-200-2011, Cars-196, In-Shop Clothes Retrieval and VehicleID.



### Fast Convergence for Object Detection by Learning how to Combine Error Functions
- **Arxiv ID**: http://arxiv.org/abs/1808.04480v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.04480v1)
- **Published**: 2018-08-13 22:20:34+00:00
- **Updated**: 2018-08-13 22:20:34+00:00
- **Authors**: Benjamin Schnieders, Karl Tuyls
- **Comment**: Accepted for publication at IROS 2018
- **Journal**: None
- **Summary**: In this paper, we introduce an innovative method to improve the convergence speed and accuracy of object detection neural networks. Our approach, CONVERGE-FAST-AUXNET, is based on employing multiple, dependent loss metrics and weighting them optimally using an on-line trained auxiliary network. Experiments are performed in the well-known RoboCup@Work challenge environment. A fully convolutional segmentation network is trained on detecting objects' pickup points. We empirically obtain an approximate measure for the rate of success of a robotic pickup operation based on the accuracy of the object detection network. Our experiments show that adding an optimally weighted Euclidean distance loss to a network trained on the commonly used Intersection over Union (IoU) metric reduces the convergence time by 42.48%. The estimated pickup rate is improved by 39.90%. Compared to state-of-the-art task weighting methods, the improvement is 24.5% in convergence, and 15.8% on the estimated pickup rate.



### CLAIRE: A distributed-memory solver for constrained large deformation diffeomorphic image registration
- **Arxiv ID**: http://arxiv.org/abs/1808.04487v2
- **DOI**: 10.1137/18M1207818
- **Categories**: **math.OC**, cs.CV, cs.DC, cs.NA, math.NA, 68U10, 49J20, 35Q93, 65K10, 65F08, 76D55
- **Links**: [PDF](http://arxiv.org/pdf/1808.04487v2)
- **Published**: 2018-08-13 22:59:25+00:00
- **Updated**: 2019-12-09 21:50:57+00:00
- **Authors**: Andreas Mang, Amir Gholami, Christos Davatzikos, George Biros
- **Comment**: 37 pages;
- **Journal**: SIAM Journal on Scientific Computing, 41(5):C548-C584, 2019
- **Summary**: With this work, we release CLAIRE, a distributed-memory implementation of an effective solver for constrained large deformation diffeomorphic image registration problems in three dimensions. We consider an optimal control formulation. We invert for a stationary velocity field that parameterizes the deformation map. Our solver is based on a globalized, preconditioned, inexact reduced space Gauss--Newton--Krylov scheme. We exploit state-of-the-art techniques in scientific computing to develop an effective solver that scales to thousands of distributed memory nodes on high-end clusters. We present the formulation, discuss algorithmic features, describe the software package, and introduce an improved preconditioner for the reduced space Hessian to speed up the convergence of our solver. We test registration performance on synthetic and real data. We demonstrate registration accuracy on several neuroimaging datasets. We compare the performance of our scheme against different flavors of the Demons algorithm for diffeomorphic image registration. We study convergence of our preconditioner and our overall algorithm. We report scalability results on state-of-the-art supercomputing platforms. We demonstrate that we can solve registration problems for clinically relevant data sizes in two to four minutes on a standard compute node with 20 cores, attaining excellent data fidelity. With the present work we achieve a speedup of (on average) 5$\times$ with a peak performance of up to 17$\times$ compared to our former work.



