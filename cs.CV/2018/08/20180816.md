# Arxiv Papers in cs.CV on 2018-08-16
### 3D Face From X: Learning Face Shape from Diverse Sources
- **Arxiv ID**: http://arxiv.org/abs/1808.05323v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1808.05323v3)
- **Published**: 2018-08-16 01:59:15+00:00
- **Updated**: 2021-03-09 09:20:30+00:00
- **Authors**: Yudong Guo, Lin Cai, Juyong Zhang
- **Comment**: Accepted by IEEE Transactions on Image Processing, 2021
- **Journal**: None
- **Summary**: We present a novel method to jointly learn a 3D face parametric model and 3D face reconstruction from diverse sources. Previous methods usually learn 3D face modeling from one kind of source, such as scanned data or in-the-wild images. Although 3D scanned data contain accurate geometric information of face shapes, the capture system is expensive and such datasets usually contain a small number of subjects. On the other hand, in-the-wild face images are easily obtained and there are a large number of facial images. However, facial images do not contain explicit geometric information. In this paper, we propose a method to learn a unified face model from diverse sources. Besides scanned face data and face images, we also utilize a large number of RGB-D images captured with an iPhone X to bridge the gap between the two sources. Experimental results demonstrate that with training data from more sources, we can learn a more powerful face model.



### On the Convergence of Learning-based Iterative Methods for Nonconvex Inverse Problems
- **Arxiv ID**: http://arxiv.org/abs/1808.05331v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.05331v1)
- **Published**: 2018-08-16 03:02:59+00:00
- **Updated**: 2018-08-16 03:02:59+00:00
- **Authors**: Risheng Liu, Shichao Cheng, Yi He, Xin Fan, Zhouchen Lin, Zhongxuan Luo
- **Comment**: None
- **Journal**: None
- **Summary**: Numerous tasks at the core of statistics, learning and vision areas are specific cases of ill-posed inverse problems. Recently, learning-based (e.g., deep) iterative methods have been empirically shown to be useful for these problems. Nevertheless, integrating learnable structures into iterations is still a laborious process, which can only be guided by intuitions or empirical insights. Moreover, there is a lack of rigorous analysis about the convergence behaviors of these reimplemented iterations, and thus the significance of such methods is a little bit vague. This paper moves beyond these limits and proposes Flexible Iterative Modularization Algorithm (FIMA), a generic and provable paradigm for nonconvex inverse problems. Our theoretical analysis reveals that FIMA allows us to generate globally convergent trajectories for learning-based iterative methods. Meanwhile, the devised scheduling policies on flexible modules should also be beneficial for classical numerical methods in the nonconvex scenario. Extensive experiments on real applications verify the superiority of FIMA.



### Simultaneous Localization And Mapping with depth Prediction using Capsule Networks for UAVs
- **Arxiv ID**: http://arxiv.org/abs/1808.05336v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1808.05336v1)
- **Published**: 2018-08-16 03:39:25+00:00
- **Updated**: 2018-08-16 03:39:25+00:00
- **Authors**: Sunil Prakash, Gaelan Gu
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose an novel implementation of a simultaneous localization and mapping (SLAM) system based on a monocular camera from an unmanned aerial vehicle (UAV) using Depth prediction performed with Capsule Networks (CapsNet), which possess improvements over the drawbacks of the more widely-used Convolutional Neural Networks (CNN). An Extended Kalman Filter will assist in estimating the position of the UAV so that we are able to update the belief for the environment. Results will be evaluated on a benchmark dataset to portray the accuracy of our intended approach.



### Egocentric Gesture Recognition for Head-Mounted AR devices
- **Arxiv ID**: http://arxiv.org/abs/1808.05380v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.05380v1)
- **Published**: 2018-08-16 09:00:56+00:00
- **Updated**: 2018-08-16 09:00:56+00:00
- **Authors**: Tejo Chalasani, Jan Ondrej, Aljosa Smolic
- **Comment**: None
- **Journal**: None
- **Summary**: Natural interaction with virtual objects in AR/VR environments makes for a smooth user experience. Gestures are a natural extension from real world to augmented space to achieve these interactions. Finding discriminating spatio-temporal features relevant to gestures and hands in ego-view is the primary challenge for recognising egocentric gestures. In this work we propose a data driven end-to-end deep learning approach to address the problem of egocentric gesture recognition, which combines an ego-hand encoder network to find ego-hand features, and a recurrent neural network to discern temporally discriminating features. Since deep learning networks are data intensive, we propose a novel data augmentation technique using green screen capture to alleviate the problem of ground truth annotation. In addition we publish a dataset of 10 gestures performed in a natural fashion in front of a green screen for training and the same 10 gestures performed in different natural scenes without green screen for validation. We also present the results of our network's performance in comparison to the state-of-the-art using the AirGest dataset



### Typhoon track prediction using satellite images in a Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/1808.05382v1
- **DOI**: None
- **Categories**: **physics.ao-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1808.05382v1)
- **Published**: 2018-08-16 09:19:57+00:00
- **Updated**: 2018-08-16 09:19:57+00:00
- **Authors**: Mario Rüttgers, Sangseung Lee, Donghyun You
- **Comment**: None
- **Journal**: None
- **Summary**: Tracks of typhoons are predicted using satellite images as input for a Generative Adversarial Network (GAN). The satellite images have time gaps of 6 hours and are marked with a red square at the location of the typhoon center. The GAN uses images from the past to generate an image one time step ahead. The generated image shows the future location of the typhoon center, as well as the future cloud structures. The errors between predicted and real typhoon centers are measured quantitatively in kilometers. 42.4% of all typhoon center predictions have absolute errors of less than 80 km, 32.1% lie within a range of 80 - 120 km and the remaining 25.5% have accuracies above 120 km. The relative error sets the above mentioned absolute error in relation to the distance that has been traveled by a typhoon over the past 6 hours. High relative errors are found in three types of situations, when a typhoon moves on the open sea far away from land, when a typhoon changes its course suddenly and when a typhoon is about to hit the mainland. The cloud structure prediction is evaluated qualitatively. It is shown that the GAN is able to predict trends in cloud motion. In order to improve both, the typhoon center and cloud motion prediction, the present study suggests to add information about the sea surface temperature, surface pressure and velocity fields to the input data.



### A Pipeline for Lenslet Light Field Quality Enhancement
- **Arxiv ID**: http://arxiv.org/abs/1808.05387v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.05387v1)
- **Published**: 2018-08-16 09:30:11+00:00
- **Updated**: 2018-08-16 09:30:11+00:00
- **Authors**: Pierre Matysiak, Mairéad Grogan, Mikaël Le Pendu, Martin Alain, Aljosa Smolic
- **Comment**: IEEE International Conference on Image Processing 2018, 5 pages, 7
  figures
- **Journal**: None
- **Summary**: In recent years, light fields have become a major research topic and their applications span across the entire spectrum of classical image processing. Among the different methods used to capture a light field are the lenslet cameras, such as those developed by Lytro. While these cameras give a lot of freedom to the user, they also create light field views that suffer from a number of artefacts. As a result, it is common to ignore a significant subset of these views when doing high-level light field processing. We propose a pipeline to process light field views, first with an enhanced processing of RAW images to extract subaperture images, then a colour correction process using a recent colour transfer algorithm, and finally a denoising process using a state of the art light field denoising approach. We show that our method improves the light field quality on many levels, by reducing ghosting artefacts and noise, as well as retrieving more accurate and homogeneous colours across the sub-aperture images.



### Landmark Weighting for 3DMM Shape Fitting
- **Arxiv ID**: http://arxiv.org/abs/1808.05399v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.05399v1)
- **Published**: 2018-08-16 09:59:03+00:00
- **Updated**: 2018-08-16 09:59:03+00:00
- **Authors**: Yu Yanga, Xiao-Jun Wu, Josef Kittler
- **Comment**: 7 pages, 7 figures
- **Journal**: None
- **Summary**: Human face is a 3D object with shape and surface texture. 3D Morphable Model (3DMM) is a powerful tool for reconstructing the 3D face from a single 2D face image. In the shape fitting process, 3DMM estimates the correspondence between 2D and 3D landmarks. Most traditional 3DMM fitting methods fail to reconstruct an accurate model because face shape fitting is a difficult non-linear optimization problem. In this paper we show that landmark weighting is instrumental to improve the accuracy of shape reconstruction and propose a novel 3D Morphable Model Fitting method. Different from previous works that treat all landmarks equally, we take into consideration the estimated errors for each pair of 2D and 3D corresponding landmarks. The landmark points are weighted in the optimization cost function based on these errors. Obviously, these landmarks have different semantics because they locate on different facial components. In the context of the solution of fitting is approximated, there are deviations in landmarks matching. However, these landmarks with different semantics have different effects on reconstructing 3D faces. Thus, it is necessary to consider each landmark individually. To our knowledge, we are the first to analyze each feature point for 3D face reconstruction by 3DMM. The weight is adaptive with the estimation residuals of landmarks. Experimental results show that the proposed method significantly reduces the reconstruction error and improves the authenticity of the 3D model expression.



### Context-Aware Visual Policy Network for Sequence-Level Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/1808.05864v3
- **DOI**: 10.1145/3240508.3240632
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.05864v3)
- **Published**: 2018-08-16 11:45:45+00:00
- **Updated**: 2018-08-22 13:32:28+00:00
- **Authors**: Daqing Liu, Zheng-Jun Zha, Hanwang Zhang, Yongdong Zhang, Feng Wu
- **Comment**: 9 pages, 6 figures, ACM MM 2018 oral
- **Journal**: None
- **Summary**: Many vision-language tasks can be reduced to the problem of sequence prediction for natural language output. In particular, recent advances in image captioning use deep reinforcement learning (RL) to alleviate the "exposure bias" during training: ground-truth subsequence is exposed in every step prediction, which introduces bias in test when only predicted subsequence is seen. However, existing RL-based image captioning methods only focus on the language policy while not the visual policy (e.g., visual attention), and thus fail to capture the visual context that are crucial for compositional reasoning such as visual relationships (e.g., "man riding horse") and comparisons (e.g., "smaller cat"). To fill the gap, we propose a Context-Aware Visual Policy network (CAVP) for sequence-level image captioning. At every time step, CAVP explicitly accounts for the previous visual attentions as the context, and then decides whether the context is helpful for the current word generation given the current visual attention. Compared against traditional visual attention that only fixes a single image region at every step, CAVP can attend to complex visual compositions over time. The whole image captioning model --- CAVP and its subsequent language policy network --- can be efficiently optimized end-to-end by using an actor-critic policy gradient method with respect to any caption evaluation metric. We demonstrate the effectiveness of CAVP by state-of-the-art performances on MS-COCO offline split and online server, using various metrics and sensible visualizations of qualitative visual context. The code is available at https://github.com/daqingliu/CAVP



### Auto-Classification of Retinal Diseases in the Limit of Sparse Data Using a Two-Streams Machine Learning Model
- **Arxiv ID**: http://arxiv.org/abs/1808.05754v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.05754v4)
- **Published**: 2018-08-16 12:53:53+00:00
- **Updated**: 2018-11-01 21:42:29+00:00
- **Authors**: C. -H. Huck Yang, Fangyu Liu, Jia-Hong Huang, Meng Tian, Hiromasa Morikawa, I-Hung Lin, Yi-Chieh Liu, Hao-Hsiang Yang, Jesper Tegner
- **Comment**: A extension work of a workshop paper arXiv admin note: substantial
  text overlap with arXiv:1806.06423
- **Journal**: Asian Conference on Computer Vision (ACCV), Artificial
  Intelligence for Retinal Image Analysis Workshop, December 2-6, 2018
- **Summary**: Automatic clinical diagnosis of retinal diseases has emerged as a promising approach to facilitate discovery in areas with limited access to specialists. Based on the fact that fundus structure and vascular disorders are the main characteristics of retinal diseases, we propose a novel visual-assisted diagnosis hybrid model mixing the support vector machine (SVM) and deep neural networks (DNNs). Furthermore, we present a new clinical retina dataset, called EyeNet2, for ophthalmology incorporating 52 retina diseases classes. Using EyeNet2, our model achieves 90.43\% diagnosis accuracy, and the model performance is comparable to the professional ophthalmologists.



### Fast and Accurate, Convolutional Neural Network Based Approach for Object Detection from UAV
- **Arxiv ID**: http://arxiv.org/abs/1808.05756v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.05756v2)
- **Published**: 2018-08-16 13:22:00+00:00
- **Updated**: 2019-01-04 14:07:31+00:00
- **Authors**: Xiaoliang Wang, Peng Cheng, Xinchuan Liu, Benedict Uzochukwu
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1803.01114
- **Journal**: None
- **Summary**: Unmanned Aerial Vehicles (UAVs), have intrigued different people from all walks of life, because of their pervasive computing capabilities. UAV equipped with vision techniques, could be leveraged to establish navigation autonomous control for UAV itself. Also, object detection from UAV could be used to broaden the utilization of drone to provide ubiquitous surveillance and monitoring services towards military operation, urban administration and agriculture management. As the data-driven technologies evolved, machine learning algorithm, especially the deep learning approach has been intensively utilized to solve different traditional computer vision research problems. Modern Convolutional Neural Networks based object detectors could be divided into two major categories: one-stage object detector and two-stage object detector. In this study, we utilize some representative CNN based object detectors to execute the computer vision task over Stanford Drone Dataset (SDD). State-of-the-art performance has been achieved in utilizing focal loss dense detector RetinaNet based approach for object detection from UAV in a fast and accurate manner.



### Metric Learning for Novelty and Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/1808.05492v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.05492v1)
- **Published**: 2018-08-16 13:53:14+00:00
- **Updated**: 2018-08-16 13:53:14+00:00
- **Authors**: Marc Masana, Idoia Ruiz, Joan Serrat, Joost van de Weijer, Antonio M. Lopez
- **Comment**: Accepted at BMVC 2018, 10 pages main article and 4 pages
  supplementary material
- **Journal**: None
- **Summary**: When neural networks process images which do not resemble the distribution seen during training, so called out-of-distribution images, they often make wrong predictions, and do so too confidently. The capability to detect out-of-distribution images is therefore crucial for many real-world applications. We divide out-of-distribution detection between novelty detection ---images of classes which are not in the training set but are related to those---, and anomaly detection ---images with classes which are unrelated to the training set. By related we mean they contain the same type of objects, like digits in MNIST and SVHN. Most existing work has focused on anomaly detection, and has addressed this problem considering networks trained with the cross-entropy loss. Differently from them, we propose to use metric learning which does not have the drawback of the softmax layer (inherent to cross-entropy methods), which forces the network to divide its prediction power over the learned classes. We perform extensive experiments and evaluate both novelty and anomaly detection, even in a relevant application such as traffic sign recognition, obtaining comparable or better results than previous works.



### Occlusion Resistant Object Rotation Regression from Point Cloud Segments
- **Arxiv ID**: http://arxiv.org/abs/1808.05498v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.05498v2)
- **Published**: 2018-08-16 14:03:58+00:00
- **Updated**: 2018-12-02 14:57:52+00:00
- **Authors**: Ge Gao, Mikko Lauri, Jianwei Zhang, Simone Frintrop
- **Comment**: Proceeding of the ECCV18 workshop on Recovering 6D Object Pose
- **Journal**: None
- **Summary**: Rotation estimation of known rigid objects is important for robotic applications such as dexterous manipulation. Most existing methods for rotation estimation use intermediate representations such as templates, global or local feature descriptors, or object coordinates, which require multiple steps in order to infer the object pose. We propose to directly regress a pose vector from raw point cloud segments using a convolutional neural network. Experimental results show that our method can potentially achieve competitive performance compared to a state-of-the-art method, while also showing more robustness against occlusion. Our method does not require any post processing such as refinement with the iterative closest point algorithm.



### Measuring the Temporal Behavior of Real-World Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1808.05499v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.05499v1)
- **Published**: 2018-08-16 14:07:03+00:00
- **Updated**: 2018-08-16 14:07:03+00:00
- **Authors**: Meng Zheng, Srikrishna Karanam, Richard J. Radke
- **Comment**: 14 pages, 14 figures
- **Journal**: None
- **Summary**: Designing real-world person re-identification (re-id) systems requires attention to operational aspects not typically considered in academic research. Typically, the probe image or image sequence is matched to a gallery set with a fixed candidate list. On the other hand, in real-world applications of re-id, we would search for a person of interest in a gallery set that is continuously populated by new candidates over time. A key question of interest for the operator of such a system is: how long is a correct match to a probe likely to remain in a rank-k shortlist of candidates? In this paper, we propose to distill this information into what we call a Rank Persistence Curve (RPC), which unlike a conventional cumulative match characteristic (CMC) curve helps directly compare the temporal performance of different re-id algorithms. To carefully illustrate the concept, we collected a new multi-shot person re-id dataset called RPIfield. The RPIfield dataset is constructed using a network of 12 cameras with 112 explicitly time-stamped actor paths among about 4000 distractors. We then evaluate the temporal performance of different re-id algorithms using the proposed RPCs using single and pairwise camera videos from RPIfield, and discuss considerations for future research.



### Robust training of recurrent neural networks to handle missing data for disease progression modeling
- **Arxiv ID**: http://arxiv.org/abs/1808.05500v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1808.05500v1)
- **Published**: 2018-08-16 14:09:22+00:00
- **Updated**: 2018-08-16 14:09:22+00:00
- **Authors**: Mostafa Mehdipour Ghazi, Mads Nielsen, Akshay Pai, M. Jorge Cardoso, Marc Modat, Sebastien Ourselin, Lauge Sørensen
- **Comment**: 9 pages, 1 figure, MIDL conference
- **Journal**: None
- **Summary**: Disease progression modeling (DPM) using longitudinal data is a challenging task in machine learning for healthcare that can provide clinicians with better tools for diagnosis and monitoring of disease. Existing DPM algorithms neglect temporal dependencies among measurements and make parametric assumptions about biomarker trajectories. In addition, they do not model multiple biomarkers jointly and need to align subjects' trajectories. In this paper, recurrent neural networks (RNNs) are utilized to address these issues. However, in many cases, longitudinal cohorts contain incomplete data, which hinders the application of standard RNNs and requires a pre-processing step such as imputation of the missing values. We, therefore, propose a generalized training rule for the most widely used RNN architecture, long short-term memory (LSTM) networks, that can handle missing values in both target and predictor variables. This algorithm is applied for modeling the progression of Alzheimer's disease (AD) using magnetic resonance imaging (MRI) biomarkers. The results show that the proposed LSTM algorithm achieves a lower mean absolute error for prediction of measurements across all considered MRI biomarkers compared to using standard LSTM networks with data imputation or using a regression-based DPM method. Moreover, applying linear discriminant analysis to the biomarkers' values predicted by the proposed algorithm results in a larger area under the receiver operating characteristic curve (AUC) for clinical diagnosis of AD compared to the same alternatives, and the AUC is comparable to state-of-the-art AUCs from a recent cross-sectional medical image classification challenge. This paper shows that built-in handling of missing values in LSTM network training paves the way for application of RNNs in disease progression modeling.



### An Experimental Evaluation of Covariates Effects on Unconstrained Face Verification
- **Arxiv ID**: http://arxiv.org/abs/1808.05508v1
- **DOI**: 10.1109/TBIOM.2018.2890577
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.05508v1)
- **Published**: 2018-08-16 14:24:37+00:00
- **Updated**: 2018-08-16 14:24:37+00:00
- **Authors**: Boyu Lu, Jun-Cheng Chen, Carlos D. Castillo, Rama Chellappa
- **Comment**: None
- **Journal**: None
- **Summary**: Covariates are factors that have a debilitating influence on face verification performance. In this paper, we comprehensively study two covariate related problems for unconstrained face verification: first, how covariates affect the performance of deep neural networks on the large-scale unconstrained face verification problem; second, how to utilize covariates to improve verification performance. To study the first problem, we implement five state-of-the-art deep convolutional networks (DCNNs) for face verification and evaluate them on three challenging covariates datasets. In total, seven covariates are considered: pose (yaw and roll), age, facial hair, gender, indoor/outdoor, occlusion (nose and mouth visibility, eyes visibility, and forehead visibility), and skin tone. These covariates cover both intrinsic subject-specific characteristics and extrinsic factors of faces. Some of the results confirm and extend the findings of previous studies, others are new findings that were rarely mentioned previously or did not show consistent trends. For the second problem, we demonstrate that with the assistance of gender information, the quality of a pre-curated noisy large-scale face dataset for face recognition can be further improved. After retraining the face recognition model using the curated data, performance improvement is observed at low False Acceptance Rates (FARs) (FAR=$10^{-5}$, $10^{-6}$, $10^{-7}$).



### Network Decoupling: From Regular to Depthwise Separable Convolutions
- **Arxiv ID**: http://arxiv.org/abs/1808.05517v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1808.05517v1)
- **Published**: 2018-08-16 14:39:10+00:00
- **Updated**: 2018-08-16 14:39:10+00:00
- **Authors**: Jianbo Guo, Yuxi Li, Weiyao Lin, Yurong Chen, Jianguo Li
- **Comment**: None
- **Journal**: None
- **Summary**: Depthwise separable convolution has shown great efficiency in network design, but requires time-consuming training procedure with full training-set available. This paper first analyzes the mathematical relationship between regular convolutions and depthwise separable convolutions, and proves that the former one could be approximated with the latter one in closed form. We show depthwise separable convolutions are principal components of regular convolutions. And then we propose network decoupling (ND), a training-free method to accelerate convolutional neural networks (CNNs) by transferring pre-trained CNN models into the MobileNet-like depthwise separable convolution structure, with a promising speedup yet negligible accuracy loss. We further verify through experiments that the proposed method is orthogonal to other training-free methods like channel decomposition, spatial decomposition, etc. Combining the proposed method with them will bring even larger CNN speedup. For instance, ND itself achieves about 2X speedup for the widely used VGG16, and combined with other methods, it reaches 3.7X speedup with graceful accuracy degradation. We demonstrate that ND is widely applicable to classification networks like ResNet, and object detection network like SSD300.



### R$^3$-Net: A Deep Network for Multi-oriented Vehicle Detection in Aerial Images and Videos
- **Arxiv ID**: http://arxiv.org/abs/1808.05560v1
- **DOI**: 10.1109/TGRS.2019.2895362
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.05560v1)
- **Published**: 2018-08-16 16:04:27+00:00
- **Updated**: 2018-08-16 16:04:27+00:00
- **Authors**: Qingpeng Li, Lichao Mou, Qizhi Xu, Yun Zhang, Xiao Xiang Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Vehicle detection is a significant and challenging task in aerial remote sensing applications. Most existing methods detect vehicles with regular rectangle boxes and fail to offer the orientation of vehicles. However, the orientation information is crucial for several practical applications, such as the trajectory and motion estimation of vehicles. In this paper, we propose a novel deep network, called rotatable region-based residual network (R$^3$-Net), to detect multi-oriented vehicles in aerial images and videos. More specially, R$^3$-Net is utilized to generate rotatable rectangular target boxes in a half coordinate system. First, we use a rotatable region proposal network (R-RPN) to generate rotatable region of interests (R-RoIs) from feature maps produced by a deep convolutional neural network. Here, a proposed batch averaging rotatable anchor (BAR anchor) strategy is applied to initialize the shape of vehicle candidates. Next, we propose a rotatable detection network (R-DN) for the final classification and regression of the R-RoIs. In R-DN, a novel rotatable position sensitive pooling (R-PS pooling) is designed to keep the position and orientation information simultaneously while downsampling the feature maps of R-RoIs. In our model, R-RPN and R-DN can be trained jointly. We test our network on two open vehicle detection image datasets, namely DLR 3K Munich Dataset and VEDAI Dataset, demonstrating the high precision and robustness of our method. In addition, further experiments on aerial videos show the good generalization capability of the proposed method and its potential for vehicle tracking in aerial videos. The demo video is available at https://youtu.be/xCYD-tYudN0.



### Emotion Recognition in Speech using Cross-Modal Transfer in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1808.05561v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.05561v1)
- **Published**: 2018-08-16 16:10:23+00:00
- **Updated**: 2018-08-16 16:10:23+00:00
- **Authors**: Samuel Albanie, Arsha Nagrani, Andrea Vedaldi, Andrew Zisserman
- **Comment**: Conference paper at ACM Multimedia 2018
- **Journal**: None
- **Summary**: Obtaining large, human labelled speech datasets to train models for emotion recognition is a notoriously challenging task, hindered by annotation cost and label ambiguity. In this work, we consider the task of learning embeddings for speech classification without access to any form of labelled audio. We base our approach on a simple hypothesis: that the emotional content of speech correlates with the facial expression of the speaker. By exploiting this relationship, we show that annotations of expression can be transferred from the visual domain (faces) to the speech domain (voices) through cross-modal distillation. We make the following contributions: (i) we develop a strong teacher network for facial emotion recognition that achieves the state of the art on a standard benchmark; (ii) we use the teacher to train a student, tabula rasa, to learn representations (embeddings) for speech emotion recognition without access to labelled audio data; and (iii) we show that the speech emotion embedding can be used for speech emotion recognition on external benchmark datasets. Code, models and data are available.



### Deeper Image Quality Transfer: Training Low-Memory Neural Networks for 3D Images
- **Arxiv ID**: http://arxiv.org/abs/1808.05577v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1808.05577v1)
- **Published**: 2018-08-16 16:42:10+00:00
- **Updated**: 2018-08-16 16:42:10+00:00
- **Authors**: Stefano B. Blumberg, Ryutaro Tanno, Iasonas Kokkinos, Daniel C. Alexander
- **Comment**: Accepted in: MICCAI 2018
- **Journal**: None
- **Summary**: In this paper we address the memory demands that come with the processing of 3-dimensional, high-resolution, multi-channeled medical images in deep learning. We exploit memory-efficient backpropagation techniques, to reduce the memory complexity of network training from being linear in the network's depth, to being roughly constant $ - $ permitting us to elongate deep architectures with negligible memory increase. We evaluate our methodology in the paradigm of Image Quality Transfer, whilst noting its potential application to various tasks that use deep learning. We study the impact of depth on accuracy and show that deeper models have more predictive power, which may exploit larger training sets. We obtain substantially better results than the previous state-of-the-art model with a slight memory increase, reducing the root-mean-squared-error by $ 13\% $. Our code is publicly available.



### BlockQNN: Efficient Block-wise Neural Network Architecture Generation
- **Arxiv ID**: http://arxiv.org/abs/1808.05584v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1808.05584v1)
- **Published**: 2018-08-16 17:02:24+00:00
- **Updated**: 2018-08-16 17:02:24+00:00
- **Authors**: Zhao Zhong, Zichen Yang, Boyang Deng, Junjie Yan, Wei Wu, Jing Shao, Cheng-Lin Liu
- **Comment**: 14 pages, 18 figures
- **Journal**: None
- **Summary**: Convolutional neural networks have gained a remarkable success in computer vision. However, most usable network architectures are hand-crafted and usually require expertise and elaborate design. In this paper, we provide a block-wise network generation pipeline called BlockQNN which automatically builds high-performance networks using the Q-Learning paradigm with epsilon-greedy exploration strategy. The optimal network block is constructed by the learning agent which is trained to choose component layers sequentially. We stack the block to construct the whole auto-generated network. To accelerate the generation process, we also propose a distributed asynchronous framework and an early stop strategy. The block-wise generation brings unique advantages: (1) it yields state-of-the-art results in comparison to the hand-crafted networks on image classification, particularly, the best network generated by BlockQNN achieves 2.35% top-1 error rate on CIFAR-10. (2) it offers tremendous reduction of the search space in designing networks, spending only 3 days with 32 GPUs. A faster version can yield a comparable result with only 1 GPU in 20 hours. (3) it has strong generalizability in that the network built on CIFAR also performs well on the larger-scale dataset. The best network achieves very competitive accuracy of 82.0% top-1 and 96.0% top-5 on ImageNet.



### Probabilistic Model of Object Detection Based on Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1808.08272v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1808.08272v1)
- **Published**: 2018-08-16 18:11:56+00:00
- **Updated**: 2018-08-16 18:11:56+00:00
- **Authors**: Fang-Qi Li, Xu-Die Ren, Hao-Nan Guo
- **Comment**: 8 pages, 8 figures, International Conference on Communication, Signal
  Processing and Systems (CSPS 2017)
- **Journal**: None
- **Summary**: The combination of a CNN detector and a search framework forms the basis for local object/pattern detection. To handle the waste of regional information and the defective compromise between efficiency and accuracy, this paper proposes a probabilistic model with a powerful search framework. By mapping an image into a probabilistic distribution of objects, this new model gives more informative outputs with less computation. The setting and analytic traits are elaborated in this paper, followed by a series of experiments carried out on FDDB, which show that the proposed model is sound, efficient and analytic.



