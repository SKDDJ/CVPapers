# Arxiv Papers in cs.CV on 2018-08-23
### Generating Magnetic Resonance Spectroscopy Imaging Data of Brain Tumours from Linear, Non-Linear and Deep Learning Models
- **Arxiv ID**: http://arxiv.org/abs/1808.07592v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.07592v1)
- **Published**: 2018-08-23 00:02:31+00:00
- **Updated**: 2018-08-23 00:02:31+00:00
- **Authors**: Nathan J Olliverre, Guang Yang, Gregory Slabaugh, Constantino Carlos Reyes-Aldasoro, Eduardo Alonso
- **Comment**: 9 pages, 4 figures, 2 tables, to be presented at Simulation and
  Synthesis in Medical Imaging (SASHIMI) 2018 and published in the book
  Simulation and Synthesis in Medical Imaging, Lecture Notes in Computer
  Science series, Volume 11037
- **Journal**: None
- **Summary**: Magnetic Resonance Spectroscopy (MRS) provides valuable information to help with the identification and understanding of brain tumors, yet MRS is not a widely available medical imaging modality. Aiming to counter this issue, this research draws on the advancements in machine learning techniques in other fields for the generation of artificial data. The generated methods were tested through the evaluation of their output against that of a real-world labelled MRS brain tumor data-set. Furthermore the resultant output from the generative techniques were each used to train separate traditional classifiers which were tested on a subset of the real MRS brain tumor dataset. The results suggest that there exist methods capable of producing accurate, ground truth based MRS voxels. These findings indicate that through generative techniques, large datasets can be made available for training deep, learning models for the use in brain tumor diagnosis.



### An Improvement of Data Classification Using Random Multimodel Deep Learning (RMDL)
- **Arxiv ID**: http://arxiv.org/abs/1808.08121v1
- **DOI**: 10.18178/ijmlc.2018.8.4.703
- **Categories**: **cs.LG**, cs.CV, cs.IR, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1808.08121v1)
- **Published**: 2018-08-23 00:38:14+00:00
- **Updated**: 2018-08-23 00:38:14+00:00
- **Authors**: Mojtaba Heidarysafa, Kamran Kowsari, Donald E. Brown, Kiana Jafari Meimandi, Laura E. Barnes
- **Comment**: published in International Journal of Machine Learning and Computing
  (IJMLC). arXiv admin note: substantial text overlap with arXiv:1805.01890
- **Journal**: None
- **Summary**: The exponential growth in the number of complex datasets every year requires more enhancement in machine learning methods to provide robust and accurate data classification. Lately, deep learning approaches have achieved surpassing results in comparison to previous machine learning algorithms. However, finding the suitable structure for these models has been a challenge for researchers. This paper introduces Random Multimodel Deep Learning (RMDL): a new ensemble, deep learning approach for classification. RMDL solves the problem of finding the best deep learning structure and architecture while simultaneously improving robustness and accuracy through ensembles of deep learning architectures. In short, RMDL trains multiple randomly generated models of Deep Neural Network (DNN), Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN) in parallel and combines their results to produce better result of any of those models individually. In this paper, we describe RMDL model and compare the results for image and text classification as well as face recognition. We used MNIST and CIFAR-10 datasets as ground truth datasets for image classification and WOS, Reuters, IMDB, and 20newsgroup datasets for text classification. Lastly, we used ORL dataset to compare the model performance on face recognition task.



### Brain Biomarker Interpretation in ASD Using Deep Learning and fMRI
- **Arxiv ID**: http://arxiv.org/abs/1808.08296v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1808.08296v1)
- **Published**: 2018-08-23 06:24:56+00:00
- **Updated**: 2018-08-23 06:24:56+00:00
- **Authors**: Xiaoxiao Li, Nicha C. Dvornek, Juntang Zhuang, Pamela Ventola, James S. Duncan
- **Comment**: 8 pagers, accepted by MICCAI 2018
- **Journal**: None
- **Summary**: Autism spectrum disorder (ASD) is a complex neurodevelopmental disorder. Finding the biomarkers associated with ASD is extremely helpful to understand the underlying roots of the disorder and can lead to earlier diagnosis and more targeted treatment. Although Deep Neural Networks (DNNs) have been applied in functional magnetic resonance imaging (fMRI) to identify ASD, understanding the data-driven computational decision making procedure has not been previously explored. Therefore, in this work, we address the problem of interpreting reliable biomarkers associated with identifying ASD; specifically, we propose a 2-stage method that classifies ASD and control subjects using fMRI images and interprets the saliency features activated by the classifier. First, we trained an accurate DNN classifier. Then, for detecting the biomarkers, different from the DNN visualization works in computer vision, we take advantage of the anatomical structure of brain fMRI and develop a frequency-normalized sampling method to corrupt images. Furthermore, in the ASD vs. control subjects classification scenario, we provide a new approach to detect and characterize important brain features into three categories. The biomarkers we found by the proposed method are robust and consistent with previous findings in the literature. We also validate the detected biomarkers by neurological function decoding and comparing with the DNN activation maps.



### PVNet: A Joint Convolutional Network of Point Cloud and Multi-View for 3D Shape Recognition
- **Arxiv ID**: http://arxiv.org/abs/1808.07659v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.07659v1)
- **Published**: 2018-08-23 08:10:08+00:00
- **Updated**: 2018-08-23 08:10:08+00:00
- **Authors**: Haoxuan You, Yifan Feng, Rongrong Ji, Yue Gao
- **Comment**: ACM Multimedia Conference 2018
- **Journal**: None
- **Summary**: 3D object recognition has attracted wide research attention in the field of multimedia and computer vision. With the recent proliferation of deep learning, various deep models with different representations have achieved the state-of-the-art performance. Among them, point cloud and multi-view based 3D shape representations are promising recently, and their corresponding deep models have shown significant performance on 3D shape recognition. However, there is little effort concentrating point cloud data and multi-view data for 3D shape representation, which is, in our consideration, beneficial and compensated to each other. In this paper, we propose the Point-View Network (PVNet), the first framework integrating both the point cloud and the multi-view data towards joint 3D shape recognition. More specifically, an embedding attention fusion scheme is proposed that could employ high-level features from the multi-view data to model the intrinsic correlation and discriminability of different structure features from the point cloud data. In particular, the discriminative descriptions are quantified and leveraged as the soft attention mask to further refine the structure feature of the 3D shape. We have evaluated the proposed method on the ModelNet40 dataset for 3D shape classification and retrieval tasks. Experimental results and comparisons with state-of-the-art methods demonstrate that our framework can achieve superior performance.



### Deep multi-task learning for a geographically-regularized semantic segmentation of aerial images
- **Arxiv ID**: http://arxiv.org/abs/1808.07675v1
- **DOI**: 10.1016/j.isprsjprs.2018.06.007
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.07675v1)
- **Published**: 2018-08-23 09:41:47+00:00
- **Updated**: 2018-08-23 09:41:47+00:00
- **Authors**: Michele Volpi, Devis Tuia
- **Comment**: None
- **Journal**: ISPRS Journal of Photogrammetry and Remote Sensing, 144, pages
  48-60, 2018
- **Summary**: When approaching the semantic segmentation of overhead imagery in the decimeter spatial resolution range, successful strategies usually combine powerful methods to learn the visual appearance of the semantic classes (e.g. convolutional neural networks) with strategies for spatial regularization (e.g. graphical models such as conditional random fields). In this paper, we propose a method to learn evidence in the form of semantic class likelihoods, semantic boundaries across classes and shallow-to-deep visual features, each one modeled by a multi-task convolutional neural network architecture. We combine this bottom-up information with top-down spatial regularization encoded by a conditional random field model optimizing the label space across a hierarchy of segments with constraints related to structural, spatial and data-dependent pairwise relationships between regions. Our results show that such strategy provide better regularization than a series of strong baselines reflecting state-of-the-art technologies. The proposed strategy offers a flexible and principled framework to include several sources of visual and structural information, while allowing for different degrees of spatial regularization accounting for priors about the expected output structures.



### Discriminative out-of-distribution detection for semantic segmentation
- **Arxiv ID**: http://arxiv.org/abs/1808.07703v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.07703v2)
- **Published**: 2018-08-23 11:28:34+00:00
- **Updated**: 2018-10-01 09:19:58+00:00
- **Authors**: Petra Bevandić, Ivan Krešo, Marin Oršić, Siniša Šegvić
- **Comment**: This paper has been withdrawn from AutoNUE workshop at ECCV 2018 due
  to ECCV registration being closed
- **Journal**: None
- **Summary**: Most classification and segmentation datasets assume a closed-world scenario in which predictions are expressed as distribution over a predetermined set of visual classes. However, such assumption implies unavoidable and often unnoticeable failures in presence of out-of-distribution (OOD) input. These failures are bound to happen in most real-life applications since current visual ontologies are far from being comprehensive. We propose to address this issue by discriminative detection of OOD pixels in input data. Different from recent approaches, we avoid to bring any decisions by only observing the training dataset of the primary model trained to solve the desired computer vision task. Instead, we train a dedicated OOD model which discriminates the primary training set from a much larger "background" dataset which approximates the variety of the visual world. We perform our experiments on high resolution natural images in a dense prediction setup. We use several road driving datasets as our training distribution, while we approximate the background distribution with the ILSVRC dataset. We evaluate our approach on WildDash test, which is currently the only public test dataset that includes out-of-distribution images. The obtained results show that the proposed approach succeeds to identify out-of-distribution pixels while outperforming previous work by a wide margin.



### Predicting Action Tubes
- **Arxiv ID**: http://arxiv.org/abs/1808.07712v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1808.07712v1)
- **Published**: 2018-08-23 12:11:06+00:00
- **Updated**: 2018-08-23 12:11:06+00:00
- **Authors**: Gurkirt Singh, Suman Saha, Fabio Cuzzolin
- **Comment**: ECCV workshop; Anticipating Human Behaviour 2018; 16 page 7 figures
- **Journal**: None
- **Summary**: In this work, we present a method to predict an entire `action tube' (a set of temporally linked bounding boxes) in a trimmed video just by observing a smaller subset of it. Predicting where an action is going to take place in the near future is essential to many computer vision based applications such as autonomous driving or surgical robotics. Importantly, it has to be done in real-time and in an online fashion. We propose a Tube Prediction network (TPnet) which jointly predicts the past, present and future bounding boxes along with their action classification scores. At test time TPnet is used in a (temporal) sliding window setting, and its predictions are put into a tube estimation framework to construct/predict the video long action tubes not only for the observed part of the video but also for the unobserved part. Additionally, the proposed action tube predictor helps in completing action tubes for unobserved segments of the video. We quantitatively demonstrate the latter ability, and the fact that TPnet improves state-of-the-art detection performance, on one of the standard action detection benchmarks - J-HMDB-21 dataset.



### Segmentation of Bleeding Regions in Wireless Capsule Endoscopy for Detection of Informative Frames
- **Arxiv ID**: http://arxiv.org/abs/1808.07746v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.07746v1)
- **Published**: 2018-08-23 13:39:04+00:00
- **Updated**: 2018-08-23 13:39:04+00:00
- **Authors**: Mohsen Hajabdollahi, Reza Esfandiarpoor, Pejman Khadivi, S. M. Reza Soroushmehr, Nader Karimi, Kayvan Najarian, Shadrokh Samavi
- **Comment**: None
- **Journal**: None
- **Summary**: Wireless capsule endoscopy (WCE) is an effective mean for diagnosis of gastrointestinal disorders. Detection of informative scenes in WCE video could reduce the length of transmitted videos and help the diagnosis procedure. In this paper, we investigate the problem of simplification of neural networks for automatic bleeding region detection inside capsule endoscopy device. Suitable color channels are selected as neural networks inputs, and image classification is conducted using a multi-layer perceptron (MLP) and a convolutional neural network (CNN) separately. Both CNN and MLP structures are simplified to reduce the number of computational operations. Performances of two simplified networks are evaluated on a WCE bleeding image dataset using the DICE score. Simulation results show that applying simplification methods on both MLP and CNN structures reduces the number of computational operations significantly with AUC greater than 0.97. Although CNN performs better in comparison with simplified MLP, the simplified MLP segments bleeding regions with a significantly smaller number of computational operations. Concerning the importance of having a simple structure or a more accurate model, each of the designed structures could be selected for inside capsule implementation.



### Recalibrating Fully Convolutional Networks with Spatial and Channel 'Squeeze & Excitation' Blocks
- **Arxiv ID**: http://arxiv.org/abs/1808.08127v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.08127v1)
- **Published**: 2018-08-23 13:45:03+00:00
- **Updated**: 2018-08-23 13:45:03+00:00
- **Authors**: Abhijit Guha Roy, Nassir Navab, Christian Wachinger
- **Comment**: Accepted for publication in IEEE Transactions on Medical Imaging.
  arXiv admin note: text overlap with arXiv:1803.02579
- **Journal**: None
- **Summary**: In a wide range of semantic segmentation tasks, fully convolutional neural networks (F-CNNs) have been successfully leveraged to achieve state-of-the-art performance. Architectural innovations of F-CNNs have mainly been on improving spatial encoding or network connectivity to aid gradient flow. In this article, we aim towards an alternate direction of recalibrating the learned feature maps adaptively; boosting meaningful features while suppressing weak ones. The recalibration is achieved by simple computational blocks that can be easily integrated in F-CNNs architectures. We draw our inspiration from the recently proposed 'squeeze & excitation' (SE) modules for channel recalibration for image classification. Towards this end, we introduce three variants of SE modules for segmentation, (i) squeezing spatially and exciting channel-wise, (ii) squeezing channel-wise and exciting spatially and (iii) joint spatial and channel 'squeeze & excitation'. We effectively incorporate the proposed SE blocks in three state-of-the-art F-CNNs and demonstrate a consistent improvement of segmentation accuracy on three challenging benchmark datasets. Importantly, SE blocks only lead to a minimal increase in model complexity of about 1.5%, while the Dice score increases by 4-9% in the case of U-Net. Hence, we believe that SE blocks can be an integral part of future F-CNN architectures.



### Deep Learning Based Vehicle Make-Model Classification
- **Arxiv ID**: http://arxiv.org/abs/1809.00953v2
- **DOI**: 10.1007/978-3-030-01424-7_53
- **Categories**: **cs.CV**, cs.AI, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.00953v2)
- **Published**: 2018-08-23 14:05:31+00:00
- **Updated**: 2019-02-09 20:46:17+00:00
- **Authors**: Burak Satar, Ahmet Emir Dirik
- **Comment**: 10 pages, ICANN 2018: Artificial Neural Networks and Machine Learning
- **Journal**: Lecture Notes in Computer Science book series 2018 (LNCS, volume
  11141). Springer, Cham
- **Summary**: This paper studies the problems of vehicle make & model classification. Some of the main challenges are reaching high classification accuracy and reducing the annotation time of the images. To address these problems, we have created a fine-grained database using online vehicle marketplaces of Turkey. A pipeline is proposed to combine an SSD (Single Shot Multibox Detector) model with a CNN (Convolutional Neural Network) model to train on the database. In the pipeline, we first detect the vehicles by following an algorithm which reduces the time for annotation. Then, we feed them into the CNN model. It is reached approximately 4% better classification accuracy result than using a conventional CNN model. Next, we propose to use the detected vehicles as ground truth bounding box (GTBB) of the images and feed them into an SSD model in another pipeline. At this stage, it is reached reasonable classification accuracy result without using perfectly shaped GTBB. Lastly, an application is implemented in a use case by using our proposed pipelines. It detects the unauthorized vehicles by comparing their license plate numbers and make & models. It is assumed that license plates are readable.



### EmotiW 2018: Audio-Video, Student Engagement and Group-Level Affect Prediction
- **Arxiv ID**: http://arxiv.org/abs/1808.07773v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.07773v1)
- **Published**: 2018-08-23 14:19:11+00:00
- **Updated**: 2018-08-23 14:19:11+00:00
- **Authors**: Abhinav Dhall, Amanjot Kaur, Roland Goecke, Tom Gedeon
- **Comment**: None
- **Journal**: None
- **Summary**: This paper details the sixth Emotion Recognition in the Wild (EmotiW) challenge. EmotiW 2018 is a grand challenge in the ACM International Conference on Multimodal Interaction 2018, Colorado, USA. The challenge aims at providing a common platform to researchers working in the affective computing community to benchmark their algorithms on `in the wild' data. This year EmotiW contains three sub-challenges: a) Audio-video based emotion recognition; b) Student engagement prediction; and c) Group-level emotion recognition. The databases, protocols and baselines are discussed in detail.



### Time-Agnostic Prediction: Predicting Predictable Video Frames
- **Arxiv ID**: http://arxiv.org/abs/1808.07784v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1808.07784v3)
- **Published**: 2018-08-23 14:52:40+00:00
- **Updated**: 2018-10-23 19:22:25+00:00
- **Authors**: Dinesh Jayaraman, Frederik Ebert, Alexei A. Efros, Sergey Levine
- **Comment**: 8 pages, plus appendices
- **Journal**: None
- **Summary**: Prediction is arguably one of the most basic functions of an intelligent system. In general, the problem of predicting events in the future or between two waypoints is exceedingly difficult. However, most phenomena naturally pass through relatively predictable bottlenecks---while we cannot predict the precise trajectory of a robot arm between being at rest and holding an object up, we can be certain that it must have picked the object up. To exploit this, we decouple visual prediction from a rigid notion of time. While conventional approaches predict frames at regularly spaced temporal intervals, our time-agnostic predictors (TAP) are not tied to specific times so that they may instead discover predictable "bottleneck" frames no matter when they occur. We evaluate our approach for future and intermediate frame prediction across three robotic manipulation tasks. Our predictions are not only of higher visual quality, but also correspond to coherent semantic subgoals in temporally extended tasks.



### Webly Supervised Joint Embedding for Cross-Modal Image-Text Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1808.07793v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1808.07793v1)
- **Published**: 2018-08-23 15:07:52+00:00
- **Updated**: 2018-08-23 15:07:52+00:00
- **Authors**: Niluthpol Chowdhury Mithun, Rameswar Panda, Evangelos E. Papalexakis, Amit K. Roy-Chowdhury
- **Comment**: ACM Multimedia 2018
- **Journal**: None
- **Summary**: Cross-modal retrieval between visual data and natural language description remains a long-standing challenge in multimedia. While recent image-text retrieval methods offer great promise by learning deep representations aligned across modalities, most of these methods are plagued by the issue of training with small-scale datasets covering a limited number of images with ground-truth sentences. Moreover, it is extremely expensive to create a larger dataset by annotating millions of images with sentences and may lead to a biased model. Inspired by the recent success of webly supervised learning in deep neural networks, we capitalize on readily-available web images with noisy annotations to learn robust image-text joint representation. Specifically, our main idea is to leverage web images and corresponding tags, along with fully annotated datasets, in training for learning the visual-semantic joint embedding. We propose a two-stage approach for the task that can augment a typical supervised pair-wise ranking loss based formulation with weakly-annotated web images to learn a more robust visual-semantic embedding. Experiments on two standard benchmark datasets demonstrate that our method achieves a significant performance gain in image-text retrieval compared to state-of-the-art approaches.



### High quality ultrasonic multi-line transmission through deep learning
- **Arxiv ID**: http://arxiv.org/abs/1808.07819v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.07819v1)
- **Published**: 2018-08-23 16:07:59+00:00
- **Updated**: 2018-08-23 16:07:59+00:00
- **Authors**: Sanketh Vedula, Ortal Senouf, Grigoriy Zurakhov, Alex M. Bronstein, Michael Zibulevsky, Oleg Michailovich, Dan Adam, Diana Gaitini
- **Comment**: To appear in Proceedings of MLMIR workshop, MICCAI 2018
- **Journal**: None
- **Summary**: Frame rate is a crucial consideration in cardiac ultrasound imaging and 3D sonography. Several methods have been proposed in the medical ultrasound literature aiming at accelerating the image acquisition. In this paper, we consider one such method called \textit{multi-line transmission} (MLT), in which several evenly separated focused beams are transmitted simultaneously. While MLT reduces the acquisition time, it comes at the expense of a heavy loss of contrast due to the interactions between the beams (cross-talk artifact). In this paper, we introduce a data-driven method to reduce the artifacts arising in MLT. To this end, we propose to train an end-to-end convolutional neural network consisting of correction layers followed by a constant apodization layer. The network is trained on pairs of raw data obtained through MLT and the corresponding \textit{single-line transmission} (SLT) data. Experimental evaluation demonstrates significant improvement both in the visual image quality and in objective measures such as contrast ratio and contrast-to-noise ratio, while preserving resolution unlike traditional apodization-based methods. We show that the proposed method is able to generalize well across different patients and anatomies on real and phantom data.



### High frame-rate cardiac ultrasound imaging with deep learning
- **Arxiv ID**: http://arxiv.org/abs/1808.07823v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.07823v1)
- **Published**: 2018-08-23 16:21:18+00:00
- **Updated**: 2018-08-23 16:21:18+00:00
- **Authors**: Ortal Senouf, Sanketh Vedula, Grigoriy Zurakhov, Alex M. Bronstein, Michael Zibulevsky, Oleg Michailovich, Dan Adam, David Blondheim
- **Comment**: To appear in the Proceedings of MICCAI, 2018
- **Journal**: None
- **Summary**: Cardiac ultrasound imaging requires a high frame rate in order to capture rapid motion. This can be achieved by multi-line acquisition (MLA), where several narrow-focused received lines are obtained from each wide-focused transmitted line. This shortens the acquisition time at the expense of introducing block artifacts. In this paper, we propose a data-driven learning-based approach to improve the MLA image quality. We train an end-to-end convolutional neural network on pairs of real ultrasound cardiac data, acquired through MLA and the corresponding single-line acquisition (SLA). The network achieves a significant improvement in image quality for both $5-$ and $7-$line MLA resulting in a decorrelation measure similar to that of SLA while having the frame rate of MLA.



### Deconvolutional Networks for Point-Cloud Vehicle Detection and Tracking in Driving Scenarios
- **Arxiv ID**: http://arxiv.org/abs/1808.07935v1
- **DOI**: 10.1109/ECMR.2017.8098657
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.07935v1)
- **Published**: 2018-08-23 20:30:04+00:00
- **Updated**: 2018-08-23 20:30:04+00:00
- **Authors**: Victor Vaquero, Ivan del Pino, Francesc Moreno-Noguer, Joan Solà, Alberto Sanfeliu, Juan Andrade-Cetto
- **Comment**: Presented in IEEE ECMR 2017. IEEE Copyrights: Personal use of this
  material is permitted. Permission from IEEE must be obtained for all other
  uses
- **Journal**: None
- **Summary**: Vehicle detection and tracking is a core ingredient for developing autonomous driving applications in urban scenarios. Recent image-based Deep Learning (DL) techniques are obtaining breakthrough results in these perceptive tasks. However, DL research has not yet advanced much towards processing 3D point clouds from lidar range-finders. These sensors are very common in autonomous vehicles since, despite not providing as semantically rich information as images, their performance is more robust under harsh weather conditions than vision sensors. In this paper we present a full vehicle detection and tracking system that works with 3D lidar information only. Our detection step uses a Convolutional Neural Network (CNN) that receives as input a featured representation of the 3D information provided by a Velodyne HDL-64 sensor and returns a per-point classification of whether it belongs to a vehicle or not. The classified point cloud is then geometrically processed to generate observations for a multi-object tracking system implemented via a number of Multi-Hypothesis Extended Kalman Filters (MH-EKF) that estimate the position and velocity of the surrounding vehicles. The system is thoroughly evaluated on the KITTI tracking dataset, and we show the performance boost provided by our CNN-based vehicle detector over a standard geometric approach. Our lidar-based approach uses about a 4% of the data needed for an image-based detector with similarly competitive results.



### From Hand-Crafted to Deep Learning-based Cancer Radiomics: Challenges and Opportunities
- **Arxiv ID**: http://arxiv.org/abs/1808.07954v3
- **DOI**: 10.1109/MSP.2019.2900993
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.07954v3)
- **Published**: 2018-08-23 21:39:12+00:00
- **Updated**: 2019-02-20 00:34:41+00:00
- **Authors**: Parnian Afshar, Arash Mohammadi, Konstantinos N. Plataniotis, Anastasia Oikonomou, Habib Benali
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advancements in signal processing and machine learning coupled with developments of electronic medical record keeping in hospitals and the availability of extensive set of medical images through internal/external communication systems, have resulted in a recent surge of significant interest in "Radiomics". Radiomics is an emerging and relatively new research field, which refers to extracting semi-quantitative and/or quantitative features from medical images with the goal of developing predictive and/or prognostic models, and is expected to become a critical component for integration of image-derived information for personalized treatment in the near future. The conventional Radiomics workflow is typically based on extracting pre-designed features (also referred to as hand-crafted or engineered features) from a segmented region of interest. Nevertheless, recent advancements in deep learning have caused trends towards deep learning-based Radiomics (also referred to as discovery Radiomics). Considering the advantages of these two approaches, there are also hybrid solutions developed to exploit the potentials of multiple data sources. Considering the variety of approaches to Radiomics, further improvements require a comprehensive and integrated sketch, which is the goal of this article. This manuscript provides a unique interdisciplinary perspective on Radiomics by discussing state-of-the-art signal processing solutions in the context of Radiomics.



### Learning Human-Object Interactions by Graph Parsing Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1808.07962v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.07962v1)
- **Published**: 2018-08-23 23:04:22+00:00
- **Updated**: 2018-08-23 23:04:22+00:00
- **Authors**: Siyuan Qi, Wenguan Wang, Baoxiong Jia, Jianbing Shen, Song-Chun Zhu
- **Comment**: This paper is published in ECCV 2018
- **Journal**: None
- **Summary**: This paper addresses the task of detecting and recognizing human-object interactions (HOI) in images and videos. We introduce the Graph Parsing Neural Network (GPNN), a framework that incorporates structural knowledge while being differentiable end-to-end. For a given scene, GPNN infers a parse graph that includes i) the HOI graph structure represented by an adjacency matrix, and ii) the node labels. Within a message passing inference framework, GPNN iteratively computes the adjacency matrices and node labels. We extensively evaluate our model on three HOI detection benchmarks on images and videos: HICO-DET, V-COCO, and CAD-120 datasets. Our approach significantly outperforms state-of-art methods, verifying that GPNN is scalable to large datasets and applies to spatial-temporal settings. The code is available at https://github.com/SiyuanQi/gpnn.



### Left ventricle quantification through spatio-temporal CNNs
- **Arxiv ID**: http://arxiv.org/abs/1808.07967v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.07967v1)
- **Published**: 2018-08-23 23:37:07+00:00
- **Updated**: 2018-08-23 23:37:07+00:00
- **Authors**: Alejandro Debus, Enzo Ferrante
- **Comment**: Accepted for publication at Statistical Atlases and Computational
  Modeling of the Heart (STACOM) workshop @ MICCAI 2018
- **Journal**: None
- **Summary**: Cardiovascular diseases are among the leading causes of death globally. Cardiac left ventricle (LV) quantification is known to be one of the most important tasks for the identification and diagnosis of such pathologies. In this paper, we propose a deep learning method that incorporates 3D spatio-temporal convolutions to perform direct left ventricle quantification from cardiac MR sequences. Instead of analysing slices independently, we process stacks of temporally adjacent slices by means of 3D convolutional kernels which fuse the spatio-temporal information, incorporating the temporal dynamics of the heart to the learned model. We show that incorporating such information by means of spatio-temporal convolutions into standard LV quantification architectures improves the accuracy of the predictions when compared with single-slice models, achieving competitive results for all cardiac indices and significantly breaking the state of the art (Xue et al., 2018, MedIA) for cardiac phase estimation.



