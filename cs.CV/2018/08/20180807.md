# Arxiv Papers in cs.CV on 2018-08-07
### Efficient Fusion of Sparse and Complementary Convolutions
- **Arxiv ID**: http://arxiv.org/abs/1808.02167v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.02167v3)
- **Published**: 2018-08-07 00:39:56+00:00
- **Updated**: 2018-09-11 03:18:39+00:00
- **Authors**: Chun-Fu Chen, Quanfu Fan, Marco Pistoia, Gwo Giun Lee
- **Comment**: 10 pages, updated with correct numbers
- **Journal**: None
- **Summary**: We propose a new method to create compact convolutional neural networks (CNNs) by exploiting sparse convolutions. Different from previous works that learn sparsity in models, we directly employ hand-crafted kernels with regular sparse patterns, which result in the computational gain in practice without sophisticated and dedicated software or hardware. The core of our approach is an efficient network module that linearly combines sparse kernels to yield feature representations as strong as those from regular kernels. We integrate this module into various network architectures and demonstrate its effectiveness on three vision tasks, object classification, localization and detection. For object classification and localization, our approach achieves comparable or better performance than several baselines and related works while providing lower computational costs with fewer parameters (on average, a $2-4\times$ reduction of convolutional parameters and computation). For object detection, our approach leads to a VGG-16-based Faster RCNN detector that is 12.4$\times$ smaller and about 3$\times$ faster than the baseline.



### Quantized Densely Connected U-Nets for Efficient Landmark Localization
- **Arxiv ID**: http://arxiv.org/abs/1808.02194v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.02194v2)
- **Published**: 2018-08-07 03:22:44+00:00
- **Updated**: 2018-08-10 00:40:06+00:00
- **Authors**: Zhiqiang Tang, Xi Peng, Shijie Geng, Lingfei Wu, Shaoting Zhang, Dimitris Metaxas
- **Comment**: ECCV2018
- **Journal**: None
- **Summary**: In this paper, we propose quantized densely connected U-Nets for efficient visual landmark localization. The idea is that features of the same semantic meanings are globally reused across the stacked U-Nets. This dense connectivity largely improves the information flow, yielding improved localization accuracy. However, a vanilla dense design would suffer from critical efficiency issue in both training and testing. To solve this problem, we first propose order-K dense connectivity to trim off long-distance shortcuts; then, we use a memory-efficient implementation to significantly boost the training efficiency and investigate an iterative refinement that may slice the model size in half. Finally, to reduce the memory consumption and high precision operations both in training and testing, we further quantize weights, inputs, and gradients of our localization network to low bit-width numbers. We validate our approach in two tasks: human pose estimation and face alignment. The results show that our approach achieves state-of-the-art localization accuracy, but using ~70% fewer parameters, ~98% less model size and saving ~75% training memory compared with other benchmark localizers. The code is available at https://github.com/zhiqiangdon/CU-Net.



### Holistic 3D Scene Parsing and Reconstruction from a Single RGB Image
- **Arxiv ID**: http://arxiv.org/abs/1808.02201v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.02201v1)
- **Published**: 2018-08-07 03:49:15+00:00
- **Updated**: 2018-08-07 03:49:15+00:00
- **Authors**: Siyuan Huang, Siyuan Qi, Yixin Zhu, Yinxue Xiao, Yuanlu Xu, Song-Chun Zhu
- **Comment**: Accepted by ECCV 2018
- **Journal**: None
- **Summary**: We propose a computational framework to jointly parse a single RGB image and reconstruct a holistic 3D configuration composed by a set of CAD models using a stochastic grammar model. Specifically, we introduce a Holistic Scene Grammar (HSG) to represent the 3D scene structure, which characterizes a joint distribution over the functional and geometric space of indoor scenes. The proposed HSG captures three essential and often latent dimensions of the indoor scenes: i) latent human context, describing the affordance and the functionality of a room arrangement, ii) geometric constraints over the scene configurations, and iii) physical constraints that guarantee physically plausible parsing and reconstruction. We solve this joint parsing and reconstruction problem in an analysis-by-synthesis fashion, seeking to minimize the differences between the input image and the rendered images generated by our 3D representation, over the space of depth, surface normal, and object segmentation map. The optimal configuration, represented by a parse graph, is inferred using Markov chain Monte Carlo (MCMC), which efficiently traverses through the non-differentiable solution space, jointly optimizing object localization, 3D layout, and hidden human context. Experimental results demonstrate that the proposed algorithm improves the generalization ability and significantly outperforms prior methods on 3D layout estimation, 3D object detection, and holistic scene understanding.



### Contemplating Visual Emotions: Understanding and Overcoming Dataset Bias
- **Arxiv ID**: http://arxiv.org/abs/1808.02212v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.02212v1)
- **Published**: 2018-08-07 05:00:51+00:00
- **Updated**: 2018-08-07 05:00:51+00:00
- **Authors**: Rameswar Panda, Jianming Zhang, Haoxiang Li, Joon-Young Lee, Xin Lu, Amit K. Roy-Chowdhury
- **Comment**: ECCV 2018
- **Journal**: None
- **Summary**: While machine learning approaches to visual emotion recognition offer great promise, current methods consider training and testing models on small scale datasets covering limited visual emotion concepts. Our analysis identifies an important but long overlooked issue of existing visual emotion benchmarks in the form of dataset biases. We design a series of tests to show and measure how such dataset biases obstruct learning a generalizable emotion recognition model. Based on our analysis, we propose a webly supervised approach by leveraging a large quantity of stock image data. Our approach uses a simple yet effective curriculum guided training strategy for learning discriminative emotion features. We discover that the models learned using our large scale stock image dataset exhibit significantly better generalization ability than the existing datasets without the manual collection of even a single label. Moreover, visual representation learned using our approach holds a lot of promise across a variety of tasks on different image and video datasets.



### Deep Learning Super-Resolution Enables Rapid Simultaneous Morphological and Quantitative Magnetic Resonance Imaging
- **Arxiv ID**: http://arxiv.org/abs/1808.04447v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1808.04447v1)
- **Published**: 2018-08-07 05:09:11+00:00
- **Updated**: 2018-08-07 05:09:11+00:00
- **Authors**: Akshay Chaudhari, Zhongnan Fang, Jin Hyung Lee, Garry Gold, Brian Hargreaves
- **Comment**: Accepted for the Machine Learning for Medical Image Reconstruction
  Workshop at MICCAI 2018
- **Journal**: None
- **Summary**: Obtaining magnetic resonance images (MRI) with high resolution and generating quantitative image-based biomarkers for assessing tissue biochemistry is crucial in clinical and research applications. How- ever, acquiring quantitative biomarkers requires high signal-to-noise ratio (SNR), which is at odds with high-resolution in MRI, especially in a single rapid sequence. In this paper, we demonstrate how super-resolution can be utilized to maintain adequate SNR for accurate quantification of the T2 relaxation time biomarker, while simultaneously generating high- resolution images. We compare the efficacy of resolution enhancement using metrics such as peak SNR and structural similarity. We assess accuracy of cartilage T2 relaxation times by comparing against a standard reference method. Our evaluation suggests that SR can successfully maintain high-resolution and generate accurate biomarkers for accelerating MRI scans and enhancing the value of clinical and research MRI.



### Grassmannian Learning: Embedding Geometry Awareness in Shallow and Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1808.02229v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.IT, eess.SP, math.IT, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1808.02229v2)
- **Published**: 2018-08-07 06:54:06+00:00
- **Updated**: 2018-08-13 02:19:08+00:00
- **Authors**: Jiayao Zhang, Guangxu Zhu, Robert W. Heath Jr., Kaibin Huang
- **Comment**: Submitted to IEEE Signal Processing Magazine
- **Journal**: None
- **Summary**: Modern machine learning algorithms have been adopted in a range of signal-processing applications spanning computer vision, natural language processing, and artificial intelligence. Many relevant problems involve subspace-structured features, orthogonality constrained or low-rank constrained objective functions, or subspace distances. These mathematical characteristics are expressed naturally using the Grassmann manifold. Unfortunately, this fact is not yet explored in many traditional learning algorithms. In the last few years, there have been growing interests in studying Grassmann manifold to tackle new learning problems. Such attempts have been reassured by substantial performance improvements in both classic learning and learning using deep neural networks. We term the former as shallow and the latter deep Grassmannian learning. The aim of this paper is to introduce the emerging area of Grassmannian learning by surveying common mathematical problems and primary solution approaches, and overviewing various applications. We hope to inspire practitioners in different fields to adopt the powerful tool of Grassmannian learning in their research.



### A Generic Multi-Projection-Center Model and Calibration Method for Light Field Cameras
- **Arxiv ID**: http://arxiv.org/abs/1808.02244v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.02244v1)
- **Published**: 2018-08-07 07:52:38+00:00
- **Updated**: 2018-08-07 07:52:38+00:00
- **Authors**: Qi Zhang, Chunping Zhang, Jinbo Ling, Qing Wang, Jingyi Yu
- **Comment**: accepted by T-PAMI
- **Journal**: None
- **Summary**: Light field cameras can capture both spatial and angular information of light rays, enabling 3D reconstruction by a single exposure. The geometry of 3D reconstruction is affected by intrinsic parameters of a light field camera significantly. In the paper, we propose a multi-projection-center (MPC) model with 6 intrinsic parameters to characterize light field cameras based on traditional two-parallel-plane (TPP) representation. The MPC model can generally parameterize light field in different imaging formations, including conventional and focused light field cameras. By the constraints of 4D ray and 3D geometry, a 3D projective transformation is deduced to describe the relationship between geometric structure and the MPC coordinates. Based on the MPC model and projective transformation, we propose a calibration algorithm to verify our light field camera model. Our calibration method includes a close-form solution and a non-linear optimization by minimizing re-projection errors. Experimental results on both simulated and real scene data have verified the performance of our algorithm.



### SAM-RCNN: Scale-Aware Multi-Resolution Multi-Channel Pedestrian Detection
- **Arxiv ID**: http://arxiv.org/abs/1808.02246v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.02246v1)
- **Published**: 2018-08-07 07:58:41+00:00
- **Updated**: 2018-08-07 07:58:41+00:00
- **Authors**: Tianrui Liu, Mohamed Elmikaty, Tania Stathaki
- **Comment**: published in British Machine Vision Conference (BMVC) 2018
- **Journal**: None
- **Summary**: Convolutional neural networks (CNN) have enabled significant improvements in pedestrian detection owing to the strong representation ability of the CNN features. Recently, aggregating features from multiple layers of a CNN has been considered as an effective approach, however, the same approach regarding feature representation is used for detecting pedestrians of varying scales. Consequently, it is not guaranteed that the feature representation for pedestrians of a particular scale is optimised. In this paper, we propose a Scale-Aware Multi-resolution (SAM) method for pedestrian detection which can adaptively select multi-resolution convolutional features according to pedestrian sizes. The proposed SAM method extracts the appropriate CNN features that have strong representation ability as well as sufficient feature resolution, given the size of the pedestrian candidate output from a region proposal network. Moreover, we propose an enhanced SAM method, termed as SAM+, which incorporates complementary features channels and achieves further performance improvement. Evaluations on the challenging Caltech and KITTI pedestrian benchmarks demonstrate the superiority of our proposed method.



### MatchBench: An Evaluation of Feature Matchers
- **Arxiv ID**: http://arxiv.org/abs/1808.02267v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.02267v1)
- **Published**: 2018-08-07 09:03:24+00:00
- **Updated**: 2018-08-07 09:03:24+00:00
- **Authors**: JiaWang Bian, Ruihan Yang, Yun Liu, Le Zhang, Ming-Ming Cheng, Ian Reid, WenHai Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Feature matching is one of the most fundamental and active research areas in computer vision. A comprehensive evaluation of feature matchers is necessary, since it would advance both the development of this field and also high-level applications such as Structure-from-Motion or Visual SLAM. However, to the best of our knowledge, no previous work targets the evaluation of feature matchers while they only focus on evaluating feature detectors and descriptors. This leads to a critical absence in this field that there is no standard datasets and evaluation metrics to evaluate different feature matchers fairly. To this end, we present the first uniform feature matching benchmark to facilitate the evaluation of feature matchers. In the proposed benchmark, matchers are evaluated in different aspects, involving matching ability, correspondence sufficiency, and efficiency. Also, their performances are investigated in different scenes and in different matching types. Subsequently, we carry out an extensive evaluation of different state-of-the-art matchers on the benchmark and make in-depth analyses based on the reported results. This can be used to design practical matching systems in real applications and also advocates the potential future research directions in the field of feature matching.



### Predicting Visual Context for Unsupervised Event Segmentation in Continuous Photo-streams
- **Arxiv ID**: http://arxiv.org/abs/1808.02289v1
- **DOI**: 10.1145/3240508.3240624
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.02289v1)
- **Published**: 2018-08-07 10:03:55+00:00
- **Updated**: 2018-08-07 10:03:55+00:00
- **Authors**: Ana Garcia del Molino, Joo-Hwee Lim, Ah-Hwee Tan
- **Comment**: Accepted for publication at the 2018 ACM Multimedia Conference (MM
  '18)
- **Journal**: None
- **Summary**: Segmenting video content into events provides semantic structures for indexing, retrieval, and summarization. Since motion cues are not available in continuous photo-streams, and annotations in lifelogging are scarce and costly, the frames are usually clustered into events by comparing the visual features between them in an unsupervised way. However, such methodologies are ineffective to deal with heterogeneous events, e.g. taking a walk, and temporary changes in the sight direction, e.g. at a meeting. To address these limitations, we propose Contextual Event Segmentation (CES), a novel segmentation paradigm that uses an LSTM-based generative network to model the photo-stream sequences, predict their visual context, and track their evolution. CES decides whether a frame is an event boundary by comparing the visual context generated from the frames in the past, to the visual context predicted from the future. We implemented CES on a new and massive lifelogging dataset consisting of more than 1.5 million images spanning over 1,723 days. Experiments on the popular EDUB-Seg dataset show that our model outperforms the state-of-the-art by over 16% in f-measure. Furthermore, CES' performance is only 3 points below that of human annotators.



### Motorcycle detection and classification in urban Scenarios using a model based on Faster R-CNN
- **Arxiv ID**: http://arxiv.org/abs/1808.02299v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.02299v2)
- **Published**: 2018-08-07 10:50:55+00:00
- **Updated**: 2018-08-08 18:04:57+00:00
- **Authors**: Jorge E. Espinosa, Sergio A. Velastin, John W. Branch
- **Comment**: Presented at 9th International Conference on Pattern Recognition
  Systems, ICPRS-18, 22-24 May 2018, Valparaiso, Chile
- **Journal**: None
- **Summary**: This paper introduces a Deep Learning Convolutional Neural Network model based on Faster-RCNN for motorcycle detection and classification on urban environments. The model is evaluated in occluded scenarios where more than 60% of the vehicles present a degree of occlusion. For training and evaluation, we introduce a new dataset of 7500 annotated images, captured under real traffic scenes, using a drone mounted camera. Several tests were carried out to design the network, achieving promising results of 75% in average precision (AP), even with the high number of occluded motorbikes, the low angle of capture and the moving camera. The model is also evaluated on low occlusions datasets, reaching results of up to 92% in AP.



### Universal Perceptual Grouping
- **Arxiv ID**: http://arxiv.org/abs/1808.02312v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.02312v1)
- **Published**: 2018-08-07 11:50:32+00:00
- **Updated**: 2018-08-07 11:50:32+00:00
- **Authors**: Ke Li, Kaiyue Pang, Jifei Song, Yi-Zhe Song, Tao Xiang, Timothy M. Hospedales, Honggang Zhang
- **Comment**: Accepted ECCV 2018
- **Journal**: None
- **Summary**: In this work we aim to develop a universal sketch grouper. That is, a grouper that can be applied to sketches of any category in any domain to group constituent strokes/segments into semantically meaningful object parts. The first obstacle to this goal is the lack of large-scale datasets with grouping annotation. To overcome this, we contribute the largest sketch perceptual grouping (SPG) dataset to date, consisting of 20,000 unique sketches evenly distributed over 25 object categories. Furthermore, we propose a novel deep universal perceptual grouping model. The model is learned with both generative and discriminative losses. The generative losses improve the generalisation ability of the model to unseen object categories and datasets. The discriminative losses include a local grouping loss and a novel global grouping loss to enforce global grouping consistency. We show that the proposed model significantly outperforms the state-of-the-art groupers. Further, we show that our grouper is useful for a number of sketch analysis tasks including sketch synthesis and fine-grained sketch-based image retrieval (FG-SBIR).



### Deep Factorised Inverse-Sketching
- **Arxiv ID**: http://arxiv.org/abs/1808.02313v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.02313v1)
- **Published**: 2018-08-07 11:51:20+00:00
- **Updated**: 2018-08-07 11:51:20+00:00
- **Authors**: Kaiyue Pang, Da Li, Jifei Song, Yi-Zhe Song, Tao Xiang, Timothy M. Hospedales
- **Comment**: Accepted to ECCV 2018
- **Journal**: None
- **Summary**: Modelling human free-hand sketches has become topical recently, driven by practical applications such as fine-grained sketch based image retrieval (FG-SBIR). Sketches are clearly related to photo edge-maps, but a human free-hand sketch of a photo is not simply a clean rendering of that photo's edge map. Instead there is a fundamental process of abstraction and iconic rendering, where overall geometry is warped and salient details are selectively included. In this paper we study this sketching process and attempt to invert it. We model this inversion by translating iconic free-hand sketches to contours that resemble more geometrically realistic projections of object boundaries, and separately factorise out the salient added details. This factorised re-representation makes it easier to match a free-hand sketch to a photo instance of an object. Specifically, we propose a novel unsupervised image style transfer model based on enforcing a cyclic embedding consistency constraint. A deep FG-SBIR model is then formulated to accommodate complementary discriminative detail from each factorised sketch for better matching with the corresponding photo. Our method is evaluated both qualitatively and quantitatively to demonstrate its superiority over a number of state-of-the-art alternatives for style transfer and FG-SBIR.



### Automatic Recognition of Student Engagement using Deep Learning and Facial Expression
- **Arxiv ID**: http://arxiv.org/abs/1808.02324v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1808.02324v5)
- **Published**: 2018-08-07 12:38:20+00:00
- **Updated**: 2019-07-08 13:29:02+00:00
- **Authors**: Omid Mohamad Nezami, Mark Dras, Len Hamey, Deborah Richards, Stephen Wan, Cecile Paris
- **Comment**: None
- **Journal**: None
- **Summary**: Engagement is a key indicator of the quality of learning experience, and one that plays a major role in developing intelligent educational interfaces. Any such interface requires the ability to recognise the level of engagement in order to respond appropriately; however, there is very little existing data to learn from, and new data is expensive and difficult to acquire. This paper presents a deep learning model to improve engagement recognition from images that overcomes the data sparsity challenge by pre-training on readily available basic facial expression data, before training on specialised engagement data. In the first of two steps, a facial expression recognition model is trained to provide a rich face representation using deep learning. In the second step, we use the model's weights to initialize our deep learning based model to recognize engagement; we term this the engagement model. We train the model on our new engagement recognition dataset with 4627 engaged and disengaged samples. We find that the engagement model outperforms effective deep learning architectures that we apply for the first time to engagement recognition, as well as approaches using histogram of oriented gradients and support vector machines.



### YOLO3D: End-to-end real-time 3D Oriented Object Bounding Box Detection from LiDAR Point Cloud
- **Arxiv ID**: http://arxiv.org/abs/1808.02350v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1808.02350v1)
- **Published**: 2018-08-07 13:19:24+00:00
- **Updated**: 2018-08-07 13:19:24+00:00
- **Authors**: Waleed Ali, Sherif Abdelkarim, Mohamed Zahran, Mahmoud Zidan, Ahmad El Sallab
- **Comment**: Paper accepted in ECCV 2018, "3D Reconstruction meets Semantics"
  workshop
- **Journal**: None
- **Summary**: Object detection and classification in 3D is a key task in Automated Driving (AD). LiDAR sensors are employed to provide the 3D point cloud reconstruction of the surrounding environment, while the task of 3D object bounding box detection in real time remains a strong algorithmic challenge. In this paper, we build on the success of the one-shot regression meta-architecture in the 2D perspective image space and extend it to generate oriented 3D object bounding boxes from LiDAR point cloud. Our main contribution is in extending the loss function of YOLO v2 to include the yaw angle, the 3D box center in Cartesian coordinates and the height of the box as a direct regression problem. This formulation enables real-time performance, which is essential for automated driving. Our results are showing promising figures on KITTI benchmark, achieving real-time performance (40 fps) on Titan X GPU.



### Capturing global spatial context for accurate cell classification in skin cancer histology
- **Arxiv ID**: http://arxiv.org/abs/1808.02355v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.02355v1)
- **Published**: 2018-08-07 13:31:09+00:00
- **Updated**: 2018-08-07 13:31:09+00:00
- **Authors**: Konstantinos Zormpas-Petridis, Henrik Failmezger, Ioannis Roxanis, Matthew Blackledge, Yann Jamin, Yinyin Yuan
- **Comment**: Accepted by MICCAI COMPAY 2018 workshop
- **Journal**: None
- **Summary**: The spectacular response observed in clinical trials of immunotherapy in patients with previously uncurable Melanoma, a highly aggressive form of skin cancer, calls for a better understanding of the cancer-immune interface. Computational pathology provides a unique opportunity to spatially dissect such interface on digitised pathological slides. Accurate cellular classification is a key to ensure meaningful results, but is often challenging even with state-of-art machine learning and deep learning methods.   We propose a hierarchical framework, which mirrors the way pathologists perceive tumour architecture and define tumour heterogeneity to improve cell classification methods that rely solely on cell nuclei morphology. The SLIC superpixel algorithm was used to segment and classify tumour regions in low resolution H&E-stained histological images of melanoma skin cancer to provide a global context. Classification of superpixels into tumour, stroma, epidermis and lumen/white space, yielded a 97.7% training set accuracy and 95.7% testing set accuracy in 58 whole-tumour images of the TCGA melanoma dataset. The superpixel classification was projected down to high resolution images to enhance the performance of a single cell classifier, based on cell nuclear morphological features, and resulted in increasing its accuracy from 86.4% to 91.6%. Furthermore, a voting scheme was proposed to use global context as biological a priori knowledge, pushing the accuracy further to 92.8%.   This study demonstrates how using the global spatial context can accurately characterise the tumour microenvironment and allow us to extend significantly beyond single-cell morphological classification.



### Spinal Cord Gray Matter-White Matter Segmentation on Magnetic Resonance AMIRA Images with MD-GRU
- **Arxiv ID**: http://arxiv.org/abs/1808.02408v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.02408v1)
- **Published**: 2018-08-07 14:53:49+00:00
- **Updated**: 2018-08-07 14:53:49+00:00
- **Authors**: Antal Horvath, Charidimos Tsagkas, Simon Andermatt, Simon Pezold, Katrin Parmar, Philippe Cattin
- **Comment**: MICCAI 2018 CSI workshop, 12 pages, 7 figures, 3 tables
- **Journal**: None
- **Summary**: The small butterfly shaped structure of spinal cord (SC) gray matter (GM) is challenging to image and to delinate from its surrounding white matter (WM). Segmenting GM is up to a point a trade-off between accuracy and precision. We propose a new pipeline for GM-WM magnetic resonance (MR) image acquisition and segmentation. We report superior results as compared to the ones recently reported in the SC GM segmentation challenge and show even better results using the averaged magnetization inversion recovery acquisitions (AMIRA) sequence. Scan-rescan experiments with the AMIRA sequence show high reproducibility in terms of Dice coefficient, Hausdorff distance and relative standard deviation. We use a recurrent neural network (RNN) with multi-dimensional gated recurrent units (MD-GRU) to train segmentation models on the AMIRA dataset of 855 slices. We added a generalized dice loss to the cross entropy loss that MD-GRU uses and were able to improve the results.



### Fast and Accurate Camera Covariance Computation for Large 3D Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1808.02414v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.02414v1)
- **Published**: 2018-08-07 15:14:24+00:00
- **Updated**: 2018-08-07 15:14:24+00:00
- **Authors**: Michal Polic, Wolfgang Förstner, Tomas Pajdla
- **Comment**: ECCV 2018
- **Journal**: None
- **Summary**: Estimating uncertainty of camera parameters computed in Structure from Motion (SfM) is an important tool for evaluating the quality of the reconstruction and guiding the reconstruction process. Yet, the quality of the estimated parameters of large reconstructions has been rarely evaluated due to the computational challenges. We present a new algorithm which employs the sparsity of the uncertainty propagation and speeds the computation up about ten times \wrt previous approaches. Our computation is accurate and does not use any approximations. We can compute uncertainties of thousands of cameras in tens of seconds on a standard PC. We also demonstrate that our approach can be effectively used for reconstructions of any size by applying it to smaller sub-reconstructions.



### Overhead Detection: Beyond 8-bits and RGB
- **Arxiv ID**: http://arxiv.org/abs/1808.02443v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.02443v1)
- **Published**: 2018-08-07 16:12:02+00:00
- **Updated**: 2018-08-07 16:12:02+00:00
- **Authors**: Eliza Mace, Keith Manville, Monica Barbu-McInnis, Michael Laielli, Matthew Klaric, Samuel Dooley
- **Comment**: 10 pages, 8 figures, 2 tables
- **Journal**: Naval Applications of Machine Learning, February 13, 2018
- **Summary**: This study uses the challenging and publicly available SpaceNet dataset to establish a performance baseline for a state-of-the-art object detector in satellite imagery. Specifically, we examine how various features of the data affect building detection accuracy with respect to the Intersection over Union metric. We demonstrate that the performance of the R-FCN detection algorithm on imagery with a 1.5 meter ground sample distance and three spectral bands increases by over 32% by using 13-bit data, as opposed to 8-bit data at the same spatial and spectral resolution. We also establish accuracy trends with respect to building size and scene density. Finally, we propose and evaluate multiple methods for integrating additional spectral information into off-the-shelf deep learning architectures. Interestingly, our methods are robust to the choice of spectral bands and we note no significant performance improvement when adding additional bands.



### Data augmentation using synthetic data for time series classification with deep residual networks
- **Arxiv ID**: http://arxiv.org/abs/1808.02455v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1808.02455v1)
- **Published**: 2018-08-07 16:48:21+00:00
- **Updated**: 2018-08-07 16:48:21+00:00
- **Authors**: Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber, Lhassane Idoumghar, Pierre-Alain Muller
- **Comment**: Accepted at AALTD'18 workshop in ECML/PKDD
- **Journal**: None
- **Summary**: Data augmentation in deep neural networks is the process of generating artificial data in order to reduce the variance of the classifier with the goal to reduce the number of errors. This idea has been shown to improve deep neural network's generalization capabilities in many computer vision tasks such as image recognition and object localization. Apart from these applications, deep Convolutional Neural Networks (CNNs) have also recently gained popularity in the Time Series Classification (TSC) community. However, unlike in image recognition problems, data augmentation techniques have not yet been investigated thoroughly for the TSC task. This is surprising as the accuracy of deep learning models for TSC could potentially be improved, especially for small datasets that exhibit overfitting, when a data augmentation method is adopted. In this paper, we fill this gap by investigating the application of a recently proposed data augmentation technique based on the Dynamic Time Warping distance, for a deep learning model for TSC. To evaluate the potential of augmenting the training set, we performed extensive experiments using the UCR TSC benchmark. Our preliminary experiments reveal that data augmentation can drastically increase deep CNN's accuracy on some datasets and significantly improve the deep model's accuracy when the method is used in an ensemble approach.



### SketchyScene: Richly-Annotated Scene Sketches
- **Arxiv ID**: http://arxiv.org/abs/1808.02473v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1808.02473v1)
- **Published**: 2018-08-07 17:47:55+00:00
- **Updated**: 2018-08-07 17:47:55+00:00
- **Authors**: Changqing Zou, Qian Yu, Ruofei Du, Haoran Mo, Yi-Zhe Song, Tao Xiang, Chengying Gao, Baoquan Chen, Hao Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: We contribute the first large-scale dataset of scene sketches, SketchyScene, with the goal of advancing research on sketch understanding at both the object and scene level. The dataset is created through a novel and carefully designed crowdsourcing pipeline, enabling users to efficiently generate large quantities of realistic and diverse scene sketches. SketchyScene contains more than 29,000 scene-level sketches, 7,000+ pairs of scene templates and photos, and 11,000+ object sketches. All objects in the scene sketches have ground-truth semantic and instance masks. The dataset is also highly scalable and extensible, easily allowing augmenting and/or changing scene composition. We demonstrate the potential impact of SketchyScene by training new computational models for semantic segmentation of scene sketches and showing how the new dataset enables several applications including image retrieval, sketch colorization, editing, and captioning, etc. The dataset and code can be found at https://github.com/SketchyScene/SketchyScene.



### Multi-Label Zero-Shot Learning with Transfer-Aware Label Embedding Projection
- **Arxiv ID**: http://arxiv.org/abs/1808.02474v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1808.02474v1)
- **Published**: 2018-08-07 17:48:40+00:00
- **Updated**: 2018-08-07 17:48:40+00:00
- **Authors**: Meng Ye, Yuhong Guo
- **Comment**: None
- **Journal**: None
- **Summary**: Zero-shot learning transfers knowledge from seen classes to novel unseen classes to reduce human labor of labelling data for building new classifiers. Much effort on zero-shot learning however has focused on the standard multi-class setting, the more challenging multi-label zero-shot problem has received limited attention. In this paper we propose a transfer-aware embedding projection approach to tackle multi-label zero-shot learning. The approach projects the label embedding vectors into a low-dimensional space to induce better inter-label relationships and explicitly facilitate information transfer from seen labels to unseen labels, while simultaneously learning a max-margin multi-label classifier with the projected label embeddings. Auxiliary information can be conveniently incorporated to guide the label embedding projection to further improve label relation structures for zero-shot knowledge transfer. We conduct experiments for zero-shot multi-label image classification. The results demonstrate the efficacy of the proposed approach.



### Detection and Segmentation of Manufacturing Defects with Convolutional Neural Networks and Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/1808.02518v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.02518v2)
- **Published**: 2018-08-07 18:57:41+00:00
- **Updated**: 2018-09-03 02:39:44+00:00
- **Authors**: Max Ferguson, Ronay Ak, Yung-Tsun Tina Lee, Kincho H. Law
- **Comment**: None
- **Journal**: None
- **Summary**: Quality control is a fundamental component of many manufacturing processes, especially those involving casting or welding. However, manual quality control procedures are often time-consuming and error-prone. In order to meet the growing demand for high-quality products, the use of intelligent visual inspection systems is becoming essential in production lines. Recently, Convolutional Neural Networks (CNNs) have shown outstanding performance in both image classification and localization tasks. In this article, a system is proposed for the identification of casting defects in X-ray images, based on the Mask Region-based CNN architecture. The proposed defect detection system simultaneously performs defect detection and segmentation on input images, making it suitable for a range of defect detection tasks. It is shown that training the network to simultaneously perform defect detection and defect instance segmentation, results in a higher defect detection accuracy than training on defect detection alone. Transfer learning is leveraged to reduce the training data demands and increase the prediction accuracy of the trained model. More specifically, the model is first trained with two large openly-available image datasets before finetuning on a relatively small metal casting X-ray dataset. The accuracy of the trained model exceeds state-of-the art performance on the GRIMA database of X-ray images (GDXray) Castings dataset and is fast enough to be used in a production setting. The system also performs well on the GDXray Welds dataset. A number of in-depth studies are conducted to explore how transfer learning, multi-task learning, and multi-class learning influence the performance of the trained system.



### SchiNet: Automatic Estimation of Symptoms of Schizophrenia from Facial Behaviour Analysis
- **Arxiv ID**: http://arxiv.org/abs/1808.02531v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.02531v1)
- **Published**: 2018-08-07 19:31:11+00:00
- **Updated**: 2018-08-07 19:31:11+00:00
- **Authors**: Mina Bishay, Petar Palasek, Stefan Priebe, Ioannis Patras
- **Comment**: 13 pages, IEEE Transactions on Affective Computing
- **Journal**: None
- **Summary**: Patients with schizophrenia often display impairments in the expression of emotion and speech and those are observed in their facial behaviour. Automatic analysis of patients' facial expressions that is aimed at estimating symptoms of schizophrenia has received attention recently. However, the datasets that are typically used for training and evaluating the developed methods, contain only a small number of patients (4-34) and are recorded while the subjects were performing controlled tasks such as listening to life vignettes, or answering emotional questions. In this paper, we use videos of professional-patient interviews, in which symptoms were assessed in a standardised way as they should/may be assessed in practice, and which were recorded in realistic conditions (i.e. varying illumination levels and camera viewpoints) at the patients' homes or at mental health services. We automatically analyse the facial behaviour of 91 out-patients - this is almost 3 times the number of patients in other studies - and propose SchiNet, a novel neural network architecture that estimates expression-related symptoms in two different assessment interviews. We evaluate the proposed SchiNet for patient-independent prediction of symptoms of schizophrenia. Experimental results show that some automatically detected facial expressions are significantly correlated to symptoms of schizophrenia, and that the proposed network for estimating symptom severity delivers promising results.



### Dynamic Temporal Pyramid Network: A Closer Look at Multi-Scale Modeling for Activity Detection
- **Arxiv ID**: http://arxiv.org/abs/1808.02536v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.02536v2)
- **Published**: 2018-08-07 20:02:36+00:00
- **Updated**: 2019-02-15 22:31:35+00:00
- **Authors**: Da Zhang, Xiyang Dai, Yuan-Fang Wang
- **Comment**: ACCV 2018 Oral
- **Journal**: None
- **Summary**: Recognizing instances at different scales simultaneously is a fundamental challenge in visual detection problems. While spatial multi-scale modeling has been well studied in object detection, how to effectively apply a multi-scale architecture to temporal models for activity detection is still under-explored. In this paper, we identify three unique challenges that need to be specifically handled for temporal activity detection compared to its spatial counterpart. To address all these issues, we propose Dynamic Temporal Pyramid Network (DTPN), a new activity detection framework with a multi-scale pyramidal architecture featuring three novel designs: (1) We sample input video frames dynamically with varying frame per seconds (FPS) to construct a natural pyramidal input for video of an arbitrary length. (2) We design a two-branch multi-scale temporal feature hierarchy to deal with the inherent temporal scale variation of activity instances. (3) We further exploit the temporal context of activities by appropriately fusing multi-scale feature maps, and demonstrate that both local and global temporal contexts are important. By combining all these components into a uniform network, we end up with a single-shot activity detector involving single-pass inferencing and end-to-end training. Extensive experiments show that the proposed DTPN achieves state-of-the-art performance on the challenging ActvityNet dataset.



### A Joint Sequence Fusion Model for Video Question Answering and Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1808.02559v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.02559v1)
- **Published**: 2018-08-07 21:33:37+00:00
- **Updated**: 2018-08-07 21:33:37+00:00
- **Authors**: Youngjae Yu, Jongseok Kim, Gunhee Kim
- **Comment**: To appear in ECCV 2018. 17 pages
- **Journal**: None
- **Summary**: We present an approach named JSFusion (Joint Sequence Fusion) that can measure semantic similarity between any pairs of multimodal sequence data (e.g. a video clip and a language sentence). Our multimodal matching network consists of two key components. First, the Joint Semantic Tensor composes a dense pairwise representation of two sequence data into a 3D tensor. Then, the Convolutional Hierarchical Decoder computes their similarity score by discovering hidden hierarchical matches between the two sequence modalities. Both modules leverage hierarchical attention mechanisms that learn to promote well-matched representation patterns while prune out misaligned ones in a bottom-up manner. Although the JSFusion is a universal model to be applicable to any multimodal sequence data, this work focuses on video-language tasks including multimodal retrieval and video QA. We evaluate the JSFusion model in three retrieval and VQA tasks in LSMDC, for which our model achieves the best performance reported so far. We also perform multiple-choice and movie retrieval tasks for the MSR-VTT dataset, on which our approach outperforms many state-of-the-art methods.



### Image Anomalies: a Review and Synthesis of Detection Methods
- **Arxiv ID**: http://arxiv.org/abs/1808.02564v2
- **DOI**: 10.1007/s10851-019-00885-0
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.02564v2)
- **Published**: 2018-08-07 22:06:44+00:00
- **Updated**: 2019-06-03 08:51:44+00:00
- **Authors**: Thibaud Ehret, Axel Davy, Jean-Michel Morel, Mauricio Delbracio
- **Comment**: Thibaud Ehret and Axel Davy contributed equally to this work
- **Journal**: None
- **Summary**: We review the broad variety of methods that have been proposed for anomaly detection in images. Most methods found in the literature have in mind a particular application. Yet we show that the methods can be classified mainly by the structural assumption they make on the "normal" image. Five different structural assumptions emerge. Our analysis leads us to reformulate the best representative algorithms by attaching to them an a contrario detection that controls the number of false positives and thus derive universal detection thresholds. By combining the most general structural assumptions expressing the background's normality with the best proposed statistical detection tools, we end up proposing generic algorithms that seem to generalize or reconcile most methods. We compare the six best representatives of our proposed classes of algorithms on anomalous images taken from classic papers on the subject, and on a synthetic database. Our conclusion is that it is possible to perform automatic anomaly detection on a single image.



