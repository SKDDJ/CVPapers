# Arxiv Papers in cs.CV on 2018-08-17
### Ensemble-based Adaptive Single-shot Multi-box Detector
- **Arxiv ID**: http://arxiv.org/abs/1808.05727v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.05727v1)
- **Published**: 2018-08-17 02:02:42+00:00
- **Updated**: 2018-08-17 02:02:42+00:00
- **Authors**: Viral Thakar, Walid Ahmed, Mohammad M Soltani, Jia Yuan Yu
- **Comment**: 6 pages, 2 figures, to appear in the Proceedings of the ISNCC 2018,
  19-21 June 2018, Rome, Italy
- **Journal**: None
- **Summary**: We propose two improvements to the SSD---single shot multibox detector. First, we propose an adaptive approach for default box selection in SSD. This uses data to reduce the uncertainty in the selection of best aspect ratios for the default boxes and improves performance of SSD for datasets containing small and complex objects (e.g., equipments at construction sites). We do so by finding the distribution of aspect ratios of the given training dataset, and then choosing representative values. Secondly, we propose an ensemble algorithm, using SSD as components, which improves the performance of SSD, especially for small amount of training datasets. Compared to the conventional SSD algorithm, adaptive box selection improves mean average precision by 3%, while ensemble-based SSD improves it by 8%.



### Efficient Single-Shot Multibox Detector for Construction Site Monitoring
- **Arxiv ID**: http://arxiv.org/abs/1808.05730v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.05730v2)
- **Published**: 2018-08-17 02:19:31+00:00
- **Updated**: 2018-08-20 00:23:29+00:00
- **Authors**: Viral Thakar, Himani Saini, Walid Ahmed, Mohammad M Soltani, Ahmed Aly, Jia Yuan Yu
- **Comment**: 6 pages, 4 figures, to appear in the Proceedings of the ISC2 2018,
  16-19 September 2018, Kansas, USA
- **Journal**: None
- **Summary**: Asset monitoring in construction sites is an intricate, manually intensive task, that can highly benefit from automated solutions engineered using deep neural networks. We use Single-Shot Multibox Detector --- SSD, for its fine balance between speed and accuracy, to leverage ubiquitously available images and videos from the surveillance cameras on the construction sites and automate the monitoring tasks, hence enabling project managers to better track the performance and optimize the utilization of each resource. We propose to improve the performance of SSD by clustering the predicted boxes instead of a greedy approach like non-maximum suppression. We do so using Affinity Propagation Clustering --- APC to cluster the predicted boxes based on the similarity index computed using the spatial features as well as location of predicted boxes. In our attempts, we have been able to improve the mean average precision of SSD by 3.77% on custom dataset consist of images from construction sites and by 1.67% on PASCAL VOC Challenge.



### Bitstream-Based JPEG Image Encryption with File-Size Preserving
- **Arxiv ID**: http://arxiv.org/abs/1808.06495v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1808.06495v1)
- **Published**: 2018-08-17 02:37:24+00:00
- **Updated**: 2018-08-17 02:37:24+00:00
- **Authors**: Hiroyuki Kobayashi, Hitoshi Kiya
- **Comment**: to appear in 2018 IEEE 7th Global Conference on Consumer Electronics,
  Nara, JAPAN, 10th Oct., 2018
- **Journal**: None
- **Summary**: An encryption scheme of JPEG images in the bitstream domain is proposed. The proposed scheme preserves the JPEG format even after encrypting the images, and the file size of encrypted images is the exact same as that of the original JPEG images. Several methods for encrypting JPEG images in the bitstream domain have been proposed. However, since some marker codes are generated or lost in the encryption process, the file size of JPEG bitstreams is generally changed due to the encryption operations. The proposed method inputs JPEG bitstreams and selectively encrypts the additional bit components of the Huffman code in the bitstreams. This feature allows us to have encrypted images with the same data size as that recoded in the image transmission process, when JPEG images are replaced with the encrypted ones by the hooking, so that the image transmission are successfully carried out after the hooking.



### Medical Image Imputation from Image Collections
- **Arxiv ID**: http://arxiv.org/abs/1808.05732v1
- **DOI**: 10.1109/TMI.2018.2866692
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.05732v1)
- **Published**: 2018-08-17 02:46:33+00:00
- **Updated**: 2018-08-17 02:46:33+00:00
- **Authors**: Adrian V. Dalca, Katherine L. Bouman, William T. Freeman, Natalia S. Rost, Mert R. Sabuncu, Polina Golland
- **Comment**: Accepted at IEEE Transactions on Medical Imaging (\c{opyright} 2018
  IEEE)
- **Journal**: None
- **Summary**: We present an algorithm for creating high resolution anatomically plausible images consistent with acquired clinical brain MRI scans with large inter-slice spacing. Although large data sets of clinical images contain a wealth of information, time constraints during acquisition result in sparse scans that fail to capture much of the anatomy. These characteristics often render computational analysis impractical as many image analysis algorithms tend to fail when applied to such images. Highly specialized algorithms that explicitly handle sparse slice spacing do not generalize well across problem domains. In contrast, we aim to enable application of existing algorithms that were originally developed for high resolution research scans to significantly undersampled scans. We introduce a generative model that captures fine-scale anatomical structure across subjects in clinical image collections and derive an algorithm for filling in the missing data in scans with large inter-slice spacing. Our experimental results demonstrate that the resulting method outperforms state-of-the-art upsampling super-resolution techniques, and promises to facilitate subsequent analysis not previously possible with scans of this quality. Our implementation is freely available at https://github.com/adalca/papago .



### Convolutional Neural Networks based Intra Prediction for HEVC
- **Arxiv ID**: http://arxiv.org/abs/1808.05734v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.05734v1)
- **Published**: 2018-08-17 02:50:49+00:00
- **Updated**: 2018-08-17 02:50:49+00:00
- **Authors**: Wenxue Cui, Tao Zhang, Shengping Zhang, Feng Jiang, Wangmeng Zuo, Debin Zhao
- **Comment**: 10 pages, This is the extended edition of poster paper accepted by
  DCC 2017
- **Journal**: None
- **Summary**: Traditional intra prediction methods for HEVC rely on using the nearest reference lines for predicting a block, which ignore much richer context between the current block and its neighboring blocks and therefore cause inaccurate prediction especially when weak spatial correlation exists between the current block and the reference lines. To overcome this problem, in this paper, an intra prediction convolutional neural network (IPCNN) is proposed for intra prediction, which exploits the rich context of the current block and therefore is capable of improving the accuracy of predicting the current block. Meanwhile, the predictions of the three nearest blocks can also be refined. To the best of our knowledge, this is the first paper that directly applies CNNs to intra prediction for HEVC. Experimental results validate the effectiveness of applying CNNs to intra prediction and achieved significant performance improvement compared to traditional intra prediction methods.



### Dynamic Routing on Deep Neural Network for Thoracic Disease Classification and Sensitive Area Localization
- **Arxiv ID**: http://arxiv.org/abs/1808.05744v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.05744v1)
- **Published**: 2018-08-17 04:00:25+00:00
- **Updated**: 2018-08-17 04:00:25+00:00
- **Authors**: Yan Shen, Mingchen Gao
- **Comment**: None
- **Journal**: MLMI 2018
- **Summary**: We present and evaluate a new deep neural network architecture for automatic thoracic disease detection on chest X-rays. Deep neural networks have shown great success in a plethora of visual recognition tasks such as image classification and object detection by stacking multiple layers of convolutional neural networks (CNN) in a feed-forward manner. However, the performance gain by going deeper has reached bottlenecks as a result of the trade-off between model complexity and discrimination power. We address this problem by utilizing the recently developed routing-by agreement mechanism in our architecture. A novel characteristic of our network structure is that it extends routing to two types of layer connections (1) connection between feature maps in dense layers, (2) connection between primary capsules and prediction capsules in final classification layer. We show that our networks achieve comparable results with much fewer layers in the measurement of AUC score. We further show the combined benefits of model interpretability by generating Gradient-weighted Class Activation Mapping (Grad-CAM) for localization. We demonstrate our results on the NIH chestX-ray14 dataset that consists of 112,120 images on 30,805 unique patients including 14 kinds of lung diseases.



### Learning to Quantize Deep Networks by Optimizing Quantization Intervals with Task Loss
- **Arxiv ID**: http://arxiv.org/abs/1808.05779v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.05779v3)
- **Published**: 2018-08-17 07:28:17+00:00
- **Updated**: 2018-11-23 02:18:04+00:00
- **Authors**: Sangil Jung, Changyong Son, Seohyung Lee, Jinwoo Son, Youngjun Kwak, Jae-Joon Han, Sung Ju Hwang, Changkyu Choi
- **Comment**: None
- **Journal**: None
- **Summary**: Reducing bit-widths of activations and weights of deep networks makes it efficient to compute and store them in memory, which is crucial in their deployments to resource-limited devices, such as mobile phones. However, decreasing bit-widths with quantization generally yields drastically degraded accuracy. To tackle this problem, we propose to learn to quantize activations and weights via a trainable quantizer that transforms and discretizes them. Specifically, we parameterize the quantization intervals and obtain their optimal values by directly minimizing the task loss of the network. This quantization-interval-learning (QIL) allows the quantized networks to maintain the accuracy of the full-precision (32-bit) networks with bit-width as low as 4-bit and minimize the accuracy degeneration with further bit-width reduction (i.e., 3 and 2-bit). Moreover, our quantizer can be trained on a heterogeneous dataset, and thus can be used to quantize pretrained networks without access to their training data. We demonstrate the effectiveness of our trainable quantizer on ImageNet dataset with various network architectures such as ResNet-18, -34 and AlexNet, on which it outperforms existing methods to achieve the state-of-the-art accuracy.



### Towards Robotic Eye Surgery: Marker-free, Online Hand-eye Calibration using Optical Coherence Tomography Images
- **Arxiv ID**: http://arxiv.org/abs/1808.05805v1
- **DOI**: 10.1109/LRA.2018.2858744
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1808.05805v1)
- **Published**: 2018-08-17 09:19:40+00:00
- **Updated**: 2018-08-17 09:19:40+00:00
- **Authors**: Mingchuan Zhou, Mahdi Hamad, Jakob Weiss, Abouzar Eslami, Kai Huang, Mathias Maier, Chris P. Lohmann, Nassir Navab, Alois Knoll, M. Ali Nasseri
- **Comment**: *The first two authors contributed equally to this paper. Accepted by
  IEEE Robotics and Automation Letters (RA-L), 2018
- **Journal**: None
- **Summary**: Ophthalmic microsurgery is known to be a challenging operation, which requires very precise and dexterous manipulation. Image guided robot-assisted surgery (RAS) is a promising solution that brings significant improvements in outcomes and reduces the physical limitations of human surgeons. However, this technology must be further developed before it can be routinely used in clinics. One of the problems is the lack of proper calibration between the robotic manipulator and appropriate imaging device. In this work, we developed a flexible framework for hand-eye calibration of an ophthalmic robot with a microscope-integrated Optical Coherence Tomography (MIOCT) without any markers. The proposed method consists of three main steps: a) we estimate the OCT calibration parameters; b) with micro-scale displacements controlled by the robot, we detect and segment the needle tip in 3D-OCT volume; c) we find the transformation between the coordinate system of the OCT camera and the coordinate system of the robot. We verified the capability of our framework in ex-vivo pig eye experiments and compared the results with a reference method (marker-based). In all experiments, our method showed a small difference from the marker based method, with a mean calibration error of 9.2 $\mu$m and 7.0 $\mu$m, respectively. Additionally, the noise test shows the robustness of the proposed method.



### Uncertainty-aware Short-term Motion Prediction of Traffic Actors for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1808.05819v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1808.05819v3)
- **Published**: 2018-08-17 10:37:51+00:00
- **Updated**: 2020-03-04 06:35:56+00:00
- **Authors**: Nemanja Djuric, Vladan Radosavljevic, Henggang Cui, Thi Nguyen, Fang-Chieh Chou, Tsung-Han Lin, Nitin Singh, Jeff Schneider
- **Comment**: Accepted for publication at IEEE Winter Conference on Applications of
  Computer Vision (WACV) 2020
- **Journal**: None
- **Summary**: We address one of the crucial aspects necessary for safe and efficient operations of autonomous vehicles, namely predicting future state of traffic actors in the autonomous vehicle's surroundings. We introduce a deep learning-based approach that takes into account a current world state and produces raster images of each actor's vicinity. The rasters are then used as inputs to deep convolutional models to infer future movement of actors while also accounting for and capturing inherent uncertainty of the prediction task. Extensive experiments on real-world data strongly suggest benefits of the proposed approach. Moreover, following completion of the offline tests the system was successfully tested onboard self-driving vehicles.



### Improving Breast Cancer Detection using Symmetry Information with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1808.08273v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.08273v1)
- **Published**: 2018-08-17 11:04:42+00:00
- **Updated**: 2018-08-17 11:04:42+00:00
- **Authors**: Yeman Brhane Hagos, Albert Gubern Merida, Jonas Teuwen
- **Comment**: 8 pages, 7 figures, accepted in MICCAI 2018 Breast Image Analysis
  (BIA)
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNN) have had a huge success in many areas of computer vision and medical image analysis. However, there is still an immense potential for performance improvement in mammogram breast cancer detection Computer-Aided Detection (CAD) systems by integrating all the information that the radiologist utilizes, such as symmetry and temporal data. In this work, we proposed a patch based multi-input CNN that learns symmetrical difference to detect breast masses. The network was trained on a large-scale dataset of 28294 mammogram images. The performance was compared to a baseline architecture without symmetry context using Area Under the ROC Curve (AUC) and Competition Performance Metric (CPM). At candidate level, AUC value of 0.933 with 95% confidence interval of [0.920, 0.954] was obtained when symmetry information is incorporated in comparison with baseline architecture which yielded AUC value of 0.929 with [0.919, 0.947] confidence interval. By incorporating symmetrical information, although there was no a significant candidate level performance again (p = 0.111), we have found a compelling result at exam level with CPM value of 0.733 (p = 0.001). We believe that including temporal data, and adding benign class to the dataset could improve the detection performance.



### Performance Analysis and Robustification of Single-query 6-DoF Camera Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1808.05848v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.05848v1)
- **Published**: 2018-08-17 13:21:24+00:00
- **Updated**: 2018-08-17 13:21:24+00:00
- **Authors**: Junsheng Fu, Said Pertuz, Jiri Matas, Joni-Kristian Kämäräinen
- **Comment**: None
- **Journal**: None
- **Summary**: We consider a single-query 6-DoF camera pose estimation with reference images and a point cloud, i.e. the problem of estimating the position and orientation of a camera by using reference images and a point cloud. In this work, we perform a systematic comparison of three state-of-the-art strategies for 6-DoF camera pose estimation, i.e. feature-based, photometric-based and mutual-information-based approaches. The performance of the studied methods is evaluated on two standard datasets in terms of success rate, translation error and max orientation error. Building on the results analysis, we propose a hybrid approach that combines feature-based and mutual-information-based pose estimation methods since it provides complementary properties for pose estimation. Experiments show that (1) in cases with large environmental variance, the hybrid approach outperforms feature-based and mutual-information-based approaches by an average of 25.1% and 5.8% in terms of success rate, respectively; (2) in cases where query and reference images are captured at similar imaging conditions, the hybrid approach performs similarly as the feature-based approach, but outperforms both photometric-based and mutual-information-based approaches with a clear margin; (3) the feature-based approach is consistently more accurate than mutual-information-based and photometric-based approaches when at least 4 consistent matching points are found between the query and reference images.



### Lifted Wasserstein Matcher for Fast and Robust Topology Tracking
- **Arxiv ID**: http://arxiv.org/abs/1808.05870v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CG, cs.CV, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1808.05870v4)
- **Published**: 2018-08-17 13:54:05+00:00
- **Updated**: 2019-01-02 08:02:12+00:00
- **Authors**: Maxime Soler, Mélanie Plainchault, Bruno Conche, Julien Tierny
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a robust and efficient method for tracking topological features in time-varying scalar data. Structures are tracked based on the optimal matching between persistence diagrams with respect to the Wasserstein metric. This fundamentally relies on solving the assignment problem, a special case of optimal transport, for all consecutive timesteps. Our approach relies on two main contributions. First, we revisit the seminal assignment algorithm by Kuhn and Munkres which we specifically adapt to the problem of matching persistence diagrams in an efficient way. Second, we propose an extension of the Wasserstein metric that significantly improves the geometrical stability of the matching of domain-embedded persistence pairs. We show that this geometrical lifting has the additional positive side-effect of improving the assignment matrix sparsity and therefore computing time. The global framework implements a coarse-grained parallelism by computing persistence diagrams and finding optimal matchings in parallel for every couple of consecutive timesteps. Critical trajectories are constructed by associating successively matched persistence pairs over time. Merging and splitting events are detected with a geometrical threshold in a post-processing stage. Extensive experiments on real-life datasets show that our matching approach is an order of magnitude faster than the seminal Munkres algorithm. Moreover, compared to a modern approximation method, our method provides competitive runtimes while yielding exact results. We demonstrate the utility of our global framework by extracting critical point trajectories from various simulated time-varying datasets and compare it to the existing methods based on associated overlaps of volumes. Robustness to noise and temporal resolution downsampling is empirically demonstrated.



### Epithelium segmentation using deep learning in H&E-stained prostate specimens with immunohistochemistry as reference standard
- **Arxiv ID**: http://arxiv.org/abs/1808.05883v2
- **DOI**: 10.1038/s41598-018-37257-4
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.05883v2)
- **Published**: 2018-08-17 14:36:43+00:00
- **Updated**: 2019-02-08 13:28:49+00:00
- **Authors**: Wouter Bulten, Péter Bándi, Jeffrey Hoven, Rob van de Loo, Johannes Lotz, Nick Weiss, Jeroen van der Laak, Bram van Ginneken, Christina Hulsbergen-van de Kaa, Geert Litjens
- **Comment**: None
- **Journal**: Nature Scientific Reports 9, 864 (2019)
- **Summary**: Prostate cancer (PCa) is graded by pathologists by examining the architectural pattern of cancerous epithelial tissue on hematoxylin and eosin (H&E) stained slides. Given the importance of gland morphology, automatically differentiating between glandular epithelial tissue and other tissues is an important prerequisite for the development of automated methods for detecting PCa. We propose a new method, using deep learning, for automatically segmenting epithelial tissue in digitized prostatectomy slides. We employed immunohistochemistry (IHC) to render the ground truth less subjective and more precise compared to manual outlining on H&E slides, especially in areas with high-grade and poorly differentiated PCa. Our dataset consisted of 102 tissue blocks, including both low and high grade PCa. From each block a single new section was cut, stained with H&E, scanned, restained using P63 and CK8/18 to highlight the epithelial structure, and scanned again. The H&E slides were co-registered to the IHC slides. On a subset of the IHC slides we applied color deconvolution, corrected stain errors manually, and trained a U-Net to perform segmentation of epithelial structures. Whole-slide segmentation masks generated by the IHC U-Net were used to train a second U-Net on H&E. Our system makes precise cell-level segmentations and segments both intact glands as well as individual (tumor) epithelial cells. We achieved an F1-score of 0.895 on a hold-out test set and 0.827 on an external reference set from a different center. We envision this segmentation as being the first part of a fully automated prostate cancer detection and grading pipeline.



### Whole-Slide Mitosis Detection in H&E Breast Histology Using PHH3 as a Reference to Train Distilled Stain-Invariant Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1808.05896v1
- **DOI**: 10.1109/TMI.2018.2820199
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.05896v1)
- **Published**: 2018-08-17 15:14:20+00:00
- **Updated**: 2018-08-17 15:14:20+00:00
- **Authors**: David Tellez, Maschenka Balkenhol, Irene Otte-Holler, Rob van de Loo, Rob Vogels, Peter Bult, Carla Wauters, Willem Vreuls, Suzanne Mol, Nico Karssemeijer, Geert Litjens, Jeroen van der Laak, Francesco Ciompi
- **Comment**: Accepted to appear in IEEE Transactions on Medical Imaging
- **Journal**: None
- **Summary**: Manual counting of mitotic tumor cells in tissue sections constitutes one of the strongest prognostic markers for breast cancer. This procedure, however, is time-consuming and error-prone. We developed a method to automatically detect mitotic figures in breast cancer tissue sections based on convolutional neural networks (CNNs). Application of CNNs to hematoxylin and eosin (H&E) stained histological tissue sections is hampered by: (1) noisy and expensive reference standards established by pathologists, (2) lack of generalization due to staining variation across laboratories, and (3) high computational requirements needed to process gigapixel whole-slide images (WSIs). In this paper, we present a method to train and evaluate CNNs to specifically solve these issues in the context of mitosis detection in breast cancer WSIs. First, by combining image analysis of mitotic activity in phosphohistone-H3 (PHH3) restained slides and registration, we built a reference standard for mitosis detection in entire H&E WSIs requiring minimal manual annotation effort. Second, we designed a data augmentation strategy that creates diverse and realistic H&E stain variations by modifying the hematoxylin and eosin color channels directly. Using it during training combined with network ensembling resulted in a stain invariant mitosis detector. Third, we applied knowledge distillation to reduce the computational requirements of the mitosis detection ensemble with a negligible loss of performance. The system was trained in a single-center cohort and evaluated in an independent multicenter cohort from The Cancer Genome Atlas on the three tasks of the Tumor Proliferation Assessment Challenge (TUPAC). We obtained a performance within the top-3 best methods for most of the tasks of the challenge.



### Learning Supervised Topic Models for Classification and Regression from Crowds
- **Arxiv ID**: http://arxiv.org/abs/1808.05902v1
- **DOI**: 10.1109/TPAMI.2017.2648786
- **Categories**: **stat.ML**, cs.CL, cs.CV, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1808.05902v1)
- **Published**: 2018-08-17 15:32:24+00:00
- **Updated**: 2018-08-17 15:32:24+00:00
- **Authors**: Filipe Rodrigues, Mariana Lourenço, Bernardete Ribeiro, Francisco Pereira
- **Comment**: 14 pages
- **Journal**: Rodrigues, F., Lourenco, M., Ribeiro, B. and Pereira, F.C., 2017.
  Learning supervised topic models for classification and regression from
  crowds. IEEE transactions on pattern analysis and machine intelligence,
  39(12), pp.2409-2422
- **Summary**: The growing need to analyze large collections of documents has led to great developments in topic modeling. Since documents are frequently associated with other related variables, such as labels or ratings, much interest has been placed on supervised topic models. However, the nature of most annotation tasks, prone to ambiguity and noise, often with high volumes of documents, deem learning under a single-annotator assumption unrealistic or unpractical for most real-world applications. In this article, we propose two supervised topic models, one for classification and another for regression problems, which account for the heterogeneity and biases among different annotators that are encountered in practice when learning from crowds. We develop an efficient stochastic variational inference algorithm that is able to scale to very large datasets, and we empirically demonstrate the advantages of the proposed model over state-of-the-art approaches.



### First Steps Toward CNN based Source Classification of Document Images Shared Over Messaging App
- **Arxiv ID**: http://arxiv.org/abs/1808.05941v1
- **DOI**: 10.1016/j.image.2019.05.020
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1808.05941v1)
- **Published**: 2018-08-17 17:49:39+00:00
- **Updated**: 2018-08-17 17:49:39+00:00
- **Authors**: Sharad Joshi, Suraj Saxena, Nitin Khanna
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Knowledge of source smartphone corresponding to a document image can be helpful in a variety of applications including copyright infringement, ownership attribution, leak identification and usage restriction. In this letter, we investigate a convolutional neural network-based approach to solve source smartphone identification problem for printed text documents which have been captured by smartphone cameras and shared over messaging platform. In absence of any publicly available dataset addressing this problem, we introduce a new image dataset consisting of 315 images of documents printed in three different fonts, captured using 21 smartphones and shared over WhatsApp. Experiments conducted on this dataset demonstrate that, in all scenarios, the proposed system performs as well as or better than the state-of-the-art system based on handcrafted features and classification of letters extracted from document images. The new dataset and code of the proposed system will be made publicly available along with this letter's publication, presently they are submitted for review.



### Neural Body Fitting: Unifying Deep Learning and Model-Based Human Pose and Shape Estimation
- **Arxiv ID**: http://arxiv.org/abs/1808.05942v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.05942v1)
- **Published**: 2018-08-17 17:56:10+00:00
- **Updated**: 2018-08-17 17:56:10+00:00
- **Authors**: Mohamed Omran, Christoph Lassner, Gerard Pons-Moll, Peter V. Gehler, Bernt Schiele
- **Comment**: 3DV 2018
- **Journal**: None
- **Summary**: Direct prediction of 3D body pose and shape remains a challenge even for highly parameterized deep learning models. Mapping from the 2D image space to the prediction space is difficult: perspective ambiguities make the loss function noisy and training data is scarce. In this paper, we propose a novel approach (Neural Body Fitting (NBF)). It integrates a statistical body model within a CNN, leveraging reliable bottom-up semantic body part segmentation and robust top-down body model constraints. NBF is fully differentiable and can be trained using 2D and 3D annotations. In detailed experiments, we analyze how the components of our model affect performance, especially the use of part segmentations as an explicit intermediate representation, and present a robust, efficiently trainable framework for 3D human pose estimation from 2D images with competitive results on standard benchmarks. Code will be made available at http://github.com/mohomran/neural_body_fitting



### On Geometric Analysis of Affine Sparse Subspace Clustering
- **Arxiv ID**: http://arxiv.org/abs/1808.05965v4
- **DOI**: 10.1109/JSTSP.2018.2867446
- **Categories**: **eess.SP**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1808.05965v4)
- **Published**: 2018-08-17 18:11:37+00:00
- **Updated**: 2018-11-21 07:39:36+00:00
- **Authors**: Chun-Guang Li, Chong You, René Vidal
- **Comment**: 15 pages, 6 figures, 2 tables. To appear on IEEE Journal of Selected
  Topics in Signal Processing
- **Journal**: None
- **Summary**: Sparse subspace clustering (SSC) is a state-of-the-art method for segmenting a set of data points drawn from a union of subspaces into their respective subspaces. It is now well understood that SSC produces subspace-preserving data affinity under broad geometric conditions but suffers from a connectivity issue. In this paper, we develop a novel geometric analysis for a variant of SSC, named affine SSC (ASSC), for the problem of clustering data from a union of affine subspaces. Our contributions include a new concept called affine independence for capturing the arrangement of a collection of affine subspaces. Under the affine independence assumption, we show that ASSC is guaranteed to produce subspace-preserving affinity. Moreover, inspired by the phenomenon that the $\ell_1$ regularization no longer induces sparsity when the solution is nonnegative, we further show that subspace-preserving recovery can be achieved under much weaker conditions for all data points other than the extreme points of samples from each subspace. In addition, we confirm a curious observation that the affinity produced by ASSC may be subspace-dense---which could guarantee the subspace-preserving affinity of ASSC to produce correct clustering under rather weak conditions. We validate the theoretical findings on carefully designed synthetic data and evaluate the performance of ASSC on several real data sets.



