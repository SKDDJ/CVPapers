# Arxiv Papers in cs.CV on 2018-08-03
### Where-and-When to Look: Deep Siamese Attention Networks for Video-based Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1808.01911v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.01911v2)
- **Published**: 2018-08-03 01:07:03+00:00
- **Updated**: 2018-10-14 04:09:48+00:00
- **Authors**: Lin Wu, Yang Wang, Junbin Gao, Xue Li
- **Comment**: Appearing in IEEE Transactions on Multimedia. arXiv admin note: text
  overlap with arXiv:1606.01609
- **Journal**: None
- **Summary**: Video-based person re-identification (re-id) is a central application in surveillance systems with significant concern in security. Matching persons across disjoint camera views in their video fragments is inherently challenging due to the large visual variations and uncontrolled frame rates. There are two steps crucial to person re-id, namely discriminative feature learning and metric learning. However, existing approaches consider the two steps independently, and they do not make full use of the temporal and spatial information in videos. In this paper, we propose a Siamese attention architecture that jointly learns spatiotemporal video representations and their similarity metrics. The network extracts local convolutional features from regions of each frame, and enhance their discriminative capability by focusing on distinct regions when measuring the similarity with another pedestrian video. The attention mechanism is embedded into spatial gated recurrent units to selectively propagate relevant features and memorize their spatial dependencies through the network. The model essentially learns which parts (\emph{where}) from which frames (\emph{when}) are relevant and distinctive for matching persons and attaches higher importance therein. The proposed Siamese model is end-to-end trainable to jointly learn comparable hidden representations for paired pedestrian videos and their similarity value. Extensive experiments on three benchmark datasets show the effectiveness of each component of the proposed deep network while outperforming state-of-the-art methods.



### Cortical Microcircuits from a Generative Vision Model
- **Arxiv ID**: http://arxiv.org/abs/1808.01058v1
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1808.01058v1)
- **Published**: 2018-08-03 01:20:08+00:00
- **Updated**: 2018-08-03 01:20:08+00:00
- **Authors**: Dileep George, Alexander Lavin, J. Swaroop Guntupalli, David Mely, Nick Hay, Miguel Lazaro-Gredilla
- **Comment**: None
- **Journal**: None
- **Summary**: Understanding the information processing roles of cortical circuits is an outstanding problem in neuroscience and artificial intelligence. The theoretical setting of Bayesian inference has been suggested as a framework for understanding cortical computation. Based on a recently published generative model for visual inference (George et al., 2017), we derive a family of anatomically instantiated and functional cortical circuit models. In contrast to simplistic models of Bayesian inference, the underlying generative model's representational choices are validated with real-world tasks that required efficient inference and strong generalization. The cortical circuit model is derived by systematically comparing the computational requirements of this model with known anatomical constraints. The derived model suggests precise functional roles for the feedforward, feedback and lateral connections observed in different laminae and columns, and assigns a computational role for the path through the thalamus.



### Online Illumination Invariant Moving Object Detection by Generative Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1808.01066v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.01066v1)
- **Published**: 2018-08-03 02:11:32+00:00
- **Updated**: 2018-08-03 02:11:32+00:00
- **Authors**: Fateme Bahri, Moein Shakeri, Nilanjan Ray
- **Comment**: None
- **Journal**: None
- **Summary**: Moving object detection (MOD) is a significant problem in computer vision that has many real world applications. Different categories of methods have been proposed to solve MOD. One of the challenges is to separate moving objects from illumination changes and shadows that are present in most real world videos. State-of-the-art methods that can handle illumination changes and shadows work in a batch mode; thus, these methods are not suitable for long video sequences or real-time applications. In this paper, we propose an extension of a state-of-the-art batch MOD method (ILISD) to an online/incremental MOD using unsupervised and generative neural networks, which use illumination invariant image representations. For each image in a sequence, we use a low-dimensional representation of a background image by a neural network and then based on the illumination invariant representation, decompose the foreground image into: illumination change and moving objects. Optimization is performed by stochastic gradient descent in an end-to-end and unsupervised fashion. Our algorithm can work in both batch and online modes. In the batch mode, like other batch methods, optimizer uses all the images. In online mode, images can be incrementally fed into the optimizer. Based on our experimental evaluation on benchmark image sequences, both the online and the batch modes of our algorithm achieve state-of-the-art accuracy on most data sets.



### CurriculumNet: Weakly Supervised Learning from Large-Scale Web Images
- **Arxiv ID**: http://arxiv.org/abs/1808.01097v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.01097v4)
- **Published**: 2018-08-03 06:42:11+00:00
- **Updated**: 2018-10-18 12:05:35+00:00
- **Authors**: Sheng Guo, Weilin Huang, Haozhi Zhang, Chenfan Zhuang, Dengke Dong, Matthew R. Scott, Dinglong Huang
- **Comment**: Accepted to ECCV 2018. 16 pages, 5 figures, 5 tables
- **Journal**: None
- **Summary**: We present a simple yet efficient approach capable of training deep neural networks on large-scale weakly-supervised web images, which are crawled raw from the Internet by using text queries, without any human annotation. We develop a principled learning strategy by leveraging curriculum learning, with the goal of handling a massive amount of noisy labels and data imbalance effectively. We design a new learning curriculum by measuring the complexity of data using its distribution density in a feature space, and rank the complexity in an unsupervised manner. This allows for an efficient implementation of curriculum learning on large-scale web images, resulting in a high-performance CNN model, where the negative impact of noisy labels is reduced substantially. Importantly, we show by experiments that those images with highly noisy labels can surprisingly improve the generalization capability of the model, by serving as a manner of regularization. Our approaches obtain state-of-the-art performance on four benchmarks: WebVision, ImageNet, Clothing-1M and Food-101. With an ensemble of multiple models, we achieved a top-5 error rate of 5.2% on the WebVision challenge for 1000-category classification. This result was the top performance by a wide margin, outperforming second place by a nearly 50% relative error rate. Code and models are available at: https://github.com/MalongTech/CurriculumNet .



### Real-Time Object Pose Estimation with Pose Interpreter Networks
- **Arxiv ID**: http://arxiv.org/abs/1808.01099v1
- **DOI**: 10.1109/IROS.2018.8593662
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1808.01099v1)
- **Published**: 2018-08-03 07:12:03+00:00
- **Updated**: 2018-08-03 07:12:03+00:00
- **Authors**: Jimmy Wu, Bolei Zhou, Rebecca Russell, Vincent Kee, Syler Wagner, Mitchell Hebert, Antonio Torralba, David M. S. Johnson
- **Comment**: To appear at 2018 IEEE/RSJ International Conference on Intelligent
  Robots and Systems (IROS 2018). Code available at
  https://github.com/jimmyyhwu/pose-interpreter-networks
- **Journal**: None
- **Summary**: In this work, we introduce pose interpreter networks for 6-DoF object pose estimation. In contrast to other CNN-based approaches to pose estimation that require expensively annotated object pose data, our pose interpreter network is trained entirely on synthetic pose data. We use object masks as an intermediate representation to bridge real and synthetic. We show that when combined with a segmentation model trained on RGB images, our synthetically trained pose interpreter network is able to generalize to real data. Our end-to-end system for object pose estimation runs in real-time (20 Hz) on live RGB data, without using depth information or ICP refinement.



### Exploiting Local Indexing and Deep Feature Confidence Scores for Fast Image-to-Video Search
- **Arxiv ID**: http://arxiv.org/abs/1808.01101v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.01101v2)
- **Published**: 2018-08-03 07:29:43+00:00
- **Updated**: 2020-12-12 14:42:35+00:00
- **Authors**: Savas Ozkan, Gozde Bozdagi Akar
- **Comment**: ICPR 2020
- **Journal**: None
- **Summary**: The cost-effective visual representation and fast query-by-example search are two challenging goals that should be maintained for web-scale visual retrieval tasks on moderate hardware. This paper introduces a fast and robust method that ensures both of these goals by obtaining state-of-the-art performance for an image-to-video search scenario. Hence, we present critical enhancements to well-known indexing and visual representation techniques by promoting faster, better and moderate retrieval performance. We also boost the superiority of our method for some visual challenges by exploiting individual decisions of local and global descriptors at query time. For instance, local content descriptors represent copied/duplicated scenes with large geometric deformations such as scale, orientation and affine transformation. In contrast, the use of global content descriptors is more practical for near-duplicate and semantic searches. Experiments are conducted on a large-scale Stanford I2V dataset. The experimental results show that our method is useful in terms of complexity and query processing time for large-scale visual retrieval scenarios, even if local and global representations are used together. The proposed method is superior and achieves state-of-the-art performance based on the mean average precision (MAP) score of this dataset. Lastly, we report additional MAP scores after updating the ground annotations unveiled by retrieval results of the proposed method, and it shows that the actual performance.



### Hallucinating Agnostic Images to Generalize Across Domains
- **Arxiv ID**: http://arxiv.org/abs/1808.01102v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.01102v2)
- **Published**: 2018-08-03 07:31:01+00:00
- **Updated**: 2019-07-10 23:16:21+00:00
- **Authors**: Fabio M. Carlucci, Paolo Russo, Tatiana Tommasi, Barbara Caputo
- **Comment**: None
- **Journal**: None
- **Summary**: The ability to generalize across visual domains is crucial for the robustness of artificial recognition systems. Although many training sources may be available in real contexts, the access to even unlabeled target samples cannot be taken for granted, which makes standard unsupervised domain adaptation methods inapplicable in the wild. In this work we investigate how to exploit multiple sources by hallucinating a deep visual domain composed of images, possibly unrealistic, able to maintain categorical knowledge while discarding specific source styles. The produced agnostic images are the result of a deep architecture that applies pixel adaptation on the original source data guided by two adversarial domain classifier branches at image and feature level. Our approach is conceived to learn only from source data, but it seamlessly extends to the use of unlabeled target samples. Remarkable results for both multi-source domain adaptation and domain generalization support the power of hallucinating agnostic images in this framework.



### Improved Deep Spectral Convolution Network For Hyperspectral Unmixing With Multinomial Mixture Kernel and Endmember Uncertainty
- **Arxiv ID**: http://arxiv.org/abs/1808.01104v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.01104v4)
- **Published**: 2018-08-03 07:40:25+00:00
- **Updated**: 2019-10-20 18:38:01+00:00
- **Authors**: Savas Ozkan, Gozde Bozdagi Akar
- **Comment**: Submitted to Journal
- **Journal**: None
- **Summary**: In this study, we propose a novel framework for hyperspectral unmixing by using an improved deep spectral convolution network (DSCN++) combined with endmember uncertainty. DSCN++ is used to compute high-level representations which are further modeled with Multinomial Mixture Model to estimate abundance maps. In the reconstruction step, a new trainable uncertainty term based on a nonlinear neural network model is introduced to provide robustness to endmember uncertainty. For the optimization of the coefficients of the multinomial model and the uncertainty term, Wasserstein Generative Adversarial Network (WGAN) is exploited to improve stability and to capture uncertainty. Experiments are performed on both real and synthetic datasets. The results validate that the proposed method obtains state-of-the-art hyperspectral unmixing performance particularly on the real datasets compared to the baseline techniques.



### Interaction-aware Spatio-temporal Pyramid Attention Networks for Action Classification
- **Arxiv ID**: http://arxiv.org/abs/1808.01106v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.01106v1)
- **Published**: 2018-08-03 07:44:25+00:00
- **Updated**: 2018-08-03 07:44:25+00:00
- **Authors**: Yang Du, Chunfeng Yuan, Bing Li, Lili Zhao, Yangxi Li, Weiming Hu
- **Comment**: Accepted by ECCV2018
- **Journal**: None
- **Summary**: Local features at neighboring spatial positions in feature maps have high correlation since their receptive fields are often overlapped. Self-attention usually uses the weighted sum (or other functions) with internal elements of each local feature to obtain its weight score, which ignores interactions among local features. To address this, we propose an effective interaction-aware self-attention model inspired by PCA to learn attention maps. Furthermore, since different layers in a deep network capture feature maps of different scales, we use these feature maps to construct a spatial pyramid and then utilize multi-scale information to obtain more accurate attention scores, which are used to weight the local features in all spatial positions of feature maps to calculate attention maps. Moreover, our spatial pyramid attention is unrestricted to the number of its input feature maps so it is easily extended to a spatio-temporal version. Finally, our model is embedded in general CNNs to form end-to-end attention networks for action classification. Experimental results show that our method achieves the state-of-the-art results on the UCF101, HMDB51 and untrimmed Charades.



### LDSO: Direct Sparse Odometry with Loop Closure
- **Arxiv ID**: http://arxiv.org/abs/1808.01111v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.01111v1)
- **Published**: 2018-08-03 08:12:29+00:00
- **Updated**: 2018-08-03 08:12:29+00:00
- **Authors**: Xiang Gao, Rui Wang, Nikolaus Demmel, Daniel Cremers
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we present an extension of Direct Sparse Odometry (DSO) to a monocular visual SLAM system with loop closure detection and pose-graph optimization (LDSO). As a direct technique, DSO can utilize any image pixel with sufficient intensity gradient, which makes it robust even in featureless areas. LDSO retains this robustness, while at the same time ensuring repeatability of some of these points by favoring corner features in the tracking frontend. This repeatability allows to reliably detect loop closure candidates with a conventional feature-based bag-of-words (BoW) approach. Loop closure candidates are verified geometrically and Sim(3) relative pose constraints are estimated by jointly minimizing 2D and 3D geometric error terms. These constraints are fused with a co-visibility graph of relative poses extracted from DSO's sliding window optimization. Our evaluation on publicly available datasets demonstrates that the modified point selection strategy retains the tracking accuracy and robustness, and the integrated pose-graph optimization significantly reduces the accumulated rotation-, translation- and scale-drift, resulting in an overall performance comparable to state-of-the-art feature-based systems, even without global bundle adjustment.



### Multi-shot Person Re-identification through Set Distance with Visual Distributional Representation
- **Arxiv ID**: http://arxiv.org/abs/1808.01119v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.01119v2)
- **Published**: 2018-08-03 08:39:23+00:00
- **Updated**: 2018-11-08 18:09:04+00:00
- **Authors**: Ting-Yao Hu, Xiaojun Chang, Alexander G. Hauptmann
- **Comment**: None
- **Journal**: None
- **Summary**: Person re-identification aims to identify a specific person at distinct times and locations. It is challenging because of occlusion, illumination, and viewpoint change in camera views. Recently, multi-shot person re-id task receives more attention since it is closer to real-world application. A key point of a good algorithm for multi-shot person re-id is the temporal aggregation of the person appearance features. While most of the current approaches apply pooling strategies and obtain a fixed-size vector representation, these may lose the matching evidence between examples. In this work, we propose the idea of visual distributional representation, which interprets an image set as samples drawn from an unknown distribution in appearance feature space. Based on the supervision signals from a downstream task of interest, the method reshapes the appearance feature space and further learns the unknown distribution of each image set. In the context of multi-shot person re-id, we apply this novel concept along with Wasserstein distance and learn a distributional set distance function between two image sets. In this way, the proper alignment between two image sets can be discovered naturally in a non-parametric manner. Our experiment results on two public datasets show the advantages of our proposed method compared to other state-of-the-art approaches.



### Diverse Conditional Image Generation by Stochastic Regression with Latent Drop-Out Codes
- **Arxiv ID**: http://arxiv.org/abs/1808.01121v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.01121v1)
- **Published**: 2018-08-03 08:47:55+00:00
- **Updated**: 2018-08-03 08:47:55+00:00
- **Authors**: Yang He, Bernt Schiele, Mario Fritz
- **Comment**: This version withdrawn by arXiv administrators because the submitter
  did not have the right to agree to our license at the time of submission
- **Journal**: None
- **Summary**: Recent advances in Deep Learning and probabilistic modeling have led to strong improvements in generative models for images. On the one hand, Generative Adversarial Networks (GANs) have contributed a highly effective adversarial learning procedure, but still suffer from stability issues. On the other hand, Conditional Variational Auto-Encoders (CVAE) models provide a sound way of conditional modeling but suffer from mode-mixing issues. Therefore, recent work has turned back to simple and stable regression models that are effective at generation but give up on the sampling mechanism and the latent code representation. We propose a novel and efficient stochastic regression approach with latent drop-out codes that combines the merits of both lines of research. In addition, a new training objective increases coverage of the training distribution leading to improvements over the state of the art in terms of accuracy as well as diversity.



### Efficient texture retrieval using multiscale local extrema descriptors and covariance embedding
- **Arxiv ID**: http://arxiv.org/abs/1808.01124v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.01124v1)
- **Published**: 2018-08-03 09:04:00+00:00
- **Updated**: 2018-08-03 09:04:00+00:00
- **Authors**: Minh-Tan Pham
- **Comment**: 15 pages, to appear in ECCV Workshops 2018
- **Journal**: None
- **Summary**: This paper presents an efficient method for texture retrieval using multiscale feature extraction and embedding based on the local extrema keypoints. The idea is to first represent each texture image by its local maximum and local minimum pixels. The image is then divided into regular overlapping blocks and each one is characterized by a feature vector constructed from the radiometric, geometric and structural information of its local extrema. All feature vectors are finally embedded into a covariance matrix which will be exploited for dissimilarity measurement within retrieval task. Thanks to the method's simplicity, multiscale scheme can be easily implemented to improve its scale-space representation capacity. We argue that our handcrafted features are easy to implement, fast to run but can provide very competitive performance compared to handcrafted and CNN-based learned descriptors from the literature. In particular, the proposed framework provides highly competitive retrieval rate for several texture databases including 94.95% for MIT Vistex, 79.87% for Stex, 76.15% for Outex TC-00013 and 89.74% for USPtex.



### iSPA-Net: Iterative Semantic Pose Alignment Network
- **Arxiv ID**: http://arxiv.org/abs/1808.01134v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.01134v1)
- **Published**: 2018-08-03 09:44:47+00:00
- **Updated**: 2018-08-03 09:44:47+00:00
- **Authors**: Jogendra Nath Kundu, Aditya Ganeshan, Rahul M. V., Aditya Prakash, R. Venkatesh Babu
- **Comment**: Accepted at ACMMM 2018. Code available at
  https://github.com/val-iisc/iSPA-Net
- **Journal**: None
- **Summary**: Understanding and extracting 3D information of objects from monocular 2D images is a fundamental problem in computer vision. In the task of 3D object pose estimation, recent data driven deep neural network based approaches suffer from scarcity of real images with 3D keypoint and pose annotations. Drawing inspiration from human cognition, where the annotators use a 3D CAD model as structural reference to acquire ground-truth viewpoints for real images; we propose an iterative Semantic Pose Alignment Network, called iSPA-Net. Our approach focuses on exploiting semantic 3D structural regularity to solve the task of fine-grained pose estimation by predicting viewpoint difference between a given pair of images. Such image comparison based approach also alleviates the problem of data scarcity and hence enhances scalability of the proposed approach for novel object categories with minimal annotation. The fine-grained object pose estimator is also aided by correspondence of learned spatial descriptor of the input image pair. The proposed pose alignment framework enjoys the faculty to refine its initial pose estimation in consecutive iterations by utilizing an online rendering setup along with effectiveness of a non-uniform bin classification of pose-difference. This enables iSPA-Net to achieve state-of-the-art performance on various real image viewpoint estimation datasets. Further, we demonstrate effectiveness of the approach for multiple applications. First, we show results for active object viewpoint localization to capture images from similar pose considering only a single image as pose reference. Second, we demonstrate the ability of the learned semantic correspondence to perform unsupervised part-segmentation transfer using only a single part-annotated 3D template model per object class. To encourage reproducible research, we have released the codes for our proposed algorithm.



### A Two-Dimensional (2-D) Learning Framework for Particle Swarm based Feature Selection
- **Arxiv ID**: http://arxiv.org/abs/1808.01150v1
- **DOI**: 10.1016/j.patcog.2017.11.027
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1808.01150v1)
- **Published**: 2018-08-03 10:50:43+00:00
- **Updated**: 2018-08-03 10:50:43+00:00
- **Authors**: Faizal Hafiz, Akshya Swain, Nitish Patel, Chirag Naik
- **Comment**: None
- **Journal**: Elsevier - Pattern Recognition, Volume 76, 2018, Pages 416-433
- **Summary**: This paper proposes a new generalized two dimensional learning approach for particle swarm based feature selection. The core idea of the proposed approach is to include the information about the subset cardinality into the learning framework by extending the dimension of the velocity. The 2D-learning framework retains all the key features of the original PSO, despite the extra learning dimension. Most of the popular variants of PSO can easily be adapted into this 2D learning framework for feature selection problems. The efficacy of the proposed learning approach has been evaluated considering several benchmark data and two induction algorithms: Naive-Bayes and k-Nearest Neighbor. The results of the comparative investigation including the time-complexity analysis with GA, ACO and five other PSO variants illustrate that the proposed 2D learning approach gives feature subset with relatively smaller cardinality and better classification performance with shorter run times.



### Ask, Acquire, and Attack: Data-free UAP Generation using Class Impressions
- **Arxiv ID**: http://arxiv.org/abs/1808.01153v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1808.01153v1)
- **Published**: 2018-08-03 11:02:26+00:00
- **Updated**: 2018-08-03 11:02:26+00:00
- **Authors**: Konda Reddy Mopuri, Phani Krishna Uppala, R. Venkatesh Babu
- **Comment**: Accepted in ECCV 2018
- **Journal**: None
- **Summary**: Deep learning models are susceptible to input specific noise, called adversarial perturbations. Moreover, there exist input-agnostic noise, called Universal Adversarial Perturbations (UAP) that can affect inference of the models over most input samples. Given a model, there exist broadly two approaches to craft UAPs: (i) data-driven: that require data, and (ii) data-free: that do not require data samples. Data-driven approaches require actual samples from the underlying data distribution and craft UAPs with high success (fooling) rate. However, data-free approaches craft UAPs without utilizing any data samples and therefore result in lesser success rates. In this paper, for data-free scenarios, we propose a novel approach that emulates the effect of data samples with class impressions in order to craft UAPs using data-driven objectives. Class impression for a given pair of category and model is a generic representation (in the input space) of the samples belonging to that category. Further, we present a neural network based generative model that utilizes the acquired class impressions to learn crafting UAPs. Experimental evaluation demonstrates that the learned generative model, (i) readily crafts UAPs via simple feed-forwarding through neural network layers, and (ii) achieves state-of-the-art success rates for data-free scenario and closer to that for data-driven setting without actually utilizing any data samples.



### Exploring Uncertainty Measures in Deep Networks for Multiple Sclerosis Lesion Detection and Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1808.01200v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.01200v2)
- **Published**: 2018-08-03 14:19:32+00:00
- **Updated**: 2018-10-16 21:07:08+00:00
- **Authors**: Tanya Nair, Doina Precup, Douglas L. Arnold, Tal Arbel
- **Comment**: Updated references in Introduction; Accepted to the 21st
  International Conference on Medical Image Computing and Computer Assisted
  Intervention (MICCAI 2018)
- **Journal**: None
- **Summary**: Deep learning (DL) networks have recently been shown to outperform other segmentation methods on various public, medical-image challenge datasets [3,11,16], especially for large pathologies. However, in the context of diseases such as Multiple Sclerosis (MS), monitoring all the focal lesions visible on MRI sequences, even very small ones, is essential for disease staging, prognosis, and evaluating treatment efficacy. Moreover, producing deterministic outputs hinders DL adoption into clinical routines. Uncertainty estimates for the predictions would permit subsequent revision by clinicians. We present the first exploration of multiple uncertainty estimates based on Monte Carlo (MC) dropout [4] in the context of deep networks for lesion detection and segmentation in medical images. Specifically, we develop a 3D MS lesion segmentation CNN, augmented to provide four different voxel-based uncertainty measures based on MC dropout. We train the network on a proprietary, large-scale, multi-site, multi-scanner, clinical MS dataset, and compute lesion-wise uncertainties by accumulating evidence from voxel-wise uncertainties within detected lesions. We analyze the performance of voxel-based segmentation and lesion-level detection by choosing operating points based on the uncertainty. Empirical evidence suggests that uncertainty measures consistently allow us to choose superior operating points compared only using the network's sigmoid output as a probability.



### Visual Reasoning with Multi-hop Feature Modulation
- **Arxiv ID**: http://arxiv.org/abs/1808.04446v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1808.04446v2)
- **Published**: 2018-08-03 14:32:02+00:00
- **Updated**: 2018-10-12 11:36:42+00:00
- **Authors**: Florian Strub, Mathieu Seurin, Ethan Perez, Harm de Vries, Jérémie Mary, Philippe Preux, Aaron Courville, Olivier Pietquin
- **Comment**: In Proc of ECCV 2018
- **Journal**: None
- **Summary**: Recent breakthroughs in computer vision and natural language processing have spurred interest in challenging multi-modal tasks such as visual question-answering and visual dialogue. For such tasks, one successful approach is to condition image-based convolutional network computation on language via Feature-wise Linear Modulation (FiLM) layers, i.e., per-channel scaling and shifting. We propose to generate the parameters of FiLM layers going up the hierarchy of a convolutional network in a multi-hop fashion rather than all at once, as in prior work. By alternating between attending to the language input and generating FiLM layer parameters, this approach is better able to scale to settings with longer input sequences such as dialogue. We demonstrate that multi-hop FiLM generation achieves state-of-the-art for the short input sequence task ReferIt --- on-par with single-hop FiLM generation --- while also significantly outperforming prior state-of-the-art and single-hop FiLM generation on the GuessWhat?! visual dialogue task.



### CornerNet: Detecting Objects as Paired Keypoints
- **Arxiv ID**: http://arxiv.org/abs/1808.01244v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.01244v2)
- **Published**: 2018-08-03 16:05:55+00:00
- **Updated**: 2019-03-18 18:58:40+00:00
- **Authors**: Hei Law, Jia Deng
- **Comment**: Extended version with additional results. Test AP on MS COOO improved
  from 42.1% to 42.2% after a bug fix
- **Journal**: None
- **Summary**: We propose CornerNet, a new approach to object detection where we detect an object bounding box as a pair of keypoints, the top-left corner and the bottom-right corner, using a single convolution neural network. By detecting objects as paired keypoints, we eliminate the need for designing a set of anchor boxes commonly used in prior single-stage detectors. In addition to our novel formulation, we introduce corner pooling, a new type of pooling layer that helps the network better localize corners. Experiments show that CornerNet achieves a 42.2% AP on MS COCO, outperforming all existing one-stage detectors.



### Model Adaptation with Synthetic and Real Data for Semantic Dense Foggy Scene Understanding
- **Arxiv ID**: http://arxiv.org/abs/1808.01265v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.01265v1)
- **Published**: 2018-08-03 17:26:39+00:00
- **Updated**: 2018-08-03 17:26:39+00:00
- **Authors**: Christos Sakaridis, Dengxin Dai, Simon Hecker, Luc Van Gool
- **Comment**: final version, ECCV 2018
- **Journal**: None
- **Summary**: This work addresses the problem of semantic scene understanding under dense fog. Although considerable progress has been made in semantic scene understanding, it is mainly related to clear-weather scenes. Extending recognition methods to adverse weather conditions such as fog is crucial for outdoor applications. In this paper, we propose a novel method, named Curriculum Model Adaptation (CMAda), which gradually adapts a semantic segmentation model from light synthetic fog to dense real fog in multiple steps, using both synthetic and real foggy data. In addition, we present three other main stand-alone contributions: 1) a novel method to add synthetic fog to real, clear-weather scenes using semantic input; 2) a new fog density estimator; 3) the Foggy Zurich dataset comprising $3808$ real foggy images, with pixel-level semantic annotations for $16$ images with dense fog. Our experiments show that 1) our fog simulation slightly outperforms a state-of-the-art competing simulation with respect to the task of semantic foggy scene understanding (SFSU); 2) CMAda improves the performance of state-of-the-art models for SFSU significantly by leveraging unlabeled real foggy data. The datasets and code are publicly available.



### The Power of Complementary Regularizers: Image Recovery via Transform Learning and Low-Rank Modeling
- **Arxiv ID**: http://arxiv.org/abs/1808.01316v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.01316v1)
- **Published**: 2018-08-03 19:29:19+00:00
- **Updated**: 2018-08-03 19:29:19+00:00
- **Authors**: Bihan Wen, Yanjun Li, Yoram Bresler
- **Comment**: 13 pages, 7 figures, submitted to TIP
- **Journal**: None
- **Summary**: Recent works on adaptive sparse and on low-rank signal modeling have demonstrated their usefulness in various image / video processing applications. Patch-based methods exploit local patch sparsity, whereas other works apply low-rankness of grouped patches to exploit image non-local structures. However, using either approach alone usually limits performance in image reconstruction or recovery applications. In this work, we propose a simultaneous sparsity and low-rank model, dubbed STROLLR, to better represent natural images. In order to fully utilize both the local and non-local image properties, we develop an image restoration framework using a transform learning scheme with joint low-rank regularization. The approach owes some of its computational efficiency and good performance to the use of transform learning for adaptive sparse representation rather than the popular synthesis dictionary learning algorithms, which involve approximation of NP-hard sparse coding and expensive learning steps. We demonstrate the proposed framework in various applications to image denoising, inpainting, and compressed sensing based magnetic resonance imaging. Results show promising performance compared to state-of-the-art competing methods.



### Parsing Geometry Using Structure-Aware Shape Templates
- **Arxiv ID**: http://arxiv.org/abs/1808.01337v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.01337v2)
- **Published**: 2018-08-03 20:14:58+00:00
- **Updated**: 2018-09-05 02:40:46+00:00
- **Authors**: Vignesh Ganapathi-Subramanian, Olga Diamanti, Soeren Pirk, Chengcheng Tang, Matthias Niessner, Leonidas J. Guibas
- **Comment**: None
- **Journal**: None
- **Summary**: Real-life man-made objects often exhibit strong and easily-identifiable structure, as a direct result of their design or their intended functionality. Structure typically appears in the form of individual parts and their arrangement. Knowing about object structure can be an important cue for object recognition and scene understanding - a key goal for various AR and robotics applications. However, commodity RGB-D sensors used in these scenarios only produce raw, unorganized point clouds, without structural information about the captured scene. Moreover, the generated data is commonly partial and susceptible to artifacts and noise, which makes inferring the structure of scanned objects challenging. In this paper, we organize large shape collections into parameterized shape templates to capture the underlying structure of the objects. The templates allow us to transfer the structural information onto new objects and incomplete scans. We employ a deep neural network that matches the partial scan with one of the shape templates, then match and fit it to complete and detailed models from the collection. This allows us to faithfully label its parts and to guide the reconstruction of the scanned object. We showcase the effectiveness of our method by comparing it to other state-of-the-art approaches.



### Detailed Human Avatars from Monocular Video
- **Arxiv ID**: http://arxiv.org/abs/1808.01338v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.01338v1)
- **Published**: 2018-08-03 20:15:29+00:00
- **Updated**: 2018-08-03 20:15:29+00:00
- **Authors**: Thiemo Alldieck, Marcus Magnor, Weipeng Xu, Christian Theobalt, Gerard Pons-Moll
- **Comment**: International Conference on 3D Vision (3DV) 2018
- **Journal**: None
- **Summary**: We present a novel method for high detail-preserving human avatar creation from monocular video. A parameterized body model is refined and optimized to maximally resemble subjects from a video showing them from all sides. Our avatars feature a natural face, hairstyle, clothes with garment wrinkles, and high-resolution texture. Our paper contributes facial landmark and shading-based human body shape refinement, a semantic texture prior, and a novel texture stitching strategy, resulting in the most sophisticated-looking human avatars obtained from a single video to date. Numerous results show the robustness and versatility of our method. A user study illustrates its superiority over the state-of-the-art in terms of identity preservation, level of detail, realism, and overall user preference.



### A Short Note about Kinetics-600
- **Arxiv ID**: http://arxiv.org/abs/1808.01340v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.01340v1)
- **Published**: 2018-08-03 20:17:05+00:00
- **Updated**: 2018-08-03 20:17:05+00:00
- **Authors**: Joao Carreira, Eric Noland, Andras Banki-Horvath, Chloe Hillier, Andrew Zisserman
- **Comment**: Companion to public release of kinetics-600 test set labels
- **Journal**: None
- **Summary**: We describe an extension of the DeepMind Kinetics human action dataset from 400 classes, each with at least 400 video clips, to 600 classes, each with at least 600 video clips. In order to scale up the dataset we changed the data collection process so it uses multiple queries per class, with some of them in a language other than english -- portuguese. This paper details the changes between the two versions of the dataset and includes a comprehensive set of statistics of the new version as well as baseline results using the I3D neural network architecture. The paper is a companion to the release of the ground truth labels for the public test set.



### Purely Geometric Scene Association and Retrieval - A Case for Macro Scale 3D Geometry
- **Arxiv ID**: http://arxiv.org/abs/1808.01343v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1808.01343v1)
- **Published**: 2018-08-03 20:20:40+00:00
- **Updated**: 2018-08-03 20:20:40+00:00
- **Authors**: Rahul Sawhney, Fuxin Li, Henrik I. Christensen, Charles L. Isbell
- **Comment**: Accepted in ICRA '18
- **Journal**: None
- **Summary**: We address the problems of measuring geometric similarity between 3D scenes, represented through point clouds or range data frames, and associating them. Our approach leverages macro-scale 3D structural geometry - the relative configuration of arbitrary surfaces and relationships among structures that are potentially far apart. We express such discriminative information in a viewpoint-invariant feature space. These are subsequently encoded in a frame-level signature that can be utilized to measure geometric similarity. Such a characterization is robust to noise, incomplete and partially overlapping data besides viewpoint changes. We show how it can be employed to select a diverse set of data frames which have structurally similar content, and how to validate whether views with similar geometric content are from the same scene. The problem is formulated as one of general purpose retrieval from an unannotated, spatio-temporally unordered database. Empirical analysis indicates that the presented approach thoroughly outperforms baselines on depth / range data. Its depth-only performance is competitive with state-of-the-art approaches with RGB or RGB-D inputs, including ones based on deep learning. Experiments show retrieval performance to hold up well with much sparser databases, which is indicative of the approach's robustness. The approach generalized well - it did not require dataset specific training, and scaled up in our experiments. Finally, we also demonstrate how geometrically diverse selection of views can result in richer 3D reconstructions.



