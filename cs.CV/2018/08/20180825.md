# Arxiv Papers in cs.CV on 2018-08-25
### Neural Task Planning with And-Or Graph Representations
- **Arxiv ID**: http://arxiv.org/abs/1808.09284v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1808.09284v1)
- **Published**: 2018-08-25 02:57:23+00:00
- **Updated**: 2018-08-25 02:57:23+00:00
- **Authors**: Tianshui Chen, Riquan Chen, Lin Nie, Xiaonan Luo, Xiaobai Liu, Liang Lin
- **Comment**: Submitted to TMM, under minor revision. arXiv admin note: text
  overlap with arXiv:1707.04677
- **Journal**: None
- **Summary**: This paper focuses on semantic task planning, i.e., predicting a sequence of actions toward accomplishing a specific task under a certain scene, which is a new problem in computer vision research. The primary challenges are how to model task-specific knowledge and how to integrate this knowledge into the learning procedure. In this work, we propose training a recurrent long short-term memory (LSTM) network to address this problem, i.e., taking a scene image (including pre-located objects) and the specified task as input and recurrently predicting action sequences. However, training such a network generally requires large numbers of annotated samples to cover the semantic space (e.g., diverse action decomposition and ordering). To overcome this issue, we introduce a knowledge and-or graph (AOG) for task description, which hierarchically represents a task as atomic actions. With this AOG representation, we can produce many valid samples (i.e., action sequences according to common sense) by training another auxiliary LSTM network with a small set of annotated samples. Furthermore, these generated samples (i.e., task-oriented action sequences) effectively facilitate training of the model for semantic task planning. In our experiments, we create a new dataset that contains diverse daily tasks and extensively evaluate the effectiveness of our approach.



### Multi-scale CNN stereo and pattern removal technique for underwater active stereo system
- **Arxiv ID**: http://arxiv.org/abs/1808.08348v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.08348v1)
- **Published**: 2018-08-25 03:17:16+00:00
- **Updated**: 2018-08-25 03:17:16+00:00
- **Authors**: Kazuto Ichimaru, Ryo Furukawa, Hiroshi Kawasaki
- **Comment**: International Conference on 3D Vision 2018
- **Journal**: None
- **Summary**: Demands on capturing dynamic scenes of underwater environments are rapidly growing. Passive stereo is applicable to capture dynamic scenes, however the shape with textureless surfaces or irregular reflections cannot be recovered by the technique. In our system, we add a pattern projector to the stereo camera pair so that artificial textures are augmented on the objects. To use the system at underwater environments, several problems should be compensated, i.e., refraction, disturbance by fluctuation and bubbles. Further, since surface of the objects are interfered by the bubbles, projected patterns, etc., those noises and patterns should be removed from captured images to recover original texture. To solve these problems, we propose three approaches; a depth-dependent calibration, Convolutional Neural Network(CNN)-stereo method and CNN-based texture recovery method. A depth-dependent calibration is our analysis to find the acceptable depth range for approximation by center projection to find the certain target depth for calibration. In terms of CNN stereo, unlike common CNNbased stereo methods which do not consider strong disturbances like refraction or bubbles, we designed a novel CNN architecture for stereo matching using multi-scale information, which is intended to be robust against such disturbances. Finally, we propose a multi-scale method for bubble and a projected-pattern removal method using CNNs to recover original textures. Experimental results are shown to prove the effectiveness of our method compared with the state of the art techniques. Furthermore, reconstruction of a live swimming fish is demonstrated to confirm the feasibility of our techniques.



### NavigationNet: A Large-scale Interactive Indoor Navigation Dataset
- **Arxiv ID**: http://arxiv.org/abs/1808.08374v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.08374v1)
- **Published**: 2018-08-25 08:24:10+00:00
- **Updated**: 2018-08-25 08:24:10+00:00
- **Authors**: He Huang, Yujing Shen, Jiankai Sun, Cewu Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Indoor navigation aims at performing navigation within buildings. In scenes like home and factory, most intelligent mobile devices require an functionality of routing to guide itself precisely through indoor scenes to complete various tasks in order to serve human. In most scenarios, we expected an intelligent device capable of navigating itself in unseen environment. Although several solutions have been proposed to deal with this issue, they usually require pre-installed beacons or a map pre-built with SLAM, which means that they are not capable of working in novel environments. To address this, we proposed NavigationNet, a computer vision dataset and benchmark to allow the utilization of deep reinforcement learning on scene-understanding-based indoor navigation. We also proposed and formalized several typical indoor routing problems that are suitable for deep reinforcement learning.



### Fusion++: Volumetric Object-Level SLAM
- **Arxiv ID**: http://arxiv.org/abs/1808.08378v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.08378v2)
- **Published**: 2018-08-25 08:37:08+00:00
- **Updated**: 2018-08-28 17:36:59+00:00
- **Authors**: John McCormac, Ronald Clark, Michael Bloesch, Andrew J. Davison, Stefan Leutenegger
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an online object-level SLAM system which builds a persistent and accurate 3D graph map of arbitrary reconstructed objects. As an RGB-D camera browses a cluttered indoor scene, Mask-RCNN instance segmentations are used to initialise compact per-object Truncated Signed Distance Function (TSDF) reconstructions with object size-dependent resolutions and a novel 3D foreground mask. Reconstructed objects are stored in an optimisable 6DoF pose graph which is our only persistent map representation. Objects are incrementally refined via depth fusion, and are used for tracking, relocalisation and loop closure detection. Loop closures cause adjustments in the relative pose estimates of object instances, but no intra-object warping. Each object also carries semantic information which is refined over time and an existence probability to account for spurious instance predictions. We demonstrate our approach on a hand-held RGB-D sequence from a cluttered office scene with a large number and variety of object instances, highlighting how the system closes loops and makes good use of existing objects on repeated loops. We quantitatively evaluate the trajectory error of our system against a baseline approach on the RGB-D SLAM benchmark, and qualitatively compare reconstruction quality of discovered objects on the YCB video dataset. Performance evaluation shows our approach is highly memory efficient and runs online at 4-8Hz (excluding relocalisation) despite not being optimised at the software level.



### Saliency Detection via Bidirectional Absorbing Markov Chain
- **Arxiv ID**: http://arxiv.org/abs/1808.08393v1
- **DOI**: None
- **Categories**: **cs.CV**, I.2.10; I.4.0; I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/1808.08393v1)
- **Published**: 2018-08-25 09:36:04+00:00
- **Updated**: 2018-08-25 09:36:04+00:00
- **Authors**: Fengling Jiang, Bin Kong, Ahsan Adeel, Yun Xiao, Amir Hussain
- **Comment**: To appear in the 9th International Conference on Brain Inspired
  Cognitive Systems (BICS 2018)
- **Journal**: None
- **Summary**: Traditional saliency detection via Markov chain only considers boundaries nodes. However, in addition to boundaries cues, background prior and foreground prior cues play a complementary role to enhance saliency detection. In this paper, we propose an absorbing Markov chain based saliency detection method considering both boundary information and foreground prior cues. The proposed approach combines both boundaries and foreground prior cues through bidirectional Markov chain. Specifically, the image is first segmented into superpixels and four boundaries nodes (duplicated as virtual nodes) are selected. Subsequently, the absorption time upon transition node's random walk to the absorbing state is calculated to obtain foreground possibility. Simultaneously, foreground prior as the virtual absorbing nodes is used to calculate the absorption time and obtain the background possibility. Finally, two obtained results are fused to obtain the combined saliency map using cost function for further optimization at multi-scale. Experimental results demonstrate the outperformance of our proposed model on 4 benchmark datasets as compared to 17 state-of-the-art methods.



### A Novel Deep Neural Network Architecture for Mars Visual Navigation
- **Arxiv ID**: http://arxiv.org/abs/1808.08395v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.08395v1)
- **Published**: 2018-08-25 09:40:06+00:00
- **Updated**: 2018-08-25 09:40:06+00:00
- **Authors**: Jiang Zhang, Yuanqing Xia, Ganghui Shen
- **Comment**: Submitted to IEEE Transactions on Neural Networks and Learning
  Systems on May 17th, 2018
- **Journal**: None
- **Summary**: In this paper, emerging deep learning techniques are leveraged to deal with Mars visual navigation problem. Specifically, to achieve precise landing and autonomous navigation, a novel deep neural network architecture with double branches and non-recurrent structure is designed, which can represent both global and local deep features of Martian environment images effectively. By employing this architecture, Mars rover can determine the optimal navigation policy to the target point directly from original Martian environment images. Moreover, compared with the existing state-of-the-art algorithm, the training time is reduced by 45.8%. Finally, experiment results demonstrate that the proposed deep neural network architecture achieves better performance and faster convergence than the existing ones and generalizes well to unknown environment.



### Noiseprint: a CNN-based camera model fingerprint
- **Arxiv ID**: http://arxiv.org/abs/1808.08396v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.08396v1)
- **Published**: 2018-08-25 09:44:26+00:00
- **Updated**: 2018-08-25 09:44:26+00:00
- **Authors**: Davide Cozzolino, Luisa Verdoliva
- **Comment**: None
- **Journal**: None
- **Summary**: Forensic analyses of digital images rely heavily on the traces of in-camera and out-camera processes left on the acquired images. Such traces represent a sort of camera fingerprint. If one is able to recover them, by suppressing the high-level scene content and other disturbances, a number of forensic tasks can be easily accomplished. A notable example is the PRNU pattern, which can be regarded as a device fingerprint, and has received great attention in multimedia forensics. In this paper we propose a method to extract a camera model fingerprint, called noiseprint, where the scene content is largely suppressed and model-related artifacts are enhanced. This is obtained by means of a Siamese network, which is trained with pairs of image patches coming from the same (label +1) or different (label -1) cameras. Although noiseprints can be used for a large variety of forensic tasks, here we focus on image forgery localization. Experiments on several datasets widespread in the forensic community show noiseprint-based methods to provide state-of-the-art performance.



### How do Convolutional Neural Networks Learn Design?
- **Arxiv ID**: http://arxiv.org/abs/1808.08402v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1808.08402v1)
- **Published**: 2018-08-25 10:34:05+00:00
- **Updated**: 2018-08-25 10:34:05+00:00
- **Authors**: Shailza Jolly, Brian Kenji Iwana, Ryohei Kuroki, Seiichi Uchida
- **Comment**: Accepted by ICPR 2018
- **Journal**: None
- **Summary**: In this paper, we aim to understand the design principles in book cover images which are carefully crafted by experts. Book covers are designed in a unique way, specific to genres which convey important information to their readers. By using Convolutional Neural Networks (CNN) to predict book genres from cover images, visual cues which distinguish genres can be highlighted and analyzed. In order to understand these visual clues contributing towards the decision of a genre, we present the application of Layer-wise Relevance Propagation (LRP) on the book cover image classification results. We use LRP to explain the pixel-wise contributions of book cover design and highlight the design elements contributing towards particular genres. In addition, with the use of state-of-the-art object and text detection methods, insights about genre-specific book cover designs are discovered.



### How many labeled license plates are needed?
- **Arxiv ID**: http://arxiv.org/abs/1808.08410v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.08410v1)
- **Published**: 2018-08-25 11:11:42+00:00
- **Updated**: 2018-08-25 11:11:42+00:00
- **Authors**: Changhao Wu, Shugong Xu, Guocong Song, Shunqing Zhang
- **Comment**: 12 pages, 3 figures, Chinese Conference on Pattern Recognition and
  Computer Vision
- **Journal**: None
- **Summary**: Training a good deep learning model often requires a lot of annotated data. As a large amount of labeled data is typically difficult to collect and even more difficult to annotate, data augmentation and data generation are widely used in the process of training deep neural networks. However, there is no clear common understanding on how much labeled data is needed to get satisfactory performance. In this paper, we try to address such a question using vehicle license plate character recognition as an example application. We apply computer graphic scripts and Generative Adversarial Networks to generate and augment a large number of annotated, synthesized license plate images with realistic colors, fonts, and character composition from a small number of real, manually labeled license plate images. Generated and augmented data are mixed and used as training data for the license plate recognition network modified from DenseNet. The experimental results show that the model trained from the generated mixed training data has good generalization ability, and the proposed approach achieves a new state-of-the-art accuracy on Dataset-1 and AOLP, even with a very limited number of original real license plates. In addition, the accuracy improvement caused by data generation becomes more significant when the number of labeled images is reduced. Data augmentation also plays a more significant role when the number of labeled images is increased.



### Analysis of adversarial attacks against CNN-based image forgery detectors
- **Arxiv ID**: http://arxiv.org/abs/1808.08426v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.08426v1)
- **Published**: 2018-08-25 13:40:36+00:00
- **Updated**: 2018-08-25 13:40:36+00:00
- **Authors**: Diego Gragnaniello, Francesco Marra, Giovanni Poggi, Luisa Verdoliva
- **Comment**: None
- **Journal**: None
- **Summary**: With the ubiquitous diffusion of social networks, images are becoming a dominant and powerful communication channel. Not surprisingly, they are also increasingly subject to manipulations aimed at distorting information and spreading fake news. In recent years, the scientific community has devoted major efforts to contrast this menace, and many image forgery detectors have been proposed. Currently, due to the success of deep learning in many multimedia processing tasks, there is high interest towards CNN-based detectors, and early results are already very promising. Recent studies in computer vision, however, have shown CNNs to be highly vulnerable to adversarial attacks, small perturbations of the input data which drive the network towards erroneous classification. In this paper we analyze the vulnerability of CNN-based image forensics methods to adversarial attacks, considering several detectors and several types of attack, and testing performance on a wide range of common manipulations, both easily and hardly detectable.



### Organ at Risk Segmentation in Head and Neck CT Images by Using a Two-Stage Segmentation Framework Based on 3D U-Net
- **Arxiv ID**: http://arxiv.org/abs/1809.00960v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.00960v2)
- **Published**: 2018-08-25 14:09:28+00:00
- **Updated**: 2019-10-11 08:40:03+00:00
- **Authors**: Yueyue Wang, Liang Zhao, Zhijian Song, Manning Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate segmentation of organ at risk (OAR) play a critical role in the treatment planning of image guided radiation treatment of head and neck cancer. This segmentation task is challenging for both human and automatic algorithms because of the relatively large number of OARs to be segmented, the large variability of the size and morphology across different OARs, and the low contrast of between some OARs and the background. In this paper, we proposed a two-stage segmentation framework based on 3D U-Net. In this framework, the segmentation of each OAR is decomposed into two sub-tasks: locating a bounding box of the OAR and segment it from a small volume within the bounding box, and each sub-tasks is fulfilled by a dedicated 3D U-Net. The decomposition makes each of the two sub-tasks much easier, so that they can be better completed. We evaluated the proposed method and compared it to state-of-the-art methods by using the MICCAI 2015 Challenge dataset. In terms of the boundary-based metric 95HD, the proposed method ranked first in eight of all nine OARs and ranked second in the other OAR. In terms of the area-based metric DSC, the proposed method ranked first in six of the nine OARs and ranked second in the other three OARs with small difference with the first one.



### Human-centric Indoor Scene Synthesis Using Stochastic Grammar
- **Arxiv ID**: http://arxiv.org/abs/1808.08473v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.08473v1)
- **Published**: 2018-08-25 21:44:18+00:00
- **Updated**: 2018-08-25 21:44:18+00:00
- **Authors**: Siyuan Qi, Yixin Zhu, Siyuan Huang, Chenfanfu Jiang, Song-Chun Zhu
- **Comment**: This paper is published in CVPR 2018
- **Journal**: None
- **Summary**: We present a human-centric method to sample and synthesize 3D room layouts and 2D images thereof, to obtain large-scale 2D/3D image data with perfect per-pixel ground truth. An attributed spatial And-Or graph (S-AOG) is proposed to represent indoor scenes. The S-AOG is a probabilistic grammar model, in which the terminal nodes are object entities. Human contexts as contextual relations are encoded by Markov Random Fields (MRF) on the terminal nodes. We learn the distributions from an indoor scene dataset and sample new layouts using Monte Carlo Markov Chain. Experiments demonstrate that our method can robustly sample a large variety of realistic room layouts based on three criteria: (i) visual realism comparing to a state-of-the-art room arrangement method, (ii) accuracy of the affordance maps with respect to groundtruth, and (ii) the functionality and naturalness of synthesized rooms evaluated by human subjects. The code is available at https://github.com/SiyuanQi/human-centric-scene-synthesis.



### MSCE: An edge preserving robust loss function for improving super-resolution algorithms
- **Arxiv ID**: http://arxiv.org/abs/1809.00961v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.00961v1)
- **Published**: 2018-08-25 22:00:10+00:00
- **Updated**: 2018-08-25 22:00:10+00:00
- **Authors**: Ram Krishna Pandey, Nabagata Saha, Samarjit Karmakar, A G Ramakrishnan
- **Comment**: Accepted in ICONIP-2018
- **Journal**: None
- **Summary**: With the recent advancement in the deep learning technologies such as CNNs and GANs, there is significant improvement in the quality of the images reconstructed by deep learning based super-resolution (SR) techniques. In this work, we propose a robust loss function based on the preservation of edges obtained by the Canny operator. This loss function, when combined with the existing loss function such as mean square error (MSE), gives better SR reconstruction measured in terms of PSNR and SSIM. Our proposed loss function guarantees improved performance on any existing algorithm using MSE loss function, without any increase in the computational complexity during testing.



### Deep-Learning Ensembles for Skin-Lesion Segmentation, Analysis, Classification: RECOD Titans at ISIC Challenge 2018
- **Arxiv ID**: http://arxiv.org/abs/1808.08480v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.08480v1)
- **Published**: 2018-08-25 22:50:43+00:00
- **Updated**: 2018-08-25 22:50:43+00:00
- **Authors**: Alceu Bissoto, Fábio Perez, Vinícius Ribeiro, Michel Fornaciali, Sandra Avila, Eduardo Valle
- **Comment**: None
- **Journal**: None
- **Summary**: This extended abstract describes the participation of RECOD Titans in parts 1 to 3 of the ISIC Challenge 2018 "Skin Lesion Analysis Towards Melanoma Detection" (MICCAI 2018). Although our team has a long experience with melanoma classification and moderate experience with lesion segmentation, the ISIC Challenge 2018 was the very first time we worked on lesion attribute detection. For each task we submitted 3 different ensemble approaches, varying combinations of models and datasets. Our best results on the official testing set, regarding the official metric of each task, were: 0.728 (segmentation), 0.344 (attribute detection) and 0.803 (classification). Those submissions reached, respectively, the 56th, 14th and 9th places.



### Road User Abnormal Trajectory Detection using a Deep Autoencoder
- **Arxiv ID**: http://arxiv.org/abs/1809.00957v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.00957v1)
- **Published**: 2018-08-25 23:18:52+00:00
- **Updated**: 2018-08-25 23:18:52+00:00
- **Authors**: Pankaj Raj Roy, Guillaume-Alexandre Bilodeau
- **Comment**: This paper has been accepted for oral presentation at ISVC'18
- **Journal**: None
- **Summary**: In this paper, we focus on the development of a method that detects abnormal trajectories of road users at traffic intersections. The main difficulty with this is the fact that there are very few abnormal data and the normal ones are insufficient for the training of any kinds of machine learning model. To tackle these problems, we proposed the solution of using a deep autoencoder network trained solely through augmented data considered as normal. By generating artificial abnormal trajectories, our method is tested on four different outdoor urban users scenes and performs better compared to some classical outlier detection methods.



### Painting Outside the Box: Image Outpainting with GANs
- **Arxiv ID**: http://arxiv.org/abs/1808.08483v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.08483v1)
- **Published**: 2018-08-25 23:38:13+00:00
- **Updated**: 2018-08-25 23:38:13+00:00
- **Authors**: Mark Sabini, Gili Rusak
- **Comment**: 5 pages, 9 figures
- **Journal**: None
- **Summary**: The challenging task of image outpainting (extrapolation) has received comparatively little attention in relation to its cousin, image inpainting (completion). Accordingly, we present a deep learning approach based on Iizuka et al. for adversarially training a network to hallucinate past image boundaries. We use a three-phase training schedule to stably train a DCGAN architecture on a subset of the Places365 dataset. In line with Iizuka et al., we also use local discriminators to enhance the quality of our output. Once trained, our model is able to outpaint $128 \times 128$ color images relatively realistically, thus allowing for recursive outpainting. Our results show that deep learning approaches to image outpainting are both feasible and promising.



