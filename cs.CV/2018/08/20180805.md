# Arxiv Papers in cs.CV on 2018-08-05
### 3D Depthwise Convolution: Reducing Model Parameters in 3D Vision Tasks
- **Arxiv ID**: http://arxiv.org/abs/1808.01556v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.01556v1)
- **Published**: 2018-08-05 03:50:54+00:00
- **Updated**: 2018-08-05 03:50:54+00:00
- **Authors**: Rongtian Ye, Fangyu Liu, Liqiang Zhang
- **Comment**: Work in progress
- **Journal**: None
- **Summary**: Standard 3D convolution operations require much larger amounts of memory and computation cost than 2D convolution operations. The fact has hindered the development of deep neural nets in many 3D vision tasks. In this paper, we investigate the possibility of applying depthwise separable convolutions in 3D scenario and introduce the use of 3D depthwise convolution. A 3D depthwise convolution splits a single standard 3D convolution into two separate steps, which would drastically reduce the number of parameters in 3D convolutions with more than one order of magnitude. We experiment with 3D depthwise convolution on popular CNN architectures and also compare it with a similar structure called pseudo-3D convolution. The results demonstrate that, with 3D depthwise convolutions, 3D vision tasks like classification and reconstruction can be carried out with more light-weighted neural networks while still delivering comparable performances.



### Deep Multi-Center Learning for Face Alignment
- **Arxiv ID**: http://arxiv.org/abs/1808.01558v2
- **DOI**: 10.1016/j.neucom.2018.11.108
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.01558v2)
- **Published**: 2018-08-05 04:01:53+00:00
- **Updated**: 2018-11-18 06:30:36+00:00
- **Authors**: Zhiwen Shao, Hengliang Zhu, Xin Tan, Yangyang Hao, Lizhuang Ma
- **Comment**: This paper has been accepted by Neurocomputing
- **Journal**: None
- **Summary**: Facial landmarks are highly correlated with each other since a certain landmark can be estimated by its neighboring landmarks. Most of the existing deep learning methods only use one fully-connected layer called shape prediction layer to estimate the locations of facial landmarks. In this paper, we propose a novel deep learning framework named Multi-Center Learning with multiple shape prediction layers for face alignment. In particular, each shape prediction layer emphasizes on the detection of a certain cluster of semantically relevant landmarks respectively. Challenging landmarks are focused firstly, and each cluster of landmarks is further optimized respectively. Moreover, to reduce the model complexity, we propose a model assembling method to integrate multiple shape prediction layers into one shape prediction layer. Extensive experiments demonstrate that our method is effective for handling complex occlusions and appearance variations with real-time performance. The code for our method is available at https://github.com/ZhiwenShao/MCNet-Extension.



### Tracklet Association Tracker: An End-to-End Learning-based Association Approach for Multi-Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/1808.01562v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.01562v1)
- **Published**: 2018-08-05 05:41:45+00:00
- **Updated**: 2018-08-05 05:41:45+00:00
- **Authors**: Han Shen, Lichao Huang, Chang Huang, Wei Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Traditional multiple object tracking methods divide the task into two parts: affinity learning and data association. The separation of the task requires to define a hand-crafted training goal in affinity learning stage and a hand-crafted cost function of data association stage, which prevents the tracking goals from learning directly from the feature. In this paper, we present a new multiple object tracking (MOT) framework with data-driven association method, named as Tracklet Association Tracker (TAT). The framework aims at gluing feature learning and data association into a unity by a bi-level optimization formulation so that the association results can be directly learned from features. To boost the performance, we also adopt the popular hierarchical association and perform the necessary alignment and selection of raw detection responses. Our model trains over 20X faster than a similar approach, and achieves the state-of-the-art performance on both MOT2016 and MOT2017 benchmarks.



### Improving Deep Visual Representation for Person Re-identification by Global and Local Image-language Association
- **Arxiv ID**: http://arxiv.org/abs/1808.01571v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.01571v1)
- **Published**: 2018-08-05 07:19:24+00:00
- **Updated**: 2018-08-05 07:19:24+00:00
- **Authors**: Dapeng Chen, Hongsheng Li, Xihui Liu, Yantao Shen, Zejian Yuan, Xiaogang Wang
- **Comment**: ECCV
- **Journal**: None
- **Summary**: Person re-identification is an important task that requires learning discriminative visual features for distinguishing different person identities. Diverse auxiliary information has been utilized to improve the visual feature learning. In this paper, we propose to exploit natural language description as additional training supervisions for effective visual features. Compared with other auxiliary information, language can describe a specific person from more compact and semantic visual aspects, thus is complementary to the pixel-level image data. Our method not only learns better global visual feature with the supervision of the overall description but also enforces semantic consistencies between local visual and linguistic features, which is achieved by building global and local image-language associations. The global image-language association is established according to the identity labels, while the local association is based upon the implicit correspondences between image regions and noun phrases. Extensive experiments demonstrate the effectiveness of employing language as training supervisions with the two association schemes. Our method achieves state-of-the-art performance without utilizing any auxiliary information during testing and shows better performance than other joint embedding methods for the image-language association.



### Video Re-localization
- **Arxiv ID**: http://arxiv.org/abs/1808.01575v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.01575v1)
- **Published**: 2018-08-05 07:52:33+00:00
- **Updated**: 2018-08-05 07:52:33+00:00
- **Authors**: Yang Feng, Lin Ma, Wei Liu, Tong Zhang, Jiebo Luo
- **Comment**: None
- **Journal**: None
- **Summary**: Many methods have been developed to help people find the video contents they want efficiently. However, there are still some unsolved problems in this area. For example, given a query video and a reference video, how to accurately localize a segment in the reference video such that the segment semantically corresponds to the query video? We define a distinctively new task, namely \textbf{video re-localization}, to address this scenario. Video re-localization is an important emerging technology implicating many applications, such as fast seeking in videos, video copy detection, video surveillance, etc. Meanwhile, it is also a challenging research task because the visual appearance of a semantic concept in videos can have large variations. The first hurdle to clear for the video re-localization task is the lack of existing datasets. It is labor expensive to collect pairs of videos with semantic coherence or correspondence and label the corresponding segments. We first exploit and reorganize the videos in ActivityNet to form a new dataset for video re-localization research, which consists of about 10,000 videos of diverse visual appearances associated with localized boundary information. Subsequently, we propose an innovative cross gated bilinear matching model such that every time-step in the reference video is matched against the attentively weighted query video. Consequently, the prediction of the starting and ending time is formulated as a classification problem based on the matching results. Extensive experimental results show that the proposed method outperforms the competing methods. Our code is available at: https://github.com/fengyang0317/video_reloc.



### Spherical Harmonic Residual Network for Diffusion Signal Harmonization
- **Arxiv ID**: http://arxiv.org/abs/1808.01595v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1808.01595v1)
- **Published**: 2018-08-05 11:05:23+00:00
- **Updated**: 2018-08-05 11:05:23+00:00
- **Authors**: Simon Koppers, Luke Bloy, Jeffrey I. Berman, Chantal M. W. Tax, J. Christopher Edgar, Dorit Merhof
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Diffusion imaging is an important method in the field of neuroscience, as it is sensitive to changes within the tissue microstructure of the human brain. However, a major challenge when using MRI to derive quantitative measures is that the use of different scanners, as used in multi-site group studies, introduces measurement variability. This can lead to an increased variance in quantitative metrics, even if the same brain is scanned.   Contrary to the assumption that these characteristics are comparable and similar, small changes in these values are observed in many clinical studies, hence harmonization of the signals is essential.   In this paper, we present a method that does not require additional preprocessing, such as segmentation or registration, and harmonizes the signal based on a deep learning residual network. For this purpose, a training database is required, which consist of the same subjects, scanned on different scanners.   The results show that harmonized signals are significantly more similar to the ground truth signal compared to no harmonization, but also improve in comparison to another deep learning method. The same effect is also demonstrated in commonly used metrics derived from the diffusion MRI signal.



### Pixel-level Semantics Guided Image Colorization
- **Arxiv ID**: http://arxiv.org/abs/1808.01597v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.01597v1)
- **Published**: 2018-08-05 11:20:12+00:00
- **Updated**: 2018-08-05 11:20:12+00:00
- **Authors**: Jiaojiao Zhao, Li Liu, Cees G. M. Snoek, Jungong Han, Ling Shao
- **Comment**: None
- **Journal**: None
- **Summary**: While many image colorization algorithms have recently shown the capability of producing plausible color versions from gray-scale photographs, they still suffer from the problems of context confusion and edge color bleeding. To address context confusion, we propose to incorporate the pixel-level object semantics to guide the image colorization. The rationale is that human beings perceive and distinguish colors based on the object's semantic categories. We propose a hierarchical neural network with two branches. One branch learns what the object is while the other branch learns the object's colors. The network jointly optimizes a semantic segmentation loss and a colorization loss. To attack edge color bleeding we generate more continuous color maps with sharp edges by adopting a joint bilateral upsamping layer at inference. Our network is trained on PASCAL VOC2012 and COCO-stuff with semantic segmentation labels and it produces more realistic and finer results compared to the colorization state-of-the-art.



### A novel method for predicting and mapping the presence of sun glare using Google Street View
- **Arxiv ID**: http://arxiv.org/abs/1808.04436v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.04436v1)
- **Published**: 2018-08-05 11:44:36+00:00
- **Updated**: 2018-08-05 11:44:36+00:00
- **Authors**: Xiaojiang Li, Bill Yang Cai, Waishan Qiu, Jinhua Zhao, Carlo Ratti
- **Comment**: 21 pages, 10 figures
- **Journal**: None
- **Summary**: The sun glare is one of the major environmental hazards that cause traffic accidents. Every year, many people died and injured in traffic accidents related to sun glare. Providing accurate information about when and where sun glare happens would be helpful to prevent sun glare caused traffic accidents and save lives. In this study, we proposed to use publicly accessible Google Street View (GSV) panorama images to estimate and predict the occurrence of sun glare. GSV images have view sight similar to drivers, which would make GSV images suitable for estimating the visibility of sun glare to drivers. A recently developed convolutional neural network algorithm was used to segment GSV images and predict obstructions on sun glare. Based on the predicted obstructions for given locations, we further estimated the time windows of sun glare by estimating the sun positions and the relative angles between drivers and the sun for those locations. We conducted a case study in Cambridge, Massachusetts, USA. Results show that the method can predict the presence of sun glare precisely. The proposed method would provide an important tool for drivers and traffic planners to mitigate the sun glare and decrease the potential traffic accidents caused by the sun glare.



### Learning monocular depth estimation with unsupervised trinocular assumptions
- **Arxiv ID**: http://arxiv.org/abs/1808.01606v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.01606v1)
- **Published**: 2018-08-05 12:33:06+00:00
- **Updated**: 2018-08-05 12:33:06+00:00
- **Authors**: Matteo Poggi, Fabio Tosi, Stefano Mattoccia
- **Comment**: 14 pages, 7 figures, 4 tables. Accepted to 3DV 2018
- **Journal**: None
- **Summary**: Obtaining accurate depth measurements out of a single image represents a fascinating solution to 3D sensing. CNNs led to considerable improvements in this field, and recent trends replaced the need for ground-truth labels with geometry-guided image reconstruction signals enabling unsupervised training. Currently, for this purpose, state-of-the-art techniques rely on images acquired with a binocular stereo rig to predict inverse depth (i.e., disparity) according to the aforementioned supervision principle. However, these methods suffer from well-known problems near occlusions, left image border, etc inherited from the stereo setup. Therefore, in this paper, we tackle these issues by moving to a trinocular domain for training. Assuming the central image as the reference, we train a CNN to infer disparity representations pairing such image with frames on its left and right side. This strategy allows obtaining depth maps not affected by typical stereo artifacts. Moreover, being trinocular datasets seldom available, we introduce a novel interleaved training procedure enabling to enforce the trinocular assumption outlined from current binocular datasets. Exhaustive experimental results on the KITTI dataset confirm that our proposal outperforms state-of-the-art methods for unsupervised monocular depth estimation trained on binocular stereo pairs as well as any known methods relying on other cues.



### Classification of Dermoscopy Images using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1808.01607v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.01607v1)
- **Published**: 2018-08-05 12:40:23+00:00
- **Updated**: 2018-08-05 12:40:23+00:00
- **Authors**: Nithin D Reddy
- **Comment**: None
- **Journal**: None
- **Summary**: Skin cancer is one of the most common forms of cancer and its incidence is projected to rise over the next decade. Artificial intelligence is a viable solution to the issue of providing quality care to patients in areas lacking access to trained dermatologists. Considerable progress has been made in the use of automated applications for accurate classification of skin lesions from digital images. In this manuscript, we discuss the design and implementation of a deep learning algorithm for classification of dermoscopy images from the HAM10000 Dataset. We trained a convolutional neural network based on the ResNet50 architecture to accurately classify dermoscopy images of skin lesions into one of seven disease categories. Using our custom model, we obtained a balanced accuracy of 91% on the validation dataset.



### Multi-Scale Supervised Network for Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1808.01623v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.01623v1)
- **Published**: 2018-08-05 14:13:10+00:00
- **Updated**: 2018-08-05 14:13:10+00:00
- **Authors**: Lipeng Ke, Ming-Ching Chang, Honggang Qi, Siwei Lyu
- **Comment**: Accepted by ICIP2018. arXiv admin note: text overlap with
  arXiv:1803.09894
- **Journal**: None
- **Summary**: Human pose estimation is an important topic in computer vision with many applications including gesture and activity recognition. However, pose estimation from image is challenging due to appearance variations, occlusions, clutter background, and complex activities. To alleviate these problems, we develop a robust pose estimation method based on the recent deep conv-deconv modules with two improvements: (1) multi-scale supervision of body keypoints, and (2) a global regression to improve structural consistency of keypoints. We refine keypoint detection heatmaps using layer-wise multi-scale supervision to better capture local contexts. Pose inference via keypoint association is optimized globally using a regression network at the end. Our method can effectively disambiguate keypoint matches in close proximity including the mismatch of left-right body parts, and better infer occluded parts. Experimental results show that our method achieves competitive performance among state-of-the-art methods on the MPII and FLIC datasets.



### Towards Closing the Gap in Weakly Supervised Semantic Segmentation with DCNNs: Combining Local and Global Models
- **Arxiv ID**: http://arxiv.org/abs/1808.01625v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.01625v3)
- **Published**: 2018-08-05 14:19:19+00:00
- **Updated**: 2019-04-29 22:39:29+00:00
- **Authors**: Christoph Mayer, Radu Timofte, Grégory Paul
- **Comment**: None
- **Journal**: None
- **Summary**: Generating training sets for deep convolutional neural networks (DCNNs) is a bottleneck for modern real-world applications. This is a demanding task for applications where annotating training data is costly, such as in semantic segmentation. In the literature, there is still a gap between the performance achieved by a network trained on full and on weak annotations. In this paper, we establish a strategy to measure this gap and to identify the ingredients necessary to reduce it.   On scribbles, we establish new state-of-the-art results: we obtain a mIoU of 75.6% without, and 75.7% with CRF post-processing. We reduce the gap by 64.2% whereas the current state-of-the-art reduces it only by 57.5%. Thanks to a systematic study of the different ingredients involved in the weakly supervised scenario and an original experimental strategy, we unravel a counter-intuitive mechanism that is simple and amenable to generalisations to other weakly-supervised scenarios: averaging poor local predicted annotations with the baseline ones and reuse them for training a DCNN yields new state-of-the-art results.



### Self-Attention Recurrent Network for Saliency Detection
- **Arxiv ID**: http://arxiv.org/abs/1808.01634v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.01634v1)
- **Published**: 2018-08-05 15:31:22+00:00
- **Updated**: 2018-08-05 15:31:22+00:00
- **Authors**: Fengdong Sun, Wenhui Li, Yuanyuan Guan
- **Comment**: None
- **Journal**: None
- **Summary**: Feature maps in deep neural network generally contain different semantics. Existing methods often omit their characteristics that may lead to sub-optimal results. In this paper, we propose a novel end-to-end deep saliency network which could effectively utilize multi-scale feature maps according to their characteristics. Shallow layers often contain more local information, and deep layers have advantages in global semantics. Therefore, the network generates elaborate saliency maps by enhancing local and global information of feature maps in different layers. On one hand, local information of shallow layers is enhanced by a recurrent structure which shared convolution kernel at different time steps. On the other hand, global information of deep layers is utilized by a self-attention module, which generates different attention weights for salient objects and backgrounds thus achieve better performance. Experimental results on four widely used datasets demonstrate that our method has advantages in performance over existing algorithms.



### Multi-Objective Cognitive Model: a supervised approach for multi-subject fMRI analysis
- **Arxiv ID**: http://arxiv.org/abs/1808.01642v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG, math.OC, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1808.01642v1)
- **Published**: 2018-08-05 16:19:56+00:00
- **Updated**: 2018-08-05 16:19:56+00:00
- **Authors**: Muhammad Yousefnezhad, Daoqiang Zhang
- **Comment**: Neuroinformatics, Springer
- **Journal**: None
- **Summary**: In order to decode the human brain, Multivariate Pattern (MVP) classification generates cognitive models by using functional Magnetic Resonance Imaging (fMRI) datasets. As a standard pipeline in the MVP analysis, brain patterns in multi-subject fMRI dataset must be mapped to a shared space and then a classification model is generated by employing the mapped patterns. However, the MVP models may not provide stable performance on a new fMRI dataset because the standard pipeline uses disjoint steps for generating these models. Indeed, each step in the pipeline includes an objective function with independent optimization approach, where the best solution of each step may not be optimum for the next steps. For tackling the mentioned issue, this paper introduces the Multi-Objective Cognitive Model (MOCM) that utilizes an integrated objective function for MVP analysis rather than just using those disjoint steps. For solving the integrated problem, we proposed a customized multi-objective optimization approach, where all possible solutions are firstly generated, and then our method ranks and selects the robust solutions as the final results. Empirical studies confirm that the proposed method can generate superior performance in comparison with other techniques.



### Dilated Convolutions in Neural Networks for Left Atrial Segmentation in 3D Gadolinium Enhanced-MRI
- **Arxiv ID**: http://arxiv.org/abs/1808.01673v1
- **DOI**: 10.1007/978-3-030-12029-0_35
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.01673v1)
- **Published**: 2018-08-05 19:04:17+00:00
- **Updated**: 2018-08-05 19:04:17+00:00
- **Authors**: Sulaiman Vesal, Nishant Ravikumar, Andreas Maier
- **Comment**: Accepted in SATCOM-MICCAI 2018 Workshop
- **Journal**: STACOM 2018 Proceedings
- **Summary**: Segmentation of the left atrial chamber and assessing its morphology, are essential for improving our understanding of atrial fibrillation, the most common type of cardiac arrhythmia. Automation of this process in 3D gadolinium enhanced-MRI (GE-MRI) data is desirable, as manual delineation is time-consuming, challenging and observer-dependent. Recently, deep convolutional neural networks (CNNs) have gained tremendous traction and achieved state-of-the-art results in medical image segmentation. However, it is difficult to incorporate local and global information without using contracting (pooling) layers, which in turn reduces segmentation accuracy for smaller structures. In this paper, we propose a 3D CNN for volumetric segmentation of the left atrial chamber in LGE-MRI. Our network is based on the well known U-Net architecture. We employ a 3D fully convolutional network, with dilated convolutions in the lowest level of the network, and residual connections between encoder blocks to incorporate local and global knowledge. The results show that including global context through the use of dilated convolutions, helps in domain adaptation, and the overall segmentation accuracy is improved in comparison to a 3D U-Net.



### 3D Conceptual Design Using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1808.01675v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.01675v1)
- **Published**: 2018-08-05 19:09:12+00:00
- **Updated**: 2018-08-05 19:09:12+00:00
- **Authors**: Zhangsihao Yang, Haoliang Jiang, Zou Lan
- **Comment**: None
- **Journal**: None
- **Summary**: This article proposes a data-driven methodology to achieve a fast design support, in order to generate or develop novel designs covering multiple object categories. This methodology implements two state-of-the-art Variational Autoencoder dealing with 3D model data. Our methodology constructs a self-defined loss function. The loss function, containing the outputs of certain layers in the autoencoder, obtains combination of different latent features from different 3D model categories.   Additionally, this article provide detail explanation to utilize the Princeton ModelNet40 database, a comprehensive clean collection of 3D CAD models for objects. After convert the original 3D mesh file to voxel and point cloud data type, we enable to feed our autoencoder with data of the same size of dimension. The novelty of this work is to leverage the power of deep learning methods as an efficient latent feature extractor to explore unknown designing areas. Through this project, we expect the output can show a clear and smooth interpretation of model from different categories to develop a fast design support to generate novel shapes. This final report will explore 1) the theoretical ideas, 2) the progresses to implement Variantional Autoencoder to attain implicit features from input shapes, 3) the results of output shapes during training in selected domains of both 3D voxel data and 3D point cloud data, and 4) our conclusion and future work to achieve the more outstanding goal.



### A Multi-task Framework for Skin Lesion Detection and Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1808.01676v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.01676v1)
- **Published**: 2018-08-05 19:13:15+00:00
- **Updated**: 2018-08-05 19:13:15+00:00
- **Authors**: Sulaiman Vesal, Shreyas Malakarjun Patil, Nishant Ravikumar, Andreas Maier
- **Comment**: Accepted in ISIC-MICCAI 2018 Workshop
- **Journal**: OR 2.0 Context-Aware Operating Theaters, Computer Assisted Robotic
  Endoscopy, Clinical Image-Based Procedures, and Skin Image Analysis 2018
- **Summary**: Early detection and segmentation of skin lesions is crucial for timely diagnosis and treatment, necessary to improve the survival rate of patients. However, manual delineation is time consuming and subject to intra- and inter-observer variations among dermatologists. This underlines the need for an accurate and automatic approach to skin lesion segmentation. To tackle this issue, we propose a multi-task convolutional neural network (CNN) based, joint detection and segmentation framework, designed to initially localize the lesion and subsequently, segment it. A `Faster region-based convolutional neural network' (Faster-RCNN) which comprises a region proposal network (RPN), is used to generate bounding boxes/region proposals, for lesion localization in each image. The proposed regions are subsequently refined using a softmax classifier and a bounding-box regressor. The refined bounding boxes are finally cropped and segmented using `SkinNet', a modified version of U-Net. We trained and evaluated the performance of our network, using the ISBI 2017 challenge and the PH2 datasets, and compared it with the state-of-the-art, using the official test data released as part of the challenge for the former. Our approach outperformed others in terms of Dice coefficients ($>0.93$), Jaccard index ($>0.88$), accuracy ($>0.96$) and sensitivity ($>0.95$), across five-fold cross validation experiments.



### Kid on The Phone! Toward Automatic Detection of Children on Mobile Devices
- **Arxiv ID**: http://arxiv.org/abs/1808.01680v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1808.01680v1)
- **Published**: 2018-08-05 19:59:35+00:00
- **Updated**: 2018-08-05 19:59:35+00:00
- **Authors**: Toan Nguyen, Aditi Roy, Nasir Memon
- **Comment**: Under peer review
- **Journal**: None
- **Summary**: Studies have shown that children can be exposed to smart devices at a very early age. This has important implications on research in children-computer interaction, children online safety and early education. Many systems have been built based on such research. In this work, we present multiple techniques to automatically detect the presence of a child on a smart device, which could be used as the first step on such systems. Our methods distinguish children from adults based on behavioral differences while operating a touch-enabled modern computing device. Behavioral differences are extracted from data recorded by the touchscreen and built-in sensors. To evaluate the effectiveness of the proposed methods, a new data set has been created from 50 children and adults who interacted with off-the-shelf applications on smart phones. Results show that it is possible to achieve 99% accuracy and less than 0.5% error rate after 8 consecutive touch gestures using only touch information or 5 seconds of sensor reading. If information is used from multiple sensors, then only after 3 gestures, similar performance could be achieved.



### Too many secants: a hierarchical approach to secant-based dimensionality reduction on large data sets
- **Arxiv ID**: http://arxiv.org/abs/1808.01686v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/1808.01686v1)
- **Published**: 2018-08-05 21:27:32+00:00
- **Updated**: 2018-08-05 21:27:32+00:00
- **Authors**: Henry Kvinge, Elin Farnell, Michael Kirby, Chris Peterson
- **Comment**: To appear in the Proceedings of the 2018 IEEE High Performance
  Extreme Computing Conference, Waltham, MA USA
- **Journal**: None
- **Summary**: A fundamental question in many data analysis settings is the problem of discerning the "natural" dimension of a data set. That is, when a data set is drawn from a manifold (possibly with noise), a meaningful aspect of the data is the dimension of that manifold. Various approaches exist for estimating this dimension, such as the method of Secant-Avoidance Projection (SAP). Intuitively, the SAP algorithm seeks to determine a projection which best preserves the lengths of all secants between points in a data set; by applying the algorithm to find the best projections to vector spaces of various dimensions, one may infer the dimension of the manifold of origination. That is, one may learn the dimension at which it is possible to construct a diffeomorphic copy of the data in a lower-dimensional Euclidean space. Using Whitney's embedding theorem, we can relate this information to the natural dimension of the data. A drawback of the SAP algorithm is that a data set with $T$ points has $O(T^2)$ secants, making the computation and storage of all secants infeasible for very large data sets. In this paper, we propose a novel algorithm that generalizes the SAP algorithm with an emphasis on addressing this issue. That is, we propose a hierarchical secant-based dimensionality-reduction method, which can be employed for data sets where explicitly calculating all secants is not feasible.



### Is Robustness the Cost of Accuracy? -- A Comprehensive Study on the Robustness of 18 Deep Image Classification Models
- **Arxiv ID**: http://arxiv.org/abs/1808.01688v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.01688v2)
- **Published**: 2018-08-05 21:43:01+00:00
- **Updated**: 2019-03-04 00:35:26+00:00
- **Authors**: Dong Su, Huan Zhang, Hongge Chen, Jinfeng Yi, Pin-Yu Chen, Yupeng Gao
- **Comment**: Accepted by the European Conference on Computer Vision (ECCV) 2018
- **Journal**: None
- **Summary**: The prediction accuracy has been the long-lasting and sole standard for comparing the performance of different image classification models, including the ImageNet competition. However, recent studies have highlighted the lack of robustness in well-trained deep neural networks to adversarial examples. Visually imperceptible perturbations to natural images can easily be crafted and mislead the image classifiers towards misclassification. To demystify the trade-offs between robustness and accuracy, in this paper we thoroughly benchmark 18 ImageNet models using multiple robustness metrics, including the distortion, success rate and transferability of adversarial examples between 306 pairs of models. Our extensive experimental results reveal several new insights: (1) linear scaling law - the empirical $\ell_2$ and $\ell_\infty$ distortion metrics scale linearly with the logarithm of classification error; (2) model architecture is a more critical factor to robustness than model size, and the disclosed accuracy-robustness Pareto frontier can be used as an evaluation criterion for ImageNet model designers; (3) for a similar network architecture, increasing network depth slightly improves robustness in $\ell_\infty$ distortion; (4) there exist models (in VGG family) that exhibit high adversarial transferability, while most adversarial examples crafted from one model can only be transferred within the same family. Experiment code is publicly available at \url{https://github.com/huanzhang12/Adversarial_Survey}.



### Skin Lesion Diagnosis using Ensembles, Unscaled Multi-Crop Evaluation and Loss Weighting
- **Arxiv ID**: http://arxiv.org/abs/1808.01694v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.01694v1)
- **Published**: 2018-08-05 22:23:23+00:00
- **Updated**: 2018-08-05 22:23:23+00:00
- **Authors**: Nils Gessert, Thilo Sentker, Frederic Madesta, Rüdiger Schmitz, Helge Kniep, Ivo Baltruschat, René Werner, Alexander Schlaefer
- **Comment**: ISIC Skin Image Analysis Workshop and Challenge @ MICCAI 2018. Second
  place at challenge, best with public data, see
  https://challenge2018.isic-archive.com/leaderboards/
- **Journal**: None
- **Summary**: In this paper we present the methods of our submission to the ISIC 2018 challenge for skin lesion diagnosis (Task 3). The dataset consists of 10000 images with seven image-level classes to be distinguished by an automated algorithm. We employ an ensemble of convolutional neural networks for this task. In particular, we fine-tune pretrained state-of-the-art deep learning models such as Densenet, SENet and ResNeXt. We identify heavy class imbalance as a key problem for this challenge and consider multiple balancing approaches such as loss weighting and balanced batch sampling. Another important feature of our pipeline is the use of a vast amount of unscaled crops for evaluation. Last, we consider meta learning approaches for the final predictions. Our team placed second at the challenge while being the best approach using only publicly available data.



