# Arxiv Papers in cs.CV on 2018-08-31
### Multi-Cell Multi-Task Convolutional Neural Networks for Diabetic Retinopathy Grading
- **Arxiv ID**: http://arxiv.org/abs/1808.10564v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1808.10564v2)
- **Published**: 2018-08-31 01:21:46+00:00
- **Updated**: 2018-10-11 06:10:57+00:00
- **Authors**: Kang Zhou, Zaiwang Gu, Wen Liu, Weixin Luo, Jun Cheng, Shenghua Gao, Jiang Liu
- **Comment**: Accepted by EMBC 2018
- **Journal**: None
- **Summary**: Diabetic Retinopathy (DR) is a non-negligible eye disease among patients with Diabetes Mellitus, and automatic retinal image analysis algorithm for the DR screening is in high demand. Considering the resolution of retinal image is very high, where small pathological tissues can be detected only with large resolution image and large local receptive field are required to identify those late stage disease, but directly training a neural network with very deep architecture and high resolution image is both time computational expensive and difficult because of gradient vanishing/exploding problem, we propose a \textbf{Multi-Cell} architecture which gradually increases the depth of deep neural network and the resolution of input image, which both boosts the training time but also improves the classification accuracy. Further, considering the different stages of DR actually progress gradually, which means the labels of different stages are related. To considering the relationships of images with different stages, we propose a \textbf{Multi-Task} learning strategy which predicts the label with both classification and regression. Experimental results on the Kaggle dataset show that our method achieves a Kappa of 0.841 on test set which is the 4-th rank of all state-of-the-arts methods. Further, our Multi-Cell Multi-Task Convolutional Neural Networks (M$^2$CNN) solution is a general framework, which can be readily integrated with many other deep neural network architectures.



### Learning to Describe Differences Between Pairs of Similar Images
- **Arxiv ID**: http://arxiv.org/abs/1808.10584v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1808.10584v1)
- **Published**: 2018-08-31 03:15:28+00:00
- **Updated**: 2018-08-31 03:15:28+00:00
- **Authors**: Harsh Jhamtani, Taylor Berg-Kirkpatrick
- **Comment**: EMNLP 2018
- **Journal**: None
- **Summary**: In this paper, we introduce the task of automatically generating text to describe the differences between two similar images. We collect a new dataset by crowd-sourcing difference descriptions for pairs of image frames extracted from video-surveillance footage. Annotators were asked to succinctly describe all the differences in a short paragraph. As a result, our novel dataset provides an opportunity to explore models that align language and vision, and capture visual salience. The dataset may also be a useful benchmark for coherent multi-sentence generation. We perform a firstpass visual analysis that exposes clusters of differing pixels as a proxy for object-level differences. We propose a model that captures visual salience by using a latent variable to align clusters of differing pixels with output sentences. We find that, for both single-sentence generation and as well as multi-sentence generation, the proposed model outperforms the models that use attention alone.



### A Unified Mammogram Analysis Method via Hybrid Deep Supervision
- **Arxiv ID**: http://arxiv.org/abs/1808.10646v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.10646v1)
- **Published**: 2018-08-31 09:32:52+00:00
- **Updated**: 2018-08-31 09:32:52+00:00
- **Authors**: Rongzhao Zhang, Han Zhang, Albert C. S. Chung
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic mammogram classification and mass segmentation play a critical role in a computer-aided mammogram screening system. In this work, we present a unified mammogram analysis framework for both whole-mammogram classification and segmentation. Our model is designed based on a deep U-Net with residual connections, and equipped with the novel hybrid deep supervision (HDS) scheme for end-to-end multi-task learning. As an extension of deep supervision (DS), HDS not only can force the model to learn more discriminative features like DS, but also seamlessly integrates segmentation and classification tasks into one model, thus the model can benefit from both pixel-wise and image-wise supervisions. We extensively validate the proposed method on the widely-used INbreast dataset. Ablation study corroborates that pixel-wise and image-wise supervisions are mutually beneficial, evidencing the efficacy of HDS. The results of 5-fold cross validation indicate that our unified model matches state-of-the-art performance on both mammogram segmentation and classification tasks, which achieves an average segmentation Dice similarity coefficient (DSC) of 0.85 and a classification accuracy of 0.89. The code is available at https://github.com/angrypudding/hybrid-ds.



### Gibson Env: Real-World Perception for Embodied Agents
- **Arxiv ID**: http://arxiv.org/abs/1808.10654v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.GR, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1808.10654v1)
- **Published**: 2018-08-31 09:56:43+00:00
- **Updated**: 2018-08-31 09:56:43+00:00
- **Authors**: Fei Xia, Amir Zamir, Zhi-Yang He, Alexander Sax, Jitendra Malik, Silvio Savarese
- **Comment**: Access the code, dataset, and project website at
  http://gibsonenv.vision/ . CVPR 2018
- **Journal**: CVPR 2018
- **Summary**: Developing visual perception models for active agents and sensorimotor control are cumbersome to be done in the physical world, as existing algorithms are too slow to efficiently learn in real-time and robots are fragile and costly. This has given rise to learning-in-simulation which consequently casts a question on whether the results transfer to real-world. In this paper, we are concerned with the problem of developing real-world perception for active agents, propose Gibson Virtual Environment for this purpose, and showcase sample perceptual tasks learned therein. Gibson is based on virtualizing real spaces, rather than using artificially designed ones, and currently includes over 1400 floor spaces from 572 full buildings. The main characteristics of Gibson are: I. being from the real-world and reflecting its semantic complexity, II. having an internal synthesis mechanism, "Goggles", enabling deploying the trained models in real-world without needing further domain adaptation, III. embodiment of agents and making them subject to constraints of physics and space.



### JuncNet: A Deep Neural Network for Road Junction Disambiguation for Autonomous Vehicles
- **Arxiv ID**: http://arxiv.org/abs/1809.01011v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1809.01011v1)
- **Published**: 2018-08-31 11:18:18+00:00
- **Updated**: 2018-08-31 11:18:18+00:00
- **Authors**: Saumya Kumaar, Navaneethkrishnan B, Sumedh Mannar, S N Omkar
- **Comment**: None
- **Journal**: None
- **Summary**: With a great amount of research going on in the field of autonomous vehicles or self-driving cars, there has been considerable progress in road detection and tracking algorithms. Most of these algorithms use GPS to handle road junctions and its subsequent decisions. However, there are places in the urban environment where it becomes difficult to get GPS fixes which render the junction decision handling erroneous or possibly risky. Vision-based junction detection, however, does not have such problems. This paper proposes a novel deep convolutional neural network architecture for disambiguation of junctions from roads with a high degree of accuracy. This network is benchmarked against other well known classifying network architectures like AlexNet and VGGnet. Further, we discuss a potential road navigation methodology which uses the proposed network model. We conclude by performing an experimental validation of the trained network and the navigational method on the roads of the Indian Institute of Science (IISc).



### MobiBits: Multimodal Mobile Biometric Database
- **Arxiv ID**: http://arxiv.org/abs/1808.10710v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.10710v1)
- **Published**: 2018-08-31 12:38:09+00:00
- **Updated**: 2018-08-31 12:38:09+00:00
- **Authors**: Ewelina Bartuzi, Katarzyna Roszczewska, Mateusz Trokielewicz, Radosław Białobrzeski
- **Comment**: Submitted for the BIOSIG2018 conference on June 18, 2018. Accepted
  for publication on July 20, 2018
- **Journal**: None
- **Summary**: This paper presents a novel database comprising representations of five different biometric characteristics, collected in a mobile, unconstrained or semi-constrained setting with three different mobile devices, including characteristics previously unavailable in existing datasets, namely hand images, thermal hand images, and thermal face images, all acquired with a mobile, off-the-shelf device. In addition to this collection of data we perform an extensive set of experiments providing insight on benchmark recognition performance that can be achieved with these data, carried out with existing commercial and academic biometric solutions. This is the first known to us mobile biometric database introducing samples of biometric traits such as thermal hand images and thermal face images. We hope that this contribution will make a valuable addition to the already existing databases and enable new experiments and studies in the field of mobile authentication. The MobiBits database is made publicly available to the research community at no cost for non-commercial purposes.



### Spoofing PRNU Patterns of Iris Sensors while Preserving Iris Recognition
- **Arxiv ID**: http://arxiv.org/abs/1808.10765v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.10765v2)
- **Published**: 2018-08-31 14:21:29+00:00
- **Updated**: 2018-12-15 17:42:47+00:00
- **Authors**: Sudipta Banerjee, Vahid Mirjalili, Arun Ross
- **Comment**: 10 pages, 7 figures
- **Journal**: None
- **Summary**: The principle of Photo Response Non-Uniformity (PRNU) is used to link an image with its source, i.e., the sensor that produced it. In this work, we investigate if it is possible to modify an iris image acquired using one sensor in order to spoof the PRNU noise pattern of a different sensor. In this regard, we develop an image perturbation routine that iteratively modifies blocks of pixels in the original iris image such that its PRNU pattern approaches that of a target sensor. Experiments indicate the efficacy of the proposed perturbation method in spoofing PRNU patterns present in an iris image whilst still retaining its biometric content.



### Seeing Colors: Learning Semantic Text Encoding for Classification
- **Arxiv ID**: http://arxiv.org/abs/1808.10822v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.10822v1)
- **Published**: 2018-08-31 15:55:56+00:00
- **Updated**: 2018-08-31 15:55:56+00:00
- **Authors**: Shah Nawaz, Alessandro Calefati, Muhammad Kamran Janjua, Ignazio Gallo
- **Comment**: 9 pages. Under review at IJDAR
- **Journal**: None
- **Summary**: The question we answer with this work is: can we convert a text document into an image to exploit best image classification models to classify documents? To answer this question we present a novel text classification method which converts a text document into an encoded image, using word embedding and capabilities of Convolutional Neural Networks (CNNs), successfully employed in image classification. We evaluate our approach by obtaining promising results on some well-known benchmark datasets for text classification. This work allows the application of many of the advanced CNN architectures developed for Computer Vision to Natural Language Processing. We test the proposed approach on a multi-modal dataset, proving that it is possible to use a single deep model to represent text and image in the same feature space.



### Automated segmentation on the entire cardiac cycle using a deep learning work-flow
- **Arxiv ID**: http://arxiv.org/abs/1809.01015v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.01015v1)
- **Published**: 2018-08-31 17:07:31+00:00
- **Updated**: 2018-08-31 17:07:31+00:00
- **Authors**: Nicoló Savioli, Miguel Silva Vieira, Pablo Lamata, Giovanni Montana
- **Comment**: 6 pages, 2 figures, published on IEEE Xplore
- **Journal**: None
- **Summary**: The segmentation of the left ventricle (LV) from CINE MRI images is essential to infer important clinical parameters. Typically, machine learning algorithms for automated LV segmentation use annotated contours from only two cardiac phases, diastole, and systole. In this work, we present an analysis work-flow for fully-automated LV segmentation that learns from images acquired through the cardiac cycle. The workflow consists of three components: first, for each image in the sequence, we perform an automated localization and subsequent cropping of the bounding box containing the cardiac silhouette. Second, we identify the LV contours using a Temporal Fully Convolutional Neural Network (T-FCNN), which extends Fully Convolutional Neural Networks (FCNN) through a recurrent mechanism enforcing temporal coherence across consecutive frames. Finally, we further defined the boundaries using either one of two components: fully-connected Conditional Random Fields (CRFs) with Gaussian edge potentials and Semantic Flow. Our initial experiments suggest that significant improvement in performance can potentially be achieved by using a recurrent neural network component that explicitly learns cardiac motion patterns whilst performing LV segmentation.



### Fully Dense UNet for 2D Sparse Photoacoustic Tomography Artifact Removal
- **Arxiv ID**: http://arxiv.org/abs/1808.10848v3
- **DOI**: 10.1109/JBHI.2019.2912935
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.10848v3)
- **Published**: 2018-08-31 17:23:44+00:00
- **Updated**: 2019-04-25 14:37:03+00:00
- **Authors**: Steven Guan, Amir Khan, Siddhartha Sikdar, Parag V. Chitnis
- **Comment**: None
- **Journal**: None
- **Summary**: Photoacoustic imaging is an emerging imaging modality that is based upon the photoacoustic effect. In photoacoustic tomography (PAT), the induced acoustic pressure waves are measured by an array of detectors and used to reconstruct an image of the initial pressure distribution. A common challenge faced in PAT is that the measured acoustic waves can only be sparsely sampled. Reconstructing sparsely sampled data using standard methods results in severe artifacts that obscure information within the image. We propose a modified convolutional neural network (CNN) architecture termed Fully Dense UNet (FD-UNet) for removing artifacts from 2D PAT images reconstructed from sparse data and compare the proposed CNN with the standard UNet in terms of reconstructed image quality.



### Automatic Lung Cancer Prediction from Chest X-ray Images Using Deep Learning Approach
- **Arxiv ID**: http://arxiv.org/abs/1808.10858v1
- **DOI**: 10.1109/BMEiCON.2018.8609997
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1808.10858v1)
- **Published**: 2018-08-31 17:36:59+00:00
- **Updated**: 2018-08-31 17:36:59+00:00
- **Authors**: Worawate Ausawalaithong, Sanparith Marukatat, Arjaree Thirach, Theerawit Wilaiprasitporn
- **Comment**: None
- **Journal**: 2018 11th Biomedical Engineering International Conference
  (BMEiCON)
- **Summary**: Since, cancer is curable when diagnosed at an early stage, lung cancer screening plays an important role in preventive care. Although both low dose computed tomography (LDCT) and computed tomography (CT) scans provide more medical information than normal chest x-rays, there is very limited access to these technologies in rural areas. Recently, there is a trend in using computer-aided diagnosis (CADx) to assist in screening and diagnosing of cancer from biomedical images. In this study, the 121-layer convolutional neural network also known as DenseNet-121 by G. Huang et. al., along with the transfer learning scheme was explored as a means to classify lung cancer using chest X-ray images. The model was trained on a lung nodules dataset before training on the lung cancer dataset to alleviate the problem of a small dataset. The proposed model yields 74.43$\pm$6.01\% of mean accuracy, 74.96$\pm$9.85\% of mean specificity, and 74.68$\pm$15.33\% of mean sensitivity. The proposed model also provides a heatmap for identifying the location of the lung nodule. These findings are promising for further development of chest x-ray-based lung cancer diagnosis using the deep learning approach. Moreover, these findings solve the problem of small dataset.



### Open Source Dataset and Machine Learning Techniques for Automatic Recognition of Historical Graffiti
- **Arxiv ID**: http://arxiv.org/abs/1808.10862v1
- **DOI**: 10.1007/978-3-030-04221-9_37
- **Categories**: **cs.LG**, cs.CV, cs.CY, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1808.10862v1)
- **Published**: 2018-08-31 17:43:21+00:00
- **Updated**: 2018-08-31 17:43:21+00:00
- **Authors**: Nikita Gordienko, Peng Gang, Yuri Gordienko, Wei Zeng, Oleg Alienin, Oleksandr Rokovyi, Sergii Stirenko
- **Comment**: 11 pages, 9 figures, accepted for 25th International Conference on
  Neural Information Processing (ICONIP 2018), 14-16 December, 2018 (Siem Reap,
  Cambodia)
- **Journal**: In: Cheng L., Leung A., Ozawa S. (eds) Neural Information
  Processing. ICONIP 2018. Lecture Notes in Computer Science, vol. 11305, pp.
  414-424. Springer, Cham
- **Summary**: Machine learning techniques are presented for automatic recognition of the historical letters (XI-XVIII centuries) carved on the stoned walls of St.Sophia cathedral in Kyiv (Ukraine). A new image dataset of these carved Glagolitic and Cyrillic letters (CGCL) was assembled and pre-processed for recognition and prediction by machine learning methods. The dataset consists of more than 4000 images for 34 types of letters. The explanatory data analysis of CGCL and notMNIST datasets shown that the carved letters can hardly be differentiated by dimensionality reduction methods, for example, by t-distributed stochastic neighbor embedding (tSNE) due to the worse letter representation by stone carving in comparison to hand writing. The multinomial logistic regression (MLR) and a 2D convolutional neural network (CNN) models were applied. The MLR model demonstrated the area under curve (AUC) values for receiver operating characteristic (ROC) are not lower than 0.92 and 0.60 for notMNIST and CGCL, respectively. The CNN model gave AUC values close to 0.99 for both notMNIST and CGCL (despite the much smaller size and quality of CGCL in comparison to notMNIST) under condition of the high lossy data augmentation. CGCL dataset was published to be available for the data science community as an open source resource.



### Performance Analysis of Plug-and-Play ADMM: A Graph Signal Processing Perspective
- **Arxiv ID**: http://arxiv.org/abs/1809.00020v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1809.00020v3)
- **Published**: 2018-08-31 18:45:02+00:00
- **Updated**: 2019-05-17 21:20:09+00:00
- **Authors**: Stanley H. Chan
- **Comment**: None
- **Journal**: None
- **Summary**: The Plug-and-Play (PnP) ADMM algorithm is a powerful image restoration framework that allows advanced image denoising priors to be integrated into physical forward models to generate high quality image restoration results. However, despite the enormous number of applications and several theoretical studies trying to prove the convergence by leveraging tools in convex analysis, very little is known about why the algorithm is doing so well. The goal of this paper is to fill the gap by discussing the performance of PnP ADMM. By restricting the denoisers to the class of graph filters under a linearity assumption, or more specifically the symmetric smoothing filters, we offer three contributions: (1) We show conditions under which an equivalent maximum-a-posteriori (MAP) optimization exists, (2) we present a geometric interpretation and show that the performance gain is due to an intrinsic pre-denoising characteristic of the PnP prior, (3) we introduce a new analysis technique via the concept of consensus equilibrium, and provide interpretations to problems involving multiple priors.



### Aesthetic Features for Personalized Photo Recommendation
- **Arxiv ID**: http://arxiv.org/abs/1809.00060v1
- **DOI**: None
- **Categories**: **cs.IR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1809.00060v1)
- **Published**: 2018-08-31 20:57:26+00:00
- **Updated**: 2018-08-31 20:57:26+00:00
- **Authors**: Yu Qing Zhou, Ga Wu, Scott Sanner, Putra Manggala
- **Comment**: In Proceedings of the Late-Breaking Results track part of the Twelfth
  ACM Conference on Recommender Systems, Vancouver, BC, Canada, October 6,
  2018, 2 pages
- **Journal**: None
- **Summary**: Many photography websites such as Flickr, 500px, Unsplash, and Adobe Behance are used by amateur and professional photography enthusiasts. Unlike content-based image search, such users of photography websites are not just looking for photos with certain content, but more generally for photos with a certain photographic "aesthetic". In this context, we explore personalized photo recommendation and propose two aesthetic feature extraction methods based on (i) color space and (ii) deep style transfer embeddings. Using a dataset from 500px, we evaluate how these features can be best leveraged by collaborative filtering methods and show that (ii) provides a significant boost in photo recommendation performance.



### RxNN: A Framework for Evaluating Deep Neural Networks on Resistive Crossbars
- **Arxiv ID**: http://arxiv.org/abs/1809.00072v3
- **DOI**: None
- **Categories**: **cs.ET**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1809.00072v3)
- **Published**: 2018-08-31 22:22:53+00:00
- **Updated**: 2020-06-02 03:33:11+00:00
- **Authors**: Shubham Jain, Abhronil Sengupta, Kaushik Roy, Anand Raghunathan
- **Comment**: 13 pages, 16 figures, Accepted in IEEE Transactions on Computer-Aided
  Design of Integrated Circuits and Systems (TCAD) 2020
- **Journal**: None
- **Summary**: Resistive crossbars designed with non-volatile memory devices have emerged as promising building blocks for Deep Neural Network (DNN) hardware, due to their ability to compactly and efficiently realize vector-matrix multiplication (VMM), the dominant computational kernel in DNNs. However, a key challenge with resistive crossbars is that they suffer from a range of device and circuit level non-idealities such as interconnect parasitics, peripheral circuits, sneak paths, and process variations. These non-idealities can lead to errors in VMMs, eventually degrading the DNN's accuracy. It is therefore critical to study the impact of crossbar non-idealities on the accuracy of large-scale DNNs. However, this is challenging because existing device and circuit models are too slow to use in application-level evaluations.   We present RxNN, a fast and accurate simulation framework to evaluate large-scale DNNs on resistive crossbar systems. RxNN splits and maps the computations involved in each DNN layer into crossbar operations, and evaluates them using a Fast Crossbar Model (FCM) that accurately captures the errors arising due to crossbar non-idealities while being four-to-five orders of magnitude faster than circuit simulation. FCM models a crossbar-based VMM operation using three stages - non-linear models for the input and output peripheral circuits (DACs and ADCs), and an equivalent non-ideal conductance matrix for the core crossbar array. We implement RxNN by extending the Caffe machine learning framework and use it to evaluate a suite of six large-scale DNNs developed for the ImageNet Challenge. Our experiments reveal that resistive crossbar non-idealities can lead to significant accuracy degradations (9.6%-32%) for these large-scale DNNs. To the best of our knowledge, this work is the first quantitative evaluation of the accuracy of large-scale DNNs on resistive crossbar based hardware.



### 3D Segmentation with Exponential Logarithmic Loss for Highly Unbalanced Object Sizes
- **Arxiv ID**: http://arxiv.org/abs/1809.00076v2
- **DOI**: 10.1007/978-3-030-00931-1_70
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.00076v2)
- **Published**: 2018-08-31 22:42:12+00:00
- **Updated**: 2018-09-24 20:25:03+00:00
- **Authors**: Ken C. L. Wong, Mehdi Moradi, Hui Tang, Tanveer Syeda-Mahmood
- **Comment**: Accepted by the International Conference on Medical Image Computing
  and Computer-Assisted Intervention - MICCAI 2018 (oral presentation,
  acceptance rate 4%)
- **Journal**: None
- **Summary**: With the introduction of fully convolutional neural networks, deep learning has raised the benchmark for medical image segmentation on both speed and accuracy, and different networks have been proposed for 2D and 3D segmentation with promising results. Nevertheless, most networks only handle relatively small numbers of labels (<10), and there are very limited works on handling highly unbalanced object sizes especially in 3D segmentation. In this paper, we propose a network architecture and the corresponding loss function which improve segmentation of very small structures. By combining skip connections and deep supervision with respect to the computational feasibility of 3D segmentation, we propose a fast converging and computationally efficient network architecture for accurate segmentation. Furthermore, inspired by the concept of focal loss, we propose an exponential logarithmic loss which balances the labels not only by their relative sizes but also by their segmentation difficulties. We achieve an average Dice coefficient of 82% on brain segmentation with 20 labels, with the ratio of the smallest to largest object sizes as 0.14%. Less than 100 epochs are required to reach such accuracy, and segmenting a 128x128x128 volume only takes around 0.4 s.



### Understanding Neural Pathways in Zebrafish through Deep Learning and High Resolution Electron Microscope Data
- **Arxiv ID**: http://arxiv.org/abs/1809.00084v1
- **DOI**: 10.1145/3219104.3229285
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.00084v1)
- **Published**: 2018-08-31 23:42:11+00:00
- **Updated**: 2018-08-31 23:42:11+00:00
- **Authors**: Ishtar Nyawira, Kristi Bushman, Iris Qian, Annie Zhang
- **Comment**: 8 pages, 5 figures (1a to 5c), PEARC '18: Practice and Experience in
  Advanced Research Computing, July 22--26, 2018, Pittsburgh, PA, USA
- **Journal**: PEARC '18 Proceedings of the Practice and Experience on Advanced
  Research Computing, Article No. 65, 2018
- **Summary**: The tracing of neural pathways through large volumes of image data is an incredibly tedious and time-consuming process that significantly encumbers progress in neuroscience. We are exploring deep learning's potential to automate segmentation of high-resolution scanning electron microscope (SEM) image data to remove that barrier. We have started with neural pathway tracing through 5.1GB of whole-brain serial-section slices from larval zebrafish collected by the Center for Brain Science at Harvard University. This kind of manual image segmentation requires years of careful work to properly trace the neural pathways in an organism as small as a zebrafish larva (approximately 5mm in total body length). In automating this process, we would vastly improve productivity, leading to faster data analysis and breakthroughs in understanding the complexity of the brain. We will build upon prior attempts to employ deep learning for automatic image segmentation extending methods for unconventional deep learning data.



### A Simplified Approach to Deep Learning for Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1809.00085v1
- **DOI**: 10.1145/3219104.3229286
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.00085v1)
- **Published**: 2018-08-31 23:46:38+00:00
- **Updated**: 2018-08-31 23:46:38+00:00
- **Authors**: Ishtar Nyawira, Kristi Bushman
- **Comment**: 8 pages, 6 figures (1a to 6c, plus 5 in appendix), PEARC '18:
  Practice and Experience in Advanced Research Computing, July 22--26, 2018,
  Pittsburgh, PA, USA
- **Journal**: PEARC '18 Proceedings of the Practice and Experience on Advanced
  Research Computing, Article No. 56, 2018
- **Summary**: Leaping into the rapidly developing world of deep learning is an exciting and sometimes confusing adventure. All of the advice and tutorials available can be hard to organize and work through, especially when training specific models on specific datasets, different from those originally used to train the network. In this short guide, we aim to walk the reader through the techniques that we have used to successfully train two deep neural networks for pixel-wise classification, including some data management and augmentation approaches for working with image data that may be insufficiently annotated or relatively homogenous.



