# Arxiv Papers in cs.CV on 2018-08-20
### Person Re-Identification by Semantic Region Representation and Topology Constraint
- **Arxiv ID**: http://arxiv.org/abs/1808.06280v1
- **DOI**: 10.1109/TCSVT.2018.2866260
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.06280v1)
- **Published**: 2018-08-20 01:25:09+00:00
- **Updated**: 2018-08-20 01:25:09+00:00
- **Authors**: Jianjun Lei, Lijie Niu, Huazhu Fu, Bo Peng, Qingming Huang, Chunping Hou
- **Comment**: 13 pages, 10 figures, Accepted by IEEE Transactions on Circuits and
  Systems for Video Technology 2018
- **Journal**: None
- **Summary**: Person re-identification is a popular research topic which aims at matching the specific person in a multi-camera network automatically. Feature representation and metric learning are two important issues for person re-identification. In this paper, we propose a novel person re-identification method, which consists of a reliable representation called Semantic Region Representation (SRR), and an effective metric learning with Mapping Space Topology Constraint (MSTC). The SRR integrates semantic representations to achieve effective similarity comparison between the corresponding regions via parsing the body into multiple parts, which focuses on the foreground context against the background interference. To learn a discriminant metric, the MSTC is proposed to take into account the topological relationship among all samples in the feature space. It considers two-fold constraints: the distribution of positive pairs should be more compact than the average distribution of negative pairs with regard to the same probe, while the average distance between different classes should be larger than that between same classes. These two aspects cooperate to maintain the compactness of the intra-class as well as the sparsity of the inter-class. Extensive experiments conducted on five challenging person re-identification datasets, VIPeR, SYSU-sReID, QUML GRID, CUHK03, and Market-1501, show that the proposed method achieves competitive performance with the state-of-the-art approaches.



### Incremental Learning in Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1808.06281v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.06281v5)
- **Published**: 2018-08-20 01:37:16+00:00
- **Updated**: 2019-08-08 12:08:30+00:00
- **Authors**: Prajjwal Bhargava
- **Comment**: The code can be found at
  https://github.com/prajjwal1/person-reid-incremental
- **Journal**: None
- **Summary**: Person Re-Identification is still a challenging task in Computer Vision due to a variety of reasons. On the other side, Incremental Learning is still an issue since deep learning models tend to face the problem of over catastrophic forgetting when trained on subsequent tasks. In this paper, we propose a model that can be used for multiple tasks in Person Re-Identification, provide state-of-the-art results on a variety of tasks and still achieve considerable accuracy subsequently. We evaluated our model on two datasets Market 1501 and Duke MTMC. Extensive experiments show that this method can achieve Incremental Learning in Person ReID efficiently as well as for other tasks in computer vision as well.



### Universal Image Manipulation Detection using Deep Siamese Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1808.06323v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.06323v2)
- **Published**: 2018-08-20 06:40:05+00:00
- **Updated**: 2018-08-23 11:27:35+00:00
- **Authors**: Aniruddha Mazumdar, Jaya Singh, Yosha Singh Tomar, Prabin Kumar Bora
- **Comment**: None
- **Journal**: None
- **Summary**: Detection of different types of image editing operations carried out on an image is an important problem in image forensics. It gives the information about the processing history of an image, and also can expose forgeries present in an image. There have been few methods proposed to detect different types of image editing operations in a single framework. However, all the operations have to be known a priori in the training phase. But, in real-forensics scenarios it may not be possible to know about the editing operations carried out on an image. To solve this problem, we propose a novel deep learning-based method which can differentiate between different types of image editing operations. The proposed method classifies image patches in a pair-wise fashion as either similarly or differently processed using a deep siamese neural network. Once the network learns feature that can discriminate between different image editing operations, it can differentiate between different image editing operations not present in the training stage. The experimental results show the efficacy of the proposed method in detecting/discriminating different image editing operations.



### Navigating the Landscape for Real-time Localisation and Mapping for Robotics and Virtual and Augmented Reality
- **Arxiv ID**: http://arxiv.org/abs/1808.06352v1
- **DOI**: 10.1109/JPROC.2018.2856739
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1808.06352v1)
- **Published**: 2018-08-20 09:06:21+00:00
- **Updated**: 2018-08-20 09:06:21+00:00
- **Authors**: Sajad Saeedi, Bruno Bodin, Harry Wagstaff, Andy Nisbet, Luigi Nardi, John Mawer, Nicolas Melot, Oscar Palomar, Emanuele Vespa, Tom Spink, Cosmin Gorgovan, Andrew Webb, James Clarkson, Erik Tomusk, Thomas Debrunner, Kuba Kaszyk, Pablo Gonzalez-de-Aledo, Andrey Rodchenko, Graham Riley, Christos Kotselidis, Björn Franke, Michael F. P. O'Boyle, Andrew J. Davison, Paul H. J. Kelly, Mikel Luján, Steve Furber
- **Comment**: Proceedings of the IEEE 2018
- **Journal**: None
- **Summary**: Visual understanding of 3D environments in real-time, at low power, is a huge computational challenge. Often referred to as SLAM (Simultaneous Localisation and Mapping), it is central to applications spanning domestic and industrial robotics, autonomous vehicles, virtual and augmented reality. This paper describes the results of a major research effort to assemble the algorithms, architectures, tools, and systems software needed to enable delivery of SLAM, by supporting applications specialists in selecting and configuring the appropriate algorithm and the appropriate hardware, and compilation pathway, to meet their performance, accuracy, and energy consumption goals. The major contributions we present are (1) tools and methodology for systematic quantitative evaluation of SLAM algorithms, (2) automated, machine-learning-guided exploration of the algorithmic and implementation design space with respect to multiple objectives, (3) end-to-end simulation tools to enable optimisation of heterogeneous, accelerated architectures for the specific algorithmic requirements of the various SLAM algorithmic approaches, and (4) tools for delivering, where appropriate, accelerated, adaptive SLAM solutions in a managed, JIT-compiled, adaptive runtime context.



### Learning to Learn from Web Data through Deep Semantic Embeddings
- **Arxiv ID**: http://arxiv.org/abs/1808.06368v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.06368v1)
- **Published**: 2018-08-20 09:58:23+00:00
- **Updated**: 2018-08-20 09:58:23+00:00
- **Authors**: Raul Gomez, Lluis Gomez, Jaume Gibert, Dimosthenis Karatzas
- **Comment**: ECCV MULA Workshop 2018
- **Journal**: None
- **Summary**: In this paper we propose to learn a multimodal image and text embedding from Web and Social Media data, aiming to leverage the semantic knowledge learnt in the text domain and transfer it to a visual model for semantic image retrieval. We demonstrate that the pipeline can learn from images with associated text without supervision and perform a thourough analysis of five different text embeddings in three different benchmarks. We show that the embeddings learnt with Web and Social Media data have competitive performances over supervised methods in the text based image retrieval task, and we clearly outperform state of the art in the MIRFlickr dataset when training in the target data. Further we demonstrate how semantic multimodal image retrieval can be performed using the learnt embeddings, going beyond classical instance-level retrieval problems. Finally, we present a new dataset, InstaCities1M, composed by Instagram images and their associated texts that can be used for fair comparison of image-text embeddings.



### Learning from #Barcelona Instagram data what Locals and Tourists post about its Neighbourhoods
- **Arxiv ID**: http://arxiv.org/abs/1808.06369v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.06369v1)
- **Published**: 2018-08-20 10:04:55+00:00
- **Updated**: 2018-08-20 10:04:55+00:00
- **Authors**: Raul Gomez, Lluis Gomez, Jaume Gibert, Dimosthenis Karatzas
- **Comment**: ECCV MULA Workshop 2018
- **Journal**: None
- **Summary**: Massive tourism is becoming a big problem for some cities, such as Barcelona, due to its concentration in some neighborhoods. In this work we gather Instagram data related to Barcelona consisting on images-captions pairs and, using the text as a supervisory signal, we learn relations between images, words and neighborhoods. Our goal is to learn which visual elements appear in photos when people is posting about each neighborhood. We perform a language separate treatment of the data and show that it can be extrapolated to a tourists and locals separate analysis, and that tourism is reflected in Social Media at a neighborhood level. The presented pipeline allows analyzing the differences between the images that tourists and locals associate to the different neighborhoods. The proposed method, which can be extended to other cities or subjects, proves that Instagram data can be used to train multi-modal (image and text) machine learning models that are useful to analyze publications about a city at a neighborhood level. We publish the collected dataset, InstaBarcelona and the code used in the analysis.



### FusionNet and AugmentedFlowNet: Selective Proxy Ground Truth for Training on Unlabeled Images
- **Arxiv ID**: http://arxiv.org/abs/1808.06389v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.06389v1)
- **Published**: 2018-08-20 11:18:06+00:00
- **Updated**: 2018-08-20 11:18:06+00:00
- **Authors**: Osama Makansi, Eddy Ilg, Thomas Brox
- **Comment**: See video at: https://www.youtube.com/watch?v=HdMeb20Rybs
- **Journal**: None
- **Summary**: Recent work has shown that convolutional neural networks (CNNs) can be used to estimate optical flow with high quality and fast runtime. This makes them preferable for real-world applications. However, such networks require very large training datasets. Engineering the training data is difficult and/or laborious. This paper shows how to augment a network trained on an existing synthetic dataset with large amounts of additional unlabelled data. In particular, we introduce a selection mechanism to assemble from multiple estimates a joint optical flow field, which outperforms that of all input methods. The latter can be used as proxy-ground-truth to train a network on real-world data and to adapt it to specific domains of interest. Our experimental results show that the performance of networks improves considerably, both, in cross-domain and in domain-specific scenarios. As a consequence, we obtain state-of-the-art results on the KITTI benchmarks.



### DeeSIL: Deep-Shallow Incremental Learning
- **Arxiv ID**: http://arxiv.org/abs/1808.06396v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1808.06396v1)
- **Published**: 2018-08-20 11:39:09+00:00
- **Updated**: 2018-08-20 11:39:09+00:00
- **Authors**: Eden Belouadah, Adrian Popescu
- **Comment**: None
- **Journal**: None
- **Summary**: Incremental Learning (IL) is an interesting AI problem when the algorithm is assumed to work on a budget. This is especially true when IL is modeled using a deep learning approach, where two com- plex challenges arise due to limited memory, which induces catastrophic forgetting and delays related to the retraining needed in order to incorpo- rate new classes. Here we introduce DeeSIL, an adaptation of a known transfer learning scheme that combines a fixed deep representation used as feature extractor and learning independent shallow classifiers to in- crease recognition capacity. This scheme tackles the two aforementioned challenges since it works well with a limited memory budget and each new concept can be added within a minute. Moreover, since no deep re- training is needed when the model is incremented, DeeSIL can integrate larger amounts of initial data that provide more transferable features. Performance is evaluated on ImageNet LSVRC 2012 against three state of the art algorithms. Results show that, at scale, DeeSIL performance is 23 and 33 points higher than the best baseline when using the same and more initial data respectively.



### CapsDeMM: Capsule network for Detection of Munro's Microabscess in skin biopsy images
- **Arxiv ID**: http://arxiv.org/abs/1808.06428v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.06428v2)
- **Published**: 2018-08-20 12:50:41+00:00
- **Updated**: 2018-08-21 13:31:15+00:00
- **Authors**: Anabik Pal, Akshay Chaturvedi, Utpal Garain, Aditi Chandra, Raghunath Chatterjee, Swapan Senapati
- **Comment**: Accepted at MICCAI 2018
- **Journal**: None
- **Summary**: This paper presents an approach for automatic detection of Munro's Microabscess in stratum corneum (SC) of human skin biopsy in order to realize a machine assisted diagnosis of Psoriasis. The challenge of detecting neutrophils in presence of nucleated cells is solved using the recent advances of deep learning algorithms. Separation of SC layer, extraction of patches from the layer followed by classification of patches with respect to presence or absence of neutrophils form the basis of the overall approach which is effected through an integration of a U-Net based segmentation network and a capsule network for classification. The novel design of the present capsule net leads to a drastic reduction in the number of parameters without any noticeable compromise in the overall performance. The research further addresses the challenge of dealing with Mega-pixel images (in 10X) vis-a-vis Giga-pixel ones (in 40X). The promising result coming out of an experiment on a dataset consisting of 273 real-life images shows that a practical system is possible based on the present research. The implementation of our system is available at https://github.com/Anabik/CapsDeMM.



### Translational Motion Compensation for Soft Tissue Velocity Images
- **Arxiv ID**: http://arxiv.org/abs/1808.06469v1
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1808.06469v1)
- **Published**: 2018-08-20 14:22:05+00:00
- **Updated**: 2018-08-20 14:22:05+00:00
- **Authors**: Christina Koutsoumpa, Jennifer Keegan, David Firmin, Guang-Zhong Yang, Duncan Gillies
- **Comment**: None
- **Journal**: None
- **Summary**: Purpose: Advancements in MRI Tissue Phase Velocity Mapping (TPM) allow for the acquisition of higher quality velocity cardiac images providing better assessment of regional myocardial deformation for accurate disease diagnosis, pre-operative planning and post-operative patient surveillance. Translation of TPM velocities from the scanner's reference coordinate system to the regional cardiac coordinate system requires decoupling of translational motion and motion due to myocardial deformation. Despite existing techniques for respiratory motion compensation in TPM, there is still a remaining translational velocity component due to the global motion of the beating heart. To compensate for translational motion in cardiac TPM, we propose an image-processing method, which we have evaluated on synthetic data and applied on in vivo TPM data. Methods: Translational motion is estimated from a suitable region of velocities automatically defined in the left-ventricular volume. The region is generated by dilating the medial axis of myocardial masks in each slice and the translational velocity is estimated by integration in this region. The method was evaluated on synthetic data and in vivo data corrupted with a translational velocity component (200% of the maximum measured velocity). Accuracy and robustness were examined and the method was applied on 10 in vivo datasets. Results: The results from synthetic and in vivo corrupted data show excellent performance with an estimation error less than 0.3% and high robustness in both cases. The effectiveness of the method is confirmed with visual observation of results from the 10 datasets. Conclusion: The proposed method is accurate and suitable for translational motion correction of the left ventricular velocity fields. The current method for translational motion compensation could be applied to any annular contracting (tissue) structure.



### Single-View Place Recognition under Seasonal Changes
- **Arxiv ID**: http://arxiv.org/abs/1808.06516v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.06516v1)
- **Published**: 2018-08-20 15:35:29+00:00
- **Updated**: 2018-08-20 15:35:29+00:00
- **Authors**: Daniel Olid, José M. Fácil, Javier Civera
- **Comment**: Accepted at 10th Planning, Perception and Navigation for Intelligent
  Vehicles (PPNIV'18), Workshop at IROS 2018
- **Journal**: None
- **Summary**: Single-view place recognition, that we can define as finding an image that corresponds to the same place as a given query image, is a key capability for autonomous navigation and mapping. Although there has been a considerable amount of research in the topic, the high degree of image variability (with viewpoint, illumination or occlusions for example) makes it a research challenge.   One of the particular challenges, that we address in this work, is weather variation. Seasonal changes can produce drastic appearance changes, that classic low-level features do not model properly. Our contributions in this paper are twofold. First we pre-process and propose a partition for the Nordland dataset, frequently used for place recognition research without consensus on the partitions. And second, we evaluate several neural network architectures such as pre-trained, siamese and triplet for this problem. Our best results outperform the state of the art of the field. A video showing our results can be found in https://youtu.be/VrlxsYZoHDM. The partitioned version of the Nordland dataset at http://webdiis.unizar.es/~jmfacil/pr-nordland/.



### Simultaneous synthesis of FLAIR and segmentation of white matter hypointensities from T1 MRIs
- **Arxiv ID**: http://arxiv.org/abs/1808.06519v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.06519v1)
- **Published**: 2018-08-20 15:43:06+00:00
- **Updated**: 2018-08-20 15:43:06+00:00
- **Authors**: Mauricio Orbes-Arteaga, M. Jorge Cardoso, Lauge Sørensen, Marc Modat, Sébastien Ourselin, Mads Nielsen, Akshay Pai
- **Comment**: Conference on Medical Imaging with Deep Learning MIDL 2018
- **Journal**: None
- **Summary**: Segmenting vascular pathologies such as white matter lesions in Brain magnetic resonance images (MRIs) require acquisition of multiple sequences such as T1-weighted (T1-w) --on which lesions appear hypointense-- and fluid attenuated inversion recovery (FLAIR) sequence --where lesions appear hyperintense--. However, most of the existing retrospective datasets do not consist of FLAIR sequences. Existing missing modality imputation methods separate the process of imputation, and the process of segmentation. In this paper, we propose a method to link both modality imputation and segmentation using convolutional neural networks. We show that by jointly optimizing the imputation network and the segmentation network, the method not only produces more realistic synthetic FLAIR images from T1-w images, but also improves the segmentation of WMH from T1-w images only.



### CU-Net: Coupled U-Nets
- **Arxiv ID**: http://arxiv.org/abs/1808.06521v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.06521v1)
- **Published**: 2018-08-20 15:45:26+00:00
- **Updated**: 2018-08-20 15:45:26+00:00
- **Authors**: Zhiqiang Tang, Xi Peng, Shijie Geng, Yizhe Zhu, Dimitris N. Metaxas
- **Comment**: BMVC 2018 (Oral)
- **Journal**: None
- **Summary**: We design a new connectivity pattern for the U-Net architecture. Given several stacked U-Nets, we couple each U-Net pair through the connections of their semantic blocks, resulting in the coupled U-Nets (CU-Net). The coupling connections could make the information flow more efficiently across U-Nets. The feature reuse across U-Nets makes each U-Net very parameter efficient. We evaluate the coupled U-Nets on two benchmark datasets of human pose estimation. Both the accuracy and model parameter number are compared. The CU-Net obtains comparable accuracy as state-of-the-art methods. However, it only has at least 60% fewer parameters than other approaches.



### Triangle Lasso for Simultaneous Clustering and Optimization in Graph Datasets
- **Arxiv ID**: http://arxiv.org/abs/1808.06556v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1808.06556v1)
- **Published**: 2018-08-20 16:31:30+00:00
- **Updated**: 2018-08-20 16:31:30+00:00
- **Authors**: Yawei Zhao, Kai Xu, Xinwang Liu, En Zhu, Xinzhong Zhu, Jianping Yin
- **Comment**: Accepted by IEEE Transactions on Knowledge and Data Engineering, 2018
- **Journal**: None
- **Summary**: Recently, network lasso has drawn many attentions due to its remarkable performance on simultaneous clustering and optimization. However, it usually suffers from the imperfect data (noise, missing values etc), and yields sub-optimal solutions. The reason is that it finds the similar instances according to their features directly, which is usually impacted by the imperfect data, and thus returns sub-optimal results. In this paper, we propose triangle lasso to avoid its disadvantage. Triangle lasso finds the similar instances according to their neighbours. If two instances have many common neighbours, they tend to become similar. Although some instances are profiled by the imperfect data, it is still able to find the similar counterparts. Furthermore, we develop an efficient algorithm based on Alternating Direction Method of Multipliers (ADMM) to obtain a moderately accurate solution. In addition, we present a dual method to obtain the accurate solution with the low additional time consumption. We demonstrate through extensive numerical experiments that triangle lasso is robust to the imperfect data. It usually yields a better performance than the state-of-the-art method when performing data analysis tasks in practical scenarios.



### Multi-View Graph Embedding Using Randomized Shortest Paths
- **Arxiv ID**: http://arxiv.org/abs/1808.06560v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1808.06560v1)
- **Published**: 2018-08-20 16:40:35+00:00
- **Updated**: 2018-08-20 16:40:35+00:00
- **Authors**: Anuththari Gamage, Brian Rappaport, Shuchin Aeron, Xiaozhe Hu
- **Comment**: None
- **Journal**: None
- **Summary**: Real-world data sets often provide multiple types of information about the same set of entities. This data is well represented by multi-view graphs, which consist of several distinct sets of edges over the same nodes. These can be used to analyze how entities interact from different viewpoints. Combining multiple views improves the quality of inferences drawn from the underlying data, which has increased interest in developing efficient multi-view graph embedding methods. We propose an algorithm, C-RSP, that generates a common (C) embedding of a multi-view graph using Randomized Shortest Paths (RSP). This algorithm generates a dissimilarity measure between nodes by minimizing the expected cost of a random walk between any two nodes across all views of a multi-view graph, in doing so encoding both the local and global structure of the graph. We test C-RSP on both real and synthetic data and show that it outperforms benchmark algorithms at embedding and clustering tasks while remaining computationally efficient.



### Class-Aware Fully-Convolutional Gaussian and Poisson Denoising
- **Arxiv ID**: http://arxiv.org/abs/1808.06562v1
- **DOI**: 10.1109/TIP.2018.2859044
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.06562v1)
- **Published**: 2018-08-20 16:46:09+00:00
- **Updated**: 2018-08-20 16:46:09+00:00
- **Authors**: Tal Remez, Or Litany, Raja Giryes, Alex M. Bronstein
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a fully-convolutional neural-network architecture for image denoising which is simple yet powerful. Its structure allows to exploit the gradual nature of the denoising process, in which shallow layers handle local noise statistics, while deeper layers recover edges and enhance textures. Our method advances the state-of-the-art when trained for different noise levels and distributions (both Gaussian and Poisson). In addition, we show that making the denoiser class-aware by exploiting semantic class information boosts performance, enhances textures and reduces artifacts.



### Learning Monocular Depth by Distilling Cross-domain Stereo Networks
- **Arxiv ID**: http://arxiv.org/abs/1808.06586v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.06586v1)
- **Published**: 2018-08-20 17:47:58+00:00
- **Updated**: 2018-08-20 17:47:58+00:00
- **Authors**: Xiaoyang Guo, Hongsheng Li, Shuai Yi, Jimmy Ren, Xiaogang Wang
- **Comment**: ECCV 2018
- **Journal**: None
- **Summary**: Monocular depth estimation aims at estimating a pixelwise depth map for a single image, which has wide applications in scene understanding and autonomous driving. Existing supervised and unsupervised methods face great challenges. Supervised methods require large amounts of depth measurement data, which are generally difficult to obtain, while unsupervised methods are usually limited in estimation accuracy. Synthetic data generated by graphics engines provide a possible solution for collecting large amounts of depth data. However, the large domain gaps between synthetic and realistic data make directly training with them challenging. In this paper, we propose to use the stereo matching network as a proxy to learn depth from synthetic data and use predicted stereo disparity maps for supervising the monocular depth estimation network. Cross-domain synthetic data could be fully utilized in this novel framework. Different strategies are proposed to ensure learned depth perception capability well transferred across different domains. Our extensive experiments show state-of-the-art results of monocular depth estimation on KITTI dataset.



### Video-to-Video Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1808.06601v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1808.06601v2)
- **Published**: 2018-08-20 17:58:42+00:00
- **Updated**: 2018-12-03 15:12:44+00:00
- **Authors**: Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin Liu, Andrew Tao, Jan Kautz, Bryan Catanzaro
- **Comment**: In NeurIPS, 2018. Code, models, and more results are available at
  https://github.com/NVIDIA/vid2vid
- **Journal**: None
- **Summary**: We study the problem of video-to-video synthesis, whose goal is to learn a mapping function from an input source video (e.g., a sequence of semantic segmentation masks) to an output photorealistic video that precisely depicts the content of the source video. While its image counterpart, the image-to-image synthesis problem, is a popular topic, the video-to-video synthesis problem is less explored in the literature. Without understanding temporal dynamics, directly applying existing image synthesis approaches to an input video often results in temporally incoherent videos of low visual quality. In this paper, we propose a novel video-to-video synthesis approach under the generative adversarial learning framework. Through carefully-designed generator and discriminator architectures, coupled with a spatio-temporal adversarial objective, we achieve high-resolution, photorealistic, temporally coherent video results on a diverse set of input formats including segmentation masks, sketches, and poses. Experiments on multiple benchmarks show the advantage of our method compared to strong baselines. In particular, our model is capable of synthesizing 2K resolution videos of street scenes up to 30 seconds long, which significantly advances the state-of-the-art of video synthesis. Finally, we apply our approach to future video prediction, outperforming several state-of-the-art competing systems.



### A Hybrid Differential Evolution Approach to Designing Deep Convolutional Neural Networks for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1808.06661v2
- **DOI**: None
- **Categories**: **cs.NE**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1808.06661v2)
- **Published**: 2018-08-20 19:24:45+00:00
- **Updated**: 2018-08-22 01:32:59+00:00
- **Authors**: Bin Wang, Yanan Sun, Bing Xue, Mengjie Zhang
- **Comment**: Accepted by The Australasian Joint Conference on Artificial
  Intelligence 2018
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) have demonstrated their superiority in image classification, and evolutionary computation (EC) methods have recently been surging to automatically design the architectures of CNNs to save the tedious work of manually designing CNNs. In this paper, a new hybrid differential evolution (DE) algorithm with a newly added crossover operator is proposed to evolve the architectures of CNNs of any lengths, which is named DECNN. There are three new ideas in the proposed DECNN method. Firstly, an existing effective encoding scheme is refined to cater for variable-length CNN architectures; Secondly, the new mutation and crossover operators are developed for variable-length DE to optimise the hyperparameters of CNNs; Finally, the new second crossover is introduced to evolve the depth of the CNN architectures. The proposed algorithm is tested on six widely-used benchmark datasets and the results are compared to 12 state-of-the-art methods, which shows the proposed method is vigorously competitive to the state-of-the-art algorithms. Furthermore, the proposed method is also compared with a method using particle swarm optimisation with a similar encoding strategy named IPPSO, and the proposed DECNN outperforms IPPSO in terms of the accuracy.



### Adversarial Sampling for Active Learning
- **Arxiv ID**: http://arxiv.org/abs/1808.06671v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1808.06671v2)
- **Published**: 2018-08-20 19:53:19+00:00
- **Updated**: 2019-12-21 13:47:45+00:00
- **Authors**: Christoph Mayer, Radu Timofte
- **Comment**: Accepted at WACV2020
- **Journal**: None
- **Summary**: This paper proposes asal, a new GAN based active learning method that generates high entropy samples. Instead of directly annotating the synthetic samples, ASAL searches similar samples from the pool and includes them for training. Hence, the quality of new samples is high and annotations are reliable. To the best of our knowledge, ASAL is the first GAN based AL method applicable to multi-class problems that outperforms random sample selection. Another benefit of ASAL is its small run-time complexity (sub-linear) compared to traditional uncertainty sampling (linear). We present a comprehensive set of experiments on multiple traditional data sets and show that ASAL outperforms similar methods and clearly exceeds the established baseline (random sampling). In the discussion section we analyze in which situations ASAL performs best and why it is sometimes hard to outperform random sample selection.



### Class2Str: End to End Latent Hierarchy Learning
- **Arxiv ID**: http://arxiv.org/abs/1808.06675v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1808.06675v1)
- **Published**: 2018-08-20 19:58:35+00:00
- **Updated**: 2018-08-20 19:58:35+00:00
- **Authors**: Soham Saha, Girish Varma, C. V. Jawahar
- **Comment**: 6 pages, ICPR 2018, Beijing
- **Journal**: None
- **Summary**: Deep neural networks for image classification typically consists of a convolutional feature extractor followed by a fully connected classifier network. The predicted and the ground truth labels are represented as one hot vectors. Such a representation assumes that all classes are equally dissimilar. However, classes have visual similarities and often form a hierarchy. Learning this latent hierarchy explicitly in the architecture could provide invaluable insights. We propose an alternate architecture to the classifier network called the Latent Hierarchy (LH) Classifier and an end to end learned Class2Str mapping which discovers a latent hierarchy of the classes. We show that for some of the best performing architectures on CIFAR and Imagenet datasets, the proposed replacement and training by LH classifier recovers the accuracy, with a fraction of the number of parameters in the classifier part. Compared to the previous work of HDCNN, which also learns a 2 level hierarchy, we are able to learn a hierarchy at an arbitrary number of levels as well as obtain an accuracy improvement on the Imagenet classification task over them. We also verify that many visually similar classes are grouped together, under the learnt hierarchy.



### Deep Multimodal Image-Repurposing Detection
- **Arxiv ID**: http://arxiv.org/abs/1808.06686v1
- **DOI**: 10.1145/3240508.3240707
- **Categories**: **cs.MM**, cs.AI, cs.CV, cs.LG, cs.SI
- **Links**: [PDF](http://arxiv.org/pdf/1808.06686v1)
- **Published**: 2018-08-20 20:47:56+00:00
- **Updated**: 2018-08-20 20:47:56+00:00
- **Authors**: Ekraam Sabir, Wael AbdAlmageed, Yue Wu, Prem Natarajan
- **Comment**: To be published at ACM Multimeda 2018 (orals)
- **Journal**: None
- **Summary**: Nefarious actors on social media and other platforms often spread rumors and falsehoods through images whose metadata (e.g., captions) have been modified to provide visual substantiation of the rumor/falsehood. This type of modification is referred to as image repurposing, in which often an unmanipulated image is published along with incorrect or manipulated metadata to serve the actor's ulterior motives. We present the Multimodal Entity Image Repurposing (MEIR) dataset, a substantially challenging dataset over that which has been previously available to support research into image repurposing detection. The new dataset includes location, person, and organization manipulations on real-world data sourced from Flickr. We also present a novel, end-to-end, deep multimodal learning model for assessing the integrity of an image by combining information extracted from the image with related information from a knowledge base. The proposed method is compared against state-of-the-art techniques on existing datasets as well as MEIR, where it outperforms existing methods across the board, with AUC improvement up to 0.23.



### VERAM: View-Enhanced Recurrent Attention Model for 3D Shape Classification
- **Arxiv ID**: http://arxiv.org/abs/1808.06698v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1808.06698v1)
- **Published**: 2018-08-20 21:08:02+00:00
- **Updated**: 2018-08-20 21:08:02+00:00
- **Authors**: Songle Chen, Lintao Zheng, Yan Zhang, Zhixin Sun, Kai Xu
- **Comment**: Accepted by IEEE Transactions on Visualization and Computer Graphics.
  Corresponding Author: Kai Xu (kevin.kai.xu@gmail.com)
- **Journal**: IEEE Transactions on Visualization and Computer Graphics, 2018
- **Summary**: Multi-view deep neural network is perhaps the most successful approach in 3D shape classification. However, the fusion of multi-view features based on max or average pooling lacks a view selection mechanism, limiting its application in, e.g., multi-view active object recognition by a robot. This paper presents VERAM, a recurrent attention model capable of actively selecting a sequence of views for highly accurate 3D shape classification. VERAM addresses an important issue commonly found in existing attention-based models, i.e., the unbalanced training of the subnetworks corresponding to next view estimation and shape classification. The classification subnetwork is easily overfitted while the view estimation one is usually poorly trained, leading to a suboptimal classification performance. This is surmounted by three essential view-enhancement strategies: 1) enhancing the information flow of gradient backpropagation for the view estimation subnetwork, 2) devising a highly informative reward function for the reinforcement training of view estimation and 3) formulating a novel loss function that explicitly circumvents view duplication. Taking grayscale image as input and AlexNet as CNN architecture, VERAM with 9 views achieves instance-level and class-level accuracy of 95:5% and 95:3% on ModelNet10, 93:7% and 92:1% on ModelNet40, both are the state-of-the-art performance under the same number of views.



