# Arxiv Papers in cs.CV on 2018-08-08
### A Semi-Supervised Data Augmentation Approach using 3D Graphical Engines
- **Arxiv ID**: http://arxiv.org/abs/1808.02595v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.02595v2)
- **Published**: 2018-08-08 01:48:38+00:00
- **Updated**: 2018-09-28 19:55:27+00:00
- **Authors**: Shuangjun Liu, Sarah Ostadabbas
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning approaches have been rapidly adopted across a wide range of fields because of their accuracy and flexibility, but require large labeled training datasets. This presents a fundamental problem for applications with limited, expensive, or private data (i.e. small data), such as human pose and behavior estimation/tracking which could be highly personalized. In this paper, we present a semi-supervised data augmentation approach that can synthesize large scale labeled training datasets using 3D graphical engines based on a physically-valid low dimensional pose descriptor. To evaluate the performance of our synthesized datasets in training deep learning-based models, we generated a large synthetic human pose dataset, called ScanAva using 3D scans of only 7 individuals based on our proposed augmentation approach. A state-of-the-art human pose estimation deep learning model then was trained from scratch using our ScanAva dataset and could achieve the pose estimation accuracy of 91.2% at PCK0.5 criteria after applying an efficient domain adaptation on the synthetic images, in which its pose estimation accuracy was comparable to the same model trained on large scale pose data from real humans such as MPII dataset and much higher than the model trained on other synthetic human dataset such as SURREAL.



### Unsupervised/Semi-supervised Deep Learning for Low-dose CT Enhancement
- **Arxiv ID**: http://arxiv.org/abs/1808.02603v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.02603v1)
- **Published**: 2018-08-08 02:23:37+00:00
- **Updated**: 2018-08-08 02:23:37+00:00
- **Authors**: Mingrui Geng, Yun Deng, Qian Zhao, Qi Xie, Dong Zeng, Dong Zeng, Wangmeng Zuo, Deyu Meng
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, deep learning(DL) methods have been proposed for the low-dose computed tomography(LdCT) enhancement, and obtain good trade-off between computational efficiency and image quality. Most of them need large number of pre-collected ground-truth/high-dose sinograms with less noise, and train the network in a supervised end-to-end manner. This may bring major limitations on these methods because the number of such low-dose/high-dose training sinogram pairs would affect the network's capability and sometimes the ground-truth sinograms are hard to be obtained in large scale. Since large number of low-dose ones are relatively easy to obtain, it should be critical to make these sources play roles in network training in an unsupervised learning manner. To address this issue, we propose an unsupervised DL method for LdCT enhancement that incorporates unlabeled LdCT sinograms directly into the network training. The proposed method effectively considers the structure characteristics and noise distribution in the measured LdCT sinogram, and then learns the proper gradient of the LdCT sinogram in a pure unsupervised manner. Similar to the labeled ground-truth, the gradient information in an unlabeled LdCT sinogram can be used for sufficient network training. The experiments on the patient data show effectiveness of the proposed method.



### Training Compact Neural Networks with Binary Weights and Low Precision Activations
- **Arxiv ID**: http://arxiv.org/abs/1808.02631v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.02631v1)
- **Published**: 2018-08-08 05:32:23+00:00
- **Updated**: 2018-08-08 05:32:23+00:00
- **Authors**: Bohan Zhuang, Chunhua Shen, Ian Reid
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose to train a network with binary weights and low-bitwidth activations, designed especially for mobile devices with limited power consumption. Most previous works on quantizing CNNs uncritically assume the same architecture, though with reduced precision. However, we take the view that for best performance it is possible (and even likely) that a different architecture may be better suited to dealing with low precision weights and activations.   Specifically, we propose a "network expansion" strategy in which we aggregate a set of homogeneous low-precision branches to implicitly reconstruct the full-precision intermediate feature maps. Moreover, we also propose a group-wise feature approximation strategy which is very flexible and highly accurate. Experiments on ImageNet classification tasks demonstrate the superior performance of the proposed model, named Group-Net, over various popular architectures. In particular, with binary weights and activations, we outperform the previous best binary neural network in terms of accuracy as well as saving more than 5 times computational complexity on ImageNet with ResNet-18 and ResNet-50.



### Question-Guided Hybrid Convolution for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1808.02632v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1808.02632v1)
- **Published**: 2018-08-08 05:39:00+00:00
- **Updated**: 2018-08-08 05:39:00+00:00
- **Authors**: Peng Gao, Pan Lu, Hongsheng Li, Shuang Li, Yikang Li, Steven Hoi, Xiaogang Wang
- **Comment**: 17 pages, 4 figures, accepted in ECCV 2018
- **Journal**: None
- **Summary**: In this paper, we propose a novel Question-Guided Hybrid Convolution (QGHC) network for Visual Question Answering (VQA). Most state-of-the-art VQA methods fuse the high-level textual and visual features from the neural network and abandon the visual spatial information when learning multi-modal features.To address these problems, question-guided kernels generated from the input question are designed to convolute with visual features for capturing the textual and visual relationship in the early stage. The question-guided convolution can tightly couple the textual and visual information but also introduce more parameters when learning kernels. We apply the group convolution, which consists of question-independent kernels and question-dependent kernels, to reduce the parameter size and alleviate over-fitting. The hybrid convolution can generate discriminative multi-modal features with fewer parameters. The proposed approach is also complementary to existing bilinear pooling fusion and attention based VQA methods. By integrating with them, our method could further boost the performance. Extensive experiments on public VQA datasets validate the effectiveness of QGHC.



### Beyond Pixel Norm-Balls: Parametric Adversaries using an Analytically Differentiable Renderer
- **Arxiv ID**: http://arxiv.org/abs/1808.02651v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.GR, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1808.02651v2)
- **Published**: 2018-08-08 08:01:18+00:00
- **Updated**: 2019-02-17 22:12:01+00:00
- **Authors**: Hsueh-Ti Derek Liu, Michael Tao, Chun-Liang Li, Derek Nowrouzezahrai, Alec Jacobson
- **Comment**: None
- **Journal**: None
- **Summary**: Many machine learning image classifiers are vulnerable to adversarial attacks, inputs with perturbations designed to intentionally trigger misclassification. Current adversarial methods directly alter pixel colors and evaluate against pixel norm-balls: pixel perturbations smaller than a specified magnitude, according to a measurement norm. This evaluation, however, has limited practical utility since perturbations in the pixel space do not correspond to underlying real-world phenomena of image formation that lead to them and has no security motivation attached. Pixels in natural images are measurements of light that has interacted with the geometry of a physical scene. As such, we propose the direct perturbation of physical parameters that underly image formation: lighting and geometry. As such, we propose a novel evaluation measure, parametric norm-balls, by directly perturbing physical parameters that underly image formation. One enabling contribution we present is a physically-based differentiable renderer that allows us to propagate pixel gradients to the parametric space of lighting and geometry. Our approach enables physically-based adversarial attacks, and our differentiable renderer leverages models from the interactive rendering literature to balance the performance and accuracy trade-offs necessary for a memory-efficient and scalable adversarial data augmentation workflow.



### An Occam's Razor View on Learning Audiovisual Emotion Recognition with Small Training Sets
- **Arxiv ID**: http://arxiv.org/abs/1808.02668v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1808.02668v1)
- **Published**: 2018-08-08 08:43:43+00:00
- **Updated**: 2018-08-08 08:43:43+00:00
- **Authors**: Valentin Vielzeuf, Corentin Kervadec, Stéphane Pateux, Alexis Lechervy, Frédéric Jurie
- **Comment**: None
- **Journal**: ICMI (EmotiW) 2018, Oct 2018, Boulder, Colorado, United States
- **Summary**: This paper presents a light-weight and accurate deep neural model for audiovisual emotion recognition. To design this model, the authors followed a philosophy of simplicity, drastically limiting the number of parameters to learn from the target datasets, always choosing the simplest earning methods: i) transfer learning and low-dimensional space embedding allows to reduce the dimensionality of the representations. ii) The isual temporal information is handled by a simple score-per-frame selection process, averaged across time. iii) A simple frame selection echanism is also proposed to weight the images of a sequence. iv) The fusion of the different modalities is performed at prediction level (late usion). We also highlight the inherent challenges of the AFEW dataset and the difficulty of model selection with as few as 383 validation equences. The proposed real-time emotion classifier achieved a state-of-the-art accuracy of 60.64 % on the test set of AFEW, and ranked 4th at he Emotion in the Wild 2018 challenge.



### Starting Movement Detection of Cyclists Using Smart Devices
- **Arxiv ID**: http://arxiv.org/abs/1808.04449v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1808.04449v1)
- **Published**: 2018-08-08 09:20:13+00:00
- **Updated**: 2018-08-08 09:20:13+00:00
- **Authors**: Maarten Bieshaar, Malte Depping, Jan Schneegans, Bernhard Sick
- **Comment**: 10 pages, accepted for publication at DSAA 2018, Turin, Italy
- **Journal**: None
- **Summary**: In near future, vulnerable road users (VRUs) such as cyclists and pedestrians will be equipped with smart devices and wearables which are capable to communicate with intelligent vehicles and other traffic participants. Road users are then able to cooperate on different levels, such as in cooperative intention detection for advanced VRU protection. Smart devices can be used to detect intentions, e.g., an occluded cyclist intending to cross the road, to warn vehicles of VRUs, and prevent potential collisions. This article presents a human activity recognition approach to detect the starting movement of cyclists wearing smart devices. We propose a novel two-stage feature selection procedure using a score specialized for robust starting detection reducing the false positive detections and leading to understandable and interpretable features. The detection is modelled as a classification problem and realized by means of a machine learning classifier. We introduce an auxiliary class, that models starting movements and allows to integrate early movement indicators, i.e., body part movements indicating future behaviour. In this way we improve the robustness and reduce the detection time of the classifier. Our empirical studies with real-world data originating from experiments which involve 49 test subjects and consists of 84 starting motions show that we are able to detect the starting movements early. Our approach reaches an F1-score of 67 % within 0.33 s after the first movement of the bicycle wheel. Investigations concerning the device wearing location show that for devices worn in the trouser pocket the detector has less false detections and detects starting movements faster on average. We found that we can further improve the results when we train distinct classifiers for different wearing locations.



### Smart Device based Initial Movement Detection of Cyclists using Convolutional Neuronal Networks
- **Arxiv ID**: http://arxiv.org/abs/1808.04451v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.04451v1)
- **Published**: 2018-08-08 09:35:33+00:00
- **Updated**: 2018-08-08 09:35:33+00:00
- **Authors**: Jan Schneegans, Maarten Bieshaar
- **Comment**: 12 pages, accepted for publication at OC-DDC 2018, W\"urzburg,
  Germany
- **Journal**: None
- **Summary**: For future traffic scenarios, we envision interconnected traffic participants, who exchange information about their current state, e.g., position, their predicted intentions, allowing to act in a cooperative manner. Vulnerable road users (VRUs), e.g., pedestrians and cyclists, will be equipped with smart device that can be used to detect their intentions and transmit these detected intention to approaching cars such that their drivers can be warned. In this article, we focus on detecting the initial movement of cyclist using smart devices. Smart devices provide the necessary sensors, namely accelerometer and gyroscope, and therefore pose an excellent instrument to detect movement transitions (e.g., waiting to moving) fast. Convolutional Neural Networks prove to be the state-of-the-art solution for many problems with an ever increasing range of applications. Therefore, we model the initial movement detection as a classification problem. In terms of Organic Computing (OC) it be seen as a step towards self-awareness and self-adaptation. We apply residual network architectures to the task of detecting the initial starting movement of cyclists.



### Omnidirectional DSO: Direct Sparse Odometry with Fisheye Cameras
- **Arxiv ID**: http://arxiv.org/abs/1808.02775v1
- **DOI**: 10.1109/LRA.2018.2855443
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.02775v1)
- **Published**: 2018-08-08 13:34:31+00:00
- **Updated**: 2018-08-08 13:34:31+00:00
- **Authors**: Hidenobu Matsuki, Lukas von Stumberg, Vladyslav Usenko, Jörg Stückler, Daniel Cremers
- **Comment**: Accepted by IEEE Robotics and Automation Letters (RA-L), 2018 and
  IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),
  2018
- **Journal**: None
- **Summary**: We propose a novel real-time direct monocular visual odometry for omnidirectional cameras. Our method extends direct sparse odometry (DSO) by using the unified omnidirectional model as a projection function, which can be applied to fisheye cameras with a field-of-view (FoV) well above 180 degrees. This formulation allows for using the full area of the input image even with strong distortion, while most existing visual odometry methods can only use a rectified and cropped part of it. Model parameters within an active keyframe window are jointly optimized, including the intrinsic/extrinsic camera parameters, 3D position of points, and affine brightness parameters. Thanks to the wide FoV, image overlap between frames becomes bigger and points are more spatially distributed. Our results demonstrate that our method provides increased accuracy and robustness over state-of-the-art visual odometry algorithms.



### Multiband SAS Imagery
- **Arxiv ID**: http://arxiv.org/abs/1808.02792v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.02792v1)
- **Published**: 2018-08-08 14:19:38+00:00
- **Updated**: 2018-08-08 14:19:38+00:00
- **Authors**: Isaac D Gerg
- **Comment**: None
- **Journal**: None
- **Summary**: Advances in unmanned synthetic aperture sonar (SAS) imaging platforms allow for the simultaneous collection of multiband SAS imagery. The imagery is collected over several octaves and the phenomenology's interactions with the sea floor vary greatly over this range -- higher frequencies resolve proud and fine structure of the seafloor while lower frequencies resolve subsurface features and often induce internal resonance in man-made objects. Currently, analysts examine multiband imagery by viewing a single band at a time. This method makes it difficult to ascertain correlations between any pair of bands collected over the same location. To mitigate this issue, we propose methods which ingest high frequency (HF) and low frequency (LF) SAS imagery and generates a color composite creating what we call a multiband SAS (MSAS) image. The MSAS image contains the relevant portions of the HF and LF images required by an analyst to interpret the scene and are defined using a spatial saliency metric computed for each image. We then combine the saliency and acoustic backscatter measures to form the final MSAS image.



### A Novel Disparity Transformation Algorithm for Road Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1808.02837v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.02837v1)
- **Published**: 2018-08-08 15:50:22+00:00
- **Updated**: 2018-08-08 15:50:22+00:00
- **Authors**: Rui Fan, Mohammud Junaid Bocus, Naim Dahnoun
- **Comment**: 16 pages, 8 figures
- **Journal**: None
- **Summary**: The disparity information provided by stereo cameras has enabled advanced driver assistance systems to estimate road area more accurately and effectively. In this paper, a novel disparity transformation algorithm is proposed to extract road areas from dense disparity maps by making the disparity value of the road pixels become similar. The transformation is achieved using two parameters: roll angle and fitted disparity value with respect to each row. To achieve a better processing efficiency, golden section search and dynamic programming are utilised to estimate the roll angle and the fitted disparity value, respectively. By performing a rotation around the estimated roll angle, the disparity distribution of each row becomes very compact. This further improves the accuracy of the road model estimation, as demonstrated by the various experimental results in this paper. Finally, the Otsu's thresholding method is applied to the transformed disparity map and the roads can be accurately segmented at pixel level.



### Pattern Recognition Approach to Violin Shapes of MIMO database
- **Arxiv ID**: http://arxiv.org/abs/1808.02848v1
- **DOI**: None
- **Categories**: **stat.AP**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1808.02848v1)
- **Published**: 2018-08-08 16:21:08+00:00
- **Updated**: 2018-08-08 16:21:08+00:00
- **Authors**: Thomas Peron, Francisco A. Rodrigues, Luciano da F. Costa
- **Comment**: None
- **Journal**: None
- **Summary**: Since the landmarks established by the Cremonese school in the 16th century, the history of violin design has been marked by experimentation. While great effort has been invested since the early 19th century by the scientific community on researching violin acoustics, substantially less attention has been given to the statistical characterization of how the violin shape evolved over time. In this paper we study the morphology of violins retrieved from the Musical Instrument Museums Online (MIMO) database -- the largest freely accessible platform providing information about instruments held in public museums. From the violin images, we derive a set of measurements that reflect relevant geometrical features of the instruments. The application of Principal Component Analysis (PCA) uncovered similarities between violin makers and their respective copyists, as well as among luthiers belonging to the same family lineage, in the context of historical narrative. Combined with a time-windowed approach, thin plate splines visualizations revealed that the average violin outline has remained mostly stable over time, not adhering to any particular trends of design across different periods in music history.



### On the Solvability of Viewing Graphs
- **Arxiv ID**: http://arxiv.org/abs/1808.02856v2
- **DOI**: None
- **Categories**: **cs.CV**, math.AG
- **Links**: [PDF](http://arxiv.org/pdf/1808.02856v2)
- **Published**: 2018-08-08 16:46:21+00:00
- **Updated**: 2018-09-18 15:12:46+00:00
- **Authors**: Matthew Trager, Brian Osserman, Jean Ponce
- **Comment**: 22 pages, 8 figures, presented at ECCV 2018
- **Journal**: None
- **Summary**: A set of fundamental matrices relating pairs of cameras in some configuration can be represented as edges of a "viewing graph". Whether or not these fundamental matrices are generically sufficient to recover the global camera configuration depends on the structure of this graph. We study characterizations of "solvable" viewing graphs and present several new results that can be applied to determine which pairs of views may be used to recover all camera parameters. We also discuss strategies for verifying the solvability of a graph computationally.



### Choose Your Neuron: Incorporating Domain Knowledge through Neuron-Importance
- **Arxiv ID**: http://arxiv.org/abs/1808.02861v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.02861v1)
- **Published**: 2018-08-08 17:00:43+00:00
- **Updated**: 2018-08-08 17:00:43+00:00
- **Authors**: Ramprasaath R. Selvaraju, Prithvijit Chattopadhyay, Mohamed Elhoseiny, Tilak Sharma, Dhruv Batra, Devi Parikh, Stefan Lee
- **Comment**: In Proceedings of ECCV 2018
- **Journal**: None
- **Summary**: Individual neurons in convolutional neural networks supervised for image-level classification tasks have been shown to implicitly learn semantically meaningful concepts ranging from simple textures and shapes to whole or partial objects - forming a "dictionary" of concepts acquired through the learning process. In this work we introduce a simple, efficient zero-shot learning approach based on this observation. Our approach, which we call Neuron Importance-AwareWeight Transfer (NIWT), learns to map domain knowledge about novel "unseen" classes onto this dictionary of learned concepts and then optimizes for network parameters that can effectively combine these concepts - essentially learning classifiers by discovering and composing learned semantic concepts in deep networks. Our approach shows improvements over previous approaches on the CUBirds and AWA2 generalized zero-shot learning benchmarks. We demonstrate our approach on a diverse set of semantic inputs as external domain knowledge including attributes and natural language captions. Moreover by learning inverse mappings, NIWT can provide visual and textual explanations for the predictions made by the newly learned classifiers and provide neuron names. Our code is available at https://github.com/ramprs/neuron-importance-zsl.



### Additional Representations for Improving Synthetic Aperture Sonar Classification Using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1808.02868v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.02868v4)
- **Published**: 2018-08-08 17:15:12+00:00
- **Updated**: 2018-10-15 15:29:26+00:00
- **Authors**: Isaac Gerg, David Williams
- **Comment**: Accepted for the Institute of Acoustics 4th International Conference
  on Synthetic Aperture Sonar and Radar Sept 2018
- **Journal**: None
- **Summary**: Object classification in synthetic aperture sonar (SAS) imagery is usually a data starved and class imbalanced problem. There are few objects of interest present among much benign seafloor. Despite these problems, current classification techniques discard a large portion of the collected SAS information. In particular, a beamformed SAS image, which we call a single-look complex (SLC) image, contains complex pixels composed of real and imaginary parts. For human consumption, the SLC is converted to a magnitude-phase representation and the phase information is discarded. Even more problematic, the magnitude information usually exhibits a large dynamic range (>80dB) and must be dynamic range compressed for human display. Often it is this dynamic range compressed representation, originally designed for human consumption, which is fed into a classifier. Consequently, the classification process is completely void of the phase information. In this work, we show improvements in classification performance using the phase information from the SLC as well as information from an alternate source: photographs. We perform statistical testing to demonstrate the validity of our results.



### Parkinson's Disease Assessment from a Wrist-Worn Wearable Sensor in Free-Living Conditions: Deep Ensemble Learning and Visualization
- **Arxiv ID**: http://arxiv.org/abs/1808.02870v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.02870v1)
- **Published**: 2018-08-08 17:17:15+00:00
- **Updated**: 2018-08-08 17:17:15+00:00
- **Authors**: Terry Taewoong Um, Franz Michael Josef Pfister, Daniel Christian Pichler, Satoshi Endo, Muriel Lang, Sandra Hirche, Urban Fietzek, Dana Kulić
- **Comment**: This is a pre-print of an article published in Annals of Biomedical
  Engineering (ABME)
- **Journal**: None
- **Summary**: Parkinson's Disease (PD) is characterized by disorders in motor function such as freezing of gait, rest tremor, rigidity, and slowed and hyposcaled movements. Medication with dopaminergic medication may alleviate those motor symptoms, however, side-effects may include uncontrolled movements, known as dyskinesia. In this paper, an automatic PD motor-state assessment in free-living conditions is proposed using an accelerometer in a wrist-worn wearable sensor. In particular, an ensemble of convolutional neural networks (CNNs) is applied to capture the large variability of daily-living activities and overcome the dissimilarity between training and test patients due to the inter-patient variability. In addition, class activation map (CAM), a visualization technique for CNNs, is applied for providing an interpretation of the results.



### Visualizing Convolutional Networks for MRI-based Diagnosis of Alzheimer's Disease
- **Arxiv ID**: http://arxiv.org/abs/1808.02874v1
- **DOI**: 10.1007/978-3-030-02628-8_3
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.02874v1)
- **Published**: 2018-08-08 17:36:10+00:00
- **Updated**: 2018-08-08 17:36:10+00:00
- **Authors**: Johannes Rieke, Fabian Eitel, Martin Weygandt, John-Dylan Haynes, Kerstin Ritter
- **Comment**: MLCN 2018
- **Journal**: None
- **Summary**: Visualizing and interpreting convolutional neural networks (CNNs) is an important task to increase trust in automatic medical decision making systems. In this study, we train a 3D CNN to detect Alzheimer's disease based on structural MRI scans of the brain. Then, we apply four different gradient-based and occlusion-based visualization methods that explain the network's classification decisions by highlighting relevant areas in the input image. We compare the methods qualitatively and quantitatively. We find that all four methods focus on brain regions known to be involved in Alzheimer's disease, such as inferior and middle temporal gyrus. While the occlusion-based methods focus more on specific regions, the gradient-based methods pick up distributed relevance patterns. Additionally, we find that the distribution of relevance varies across patients, with some having a stronger focus on the temporal lobe, whereas for others more cortical areas are relevant. In summary, we show that applying different visualization methods is important to understand the decisions of a CNN, a step that is crucial to increase clinical impact and trust in computer-based decision support systems.



### OCT segmentation: Integrating open parametric contour model of the retinal layers and shape constraint to the Mumford-Shah functional
- **Arxiv ID**: http://arxiv.org/abs/1808.02917v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.02917v1)
- **Published**: 2018-08-08 19:29:21+00:00
- **Updated**: 2018-08-08 19:29:21+00:00
- **Authors**: Jinming Duan, Weicheng Xie, Ryan Wen Liu, Christopher Tench, Irene Gottlob, Frank Proudlock, Li Bai
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel retinal layer boundary model for segmentation of optical coherence tomography (OCT) images. The retinal layer boundary model consists of 9 open parametric contours representing the 9 retinal layers in OCT images. An intensity-based Mumford-Shah (MS) variational functional is first defined to evolve the retinal layer boundary model to segment the 9 layers simultaneously. By making use of the normals of open parametric contours, we construct equal sized adjacent narrowbands that are divided by each contour. Regional information in each narrowband can thus be integrated into the MS energy functional such that its optimisation is robust against different initialisations. A statistical prior is also imposed on the shape of the segmented parametric contours for the functional. As such, by minimising the MS energy functional the parametric contours can be driven towards the true boundaries of retinal layers, while the similarity of the contours with respect to training OCT shapes is preserved. Experimental results on real OCT images demonstrate that the method is accurate and robust to low quality OCT images with low contrast and high-level speckle noise, and it outperforms the recent geodesic distance based method for segmenting 9 layers of the retina in OCT images.



### Feature Dimensionality Reduction for Video Affect Classification: A Comparative Study
- **Arxiv ID**: http://arxiv.org/abs/1808.02956v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.HC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1808.02956v1)
- **Published**: 2018-08-08 22:29:45+00:00
- **Updated**: 2018-08-08 22:29:45+00:00
- **Authors**: Chenfeng Guo, Dongrui Wu
- **Comment**: 1st Asian Affective Computing and Intelligent Interaction Conference,
  Beijing, May 2018
- **Journal**: None
- **Summary**: Affective computing has become a very important research area in human-machine interaction. However, affects are subjective, subtle, and uncertain. So, it is very difficult to obtain a large number of labeled training samples, compared with the number of possible features we could extract. Thus, dimensionality reduction is critical in affective computing. This paper presents our preliminary study on dimensionality reduction for affect classification. Five popular dimensionality reduction approaches are introduced and compared. Experiments on the DEAP dataset showed that no approach can universally outperform others, and performing classification using the raw features directly may not always be a bad choice.



### Transfer Learning Enhanced Common Spatial Pattern Filtering for Brain Computer Interfaces (BCIs): Overview and a New Approach
- **Arxiv ID**: http://arxiv.org/abs/1808.05853v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1808.05853v1)
- **Published**: 2018-08-08 22:52:12+00:00
- **Updated**: 2018-08-08 22:52:12+00:00
- **Authors**: He He, Dongrui Wu
- **Comment**: None
- **Journal**: None
- **Summary**: The electroencephalogram (EEG) is the most widely used input for brain computer interfaces (BCIs), and common spatial pattern (CSP) is frequently used to spatially filter it to increase its signal-to-noise ratio. However, CSP is a supervised filter, which needs some subject-specific calibration data to design. This is time-consuming and not user-friendly. A promising approach for shortening or even completely eliminating this calibration session is transfer learning, which leverages relevant data or knowledge from other subjects or tasks. This paper reviews three existing approaches for incorporating transfer learning into CSP, and also proposes a new transfer learning enhanced CSP approach. Experiments on motor imagery classification demonstrate their effectiveness. Particularly, our proposed approach achieves the best performance when the number of target domain calibration samples is small.



