# Arxiv Papers in cs.CV on 2018-08-22
### Asymptotic Soft Filter Pruning for Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1808.07471v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.07471v4)
- **Published**: 2018-08-22 00:32:41+00:00
- **Updated**: 2019-11-11 02:10:29+00:00
- **Authors**: Yang He, Xuanyi Dong, Guoliang Kang, Yanwei Fu, Chenggang Yan, Yi Yang
- **Comment**: Extended Journal Version of arXiv:1808.06866
- **Journal**: None
- **Summary**: Deeper and wider Convolutional Neural Networks (CNNs) achieve superior performance but bring expensive computation cost. Accelerating such over-parameterized neural network has received increased attention. A typical pruning algorithm is a three-stage pipeline, i.e., training, pruning, and retraining. Prevailing approaches fix the pruned filters to zero during retraining, and thus significantly reduce the optimization space. Besides, they directly prune a large number of filters at first, which would cause unrecoverable information loss. To solve these problems, we propose an Asymptotic Soft Filter Pruning (ASFP) method to accelerate the inference procedure of the deep neural networks. First, we update the pruned filters during the retraining stage. As a result, the optimization space of the pruned model would not be reduced but be the same as that of the original model. In this way, the model has enough capacity to learn from the training data. Second, we prune the network asymptotically. We prune few filters at first and asymptotically prune more filters during the training procedure. With asymptotic pruning, the information of the training set would be gradually concentrated in the remaining filters, so the subsequent training and pruning process would be stable. Experiments show the effectiveness of our ASFP on image classification benchmarks. Notably, on ILSVRC-2012, our ASFP reduces more than 40% FLOPs on ResNet-50 with only 0.14% top-5 accuracy degradation, which is higher than the soft filter pruning (SFP) by 8%.



### Can 3D Pose be Learned from 2D Projections Alone?
- **Arxiv ID**: http://arxiv.org/abs/1808.07182v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.07182v1)
- **Published**: 2018-08-22 01:57:33+00:00
- **Updated**: 2018-08-22 01:57:33+00:00
- **Authors**: Dylan Drover, Rohith MV, Ching-Hang Chen, Amit Agrawal, Ambrish Tyagi, Cong Phuoc Huynh
- **Comment**: Appearing in ECCVW 2018 proceedings
- **Journal**: None
- **Summary**: 3D pose estimation from a single image is a challenging task in computer vision. We present a weakly supervised approach to estimate 3D pose points, given only 2D pose landmarks. Our method does not require correspondences between 2D and 3D points to build explicit 3D priors. We utilize an adversarial framework to impose a prior on the 3D structure, learned solely from their random 2D projections. Given a set of 2D pose landmarks, the generator network hypothesizes their depths to obtain a 3D skeleton. We propose a novel Random Projection layer, which randomly projects the generated 3D skeleton and sends the resulting 2D pose to the discriminator. The discriminator improves by discriminating between the generated poses and pose samples from a real distribution of 2D poses. Training does not require correspondence between the 2D inputs to either the generator or the discriminator. We apply our approach to the task of 3D human pose estimation. Results on Human3.6M dataset demonstrates that our approach outperforms many previous supervised and weakly supervised approaches.



### Coarse-to-Fine Annotation Enrichment for Semantic Segmentation Learning
- **Arxiv ID**: http://arxiv.org/abs/1808.07209v1
- **DOI**: 10.1145/3269206.3271672
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1808.07209v1)
- **Published**: 2018-08-22 03:55:33+00:00
- **Updated**: 2018-08-22 03:55:33+00:00
- **Authors**: Yadan Luo, Ziwei Wang, Zi Huang, Yang Yang, Cong Zhao
- **Comment**: CIKM 2018 International Conference on Information and Knowledge
  Management
- **Journal**: None
- **Summary**: Rich high-quality annotated data is critical for semantic segmentation learning, yet acquiring dense and pixel-wise ground-truth is both labor- and time-consuming. Coarse annotations (e.g., scribbles, coarse polygons) offer an economical alternative, with which training phase could hardly generate satisfactory performance unfortunately. In order to generate high-quality annotated data with a low time cost for accurate segmentation, in this paper, we propose a novel annotation enrichment strategy, which expands existing coarse annotations of training data to a finer scale. Extensive experiments on the Cityscapes and PASCAL VOC 2012 benchmarks have shown that the neural networks trained with the enriched annotations from our framework yield a significant improvement over that trained with the original coarse labels. It is highly competitive to the performance obtained by using human annotated dense annotations. The proposed method also outperforms among other state-of-the-art weakly-supervised segmentation methods.



### Nuclei Detection Using Mixture Density Networks
- **Arxiv ID**: http://arxiv.org/abs/1808.08279v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.08279v1)
- **Published**: 2018-08-22 05:59:19+00:00
- **Updated**: 2018-08-22 05:59:19+00:00
- **Authors**: Navid Alemi Koohababni, Mostafa Jahanifar, Ali Gooya, Nasir Rajpoot
- **Comment**: 8 pages, 3 figures
- **Journal**: None
- **Summary**: Nuclei detection is an important task in the histology domain as it is a main step toward further analysis such as cell counting, cell segmentation, study of cell connections, etc. This is a challenging task due to the complex texture of histology image, variation in shape, and touching cells. To tackle these hurdles, many approaches have been proposed in the literature where deep learning methods stand on top in terms of performance. Hence, in this paper, we propose a novel framework for nuclei detection based on Mixture Density Networks (MDNs). These networks are suitable to map a single input to several possible outputs and we utilize this property to detect multiple seeds in a single image patch. A new modified form of a cost function is proposed for training and handling patches with missing nuclei. The probability maps of the nuclei in the individual patches are next combined to generate the final image-wide result. The experimental results show the state-of-the-art performance on complex colorectal adenocarcinoma dataset.



### Deep multiscale convolutional feature learning for weakly supervised localization of chest pathologies in X-ray images
- **Arxiv ID**: http://arxiv.org/abs/1808.08280v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1808.08280v1)
- **Published**: 2018-08-22 06:08:45+00:00
- **Updated**: 2018-08-22 06:08:45+00:00
- **Authors**: Suman Sedai, Dwarikanath Mahapatra, Zongyuan Ge, Rajib Chakravorty, Rahil Garnavi
- **Comment**: None
- **Journal**: None
- **Summary**: Localization of chest pathologies in chest X-ray images is a challenging task because of their varying sizes and appearances. We propose a novel weakly supervised method to localize chest pathologies using class aware deep multiscale feature learning. Our method leverages intermediate feature maps from CNN layers at different stages of a deep network during the training of a classification model using image level annotations of pathologies. During the training phase, a set of \emph{layer relevance weights} are learned for each pathology class and the CNN is optimized to perform pathology classification by convex combination of feature maps from both shallow and deep layers using the learned weights. During the test phase, to localize the predicted pathology, the multiscale attention map is obtained by convex combination of class activation maps from each stage using the \emph{layer relevance weights} learned during the training phase. We have validated our method using 112000 X-ray images and compared with the state-of-the-art localization methods. We experimentally demonstrate that the proposed weakly supervised method can improve the localization performance of small pathologies such as nodule and mass while giving comparable performance for bigger pathologies e.g., Cardiomegaly



### A Survey of Modern Object Detection Literature using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1808.07256v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.07256v1)
- **Published**: 2018-08-22 07:43:50+00:00
- **Updated**: 2018-08-22 07:43:50+00:00
- **Authors**: Karanbir Singh Chahal, Kuntal Dey
- **Comment**: None
- **Journal**: None
- **Summary**: Object detection is the identification of an object in the image along with its localisation and classification. It has wide spread applications and is a critical component for vision based software systems. This paper seeks to perform a rigorous survey of modern object detection algorithms that use deep learning. As part of the survey, the topics explored include various algorithms, quality metrics, speed/size trade offs and training methodologies. This paper focuses on the two types of object detection algorithms- the SSD class of single step detectors and the Faster R-CNN class of two step detectors. Techniques to construct detectors that are portable and fast on low powered devices are also addressed by exploring new lightweight convolutional base architectures. Ultimately, a rigorous review of the strengths and weaknesses of each detector leads us to the present state of the art.



### Escaping from Collapsing Modes in a Constrained Space
- **Arxiv ID**: http://arxiv.org/abs/1808.07258v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1808.07258v1)
- **Published**: 2018-08-22 07:51:22+00:00
- **Updated**: 2018-08-22 07:51:22+00:00
- **Authors**: Chia-Che Chang, Chieh Hubert Lin, Che-Rung Lee, Da-Cheng Juan, Wei Wei, Hwann-Tzong Chen
- **Comment**: To appear in ECCV 2018
- **Journal**: None
- **Summary**: Generative adversarial networks (GANs) often suffer from unpredictable mode-collapsing during training. We study the issue of mode collapse of Boundary Equilibrium Generative Adversarial Network (BEGAN), which is one of the state-of-the-art generative models. Despite its potential of generating high-quality images, we find that BEGAN tends to collapse at some modes after a period of training. We propose a new model, called \emph{BEGAN with a Constrained Space} (BEGAN-CS), which includes a latent-space constraint in the loss function. We show that BEGAN-CS can significantly improve training stability and suppress mode collapse without either increasing the model complexity or degrading the image quality. Further, we visualize the distribution of latent vectors to elucidate the effect of latent-space constraint. The experimental results show that our method has additional advantages of being able to train on small datasets and to generate images similar to a given real image yet with variations of designated attributes on-the-fly.



### A Deep Neural Network for Pixel-Level Electromagnetic Particle Identification in the MicroBooNE Liquid Argon Time Projection Chamber
- **Arxiv ID**: http://arxiv.org/abs/1808.07269v1
- **DOI**: 10.1103/PhysRevD.99.092001
- **Categories**: **hep-ex**, cs.CV, physics.data-an, physics.ins-det
- **Links**: [PDF](http://arxiv.org/pdf/1808.07269v1)
- **Published**: 2018-08-22 08:27:28+00:00
- **Updated**: 2018-08-22 08:27:28+00:00
- **Authors**: MicroBooNE collaboration, C. Adams, M. Alrashed, R. An, J. Anthony, J. Asaadi, A. Ashkenazi, M. Auger, S. Balasubramanian, B. Baller, C. Barnes, G. Barr, M. Bass, F. Bay, A. Bhat, K. Bhattacharya, M. Bishai, A. Blake, T. Bolton, L. Camilleri, D. Caratelli, I. Caro Terrazas, R. Carr, R. Castillo Fernandez, F. Cavanna, G. Cerati, Y. Chen, E. Church, D. Cianci, E. Cohen, G. H. Collin, J. M. Conrad, M. Convery, L. Cooper-Troendle, J. I. Crespo-Anadon, M. Del Tutto, D. Devitt, A. Diaz, K. Duffy, S. Dytman, B. Eberly, A. Ereditato, L. Escudero Sanchez, J. Esquivel, J. J. Evans, A. A. Fadeeva, R. S. Fitzpatrick, B. T. Fleming, D. Franco, A. P. Furmanski, D. Garcia-Gamez, G. T. Garvey, V. Genty, D. Goeldi, S. Gollapinni, O. Goodwin, E. Gramellini, H. Greenlee, R. Grosso, R. Guenette, P. Guzowski, A. Hackenburg, P. Hamilton, O. Hen, V Hewes, C. Hill, G. A. Horton-Smith, A. Hourlier, E. -C. Huang, C. James, J. Jan de Vries, L. Jiang, R. A. Johnson, J. Joshi, H. Jostlein, Y. -J. Jwa, G. Karagiorgi, W. Ketchum, B. Kirby, M. Kirby, T. Kobilarcik, I. Kreslo, Y. Li, A. Lister, B. R. Littlejohn, S. Lockwitz, D. Lorca, W. C. Louis, M. Luethi, B. Lundberg, X. Luo, A. Marchionni, S. Marcocci, C. Mariani, J. Marshall, J. Martin-Albo, D. A. Martinez Caicedo, A. Mastbaum, V. Meddage, T. Mettler, G. B. Mills, K. Mistry, A. Mogan, J. Moon, M. Mooney, C. D. Moore, J. Mousseau, M. Murphy, R. Murrells, D. Naples, P. Nienaber, J. Nowak, O. Palamara, V. Pandey, V. Paolone, A. Papadopoulou, V. Papavassiliou, S. F. Pate, Z. Pavlovic, E. Piasetzky, D. Porzio, G. Pulliam, X. Qian, J. L. Raaf, A. Rafique, L. Rochester, M. Ross-Lonergan, C. Rudolf von Rohr, B. Russell, D. W. Schmitz, A. Schukraft, W. Seligman, M. H. Shaevitz, R. Sharankova, J. Sinclair, A. Smith, E. L. Snider, M. Soderberg, S. Soldner-Rembold, S. R. Soleti, P. Spentzouris, J. Spitz, J. St. John, T. Strauss, K. Sutton, S. Sword-Fehlberg, A. M. Szelc, N. Tagg, W. Tang, K. Terao, M. Thomson, R. T. Thornton, M. Toups, Y. -T. Tsai, S. Tufanli, T. Usher, W. Van De Pontseele, R. G. Van de Water, B. Viren, M. Weber, H. Wei, D. A. Wickremasinghe, K. Wierman, Z. Williams, S. Wolbers, T. Wongjirad, K. Woodruff, T. Yang, G. Yarbrough, L. E. Yates, G. P. Zeller, J. Zennamo, C. Zhang
- **Comment**: None
- **Journal**: Phys. Rev. D 99, 092001 (2019)
- **Summary**: We have developed a convolutional neural network (CNN) that can make a pixel-level prediction of objects in image data recorded by a liquid argon time projection chamber (LArTPC) for the first time. We describe the network design, training techniques, and software tools developed to train this network. The goal of this work is to develop a complete deep neural network based data reconstruction chain for the MicroBooNE detector. We show the first demonstration of a network's validity on real LArTPC data using MicroBooNE collection plane images. The demonstration is performed for stopping muon and a $\nu_\mu$ charged current neutral pion data samples.



### Deep Adaptive Temporal Pooling for Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/1808.07272v1
- **DOI**: 10.1145/3240508.3240713
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1808.07272v1)
- **Published**: 2018-08-22 08:29:38+00:00
- **Updated**: 2018-08-22 08:29:38+00:00
- **Authors**: Sibo Song, Ngai-Man Cheung, Vijay Chandrasekhar, Bappaditya Mandal
- **Comment**: Accepted by ACM Multimedia 2018
- **Journal**: None
- **Summary**: Deep neural networks have recently achieved competitive accuracy for human activity recognition. However, there is room for improvement, especially in modeling long-term temporal importance and determining the activity relevance of different temporal segments in a video. To address this problem, we propose a learnable and differentiable module: Deep Adaptive Temporal Pooling (DATP). DATP applies a self-attention mechanism to adaptively pool the classification scores of different video segments. Specifically, using frame-level features, DATP regresses importance of different temporal segments and generates weights for them. Remarkably, DATP is trained using only the video-level label. There is no need of additional supervision except video-level activity class label. We conduct extensive experiments to investigate various input features and different weight models. Experimental results show that DATP can learn to assign large weights to key video segments. More importantly, DATP can improve training of frame-level feature extractor. This is because relevant temporal segments are assigned large weights during back-propagation. Overall, we achieve state-of-the-art performance on UCF101, HMDB51 and Kinetics datasets.



### CentralNet: a Multilayer Approach for Multimodal Fusion
- **Arxiv ID**: http://arxiv.org/abs/1808.07275v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1808.07275v1)
- **Published**: 2018-08-22 08:37:55+00:00
- **Updated**: 2018-08-22 08:37:55+00:00
- **Authors**: Valentin Vielzeuf, Alexis Lechervy, Stéphane Pateux, Frédéric Jurie
- **Comment**: None
- **Journal**: European Conference on Computer Vision Workshops: Multimodal
  Learning and Applications, Sep 2018, Munich, Germany.
  https://mula2018.github.io/
- **Summary**: This paper proposes a novel multimodal fusion approach, aiming to produce best possible decisions by integrating information coming from multiple media. While most of the past multimodal approaches either work by projecting the features of different modalities into the same space, or by coordinating the representations of each modality through the use of constraints, our approach borrows from both visions. More specifically, assuming each modality can be processed by a separated deep convolutional network, allowing to take decisions independently from each modality, we introduce a central network linking the modality specific networks. This central network not only provides a common feature embedding but also regularizes the modality specific networks through the use of multi-task learning. The proposed approach is validated on 4 different computer vision tasks on which it consistently improves the accuracy of existing multimodal fusion approaches.



### A syllable based model for handwriting recognition
- **Arxiv ID**: http://arxiv.org/abs/1808.07277v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.07277v1)
- **Published**: 2018-08-22 08:45:43+00:00
- **Updated**: 2018-08-22 08:45:43+00:00
- **Authors**: Wassim Swaileh, Thierry Paquet
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we introduce a new modeling approach of texts for handwriting recognition based on syllables. We propose a supervised syllabification approach for the French and English languages for building a vocabulary of syllables. Statistical n-gram language models of syllables are trained on French and English Wikipedia corpora. The handwriting recognition system, based on optical HMM context independent character models, performs a two pass decoding, integrating the proposed syllabic models. Evaluation is carried out on the French RIMES dataset and English IAM dataset by analyzing the performance for various coverage of the syllable models. We also compare the syllable models with lexicon and character n-gram models. The proposed approach reaches interesting performances thanks to its capacity to cover a large amount of out of vocabulary words working with a limited amount of syllables combined with statistical n-gram of reasonable order.



### Deep Association Learning for Unsupervised Video Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1808.07301v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.07301v1)
- **Published**: 2018-08-22 10:16:43+00:00
- **Updated**: 2018-08-22 10:16:43+00:00
- **Authors**: Yanbei Chen, Xiatian Zhu, Shaogang Gong
- **Comment**: Accepted by BMVC2018
- **Journal**: None
- **Summary**: Deep learning methods have started to dominate the research progress of video-based person re-identification (re-id). However, existing methods mostly consider supervised learning, which requires exhaustive manual efforts for labelling cross-view pairwise data. Therefore, they severely lack scalability and practicality in real-world video surveillance applications. In this work, to address the video person re-id task, we formulate a novel Deep Association Learning (DAL) scheme, the first end-to-end deep learning method using none of the identity labels in model initialisation and training. DAL learns a deep re-id matching model by jointly optimising two margin-based association losses in an end-to-end manner, which effectively constrains the association of each frame to the best-matched intra-camera representation and cross-camera representation. Existing standard CNNs can be readily employed within our DAL scheme. Experiment results demonstrate that our proposed DAL significantly outperforms current state-of-the-art unsupervised video person re-id methods on three benchmarks: PRID 2011, iLIDS-VID and MARS.



### Multidomain Document Layout Understanding using Few Shot Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1808.07330v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.07330v1)
- **Published**: 2018-08-22 12:23:51+00:00
- **Updated**: 2018-08-22 12:23:51+00:00
- **Authors**: Pranaydeep Singh, Srikrishna Varadarajan, Ankit Narayan Singh, Muktabh Mayank Srivastava
- **Comment**: None
- **Journal**: None
- **Summary**: We try to address the problem of document layout understanding using a simple algorithm which generalizes across multiple domains while training on just few examples per domain. We approach this problem via supervised object detection method and propose a methodology to overcome the requirement of large datasets. We use the concept of transfer learning by pre-training our object detector on a simple artificial (source) dataset and fine-tuning it on a tiny domain specific (target) dataset. We show that this methodology works for multiple domains with training samples as less as 10 documents. We demonstrate the effect of each component of the methodology in the end result and show the superiority of this methodology over simple object detectors.



### A method for automatic forensic facial reconstruction based on dense statistics of soft tissue thickness
- **Arxiv ID**: http://arxiv.org/abs/1808.07334v1
- **DOI**: 10.1371/journal.pone.0210257
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.07334v1)
- **Published**: 2018-08-22 12:43:51+00:00
- **Updated**: 2018-08-22 12:43:51+00:00
- **Authors**: Thomas Gietzen, Robert Brylka, Jascha Achenbach, Katja zum Hebel, Elmar Schömer, Mario Botsch, Ulrich Schwanecke, Ralf Schulze
- **Comment**: 19 pages, 12 figures
- **Journal**: None
- **Summary**: In this paper, we present a method for automated estimation of a human face given a skull remain. The proposed method is based on three statistical models. A volumetric (tetrahedral) skull model encoding the variations of different skulls, a surface head model encoding the head variations, and a dense statistic of facial soft tissue thickness (FSTT). All data are automatically derived from computed tomography (CT) head scans and optical face scans. In order to obtain a proper dense FSTT statistic, we register a skull model to each skull extracted from a CT scan and determine the FSTT value for each vertex of the skull model towards the associated extracted skin surface. The FSTT values at predefined landmarks from our statistic are well in agreement with data from the literature.   To recover a face from a skull remain, we first fit our skull model to the given skull. Next, we generate spheres with radius of the respective FSTT value obtained from our statistic at each vertex of the registered skull. Finally, we fit a head model to the union of all spheres. The proposed automated method enables a probabilistic face-estimation that facilitates forensic recovery even from incomplete skull remains. The FSTT statistic allows the generation of plausible head variants, which can be adjusted intuitively using principal component analysis. We validate our face recovery process using an anonymized head CT scan. The estimation generated from the given skull visually compares well with the skin surface extracted from the CT scan itself.



### Multi-Branch Siamese Networks with Online Selection for Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/1808.07349v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.07349v3)
- **Published**: 2018-08-22 13:30:22+00:00
- **Updated**: 2018-08-31 20:29:27+00:00
- **Authors**: Zhenxi Li, Guillaume-Alexandre Bilodeau, Wassim Bouachir
- **Comment**: ISVC2018, oral presentation
- **Journal**: None
- **Summary**: In this paper, we propose a robust object tracking algorithm based on a branch selection mechanism to choose the most efficient object representations from multi-branch siamese networks. While most deep learning trackers use a single CNN for target representation, the proposed Multi-Branch Siamese Tracker (MBST) employs multiple branches of CNNs pre-trained for different tasks, and used for various target representations in our tracking method. With our branch selection mechanism, the appropriate CNN branch is selected depending on the target characteristics in an online manner. By using the most adequate target representation with respect to the tracked object, our method achieves real-time tracking, while obtaining improved performance compared to standard Siamese network trackers on object tracking benchmarks.



### Everybody Dance Now
- **Arxiv ID**: http://arxiv.org/abs/1808.07371v2
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1808.07371v2)
- **Published**: 2018-08-22 13:58:36+00:00
- **Updated**: 2019-08-27 21:10:54+00:00
- **Authors**: Caroline Chan, Shiry Ginosar, Tinghui Zhou, Alexei A. Efros
- **Comment**: In ICCV 2019
- **Journal**: None
- **Summary**: This paper presents a simple method for "do as I do" motion transfer: given a source video of a person dancing, we can transfer that performance to a novel (amateur) target after only a few minutes of the target subject performing standard moves. We approach this problem as video-to-video translation using pose as an intermediate representation. To transfer the motion, we extract poses from the source subject and apply the learned pose-to-appearance mapping to generate the target subject. We predict two consecutive frames for temporally coherent video results and introduce a separate pipeline for realistic face synthesis. Although our method is quite simple, it produces surprisingly compelling results (see video). This motivates us to also provide a forensics tool for reliable synthetic content detection, which is able to distinguish videos synthesized by our system from real data. In addition, we release a first-of-its-kind open-source dataset of videos that can be legally used for training and motion transfer.



### Manipulating Attributes of Natural Scenes via Hallucination
- **Arxiv ID**: http://arxiv.org/abs/1808.07413v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.07413v3)
- **Published**: 2018-08-22 16:01:18+00:00
- **Updated**: 2019-10-09 06:18:45+00:00
- **Authors**: Levent Karacan, Zeynep Akata, Aykut Erdem, Erkut Erdem
- **Comment**: Accepted for publication in ACM Transactions on Graphics
- **Journal**: None
- **Summary**: In this study, we explore building a two-stage framework for enabling users to directly manipulate high-level attributes of a natural scene. The key to our approach is a deep generative network which can hallucinate images of a scene as if they were taken at a different season (e.g. during winter), weather condition (e.g. in a cloudy day) or time of the day (e.g. at sunset). Once the scene is hallucinated with the given attributes, the corresponding look is then transferred to the input image while preserving the semantic details intact, giving a photo-realistic manipulation result. As the proposed framework hallucinates what the scene will look like, it does not require any reference style image as commonly utilized in most of the appearance or style transfer approaches. Moreover, it allows to simultaneously manipulate a given scene according to a diverse set of transient attributes within a single model, eliminating the need of training multiple networks per each translation task. Our comprehensive set of qualitative and quantitative results demonstrate the effectiveness of our approach against the competing methods.



### Joint Coarse-And-Fine Reasoning for Deep Optical Flow
- **Arxiv ID**: http://arxiv.org/abs/1808.07416v1
- **DOI**: 10.1109/ICIP.2017.8296744
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.07416v1)
- **Published**: 2018-08-22 16:07:11+00:00
- **Updated**: 2018-08-22 16:07:11+00:00
- **Authors**: Victor Vaquero, German Ros, Francesc Moreno-Noguer, Antonio M. Lopez, Alberto Sanfeliu
- **Comment**: Accepted in IEEE ICIP 2017. IEEE Copyrights: Personal use of this
  material is permitted. Permission from IEEE must be obtained for all other
  uses
- **Journal**: None
- **Summary**: We propose a novel representation for dense pixel-wise estimation tasks using CNNs that boosts accuracy and reduces training time, by explicitly exploiting joint coarse-and-fine reasoning. The coarse reasoning is performed over a discrete classification space to obtain a general rough solution, while the fine details of the solution are obtained over a continuous regression space. In our approach both components are jointly estimated, which proved to be beneficial for improving estimation accuracy. Additionally, we propose a new network architecture, which combines coarse and fine components by treating the fine estimation as a refinement built on top of the coarse solution, and therefore adding details to the general prediction. We apply our approach to the challenging problem of optical flow estimation and empirically validate it against state-of-the-art CNN-based solutions trained from scratch and tested on large optical flow datasets.



### Stacked Pooling: Improving Crowd Counting by Boosting Scale Invariance
- **Arxiv ID**: http://arxiv.org/abs/1808.07456v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.07456v1)
- **Published**: 2018-08-22 17:46:06+00:00
- **Updated**: 2018-08-22 17:46:06+00:00
- **Authors**: Siyu Huang, Xi Li, Zhi-Qi Cheng, Zhongfei Zhang, Alexander Hauptmann
- **Comment**: The code is available at
  http://github.com/siyuhuang/crowdcount-stackpool
- **Journal**: None
- **Summary**: In this work, we explore the cross-scale similarity in crowd counting scenario, in which the regions of different scales often exhibit high visual similarity. This feature is universal both within an image and across different images, indicating the importance of scale invariance of a crowd counting model. Motivated by this, in this paper we propose simple but effective variants of pooling module, i.e., multi-kernel pooling and stacked pooling, to boost the scale invariance of convolutional neural networks (CNNs), benefiting much the crowd density estimation and counting. Specifically, the multi-kernel pooling comprises of pooling kernels with multiple receptive fields to capture the responses at multi-scale local ranges. The stacked pooling is an equivalent form of multi-kernel pooling, while, it reduces considerable computing cost. Our proposed pooling modules do not introduce extra parameters into model and can easily take place of the vanilla pooling layer in implementation. In empirical study on two benchmark crowd counting datasets, the stacked pooling beats the vanilla pooling layer in most cases.



### Second-order Democratic Aggregation
- **Arxiv ID**: http://arxiv.org/abs/1808.07503v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.07503v1)
- **Published**: 2018-08-22 18:07:26+00:00
- **Updated**: 2018-08-22 18:07:26+00:00
- **Authors**: Tsung-Yu Lin, Subhransu Maji, Piotr Koniusz
- **Comment**: None
- **Journal**: None
- **Summary**: Aggregated second-order features extracted from deep convolutional networks have been shown to be effective for texture generation, fine-grained recognition, material classification, and scene understanding. In this paper, we study a class of orderless aggregation functions designed to minimize interference or equalize contributions in the context of second-order features and we show that they can be computed just as efficiently as their first-order counterparts and they have favorable properties over aggregation by summation. Another line of work has shown that matrix power normalization after aggregation can significantly improve the generalization of second-order representations. We show that matrix power normalization implicitly equalizes contributions during aggregation thus establishing a connection between matrix normalization techniques and prior work on minimizing interference. Based on the analysis we present {\gamma}-democratic aggregators that interpolate between sum ({\gamma}=1) and democratic pooling ({\gamma}=0) outperforming both on several classification tasks. Moreover, unlike power normalization, the {\gamma}-democratic aggregations can be computed in a low dimensional space by sketching that allows the use of very high-dimensional second-order features. This results in a state-of-the-art performance on several datasets.



### Video Jigsaw: Unsupervised Learning of Spatiotemporal Context for Video Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1808.07507v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.07507v1)
- **Published**: 2018-08-22 18:18:44+00:00
- **Updated**: 2018-08-22 18:18:44+00:00
- **Authors**: Unaiza Ahsan, Rishi Madhok, Irfan Essa
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a self-supervised learning method to jointly reason about spatial and temporal context for video recognition. Recent self-supervised approaches have used spatial context [9, 34] as well as temporal coherency [32] but a combination of the two requires extensive preprocessing such as tracking objects through millions of video frames [59] or computing optical flow to determine frame regions with high motion [30]. We propose to combine spatial and temporal context in one self-supervised framework without any heavy preprocessing. We divide multiple video frames into grids of patches and train a network to solve jigsaw puzzles on these patches from multiple frames. So the network is trained to correctly identify the position of a patch within a video frame as well as the position of a patch over time. We also propose a novel permutation strategy that outperforms random permutations while significantly reducing computational and memory constraints. We use our trained network for transfer learning tasks such as video activity recognition and demonstrate the strength of our approach on two benchmark video action recognition datasets without using a single frame from these datasets for unsupervised pretraining of our proposed video jigsaw network.



### Vehicles Lane-changing Behavior Detection
- **Arxiv ID**: http://arxiv.org/abs/1808.07518v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.07518v1)
- **Published**: 2018-08-22 18:37:10+00:00
- **Updated**: 2018-08-22 18:37:10+00:00
- **Authors**: Iljoo Baek, Mengwen He
- **Comment**: None
- **Journal**: None
- **Summary**: The lane-level localization accuracy is very important for autonomous vehicles. The Global Navigation Satellite System (GNSS), e.g. GPS, is a generic localization method for vehicles, but is vulnerable to the multi-path interference in the urban environment. Integrating the vision-based relative localization result and a digital map with the GNSS is a common and cheap way to increase the global localization accuracy and thus to realize the lane-level localization. This project is to develop a mono-camera based lane-changing behavior detection module for the correction of lateral GPS localization. We implemented a Support Vector Machine (SVM) based framework to directly classify the driving behavior, including the lane keeping, left and right lane changing, from a sampled data of the raw image captured by the mono-camera installed behind the window shield. The training data was collected from the driving around Carnegie Mellon University, and we compared the trained SVM models w/ and w/o the Principle Component Analysis (PCA) dimension reduction technique. The performance of the SVM based classification method was compared with the CNN method.



### Rethinking Monocular Depth Estimation with Adversarial Training
- **Arxiv ID**: http://arxiv.org/abs/1808.07528v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.07528v3)
- **Published**: 2018-08-22 19:11:41+00:00
- **Updated**: 2019-06-15 18:37:26+00:00
- **Authors**: Richard Chen, Faisal Mahmood, Alan Yuille, Nicholas J. Durr
- **Comment**: None
- **Journal**: None
- **Summary**: Monocular depth estimation is an extensively studied computer vision problem with a vast variety of applications. Deep learning-based methods have demonstrated promise for both supervised and unsupervised depth estimation from monocular images. Most existing approaches treat depth estimation as a regression problem with a local pixel-wise loss function. In this work, we innovate beyond existing approaches by using adversarial training to learn a context-aware, non-local loss function. Such an approach penalizes the joint configuration of predicted depth values at the patch-level instead of the pixel-level, which allows networks to incorporate more global information. In this framework, the generator learns a mapping between RGB images and its corresponding depth map, while the discriminator learns to distinguish depth map and RGB pairs from ground truth. This conditional GAN depth estimation framework is stabilized using spectral normalization to prevent mode collapse when learning from diverse datasets. We test this approach using a diverse set of generators that include U-Net and joint CNN-CRF. We benchmark this approach on the NYUv2, Make3D and KITTI datasets, and observe that adversarial training reduces relative error by several fold, achieving state-of-the-art performance.



### Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images
- **Arxiv ID**: http://arxiv.org/abs/1808.08114v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.08114v2)
- **Published**: 2018-08-22 19:17:23+00:00
- **Updated**: 2019-01-20 00:29:45+00:00
- **Authors**: Jo Schlemper, Ozan Oktay, Michiel Schaap, Mattias Heinrich, Bernhard Kainz, Ben Glocker, Daniel Rueckert
- **Comment**: Accepted for Medical Image Analysis (Special Issue on Medical Imaging
  with Deep Learning). arXiv admin note: substantial text overlap with
  arXiv:1804.03999, arXiv:1804.05338
- **Journal**: None
- **Summary**: We propose a novel attention gate (AG) model for medical image analysis that automatically learns to focus on target structures of varying shapes and sizes. Models trained with AGs implicitly learn to suppress irrelevant regions in an input image while highlighting salient features useful for a specific task. This enables us to eliminate the necessity of using explicit external tissue/organ localisation modules when using convolutional neural networks (CNNs). AGs can be easily integrated into standard CNN models such as VGG or U-Net architectures with minimal computational overhead while increasing the model sensitivity and prediction accuracy. The proposed AG models are evaluated on a variety of tasks, including medical image classification and segmentation. For classification, we demonstrate the use case of AGs in scan plane detection for fetal ultrasound screening. We show that the proposed attention mechanism can provide efficient object localisation while improving the overall prediction performance by reducing false positives. For segmentation, the proposed architecture is evaluated on two large 3D CT abdominal datasets with manual annotations for multiple organs. Experimental results show that AG models consistently improve the prediction performance of the base architectures across different datasets and training sizes while preserving computational efficiency. Moreover, AGs guide the model activations to be focused around salient regions, which provides better insights into how model predictions are made. The source code for the proposed AG models is publicly available.



### Learning Hierarchical Semantic Image Manipulation through Structured Representations
- **Arxiv ID**: http://arxiv.org/abs/1808.07535v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.07535v2)
- **Published**: 2018-08-22 19:33:31+00:00
- **Updated**: 2018-08-28 00:33:27+00:00
- **Authors**: Seunghoon Hong, Xinchen Yan, Thomas Huang, Honglak Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Understanding, reasoning, and manipulating semantic concepts of images have been a fundamental research problem for decades. Previous work mainly focused on direct manipulation on natural image manifold through color strokes, key-points, textures, and holes-to-fill. In this work, we present a novel hierarchical framework for semantic image manipulation. Key to our hierarchical framework is that we employ a structured semantic layout as our intermediate representation for manipulation. Initialized with coarse-level bounding boxes, our structure generator first creates pixel-wise semantic layout capturing the object shape, object-object interactions, and object-scene relations. Then our image generator fills in the pixel-level textures guided by the semantic layout. Such framework allows a user to manipulate images at object-level by adding, removing, and moving one bounding box at a time. Experimental evaluations demonstrate the advantages of the hierarchical manipulation framework over existing image generation and context hole-filing models, both qualitatively and quantitatively. Benefits of the hierarchical framework are further demonstrated in applications such as semantic object manipulation, interactive image editing, and data-driven image manipulation.



