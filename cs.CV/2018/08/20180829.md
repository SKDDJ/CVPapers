# Arxiv Papers in cs.CV on 2018-08-29
### Adapting Visual Question Answering Models for Enhancing Multimodal Community Q&A Platforms
- **Arxiv ID**: http://arxiv.org/abs/1808.09648v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1808.09648v2)
- **Published**: 2018-08-29 05:53:17+00:00
- **Updated**: 2019-05-25 20:24:44+00:00
- **Authors**: Avikalp Srivastava, Hsin Wen Liu, Sumio Fujita
- **Comment**: Submitted for review at CIKM 2019
- **Journal**: None
- **Summary**: Question categorization and expert retrieval methods have been crucial for information organization and accessibility in community question & answering (CQA) platforms. Research in this area, however, has dealt with only the text modality. With the increasing multimodal nature of web content, we focus on extending these methods for CQA questions accompanied by images. Specifically, we leverage the success of representation learning for text and images in the visual question answering (VQA) domain, and adapt the underlying concept and architecture for automated category classification and expert retrieval on image-based questions posted on Yahoo! Chiebukuro, the Japanese counterpart of Yahoo! Answers.   To the best of our knowledge, this is the first work to tackle the multimodality challenge in CQA, and to adapt VQA models for tasks on a more ecologically valid source of visual questions. Our analysis of the differences between visual QA and community QA data drives our proposal of novel augmentations of an attention method tailored for CQA, and use of auxiliary tasks for learning better grounding features. Our final model markedly outperforms the text-only and VQA model baselines for both tasks of classification and expert retrieval on real-world multimodal CQA data.



### Wavelet based edge feature enhancement for convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/1809.00982v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.00982v2)
- **Published**: 2018-08-29 06:13:01+00:00
- **Updated**: 2019-02-04 17:56:20+00:00
- **Authors**: D. D. N. De Silva, S. Fernando, I. T. S. Piyatilake, A. V. S. Karunarathne
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks are able to perform a hierarchical learning process starting with local features. However, a limited attention is paid to enhancing such elementary level features like edges. We propose and evaluate two wavelet-based edge feature enhancement methods to preprocess the input images to convolutional neural networks. The first method develops feature enhanced representations by decomposing the input images using wavelet transform and limited reconstructing subsequently. The second method develops such feature enhanced inputs to the network using local modulus maxima of wavelet coefficients. For each method, we have developed a new preprocessing layer by implementing each purposed method and have appended to the network architecture. Our empirical evaluations demonstrate that the proposed methods are outperforming the baselines and previously published work with significant accuracy gains.



### Image-based Survival Analysis for Lung Cancer Patients using CNNs
- **Arxiv ID**: http://arxiv.org/abs/1808.09679v2
- **DOI**: 10.1109/ISBI.2019.8759499
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.09679v2)
- **Published**: 2018-08-29 08:30:46+00:00
- **Updated**: 2018-10-08 12:47:57+00:00
- **Authors**: Christoph Haarburger, Philippe Weitz, Oliver Rippel, Dorit Merhof
- **Comment**: None
- **Journal**: None
- **Summary**: Traditional survival models such as the Cox proportional hazards model are typically based on scalar or categorical clinical features. With the advent of increasingly large image datasets, it has become feasible to incorporate quantitative image features into survival prediction. So far, this kind of analysis is mostly based on radiomics features, i.e. a fixed set of features that is mathematically defined a priori. To capture highly abstract information, it is desirable to learn the feature extraction using convolutional neural networks. However, for tomographic medical images, model training is difficult because on the one hand, only few samples of 3D image data fit into one batch at once and on the other hand, survival loss functions are essentially ordering measures that require large batch sizes. In this work, we show that by simplifying survival analysis to median survival classification, convolutional neural networks can be trained with small batch sizes and learn features that predict survival equally well as end-to-end hazard prediction networks. Our approach outperforms the previous state of the art in a publicly available lung cancer dataset.



### DADA: Deep Adversarial Data Augmentation for Extremely Low Data Regime Classification
- **Arxiv ID**: http://arxiv.org/abs/1809.00981v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.00981v1)
- **Published**: 2018-08-29 09:01:31+00:00
- **Updated**: 2018-08-29 09:01:31+00:00
- **Authors**: Xiaofeng Zhang, Zhangyang Wang, Dong Liu, Qing Ling
- **Comment**: 15 pages, 5 figures
- **Journal**: None
- **Summary**: Deep learning has revolutionized the performance of classification, but meanwhile demands sufficient labeled data for training. Given insufficient data, while many techniques have been developed to help combat overfitting, the challenge remains if one tries to train deep networks, especially in the ill-posed extremely low data regimes: only a small set of labeled data are available, and nothing -- including unlabeled data -- else. Such regimes arise from practical situations where not only data labeling but also data collection itself is expensive. We propose a deep adversarial data augmentation (DADA) technique to address the problem, in which we elaborately formulate data augmentation as a problem of training a class-conditional and supervised generative adversarial network (GAN). Specifically, a new discriminator loss is proposed to fit the goal of data augmentation, through which both real and augmented samples are enforced to contribute to and be consistent in finding the decision boundaries. Tailored training techniques are developed accordingly. To quantitatively validate its effectiveness, we first perform extensive simulations to show that DADA substantially outperforms both traditional data augmentation and a few GAN-based options. We then extend experiments to three real-world small labeled datasets where existing data augmentation and/or transfer learning strategies are either less effective or infeasible. All results endorse the superior capability of DADA in enhancing the generalization ability of deep networks trained in practical extremely low data regimes. Source code is available at https://github.com/SchafferZhang/DADA.



### Fractional Multiscale Fusion-based De-hazing
- **Arxiv ID**: http://arxiv.org/abs/1808.09697v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.09697v1)
- **Published**: 2018-08-29 09:06:05+00:00
- **Updated**: 2018-08-29 09:06:05+00:00
- **Authors**: Uche A. Nnolim
- **Comment**: 23 pages, 13 figures, 2 tables
- **Journal**: None
- **Summary**: This report presents the results of a proposed multi-scale fusion-based single image de-hazing algorithm, which can also be used for underwater image enhancement. Furthermore, the algorithm was designed for very fast operation and minimal run-time. The proposed scheme is the faster than existing algorithms for both de-hazing and underwater image enhancement and amenable to digital hardware implementation. Results indicate mostly consistent and good results for both categories of images when compared with other algorithms from the literature.



### Camera-based Image Forgery Localization using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1808.09714v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.09714v1)
- **Published**: 2018-08-29 10:20:52+00:00
- **Updated**: 2018-08-29 10:20:52+00:00
- **Authors**: Davide Cozzolino, Luisa Verdoliva
- **Comment**: None
- **Journal**: None
- **Summary**: Camera fingerprints are precious tools for a number of image forensics tasks. A well-known example is the photo response non-uniformity (PRNU) noise pattern, a powerful device fingerprint. Here, to address the image forgery localization problem, we rely on noiseprint, a recently proposed CNN-based camera model fingerprint. The CNN is trained to minimize the distance between same-model patches, and maximize the distance otherwise. As a result, the noiseprint accounts for model-related artifacts just like the PRNU accounts for device-related non-uniformities. However, unlike the PRNU, it is only mildly affected by residuals of high-level scene content. The experiments show that the proposed noiseprint-based forgery localization method improves over the PRNU-based reference.



### Cross-Domain Collaborative Learning via Cluster Canonical Correlation Analysis and Random Walker for Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1808.09740v2
- **DOI**: 10.1109/TGRS.2018.2889195
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1808.09740v2)
- **Published**: 2018-08-29 11:37:23+00:00
- **Updated**: 2018-10-30 08:31:28+00:00
- **Authors**: Yao Qin, Lorenzo Bruzzone, Biao Li, Yuanxin Ye
- **Comment**: 14 pages, 10 figures
- **Journal**: None
- **Summary**: This paper introduces a novel heterogenous domain adaptation (HDA) method for hyperspectral image classification with a limited amount of labeled samples in both domains. The method is achieved in the way of cross-domain collaborative learning (CDCL), which is addressed via cluster canonical correlation analysis (C-CCA) and random walker (RW) algorithms. To be specific, the proposed CDCL method is an iterative process of three main stages, i.e. twice of RW-based pseudolabeling and cross domain learning via C-CCA. Firstly, given the initially labeled target samples as training set ($\mathbf{TS}$), the RW-based pseudolabeling is employed to update $\mathbf{TS}$ and extract target clusters ($\mathbf{TCs}$) by fusing the segmentation results obtained by RW and extended RW (ERW) classifiers. Secondly, cross domain learning via C-CCA is applied using labeled source samples and $\mathbf{TCs}$. The unlabeled target samples are then classified with the estimated probability maps using the model trained in the projected correlation subspace. Thirdly, both $\mathbf{TS}$ and estimated probability maps are used for updating $\mathbf{TS}$ again via RW-based pseudolabeling. When the iterative process finishes, the result obtained by the ERW classifier using the final $\mathbf{TS}$ and estimated probability maps is regarded as the final classification map. Experimental results on four real HSIs demonstrate that the proposed method can achieve better performance compared with the state-of-the-art HDA and ERW methods.



### Tensor Alignment Based Domain Adaptation for Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1808.09769v2
- **DOI**: 10.1109/TGRS.2019.2926069
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1808.09769v2)
- **Published**: 2018-08-29 12:51:59+00:00
- **Updated**: 2018-09-04 16:15:57+00:00
- **Authors**: Yao Qin, Lorenzo Bruzzone, Biao Li
- **Comment**: 15 pages, 10 figures
- **Journal**: None
- **Summary**: This paper presents a tensor alignment (TA) based domain adaptation method for hyperspectral image (HSI) classification. To be specific, HSIs in both domains are first segmented into superpixels and tensors of both domains are constructed to include neighboring samples from single superpixel. Then we consider the subspace invariance between two domains as projection matrices and original tensors are projected as core tensors with lower dimensions into the invariant tensor subspace by applying Tucker decomposition. To preserve geometric information in original tensors, we employ a manifold regularization term for core tensors into the decomposition progress. The projection matrices and core tensors are solved in an alternating optimization manner and the convergence of TA algorithm is analyzed. In addition, a post-processing strategy is defined via pure samples extraction for each superpixel to further improve classification performance. Experimental results on four real HSIs demonstrate that the proposed method can achieve better performance compared with the state-of-the-art subspace learning methods when a limited amount of source labeled samples are available.



### Interact as You Intend: Intention-Driven Human-Object Interaction Detection
- **Arxiv ID**: http://arxiv.org/abs/1808.09796v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.09796v2)
- **Published**: 2018-08-29 13:25:50+00:00
- **Updated**: 2019-09-22 11:45:38+00:00
- **Authors**: Bingjie Xu, Junnan Li, Yongkang Wong, Mohan S. Kankanhalli, Qi Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: The recent advances in instance-level detection tasks lay strong foundation for genuine comprehension of the visual scenes. However, the ability to fully comprehend a social scene is still in its preliminary stage. In this work, we focus on detecting human-object interactions (HOIs) in social scene images, which is demanding in terms of research and increasingly useful for practical applications. To undertake social tasks interacting with objects, humans direct their attention and move their body based on their intention. Based on this observation, we provide a unique computational perspective to explore human intention in HOI detection. Specifically, the proposed human intention-driven HOI detection (iHOI) framework models human pose with the relative distances from body joints to the object instances. It also utilizes human gaze to guide the attended contextual regions in a weakly-supervised setting. In addition, we propose a hard negative sampling strategy to address the problem of mis-grouping. We perform extensive experiments on two benchmark datasets, namely V-COCO and HICO-DET. The efficacy of each proposed component has also been validated.



### MACNet: Multi-scale Atrous Convolution Networks for Food Places Classification in Egocentric Photo-streams
- **Arxiv ID**: http://arxiv.org/abs/1808.09829v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.09829v1)
- **Published**: 2018-08-29 13:51:00+00:00
- **Updated**: 2018-08-29 13:51:00+00:00
- **Authors**: Md. Mostafa Kamal Sarker, Hatem A. Rashwan, Estefania Talavera, Syeda Furruka Banu, Petia Radeva, Domenec Puig
- **Comment**: 10 pages, accepted in ECCV at EPIC 2018
- **Journal**: None
- **Summary**: First-person (wearable) camera continually captures unscripted interactions of the camera user with objects, people, and scenes reflecting his personal and relational tendencies. One of the preferences of people is their interaction with food events. The regulation of food intake and its duration has a great importance to protect against diseases. Consequently, this work aims to develop a smart model that is able to determine the recurrences of a person on food places during a day. This model is based on a deep end-to-end model for automatic food places recognition by analyzing egocentric photo-streams. In this paper, we apply multi-scale Atrous convolution networks to extract the key features related to food places of the input images. The proposed model is evaluated on an in-house private dataset called "EgoFoodPlaces". Experimental results shows promising results of food places classification recognition in egocentric photo-streams.



### Semi-Metrification of the Dynamic Time Warping Distance
- **Arxiv ID**: http://arxiv.org/abs/1808.09964v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1808.09964v2)
- **Published**: 2018-08-29 14:46:35+00:00
- **Updated**: 2018-09-02 06:17:56+00:00
- **Authors**: Brijnesh J. Jain
- **Comment**: None
- **Journal**: None
- **Summary**: The dynamic time warping (dtw) distance fails to satisfy the triangle inequality and the identity of indiscernibles. As a consequence, the dtw-distance is not warping-invariant, which in turn results in peculiarities in data mining applications. This article converts the dtw-distance to a semi-metric and shows that its canonical extension is warping-invariant. Empirical results indicate that the nearest-neighbor classifier in the proposed semi-metric space performs comparably to the same classifier in the standard dtw-space. To overcome the undesirable peculiarities of dtw-spaces, this result suggests to further explore the semi-metric space for data mining applications.



### PanoRoom: From the Sphere to the 3D Layout
- **Arxiv ID**: http://arxiv.org/abs/1808.09879v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.09879v1)
- **Published**: 2018-08-29 15:19:03+00:00
- **Updated**: 2018-08-29 15:19:03+00:00
- **Authors**: Clara Fernandez-Labrador, Jose M. Facil, Alejandro Perez-Yus, Cedric Demonceaux, Jose J. Guerrero
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel FCN able to work with omnidirectional images that outputs accurate probability maps representing the main structure of indoor scenes, which is able to generalize on different data. Our approach handles occlusions and recovers complex shaped rooms more faithful to the actual shape of the real scenes. We outperform the state of the art not only in accuracy of the 3D models but also in speed.



### Top-down Attention Recurrent VLAD Encoding for Action Recognition in Videos
- **Arxiv ID**: http://arxiv.org/abs/1808.09892v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.09892v1)
- **Published**: 2018-08-29 15:41:36+00:00
- **Updated**: 2018-08-29 15:41:36+00:00
- **Authors**: Swathikiran Sudhakaran, Oswald Lanz
- **Comment**: Accepted to the 17th International Conference of the Italian
  Association for Artificial Intelligence
- **Journal**: None
- **Summary**: Most recent approaches for action recognition from video leverage deep architectures to encode the video clip into a fixed length representation vector that is then used for classification. For this to be successful, the network must be capable of suppressing irrelevant scene background and extract the representation from the most discriminative part of the video. Our contribution builds on the observation that spatio-temporal patterns characterizing actions in videos are highly correlated with objects and their location in the video. We propose Top-down Attention Action VLAD (TA-VLAD), a deep recurrent architecture with built-in spatial attention that performs temporally aggregated VLAD encoding for action recognition from videos. We adopt a top-down approach of attention, by using class specific activation maps obtained from a deep CNN pre-trained for image classification, to weight appearance features before encoding them into a fixed-length video descriptor using Gated Recurrent Units. Our method achieves state of the art recognition accuracy on HMDB51 and UCF101 benchmarks.



### Autoencoders, Kernels, and Multilayer Perceptrons for Electron Micrograph Restoration and Compression
- **Arxiv ID**: http://arxiv.org/abs/1808.09916v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.09916v1)
- **Published**: 2018-08-29 16:33:49+00:00
- **Updated**: 2018-08-29 16:33:49+00:00
- **Authors**: Jeffrey M. Ede
- **Comment**: 16 pages, 12 figures, 3 tables
- **Journal**: None
- **Summary**: We present 14 autoencoders, 15 kernels and 14 multilayer perceptrons for electron micrograph restoration and compression. These have been trained for transmission electron microscopy (TEM), scanning transmission electron microscopy (STEM) and for both (TEM+STEM). TEM autoencoders have been trained for 1$\times$, 4$\times$, 16$\times$ and 64$\times$ compression, STEM autoencoders for 1$\times$, 4$\times$ and 16$\times$ compression and TEM+STEM autoencoders for 1$\times$, 2$\times$, 4$\times$, 8$\times$, 16$\times$, 32$\times$ and 64$\times$ compression. Kernels and multilayer perceptrons have been trained to approximate the denoising effect of the 4$\times$ compression autoencoders. Kernels for input sizes of 3, 5, 7, 11 and 15 have been fitted for TEM, STEM and TEM+STEM. TEM multilayer perceptrons have been trained with 1 hidden layer for input sizes of 3, 5 and 7 and with 2 hidden layers for input sizes of 5 and 7. STEM multilayer perceptrons have been trained with 1 hidden layer for input sizes of 3, 5 and 7. TEM+STEM multilayer perceptrons have been trained with 1 hidden layer for input sizes of 3, 5, 7 and 11 and with 2 hidden layers for input sizes of 3 and 7. Our code, example usage and pre-trained models are available at https://github.com/Jeffrey-Ede/Denoising-Kernels-MLPs-Autoencoders



### Fixed-Point Convolutional Neural Network for Real-Time Video Processing in FPGA
- **Arxiv ID**: http://arxiv.org/abs/1808.09945v2
- **DOI**: 10.1109/EIConRus.2019.8656778
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.09945v2)
- **Published**: 2018-08-29 17:51:39+00:00
- **Updated**: 2020-12-03 15:51:08+00:00
- **Authors**: Roman Solovyev, Alexander Kustov, Dmitry Telpukhov, Vladimir Rukhlov, Alexandr Kalinin
- **Comment**: 2019 IEEE Conference of Russian Young Researchers in Electrical and
  Electronic Engineering (EIConRus)
- **Journal**: None
- **Summary**: Modern mobile neural networks with a reduced number of weights and parameters do a good job with image classification tasks, but even they may be too complex to be implemented in an FPGA for video processing tasks. The article proposes neural network architecture for the practical task of recognizing images from a camera, which has several advantages in terms of speed. This is achieved by reducing the number of weights, moving from a floating-point to a fixed-point arithmetic, and due to a number of hardware-level optimizations associated with storing weights in blocks, a shift register, and an adjustable number of convolutional blocks that work in parallel. The article also proposed methods for adapting the existing data set for solving a different task. As the experiments showed, the proposed neural network copes well with real-time video processing even on the cheap FPGAs.



### Interpretable Intuitive Physics Model
- **Arxiv ID**: http://arxiv.org/abs/1808.10002v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.10002v1)
- **Published**: 2018-08-29 18:22:21+00:00
- **Updated**: 2018-08-29 18:22:21+00:00
- **Authors**: Tian Ye, Xiaolong Wang, James Davidson, Abhinav Gupta
- **Comment**: None
- **Journal**: None
- **Summary**: Humans have a remarkable ability to use physical commonsense and predict the effect of collisions. But do they understand the underlying factors? Can they predict if the underlying factors have changed? Interestingly, in most cases humans can predict the effects of similar collisions with different conditions such as changes in mass, friction, etc. It is postulated this is primarily because we learn to model physics with meaningful latent variables. This does not imply we can estimate the precise values of these meaningful variables (estimate exact values of mass or friction). Inspired by this observation, we propose an interpretable intuitive physics model where specific dimensions in the bottleneck layers correspond to different physical properties. In order to demonstrate that our system models these underlying physical properties, we train our model on collisions of different shapes (cube, cone, cylinder, spheres etc.) and test on collisions of unseen combinations of shapes. Furthermore, we demonstrate our model generalizes well even when similar scenes are simulated with different underlying properties.



### Learning a Policy for Opportunistic Active Learning
- **Arxiv ID**: http://arxiv.org/abs/1808.10009v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1808.10009v1)
- **Published**: 2018-08-29 18:40:26+00:00
- **Updated**: 2018-08-29 18:40:26+00:00
- **Authors**: Aishwarya Padmakumar, Peter Stone, Raymond J. Mooney
- **Comment**: EMNLP 2018 Camera Ready
- **Journal**: EMNLP 2018
- **Summary**: Active learning identifies data points to label that are expected to be the most useful in improving a supervised model. Opportunistic active learning incorporates active learning into interactive tasks that constrain possible queries during interactions. Prior work has shown that opportunistic active learning can be used to improve grounding of natural language descriptions in an interactive object retrieval task. In this work, we use reinforcement learning for such an object retrieval task, to learn a policy that effectively trades off task completion with model improvement that would benefit future tasks.



### The Impact of Preprocessing on Deep Representations for Iris Recognition on Unconstrained Environments
- **Arxiv ID**: http://arxiv.org/abs/1808.10032v1
- **DOI**: 10.1109/SIBGRAPI.2018.00044
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.10032v1)
- **Published**: 2018-08-29 20:22:41+00:00
- **Updated**: 2018-08-29 20:22:41+00:00
- **Authors**: Luiz A. Zanlorensi, Eduardo Luz, Rayson Laroca, Alceu S. Britto Jr., Luiz S. Oliveira, David Menotti
- **Comment**: Accepted for presentation at the Conference on Graphics, Patterns and
  Images (SIBGRAPI) 2018
- **Journal**: None
- **Summary**: The use of iris as a biometric trait is widely used because of its high level of distinction and uniqueness. Nowadays, one of the major research challenges relies on the recognition of iris images obtained in visible spectrum under unconstrained environments. In this scenario, the acquired iris are affected by capture distance, rotation, blur, motion blur, low contrast and specular reflection, creating noises that disturb the iris recognition systems. Besides delineating the iris region, usually preprocessing techniques such as normalization and segmentation of noisy iris images are employed to minimize these problems. But these techniques inevitably run into some errors. In this context, we propose the use of deep representations, more specifically, architectures based on VGG and ResNet-50 networks, for dealing with the images using (and not) iris segmentation and normalization. We use transfer learning from the face domain and also propose a specific data augmentation technique for iris images. Our results show that the approach using non-normalized and only circle-delimited iris images reaches a new state of the art in the official protocol of the NICE.II competition, a subset of the UBIRIS database, one of the most challenging databases on unconstrained environments, reporting an average Equal Error Rate (EER) of 13.98% which represents an absolute reduction of about 5%.



### AAD: Adaptive Anomaly Detection through traffic surveillance videos
- **Arxiv ID**: http://arxiv.org/abs/1808.10044v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.10044v1)
- **Published**: 2018-08-29 21:12:57+00:00
- **Updated**: 2018-08-29 21:12:57+00:00
- **Authors**: Mohammmad Farhadi Bajestani, Seyed Soroush Heidari Rahmat Abadi, Seyed Mostafa Derakhshandeh Fard, Roozbeh Khodadadeh
- **Comment**: None
- **Journal**: None
- **Summary**: Anomaly detection through video analysis is of great importance to detect any anomalous vehicle/human behavior at a traffic intersection. While most existing works use neural networks and conventional machine learning methods based on provided dataset, we will use object recognition (Faster R-CNN) to identify objects labels and their corresponding location in the video scene as the first step to implement anomaly detection. Then, the optical flow will be utilized to identify adaptive traffic flows in each region of the frame. Basically, we propose an alternative method for unusual activity detection using an adaptive anomaly detection framework. Compared to the baseline method described in the reference paper, our method is more efficient and yields the comparable accuracy.



