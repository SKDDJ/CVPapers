# Arxiv Papers in cs.CV on 2018-08-10
### Distinctiveness, complexity, and repeatability of online signature templates
- **Arxiv ID**: http://arxiv.org/abs/1808.03399v1
- **DOI**: 10.1016/j.patcog.2018.07.024
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.03399v1)
- **Published**: 2018-08-10 03:01:45+00:00
- **Updated**: 2018-08-10 03:01:45+00:00
- **Authors**: NapaSae-Bae, NasirMemon, Pitikhate Sooraksa
- **Comment**: Preprint version of Pattern Recognition
- **Journal**: None
- **Summary**: This paper proposes three measures to quantify the characteristics of online signature templates in terms of distinctiveness, complexity and repeatability. A distinctiveness measure of a signature template is computed from a set of enrolled signature samples and a statistical assumption about random signatures. Secondly, a complexity measure of the template is derived from a set of enrolled signature samples. Finally, given a signature template, a measure to quantify the repeatability of the online signature is derived from a validation set of samples. These three measures can then be used as an indicator for the performance of the system in rejecting random forgery samples and skilled forgery samples and the performance of users in providing accepted genuine samples, respectively. The effectiveness of these three measures and their applications are demonstrated through experiments performed on three online signature datasets and one keystroke dynamics dataset using different verification algorithms.



### End-to-end Active Object Tracking and Its Real-world Deployment via Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/1808.03405v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.03405v2)
- **Published**: 2018-08-10 04:04:19+00:00
- **Updated**: 2019-02-12 09:20:10+00:00
- **Authors**: Wenhan Luo, Peng Sun, Fangwei Zhong, Wei Liu, Tong Zhang, Yizhou Wang
- **Comment**: To appear in Transactions on Pattern Analysis and Machine
  Intelligence. arXiv admin note: text overlap with arXiv:1705.10561
- **Journal**: None
- **Summary**: We study active object tracking, where a tracker takes visual observations (i.e., frame sequences) as input and produces the corresponding camera control signals as output (e.g., move forward, turn left, etc.). Conventional methods tackle tracking and camera control tasks separately, and the resulting system is difficult to tune jointly. These methods also require significant human efforts for image labeling and expensive trial-and-error system tuning in the real world. To address these issues, we propose, in this paper, an end-to-end solution via deep reinforcement learning. A ConvNet-LSTM function approximator is adopted for the direct frame-to-action prediction. We further propose an environment augmentation technique and a customized reward function, which are crucial for successful training. The tracker trained in simulators (ViZDoom and Unreal Engine) demonstrates good generalization behaviors in the case of unseen object moving paths, unseen object appearances, unseen backgrounds, and distracting objects. The system is robust and can restore tracking after occasional lost of the target being tracked. We also find that the tracking ability, obtained solely from simulators, can potentially transfer to real-world scenarios. We demonstrate successful examples of such transfer, via experiments over the VOT dataset and the deployment of a real-world robot using the proposed active tracker trained in simulation.



### CT Super-resolution GAN Constrained by the Identical, Residual, and Cycle Learning Ensemble(GAN-CIRCLE)
- **Arxiv ID**: http://arxiv.org/abs/1808.04256v3
- **DOI**: 10.1109/TMI.2019.2922960
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1808.04256v3)
- **Published**: 2018-08-10 05:33:23+00:00
- **Updated**: 2018-09-06 20:28:30+00:00
- **Authors**: Chenyu You, Guang Li, Yi Zhang, Xiaoliu Zhang, Hongming Shan, Shenghong Ju, Zhen Zhao, Zhuiyang Zhang, Wenxiang Cong, Michael W. Vannier, Punam K. Saha, Ge Wang
- **Comment**: None
- **Journal**: IEEE Transactions on Medical Imaging 2019
- **Summary**: Computed tomography (CT) is widely used in screening, diagnosis, and image-guided therapy for both clinical and research purposes. Since CT involves ionizing radiation, an overarching thrust of related technical research is development of novel methods enabling ultrahigh quality imaging with fine structural details while reducing the X-ray radiation. In this paper, we present a semi-supervised deep learning approach to accurately recover high-resolution (HR) CT images from low-resolution (LR) counterparts. Specifically, with the generative adversarial network (GAN) as the building block, we enforce the cycle-consistency in terms of the Wasserstein distance to establish a nonlinear end-to-end mapping from noisy LR input images to denoised and deblurred HR outputs. We also include the joint constraints in the loss function to facilitate structural preservation. In this deep imaging process, we incorporate deep convolutional neural network (CNN), residual learning, and network in network techniques for feature extraction and restoration. In contrast to the current trend of increasing network depth and complexity to boost the CT imaging performance, which limit its real-world applications by imposing considerable computational and memory overheads, we apply a parallel $1\times1$ CNN to compress the output of the hidden layer and optimize the number of layers and the number of filters for each convolutional layer. Quantitative and qualitative evaluations demonstrate that our proposed model is accurate, efficient and robust for super-resolution (SR) image restoration from noisy LR input images. In particular, we validate our composite SR networks on three large-scale CT datasets, and obtain promising results as compared to the other state-of-the-art methods.



### DeepWrinkles: Accurate and Realistic Clothing Modeling
- **Arxiv ID**: http://arxiv.org/abs/1808.03417v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.03417v1)
- **Published**: 2018-08-10 05:45:36+00:00
- **Updated**: 2018-08-10 05:45:36+00:00
- **Authors**: Zorah Laehner, Daniel Cremers, Tony Tung
- **Comment**: 18 pages, 12 figures, 15th European Conference on Computer Vision
  (ECCV) 2018, Oral Presentation
- **Journal**: None
- **Summary**: We present a novel method to generate accurate and realistic clothing deformation from real data capture. Previous methods for realistic cloth modeling mainly rely on intensive computation of physics-based simulation (with numerous heuristic parameters), while models reconstructed from visual observations typically suffer from lack of geometric details. Here, we propose an original framework consisting of two modules that work jointly to represent global shape deformation as well as surface details with high fidelity. Global shape deformations are recovered from a subspace model learned from 3D data of clothed people in motion, while high frequency details are added to normal maps created using a conditional Generative Adversarial Network whose architecture is designed to enforce realism and temporal consistency. This leads to unprecedented high-quality rendering of clothing deformation sequences, where fine wrinkles from (real) high resolution observations can be recovered. In addition, as the model is learned independently from body shape and pose, the framework is suitable for applications that require retargeting (e.g., body animation). Our experiments show original high quality results with a flexible model. We claim an entirely data-driven approach to realistic cloth wrinkle generation is possible.



### WonDerM: Skin Lesion Classification with Fine-tuned Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1808.03426v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.03426v3)
- **Published**: 2018-08-10 06:40:58+00:00
- **Updated**: 2019-05-10 05:18:12+00:00
- **Authors**: Yeong Chan Lee, Sang-Hyuk Jung, Hong-Hee Won
- **Comment**: 4 pages, 4 figures
- **Journal**: None
- **Summary**: As skin cancer is one of the most frequent cancers globally, accurate, non-invasive dermoscopy-based diagnosis becomes essential and promising. A task of the Part 3 of the ISIC Skin Image Analysis Challenge at MICCAI 2018 is to predict seven disease classes with skin lesion images, including melanoma (MEL), melanocytic nevus (NV), basal cell carcinoma (BCC), actinic keratosis / Bowen's disease (intraepithelial carcinoma) (AKIEC), benign keratosis (solar lentigo / seborrheic keratosis / lichen planus-like keratosis) (BKL), dermatofibroma (DF) and vascular lesion (VASC) as defined by the International Dermatology Society.   In this work, we design the WonDerM pipeline, that resamples the preprocessed skin lesion images, builds neural network architecture fine-tuned with segmentation task data (the Part 1), and uses an ensemble method to classify the seven skin diseases. Our model achieved an accuracy of 0.899 and 0.785 in the validation set and test set, respectively.



### Facial Action Unit Detection Using Attention and Relation Learning
- **Arxiv ID**: http://arxiv.org/abs/1808.03457v3
- **DOI**: 10.1109/TAFFC.2019.2948635
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.03457v3)
- **Published**: 2018-08-10 08:51:30+00:00
- **Updated**: 2019-10-17 07:36:31+00:00
- **Authors**: Zhiwen Shao, Zhilei Liu, Jianfei Cai, Yunsheng Wu, Lizhuang Ma
- **Comment**: This paper is accepted by IEEE Transactions on Affective Computing
- **Journal**: None
- **Summary**: Attention mechanism has recently attracted increasing attentions in the field of facial action unit (AU) detection. By finding the region of interest of each AU with the attention mechanism, AU-related local features can be captured. Most of the existing attention based AU detection works use prior knowledge to predefine fixed attentions or refine the predefined attentions within a small range, which limits their capacity to model various AUs. In this paper, we propose an end-to-end deep learning based attention and relation learning framework for AU detection with only AU labels, which has not been explored before. In particular, multi-scale features shared by each AU are learned firstly, and then both channel-wise and spatial attentions are adaptively learned to select and extract AU-related local features. Moreover, pixel-level relations for AUs are further captured to refine spatial attentions so as to extract more relevant local features. Without changing the network architecture, our framework can be easily extended for AU intensity estimation. Extensive experiments show that our framework (i) soundly outperforms the state-of-the-art methods for both AU detection and AU intensity estimation on the challenging BP4D, DISFA, FERA 2015 and BP4D+ benchmarks, (ii) can adaptively capture the correlated regions of each AU, and (iii) also works well under severe occlusions and large poses.



### Out of the Black Box: Properties of deep neural networks and their applications
- **Arxiv ID**: http://arxiv.org/abs/1808.04433v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1808.04433v1)
- **Published**: 2018-08-10 09:30:52+00:00
- **Updated**: 2018-08-10 09:30:52+00:00
- **Authors**: Nizar Ouarti, David Carmona
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks are powerful machine learning approaches that have exhibited excellent results on many classification tasks. However, they are considered as black boxes and some of their properties remain to be formalized. In the context of image recognition, it is still an arduous task to understand why an image is recognized or not. In this study, we formalize some properties shared by eight state-of-the-art deep neural networks in order to grasp the principles allowing a given deep neural network to classify an image. Our results, tested on these eight networks, show that an image can be sub-divided into several regions (patches) responding at different degrees of probability (local property). With the same patch, some locations in the image can answer two (or three) orders of magnitude higher than other locations (spatial property). Some locations are activators and others inhibitors (activation-inhibition property). The repetition of the same patch can increase (or decrease) the probability of recognition of an object (cumulative property). Furthermore, we propose a new approach called Deepception that exploits these properties to deceive a deep neural network. We obtain for the VGG-VDD-19 neural network a fooling ratio of 88\%. Thanks to our "Psychophysics" approach, no prior knowledge on the networks architectures is required.



### Deep Learning Based Speed Estimation for Constraining Strapdown Inertial Navigation on Smartphones
- **Arxiv ID**: http://arxiv.org/abs/1808.03485v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.03485v1)
- **Published**: 2018-08-10 11:03:58+00:00
- **Updated**: 2018-08-10 11:03:58+00:00
- **Authors**: Santiago Cortés, Arno Solin, Juho Kannala
- **Comment**: To appear in IEEE International Workshop on Machine Learning for
  Signal Processing (MLSP) 2018
- **Journal**: None
- **Summary**: Strapdown inertial navigation systems are sensitive to the quality of the data provided by the accelerometer and gyroscope. Low-grade IMUs in handheld smart-devices pose a problem for inertial odometry on these devices. We propose a scheme for constraining the inertial odometry problem by complementing non-linear state estimation by a CNN-based deep-learning model for inferring the momentary speed based on a window of IMU samples. We show the feasibility of the model using a wide range of data from an iPhone, and present proof-of-concept results for how the model can be combined with an inertial navigation system for three-dimensional inertial navigation.



### Road Segmentation Using CNN and Distributed LSTM
- **Arxiv ID**: http://arxiv.org/abs/1808.04450v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1808.04450v2)
- **Published**: 2018-08-10 12:35:35+00:00
- **Updated**: 2019-03-05 23:38:10+00:00
- **Authors**: Yecheng Lyu, Lin Bai, Xinming Huang
- **Comment**: 6 pages. arXiv admin note: text overlap with arXiv:1804.05164
- **Journal**: None
- **Summary**: In automated driving systems (ADS) and advanced driver-assistance systems (ADAS), an efficient road segmentation is necessary to perceive the drivable region and build an occupancy map for path planning. The existing algorithms implement gigantic convolutional neural networks (CNNs) that are computationally expensive and time consuming. In this paper, we introduced distributed LSTM, a neural network widely used in audio and video processing, to process rows and columns in images and feature maps. We then propose a new network combining the convolutional and distributed LSTM layers to solve the road segmentation problem. In the end, the network is trained and tested in KITTI road benchmark. The result shows that the combined structure enhances the feature extraction and processing but takes less processing time than pure CNN structure.



### Band selection with Higher Order Multivariate Cumulants for small target detection in hyperspectral images
- **Arxiv ID**: http://arxiv.org/abs/1808.03513v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.03513v1)
- **Published**: 2018-08-10 12:50:52+00:00
- **Updated**: 2018-08-10 12:50:52+00:00
- **Authors**: Przemysław Głomb, Krzysztof Domino, Michał Romaszewski, Michał Cholewa
- **Comment**: None
- **Journal**: None
- **Summary**: In the small target detection problem a pattern to be located is on the order of magnitude less numerous than other patterns present in the dataset. This applies both to the case of supervised detection, where the known template is expected to match in just a few areas and unsupervised anomaly detection, as anomalies are rare by definition. This problem is frequently related to the imaging applications, i.e. detection within the scene acquired by a camera. To maximize available data about the scene, hyperspectral cameras are used; at each pixel, they record spectral data in hundreds of narrow bands.   The typical feature of hyperspectral imaging is that characteristic properties of target materials are visible in the small number of bands, where light of certain wavelength interacts with characteristic molecules. A target-independent band selection method based on statistical principles is a versatile tool for solving this problem in different practical applications.   Combination of a regular background and a rare standing out anomaly will produce a distortion in the joint distribution of hyperspectral pixels. Higher Order Cumulants Tensors are a natural `window' into this distribution, allowing to measure properties and suggest candidate bands for removal. While there have been attempts at producing band selection algorithms based on the 3 rd cumulant's tensor i.e. the joint skewness, the literature lacks a systematic analysis of how the order of the cumulant tensor used affects effectiveness of band selection in detection applications. In this paper we present an analysis of a general algorithm for band selection based on higher order cumulants. We discuss its usability related to the observed breaking points in performance, depending both on method order and the desired number of bands. Finally we perform experiments and evaluate these methods in a hyperspectral detection scenario.



### Atmospheric turbulence mitigation for sequences with moving objects using recursive image fusion
- **Arxiv ID**: http://arxiv.org/abs/1808.03550v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.03550v1)
- **Published**: 2018-08-10 13:59:54+00:00
- **Updated**: 2018-08-10 13:59:54+00:00
- **Authors**: N. Anantrasirichai, Alin Achim, David Bull
- **Comment**: IEEE International Conference on Image Processing 2018
- **Journal**: None
- **Summary**: This paper describes a new method for mitigating the effects of atmospheric distortion on observed sequences that include large moving objects. In order to provide accurate detail from objects behind the distorting layer, we solve the space-variant distortion problem using recursive image fusion based on the Dual Tree Complex Wavelet Transform (DT-CWT). The moving objects are detected and tracked using the improved Gaussian mixture models (GMM) and Kalman filtering. New fusion rules are introduced which work on the magnitudes and angles of the DT-CWT coefficients independently to achieve a sharp image and to reduce atmospheric distortion, respectively. The subjective results show that the proposed method achieves better video quality than other existing methods with competitive speed.



### Physics-based Learned Design: Optimized Coded-Illumination for Quantitative Phase Imaging
- **Arxiv ID**: http://arxiv.org/abs/1808.03571v3
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1808.03571v3)
- **Published**: 2018-08-10 14:58:28+00:00
- **Updated**: 2019-02-06 04:09:12+00:00
- **Authors**: Michael R. Kellman, Emrah Bostan, Nicole Repina, Laura Waller
- **Comment**: 9 pages, 6 figures
- **Journal**: None
- **Summary**: Coded-illumination can enable quantitative phase microscopy of transparent samples with minimal hardware requirements. Intensity images are captured with different source patterns and a non-linear phase retrieval optimization reconstructs the image. The non-linear nature of the processing makes optimizing the illumination pattern designs complicated. Traditional techniques for experimental design (e.g. condition number optimization, spectral analysis) consider only linear measurement formation models and linear reconstructions. Deep neural networks (DNNs) can efficiently represent the non-linear process and can be optimized over via training in an end-to-end framework. However, DNNs typically require a large amount of training examples and parameters to properly learn the phase retrieval process, without making use of the known physical models. Here, we aim to use both our knowledge of the physics and the power of machine learning together. We develop a new data-driven approach to optimizing coded-illumination patterns for a LED array microscope for a given phase reconstruction algorithm. Our method incorporates both the physics of the measurement scheme and the non-linearity of the reconstruction algorithm into the design problem. This enables efficient parameterization, which allows us to use only a small number of training examples to learn designs that generalize well in the experimental setting without retraining. We show experimental results for both a well-characterized phase target and mouse fibroblast cells using coded-illumination patterns optimized for a sparsity-based phase reconstruction algorithm. Our learned design results using 2 measurements demonstrate similar accuracy to Fourier Ptychography with 69 measurements.



### Weakly- and Semi-Supervised Panoptic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1808.03575v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.03575v3)
- **Published**: 2018-08-10 15:04:14+00:00
- **Updated**: 2019-01-13 00:57:49+00:00
- **Authors**: Qizhu Li, Anurag Arnab, Philip H. S. Torr
- **Comment**: ECCV 2018. The first two authors contributed equally
- **Journal**: None
- **Summary**: We present a weakly supervised model that jointly performs both semantic- and instance-segmentation -- a particularly relevant problem given the substantial cost of obtaining pixel-perfect annotation for these tasks. In contrast to many popular instance segmentation approaches based on object detectors, our method does not predict any overlapping instances. Moreover, we are able to segment both "thing" and "stuff" classes, and thus explain all the pixels in the image. "Thing" classes are weakly-supervised with bounding boxes, and "stuff" with image-level tags. We obtain state-of-the-art results on Pascal VOC, for both full and weak supervision (which achieves about 95% of fully-supervised performance). Furthermore, we present the first weakly-supervised results on Cityscapes for both semantic- and instance-segmentation. Finally, we use our weakly supervised framework to analyse the relationship between annotation quality and predictive performance, which is of interest to dataset creators.



### Dropout is a special case of the stochastic delta rule: faster and more accurate deep learning
- **Arxiv ID**: http://arxiv.org/abs/1808.03578v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1808.03578v2)
- **Published**: 2018-08-10 15:06:05+00:00
- **Updated**: 2019-02-07 20:58:09+00:00
- **Authors**: Noah Frazier-Logue, Stephen José Hanson
- **Comment**: 6 pages, 7 figures; submitted to ICML
- **Journal**: None
- **Summary**: Multi-layer neural networks have lead to remarkable performance on many kinds of benchmark tasks in text, speech and image processing. Nonlinear parameter estimation in hierarchical models is known to be subject to overfitting and misspecification. One approach to these estimation and related problems (local minima, colinearity, feature discovery etc.) is called Dropout (Hinton, et al 2012, Baldi et al 2016). The Dropout algorithm removes hidden units according to a Bernoulli random variable with probability $p$ prior to each update, creating random "shocks" to the network that are averaged over updates. In this paper we will show that Dropout is a special case of a more general model published originally in 1990 called the Stochastic Delta Rule, or SDR (Hanson, 1990). SDR redefines each weight in the network as a random variable with mean $\mu_{w_{ij}}$ and standard deviation $\sigma_{w_{ij}}$. Each weight random variable is sampled on each forward activation, consequently creating an exponential number of potential networks with shared weights. Both parameters are updated according to prediction error, thus resulting in weight noise injections that reflect a local history of prediction error and local model averaging. SDR therefore implements a more sensitive local gradient-dependent simulated annealing per weight converging in the limit to a Bayes optimal network. Tests on standard benchmarks (CIFAR) using a modified version of DenseNet shows the SDR outperforms standard Dropout in test error by approx. $17\%$ with DenseNet-BC 250 on CIFAR-100 and approx. $12-14\%$ in smaller networks. We also show that SDR reaches the same accuracy that Dropout attains in 100 epochs in as few as 35 epochs.



### Image Registration and Predictive Modeling: Learning the Metric on the Space of Diffeomorphisms
- **Arxiv ID**: http://arxiv.org/abs/1808.04439v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1808.04439v1)
- **Published**: 2018-08-10 15:25:10+00:00
- **Updated**: 2018-08-10 15:25:10+00:00
- **Authors**: Ayagoz Mussabayeva, Alexey Kroshnin, Anvar Kurmukov, Yulia Dodonova, Li Shen, Shan Cong, Lei Wang, Boris A. Gutman
- **Comment**: Accepted to ShapeMI workshop
- **Journal**: None
- **Summary**: We present a method for metric optimization in the Large Deformation Diffeomorphic Metric Mapping (LDDMM) framework, by treating the induced Riemannian metric on the space of diffeomorphisms as a kernel in a machine learning context. For simplicity, we choose the kernel Fischer Linear Discriminant Analysis (KLDA) as the framework. Optimizing the kernel parameters in an Expectation-Maximization framework, we define model fidelity via the hinge loss of the decision function. The resulting algorithm optimizes the parameters of the LDDMM norm-inducing differential operator as a solution to a group-wise registration and classification problem. In practice, this may lead to a biology-aware registration, focusing its attention on the predictive task at hand such as identifying the effects of disease. We first tested our algorithm on a synthetic dataset, showing that our parameter selection improves registration quality and classification accuracy. We then tested the algorithm on 3D subcortical shapes from the Schizophrenia cohort Schizconnect. Our Schizpohrenia-Control predictive model showed significant improvement in ROC AUC compared to baseline parameters.



### Weakly supervised learning of indoor geometry by dual warping
- **Arxiv ID**: http://arxiv.org/abs/1808.03609v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.03609v1)
- **Published**: 2018-08-10 16:31:51+00:00
- **Updated**: 2018-08-10 16:31:51+00:00
- **Authors**: Pulak Purkait, Ujwal Bonde, Christopher Zach
- **Comment**: 3DV 2018, to appear, International Conference on 3D Vision 2018
- **Journal**: None
- **Summary**: A major element of depth perception and 3D understanding is the ability to predict the 3D layout of a scene and its contained objects for a novel pose. Indoor environments are particularly suitable for novel view prediction, since the set of objects in such environments is relatively restricted. In this work we address the task of 3D prediction especially for indoor scenes by leveraging only weak supervision. In the literature 3D scene prediction is usually solved via a 3D voxel grid. However, such methods are limited to estimating rather coarse 3D voxel grids, since predicting entire voxel spaces has large computational costs. Hence, our method operates in image-space rather than in voxel space, and the task of 3D estimation essentially becomes a depth image completion problem. We propose a novel approach to easily generate training data containing depth maps with realistic occlusions, and subsequently train a network for completing those occluded regions. Using multiple publicly available dataset~\cite{song2017semantic,Silberman:ECCV12} we benchmark our method against existing approaches and are able to obtain superior performance. We further demonstrate the flexibility of our method by presenting results for new view synthesis of RGB-D images.



### Partial Adversarial Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1808.04205v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.04205v1)
- **Published**: 2018-08-10 17:18:33+00:00
- **Updated**: 2018-08-10 17:18:33+00:00
- **Authors**: Zhangjie Cao, Lijia Ma, Mingsheng Long, Jianmin Wang
- **Comment**: 14 pages, ECCV 2018 poster. arXiv admin note: text overlap with
  arXiv:1707.07901
- **Journal**: None
- **Summary**: Domain adversarial learning aligns the feature distributions across the source and target domains in a two-player minimax game. Existing domain adversarial networks generally assume identical label space across different domains. In the presence of big data, there is strong motivation of transferring deep models from existing big domains to unknown small domains. This paper introduces partial domain adaptation as a new domain adaptation scenario, which relaxes the fully shared label space assumption to that the source label space subsumes the target label space. Previous methods typically match the whole source domain to the target domain, which are vulnerable to negative transfer for the partial domain adaptation problem due to the large mismatch between label spaces. We present Partial Adversarial Domain Adaptation (PADA), which simultaneously alleviates negative transfer by down-weighing the data of outlier source classes for training both source classifier and domain adversary, and promotes positive transfer by matching the feature distributions in the shared label space. Experiments show that PADA exceeds state-of-the-art results for partial domain adaptation tasks on several datasets.



### Community Regularization of Visually-Grounded Dialog
- **Arxiv ID**: http://arxiv.org/abs/1808.04359v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.MA
- **Links**: [PDF](http://arxiv.org/pdf/1808.04359v2)
- **Published**: 2018-08-10 22:09:43+00:00
- **Updated**: 2018-09-06 20:01:29+00:00
- **Authors**: Akshat Agarwal, Swaminathan Gurumurthy, Vasu Sharma, Mike Lewis, Katia Sycara
- **Comment**: 7 pages, ICML/AAMAS Adaptive Learning Agents Workshop 2018 and CVPR
  Visual Dialog Workshop 2018. Code available at
  https://github.com/agakshat/visualdialog-pytorch
- **Journal**: None
- **Summary**: The task of conducting visually grounded dialog involves learning goal-oriented cooperative dialog between autonomous agents who exchange information about a scene through several rounds of questions and answers in natural language. We posit that requiring artificial agents to adhere to the rules of human language, while also requiring them to maximize information exchange through dialog is an ill-posed problem. We observe that humans do not stray from a common language because they are social creatures who live in communities, and have to communicate with many people everyday, so it is far easier to stick to a common language even at the cost of some efficiency loss. Using this as inspiration, we propose and evaluate a multi-agent community-based dialog framework where each agent interacts with, and learns from, multiple agents, and show that this community-enforced regularization results in more relevant and coherent dialog (as judged by human evaluators) without sacrificing task performance (as judged by quantitative metrics).



