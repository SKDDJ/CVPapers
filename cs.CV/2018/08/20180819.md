# Arxiv Papers in cs.CV on 2018-08-19
### Deep Multiple Instance Learning for Airplane Detection in High Resolution Imagery
- **Arxiv ID**: http://arxiv.org/abs/1808.06178v2
- **DOI**: 10.1007/s00138-020-01153-7
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.06178v2)
- **Published**: 2018-08-19 07:36:22+00:00
- **Updated**: 2020-10-28 04:40:36+00:00
- **Authors**: Mohammad Reza Mohammadi
- **Comment**: 12 pages, 12 figures, 5 tables
- **Journal**: None
- **Summary**: Automatic airplane detection in aerial imagery has a variety of applications. Two of the significant challenges in this task are variations in the scale and direction of the airplanes. To solve these challenges, we present a rotation-and-scale invariant airplane proposal generator. We call this generator symmetric line segments (SLS) that is developed based on the symmetric and regular boundaries of airplanes from the top view. Then, the generated proposals are used to train a deep convolutional neural network for removing non-airplane proposals. Since each airplane can have multiple SLS proposals, where some of them are not in the direction of the fuselage, we collect all proposals corresponding to one ground-truth as a positive bag and the others as the negative instances. To have multiple instance deep learning, we modify the loss function of the network to learn from each positive bag at least one instance as well as all negative instances. Finally, we employ non-maximum suppression to remove duplicate detections. Our experiments on NWPU VHR-10 and DOTA datasets show that our method is a promising approach for automatic airplane detection in very high-resolution images. Moreover, we estimate the direction of the airplanes using box-level annotations as an extra achievement.



### Fast and Robust Matching for Multimodal Remote Sensing Image Registration
- **Arxiv ID**: http://arxiv.org/abs/1808.06194v8
- **DOI**: 10.1109/TGRS.2019.2924684
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.06194v8)
- **Published**: 2018-08-19 10:19:06+00:00
- **Updated**: 2021-03-31 08:28:33+00:00
- **Authors**: Yuanxin Ye, Lorenzo Bruzzone, Jie Shan, Francesca Bovolo, Qing Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: While image registration has been studied in remote sensing community for decades, registering multimodal data [e.g., optical, LiDAR, SAR, and map] remains a challenging problem because of significant nonlinear intensity differences between such data. To address this problem, this paper presents a fast and robust matching framework integrating local descriptors for multimodal registration. In the proposed framework, a local descriptor, such as Histogram of Oriented Gradient (HOG), Local Self Similarity (LSS), or Speeded-Up Robust Feature (SURF), is first extracted at each pixel to form a pixel-wise feature representation of an image. Then we define a similarity measure based on the feature representation in frequency domain using the 3 Dimensional Fast Fourier Transform (3DFFT) technique, followed by a template matching scheme to detect control points between images. In this procedure, we also propose a novel pixel-wise feature representation using orientated gradients of images, which is named channel features of orientated gradients (CFOG). This novel feature is an extension of the pixel-wise HOG descriptors, and outperforms that both in matching performance and computational efficiency. The major advantage of the proposed framework includes: (1) structural similarity representation using the pixel-wise feature description and (2) high computational efficiency due to the use of 3DFFT. Experimental results on different types of multimodal images show the superior matching performance of the proposed framework than the state-of-the-art methods.The proposed matching framework have been used in the software products of a Chinese listed company. The matlab code is available in this manuscript.



### Segmentation of Microscopy Data for finding Nuclei in Divergent Images
- **Arxiv ID**: http://arxiv.org/abs/1808.06914v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1808.06914v2)
- **Published**: 2018-08-19 11:01:48+00:00
- **Updated**: 2018-08-23 01:45:09+00:00
- **Authors**: Shivam Singh, Stuti Pathak
- **Comment**: 7 pages, 7 figures, 1 table. arXiv admin note: text overlap with
  arXiv:1807.04459, arXiv:1802.10548, arXiv:1807.10165 by other authors
- **Journal**: None
- **Summary**: Every year millions of people die due to disease of Cancer. Due to its invasive nature it is very complex to cure even in primary stages. Hence, only method to survive this disease completely is via forecasting by analyzing the early mutation in cells of the patient biopsy. Cell Segmentation can be used to find cell which have left their nuclei. This enables faster cure and high rate of survival. Cell counting is a hard, yet tedious task that would greatly benefit from automation. To accomplish this task, segmentation of cells need to be accurate. In this paper, we have improved the learning of training data by our network. It can annotate precise masks on test data. we examine the strength of activation functions in medical image segmentation task by improving learning rates by our proposed Carving Technique. Identifying the cells nuclei is the starting point for most analyses, identifying nuclei allows researchers to identify each individual cell in a sample, and by measuring how cells react to various treatments, the researcher can understand the underlying biological processes at work. Experimental results shows the efficiency of the proposed work.



### Haze Density Estimation via Modeling of Scattering Coefficients of Iso-depth Regions
- **Arxiv ID**: http://arxiv.org/abs/1808.06207v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.06207v1)
- **Published**: 2018-08-19 13:17:51+00:00
- **Updated**: 2018-08-19 13:17:51+00:00
- **Authors**: Jie Chen, Cheen-Hau Tan, Lap-Pui Chau
- **Comment**: None
- **Journal**: None
- **Summary**: Vision based haze density estimation is of practical implications for the purpose of precaution alarm and emergency reactions toward disastrous hazy weathers. In this paper, we introduce a haze density estimation framework based on modeling of scattering coefficients of iso-depth regions. A haze density metric of Normalized Scattering Coefficient (NSC) is proposed to measure current haze density level with reference to two reference scales. Iso-depth regions are determined via superpixel segmentation. Efficient searching and matching of iso-depth units could be carried out for measurements via unstationary cameras. A robust dark SP selection method is used to produce reliable predictions for most out-door scenarios.



### GridFace: Face Rectification via Learning Local Homography Transformations
- **Arxiv ID**: http://arxiv.org/abs/1808.06210v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.06210v1)
- **Published**: 2018-08-19 13:37:11+00:00
- **Updated**: 2018-08-19 13:37:11+00:00
- **Authors**: Erjin Zhou, Zhimin Cao, Jian Sun
- **Comment**: To appear in ECCV 2018
- **Journal**: None
- **Summary**: In this paper, we propose a method, called GridFace, to reduce facial geometric variations and improve the recognition performance. Our method rectifies the face by local homography transformations, which are estimated by a face rectification network. To encourage the image generation with canonical views, we apply a regularization based on the natural face distribution. We learn the rectification network and recognition network in an end-to-end manner. Extensive experiments show our method greatly reduces geometric variations, and gains significant improvements in unconstrained face recognition scenarios.



### Jointly Deep Multi-View Learning for Clustering Analysis
- **Arxiv ID**: http://arxiv.org/abs/1808.06220v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.06220v2)
- **Published**: 2018-08-19 15:17:34+00:00
- **Updated**: 2018-11-23 09:09:28+00:00
- **Authors**: Bingqian Lin, Yuan Xie, Yanyun Qu, Cuihua Li, Xiaodan Liang
- **Comment**: 10 pages, 4 figures
- **Journal**: None
- **Summary**: In this paper, we propose a novel Joint framework for Deep Multi-view Clustering (DMJC), where multiple deep embedded features, multi-view fusion mechanism and clustering assignments can be learned simultaneously. Our key idea is that the joint learning strategy can sufficiently exploit clustering-friendly multi-view features and useful multi-view complementary information to improve the clustering performance. How to realize the multi-view fusion in such a joint framework is the primary challenge. To do so, we design two ingenious variants of deep multi-view joint clustering models under the proposed framework, where multi-view fusion is implemented by two different schemes. The first model, called DMJC-S, performs multi-view fusion in an implicit way via a novel multi-view soft assignment distribution. The second model, termed DMJC-T, defines a novel multi-view auxiliary target distribution to conduct the multi-view fusion explicitly. Both DMJC-S and DMJC-T are optimized under a KL divergence like clustering objective. Experiments on six challenging image datasets demonstrate the superiority of both DMJC-S and DMJC-T over single/multi-view baselines and the state-of-the-art multiview clustering methods, which proves the effectiveness of the proposed DMJC framework. To our best knowledge, this is the first work to model the multi-view clustering in a deep joint framework, which will provide a meaningful thinking in unsupervised multi-view learning.



### Dynamic Temporal Alignment of Speech to Lips
- **Arxiv ID**: http://arxiv.org/abs/1808.06250v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.06250v1)
- **Published**: 2018-08-19 19:58:05+00:00
- **Updated**: 2018-08-19 19:58:05+00:00
- **Authors**: Tavi Halperin, Ariel Ephrat, Shmuel Peleg
- **Comment**: None
- **Journal**: None
- **Summary**: Many speech segments in movies are re-recorded in a studio during postproduction, to compensate for poor sound quality as recorded on location. Manual alignment of the newly-recorded speech with the original lip movements is a tedious task. We present an audio-to-video alignment method for automating speech to lips alignment, stretching and compressing the audio signal to match the lip movements. This alignment is based on deep audio-visual features, mapping the lips video and the speech signal to a shared representation. Using this shared representation we compute the lip-sync error between every short speech period and every video frame, followed by the determination of the optimal corresponding frame for each short sound period over the entire video clip. We demonstrate successful alignment both quantitatively, using a human perception-inspired metric, as well as qualitatively. The strongest advantage of our audio-to-video approach is in cases where the original voice in unclear, and where a constant shift of the sound can not give a perfect alignment. In these cases state-of-the-art methods will fail.



### Predictive Image Regression for Longitudinal Studies with Missing Data
- **Arxiv ID**: http://arxiv.org/abs/1808.07553v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.07553v1)
- **Published**: 2018-08-19 20:31:54+00:00
- **Updated**: 2018-08-19 20:31:54+00:00
- **Authors**: Sharmin Pathan, Yi Hong
- **Comment**: 1st Conference on Medical Imaging with Deep Learning (MIDL 2018),
  Amsterdam, The Netherlands
- **Journal**: None
- **Summary**: In this paper, we propose a predictive regression model for longitudinal images with missing data based on large deformation diffeomorphic metric mapping (LDDMM) and deep neural networks. Instead of directly predicting image scans, our model predicts a vector momentum sequence associated with a baseline image. This momentum sequence parameterizes the original image sequence in the LDDMM framework and lies in the tangent space of the baseline image, which is Euclidean. A recurrent network with long term-short memory (LSTM) units encodes the time-varying changes in the vector-momentum sequence, and a convolutional neural network (CNN) encodes the baseline image of the vector momenta. Features extracted by the LSTM and CNN are fed into a decoder network to reconstruct the vector momentum sequence, which is used for the image sequence prediction by deforming the baseline image with LDDMM shooting. To handle the missing images at some time points, we adopt a binary mask to ignore their reconstructions in the loss calculation. We evaluate our model on synthetically generated images and the brain MRIs from the OASIS dataset. Experimental results demonstrate the promising predictions of the spatiotemporal changes in both datasets, irrespective of large or subtle changes in longitudinal image sequences.



### Eliminating the Blind Spot: Adapting 3D Object Detection and Monocular Depth Estimation to 360° Panoramic Imagery
- **Arxiv ID**: http://arxiv.org/abs/1808.06253v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.06253v1)
- **Published**: 2018-08-19 20:38:08+00:00
- **Updated**: 2018-08-19 20:38:08+00:00
- **Authors**: Grégoire Payen de La Garanderie, Amir Atapour Abarghouei, Toby P. Breckon
- **Comment**: This work is accepted in ECCV 2018
- **Journal**: None
- **Summary**: Recent automotive vision work has focused almost exclusively on processing forward-facing cameras. However, future autonomous vehicles will not be viable without a more comprehensive surround sensing, akin to a human driver, as can be provided by 360{\deg} panoramic cameras. We present an approach to adapt contemporary deep network architectures developed on conventional rectilinear imagery to work on equirectangular 360{\deg} panoramic imagery. To address the lack of annotated panoramic automotive datasets availability, we adapt a contemporary automotive dataset, via style and projection transformations, to facilitate the cross-domain retraining of contemporary algorithms for panoramic imagery. Following this approach we retrain and adapt existing architectures to recover scene depth and 3D pose of vehicles from monocular panoramic imagery without any panoramic training labels or calibration parameters. Our approach is evaluated qualitatively on crowd-sourced panoramic images and quantitatively using an automotive environment simulator to provide the first benchmark for such techniques within panoramic imagery.



### Deep Mask For X-ray Based Heart Disease Classification
- **Arxiv ID**: http://arxiv.org/abs/1808.08277v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.08277v2)
- **Published**: 2018-08-19 21:41:45+00:00
- **Updated**: 2020-07-01 04:05:57+00:00
- **Authors**: Xupeng Chen, Binbin Shi
- **Comment**: outdated work
- **Journal**: None
- **Summary**: We build a deep learning model to detect and classify heart disease using $X-ray$. We collect data from several hospitals and public datasets. After preprocess we get 3026 images including disease type VSD, ASD, TOF and normal control. The main problem we have to solve is to enable the network to accurately learn the characteristics of the heart, to ensure the reliability of the network while increasing accuracy. By learning the doctor's diagnostic experience, labeling the image and using tools to extract masks of heart region, we train a U-net to generate a mask to give more attention. It forces the model to focus on the characteristics of the heart region and obtain more reliable results.



