# Arxiv Papers in cs.CV on 2018-05-29
### Automatic Exposure Compensation for Multi-Exposure Image Fusion
- **Arxiv ID**: http://arxiv.org/abs/1805.11211v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.11211v1)
- **Published**: 2018-05-29 01:29:25+00:00
- **Updated**: 2018-05-29 01:29:25+00:00
- **Authors**: Yuma Kinoshita, Sayaka Shiota, Hitoshi Kiya
- **Comment**: To appear in Proc. ICIP2018 October 07-10, 2018, Athens, Greece
- **Journal**: None
- **Summary**: This paper proposes a novel luminance adjustment method based on automatic exposure compensation for multi-exposure image fusion. Multi-exposure image fusion is a method to produce images without saturation regions, by using photos with different exposures. In conventional works, it has been pointed out that the quality of those multi-exposure images can be improved by adjusting the luminance of them. However, how to determine the degree of adjustment has never been discussed. This paper therefore proposes a way to automatically determines the degree on the basis of the luminance distribution of input multi-exposure images. Moreover, new weights, called "simple weights", for image fusion are also considered for the proposed luminance adjustment method. Experimental results show that the multi-exposure images adjusted by the proposed method have better quality than the input multi-exposure ones in terms of the well-exposedness. It is also confirmed that the proposed simple weights provide the highest score of statistical naturalness and discrete entropy in all fusion methods.



### Non-rigid Reconstruction with a Single Moving RGB-D Camera
- **Arxiv ID**: http://arxiv.org/abs/1805.11219v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.11219v2)
- **Published**: 2018-05-29 02:23:30+00:00
- **Updated**: 2018-05-30 02:22:09+00:00
- **Authors**: Shafeeq Elanattil, Peyman Moghadam, Sridha Sridharan, Clinton Fookes, Mark Cox
- **Comment**: Accepted in International Conference on Pattern Recognition (ICPR
  2018)
- **Journal**: None
- **Summary**: We present a novel non-rigid reconstruction method using a moving RGB-D camera. Current approaches use only non-rigid part of the scene and completely ignore the rigid background. Non-rigid parts often lack sufficient geometric and photometric information for tracking large frame-to-frame motion. Our approach uses camera pose estimated from the rigid background for foreground tracking. This enables robust foreground tracking in situations where large frame-to-frame motion occurs. Moreover, we are proposing a multi-scale deformation graph which improves non-rigid tracking without compromising the quality of the reconstruction. We are also contributing a synthetic dataset which is made publically available for evaluating non-rigid reconstruction methods. The dataset provides frame-by-frame ground truth geometry of the scene, the camera trajectory, and masks for background foreground. Experimental results show that our approach is more robust in handling larger frame-to-frame motions and provides better reconstruction compared to state-of-the-art approaches.



### Video Anomaly Detection and Localization via Gaussian Mixture Fully Convolutional Variational Autoencoder
- **Arxiv ID**: http://arxiv.org/abs/1805.11223v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.11223v1)
- **Published**: 2018-05-29 02:37:19+00:00
- **Updated**: 2018-05-29 02:37:19+00:00
- **Authors**: Yaxiang Fan, Gongjian Wen, Deren Li, Shaohua Qiu, Martin D. Levine
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel end-to-end partially supervised deep learning approach for video anomaly detection and localization using only normal samples. The insight that motivates this study is that the normal samples can be associated with at least one Gaussian component of a Gaussian Mixture Model (GMM), while anomalies either do not belong to any Gaussian component. The method is based on Gaussian Mixture Variational Autoencoder, which can learn feature representations of the normal samples as a Gaussian Mixture Model trained using deep learning. A Fully Convolutional Network (FCN) that does not contain a fully-connected layer is employed for the encoder-decoder structure to preserve relative spatial coordinates between the input image and the output feature map. Based on the joint probabilities of each of the Gaussian mixture components, we introduce a sample energy based method to score the anomaly of image test patches. A two-stream network framework is employed to combine the appearance and motion anomalies, using RGB frames for the former and dynamic flow images, for the latter. We test our approach on two popular benchmarks (UCSD Dataset and Avenue Dataset). The experimental results verify the superiority of our method compared to the state of the arts.



### Getting to Know Low-light Images with The Exclusively Dark Dataset
- **Arxiv ID**: http://arxiv.org/abs/1805.11227v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.11227v1)
- **Published**: 2018-05-29 02:59:41+00:00
- **Updated**: 2018-05-29 02:59:41+00:00
- **Authors**: Yuen Peng Loh, Chee Seng Chan
- **Comment**: Exclusively Dark (ExDARK) dataset is a collection of 7,363 low-light
  images from very low-light environments to twilight (i.e 10 different
  conditions), and 12 object classes (as to PASCAL VOC) annotated on both image
  class level and local object bounding boxes. 16 pages, 13 figures, submitted
  to CVIU
- **Journal**: None
- **Summary**: Low-light is an inescapable element of our daily surroundings that greatly affects the efficiency of our vision. Research works on low-light has seen a steady growth, particularly in the field of image enhancement, but there is still a lack of a go-to database as benchmark. Besides, research fields that may assist us in low-light environments, such as object detection, has glossed over this aspect even though breakthroughs-after-breakthroughs had been achieved in recent years, most noticeably from the lack of low-light data (less than 2% of the total images) in successful public benchmark dataset such as PASCAL VOC, ImageNet, and Microsoft COCO. Thus, we propose the Exclusively Dark dataset to elevate this data drought, consisting exclusively of ten different types of low-light images (i.e. low, ambient, object, single, weak, strong, screen, window, shadow and twilight) captured in visible light only with image and object level annotations. Moreover, we share insightful findings in regards to the effects of low-light on the object detection task by analyzing visualizations of both hand-crafted and learned features. Most importantly, we found that the effects of low-light reaches far deeper into the features than can be solved by simple "illumination invariance'". It is our hope that this analysis and the Exclusively Dark dataset can encourage the growth in low-light domain researches on different fields. The Exclusively Dark dataset with its annotation is available at https://github.com/cs-chan/Exclusively-Dark-Image-Dataset



### Microscopy Cell Segmentation via Convolutional LSTM Networks
- **Arxiv ID**: http://arxiv.org/abs/1805.11247v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.11247v2)
- **Published**: 2018-05-29 05:21:36+00:00
- **Updated**: 2019-01-06 19:19:52+00:00
- **Authors**: Assaf Arbelle, Tammy Riklin Raviv
- **Comment**: Accepted to ISBI 2019
- **Journal**: None
- **Summary**: Live cell microscopy sequences exhibit complex spatial structures and complicated temporal behaviour, making their analysis a challenging task. Considering cell segmentation problem, which plays a significant role in the analysis, the spatial properties of the data can be captured using Convolutional Neural Networks (CNNs). Recent approaches show promising segmentation results using convolutional encoder-decoders such as the U-Net. Nevertheless, these methods are limited by their inability to incorporate temporal information, that can facilitate segmentation of individual touching cells or of cells that are partially visible. In order to exploit cell dynamics we propose a novel segmentation architecture which integrates Convolutional Long Short Term Memory (C-LSTM) with the U-Net. The network's unique architecture allows it to capture multi-scale, compact, spatio-temporal encoding in the C-LSTMs memory units. The method was evaluated on the Cell Tracking Challenge and achieved state-of-the-art results (1st on Fluo-N2DH-SIM+ and 2nd on DIC-C2DL-HeLa datasets) The code is freely available at: https://github.com/arbellea/LSTM-UNet.git



### Improved Mixed-Example Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/1805.11272v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1805.11272v4)
- **Published**: 2018-05-29 07:06:58+00:00
- **Updated**: 2019-01-19 07:04:35+00:00
- **Authors**: Cecilia Summers, Michael J. Dinneen
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: In order to reduce overfitting, neural networks are typically trained with data augmentation, the practice of artificially generating additional training data via label-preserving transformations of existing training examples. While these types of transformations make intuitive sense, recent work has demonstrated that even non-label-preserving data augmentation can be surprisingly effective, examining this type of data augmentation through linear combinations of pairs of examples. Despite their effectiveness, little is known about why such methods work. In this work, we aim to explore a new, more generalized form of this type of data augmentation in order to determine whether such linearity is necessary. By considering this broader scope of "mixed-example data augmentation", we find a much larger space of practical augmentation techniques, including methods that improve upon previous state-of-the-art. This generalization has benefits beyond the promise of improved performance, revealing a number of types of mixed-example data augmentation that are radically different from those considered in prior work, which provides evidence that current theories for the effectiveness of such methods are incomplete and suggests that any such theory must explain a much broader phenomenon. Code is available at https://github.com/ceciliaresearch/MixedExample.



### Learning Data Augmentation for Brain Tumor Segmentation with Coarse-to-Fine Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1805.11291v2
- **DOI**: 10.1007/978-3-030-11723-8_7
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.11291v2)
- **Published**: 2018-05-29 08:17:13+00:00
- **Updated**: 2018-08-31 07:15:12+00:00
- **Authors**: Tony C. W Mok, Albert C. S Chung
- **Comment**: None
- **Journal**: None
- **Summary**: There is a common belief that the successful training of deep neural networks requires many annotated training samples, which are often expensive and difficult to obtain especially in the biomedical imaging field. While it is often easy for researchers to use data augmentation to expand the size of training sets, constructing and generating generic augmented data that is able to teach the network the desired invariance and robustness properties using traditional data augmentation techniques is challenging in practice. In this paper, we propose a novel automatic data augmentation method that uses generative adversarial networks to learn augmentations that enable machine learning based method to learn the available annotated samples more efficiently. The architecture consists of a coarse-to-fine generator to capture the manifold of the training sets and generate generic augmented data. In our experiments, we show the efficacy of our approach on a Magnetic Resonance Imaging (MRI) image, achieving improvements of 3.5% Dice coefficient on the BRATS15 Challenge dataset as compared to traditional augmentation approaches. Also, our proposed method successfully boosts a common segmentation network to reach the state-of-the-art performance on the BRATS15 Challenge.



### CNN-Based Detection of Generic Constrast Adjustment with JPEG Post-processing
- **Arxiv ID**: http://arxiv.org/abs/1805.11318v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1805.11318v1)
- **Published**: 2018-05-29 09:15:26+00:00
- **Updated**: 2018-05-29 09:15:26+00:00
- **Authors**: Mauro Barni, Andrea Costanzo, Ehsan Nowroozi, Benedetta Tondi
- **Comment**: To be presented at the 25th IEEE International Conference on Image
  Processing (ICIP 2018)
- **Journal**: None
- **Summary**: Detection of contrast adjustments in the presence of JPEG postprocessing is known to be a challenging task. JPEG post processing is often applied innocently, as JPEG is the most common image format, or it may correspond to a laundering attack, when it is purposely applied to erase the traces of manipulation. In this paper, we propose a CNN-based detector for generic contrast adjustment, which is robust to JPEG compression. The proposed system relies on a patch-based Convolutional Neural Network (CNN), trained to distinguish pristine images from contrast adjusted images, for some selected adjustment operators of different nature. Robustness to JPEG compression is achieved by training the CNN with JPEG examples, compressed over a range of Quality Factors (QFs). Experimental results show that the detector works very well and scales well with respect to the adjustment type, yielding very good performance under a large variety of unseen tonal adjustments.



### Lightweight Probabilistic Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/1805.11327v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.11327v1)
- **Published**: 2018-05-29 09:40:52+00:00
- **Updated**: 2018-05-29 09:40:52+00:00
- **Authors**: Jochen Gast, Stefan Roth
- **Comment**: To appear at CVPR 2018
- **Journal**: None
- **Summary**: Even though probabilistic treatments of neural networks have a long history, they have not found widespread use in practice. Sampling approaches are often too slow already for simple networks. The size of the inputs and the depth of typical CNN architectures in computer vision only compound this problem. Uncertainty in neural networks has thus been largely ignored in practice, despite the fact that it may provide important information about the reliability of predictions and the inner workings of the network. In this paper, we introduce two lightweight approaches to making supervised learning with probabilistic deep networks practical: First, we suggest probabilistic output layers for classification and regression that require only minimal changes to existing networks. Second, we employ assumed density filtering and show that activation uncertainties can be propagated in a practical fashion through the entire network, again with minor changes. Both probabilistic networks retain the predictive power of the deterministic counterpart, but yield uncertainties that correlate well with the empirical error induced by their predictions. Moreover, the robustness to adversarial examples is significantly increased.



### Pointly-Supervised Action Localization
- **Arxiv ID**: http://arxiv.org/abs/1805.11333v2
- **DOI**: 10.1007/s11263-018-1120-4
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.11333v2)
- **Published**: 2018-05-29 09:52:37+00:00
- **Updated**: 2018-10-01 12:38:33+00:00
- **Authors**: Pascal Mettes, Cees G. M. Snoek
- **Comment**: International Journal of Computer Vision, 2018
- **Journal**: None
- **Summary**: This paper strives for spatio-temporal localization of human actions in videos. In the literature, the consensus is to achieve localization by training on bounding box annotations provided for each frame of each training video. As annotating boxes in video is expensive, cumbersome and error-prone, we propose to bypass box-supervision. Instead, we introduce action localization based on point-supervision. We start from unsupervised spatio-temporal proposals, which provide a set of candidate regions in videos. While normally used exclusively for inference, we show spatio-temporal proposals can also be leveraged during training when guided by a sparse set of point annotations. We introduce an overlap measure between points and spatio-temporal proposals and incorporate them all into a new objective of a Multiple Instance Learning optimization. During inference, we introduce pseudo-points, visual cues from videos, that automatically guide the selection of spatio-temporal proposals. We outline five spatial and one temporal pseudo-point, as well as a measure to best leverage pseudo-points at test time. Experimental evaluation on three action localization datasets shows our pointly-supervised approach (i) is as effective as traditional box-supervision at a fraction of the annotation cost, (ii) is robust to sparse and noisy point annotations, (iii) benefits from pseudo-points during inference, and (iv) outperforms recent weakly-supervised alternatives. This leads us to conclude that points provide a viable alternative to boxes for action localization.



### Uncertainty Gated Network for Land Cover Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1805.11348v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.11348v1)
- **Published**: 2018-05-29 10:40:40+00:00
- **Updated**: 2018-05-29 10:40:40+00:00
- **Authors**: Guillem Pascual, Santi Seguí, Jordi Vitrià
- **Comment**: Accepted in CVPR18 workshop: "DeepGlobe: A Challenge for Parsing the
  Earth through Satellite Images"
- **Journal**: None
- **Summary**: The production of thematic maps depicting land cover is one of the most common applications of remote sensing. To this end, several semantic segmentation approaches, based on deep learning, have been proposed in the literature, but land cover segmentation is still considered an open problem due to some specific problems related to remote sensing imaging. In this paper we propose a novel approach to deal with the problem of modelling multiscale contexts surrounding pixels of different land cover categories. The approach leverages the computation of a heteroscedastic measure of uncertainty when classifying individual pixels in an image. This classification uncertainty measure is used to define a set of memory gates between layers that allow a principled method to select the optimal decision for each pixel.



### CocoNet: A deep neural network for mapping pixel coordinates to color values
- **Arxiv ID**: http://arxiv.org/abs/1805.11357v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.11357v3)
- **Published**: 2018-05-29 11:19:20+00:00
- **Updated**: 2018-11-22 12:39:33+00:00
- **Authors**: Paul Andrei Bricman, Radu Tudor Ionescu
- **Comment**: Accepted at the International Conference on Neural Information
  Processing 2018
- **Journal**: None
- **Summary**: In this paper, we propose a deep neural network approach for mapping the 2D pixel coordinates in an image to the corresponding Red-Green-Blue (RGB) color values. The neural network is termed CocoNet, i.e. coordinates-to-color network. During the training process, the neural network learns to encode the input image within its layers. More specifically, the network learns a continuous function that approximates the discrete RGB values sampled over the discrete 2D pixel locations. At test time, given a 2D pixel coordinate, the neural network will output the approximate RGB values of the corresponding pixel. By considering every 2D pixel location, the network can actually reconstruct the entire learned image. It is important to note that we have to train an individual neural network for each input image, i.e. one network encodes a single image only. To the best of our knowledge, we are the first to propose a neural approach for encoding images individually, by learning a mapping from the 2D pixel coordinate space to the RGB color space. Our neural image encoding approach has various low-level image processing applications ranging from image encoding, image compression and image denoising to image resampling and image completion. We conduct experiments that include both quantitative and qualitative results, demonstrating the utility of our approach and its superiority over standard baselines, e.g. bilateral filtering or bicubic interpolation. Our code is available at https://github.com/paubric/python-fuse-coconet.



### "How to rate a video game?" - A prediction system for video games based on multimodal information
- **Arxiv ID**: http://arxiv.org/abs/1805.11372v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.11372v1)
- **Published**: 2018-05-29 12:02:18+00:00
- **Updated**: 2018-05-29 12:02:18+00:00
- **Authors**: Vishal Batchu, Varshit Battu, Murali Krishna Reddy, Radhika Mamidi
- **Comment**: ICPRAI-18
- **Journal**: None
- **Summary**: Video games have become an integral part of most people's lives in recent times. This led to an abundance of data related to video games being shared online. However, this comes with issues such as incorrect ratings, reviews or anything that is being shared. Recommendation systems are powerful tools that help users by providing them with meaningful recommendations. A straightforward approach would be to predict the scores of video games based on other information related to the game. It could be used as a means to validate user-submitted ratings as well as provide recommendations. This work provides a method to predict the G-Score, that defines how good a video game is, from its trailer (video) and summary (text). We first propose models to predict the G-Score based on the trailer alone (unimodal). Later on, we show that considering information from multiple modalities helps the models perform better compared to using information from videos alone. Since we couldn't find any suitable multimodal video game dataset, we created our own dataset named VGD (Video Game Dataset) and provide it along with this work. The approach mentioned here can be generalized to other multimodal datasets such as movie trailers and summaries etc. Towards the end, we talk about the shortcomings of the work and some methods to overcome them.



### Webpage Saliency Prediction with Two-stage Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1805.11374v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.11374v1)
- **Published**: 2018-05-29 12:03:42+00:00
- **Updated**: 2018-05-29 12:03:42+00:00
- **Authors**: Yu Li, Ya Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Web page saliency prediction is a challenge problem in image transformation and computer vision. In this paper, we propose a new model combined with web page outline information to prediction people's interest region in web page. For each web page image, our model can generate the saliency map which indicates the region of interest for people. A two-stage generative adversarial networks are proposed and image outline information is introduced for better transferring. Experiment results on FIWI dataset show that our model have better performance in terms of saliency prediction.



### Robust Tumor Localization with Pyramid Grad-CAM
- **Arxiv ID**: http://arxiv.org/abs/1805.11393v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.11393v1)
- **Published**: 2018-05-29 12:36:24+00:00
- **Updated**: 2018-05-29 12:36:24+00:00
- **Authors**: Sungmin Lee, Jangho Lee, Jungbeom Lee, Chul-Kee Park, Sungroh Yoon
- **Comment**: 10 pages, 3 figures, conference
- **Journal**: None
- **Summary**: A meningioma is a type of brain tumor that requires tumor volume size follow ups in order to reach appropriate clinical decisions. A fully automated tool for meningioma detection is necessary for reliable and consistent tumor surveillance. There have been various studies concerning automated lesion detection. Studies on the application of convolutional neural network (CNN)-based methods, which have achieved a state-of-the-art level of performance in various computer vision tasks, have been carried out. However, the applicable diseases are limited, owing to a lack of strongly annotated data being present in medical image analysis. In order to resolve the above issue we propose pyramid gradient-based class activation mapping (PG-CAM) which is a novel method for tumor localization that can be trained in weakly supervised manner. PG-CAM uses a densely connected encoder-decoder-based feature pyramid network (DC-FPN) as a backbone structure, and extracts a multi-scale Grad-CAM that captures hierarchical features of a tumor. We tested our model using meningioma brain magnetic resonance (MR) data collected from the collaborating hospital. In our experiments, PG-CAM outperformed Grad-CAM by delivering a 23 percent higher localization accuracy for the validation set.



### A novel channel pruning method for deep neural network compression
- **Arxiv ID**: http://arxiv.org/abs/1805.11394v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.11394v1)
- **Published**: 2018-05-29 12:37:46+00:00
- **Updated**: 2018-05-29 12:37:46+00:00
- **Authors**: Yiming Hu, Siyang Sun, Jianquan Li, Xingang Wang, Qingyi Gu
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, deep neural networks have achieved great success in the field of computer vision. However, it is still a big challenge to deploy these deep models on resource-constrained embedded devices such as mobile robots, smart phones and so on. Therefore, network compression for such platforms is a reasonable solution to reduce memory consumption and computation complexity. In this paper, a novel channel pruning method based on genetic algorithm is proposed to compress very deep Convolution Neural Networks (CNNs). Firstly, a pre-trained CNN model is pruned layer by layer according to the sensitivity of each layer. After that, the pruned model is fine-tuned based on knowledge distillation framework. These two improvements significantly decrease the model redundancy with less accuracy drop. Channel selection is a combinatorial optimization problem that has exponential solution space. In order to accelerate the selection process, the proposed method formulates it as a search problem, which can be solved efficiently by genetic algorithm. Meanwhile, a two-step approximation fitness function is designed to further improve the efficiency of genetic process. The proposed method has been verified on three benchmark datasets with two popular CNN models: VGGNet and ResNet. On the CIFAR-100 and ImageNet datasets, our approach outperforms several state-of-the-art methods. On the CIFAR-10 and SVHN datasets, the pruned VGGNet achieves better performance than the original model with 8 times parameters compression and 3 times FLOPs reduction.



### Rice Classification Using Spatio-Spectral Deep Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1805.11491v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.11491v3)
- **Published**: 2018-05-29 14:17:12+00:00
- **Updated**: 2019-06-25 07:48:35+00:00
- **Authors**: Itthi Chatnuntawech, Kittipong Tantisantisom, Paisan Khanchaitit, Thitikorn Boonkoom, Berkin Bilgic, Ekapol Chuangsuwanich
- **Comment**: 22 pages, 10 figures, 6 tables; more methods and experiments included
  with references; link to github included; article restructured for clarity;
  typos fixed
- **Journal**: None
- **Summary**: Rice has been one of the staple foods that contribute significantly to human food supplies. Numerous rice varieties have been cultivated, imported, and exported worldwide. Different rice varieties could be mixed during rice production and trading. Rice impurities could damage the trust between rice importers and exporters, calling for the need to develop a rice variety inspection system. In this work, we develop a non-destructive rice variety classification system that benefits from the synergy between hyperspectral imaging and deep convolutional neural network (CNN). The proposed method uses a hyperspectral imaging system to simultaneously acquire complementary spatial and spectral information of rice seeds. The rice varieties are then determined from the acquired spatio-spectral data using a deep CNN. As opposed to several existing rice variety classification methods that require hand-engineered features, the proposed method automatically extracts spatio-spectral features from the raw sensor data. As demonstrated using two types of rice datasets, the proposed method achieved up to 11.9% absolute improvement in the mean classification accuracy, compared to the commonly used classification methods based on support vector machines.



### Capturing Variabilities from Computed Tomography Images with Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1805.11504v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.11504v1)
- **Published**: 2018-05-29 14:34:56+00:00
- **Updated**: 2018-05-29 14:34:56+00:00
- **Authors**: Umair Javaid, John A. Lee
- **Comment**: None
- **Journal**: European Symposium on Artificial Neural Networks, Computational
  Intelligence and Machine Learning (ESANN) Proceedings, pages 403-408, 25-27th
  April, 2018
- **Summary**: With the advent of Deep Learning (DL) techniques, especially Generative Adversarial Networks (GANs), data augmentation and generation are quickly evolving domains that have raised much interest recently. However, the DL techniques are data demanding and since, medical data is not easily accessible, they suffer from data insufficiency. To deal with this limitation, different data augmentation techniques are used. Here, we propose a novel unsupervised data-driven approach for data augmentation that can generate 2D Computed Tomography (CT) images using a simple GAN. The generated CT images have good global and local features of a real CT image and can be used to augment the training datasets for effective learning. In this proof-of-concept study, we show that our proposed solution using GANs is able to capture some of the global and local CT variabilities. Our network is able to generate visually realistic CT images and we aim to further enhance its output by scaling it to a higher resolution and potentially from 2D to 3D.



### Face Recognition in Low Quality Images: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1805.11519v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.11519v3)
- **Published**: 2018-05-29 14:50:39+00:00
- **Updated**: 2019-03-28 23:34:23+00:00
- **Authors**: Pei Li, Loreto Prieto, Domingo Mery, Patrick Flynn
- **Comment**: There are some mistakes addressing in this paper which will be
  misleading to the reader and we wont have a new version in short time. We
  will resubmit once it is being corected
- **Journal**: None
- **Summary**: Low-resolution face recognition (LRFR) has received increasing attention over the past few years. Its applications lie widely in the real-world environment when high-resolution or high-quality images are hard to capture. One of the biggest demands for LRFR technologies is video surveillance. As the the number of surveillance cameras in the city increases, the videos that captured will need to be processed automatically. However, those videos or images are usually captured with large standoffs, arbitrary illumination condition, and diverse angles of view. Faces in these images are generally small in size. Several studies addressed this problem employed techniques like super resolution, deblurring, or learning a relationship between different resolution domains. In this paper, we provide a comprehensive review of approaches to low-resolution face recognition in the past five years. First, a general problem definition is given. Later, systematically analysis of the works on this topic is presented by catogory. In addition to describing the methods, we also focus on datasets and experiment settings. We further address the related works on unconstrained low-resolution face recognition and compare them with the result that use synthetic low-resolution data. Finally, we summarized the general limitations and speculate a priorities for the future effort.



### On Low-Resolution Face Recognition in the Wild: Comparisons and New Techniques
- **Arxiv ID**: http://arxiv.org/abs/1805.11529v2
- **DOI**: 10.1109/TIFS.2018.2890812
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.11529v2)
- **Published**: 2018-05-29 15:04:19+00:00
- **Updated**: 2019-03-28 22:48:59+00:00
- **Authors**: Pei Li, Loreto Prieto, Domingo Mery, Patrick Flynn
- **Comment**: None
- **Journal**: None
- **Summary**: Although face recognition systems have achieved impressive performance in recent years, the low-resolution face recognition (LRFR) task remains challenging, especially when the LR faces are captured under non-ideal conditions, as is common in surveillance-based applications. Faces captured in such conditions are often contaminated by blur, nonuniform lighting, and nonfrontal face pose. In this paper, we analyze face recognition techniques using data captured under low-quality conditions in the wild. We provide a comprehensive analysis of experimental results for two of the most important applications in real surveillance applications, and demonstrate practical approaches to handle both cases that show promising performance. The following three contributions are made: {\em (i)} we conduct experiments to evaluate super-resolution methods for low-resolution face recognition; {\em (ii)} we study face re-identification on various public face datasets including real surveillance and low-resolution subsets of large-scale datasets, present a baseline result for several deep learning based approaches, and improve them by introducing a GAN pre-training approach and fully convolutional architecture; and {\em (iii)} we explore low-resolution face identification by employing a state-of-the-art supervised discriminative learning approach. Evaluations are conducted on challenging portions of the SCFace and UCCSface datasets.



### Adversarial Regularizers in Inverse Problems
- **Arxiv ID**: http://arxiv.org/abs/1805.11572v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, math.NA, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.11572v2)
- **Published**: 2018-05-29 16:40:37+00:00
- **Updated**: 2019-01-11 17:24:06+00:00
- **Authors**: Sebastian Lunz, Ozan Öktem, Carola-Bibiane Schönlieb
- **Comment**: published at NeurIPS 2018
- **Journal**: None
- **Summary**: Inverse Problems in medical imaging and computer vision are traditionally solved using purely model-based methods. Among those variational regularization models are one of the most popular approaches. We propose a new framework for applying data-driven approaches to inverse problems, using a neural network as a regularization functional. The network learns to discriminate between the distribution of ground truth images and the distribution of unregularized reconstructions. Once trained, the network is applied to the inverse problem by solving the corresponding variational problem. Unlike other data-based approaches for inverse problems, the algorithm can be applied even if only unsupervised training data is available. Experiments demonstrate the potential of the framework for denoising on the BSDS dataset and for computed tomography reconstruction on the LIDC dataset.



### Mirror, Mirror, on the Wall, Who's Got the Clearest Image of Them All? - A Tailored Approach to Single Image Reflection Removal
- **Arxiv ID**: http://arxiv.org/abs/1805.11589v2
- **DOI**: 10.1109/TIP.2019.2923559
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.11589v2)
- **Published**: 2018-05-29 17:12:33+00:00
- **Updated**: 2019-01-25 20:49:29+00:00
- **Authors**: Daniel Heydecker, Georg Maierhofer, Angelica I. Aviles-Rivero, Qingnan Fan, Dongdong Chen, Carola-Bibiane Schönlieb, Sabine Süsstrunk
- **Comment**: None
- **Journal**: None
- **Summary**: Removing reflection artefacts from a single image is a problem of both theoretical and practical interest, which still presents challenges because of the massively ill-posed nature of the problem. In this work, we propose a technique based on a novel optimisation problem. Firstly, we introduce a simple user interaction scheme, which helps minimise information loss in reflection-free regions. Secondly, we introduce an $H^2$ fidelity term, which preserves fine detail while enforcing global colour similarity. We show that this combination allows us to mitigate some major drawbacks of the existing methods for reflection removal. We demonstrate, through numerical and visual experiments, that our method is able to outperform the state-of-the-art methods and compete with recent deep-learning approaches.



### Playing hard exploration games by watching YouTube
- **Arxiv ID**: http://arxiv.org/abs/1805.11592v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.11592v2)
- **Published**: 2018-05-29 17:19:36+00:00
- **Updated**: 2018-11-30 15:59:27+00:00
- **Authors**: Yusuf Aytar, Tobias Pfaff, David Budden, Tom Le Paine, Ziyu Wang, Nando de Freitas
- **Comment**: None
- **Journal**: None
- **Summary**: Deep reinforcement learning methods traditionally struggle with tasks where environment rewards are particularly sparse. One successful method of guiding exploration in these domains is to imitate trajectories provided by a human demonstrator. However, these demonstrations are typically collected under artificial conditions, i.e. with access to the agent's exact environment setup and the demonstrator's action and reward trajectories. Here we propose a two-stage method that overcomes these limitations by relying on noisy, unaligned footage without access to such data. First, we learn to map unaligned videos from multiple sources to a common representation using self-supervised objectives constructed over both time and modality (i.e. vision and sound). Second, we embed a single YouTube video in this representation to construct a reward function that encourages an agent to imitate human gameplay. This method of one-shot imitation allows our agent to convincingly exceed human-level performance on the infamously hard exploration games Montezuma's Revenge, Pitfall! and Private Eye for the first time, even if the agent is not presented with any environment rewards.



### AdapterNet - learning input transformation for domain adaptation
- **Arxiv ID**: http://arxiv.org/abs/1805.11601v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.11601v2)
- **Published**: 2018-05-29 17:38:38+00:00
- **Updated**: 2018-11-15 12:56:58+00:00
- **Authors**: Alon Hazan, Yoel Shoshan, Daniel Khapun, Roy Aladjem, Vadim Ratner
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks have demonstrated impressive performance in various machine learning tasks. However, they are notoriously sensitive to changes in data distribution. Often, even a slight change in the distribution can lead to drastic performance reduction. Artificially augmenting the data may help to some extent, but in most cases, fails to achieve model invariance to the data distribution. Some examples where this sub-class of domain adaptation can be valuable are various imaging modalities such as thermal imaging, X-ray, ultrasound, and MRI, where changes in acquisition parameters or acquisition device manufacturer will result in a different representation of the same input. Our work shows that standard fine-tuning fails to adapt the model in certain important cases. We propose a novel method of adapting to a new data source, and demonstrate near perfect adaptation on a customized ImageNet benchmark. Moreover, our method does not require any samples from the original data set, it is completely explainable and can be tailored to the task.



### Can DNNs Learn to Lipread Full Sentences?
- **Arxiv ID**: http://arxiv.org/abs/1805.11685v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1805.11685v1)
- **Published**: 2018-05-29 19:54:19+00:00
- **Updated**: 2018-05-29 19:54:19+00:00
- **Authors**: George Sterpu, Christian Saam, Naomi Harte
- **Comment**: Accepted at the 2018 IEEE International Conference on Image
  Processing (ICIP 2018)
- **Journal**: None
- **Summary**: Finding visual features and suitable models for lipreading tasks that are more complex than a well-constrained vocabulary has proven challenging. This paper explores state-of-the-art Deep Neural Network architectures for lipreading based on a Sequence to Sequence Recurrent Neural Network. We report results for both hand-crafted and 2D/3D Convolutional Neural Network visual front-ends, online monotonic attention, and a joint Connectionist Temporal Classification-Sequence-to-Sequence loss. The system is evaluated on the publicly available TCD-TIMIT dataset, with 59 speakers and a vocabulary of over 6000 words. Results show a major improvement on a Hidden Markov Model framework. A fuller analysis of performance across visemes demonstrates that the network is not only learning the language model, but actually learning to lipread.



### Channel Gating Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1805.12549v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.12549v2)
- **Published**: 2018-05-29 20:11:56+00:00
- **Updated**: 2019-10-28 23:53:50+00:00
- **Authors**: Weizhe Hua, Yuan Zhou, Christopher De Sa, Zhiru Zhang, G. Edward Suh
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces channel gating, a dynamic, fine-grained, and hardware-efficient pruning scheme to reduce the computation cost for convolutional neural networks (CNNs). Channel gating identifies regions in the features that contribute less to the classification result, and skips the computation on a subset of the input channels for these ineffective regions. Unlike static network pruning, channel gating optimizes CNN inference at run-time by exploiting input-specific characteristics, which allows substantially reducing the compute cost with almost no accuracy loss. We experimentally show that applying channel gating in state-of-the-art networks achieves 2.7-8.0$\times$ reduction in floating-point operations (FLOPs) and 2.0-4.4$\times$ reduction in off-chip memory accesses with a minimal accuracy loss on CIFAR-10. Combining our method with knowledge distillation reduces the compute cost of ResNet-18 by 2.6$\times$ without accuracy drop on ImageNet. We further demonstrate that channel gating can be realized in hardware efficiently. Our approach exhibits sparsity patterns that are well-suited to dense systolic arrays with minimal additional hardware. We have designed an accelerator for channel gating networks, which can be implemented using either FPGAs or ASICs. Running a quantized ResNet-18 model for ImageNet, our accelerator achieves an encouraging speedup of 2.4$\times$ on average, with a theoretical FLOP reduction of 2.8$\times$.



### A Novel Multi-clustering Method for Hierarchical Clusterings, Based on Boosting
- **Arxiv ID**: http://arxiv.org/abs/1805.11712v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.11712v1)
- **Published**: 2018-05-29 21:22:53+00:00
- **Updated**: 2018-05-29 21:22:53+00:00
- **Authors**: Elaheh Rashedi, Abdolreza Mirzaei
- **Comment**: 19th Iranian Conference on Electrical Engineering (ICEE 2011)
- **Journal**: None
- **Summary**: Bagging and boosting are proved to be the best methods of building multiple classifiers in classification combination problems. In the area of "flat clustering" problems, it is also recognized that multi-clustering methods based on boosting provide clusterings of an improved quality. In this paper, we introduce a novel multi-clustering method for "hierarchical clusterings" based on boosting theory, which creates a more stable hierarchical clustering of a dataset. The proposed algorithm includes a boosting iteration in which a bootstrap of samples is created by weighted random sampling of elements from the original dataset. A hierarchical clustering algorithm is then applied to selected subsample to build a dendrogram which describes the hierarchy. Finally, dissimilarity description matrices of multiple dendrogram results are combined to a consensus one, using a hierarchical-clustering-combination approach. Experiments on real popular datasets show that boosted method provides superior quality solutions compared to standard hierarchical clustering methods.



### Deep Video Portraits
- **Arxiv ID**: http://arxiv.org/abs/1805.11714v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1805.11714v1)
- **Published**: 2018-05-29 21:31:14+00:00
- **Updated**: 2018-05-29 21:31:14+00:00
- **Authors**: Hyeongwoo Kim, Pablo Garrido, Ayush Tewari, Weipeng Xu, Justus Thies, Matthias Nießner, Patrick Pérez, Christian Richardt, Michael Zollhöfer, Christian Theobalt
- **Comment**: SIGGRAPH 2018, Video: https://www.youtube.com/watch?v=qc5P2bvfl44
- **Journal**: None
- **Summary**: We present a novel approach that enables photo-realistic re-animation of portrait videos using only an input video. In contrast to existing approaches that are restricted to manipulations of facial expressions only, we are the first to transfer the full 3D head position, head rotation, face expression, eye gaze, and eye blinking from a source actor to a portrait video of a target actor. The core of our approach is a generative neural network with a novel space-time architecture. The network takes as input synthetic renderings of a parametric face model, based on which it predicts photo-realistic video frames for a given target actor. The realism in this rendering-to-video transfer is achieved by careful adversarial training, and as a result, we can create modified target videos that mimic the behavior of the synthetically-created input. In order to enable source-to-target video re-animation, we render a synthetic target video with the reconstructed head animation parameters from a source video, and feed it into the trained network -- thus taking full control of the target. With the ability to freely recombine source and target parameters, we are able to demonstrate a large variety of video rewrite applications without explicitly modeling hair, body or background. For instance, we can reenact the full head using interactive user-controlled editing, and realize high-fidelity visual dubbing. To demonstrate the high quality of our output, we conduct an extensive series of experiments and evaluations, where for instance a user study shows that our video edits are hard to detect.



### Random mesh projectors for inverse problems
- **Arxiv ID**: http://arxiv.org/abs/1805.11718v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.11718v3)
- **Published**: 2018-05-29 21:36:05+00:00
- **Updated**: 2018-12-06 04:31:08+00:00
- **Authors**: Sidharth Gupta, Konik Kothari, Maarten V. de Hoop, Ivan Dokmanić
- **Comment**: S. Gupta and K. Kothari contributed equally
- **Journal**: None
- **Summary**: We propose a new learning-based approach to solve ill-posed inverse problems in imaging. We address the case where ground truth training samples are rare and the problem is severely ill-posed - both because of the underlying physics and because we can only get few measurements. This setting is common in geophysical imaging and remote sensing. We show that in this case the common approach to directly learn the mapping from the measured data to the reconstruction becomes unstable. Instead, we propose to first learn an ensemble of simpler mappings from the data to projections of the unknown image into random piecewise-constant subspaces. We then combine the projections to form a final reconstruction by solving a deconvolution-like problem. We show experimentally that the proposed method is more robust to measurement noise and corruptions not seen during training than a directly learned inverse.



### Rethinking Knowledge Graph Propagation for Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1805.11724v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.11724v3)
- **Published**: 2018-05-29 21:55:46+00:00
- **Updated**: 2019-03-27 17:26:38+00:00
- **Authors**: Michael Kampffmeyer, Yinbo Chen, Xiaodan Liang, Hao Wang, Yujia Zhang, Eric P. Xing
- **Comment**: The first two authors contributed equally. Code at
  https://github.com/cyvius96/adgpm. To appear in CVPR 2019
- **Journal**: None
- **Summary**: Graph convolutional neural networks have recently shown great potential for the task of zero-shot learning. These models are highly sample efficient as related concepts in the graph structure share statistical strength allowing generalization to new classes when faced with a lack of data. However, multi-layer architectures, which are required to propagate knowledge to distant nodes in the graph, dilute the knowledge by performing extensive Laplacian smoothing at each layer and thereby consequently decrease performance. In order to still enjoy the benefit brought by the graph structure while preventing dilution of knowledge from distant nodes, we propose a Dense Graph Propagation (DGP) module with carefully designed direct links among distant nodes. DGP allows us to exploit the hierarchical graph structure of the knowledge graph through additional connections. These connections are added based on a node's relationship to its ancestors and descendants. A weighting scheme is further used to weigh their contribution depending on the distance to the node to improve information propagation in the graph. Combined with finetuning of the representations in a two-stage training approach our method outperforms state-of-the-art zero-shot learning approaches.



### HeadOn: Real-time Reenactment of Human Portrait Videos
- **Arxiv ID**: http://arxiv.org/abs/1805.11729v1
- **DOI**: 10.1145/3197517.3201350
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1805.11729v1)
- **Published**: 2018-05-29 22:24:13+00:00
- **Updated**: 2018-05-29 22:24:13+00:00
- **Authors**: Justus Thies, Michael Zollhöfer, Christian Theobalt, Marc Stamminger, Matthias Nießner
- **Comment**: Video: https://www.youtube.com/watch?v=7Dg49wv2c_g Presented at
  Siggraph'18
- **Journal**: None
- **Summary**: We propose HeadOn, the first real-time source-to-target reenactment approach for complete human portrait videos that enables transfer of torso and head motion, face expression, and eye gaze. Given a short RGB-D video of the target actor, we automatically construct a personalized geometry proxy that embeds a parametric head, eye, and kinematic torso model. A novel real-time reenactment algorithm employs this proxy to photo-realistically map the captured motion from the source actor to the target actor. On top of the coarse geometric proxy, we propose a video-based rendering technique that composites the modified target portrait video via view- and pose-dependent texturing, and creates photo-realistic imagery of the target actor under novel torso and head poses, facial expressions, and gaze directions. To this end, we propose a robust tracking of the face and torso of the source actor. We extensively evaluate our approach and show significant improvements in enabling much greater flexibility in creating realistic reenacted output videos.



### Superpixel-enhanced Pairwise Conditional Random Field for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1805.11737v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.11737v1)
- **Published**: 2018-05-29 22:59:32+00:00
- **Updated**: 2018-05-29 22:59:32+00:00
- **Authors**: Li Sulimowicz, Ishfaq Ahmad, Alexander Aved
- **Comment**: 5 pages
- **Journal**: ICIP 2018
- **Summary**: Superpixel-based Higher-order Conditional Random Fields (CRFs) are effective in enforcing long-range consistency in pixel-wise labeling problems, such as semantic segmentation. However, their major short coming is considerably longer time to learn higher-order potentials and extra hyperparameters and/or weights compared with pairwise models. This paper proposes a superpixel-enhanced pairwise CRF framework that consists of the conventional pairwise as well as our proposed superpixel-enhanced pairwise (SP-Pairwise) potentials. SP-Pairwise potentials incorporate the superpixel-based higher-order cues by conditioning on a segment filtered image and share the same set of parameters as the conventional pairwise potentials. Therefore, the proposed superpixel-enhanced pairwise CRF has a lower time complexity in parameter learning and at the same time it outperforms higher-order CRF in terms of inference accuracy. Moreover, the new scheme takes advantage of the pre-trained pairwise models by reusing their parameters and/or weights, which provides a significant accuracy boost on the basis of CRF-RNN even without training. Experiments on MSRC-21 and PASCAL VOC 2012 dataset confirm the effectiveness of our method.



### Semantic Road Layout Understanding by Generative Adversarial Inpainting
- **Arxiv ID**: http://arxiv.org/abs/1805.11746v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.11746v2)
- **Published**: 2018-05-29 23:51:41+00:00
- **Updated**: 2018-11-20 12:24:37+00:00
- **Authors**: Lorenzo Berlincioni, Federico Becattini, Leonardo Galteri, Lorenzo Seidenari, Alberto Del Bimbo
- **Comment**: None
- **Journal**: None
- **Summary**: Autonomous driving is becoming a reality, yet vehicles still need to rely on complex sensor fusion to understand the scene they act in. The ability to discern static environment and dynamic entities provides a comprehension of the road layout that poses constraints to the reasoning process about moving objects. We pursue this through a GAN-based semantic segmentation inpainting model to remove all dynamic objects from the scene and focus on understanding its static components such as streets, sidewalks and buildings. We evaluate this task on the Cityscapes dataset and on a novel synthetically generated dataset obtained with the CARLA simulator and specifically designed to quantitatively evaluate semantic segmentation inpaintings. We compare our methods with a variety of baselines working both in the RGB and segmentation domains.



