# Arxiv Papers in cs.CV on 2018-05-25
### Greedy Graph Searching for Vascular Tracking in Angiographic Image Sequences
- **Arxiv ID**: http://arxiv.org/abs/1805.09940v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1805.09940v1)
- **Published**: 2018-05-25 00:53:57+00:00
- **Updated**: 2018-05-25 00:53:57+00:00
- **Authors**: Huihui Fang, Jian Yang, Jianjun Zhu, Danni Ai, Yong Huang, Yurong Jiang, Hong Song, Yongtian Wang
- **Comment**: Submitted to Medical Physics; 30 pages, 11 figures
- **Journal**: None
- **Summary**: Vascular tracking of angiographic image sequences is one of the most clinically important tasks in the diagnostic assessment and interventional guidance of cardiac disease. However, this task can be challenging to accomplish because of unsatisfactory angiography image quality and complex vascular structures. Thus, this study proposed a new greedy graph search-based method for vascular tracking. Each vascular branch is separated from the vasculature and is tracked independently. Then, all branches are combined using topology optimization, thereby resulting in complete vasculature tracking. A gray-based image registration method was applied to determine the tracking range, and the deformation field between two consecutive frames was calculated. The vascular branch was described using a vascular centerline extraction method with multi-probability fusion-based topology optimization. We introduce an undirected acyclic graph establishment technique. A greedy search method was proposed to acquire all possible paths in the graph that might match the tracked vascular branch. The final tracking result was selected by branch matching using dynamic time warping with a DAISY descriptor. The solution to the problem reflected both the spatial and textural information between successive frames. Experimental results demonstrated that the proposed method was effective and robust for vascular tracking, attaining a F1 score of 0.89 on a single branch dataset and 0.88 on a vessel tree dataset. This approach provided a universal solution to address the problem of filamentary structure tracking.



### Meta Transfer Learning for Facial Emotion Recognition
- **Arxiv ID**: http://arxiv.org/abs/1805.09946v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.09946v1)
- **Published**: 2018-05-25 01:56:30+00:00
- **Updated**: 2018-05-25 01:56:30+00:00
- **Authors**: Dung Nguyen, Kien Nguyen, Sridha Sridharan, Iman Abbasnejad, David Dean, Clinton Fookes
- **Comment**: None
- **Journal**: None
- **Summary**: The use of deep learning techniques for automatic facial expression recognition has recently attracted great interest but developed models are still unable to generalize well due to the lack of large emotion datasets for deep learning. To overcome this problem, in this paper, we propose utilizing a novel transfer learning approach relying on PathNet and investigate how knowledge can be accumulated within a given dataset and how the knowledge captured from one emotion dataset can be transferred into another in order to improve the overall performance. To evaluate the robustness of our system, we have conducted various sets of experiments on two emotion datasets: SAVEE and eNTERFACE. The experimental results demonstrate that our proposed system leads to improvement in performance of emotion recognition and performs significantly better than the recent state-of-the-art schemes adopting fine-\ tuning/pre-trained approaches.



### Deep Functional Dictionaries: Learning Consistent Semantic Structures on 3D Models from Functions
- **Arxiv ID**: http://arxiv.org/abs/1805.09957v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.09957v3)
- **Published**: 2018-05-25 03:07:15+00:00
- **Updated**: 2018-11-28 06:02:55+00:00
- **Authors**: Minhyuk Sung, Hao Su, Ronald Yu, Leonidas Guibas
- **Comment**: None
- **Journal**: NeurIPS 2018
- **Summary**: Various 3D semantic attributes such as segmentation masks, geometric features, keypoints, and materials can be encoded as per-point probe functions on 3D geometries. Given a collection of related 3D shapes, we consider how to jointly analyze such probe functions over different shapes, and how to discover common latent structures using a neural network --- even in the absence of any correspondence information. Our network is trained on point cloud representations of shape geometry and associated semantic functions on that point cloud. These functions express a shared semantic understanding of the shapes but are not coordinated in any way. For example, in a segmentation task, the functions can be indicator functions of arbitrary sets of shape parts, with the particular combination involved not known to the network. Our network is able to produce a small dictionary of basis functions for each shape, a dictionary whose span includes the semantic functions provided for that shape. Even though our shapes have independent discretizations and no functional correspondences are provided, the network is able to generate latent bases, in a consistent order, that reflect the shared semantic structure among the shapes. We demonstrate the effectiveness of our technique in various segmentation and keypoint selection applications.



### Cooking State Recognition from Images Using Inception Architecture
- **Arxiv ID**: http://arxiv.org/abs/1805.09967v2
- **DOI**: 10.1109/ICREST.2019.8644262
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.09967v2)
- **Published**: 2018-05-25 03:42:58+00:00
- **Updated**: 2019-01-17 01:42:00+00:00
- **Authors**: Md Sirajus Salekin, Ahmad Babaeian Jelodar, Rafsanjany Kushol
- **Comment**: 6 pages, 8 figures, 4 tables
- **Journal**: None
- **Summary**: A kitchen robot properly needs to understand the cooking environment to continue any cooking activities. But object's state detection has not been researched well so far as like object detection. In this paper, we propose a deep learning approach to identify different cooking states from images for a kitchen robot. In our research, we investigate particularly the performance of Inception architecture and propose a modified architecture based on Inception model to classify different cooking states. The model is analyzed robustly in terms of different layers, and optimizers. Experimental results on a cooking datasets demonstrate that proposed model can be a potential solution to the cooking state recognition problem.



### Part-based Visual Tracking via Structural Support Correlation Filter
- **Arxiv ID**: http://arxiv.org/abs/1805.09971v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.09971v1)
- **Published**: 2018-05-25 04:04:00+00:00
- **Updated**: 2018-05-25 04:04:00+00:00
- **Authors**: Zhangjian Ji, Kai Feng, Yuhua Qian
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, part-based and support vector machines (SVM) based trackers have shown favorable performance. Nonetheless, the time-consuming online training and updating process limit their real-time applications. In order to better deal with the partial occlusion issue and improve their efficiency, we propose a novel part-based structural support correlation filter tracking method, which absorbs the strong discriminative ability from SVM and the excellent property of part-based tracking methods which is less sensitive to partial occlusion. Then, our proposed model can learn the support correlation filter of each part jointly by a star structure model, which preserves the spatial layout structure among parts and tolerates outliers of parts. In addition, to mitigate the issue of drift away from object further, we introduce inter-frame consistencies of local parts into our model. Finally, in our model, we accurately estimate the scale changes of object by the relative distance change among reliable parts. The extensive empirical evaluations on three benchmark datasets: OTB2015, TempleColor128 and VOT2015 demonstrate that the proposed method performs superiorly against several state-of-the-art trackers in terms of tracking accuracy, speed and robustness.



### Learning from Multi-domain Artistic Images for Arbitrary Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/1805.09987v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1805.09987v2)
- **Published**: 2018-05-25 05:54:39+00:00
- **Updated**: 2019-04-14 06:22:51+00:00
- **Authors**: Zheng Xu, Michael Wilber, Chen Fang, Aaron Hertzmann, Hailin Jin
- **Comment**: Update code link
- **Journal**: None
- **Summary**: We propose a fast feed-forward network for arbitrary style transfer, which can generate stylized image for previously unseen content and style image pairs. Besides the traditional content and style representation based on deep features and statistics for textures, we use adversarial networks to regularize the generation of stylized images. Our adversarial network learns the intrinsic property of image styles from large-scale multi-domain artistic images. The adversarial training is challenging because both the input and output of our generator are diverse multi-domain images. We use a conditional generator that stylized content by shifting the statistics of deep features, and a conditional discriminator based on the coarse category of styles. Moreover, we propose a mask module to spatially decide the stylization level and stabilize adversarial training by avoiding mode collapse. As a side effect, our trained discriminator can be applied to rank and select representative stylized images. We qualitatively and quantitatively evaluate the proposed method, and compare with recent style transfer methods. We release our code and model at https://github.com/nightldj/behance_release.



### Learning to Propagate Labels: Transductive Propagation Network for Few-shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1805.10002v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML, 68T10, 97R40, I.5.1; I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/1805.10002v5)
- **Published**: 2018-05-25 07:00:31+00:00
- **Updated**: 2019-02-08 09:30:53+00:00
- **Authors**: Yanbin Liu, Juho Lee, Minseop Park, Saehoon Kim, Eunho Yang, Sung Ju Hwang, Yi Yang
- **Comment**: Accepted in ICLR 2019; code available at
  https://github.com/csyanbin/TPN
- **Journal**: None
- **Summary**: The goal of few-shot learning is to learn a classifier that generalizes well even when trained with a limited number of training instances per class. The recently introduced meta-learning approaches tackle this problem by learning a generic classifier across a large number of multiclass classification tasks and generalizing the model to a new task. Yet, even with such meta-learning, the low-data problem in the novel classification task still remains. In this paper, we propose Transductive Propagation Network (TPN), a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem. Specifically, we propose to learn to propagate labels from labeled instances to unlabeled test instances, by learning a graph construction module that exploits the manifold structure in the data. TPN jointly learns both the parameters of feature embedding and the graph construction in an end-to-end manner. We validate TPN on multiple benchmark datasets, on which it largely outperforms existing few-shot learning approaches and achieves the state-of-the-art results.



### Key Person Aided Re-identification in Partially Ordered Pedestrian Set
- **Arxiv ID**: http://arxiv.org/abs/1805.10017v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.10017v1)
- **Published**: 2018-05-25 07:46:36+00:00
- **Updated**: 2018-05-25 07:46:36+00:00
- **Authors**: Chen Chen, Min Cao, Xiyuan Hu, Silong Peng
- **Comment**: 12 pages,6 figures, BMVC conference
- **Journal**: None
- **Summary**: Ideally person re-identification seeks for perfect feature representation and metric model that re-identify all various pedestrians well in non-overlapping views at different locations with different camera configurations, which is very challenging. However, in most pedestrian sets, there always are some outstanding persons who are relatively easy to re-identify. Inspired by the existence of such data division, we propose a novel key person aided person re-identification framework based on the re-defined partially ordered pedestrian sets. The outstanding persons, namely "key persons", are selected by the K-nearest neighbor based saliency measurement. The partial order defined by pedestrian entering time in surveillance associates the key persons with the query person temporally and helps to locate the possible candidates. Experiments conducted on two video datasets show that the proposed key person aided framework outperforms the state-of-the-art methods and improves the matching accuracy greatly at all ranks.



### DIF : Dataset of Perceived Intoxicated Faces for Drunk Person Identification
- **Arxiv ID**: http://arxiv.org/abs/1805.10030v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.10030v3)
- **Published**: 2018-05-25 08:25:26+00:00
- **Updated**: 2019-09-08 23:25:07+00:00
- **Authors**: Vineet Mehta, Devendra Pratap Yadav, Sai Srinadhu Katta, Abhinav Dhall
- **Comment**: None
- **Journal**: None
- **Summary**: Traffic accidents cause over a million deaths every year, of which a large fraction is attributed to drunk driving. An automated intoxicated driver detection system in vehicles will be useful in reducing accidents and related financial costs. Existing solutions require special equipment such as electrocardiogram, infrared cameras or breathalyzers. In this work, we propose a new dataset called DIF (Dataset of perceived Intoxicated Faces) which contains audio-visual data of intoxicated and sober people obtained from online sources. To the best of our knowledge, this is the first work for automatic bimodal non-invasive intoxication detection. Convolutional Neural Networks (CNN) and Deep Neural Networks (DNN) are trained for computing the video and audio baselines, respectively. 3D CNN is used to exploit the Spatio-temporal changes in the video. A simple variation of the traditional 3D convolution block is proposed based on inducing non-linearity between the spatial and temporal channels. Extensive experiments are performed to validate the approach and baselines.



### Learning Unit State Recognition Based on Multi-channel Data Fusion
- **Arxiv ID**: http://arxiv.org/abs/1806.07372v1
- **DOI**: 10.1109/ACCESS.2019.2947091
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.07372v1)
- **Published**: 2018-05-25 08:59:35+00:00
- **Updated**: 2018-05-25 08:59:35+00:00
- **Authors**: Feng Tian, Jia Yue, Xing Wan, Kuo-Min Chao, Qinghua Zheng
- **Comment**: 4 pages, 4 figures, 2 tables
- **Journal**: None
- **Summary**: Despite recent advances in MOOC, the current e-learning systems have advantages of alleviating barriers by time differences, and geographically spatial separation between teachers and students. However, there has been a 'lack of supervision' problem that e-learner's learning unit state(LUS) can't be supervised automatically. In this paper, we present a fusion framework considering three channel data sources: 1) videos/images from a camera, 2) eye movement information tracked by a low solution eye tracker and 3) mouse movement. Based on these data modalities, we propose a novel approach of multi-channel data fusion to explore the learning unit state recognition. We also propose a method to build a learning state recognition model to avoid manually labeling image data. The experiments were carried on our designed online learning prototype system, and we choose CART, Random Forest and GBDT regression model to predict e-learner's learning state. The results show that multi-channel data fusion model have a better recognition performance in comparison with single channel model. In addition, a best recognition performance can be reached when image, eye movement and mouse movement features are fused.



### Unsupervisedly Training GANs for Segmenting Digital Pathology with Automatically Generated Annotations
- **Arxiv ID**: http://arxiv.org/abs/1805.10059v2
- **DOI**: None
- **Categories**: **cs.CV**, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/1805.10059v2)
- **Published**: 2018-05-25 09:42:59+00:00
- **Updated**: 2018-08-01 09:01:27+00:00
- **Authors**: Michael Gadermayr, Laxmi Gupta, Barbara M. Klinkhammer, Peter Boor, Dorit Merhof
- **Comment**: Submitted to ISBI'19
- **Journal**: None
- **Summary**: Recently, generative adversarial networks exhibited excellent performances in semi-supervised image analysis scenarios. In this paper, we go even further by proposing a fully unsupervised approach for segmentation applications with prior knowledge of the objects' shapes. We propose and investigate different strategies to generate simulated label data and perform image-to-image translation between the image and the label domain using an adversarial model. Specifically, we assess the impact of the annotation model's accuracy as well as the effect of simulating additional low-level image features. For experimental evaluation, we consider the segmentation of the glomeruli, an application scenario from renal pathology. Experiments provide proof of concept and also confirm that the strategy for creating the simulated label data is of particular relevance considering the stability of GAN trainings.



### A Double-Deep Spatio-Angular Learning Framework for Light Field based Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1805.10078v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.10078v3)
- **Published**: 2018-05-25 10:59:16+00:00
- **Updated**: 2019-04-24 23:12:01+00:00
- **Authors**: Alireza Sepas-Moghaddam, Mohammad A. Haque, Paulo Lobato Correia, Kamal Nasrollahi, Thomas B. Moeslund, Fernando Pereira
- **Comment**: Submitted to IEEE Transactions on Circuits and Systems for Video
  Technology
- **Journal**: None
- **Summary**: Face recognition has attracted increasing attention due to its wide range of applications, but it is still challenging when facing large variations in the biometric data characteristics. Lenslet light field cameras have recently come into prominence to capture rich spatio-angular information, thus offering new possibilities for advanced biometric recognition systems. This paper proposes a double-deep spatio-angular learning framework for light field based face recognition, which is able to learn both texture and angular dynamics in sequence using convolutional representations; this is a novel recognition framework that has never been proposed before for either face recognition or any other visual recognition task. The proposed double-deep learning framework includes a long short-term memory (LSTM) recurrent network whose inputs are VGG-Face descriptions that are computed using a VGG-Very-Deep-16 convolutional neural network (CNN). The VGG-16 network uses different face viewpoints rendered from a full light field image, which are organised as a pseudo-video sequence. A comprehensive set of experiments has been conducted with the IST-EURECOM light field face database, for varied and challenging recognition tasks. Results show that the proposed framework achieves superior face recognition performance when compared to the state-of-the-art.



### Underwater Fish Species Classification using Convolutional Neural Network and Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1805.10106v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.10106v1)
- **Published**: 2018-05-25 12:34:10+00:00
- **Updated**: 2018-05-25 12:34:10+00:00
- **Authors**: Dhruv Rathi, Sushant Jain, Dr. S. Indu
- **Comment**: Pre-print of paper to be published in IEEEXplore, accepted under the
  International Conference of Advances in Pattern Recognition 2017
- **Journal**: None
- **Summary**: The target of this paper is to recommend a way for Automated classification of Fish species. A high accuracy fish classification is required for greater understanding of fish behavior in Ichthyology and by marine biologists. Maintaining a ledger of the number of fishes per species and marking the endangered species in large and small water bodies is required by concerned institutions. Majority of available methods focus on classification of fishes outside of water because underwater classification poses challenges such as background noises, distortion of images, the presence of other water bodies in images, image quality and occlusion. This method uses a novel technique based on Convolutional Neural Networks, Deep Learning and Image Processing to achieve an accuracy of 96.29%. This method ensures considerably discrimination accuracy improvements than the previously proposed methods.



### Generating protected fingerprint template utilizing coprime mapping transformation
- **Arxiv ID**: http://arxiv.org/abs/1805.10108v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.10108v1)
- **Published**: 2018-05-25 12:38:03+00:00
- **Updated**: 2018-05-25 12:38:03+00:00
- **Authors**: Rudresh Dwivedi, Somnath Dey
- **Comment**: None
- **Journal**: None
- **Summary**: The identity of a user is permanently lost if biometric data gets compromised since the biometric information is irreplaceable and irrevocable. To revoke and reissue a new template in place of the compromised biometric template, the idea of cancelable biometrics has been introduced. The concept behind cancelable biometric is to irreversibly transform the original biometric template and perform the comparison in the protected domain. In this paper, a coprime transformation scheme has been proposed to derive a protected fingerprint template. The method divides the fingerprint region into a number of sectors with respect to each minutiae point and identifies the nearest-neighbor minutiae in each sector. Then, ridge features for all neighboring minutiae points are computed and mapped onto co-prime positions of a random matrix to generate the cancelable template. The proposed approach achieves an EER of 1.82, 1.39, 4.02 and 5.77 on DB1, DB2, DB3 and DB4 datasets of the FVC2002 and an EER of 8.70, 7.95, 5.23 and 4.87 on DB1, DB2, DB3 and DB4 datasets of FVC2004 databases, respectively. Experimental evaluations indicate that the method outperforms in comparison to the current state-of-the-art. Moreover, it has been confirmed from the security analysis that the proposed method fulfills the desired characteristics of diversity, revocability, and non-invertibility with a minor performance degradation caused by the transformation.



### Object-Oriented Dynamics Predictor
- **Arxiv ID**: http://arxiv.org/abs/1806.07371v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1806.07371v3)
- **Published**: 2018-05-25 13:54:36+00:00
- **Updated**: 2018-10-30 05:39:08+00:00
- **Authors**: Guangxiang Zhu, Zhiao Huang, Chongjie Zhang
- **Comment**: Accepted to NIPS 2018
- **Journal**: None
- **Summary**: Generalization has been one of the major challenges for learning dynamics models in model-based reinforcement learning. However, previous work on action-conditioned dynamics prediction focuses on learning the pixel-level motion and thus does not generalize well to novel environments with different object layouts. In this paper, we present a novel object-oriented framework, called object-oriented dynamics predictor (OODP), which decomposes the environment into objects and predicts the dynamics of objects conditioned on both actions and object-to-object relations. It is an end-to-end neural network and can be trained in an unsupervised manner. To enable the generalization ability of dynamics learning, we design a novel CNN-based relation mechanism that is class-specific (rather than object-specific) and exploits the locality principle. Empirical results show that OODP significantly outperforms previous methods in terms of generalization over novel environments with various object layouts. OODP is able to learn from very few environments and accurately predict dynamics in a large number of unseen environments. In addition, OODP learns semantically and visually interpretable dynamics models.



### f-CNN$^{\text{x}}$: A Toolflow for Mapping Multi-CNN Applications on FPGAs
- **Arxiv ID**: http://arxiv.org/abs/1805.10174v2
- **DOI**: 10.1109/FPL.2018.00072
- **Categories**: **cs.CV**, cs.AI, cs.AR
- **Links**: [PDF](http://arxiv.org/pdf/1805.10174v2)
- **Published**: 2018-05-25 14:25:18+00:00
- **Updated**: 2021-06-07 20:16:36+00:00
- **Authors**: Stylianos I. Venieris, Christos-Savvas Bouganis
- **Comment**: Accepted at the 28th International Conference on Field Programmable
  Logic & Applications (FPL) 2018
- **Journal**: None
- **Summary**: The predictive power of Convolutional Neural Networks (CNNs) has been an integral factor for emerging latency-sensitive applications, such as autonomous drones and vehicles. Such systems employ multiple CNNs, each one trained for a particular task. The efficient mapping of multiple CNNs on a single FPGA device is a challenging task as the allocation of compute resources and external memory bandwidth needs to be optimised at design time. This paper proposes f-CNN$^{\text{x}}$, an automated toolflow for the optimised mapping of multiple CNNs on FPGAs, comprising a novel multi-CNN hardware architecture together with an automated design space exploration method that considers the user-specified performance requirements for each model to allocate compute resources and generate a synthesisable accelerator. Moreover, f-CNN$^{\text{x}}$ employs a novel scheduling algorithm that alleviates the limitations of the memory bandwidth contention between CNNs and sustains the high utilisation of the architecture. Experimental evaluation shows that f-CNN$^{\text{x}}$'s designs outperform contention-unaware FPGA mappings by up to 50% and deliver up to 6.8x higher performance-per-Watt over highly optimised GPU designs for multi-CNN systems.



### Pyramid Attention Network for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1805.10180v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.10180v3)
- **Published**: 2018-05-25 14:40:14+00:00
- **Updated**: 2018-11-25 11:46:47+00:00
- **Authors**: Hanchao Li, Pengfei Xiong, Jie An, Lingxue Wang
- **Comment**: None
- **Journal**: None
- **Summary**: A Pyramid Attention Network(PAN) is proposed to exploit the impact of global contextual information in semantic segmentation. Different from most existing works, we combine attention mechanism and spatial pyramid to extract precise dense features for pixel labeling instead of complicated dilated convolution and artificially designed decoder networks. Specifically, we introduce a Feature Pyramid Attention module to perform spatial pyramid attention structure on high-level output and combining global pooling to learn a better feature representation, and a Global Attention Upsample module on each decoder layer to provide global context as a guidance of low-level features to select category localization details. The proposed approach achieves state-of-the-art performance on PASCAL VOC 2012 and Cityscapes benchmarks with a new record of mIoU accuracy 84.0% on PASCAL VOC 2012, while training without COCO dataset.



### Qunatification of Metabolites in MR Spectroscopic Imaging using Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/1805.10201v1
- **DOI**: 10.1007/978-3-319-66179-7_53
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.10201v1)
- **Published**: 2018-05-25 15:34:22+00:00
- **Updated**: 2018-05-25 15:34:22+00:00
- **Authors**: Dhritiman Das, Eduardo Coello, Rolf F Schulte, Bjoern H Menze
- **Comment**: None
- **Journal**: None
- **Summary**: Magnetic Resonance Spectroscopic Imaging (MRSI) is a clinical imaging modality for measuring tissue metabolite levels in-vivo. An accurate estimation of spectral parameters allows for better assessment of spectral quality and metabolite concentration levels. The current gold standard quantification method is the LCModel - a commercial fitting tool. However, this fails for spectra having poor signal-to-noise ratio (SNR) or a large number of artifacts. This paper introduces a framework based on random forest regression for accurate estimation of the output parameters of a model based analysis of MR spectroscopy data. The goal of our proposed framework is to learn the spectral features from a training set comprising of different variations of both simulated and in-vivo brain spectra and then use this learning for the subsequent metabolite quantification. Experiments involve training and testing on simulated and in-vivo human brain spectra. We estimate parameters such as concentration of metabolites and compare our results with that from the LCModel.



### Conditional Generative Adversarial and Convolutional Networks for X-ray Breast Mass Segmentation and Shape Classification
- **Arxiv ID**: http://arxiv.org/abs/1805.10207v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.10207v2)
- **Published**: 2018-05-25 15:44:20+00:00
- **Updated**: 2018-06-10 20:21:52+00:00
- **Authors**: Vivek Kumar Singh, Santiago Romani, Hatem A. Rashwan, Farhan Akram, Nidhi Pandey, Md. Mostafa Kamal Sarker, Jordina Torrents Barrena, Saddam Abdulwahab, Adel Saleh, Miguel Arquez, Meritxell Arenas, Domenec Puig
- **Comment**: 8 pages, Accepted at Medical Image Computing and Computer Assisted
  Intervention (MICCAI) 2018
- **Journal**: None
- **Summary**: This paper proposes a novel approach based on conditional Generative Adversarial Networks (cGAN) for breast mass segmentation in mammography. We hypothesized that the cGAN structure is well-suited to accurately outline the mass area, especially when the training data is limited. The generative network learns intrinsic features of tumors while the adversarial network enforces segmentations to be similar to the ground truth. Experiments performed on dozens of malignant tumors extracted from the public DDSM dataset and from our in-house private dataset confirm our hypothesis with very high Dice coefficient and Jaccard index (>94% and >89%, respectively) outperforming the scores obtained by other state-of-the-art approaches. Furthermore, in order to detect portray significant morphological features of the segmented tumor, a specific Convolutional Neural Network (CNN) have also been designed for classifying the segmented tumor areas into four types (irregular, lobular, oval and round), which provides an overall accuracy about 72% with the DDSM dataset.



### Psychophysics, Gestalts and Games
- **Arxiv ID**: http://arxiv.org/abs/1805.10210v1
- **DOI**: 10.1007/978-3-642-34444-2_6
- **Categories**: **cs.CV**, cs.CG
- **Links**: [PDF](http://arxiv.org/pdf/1805.10210v1)
- **Published**: 2018-05-25 15:48:14+00:00
- **Updated**: 2018-05-25 15:48:14+00:00
- **Authors**: José Lezama, Samy Blusseau, Jean-Michel Morel, Gregory Randall, Rafael Grompone von Gioi
- **Comment**: None
- **Journal**: Giovanna Citti, Alessandro Sarti. Neuromathematics of Vision,
  Springer Berlin Heidelberg, pp.217-242, 2014, Lecture Notes in Morphogenesis
- **Summary**: Many psychophysical studies are dedicated to the evaluation of the human gestalt detection on dot or Gabor patterns, and to model its dependence on the pattern and background parameters. Nevertheless, even for these constrained percepts, psychophysics have not yet reached the challenging prediction stage, where human detection would be quantitatively predicted by a (generic) model. On the other hand, Computer Vision has attempted at defining automatic detection thresholds. This chapter sketches a procedure to confront these two methodologies inspired in gestaltism. Using a computational quantitative version of the non-accidentalness principle, we raise the possibility that the psychophysical and the (older) gestaltist setups, both applicable on dot or Gabor patterns, find a useful complement in a Turing test. In our perceptual Turing test, human performance is compared by the scientist to the detection result given by a computer. This confrontation permits to revive the abandoned method of gestaltic games. We sketch the elaboration of such a game, where the subjects of the experiment are confronted to an alignment detection algorithm, and are invited to draw examples that will fool it. We show that in that way a more precise definition of the alignment gestalt and of its computational formulation seems to emerge. Detection algorithms might also be relevant to more classic psychophysical setups, where they can again play the role of a Turing test. To a visual experiment where subjects were invited to detect alignments in Gabor patterns, we associated a single function measuring the alignment detectability in the form of a number of false alarms (NFA). The first results indicate that the values of the NFA, as a function of all simulation parameters, are highly correlated to the human detection. This fact, that we intend to support by further experiments , might end up confirming that human alignment detection is the result of a single mechanism.



### SLSDeep: Skin Lesion Segmentation Based on Dilated Residual and Pyramid Pooling Networks
- **Arxiv ID**: http://arxiv.org/abs/1805.10241v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.10241v2)
- **Published**: 2018-05-25 16:38:50+00:00
- **Updated**: 2018-05-31 00:31:18+00:00
- **Authors**: Md. Mostafa Kamal Sarker, Hatem A. Rashwan, Farhan Akram, Syeda Furruka Banu, Adel Saleh, Vivek Kumar Singh, Forhad U H Chowdhury, Saddam Abdulwahab, Santiago Romani, Petia Radeva, Domenec Puig
- **Comment**: Accepted in MICCAI 2018, 9 pages
- **Journal**: None
- **Summary**: Skin lesion segmentation (SLS) in dermoscopic images is a crucial task for automated diagnosis of melanoma. In this paper, we present a robust deep learning SLS model, so-called SLSDeep, which is represented as an encoder-decoder network. The encoder network is constructed by dilated residual layers, in turn, a pyramid pooling network followed by three convolution layers is used for the decoder. Unlike the traditional methods employing a cross-entropy loss, we investigated a loss function by combining both Negative Log Likelihood (NLL) and End Point Error (EPE) to accurately segment the melanoma regions with sharp boundaries. The robustness of the proposed model was evaluated on two public databases: ISBI 2016 and 2017 for skin lesion analysis towards melanoma detection challenge. The proposed model outperforms the state-of-the-art methods in terms of segmentation accuracy. Moreover, it is capable to segment more than $100$ images of size 384x384 per second on a recent GPU.



### Few-Shot Segmentation Propagation with Guided Networks
- **Arxiv ID**: http://arxiv.org/abs/1806.07373v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.07373v1)
- **Published**: 2018-05-25 17:02:03+00:00
- **Updated**: 2018-05-25 17:02:03+00:00
- **Authors**: Kate Rakelly, Evan Shelhamer, Trevor Darrell, Alexei A. Efros, Sergey Levine
- **Comment**: None
- **Journal**: None
- **Summary**: Learning-based methods for visual segmentation have made progress on particular types of segmentation tasks, but are limited by the necessary supervision, the narrow definitions of fixed tasks, and the lack of control during inference for correcting errors. To remedy the rigidity and annotation burden of standard approaches, we address the problem of few-shot segmentation: given few image and few pixel supervision, segment any images accordingly. We propose guided networks, which extract a latent task representation from any amount of supervision, and optimize our architecture end-to-end for fast, accurate few-shot segmentation. Our method can switch tasks without further optimization and quickly update when given more guidance. We report the first results for segmentation from one pixel per concept and show real-time interactive video segmentation. Our unified approach propagates pixel annotations across space for interactive segmentation, across time for video segmentation, and across scenes for semantic segmentation. Our guided segmentor is state-of-the-art in accuracy for the amount of annotation and time. See http://github.com/shelhamer/revolver for code, models, and more details.



### Intrinsic Image Transformation via Scale Space Decomposition
- **Arxiv ID**: http://arxiv.org/abs/1805.10253v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.10253v1)
- **Published**: 2018-05-25 17:11:23+00:00
- **Updated**: 2018-05-25 17:11:23+00:00
- **Authors**: Lechao Cheng, Chengyi Zhang, Zicheng Liao
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: We introduce a new network structure for decomposing an image into its intrinsic albedo and shading. We treat this as an image-to-image transformation problem and explore the scale space of the input and output. By expanding the output images (albedo and shading) into their Laplacian pyramid components, we develop a multi-channel network structure that learns the image-to-image transformation function in successive frequency bands in parallel, within each channel is a fully convolutional neural network with skip connections. This network structure is general and extensible, and has demonstrated excellent performance on the intrinsic image decomposition problem. We evaluate the network on two benchmark datasets: the MPI-Sintel dataset and the MIT Intrinsic Images dataset. Both quantitative and qualitative results show our model delivers a clear progression over state-of-the-art.



### Parallel Architecture and Hyperparameter Search via Successive Halving and Classification
- **Arxiv ID**: http://arxiv.org/abs/1805.10255v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1805.10255v1)
- **Published**: 2018-05-25 17:12:38+00:00
- **Updated**: 2018-05-25 17:12:38+00:00
- **Authors**: Manoj Kumar, George E. Dahl, Vijay Vasudevan, Mohammad Norouzi
- **Comment**: None
- **Journal**: None
- **Summary**: We present a simple and powerful algorithm for parallel black box optimization called Successive Halving and Classification (SHAC). The algorithm operates in $K$ stages of parallel function evaluations and trains a cascade of binary classifiers to iteratively cull the undesirable regions of the search space. SHAC is easy to implement, requires no tuning of its own configuration parameters, is invariant to the scale of the objective function and can be built using any choice of binary classifier. We adopt tree-based classifiers within SHAC and achieve competitive performance against several strong baselines for optimizing synthetic functions, hyperparameters and architectures.



### Unsupervised Learning for Large-Scale Fiber Detection and Tracking in Microscopic Material Images
- **Arxiv ID**: http://arxiv.org/abs/1805.10256v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.10256v1)
- **Published**: 2018-05-25 17:14:27+00:00
- **Updated**: 2018-05-25 17:14:27+00:00
- **Authors**: Hongkai Yu, Dazhou Guo, Zhipeng Yan, Wei Liu, Jeff Simmons, Craig P. Przybyla, Song Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Constructing 3D structures from serial section data is a long standing problem in microscopy. The structure of a fiber reinforced composite material can be reconstructed using a tracking-by-detection model. Tracking-by-detection algorithms rely heavily on detection accuracy, especially the recall performance. The state-of-the-art fiber detection algorithms perform well under ideal conditions, but are not accurate where there are local degradations of image quality, due to contaminants on the material surface and/or defocus blur. Convolutional Neural Networks (CNN) could be used for this problem, but would require a large number of manual annotated fibers, which are not available. We propose an unsupervised learning method to accurately detect fibers on the large scale, that is robust against local degradations of image quality. The proposed method does not require manual annotations, but uses fiber shape/size priors and spatio-temporal consistency in tracking to simulate the supervision in the training of the CNN. Experiments show significant improvements over state-of-the-art fiber detection algorithms together with advanced tracking performance.



### Towards a glaucoma risk index based on simulated hemodynamics from fundus images
- **Arxiv ID**: http://arxiv.org/abs/1805.10273v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.10273v4)
- **Published**: 2018-05-25 17:46:49+00:00
- **Updated**: 2018-10-25 08:22:33+00:00
- **Authors**: José Ignacio Orlando, João Barbosa Breda, Karel van Keer, Matthew B. Blaschko, Pablo J. Blanco, Carlos A. Bulant
- **Comment**: MICCAI 2018 paper
- **Journal**: None
- **Summary**: Glaucoma is the leading cause of irreversible but preventable blindness in the world. Its major treatable risk factor is the intra-ocular pressure, although other biomarkers are being explored to improve the understanding of the pathophysiology of the disease. It has been recently observed that glaucoma induces changes in the ocular hemodynamics. However, its effects on the functional behavior of the retinal arterioles have not been studied yet. In this paper we propose a first approach for characterizing those changes using computational hemodynamics. The retinal blood flow is simulated using a 0D model for a steady, incompressible non Newtonian fluid in rigid domains. The simulation is performed on patient-specific arterial trees extracted from fundus images. We also propose a novel feature representation technique to comprise the outcomes of the simulation stage into a fixed length feature vector that can be used for classification studies. Our experiments on a new database of fundus images show that our approach is able to capture representative changes in the hemodynamics of glaucomatous patients. Code and data are publicly available in https://ignaciorlando.github.io.



### Pathology Segmentation using Distributional Differences to Images of Healthy Origin
- **Arxiv ID**: http://arxiv.org/abs/1805.10344v2
- **DOI**: 10.1007/978-3-030-11723-8_23
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.10344v2)
- **Published**: 2018-05-25 19:42:47+00:00
- **Updated**: 2019-08-21 08:28:24+00:00
- **Authors**: Simon Andermatt, Antal Horváth, Simon Pezold, Philippe Cattin
- **Comment**: None
- **Journal**: In International MICCAI Brainlesion Workshop, pp. 228-238.
  Springer, Cham, 2018
- **Summary**: Fully supervised segmentation methods require a large training cohort of already segmented images, providing information at the pixel level of each image. We present a method to automatically segment and model pathologies in medical images, trained solely on data labelled on the image level as either healthy or containing a visual defect. We base our method on CycleGAN, an image-to-image translation technique, to translate images between the domains of healthy and pathological images. We extend the core idea with two key contributions. Implementing the generators as residual generators allows us to explicitly model the segmentation of the pathology. Realizing the translation from the healthy to the pathological domain using a variational autoencoder allows us to specify one representation of the pathology, as this transformation is otherwise not unique. Our model hence not only allows us to create pixelwise semantic segmentations, it is also able to create inpaintings for the segmentations to render the pathological image healthy. Furthermore, we can draw new unseen pathology samples from this model based on the distribution in the data. We show quantitatively, that our method is able to segment pathologies with a surprising accuracy being only slightly inferior to a state-of-the-art fully supervised method, although the latter has per-pixel rather than per-image training information. Moreover, we show qualitative results of both the segmentations and inpaintings. Our findings motivate further research into weakly-supervised segmentation using image level annotations, allowing for faster and cheaper acquisition of training data without a large sacrifice in segmentation accuracy.



### What Face and Body Shapes Can Tell About Height
- **Arxiv ID**: http://arxiv.org/abs/1805.10355v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.10355v1)
- **Published**: 2018-05-25 20:25:02+00:00
- **Updated**: 2018-05-25 20:25:02+00:00
- **Authors**: Semih Günel, Helge Rhodin, Pascal Fua
- **Comment**: None
- **Journal**: None
- **Summary**: Recovering a person's height from a single image is important for virtual garment fitting, autonomous driving and surveillance, however, it is also very challenging due to the absence of absolute scale information. We tackle the rarely addressed case, where camera parameters and scene geometry is unknown. To nevertheless resolve the inherent scale ambiguity, we infer height from statistics that are intrinsic to human anatomy and can be estimated from images directly, such as articulated pose, bone length proportions, and facial features. Our contribution is twofold. First, we experiment with different machine learning models to capture the relation between image content and human height. Second, we show that performance is predominantly limited by dataset size and create a new dataset that is three magnitudes larger, by mining explicit height labels and propagating them to additional images through face recognition and assignment consistency. Our evaluation shows that monocular height estimation is possible with a MAE of 5.56cm.



### Heterogeneous Bitwidth Binarization in Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1805.10368v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.10368v2)
- **Published**: 2018-05-25 21:21:32+00:00
- **Updated**: 2018-10-31 19:35:07+00:00
- **Authors**: Josh Fromm, Shwetak Patel, Matthai Philipose
- **Comment**: NIPS 2018 camera ready update
- **Journal**: None
- **Summary**: Recent work has shown that fast, compact low-bitwidth neural networks can be surprisingly accurate. These networks use homogeneous binarization: all parameters in each layer or (more commonly) the whole model have the same low bitwidth (e.g., 2 bits). However, modern hardware allows efficient designs where each arithmetic instruction can have a custom bitwidth, motivating heterogeneous binarization, where every parameter in the network may have a different bitwidth. In this paper, we show that it is feasible and useful to select bitwidths at the parameter granularity during training. For instance a heterogeneously quantized version of modern networks such as AlexNet and MobileNet, with the right mix of 1-, 2- and 3-bit parameters that average to just 1.4 bits can equal the accuracy of homogeneous 2-bit versions of these networks. Further, we provide analyses to show that the heterogeneously binarized systems yield FPGA- and ASIC-based implementations that are correspondingly more efficient in both circuit area and energy efficiency than their homogeneous counterparts.



### Less is More: Simultaneous View Classification and Landmark Detection for Abdominal Ultrasound Images
- **Arxiv ID**: http://arxiv.org/abs/1805.10376v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.10376v2)
- **Published**: 2018-05-25 21:55:09+00:00
- **Updated**: 2018-06-04 19:17:02+00:00
- **Authors**: Zhoubing Xu, Yuankai Huo, JinHyeong Park, Bennett Landman, Andy Milkowski, Sasa Grbic, Shaohua Zhou
- **Comment**: Accepted to MICCAI 2018
- **Journal**: None
- **Summary**: An abdominal ultrasound examination, which is the most common ultrasound examination, requires substantial manual efforts to acquire standard abdominal organ views, annotate the views in texts, and record clinically relevant organ measurements. Hence, automatic view classification and landmark detection of the organs can be instrumental to streamline the examination workflow. However, this is a challenging problem given not only the inherent difficulties from the ultrasound modality, e.g., low contrast and large variations, but also the heterogeneity across tasks, i.e., one classification task for all views, and then one landmark detection task for each relevant view. While convolutional neural networks (CNN) have demonstrated more promising outcomes on ultrasound image analytics than traditional machine learning approaches, it becomes impractical to deploy multiple networks (one for each task) due to the limited computational and memory resources on most existing ultrasound scanners. To overcome such limits, we propose a multi-task learning framework to handle all the tasks by a single network. This network is integrated to perform view classification and landmark detection simultaneously; it is also equipped with global convolutional kernels, coordinate constraints, and a conditional adversarial module to leverage the performances. In an experimental study based on 187,219 ultrasound images, with the proposed simplified approach we achieve (1) view classification accuracy better than the agreement between two clinical experts and (2) landmark-based measurement errors on par with inter-user variability. The multi-task approach also benefits from sharing the feature extraction during the training process across all tasks and, as a result, outperforms the approaches that address each task individually.



### Large-scale Distance Metric Learning with Uncertainty
- **Arxiv ID**: http://arxiv.org/abs/1805.10384v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.10384v1)
- **Published**: 2018-05-25 22:44:59+00:00
- **Updated**: 2018-05-25 22:44:59+00:00
- **Authors**: Qi Qian, Jiasheng Tang, Hao Li, Shenghuo Zhu, Rong Jin
- **Comment**: accepted by CVPR'18
- **Journal**: None
- **Summary**: Distance metric learning (DML) has been studied extensively in the past decades for its superior performance with distance-based algorithms. Most of the existing methods propose to learn a distance metric with pairwise or triplet constraints. However, the number of constraints is quadratic or even cubic in the number of the original examples, which makes it challenging for DML to handle the large-scale data set. Besides, the real-world data may contain various uncertainty, especially for the image data. The uncertainty can mislead the learning procedure and cause the performance degradation. By investigating the image data, we find that the original data can be observed from a small set of clean latent examples with different distortions. In this work, we propose the margin preserving metric learning framework to learn the distance metric and latent examples simultaneously. By leveraging the ideal properties of latent examples, the training efficiency can be improved significantly while the learned metric also becomes robust to the uncertainty in the original data. Furthermore, we can show that the metric is learned from latent examples only, but it can preserve the large margin property even for the original data. The empirical study on the benchmark image data sets demonstrates the efficacy and efficiency of the proposed method.



### Three-Dimensional Radiotherapy Dose Prediction on Head and Neck Cancer Patients with a Hierarchically Densely Connected U-net Deep Learning Architecture
- **Arxiv ID**: http://arxiv.org/abs/1805.10397v3
- **DOI**: 10.1088/1361-6560/ab039b
- **Categories**: **physics.med-ph**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1805.10397v3)
- **Published**: 2018-05-25 23:40:32+00:00
- **Updated**: 2019-02-04 20:46:44+00:00
- **Authors**: Dan Nguyen, Xun Jia, David Sher, Mu-Han Lin, Zohaib Iqbal, Hui Liu, Steve Jiang
- **Comment**: None
- **Journal**: None
- **Summary**: The treatment planning process for patients with head and neck (H&N) cancer is regarded as one of the most complicated due to large target volume, multiple prescription dose levels, and many radiation-sensitive critical structures near the target. Treatment planning for this site requires a high level of human expertise and a tremendous amount of effort to produce personalized high quality plans, taking as long as a week, which deteriorates the chances of tumor control and patient survival. To solve this problem, we propose to investigate a deep learning-based dose prediction model, Hierarchically Densely Connected U-net, based on two highly popular network architectures: U-net and DenseNet. We find that this new architecture is able to accurately and efficiently predict the dose distribution, outperforming the other two models, the Standard U-net and DenseNet, in homogeneity, dose conformity, and dose coverage on the test data. Averaging across all organs at risk, our proposed model is capable of predicting the organ-at-risk max dose within 6.3% and mean dose within 5.1% of the prescription dose on the test data. The other models, the Standard U-net and DenseNet, performed worse, having an averaged organ-at-risk max dose prediction error of 8.2% and 9.3%, respectively, and averaged mean dose prediction error of 6.4% and 6.8%, respectively. In addition, our proposed model used 12 times less trainable parameters than the Standard U-net, and predicted the patient dose 4 times faster than DenseNet.



