# Arxiv Papers in cs.CV on 2018-05-12
### Constrained-CNN losses for weakly supervised segmentation
- **Arxiv ID**: http://arxiv.org/abs/1805.04628v2
- **DOI**: 10.1016/j.media.2019.02.009
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.04628v2)
- **Published**: 2018-05-12 00:51:54+00:00
- **Updated**: 2019-02-08 20:06:55+00:00
- **Authors**: Hoel Kervadec, Jose Dolz, Meng Tang, Eric Granger, Yuri Boykov, Ismail Ben Ayed
- **Comment**: Extended work of the work presented at the 1st conference on Medical
  Image with Deep Learning (MIDL). Currently under review at Medical Image
  Analysis
- **Journal**: None
- **Summary**: Weakly-supervised learning based on, e.g., partially labelled images or image-tags, is currently attracting significant attention in CNN segmentation as it can mitigate the need for full and laborious pixel/voxel annotations. Enforcing high-order (global) inequality constraints on the network output (for instance, to constrain the size of the target region) can leverage unlabeled data, guiding the training process with domain-specific knowledge. Inequality constraints are very flexible because they do not assume exact prior knowledge. However, constrained Lagrangian dual optimization has been largely avoided in deep networks, mainly for computational tractability reasons. To the best of our knowledge, the method of [Pathak et al., 2015] is the only prior work that addresses deep CNNs with linear constraints in weakly supervised segmentation. It uses the constraints to synthesize fully-labeled training masks (proposals) from weak labels, mimicking full supervision and facilitating dual optimization. We propose to introduce a differentiable penalty, which enforces inequality constraints directly in the loss function, avoiding expensive Lagrangian dual iterates and proposal generation. From constrained-optimization perspective, our simple penalty-based approach is not optimal as there is no guarantee that the constraints are satisfied. However, surprisingly, it yields substantially better results than the Lagrangian-based constrained CNNs in [Pathak et al., 2015], while reducing the computational demand for training. By annotating only a small fraction of the pixels, the proposed approach can reach a level of segmentation performance that is comparable to full supervision on three separate tasks. While our experiments focused on basic linear constraints such as the target-region size and image tags, our framework can be easily extended to other non-linear constraints.



### Image-derived generative modeling of pseudo-macromolecular structures - towards the statistical assessment of Electron CryoTomography template matching
- **Arxiv ID**: http://arxiv.org/abs/1805.04634v1
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV, stat.AP, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.04634v1)
- **Published**: 2018-05-12 02:00:30+00:00
- **Updated**: 2018-05-12 02:00:30+00:00
- **Authors**: Kai Wen Wang, Xiangrui Zeng, Xiaodan Liang, Zhiguang Huo, Eric P. Xing, Min Xu
- **Comment**: None
- **Journal**: British Machine Vision Conference (BMVC) 2018
- **Summary**: Cellular Electron CryoTomography (CECT) is a 3D imaging technique that captures information about the structure and spatial organization of macromolecular complexes within single cells, in near-native state and at sub-molecular resolution. Although template matching is often used to locate macromolecules in a CECT image, it is insufficient as it only measures the relative structural similarity. Therefore, it is preferable to assess the statistical credibility of the decision through hypothesis testing, requiring many templates derived from a diverse population of macromolecular structures. Due to the very limited number of known structures, we need a generative model to efficiently and reliably sample pseudo-structures from the complex distribution of macromolecular structures. To address this challenge, we propose a novel image-derived approach for performing hypothesis testing for template matching by constructing generative models using the generative adversarial network. Finally, we conducted hypothesis testing experiments for template matching on both simulated and experimental subtomograms, allowing us to conclude the identity of subtomograms with high statistical credibility and significantly reducing false positives.



### Direction-aware Spatial Context Features for Shadow Detection and Removal
- **Arxiv ID**: http://arxiv.org/abs/1805.04635v2
- **DOI**: 10.1109/TPAMI.2019.2919616
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.04635v2)
- **Published**: 2018-05-12 02:15:42+00:00
- **Updated**: 2019-05-25 05:38:18+00:00
- **Authors**: Xiaowei Hu, Chi-Wing Fu, Lei Zhu, Jing Qin, Pheng-Ann Heng
- **Comment**: Accepted to IEEE Transactions on Pattern Analysis and Machine
  Intelligence (TPAMI). This is the journal version of arXiv:1712.04142, which
  was accepted for oral presentation in CVPR 2018
- **Journal**: None
- **Summary**: Shadow detection and shadow removal are fundamental and challenging tasks, requiring an understanding of the global image semantics. This paper presents a novel deep neural network design for shadow detection and removal by analyzing the spatial image context in a direction-aware manner. To achieve this, we first formulate the direction-aware attention mechanism in a spatial recurrent neural network (RNN) by introducing attention weights when aggregating spatial context features in the RNN. By learning these weights through training, we can recover direction-aware spatial context (DSC) for detecting and removing shadows. This design is developed into the DSC module and embedded in a convolutional neural network (CNN) to learn the DSC features at different levels. Moreover, we design a weighted cross entropy loss to make effective the training for shadow detection and further adopt the network for shadow removal by using a Euclidean loss function and formulating a color transfer function to address the color and luminosity inconsistencies in the training pairs. We employed two shadow detection benchmark datasets and two shadow removal benchmark datasets, and performed various experiments to evaluate our method. Experimental results show that our method performs favorably against the state-of-the-art methods for both shadow detection and shadow removal.



### Fast Symbolic 3D Registration Solution
- **Arxiv ID**: http://arxiv.org/abs/1805.08703v3
- **DOI**: 10.1109/TASE.2019.2942324
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1805.08703v3)
- **Published**: 2018-05-12 05:05:06+00:00
- **Updated**: 2019-09-17 07:47:24+00:00
- **Authors**: Jin Wu, Ming Liu, Zebo Zhou, Rui Li
- **Comment**: 10 pages, 7 figures, 5 tables
- **Journal**: IEEE Transactions on Automation Science and Engineering, 2019
- **Summary**: 3D registration has always been performed invoking singular value decomposition (SVD) or eigenvalue decomposition (EIG) in real engineering practices. However, these numerical algorithms suffer from uncertainty of convergence in many cases. A novel fast symbolic solution is proposed in this paper by following our recent publication in this journal. The equivalence analysis shows that our previous solver can be converted to deal with the 3D registration problem. Rather, the computation procedure is studied for further simplification of computing without complex-number support. Experimental results show that the proposed solver does not loose accuracy and robustness but improves the execution speed to a large extent by almost %50 to %80, on both personal computer and embedded processor.



### I Have Seen Enough: A Teacher Student Network for Video Classification Using Fewer Frames
- **Arxiv ID**: http://arxiv.org/abs/1805.04668v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.04668v1)
- **Published**: 2018-05-12 06:22:54+00:00
- **Updated**: 2018-05-12 06:22:54+00:00
- **Authors**: Shweta Bhardwaj, Mitesh M. Khapra
- **Comment**: CVPR Workshop on Brave New Ideas for Video Understanding (BIVU)
- **Journal**: None
- **Summary**: Over the past few years, various tasks involving videos such as classification, description, summarization and question answering have received a lot of attention. Current models for these tasks compute an encoding of the video by treating it as a sequence of images and going over every image in the sequence. However, for longer videos this is very time consuming. In this paper, we focus on the task of video classification and aim to reduce the computational time by using the idea of distillation. Specifically, we first train a teacher network which looks at all the frames in a video and computes a representation for the video. We then train a student network whose objective is to process only a small fraction of the frames in the video and still produce a representation which is very close to the representation computed by the teacher network. This smaller student network involving fewer computations can then be employed at inference time for video classification. We experiment with the YouTube-8M dataset and show that the proposed student network can reduce the inference time by upto 30% with a very small drop in the performance



### BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning
- **Arxiv ID**: http://arxiv.org/abs/1805.04687v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.04687v2)
- **Published**: 2018-05-12 09:24:21+00:00
- **Updated**: 2020-04-08 09:25:06+00:00
- **Authors**: Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingying Chen, Fangchen Liu, Vashisht Madhavan, Trevor Darrell
- **Comment**: Published at IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR) 2020
- **Journal**: None
- **Summary**: Datasets drive vision progress, yet existing driving datasets are impoverished in terms of visual content and supported tasks to study multitask learning for autonomous driving. Researchers are usually constrained to study a small set of problems on one dataset, while real-world computer vision applications require performing tasks of various complexities. We construct BDD100K, the largest driving video dataset with 100K videos and 10 tasks to evaluate the exciting progress of image recognition algorithms on autonomous driving. The dataset possesses geographic, environmental, and weather diversity, which is useful for training models that are less likely to be surprised by new conditions. Based on this diverse dataset, we build a benchmark for heterogeneous multitask learning and study how to solve the tasks together. Our experiments show that special training strategies are needed for existing models to perform such heterogeneous tasks. BDD100K opens the door for future studies in this important venue.



### Exploring object-centric and scene-centric CNN features and their complementarity for human rights violations recognition in images
- **Arxiv ID**: http://arxiv.org/abs/1805.04714v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.04714v1)
- **Published**: 2018-05-12 12:50:03+00:00
- **Updated**: 2018-05-12 12:50:03+00:00
- **Authors**: Grigorios Kalliatakis, Shoaib Ehsan, Ales Leonardis, Klaus McDonald-Maier
- **Comment**: 19 pages, 13 figures; Submitted to PLOS ONE
- **Journal**: None
- **Summary**: Identifying potential abuses of human rights through imagery is a novel and challenging task in the field of computer vision, that will enable to expose human rights violations over large-scale data that may otherwise be impossible. While standard databases for object and scene categorisation contain hundreds of different classes, the largest available dataset of human rights violations contains only 4 classes. Here, we introduce the `Human Rights Archive Database' (HRA), a verified-by-experts repository of 3050 human rights violations photographs, labelled with human rights semantic categories, comprising a list of the types of human rights abuses encountered at present. With the HRA dataset and a two-phase transfer learning scheme, we fine-tuned the state-of-the-art deep convolutional neural networks (CNNs) to provide human rights violations classification CNNs (HRA-CNNs). We also present extensive experiments refined to evaluate how well object-centric and scene-centric CNN features can be combined for the task of recognising human rights abuses. With this, we show that HRA database poses a challenge at a higher level for the well studied representation learning methods, and provide a benchmark in the task of human rights violations recognition in visual context. We expect this dataset can help to open up new horizons on creating systems able of recognising rich information about human rights violations. Our dataset, codes and trained models are available online at https://github.com/GKalliatakis/Human-Rights-Archive-CNNs.



### Learning to Find Eye Region Landmarks for Remote Gaze Estimation in Unconstrained Settings
- **Arxiv ID**: http://arxiv.org/abs/1805.04771v1
- **DOI**: 10.1145/3204493.3204545
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.04771v1)
- **Published**: 2018-05-12 19:50:32+00:00
- **Updated**: 2018-05-12 19:50:32+00:00
- **Authors**: Seonwook Park, Xucong Zhang, Andreas Bulling, Otmar Hilliges
- **Comment**: None
- **Journal**: None
- **Summary**: Conventional feature-based and model-based gaze estimation methods have proven to perform well in settings with controlled illumination and specialized cameras. In unconstrained real-world settings, however, such methods are surpassed by recent appearance-based methods due to difficulties in modeling factors such as illumination changes and other visual artifacts. We present a novel learning-based method for eye region landmark localization that enables conventional methods to be competitive to latest appearance-based methods. Despite having been trained exclusively on synthetic data, our method exceeds the state of the art for iris localization and eye shape registration on real-world imagery. We then use the detected landmarks as input to iterative model-fitting and lightweight learning-based gaze estimation methods. Our approach outperforms existing model-fitting and appearance-based methods in the context of person-independent and personalized gaze estimation.



### Convolutional CRFs for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1805.04777v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.04777v2)
- **Published**: 2018-05-12 20:37:27+00:00
- **Updated**: 2018-05-15 12:36:12+00:00
- **Authors**: Marvin T. T. Teichmann, Roberto Cipolla
- **Comment**: 8 Pages + Appendix, references. Code can be found under:
  https://github.com/MarvinTeichmann/ConvCRF
- **Journal**: None
- **Summary**: For the challenging semantic image segmentation task the most efficient models have traditionally combined the structured modelling capabilities of Conditional Random Fields (CRFs) with the feature extraction power of CNNs. In more recent works however, CRF post-processing has fallen out of favour. We argue that this is mainly due to the slow training and inference speeds of CRFs, as well as the difficulty of learning the internal CRF parameters. To overcome both issues we propose to add the assumption of conditional independence to the framework of fully-connected CRFs. This allows us to reformulate the inference in terms of convolutions, which can be implemented highly efficiently on GPUs. Doing so speeds up inference and training by a factor of more then 100. All parameters of the convolutional CRFs can easily be optimized using backpropagation. To facilitating further CRF research we make our implementation publicly available. Please visit: https://github.com/MarvinTeichmann/ConvCRF



### Scene-Aware Audio for 360\textdegree{} Videos
- **Arxiv ID**: http://arxiv.org/abs/1805.04792v1
- **DOI**: 10.1145/3197517.3201391
- **Categories**: **cs.GR**, cs.CV, cs.ET, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1805.04792v1)
- **Published**: 2018-05-12 22:06:04+00:00
- **Updated**: 2018-05-12 22:06:04+00:00
- **Authors**: Dingzeyu Li, Timothy R. Langlois, Changxi Zheng
- **Comment**: SIGGRAPH 2018, Technical Papers, 12 pages, 17 figures,
  http://www.cs.columbia.edu/cg/360audio/
- **Journal**: None
- **Summary**: Although 360\textdegree{} cameras ease the capture of panoramic footage, it remains challenging to add realistic 360\textdegree{} audio that blends into the captured scene and is synchronized with the camera motion. We present a method for adding scene-aware spatial audio to 360\textdegree{} videos in typical indoor scenes, using only a conventional mono-channel microphone and a speaker. We observe that the late reverberation of a room's impulse response is usually diffuse spatially and directionally. Exploiting this fact, we propose a method that synthesizes the directional impulse response between any source and listening locations by combining a synthesized early reverberation part and a measured late reverberation tail. The early reverberation is simulated using a geometric acoustic simulation and then enhanced using a frequency modulation method to capture room resonances. The late reverberation is extracted from a recorded impulse response, with a carefully chosen time duration that separates out the late reverberation from the early reverberation. In our validations, we show that our synthesized spatial audio matches closely with recordings using ambisonic microphones. Lastly, we demonstrate the strength of our method in several applications.



