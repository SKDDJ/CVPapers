# Arxiv Papers in cs.CV on 2018-05-20
### Two-Stream Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1805.07694v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.07694v3)
- **Published**: 2018-05-20 02:48:38+00:00
- **Updated**: 2019-07-10 03:09:49+00:00
- **Authors**: Lei Shi, Yifan Zhang, Jian Cheng, Hanqing Lu
- **Comment**: None
- **Journal**: The IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR), 2019, pp. 12026-12035
- **Summary**: In skeleton-based action recognition, graph convolutional networks (GCNs), which model the human body skeletons as spatiotemporal graphs, have achieved remarkable performance. However, in existing GCN-based methods, the topology of the graph is set manually, and it is fixed over all layers and input samples. This may not be optimal for the hierarchical GCN and diverse samples in action recognition tasks. In addition, the second-order information (the lengths and directions of bones) of the skeleton data, which is naturally more informative and discriminative for action recognition, is rarely investigated in existing methods. In this work, we propose a novel two-stream adaptive graph convolutional network (2s-AGCN) for skeleton-based action recognition. The topology of the graph in our model can be either uniformly or individually learned by the BP algorithm in an end-to-end manner. This data-driven method increases the flexibility of the model for graph construction and brings more generality to adapt to various data samples. Moreover, a two-stream framework is proposed to model both the first-order and the second-order information simultaneously, which shows notable improvement for the recognition accuracy. Extensive experiments on the two large-scale datasets, NTU-RGBD and Kinetics-Skeleton, demonstrate that the performance of our model exceeds the state-of-the-art with a significant margin.



### RGB-Depth SLAM Review
- **Arxiv ID**: http://arxiv.org/abs/1805.07696v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.07696v1)
- **Published**: 2018-05-20 03:20:38+00:00
- **Updated**: 2018-05-20 03:20:38+00:00
- **Authors**: Redhwan Jamiruddin, Ali Osman Sari, Jahanzaib Shabbir, Tarique Anwer
- **Comment**: None
- **Journal**: None
- **Summary**: Simultaneous Localization and Mapping (SLAM) have made the real-time dense reconstruction possible increasing the prospects of navigation, tracking, and augmented reality problems. Some breakthroughs have been achieved in this regard during past few decades and more remarkable works are still going on. This paper presents an overview of SLAM approaches that have been developed till now. Kinect Fusion algorithm, its variants, and further developed approaches are discussed in detailed. The algorithms and approaches are compared for their effectiveness in tracking and mapping based on Root Mean Square error over online available datasets.



### Density-Adaptive Kernel based Efficient Reranking Approaches for Person Reidentification
- **Arxiv ID**: http://arxiv.org/abs/1805.07698v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.07698v3)
- **Published**: 2018-05-20 03:44:09+00:00
- **Updated**: 2020-05-20 12:43:54+00:00
- **Authors**: Ruo-Pei Guo, Chun-Guang Li, Yonghua Li, Jiaru Lin, Jun Guo
- **Comment**: 39 pages, 18 figures and 12 tables. This paper is an extended version
  of our preliminary work on ICPR 2018
- **Journal**: None
- **Summary**: Person reidentification (ReID) refers to the task of verifying the identity of a pedestrian observed from nonoverlapping views in a surveillance camera network. It has recently been validated that reranking can achieve remarkable performance improvements in person ReID systems. However, current reranking approaches either require feedback from users or suffer from burdensome computational costs. In this paper, we propose to exploit a density-adaptive smooth kernel technique to achieve efficient and effective reranking. Specifically, we adopt a smooth kernel function to formulate the neighbor relationships among data samples with a density-adaptive parameter. Based on this new formulation, we present two simple yet effective reranking methods, termed \emph{inverse} density-adaptive kernel based reranking (inv-DAKR) and \emph{bidirectional} density-adaptive kernel based reranking (bi-DAKR), in which the local density information in the vicinity of each gallery sample is elegantly exploited. Moreover, we extend the proposed inv-DAKR and bi-DAKR methods to incorporate the available extra probe samples and demonstrate that when and why these extra probe samples are able to improve the local neighborhood and thus further refine the ranking results. Extensive experiments are conducted on six benchmark datasets, including: PRID450s, VIPeR, CUHK03, GRID, Market-1501 and Mars. The experimental results demonstrate that our proposals are effective and efficient.



### Object Localization with a Weakly Supervised CapsNet
- **Arxiv ID**: http://arxiv.org/abs/1805.07706v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.07706v3)
- **Published**: 2018-05-20 05:07:27+00:00
- **Updated**: 2019-12-02 18:27:17+00:00
- **Authors**: Weitang Liu, Emad Barsoum, John D. Owens
- **Comment**: None
- **Journal**: None
- **Summary**: Inspired by CapsNet's routing-by-agreement mechanism with its ability to learn object properties, we explore if those properties in turn can determine new properties of the objects, such as the locations. We then propose a CapsNet architecture with object coordinate atoms and a modified routing-by-agreement algorithm with unevenly distributed initial routing probabilities. The model is based on CapsNet but uses a routing algorithm to find the objects' approximate positions in the image coordinate system. We also discussed how to derive the property of translation through coordinate atoms and we show the importance of sparse representation. We train our model on the single moving MNIST dataset with class labels. Our model can learn and derive the coordinates of the digits better than its convolution counterpart that lacks a routing-by-agreement algorithm, and can also perform well when testing on the multi-digit moving MNIST and KTH datasets. The results show our method reaches the state-of-art performance on object localization without any extra localization techniques and modules as in prior work.



### Dynamically Unfolding Recurrent Restorer: A Moving Endpoint Control Method for Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/1805.07709v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.07709v2)
- **Published**: 2018-05-20 05:18:28+00:00
- **Updated**: 2018-10-10 16:12:37+00:00
- **Authors**: Xiaoshuai Zhang, Yiping Lu, Jiaying Liu, Bin Dong
- **Comment**: The first two authors contributed equally
- **Journal**: None
- **Summary**: In this paper, we propose a new control framework called the moving endpoint control to restore images corrupted by different degradation levels in one model. The proposed control problem contains a restoration dynamics which is modeled by an RNN. The moving endpoint, which is essentially the terminal time of the associated dynamics, is determined by a policy network. We call the proposed model the dynamically unfolding recurrent restorer (DURR). Numerical experiments show that DURR is able to achieve state-of-the-art performances on blind image denoising and JPEG image deblocking. Furthermore, DURR can well generalize to images with higher degradation levels that are not included in the training stage.



### Wavelet Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1805.08620v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1805.08620v1)
- **Published**: 2018-05-20 07:18:54+00:00
- **Updated**: 2018-05-20 07:18:54+00:00
- **Authors**: Shin Fujieda, Kohei Takayama, Toshiya Hachisuka
- **Comment**: 10 pages, 7 figures, 5 tables. arXiv admin note: substantial text
  overlap with arXiv:1707.07394
- **Journal**: None
- **Summary**: Spatial and spectral approaches are two major approaches for image processing tasks such as image classification and object recognition. Among many such algorithms, convolutional neural networks (CNNs) have recently achieved significant performance improvement in many challenging tasks. Since CNNs process images directly in the spatial domain, they are essentially spatial approaches. Given that spatial and spectral approaches are known to have different characteristics, it will be interesting to incorporate a spectral approach into CNNs. We propose a novel CNN architecture, wavelet CNNs, which combines a multiresolution analysis and CNNs into one model. Our insight is that a CNN can be viewed as a limited form of a multiresolution analysis. Based on this insight, we supplement missing parts of the multiresolution analysis via wavelet transform and integrate them as additional components in the entire architecture. Wavelet CNNs allow us to utilize spectral information which is mostly lost in conventional CNNs but useful in most image processing tasks. We evaluate the practical performance of wavelet CNNs on texture classification and image annotation. The experiments show that wavelet CNNs can achieve better accuracy in both tasks than existing models while having significantly fewer parameters than conventional CNNs.



### A Deep Structure of Person Re-Identification using Multi-Level Gaussian Models
- **Arxiv ID**: http://arxiv.org/abs/1805.07720v1
- **DOI**: 10.1109/TMSCS.2018.2870592
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.07720v1)
- **Published**: 2018-05-20 07:21:52+00:00
- **Updated**: 2018-05-20 07:21:52+00:00
- **Authors**: Dinesh Kumar Vishwakarma, Sakshi Upadhyay
- **Comment**: 9 pages
- **Journal**: IEEE Transactions on Multi-Scale Computing Systems 4 (2018) 513 -
  521
- **Summary**: Person re-identification is being widely used in the forensic, and security and surveillance system, but person re-identification is a challenging task in real life scenario. Hence, in this work, a new feature descriptor model has been proposed using a multilayer framework of Gaussian distribution model on pixel features, which include color moments, color space values and Schmid filter responses. An image of a person usually consists of distinct body regions, usually with differentiable clothing followed by local colors and texture patterns. Thus, the image is evaluated locally by dividing the image into overlapping regions. Each region is further fragmented into a set of local Gaussians on small patches. A global Gaussian encodes, these local Gaussians for each region creating a multi-level structure. Hence, the global picture of a person is described by local level information present in it, which is often ignored. Also, we have analyzed the efficiency of earlier metric learning methods on this descriptor. The performance of the descriptor is evaluated on four public available challenging datasets and the highest accuracy achieved on these datasets are compared with similar state-of-the-arts, which demonstrate the superior performance.



### STS Classification with Dual-stream CNN
- **Arxiv ID**: http://arxiv.org/abs/1805.07740v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.07740v1)
- **Published**: 2018-05-20 08:59:17+00:00
- **Updated**: 2018-05-20 08:59:17+00:00
- **Authors**: Shuchen Weng, Wenbo Li, Yi Zhang, Siwei Lyu
- **Comment**: None
- **Journal**: None
- **Summary**: The structured time series (STS) classification problem requires the modeling of interweaved spatiotemporal dependency. most previous STS classification methods model the spatial and temporal dependencies independently. Due to the complexity of the STS data, we argue that a desirable STS classification method should be a holistic framework that can be made as adaptive and flexible as possible. This motivates us to design a deep neural network with such merits. Inspired by the dual-stream hypothesis in neural science, we propose a novel dual-stream framework for modeling the interweaved spatiotemporal dependency, and develop a convolutional neural network within this framework that aims to achieve high adaptability and flexibility in STS configurations from various diagonals, i.e., sequential order, dependency range and features. The proposed architecture is highly modularized and scalable, making it easy to be adapted to specific tasks. The effectiveness of our model is demonstrated through experiments on synthetic data as well as benchmark datasets for skeleton based activity recognition.



### DLBI: Deep learning guided Bayesian inference for structure reconstruction of super-resolution fluorescence microscopy
- **Arxiv ID**: http://arxiv.org/abs/1805.07777v3
- **DOI**: 10.1093/bioinformatics/bty241
- **Categories**: **cs.CV**, stat.AP, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.07777v3)
- **Published**: 2018-05-20 15:28:56+00:00
- **Updated**: 2018-09-01 20:07:42+00:00
- **Authors**: Yu Li, Fan Xu, Fa Zhang, Pingyong Xu, Mingshu Zhang, Ming Fan, Lihua Li, Xin Gao, Renmin Han
- **Comment**: Accepted by ISMB 2018
- **Journal**: Bioinformatics, Volume 34, Issue 13, 1 July 2018
- **Summary**: Super-resolution fluorescence microscopy, with a resolution beyond the diffraction limit of light, has become an indispensable tool to directly visualize biological structures in living cells at a nanometer-scale resolution. Despite advances in high-density super-resolution fluorescent techniques, existing methods still have bottlenecks, including extremely long execution time, artificial thinning and thickening of structures, and lack of ability to capture latent structures. Here we propose a novel deep learning guided Bayesian inference approach, DLBI, for the time-series analysis of high-density fluorescent images. Our method combines the strength of deep learning and statistical inference, where deep learning captures the underlying distribution of the fluorophores that are consistent with the observed time-series fluorescent images by exploring local features and correlation along time-axis, and statistical inference further refines the ultrastructure extracted by deep learning and endues physical meaning to the final image. Comprehensive experimental results on both real and simulated datasets demonstrate that our method provides more accurate and realistic local patch and large-field reconstruction than the state-of-the-art method, the 3B analysis, while our method is more than two orders of magnitude faster. The main program is available at https://github.com/lykaust15/DLBI



### Unsupervised Video Object Segmentation for Deep Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/1805.07780v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1805.07780v1)
- **Published**: 2018-05-20 15:45:03+00:00
- **Updated**: 2018-05-20 15:45:03+00:00
- **Authors**: Vik Goel, Jameson Weng, Pascal Poupart
- **Comment**: None
- **Journal**: None
- **Summary**: We present a new technique for deep reinforcement learning that automatically detects moving objects and uses the relevant information for action selection. The detection of moving objects is done in an unsupervised way by exploiting structure from motion. Instead of directly learning a policy from raw images, the agent first learns to detect and segment moving objects by exploiting flow information in video sequences. The learned representation is then used to focus the policy of the agent on the moving objects. Over time, the agent identifies which objects are critical for decision making and gradually builds a policy based on relevant moving objects. This approach, which we call Motion-Oriented REinforcement Learning (MOREL), is demonstrated on a suite of Atari games where the ability to detect moving objects reduces the amount of interaction needed with the environment to obtain a good policy. Furthermore, the resulting policy is more interpretable than policies that directly map images to actions or values with a black box neural network. We can gain insight into the policy by inspecting the segmentation and motion of each object detected by the agent. This allows practitioners to confirm whether a policy is making decisions based on sensible information.



### Conditional Inference in Pre-trained Variational Autoencoders via Cross-coding
- **Arxiv ID**: http://arxiv.org/abs/1805.07785v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1805.07785v2)
- **Published**: 2018-05-20 16:13:09+00:00
- **Updated**: 2018-10-03 18:31:02+00:00
- **Authors**: Ga Wu, Justin Domke, Scott Sanner
- **Comment**: 8 pages main content, 4 pages appendix
- **Journal**: None
- **Summary**: Variational Autoencoders (VAEs) are a popular generative model, but one in which conditional inference can be challenging. If the decomposition into query and evidence variables is fixed, conditional VAEs provide an attractive solution. To support arbitrary queries, one is generally reduced to Markov Chain Monte Carlo sampling methods that can suffer from long mixing times. In this paper, we propose an idea we term cross-coding to approximate the distribution over the latent variables after conditioning on an evidence assignment to some subset of the variables. This allows generating query samples without retraining the full VAE. We experimentally evaluate three variations of cross-coding showing that (i) they can be quickly optimized for different decompositions of evidence and query and (ii) they quantitatively and qualitatively outperform Hamiltonian Monte Carlo.



### Improved Learning of One-hidden-layer Convolutional Neural Networks with Overlaps
- **Arxiv ID**: http://arxiv.org/abs/1805.07798v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.DS, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.07798v2)
- **Published**: 2018-05-20 17:07:25+00:00
- **Updated**: 2018-06-01 23:29:24+00:00
- **Authors**: Simon S. Du, Surbhi Goel
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new algorithm to learn a one-hidden-layer convolutional neural network where both the convolutional weights and the outputs weights are parameters to be learned. Our algorithm works for a general class of (potentially overlapping) patches, including commonly used structures for computer vision tasks. Our algorithm draws ideas from (1) isotonic regression for learning neural networks and (2) landscape analysis of non-convex matrix factorization problems. We believe these findings may inspire further development in designing provable algorithms for learning neural networks and other complex models.



### Learning Real-World Robot Policies by Dreaming
- **Arxiv ID**: http://arxiv.org/abs/1805.07813v4
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.07813v4)
- **Published**: 2018-05-20 19:18:32+00:00
- **Updated**: 2019-08-01 17:37:41+00:00
- **Authors**: AJ Piergiovanni, Alan Wu, Michael S. Ryoo
- **Comment**: None
- **Journal**: IROS 2019
- **Summary**: Learning to control robots directly based on images is a primary challenge in robotics. However, many existing reinforcement learning approaches require iteratively obtaining millions of robot samples to learn a policy, which can take significant time. In this paper, we focus on learning a realistic world model capturing the dynamics of scene changes conditioned on robot actions. Our dreaming model can emulate samples equivalent to a sequence of images from the actual environment, technically by learning an action-conditioned future representation/scene regressor. This allows the agent to learn action policies (i.e., visuomotor policies) by interacting with the dreaming model rather than the real-world. We experimentally confirm that our dreaming model enables robot learning of policies that transfer to the real-world.



### Generalized Cross Entropy Loss for Training Deep Neural Networks with Noisy Labels
- **Arxiv ID**: http://arxiv.org/abs/1805.07836v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.07836v4)
- **Published**: 2018-05-20 23:01:49+00:00
- **Updated**: 2018-11-29 22:41:40+00:00
- **Authors**: Zhilu Zhang, Mert R. Sabuncu
- **Comment**: 32nd Conference on Neural Information Processing Systems (NeurIPS
  2018)
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) have achieved tremendous success in a variety of applications across many disciplines. Yet, their superior performance comes with the expensive cost of requiring correctly annotated large-scale datasets. Moreover, due to DNNs' rich capacity, errors in training labels can hamper performance. To combat this problem, mean absolute error (MAE) has recently been proposed as a noise-robust alternative to the commonly-used categorical cross entropy (CCE) loss. However, as we show in this paper, MAE can perform poorly with DNNs and challenging datasets. Here, we present a theoretically grounded set of noise-robust loss functions that can be seen as a generalization of MAE and CCE. Proposed loss functions can be readily applied with any existing DNN architecture and algorithm, while yielding good performance in a wide range of noisy label scenarios. We report results from experiments conducted with CIFAR-10, CIFAR-100 and FASHION-MNIST datasets and synthetically generated noisy labels.



