# Arxiv Papers in cs.CV on 2018-05-10
### k-Space Deep Learning for Accelerated MRI
- **Arxiv ID**: http://arxiv.org/abs/1805.03779v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.03779v3)
- **Published**: 2018-05-10 01:43:19+00:00
- **Updated**: 2019-07-03 15:44:52+00:00
- **Authors**: Yoseob Han, Leonard Sunwoo, Jong Chul Ye
- **Comment**: Accepted to IEEE Transactions on Medical Imaging
- **Journal**: None
- **Summary**: The annihilating filter-based low-rank Hankel matrix approach (ALOHA) is one of the state-of-the-art compressed sensing approaches that directly interpolates the missing k-space data using low-rank Hankel matrix completion. The success of ALOHA is due to the concise signal representation in the k-space domain thanks to the duality between structured low-rankness in the k-space domain and the image domain sparsity. Inspired by the recent mathematical discovery that links convolutional neural networks to Hankel matrix decomposition using data-driven framelet basis, here we propose a fully data-driven deep learning algorithm for k-space interpolation. Our network can be also easily applied to non-Cartesian k-space trajectories by simply adding an additional regridding layer. Extensive numerical experiments show that the proposed deep learning method consistently outperforms the existing image-domain deep learning approaches.



### Dust concentration vision measurement based on moment of inertia in gray level-rank co-occurrence matrix
- **Arxiv ID**: http://arxiv.org/abs/1805.03788v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.03788v1)
- **Published**: 2018-05-10 02:20:48+00:00
- **Updated**: 2018-05-10 02:20:48+00:00
- **Authors**: Zhiwen Luo, Guohui Li, Junfeng Du, Jieping Wu
- **Comment**: None
- **Journal**: None
- **Summary**: To improve the accuracy of existing dust concentration measurements, a dust concentration measurement based on Moment of inertia in Gray level-Rank Co-occurrence Matrix (GRCM), which is from the dust image sample measured by a machine vision system is proposed in this paper. Firstly, a Polynomial computational model between dust Concentration and Moment of inertia (PCM) is established by experimental methods and fitting methods. Then computing methods for GRCM and its Moment of inertia are constructed by theoretical and mathematical analysis methods. And then developing an on-line dust concentration vision measurement experimental system, the cement dust concentration measurement in a cement production workshop is taken as a practice example with the system and the PCM measurement. The results show that measurement error is within 9%, and the measurement range is 0.5-1000 mg/m3. Finally, comparing with the filter membrane weighing measurement, light scattering measurement and laser measurement, the proposed PCM measurement has advantages on error and cost, which can be provided a valuable reference for the dust concentration vision measurements.



### OFF-ApexNet on Micro-expression Recognition System
- **Arxiv ID**: http://arxiv.org/abs/1805.08699v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1805.08699v1)
- **Published**: 2018-05-10 02:22:47+00:00
- **Updated**: 2018-05-10 02:22:47+00:00
- **Authors**: Sze-Teng Liong, Y. S. Gan, Wei-Chuen Yau, Yen-Chang Huang, Tan Lit Ken
- **Comment**: None
- **Journal**: None
- **Summary**: When a person attempts to conceal an emotion, the genuine emotion is manifest as a micro-expression. Exploration of automatic facial micro-expression recognition systems is relatively new in the computer vision domain. This is due to the difficulty in implementing optimal feature extraction methods to cope with the subtlety and brief motion characteristics of the expression. Most of the existing approaches extract the subtle facial movements based on hand-crafted features. In this paper, we address the micro-expression recognition task with a convolutional neural network (CNN) architecture, which well integrates the features extracted from each video. A new feature descriptor, Optical Flow Features from Apex frame Network (OFF-ApexNet) is introduced. This feature descriptor combines the optical ow guided context with the CNN. Firstly, we obtain the location of the apex frame from each video sequence as it portrays the highest intensity of facial motion among all frames. Then, the optical ow information are attained from the apex frame and a reference frame (i.e., onset frame). Finally, the optical flow features are fed into a pre-designed CNN model for further feature enhancement as well as to carry out the expression classification. To evaluate the effectiveness of OFF-ApexNet, comprehensive evaluations are conducted on three public spontaneous micro-expression datasets (i.e., SMIC, CASME II and SAMM). The promising recognition result suggests that the proposed method can optimally describe the significant micro-expression details. In particular, we report that, in a multi-database with leave-one-subject-out cross-validation experimental protocol, the recognition performance reaches 74.60% of recognition accuracy and F-measure of 71.04%. We also note that this is the first work that performs cross-dataset validation on three databases in this domain.



### Avatar-Net: Multi-scale Zero-shot Style Transfer by Feature Decoration
- **Arxiv ID**: http://arxiv.org/abs/1805.03857v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.03857v2)
- **Published**: 2018-05-10 07:17:32+00:00
- **Updated**: 2018-05-22 08:21:10+00:00
- **Authors**: Lu Sheng, Ziyi Lin, Jing Shao, Xiaogang Wang
- **Comment**: Accepted at CVPR 2018
- **Journal**: None
- **Summary**: Zero-shot artistic style transfer is an important image synthesis problem aiming at transferring arbitrary style into content images. However, the trade-off between the generalization and efficiency in existing methods impedes a high quality zero-shot style transfer in real-time. In this paper, we resolve this dilemma and propose an efficient yet effective Avatar-Net that enables visually plausible multi-scale transfer for arbitrary style. The key ingredient of our method is a style decorator that makes up the content features by semantically aligned style features from an arbitrary style image, which does not only holistically match their feature distributions but also preserve detailed style patterns in the decorated features. By embedding this module into an image reconstruction network that fuses multi-scale style abstractions, the Avatar-Net renders multi-scale stylization for any style image in one feed-forward pass. We demonstrate the state-of-the-art effectiveness and efficiency of the proposed method in generating high-quality stylized images, with a series of applications include multiple style integration, video stylization and etc.



### Deep Covariance Descriptors for Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/1805.03869v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.03869v1)
- **Published**: 2018-05-10 07:53:26+00:00
- **Updated**: 2018-05-10 07:53:26+00:00
- **Authors**: Naima Otberdout, Anis Kacem, Mohamed Daoudi, Lahoucine Ballihi, Stefano Berretti
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, covariance matrices are exploited to encode the deep convolutional neural networks (DCNN) features for facial expression recognition. The space geometry of the covariance matrices is that of Symmetric Positive Definite (SPD) matrices. By performing the classification of the facial expressions using Gaussian kernel on SPD manifold, we show that the covariance descriptors computed on DCNN features are more efficient than the standard classification with fully connected layers and softmax. By implementing our approach using the VGG-face and ExpNet architectures with extensive experiments on the Oulu-CASIA and SFEW datasets, we show that the proposed approach achieves performance at the state of the art for facial expression recognition.



### Structure-from-Motion using Dense CNN Features with Keypoint Relocalization
- **Arxiv ID**: http://arxiv.org/abs/1805.03879v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.03879v2)
- **Published**: 2018-05-10 08:35:06+00:00
- **Updated**: 2018-05-11 01:43:32+00:00
- **Authors**: Aji Resindra Widya, Akihiko Torii, Masatoshi Okutomi
- **Comment**: None
- **Journal**: None
- **Summary**: Structure from Motion (SfM) using imagery that involves extreme appearance changes is yet a challenging task due to a loss of feature repeatability. Using feature correspondences obtained by matching densely extracted convolutional neural network (CNN) features significantly improves the SfM reconstruction capability. However, the reconstruction accuracy is limited by the spatial resolution of the extracted CNN features which is not even pixel-level accuracy in the existing approach. Providing dense feature matches with precise keypoint positions is not trivial because of memory limitation and computational burden of dense features. To achieve accurate SfM reconstruction with highly repeatable dense features, we propose an SfM pipeline that uses dense CNN features with relocalization of keypoint position that can efficiently and accurately provide pixel-level feature correspondences. Then, we demonstrate on the Aachen Day-Night dataset that the proposed SfM using dense CNN features with the keypoint relocalization outperforms a state-of-the-art SfM (COLMAP using RootSIFT) by a large margin.



### Dealing with sequences in the RGBDT space
- **Arxiv ID**: http://arxiv.org/abs/1805.03897v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.03897v1)
- **Published**: 2018-05-10 09:02:30+00:00
- **Updated**: 2018-05-10 09:02:30+00:00
- **Authors**: Gabriel Moyà, Antoni Jaume-i-Capó, Javier Varona
- **Comment**: 4 pages CVPR'18 Workshop: Brave New Ideas for Video Understanding
- **Journal**: None
- **Summary**: Most of the current research in computer vision is focused on working with single images without taking in account temporal information. We present a probabilistic non-parametric model that mixes multiple information cues from devices to segment regions that contain moving objects in image sequences. We prepared an experimental setup to show the importance of using previous information for obtaining an accurate segmentation result, using a novel dataset that provides sequences in the RGBDT space. We label the detected regions ts with a state-of-the-art human detector. Each one of the detected regions is at least marked as human once.



### Ensemble Soft-Margin Softmax Loss for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1805.03922v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.03922v1)
- **Published**: 2018-05-10 10:47:13+00:00
- **Updated**: 2018-05-10 10:47:13+00:00
- **Authors**: Xiaobo Wang, Shifeng Zhang, Zhen Lei, Si Liu, Xiaojie Guo, Stan Z. Li
- **Comment**: Accepted by IJCAI 2018
- **Journal**: None
- **Summary**: Softmax loss is arguably one of the most popular losses to train CNN models for image classification. However, recent works have exposed its limitation on feature discriminability. This paper casts a new viewpoint on the weakness of softmax loss. On the one hand, the CNN features learned using the softmax loss are often inadequately discriminative. We hence introduce a soft-margin softmax function to explicitly encourage the discrimination between different classes. On the other hand, the learned classifier of softmax loss is weak. We propose to assemble multiple these weak classifiers to a strong one, inspired by the recognition that the diversity among weak classifiers is critical to a good ensemble. To achieve the diversity, we adopt the Hilbert-Schmidt Independence Criterion (HSIC). Considering these two aspects in one framework, we design a novel loss, named as Ensemble soft-Margin Softmax (EM-Softmax). Extensive experiments on benchmark datasets are conducted to show the superiority of our design over the baseline softmax loss and several state-of-the-art alternatives.



### ABMOF: A Novel Optical Flow Algorithm for Dynamic Vision Sensors
- **Arxiv ID**: http://arxiv.org/abs/1805.03988v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.03988v1)
- **Published**: 2018-05-10 14:07:31+00:00
- **Updated**: 2018-05-10 14:07:31+00:00
- **Authors**: Min Liu, Tobi Delbruck
- **Comment**: 11 pages, 10 figures, Video of result: https://youtu.be/Ss-MciioqTk
- **Journal**: None
- **Summary**: Dynamic Vision Sensors (DVS), which output asynchronous log intensity change events, have potential applications in high-speed robotics, autonomous cars and drones. The precise event timing, sparse output, and wide dynamic range of the events are well suited for optical flow, but conventional optical flow (OF) algorithms are not well matched to the event stream data. This paper proposes an event-driven OF algorithm called adaptive block-matching optical flow (ABMOF). ABMOF uses time slices of accumulated DVS events. The time slices are adaptively rotated based on the input events and OF results. Compared with other methods such as gradient-based OF, ABMOF can efficiently be implemented in compact logic circuits. Results show that ABMOF achieves comparable accuracy to conventional standards such as Lucas-Kanade (LK). The main contributions of our paper are new adaptive time-slice rotation methods that ensure the generated slices have sufficient features for matching,including a feedback mechanism that controls the generated slices to have average slice displacement within the block search range. An LK method using our adapted slices is also implemented. The ABMOF accuracy is compared with this LK method on natural scene data including sparse and dense texture, high dynamic range, and fast motion exceeding 30,000 pixels per second.The paper dataset and source code are available from http://sensors.ini.uzh.ch/databases.html.



### Multi-View Semantic Labeling of 3D Point Clouds for Automated Plant Phenotyping
- **Arxiv ID**: http://arxiv.org/abs/1805.03994v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.03994v2)
- **Published**: 2018-05-10 14:19:50+00:00
- **Updated**: 2018-05-29 16:28:18+00:00
- **Authors**: Bernhard Japes, Jennifer Mack, Florian Rist, Katja Herzog, Reinhard Töpfer, Volker Steinhage
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic labeling of 3D point clouds is important for the derivation of 3D models from real world scenarios in several economic fields such as building industry, facility management, town planning or heritage conservation. In contrast to these most common applications, we describe in this study the semantic labeling of 3D point clouds derived from plant organs by high-precision scanning. Our approach is optimized for the task of plant phenotyping with its very specific challenges and is employing a deep learning framework. Thereby, we report important experiences concerning detailed parameter initialization and optimization techniques. By evaluating our approach with challenging datasets we achieve state-of-the-art results without difficult and time consuming feature engineering as being necessary in traditional approaches to semantic labeling.



### Dense and Diverse Capsule Networks: Making the Capsules Learn Better
- **Arxiv ID**: http://arxiv.org/abs/1805.04001v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.04001v1)
- **Published**: 2018-05-10 14:29:17+00:00
- **Updated**: 2018-05-10 14:29:17+00:00
- **Authors**: Sai Samarth R Phaye, Apoorva Sikka, Abhinav Dhall, Deepti Bathula
- **Comment**: None
- **Journal**: None
- **Summary**: Past few years have witnessed exponential growth of interest in deep learning methodologies with rapidly improving accuracies and reduced computational complexity. In particular, architectures using Convolutional Neural Networks (CNNs) have produced state-of-the-art performances for image classification and object recognition tasks. Recently, Capsule Networks (CapsNet) achieved significant increase in performance by addressing an inherent limitation of CNNs in encoding pose and deformation. Inspired by such advancement, we asked ourselves, can we do better? We propose Dense Capsule Networks (DCNet) and Diverse Capsule Networks (DCNet++). The two proposed frameworks customize the CapsNet by replacing the standard convolutional layers with densely connected convolutions. This helps in incorporating feature maps learned by different layers in forming the primary capsules. DCNet, essentially adds a deeper convolution network, which leads to learning of discriminative feature maps. Additionally, DCNet++ uses a hierarchical architecture to learn capsules that represent spatial information in a fine-to-coarser manner, which makes it more efficient for learning complex data. Experiments on image classification task using benchmark datasets demonstrate the efficacy of the proposed architectures. DCNet achieves state-of-the-art performance (99.75%) on MNIST dataset with twenty fold decrease in total training iterations, over the conventional CapsNet. Furthermore, DCNet++ performs better than CapsNet on SVHN dataset (96.90%), and outperforms the ensemble of seven CapsNet models on CIFAR-10 by 0.31% with seven fold decrease in number of parameters.



### Deep Nets: What have they ever done for Vision?
- **Arxiv ID**: http://arxiv.org/abs/1805.04025v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1805.04025v4)
- **Published**: 2018-05-10 15:43:44+00:00
- **Updated**: 2020-11-25 15:34:56+00:00
- **Authors**: Alan L. Yuille, Chenxi Liu
- **Comment**: To appear in IJCV
- **Journal**: None
- **Summary**: This is an opinion paper about the strengths and weaknesses of Deep Nets for vision. They are at the heart of the enormous recent progress in artificial intelligence and are of growing importance in cognitive science and neuroscience. They have had many successes but also have several limitations and there is limited understanding of their inner workings. At present Deep Nets perform very well on specific visual tasks with benchmark datasets but they are much less general purpose, flexible, and adaptive than the human visual system. We argue that Deep Nets in their current form are unlikely to be able to overcome the fundamental problem of computer vision, namely how to deal with the combinatorial explosion, caused by the enormous complexity of natural images, and obtain the rich understanding of visual scenes that the human visual achieves. We argue that this combinatorial explosion takes us into a regime where "big data is not enough" and where we need to rethink our methods for benchmarking performance and evaluating vision algorithms. We stress that, as vision algorithms are increasingly used in real world applications, that performance evaluation is not merely an academic exercise but has important consequences in the real world. It is impractical to review the entire Deep Net literature so we restrict ourselves to a limited range of topics and references which are intended as entry points into the literature. The views expressed in this paper are our own and do not necessarily represent those of anybody else in the computer vision community.



### Towards an Unequivocal Representation of Actions
- **Arxiv ID**: http://arxiv.org/abs/1805.04026v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.04026v1)
- **Published**: 2018-05-10 15:48:26+00:00
- **Updated**: 2018-05-10 15:48:26+00:00
- **Authors**: Michael Wray, Davide Moltisanti, Dima Damen
- **Comment**: None
- **Journal**: None
- **Summary**: This work introduces verb-only representations for actions and interactions; the problem of describing similar motions (e.g. 'open door', 'open cupboard'), and distinguish differing ones (e.g. 'open door' vs 'open bottle') using verb-only labels. Current approaches for action recognition neglect legitimate semantic ambiguities and class overlaps between verbs (Fig. 1), relying on the objects to disambiguate interactions. We deviate from single-verb labels and introduce a mapping between observations and multiple verb labels - in order to create an Unequivocal Representation of Actions. The new representation benefits from increased vocabulary and a soft assignment to an enriched space of verb labels. We learn these representations as multi-output regression, using a two-stream fusion CNN. The proposed approach outperforms conventional single-verb labels (also known as majority voting) on three egocentric datasets for both recognition and retrieval.



### Learning to Estimate 3D Human Pose and Shape from a Single Color Image
- **Arxiv ID**: http://arxiv.org/abs/1805.04092v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.04092v1)
- **Published**: 2018-05-10 17:46:12+00:00
- **Updated**: 2018-05-10 17:46:12+00:00
- **Authors**: Georgios Pavlakos, Luyang Zhu, Xiaowei Zhou, Kostas Daniilidis
- **Comment**: CVPR 2018 Camera Ready
- **Journal**: None
- **Summary**: This work addresses the problem of estimating the full body 3D human pose and shape from a single color image. This is a task where iterative optimization-based solutions have typically prevailed, while Convolutional Networks (ConvNets) have suffered because of the lack of training data and their low resolution 3D predictions. Our work aims to bridge this gap and proposes an efficient and effective direct prediction method based on ConvNets. Central part to our approach is the incorporation of a parametric statistical body shape model (SMPL) within our end-to-end framework. This allows us to get very detailed 3D mesh results, while requiring estimation only of a small number of parameters, making it friendly for direct network prediction. Interestingly, we demonstrate that these parameters can be predicted reliably only from 2D keypoints and masks. These are typical outputs of generic 2D human analysis ConvNets, allowing us to relax the massive requirement that images with 3D shape ground truth are available for training. Simultaneously, by maintaining differentiability, at training time we generate the 3D mesh from the estimated parameters and optimize explicitly for the surface using a 3D per-vertex loss. Finally, a differentiable renderer is employed to project the 3D mesh to the image, which enables further refinement of the network, by optimizing for the consistency of the projection with 2D annotations (i.e., 2D keypoints or masks). The proposed approach outperforms previous baselines on this task and offers an attractive solution for direct prediction of 3D shape from a single color image.



### Ordinal Depth Supervision for 3D Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1805.04095v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.04095v1)
- **Published**: 2018-05-10 17:47:26+00:00
- **Updated**: 2018-05-10 17:47:26+00:00
- **Authors**: Georgios Pavlakos, Xiaowei Zhou, Kostas Daniilidis
- **Comment**: CVPR 2018 Camera Ready
- **Journal**: None
- **Summary**: Our ability to train end-to-end systems for 3D human pose estimation from single images is currently constrained by the limited availability of 3D annotations for natural images. Most datasets are captured using Motion Capture (MoCap) systems in a studio setting and it is difficult to reach the variability of 2D human pose datasets, like MPII or LSP. To alleviate the need for accurate 3D ground truth, we propose to use a weaker supervision signal provided by the ordinal depths of human joints. This information can be acquired by human annotators for a wide range of images and poses. We showcase the effectiveness and flexibility of training Convolutional Networks (ConvNets) with these ordinal relations in different settings, always achieving competitive performance with ConvNets trained with accurate 3D joint coordinates. Additionally, to demonstrate the potential of the approach, we augment the popular LSP and MPII datasets with ordinal depth annotations. This extension allows us to present quantitative and qualitative evaluation in non-studio conditions. Simultaneously, these ordinal annotations can be easily incorporated in the training procedure of typical ConvNets for 3D human pose. Through this inclusion we achieve new state-of-the-art performance for the relevant benchmarks and validate the effectiveness of ordinal depth supervision for 3D human pose.



### Fighting Fake News: Image Splice Detection via Learned Self-Consistency
- **Arxiv ID**: http://arxiv.org/abs/1805.04096v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.04096v3)
- **Published**: 2018-05-10 17:49:03+00:00
- **Updated**: 2018-09-05 18:16:41+00:00
- **Authors**: Minyoung Huh, Andrew Liu, Andrew Owens, Alexei A. Efros
- **Comment**: None
- **Journal**: None
- **Summary**: Advances in photo editing and manipulation tools have made it significantly easier to create fake imagery. Learning to detect such manipulations, however, remains a challenging problem due to the lack of sufficient amounts of manipulated training data. In this paper, we propose a learning algorithm for detecting visual image manipulations that is trained only using a large dataset of real photographs. The algorithm uses the automatically recorded photo EXIF metadata as supervisory signal for training a model to determine whether an image is self-consistent -- that is, whether its content could have been produced by a single imaging pipeline. We apply this self-consistency model to the task of detecting and localizing image splices. The proposed method obtains state-of-the-art performance on several image forensics benchmarks, despite never seeing any manipulated images at training. That said, it is merely a step in the long quest for a truly general purpose visual forensics tool.



### Arbitrary Style Transfer with Deep Feature Reshuffle
- **Arxiv ID**: http://arxiv.org/abs/1805.04103v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.04103v4)
- **Published**: 2018-05-10 17:58:11+00:00
- **Updated**: 2018-06-20 14:26:42+00:00
- **Authors**: Shuyang Gu, Congliang Chen, Jing Liao, Lu Yuan
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces a novel method by reshuffling deep features (i.e., permuting the spacial locations of a feature map) of the style image for arbitrary style transfer. We theoretically prove that our new style loss based on reshuffle connects both global and local style losses respectively used by most parametric and non-parametric neural style transfer methods. This simple idea can effectively address the challenging issues in existing style transfer methods. On one hand, it can avoid distortions in local style patterns, and allow semantic-level transfer, compared with neural parametric methods. On the other hand, it can preserve globally similar appearance to the style image, and avoid wash-out artifacts, compared with neural non-parametric methods. Based on the proposed loss, we also present a progressive feature-domain optimization approach. The experiments show that our method is widely applicable to various styles, and produces better quality than existing methods.



### Boosting up Scene Text Detectors with Guided CNN
- **Arxiv ID**: http://arxiv.org/abs/1805.04132v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1805.04132v2)
- **Published**: 2018-05-10 18:51:19+00:00
- **Updated**: 2018-05-14 03:38:25+00:00
- **Authors**: Xiaoyu Yue, Zhanghui Kuang, Zhaoyang Zhang, Zhenfang Chen, Pan He, Yu Qiao, Wei Zhang
- **Comment**: Submitted to British Machine Vision Conference (BMVC), 2018
- **Journal**: None
- **Summary**: Deep CNNs have achieved great success in text detection. Most of existing methods attempt to improve accuracy with sophisticated network design, while paying less attention on speed. In this paper, we propose a general framework for text detection called Guided CNN to achieve the two goals simultaneously. The proposed model consists of one guidance subnetwork, where a guidance mask is learned from the input image itself, and one primary text detector, where every convolution and non-linear operation are conducted only in the guidance mask. On the one hand, the guidance subnetwork filters out non-text regions coarsely, greatly reduces the computation complexity. On the other hand, the primary text detector focuses on distinguishing between text and hard non-text regions and regressing text bounding boxes, achieves a better detection accuracy. A training strategy, called background-aware block-wise random synthesis, is proposed to further boost up the performance. We demonstrate that the proposed Guided CNN is not only effective but also efficient with two state-of-the-art methods, CTPN and EAST, as backbones. On the challenging benchmark ICDAR 2013, it speeds up CTPN by 2.9 times on average, while improving the F-measure by 1.5%. On ICDAR 2015, it speeds up EAST by 2.0 times while improving the F-measure by 1.0%.



### Unsupervised Deep Representations for Learning Audience Facial Behaviors
- **Arxiv ID**: http://arxiv.org/abs/1805.04136v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.04136v1)
- **Published**: 2018-05-10 19:01:02+00:00
- **Updated**: 2018-05-10 19:01:02+00:00
- **Authors**: Suman Saha, Rajitha Navarathna, Leonhard Helminger, Romann Weber
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present an unsupervised learning approach for analyzing facial behavior based on a deep generative model combined with a convolutional neural network (CNN). We jointly train a variational auto-encoder (VAE) and a generative adversarial network (GAN) to learn a powerful latent representation from footage of audiences viewing feature-length movies. We show that the learned latent representation successfully encodes meaningful signatures of behaviors related to audience engagement (smiling & laughing) and disengagement (yawning). Our results provide a proof of concept for a more general methodology for annotating hard-to-label multimedia data featuring sparse examples of signals of interest.



### Neural Best-Buddies: Sparse Cross-Domain Correspondence
- **Arxiv ID**: http://arxiv.org/abs/1805.04140v2
- **DOI**: 10.1145/3197517.3201332
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.04140v2)
- **Published**: 2018-05-10 19:11:04+00:00
- **Updated**: 2018-08-21 10:28:33+00:00
- **Authors**: Kfir Aberman, Jing Liao, Mingyi Shi, Dani Lischinski, Baoquan Chen, Daniel Cohen-Or
- **Comment**: SIGGRAPH 2018
- **Journal**: None
- **Summary**: Correspondence between images is a fundamental problem in computer vision, with a variety of graphics applications. This paper presents a novel method for sparse cross-domain correspondence. Our method is designed for pairs of images where the main objects of interest may belong to different semantic categories and differ drastically in shape and appearance, yet still contain semantically related or geometrically similar parts. Our approach operates on hierarchies of deep features, extracted from the input images by a pre-trained CNN. Specifically, starting from the coarsest layer in both hierarchies, we search for Neural Best Buddies (NBB): pairs of neurons that are mutual nearest neighbors. The key idea is then to percolate NBBs through the hierarchy, while narrowing down the search regions at each level and retaining only NBBs with significant activations. Furthermore, in order to overcome differences in appearance, each pair of search regions is transformed into a common appearance. We evaluate our method via a user study, in addition to comparisons with alternative correspondence approaches. The usefulness of our method is demonstrated using a variety of graphics applications, including cross-domain image alignment, creation of hybrid images, automatic image morphing, and more.



### Semi-Supervised Domain Adaptation with Representation Learning for Semantic Segmentation across Time
- **Arxiv ID**: http://arxiv.org/abs/1805.04141v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.04141v2)
- **Published**: 2018-05-10 19:14:06+00:00
- **Updated**: 2019-10-06 19:34:31+00:00
- **Authors**: Assia Benbihi, Matthieu Geist, Cédric Pradalier
- **Comment**: None
- **Journal**: Neural Information Processing - 26th International Conference,
  {ICONIP} 2019, Sydney, Australia, December 12-15, 2019, Proceedings,
- **Summary**: Deep learning generates state-of-the-art semantic segmentation provided that a large number of images together with pixel-wise annotations are available. To alleviate the expensive data collection process, we propose a semi-supervised domain adaptation method for the specific case of images with similar semantic content but different pixel distributions. A network trained with supervision on a past dataset is finetuned on the new dataset to conserve its features maps. The domain adaptation becomes a simple regression between feature maps and does not require annotations on the new dataset. This method reaches performances similar to classic transfer learning on the PASCAL VOC dataset with synthetic transformations.



