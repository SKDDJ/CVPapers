# Arxiv Papers in cs.CV on 2018-05-15
### Multi-Modal Trajectory Prediction of Surrounding Vehicles with Maneuver based LSTMs
- **Arxiv ID**: http://arxiv.org/abs/1805.05499v1
- **DOI**: 10.1109/IVS.2018.8500493
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.05499v1)
- **Published**: 2018-05-15 00:10:45+00:00
- **Updated**: 2018-05-15 00:10:45+00:00
- **Authors**: Nachiket Deo, Mohan M. Trivedi
- **Comment**: accepted for publication at IV 2018
- **Journal**: None
- **Summary**: To safely and efficiently navigate through complex traffic scenarios, autonomous vehicles need to have the ability to predict the future motion of surrounding vehicles. Multiple interacting agents, the multi-modal nature of driver behavior, and the inherent uncertainty involved in the task make motion prediction of surrounding vehicles a challenging problem. In this paper, we present an LSTM model for interaction aware motion prediction of surrounding vehicles on freeways. Our model assigns confidence values to maneuvers being performed by vehicles and outputs a multi-modal distribution over future motion based on them. We compare our approach with the prior art for vehicle motion prediction on the publicly available NGSIM US-101 and I-80 datasets. Our results show an improvement in terms of RMS values of prediction error. We also present an ablative analysis of the components of our proposed model and analyze the predictions made by the model in complex traffic scenarios.



### Convolutional Social Pooling for Vehicle Trajectory Prediction
- **Arxiv ID**: http://arxiv.org/abs/1805.06771v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.06771v1)
- **Published**: 2018-05-15 00:24:38+00:00
- **Updated**: 2018-05-15 00:24:38+00:00
- **Authors**: Nachiket Deo, Mohan M. Trivedi
- **Comment**: Accepted for publication at CVPR TrajNet Workshop, 2018. arXiv admin
  note: text overlap with arXiv:1805.05499
- **Journal**: The IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR) Workshops, 2018, pp. 1468-1476
- **Summary**: Forecasting the motion of surrounding vehicles is a critical ability for an autonomous vehicle deployed in complex traffic. Motion of all vehicles in a scene is governed by the traffic context, i.e., the motion and relative spatial configuration of neighboring vehicles. In this paper we propose an LSTM encoder-decoder model that uses convolutional social pooling as an improvement to social pooling layers for robustly learning interdependencies in vehicle motion. Additionally, our model outputs a multi-modal predictive distribution over future trajectories based on maneuver classes. We evaluate our model using the publicly available NGSIM US-101 and I-80 datasets. Our results show improvement over the state of the art in terms of RMS values of prediction error and negative log-likelihoods of true future trajectories under the model's predictive distribution. We also present a qualitative analysis of the model's predicted distributions for various traffic scenarios.



### Learning to Deblur Images with Exemplars
- **Arxiv ID**: http://arxiv.org/abs/1805.05503v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.05503v1)
- **Published**: 2018-05-15 00:26:15+00:00
- **Updated**: 2018-05-15 00:26:15+00:00
- **Authors**: Jinshan Pan, Wenqi Ren, Zhe Hu, Ming-Hsuan Yang
- **Comment**: Accepted by IEEE Transactions on Pattern Analysis and Machine
  Intelligence 2018
- **Journal**: None
- **Summary**: Human faces are one interesting object class with numerous applications. While significant progress has been made in the generic deblurring problem, existing methods are less effective for blurry face images. The success of the state-of-the-art image deblurring algorithms stems mainly from implicit or explicit restoration of salient edges for kernel estimation. However, existing methods are less effective as only few edges can be restored from blurry face images for kernel estimation. In this paper, we address the problem of deblurring face images by exploiting facial structures. We propose a deblurring algorithm based on an exemplar dataset without using coarse-to-fine strategies or heuristic edge selections. In addition, we develop a convolutional neural network to restore sharp edges from blurry images for deblurring. Extensive experiments against the state-of-the-art methods demonstrate the effectiveness of the proposed algorithms for deblurring face images. In addition, we show the proposed algorithms can be applied to image deblurring for other object classes.



### A Multilayer Framework for Online Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/1805.05510v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1805.05510v3)
- **Published**: 2018-05-15 01:10:18+00:00
- **Updated**: 2023-08-24 05:37:44+00:00
- **Authors**: Wenbin Li, Yanfang Liu, Jing Huo, Yinghuan Shi, Yang Gao, Lei Wang, Jiebo Luo
- **Comment**: Accepted to IEEE Transactions on Neural Networks and Learning Systems
  (TNNLS) 2022
- **Journal**: None
- **Summary**: Online metric learning has been widely applied in classification and retrieval. It can automatically learn a suitable metric from data by restricting similar instances to be separated from dissimilar instances with a given margin. However, the existing online metric learning algorithms have limited performance in real-world classifications, especially when data distributions are complex. To this end, this paper proposes a multilayer framework for online metric learning to capture the nonlinear similarities among instances. Different from the traditional online metric learning, which can only learn one metric space, the proposed Multi-Layer Online Metric Learning (MLOML) takes an online metric learning algorithm as a metric layer and learns multiple hierarchical metric spaces, where each metric layer follows a nonlinear layers for the complicated data distribution. Moreover, the forward propagation (FP) strategy and backward propagation (BP) strategy are employed to train the hierarchical metric layers. To build a metric layer of the proposed MLOML, a new Mahalanobis-based Online Metric Learning (MOML) algorithm is presented based on the passive-aggressive strategy and one-pass triplet construction strategy. Furthermore, in a progressively and nonlinearly learning way, MLOML has a stronger learning ability than traditional online metric learning in the case of limited available training data. To make the learning process more explainable and theoretically guaranteed, theoretical analysis is provided. The proposed MLOML enjoys several nice properties, indeed learns a metric progressively, and performs better on the benchmark datasets. Extensive experiments with different settings have been conducted to verify these properties of the proposed MLOML.



### Fully Associative Patch-based 1-to-N Matcher for Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1805.06306v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.06306v1)
- **Published**: 2018-05-15 01:42:46+00:00
- **Updated**: 2018-05-15 01:42:46+00:00
- **Authors**: Lingfeng Zhang, Ioannis A. Kakadiaris
- **Comment**: Accepted in ICB2018. arXiv admin note: text overlap with
  arXiv:1804.01417, arXiv:1805.02339 and substantial text overlap with
  arXiv:1803.09359
- **Journal**: None
- **Summary**: This paper focuses on improving face recognition performance by a patch-based 1-to-N signature matcher that learns correlations between different facial patches. A Fully Associative Patch-based Signature Matcher (FAPSM) is proposed so that the local matching identity of each patch contributes to the global matching identities of all the patches. The proposed matcher consists of three steps. First, based on the signature, the local matching identity and the corresponding matching score of each patch are computed. Then, a fully associative weight matrix is learned to obtain the global matching identities and scores of all the patches. At last, the l1-regularized weighting is applied to combine the global matching identity of each patch and obtain a final matching identity. The proposed matcher has been integrated with the UR2D system for evaluation. The experimental results indicate that the proposed matcher achieves better performance than the current UR2D system. The Rank-1 accuracy is improved significantly by 3% and 0.55% on the UHDB31 dataset and the IJB-A dataset, respectively.



### Knowledge Distillation with Adversarial Samples Supporting Decision Boundary
- **Arxiv ID**: http://arxiv.org/abs/1805.05532v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.05532v4)
- **Published**: 2018-05-15 02:42:40+00:00
- **Updated**: 2018-12-14 15:20:19+00:00
- **Authors**: Byeongho Heo, Minsik Lee, Sangdoo Yun, Jin Young Choi
- **Comment**: Accepted to AAAI 2019
- **Journal**: None
- **Summary**: Many recent works on knowledge distillation have provided ways to transfer the knowledge of a trained network for improving the learning process of a new one, but finding a good technique for knowledge distillation is still an open problem. In this paper, we provide a new perspective based on a decision boundary, which is one of the most important component of a classifier. The generalization performance of a classifier is closely related to the adequacy of its decision boundary, so a good classifier bears a good decision boundary. Therefore, transferring information closely related to the decision boundary can be a good attempt for knowledge distillation. To realize this goal, we utilize an adversarial attack to discover samples supporting a decision boundary. Based on this idea, to transfer more accurate information about the decision boundary, the proposed algorithm trains a student classifier based on the adversarial samples supporting the decision boundary. Experiments show that the proposed method indeed improves knowledge distillation and achieves the state-of-the-arts performance.



### Knowledge Distillation in Generations: More Tolerant Teachers Educate Better Students
- **Arxiv ID**: http://arxiv.org/abs/1805.05551v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.05551v2)
- **Published**: 2018-05-15 03:51:03+00:00
- **Updated**: 2018-09-07 17:37:25+00:00
- **Authors**: Chenglin Yang, Lingxi Xie, Siyuan Qiao, Alan Yuille
- **Comment**: 9 pages, 2 figures, major changes beyond v1
- **Journal**: None
- **Summary**: We focus on the problem of training a deep neural network in generations. The flowchart is that, in order to optimize the target network (student), another network (teacher) with the same architecture is first trained, and used to provide part of supervision signals in the next stage. While this strategy leads to a higher accuracy, many aspects (e.g., why teacher-student optimization helps) still need further explorations.   This paper studies this problem from a perspective of controlling the strictness in training the teacher network. Existing approaches mostly used a hard distribution (e.g., one-hot vectors) in training, leading to a strict teacher which itself has a high accuracy, but we argue that the teacher needs to be more tolerant, although this often implies a lower accuracy. The implementation is very easy, with merely an extra loss term added to the teacher network, facilitating a few secondary classes to emerge and complement to the primary class. Consequently, the teacher provides a milder supervision signal (a less peaked distribution), and makes it possible for the student to learn from inter-class similarity and potentially lower the risk of over-fitting. Experiments are performed on standard image classification tasks (CIFAR100 and ILSVRC2012). Although the teacher network behaves less powerful, the students show a persistent ability growth and eventually achieve higher classification accuracies than other competitors. Model ensemble and transfer feature extraction also verify the effectiveness of our approach.



### On Learning Associations of Faces and Voices
- **Arxiv ID**: http://arxiv.org/abs/1805.05553v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.05553v3)
- **Published**: 2018-05-15 03:59:08+00:00
- **Updated**: 2018-11-02 00:52:57+00:00
- **Authors**: Changil Kim, Hijung Valentina Shin, Tae-Hyun Oh, Alexandre Kaspar, Mohamed Elgharib, Wojciech Matusik
- **Comment**: 27 pages including the supplementary material; Accepted to ACCV 2018
- **Journal**: None
- **Summary**: In this paper, we study the associations between human faces and voices. Audiovisual integration, specifically the integration of facial and vocal information is a well-researched area in neuroscience. It is shown that the overlapping information between the two modalities plays a significant role in perceptual tasks such as speaker identification. Through an online study on a new dataset we created, we confirm previous findings that people can associate unseen faces with corresponding voices and vice versa with greater than chance accuracy. We computationally model the overlapping information between faces and voices and show that the learned cross-modal representation contains enough information to identify matching faces and voices with performance similar to that of humans. Our representation exhibits correlations to certain demographic attributes and features obtained from either visual or aural modality alone. We release our dataset of audiovisual recordings and demographic annotations of people reading out short text used in our studies.



### Facial Landmark Detection: a Literature Survey
- **Arxiv ID**: http://arxiv.org/abs/1805.05563v1
- **DOI**: 10.1007/s11263-018-1097-z
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.05563v1)
- **Published**: 2018-05-15 05:22:16+00:00
- **Updated**: 2018-05-15 05:22:16+00:00
- **Authors**: Yue Wu, Qiang Ji
- **Comment**: None
- **Journal**: International Journal on Computer Vision, 2017
- **Summary**: The locations of the fiducial facial landmark points around facial components and facial contour capture the rigid and non-rigid facial deformations due to head movements and facial expressions. They are hence important for various facial analysis tasks. Many facial landmark detection algorithms have been developed to automatically detect those key points over the years, and in this paper, we perform an extensive review of them. We classify the facial landmark detection algorithms into three major categories: holistic methods, Constrained Local Model (CLM) methods, and the regression-based methods. They differ in the ways to utilize the facial appearance and shape information. The holistic methods explicitly build models to represent the global facial appearance and shape information. The CLMs explicitly leverage the global shape model but build the local appearance models. The regression-based methods implicitly capture facial shape and appearance information. For algorithms within each category, we discuss their underlying theories as well as their differences. We also compare their performances on both controlled and in the wild benchmark datasets, under varying facial expressions, head poses, and occlusion. Based on the evaluations, we point out their respective strengths and weaknesses. There is also a separate section to review the latest deep learning-based algorithms.   The survey also includes a listing of the benchmark databases and existing software. Finally, we identify future research directions, including combining methods in different categories to leverage their respective strengths to solve landmark detection "in-the-wild".



### Cross-connected Networks for Multi-task Learning of Detection and Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1805.05569v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.05569v1)
- **Published**: 2018-05-15 05:32:55+00:00
- **Updated**: 2018-05-15 05:32:55+00:00
- **Authors**: Seiichiro Fukuda, Ryota Yoshihashi, Rei Kawakami, Shaodi You, Makoto Iida, Takeshi Naemura
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-task learning improves generalization performance by sharing knowledge among related tasks. Existing models are for task combinations annotated on the same dataset, while there are cases where multiple datasets are available for each task. How to utilize knowledge of successful single-task CNNs that are trained on each dataset has been explored less than multi-task learning with a single dataset. We propose a cross-connected CNN, a new architecture that connects single-task CNNs through convolutional layers, which transfer useful information for the counterpart. We evaluated our proposed architecture on a combination of detection and segmentation using two datasets. Experiments on pedestrians show our CNN achieved a higher detection performance compared to baseline CNNs, while maintaining high quality for segmentation. It is the first known attempt to tackle multi-task learning with different training datasets between detection and segmentation. Experiments with wild birds demonstrate how our CNN learns general representations from limited datasets.



### Corpus Conversion Service: A machine learning platform to ingest documents at scale [Poster abstract]
- **Arxiv ID**: http://arxiv.org/abs/1805.09687v1
- **DOI**: 10.13140/RG.2.2.10858.82888
- **Categories**: **cs.DL**, cs.CL, cs.CV, cs.DC, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1805.09687v1)
- **Published**: 2018-05-15 07:05:52+00:00
- **Updated**: 2018-05-15 07:05:52+00:00
- **Authors**: Peter W J Staar, Michele Dolfi, Christoph Auer, Costas Bekas
- **Comment**: Accepted in SysML 2018 (www.sysml.cc)
- **Journal**: None
- **Summary**: Over the past few decades, the amount of scientific articles and technical literature has increased exponentially in size. Consequently, there is a great need for systems that can ingest these documents at scale and make their content discoverable. Unfortunately, both the format of these documents (e.g. the PDF format or bitmap images) as well as the presentation of the data (e.g. complex tables) make the extraction of qualitative and quantitive data extremely challenging. We present a platform to ingest documents at scale which is powered by Machine Learning techniques and allows the user to train custom models on document collections. We show precision/recall results greater than 97% with regard to conversion to structured formats, as well as scaling evidence for each of the microservices constituting the platform.



### Image Co-segmentation via Multi-scale Local Shape Transfer
- **Arxiv ID**: http://arxiv.org/abs/1805.05610v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.05610v1)
- **Published**: 2018-05-15 07:48:52+00:00
- **Updated**: 2018-05-15 07:48:52+00:00
- **Authors**: Wei Teng, Yu Zhang, Xiaowu Chen, Jia Li, Zhiqiang He
- **Comment**: An extention of our previous study
- **Journal**: None
- **Summary**: Image co-segmentation is a challenging task in computer vision that aims to segment all pixels of the objects from a predefined semantic category. In real-world cases, however, common foreground objects often vary greatly in appearance, making their global shapes highly inconsistent across images and difficult to be segmented. To address this problem, this paper proposes a novel co-segmentation approach that transfers patch-level local object shapes which appear more consistent across different images. In our framework, a multi-scale patch neighbourhood system is first generated using proposal flow on arbitrary image-pair, which is further refined by Locally Linear Embedding. Based on the patch relationships, we propose an efficient algorithm to jointly segment the objects in each image while transferring their local shapes across different images. Extensive experiments demonstrate that the proposed method can robustly and effectively segment common objects from an image set. On iCoseg, MSRC and Coseg-Rep dataset, the proposed approach performs comparable or better than the state-of-thearts, while on a more challenging benchmark Fashionista dataset, our method achieves significant improvements.



### Robust Facial Landmark Localization Based on Texture and Pose Correlated Initialization
- **Arxiv ID**: http://arxiv.org/abs/1805.05612v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.05612v1)
- **Published**: 2018-05-15 07:51:45+00:00
- **Updated**: 2018-05-15 07:51:45+00:00
- **Authors**: Yiyun Pan, Junwei Zhou, Yongsheng Gao, Shengwu Xiong
- **Comment**: None
- **Journal**: None
- **Summary**: Robust facial landmark localization remains a challenging task when faces are partially occluded. Recently, the cascaded pose regression has attracted increasing attentions, due to it's superior performance in facial landmark localization and occlusion detection. However, such an approach is sensitive to initialization, where an improper initialization can severly degrade the performance. In this paper, we propose a Robust Initialization for Cascaded Pose Regression (RICPR) by providing texture and pose correlated initial shapes for the testing face. By examining the correlation of local binary patterns histograms between the testing face and the training faces, the shapes of the training faces that are most correlated with the testing face are selected as the texture correlated initialization. To make the initialization more robust to various poses, we estimate the rough pose of the testing face according to five fiducial landmarks located by multitask cascaded convolutional networks. Then the pose correlated initial shapes are constructed by the mean face's shape and the rough testing face pose. Finally, the texture correlated and the pose correlated initial shapes are joined together as the robust initialization. We evaluate RICPR on the challenging dataset of COFW. The experimental results demonstrate that the proposed scheme achieves better performances than the state-of-the-art methods in facial landmark localization and occlusion detection.



### Stories for Images-in-Sequence by using Visual and Narrative Components
- **Arxiv ID**: http://arxiv.org/abs/1805.05622v3
- **DOI**: 10.1007/978-3-030-00825-3_13
- **Categories**: **cs.AI**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1805.05622v3)
- **Published**: 2018-05-15 08:15:40+00:00
- **Updated**: 2018-09-22 23:49:05+00:00
- **Authors**: Marko Smilevski, Ilija Lalkovski, Gjorgji Madjarov
- **Comment**: 12 pages, 4 figures, ICT Innovations 2018
- **Journal**: ICT Innovations 2018. Engineering and Life Sciences. ICT 2018.
  Communications in Computer and Information Science, vol 940. Springer, Cham
  (2018) pp. 148-159
- **Summary**: Recent research in AI is focusing towards generating narrative stories about visual scenes. It has the potential to achieve more human-like understanding than just basic description generation of images- in-sequence. In this work, we propose a solution for generating stories for images-in-sequence that is based on the Sequence to Sequence model. As a novelty, our encoder model is composed of two separate encoders, one that models the behaviour of the image sequence and other that models the sentence-story generated for the previous image in the sequence of images. By using the image sequence encoder we capture the temporal dependencies between the image sequence and the sentence-story and by using the previous sentence-story encoder we achieve a better story flow. Our solution generates long human-like stories that not only describe the visual context of the image sequence but also contains narrative and evaluative language. The obtained results were confirmed by manual human evaluation.



### A Deeply-Recursive Convolutional Network for Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/1805.05633v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.05633v1)
- **Published**: 2018-05-15 08:24:53+00:00
- **Updated**: 2018-05-15 08:24:53+00:00
- **Authors**: Xinghao Ding, Zhirui Lin, Fujin He, Yu Wang, Yue Huang
- **Comment**: None
- **Journal**: None
- **Summary**: The estimation of crowd count in images has a wide range of applications such as video surveillance, traffic monitoring, public safety and urban planning. Recently, the convolutional neural network (CNN) based approaches have been shown to be more effective in crowd counting than traditional methods that use handcrafted features. However, the existing CNN-based methods still suffer from large number of parameters and large storage space, which require high storage and computing resources and thus limit the real-world application. Consequently, we propose a deeply-recursive network (DR-ResNet) based on ResNet blocks for crowd counting. The recursive structure makes the network deeper while keeping the number of parameters unchanged, which enhances network capability to capture statistical regularities in the context of the crowd. Besides, we generate a new dataset from the video-monitoring data of Beijing bus station. Experimental results have demonstrated that proposed method outperforms most state-of-the-art methods with far less number of parameters.



### Ro-SOS: Metric Expression Network (MEnet) for Robust Salient Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1805.05638v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.05638v3)
- **Published**: 2018-05-15 08:32:42+00:00
- **Updated**: 2020-01-21 20:41:52+00:00
- **Authors**: Delu Zeng, Yixuan He, Li Liu, Zhihong Chen, Jiabin Huang, Jie Chen, John Paisley
- **Comment**: This version: 11 pages (12 with reference), 12 figures, 5 table;
  Version 1: 7 pages,7 figures, 4 tables; The paper for version 1 has been
  accepted by International Joint Conference on Artificial Intelligence
  (IJCAI),2018
- **Journal**: None
- **Summary**: Although deep CNNs have brought significant improvement to image saliency detection, most CNN based models are sensitive to distortion such as compression and noise. In this paper, we propose an end-to-end generic salient object segmentation model called Metric Expression Network (MEnet) to deal with saliency detection with the tolerance of distortion. Within MEnet, a new topological metric space is constructed, whose implicit metric is determined by the deep network. As a result, we manage to group all the pixels in the observed image semantically within this latent space into two regions: a salient region and a non-salient region. With this architecture, all feature extractions are carried out at the pixel level, enabling fine granularity of output boundaries of the salient objects. What's more, we try to give a general analysis for the noise robustness of the network in the sense of Lipschitz and Jacobian literature. Experiments demonstrate that robust salient maps facilitating object segmentation can be generated by the proposed metric. Tests on several public benchmarks show that MEnet has achieved desirable performance. Furthermore, by direct computation and measuring the robustness, the proposed method outperforms previous CNN-based methods on distorted inputs.



### Semantic Cluster Unary Loss for Efficient Deep Hashing
- **Arxiv ID**: http://arxiv.org/abs/1805.08705v2
- **DOI**: 10.1109/TIP.2019.2891967
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.08705v2)
- **Published**: 2018-05-15 08:59:45+00:00
- **Updated**: 2019-02-02 04:35:44+00:00
- **Authors**: Shifeng Zhang, Jianmin Li, Bo Zhang
- **Comment**: 13 pages
- **Journal**: IEEE Transactions on Image Processing, 2019
- **Summary**: Hashing method maps similar data to binary hashcodes with smaller hamming distance, which has received a broad attention due to its low storage cost and fast retrieval speed. With the rapid development of deep learning, deep hashing methods have achieved promising results in efficient information retrieval. Most of the existing deep hashing methods adopt pairwise or triplet losses to deal with similarities underlying the data, but the training is difficult and less efficient because $O(n^2)$ data pairs and $O(n^3)$ triplets are involved. To address these issues, we propose a novel deep hashing algorithm with unary loss which can be trained very efficiently. We first of all introduce a Unary Upper Bound of the traditional triplet loss, thus reducing the complexity to $O(n)$ and bridging the classification-based unary loss and the triplet loss. Second, we propose a novel Semantic Cluster Deep Hashing (SCDH) algorithm by introducing a modified Unary Upper Bound loss, named Semantic Cluster Unary Loss (SCUL). The resultant hashcodes form several compact clusters, which means hashcodes in the same cluster have similar semantic information. We also demonstrate that the proposed SCDH is easy to be extended to semi-supervised settings by incorporating the state-of-the-art semi-supervised learning algorithms. Experiments on large-scale datasets show that the proposed method is superior to state-of-the-art hashing algorithms.



### Distribution-based Label Space Transformation for Multi-label Learning
- **Arxiv ID**: http://arxiv.org/abs/1805.05687v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.05687v1)
- **Published**: 2018-05-15 10:29:01+00:00
- **Updated**: 2018-05-15 10:29:01+00:00
- **Authors**: Zongting Lyu, Yan Yan, Fei Wu
- **Comment**: 11 pages, journal
- **Journal**: None
- **Summary**: Multi-label learning problems have manifested themselves in various machine learning applications. The key to successful multi-label learning algorithms lies in the exploration of inter-label correlations, which usually incur great computational cost. Another notable factor in multi-label learning is that the label vectors are usually extremely sparse, especially when the candidate label vocabulary is very large and only a few instances are assigned to each category. Recently, a label space transformation (LST) framework has been proposed targeting these challenges. However, current methods based on LST usually suffer from information loss in the label space dimension reduction process and fail to address the sparsity problem effectively. In this paper, we propose a distribution-based label space transformation (DLST) model. By defining the distribution based on the similarity of label vectors, a more comprehensive label structure can be captured. Then, by minimizing KL-divergence of two distributions, the information of the original label space can be approximately preserved in the latent space. Consequently, multi-label classifier trained using the dense latent codes yields better performance. The leverage of distribution enables DLST to fill out additional information about the label correlations. This endows DLST the capability to handle label set sparsity and training data sparsity in multi-label learning problems. With the optimal latent code, a kernel logistic regression function is learned for the mapping from feature space to the latent space. Then ML-KNN is employed to recover the original label vector from the transformed latent code. Extensive experiments on several benchmark datasets demonstrate that DLST not only achieves high classification performance but also is computationally more efficient.



### 2sRanking-CNN: A 2-stage ranking-CNN for diagnosis of glaucoma from fundus images using CAM-extracted ROI as an intermediate input
- **Arxiv ID**: http://arxiv.org/abs/1805.05727v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.05727v2)
- **Published**: 2018-05-15 12:27:00+00:00
- **Updated**: 2018-07-04 05:56:39+00:00
- **Authors**: Tae Joon Jun, Dohyeun Kim, Hoang Minh Nguyen, Daeyoung Kim, Youngsub Eom
- **Comment**: Accepted at BMVC 2018
- **Journal**: None
- **Summary**: Glaucoma is a disease in which the optic nerve is chronically damaged by the elevation of the intra-ocular pressure, resulting in visual field defect. Therefore, it is important to monitor and treat suspected patients before they are confirmed with glaucoma. In this paper, we propose a 2-stage ranking-CNN that classifies fundus images as normal, suspicious, and glaucoma. Furthermore, we propose a method of using the class activation map as a mask filter and combining it with the original fundus image as an intermediate input. Our results have improved the average accuracy by about 10% over the existing 3-class CNN and ranking-CNN, and especially improved the sensitivity of suspicious class by more than 20% over 3-class CNN. In addition, the extracted ROI was also found to overlap with the diagnostic criteria of the physician. The method we propose is expected to be efficiently applied to any medical data where there is a suspicious condition between normal and disease.



### Robust Adaptive Median Binary Pattern for noisy texture classification and retrieval
- **Arxiv ID**: http://arxiv.org/abs/1805.05732v1
- **DOI**: 10.1109/TIP.2019.2916742
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.05732v1)
- **Published**: 2018-05-15 12:41:48+00:00
- **Updated**: 2018-05-15 12:41:48+00:00
- **Authors**: Mohammad Alkhatib, Adel Hafiane
- **Comment**: None
- **Journal**: None
- **Summary**: Texture is an important cue for different computer vision tasks and applications. Local Binary Pattern (LBP) is considered one of the best yet efficient texture descriptors. However, LBP has some notable limitations, mostly the sensitivity to noise. In this paper, we address these criteria by introducing a novel texture descriptor, Robust Adaptive Median Binary Pattern (RAMBP). RAMBP based on classification process of noisy pixels, adaptive analysis window, scale analysis and image regions median comparison. The proposed method handles images with high noisy textures, and increases the discriminative properties by capturing microstructure and macrostructure texture information. The proposed method has been evaluated on popular texture datasets for classification and retrieval tasks, and under different high noise conditions. Without any train or prior knowledge of noise type, RAMBP achieved the best classification compared to state-of-the-art techniques. It scored more than $90\%$ under $50\%$ impulse noise densities, more than $95\%$ under Gaussian noised textures with standard deviation $\sigma = 5$, and more than $99\%$ under Gaussian blurred textures with standard deviation $\sigma = 1.25$. The proposed method yielded competitive results and high performance as one of the best descriptors in noise-free texture classification. Furthermore, RAMBP showed also high performance for the problem of noisy texture retrieval providing high scores of recall and precision measures for textures with high levels of noise.



### Multi-label Classification of Surgical Tools with Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1805.05760v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.05760v1)
- **Published**: 2018-05-15 13:39:54+00:00
- **Updated**: 2018-05-15 13:39:54+00:00
- **Authors**: Jonas Prellberg, Oliver Kramer
- **Comment**: Accepted at IJCNN 2018
- **Journal**: None
- **Summary**: Automatic tool detection from surgical imagery has a multitude of useful applications, such as real-time computer assistance for the surgeon. Using the successful residual network architecture, a system that can distinguish 21 different tools in cataract surgery videos is created. The videos are provided as part of the 2017 CATARACTS challenge and pose difficulties found in many real-world datasets, for example a strong class imbalance. The construction of the detection system is guided by a wide array of experiments that explore different design decisions.



### Robust and Efficient Graph Correspondence Transfer for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1805.06323v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.06323v1)
- **Published**: 2018-05-15 13:52:01+00:00
- **Updated**: 2018-05-15 13:52:01+00:00
- **Authors**: Qin Zhou, Heng Fan, Hua Yang, Hang Su, Shibao Zheng, Shuang Wu, Haibin Ling
- **Comment**: Tech. Report. The source code is available at
  http://www.dabi.temple.edu/~hbling/code/gct.htm. arXiv admin note: text
  overlap with arXiv:1804.00242
- **Journal**: None
- **Summary**: Spatial misalignment caused by variations in poses and viewpoints is one of the most critical issues that hinders the performance improvement in existing person re-identification (Re-ID) algorithms. To address this problem, in this paper, we present a robust and efficient graph correspondence transfer (REGCT) approach for explicit spatial alignment in Re-ID. Specifically, we propose to establish the patch-wise correspondences of positive training pairs via graph matching. By exploiting both spatial and visual contexts of human appearance in graph matching, meaningful semantic correspondences can be obtained. To circumvent the cumbersome \emph{on-line} graph matching in testing phase, we propose to transfer the \emph{off-line} learned patch-wise correspondences from the positive training pairs to test pairs. In detail, for each test pair, the training pairs with similar pose-pair configurations are selected as references. The matching patterns (i.e., the correspondences) of the selected references are then utilized to calculate the patch-wise feature distances of this test pair. To enhance the robustness of correspondence transfer, we design a novel pose context descriptor to accurately model human body configurations, and present an approach to measure the similarity between a pair of pose context descriptors. Meanwhile, to improve testing efficiency, we propose a correspondence template ensemble method using the voting mechanism, which significantly reduces the amount of patch-wise matchings involved in distance calculation. With aforementioned strategies, the REGCT model can effectively and efficiently handle the spatial misalignment problem in Re-ID. Extensive experiments on five challenging benchmarks, including VIPeR, Road, PRID450S, 3DPES and CUHK01, evidence the superior performance of REGCT over other state-of-the-art approaches.



### Efficient end-to-end learning for quantizable representations
- **Arxiv ID**: http://arxiv.org/abs/1805.05809v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.05809v3)
- **Published**: 2018-05-15 14:32:31+00:00
- **Updated**: 2018-06-12 03:11:59+00:00
- **Authors**: Yeonwoo Jeong, Hyun Oh Song
- **Comment**: Accepted and to appear at ICML 2018. Camera ready version
- **Journal**: None
- **Summary**: Embedding representation learning via neural networks is at the core foundation of modern similarity based search. While much effort has been put in developing algorithms for learning binary hamming code representations for search efficiency, this still requires a linear scan of the entire dataset per each query and trades off the search accuracy through binarization. To this end, we consider the problem of directly learning a quantizable embedding representation and the sparse binary hash code end-to-end which can be used to construct an efficient hash table not only providing significant search reduction in the number of data but also achieving the state of the art search accuracy outperforming previous state of the art deep metric learning methods. We also show that finding the optimal sparse binary hash code in a mini-batch can be computed exactly in polynomial time by solving a minimum cost flow problem. Our results on Cifar-100 and on ImageNet datasets show the state of the art search accuracy in precision@k and NMI metrics while providing up to 98X and 478X search speedup respectively over exhaustive linear search. The source code is available at https://github.com/maestrojeong/Deep-Hash-Table-ICML18



### Gradient-Leaks: Understanding and Controlling Deanonymization in Federated Learning
- **Arxiv ID**: http://arxiv.org/abs/1805.05838v3
- **DOI**: None
- **Categories**: **cs.CR**, cs.AI, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.05838v3)
- **Published**: 2018-05-15 15:12:45+00:00
- **Updated**: 2020-09-13 15:56:36+00:00
- **Authors**: Tribhuvanesh Orekondy, Seong Joon Oh, Yang Zhang, Bernt Schiele, Mario Fritz
- **Comment**: None
- **Journal**: None
- **Summary**: Federated Learning (FL) systems are gaining popularity as a solution to training Machine Learning (ML) models from large-scale user data collected on personal devices (e.g., smartphones) without their raw data leaving the device. At the core of FL is a network of anonymous user devices sharing training information (model parameter updates) computed locally on personal data. However, the type and degree to which user-specific information is encoded in the model updates is poorly understood. In this paper, we identify model updates encode subtle variations in which users capture and generate data. The variations provide a strong statistical signal, allowing an adversary to effectively deanonymize participating devices using a limited set of auxiliary data. We analyze resulting deanonymization attacks on diverse tasks on real-world (anonymized) user-generated data across a range of closed- and open-world scenarios. We study various strategies to mitigate the risks of deanonymization. As random perturbation methods do not offer convincing operating points, we propose data-augmentation strategies which introduces adversarial biases in device data and thereby, offer substantial protection against deanonymization threats with little effect on utility.



### Crick-net: A Convolutional Neural Network based Classification Approach for Detecting Waist High No Balls in Cricket
- **Arxiv ID**: http://arxiv.org/abs/1805.05974v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.05974v2)
- **Published**: 2018-05-15 18:15:26+00:00
- **Updated**: 2021-01-30 08:32:52+00:00
- **Authors**: Md. Harun-Ur-Rashid, Shekina Khatun, Mehe Zabin Trisha, Nafis Neehal, Md. Zahid Hasan
- **Comment**: Working on a different trajectory with this dataset
- **Journal**: None
- **Summary**: Cricket is undoubtedly one of the most popular games in this modern era. As human beings are prone to error, there remains a constant need for automated analysis and decision making of different events in this game. Simultaneously, with advent and advances in Artificial Intelligence and Computer Vision, application of these two in different domains has become an emerging trend. Applying several computer vision techniques in analyzing different Cricket events and automatically coming into decisions has become popular in recent days. In this paper, we have deployed a CNN based classification method with Inception V3 in order to automatically detect and differentiate waist high no balls with fair balls. Our approach achieves an overall average accuracy of 88% with a fairly low cross-entropy value.



### String Methods for Stochastic Image and Shape Matching
- **Arxiv ID**: http://arxiv.org/abs/1805.06038v3
- **DOI**: 10.1007/s10851-018-0823-z
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.06038v3)
- **Published**: 2018-05-15 21:26:35+00:00
- **Updated**: 2018-10-20 19:28:15+00:00
- **Authors**: Alexis Arnaudon, Darryl Holm, Stefan Sommer
- **Comment**: None
- **Journal**: J Math Imaging Vis (2018) 60: 953--967
- **Summary**: Matching of images and analysis of shape differences is traditionally pursued by energy minimization of paths of deformations acting to match the shape objects. In the Large Deformation Diffeomorphic Metric Mapping (LDDMM) framework, iterative gradient descents on the matching functional lead to matching algorithms informally known as Beg algorithms. When stochasticity is introduced to model stochastic variability of shapes and to provide more realistic models of observed shape data, the corresponding matching problem can be solved with a stochastic Beg algorithm, similar to the finite temperature string method used in rare event sampling. In this paper, we apply a stochastic model compatible with the geometry of the LDDMM framework to obtain a stochastic model of images and we derive the stochastic version of the Beg algorithm which we compare with the string method and an expectation-maximization optimization of posterior likelihoods. The algorithm and its use for statistical inference is tested on stochastic LDDMM landmarks and images.



### Vision-based Automated Bridge Component Recognition Integrated With High-level Scene Understanding
- **Arxiv ID**: http://arxiv.org/abs/1805.06041v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.06041v1)
- **Published**: 2018-05-15 21:37:47+00:00
- **Updated**: 2018-05-15 21:37:47+00:00
- **Authors**: Yasutaka Narazaki, Vedhus Hoskere, Tu A. Hoang, Billie F. Spencer
- **Comment**: None
- **Journal**: None
- **Summary**: Image data has a great potential of helping conventional visual inspections of civil engineering structures due to the ease of data acquisition and the advantages in capturing visual information. A variety of techniques have been proposed to detect damages, such as cracks and spalling on a close-up image of a single component (columns and road surfaces etc.). However, these techniques commonly suffer from severe false-positives especially when the image includes multiple components of different structures. To reduce the false-positives and extract reliable information about the structures' conditions, detection and localization of critical structural components are important first steps preceding the damage assessment. This study aims at recognizing bridge structural and non-structural components from images of urban scenes. During the bridge component recognition, every image pixel is classified into one of the five classes (non-bridge, columns, beams and slabs, other structural, other nonstructural) by multi-scale convolutional neural networks (multi-scale CNNs). To reduce false-positives and get consistent labels, the component classifications are integrated with scene understanding by an additional classifier with 10 higher-level scene classes (building, greenery, person, pavement, signs and poles, vehicles, bridges, water, sky, and others). The bridge component recognition integrated with the scene understanding is compared with the naive approach without scene classification in terms of accuracy, false-positives and consistencies to demonstrate the effectiveness of the integrated approach.



### Automated Vision-based Bridge Component Extraction Using Multiscale Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1805.06042v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.06042v1)
- **Published**: 2018-05-15 21:40:34+00:00
- **Updated**: 2018-05-15 21:40:34+00:00
- **Authors**: Yasutaka Narazaki, Vedhus Hoskere, Tu A. Hoang, Billie F. Spencer Jr
- **Comment**: None
- **Journal**: None
- **Summary**: Image data has a great potential of helping post-earthquake visual inspections of civil engineering structures due to the ease of data acquisition and the advantages in capturing visual information. A variety of techniques have been applied to detect damages automatically from a close-up image of a structural component. However, the application of the automatic damage detection methods become increasingly difficult when the image includes multiple components from different structures. To reduce the inaccurate false positive alarms, critical structural components need to be recognized first, and the damage alarms need to be cleaned using the component recognition results. To achieve the goal, this study aims at recognizing and extracting bridge components from images of urban scenes. The bridge component recognition begins with pixel-wise classifications of an image into 10 scene classes. Then, the original image and the scene classification results are combined to classify the image pixels into five component classes. The multi-scale convolutional neural networks (multi-scale CNNs) are used to perform pixel-wise classification, and the classification results are post-processed by averaging within superpixels and smoothing by conditional random fields (CRFs). The performance of the bridge component extraction is tested in terms of accuracy and consistency.



### Visual Representations for Semantic Target Driven Navigation
- **Arxiv ID**: http://arxiv.org/abs/1805.06066v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.06066v3)
- **Published**: 2018-05-15 23:26:52+00:00
- **Updated**: 2019-07-02 18:05:37+00:00
- **Authors**: Arsalan Mousavian, Alexander Toshev, Marek Fiser, Jana Kosecka, Ayzaan Wahid, James Davidson
- **Comment**: Accepted to ICRA 2019 and ECCV 2018 Workshop on Visual Learning and
  Embodied Agents in Simulation Environments
- **Journal**: None
- **Summary**: What is a good visual representation for autonomous agents? We address this question in the context of semantic visual navigation, which is the problem of a robot finding its way through a complex environment to a target object, e.g. go to the refrigerator. Instead of acquiring a metric semantic map of an environment and using planning for navigation, our approach learns navigation policies on top of representations that capture spatial layout and semantic contextual cues. We propose to using high level semantic and contextual features including segmentation and detection masks obtained by off-the-shelf state-of-the-art vision as observations and use deep network to learn the navigation policy. This choice allows using additional data, from orthogonal sources, to better train different parts of the model the representation extraction is trained on large standard vision datasets while the navigation component leverages large synthetic environments for training. This combination of real and synthetic is possible because equitable feature representations are available in both (e.g., segmentation and detection masks), which alleviates the need for domain adaptation. Both the representation and the navigation policy can be readily applied to real non-synthetic environments as demonstrated on the Active Vision Dataset [1]. Our approach gets successfully to the target in 54% of the cases in unexplored environments, compared to 46% for non-learning based approach, and 28% for the learning-based baseline.



