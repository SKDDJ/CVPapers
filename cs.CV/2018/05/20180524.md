# Arxiv Papers in cs.CV on 2018-05-24
### Cautious Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1805.09460v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.AI, cs.CV, cs.LG, stat.ME
- **Links**: [PDF](http://arxiv.org/pdf/1805.09460v2)
- **Published**: 2018-05-24 00:17:24+00:00
- **Updated**: 2019-02-27 19:27:58+00:00
- **Authors**: Yotam Hechtlinger, Barnabás Póczos, Larry Wasserman
- **Comment**: None
- **Journal**: None
- **Summary**: Most classifiers operate by selecting the maximum of an estimate of the conditional distribution $p(y|x)$ where $x$ stands for the features of the instance to be classified and $y$ denotes its label. This often results in a {\em hubristic bias}: overconfidence in the assignment of a definite label. Usually, the observations are concentrated on a small volume but the classifier provides definite predictions for the entire space. We propose constructing conformal prediction sets which contain a set of labels rather than a single label. These conformal prediction sets contain the true label with probability $1-\alpha$. Our construction is based on $p(x|y)$ rather than $p(y|x)$ which results in a classifier that is very cautious: it outputs the null set --- meaning "I don't know" --- when the object does not resemble the training examples. An important property of our approach is that adversarial attacks are likely to be predicted as the null set or would also include the true label. We demonstrate the performance on the ImageNet ILSVRC dataset and the CelebA and IMDB-Wiki facial datasets using high dimensional features obtained from state of the art convolutional neural networks.



### Complex Relations in a Deep Structured Prediction Model for Fine Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1805.09462v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.09462v1)
- **Published**: 2018-05-24 00:21:40+00:00
- **Updated**: 2018-05-24 00:21:40+00:00
- **Authors**: Cristina Mata, Guy Ben-Yosef, Boris Katz
- **Comment**: None
- **Journal**: None
- **Summary**: Many deep learning architectures for semantic segmentation involve a Fully Convolutional Neural Network (FCN) followed by a Conditional Random Field (CRF) to carry out inference over an image. These models typically involve unary potentials based on local appearance features computed by FCNs, and binary potentials based on the displacement between pixels. We show that while current methods succeed in segmenting whole objects, they perform poorly in situations involving a large number of object parts. We therefore suggest incorporating into the inference algorithm additional higher-order potentials inspired by the way humans identify and localize parts. We incorporate two relations that were shown to be useful to human object identification - containment and attachment - into the energy term of the CRF and evaluate their performance on the Pascal VOC Parts dataset. Our experimental results show that the segmentation of fine parts is positively affected by the addition of these two relations, and that the segmentation of fine parts can be further influenced by complex structural features.



### VisualBackProp for learning using privileged information with CNNs
- **Arxiv ID**: http://arxiv.org/abs/1805.09474v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.09474v1)
- **Published**: 2018-05-24 01:19:52+00:00
- **Updated**: 2018-05-24 01:19:52+00:00
- **Authors**: Devansh Bisla, Anna Choromanska
- **Comment**: None
- **Journal**: None
- **Summary**: In many machine learning applications, from medical diagnostics to autonomous driving, the availability of prior knowledge can be used to improve the predictive performance of learning algorithms and incorporate `physical,' `domain knowledge,' or `common sense' concepts into training of machine learning systems as well as verify constraints/properties of the systems. We explore the learning using privileged information paradigm and show how to incorporate the privileged information, such as segmentation mask available along with the classification label of each example, into the training stage of convolutional neural networks. This is done by augmenting the CNN model with an architectural component that effectively focuses model's attention on the desired region of the input image during the training process and that is transparent to the network's label prediction mechanism at testing. This component effectively corresponds to the visualization strategy for identifying the parts of the input, often referred to as visualization mask, that most contribute to the prediction, yet uses this strategy in reverse to the classical setting in order to enforce the desired visualization mask instead. We verify our proposed algorithms through exhaustive experiments on benchmark ImageNet and PASCAL VOC data sets and achieve improvements in the performance of $2.4\%$ and $2.7\%$ over standard single-supervision model training. Finally, we confirm the effectiveness of our approach on skin lesion classification problem.



### Cross Domain Image Generation through Latent Space Exploration with Adversarial Loss
- **Arxiv ID**: http://arxiv.org/abs/1805.10130v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.10130v1)
- **Published**: 2018-05-24 04:02:26+00:00
- **Updated**: 2018-05-24 04:02:26+00:00
- **Authors**: Yingjing Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Conditional domain generation is a good way to interactively control sample generation process of deep generative models. However, once a conditional generative model has been created, it is often expensive to allow it to adapt to new conditional controls, especially the network structure is relatively deep. We propose a conditioned latent domain transfer framework across latent spaces of unconditional variational autoencoders(VAE). With this framework, we can allow unconditionally trained VAEs to generate images in its domain with conditionals provided by a latent representation of another domain. This framework does not assume commonalities between two domains. We demonstrate effectiveness and robustness of our model under widely used image datasets.



### AutoAugment: Learning Augmentation Policies from Data
- **Arxiv ID**: http://arxiv.org/abs/1805.09501v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.09501v3)
- **Published**: 2018-05-24 04:05:42+00:00
- **Updated**: 2019-04-11 22:39:27+00:00
- **Authors**: Ekin D. Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, Quoc V. Le
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: Data augmentation is an effective technique for improving the accuracy of modern image classifiers. However, current data augmentation implementations are manually designed. In this paper, we describe a simple procedure called AutoAugment to automatically search for improved data augmentation policies. In our implementation, we have designed a search space where a policy consists of many sub-policies, one of which is randomly chosen for each image in each mini-batch. A sub-policy consists of two operations, each operation being an image processing function such as translation, rotation, or shearing, and the probabilities and magnitudes with which the functions are applied. We use a search algorithm to find the best policy such that the neural network yields the highest validation accuracy on a target dataset. Our method achieves state-of-the-art accuracy on CIFAR-10, CIFAR-100, SVHN, and ImageNet (without additional data). On ImageNet, we attain a Top-1 accuracy of 83.5% which is 0.4% better than the previous record of 83.1%. On CIFAR-10, we achieve an error rate of 1.5%, which is 0.6% better than the previous state-of-the-art. Augmentation policies we find are transferable between datasets. The policy learned on ImageNet transfers well to achieve significant improvements on other datasets, such as Oxford Flowers, Caltech-101, Oxford-IIT Pets, FGVC Aircraft, and Stanford Cars.



### Estimating Carotid Pulse and Breathing Rate from Near-infrared Video of the Neck
- **Arxiv ID**: http://arxiv.org/abs/1805.09511v1
- **DOI**: 10.1088/1361-6579/aae625
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1805.09511v1)
- **Published**: 2018-05-24 05:15:18+00:00
- **Updated**: 2018-05-24 05:15:18+00:00
- **Authors**: Weixuan Chen, Javier Hernandez, Rosalind W. Picard
- **Comment**: 21 pages, 15 figures
- **Journal**: None
- **Summary**: Objective: Non-contact physiological measurement is a growing research area that allows capturing vital signs such as heart rate (HR) and breathing rate (BR) comfortably and unobtrusively with remote devices. However, most of the approaches work only in bright environments in which subtle photoplethysmographic and ballistocardiographic signals can be easily analyzed and/or require expensive and custom hardware to perform the measurements.   Approach: This work introduces a low-cost method to measure subtle motions associated with the carotid pulse and breathing movement from the neck using near-infrared (NIR) video imaging. A skin reflection model of the neck was established to provide a theoretical foundation for the method. In particular, the method relies on template matching for neck detection, Principal Component Analysis for feature extraction, and Hidden Markov Models for data smoothing.   Main Results: We compared the estimated HR and BR measures with ones provided by an FDA-cleared device in a 12-participant laboratory study: the estimates achieved a mean absolute error of 0.36 beats per minute and 0.24 breaths per minute under both bright and dark lighting.   Significance: This work advances the possibilities of non-contact physiological measurement in real-life conditions in which environmental illumination is limited and in which the face of the person is not readily available or needs to be protected. Due to the increasing availability of NIR imaging devices, the described methods are readily scalable.



### You Only Look Twice: Rapid Multi-Scale Object Detection In Satellite Imagery
- **Arxiv ID**: http://arxiv.org/abs/1805.09512v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.09512v1)
- **Published**: 2018-05-24 05:17:51+00:00
- **Updated**: 2018-05-24 05:17:51+00:00
- **Authors**: Adam Van Etten
- **Comment**: 8 pages, 14 figures, 3 tables
- **Journal**: None
- **Summary**: Detection of small objects in large swaths of imagery is one of the primary problems in satellite imagery analytics. While object detection in ground-based imagery has benefited from research into new deep learning approaches, transitioning such technology to overhead imagery is nontrivial. Among the challenges is the sheer number of pixels and geographic extent per image: a single DigitalGlobe satellite image encompasses >64 km2 and over 250 million pixels. Another challenge is that objects of interest are minuscule (often only ~10 pixels in extent), which complicates traditional computer vision techniques. To address these issues, we propose a pipeline (You Only Look Twice, or YOLT) that evaluates satellite images of arbitrary size at a rate of >0.5 km2/s. The proposed approach can rapidly detect objects of vastly different scales with relatively little training data over multiple sensors. We evaluate large test images at native resolution, and yield scores of F1 > 0.8 for vehicle localization. We further explore resolution and object size requirements by systematically testing the pipeline at decreasing resolution, and conclude that objects only ~5 pixels in size can still be localized with high confidence. Code is available at https://github.com/CosmiQ/yolt.



### AVID: Adversarial Visual Irregularity Detection
- **Arxiv ID**: http://arxiv.org/abs/1805.09521v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.09521v2)
- **Published**: 2018-05-24 06:23:45+00:00
- **Updated**: 2018-07-17 22:37:53+00:00
- **Authors**: Mohammad Sabokrou, Masoud Pourreza, Mohsen Fayyaz, Rahim Entezari, Mahmood Fathy, Jürgen Gall, Ehsan Adeli
- **Comment**: None
- **Journal**: None
- **Summary**: Real-time detection of irregularities in visual data is very invaluable and useful in many prospective applications including surveillance, patient monitoring systems, etc. With the surge of deep learning methods in the recent years, researchers have tried a wide spectrum of methods for different applications. However, for the case of irregularity or anomaly detection in videos, training an end-to-end model is still an open challenge, since often irregularity is not well-defined and there are not enough irregular samples to use during training. In this paper, inspired by the success of generative adversarial networks (GANs) for training deep models in unsupervised or self-supervised settings, we propose an end-to-end deep network for detection and fine localization of irregularities in videos (and images). Our proposed architecture is composed of two networks, which are trained in competing with each other while collaborating to find the irregularity. One network works as a pixel-level irregularity Inpainter, and the other works as a patch-level Detector. After an adversarial self-supervised training, in which I tries to fool D into accepting its inpainted output as regular (normal), the two networks collaborate to detect and fine-segment the irregularity in any given testing video. Our results on three different datasets show that our method can outperform the state-of-the-art and fine-segment the irregularity.



### Corpus Conversion Service: A Machine Learning Platform to Ingest Documents at Scale
- **Arxiv ID**: http://arxiv.org/abs/1806.02284v1
- **DOI**: 10.1145/3219819.3219834
- **Categories**: **cs.DL**, cs.CV, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/1806.02284v1)
- **Published**: 2018-05-24 09:44:07+00:00
- **Updated**: 2018-05-24 09:44:07+00:00
- **Authors**: Peter W J Staar, Michele Dolfi, Christoph Auer, Costas Bekas
- **Comment**: Accepted paper at KDD 2018 conference
- **Journal**: None
- **Summary**: Over the past few decades, the amount of scientific articles and technical literature has increased exponentially in size. Consequently, there is a great need for systems that can ingest these documents at scale and make the contained knowledge discoverable. Unfortunately, both the format of these documents (e.g. the PDF format or bitmap images) as well as the presentation of the data (e.g. complex tables) make the extraction of qualitative and quantitive data extremely challenging. In this paper, we present a modular, cloud-based platform to ingest documents at scale. This platform, called the Corpus Conversion Service (CCS), implements a pipeline which allows users to parse and annotate documents (i.e. collect ground-truth), train machine-learning classification algorithms and ultimately convert any type of PDF or bitmap-documents to a structured content representation format. We will show that each of the modules is scalable due to an asynchronous microservice architecture and can therefore handle massive amounts of documents. Furthermore, we will show that our capability to gather ground-truth is accelerated by machine-learning algorithms by at least one order of magnitude. This allows us to both gather large amounts of ground-truth in very little time and obtain very good precision/recall metrics in the range of 99\% with regard to content conversion to structured output. The CCS platform is currently deployed on IBM internal infrastructure and serving more than 250 active users for knowledge-engineering project engagements.



### Coarse-to-fine Seam Estimation for Image Stitching
- **Arxiv ID**: http://arxiv.org/abs/1805.09578v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.09578v1)
- **Published**: 2018-05-24 09:49:06+00:00
- **Updated**: 2018-05-24 09:49:06+00:00
- **Authors**: Tianli Liao, Jing Chen, Yifang Xu
- **Comment**: 5 pages, 4 figures
- **Journal**: None
- **Summary**: Seam-cutting and seam-driven techniques have been proven effective for handling imperfect image series in image stitching. Generally, seam-driven is to utilize seam-cutting to find a best seam from one or finite alignment hypotheses based on a predefined seam quality metric. However, the quality metrics in most methods are defined to measure the average performance of the pixels on the seam without considering the relevance and variance among them. This may cause that the seam with the minimal measure is not optimal (perception-inconsistent) in human perception. In this paper, we propose a novel coarse-to-fine seam estimation method which applies the evaluation in a different way. For pixels on the seam, we develop a patch-point evaluation algorithm concentrating more on the correlation and variation of them. The evaluations are then used to recalculate the difference map of the overlapping region and reestimate a stitching seam. This evaluation-reestimation procedure iterates until the current seam changes negligibly comparing with the previous seams. Experiments show that our proposed method can finally find a nearly perception-consistent seam after several iterations, which outperforms the conventional seam-cutting and other seam-driven methods.



### Residual Networks as Geodesic Flows of Diffeomorphisms
- **Arxiv ID**: http://arxiv.org/abs/1805.09585v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1805.09585v2)
- **Published**: 2018-05-24 10:07:46+00:00
- **Updated**: 2018-06-22 13:02:03+00:00
- **Authors**: Francois Rousseau, Ronan Fablet
- **Comment**: None
- **Journal**: None
- **Summary**: This paper addresses the understanding and characterization of residual networks (ResNet), which are among the state-of-the-art deep learning architectures for a variety of supervised learning problems. We focus on the mapping component of ResNets, which map the embedding space towards a new unknown space where the prediction or classification can be stated according to linear criteria. We show that this mapping component can be regarded as the numerical implementation of continuous flows of diffeomorphisms governed by ordinary differential equations. Especially, ResNets with shared weights are fully characterized as numerical approximation of exponential diffeomorphic operators. We stress both theoretically and numerically the relevance of the enforcement of diffeormorphic properties and the importance of numerical issues to make consistent the continuous formulation and the discretized ResNet implementation. We further discuss the resulting theoretical and computational insights on ResNet architectures.



### Multi-Scale DenseNet-Based Electricity Theft Detection
- **Arxiv ID**: http://arxiv.org/abs/1805.09591v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.09591v1)
- **Published**: 2018-05-24 10:25:16+00:00
- **Updated**: 2018-05-24 10:25:16+00:00
- **Authors**: Bo Li, Kele Xu, Xiaoyan Cui, Yiheng Wang, Xinbo Ai, Yanbo Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Electricity theft detection issue has drawn lots of attention during last decades. Timely identification of the electricity theft in the power system is crucial for the safety and availability of the system. Although sustainable efforts have been made, the detection task remains challenging and falls short of accuracy and efficiency, especially with the increase of the data size. Recently, convolutional neural network-based methods have achieved better performance in comparison with traditional methods, which employ handcrafted features and shallow-architecture classifiers. In this paper, we present a novel approach for automatic detection by using a multi-scale dense connected convolution neural network (multi-scale DenseNet) in order to capture the long-term and short-term periodic features within the sequential data. We compare the proposed approaches with the classical algorithms, and the experimental results demonstrate that the multiscale DenseNet approach can significantly improve the accuracy of the detection. Moreover, our method is scalable, enabling larger data processing while no handcrafted feature engineering is needed.



### Backpropagation with N-D Vector-Valued Neurons Using Arbitrary Bilinear Products
- **Arxiv ID**: http://arxiv.org/abs/1805.09621v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.09621v1)
- **Published**: 2018-05-24 12:01:53+00:00
- **Updated**: 2018-05-24 12:01:53+00:00
- **Authors**: Zhe-Cheng Fan, Tak-Shing T. Chan, Yi-Hsuan Yang, Jyh-Shing R. Jang
- **Comment**: 14 pages, 8 figures, 3 tables
- **Journal**: None
- **Summary**: Vector-valued neural learning has emerged as a promising direction in deep learning recently. Traditionally, training data for neural networks (NNs) are formulated as a vector of scalars; however, its performance may not be optimal since associations among adjacent scalars are not modeled. In this paper, we propose a new vector neural architecture called the Arbitrary BIlinear Product Neural Network (ABIPNN), which processes information as vectors in each neuron, and the feedforward projections are defined using arbitrary bilinear products. Such bilinear products can include circular convolution, seven-dimensional vector product, skew circular convolution, reversed- time circular convolution, or other new products not seen in previous work. As a proof-of-concept, we apply our proposed network to multispectral image denoising and singing voice sepa- ration. Experimental results show that ABIPNN gains substantial improvements when compared to conventional NNs, suggesting that associations are learned during training.



### SOSELETO: A Unified Approach to Transfer Learning and Training with Noisy Labels
- **Arxiv ID**: http://arxiv.org/abs/1805.09622v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1805.09622v2)
- **Published**: 2018-05-24 12:03:58+00:00
- **Updated**: 2019-05-09 15:41:44+00:00
- **Authors**: Or Litany, Daniel Freedman
- **Comment**: ICLR workshop on Learning from Limited Labeled Data (LLD) -- Best
  Paper Award
- **Journal**: None
- **Summary**: We present SOSELETO (SOurce SELEction for Target Optimization), a new method for exploiting a source dataset to solve a classification problem on a target dataset. SOSELETO is based on the following simple intuition: some source examples are more informative than others for the target problem. To capture this intuition, source samples are each given weights; these weights are solved for jointly with the source and target classification problems via a bilevel optimization scheme. The target therefore gets to choose the source samples which are most informative for its own classification task. Furthermore, the bilevel nature of the optimization acts as a kind of regularization on the target, mitigating overfitting. SOSELETO may be applied to both classic transfer learning, as well as the problem of training on datasets with noisy labels; we show state of the art results on both of these problems.



### LF-Net: Learning Local Features from Images
- **Arxiv ID**: http://arxiv.org/abs/1805.09662v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.09662v2)
- **Published**: 2018-05-24 13:42:17+00:00
- **Updated**: 2018-11-22 11:10:42+00:00
- **Authors**: Yuki Ono, Eduard Trulls, Pascal Fua, Kwang Moo Yi
- **Comment**: NIPS 2018
- **Journal**: None
- **Summary**: We present a novel deep architecture and a training strategy to learn a local feature pipeline from scratch, using collections of images without the need for human supervision. To do so we exploit depth and relative camera pose cues to create a virtual target that the network should achieve on one image, provided the outputs of the network for the other image. While this process is inherently non-differentiable, we show that we can optimize the network in a two-branch setup by confining it to one branch, while preserving differentiability in the other. We train our method on both indoor and outdoor datasets, with depth data from 3D sensors for the former, and depth estimates from an off-the-shelf Structure-from-Motion solution for the latter. Our models outperform the state of the art on sparse feature matching on both datasets, while running at 60+ fps for QVGA images.



### Deep Residual Networks with a Fully Connected Recon-struction Layer for Single Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1805.10143v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.10143v2)
- **Published**: 2018-05-24 14:11:19+00:00
- **Updated**: 2019-05-17 16:48:09+00:00
- **Authors**: Yongliang Tang, Jiashui Huang, Faen Zhang, Weiguo Gong
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, deep neural networks have achieved impressive performance in terms of both reconstruction accuracy and efficiency for single image super-resolution (SISR). However, the network model of these methods is a fully convolutional neural network, which is limit to exploit the differentiated contextual information over the global region of the input image because of the weight sharing in convolution height and width extent. In this paper, we discuss a new SISR architecture where features are extracted in the low-resolution (LR) space, and then we use a fully connected layer which learns an array of differentiated upsampling weights to reconstruct the desired high-resolution (HR) image from the final obtained LR features. By doing so, we effectively exploit the differentiated contextual information over the whole input image region, whilst maintaining the low computational complexity for the overall SR operations. In addition, we introduce an edge difference constraint into our loss function to preserve edges and texture structures. Extensive experiments validate that our SISR method outperforms the existing state-of-the-art methods.



### Dictionary Learning for Adaptive GPR Landmine Classification
- **Arxiv ID**: http://arxiv.org/abs/1806.04599v2
- **DOI**: 10.1109/TGRS.2019.2931134
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.04599v2)
- **Published**: 2018-05-24 14:42:42+00:00
- **Updated**: 2019-06-12 10:08:05+00:00
- **Authors**: Fabio Giovanneschi, Kumar Vijay Mishra, Maria Antonia Gonzalez-Huici, Yonina C. Eldar, Joachim H. G. Ender
- **Comment**: 16 pages, 11 figures, 10 tables
- **Journal**: None
- **Summary**: Ground penetrating radar (GPR) target detection and classification is a challenging task. Here, we consider online dictionary learning (DL) methods to obtain sparse representations (SR) of the GPR data to enhance feature extraction for target classification via support vector machines. Online methods are preferred because traditional batch DL like K-SVD is not scalable to high-dimensional training sets and infeasible for real-time operation. We also develop Drop-Off MINi-batch Online Dictionary Learning (DOMINODL) which exploits the fact that a lot of the training data may be correlated. The DOMINODL algorithm iteratively considers elements of the training set in small batches and drops off samples which become less relevant. For the case of abandoned anti-personnel landmines classification, we compare the performance of K-SVD with three online algorithms: classical Online Dictionary Learning, its correlation-based variant, and DOMINODL. Our experiments with real data from L-band GPR show that online DL methods reduce learning time by 36-93% and increase mine detection by 4-28% over K-SVD. Our DOMINODL is the fastest and retains similar classification performance as the other two online DL approaches. We use a Kolmogorov-Smirnoff test distance and the Dvoretzky-Kiefer-Wolfowitz inequality for the selection of DL input parameters leading to enhanced classification results. To further compare with state-of-the-art classification approaches, we evaluate a convolutional neural network (CNN) classifier which performs worse than the proposed approach. Moreover, when the acquired samples are randomly reduced by 25%, 50% and 75%, sparse decomposition based classification with DL remains robust while the CNN accuracy is drastically compromised.



### R-VQA: Learning Visual Relation Facts with Semantic Attention for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1805.09701v2
- **DOI**: 10.1145/3219819.3220036
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1805.09701v2)
- **Published**: 2018-05-24 14:43:30+00:00
- **Updated**: 2018-07-20 03:45:04+00:00
- **Authors**: Pan Lu, Lei Ji, Wei Zhang, Nan Duan, Ming Zhou, Jianyong Wang
- **Comment**: 10 pages, 5 figures, accepted as an oral paper in SIGKDD 2018
- **Journal**: None
- **Summary**: Recently, Visual Question Answering (VQA) has emerged as one of the most significant tasks in multimodal learning as it requires understanding both visual and textual modalities. Existing methods mainly rely on extracting image and question features to learn their joint feature embedding via multimodal fusion or attention mechanism. Some recent studies utilize external VQA-independent models to detect candidate entities or attributes in images, which serve as semantic knowledge complementary to the VQA task. However, these candidate entities or attributes might be unrelated to the VQA task and have limited semantic capacities. To better utilize semantic knowledge in images, we propose a novel framework to learn visual relation facts for VQA. Specifically, we build up a Relation-VQA (R-VQA) dataset based on the Visual Genome dataset via a semantic similarity module, in which each data consists of an image, a corresponding question, a correct answer and a supporting relation fact. A well-defined relation detector is then adopted to predict visual question-related relation facts. We further propose a multi-step attention model composed of visual attention and semantic attention sequentially to extract related visual knowledge and semantic knowledge. We conduct comprehensive experiments on the two benchmark datasets, demonstrating that our model achieves state-of-the-art performance and verifying the benefit of considering visual relation facts.



### Jointly Optimize Data Augmentation and Network Training: Adversarial Data Augmentation in Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1805.09707v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.09707v1)
- **Published**: 2018-05-24 14:53:04+00:00
- **Updated**: 2018-05-24 14:53:04+00:00
- **Authors**: Xi Peng, Zhiqiang Tang, Fei Yang, Rogerio Feris, Dimitris Metaxas
- **Comment**: CVPR 2018
- **Journal**: None
- **Summary**: Random data augmentation is a critical technique to avoid overfitting in training deep neural network models. However, data augmentation and network training are usually treated as two isolated processes, limiting the effectiveness of network training. Why not jointly optimize the two? We propose adversarial data augmentation to address this limitation. The main idea is to design an augmentation network (generator) that competes against a target network (discriminator) by generating `hard' augmentation operations online. The augmentation network explores the weaknesses of the target network, while the latter learns from `hard' augmentations to achieve better performance. We also design a reward/penalty strategy for effective joint training. We demonstrate our approach on the problem of human pose estimation and carry out a comprehensive experimental analysis, showing that our method can significantly improve state-of-the-art models without additional data efforts.



### Image-to-image translation for cross-domain disentanglement
- **Arxiv ID**: http://arxiv.org/abs/1805.09730v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.09730v3)
- **Published**: 2018-05-24 15:30:23+00:00
- **Updated**: 2018-11-04 17:27:04+00:00
- **Authors**: Abel Gonzalez-Garcia, Joost van de Weijer, Yoshua Bengio
- **Comment**: Accepted to NIPS 2018
- **Journal**: None
- **Summary**: Deep image translation methods have recently shown excellent results, outputting high-quality images covering multiple modes of the data distribution. There has also been increased interest in disentangling the internal representations learned by deep methods to further improve their performance and achieve a finer control. In this paper, we bridge these two objectives and introduce the concept of cross-domain disentanglement. We aim to separate the internal representation into three parts. The shared part contains information for both domains. The exclusive parts, on the other hand, contain only factors of variation that are particular to each domain. We achieve this through bidirectional image translation based on Generative Adversarial Networks and cross-domain autoencoders, a novel network component. Our model offers multiple advantages. We can output diverse samples covering multiple modes of the distributions of both domains, perform domain-specific image transfer and interpolation, and cross-domain retrieval without the need of labeled data, only paired images. We compare our model to the state-of-the-art in multi-modal image translation and achieve better results for translation on challenging datasets as well as for cross-domain retrieval on realistic datasets.



### MobiFace: A Novel Dataset for Mobile Face Tracking in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1805.09749v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.09749v2)
- **Published**: 2018-05-24 15:59:22+00:00
- **Updated**: 2019-01-22 12:39:19+00:00
- **Authors**: Yiming Lin, Shiyang Cheng, Jie Shen, Maja Pantic
- **Comment**: To appear on The 14th IEEE International Conference on Automatic Face
  and Gesture Recognition (FG 2019)
- **Journal**: None
- **Summary**: Face tracking serves as the crucial initial step in mobile applications trying to analyse target faces over time in mobile settings. However, this problem has received little attention, mainly due to the scarcity of dedicated face tracking benchmarks. In this work, we introduce MobiFace, the first dataset for single face tracking in mobile situations. It consists of 80 unedited live-streaming mobile videos captured by 70 different smartphone users in fully unconstrained environments. Over $95K$ bounding boxes are manually labelled. The videos are carefully selected to cover typical smartphone usage. The videos are also annotated with 14 attributes, including 6 newly proposed attributes and 8 commonly seen in object tracking. 36 state-of-the-art trackers, including facial landmark trackers, generic object trackers and trackers that we have fine-tuned or improved, are evaluated. The results suggest that mobile face tracking cannot be solved through existing approaches. In addition, we show that fine-tuning on the MobiFace training data significantly boosts the performance of deep learning-based trackers, suggesting that MobiFace captures the unique characteristics of mobile face tracking. Our goal is to offer the community a diverse dataset to enable the design and evaluation of mobile face trackers. The dataset, annotations and the evaluation server will be on \url{https://mobiface.github.io/}.



### Multi-Task Zipping via Layer-wise Neuron Sharing
- **Arxiv ID**: http://arxiv.org/abs/1805.09791v2
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1805.09791v2)
- **Published**: 2018-05-24 17:33:38+00:00
- **Updated**: 2019-03-13 09:05:31+00:00
- **Authors**: Xiaoxi He, Zimu Zhou, Lothar Thiele
- **Comment**: Published as a conference paper at NeurIPS 2018
- **Journal**: None
- **Summary**: Future mobile devices are anticipated to perceive, understand and react to the world on their own by running multiple correlated deep neural networks on-device. Yet the complexity of these neural networks needs to be trimmed down both within-model and cross-model to fit in mobile storage and memory. Previous studies focus on squeezing the redundancy within a single neural network. In this work, we aim to reduce the redundancy across multiple models. We propose Multi-Task Zipping (MTZ), a framework to automatically merge correlated, pre-trained deep neural networks for cross-model compression. Central in MTZ is a layer-wise neuron sharing and incoming weight updating scheme that induces a minimal change in the error function. MTZ inherits information from each model and demands light retraining to re-boost the accuracy of individual tasks. Evaluations show that MTZ is able to fully merge the hidden layers of two VGG-16 networks with a 3.18% increase in the test error averaged on ImageNet and CelebA, or share 39.61% parameters between the two networks with <0.5% increase in the test errors for both tasks. The number of iterations to retrain the combined network is at least 17.8 times lower than that of training a single VGG-16 network. Moreover, experiments show that MTZ is also able to effectively merge multiple residual networks.



### Prediction of Autism Treatment Response from Baseline fMRI using Random Forests and Tree Bagging
- **Arxiv ID**: http://arxiv.org/abs/1805.09799v1
- **DOI**: None
- **Categories**: **stat.AP**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1805.09799v1)
- **Published**: 2018-05-24 17:41:40+00:00
- **Updated**: 2018-05-24 17:41:40+00:00
- **Authors**: Nicha C. Dvornek, Daniel Yang, Archana Venkataraman, Pamela Ventola, Lawrence H. Staib, Kevin A. Pelphrey, James S. Duncan
- **Comment**: Multimodal Learning for Clinical Decision Support (ML-CDS) 2016
- **Journal**: None
- **Summary**: Treating children with autism spectrum disorders (ASD) with behavioral interventions, such as Pivotal Response Treatment (PRT), has shown promise in recent studies. However, deciding which therapy is best for a given patient is largely by trial and error, and choosing an ineffective intervention results in loss of valuable treatment time. We propose predicting patient response to PRT from baseline task-based fMRI by the novel application of a random forest and tree bagging strategy. Our proposed learning pipeline uses random forest regression to determine candidate brain voxels that may be informative in predicting treatment response. The candidate voxels are then tested stepwise for inclusion in a bagged tree ensemble. After the predictive model is constructed, bias correction is performed to further increase prediction accuracy. Using data from 19 ASD children who underwent a 16 week trial of PRT and a leave-one-out cross-validation framework, the presented learning pipeline was tested against several standard methods and variations of the pipeline and resulted in the highest prediction accuracy.



### Competitive Collaboration: Joint Unsupervised Learning of Depth, Camera Motion, Optical Flow and Motion Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1805.09806v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.09806v3)
- **Published**: 2018-05-24 17:49:05+00:00
- **Updated**: 2019-03-11 19:03:56+00:00
- **Authors**: Anurag Ranjan, Varun Jampani, Lukas Balles, Kihwan Kim, Deqing Sun, Jonas Wulff, Michael J. Black
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: We address the unsupervised learning of several interconnected problems in low-level vision: single view depth prediction, camera motion estimation, optical flow, and segmentation of a video into the static scene and moving regions. Our key insight is that these four fundamental vision problems are coupled through geometric constraints. Consequently, learning to solve them together simplifies the problem because the solutions can reinforce each other. We go beyond previous work by exploiting geometry more explicitly and segmenting the scene into static and moving regions. To that end, we introduce Competitive Collaboration, a framework that facilitates the coordinated training of multiple specialized neural networks to solve complex problems. Competitive Collaboration works much like expectation-maximization, but with neural networks that act as both competitors to explain pixels that correspond to static or moving regions, and as collaborators through a moderator that assigns pixels to be either static or independently moving. Our novel method integrates all these problems in a common framework and simultaneously reasons about the segmentation of the scene into moving objects and the static background, the camera motion, depth of the static scene structure, and the optical flow of moving objects. Our model is trained without any supervision and achieves state-of-the-art performance among joint unsupervised methods on all sub-problems.



### Stereo Magnification: Learning View Synthesis using Multiplane Images
- **Arxiv ID**: http://arxiv.org/abs/1805.09817v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1805.09817v1)
- **Published**: 2018-05-24 17:58:02+00:00
- **Updated**: 2018-05-24 17:58:02+00:00
- **Authors**: Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, Noah Snavely
- **Comment**: Accepted to SIGGRAPH 2018. Project webpage:
  https://people.eecs.berkeley.edu/~tinghuiz/projects/mpi/
- **Journal**: None
- **Summary**: The view synthesis problem--generating novel views of a scene from known imagery--has garnered recent attention due in part to compelling applications in virtual and augmented reality. In this paper, we explore an intriguing scenario for view synthesis: extrapolating views from imagery captured by narrow-baseline stereo cameras, including VR cameras and now-widespread dual-lens camera phones. We call this problem stereo magnification, and propose a learning framework that leverages a new layered representation that we call multiplane images (MPIs). Our method also uses a massive new data source for learning view extrapolation: online videos on YouTube. Using data mined from such videos, we train a deep network that predicts an MPI from an input stereo image pair. This inferred MPI can then be used to synthesize a range of novel views of the scene, including views that extrapolate significantly beyond the input baseline. We show that our method compares favorably with several recent view synthesis methods, and demonstrate applications in magnifying narrow-baseline stereo images.



