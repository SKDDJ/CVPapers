# Arxiv Papers in cs.CV on 2018-05-26
### Human Action Generation with Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1805.10416v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.10416v1)
- **Published**: 2018-05-26 02:57:34+00:00
- **Updated**: 2018-05-26 02:57:34+00:00
- **Authors**: Mohammad Ahangar Kiasari, Dennis Singh Moirangthem, Minho Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Inspired by the recent advances in generative models, we introduce a human action generation model in order to generate a consecutive sequence of human motions to formulate novel actions. We propose a framework of an autoencoder and a generative adversarial network (GAN) to produce multiple and consecutive human actions conditioned on the initial state and the given class label. The proposed model is trained in an end-to-end fashion, where the autoencoder is jointly trained with the GAN. The model is trained on the NTU RGB+D dataset and we show that the proposed model can generate different styles of actions. Moreover, the model can successfully generate a sequence of novel actions given different action labels as conditions. The conventional human action prediction and generation models lack those features, which are essential for practical applications.



### Enhanced-alignment Measure for Binary Foreground Map Evaluation
- **Arxiv ID**: http://arxiv.org/abs/1805.10421v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.10421v2)
- **Published**: 2018-05-26 03:29:38+00:00
- **Updated**: 2018-07-24 03:04:14+00:00
- **Authors**: Deng-Ping Fan, Cheng Gong, Yang Cao, Bo Ren, Ming-Ming Cheng, Ali Borji
- **Comment**: 8pages, 10 figures, IJCAI 2018 (oral)
- **Journal**: IJCAI 2018
- **Summary**: The existing binary foreground map (FM) measures to address various types of errors in either pixel-wise or structural ways. These measures consider pixel-level match or image-level information independently, while cognitive vision studies have shown that human vision is highly sensitive to both global information and local details in scenes. In this paper, we take a detailed look at current binary FM evaluation measures and propose a novel and effective E-measure (Enhanced-alignment measure). Our measure combines local pixel values with the image-level mean value in one term, jointly capturing image-level statistics and local pixel matching information. We demonstrate the superiority of our measure over the available measures on 4 popular datasets via 5 meta-measures, including ranking models for applications, demoting generic, random Gaussian noise maps, ground-truth switch, as well as human judgments. We find large improvements in almost all the meta-measures. For instance, in terms of application ranking, we observe improvementrangingfrom9.08% to 19.65% compared with other popular measures.



### A novel hybrid score level and decision level fusion scheme for cancelable multi-biometric verification
- **Arxiv ID**: http://arxiv.org/abs/1805.10433v1
- **DOI**: 10.1007/s10489-018-1311-2
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.10433v1)
- **Published**: 2018-05-26 06:04:45+00:00
- **Updated**: 2018-05-26 06:04:45+00:00
- **Authors**: Rudresh Dwivedi, Somnath Dey
- **Comment**: None
- **Journal**: None
- **Summary**: In spite of the benefits of biometric-based authentication systems, there are few concerns raised because of the sensitivity of biometric data to outliers, low performance caused due to intra-class variations and privacy invasion caused by information leakage. To address these issues, we propose a hybrid fusion framework where only the protected modalities are combined to fulfill the requirement of secrecy and performance improvement. This paper presents a method to integrate cancelable modalities utilizing mean-closure weighting (MCW) score level and Dempster-Shafer (DS) theory based decision level fusion for iris and fingerprint to mitigate the limitations in the individual score or decision fusion mechanisms. The proposed hybrid fusion scheme incorporates the similarity scores from different matchers corresponding to each protected modality. The individual scores obtained from different matchers for each modality are combined using MCW score fusion method. The MCW technique achieves the optimal weight for each matcher involved in the score computation. Further, DS theory is applied to the induced scores to output the final decision. The rigorous experimental evaluations on three virtual databases indicate that the proposed hybrid fusion framework outperforms over the component level or individual fusion methods (score level and decision level fusion). As a result, we achieve (48%,66%), (72%,86%) and (49%,38%) of performance improvement over unimodal cancelable iris and unimodal cancelable fingerprint verification systems for Virtual_A, Virtual_B and Virtual_C databases, respectively. Also, the proposed method is robust enough to the variability of scores and outliers satisfying the requirement of secure authentication.



### Multichannel Sparse Blind Deconvolution on the Sphere
- **Arxiv ID**: http://arxiv.org/abs/1805.10437v2
- **DOI**: None
- **Categories**: **cs.IT**, cs.CV, cs.LG, math.IT, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/1805.10437v2)
- **Published**: 2018-05-26 07:04:18+00:00
- **Updated**: 2019-03-16 20:59:36+00:00
- **Authors**: Yanjun Li, Yoram Bresler
- **Comment**: 50 pages, 10 figures. Some of the results in this paper were
  presented at NeurIPS 2018
- **Journal**: None
- **Summary**: Multichannel blind deconvolution is the problem of recovering an unknown signal $f$ and multiple unknown channels $x_i$ from their circular convolution $y_i=x_i \circledast f$ ($i=1,2,\dots,N$). We consider the case where the $x_i$'s are sparse, and convolution with $f$ is invertible. Our nonconvex optimization formulation solves for a filter $h$ on the unit sphere that produces sparse output $y_i\circledast h$. Under some technical assumptions, we show that all local minima of the objective function correspond to the inverse filter of $f$ up to an inherent sign and shift ambiguity, and all saddle points have strictly negative curvatures. This geometric structure allows successful recovery of $f$ and $x_i$ using a simple manifold gradient descent (MGD) algorithm. Our theoretical findings are complemented by numerical experiments, which demonstrate superior performance of the proposed approach over the previous methods.



### Fine-Grained Age Estimation in the wild with Attention LSTM Networks
- **Arxiv ID**: http://arxiv.org/abs/1805.10445v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.10445v2)
- **Published**: 2018-05-26 08:27:01+00:00
- **Updated**: 2019-08-15 11:47:05+00:00
- **Authors**: Ke Zhang, Na Liu, Xingfang Yuan, Xinyao Guo, Ce Gao, Zhenbing Zhao, Zhanyu Ma
- **Comment**: Accepted by IEEE Transactions on Circuits and Systems for Video
  Technology
- **Journal**: None
- **Summary**: Age estimation from a single face image has been an essential task in the field of human-computer interaction and computer vision, which has a wide range of practical application values. Accuracy of age estimation of face images in the wild is relatively low for existing methods, because they only take into account the global features, while neglecting the fine-grained features of age-sensitive areas. We propose a novel method based on our attention long short-term memory (AL) network for fine-grained age estimation in the wild, inspired by the fine-grained categories and the visual attention mechanism. This method combines the residual networks (ResNets) or the residual network of residual network (RoR) models with LSTM units to construct AL-ResNets or AL-RoR networks to extract local features of age-sensitive regions, which effectively improves the age estimation accuracy. First, a ResNets or a RoR model pretrained on ImageNet dataset is selected as the basic model, which is then fine-tuned on the IMDB-WIKI-101 dataset for age estimation. Then, we fine-tune the ResNets or the RoR on the target age datasets to extract the global features of face images. To extract the local features of age-sensitive regions, the LSTM unit is then presented to obtain the coordinates of the agesensitive region automatically. Finally, the age group classification is conducted directly on the Adience dataset, and age-regression experiments are performed by the Deep EXpectation algorithm (DEX) on MORPH Album 2, FG-NET and 15/16LAP datasets. By combining the global and the local features, we obtain our final prediction results. Experimental results illustrate the effectiveness and robustness of the proposed AL-ResNets or AL-RoR for age estimation in the wild, where it achieves better state-of-the-art performance than all other convolutional neural network.



### Accelerating CNN inference on FPGAs: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1806.01683v1
- **DOI**: None
- **Categories**: **cs.DC**, cs.AR, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1806.01683v1)
- **Published**: 2018-05-26 12:24:49+00:00
- **Updated**: 2018-05-26 12:24:49+00:00
- **Authors**: Kamel Abdelouahab, Maxime Pelcat, Jocelyn Serot, Fran√ßois Berry
- **Comment**: Cloning our HAL submission in ArXiv, Technical Report - Universite
  Clermont Auvergne, January 2018
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) are currently adopted to solve an ever greater number of problems, ranging from speech recognition to image classification and segmentation. The large amount of processing required by CNNs calls for dedicated and tailored hardware support methods. Moreover, CNN workloads have a streaming nature, well suited to reconfigurable hardware architectures such as FPGAs. The amount and diversity of research on the subject of CNN FPGA acceleration within the last 3 years demonstrates the tremendous industrial and academic interest. This paper presents a state-of-the-art of CNN inference accelerators over FPGAs. The computational workloads, their parallelism and the involved memory accesses are analyzed. At the level of neurons, optimizations of the convolutional and fully connected layers are explained and the performances of the different methods compared. At the network level, approximate computing and datapath optimization methods are covered and state-of-the-art approaches compared. The methods and tools investigated in this survey represent the recent trends in FPGA CNN inference accelerators and will fuel the future advances on efficient hardware deep learning.



### L1-(2D)2PCANet: A Deep Learning Network for Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1805.10476v1
- **DOI**: 10.1117/1.JEI.28.2.023016
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.10476v1)
- **Published**: 2018-05-26 12:56:21+00:00
- **Updated**: 2018-05-26 12:56:21+00:00
- **Authors**: YunKun Li, XiaoJun Wu, Josef Kittler
- **Comment**: 8 pages and 5 figures
- **Journal**: None
- **Summary**: In this paper, we propose a novel deep learning network L1-(2D)2PCANet for face recognition, which is based on L1-norm-based two-directional two-dimensional principal component analysis (L1-(2D)2PCA). In our network, the role of L1-(2D)2PCA is to learn the filters of multiple convolution layers. After the convolution layers, we deploy binary hashing and block-wise histogram for pooling. We test our network on some benchmark facial datasets YALE, AR, Extended Yale B, LFW-a and FERET with CNN, PCANet, 2DPCANet and L1-PCANet as comparison. The results show that the recognition performance of L1-(2D)2PCANet in all tests is better than baseline networks, especially when there are outliers in the test data. Owing to the L1-norm, L1-2D2PCANet is robust to outliers and changes of the training images.



### Look at Boundary: A Boundary-Aware Face Alignment Algorithm
- **Arxiv ID**: http://arxiv.org/abs/1805.10483v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.10483v1)
- **Published**: 2018-05-26 13:19:02+00:00
- **Updated**: 2018-05-26 13:19:02+00:00
- **Authors**: Wayne Wu, Chen Qian, Shuo Yang, Quan Wang, Yici Cai, Qiang Zhou
- **Comment**: Accepted to CVPR 2018. Project page:
  https://wywu.github.io/projects/LAB/LAB.html
- **Journal**: None
- **Summary**: We present a novel boundary-aware face alignment algorithm by utilising boundary lines as the geometric structure of a human face to help facial landmark localisation. Unlike the conventional heatmap based method and regression based method, our approach derives face landmarks from boundary lines which remove the ambiguities in the landmark definition. Three questions are explored and answered by this work: 1. Why using boundary? 2. How to use boundary? 3. What is the relationship between boundary estimation and landmarks localisation? Our boundary- aware face alignment algorithm achieves 3.49% mean error on 300-W Fullset, which outperforms state-of-the-art methods by a large margin. Our method can also easily integrate information from other datasets. By utilising boundary information of 300-W dataset, our method achieves 3.92% mean error with 0.39% failure rate on COFW dataset, and 1.25% mean error on AFLW-Full dataset. Moreover, we propose a new dataset WFLW to unify training and testing across different factors, including poses, expressions, illuminations, makeups, occlusions, and blurriness. Dataset and model will be publicly available at https://wywu.github.io/projects/LAB/LAB.html



### Vehicle Instance Segmentation from Aerial Image and Video Using a Multi-Task Learning Residual Fully Convolutional Network
- **Arxiv ID**: http://arxiv.org/abs/1805.10485v1
- **DOI**: 10.1109/TGRS.2018.2841808
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.10485v1)
- **Published**: 2018-05-26 13:56:47+00:00
- **Updated**: 2018-05-26 13:56:47+00:00
- **Authors**: Lichao Mou, Xiao Xiang Zhu
- **Comment**: Preprint of a paper accepted by IEEE Transactions On Geoscience and
  Remote Sensing
- **Journal**: None
- **Summary**: Object detection and semantic segmentation are two main themes in object retrieval from high-resolution remote sensing images, which have recently achieved remarkable performance by surfing the wave of deep learning and, more notably, convolutional neural networks (CNNs). In this paper, we are interested in a novel, more challenging problem of vehicle instance segmentation, which entails identifying, at a pixel-level, where the vehicles appear as well as associating each pixel with a physical instance of a vehicle. In contrast, vehicle detection and semantic segmentation each only concern one of the two. We propose to tackle this problem with a semantic boundary-aware multi-task learning network. More specifically, we utilize the philosophy of residual learning (ResNet) to construct a fully convolutional network that is capable of harnessing multi-level contextual feature representations learned from different residual blocks. We theoretically analyze and discuss why residual networks can produce better probability maps for pixel-wise segmentation tasks. Then, based on this network architecture, we propose a unified multi-task learning network that can simultaneously learn two complementary tasks, namely, segmenting vehicle regions and detecting semantic boundaries. The latter subproblem is helpful for differentiating closely spaced vehicles, which are usually not correctly separated into instances. Currently, datasets with pixel-wise annotation for vehicle extraction are ISPRS dataset and IEEE GRSS DFC2015 dataset over Zeebrugge, which specializes in semantic segmentation. Therefore, we built a new, more challenging dataset for vehicle instance segmentation, called the Busy Parking Lot UAV Video dataset, and we make our dataset available at http://www.sipeo.bgu.tum.de/download so that it can be used to benchmark future vehicle instance segmentation algorithms.



### Unsupervised Learning with Stein's Unbiased Risk Estimator
- **Arxiv ID**: http://arxiv.org/abs/1805.10531v3
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1805.10531v3)
- **Published**: 2018-05-26 20:01:15+00:00
- **Updated**: 2020-07-22 20:24:17+00:00
- **Authors**: Christopher A. Metzler, Ali Mousavi, Reinhard Heckel, Richard G. Baraniuk
- **Comment**: None
- **Journal**: None
- **Summary**: Learning from unlabeled and noisy data is one of the grand challenges of machine learning. As such, it has seen a flurry of research with new ideas proposed continuously. In this work, we revisit a classical idea: Stein's Unbiased Risk Estimator (SURE). We show that, in the context of image recovery, SURE and its generalizations can be used to train convolutional neural networks (CNNs) for a range of image denoising and recovery problems without any ground truth data.   Specifically, our goal is to reconstruct an image $x$ from a noisy linear transformation (measurement) of the image. We consider two scenarios: one where no additional data is available and one where we have measurements of other images that are drawn from the same noisy distribution as $x$, but have no access to the clean images. Such is the case, for instance, in the context of medical imaging, microscopy, and astronomy, where noise-less ground truth data is rarely available.   We show that in this situation, SURE can be used to estimate the mean-squared-error loss associated with an estimate of $x$. Using this estimate of the loss, we train networks to perform denoising and compressed sensing recovery. In addition, we also use the SURE framework to partially explain and improve upon an intriguing results presented by Ulyanov et al. in "Deep Image Prior": that a network initialized with random weights and fit to a single noisy image can effectively denoise that image.   Public implementations of the networks and methods described in this paper can be found at https://github.com/ricedsp/D-AMP_Toolbox.



### Video Summarization Using Fully Convolutional Sequence Networks
- **Arxiv ID**: http://arxiv.org/abs/1805.10538v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.10538v2)
- **Published**: 2018-05-26 20:58:38+00:00
- **Updated**: 2018-08-30 21:10:08+00:00
- **Authors**: Mrigank Rochan, Linwei Ye, Yang Wang
- **Comment**: Accepted to ECCV 2018
- **Journal**: None
- **Summary**: This paper addresses the problem of video summarization. Given an input video, the goal is to select a subset of the frames to create a summary video that optimally captures the important information of the input video. With the large amount of videos available online, video summarization provides a useful tool that assists video search, retrieval, browsing, etc. In this paper, we formulate video summarization as a sequence labeling problem. Unlike existing approaches that use recurrent models, we propose fully convolutional sequence models to solve video summarization. We firstly establish a novel connection between semantic segmentation and video summarization, and then adapt popular semantic segmentation networks for video summarization. Extensive experiments and analysis on two benchmark datasets demonstrate the effectiveness of our models.



### Transductive Label Augmentation for Improved Deep Network Learning
- **Arxiv ID**: http://arxiv.org/abs/1805.10546v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.10546v1)
- **Published**: 2018-05-26 21:59:09+00:00
- **Updated**: 2018-05-26 21:59:09+00:00
- **Authors**: Ismail Elezi, Alessandro Torcinovich, Sebastiano Vascon, Marcello Pelillo
- **Comment**: Accepted on IEEE International Conference on Pattern Recognition
- **Journal**: None
- **Summary**: A major impediment to the application of deep learning to real-world problems is the scarcity of labeled data. Small training sets are in fact of no use to deep networks as, due to the large number of trainable parameters, they will very likely be subject to overfitting phenomena. On the other hand, the increment of the training set size through further manual or semi-automatic labellings can be costly, if not possible at times. Thus, the standard techniques to address this issue are transfer learning and data augmentation, which consists of applying some sort of "transformation" to existing labeled instances to let the training set grow in size. Although this approach works well in applications such as image classification, where it is relatively simple to design suitable transformation operators, it is not obvious how to apply it in more structured scenarios. Motivated by the observation that in virtually all application domains it is easy to obtain unlabeled data, in this paper we take a different perspective and propose a \emph{label augmentation} approach. We start from a small, curated labeled dataset and let the labels propagate through a larger set of unlabeled data using graph transduction techniques. This allows us to naturally use (second-order) similarity information which resides in the data, a source of information which is typically neglected by standard augmentation techniques. In particular, we show that by using known game theoretic transductive processes we can create larger and accurate enough labeled datasets which use results in better trained neural networks. Preliminary experiments are reported which demonstrate a consistent improvement over standard image classification datasets.



### Using Syntax to Ground Referring Expressions in Natural Images
- **Arxiv ID**: http://arxiv.org/abs/1805.10547v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1805.10547v1)
- **Published**: 2018-05-26 22:02:05+00:00
- **Updated**: 2018-05-26 22:02:05+00:00
- **Authors**: Volkan Cirik, Taylor Berg-Kirkpatrick, Louis-Philippe Morency
- **Comment**: AAAI 2018
- **Journal**: None
- **Summary**: We introduce GroundNet, a neural network for referring expression recognition -- the task of localizing (or grounding) in an image the object referred to by a natural language expression. Our approach to this task is the first to rely on a syntactic analysis of the input referring expression in order to inform the structure of the computation graph. Given a parse tree for an input expression, we explicitly map the syntactic constituents and relationships present in the tree to a composed graph of neural modules that defines our architecture for performing localization. This syntax-based approach aids localization of \textit{both} the target object and auxiliary supporting objects mentioned in the expression. As a result, GroundNet is more interpretable than previous methods: we can (1) determine which phrase of the referring expression points to which object in the image and (2) track how the localization of the target object is determined by the network. We study this property empirically by introducing a new set of annotations on the GoogleRef dataset to evaluate localization of supporting objects. Our experiments show that GroundNet achieves state-of-the-art accuracy in identifying supporting objects, while maintaining comparable performance in the localization of target objects.



### Deep Watershed Detector for Music Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/1805.10548v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1805.10548v1)
- **Published**: 2018-05-26 22:13:16+00:00
- **Updated**: 2018-05-26 22:13:16+00:00
- **Authors**: Lukas Tuggener, Ismail Elezi, Jurgen Schmidhuber, Thilo Stadelmann
- **Comment**: Accepted on The 19th International Society for Music Information
  Retrieval Conference 2018
- **Journal**: None
- **Summary**: Optical Music Recognition (OMR) is an important and challenging area within music information retrieval, the accurate detection of music symbols in digital images is a core functionality of any OMR pipeline. In this paper, we introduce a novel object detection method, based on synthetic energy maps and the watershed transform, called Deep Watershed Detector (DWD). Our method is specifically tailored to deal with high resolution images that contain a large number of very small objects and is therefore able to process full pages of written music. We present state-of-the-art detection results of common music symbols and show DWD's ability to work with synthetic scores equally well as on handwritten music.



