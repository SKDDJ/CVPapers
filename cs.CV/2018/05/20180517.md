# Arxiv Papers in cs.CV on 2018-05-17
### Recurrent Neural Network for Learning DenseDepth and Ego-Motion from Video
- **Arxiv ID**: http://arxiv.org/abs/1805.06558v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.06558v1)
- **Published**: 2018-05-17 00:18:08+00:00
- **Updated**: 2018-05-17 00:18:08+00:00
- **Authors**: Rui Wang, Jan-Michael Frahm, Stephen M. Pizer
- **Comment**: None
- **Journal**: None
- **Summary**: Learning-based, single-view depth estimation often generalizes poorly to unseen datasets. While learning-based, two-frame depth estimation solves this problem to some extent by learning to match features across frames, it performs poorly at large depth where the uncertainty is high. There exists few learning-based, multi-view depth estimation methods. In this paper, we present a learning-based, multi-view dense depth map and ego-motion estimation method that uses Recurrent Neural Networks (RNN). Our model is designed for 3D reconstruction from video where the input frames are temporally correlated. It is generalizable to single- or two-view dense depth estimation. Compared to recent single- or two-view CNN-based depth estimation methods, our model leverages more views and achieves more accurate results, especially at large distances. Our method produces superior results to the state-of-the-art learning-based, single- or two-view depth estimation methods on both indoor and outdoor benchmark datasets. We also demonstrate that our method can even work on extremely difficult sequences, such as endoscopic video, where none of the assumptions (static scene, constant lighting, Lambertian reflection, etc.) from traditional 3D reconstruction methods hold.



### DeepGlobe 2018: A Challenge to Parse the Earth through Satellite Images
- **Arxiv ID**: http://arxiv.org/abs/1805.06561v1
- **DOI**: 10.1109/CVPRW.2018.00031
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.06561v1)
- **Published**: 2018-05-17 00:45:37+00:00
- **Updated**: 2018-05-17 00:45:37+00:00
- **Authors**: Ilke Demir, Krzysztof Koperski, David Lindenbaum, Guan Pang, Jing Huang, Saikat Basu, Forest Hughes, Devis Tuia, Ramesh Raskar
- **Comment**: Dataset description for DeepGlobe 2018 Challenge at CVPR 2018
- **Journal**: None
- **Summary**: We present the DeepGlobe 2018 Satellite Image Understanding Challenge, which includes three public competitions for segmentation, detection, and classification tasks on satellite images. Similar to other challenges in computer vision domain such as DAVIS and COCO, DeepGlobe proposes three datasets and corresponding evaluation methodologies, coherently bundled in three competitions with a dedicated workshop co-located with CVPR 2018.   We observed that satellite imagery is a rich and structured source of information, yet it is less investigated than everyday images by computer vision researchers. However, bridging modern computer vision with remote sensing data analysis could have critical impact to the way we understand our environment and lead to major breakthroughs in global urban planning or climate change research. Keeping such bridging objective in mind, DeepGlobe aims to bring together researchers from different domains to raise awareness of remote sensing in the computer vision community and vice-versa. We aim to improve and evaluate state-of-the-art satellite image understanding approaches, which can hopefully serve as reference benchmarks for future research in the same topic. In this paper, we analyze characteristics of each dataset, define the evaluation criteria of the competitions, and provide baselines for each task.



### Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models
- **Arxiv ID**: http://arxiv.org/abs/1805.06605v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.06605v2)
- **Published**: 2018-05-17 05:38:55+00:00
- **Updated**: 2018-05-18 00:20:52+00:00
- **Authors**: Pouya Samangouei, Maya Kabkab, Rama Chellappa
- **Comment**: Published as a conference paper at the Sixth International Conference
  on Learning Representations (ICLR 2018)
- **Journal**: None
- **Summary**: In recent years, deep neural network approaches have been widely adopted for machine learning tasks, including classification. However, they were shown to be vulnerable to adversarial perturbations: carefully crafted small perturbations can cause misclassification of legitimate images. We propose Defense-GAN, a new framework leveraging the expressive capability of generative models to defend deep neural networks against such attacks. Defense-GAN is trained to model the distribution of unperturbed images. At inference time, it finds a close output to a given image which does not contain the adversarial changes. This output is then fed to the classifier. Our proposed method can be used with any classification model and does not modify the classifier structure or training procedure. It can also be used as a defense against any attack as it does not assume knowledge of the process for generating the adversarial examples. We empirically show that Defense-GAN is consistently effective against different attack methods and improves on existing defense strategies. Our code has been made publicly available at https://github.com/kabkabm/defensegan



### Optimization of Transfer Learning for Sign Language Recognition Targeting Mobile Platform
- **Arxiv ID**: http://arxiv.org/abs/1805.06618v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.06618v1)
- **Published**: 2018-05-17 06:50:57+00:00
- **Updated**: 2018-05-17 06:50:57+00:00
- **Authors**: Dhruv Rathi
- **Comment**: 6 Pages, Journal
- **Journal**: None
- **Summary**: The target of this research is to experiment, iterate and recommend a system that is successful in recognition of American Sign Language (ASL). It is a challenging as well as an interesting problem that if solved will bring a leap in social and technological aspects alike. In this paper, we propose a real-time recognizer of ASL based on a mobile platform, so that it will have more accessibility and provides an ease of use. The technique implemented is Transfer Learning of new data of Hand gestures for alphabets in ASL to be modelled on various pre-trained high- end models and optimize the best model to run on a mobile platform considering the various limitations of the same during optimization. The data used consists of 27,455 images of 24 alphabets of ASL. The optimized model when ran over a memory-efficient mobile application, provides an accuracy of 95.03% of accurate recognition with an average recognition time of 2.42 seconds. This method ensures considerable discrimination in accuracy and recognition time than the previous research.



### Structure-preserving Guided Retinal Image Filtering and Its Application for Optic Disc Analysis
- **Arxiv ID**: http://arxiv.org/abs/1805.06625v2
- **DOI**: 10.1109/TMI.2018.2838550
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.06625v2)
- **Published**: 2018-05-17 07:17:51+00:00
- **Updated**: 2018-05-22 09:11:19+00:00
- **Authors**: Jun Cheng, Zhengguo Li, Zaiwang Gu, Huazhu Fu, Damon Wing Kee Wong, Jiang Liu
- **Comment**: Accepted for publication on IEEE Trans. on Medical Imaging
- **Journal**: IEEE Transactions on Medical Imaging ( Volume: 37 , Issue: 11 ,
  Pages 2536 - 2546, Nov. 2018 )
- **Summary**: Retinal fundus photographs have been used in the diagnosis of many ocular diseases such as glaucoma, pathological myopia, age-related macular degeneration and diabetic retinopathy. With the development of computer science, computer aided diagnosis has been developed to process and analyse the retinal images automatically. One of the challenges in the analysis is that the quality of the retinal image is often degraded. For example, a cataract in human lens will attenuate the retinal image, just as a cloudy camera lens which reduces the quality of a photograph. It often obscures the details in the retinal images and posts challenges in retinal image processing and analysing tasks. In this paper, we approximate the degradation of the retinal images as a combination of human-lens attenuation and scattering. A novel structure-preserving guided retinal image filtering (SGRIF) is then proposed to restore images based on the attenuation and scattering model. The proposed SGRIF consists of a step of global structure transferring and a step of global edge-preserving smoothing. Our results show that the proposed SGRIF method is able to improve the contrast of retinal images, measured by histogram flatness measure, histogram spread and variability of local luminosity. In addition, we further explored the benefits of SGRIF for subsequent retinal image processing and analysing tasks. In the two applications of deep learning based optic cup segmentation and sparse learning based cup-to-disc ratio (CDR) computation, our results show that we are able to achieve more accurate optic cup segmentation and CDR measurements from images processed by SGRIF.



### Automatic Data Registration of Geostationary Payloads for Meteorological Applications at ISRO
- **Arxiv ID**: http://arxiv.org/abs/1805.08706v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CE
- **Links**: [PDF](http://arxiv.org/pdf/1805.08706v1)
- **Published**: 2018-05-17 07:41:48+00:00
- **Updated**: 2018-05-17 07:41:48+00:00
- **Authors**: Jignesh S. Bhatt, N. Padmanabhan
- **Comment**: 16 pages, 13 figures
- **Journal**: None
- **Summary**: The launch of KALPANA-1 satellite in the year 2002 heralded the establishment of an indigenous operational payload for meteorological predictions. This was further enhanced in the year 2003 with the launching of INSAT-3A satellite. The software for generating products from the data of these two satellites was taken up subsequently in the year 2004 and the same was installed at the Indian Meteorological Department, New Delhi in January 2006. Registration has been one of the most fundamental operations to generate almost all the data products from the remotely sensed data. Registration is a challenging task due to inevitable radiometric and geometric distortions during the acquisition process. Besides the presence of clouds makes the problem more complicated. In this paper, we present an algorithm for multitemporal and multiband registration. In addition, India facing reference boundaries for the CCD data of INSAT-3A have also been generated. The complete implementation is made up of the following steps: 1) automatic identification of the ground control points (GCPs) in the sensed data, 2) finding the optimal transformation model based on the match-points, and 3) resampling the transformed imagery to the reference coordinates. The proposed algorithm is demonstrated using the real datasets from KALPANA-1 and INSAT-3A. Both KALAPANA-1 and INSAT-3A have recently been decommissioned due to lack of fuel, however, the experience gained from them have given rise to a series of meteorological satellites and associated software; like INSAT-3D series which give continuous weather forecasting for the country. This paper is not so much focused on the theory (widely available in the literature) but concentrates on the implementation of operational software.



### Joint direct estimation of 3D geometry and 3D motion using spatio temporal gradients
- **Arxiv ID**: http://arxiv.org/abs/1805.06641v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.06641v1)
- **Published**: 2018-05-17 07:54:24+00:00
- **Updated**: 2018-05-17 07:54:24+00:00
- **Authors**: Francisco Barranco, Cornelia Fermüller, Yiannis Aloimonos, Eduardo Ros
- **Comment**: None
- **Journal**: None
- **Summary**: Conventional image motion based structure from motion methods first compute optical flow, then solve for the 3D motion parameters based on the epipolar constraint, and finally recover the 3D geometry of the scene. However, errors in optical flow due to regularization can lead to large errors in 3D motion and structure. This paper investigates whether performance and consistency can be improved by avoiding optical flow estimation in the early stages of the structure from motion pipeline, and it proposes a new direct method based on image gradients (normal flow) only. The main idea lies in a reformulation of the positive-depth constraint, which allows the use of well-known minimization techniques to solve for 3D motion. The 3D motion estimate is then refined and structure estimated adding a regularization based on depth. Experimental comparisons on standard synthetic datasets and the real-world driving benchmark dataset KITTI using three different optic flow algorithms show that the method achieves better accuracy in all but one case. Furthermore, it outperforms existing normal flow based 3D motion estimation techniques. Finally, the recovered 3D geometry is shown to be also very accurate.



### Single Shot Active Learning using Pseudo Annotators
- **Arxiv ID**: http://arxiv.org/abs/1805.06660v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1805.06660v1)
- **Published**: 2018-05-17 09:05:28+00:00
- **Updated**: 2018-05-17 09:05:28+00:00
- **Authors**: Yazhou Yang, Marco Loog
- **Comment**: 12 pages, 8 figure, submitted to Pattern Recognition
- **Journal**: None
- **Summary**: Standard myopic active learning assumes that human annotations are always obtainable whenever new samples are selected. This, however, is unrealistic in many real-world applications where human experts are not readily available at all times. In this paper, we consider the single shot setting: all the required samples should be chosen in a single shot and no human annotation can be exploited during the selection process. We propose a new method, Active Learning through Random Labeling (ALRL), which substitutes single human annotator for multiple, what we will refer to as, pseudo annotators. These pseudo annotators always provide uniform and random labels whenever new unlabeled samples are queried. This random labeling enables standard active learning algorithms to also exhibit the exploratory behavior needed for single shot active learning. The exploratory behavior is further enhanced by selecting the most representative sample via minimizing nearest neighbor distance between unlabeled samples and queried samples. Experiments on real-world datasets demonstrate that the proposed method outperforms several state-of-the-art approaches.



### GANomaly: Semi-Supervised Anomaly Detection via Adversarial Training
- **Arxiv ID**: http://arxiv.org/abs/1805.06725v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.06725v3)
- **Published**: 2018-05-17 12:36:02+00:00
- **Updated**: 2018-11-13 15:56:32+00:00
- **Authors**: Samet Akcay, Amir Atapour-Abarghouei, Toby P. Breckon
- **Comment**: ACCV 2018
- **Journal**: None
- **Summary**: Anomaly detection is a classical problem in computer vision, namely the determination of the normal from the abnormal when datasets are highly biased towards one class (normal) due to the insufficient sample size of the other class (abnormal). While this can be addressed as a supervised learning problem, a significantly more challenging problem is that of detecting the unknown/unseen anomaly case that takes us instead into the space of a one-class, semi-supervised learning paradigm. We introduce such a novel anomaly detection model, by using a conditional generative adversarial network that jointly learns the generation of high-dimensional image space and the inference of latent space. Employing encoder-decoder-encoder sub-networks in the generator network enables the model to map the input image to a lower dimension vector, which is then used to reconstruct the generated output image. The use of the additional encoder network maps this generated image to its latent representation. Minimizing the distance between these images and the latent vectors during training aids in learning the data distribution for the normal samples. As a result, a larger distance metric from this learned data distribution at inference time is indicative of an outlier from that distribution - an anomaly. Experimentation over several benchmark datasets, from varying domains, shows the model efficacy and superiority over previous state-of-the-art approaches.



### A Robust Background Initialization Algorithm with Superpixel Motion Detection
- **Arxiv ID**: http://arxiv.org/abs/1805.06737v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.06737v1)
- **Published**: 2018-05-17 12:52:57+00:00
- **Updated**: 2018-05-17 12:52:57+00:00
- **Authors**: Zhe Xu, Biao Min, Ray C. C. Cheung
- **Comment**: submitted to Elsevier Signal Processing: Image Communication
- **Journal**: None
- **Summary**: Scene background initialization allows the recovery of a clear image without foreground objects from a video sequence, which is generally the first step in many computer vision and video processing applications. The process may be strongly affected by some challenges such as illumination changes, foreground cluttering, intermittent movement, etc. In this paper, a robust background initialization approach based on superpixel motion detection is proposed. Both spatial and temporal characteristics of frames are adopted to effectively eliminate foreground objects. A subsequence with stable illumination condition is first selected for background estimation. Images are segmented into superpixels to preserve spatial texture information and foreground objects are eliminated by superpixel motion filtering process. A low-complexity density-based clustering is then performed to generate reliable background candidates for final background determination. The approach has been evaluated on SBMnet dataset and it achieves a performance superior or comparable to other state-of-the-art works with faster processing speed. Moreover, in those complex and dynamic categories, the algorithm produces the best results showing the robustness against very challenging scenarios.



### Minimum Margin Loss for Deep Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1805.06741v4
- **DOI**: 10.1016/j.patcog.2019.107012
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.06741v4)
- **Published**: 2018-05-17 13:02:23+00:00
- **Updated**: 2018-08-20 15:27:51+00:00
- **Authors**: Xin Wei, Hui Wang, Bryan Scotney, Huan Wan
- **Comment**: None
- **Journal**: None
- **Summary**: Face recognition has achieved great progress owing to the fast development of the deep neural network in the past a few years. As an important part of deep neural networks, a number of the loss functions have been proposed which significantly improve the state-of-the-art methods. In this paper, we proposed a new loss function called Minimum Margin Loss (MML) which aims at enlarging the margin of those overclose class centre pairs so as to enhance the discriminative ability of the deep features. MML supervises the training process together with the Softmax Loss and the Centre Loss, and also makes up the defect of Softmax + Centre Loss. The experimental results on MegaFace, LFW and YTF datasets show that the proposed method achieves the state-of-the-art performance, which demonstrates the effectiveness of the proposed MML.



### Action Completion: A Temporal Model for Moment Detection
- **Arxiv ID**: http://arxiv.org/abs/1805.06749v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.06749v2)
- **Published**: 2018-05-17 13:21:43+00:00
- **Updated**: 2018-07-23 22:06:19+00:00
- **Authors**: Farnoosh Heidarivincheh, Majid Mirmehdi, Dima Damen
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce completion moment detection for actions - the problem of locating the moment of completion, when the action's goal is confidently considered achieved. The paper proposes a joint classification-regression recurrent model that predicts completion from a given frame, and then integrates frame-level contributions to detect sequence-level completion moment. We introduce a recurrent voting node that predicts the frame's relative position of the completion moment by either classification or regression. The method is also capable of detecting incompletion. For example, the method is capable of detecting a missed ball-catch, as well as the moment at which the ball is safely caught. We test the method on 16 actions from three public datasets, covering sports as well as daily actions. Results show that when combining contributions from frames prior to the completion moment as well as frames post completion, the completion moment is detected within one second in 89% of all tested sequences.



### Situation Assessment for Planning Lane Changes: Combining Recurrent Models and Prediction
- **Arxiv ID**: http://arxiv.org/abs/1805.06776v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.06776v1)
- **Published**: 2018-05-17 13:52:34+00:00
- **Updated**: 2018-05-17 13:52:34+00:00
- **Authors**: Oliver Scheel, Loren Schwarz, Nassir Navab, Federico Tombari
- **Comment**: ICRA 2018
- **Journal**: None
- **Summary**: One of the greatest challenges towards fully autonomous cars is the understanding of complex and dynamic scenes. Such understanding is needed for planning of maneuvers, especially those that are particularly frequent such as lane changes. While in recent years advanced driver-assistance systems have made driving safer and more comfortable, these have mostly focused on car following scenarios, and less on maneuvers involving lane changes. In this work we propose a situation assessment algorithm for classifying driving situations with respect to their suitability for lane changing. For this, we propose a deep learning architecture based on a Bidirectional Recurrent Neural Network, which uses Long Short-Term Memory units, and integrates a prediction component in the form of the Intelligent Driver Model. We prove the feasibility of our algorithm on the publicly available NGSIM datasets, where we outperform existing methods.



### Cross-domain attribute representation based on convolutional neural network
- **Arxiv ID**: http://arxiv.org/abs/1805.07295v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.07295v1)
- **Published**: 2018-05-17 14:42:33+00:00
- **Updated**: 2018-05-17 14:42:33+00:00
- **Authors**: Guohui Zhang, Gaoyuan Liang, Fang Su, Fanxin Qu, Jing-Yan Wang
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1803.09733
- **Journal**: None
- **Summary**: In the problem of domain transfer learning, we learn a model for the predic-tion in a target domain from the data of both some source domains and the target domain, where the target domain is in lack of labels while the source domain has sufficient labels. Besides the instances of the data, recently the attributes of data shared across domains are also explored and proven to be very helpful to leverage the information of different domains. In this paper, we propose a novel learning framework for domain-transfer learning based on both instances and attributes. We proposed to embed the attributes of dif-ferent domains by a shared convolutional neural network (CNN), learn a domain-independent CNN model to represent the information shared by dif-ferent domains by matching across domains, and a domain-specific CNN model to represent the information of each domain. The concatenation of the three CNN model outputs is used to predict the class label. An iterative algo-rithm based on gradient descent method is developed to learn the parameters of the model. The experiments over benchmark datasets show the advantage of the proposed model.



### Disparity Sliding Window: Object Proposals From Disparity Images
- **Arxiv ID**: http://arxiv.org/abs/1805.06830v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.06830v2)
- **Published**: 2018-05-17 15:45:41+00:00
- **Updated**: 2018-08-06 09:16:04+00:00
- **Authors**: Julian Müller, Andreas Fregin, Klaus Dietmayer
- **Comment**: Submitted to IROS 2018
- **Journal**: None
- **Summary**: Sliding window approaches have been widely used for object recognition tasks in recent years. They guarantee an investigation of the entire input image for the object to be detected and allow a localization of that object. Despite the current trend towards deep neural networks, sliding window methods are still used in combination with convolutional neural networks. The risk of overlooking an object is clearly reduced compared to alternative detection approaches which detect objects based on shape, edges or color. Nevertheless, the sliding window technique strongly increases the computational effort as the classifier has to verify a large number of object candidates. This paper proposes a sliding window approach which also uses depth information from a stereo camera. This leads to a greatly decreased number of object candidates without significantly reducing the detection accuracy. A theoretical investigation of the conventional sliding window approach is presented first. Other publications to date only mentioned rough estimations of the computational cost. A mathematical derivation clarifies the number of object candidates with respect to parameters such as image and object size. Subsequently, the proposed disparity sliding window approach is presented in detail. The approach is evaluated on pedestrian detection with annotations and images from the KITTI object detection benchmark. Furthermore, a comparison with two state-of-the-art methods is made. Code is available in C++ and Python https://github.com/julimueller/ disparity-sliding-window.



### ScaffoldNet: Detecting and Classifying Biomedical Polymer-Based Scaffolds via a Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1805.08702v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.08702v1)
- **Published**: 2018-05-17 16:19:40+00:00
- **Updated**: 2018-05-17 16:19:40+00:00
- **Authors**: Darlington Ahiale Akogo, Xavier-Lewis Palmer
- **Comment**: 4 figures
- **Journal**: None
- **Summary**: We developed a Convolutional Neural Network model to identify and classify Airbrushed (alternatively known as Blow-spun), Electrospun and Steel Wire scaffolds. Our model ScaffoldNet is a 6-layer Convolutional Neural Network trained and tested on 3,043 images of Airbrushed, Electrospun and Steel Wire scaffolds. The model takes in as input an imaged scaffold and then outputs the scaffold type (Airbrushed, Electrospun or Steel Wire) as predicted probabilities for the 3 classes. Our model scored a 99.44% Accuracy, demonstrating potential for adaptation to investigating and solving complex machine learning problems aimed at abstract spatial contexts, or in screening complex, biological, fibrous structures seen in cortical bone and fibrous shells.



### RotDCF: Decomposition of Convolutional Filters for Rotation-Equivariant Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/1805.06846v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.06846v1)
- **Published**: 2018-05-17 16:24:51+00:00
- **Updated**: 2018-05-17 16:24:51+00:00
- **Authors**: Xiuyuan Cheng, Qiang Qiu, Robert Calderbank, Guillermo Sapiro
- **Comment**: None
- **Journal**: None
- **Summary**: Explicit encoding of group actions in deep features makes it possible for convolutional neural networks (CNNs) to handle global deformations of images, which is critical to success in many vision tasks. This paper proposes to decompose the convolutional filters over joint steerable bases across the space and the group geometry simultaneously, namely a rotation-equivariant CNN with decomposed convolutional filters (RotDCF). This decomposition facilitates computing the joint convolution, which is proved to be necessary for the group equivariance. It significantly reduces the model size and computational complexity while preserving performance, and truncation of the bases expansion serves implicitly to regularize the filters. On datasets involving in-plane and out-of-plane object rotations, RotDCF deep features demonstrate greater robustness and interpretability than regular CNNs. The stability of the equivariant representation to input variations is also proved theoretically under generic assumptions on the filters in the decomposed form. The RotDCF framework can be extended to groups other than rotations, providing a general approach which achieves both group equivariance and representation stability at a reduced model size.



### NeuralNetwork-Viterbi: A Framework for Weakly Supervised Video Learning
- **Arxiv ID**: http://arxiv.org/abs/1805.06875v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.06875v1)
- **Published**: 2018-05-17 17:36:42+00:00
- **Updated**: 2018-05-17 17:36:42+00:00
- **Authors**: Alexander Richard, Hilde Kuehne, Ahsan Iqbal, Juergen Gall
- **Comment**: CVPR 2018
- **Journal**: None
- **Summary**: Video learning is an important task in computer vision and has experienced increasing interest over the recent years. Since even a small amount of videos easily comprises several million frames, methods that do not rely on a frame-level annotation are of special importance. In this work, we propose a novel learning algorithm with a Viterbi-based loss that allows for online and incremental learning of weakly annotated video data. We moreover show that explicit context and length modeling leads to huge improvements in video segmentation and labeling tasks andinclude these models into our framework. On several action segmentation benchmarks, we obtain an improvement of up to 10% compared to current state-of-the-art methods.



### It's all Relative: Monocular 3D Human Pose Estimation from Weakly Supervised Data
- **Arxiv ID**: http://arxiv.org/abs/1805.06880v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.06880v2)
- **Published**: 2018-05-17 17:40:18+00:00
- **Updated**: 2018-07-28 00:37:41+00:00
- **Authors**: Matteo Ruggero Ronchi, Oisin Mac Aodha, Robert Eng, Pietro Perona
- **Comment**: BMVC 2018. Project page available at
  http://www.vision.caltech.edu/~mronchi/projects/RelativePose
- **Journal**: None
- **Summary**: We address the problem of 3D human pose estimation from 2D input images using only weakly supervised training data. Despite showing considerable success for 2D pose estimation, the application of supervised machine learning to 3D pose estimation in real world images is currently hampered by the lack of varied training images with corresponding 3D poses. Most existing 3D pose estimation algorithms train on data that has either been collected in carefully controlled studio settings or has been generated synthetically. Instead, we take a different approach, and propose a 3D human pose estimation algorithm that only requires relative estimates of depth at training time. Such training signal, although noisy, can be easily collected from crowd annotators, and is of sufficient quality for enabling successful training and evaluation of 3D pose algorithms. Our results are competitive with fully supervised regression based approaches on the Human3.6M dataset, despite using significantly weaker training data. Our proposed algorithm opens the door to using existing widespread 2D datasets for 3D pose estimation by allowing fine-tuning with noisy relative constraints, resulting in more accurate 3D poses.



### Fully Convolutional Model for Variable Bit Length and Lossy High Density Compression of Mammograms
- **Arxiv ID**: http://arxiv.org/abs/1805.06909v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.06909v1)
- **Published**: 2018-05-17 18:01:39+00:00
- **Updated**: 2018-05-17 18:01:39+00:00
- **Authors**: Aupendu Kar, Sri Phani Krishna Karri, Nirmalya Ghosh, Ramanathan Sethuraman, Debdoot Sheet
- **Comment**: 4 pages, 3 figures, To appear in Workshop on Learned Image
  Compression, CVPR 2018
- **Journal**: None
- **Summary**: Early works on medical image compression date to the 1980's with the impetus on deployment of teleradiology systems for high-resolution digital X-ray detectors. Commercially deployed systems during the period could compress 4,096 x 4,096 sized images at 12 bpp to 2 bpp using lossless arithmetic coding, and over the years JPEG and JPEG2000 were imbibed reaching upto 0.1 bpp. Inspired by the reprise of deep learning based compression for natural images over the last two years, we propose a fully convolutional autoencoder for diagnostically relevant feature preserving lossy compression. This is followed by leveraging arithmetic coding for encapsulating high redundancy of features for further high-density code packing leading to variable bit length. We demonstrate performance on two different publicly available digital mammography datasets using peak signal-to-noise ratio (pSNR), structural similarity (SSIM) index and domain adaptability tests between datasets. At high density compression factors of >300x (~0.04 bpp), our approach rivals JPEG and JPEG2000 as evaluated through a Radiologist's visual Turing test.



### Identifying Object States in Cooking-Related Images
- **Arxiv ID**: http://arxiv.org/abs/1805.06956v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.06956v3)
- **Published**: 2018-05-17 20:18:56+00:00
- **Updated**: 2018-10-30 14:37:19+00:00
- **Authors**: Ahmad Babaeian Jelodar, Md Sirajus Salekin, Yu Sun
- **Comment**: 7 pages, 8 figures
- **Journal**: None
- **Summary**: Understanding object states is as important as object recognition for robotic task planning and manipulation. To our knowledge, this paper explicitly introduces and addresses the state identification problem in cooking related images for the first time. In this paper, objects and ingredients in cooking videos are explored and the most frequent objects are analyzed. Eleven states from the most frequent cooking objects are examined and a dataset of images containing those objects and their states is created. As a solution to the state identification problem, a Resnet based deep model is proposed. The model is initialized with Imagenet weights and trained on the dataset of eleven classes. The trained state identification model is evaluated on a subset of the Imagenet dataset and state labels are provided using a combination of the model with manual checking. Moreover, an individual model is fine-tuned for each object in the dataset using the weights from the initially trained model and object-specific images, where significant improvement is demonstrated.



### Generalizing multistain immunohistochemistry tissue segmentation using one-shot color deconvolution deep neural networks
- **Arxiv ID**: http://arxiv.org/abs/1805.06958v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.06958v3)
- **Published**: 2018-05-17 20:23:00+00:00
- **Updated**: 2018-09-22 20:41:54+00:00
- **Authors**: Amal Lahiani, Jacob Gildenblat, Irina Klaman, Nassir Navab, Eldad Klaiman
- **Comment**: None
- **Journal**: None
- **Summary**: A key challenge in cancer immunotherapy biomarker research is quantification of pattern changes in microscopic whole slide images of tumor biopsies. Different cell types tend to migrate into various tissue compartments and form variable distribution patterns. Drug development requires correlative analysis of various biomarkers in and between the tissue compartments. To enable that, tissue slides are manually annotated by expert pathologists. Manual annotation of tissue slides is a labor intensive, tedious and error-prone task. Automation of this annotation process can improve accuracy and consistency while reducing workload and cost in a way that will positively influence drug development efforts. In this paper we present a novel one-shot color deconvolution deep learning method to automatically segment and annotate digitized slide images with multiple stainings into compartments of tumor, healthy tissue, and necrosis. We address the task in the context of drug development where multiple stains, tissue and tumor types exist and look into solutions for generalizations over these image populations.



### Ask No More: Deciding when to guess in referential visual dialogue
- **Arxiv ID**: http://arxiv.org/abs/1805.06960v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1805.06960v2)
- **Published**: 2018-05-17 20:32:08+00:00
- **Updated**: 2018-06-12 10:43:56+00:00
- **Authors**: Ravi Shekhar, Tim Baumgartner, Aashish Venkatesh, Elia Bruni, Raffaella Bernardi, Raquel Fernandez
- **Comment**: COLING 2018 (accepted)
- **Journal**: None
- **Summary**: Our goal is to explore how the abilities brought in by a dialogue manager can be included in end-to-end visually grounded conversational agents. We make initial steps towards this general goal by augmenting a task-oriented visual dialogue model with a decision-making component that decides whether to ask a follow-up question to identify a target referent in an image, or to stop the conversation to make a guess. Our analyses show that adding a decision making component produces dialogues that are less repetitive and that include fewer unnecessary questions, thus potentially leading to more efficient and less unnatural interactions.



### Terabyte-scale Deep Multiple Instance Learning for Classification and Localization in Pathology
- **Arxiv ID**: http://arxiv.org/abs/1805.06983v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.06983v2)
- **Published**: 2018-05-17 22:43:46+00:00
- **Updated**: 2018-09-27 22:42:02+00:00
- **Authors**: Gabriele Campanella, Vitor Werneck Krauss Silva, Thomas J. Fuchs
- **Comment**: None
- **Journal**: None
- **Summary**: In the field of computational pathology, the use of decision support systems powered by state-of-the-art deep learning solutions has been hampered by the lack of large labeled datasets. Until recently, studies relied on datasets in the order of few hundreds of slides which are not enough to train a model that can work at scale in the clinic. Here, we have gathered a dataset consisting of 12,160 slides, two orders of magnitude larger than previous datasets in pathology and equivalent to 25 times the pixel count of the entire ImageNet dataset. Given the size of our dataset it is possible for us to train a deep learning model under the Multiple Instance Learning (MIL) assumption where only the overall slide diagnosis is necessary for training, avoiding all the expensive pixel-wise annotations that are usually part of supervised learning approaches. We test our framework on a complex task, that of prostate cancer diagnosis on needle biopsies. We performed a thorough evaluation of the performance of our MIL pipeline under several conditions achieving an AUC of 0.98 on a held-out test set of 1,824 slides. These results open the way for training accurate diagnosis prediction models at scale, laying the foundation for decision support system deployment in the clinic.



