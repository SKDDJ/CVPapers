# Arxiv Papers in cs.CV on 2018-05-14
### Unifying and Merging Well-trained Deep Neural Networks for Inference Stage
- **Arxiv ID**: http://arxiv.org/abs/1805.04980v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.04980v1)
- **Published**: 2018-05-14 01:32:02+00:00
- **Updated**: 2018-05-14 01:32:02+00:00
- **Authors**: Yi-Min Chou, Yi-Ming Chan, Jia-Hong Lee, Chih-Yi Chiu, Chu-Song Chen
- **Comment**: To appear in the 27th International Joint Conference on Artificial
  Intelligence and the 23rd European Conference on Artificial Intelligence,
  2018. (IJCAI-ECAI 2018)
- **Journal**: None
- **Summary**: We propose a novel method to merge convolutional neural-nets for the inference stage. Given two well-trained networks that may have different architectures that handle different tasks, our method aligns the layers of the original networks and merges them into a unified model by sharing the representative codes of weights. The shared weights are further re-trained to fine-tune the performance of the merged model. The proposed method effectively produces a compact model that may run original tasks simultaneously on resource-limited devices. As it preserves the general architectures and leverages the co-used weights of well-trained networks, a substantial training overhead can be reduced to shorten the system development time. Experimental results demonstrate a satisfactory performance and validate the effectiveness of the method.



### Deep Decision Trees for Discriminative Dictionary Learning with Adversarial Multi-Agent Trajectories
- **Arxiv ID**: http://arxiv.org/abs/1805.05009v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.05009v1)
- **Published**: 2018-05-14 04:25:14+00:00
- **Updated**: 2018-05-14 04:25:14+00:00
- **Authors**: Tharindu Fernando, Sridha Sridharan, Clinton Fookes, Simon Denman
- **Comment**: To appear in 4th International Workshop on Computer Vision in Sports
  (CVsports) at CVPR 2018
- **Journal**: None
- **Summary**: With the explosion in the availability of spatio-temporal tracking data in modern sports, there is an enormous opportunity to better analyse, learn and predict important events in adversarial group environments. In this paper, we propose a deep decision tree architecture for discriminative dictionary learning from adversarial multi-agent trajectories. We first build up a hierarchy for the tree structure by adding each layer and performing feature weight based clustering in the forward pass. We then fine tune the player role weights using back propagation. The hierarchical architecture ensures the interpretability and the integrity of the group representation. The resulting architecture is a decision tree, with leaf-nodes capturing a dictionary of multi-agent group interactions. Due to the ample volume of data available, we focus on soccer tracking data, although our approach can be used in any adversarial multi-agent domain. We present applications of proposed method for simulating soccer games as well as evaluating and quantifying team strategies.



### Replicating Active Appearance Model by Generator Network
- **Arxiv ID**: http://arxiv.org/abs/1805.08704v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.08704v1)
- **Published**: 2018-05-14 04:54:00+00:00
- **Updated**: 2018-05-14 04:54:00+00:00
- **Authors**: Tian Han, Jiawen Wu, Ying Nian Wu
- **Comment**: None
- **Journal**: None
- **Summary**: A recent Cell paper [Chang and Tsao, 2017] reports an interesting discovery. For the face stimuli generated by a pre-trained active appearance model (AAM), the responses of neurons in the areas of the primate brain that are responsible for face recognition exhibit strong linear relationship with the shape variables and appearance variables of the AAM that generates the face stimuli. In this paper, we show that this behavior can be replicated by a deep generative model called the generator network, which assumes that the observed signals are generated by latent random variables via a top-down convolutional neural network. Specifically, we learn the generator network from the face images generated by a pre-trained AAM model using variational auto-encoder, and we show that the inferred latent variables of the learned generator network have strong linear relationship with the shape and appearance variables of the AAM model that generates the face images. Unlike the AAM model that has an explicit shape model where the shape variables generate the control points or landmarks, the generator network has no such shape model and shape variables. Yet the generator network can learn the shape knowledge in the sense that some of the latent variables of the learned generator network capture the shape variations in the face images generated by AAM.



### Learning Dual Convolutional Neural Networks for Low-Level Vision
- **Arxiv ID**: http://arxiv.org/abs/1805.05020v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.05020v1)
- **Published**: 2018-05-14 06:24:04+00:00
- **Updated**: 2018-05-14 06:24:04+00:00
- **Authors**: Jinshan Pan, Sifei Liu, Deqing Sun, Jiawei Zhang, Yang Liu, Jimmy Ren, Zechao Li, Jinhui Tang, Huchuan Lu, Yu-Wing Tai, Ming-Hsuan Yang
- **Comment**: CVPR 2018
- **Journal**: None
- **Summary**: In this paper, we propose a general dual convolutional neural network (DualCNN) for low-level vision problems, e.g., super-resolution, edge-preserving filtering, deraining and dehazing. These problems usually involve the estimation of two components of the target signals: structures and details. Motivated by this, our proposed DualCNN consists of two parallel branches, which respectively recovers the structures and details in an end-to-end manner. The recovered structures and details can generate the target signals according to the formation model for each particular application. The DualCNN is a flexible framework for low-level vision tasks and can be easily incorporated into existing CNNs. Experimental results show that the DualCNN can be effectively applied to numerous low-level vision tasks with favorable performance against the state-of-the-art methods.



### Multi-view Common Component Discriminant Analysis for Cross-view Classification
- **Arxiv ID**: http://arxiv.org/abs/1805.05029v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.05029v2)
- **Published**: 2018-05-14 07:09:33+00:00
- **Updated**: 2018-05-20 03:56:44+00:00
- **Authors**: Xinge You, Jiamiao Xu, Wei Yuan, Xiao-Yuan Jing, Dacheng Tao, Taiping Zhang
- **Comment**: This paper needs more revision
- **Journal**: None
- **Summary**: Cross-view classification that means to classify samples from heterogeneous views is a significant yet challenging problem in computer vision. A promising approach to handle this problem is the multi-view subspace learning (MvSL), which intends to find a common subspace for multi-view data. Despite the satisfactory results achieved by existing methods, the performance of previous work will be dramatically degraded when multi-view data lies on nonlinear manifolds. To circumvent this drawback, we propose Multi-view Common Component Discriminant Analysis (MvCCDA) to handle view discrepancy, discriminability and nonlinearity in a joint manner. Specifically, our MvCCDA incorporates supervised information and local geometric information into the common component extraction process to learn a discriminant common subspace and to discover the nonlinear structure embedded in multi-view data. We develop a kernel method of MvCCDA to further boost the performance of MvCCDA. Beyond kernel extension, optimization and complexity analysis of MvCCDA are also presented for completeness. Our MvCCDA is competitive with the state-of-the-art MvSL based methods on four benchmark datasets, demonstrating its superiority.



### Token-level and sequence-level loss smoothing for RNN language models
- **Arxiv ID**: http://arxiv.org/abs/1805.05062v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1805.05062v1)
- **Published**: 2018-05-14 08:37:50+00:00
- **Updated**: 2018-05-14 08:37:50+00:00
- **Authors**: Maha Elbayad, Laurent Besacier, Jakob Verbeek
- **Comment**: Accepted by ACL 2018
- **Journal**: None
- **Summary**: Despite the effectiveness of recurrent neural network language models, their maximum likelihood estimation suffers from two limitations. It treats all sentences that do not match the ground truth as equally poor, ignoring the structure of the output space. Second, it suffers from "exposure bias": during training tokens are predicted given ground-truth sequences, while at test time prediction is conditioned on generated output sequences. To overcome these limitations we build upon the recent reward augmented maximum likelihood approach \ie sequence-level smoothing that encourages the model to predict sentences close to the ground truth according to a given performance metric. We extend this approach to token-level loss smoothing, and propose improvements to the sequence-level smoothing approach. Our experiments on two different tasks, image captioning and machine translation, show that token-level and sequence-level loss smoothing are complementary, and significantly improve results.



### Attaining human-level performance with atlas location autocontext for anatomical landmark detection in 3D CT data
- **Arxiv ID**: http://arxiv.org/abs/1805.08687v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.08687v2)
- **Published**: 2018-05-14 09:12:03+00:00
- **Updated**: 2018-09-30 10:01:07+00:00
- **Authors**: Alison Q O'Neil, Antanas Kascenas, Joseph Henry, Daniel Wyeth, Matthew Shepherd, Erin Beveridge, Lauren Clunie, Carrie Sansom, Evelina Šeduikytė, Keith Muir, Ian Poole
- **Comment**: None
- **Journal**: None
- **Summary**: We present an efficient neural network method for locating anatomical landmarks in 3D medical CT scans, using atlas location autocontext in order to learn long-range spatial context. Location predictions are made by regression to Gaussian heatmaps, one heatmap per landmark. This system allows patchwise application of a shallow network, thus enabling multiple volumetric heatmaps to be predicted concurrently without prohibitive GPU memory requirements. Further, the system allows inter-landmark spatial relationships to be exploited using a simple overdetermined affine mapping that is robust to detection failures and occlusion or partial views. Evaluation is performed for 22 landmarks defined on a range of structures in head CT scans. Models are trained and validated on 201 scans. Over the final test set of 20 scans which was independently annotated by 2 human annotators, the neural network reaches an accuracy which matches the annotator variability, with similar human and machine patterns of variability across landmark classes.



### Unsupervised Intuitive Physics from Visual Observations
- **Arxiv ID**: http://arxiv.org/abs/1805.05086v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1805.05086v2)
- **Published**: 2018-05-14 09:41:23+00:00
- **Updated**: 2019-03-29 11:15:22+00:00
- **Authors**: Sebastien Ehrhardt, Aron Monszpart, Niloy Mitra, Andrea Vedaldi
- **Comment**: None
- **Journal**: None
- **Summary**: While learning models of intuitive physics is an increasingly active area of research, current approaches still fall short of natural intelligences in one important regard: they require external supervision, such as explicit access to physical states, at training and sometimes even at test times. Some authors have relaxed such requirements by supplementing the model with an handcrafted physical simulator. Still, the resulting methods are unable to automatically learn new complex environments and to understand physical interactions within them. In this work, we demonstrated for the first time learning such predictors directly from raw visual observations and without relying on simulators. We do so in two steps: first, we learn to track mechanically-salient objects in videos using causality and equivariance, two unsupervised learning principles that do not require auto-encoding. Second, we demonstrate that the extracted positions are sufficient to successfully train visual motion predictors that can take the underlying environment into account. We validate our predictors on synthetic datasets; then, we introduce a new dataset, ROLL4REAL, consisting of real objects rolling on complex terrains (pool table, elliptical bowl, and random height-field). We show that in all such cases it is possible to learn reliable extrapolators of the object trajectories from raw videos alone, without any form of external supervision and with no more prior knowledge than the choice of a convolutional neural network architecture.



### Hu-Fu: Hardware and Software Collaborative Attack Framework against Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1805.05098v2
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1805.05098v2)
- **Published**: 2018-05-14 10:15:29+00:00
- **Updated**: 2018-12-12 06:33:45+00:00
- **Authors**: Wenshuo Li, Jincheng Yu, Xuefei Ning, Pengjun Wang, Qi Wei, Yu Wang, Huazhong Yang
- **Comment**: 6 pages, 8 figures, 5 tables, accepted to ISVLSI 2018
- **Journal**: None
- **Summary**: Recently, Deep Learning (DL), especially Convolutional Neural Network (CNN), develops rapidly and is applied to many tasks, such as image classification, face recognition, image segmentation, and human detection. Due to its superior performance, DL-based models have a wide range of application in many areas, some of which are extremely safety-critical, e.g. intelligent surveillance and autonomous driving. Due to the latency and privacy problem of cloud computing, embedded accelerators are popular in these safety-critical areas. However, the robustness of the embedded DL system might be harmed by inserting hardware/software Trojans into the accelerator and the neural network model, since the accelerator and deploy tool (or neural network model) are usually provided by third-party companies. Fortunately, inserting hardware Trojans can only achieve inflexible attack, which means that hardware Trojans can easily break down the whole system or exchange two outputs, but can't make CNN recognize unknown pictures as targets. Though inserting software Trojans has more freedom of attack, it often requires tampering input images, which is not easy for attackers. So, in this paper, we propose a hardware-software collaborative attack framework to inject hidden neural network Trojans, which works as a back-door without requiring manipulating input images and is flexible for different scenarios. We test our attack framework for image classification and face recognition tasks, and get attack success rate of 92.6% and 100% on CIFAR10 and YouTube Faces, respectively, while keeping almost the same accuracy as the unattacked model in the normal mode. In addition, we show a specific attack scenario in which a face recognition system is attacked and gives a specific wrong answer.



### Exploiting the Value of the Center-dark Channel Prior for Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1805.05132v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.05132v1)
- **Published**: 2018-05-14 12:02:20+00:00
- **Updated**: 2018-05-14 12:02:20+00:00
- **Authors**: Chunbiao Zhu, Wenhao Zhang, Thomas H. Li, Ge Li
- **Comment**: Project website: https://chunbiaozhu.github.io/ACVR2017/
- **Journal**: None
- **Summary**: Saliency detection aims to detect the most attractive objects in images and is widely used as a foundation for various applications. In this paper, we propose a novel salient object detection algorithm for RGB-D images using center-dark channel priors. First, we generate an initial saliency map based on a color saliency map and a depth saliency map of a given RGB-D image. Then, we generate a center-dark channel map based on center saliency and dark channel priors. Finally, we fuse the initial saliency map with the center dark channel map to generate the final saliency map. Extensive evaluations over four benchmark datasets demonstrate that our proposed method performs favorably against most of the state-of-the-art approaches. Besides, we further discuss the application of the proposed algorithm in small target detection and demonstrate the universal value of center-dark channel priors in the field of object detection.



### Normal Similarity Network for Generative Modelling
- **Arxiv ID**: http://arxiv.org/abs/1805.05269v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.05269v1)
- **Published**: 2018-05-14 16:34:20+00:00
- **Updated**: 2018-05-14 16:34:20+00:00
- **Authors**: Jay Nandy, Wynne Hsu, Mong Li Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Gaussian distributions are commonly used as a key building block in many generative models. However, their applicability has not been well explored in deep networks. In this paper, we propose a novel deep generative model named as Normal Similarity Network (NSN) where the layers are constructed with Gaussian-style filters. NSN is trained with a layer-wise non-parametric density estimation algorithm that iteratively down-samples the training images and captures the density of the down-sampled training images in the final layer. Additionally, we propose NSN-Gen for generating new samples from noise vectors by iteratively reconstructing feature maps in the hidden layers of NSN. Our experiments suggest encouraging results of the proposed model for a wide range of computer vision applications including image generation, styling and reconstruction from occluded images.



### Cycle-Dehaze: Enhanced CycleGAN for Single Image Dehazing
- **Arxiv ID**: http://arxiv.org/abs/1805.05308v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.05308v1)
- **Published**: 2018-05-14 17:34:21+00:00
- **Updated**: 2018-05-14 17:34:21+00:00
- **Authors**: Deniz Engin, Anıl Genç, Hazım Kemal Ekenel
- **Comment**: Accepted at CVPRW: NTIRE 2018
- **Journal**: None
- **Summary**: In this paper, we present an end-to-end network, called Cycle-Dehaze, for single image dehazing problem, which does not require pairs of hazy and corresponding ground truth images for training. That is, we train the network by feeding clean and hazy images in an unpaired manner. Moreover, the proposed approach does not rely on estimation of the atmospheric scattering model parameters. Our method enhances CycleGAN formulation by combining cycle-consistency and perceptual losses in order to improve the quality of textural information recovery and generate visually better haze-free images. Typically, deep learning models for dehazing take low resolution images as input and produce low resolution outputs. However, in the NTIRE 2018 challenge on single image dehazing, high resolution images were provided. Therefore, we apply bicubic downscaling. After obtaining low-resolution outputs from the network, we utilize the Laplacian pyramid to upscale the output images to the original resolution. We conduct experiments on NYU-Depth, I-HAZE, and O-HAZE datasets. Extensive experiments demonstrate that the proposed approach improves CycleGAN method both quantitatively and qualitatively.



### SAVERS: SAR ATR with Verification Support Based on Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1805.06298v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.06298v1)
- **Published**: 2018-05-14 18:03:35+00:00
- **Updated**: 2018-05-14 18:03:35+00:00
- **Authors**: Hidetoshi Furukawa
- **Comment**: Technical Report, 6 pages, 8 figures, 5 tables, Copyright(C)2018
  IEICE. arXiv admin note: substantial text overlap with arXiv:1801.08558
- **Journal**: IEICE Technical Report, vol.118, no.28, SANE2018-5, pp.23-28, May
  2018
- **Summary**: We propose a new convolutional neural network (CNN) which performs coarse and fine segmentation for end-to-end synthetic aperture radar (SAR) automatic target recognition (ATR) system. In recent years, many CNNs for SAR ATR using deep learning have been proposed, but most of them classify target classes from fixed size target chips extracted from SAR imagery. On the other hand, we proposed the CNN which outputs the score of the multiple target classes and a background class for each pixel from the SAR imagery of arbitrary size and multiple targets as fine segmentation. However, it was necessary for humans to judge the CNN segmentation result. In this report, we propose a CNN called SAR ATR with verification support (SAVERS), which performs region-wise (i.e. coarse) segmentation and pixel-wise segmentation. SAVERS discriminates between target and non-target, and classifies multiple target classes and non-target class by coarse segmentation. This report describes the evaluation results of SAVERS using the Moving and Stationary Target Acquisition and Recognition (MSTAR) dataset.



### DeepEM: Deep 3D ConvNets With EM For Weakly Supervised Pulmonary Nodule Detection
- **Arxiv ID**: http://arxiv.org/abs/1805.05373v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1805.05373v3)
- **Published**: 2018-05-14 18:31:11+00:00
- **Updated**: 2018-05-26 16:41:34+00:00
- **Authors**: Wentao Zhu, Yeeleng S. Vang, Yufang Huang, Xiaohui Xie
- **Comment**: MICCAI2018 Early Accept, Code
  https://github.com/uci-cbcl/DeepEM-for-Weakly-Supervised-Detection.git
- **Journal**: None
- **Summary**: Recently deep learning has been witnessing widespread adoption in various medical image applications. However, training complex deep neural nets requires large-scale datasets labeled with ground truth, which are often unavailable in many medical image domains. For instance, to train a deep neural net to detect pulmonary nodules in lung computed tomography (CT) images, current practice is to manually label nodule locations and sizes in many CT images to construct a sufficiently large training dataset, which is costly and difficult to scale. On the other hand, electronic medical records (EMR) contain plenty of partial information on the content of each medical image. In this work, we explore how to tap this vast, but currently unexplored data source to improve pulmonary nodule detection. We propose DeepEM, a novel deep 3D ConvNet framework augmented with expectation-maximization (EM), to mine weakly supervised labels in EMRs for pulmonary nodule detection. Experimental results show that DeepEM can lead to 1.5\% and 3.9\% average improvement in free-response receiver operating characteristic (FROC) scores on LUNA16 and Tianchi datasets, respectively, demonstrating the utility of incomplete information in EMRs for improving deep learning algorithms.\footnote{https://github.com/uci-cbcl/DeepEM-for-Weakly-Supervised-Detection.git}



### Deep Attentional Structured Representation Learning for Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/1805.05389v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.05389v1)
- **Published**: 2018-05-14 19:13:16+00:00
- **Updated**: 2018-05-14 19:13:16+00:00
- **Authors**: Krishna Kanth Nakka, Mathieu Salzmann
- **Comment**: None
- **Journal**: None
- **Summary**: Structured representations, such as Bags of Words, VLAD and Fisher Vectors, have proven highly effective to tackle complex visual recognition tasks. As such, they have recently been incorporated into deep architectures. However, while effective, the resulting deep structured representation learning strategies typically aggregate local features from the entire image, ignoring the fact that, in complex recognition tasks, some regions provide much more discriminative information than others.   In this paper, we introduce an attentional structured representation learning framework that incorporates an image-specific attention mechanism within the aggregation process. Our framework learns to predict jointly the image class label and an attention map in an end-to-end fashion and without any other supervision than the target label. As evidenced by our experiments, this consistently outperforms attention-less structured representation learning and yields state-of-the-art results on standard scene recognition and fine-grained categorization benchmarks.



### An Automatic Patch-based Approach for HER-2 Scoring in Immunohistochemical Breast Cancer Images Using Color Features
- **Arxiv ID**: http://arxiv.org/abs/1805.05392v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.05392v1)
- **Published**: 2018-05-14 19:17:32+00:00
- **Updated**: 2018-05-14 19:17:32+00:00
- **Authors**: Caroline Q. Cordeiro, Sergio O. Ioshii, Jeovane H. Alves, Lucas F. Oliveira
- **Comment**: Accepted for presentation at the Brazilian Symposium of Applied
  Computing in Health (SBCAS) 2018
- **Journal**: None
- **Summary**: Breast cancer (BC) is the most common cancer among women world-wide, approximately 20-25% of BCs are HER-2 positive. Analysis of HER-2 is fundamental to defining the appropriate therapy for patients with breast cancer. Inter-pathologist variability in the test results can affect diagnostic accuracy. The present study intends to propose an automatic scoring HER-2 algorithm. Based on color features, the technique is fully-automated and avoids segmentation, showing a concordance higher than 90% with a pathologist in the experiments realized.



### Energy Efficient Hadamard Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1805.05421v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.05421v1)
- **Published**: 2018-05-14 20:19:24+00:00
- **Updated**: 2018-05-14 20:19:24+00:00
- **Authors**: T. Ceren Deveci, Serdar Cakir, A. Enis Cetin
- **Comment**: 15 pages, 3 figures
- **Journal**: None
- **Summary**: Deep learning has made significant improvements at many image processing tasks in recent years, such as image classification, object recognition and object detection. Convolutional neural networks (CNN), which is a popular deep learning architecture designed to process data in multiple array form, show great success to almost all detection \& recognition problems and computer vision tasks. However, the number of parameters in a CNN is too high such that the computers require more energy and larger memory size. In order to solve this problem, we propose a novel energy efficient model Binary Weight and Hadamard-transformed Image Network (BWHIN), which is a combination of Binary Weight Network (BWN) and Hadamard-transformed Image Network (HIN). It is observed that energy efficiency is achieved with a slight sacrifice at classification accuracy. Among all energy efficient networks, our novel ensemble model outperforms other energy efficient models.



### A CNN for homogneous Riemannian manifolds with applications to Neuroimaging
- **Arxiv ID**: http://arxiv.org/abs/1805.05487v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.05487v3)
- **Published**: 2018-05-14 22:56:46+00:00
- **Updated**: 2018-08-06 20:46:42+00:00
- **Authors**: Rudrasis Chakraborty, Monami Banerjee, Baba C. Vemuri
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks are ubiquitous in Machine Learning applications for solving a variety of problems. They however can not be used in their native form when the domain of the data is commonly encountered manifolds such as the sphere, the special orthogonal group, the Grassmanian, the manifold of symmetric positive definite matrices and others. Most recently, generalization of CNNs to data domains such as the 2-sphere has been reported by some research groups, which is referred to as the spherical CNNs (SCNNs). The key property of SCNNs distinct from CNNs is that they exhibit the rotational equivariance property that allows for sharing learned weights within a layer. In this paper, we theoretically generalize the CNNs to Riemannian homogeneous manifolds, that include but are not limited to the aforementioned example manifolds. Our key contributions in this work are: (i) A theorem stating that linear group equivariance systems are fully characterized by correlation of functions on the domain manifold and vice-versa. This is fundamental to the characterization of all linear group equivariant systems and parallels the widely used result in linear system theory for vector spaces. (ii) As a corrolary, we prove the equivariance of the correlation operation to group actions admitted by the input domains which are Riemannian homogeneous manifolds. (iii) We present the first end-to-end deep network architecture for classification of diffusion magnetic resonance image (dMRI) scans acquired from a cohort of 44 Parkinson Disease patients and 50 control/normal subjects. (iv) A proof of concept experiment involving synthetic data generated on the manifold of symmetric positive definite matrices is presented to demonstrate the applicability of our network to other types of domains.



