# Arxiv Papers in cs.CV on 2018-05-18
### MDSSD: Multi-scale Deconvolutional Single Shot Detector for Small Objects
- **Arxiv ID**: http://arxiv.org/abs/1805.07009v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.07009v3)
- **Published**: 2018-05-18 01:09:25+00:00
- **Updated**: 2020-02-25 14:13:35+00:00
- **Authors**: Lisha Cui, Rui Ma, Pei Lv, Xiaoheng Jiang, Zhimin Gao, Bing Zhou, Mingliang Xu
- **Comment**: None
- **Journal**: None
- **Summary**: For most of the object detectors based on multi-scale feature maps, the shallow layers are rich in fine spatial information and thus mainly responsible for small object detection. The performance of small object detection, however, is still less than satisfactory because of the deficiency of semantic information on shallow feature maps. In this paper, we design a Multi-scale Deconvolutional Single Shot Detector (MDSSD), especially for small object detection. In MDSSD, multiple high-level feature maps at different scales are upsampled simultaneously to increase the spatial resolution. Afterwards, we implement the skip connections with low-level feature maps via Fusion Block. The fusion feature maps, named Fusion Module, are of strong feature representational power of small instances. It is noteworthy that these high-level feature maps utilized in Fusion Block preserve both strong semantic information and some fine details of small instances, rather than the top-most layer where the representation of fine details for small objects are potentially wiped out. The proposed framework achieves 77.6% mAP for small object detection on the challenging dataset TT100K with 512 x 512 input, outperforming other detectors with a large margin. Moreover, it can also achieve state-of-the-art results for general object detection on PASCAL VOC2007 test and MS COCO test-dev2015, especially achieving 2 to 5 points improvement on small object categories.



### Understanding and Improving Deep Neural Network for Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/1805.07020v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.07020v1)
- **Published**: 2018-05-18 02:01:45+00:00
- **Updated**: 2018-05-18 02:01:45+00:00
- **Authors**: Li Xue, Si Xiandong, Nie Lanshun, Li Jiazhen, Ding Renjie, Zhan Dechen, Chu Dianhui
- **Comment**: 10 pages, 9 figures, 6 Tables
- **Journal**: None
- **Summary**: Activity recognition has become a popular research branch in the field of pervasive computing in recent years. A large number of experiments can be obtained that activity sensor-based data's characteristic in activity recognition is variety, volume, and velocity. Deep learning technology, together with its various models, is one of the most effective ways of working on activity data. Nevertheless, there is no clear understanding of why it performs so well or how to make it more effective. In order to solve this problem, first, we applied convolution neural network on Human Activity Recognition Using Smart phones Data Set. Second, we realized the visualization of the sensor-based activity's data features extracted from the neural network. Then we had in-depth analysis of the visualization of features, explored the relationship between activity and features, and analyzed how Neural Networks identify activity based on these features. After that, we extracted the significant features related to the activities and sent the features to the DNN-based fusion model, which improved the classification rate to 96.1%. This is the first work to our knowledge that visualizes abstract sensor-based activity data features. Based on the results, the method proposed in the paper promises to realize the accurate classification of sensor- based activity recognition.



### Scene Understanding Networks for Autonomous Driving based on Around View Monitoring System
- **Arxiv ID**: http://arxiv.org/abs/1805.07029v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1805.07029v1)
- **Published**: 2018-05-18 02:54:54+00:00
- **Updated**: 2018-05-18 02:54:54+00:00
- **Authors**: JeongYeol Baek, Ioana Veronica Chelu, Livia Iordache, Vlad Paunescu, HyunJoo Ryu, Alexandru Ghiuta, Andrei Petreanu, YunSung Soh, Andrei Leica, ByeongMoon Jeon
- **Comment**: Accepted by CVPR 2018 Workshop on Autonomous Driving
- **Journal**: None
- **Summary**: Modern driver assistance systems rely on a wide range of sensors (RADAR, LIDAR, ultrasound and cameras) for scene understanding and prediction. These sensors are typically used for detecting traffic participants and scene elements required for navigation. In this paper we argue that relying on camera based systems, specifically Around View Monitoring (AVM) system has great potential to achieve these goals in both parking and driving modes with decreased costs. The contributions of this paper are as follows: we present a new end-to-end solution for delimiting the safe drivable area for each frame by means of identifying the closest obstacle in each direction from the driving vehicle, we use this approach to calculate the distance to the nearest obstacles and we incorporate it into a unified end-to-end architecture capable of joint object detection, curb detection and safe drivable area detection. Furthermore, we describe the family of networks for both a high accuracy solution and a low complexity solution. We also introduce further augmentation of the base architecture with 3D object detection.



### SemStyle: Learning to Generate Stylised Image Captions using Unaligned Text
- **Arxiv ID**: http://arxiv.org/abs/1805.07030v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.07030v1)
- **Published**: 2018-05-18 03:01:45+00:00
- **Updated**: 2018-05-18 03:01:45+00:00
- **Authors**: Alexander Mathews, Lexing Xie, Xuming He
- **Comment**: Accepted at CVPR 2018
- **Journal**: None
- **Summary**: Linguistic style is an essential part of written communication, with the power to affect both clarity and attractiveness. With recent advances in vision and language, we can start to tackle the problem of generating image captions that are both visually grounded and appropriately styled. Existing approaches either require styled training captions aligned to images or generate captions with low relevance. We develop a model that learns to generate visually relevant styled captions from a large corpus of styled text without aligned images. The core idea of this model, called SemStyle, is to separate semantics and style. One key component is a novel and concise semantic term representation generated using natural language processing techniques and frame semantics. In addition, we develop a unified language model that decodes sentences with diverse word choices and syntax for different styles. Evaluations, both automatic and manual, show captions from SemStyle preserve image semantics, are descriptive, and are style shifted. More broadly, this work provides possibilities to learn richer image descriptions from the plethora of linguistic data available on the web.



### LiteFlowNet: A Lightweight Convolutional Neural Network for Optical Flow Estimation
- **Arxiv ID**: http://arxiv.org/abs/1805.07036v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.07036v1)
- **Published**: 2018-05-18 03:29:54+00:00
- **Updated**: 2018-05-18 03:29:54+00:00
- **Authors**: Tak-Wai Hui, Xiaoou Tang, Chen Change Loy
- **Comment**: Accepted to CVPR 2018 (spotlight). Project page:
  http://mmlab.ie.cuhk.edu.hk/projects/LiteFlowNet/
- **Journal**: None
- **Summary**: FlowNet2, the state-of-the-art convolutional neural network (CNN) for optical flow estimation, requires over 160M parameters to achieve accurate flow estimation. In this paper we present an alternative network that outperforms FlowNet2 on the challenging Sintel final pass and KITTI benchmarks, while being 30 times smaller in the model size and 1.36 times faster in the running speed. This is made possible by drilling down to architectural details that might have been missed in the current frameworks: (1) We present a more effective flow inference approach at each pyramid level through a lightweight cascaded network. It not only improves flow estimation accuracy through early correction, but also permits seamless incorporation of descriptor matching in our network. (2) We present a novel flow regularization layer to ameliorate the issue of outliers and vague flow boundaries by using a feature-driven local convolution. (3) Our network owns an effective structure for pyramidal feature extraction and embraces feature warping rather than image warping as practiced in FlowNet2. Our code and trained models are available at https://github.com/twhui/LiteFlowNet .



### A Theoretical Explanation for Perplexing Behaviors of Backpropagation-based Visualizations
- **Arxiv ID**: http://arxiv.org/abs/1805.07039v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1805.07039v4)
- **Published**: 2018-05-18 03:45:06+00:00
- **Updated**: 2020-02-13 16:23:39+00:00
- **Authors**: Weili Nie, Yang Zhang, Ankit Patel
- **Comment**: 21 pages, ICML 2018 (We revised the proofs of Theorem 1 and 2 in
  Appendix)
- **Journal**: None
- **Summary**: Backpropagation-based visualizations have been proposed to interpret convolutional neural networks (CNNs), however a theory is missing to justify their behaviors: Guided backpropagation (GBP) and deconvolutional network (DeconvNet) generate more human-interpretable but less class-sensitive visualizations than saliency map. Motivated by this, we develop a theoretical explanation revealing that GBP and DeconvNet are essentially doing (partial) image recovery which is unrelated to the network decisions. Specifically, our analysis shows that the backward ReLU introduced by GBP and DeconvNet, and the local connections in CNNs are the two main causes of compelling visualizations. Extensive experiments are provided that support the theoretical analysis.



### Multi-level Wavelet-CNN for Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/1805.07071v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.07071v2)
- **Published**: 2018-05-18 06:59:00+00:00
- **Updated**: 2018-05-22 14:02:13+00:00
- **Authors**: Pengju Liu, Hongzhi Zhang, Kai Zhang, Liang Lin, Wangmeng Zuo
- **Comment**: Accepted for publication at CVPR NTIRE Workshop, 2018
- **Journal**: None
- **Summary**: The tradeoff between receptive field size and efficiency is a crucial issue in low level vision. Plain convolutional networks (CNNs) generally enlarge the receptive field at the expense of computational cost. Recently, dilated filtering has been adopted to address this issue. But it suffers from gridding effect, and the resulting receptive field is only a sparse sampling of input image with checkerboard patterns. In this paper, we present a novel multi-level wavelet CNN (MWCNN) model for better tradeoff between receptive field size and computational efficiency. With the modified U-Net architecture, wavelet transform is introduced to reduce the size of feature maps in the contracting subnetwork. Furthermore, another convolutional layer is further used to decrease the channels of feature maps. In the expanding subnetwork, inverse wavelet transform is then deployed to reconstruct the high resolution feature maps. Our MWCNN can also be explained as the generalization of dilated filtering and subsampling, and can be applied to many image restoration tasks. The experimental results clearly show the effectiveness of MWCNN for image denoising, single image super-resolution, and JPEG image artifacts removal.



### TractSeg - Fast and accurate white matter tract segmentation
- **Arxiv ID**: http://arxiv.org/abs/1805.07103v2
- **DOI**: 10.1016/j.neuroimage.2018.07.070
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1805.07103v2)
- **Published**: 2018-05-18 09:05:54+00:00
- **Updated**: 2018-08-20 15:56:06+00:00
- **Authors**: Jakob Wasserthal, Peter Neher, Klaus H. Maier-Hein
- **Comment**: None
- **Journal**: Wasserthal, J., Neher, P., Maier-Hein, K.H., 2018. TractSeg - Fast
  and accurate white matter tract segmentation. NeuroImage
- **Summary**: The individual course of white matter fiber tracts is an important key for analysis of white matter characteristics in healthy and diseased brains. Uniquely, diffusion-weighted MRI tractography in combination with region-based or clustering-based selection of streamlines allows for the in-vivo delineation and analysis of anatomically well known tracts. This, however, currently requires complex, computationally intensive and tedious-to-set-up processing pipelines. TractSeg is a novel convolutional neural network-based approach that directly segments tracts in the field of fiber orientation distribution function (fODF) peaks without requiring tractography, image registration or parcellation. We demonstrate in 105 subjects from the Human Connectome Project that the proposed approach is much faster than existing methods while providing unprecedented accuracy. The code and data are openly available at https://github.com/MIC-DKFZ/TractSeg/ and https://doi.org/10.5281/zenodo.1088277, respectively.



### Improving Image Captioning with Conditional Generative Adversarial Nets
- **Arxiv ID**: http://arxiv.org/abs/1805.07112v4
- **DOI**: 10.1609/aaai.v33i01.33018142
- **Categories**: **cs.CV**, cs.LG, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/1805.07112v4)
- **Published**: 2018-05-18 09:31:53+00:00
- **Updated**: 2019-02-13 03:02:47+00:00
- **Authors**: Chen Chen, Shuai Mu, Wanpeng Xiao, Zexiong Ye, Liesi Wu, Qi Ju
- **Comment**: 12 pages; 33 figures; 36 refenences; Accepted by AAAI2019
- **Journal**: AAAI2019
- **Summary**: In this paper, we propose a novel conditional-generative-adversarial-nets-based image captioning framework as an extension of traditional reinforcement-learning (RL)-based encoder-decoder architecture. To deal with the inconsistent evaluation problem among different objective language metrics, we are motivated to design some "discriminator" networks to automatically and progressively determine whether generated caption is human described or machine generated. Two kinds of discriminator architectures (CNN and RNN-based structures) are introduced since each has its own advantages. The proposed algorithm is generic so that it can enhance any existing RL-based image captioning framework and we show that the conventional RL training method is just a special case of our approach. Empirically, we show consistent improvements over all language evaluation metrics for different state-of-the-art image captioning models. In addition, the well-trained discriminators can also be viewed as objective image captioning evaluators



### Recurrent knowledge distillation
- **Arxiv ID**: http://arxiv.org/abs/1805.07170v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.07170v1)
- **Published**: 2018-05-18 12:32:16+00:00
- **Updated**: 2018-05-18 12:32:16+00:00
- **Authors**: Silvia L. Pintea, Yue Liu, Jan C. van Gemert
- **Comment**: International Conference on Image Processing (ICIP), 2018
- **Journal**: None
- **Summary**: Knowledge distillation compacts deep networks by letting a small student network learn from a large teacher network. The accuracy of knowledge distillation recently benefited from adding residual layers. We propose to reduce the size of the student network even further by recasting multiple residual layers in the teacher network into a single recurrent student layer. We propose three variants of adding recurrent connections into the student network, and show experimentally on CIFAR-10, Scenes and MiniPlaces, that we can reduce the number of parameters at little loss in accuracy.



### The EuroCity Persons Dataset: A Novel Benchmark for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1805.07193v2
- **DOI**: 10.1109/TPAMI.2019.2897684
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1805.07193v2)
- **Published**: 2018-05-18 13:17:46+00:00
- **Updated**: 2018-06-05 15:20:52+00:00
- **Authors**: Markus Braun, Sebastian Krebs, Fabian Flohr, Dariu M. Gavrila
- **Comment**: Submitted to IEEE Trans. on Pattern Analysis and Machine Intelligence
- **Journal**: Published in IEEE Trans. on Pattern Analysis and Machine
  Intelligence, 2019
- **Summary**: Big data has had a great share in the success of deep learning in computer vision. Recent works suggest that there is significant further potential to increase object detection performance by utilizing even bigger datasets. In this paper, we introduce the EuroCity Persons dataset, which provides a large number of highly diverse, accurate and detailed annotations of pedestrians, cyclists and other riders in urban traffic scenes. The images for this dataset were collected on-board a moving vehicle in 31 cities of 12 European countries. With over 238200 person instances manually labeled in over 47300 images, EuroCity Persons is nearly one order of magnitude larger than person datasets used previously for benchmarking. The dataset furthermore contains a large number of person orientation annotations (over 211200). We optimize four state-of-the-art deep learning approaches (Faster R-CNN, R-FCN, SSD and YOLOv3) to serve as baselines for the new object detection benchmark. In experiments with previous datasets we analyze the generalization capabilities of these detectors when trained with the new dataset. We furthermore study the effect of the training set size, the dataset diversity (day- vs. night-time, geographical region), the dataset detail (i.e. availability of object orientation information) and the annotation quality on the detector performance. Finally, we analyze error sources and discuss the road ahead.



### Recognition of Activities from Eye Gaze and Egocentric Video
- **Arxiv ID**: http://arxiv.org/abs/1805.07253v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.07253v1)
- **Published**: 2018-05-18 14:52:25+00:00
- **Updated**: 2018-05-18 14:52:25+00:00
- **Authors**: Anjith George, Aurobinda Routray
- **Comment**: 7 pages, 9 figures
- **Journal**: None
- **Summary**: This paper presents a framework for recognition of human activity from egocentric video and eye tracking data obtained from a head-mounted eye tracker. Three channels of information such as eye movement, ego-motion, and visual features are combined for the classification of activities. Image features were extracted using a pre-trained convolutional neural network. Eye and ego-motion are quantized, and the windowed histograms are used as the features. The combination of features obtains better accuracy for activity classification as compared to individual features.



### Neural Network Compression using Transform Coding and Clustering
- **Arxiv ID**: http://arxiv.org/abs/1805.07258v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.07258v1)
- **Published**: 2018-05-18 14:57:52+00:00
- **Updated**: 2018-05-18 14:57:52+00:00
- **Authors**: Thorsten Laude, Yannick Richter, Jörn Ostermann
- **Comment**: None
- **Journal**: None
- **Summary**: With the deployment of neural networks on mobile devices and the necessity of transmitting neural networks over limited or expensive channels, the file size of the trained model was identified as bottleneck. In this paper, we propose a codec for the compression of neural networks which is based on transform coding for convolutional and dense layers and on clustering for biases and normalizations. By using this codec, we achieve average compression factors between 7.9-9.3 while the accuracy of the compressed networks for image classification decreases only by 1%-2%, respectively.



### XOGAN: One-to-Many Unsupervised Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/1805.07277v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.07277v1)
- **Published**: 2018-05-18 15:19:22+00:00
- **Updated**: 2018-05-18 15:19:22+00:00
- **Authors**: Yongqi Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised image-to-image translation aims at learning the relationship between samples from two image domains without supervised pair information. The relationship between two domain images can be one-to-one, one-to-many or many-to-many. In this paper, we study the one-to-many unsupervised image translation problem in which an input sample from one domain can correspond to multiple samples in the other domain. To learn the complex relationship between the two domains, we introduce an additional variable to control the variations in our one-to-many mapping. A generative model with an XO-structure, called the XOGAN, is proposed to learn the cross domain relationship among the two domains and the ad- ditional variables. Not only can we learn to translate between the two image domains, we can also handle the translated images with additional variations. Experiments are performed on unpaired image generation tasks, including edges-to-objects translation and facial image translation. We show that the proposed XOGAN model can generate plausible images and control variations, such as color and texture, of the generated images. Moreover, while state-of-the-art unpaired image generation algorithms tend to generate images with monotonous colors, XOGAN can generate more diverse results.



### An Unsupervised Approach to Solving Inverse Problems using Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1805.07281v2
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.07281v2)
- **Published**: 2018-05-18 15:23:01+00:00
- **Updated**: 2018-06-04 16:30:08+00:00
- **Authors**: Rushil Anirudh, Jayaraman J. Thiagarajan, Bhavya Kailkhura, Timo Bremer
- **Comment**: None
- **Journal**: None
- **Summary**: Solving inverse problems continues to be a challenge in a wide array of applications ranging from deblurring, image inpainting, source separation etc. Most existing techniques solve such inverse problems by either explicitly or implicitly finding the inverse of the model. The former class of techniques require explicit knowledge of the measurement process which can be unrealistic, and rely on strong analytical regularizers to constrain the solution space, which often do not generalize well. The latter approaches have had remarkable success in part due to deep learning, but require a large collection of source-observation pairs, which can be prohibitively expensive. In this paper, we propose an unsupervised technique to solve inverse problems with generative adversarial networks (GANs). Using a pre-trained GAN in the space of source signals, we show that one can reliably recover solutions to under determined problems in a `blind' fashion, i.e., without knowledge of the measurement process. We solve this by making successive estimates on the model and the solution in an iterative fashion. We show promising results in three challenging applications -- blind source separation, image deblurring, and recovering an image from its edge map, and perform better than several baselines.



### Learning 3D Shape Completion under Weak Supervision
- **Arxiv ID**: http://arxiv.org/abs/1805.07290v2
- **DOI**: 10.1007/s11263-018-1126-y
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.07290v2)
- **Published**: 2018-05-18 15:37:28+00:00
- **Updated**: 2018-11-27 15:03:06+00:00
- **Authors**: David Stutz, Andreas Geiger
- **Comment**: None
- **Journal**: David Stutz, Andreas Geiger. Learning 3D Shape Completion under
  Weak Supervision. International Journal of Computer Vision (2018)
- **Summary**: We address the problem of 3D shape completion from sparse and noisy point clouds, a fundamental problem in computer vision and robotics. Recent approaches are either data-driven or learning-based: Data-driven approaches rely on a shape model whose parameters are optimized to fit the observations; Learning-based approaches, in contrast, avoid the expensive optimization step by learning to directly predict complete shapes from incomplete observations in a fully-supervised setting. However, full supervision is often not available in practice. In this work, we propose a weakly-supervised learning-based approach to 3D shape completion which neither requires slow optimization nor direct supervision. While we also learn a shape prior on synthetic data, we amortize, i.e., learn, maximum likelihood fitting using deep neural networks resulting in efficient shape completion without sacrificing accuracy. On synthetic benchmarks based on ShapeNet and ModelNet as well as on real robotics data from KITTI and Kinect, we demonstrate that the proposed amortized maximum likelihood approach is able to compete with recent fully supervised baselines and outperforms data-driven approaches, while requiring less supervision and being significantly faster.



### Stop memorizing: A data-dependent regularization framework for intrinsic pattern learning
- **Arxiv ID**: http://arxiv.org/abs/1805.07291v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.07291v2)
- **Published**: 2018-05-18 15:38:06+00:00
- **Updated**: 2018-09-23 00:46:57+00:00
- **Authors**: Wei Zhu, Qiang Qiu, Bao Wang, Jianfeng Lu, Guillermo Sapiro, Ingrid Daubechies
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) typically have enough capacity to fit random data by brute force even when conventional data-dependent regularizations focusing on the geometry of the features are imposed. We find out that the reason for this is the inconsistency between the enforced geometry and the standard softmax cross entropy loss. To resolve this, we propose a new framework for data-dependent DNN regularization, the Geometrically-Regularized-Self-Validating neural Networks (GRSVNet). During training, the geometry enforced on one batch of features is simultaneously validated on a separate batch using a validation loss consistent with the geometry. We study a particular case of GRSVNet, the Orthogonal-Low-rank Embedding (OLE)-GRSVNet, which is capable of producing highly discriminative features residing in orthogonal low-rank subspaces. Numerical experiments show that OLE-GRSVNet outperforms DNNs with conventional regularization when trained on real data. More importantly, unlike conventional DNNs, OLE-GRSVNet refuses to memorize random data or random labels, suggesting it only learns intrinsic patterns by reducing the memorizing capacity of the baseline DNN.



### Mixup-Based Acoustic Scene Classification Using Multi-Channel Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1805.07319v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.07319v1)
- **Published**: 2018-05-18 16:36:19+00:00
- **Updated**: 2018-05-18 16:36:19+00:00
- **Authors**: Kele Xu, Dawei Feng, Haibo Mi, Boqing Zhu, Dezhi Wang, Lilun Zhang, Hengxing Cai, Shuwen Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Audio scene classification, the problem of predicting class labels of audio scenes, has drawn lots of attention during the last several years. However, it remains challenging and falls short of accuracy and efficiency. Recently, Convolutional Neural Network (CNN)-based methods have achieved better performance with comparison to the traditional methods. Nevertheless, conventional single channel CNN may fail to consider the fact that additional cues may be embedded in the multi-channel recordings. In this paper, we explore the use of Multi-channel CNN for the classification task, which aims to extract features from different channels in an end-to-end manner. We conduct the evaluation compared with the conventional CNN and traditional Gaussian Mixture Model-based methods. Moreover, to improve the classification accuracy further, this paper explores the using of mixup method. In brief, mixup trains the neural network on linear combinations of pairs of the representation of audio scene examples and their labels. By employing the mixup approach for data argumentation, the novel model can provide higher prediction accuracy and robustness in contrast with previous models, while the generalization error can also be reduced on the evaluation data.



### Scanner: Efficient Video Analysis at Scale
- **Arxiv ID**: http://arxiv.org/abs/1805.07339v1
- **DOI**: 10.1145/3197517.3201394
- **Categories**: **cs.CV**, cs.DC, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1805.07339v1)
- **Published**: 2018-05-18 17:43:55+00:00
- **Updated**: 2018-05-18 17:43:55+00:00
- **Authors**: Alex Poms, Will Crichton, Pat Hanrahan, Kayvon Fatahalian
- **Comment**: 14 pages, 14 figuers
- **Journal**: None
- **Summary**: A growing number of visual computing applications depend on the analysis of large video collections. The challenge is that scaling applications to operate on these datasets requires efficient systems for pixel data access and parallel processing across large numbers of machines. Few programmers have the capability to operate efficiently at these scales, limiting the field's ability to explore new applications that leverage big video data. In response, we have created Scanner, a system for productive and efficient video analysis at scale. Scanner organizes video collections as tables in a data store optimized for sampling frames from compressed video, and executes pixel processing computations, expressed as dataflow graphs, on these frames. Scanner schedules video analysis applications expressed using these abstractions onto heterogeneous throughput computing hardware, such as multi-core CPUs, GPUs, and media processing ASICs, for high-throughput pixel processing. We demonstrate the productivity of Scanner by authoring a variety of video processing applications including the synthesis of stereo VR video streams from multi-camera rigs, markerless 3D human pose reconstruction from video, and data-mining big video datasets such as hundreds of feature-length films or over 70,000 hours of TV news. These applications achieve near-expert performance on a single machine and scale efficiently to hundreds of machines, enabling formerly long-running big video data analysis tasks to be carried out in minutes to hours.



### Batch Normalization in the final layer of generative networks
- **Arxiv ID**: http://arxiv.org/abs/1805.07389v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.07389v1)
- **Published**: 2018-05-18 18:40:51+00:00
- **Updated**: 2018-05-18 18:40:51+00:00
- **Authors**: Sean Mullery, Paul F. Whelan
- **Comment**: 8 pages, 3 figures, under IMVIP 2018 submission
- **Journal**: None
- **Summary**: Generative Networks have shown great promise in generating photo-realistic images. Despite this, the theory surrounding them is still an active research area. Much of the useful work with Generative networks rely on heuristics that tend to produce good results. One of these heuristics is the advice not to use Batch Normalization in the final layer of the generator network. Many of the state-of-the-art generative network architectures use this heuristic, but the reasons for doing so are inconsistent. This paper will show that this is not necessarily a good heuristic and that Batch Normalization can be beneficial in the final layer of the generator network either by placing it before the final non-linear activation, usually a $tanh$ or replacing the final $tanh$ activation altogether with Batch Normalization and clipping. We show that this can lead to the faster training of Generator networks by matching the generator to the mean and standard deviation of the target distribution's image colour values.



### Incept-N: A Convolutional Neural Network based Classification Approach for Predicting Nationality from Facial Features
- **Arxiv ID**: http://arxiv.org/abs/1805.07426v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.07426v1)
- **Published**: 2018-05-18 20:12:07+00:00
- **Updated**: 2018-05-18 20:12:07+00:00
- **Authors**: Masum Shah Junayed, Afsana Ahsan Jeny, Nafis Neehal
- **Comment**: 5 Pages, 7 Figures, 3 Tables
- **Journal**: None
- **Summary**: The nationality of a human being is a well-known identifying characteristic used for every major authentication purpose in every country. Albeit advances in the application of Artificial Intelligence and Computer Vision in different aspects, its contribution to this specific security procedure is yet to be cultivated. With a goal to successfully applying computer vision techniques to predict the nationality of a person based on his facial features, we have proposed this novel method and have achieved an average of 93.6% accuracy with very low misclassification rate.



### Neural Architecture Search using Deep Neural Networks and Monte Carlo Tree Search
- **Arxiv ID**: http://arxiv.org/abs/1805.07440v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.DC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.07440v5)
- **Published**: 2018-05-18 20:57:41+00:00
- **Updated**: 2019-11-21 17:45:31+00:00
- **Authors**: Linnan Wang, Yiyang Zhao, Yuu Jinnai, Yuandong Tian, Rodrigo Fonseca
- **Comment**: To appear in the Thirty-Fourth AAAI conference on Artificial
  Intelligence (AAAI-2020)
- **Journal**: None
- **Summary**: Neural Architecture Search (NAS) has shown great success in automating the design of neural networks, but the prohibitive amount of computations behind current NAS methods requires further investigations in improving the sample efficiency and the network evaluation cost to get better results in a shorter time. In this paper, we present a novel scalable Monte Carlo Tree Search (MCTS) based NAS agent, named AlphaX, to tackle these two aspects. AlphaX improves the search efficiency by adaptively balancing the exploration and exploitation at the state level, and by a Meta-Deep Neural Network (DNN) to predict network accuracies for biasing the search toward a promising region. To amortize the network evaluation cost, AlphaX accelerates MCTS rollouts with a distributed design and reduces the number of epochs in evaluating a network by transfer learning, which is guided with the tree structure in MCTS. In 12 GPU days and 1000 samples, AlphaX found an architecture that reaches 97.84\% top-1 accuracy on CIFAR-10, and 75.5\% top-1 accuracy on ImageNet, exceeding SOTA NAS methods in both the accuracy and sampling efficiency. Particularly, we also evaluate AlphaX on NASBench-101, a large scale NAS dataset; AlphaX is 3x and 2.8x more sample efficient than Random Search and Regularized Evolution in finding the global optimum. Finally, we show the searched architecture improves a variety of vision applications from Neural Style Transfer, to Image Captioning and Object Detection.



### My camera can see through fences: A deep learning approach for image de-fencing
- **Arxiv ID**: http://arxiv.org/abs/1805.07442v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.07442v1)
- **Published**: 2018-05-18 21:02:04+00:00
- **Updated**: 2018-05-18 21:02:04+00:00
- **Authors**: Sankaraganesh Jonna, Krishna Kanth Nakka, Rajiv R. Sahay
- **Comment**: ACPR 2015, Kuala Lumpur
- **Journal**: None
- **Summary**: In recent times, the availability of inexpensive image capturing devices such as smartphones/tablets has led to an exponential increase in the number of images/videos captured. However, sometimes the amateur photographer is hindered by fences in the scene which have to be removed after the image has been captured. Conventional approaches to image de-fencing suffer from inaccurate and non-robust fence detection apart from being limited to processing images of only static occluded scenes. In this paper, we propose a semi-automated de-fencing algorithm using a video of the dynamic scene. We use convolutional neural networks for detecting fence pixels. We provide qualitative as well as quantitative comparison results with existing lattice detection algorithms on the existing PSU NRT data set and a proposed challenging fenced image dataset. The inverse problem of fence removal is solved using split Bregman technique assuming total variation of the de-fenced image as the regularization constraint.



### Adversarial Structure Matching for Structured Prediction Tasks
- **Arxiv ID**: http://arxiv.org/abs/1805.07457v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.07457v2)
- **Published**: 2018-05-18 22:03:58+00:00
- **Updated**: 2019-10-21 16:46:07+00:00
- **Authors**: Jyh-Jing Hwang, Tsung-Wei Ke, Jianbo Shi, Stella X. Yu
- **Comment**: In CVPR 2019. Webpage & Code:
  https://jyhjinghwang.github.io/projects/asm.html
- **Journal**: None
- **Summary**: Pixel-wise losses, e.g., cross-entropy or L2, have been widely used in structured prediction tasks as a spatial extension of generic image classification or regression. However, its i.i.d. assumption neglects the structural regularity present in natural images. Various attempts have been made to incorporate structural reasoning mostly through structure priors in a cooperative way where co-occurring patterns are encouraged.   We, on the other hand, approach this problem from an opposing angle and propose a new framework, Adversarial Structure Matching (ASM), for training such structured prediction networks via an adversarial process, in which we train a structure analyzer that provides the supervisory signals, the ASM loss. The structure analyzer is trained to maximize the ASM loss, or to emphasize recurring multi-scale hard negative structural mistakes among co-occurring patterns. On the contrary, the structured prediction network is trained to reduce those mistakes and is thus enabled to distinguish fine-grained structures. As a result, training structured prediction networks using ASM reduces contextual confusion among objects and improves boundary localization. We demonstrate that our ASM outperforms pixel-wise IID loss or structural prior GAN loss on three different structured prediction tasks: semantic segmentation, monocular depth estimation, and surface normal prediction.



### Unsupervised Learning of Neural Networks to Explain Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1805.07468v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1805.07468v1)
- **Published**: 2018-05-18 23:02:14+00:00
- **Updated**: 2018-05-18 23:02:14+00:00
- **Authors**: Quanshi Zhang, Yu Yang, Yuchen Liu, Ying Nian Wu, Song-Chun Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents an unsupervised method to learn a neural network, namely an explainer, to interpret a pre-trained convolutional neural network (CNN), i.e., explaining knowledge representations hidden in middle conv-layers of the CNN. Given feature maps of a certain conv-layer of the CNN, the explainer performs like an auto-encoder, which first disentangles the feature maps into object-part features and then inverts object-part features back to features of higher conv-layers of the CNN. More specifically, the explainer contains interpretable conv-layers, where each filter disentangles the representation of a specific object part from chaotic input feature maps. As a paraphrase of CNN features, the disentangled representations of object parts help people understand the logic inside the CNN. We also learn the explainer to use object-part features to reconstruct features of higher CNN layers, in order to minimize loss of information during the feature disentanglement. More crucially, we learn the explainer via network distillation without using any annotations of sample labels, object parts, or textures for supervision. We have applied our method to different types of CNNs for evaluation, and explainers have significantly boosted the interpretability of CNN features.



### Progressive Ensemble Networks for Zero-Shot Recognition
- **Arxiv ID**: http://arxiv.org/abs/1805.07473v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.07473v2)
- **Published**: 2018-05-18 23:24:12+00:00
- **Updated**: 2019-04-06 22:07:42+00:00
- **Authors**: Meng Ye, Yuhong Guo
- **Comment**: CVPR19
- **Journal**: None
- **Summary**: Despite the advancement of supervised image recognition algorithms, their dependence on the availability of labeled data and the rapid expansion of image categories raise the significant challenge of zero-shot learning. Zero-shot learning (ZSL) aims to transfer knowledge from labeled classes into unlabeled classes to reduce human labeling effort. In this paper, we propose a novel progressive ensemble network model with multiple projected label embeddings to address zero-shot image recognition. The ensemble network is built by learning multiple image classification functions with a shared feature extraction network but different label embedding representations, which enhance the diversity of the classifiers and facilitate information transfer to unlabeled classes. A progressive training framework is then deployed to gradually label the most confident images in each unlabeled class with predicted pseudo-labels and update the ensemble network with the training data augmented by the pseudo-labels. The proposed model performs training on both labeled and unlabeled data. It can naturally bridge the domain shift problem in visual appearances and be extended to the generalized zero-shot learning scenario. We conduct experiments on multiple ZSL datasets and the empirical results demonstrate the efficacy of the proposed model.



### Norm-Preservation: Why Residual Networks Can Become Extremely Deep?
- **Arxiv ID**: http://arxiv.org/abs/1805.07477v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.07477v5)
- **Published**: 2018-05-18 23:37:17+00:00
- **Updated**: 2020-04-22 19:05:09+00:00
- **Authors**: Alireza Zaeemzadeh, Nazanin Rahnavard, Mubarak Shah
- **Comment**: None
- **Journal**: None
- **Summary**: Augmenting neural networks with skip connections, as introduced in the so-called ResNet architecture, surprised the community by enabling the training of networks of more than 1,000 layers with significant performance gains. This paper deciphers ResNet by analyzing the effect of skip connections, and puts forward new theoretical results on the advantages of identity skip connections in neural networks. We prove that the skip connections in the residual blocks facilitate preserving the norm of the gradient, and lead to stable back-propagation, which is desirable from optimization perspective. We also show that, perhaps surprisingly, as more residual blocks are stacked, the norm-preservation of the network is enhanced. Our theoretical arguments are supported by extensive empirical evidence. Can we push for extra norm-preservation? We answer this question by proposing an efficient method to regularize the singular values of the convolution operator and making the ResNet's transition layers extra norm-preserving. Our numerical investigations demonstrate that the learning dynamics and the classification performance of ResNet can be improved by making it even more norm preserving. Our results and the introduced modification for ResNet, referred to as Procrustes ResNets, can be used as a guide for training deeper networks and can also inspire new deeper architectures.



