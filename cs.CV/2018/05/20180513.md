# Arxiv Papers in cs.CV on 2018-05-13
### Enhanced Signal Recovery via Sparsity Inducing Image Priors
- **Arxiv ID**: http://arxiv.org/abs/1805.04828v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.04828v1)
- **Published**: 2018-05-13 06:13:47+00:00
- **Updated**: 2018-05-13 06:13:47+00:00
- **Authors**: Hojjat Seyed Mousavi
- **Comment**: PhD dissertation
- **Journal**: None
- **Summary**: Parsimony in signal representation is a topic of active research. Sparse signal processing and representation is the outcome of this line of research which has many applications in information processing and has shown significant improvement in real-world applications such as recovery, classification, clustering, super resolution, etc. This vast influence of sparse signal processing in real-world problems raises a significant need in developing novel sparse signal representation algorithms to obtain more robust systems. In such algorithms, a few open challenges remain in (a) efficiently posing sparsity on signals that can capture the structure of underlying signal and (b) the design of tractable algorithms that can recover signals under aforementioned sparse models.



### Covariance Pooling For Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/1805.04855v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.04855v1)
- **Published**: 2018-05-13 10:31:14+00:00
- **Updated**: 2018-05-13 10:31:14+00:00
- **Authors**: Dinesh Acharya, Zhiwu Huang, Danda Paudel, Luc Van Gool
- **Comment**: None
- **Journal**: None
- **Summary**: Classifying facial expressions into different categories requires capturing regional distortions of facial landmarks. We believe that second-order statistics such as covariance is better able to capture such distortions in regional facial fea- tures. In this work, we explore the benefits of using a man- ifold network structure for covariance pooling to improve facial expression recognition. In particular, we first employ such kind of manifold networks in conjunction with tradi- tional convolutional networks for spatial pooling within in- dividual image feature maps in an end-to-end deep learning manner. By doing so, we are able to achieve a recognition accuracy of 58.14% on the validation set of Static Facial Expressions in the Wild (SFEW 2.0) and 87.0% on the vali- dation set of Real-World Affective Faces (RAF) Database. Both of these results are the best results we are aware of. Besides, we leverage covariance pooling to capture the tem- poral evolution of per-frame features for video-based facial expression recognition. Our reported results demonstrate the advantage of pooling image-set features temporally by stacking the designed manifold network of covariance pool-ing on top of convolutional network layers.



### LMNet: Real-time Multiclass Object Detection on CPU using 3D LiDAR
- **Arxiv ID**: http://arxiv.org/abs/1805.04902v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.04902v2)
- **Published**: 2018-05-13 15:55:33+00:00
- **Updated**: 2018-05-18 15:41:47+00:00
- **Authors**: Kazuki Minemura, Hengfui Liau, Abraham Monrroy, Shinpei Kato
- **Comment**: None
- **Journal**: None
- **Summary**: This paper describes an optimized single-stage deep convolutional neural network to detect objects in urban environments, using nothing more than point cloud data. This feature enables our method to work regardless the time of the day and the lighting conditions.The proposed network structure employs dilated convolutions to gradually increase the perceptive field as depth increases, this helps to reduce the computation time by about 30%. The network input consists of five perspective representations of the unorganized point cloud data. The network outputs an objectness map and the bounding box offset values for each point. Our experiments showed that using reflection, range, and the position on each of the three axes helped to improve the location and orientation of the output bounding box. We carried out quantitative evaluations with the help of the KITTI dataset evaluation server. It achieved the fastest processing speed among the other contenders, making it suitable for real-time applications. We implemented and tested it on a real vehicle with a Velodyne HDL-64 mounted on top of it. We achieved execution times as fast as 50 FPS using desktop GPUs, and up to 10 FPS on a single Intel Core i5 CPU. The deploy implementation is open-sourced and it can be found as a feature branch inside the autonomous driving framework Autoware. Code is available at: https://github.com/CPFL/Autoware/tree/feature/cnn_lidar_detection



### A Tempt to Unify Heterogeneous Driving Databases using Traffic Primitives
- **Arxiv ID**: http://arxiv.org/abs/1805.04925v1
- **DOI**: None
- **Categories**: **cs.CV**, 62G08
- **Links**: [PDF](http://arxiv.org/pdf/1805.04925v1)
- **Published**: 2018-05-13 18:42:59+00:00
- **Updated**: 2018-05-13 18:42:59+00:00
- **Authors**: Jiacheng Zhu, Wenshuo Wang, Ding Zhao
- **Comment**: 6 pages, 7 figures, 1 table, ITSC 2018
- **Journal**: None
- **Summary**: A multitude of publicly-available driving datasets and data platforms have been raised for autonomous vehicles (AV). However, the heterogeneities of databases in size, structure and driving context make existing datasets practically ineffective due to a lack of uniform frameworks and searchable indexes. In order to overcome these limitations on existing public datasets, this paper proposes a data unification framework based on traffic primitives with ability to automatically unify and label heterogeneous traffic data. This is achieved by two steps: 1) Carefully arrange raw multidimensional time series driving data into a relational database and then 2) automatically extract labeled and indexed traffic primitives from traffic data through a Bayesian nonparametric learning method. Finally, we evaluate the effectiveness of our developed framework using the collected real vehicle data.



### Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/1805.09137v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1805.09137v1)
- **Published**: 2018-05-13 19:13:16+00:00
- **Updated**: 2018-05-13 19:13:16+00:00
- **Authors**: Vikram Mullachery, Vishal Motwani
- **Comment**: arXiv admin note: text overlap with arXiv:1609.06647 by other authors
- **Journal**: None
- **Summary**: This paper discusses and demonstrates the outcomes from our experimentation on Image Captioning. Image captioning is a much more involved task than image recognition or classification, because of the additional challenge of recognizing the interdependence between the objects/concepts in the image and the creation of a succinct sentential narration. Experiments on several labeled datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. As a toy application, we apply image captioning to create video captions, and we advance a few hypotheses on the challenges we encountered.



### DeLS-3D: Deep Localization and Segmentation with a 3D Semantic Map
- **Arxiv ID**: http://arxiv.org/abs/1805.04949v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.04949v1)
- **Published**: 2018-05-13 21:18:30+00:00
- **Updated**: 2018-05-13 21:18:30+00:00
- **Authors**: Peng Wang, Ruigang Yang, Binbin Cao, Wei Xu, Yuanqing Lin
- **Comment**: Accepted in CVPR 2018. arXiv admin note: substantial text overlap
  with arXiv:1803.06184
- **Journal**: None
- **Summary**: For applications such as autonomous driving, self-localization/camera pose estimation and scene parsing are crucial technologies. In this paper, we propose a unified framework to tackle these two problems simultaneously. The uniqueness of our design is a sensor fusion scheme which integrates camera videos, motion sensors (GPS/IMU), and a 3D semantic map in order to achieve robustness and efficiency of the system. Specifically, we first have an initial coarse camera pose obtained from consumer-grade GPS/IMU, based on which a label map can be rendered from the 3D semantic map. Then, the rendered label map and the RGB image are jointly fed into a pose CNN, yielding a corrected camera pose. In addition, to incorporate temporal information, a multi-layer recurrent neural network (RNN) is further deployed improve the pose accuracy. Finally, based on the pose from RNN, we render a new label map, which is fed together with the RGB image into a segment CNN which produces per-pixel semantic label. In order to validate our approach, we build a dataset with registered 3D point clouds and video camera images. Both the point clouds and the images are semantically-labeled. Each video frame has ground truth pose from highly accurate motion sensors. We show that practically, pose estimation solely relying on images like PoseNet may fail due to street view confusion, and it is important to fuse multiple sensors. Finally, various ablation studies are performed, which demonstrate the effectiveness of the proposed system. In particular, we show that scene parsing and pose estimation are mutually beneficial to achieve a more robust and accurate system.



### Learning Rich Features for Image Manipulation Detection
- **Arxiv ID**: http://arxiv.org/abs/1805.04953v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.04953v1)
- **Published**: 2018-05-13 21:29:38+00:00
- **Updated**: 2018-05-13 21:29:38+00:00
- **Authors**: Peng Zhou, Xintong Han, Vlad I. Morariu, Larry S. Davis
- **Comment**: CVPR 2018 Camera Ready
- **Journal**: None
- **Summary**: Image manipulation detection is different from traditional semantic object detection because it pays more attention to tampering artifacts than to image content, which suggests that richer features need to be learned. We propose a two-stream Faster R-CNN network and train it endto- end to detect the tampered regions given a manipulated image. One of the two streams is an RGB stream whose purpose is to extract features from the RGB image input to find tampering artifacts like strong contrast difference, unnatural tampered boundaries, and so on. The other is a noise stream that leverages the noise features extracted from a steganalysis rich model filter layer to discover the noise inconsistency between authentic and tampered regions. We then fuse features from the two streams through a bilinear pooling layer to further incorporate spatial co-occurrence of these two modalities. Experiments on four standard image manipulation datasets demonstrate that our two-stream framework outperforms each individual stream, and also achieves state-of-the-art performance compared to alternative methods with robustness to resizing and compression.



### Learning Temporal Strategic Relationships using Generative Adversarial Imitation Learning
- **Arxiv ID**: http://arxiv.org/abs/1805.04969v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.04969v1)
- **Published**: 2018-05-13 22:56:58+00:00
- **Updated**: 2018-05-13 22:56:58+00:00
- **Authors**: Tharindu Fernando, Simon Denman, Sridha Sridharan, Clinton Fookes
- **Comment**: International Foundation for Autonomous Agents and Multiagent
  Systems, 2018
- **Journal**: None
- **Summary**: This paper presents a novel framework for automatic learning of complex strategies in human decision making. The task that we are interested in is to better facilitate long term planning for complex, multi-step events. We observe temporal relationships at the subtask level of expert demonstrations, and determine the different strategies employed in order to successfully complete a task. To capture the relationship between the subtasks and the overall goal, we utilise two external memory modules, one for capturing dependencies within a single expert demonstration, such as the sequential relationship among different sub tasks, and a global memory module for modelling task level characteristics such as best practice employed by different humans based on their domain expertise. Furthermore, we demonstrate how the hidden state representation of the memory can be used as a reward signal to smooth the state transitions, eradicating subtle changes. We evaluate the effectiveness of the proposed model for an autonomous highway driving application, where we demonstrate its capability to learn different expert policies and outperform state-of-the-art methods. The scope in industrial applications extends to any robotics and automation application which requires learning from complex demonstrations containing series of subtasks.



