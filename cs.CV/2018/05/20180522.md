# Arxiv Papers in cs.CV on 2018-05-22
### Reducing Parameter Space for Neural Network Training
- **Arxiv ID**: http://arxiv.org/abs/1805.08340v3
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1805.08340v3)
- **Published**: 2018-05-22 01:08:40+00:00
- **Updated**: 2020-01-29 18:10:43+00:00
- **Authors**: Tong Qin, Ling Zhou, Dongbin Xiu
- **Comment**: 17 pages, 8 figures
- **Journal**: None
- **Summary**: For neural networks (NNs) with rectified linear unit (ReLU) or binary activation functions, we show that their training can be accomplished in a reduced parameter space. Specifically, the weights in each neuron can be trained on the unit sphere, as opposed to the entire space, and the threshold can be trained in a bounded interval, as opposed to the real line. We show that the NNs in the reduced parameter space are mathematically equivalent to the standard NNs with parameters in the whole space. The reduced parameter space shall facilitate the optimization procedure for the network training, as the search space becomes (much) smaller. We demonstrate the improved training performance using numerical examples.



### Learning Markov Clustering Networks for Scene Text Detection
- **Arxiv ID**: http://arxiv.org/abs/1805.08365v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.08365v1)
- **Published**: 2018-05-22 02:46:39+00:00
- **Updated**: 2018-05-22 02:46:39+00:00
- **Authors**: Zichuan Liu, Guosheng Lin, Sheng Yang, Jiashi Feng, Weisi Lin, Wang Ling Goh
- **Comment**: None
- **Journal**: None
- **Summary**: A novel framework named Markov Clustering Network (MCN) is proposed for fast and robust scene text detection. MCN predicts instance-level bounding boxes by firstly converting an image into a Stochastic Flow Graph (SFG) and then performing Markov Clustering on this graph. Our method can detect text objects with arbitrary size and orientation without prior knowledge of object size. The stochastic flow graph encode objects' local correlation and semantic information. An object is modeled as strongly connected nodes, which allows flexible bottom-up detection for scale-varying and rotated objects. MCN generates bounding boxes without using Non-Maximum Suppression, and it can be fully parallelized on GPUs. The evaluation on public benchmarks shows that our method outperforms the existing methods by a large margin in detecting multioriented text objects. MCN achieves new state-of-art performance on challenging MSRA-TD500 dataset with precision of 0.88, recall of 0.79 and F-score of 0.83. Also, MCN achieves realtime inference with frame rate of 34 FPS, which is $1.5\times$ speedup when compared with the fastest scene text detection algorithm.



### Joint Image Captioning and Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1805.08389v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1805.08389v1)
- **Published**: 2018-05-22 04:41:37+00:00
- **Updated**: 2018-05-22 04:41:37+00:00
- **Authors**: Jialin Wu, Zeyuan Hu, Raymond J. Mooney
- **Comment**: None
- **Journal**: None
- **Summary**: Answering visual questions need acquire daily common knowledge and model the semantic connection among different parts in images, which is too difficult for VQA systems to learn from images with the only supervision from answers. Meanwhile, image captioning systems with beam search strategy tend to generate similar captions and fail to diversely describe images. To address the aforementioned issues, we present a system to have these two tasks compensate with each other, which is capable of jointly producing image captions and answering visual questions. In particular, we utilize question and image features to generate question-related captions and use the generated captions as additional features to provide new knowledge to the VQA system. For image captioning, our system attains more informative results in term of the relative improvements on VQA tasks as well as competitive results using automated metrics. Applying our system to the VQA tasks, our results on VQA v2 dataset achieve 65.8% using generated captions and 69.1% using annotated captions in validation set and 68.4% in the test-standard set. Further, an ensemble of 10 models results in 69.7% in the test-standard split.



### Deep Learning with Cinematic Rendering: Fine-Tuning Deep Neural Networks Using Photorealistic Medical Images
- **Arxiv ID**: http://arxiv.org/abs/1805.08400v3
- **DOI**: 10.1088/1361-6560/aada93
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.08400v3)
- **Published**: 2018-05-22 05:24:41+00:00
- **Updated**: 2018-09-29 19:09:43+00:00
- **Authors**: Faisal Mahmood, Richard Chen, Sandra Sudarsky, Daphne Yu, Nicholas J. Durr
- **Comment**: 14 Pages, 4 Figures, 3 Tables, Physics in Medicine and Biology (2018)
- **Journal**: None
- **Summary**: Deep learning has emerged as a powerful artificial intelligence tool to interpret medical images for a growing variety of applications. However, the paucity of medical imaging data with high-quality annotations that is necessary for training such methods ultimately limits their performance. Medical data is challenging to acquire due to privacy issues, shortage of experts available for annotation, limited representation of rare conditions and cost. This problem has previously been addressed by using synthetically generated data. However, networks trained on synthetic data often fail to generalize to real data. Cinematic rendering simulates the propagation and interaction of light passing through tissue models reconstructed from CT data, enabling the generation of photorealistic images. In this paper, we present one of the first applications of cinematic rendering in deep learning, in which we propose to fine-tune synthetic data-driven networks using cinematically rendered CT data for the task of monocular depth estimation in endoscopy. Our experiments demonstrate that: (a) Convolutional Neural Networks (CNNs) trained on synthetic data and fine-tuned on photorealistic cinematically rendered data adapt better to real medical images and demonstrate more robust performance when compared to networks with no fine-tuning, (b) these fine-tuned networks require less training data to converge to an optimal solution, and (c) fine-tuning with data from a variety of photorealistic rendering conditions of the same scene prevents the network from learning patient-specific information and aids in generalizability of the model. Our empirical evaluation demonstrates that networks fine-tuned with cinematically rendered data predict depth with 56.87% less error for rendered endoscopy images and 27.49% less error for real porcine colon endoscopy images.



### Autofocus Layer for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1805.08403v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.08403v3)
- **Published**: 2018-05-22 05:35:20+00:00
- **Updated**: 2018-06-11 06:30:35+00:00
- **Authors**: Yao Qin, Konstantinos Kamnitsas, Siddharth Ancha, Jay Nanavati, Garrison Cottrell, Antonio Criminisi, Aditya Nori
- **Comment**: Published on MICCAI 2018
- **Journal**: None
- **Summary**: We propose the autofocus convolutional layer for semantic segmentation with the objective of enhancing the capabilities of neural networks for multi-scale processing. Autofocus layers adaptively change the size of the effective receptive field based on the processed context to generate more powerful features. This is achieved by parallelising multiple convolutional layers with different dilation rates, combined by an attention mechanism that learns to focus on the optimal scales driven by context. By sharing the weights of the parallel convolutions we make the network scale-invariant, with only a modest increase in the number of parameters. The proposed autofocus layer can be easily integrated into existing networks to improve a model's representational power. We evaluate our models on the challenging tasks of multi-organ segmentation in pelvic CT and brain tumor segmentation in MRI and achieve very promising performance.



### Training Convolutional Networks with Web Images
- **Arxiv ID**: http://arxiv.org/abs/1805.08416v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.08416v1)
- **Published**: 2018-05-22 06:11:00+00:00
- **Updated**: 2018-05-22 06:11:00+00:00
- **Authors**: Nizar Massouh
- **Comment**: None
- **Journal**: None
- **Summary**: In this thesis we investigate the effect of using web images to build a large scale database to be used along a deep learning method for a classification task. We replicate the ImageNet large scale database (ILSVRC-2012) from images collected from the web using 4 different download strategies varying: the search engine, the query and the image resolution. As a deep learning method, we will choose the Convolutional Neural Network that was very successful with recognition tasks; the AlexNet.



### Enriched Long-term Recurrent Convolutional Network for Facial Micro-Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/1805.08417v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.08417v1)
- **Published**: 2018-05-22 06:18:30+00:00
- **Updated**: 2018-05-22 06:18:30+00:00
- **Authors**: Huai-Qian Khor, John See, Raphael C. W. Phan, Weiyao Lin
- **Comment**: Published in Micro-Expression Grand Challenge 2018, Workshop of 13th
  IEEE Facial & Gesture 2018
- **Journal**: None
- **Summary**: Facial micro-expression (ME) recognition has posed a huge challenge to researchers for its subtlety in motion and limited databases. Recently, handcrafted techniques have achieved superior performance in micro-expression recognition but at the cost of domain specificity and cumbersome parametric tunings. In this paper, we propose an Enriched Long-term Recurrent Convolutional Network (ELRCN) that first encodes each micro-expression frame into a feature vector through CNN module(s), then predicts the micro-expression by passing the feature vector through a Long Short-term Memory (LSTM) module. The framework contains two different network variants: (1) Channel-wise stacking of input data for spatial enrichment, (2) Feature-wise stacking of features for temporal enrichment. We demonstrate that the proposed approach is able to achieve reasonably good performance, without data augmentation. In addition, we also present ablation studies conducted on the framework and visualizations of what CNN "sees" when predicting the micro-expression classes.



### Classification Uncertainty of Deep Neural Networks Based on Gradient Information
- **Arxiv ID**: http://arxiv.org/abs/1805.08440v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/1805.08440v2)
- **Published**: 2018-05-22 08:07:14+00:00
- **Updated**: 2018-07-26 10:37:11+00:00
- **Authors**: Philipp Oberdiek, Matthias Rottmann, Hanno Gottschalk
- **Comment**: None
- **Journal**: None
- **Summary**: We study the quantification of uncertainty of Convolutional Neural Networks (CNNs) based on gradient metrics. Unlike the classical softmax entropy, such metrics gather information from all layers of the CNN. We show for the EMNIST digits data set that for several such metrics we achieve the same meta classification accuracy -- i.e. the task of classifying predictions as correct or incorrect without knowing the actual label -- as for entropy thresholding. We apply meta classification to unknown concepts (out-of-distribution samples) -- EMNIST/Omniglot letters, CIFAR10 and noise -- and demonstrate that meta classification rates for unknown concepts can be increased when using entropy together with several gradient based metrics as input quantities for a meta classifier. Meta classifiers only trained on the uncertainty metrics of known concepts, i.e. EMNIST digits, usually do not perform equally well for all unknown concepts. If we however allow the meta classifier to be trained on uncertainty metrics for some out-of-distribution samples, meta classification for concepts remote from EMNIST digits (then termed known unknowns) can be improved considerably.



### Scene Coordinate and Correspondence Learning for Image-Based Localization
- **Arxiv ID**: http://arxiv.org/abs/1805.08443v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.08443v4)
- **Published**: 2018-05-22 08:12:03+00:00
- **Updated**: 2018-07-26 11:56:03+00:00
- **Authors**: Mai Bui, Shadi Albarqouni, Slobodan Ilic, Nassir Navab
- **Comment**: None
- **Journal**: None
- **Summary**: Scene coordinate regression has become an essential part of current camera re-localization methods. Different versions, such as regression forests and deep learning methods, have been successfully applied to estimate the corresponding camera pose given a single input image. In this work, we propose to regress the scene coordinates pixel-wise for a given RGB image by using deep learning. Compared to the recent methods, which usually employ RANSAC to obtain a robust pose estimate from the established point correspondences, we propose to regress confidences of these correspondences, which allows us to immediately discard erroneous predictions and improve the initial pose estimates. Finally, the resulting confidences can be used to score initial pose hypothesis and aid in pose refinement, offering a generalized solution to solve this task.



### The Topology ToolKit
- **Arxiv ID**: http://arxiv.org/abs/1805.09110v2
- **DOI**: 10.1109/TVCG.2017.2743938
- **Categories**: **cs.GR**, cs.CG, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1805.09110v2)
- **Published**: 2018-05-22 10:27:24+00:00
- **Updated**: 2019-07-16 22:53:53+00:00
- **Authors**: Julien Tierny, Guillaume Favelier, Joshua A. Levine, Charles Gueunet, Michael Michaux
- **Comment**: None
- **Journal**: IEEE Trans. Vis. Comput. Graph. 24(1) (2018) 832-842
- **Summary**: This system paper presents the Topology ToolKit (TTK), a software platform designed for topological data analysis in scientific visualization. TTK provides a unified, generic, efficient, and robust implementation of key algorithms for the topological analysis of scalar data, including: critical points, integral lines, persistence diagrams, persistence curves, merge trees, contour trees, Morse-Smale complexes, fiber surfaces, continuous scatterplots, Jacobi sets, Reeb spaces, and more. TTK is easily accessible to end users due to a tight integration with ParaView. It is also easily accessible to developers through a variety of bindings (Python, VTK/C++) for fast prototyping or through direct, dependence-free, C++, to ease integration into pre-existing complex systems. While developing TTK, we faced several algorithmic and software engineering challenges, which we document in this paper. In particular, we present an algorithm for the construction of a discrete gradient that complies to the critical points extracted in the piecewise-linear setting. This algorithm guarantees a combinatorial consistency across the topological abstractions supported by TTK, and importantly, a unified implementation of topological data simplification for multi-scale exploration and analysis. We also present a cached triangulation data structure, that supports time efficient and generic traversals, which self-adjusts its memory usage on demand for input simplicial meshes and which implicitly emulates a triangulation for regular grids with no memory overhead. Finally, we describe an original software architecture, which guarantees memory efficient and direct accesses to TTK features, while still allowing for researchers powerful and easy bindings and extensions. TTK is open source (BSD license) and its code, online documentation and video tutorials are available on TTK's website.



### Pose-Based Two-Stream Relational Networks for Action Recognition in Videos
- **Arxiv ID**: http://arxiv.org/abs/1805.08484v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.08484v1)
- **Published**: 2018-05-22 10:30:20+00:00
- **Updated**: 2018-05-22 10:30:20+00:00
- **Authors**: Wei Wang, Jinjin Zhang, Chenyang Si, Liang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, pose-based action recognition has gained more and more attention due to the better performance compared with traditional appearance-based methods. However, there still exist two problems to be further solved. First, existing pose-based methods generally recognize human actions with captured 3D human poses which are very difficult to obtain in real scenarios. Second, few pose-based methods model the action-related objects in recognizing human-object interaction actions in which objects play an important role. To solve the problems above, we propose a pose-based two-stream relational network (PSRN) for action recognition. In PSRN, one stream models the temporal dynamics of the targeted 2D human pose sequences which are directly extracted from raw videos, and the other stream models the action-related objects from a randomly sampled video frame. Most importantly, instead of fusing two-streams in the class score layer as before, we propose a pose-object relational network to model the relationship between human poses and action-related objects. We evaluate the proposed PSRN on two challenging benchmarks, i.e., Sub-JHMDB and PennAction. Experimental results show that our PSRN obtains the state-the-of-art performance on Sub-JHMDB (80.2%) and PennAction (98.1%). Our work opens a new door to action recognition by combining 2D human pose extracted from raw video and image appearance.



### Knowledge-based Fully Convolutional Network and Its Application in Segmentation of Lung CT Images
- **Arxiv ID**: http://arxiv.org/abs/1805.08492v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.08492v1)
- **Published**: 2018-05-22 10:43:17+00:00
- **Updated**: 2018-05-22 10:43:17+00:00
- **Authors**: Tao Yu, Yu Qiao, Huan Long
- **Comment**: 9 pages, 5 figures
- **Journal**: None
- **Summary**: A variety of deep neural networks have been applied in medical image segmentation and achieve good performance. Unlike natural images, medical images of the same imaging modality are characterized by the same pattern, which indicates that same normal organs or tissues locate at similar positions in the images. Thus, in this paper we try to incorporate the prior knowledge of medical images into the structure of neural networks such that the prior knowledge can be utilized for accurate segmentation. Based on this idea, we propose a novel deep network called knowledge-based fully convolutional network (KFCN) for medical image segmentation. The segmentation function and corresponding error is analyzed. We show the existence of an asymptotically stable region for KFCN which traditional FCN doesn't possess. Experiments validate our knowledge assumption about the incorporation of prior knowledge into the convolution kernels of KFCN and show that KFCN can achieve a reasonable segmentation and a satisfactory accuracy.



### Blind Predicting Similar Quality Map for Image Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/1805.08493v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.08493v2)
- **Published**: 2018-05-22 10:49:01+00:00
- **Updated**: 2019-03-10 07:09:16+00:00
- **Authors**: Da Pan, Ping Shi, Ming Hou, Zefeng Ying, Sizhe Fu, Yuan Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: A key problem in blind image quality assessment (BIQA) is how to effectively model the properties of human visual system in a data-driven manner. In this paper, we propose a simple and efficient BIQA model based on a novel framework which consists of a fully convolutional neural network (FCNN) and a pooling network to solve this problem. In principle, FCNN is capable of predicting a pixel-by-pixel similar quality map only from a distorted image by using the intermediate similarity maps derived from conventional full-reference image quality assessment methods. The predicted pixel-by-pixel quality maps have good consistency with the distortion correlations between the reference and distorted images. Finally, a deep pooling network regresses the quality map into a score. Experiments have demonstrated that our predictions outperform many state-of-the-art BIQA methods.



### Improved Person Detection on Omnidirectional Images with Non-maxima Suppression
- **Arxiv ID**: http://arxiv.org/abs/1805.08503v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.08503v4)
- **Published**: 2018-05-22 11:14:14+00:00
- **Updated**: 2019-03-25 09:34:10+00:00
- **Authors**: Roman Seidel, André Apitzsch, Gangolf Hirtz
- **Comment**: 8 pages, VISAPP 2019
- **Journal**: None
- **Summary**: We propose a person detector on omnidirectional images, an accurate method to generate minimal enclosing rectangles of persons. The basic idea is to adapt the qualitative detection performance of a convolutional neural network based method, namely YOLOv2 to fish-eye images. The design of our approach picks up the idea of a state-of-the-art object detector and highly overlapping areas of images with their regions of interests. This overlap reduces the number of false negatives. Based on the raw bounding boxes of the detector we fine-tuned overlapping bounding boxes by three approaches: non-maximum suppression, soft non-maximum suppression and soft non-maximum suppression with Gaussian smoothing. The evaluation was done on the PIROPO database and an own annotated Flat dataset, supplemented with bounding boxes on omnidirectional images. We achieve an average precision of 64.4 % with YOLOv2 for the class person on PIROPO and 77.6 % on Flat. For this purpose we fine-tuned the soft non-maximum suppression with Gaussian smoothing.



### Part-based Tracking by Sampling
- **Arxiv ID**: http://arxiv.org/abs/1805.08511v2
- **DOI**: None
- **Categories**: **cs.CV**, I.4.8; I.4.6; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/1805.08511v2)
- **Published**: 2018-05-22 11:38:04+00:00
- **Updated**: 2019-10-10 10:14:48+00:00
- **Authors**: George De Ath, Richard M. Everson
- **Comment**: Submitted to IEEE Winter Conference on Applications of Computer
  Vision 2020 (WACV 2020)
- **Journal**: None
- **Summary**: We propose a novel part-based method for tracking an arbitrary object in challenging video sequences. The colour distribution of tracked image patches on the target object are represented by pairs of RGB samples and counts of how many pixels in the patch are similar to them. Patches are placed by segmenting the object in the given bounding box and placing patches in homogeneous regions of the object. These are located in subsequent image frames by applying non-shearing affine transformations to the patches' previous locations, locally optimising the best of these, and evaluating their quality using a modified Bhattacharyya distance. In experiments carried out on VOT2018 and OTB100 benchmarks, the tracker achieves higher performance than all other part-based trackers. An ablation study is used to reveal the effectiveness of each tracking component, with largest performance gains found when using the patch placement scheme.



### Fast Motion Deblurring for Feature Detection and Matching Using Inertial Measurements
- **Arxiv ID**: http://arxiv.org/abs/1805.08542v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.08542v1)
- **Published**: 2018-05-22 12:35:47+00:00
- **Updated**: 2018-05-22 12:35:47+00:00
- **Authors**: Janne Mustaniemi, Juho Kannala, Simo Särkkä, Jiri Matas, Janne Heikkilä
- **Comment**: None
- **Journal**: None
- **Summary**: Many computer vision and image processing applications rely on local features. It is well-known that motion blur decreases the performance of traditional feature detectors and descriptors. We propose an inertial-based deblurring method for improving the robustness of existing feature detectors and descriptors against the motion blur. Unlike most deblurring algorithms, the method can handle spatially-variant blur and rolling shutter distortion. Furthermore, it is capable of running in real-time contrary to state-of-the-art algorithms. The limitations of inertial-based blur estimation are taken into account by validating the blur estimates using image data. The evaluation shows that when the method is used with traditional feature detector and descriptor, it increases the number of detected keypoints, provides higher repeatability and improves the localization accuracy. We also demonstrate that such features will lead to more accurate and complete reconstructions when used in the application of 3D visual reconstruction.



### A Recurrent Convolutional Neural Network Approach for Sensorless Force Estimation in Robotic Surgery
- **Arxiv ID**: http://arxiv.org/abs/1805.08545v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1805.08545v1)
- **Published**: 2018-05-22 12:39:24+00:00
- **Updated**: 2018-05-22 12:39:24+00:00
- **Authors**: Arturo Marban, Vignesh Srinivasan, Wojciech Samek, Josep Fernández, Alicia Casals
- **Comment**: None
- **Journal**: None
- **Summary**: Providing force feedback as relevant information in current Robot-Assisted Minimally Invasive Surgery systems constitutes a technological challenge due to the constraints imposed by the surgical environment. In this context, Sensorless Force Estimation techniques represent a potential solution, enabling to sense the interaction forces between the surgical instruments and soft-tissues. Specifically, if visual feedback is available for observing soft-tissues' deformation, this feedback can be used to estimate the forces applied to these tissues. To this end, a force estimation model, based on Convolutional Neural Networks and Long-Short Term Memory networks, is proposed in this work. This model is designed to process both, the spatiotemporal information present in video sequences and the temporal structure of tool data (the surgical tool-tip trajectory and its grasping status). A series of analyses are carried out to reveal the advantages of the proposal and the challenges that remain for real applications. This research work focuses on two surgical task scenarios, referred to as pushing and pulling tissue. For these two scenarios, different input data modalities and their effect on the force estimation quality are investigated. These input data modalities are tool data, video sequences and a combination of both. The results suggest that the force estimation quality is better when both, the tool data and video sequences, are processed by the neural network model. Moreover, this study reveals the need for a loss function, designed to promote the modeling of smooth and sharp details found in force signals. Finally, the results show that the modeling of forces due to pulling tasks is more challenging than for the simplest pushing actions.



### A 2D laser rangefinder scans dataset of standard EUR pallets
- **Arxiv ID**: http://arxiv.org/abs/1805.08564v2
- **DOI**: 10.1016/j.dib.2019.103837
- **Categories**: **cs.RO**, cs.CV, 68T40
- **Links**: [PDF](http://arxiv.org/pdf/1805.08564v2)
- **Published**: 2018-05-22 13:16:54+00:00
- **Updated**: 2019-03-13 21:48:59+00:00
- **Authors**: Ihab S. Mohamed, Alessio Capitanelli, Fulvio Mastrogiovanni, Stefano Rovetta, Renato Zaccaria
- **Comment**: This paper has been accepted to be published in "Data in Brief
  (DiB)". 10 pages, 4 figures, and 2 tables
- **Journal**: None
- **Summary**: In the past few years, the technology of automated guided vehicles (AGVs) has notably advanced. In particular, in the context of factory and warehouse automation, different approaches have been presented for detecting and localizing pallets inside warehouses and shop-floor environments. In a related research paper [1], we show that an AGVs can detect, localize, and track pallets using machine learning techniques based only on the data of an on-board 2D laser rangefinder. Such sensor is very common in industrial scenarios due to its simplicity and robustness, but it can only provide a limited amount of data. Therefore, it has been neglected in the past in favor of more complex solutions. In this paper, we release to the community the data we collected in [1] for further research activities in the field of pallet localization and tracking. The dataset comprises a collection of 565 2D scans from real-world environments, which are divided into 340 samples where pallets are present, and 225 samples where they are not. The data have been manually labelled and are provided in different formats.



### Less is More: Surgical Phase Recognition with Less Annotations through Self-Supervised Pre-training of CNN-LSTM Networks
- **Arxiv ID**: http://arxiv.org/abs/1805.08569v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.08569v1)
- **Published**: 2018-05-22 13:28:59+00:00
- **Updated**: 2018-05-22 13:28:59+00:00
- **Authors**: Gaurav Yengera, Didier Mutter, Jacques Marescaux, Nicolas Padoy
- **Comment**: 15 pages, 22 figures
- **Journal**: None
- **Summary**: Real-time algorithms for automatically recognizing surgical phases are needed to develop systems that can provide assistance to surgeons, enable better management of operating room (OR) resources and consequently improve safety within the OR. State-of-the-art surgical phase recognition algorithms using laparoscopic videos are based on fully supervised training. This limits their potential for widespread application, since creation of manual annotations is an expensive process considering the numerous types of existing surgeries and the vast amount of laparoscopic videos available. In this work, we propose a new self-supervised pre-training approach based on the prediction of remaining surgery duration (RSD) from laparoscopic videos. The RSD prediction task is used to pre-train a convolutional neural network (CNN) and long short-term memory (LSTM) network in an end-to-end manner. Our proposed approach utilizes all available data and reduces the reliance on annotated data, thereby facilitating the scaling up of surgical phase recognition algorithms to different kinds of surgeries. Additionally, we present EndoN2N, an end-to-end trained CNN-LSTM model for surgical phase recognition and evaluate the performance of our approach on a dataset of 120 Cholecystectomy laparoscopic videos (Cholec120). This work also presents the first systematic study of self-supervised pre-training approaches to understand the amount of annotations required for surgical phase recognition. Interestingly, the proposed RSD pre-training approach leads to performance improvement even when all the training data is manually annotated and outperforms the single pre-training approach for surgical phase recognition presently published in the literature. It is also observed that end-to-end training of CNN-LSTM networks boosts surgical phase recognition performance.



### Deep Feature Aggregation and Image Re-ranking with Heat Diffusion for Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1805.08587v5
- **DOI**: None
- **Categories**: **cs.IR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1805.08587v5)
- **Published**: 2018-05-22 14:06:28+00:00
- **Updated**: 2018-10-09 02:30:27+00:00
- **Authors**: Shanmin Pang, Jin Ma, Jianru Xue, Jihua Zhu, Vicente Ordonez
- **Comment**: The paper has been accepted to IEEE Transactions on Multimedia
- **Journal**: None
- **Summary**: Image retrieval based on deep convolutional features has demonstrated state-of-the-art performance in popular benchmarks. In this paper, we present a unified solution to address deep convolutional feature aggregation and image re-ranking by simulating the dynamics of heat diffusion. A distinctive problem in image retrieval is that repetitive or \emph{bursty} features tend to dominate final image representations, resulting in representations less distinguishable. We show that by considering each deep feature as a heat source, our unsupervised aggregation method is able to avoid over-representation of \emph{bursty} features. We additionally provide a practical solution for the proposed aggregation method and further show the efficiency of our method in experimental evaluation. Inspired by the aforementioned deep feature aggregation method, we also propose a method to re-rank a number of top ranked images for a given query image by considering the query as the heat source. Finally, we extensively evaluate the proposed approach with pre-trained and fine-tuned deep networks on common public benchmarks and show superior performance compared to previous work.



### Deep Learning Inference on Embedded Devices: Fixed-Point vs Posit
- **Arxiv ID**: http://arxiv.org/abs/1805.08624v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.08624v1)
- **Published**: 2018-05-22 14:31:27+00:00
- **Updated**: 2018-05-22 14:31:27+00:00
- **Authors**: Seyed H. F. Langroudi, Tej Pandit, Dhireesha Kudithipudi
- **Comment**: None
- **Journal**: None
- **Summary**: Performing the inference step of deep learning in resource constrained environments, such as embedded devices, is challenging. Success requires optimization at both software and hardware levels. Low precision arithmetic and specifically low precision fixed-point number systems have become the standard for performing deep learning inference. However, representing non-uniform data and distributed parameters (e.g. weights) by using uniformly distributed fixed-point values is still a major drawback when using this number system. Recently, the posit number system was proposed, which represents numbers in a non-uniform manner. Therefore, in this paper we are motivated to explore using the posit number system to represent the weights of Deep Convolutional Neural Networks. However, we do not apply any quantization techniques and hence the network weights do not require re-training. The results of this exploration show that using the posit number system outperformed the fixed point number system in terms of accuracy and memory utilization.



### Constructing Compact Brain Connectomes for Individual Fingerprinting
- **Arxiv ID**: http://arxiv.org/abs/1805.08649v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.08649v2)
- **Published**: 2018-05-22 14:58:52+00:00
- **Updated**: 2019-08-08 19:56:36+00:00
- **Authors**: Vikram Ravindra, Petros Drineas, Ananth Grama
- **Comment**: None
- **Journal**: None
- **Summary**: Recent neuroimaging studies have shown that functional connectomes are unique to individuals, i.e., two distinct fMRIs taken over different sessions of the same subject are more similar in terms of their connectomes than those from two different subjects. In this study, we present significant new results that identify, for the first time, specific parts of resting-state and task-specific connectomes that code the unique signatures. We show that a very small part of the connectome codes the signatures. A network of these features is shown to achieve excellent training and test accuracy in matching imaging datasets. We show that these features are statistically significant, robust to perturbations, invariant across populations, and are localized to a small number of structural regions of the brain. Furthermore, we show that for task-specific connectomes, the regions identified by our method are consistent with their known functional characterization. We present a new matrix sampling technique to derive computationally efficient and accurate methods for identifying the discriminating sub-connectome and support all of our claims using state-of-the-art statistical tests and computational techniques.



### LMKL-Net: A Fast Localized Multiple Kernel Learning Solver via Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1805.08656v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1805.08656v1)
- **Published**: 2018-05-22 15:12:38+00:00
- **Updated**: 2018-05-22 15:12:38+00:00
- **Authors**: Ziming Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we propose solving localized multiple kernel learning (LMKL) using LMKL-Net, a feedforward deep neural network. In contrast to previous works, as a learning principle we propose {\em parameterizing} both the gating function for learning kernel combination weights and the multiclass classifier in LMKL using an attentional network (AN) and a multilayer perceptron (MLP), respectively. In this way we can learn the (nonlinear) decision function in LMKL (approximately) by sequential applications of AN and MLP. Empirically on benchmark datasets we demonstrate that overall LMKL-Net can not only outperform the state-of-the-art MKL solvers in terms of accuracy, but also be trained about {\em two orders of magnitude} faster with much smaller memory footprint for large-scale learning.



### Robust Conditional Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1805.08657v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.08657v2)
- **Published**: 2018-05-22 15:14:11+00:00
- **Updated**: 2019-03-13 07:59:35+00:00
- **Authors**: Grigorios G. Chrysos, Jean Kossaifi, Stefanos Zafeiriou
- **Comment**: To appear in ICLR 2019
- **Journal**: None
- **Summary**: Conditional generative adversarial networks (cGAN) have led to large improvements in the task of conditional image generation, which lies at the heart of computer vision. The major focus so far has been on performance improvement, while there has been little effort in making cGAN more robust to noise. The regression (of the generator) might lead to arbitrarily large errors in the output, which makes cGAN unreliable for real-world applications. In this work, we introduce a novel conditional GAN model, called RoCGAN, which leverages structure in the target space of the model to address the issue. Our model augments the generator with an unsupervised pathway, which promotes the outputs of the generator to span the target manifold even in the presence of intense noise. We prove that RoCGAN share similar theoretical properties as GAN and experimentally verify that our model outperforms existing state-of-the-art cGAN architectures by a large margin in a variety of domains including images from natural scenes and faces.



### COCO-CN for Cross-Lingual Image Tagging, Captioning and Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1805.08661v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1805.08661v2)
- **Published**: 2018-05-22 15:26:15+00:00
- **Updated**: 2019-01-15 00:24:08+00:00
- **Authors**: Xirong Li, Chaoxi Xu, Xiaoxu Wang, Weiyu Lan, Zhengxiong Jia, Gang Yang, Jieping Xu
- **Comment**: accepted for publication as a regular paper in the IEEE Transactions
  on Multimedia
- **Journal**: None
- **Summary**: This paper contributes to cross-lingual image annotation and retrieval in terms of data and baseline methods. We propose COCO-CN, a novel dataset enriching MS-COCO with manually written Chinese sentences and tags. For more effective annotation acquisition, we develop a recommendation-assisted collective annotation system, automatically providing an annotator with several tags and sentences deemed to be relevant with respect to the pictorial content. Having 20,342 images annotated with 27,218 Chinese sentences and 70,993 tags, COCO-CN is currently the largest Chinese-English dataset that provides a unified and challenging platform for cross-lingual image tagging, captioning and retrieval. We develop conceptually simple yet effective methods per task for learning from cross-lingual resources. Extensive experiments on the three tasks justify the viability of the proposed dataset and methods. Data and code are publicly available at https://github.com/li-xirong/coco-cn



### Convexity Shape Prior for Level Set based Image Segmentation Method
- **Arxiv ID**: http://arxiv.org/abs/1805.08676v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.08676v1)
- **Published**: 2018-05-22 15:47:41+00:00
- **Updated**: 2018-05-22 15:47:41+00:00
- **Authors**: Shi Yan, Xue-cheng Tai, Jun Liu, Hai-yang Huang
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a geometric convexity shape prior preservation method for variational level set based image segmentation methods. Our method is built upon the fact that the level set of a convex signed distanced function must be convex. This property enables us to transfer a complicated geometrical convexity prior into a simple inequality constraint on the function. An active set based Gauss-Seidel iteration is used to handle this constrained minimization problem to get an efficient algorithm. We apply our method to region and edge based level set segmentation models including Chan-Vese (CV) model with guarantee that the segmented region will be convex. Experimental results show the effectiveness and quality of the proposed model and algorithm.



### Aesthetics Assessment of Images Containing Faces
- **Arxiv ID**: http://arxiv.org/abs/1805.08685v1
- **DOI**: 10.1109/ICIP.2018.8451368
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.08685v1)
- **Published**: 2018-05-22 16:00:16+00:00
- **Updated**: 2018-05-22 16:00:16+00:00
- **Authors**: Simone Bianco, Luigi Celona, Raimondo Schettini
- **Comment**: Accepted by ICIP 2018
- **Journal**: None
- **Summary**: Recent research has widely explored the problem of aesthetics assessment of images with generic content. However, few approaches have been specifically designed to predict the aesthetic quality of images containing human faces, which make up a massive portion of photos in the web. This paper introduces a method for aesthetic quality assessment of images with faces. We exploit three different Convolutional Neural Networks to encode information regarding perceptual quality, global image aesthetics, and facial attributes; then, a model is trained to combine these features to explicitly predict the aesthetics of images containing faces. Experimental results show that our approach outperforms existing methods for both binary, i.e. low/high, and continuous aesthetic score prediction on four different databases in the state-of-the-art.



### Self-supervised Multi-view Person Association and Its Applications
- **Arxiv ID**: http://arxiv.org/abs/1805.08717v3
- **DOI**: 10.1109/TPAMI.2020.2974726
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.08717v3)
- **Published**: 2018-05-22 16:25:26+00:00
- **Updated**: 2020-04-18 06:16:40+00:00
- **Authors**: Minh Vo, Ersin Yumer, Kalyan Sunkavalli, Sunil Hadap, Yaser Sheikh, Srinivasa Narasimhan
- **Comment**: Accepted to IEEE TPAMI
- **Journal**: None
- **Summary**: Reliable markerless motion tracking of people participating in a complex group activity from multiple moving cameras is challenging due to frequent occlusions, strong viewpoint and appearance variations, and asynchronous video streams. To solve this problem, reliable association of the same person across distant viewpoints and temporal instances is essential. We present a self-supervised framework to adapt a generic person appearance descriptor to the unlabeled videos by exploiting motion tracking, mutual exclusion constraints, and multi-view geometry. The adapted discriminative descriptor is used in a tracking-by-clustering formulation. We validate the effectiveness of our descriptor learning on WILDTRACK [14] and three new complex social scenes captured by multiple cameras with up to 60 people "in the wild". We report significant improvement in association accuracy (up to 18%) and stable and coherent 3D human skeleton tracking (5 to 10 times) over the baseline. Using the reconstructed 3D skeletons, we cut the input videos into a multi-angle video where the image of a specified person is shown from the best visible front-facing camera. Our algorithm detects inter-human occlusion to determine the camera switching moment while still maintaining the flow of the action well.



### CascadeCNN: Pushing the performance limits of quantisation
- **Arxiv ID**: http://arxiv.org/abs/1805.08743v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1805.08743v1)
- **Published**: 2018-05-22 17:06:02+00:00
- **Updated**: 2018-05-22 17:06:02+00:00
- **Authors**: Alexandros Kouris, Stylianos I. Venieris, Christos-Savvas Bouganis
- **Comment**: Accepted at SysML Conference 2018
- **Journal**: None
- **Summary**: This work presents CascadeCNN, an automated toolflow that pushes the quantisation limits of any given CNN model, to perform high-throughput inference by exploiting the computation time-accuracy trade-off. Without the need for retraining, a two-stage architecture tailored for any given FPGA device is generated, consisting of a low- and a high-precision unit. A confidence evaluation unit is employed between them to identify misclassified cases at run time and forward them to the high-precision unit or terminate computation. Experiments demonstrate that CascadeCNN achieves a performance boost of up to 55% for VGG-16 and 48% for AlexNet over the baseline design for the same resource budget and accuracy.



### Clinical Parameters Prediction for Gait Disorder Recognition
- **Arxiv ID**: http://arxiv.org/abs/1806.04627v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1806.04627v1)
- **Published**: 2018-05-22 17:29:27+00:00
- **Updated**: 2018-05-22 17:29:27+00:00
- **Authors**: Soheil Esmaeilzadeh, Ouassim Khebzegga, Mehrad Moradshahi
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: Being able to predict clinical parameters in order to diagnose gait disorders in a patient is of great value in planning treatments. It is known that \textit{decision parameters} such as cadence, step length, and walking speed are critical in the diagnosis of gait disorders in patients. This project aims to predict the decision parameters using two ways and afterwards giving advice on whether a patient needs treatment or not. In one way, we use clinically measured parameters such as Ankle Dorsiflexion, age, walking speed, step length, stride length, weight over height squared (BMI) and etc. to predict the decision parameters. In a second way, we use videos recorded from patient's walking tests in a clinic in order to extract the coordinates of the joints of the patient over time and predict the decision parameters. Finally, having the decision parameters we pre-classify gait disorder intensity of a patient and as the result make decisions on whether a patient needs treatment or not.



### A Convolutional Feature Map based Deep Network targeted towards Traffic Detection and Classification
- **Arxiv ID**: http://arxiv.org/abs/1805.08769v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.08769v1)
- **Published**: 2018-05-22 17:56:02+00:00
- **Updated**: 2018-05-22 17:56:02+00:00
- **Authors**: Baljit Kaur, Jhilik Bhattacharya
- **Comment**: 28 pages
- **Journal**: None
- **Summary**: This research mainly emphasizes on traffic detection thus essentially involving object detection and classification. The particular work discussed here is motivated from unsatisfactory attempts of re-using well known pre-trained object detection networks for domain specific data. In this course, some trivial issues leading to prominent performance drop are identified and ways to resolve them are discussed. For example, some simple yet relevant tricks regarding data collection and sampling prove to be very beneficial. Also, introducing a blur net to deal with blurred real time data is another important factor promoting performance elevation. We further study the neural network design issues for beneficial object classification and involve shared, region-independent convolutional features. Adaptive learning rates to deal with saddle points are also investigated and an average covariance matrix based pre-conditioned approach is proposed. We also introduce the use of optical flow features to accommodate orientation information. Experimental results demonstrate that this results in a steady rise in the performance rate.



### A scene perception system for visually impaired based on object detection and classification using multi-modal DCNN
- **Arxiv ID**: http://arxiv.org/abs/1805.08798v1
- **DOI**: 10.1117/1.JEI.28.1.013031
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.08798v1)
- **Published**: 2018-05-22 18:02:08+00:00
- **Updated**: 2018-05-22 18:02:08+00:00
- **Authors**: Baljit Kaur, Jhilik Bhattacharya
- **Comment**: 33pages
- **Journal**: None
- **Summary**: This paper represents a cost-effective scene perception system aimed towards visually impaired individual. We use an odroid system integrated with an USB camera and USB laser that can be attached on the chest. The system classifies the detected objects along with its distance from the user and provides a voice output. Experimental results provided in this paper use outdoor traffic scenes. The object detection and classification framework exploits a multi-modal fusion based faster RCNN using motion, sharpening and blurring filters for efficient feature representation.



### Multi-View Graph Convolutional Network and Its Applications on Neuroimage Analysis for Parkinson's Disease
- **Arxiv ID**: http://arxiv.org/abs/1805.08801v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.08801v4)
- **Published**: 2018-05-22 18:11:04+00:00
- **Updated**: 2019-05-07 03:55:16+00:00
- **Authors**: Xi Sheryl Zhang, Lifang He, Kun Chen, Yuan Luo, Jiayu Zhou, Fei Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Parkinson's Disease (PD) is one of the most prevalent neurodegenerative diseases that affects tens of millions of Americans. PD is highly progressive and heterogeneous. Quite a few studies have been conducted in recent years on predictive or disease progression modeling of PD using clinical and biomarkers data. Neuroimaging, as another important information source for neurodegenerative disease, has also arisen considerable interests from the PD community. In this paper, we propose a deep learning method based on Graph Convolutional Networks (GCN) for fusing multiple modalities of brain images in relationship prediction which is useful for distinguishing PD cases from controls. On Parkinson's Progression Markers Initiative (PPMI) cohort, our approach achieved $0.9537\pm 0.0587$ AUC, compared with $0.6443\pm 0.0223$ AUC achieved by traditional approaches such as PCA.



### Resource Aware Person Re-identification across Multiple Resolutions
- **Arxiv ID**: http://arxiv.org/abs/1805.08805v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.08805v3)
- **Published**: 2018-05-22 18:17:37+00:00
- **Updated**: 2018-10-02 00:06:00+00:00
- **Authors**: Yan Wang, Lequn Wang, Yurong You, Xu Zou, Vincent Chen, Serena Li, Gao Huang, Bharath Hariharan, Kilian Q. Weinberger
- **Comment**: 8 pages, 8 figures, CVPR 2018
- **Journal**: None
- **Summary**: Not all people are equally easy to identify: color statistics might be enough for some cases while others might require careful reasoning about high- and low-level details. However, prevailing person re-identification(re-ID) methods use one-size-fits-all high-level embeddings from deep convolutional networks for all cases. This might limit their accuracy on difficult examples or makes them needlessly expensive for the easy ones. To remedy this, we present a new person re-ID model that combines effective embeddings built on multiple convolutional network layers, trained with deep-supervision. On traditional re-ID benchmarks, our method improves substantially over the previous state-of-the-art results on all five datasets that we evaluate on. We then propose two new formulations of the person re-ID problem under resource-constraints, and show how our model can be used to effectively trade off accuracy and computation in the presence of resource constraints. Code and pre-trained models are available at https://github.com/mileyan/DARENet.



### Deformable Part Networks
- **Arxiv ID**: http://arxiv.org/abs/1805.08808v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1805.08808v1)
- **Published**: 2018-05-22 18:35:28+00:00
- **Updated**: 2018-05-22 18:35:28+00:00
- **Authors**: Ziming Zhang, Rongmei Lin, Alan Sullivan
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we propose novel Deformable Part Networks (DPNs) to learn {\em pose-invariant} representations for 2D object recognition. In contrast to the state-of-the-art pose-aware networks such as CapsNet \cite{sabour2017dynamic} and STN \cite{jaderberg2015spatial}, DPNs can be naturally {\em interpreted} as an efficient solver for a challenging detection problem, namely Localized Deformable Part Models (LDPMs) where localization is introduced to DPMs as another latent variable for searching for the best poses of objects over all pixels and (predefined) scales. In particular we construct DPNs as sequences of such LDPM units to model the semantic and spatial relations among the deformable parts as hierarchical composition and spatial parsing trees. Empirically our 17-layer DPN can outperform both CapsNets and STNs significantly on affNIST \cite{sabour2017dynamic}, for instance, by 19.19\% and 12.75\%, respectively, with better generalization and better tolerance to affine transformations.



### Learning what and where to attend
- **Arxiv ID**: http://arxiv.org/abs/1805.08819v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.08819v4)
- **Published**: 2018-05-22 19:12:47+00:00
- **Updated**: 2019-06-11 14:14:34+00:00
- **Authors**: Drew Linsley, Dan Shiebler, Sven Eberhardt, Thomas Serre
- **Comment**: Previously called Global-and-local attention networks for visual
  recognition. Current version published in ICLR 2019:
  https://openreview.net/forum?id=BJgLg3R9KQ
- **Journal**: None
- **Summary**: Most recent gains in visual recognition have originated from the inclusion of attention mechanisms in deep convolutional networks (DCNs). Because these networks are optimized for object recognition, they learn where to attend using only a weak form of supervision derived from image class labels. Here, we demonstrate the benefit of using stronger supervisory signals by teaching DCNs to attend to image regions that humans deem important for object recognition. We first describe a large-scale online experiment (ClickMe) used to supplement ImageNet with nearly half a million human-derived "top-down" attention maps. Using human psychophysics, we confirm that the identified top-down features from ClickMe are more diagnostic than "bottom-up" saliency features for rapid image categorization. As a proof of concept, we extend a state-of-the-art attention network and demonstrate that adding ClickMe supervision significantly improves its accuracy and yields visual features that are more interpretable and more similar to those used by human observers.



### Rapid seismic domain transfer: Seismic velocity inversion and modeling using deep generative neural networks
- **Arxiv ID**: http://arxiv.org/abs/1805.08826v1
- **DOI**: 10.3997/2214-4609.201800734
- **Categories**: **physics.geo-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1805.08826v1)
- **Published**: 2018-05-22 19:32:56+00:00
- **Updated**: 2018-05-22 19:32:56+00:00
- **Authors**: Lukas Mosser, Wouter Kimman, Jesper Dramsch, Steve Purves, Alfredo De la Fuente, Graham Ganssle
- **Comment**: Extended abstract submitted to EAGE 2018, 5 pages, 3 figures
- **Journal**: None
- **Summary**: Traditional physics-based approaches to infer sub-surface properties such as full-waveform inversion or reflectivity inversion are time-consuming and computationally expensive. We present a deep-learning technique that eliminates the need for these computationally complex methods by posing the problem as one of domain transfer. Our solution is based on a deep convolutional generative adversarial network and dramatically reduces computation time. Training based on two different types of synthetic data produced a neural network that generates realistic velocity models when applied to a real dataset. The system's ability to generalize means it is robust against the inherent occurrence of velocity errors and artifacts in both training and test datasets.



### Distribution Matching Losses Can Hallucinate Features in Medical Image Translation
- **Arxiv ID**: http://arxiv.org/abs/1805.08841v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1805.08841v3)
- **Published**: 2018-05-22 20:06:33+00:00
- **Updated**: 2018-10-03 05:44:12+00:00
- **Authors**: Joseph Paul Cohen, Margaux Luck, Sina Honari
- **Comment**: Published at Medical Image Computing & Computer Assisted Intervention
  (MICCAI 2018). An abstract is published at the Medical Imaging with Deep
  Learning Conference (MIDL 2018) as "How to Cure Cancer (in images) with
  Unpaired Image Translation"
- **Journal**: Medical Image Computing & Computer Assisted Intervention (MICCAI
  2018 Oral)
- **Summary**: This paper discusses how distribution matching losses, such as those used in CycleGAN, when used to synthesize medical images can lead to mis-diagnosis of medical conditions. It seems appealing to use these new image synthesis methods for translating images from a source to a target domain because they can produce high quality images and some even do not require paired data. However, the basis of how these image translation models work is through matching the translation output to the distribution of the target domain. This can cause an issue when the data provided in the target domain has an over or under representation of some classes (e.g. healthy or sick). When the output of an algorithm is a transformed image there are uncertainties whether all known and unknown class labels have been preserved or changed. Therefore, we recommend that these translated images should not be used for direct interpretation (e.g. by doctors) because they may lead to misdiagnosis of patients based on hallucinated image features by an algorithm that matches a distribution. However there are many recent papers that seem as though this is the goal.



### Unsupervised Domain Adaptation using Regularized Hyper-graph Matching
- **Arxiv ID**: http://arxiv.org/abs/1805.08874v2
- **DOI**: 10.1109/ICIP.2018.8451152
- **Categories**: **cs.CV**, cs.AI, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.08874v2)
- **Published**: 2018-05-22 21:38:38+00:00
- **Updated**: 2018-11-30 17:18:53+00:00
- **Authors**: Debasmit Das, C. S. George Lee
- **Comment**: Final version appeared in IEEE International Conference on Image
  Processing 2018
- **Journal**: None
- **Summary**: Domain adaptation (DA) addresses the real-world image classification problem of discrepancy between training (source) and testing (target) data distributions. We propose an unsupervised DA method that considers the presence of only unlabelled data in the target domain. Our approach centers on finding matches between samples of the source and target domains. The matches are obtained by treating the source and target domains as hyper-graphs and carrying out a class-regularized hyper-graph matching using first-, second- and third-order similarities between the graphs. We have also developed a computationally efficient algorithm by initially selecting a subset of the samples to construct a graph and then developing a customized optimization routine for graph-matching based on Conditional Gradient and Alternating Direction Multiplier Method. This allows the proposed method to be used widely. We also performed a set of experiments on standard object recognition datasets to validate the effectiveness of our framework over state-of-the-art approaches.



### Teacher's Perception in the Classroom
- **Arxiv ID**: http://arxiv.org/abs/1805.08897v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.08897v1)
- **Published**: 2018-05-22 22:48:47+00:00
- **Updated**: 2018-05-22 22:48:47+00:00
- **Authors**: Ömer Sümer, Patricia Goldberg, Kathleen Stürmer, Tina Seidel, Peter Gerjets, Ulrich Trautwein, Enkelejda Kasneci
- **Comment**: Accepted by CVPRW 2018. The Second Workshop on Computational Models
  Learning Systems and Educational Assessment (CMLA)
- **Journal**: None
- **Summary**: The ability for a teacher to engage all students in active learning processes in classroom constitutes a crucial prerequisite for enhancing students achievement. Teachers' attentional processes provide important insights into teachers' ability to focus their attention on relevant information in the complexity of classroom interaction and distribute their attention across students in order to recognize the relevant needs for learning. In this context, mobile eye tracking is an innovative approach within teaching effectiveness research to capture teachers' attentional processes while teaching. However, analyzing mobile eye-tracking data by hand is time consuming and still limited. In this paper, we introduce a new approach to enhance the impact of mobile eye tracking by connecting it with computer vision. In mobile eye tracking videos from an educational study using a standardized small group situation, we apply a state-ofthe-art face detector, create face tracklets, and introduce a novel method to cluster faces into the number of identity. Subsequently, teachers' attentional focus is calculated per student during a teaching unit by associating eye tracking fixations and face tracklets. To the best of our knowledge, this is the first work to combine computer vision and mobile eye tracking to model teachers' attention while instructing.



