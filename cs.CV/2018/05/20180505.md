# Arxiv Papers in cs.CV on 2018-05-05
### Fast-converging Conditional Generative Adversarial Networks for Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1805.01972v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.01972v1)
- **Published**: 2018-05-05 00:18:19+00:00
- **Updated**: 2018-05-05 00:18:19+00:00
- **Authors**: Chengcheng Li, Zi Wang, Hairong Qi
- **Comment**: Accepted by ICIP 2018
- **Journal**: None
- **Summary**: Building on top of the success of generative adversarial networks (GANs), conditional GANs attempt to better direct the data generation process by conditioning with certain additional information. Inspired by the most recent AC-GAN, in this paper we propose a fast-converging conditional GAN (FC-GAN). In addition to the real/fake classifier used in vanilla GANs, our discriminator has an advanced auxiliary classifier which distinguishes each real class from an extra `fake' class. The `fake' class avoids mixing generated data with real data, which can potentially confuse the classification of real data as AC-GAN does, and makes the advanced auxiliary classifier behave as another real/fake classifier. As a result, FC-GAN can accelerate the process of differentiation of all classes, thus boost the convergence speed. Experimental results on image synthesis demonstrate our model is competitive in the quality of images generated while achieving a faster convergence rate.



### Unsupervised Feature Learning via Non-Parametric Instance-level Discrimination
- **Arxiv ID**: http://arxiv.org/abs/1805.01978v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1805.01978v1)
- **Published**: 2018-05-05 00:47:01+00:00
- **Updated**: 2018-05-05 00:47:01+00:00
- **Authors**: Zhirong Wu, Yuanjun Xiong, Stella Yu, Dahua Lin
- **Comment**: CVPR 2018 spotlight paper. Code:
  https://github.com/zhirongw/lemniscate.pytorch
- **Journal**: None
- **Summary**: Neural net classifiers trained on data with annotated class labels can also capture apparent visual similarity among categories without being directed to do so. We study whether this observation can be extended beyond the conventional domain of supervised learning: Can we learn a good feature representation that captures apparent similarity among instances, instead of classes, by merely asking the feature to be discriminative of individual instances? We formulate this intuition as a non-parametric classification problem at the instance-level, and use noise-contrastive estimation to tackle the computational challenges imposed by the large number of instance classes. Our experimental results demonstrate that, under unsupervised learning settings, our method surpasses the state-of-the-art on ImageNet classification by a large margin. Our method is also remarkable for consistently improving test performance with more training data and better network architectures. By fine-tuning the learned feature, we further obtain competitive results for semi-supervised learning and object detection tasks. Our non-parametric model is highly compact: With 128 features per image, our method requires only 600MB storage for a million images, enabling fast nearest neighbour retrieval at the run time.



### Position Estimation of Camera Based on Unsupervised Learning
- **Arxiv ID**: http://arxiv.org/abs/1805.02020v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.02020v1)
- **Published**: 2018-05-05 08:35:23+00:00
- **Updated**: 2018-05-05 08:35:23+00:00
- **Authors**: YanTong Wu, Yang Liu
- **Comment**: 6 pages,5 figures,1 table
- **Journal**: None
- **Summary**: It is an exciting task to recover the scene's 3d-structure and camera pose from the video sequence. Most of the current solutions divide it into two parts, monocular depth recovery and camera pose estimation. The monocular depth recovery is often studied as an independent part, and a better depth estimation is used to solve the pose. While camera pose is still estimated by traditional SLAM (Simultaneous Localization And Mapping) methods in most cases. The use of unsupervised method for monocular depth recovery and pose estimation has benefited from the study of [1] and achieved good results. In this paper, we improve the method of [1]. Our emphasis is laid on the improvement of the idea and related theory, introducing a more reasonable inter frame constraints and finally synthesize the camera trajectory with inter frame pose estimation in the unified world coordinate system. And our results get better performance.



### Weakly-supervised Visual Instrument-playing Action Detection in Videos
- **Arxiv ID**: http://arxiv.org/abs/1805.02031v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1805.02031v1)
- **Published**: 2018-05-05 09:52:36+00:00
- **Updated**: 2018-05-05 09:52:36+00:00
- **Authors**: Jen-Yu Liu, Yi-Hsuan Yang, Shyh-Kang Jeng
- **Comment**: None
- **Journal**: None
- **Summary**: Instrument playing is among the most common scenes in music-related videos, which represent nowadays one of the largest sources of online videos. In order to understand the instrument-playing scenes in the videos, it is important to know what instruments are played, when they are played, and where the playing actions occur in the scene. While audio-based recognition of instruments has been widely studied, the visual aspect of the music instrument playing remains largely unaddressed in the literature. One of the main obstacles is the difficulty in collecting annotated data of the action locations for training-based methods. To address this issue, we propose a weakly-supervised framework to find when and where the instruments are played in the videos. We propose to use two auxiliary models, a sound model and an object model, to provide supervisions for training the instrument-playing action model. The sound model provides temporal supervisions, while the object model provides spatial supervisions. They together can simultaneously provide temporal and spatial supervisions. The resulted model only needs to analyze the visual part of a music video to deduce which, when and where instruments are played. We found that the proposed method significantly improves the localization accuracy. We evaluate the result of the proposed method temporally and spatially on a small dataset (totally 5,400 frames) that we manually annotated.



### Bone marrow cells detection: A technique for the microscopic image analysis
- **Arxiv ID**: http://arxiv.org/abs/1805.02058v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.02058v1)
- **Published**: 2018-05-05 13:56:03+00:00
- **Updated**: 2018-05-05 13:56:03+00:00
- **Authors**: Haichao Cao, Hong Liu, Enmin Song
- **Comment**: 11 pages, 8 figures and 4 tables
- **Journal**: None
- **Summary**: In the detection of myeloproliferative, the number of cells in each type of bone marrow cells (BMC) is an important parameter for the evaluation. In this study, we propose a new counting method, which also consists of three modules including localization, segmentation and classification. The localization of BMC is achieved from a color transformation enhanced BMC sample image and stepwise averaging method (SAM). In the nucleus segmentation, both SAM and Otsu's method will be applied to obtain a weighted threshold for segmenting the patch into nucleus and non-nucleus. In the cytoplasm segmentation, a color weakening transformation, an improved region growing method and the K-Means algorithm are used. The connected cells with BMC will be separated by the marker-controlled watershed algorithm. The features will be extracted for the classification after the segmentation. In this study, the BMC are classified using the SVM, Random Forest, Artificial Neural Networks, Adaboost and Bayesian Networks into five classes including one outlier, namely, neutrophilic split granulocyte, neutrophilic stab granulocyte, metarubricyte, mature lymphocytes and the outlier (all other cells not listed). Our experimental results show that the best average recognition rate is 87.49% for the SVM.



### Learning Selfie-Friendly Abstraction from Artistic Style Images
- **Arxiv ID**: http://arxiv.org/abs/1805.02085v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.02085v2)
- **Published**: 2018-05-05 17:06:30+00:00
- **Updated**: 2018-05-21 18:01:12+00:00
- **Authors**: Yicun Liu, Jimmy Ren, Jianbo Liu, Jiawei Zhang, Xiaohao Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Artistic style transfer can be thought as a process to generate different versions of abstraction of the original image. However, most of the artistic style transfer operators are not optimized for human faces thus mainly suffers from two undesirable features when applying them to selfies. First, the edges of human faces may unpleasantly deviate from the ones in the original image. Second, the skin color is far from faithful to the original one which is usually problematic in producing quality selfies. In this paper, we take a different approach and formulate this abstraction process as a gradient domain learning problem. We aim to learn a type of abstraction which not only achieves the specified artistic style but also circumvents the two aforementioned drawbacks thus highly applicable to selfie photography. We also show that our method can be directly generalized to videos with high inter-frame consistency. Our method is also robust to non-selfie images, and the generalization to various kinds of real-life scenes is discussed. We will make our code publicly available.



### RiFCN: Recurrent Network in Fully Convolutional Network for Semantic Segmentation of High Resolution Remote Sensing Images
- **Arxiv ID**: http://arxiv.org/abs/1805.02091v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.02091v1)
- **Published**: 2018-05-05 18:00:43+00:00
- **Updated**: 2018-05-05 18:00:43+00:00
- **Authors**: Lichao Mou, Xiao Xiang Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic segmentation in high resolution remote sensing images is a fundamental and challenging task. Convolutional neural networks (CNNs), such as fully convolutional network (FCN) and SegNet, have shown outstanding performance in many segmentation tasks. One key pillar of these successes is mining useful information from features in convolutional layers for producing high resolution segmentation maps. For example, FCN nonlinearly combines high-level features extracted from last convolutional layers; whereas SegNet utilizes a deconvolutional network which takes as input only coarse, high-level feature maps of the last convolutional layer. However, how to better fuse multi-level convolutional feature maps for semantic segmentation of remote sensing images is underexplored. In this work, we propose a novel bidirectional network called recurrent network in fully convolutional network (RiFCN), which is end-to-end trainable. It has a forward stream and a backward stream. The former is a classification CNN architecture for feature extraction, which takes an input image and produces multi-level convolutional feature maps from shallow to deep; while in the later, to achieve accurate boundary inference and semantic segmentation, boundary-aware high resolution feature maps in shallower layers and high-level but low-resolution features are recursively embedded into the learning framework (from deep to shallow) to generate a fused feature representation that draws a holistic picture of not only high-level semantic information but also low-level fine-grained details. Experimental results on two widely-used high resolution remote sensing data sets for semantic segmentation tasks, ISPRS Potsdam and Inria Aerial Image Labeling Data Set, demonstrate competitive performance obtained by the proposed methodology compared to other studied approaches.



### Revisiting Temporal Modeling for Video-based Person ReID
- **Arxiv ID**: http://arxiv.org/abs/1805.02104v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.02104v2)
- **Published**: 2018-05-05 18:56:38+00:00
- **Updated**: 2018-05-08 02:59:16+00:00
- **Authors**: Jiyang Gao, Ram Nevatia
- **Comment**: codes available at https://github.com/jiyanggao/Video-Person-ReID
- **Journal**: None
- **Summary**: Video-based person reID is an important task, which has received much attention in recent years due to the increasing demand in surveillance and camera networks. A typical video-based person reID system consists of three parts: an image-level feature extractor (e.g. CNN), a temporal modeling method to aggregate temporal features and a loss function. Although many methods on temporal modeling have been proposed, it is hard to directly compare these methods, because the choice of feature extractor and loss function also have a large impact on the final performance. We comprehensively study and compare four different temporal modeling methods (temporal pooling, temporal attention, RNN and 3D convnets) for video-based person reID. We also propose a new attention generation network which adopts temporal convolution to extract temporal information among frames. The evaluation is done on the MARS dataset, and our methods outperform state-of-the-art methods by a large margin. Our source codes are released at https://github.com/jiyanggao/Video-Person-ReID.



### Estimation and Tracking of AP-diameter of the Inferior Vena Cava in Ultrasound Images Using a Novel Active Circle Algorithm
- **Arxiv ID**: http://arxiv.org/abs/1805.02125v3
- **DOI**: 10.1016/j.compbiomed.2018.05.001
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.02125v3)
- **Published**: 2018-05-05 23:47:24+00:00
- **Updated**: 2018-08-08 18:41:43+00:00
- **Authors**: Ebrahim Karami, Mohamed Shehata, Andrew Smith
- **Comment**: Published in Computers in Biology and Medicine
- **Journal**: In Computers in Biology and Medicine, vol. 98, pp. 16-25, July
  2018
- **Summary**: Medical research suggests that the anterior-posterior (AP)-diameter of the inferior vena cava (IVC) and its associated temporal variation as imaged by bedside ultrasound is useful in guiding fluid resuscitation of the critically-ill patient. Unfortunately, indistinct edges and gaps in vessel walls are frequently present which impede accurate estimation of the IVC AP-diameter for both human operators and segmentation algorithms. The majority of research involving use of the IVC to guide fluid resuscitation involves manual measurement of the maximum and minimum AP-diameter as it varies over time. This effort proposes using a time-varying circle fitted inside the typically ellipsoid IVC as an efficient, consistent and novel approach to tracking and approximating the AP-diameter even in the context of poor image quality. In this active-circle algorithm, a novel evolution functional is proposed and shown to be a useful tool for ultrasound image processing. The proposed algorithm is compared with an expert manual measurement, and state-of-the-art relevant algorithms. It is shown that the algorithm outperforms other techniques and performs very close to manual measurement.



