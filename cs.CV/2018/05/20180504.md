# Arxiv Papers in cs.CV on 2018-05-04
### Highly Efficient 8-bit Low Precision Inference of Convolutional Neural Networks with IntelCaffe
- **Arxiv ID**: http://arxiv.org/abs/1805.08691v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.08691v1)
- **Published**: 2018-05-04 07:58:33+00:00
- **Updated**: 2018-05-04 07:58:33+00:00
- **Authors**: Jiong Gong, Haihao Shen, Guoming Zhang, Xiaoli Liu, Shane Li, Ge Jin, Niharika Maheshwari, Evarist Fomenko, Eden Segal
- **Comment**: 1st Reproducible Tournament on Pareto-efficient Image Classification,
  co-held with ASPLOS 2018
- **Journal**: None
- **Summary**: High throughput and low latency inference of deep neural networks are critical for the deployment of deep learning applications. This paper presents the efficient inference techniques of IntelCaffe, the first Intel optimized deep learning framework that supports efficient 8-bit low precision inference and model optimization techniques of convolutional neural networks on Intel Xeon Scalable Processors. The 8-bit optimized model is automatically generated with a calibration process from FP32 model without the need of fine-tuning or retraining. We show that the inference throughput and latency with ResNet-50, Inception-v3 and SSD are improved by 1.38X-2.9X and 1.35X-3X respectively with neglectable accuracy loss from IntelCaffe FP32 baseline and by 56X-75X and 26X-37X from BVLC Caffe. All these techniques have been open-sourced on IntelCaffe GitHub1, and the artifact is provided to reproduce the result on Amazon AWS Cloud.



### Transferring GANs: generating images from limited data
- **Arxiv ID**: http://arxiv.org/abs/1805.01677v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.01677v2)
- **Published**: 2018-05-04 09:23:20+00:00
- **Updated**: 2018-10-02 09:55:10+00:00
- **Authors**: Yaxing Wang, Chenshen Wu, Luis Herranz, Joost van de Weijer, Abel Gonzalez-Garcia, Bogdan Raducanu
- **Comment**: ECCV2018-camera ready
- **Journal**: None
- **Summary**: Transferring the knowledge of pretrained networks to new domains by means of finetuning is a widely used practice for applications based on discriminative models. To the best of our knowledge this practice has not been studied within the context of generative deep networks. Therefore, we study domain adaptation applied to image generation with generative adversarial networks. We evaluate several aspects of domain adaptation, including the impact of target domain size, the relative distance between source and target domain, and the initialization of conditional GANs. Our results show that using knowledge from pretrained networks can shorten the convergence time and can significantly improve the quality of the generated images, especially when the target data is limited. We show that these conclusions can also be drawn for conditional GANs even when the pretrained model was trained without conditioning. Our results also suggest that density may be more important than diversity and a dataset with one or few densely sampled classes may be a better source model than more diverse datasets such as ImageNet or Places.



### Feature extraction with regularized siamese networks for outlier detection: application to lesion screening in medical imaging
- **Arxiv ID**: http://arxiv.org/abs/1805.01717v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.01717v1)
- **Published**: 2018-05-04 11:25:02+00:00
- **Updated**: 2018-05-04 11:25:02+00:00
- **Authors**: Z. Alaverdyan, C. Lartizien
- **Comment**: Accepted to Conf\'erence sur l'Apprentissage automatique (CAp) 2017
- **Journal**: None
- **Summary**: Computer aided diagnosis (CAD) systems are designed to assist clinicians in various tasks, including highlighting abnormal regions in a medical image. A common approach consists in training a voxel-level binary classifier on a set of feature vectors extracted from normal and pathological areas in patients' scans. However, many pathologies (such as epilepsy) are characterized by lesions that may be located anywhere in the brain, have various shapes, sizes and texture. An adequate representation of such a heterogeneity requires a significant amount of annotated data which is a major issue in the medical domain. Therefore, we built on a previously proposed approach that considers epilepsy lesion detection task as a voxel-level outlier detection problem. It consists in building a oc-SVM classifier for each voxel in the brain volume using a small number of clinically-guided features El Azami et al., 2016. Our goal in this study is to make a step forward by replacing the handcrafted features with automatically learnt representations using neural networks. We propose a novel version of siamese networks trained on patches extracted from healthy patients' scans only. This network, composed of stacked autoencoders as subnetworks, is regularized by the reconstruction error of the patches. It is designed to learn representations that bring patches centered at the same voxel localization 'closer' with respect to the chosen metric (i.e. cosine). Finally, the middle layer representations of the subnetworks are fed to oc-SVM classifiers at voxel-level. The method is validated on 3 patients' MRI scans with confirmed epilepsy lesions and shows a promising performance.



### Unsupervised learning for concept detection in medical images: a comparative analysis
- **Arxiv ID**: http://arxiv.org/abs/1805.01803v1
- **DOI**: 10.3390/app8081213
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.01803v1)
- **Published**: 2018-05-04 14:36:09+00:00
- **Updated**: 2018-05-04 14:36:09+00:00
- **Authors**: Eduardo Pinho, Carlos Costa
- **Comment**: None
- **Journal**: None
- **Summary**: As digital medical imaging becomes more prevalent and archives increase in size, representation learning exposes an interesting opportunity for enhanced medical decision support systems. On the other hand, medical imaging data is often scarce and short on annotations. In this paper, we present an assessment of unsupervised feature learning approaches for images in the biomedical literature, which can be applied to automatic biomedical concept detection. Six unsupervised representation learning methods were built, including traditional bags of visual words, autoencoders, and generative adversarial networks. Each model was trained, and their respective feature space evaluated using images from the ImageCLEF 2017 concept detection task. We conclude that it is possible to obtain more powerful representations with modern deep learning approaches, in contrast with previously popular computer vision methods. Although generative adversarial networks can provide good results, they are harder to succeed in highly varied data sets. The possibility of semi-supervised learning, as well as their use in medical information retrieval problems, are the next steps to be strongly considered.



### Failure Prediction for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1805.01811v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.01811v1)
- **Published**: 2018-05-04 14:56:00+00:00
- **Updated**: 2018-05-04 14:56:00+00:00
- **Authors**: Simon Hecker, Dengxin Dai, Luc Van Gool
- **Comment**: published in IEEE Intelligent Vehicle Symposium 2018
- **Journal**: None
- **Summary**: The primary focus of autonomous driving research is to improve driving accuracy. While great progress has been made, state-of-the-art algorithms still fail at times. Such failures may have catastrophic consequences. It therefore is important that automated cars foresee problems ahead as early as possible. This is also of paramount importance if the driver will be asked to take over. We conjecture that failures do not occur randomly. For instance, driving models may fail more likely at places with heavy traffic, at complex intersections, and/or under adverse weather/illumination conditions. This work presents a method to learn to predict the occurrence of these failures, i.e. to assess how difficult a scene is to a given driving model and to possibly give the human driver an early headsup. A camera-based driving model is developed and trained over real driving datasets. The discrepancies between the model's predictions and the human `ground-truth' maneuvers were then recorded, to yield the `failure' scores. Experimental results show that the failure score can indeed be learned and predicted. Thus, our prediction method is able to improve the overall safety of an automated driving model by alerting the human driver timely, leading to better human-vehicle collaborative driving.



### Assessing a mobile-based deep learning model for plant disease surveillance
- **Arxiv ID**: http://arxiv.org/abs/1805.08692v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/1805.08692v1)
- **Published**: 2018-05-04 15:07:29+00:00
- **Updated**: 2018-05-04 15:07:29+00:00
- **Authors**: Amanda Ramcharan, Peter McCloskey, Kelsee Baranowski, Neema Mbilinyi, Latifa Mrisho, Mathias Ndalahwa, James Legg, David Hughes
- **Comment**: 12 pages, 3 figures, 3 tables
- **Journal**: None
- **Summary**: Convolutional neural network models (CNNs) have made major advances in computer vision tasks in the last five years. Given the challenge in collecting real world datasets, most studies report performance metrics based on available research datasets. In scenarios where CNNs are to be deployed on images or videos from mobile devices, models are presented with new challenges due to lighting, angle, and camera specifications, which are not accounted for in research datasets. It is essential for assessment to also be conducted on real world datasets if such models are to be reliably integrated with products and services in society. Plant disease datasets can be used to test CNNs in real time and gain insight into real world performance. We train a CNN object detection model to identify foliar symptoms of diseases (or lack thereof) in cassava (Manihot esculenta Crantz). We then deploy the model on a mobile app and test its performance on mobile images and video of 720 diseased leaflets in an agricultural field in Tanzania. Within each disease category we test two levels of severity of symptoms - mild and pronounced, to assess the model performance for early detection of symptoms. In both severities we see a decrease in the F-1 score for real world images and video. The F-1 score dropped by 32% for pronounced symptoms in real world images (the closest data to the training data) due to a drop in model recall. If the potential of smartphone CNNs are to be realized our data suggest it is crucial to consider tuning precision and recall performance in order to achieve the desired performance in real world settings. In addition, the varied performance related to different input data (image or video) is an important consideration for the design of CNNs in real world applications.



### Object and Text-guided Semantics for CNN-based Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/1805.01818v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.01818v1)
- **Published**: 2018-05-04 15:09:48+00:00
- **Updated**: 2018-05-04 15:09:48+00:00
- **Authors**: Sungmin Eum, Christopher Reale, Heesung Kwon, Claire Bonial, Clare Voss
- **Comment**: Submitted to ICIP 2018
- **Journal**: None
- **Summary**: Many previous methods have demonstrated the importance of considering semantically relevant objects for carrying out video-based human activity recognition, yet none of the methods have harvested the power of large text corpora to relate the objects and the activities to be transferred into learning a unified deep convolutional neural network. We present a novel activity recognition CNN which co-learns the object recognition task in an end-to-end multitask learning scheme to improve upon the baseline activity recognition performance. We further improve upon the multitask learning approach by exploiting a text-guided semantic space to select the most relevant objects with respect to the target activities. To the best of our knowledge, we are the first to investigate this approach.



### Towards a Spectrum of Graph Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1805.01837v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.01837v1)
- **Published**: 2018-05-04 16:13:36+00:00
- **Updated**: 2018-05-04 16:13:36+00:00
- **Authors**: Mathias Niepert, Alberto Garcia-Duran
- **Comment**: None
- **Journal**: None
- **Summary**: We present our ongoing work on understanding the limitations of graph convolutional networks (GCNs) as well as our work on generalizations of graph convolutions for representing more complex node attribute dependencies. Based on an analysis of GCNs with the help of the corresponding computation graphs, we propose a generalization of existing GCNs where the aggregation operations are (a) determined by structural properties of the local neighborhood graphs and (b) not restricted to weighted averages. We show that the proposed approach is strictly more expressive while requiring only a modest increase in the number of parameters and computations. We also show that the proposed generalization is identical to standard convolutional layers when applied to regular grid graphs.



### High throughput quantitative metallography for complex microstructures using deep learning: A case study in ultrahigh carbon steel
- **Arxiv ID**: http://arxiv.org/abs/1805.08693v2
- **DOI**: 10.1017/S1431927618015635
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.08693v2)
- **Published**: 2018-05-04 17:22:34+00:00
- **Updated**: 2019-02-04 19:29:32+00:00
- **Authors**: Brian L. DeCost, Bo Lei, Toby Francis, Elizabeth A. Holm
- **Comment**: Updated with minor revisions reflecting the review process at
  Microscopy and Microanalysis. Full supplementary materials will be available
  at https://holmgroup.github.io/publications/
- **Journal**: None
- **Summary**: We apply a deep convolutional neural network segmentation model to enable novel automated microstructure segmentation applications for complex microstructures typically evaluated manually and subjectively. We explore two microstructure segmentation tasks in an openly-available ultrahigh carbon steel microstructure dataset: segmenting cementite particles in the spheroidized matrix, and segmenting larger fields of view featuring grain boundary carbide, spheroidized particle matrix, particle-free grain boundary denuded zone, and Widmanst\"atten cementite. We also demonstrate how to combine these data-driven microstructure segmentation models to obtain empirical cementite particle size and denuded zone width distributions from more complex micrographs containing multiple microconstituents. The full annotated dataset is available on materialsdata.nist.gov (https://materialsdata.nist.gov/handle/11256/964).



### Automatic Estimation of Modulation Transfer Functions
- **Arxiv ID**: http://arxiv.org/abs/1805.01872v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.01872v1)
- **Published**: 2018-05-04 17:36:41+00:00
- **Updated**: 2018-05-04 17:36:41+00:00
- **Authors**: Matthias Bauer, Valentin Volchkov, Michael Hirsch, Bernhard Schölkopf
- **Comment**: None
- **Journal**: None
- **Summary**: The modulation transfer function (MTF) is widely used to characterise the performance of optical systems. Measuring it is costly and it is thus rarely available for a given lens specimen. Instead, MTFs based on simulations or, at best, MTFs measured on other specimens of the same lens are used. Fortunately, images recorded through an optical system contain ample information about its MTF, only that it is confounded with the statistics of the images. This work presents a method to estimate the MTF of camera lens systems directly from photographs, without the need for expensive equipment. We use a custom grid display to accurately measure the point response of lenses to acquire ground truth training data. We then use the same lenses to record natural images and employ a data-driven supervised learning approach using a convolutional neural network to estimate the MTF on small image patches, aggregating the information into MTF charts over the entire field of view. It generalises to unseen lenses and can be applied for single photographs, with the performance improving if multiple photographs are available.



### Analyzing Covariate Influence on Gender and Race Prediction from Near-Infrared Ocular Images
- **Arxiv ID**: http://arxiv.org/abs/1805.01912v4
- **DOI**: 10.1109/ACCESS.2018.2886275
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.01912v4)
- **Published**: 2018-05-04 18:42:50+00:00
- **Updated**: 2019-01-17 22:10:51+00:00
- **Authors**: Denton Bobeldyk, Arun Ross
- **Comment**: 15 pages, 20 tables, 11 figures
- **Journal**: None
- **Summary**: Recent research has explored the possibility of automatically deducing information such as gender, age and race of an individual from their biometric data. While the face modality has been extensively studied in this regard, the iris modality less so. In this paper, we first review the medical literature to establish a biological basis for extracting gender and race cues from the iris. Then, we demonstrate that it is possible to use simple texture descriptors, like BSIF (Binarized Statistical Image Feature) and LBP (Local Binary Patterns), to extract gender and race attributes from an NIR ocular image used in a typical iris recognition system. The proposed method predicts gender and race from a single eye image with an accuracy of 86% and 90%, respectively. In addition, the following analysis are conducted: (a) the role of different parts of the ocular region on attribute prediction; (b) the influence of gender on race prediction, and vice-versa; (c) the impact of eye color on gender and race prediction; (d) the impact of image blur on gender and race prediction; (e) the generalizability of the method across different datasets; and (f) the consistency of prediction performance across the left and right eyes.



### Learning to See in the Dark
- **Arxiv ID**: http://arxiv.org/abs/1805.01934v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1805.01934v1)
- **Published**: 2018-05-04 21:03:12+00:00
- **Updated**: 2018-05-04 21:03:12+00:00
- **Authors**: Chen Chen, Qifeng Chen, Jia Xu, Vladlen Koltun
- **Comment**: Published at the Conference on Computer Vision and Pattern
  Recognition (CVPR 2018)
- **Journal**: None
- **Summary**: Imaging in low light is challenging due to low photon count and low SNR. Short-exposure images suffer from noise, while long exposure can induce blur and is often impractical. A variety of denoising, deblurring, and enhancement techniques have been proposed, but their effectiveness is limited in extreme conditions, such as video-rate imaging at night. To support the development of learning-based pipelines for low-light image processing, we introduce a dataset of raw short-exposure low-light images, with corresponding long-exposure reference images. Using the presented dataset, we develop a pipeline for processing low-light images, based on end-to-end training of a fully-convolutional network. The network operates directly on raw sensor data and replaces much of the traditional image processing pipeline, which tends to perform poorly on such data. We report promising results on the new dataset, analyze factors that affect performance, and highlight opportunities for future work. The results are shown in the supplementary video at https://youtu.be/qWKUFK7MWvg



### Reliability Map Estimation For CNN-Based Camera Model Attribution
- **Arxiv ID**: http://arxiv.org/abs/1805.01946v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.01946v1)
- **Published**: 2018-05-04 22:07:20+00:00
- **Updated**: 2018-05-04 22:07:20+00:00
- **Authors**: David Güera, Sri Kalyan Yarlagadda, Paolo Bestagini, Fengqing Zhu, Stefano Tubaro, Edward J. Delp
- **Comment**: Presented at the IEEE Winter Conference on Applications of Computer
  Vision (WACV18)
- **Journal**: None
- **Summary**: Among the image forensic issues investigated in the last few years, great attention has been devoted to blind camera model attribution. This refers to the problem of detecting which camera model has been used to acquire an image by only exploiting pixel information. Solving this problem has great impact on image integrity assessment as well as on authenticity verification. Recent advancements that use convolutional neural networks (CNNs) in the media forensic field have enabled camera model attribution methods to work well even on small image patches. These improvements are also important for determining forgery localization. Some patches of an image may not contain enough information related to the camera model (e.g., saturated patches). In this paper, we propose a CNN-based solution to estimate the camera model attribution reliability of a given image patch. We show that we can estimate a reliability-map indicating which portions of the image contain reliable camera traces. Testing using a well known dataset confirms that by using this information, it is possible to increase small patch camera model attribution accuracy by more than 8% on a single patch.



### Advanced local motion patterns for macro and micro facial expression recognition
- **Arxiv ID**: http://arxiv.org/abs/1805.01951v1
- **DOI**: 10.1109/TAFFC.2019.2949559
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.01951v1)
- **Published**: 2018-05-04 22:19:02+00:00
- **Updated**: 2018-05-04 22:19:02+00:00
- **Authors**: B. Allaert, IM. Bilasco, C. Djeraba
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we develop a new method that recognizes facial expressions, on the basis of an innovative local motion patterns feature, with three main contributions. The first one is the analysis of the face skin temporal elasticity and face deformations during expression. The second one is a unified approach for both macro and micro expression recognition. And, the third one is the step forward towards in-the-wild expression recognition, dealing with challenges such as various intensity and various expression activation patterns, illumination variation and small head pose variations. Our method outperforms state-of-the-art methods for micro expression recognition and positions itself among top-rank state-of-the-art methods for macro expression recognition.



### MTFH: A Matrix Tri-Factorization Hashing Framework for Efficient Cross-Modal Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1805.01963v2
- **DOI**: 10.1109/TPAMI.2019.2940446
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.01963v2)
- **Published**: 2018-05-04 23:21:15+00:00
- **Updated**: 2019-09-12 00:30:51+00:00
- **Authors**: Xin Liu, Zhikai Hu, Haibin Ling, Yiu-ming Cheung
- **Comment**: 16 pages, accepted by IEEE T-PAMI
- **Journal**: IEEE Transactions on Pattern Analysis and Machine
  Intelligence,2019
- **Summary**: Hashing has recently sparked a great revolution in cross-modal retrieval because of its low storage cost and high query speed. Recent cross-modal hashing methods often learn unified or equal-length hash codes to represent the multi-modal data and make them intuitively comparable. However, such unified or equal-length hash representations could inherently sacrifice their representation scalability because the data from different modalities may not have one-to-one correspondence and could be encoded more efficiently by different hash codes of unequal lengths. To mitigate these problems, this paper exploits a related and relatively unexplored problem: encode the heterogeneous data with varying hash lengths and generalize the cross-modal retrieval in various challenging scenarios. To this end, a generalized and flexible cross-modal hashing framework, termed Matrix Tri-Factorization Hashing (MTFH), is proposed to work seamlessly in various settings including paired or unpaired multi-modal data, and equal or varying hash length encoding scenarios. More specifically, MTFH exploits an efficient objective function to flexibly learn the modality-specific hash codes with different length settings, while synchronously learning two semantic correlation matrices to semantically correlate the different hash representations for heterogeneous data comparable. As a result, the derived hash codes are more semantically meaningful for various challenging cross-modal retrieval tasks. Extensive experiments evaluated on public benchmark datasets highlight the superiority of MTFH under various retrieval scenarios and show its competitive performance with the state-of-the-arts.



