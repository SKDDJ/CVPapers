# Arxiv Papers in cs.CV on 2018-05-31
### Learning Factorized Representations for Open-set Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1805.12277v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.12277v1)
- **Published**: 2018-05-31 01:08:20+00:00
- **Updated**: 2018-05-31 01:08:20+00:00
- **Authors**: Mahsa Baktashmotlagh, Masoud Faraki, Tom Drummond, Mathieu Salzmann
- **Comment**: None
- **Journal**: None
- **Summary**: Domain adaptation for visual recognition has undergone great progress in the past few years. Nevertheless, most existing methods work in the so-called closed-set scenario, assuming that the classes depicted by the target images are exactly the same as those of the source domain. In this paper, we tackle the more challenging, yet more realistic case of open-set domain adaptation, where new, unknown classes can be present in the target data. While, in the unsupervised scenario, one cannot expect to be able to identify each specific new class, we aim to automatically detect which samples belong to these new classes and discard them from the recognition process. To this end, we rely on the intuition that the source and target samples depicting the known classes can be generated by a shared subspace, whereas the target samples from unknown classes come from a different, private subspace. We therefore introduce a framework that factorizes the data into shared and private parts, while encouraging the shared representation to be discriminative. Our experiments on standard benchmarks evidence that our approach significantly outperforms the state-of-the-art in open-set domain adaptation.



### Bayesian Pose Graph Optimization via Bingham Distributions and Tempered Geodesic MCMC
- **Arxiv ID**: http://arxiv.org/abs/1805.12279v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CG, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.12279v2)
- **Published**: 2018-05-31 01:14:57+00:00
- **Updated**: 2019-03-30 17:23:30+00:00
- **Authors**: Tolga Birdal, Umut Şimşekli, M. Onur Eken, Slobodan Ilic
- **Comment**: Published at NeurIPS 2018, 25 pages with supplements
- **Journal**: None
- **Summary**: We introduce Tempered Geodesic Markov Chain Monte Carlo (TG-MCMC) algorithm for initializing pose graph optimization problems, arising in various scenarios such as SFM (structure from motion) or SLAM (simultaneous localization and mapping). TG-MCMC is first of its kind as it unites asymptotically global non-convex optimization on the spherical manifold of quaternions with posterior sampling, in order to provide both reliable initial poses and uncertainty estimates that are informative about the quality of individual solutions. We devise rigorous theoretical convergence guarantees for our method and extensively evaluate it on synthetic and real benchmark datasets. Besides its elegance in formulation and theory, we show that our method is robust to missing data, noise and the estimated uncertainties capture intuitive properties of the data.



### Efficient Traffic-Sign Recognition with Scale-aware CNN
- **Arxiv ID**: http://arxiv.org/abs/1805.12289v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.12289v1)
- **Published**: 2018-05-31 02:09:20+00:00
- **Updated**: 2018-05-31 02:09:20+00:00
- **Authors**: Yuchen Yang, Shuo Liu, Wei Ma, Qiuyuan Wang, Zheng Liu
- **Comment**: This paper has been published on BMVC 2017
- **Journal**: None
- **Summary**: The paper presents a Traffic Sign Recognition (TSR) system, which can fast and accurately recognize traffic signs of different sizes in images. The system consists of two well-designed Convolutional Neural Networks (CNNs), one for region proposals of traffic signs and one for classification of each region. In the proposal CNN, a Fully Convolutional Network (FCN) with a dual multi-scale architecture is proposed to achieve scale invariant detection. In training the proposal network, a modified "Online Hard Example Mining" (OHEM) scheme is adopted to suppress false positives. The classification network fuses multi-scale features as representation and adopts an "Inception" module for efficiency. We evaluate the proposed TSR system and its components with extensive experiments. Our method obtains $99.88\%$ precision and $96.61\%$ recall on the Swedish Traffic Signs Dataset (STSD), higher than state-of-the-art methods. Besides, our system is faster and more lightweight than state-of-the-art deep learning networks for traffic sign recognition.



### Image-Dependent Local Entropy Models for Learned Image Compression
- **Arxiv ID**: http://arxiv.org/abs/1805.12295v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.12295v1)
- **Published**: 2018-05-31 02:32:48+00:00
- **Updated**: 2018-05-31 02:32:48+00:00
- **Authors**: David Minnen, George Toderici, Saurabh Singh, Sung Jin Hwang, Michele Covell
- **Comment**: None
- **Journal**: International Conference on Image Processing 2018
- **Summary**: The leading approach for image compression with artificial neural networks (ANNs) is to learn a nonlinear transform and a fixed entropy model that are optimized for rate-distortion performance. We show that this approach can be significantly improved by incorporating spatially local, image-dependent entropy models. The key insight is that existing ANN-based methods learn an entropy model that is shared between the encoder and decoder, but they do not transmit any side information that would allow the model to adapt to the structure of a specific image. We present a method for augmenting ANN-based image coders with image-dependent side information that leads to a 17.8% rate reduction over a state-of-the-art ANN-based baseline model on a standard evaluation set, and 70-98% reductions on images with low visual complexity that are poorly captured by a fixed, global entropy model.



### Rotation Equivariance and Invariance in Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1805.12301v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1805.12301v1)
- **Published**: 2018-05-31 03:13:41+00:00
- **Updated**: 2018-05-31 03:13:41+00:00
- **Authors**: Benjamin Chidester, Minh N. Do, Jian Ma
- **Comment**: None
- **Journal**: None
- **Summary**: Performance of neural networks can be significantly improved by encoding known invariance for particular tasks. Many image classification tasks, such as those related to cellular imaging, exhibit invariance to rotation. We present a novel scheme using the magnitude response of the 2D-discrete-Fourier transform (2D-DFT) to encode rotational invariance in neural networks, along with a new, efficient convolutional scheme for encoding rotational equivariance throughout convolutional layers. We implemented this scheme for several image classification tasks and demonstrated improved performance, in terms of classification accuracy, time required to train the model, and robustness to hyperparameter selection, over a standard CNN and another state-of-the-art method.



### Adversarial Attacks on Face Detectors using Neural Net based Constrained Optimization
- **Arxiv ID**: http://arxiv.org/abs/1805.12302v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1805.12302v1)
- **Published**: 2018-05-31 03:18:32+00:00
- **Updated**: 2018-05-31 03:18:32+00:00
- **Authors**: Avishek Joey Bose, Parham Aarabi
- **Comment**: Accepted to IEEE MMSP
- **Journal**: None
- **Summary**: Adversarial attacks involve adding, small, often imperceptible, perturbations to inputs with the goal of getting a machine learning model to misclassifying them. While many different adversarial attack strategies have been proposed on image classification models, object detection pipelines have been much harder to break. In this paper, we propose a novel strategy to craft adversarial examples by solving a constrained optimization problem using an adversarial generator network. Our approach is fast and scalable, requiring only a forward pass through our trained generator network to craft an adversarial sample. Unlike in many attack strategies, we show that the same trained generator is capable of attacking new images without explicitly optimizing on them. We evaluate our attack on a trained Faster R-CNN face detector on the cropped 300-W face dataset where we manage to reduce the number of detected faces to $0.5\%$ of all originally detected faces. In a different experiment, also on 300-W, we demonstrate the robustness of our attack to a JPEG compression based defense typical JPEG compression level of $75\%$ reduces the effectiveness of our attack from only $0.5\%$ of detected faces to a modest $5.0\%$.



### DeepMiner: Discovering Interpretable Representations for Mammogram Classification and Explanation
- **Arxiv ID**: http://arxiv.org/abs/1805.12323v2
- **DOI**: 10.1162/99608f92.8b81b005
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.12323v2)
- **Published**: 2018-05-31 05:42:45+00:00
- **Updated**: 2021-08-18 03:17:53+00:00
- **Authors**: Jimmy Wu, Bolei Zhou, Diondra Peck, Scott Hsieh, Vandana Dialani, Lester Mackey, Genevieve Patterson
- **Comment**: Harvard Data Science Review (HDSR), 2021. Code available at
  https://github.com/jimmyyhwu/ddsm-visual-primitives
- **Journal**: None
- **Summary**: We propose DeepMiner, a framework to discover interpretable representations in deep neural networks and to build explanations for medical predictions. By probing convolutional neural networks (CNNs) trained to classify cancer in mammograms, we show that many individual units in the final convolutional layer of a CNN respond strongly to diseased tissue concepts specified by the BI-RADS lexicon. After expert annotation of the interpretable units, our proposed method is able to generate explanations for CNN mammogram classification that are consistent with ground truth radiology reports on the Digital Database for Screening Mammography. We show that DeepMiner not only enables better understanding of the nuances of CNN classification decisions but also possibly discovers new visual knowledge relevant to medical diagnosis.



### Simultaneous Optical Flow and Segmentation (SOFAS) using Dynamic Vision Sensor
- **Arxiv ID**: http://arxiv.org/abs/1805.12326v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.12326v1)
- **Published**: 2018-05-31 05:57:09+00:00
- **Updated**: 2018-05-31 05:57:09+00:00
- **Authors**: Timo Stoffregen, Lindsay Kleeman
- **Comment**: None
- **Journal**: Australasian Conference on Robotics and Automation 2017
- **Summary**: We present an algorithm (SOFAS) to estimate the optical flow of events generated by a dynamic vision sensor (DVS). Where traditional cameras produce frames at a fixed rate, DVSs produce asynchronous events in response to intensity changes with a high temporal resolution. Our algorithm uses the fact that events are generated by edges in the scene to not only estimate the optical flow but also to simultaneously segment the image into objects which are travelling at the same velocity. This way it is able to avoid the aperture problem which affects other implementations such as Lucas-Kanade. Finally, we show that SOFAS produces more accurate results than traditional optic flow algorithms.



### Distinguishing Refracted Features using Light Field Cameras with Application to Structure from Motion
- **Arxiv ID**: http://arxiv.org/abs/1806.07375v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1806.07375v1)
- **Published**: 2018-05-31 06:38:28+00:00
- **Updated**: 2018-05-31 06:38:28+00:00
- **Authors**: Dorian Tsai, Donald G Dansereau, Thierry Peynot, Peter Corke
- **Comment**: 8 pages, 8 figures, submission to IROS 2018
- **Journal**: None
- **Summary**: Robots must reliably interact with refractive objects in many applications; however, refractive objects can cause many robotic vision algorithms to become unreliable or even fail, particularly feature-based matching applications, such as structure-from-motion. We propose a method to distinguish between refracted and Lambertian image features using a light field camera. Specifically, we propose to use textural cross-correlation to characterise apparent feature motion in a single light field, and compare this motion to its Lambertian equivalent based on 4D light field geometry. Our refracted feature distinguisher has a 34.3% higher rate of detection compared to state-of-the-art for light fields captured with large baselines relative to the refractive object. Our method also applies to light field cameras with much smaller baselines than previously considered, yielding up to 2 times better detection for 2D-refractive objects, such as a sphere, and up to 8 times better for 1D-refractive objects, such as a cylinder. For structure from motion, we demonstrate that rejecting refracted features using our distinguisher yields up to 42.4% lower reprojection error, and lower failure rate when the robot is approaching refractive objects. Our method lead to more robust robot vision in the presence of refractive objects.



### Classification of volcanic ash particles using a convolutional neural network and probability
- **Arxiv ID**: http://arxiv.org/abs/1805.12353v1
- **DOI**: 10.1038/s41598-018-26200-2
- **Categories**: **physics.geo-ph**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1805.12353v1)
- **Published**: 2018-05-31 07:30:48+00:00
- **Updated**: 2018-05-31 07:30:48+00:00
- **Authors**: Daigo Shoji, Rina Noguchi, Shizuka Otsuki, Hideitsu Hino
- **Comment**: 25 pages, 4 tables, 7 figurs, published in Scientific Reports
- **Journal**: D. Shoji, R. Noguchi, S. Otsuki and H. Hino. Classification of
  volcanic ash particles using a convolutional neural network and probability.
  Scientific Reports 8, 8111 (2018)
- **Summary**: Analyses of volcanic ash are typically performed either by qualitatively classifying ash particles by eye or by quantitatively parameterizing its shape and texture. While complex shapes can be classified through qualitative analyses, the results are subjective due to the difficulty of categorizing complex shapes into a single class. Although quantitative analyses are objective, selection of shape parameters is required. Here, we applied a convolutional neural network (CNN) for the classification of volcanic ash. First, we defined four basal particle shapes (blocky, vesicular, elongated, rounded) generated by different eruption mechanisms (e.g., brittle fragmentation), and then trained the CNN using particles composed of only one basal shape. The CNN could recognize the basal shapes with over 90% accuracy. Using the trained network, we classified ash particles composed of multiple basal shapes based on the output of the network, which can be interpreted as a mixing ratio of the four basal shapes. Clustering of samples by the averaged probabilities and the intensity is consistent with the eruption type. The mixing ratio output by the CNN can be used to quantitatively classify complex shapes in nature without categorizing forcibly and without the need for shape parameters, which may lead to a new taxonomy.



### Light Field Denoising via Anisotropic Parallax Analysis in a CNN Framework
- **Arxiv ID**: http://arxiv.org/abs/1805.12358v2
- **DOI**: 10.1109/LSP.2018.2861212
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.12358v2)
- **Published**: 2018-05-31 07:41:37+00:00
- **Updated**: 2018-07-26 02:36:57+00:00
- **Authors**: Jie Chen, Junhui Hou, Lap-Pui Chau
- **Comment**: None
- **Journal**: None
- **Summary**: Light field (LF) cameras provide perspective information of scenes by taking directional measurements of the focusing light rays. The raw outputs are usually dark with additive camera noise, which impedes subsequent processing and applications. We propose a novel LF denoising framework based on anisotropic parallax analysis (APA). Two convolutional neural networks are jointly designed for the task: first, the structural parallax synthesis network predicts the parallax details for the entire LF based on a set of anisotropic parallax features. These novel features can efficiently capture the high frequency perspective components of a LF from noisy observations. Second, the view-dependent detail compensation network restores non-Lambertian variation to each LF view by involving view-specific spatial energies. Extensive experiments show that the proposed APA LF denoiser provides a much better denoising performance than state-of-the-art methods in terms of visual quality and in preservation of parallax details.



### Reinforced Continual Learning
- **Arxiv ID**: http://arxiv.org/abs/1805.12369v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.12369v1)
- **Published**: 2018-05-31 08:11:12+00:00
- **Updated**: 2018-05-31 08:11:12+00:00
- **Authors**: Ju Xu, Zhanxing Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Most artificial intelligence models have limiting ability to solve new tasks faster, without forgetting previously acquired knowledge. The recently emerging paradigm of continual learning aims to solve this issue, in which the model learns various tasks in a sequential fashion. In this work, a novel approach for continual learning is proposed, which searches for the best neural architecture for each coming task via sophisticatedly designed reinforcement learning strategies. We name it as Reinforced Continual Learning. Our method not only has good performance on preventing catastrophic forgetting but also fits new tasks well. The experiments on sequential classification tasks for variants of MNIST and CIFAR-100 datasets demonstrate that the proposed approach outperforms existing continual learning alternatives for deep networks.



### Lip Reading Using Convolutional Auto Encoders as Feature Extractor
- **Arxiv ID**: http://arxiv.org/abs/1805.12371v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/1805.12371v1)
- **Published**: 2018-05-31 08:20:12+00:00
- **Updated**: 2018-05-31 08:20:12+00:00
- **Authors**: Dharin Parekh, Ankitesh Gupta, Shharrnam Chhatpar, Anmol Yash Kumar, Manasi Kulkarni
- **Comment**: 6 pages, 6 tables, 9 Figures
- **Journal**: None
- **Summary**: Visual recognition of speech using the lip movement is called Lip-reading. Recent developments in this nascent field uses different neural networks as feature extractors which serve as input to a model which can map the temporal relationship and classify. Though end to end sentence level Lip-reading is the current trend, we proposed a new model which employs word level classification and breaks the set benchmarks for standard datasets. In our model we use convolutional autoencoders as feature extractors which are then fed to a Long short-term memory model. We tested our proposed model on BBC's LRW dataset, MIRACL-VC1 and GRID dataset. Achieving a classification accuracy of 98% on MIRACL-VC1 as compared to 93.4% of the set benchmark (Rekik et al., 2014). On BBC's LRW the proposed model performed better than the baseline model of convolutional neural networks and Long short-term memory model (Garg et al., 2016). Showing the features learned by the models we clearly indicate how the proposed model works better than the baseline model. The same model can also be extended for end to end sentence level classification.



### Transfer Learning for Related Reinforcement Learning Tasks via Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/1806.07377v6
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1806.07377v6)
- **Published**: 2018-05-31 09:25:29+00:00
- **Updated**: 2019-07-04 06:51:34+00:00
- **Authors**: Shani Gamrian, Yoav Goldberg
- **Comment**: Proceedings of the 36th International Conference on Machine Learning
  (ICML 2019)
- **Journal**: Proceedings of the 36th International Conference on Machine
  Learning, PMLR 97:2063-2072, 2019
- **Summary**: Despite the remarkable success of Deep RL in learning control policies from raw pixels, the resulting models do not generalize. We demonstrate that a trained agent fails completely when facing small visual changes, and that fine-tuning---the common transfer learning paradigm---fails to adapt to these changes, to the extent that it is faster to re-train the model from scratch. We show that by separating the visual transfer task from the control policy we achieve substantially better sample efficiency and transfer behavior, allowing an agent trained on the source task to transfer well to the target tasks. The visual mapping from the target to the source domain is performed using unaligned GANs, resulting in a control policy that can be further improved using imitation learning from imperfect demonstrations. We demonstrate the approach on synthetic visual variants of the Breakout game, as well as on transfer between subsequent levels of Road Fighter, a Nintendo car-driving game. A visualization of our approach can be seen in https://youtu.be/4mnkzYyXMn4 and https://youtu.be/KCGTrQi6Ogo .



### Deep Learning with unsupervised data labeling for weeds detection on UAV images
- **Arxiv ID**: http://arxiv.org/abs/1805.12395v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.12395v1)
- **Published**: 2018-05-31 09:43:40+00:00
- **Updated**: 2018-05-31 09:43:40+00:00
- **Authors**: M. Dian. Bah, Adel Hafiane, Raphael Canals
- **Comment**: None
- **Journal**: None
- **Summary**: In modern agriculture, usually weeds control consists in spraying herbicides all over the agricultural field. This practice involves significant waste and cost of herbicide for farmers and environmental pollution. One way to reduce the cost and environmental impact is to allocate the right doses of herbicide at the right place and at the right time (Precision Agriculture). Nowadays, Unmanned Aerial Vehicle (UAV) is becoming an interesting acquisition system for weeds localization and management due to its ability to obtain the images of the entire agricultural field with a very high spatial resolution and at low cost. Despite the important advances in UAV acquisition systems, automatic weeds detection remains a challenging problem because of its strong similarity with the crops. Recently Deep Learning approach has shown impressive results in different complex classification problem. However, this approach needs a certain amount of training data but, creating large agricultural datasets with pixel-level annotations by expert is an extremely time consuming task. In this paper, we propose a novel fully automatic learning method using Convolutional Neuronal Networks (CNNs) with unsupervised training dataset collection for weeds detection from UAV images. The proposed method consists in three main phases. First we automatically detect the crop lines and using them to identify the interline weeds. In the second phase, interline weeds are used to constitute the training dataset. Finally, we performed CNNs on this dataset to build a model able to detect the crop and weeds in the images. The results obtained are comparable to the traditional supervised training data labeling. The accuracy gaps are 1.5% in the spinach field and 6% in the bean field.



### One-shot domain adaptation in multiple sclerosis lesion segmentation using convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/1805.12415v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.12415v1)
- **Published**: 2018-05-31 10:50:11+00:00
- **Updated**: 2018-05-31 10:50:11+00:00
- **Authors**: Sergi Valverde, Mostafa Salem, Mariano Cabezas, Deborah Pareto, Joan C. Vilanova, Lluís Ramió-Torrentà, Àlex Rovira, Joaquim Salvi, Arnau Oliver, Xavier Lladó
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, several convolutional neural network (CNN) methods have been proposed for the automated white matter lesion segmentation of multiple sclerosis (MS) patient images, due to their superior performance compared with those of other state-of-the-art methods. However, the accuracies of CNN methods tend to decrease significantly when evaluated on different image domains compared with those used for training, which demonstrates the lack of adaptability of CNNs to unseen imaging data. In this study, we analyzed the effect of intensity domain adaptation on our recently proposed CNN-based MS lesion segmentation method. Given a source model trained on two public MS datasets, we investigated the transferability of the CNN model when applied to other MRI scanners and protocols, evaluating the minimum number of annotated images needed from the new domain and the minimum number of layers needed to re-train to obtain comparable accuracy. Our analysis comprised MS patient data from both a clinical center and the public ISBI2015 challenge database, which permitted us to compare the domain adaptation capability of our model to that of other state-of-the-art methods. For the ISBI2015 challenge, our one-shot domain adaptation model trained using only a single image showed a performance similar to that of other CNN methods that were fully trained using the entire available training set, yielding a comparable human expert rater performance. We believe that our experiments will encourage the MS community to incorporate its use in different clinical settings with reduced amounts of annotated data. This approach could be meaningful not only in terms of the accuracy in delineating MS lesions but also in the related reductions in time and economic costs derived from manual lesion labeling.



### Semantic Analysis of (Reflectional) Visual Symmetry: A Human-Centred Computational Model for Declarative Explainability
- **Arxiv ID**: http://arxiv.org/abs/1806.07376v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LO
- **Links**: [PDF](http://arxiv.org/pdf/1806.07376v2)
- **Published**: 2018-05-31 11:47:46+00:00
- **Updated**: 2018-09-14 17:59:34+00:00
- **Authors**: Jakob Suchan, Mehul Bhatt, Srikrishna Vardarajan, Seyed Ali Amirshahi, Stella Yu
- **Comment**: Preprint of accepted article / Journal: Advances in Cognitive
  Systems. ( http://www.cogsys.org/journal )
- **Journal**: Advances in Cognitive Systems. (http://www.cogsys.org/journal),
  2018
- **Summary**: We present a computational model for the semantic interpretation of symmetry in naturalistic scenes. Key features include a human-centred representation, and a declarative, explainable interpretation model supporting deep semantic question-answering founded on an integration of methods in knowledge representation and deep learning based computer vision. In the backdrop of the visual arts, we showcase the framework's capability to generate human-centred, queryable, relational structures, also evaluating the framework with an empirical study on the human perception of visual symmetry. Our framework represents and is driven by the application of foundational, integrated Vision and Knowledge Representation and Reasoning methods for applications in the arts, and the psychological and social sciences.



### New Feature Detection Mechanism for Extended Kalman Filter Based Monocular SLAM with 1-Point RANSAC
- **Arxiv ID**: http://arxiv.org/abs/1805.12443v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.12443v1)
- **Published**: 2018-05-31 12:49:13+00:00
- **Updated**: 2018-05-31 12:49:13+00:00
- **Authors**: Agniva Sengupta, Shafeeq Elanattil
- **Comment**: Accepted in Third International Conference of Mining Intelligence and
  Knowledge Exploration (MIKE) 2015
- **Journal**: None
- **Summary**: We present a different approach of feature point detection for improving the accuracy of SLAM using single, monocular camera. Traditionally, Harris Corner detection, SURF or FAST corner detectors are used for finding feature points of interest in the image. We replace this with another approach, which involves building a non-linear scale-space representation of images using Perona and Malik Diffusion equation and computing the scale normalized Hessian at multiple scale levels (KAZE feature). The feature points so detected are used to estimate the state and pose of a mono camera using extended Kalman filter. By using accelerated KAZE features and a more rigorous feature rejection routine combined with 1-point RANSAC for outlier rejection, short baseline matching of features are significantly improved, even with a lesser number of feature points, especially in the presence of motion blur. We present a comparative study of our proposal with FAST and show improved localization accuracy in terms of absolute trajectory error.



### Towards a new system for drowsiness detection based on eye blinking and head posture estimation
- **Arxiv ID**: http://arxiv.org/abs/1806.00360v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1806.00360v1)
- **Published**: 2018-05-31 13:24:10+00:00
- **Updated**: 2018-05-31 13:24:10+00:00
- **Authors**: M. Ben Dkhil, A. Wali, Adel M. Alimi
- **Comment**: None
- **Journal**: None
- **Summary**: Driver drowsiness problem is considered as one of the most important reasons that increases road accidents number. We propose in this paper a new approach for realtime driver drowsiness in order to prevent road accidents. The system uses a smart video camera that takes drivers faces images and supervises the eye blink (open and close) state and head posture to detect the different drowsiness states. Face and eye detection are done by Viola and Jones technique.



### On GANs and GMMs
- **Arxiv ID**: http://arxiv.org/abs/1805.12462v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1805.12462v2)
- **Published**: 2018-05-31 13:37:59+00:00
- **Updated**: 2018-11-03 18:58:50+00:00
- **Authors**: Eitan Richardson, Yair Weiss
- **Comment**: Accepted to NIPS 2018
- **Journal**: None
- **Summary**: A longstanding problem in machine learning is to find unsupervised methods that can learn the statistical structure of high dimensional signals. In recent years, GANs have gained much attention as a possible solution to the problem, and in particular have shown the ability to generate remarkably realistic high resolution sampled images. At the same time, many authors have pointed out that GANs may fail to model the full distribution ("mode collapse") and that using the learned models for anything other than generating samples may be very difficult. In this paper, we examine the utility of GANs in learning statistical models of images by comparing them to perhaps the simplest statistical model, the Gaussian Mixture Model. First, we present a simple method to evaluate generative models based on relative proportions of samples that fall into predetermined bins. Unlike previous automatic methods for evaluating models, our method does not rely on an additional neural network nor does it require approximating intractable computations. Second, we compare the performance of GANs to GMMs trained on the same datasets. While GMMs have previously been shown to be successful in modeling small patches of images, we show how to train them on full sized images despite the high dimensionality. Our results show that GMMs can generate realistic samples (although less sharp than those of GANs) but also capture the full distribution, which GANs fail to do. Furthermore, GMMs allow efficient inference and explicit representation of the underlying statistical structure. Finally, we discuss how GMMs can be used to generate sharp images.



### A Method Based on Convex Cone Model for Image-Set Classification with CNN Features
- **Arxiv ID**: http://arxiv.org/abs/1805.12467v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.12467v1)
- **Published**: 2018-05-31 13:47:09+00:00
- **Updated**: 2018-05-31 13:47:09+00:00
- **Authors**: Naoya Sogi, Taku Nakayama, Kazuhiro Fukui
- **Comment**: Accepted at the International Joint Conference on Neural Networks,
  IJCNN, 2018
- **Journal**: None
- **Summary**: In this paper, we propose a method for image-set classification based on convex cone models, focusing on the effectiveness of convolutional neural network (CNN) features as inputs. CNN features have non-negative values when using the rectified linear unit as an activation function. This naturally leads us to model a set of CNN features by a convex cone and measure the geometric similarity of convex cones for classification. To establish this framework, we sequentially define multiple angles between two convex cones by repeating the alternating least squares method and then define the geometric similarity between the cones using the obtained angles. Moreover, to enhance our method, we introduce a discriminant space, maximizing the between-class variance (gaps) and minimizes the within-class variance of the projected convex cones onto the discriminant space, similar to a Fisher discriminant analysis. Finally, classification is based on the similarity between projected convex cones. The effectiveness of the proposed method was demonstrated experimentally using a private, multi-view hand shape dataset and two public databases.



### Sequential Attacks on Agents for Long-Term Adversarial Goals
- **Arxiv ID**: http://arxiv.org/abs/1805.12487v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.12487v2)
- **Published**: 2018-05-31 14:22:09+00:00
- **Updated**: 2018-07-05 17:31:14+00:00
- **Authors**: Edgar Tretschk, Seong Joon Oh, Mario Fritz
- **Comment**: None
- **Journal**: None
- **Summary**: Reinforcement learning (RL) has advanced greatly in the past few years with the employment of effective deep neural networks (DNNs) on the policy networks. With the great effectiveness came serious vulnerability issues with DNNs that small adversarial perturbations on the input can change the output of the network. Several works have pointed out that learned agents with a DNN policy network can be manipulated against achieving the original task through a sequence of small perturbations on the input states. In this paper, we demonstrate furthermore that it is also possible to impose an arbitrary adversarial reward on the victim policy network through a sequence of attacks. Our method involves the latest adversarial attack technique, Adversarial Transformer Network (ATN), that learns to generate the attack and is easy to integrate into the policy network. As a result of our attack, the victim agent is misguided to optimise for the adversarial reward over time. Our results expose serious security threats for RL applications in safety-critical systems including drones, medical analysis, and self-driving cars.



### Robust Gyroscope-Aided Camera Self-Calibration
- **Arxiv ID**: http://arxiv.org/abs/1805.12506v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1805.12506v1)
- **Published**: 2018-05-31 15:08:35+00:00
- **Updated**: 2018-05-31 15:08:35+00:00
- **Authors**: Santiago Cortés Reina, Arno Solin, Juho Kannala
- **Comment**: Appearing in Proceedings of the International Conference on
  Information Fusion (FUSION 2018)
- **Journal**: None
- **Summary**: Camera calibration for estimating the intrinsic parameters and lens distortion is a prerequisite for various monocular vision applications including feature tracking and video stabilization. This application paper proposes a model for estimating the parameters on the fly by fusing gyroscope and camera data, both readily available in modern day smartphones. The model is based on joint estimation of visual feature positions, camera parameters, and the camera pose, the movement of which is assumed to follow the movement predicted by the gyroscope. Our model assumes the camera movement to be free, but continuous and differentiable, and individual features are assumed to stay stationary. The estimation is performed online using an extended Kalman filter, and it is shown to outperform existing methods in robustness and insensitivity to initialization. We demonstrate the method using simulated data and empirical data from an iPad.



### Accurate pedestrian localization in overhead depth images via Height-Augmented HOG
- **Arxiv ID**: http://arxiv.org/abs/1805.12510v1
- **DOI**: 10.17815/CD.2020.30
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.12510v1)
- **Published**: 2018-05-31 15:16:55+00:00
- **Updated**: 2018-05-31 15:16:55+00:00
- **Authors**: Werner Kroneman, Alessandro Corbetta, Federico Toschi
- **Comment**: 8 pages
- **Journal**: Collective Dynamics, v. 5, p. 33-40, mar. 2020. ISSN 2366-8539
- **Summary**: We tackle the challenge of reliably and automatically localizing pedestrians in real-life conditions through overhead depth imaging at unprecedented high-density conditions. Leveraging upon a combination of Histogram of Oriented Gradients-like feature descriptors, neural networks, data augmentation and custom data annotation strategies, this work contributes a robust and scalable machine learning-based localization algorithm, which delivers near-human localization performance in real-time, even with local pedestrian density of about 3 ped/m2, a case in which most state-of-the art algorithms degrade significantly in performance.



### Whole Brain Susceptibility Mapping Using Harmonic Incompatibility Removal
- **Arxiv ID**: http://arxiv.org/abs/1805.12521v2
- **DOI**: None
- **Categories**: **math.NA**, cs.CV, 35R30, 42B20, 45E10, 65K10, 68U10, 90C90, 92C55
- **Links**: [PDF](http://arxiv.org/pdf/1805.12521v2)
- **Published**: 2018-05-31 15:40:54+00:00
- **Updated**: 2018-12-28 13:16:54+00:00
- **Authors**: Chenglong Bao, Jae Kyu Choi, Bin Dong
- **Comment**: Accepted for publication in SIAM Journal on Imaging Sciences
- **Journal**: None
- **Summary**: Quantitative susceptibility mapping (QSM) aims to visualize the three dimensional susceptibility distribution by solving the field-to-source inverse problem using the phase data in magnetic resonance signal. However, the inverse problem is ill-posed since the Fourier transform of integral kernel has zeroes in the frequency domain. Although numerous regularization based models have been proposed to overcome this problem, the incompatibility in the field data has not received enough attention, which leads to deterioration of the recovery. In this paper, we show that the data acquisition process of QSM inherently generates a harmonic incompatibility in the measured local field. Based on such discovery, we propose a novel regularization based susceptibility reconstruction model with an additional sparsity based regularization term on the harmonic incompatibility. Numerical experiments show that the proposed method achieves better performance than the existing approaches.



### Fully Automated Organ Segmentation in Male Pelvic CT Images
- **Arxiv ID**: http://arxiv.org/abs/1805.12526v2
- **DOI**: 10.1088/1361-6560/aaf11c
- **Categories**: **physics.med-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1805.12526v2)
- **Published**: 2018-05-31 15:56:22+00:00
- **Updated**: 2021-04-26 14:54:01+00:00
- **Authors**: Anjali Balagopal, Samaneh Kazemifar, Dan Nguyen, Mu-Han Lin, Raquibul Hannan, Amir Owrangi, Steve Jiang
- **Comment**: 21 pages; 11 figures; 4 tables
- **Journal**: Balagopal A, Kazemifar S, Nguyen D, Lin M H, Hannan R, Owrangi A
  and Jiang S 2018 Fully automated organ segmentation in male pelvic CT images
  Phys. Med. Biol. 63 245015
- **Summary**: Accurate segmentation of prostate and surrounding organs at risk is important for prostate cancer radiotherapy treatment planning. We present a fully automated workflow for male pelvic CT image segmentation using deep learning. The architecture consists of a 2D localization network followed by a 3D segmentation network for volumetric segmentation of prostate, bladder, rectum, and femoral heads. We used a multi-channel 2D U-Net followed by a 3D U-Net with encoding arm modified with aggregated residual networks, known as ResNeXt. The models were trained and tested on a pelvic CT image dataset comprising 136 patients. Test results show that 3D U-Net based segmentation achieves mean (SD) Dice coefficient values of 90 (2.0)% ,96 (3.0)%, 95 (1.3)%, 95 (1.5)%, and 84 (3.7)% for prostate, left femoral head, right femoral head, bladder, and rectum, respectively, using the proposed fully automated segmentation method.



### Modeling 4D fMRI Data via Spatio-Temporal Convolutional Neural Networks (ST-CNN)
- **Arxiv ID**: http://arxiv.org/abs/1805.12564v3
- **DOI**: 10.1109/TCDS.2019.2916916
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.12564v3)
- **Published**: 2018-05-31 17:08:10+00:00
- **Updated**: 2018-08-06 18:29:39+00:00
- **Authors**: Yu Zhao, Xiang Li, Wei Zhang, Shijie Zhao, Milad Makkie, Mo Zhang, Quanzheng Li, Tianming Liu
- **Comment**: Yu Zhao and Xiang Li contribute equally to this work
- **Journal**: None
- **Summary**: Simultaneous modeling of the spatio-temporal variation patterns of brain functional network from 4D fMRI data has been an important yet challenging problem for the field of cognitive neuroscience and medical image analysis. Inspired by the recent success in applying deep learning for functional brain decoding and encoding, in this work we propose a spatio-temporal convolutional neural network (ST-CNN)to jointly learn the spatial and temporal patterns of targeted network from the training data and perform automatic, pin-pointing functional network identification. The proposed ST-CNN is evaluated by the task of identifying the Default Mode Network (DMN) from fMRI data. Results show that while the framework is only trained on one fMRI dataset,it has the sufficient generalizability to identify the DMN from different populations of data as well as different cognitive tasks. Further investigation into the results show that the superior performance of ST-CNN is driven by the jointly-learning scheme, which capture the intrinsic relationship between the spatial and temporal characteristic of DMN and ensures the accurate identification.



### Fast, Diverse and Accurate Image Captioning Guided By Part-of-Speech
- **Arxiv ID**: http://arxiv.org/abs/1805.12589v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.12589v3)
- **Published**: 2018-05-31 17:56:17+00:00
- **Updated**: 2019-04-11 00:44:34+00:00
- **Authors**: Aditya Deshpande, Jyoti Aneja, Liwei Wang, Alexander Schwing, D. A. Forsyth
- **Comment**: 12 pages with references and appendix. To appear CVPR'19
- **Journal**: None
- **Summary**: Image captioning is an ambiguous problem, with many suitable captions for an image. To address ambiguity, beam search is the de facto method for sampling multiple captions. However, beam search is computationally expensive and known to produce generic captions. To address this concern, some variational auto-encoder (VAE) and generative adversarial net (GAN) based methods have been proposed. Though diverse, GAN and VAE are less accurate. In this paper, we first predict a meaningful summary of the image, then generate the caption based on that summary. We use part-of-speech as summaries, since our summary should drive caption generation. We achieve the trifecta: (1) High accuracy for the diverse captions as evaluated by standard captioning metrics and user studies; (2) Faster computation of diverse captions compared to beam search and diverse beam search; and (3) High diversity as evaluated by counting novel sentences, distinct n-grams and mutual overlap (i.e., mBleu-4) scores.



### Following High-level Navigation Instructions on a Simulated Quadcopter with Imitation Learning
- **Arxiv ID**: http://arxiv.org/abs/1806.00047v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CL, cs.CV, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1806.00047v1)
- **Published**: 2018-05-31 18:42:26+00:00
- **Updated**: 2018-05-31 18:42:26+00:00
- **Authors**: Valts Blukis, Nataly Brukhim, Andrew Bennett, Ross A. Knepper, Yoav Artzi
- **Comment**: To appear in Robotics: Science and Systems (RSS), 2018
- **Journal**: None
- **Summary**: We introduce a method for following high-level navigation instructions by mapping directly from images, instructions and pose estimates to continuous low-level velocity commands for real-time control. The Grounded Semantic Mapping Network (GSMN) is a fully-differentiable neural network architecture that builds an explicit semantic map in the world reference frame by incorporating a pinhole camera projection model within the network. The information stored in the map is learned from experience, while the local-to-world transformation is computed explicitly. We train the model using DAggerFM, a modified variant of DAgger that trades tabular convergence guarantees for improved training speed and memory use. We test GSMN in virtual environments on a realistic quadcopter simulator and show that incorporating an explicit mapping and grounding modules allows GSMN to outperform strong neural baselines and almost reach an expert policy performance. Finally, we analyze the learned map representations and show that using an explicit map leads to an interpretable instruction-following model.



### PeerNets: Exploiting Peer Wisdom Against Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/1806.00088v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.00088v1)
- **Published**: 2018-05-31 20:33:21+00:00
- **Updated**: 2018-05-31 20:33:21+00:00
- **Authors**: Jan Svoboda, Jonathan Masci, Federico Monti, Michael M. Bronstein, Leonidas Guibas
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning systems have become ubiquitous in many aspects of our lives. Unfortunately, it has been shown that such systems are vulnerable to adversarial attacks, making them prone to potential unlawful uses. Designing deep neural networks that are robust to adversarial attacks is a fundamental step in making such systems safer and deployable in a broader variety of applications (e.g. autonomous driving), but more importantly is a necessary step to design novel and more advanced architectures built on new computational paradigms rather than marginally building on the existing ones. In this paper we introduce PeerNets, a novel family of convolutional networks alternating classical Euclidean convolutions with graph convolutions to harness information from a graph of peer samples. This results in a form of non-local forward propagation in the model, where latent features are conditioned on the global structure induced by the graph, that is up to 3 times more robust to a variety of white- and black-box adversarial attacks compared to conventional architectures with almost no drop in accuracy.



### Imaging with SPADs and DMDs: Seeing through Diffraction-Photons
- **Arxiv ID**: http://arxiv.org/abs/1806.00094v2
- **DOI**: 10.1109/TIP.2019.2941315
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.00094v2)
- **Published**: 2018-05-31 20:44:24+00:00
- **Updated**: 2019-11-13 12:13:28+00:00
- **Authors**: Ibrahim Alsolami, Wolfgang Heidrich
- **Comment**: None
- **Journal**: in IEEE Transactions on Image Processing, vol. 29, pp. 1440-1449,
  2020
- **Summary**: This paper addresses the problem of imaging in the presence of diffraction-photons. Diffraction-photons arise from the low contrast ratio of DMDs ($\sim\,1000:1$), and very much degrade the quality of images captured by SPAD-based systems. Herein, a joint illumination-deconvolution scheme is designed to overcome diffraction-photons, enabling the acquisition of intensity and depth images. Additionally, a proof-of-concept experiment is conducted to demonstrate the viability of the designed scheme. It is shown that by co-designing the illumination and deconvolution phases of imaging, one can substantially overcome diffraction-photons.



### Respond-CAM: Analyzing Deep Models for 3D Imaging Data by Visualizations
- **Arxiv ID**: http://arxiv.org/abs/1806.00102v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.00102v2)
- **Published**: 2018-05-31 21:18:17+00:00
- **Updated**: 2018-06-07 06:11:46+00:00
- **Authors**: Guannan Zhao, Bo Zhou, Kaiwen Wang, Rui Jiang, Min Xu
- **Comment**: None
- **Journal**: Medical Image Computing & Computer Assisted Intervention (MICCAI)
  2018
- **Summary**: The convolutional neural network (CNN) has become a powerful tool for various biomedical image analysis tasks, but there is a lack of visual explanation for the machinery of CNNs. In this paper, we present a novel algorithm, Respond-weighted Class Activation Mapping (Respond-CAM), for making CNN-based models interpretable by visualizing input regions that are important for predictions, especially for biomedical 3D imaging data inputs. Our method uses the gradients of any target concept (e.g. the score of target class) that flows into a convolutional layer. The weighted feature maps are combined to produce a heatmap that highlights the important regions in the image for predicting the target concept. We prove a preferable sum-to-score property of the Respond-CAM and verify its significant improvement on 3D images from the current state-of-the-art approach. Our tests on Cellular Electron Cryo-Tomography 3D images show that Respond-CAM achieves superior performance on visualizing the CNNs with 3D biomedical images inputs, and is able to get reasonably good results on visualizing the CNNs with natural image inputs. The Respond-CAM is an efficient and reliable approach for visualizing the CNN machinery, and is applicable to a wide variety of CNN model families and image analysis tasks.



### MONET: Multiview Semi-supervised Keypoint Detection via Epipolar Divergence
- **Arxiv ID**: http://arxiv.org/abs/1806.00104v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.00104v2)
- **Published**: 2018-05-31 21:27:33+00:00
- **Updated**: 2019-08-17 00:14:15+00:00
- **Authors**: Yuan Yao, Yasamin Jafarian, Hyun Soo Park
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents MONET -- an end-to-end semi-supervised learning framework for a keypoint detector using multiview image streams. In particular, we consider general subjects such as non-human species where attaining a large scale annotated dataset is challenging. While multiview geometry can be used to self-supervise the unlabeled data, integrating the geometry into learning a keypoint detector is challenging due to representation mismatch. We address this mismatch by formulating a new differentiable representation of the epipolar constraint called epipolar divergence---a generalized distance from the epipolar lines to the corresponding keypoint distribution. Epipolar divergence characterizes when two view keypoint distributions produce zero reprojection error. We design a twin network that minimizes the epipolar divergence through stereo rectification that can significantly alleviate computational complexity and sampling aliasing in training. We demonstrate that our framework can localize customized keypoints of diverse species, e.g., humans, dogs, and monkeys.



### Probabilistic Model of Visual Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1806.00111v3
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1806.00111v3)
- **Published**: 2018-05-31 21:48:43+00:00
- **Updated**: 2019-05-01 18:54:39+00:00
- **Authors**: Jonathan Vacher, Pascal Mamassian, Ruben Coen-Cagli
- **Comment**: 13 pages, 5 figures, 1 table
- **Journal**: None
- **Summary**: Visual segmentation is a key perceptual function that partitions visual space and allows for detection, recognition and discrimination of objects in complex environments. The processes underlying human segmentation of natural images are still poorly understood. In part, this is because we lack segmentation models consistent with experimental and theoretical knowledge in visual neuroscience. Biological sensory systems have been shown to approximate probabilistic inference to interpret their inputs. This requires a generative model that captures both the statistics of the sensory inputs and expectations about the causes of those inputs. Following this hypothesis, we propose a probabilistic generative model of visual segmentation that combines knowledge about 1) the sensitivity of neurons in the visual cortex to statistical regularities in natural images; and 2) the preference of humans to form contiguous partitions of visual space. We develop an efficient algorithm for training and inference based on expectation-maximization and validate it on synthetic data. Importantly, with the appropriate choice of the prior, we derive an intuitive closed--form update rule for assigning pixels to segments: at each iteration, the pixel assignment probabilities to segments is the sum of the evidence (i.e. local pixel statistics) and prior (i.e. the assignments of neighboring pixels) weighted by their relative uncertainty. The model performs competitively on natural images from the Berkeley Segmentation Dataset (BSD), and we illustrate how the likelihood and prior components improve segmentation relative to traditional mixture models. Furthermore, our model explains some variability across human subjects as reflecting local uncertainty about the number of segments. Our model thus provides a viable approach to probe human visual segmentation.



