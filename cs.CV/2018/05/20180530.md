# Arxiv Papers in cs.CV on 2018-05-30
### Collaborative Learning for Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1805.11761v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1805.11761v2)
- **Published**: 2018-05-30 00:46:33+00:00
- **Updated**: 2018-11-07 00:06:03+00:00
- **Authors**: Guocong Song, Wei Chai
- **Comment**: To appear in NIPS 2018
- **Journal**: None
- **Summary**: We introduce collaborative learning in which multiple classifier heads of the same network are simultaneously trained on the same training data to improve generalization and robustness to label noise with no extra inference cost. It acquires the strengths from auxiliary training, multi-task learning and knowledge distillation. There are two important mechanisms involved in collaborative learning. First, the consensus of multiple views from different classifier heads on the same example provides supplementary information as well as regularization to each classifier, thereby improving generalization. Second, intermediate-level representation (ILR) sharing with backpropagation rescaling aggregates the gradient flows from all heads, which not only reduces training computational complexity, but also facilitates supervision to the shared layers. The empirical results on CIFAR and ImageNet datasets demonstrate that deep neural networks learned as a group in a collaborative way significantly reduce the generalization error and increase the robustness to label noise.



### AutoZOOM: Autoencoder-based Zeroth Order Optimization Method for Attacking Black-box Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1805.11770v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.11770v5)
- **Published**: 2018-05-30 01:39:34+00:00
- **Updated**: 2020-01-31 11:46:26+00:00
- **Authors**: Chun-Chen Tu, Paishun Ting, Pin-Yu Chen, Sijia Liu, Huan Zhang, Jinfeng Yi, Cho-Jui Hsieh, Shin-Ming Cheng
- **Comment**: Chun-Chen Tu, Paishun Ting and Pin-Yu Chen contribute equally to this
  work; Paper accepted to AAAI 2019; updated model information in Table S2
- **Journal**: None
- **Summary**: Recent studies have shown that adversarial examples in state-of-the-art image classifiers trained by deep neural networks (DNN) can be easily generated when the target model is transparent to an attacker, known as the white-box setting. However, when attacking a deployed machine learning service, one can only acquire the input-output correspondences of the target model; this is the so-called black-box attack setting. The major drawback of existing black-box attacks is the need for excessive model queries, which may give a false sense of model robustness due to inefficient query designs. To bridge this gap, we propose a generic framework for query-efficient black-box attacks. Our framework, AutoZOOM, which is short for Autoencoder-based Zeroth Order Optimization Method, has two novel building blocks towards efficient black-box attacks: (i) an adaptive random gradient estimation strategy to balance query counts and distortion, and (ii) an autoencoder that is either trained offline with unlabeled data or a bilinear resizing operation for attack acceleration. Experimental results suggest that, by applying AutoZOOM to a state-of-the-art black-box attack (ZOO), a significant reduction in model queries can be achieved without sacrificing the attack success rate and the visual quality of the resulting adversarial examples. In particular, when compared to the standard ZOO method, AutoZOOM can consistently reduce the mean query counts in finding successful adversarial examples (or reaching the same distortion level) by at least 93% on MNIST, CIFAR-10 and ImageNet datasets, leading to novel insights on adversarial robustness.



### Autonomous Vehicles that Interact with Pedestrians: A Survey of Theory and Practice
- **Arxiv ID**: http://arxiv.org/abs/1805.11773v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1805.11773v1)
- **Published**: 2018-05-30 02:01:17+00:00
- **Updated**: 2018-05-30 02:01:17+00:00
- **Authors**: Amir Rasouli, John K. Tsotsos
- **Comment**: This work has been submitted to the IEEE Transactions on Intelligent
  Transportation Systems
- **Journal**: None
- **Summary**: One of the major challenges that autonomous cars are facing today is driving in urban environments. To make it a reality, autonomous vehicles require the ability to communicate with other road users and understand their intentions. Such interactions are essential between the vehicles and pedestrians as the most vulnerable road users. Understanding pedestrian behavior, however, is not intuitive and depends on various factors such as demographics of the pedestrians, traffic dynamics, environmental conditions, etc. In this paper, we identify these factors by surveying pedestrian behavior studies, both the classical works on pedestrian-driver interaction and the modern ones that involve autonomous vehicles. To this end, we will discuss various methods of studying pedestrian behavior, and analyze how the factors identified in the literature are interrelated. We will also review the practical applications aimed at solving the interaction problem including design approaches for autonomous vehicles that communicate with pedestrians and visual perception and reasoning algorithms tailored to understanding pedestrian intention. Based on our findings, we will discuss the open problems and propose future research directions.



### Object Detection using Domain Randomization and Generative Adversarial Refinement of Synthetic Images
- **Arxiv ID**: http://arxiv.org/abs/1805.11778v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1805.11778v2)
- **Published**: 2018-05-30 02:27:10+00:00
- **Updated**: 2018-06-11 09:50:33+00:00
- **Authors**: Fernando Camaro Nogues, Andrew Huie, Sakyasingha Dasgupta
- **Comment**: CVPR 2018 Deep Vision Workshop
- **Journal**: None
- **Summary**: In this work, we present an application of domain randomization and generative adversarial networks (GAN) to train a near real-time object detector for industrial electric parts, entirely in a simulated environment. Large scale availability of labelled real world data is typically rare and difficult to obtain in many industrial settings. As such here, only a few hundred of unlabelled real images are used to train a Cyclic-GAN network, in combination with various degree of domain randomization procedures. We demonstrate that this enables robust translation of synthetic images to the real world domain. We show that a combination of the original synthetic (simulation) and GAN translated images, when used for training a Mask-RCNN object detection network achieves greater than 0.95 mean average precision in detecting and classifying a collection of industrial electric parts. We evaluate the performance across different combinations of training data.



### Hyperspectral Imaging Technology and Transfer Learning Utilized in Identification Haploid Maize Seeds
- **Arxiv ID**: http://arxiv.org/abs/1805.11784v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.11784v1)
- **Published**: 2018-05-30 02:55:14+00:00
- **Updated**: 2018-05-30 02:55:14+00:00
- **Authors**: Wen-Xuan Liao, Xuan-Yu Wang, Dong An, Yao-Guang Wei
- **Comment**: None
- **Journal**: None
- **Summary**: It is extremely important to correctly identify the cultivars of maize seeds in the breeding process of maize. In this paper, the transfer learning as a method of deep learning is adopted to establish a model by combining with the hyperspectral imaging technology. The haploid seeds can be recognized from large amount of diploid maize ones with great accuracy through the model. First, the information of maize seeds on each wave band is collected using the hyperspectral imaging technology, and then the recognition model is built on VGG-19 network, which is pre-trained by large-scale computer vision database (Image-Net). The correct identification rate of model utilizing seed spectral images containing 256 wave bands (862.5-1704.2nm) reaches 96.32%, and the correct identification rate of the model utilizing the seed spectral images containing single-band reaches 95.75%. The experimental results show that, CNN model which is pre-trained by visible light image database can be applied to the near-infrared hyperspectral imaging-based identification of maize seeds, and high accurate identification rate can be achieved. Meanwhile, when there is small amount of data samples, it can still realize high recognition by using transfer learning. The model not only meets the requirements of breeding recognition, but also greatly reduce the cost occurred in sample collection.



### Multi-function Convolutional Neural Networks for Improving Image Classification Performance
- **Arxiv ID**: http://arxiv.org/abs/1805.11788v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.11788v1)
- **Published**: 2018-05-30 03:14:03+00:00
- **Updated**: 2018-05-30 03:14:03+00:00
- **Authors**: Luna M. Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Traditional Convolutional Neural Networks (CNNs) typically use the same activation function (usually ReLU) for all neurons with non-linear mapping operations. For example, the deep convolutional architecture Inception-v4 uses ReLU. To improve the classification performance of traditional CNNs, a new "Multi-function Convolutional Neural Network" (MCNN) is created by using different activation functions for different neurons. For $n$ neurons and $m$ different activation functions, there are a total of $m^n-m$ MCNNs and only $m$ traditional CNNs. Therefore, the best model is very likely to be chosen from MCNNs because there are $m^n-2m$ more MCNNs than traditional CNNs. For performance analysis, two different datasets for two applications (classifying handwritten digits from the MNIST database and classifying brain MRI images into one of the four stages of Alzheimer's disease (AD)) are used. For both applications, an activation function is randomly selected for each layer of a MCNN. For the AD diagnosis application, MCNNs using a newly created multi-function Inception-v4 architecture are constructed. Overall, simulations show that MCNNs can outperform traditional CNNs in terms of multi-class classification accuracy for both applications. An important future research work will be to efficiently select the best MCNN from $m^n-m$ candidate MCNNs. Current CNN software only provides users with partial functionality of MCNNs since different layers can use different activation functions but not individual neurons in the same layer. Thus, modifying current CNN software systems such as ResNets, DenseNets, and Dual Path Networks by using multiple activation functions and developing more effective and faster MCNN software systems and tools would be very useful to solve difficult practical image classification problems.



### A Fine-to-Coarse Convolutional Neural Network for 3D Human Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1805.11790v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.11790v2)
- **Published**: 2018-05-30 03:25:14+00:00
- **Updated**: 2018-08-18 08:20:52+00:00
- **Authors**: Thao Minh Le, Nakamasa Inoue, Koichi Shinoda
- **Comment**: Camera-ready manuscript for BMVC2018
- **Journal**: None
- **Summary**: This paper presents a new framework for human action recognition from a 3D skeleton sequence. Previous studies do not fully utilize the temporal relationships between video segments in a human action. Some studies successfully used very deep Convolutional Neural Network (CNN) models but often suffer from the data insufficiency problem. In this study, we first segment a skeleton sequence into distinct temporal segments in order to exploit the correlations between them. The temporal and spatial features of a skeleton sequence are then extracted simultaneously by utilizing a fine-to-coarse (F2C) CNN architecture optimized for human skeleton sequences. We evaluate our proposed method on NTU RGB+D and SBU Kinect Interaction dataset. It achieves 79.6% and 84.6% of accuracies on NTU RGB+D with cross-object and cross-view protocol, respectively, which are almost identical with the state-of-the-art performance. In addition, our method significantly improves the accuracy of the actions in two-person interactions.



### Grow and Prune Compact, Fast, and Accurate LSTMs
- **Arxiv ID**: http://arxiv.org/abs/1805.11797v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, cs.SD, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.11797v2)
- **Published**: 2018-05-30 04:15:58+00:00
- **Updated**: 2018-05-31 03:49:25+00:00
- **Authors**: Xiaoliang Dai, Hongxu Yin, Niraj K. Jha
- **Comment**: None
- **Journal**: None
- **Summary**: Long short-term memory (LSTM) has been widely used for sequential data modeling. Researchers have increased LSTM depth by stacking LSTM cells to improve performance. This incurs model redundancy, increases run-time delay, and makes the LSTMs more prone to overfitting. To address these problems, we propose a hidden-layer LSTM (H-LSTM) that adds hidden layers to LSTM's original one level non-linear control gates. H-LSTM increases accuracy while employing fewer external stacked layers, thus reducing the number of parameters and run-time latency significantly. We employ grow-and-prune (GP) training to iteratively adjust the hidden layers through gradient-based growth and magnitude-based pruning of connections. This learns both the weights and the compact architecture of H-LSTM control gates. We have GP-trained H-LSTMs for image captioning and speech recognition applications. For the NeuralTalk architecture on the MSCOCO dataset, our three models reduce the number of parameters by 38.7x [floating-point operations (FLOPs) by 45.5x], run-time latency by 4.5x, and improve the CIDEr score by 2.6. For the DeepSpeech2 architecture on the AN4 dataset, our two models reduce the number of parameters by 19.4x (FLOPs by 23.5x), run-time latency by 15.7%, and the word error rate from 12.9% to 8.7%. Thus, GP-trained H-LSTMs can be seen to be compact, fast, and accurate.



### CRRN: Multi-Scale Guided Concurrent Reflection Removal Network
- **Arxiv ID**: http://arxiv.org/abs/1805.11802v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.11802v1)
- **Published**: 2018-05-30 04:29:42+00:00
- **Updated**: 2018-05-30 04:29:42+00:00
- **Authors**: Renjie Wan, Boxin Shi, Ling-Yu Duan, Ah-Hwee Tan, Alex C. Kot
- **Comment**: Accepted by CVPR 2018
- **Journal**: None
- **Summary**: Removing the undesired reflections from images taken through the glass is of broad application to various computer vision tasks. Non-learning based methods utilize different handcrafted priors such as the separable sparse gradients caused by different levels of blurs, which often fail due to their limited description capability to the properties of real-world reflections. In this paper, we propose the Concurrent Reflection Removal Network (CRRN) to tackle this problem in a unified framework. Our proposed network integrates image appearance information and multi-scale gradient information with human perception inspired loss function, and is trained on a new dataset with 3250 reflection images taken under diverse real-world scenes. Extensive experiments on a public benchmark dataset show that the proposed method performs favorably against state-of-the-art methods.



### Enabling Pedestrian Safety using Computer Vision Techniques: A Case Study of the 2018 Uber Inc. Self-driving Car Crash
- **Arxiv ID**: http://arxiv.org/abs/1805.11815v1
- **DOI**: 10.1007/978-3-030-12388-8_19
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1805.11815v1)
- **Published**: 2018-05-30 05:38:30+00:00
- **Updated**: 2018-05-30 05:38:30+00:00
- **Authors**: Puneet Kohli, Anjali Chadha
- **Comment**: 10 pages, 8 figures, 3 tables
- **Journal**: Arai K., Bhatia R. (eds) Advances in Information and
  Communication. FICC 2019. Lecture Notes in Networks and Systems, vol 69.
  Springer, Cham
- **Summary**: Human lives are important. The decision to allow self-driving vehicles operate on our roads carries great weight. This has been a hot topic of debate between policy-makers, technologists and public safety institutions. The recent Uber Inc. self-driving car crash, resulting in the death of a pedestrian, has strengthened the argument that autonomous vehicle technology is still not ready for deployment on public roads. In this work, we analyze the Uber car crash and shed light on the question, "Could the Uber Car Crash have been avoided?". We apply state-of-the-art Computer Vision models to this highly practical scenario. More generally, our experimental results are an evaluation of various image enhancement and object recognition techniques for enabling pedestrian safety in low-lighting conditions using the Uber crash as a case study.



### Visual Referring Expression Recognition: What Do Systems Actually Learn?
- **Arxiv ID**: http://arxiv.org/abs/1805.11818v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1805.11818v1)
- **Published**: 2018-05-30 06:03:21+00:00
- **Updated**: 2018-05-30 06:03:21+00:00
- **Authors**: Volkan Cirik, Louis-Philippe Morency, Taylor Berg-Kirkpatrick
- **Comment**: NAACL2018 short
- **Journal**: None
- **Summary**: We present an empirical analysis of the state-of-the-art systems for referring expression recognition -- the task of identifying the object in an image referred to by a natural language expression -- with the goal of gaining insight into how these systems reason about language and vision. Surprisingly, we find strong evidence that even sophisticated and linguistically-motivated models for this task may ignore the linguistic structure, instead relying on shallow correlations introduced by unintended biases in the data selection and annotation process. For example, we show that a system trained and tested on the input image $\textit{without the input referring expression}$ can achieve a precision of 71.2% in top-2 predictions. Furthermore, a system that predicts only the object category given the input can achieve a precision of 84.2% in top-2 predictions. These surprisingly positive results for what should be deficient prediction scenarios suggest that careful analysis of what our models are learning -- and further, how our data is constructed -- is critical as we seek to make substantive progress on grounded language tasks.



### Learning multiple non-mutually-exclusive tasks for improved classification of inherently ordered labels
- **Arxiv ID**: http://arxiv.org/abs/1805.11837v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1805.11837v2)
- **Published**: 2018-05-30 07:25:54+00:00
- **Updated**: 2018-11-21 07:06:08+00:00
- **Authors**: Vadim Ratner, Yoel Shoshan, Tal Kachman
- **Comment**: None
- **Journal**: None
- **Summary**: Medical image classification involves thresholding of labels that represent malignancy risk levels. Usually, a task defines a single threshold, and when developing computer-aided diagnosis tools, a single network is trained per such threshold, e.g. as screening out healthy (very low risk) patients to leave possibly sick ones for further analysis (low threshold), or trying to find malignant cases among those marked as non-risk by the radiologist ("second reading", high threshold). We propose a way to rephrase the classification problem in a manner that yields several problems (corresponding to different thresholds) to be solved simultaneously. This allows the use of Multiple Task Learning (MTL) methods, significantly improving the performance of the original classifier, by facilitating effective extraction of information from existing data.



### Neural Joking Machine : Humorous image captioning
- **Arxiv ID**: http://arxiv.org/abs/1805.11850v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1805.11850v1)
- **Published**: 2018-05-30 08:20:55+00:00
- **Updated**: 2018-05-30 08:20:55+00:00
- **Authors**: Kota Yoshida, Munetaka Minoguchi, Kenichiro Wani, Akio Nakamura, Hirokatsu Kataoka
- **Comment**: Accepted to CVPR 2018 Language & Vision Workshop
- **Journal**: None
- **Summary**: What is an effective expression that draws laughter from human beings? In the present paper, in order to consider this question from an academic standpoint, we generate an image caption that draws a "laugh" by a computer. A system that outputs funny captions based on the image caption proposed in the computer vision field is constructed. Moreover, we also propose the Funny Score, which flexibly gives weights according to an evaluation database. The Funny Score more effectively brings out "laughter" to optimize a model. In addition, we build a self-collected BoketeDB, which contains a theme (image) and funny caption (text) posted on "Bokete", which is an image Ogiri website. In an experiment, we use BoketeDB to verify the effectiveness of the proposed method by comparing the results obtained using the proposed method and those obtained using MS COCO Pre-trained CNN+LSTM, which is the baseline and idiot created by humans. We refer to the proposed method, which uses the BoketeDB pre-trained model, as the Neural Joking Machine (NJM).



### RUN:Residual U-Net for Computer-Aided Detection of Pulmonary Nodules without Candidate Selection
- **Arxiv ID**: http://arxiv.org/abs/1805.11856v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.11856v1)
- **Published**: 2018-05-30 08:42:20+00:00
- **Updated**: 2018-05-30 08:42:20+00:00
- **Authors**: Tian Lan, Yuanyuan Li, Jonah Kimani Murugi, Yi Ding, Zhiguang Qin
- **Comment**: 15 pages, 5 figures, manuscript for Neurocomputing
- **Journal**: None
- **Summary**: The early detection and early diagnosis of lung cancer are crucial to improve the survival rate of lung cancer patients. Pulmonary nodules detection results have a significant impact on the later diagnosis. In this work, we propose a new network named RUN to complete nodule detection in a single step by bypassing the candidate selection. The system introduces the shortcut of the residual network to improve the traditional U-Net, thereby solving the disadvantage of poor results due to its lack of depth. Furthermore, we compare the experimental results with the traditional U-Net. We validate our method in LUng Nodule Analysis 2016 (LUNA16) Nodule Detection Challenge. We acquire a sensitivity of 90.90% at 2 false positives per scan and therefore achieve better performance than the current state-of-the-art approaches.



### Needle Tip Force Estimation using an OCT Fiber and a Fused convGRU-CNN Architecture
- **Arxiv ID**: http://arxiv.org/abs/1805.11911v1
- **DOI**: 10.1007/978-3-030-00937-3_26
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.11911v1)
- **Published**: 2018-05-30 11:53:17+00:00
- **Updated**: 2018-05-30 11:53:17+00:00
- **Authors**: Nils Gessert, Torben Priegnitz, Thore Saathoff, Sven-Thomas Antoni, David Meyer, Moritz Franz Hamann, Klaus-Peter Jünemann, Christoph Otte, Alexander Schlaefer
- **Comment**: Accepted for Publication at MICCAI 2018
- **Journal**: None
- **Summary**: Needle insertion is common during minimally invasive interventions such as biopsy or brachytherapy. During soft tissue needle insertion, forces acting at the needle tip cause tissue deformation and needle deflection. Accurate needle tip force measurement provides information on needle-tissue interaction and helps detecting and compensating potential misplacement. For this purpose we introduce an image-based needle tip force estimation method using an optical fiber imaging the deformation of an epoxy layer below the needle tip over time. For calibration and force estimation, we introduce a novel deep learning-based fused convolutional GRU-CNN model which effectively exploits the spatio-temporal data structure. The needle is easy to manufacture and our model achieves a mean absolute error of 1.76 +- 1.5 mN with a cross-correlation coefficient of 0.9996, clearly outperforming other methods. We test needles with different materials to demonstrate that the approach can be adapted for different sensitivities and force ranges. Furthermore, we validate our approach in an ex-vivo prostate needle insertion scenario.



### Propagating Confidences through CNNs for Sparse Data Regression
- **Arxiv ID**: http://arxiv.org/abs/1805.11913v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1805.11913v3)
- **Published**: 2018-05-30 12:09:51+00:00
- **Updated**: 2018-08-03 09:05:49+00:00
- **Authors**: Abdelrahman Eldesokey, Michael Felsberg, Fahad Shahbaz Khan
- **Comment**: To appear in the British Machine Vision Conference (BMVC2018)
- **Journal**: None
- **Summary**: In most computer vision applications, convolutional neural networks (CNNs) operate on dense image data generated by ordinary cameras. Designing CNNs for sparse and irregularly spaced input data is still an open problem with numerous applications in autonomous driving, robotics, and surveillance. To tackle this challenging problem, we introduce an algebraically-constrained convolution layer for CNNs with sparse input and demonstrate its capabilities for the scene depth completion task. We propose novel strategies for determining the confidence from the convolution operation and propagating it to consecutive layers. Furthermore, we propose an objective function that simultaneously minimizes the data error while maximizing the output confidence. Comprehensive experiments are performed on the KITTI depth benchmark and the results clearly demonstrate that the proposed approach achieves superior performance while requiring three times fewer parameters than the state-of-the-art methods. Moreover, our approach produces a continuous pixel-wise confidence map enabling information fusion, state inference, and decision support.



### Multiple Manifolds Metric Learning with Application to Image Set Classification
- **Arxiv ID**: http://arxiv.org/abs/1805.11918v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.11918v1)
- **Published**: 2018-05-30 12:23:52+00:00
- **Updated**: 2018-05-30 12:23:52+00:00
- **Authors**: Rui Wang, Xiao-Jun Wu, Kai-Xuan Chen, Josef Kittler
- **Comment**: 6 pages, 4 figures,ICPR 2018(accepted)
- **Journal**: None
- **Summary**: In image set classification, a considerable advance has been made by modeling the original image sets by second order statistics or linear subspace, which typically lie on the Riemannian manifold. Specifically, they are Symmetric Positive Definite (SPD) manifold and Grassmann manifold respectively, and some algorithms have been developed on them for classification tasks. Motivated by the inability of existing methods to extract discriminatory features for data on Riemannian manifolds, we propose a novel algorithm which combines multiple manifolds as the features of the original image sets. In order to fuse these manifolds, the well-studied Riemannian kernels have been utilized to map the original Riemannian spaces into high dimensional Hilbert spaces. A metric Learning method has been devised to embed these kernel spaces into a lower dimensional common subspace for classification. The state-of-the-art results achieved on three datasets corresponding to two different classification tasks, namely face recognition and object categorization, demonstrate the effectiveness of the proposed method.



### Learning to Generate Facial Depth Maps
- **Arxiv ID**: http://arxiv.org/abs/1805.11927v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.11927v1)
- **Published**: 2018-05-30 13:00:42+00:00
- **Updated**: 2018-05-30 13:00:42+00:00
- **Authors**: Stefano Pini, Filippo Grazioli, Guido Borghi, Roberto Vezzani, Rita Cucchiara
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, an adversarial architecture for facial depth map estimation from monocular intensity images is presented. By following an image-to-image approach, we combine the advantages of supervised learning and adversarial training, proposing a conditional Generative Adversarial Network that effectively learns to translate intensity face images into the corresponding depth maps. Two public datasets, namely Biwi database and Pandora dataset, are exploited to demonstrate that the proposed model generates high-quality synthetic depth images, both in terms of visual appearance and informative content. Furthermore, we show that the model is capable of predicting distinctive facial details by testing the generated depth maps through a deep model trained on authentic depth maps for the face verification task.



### Automatic Large-Scale Data Acquisition via Crowdsourcing for Crosswalk Classification: A Deep Learning Approach
- **Arxiv ID**: http://arxiv.org/abs/1805.11970v1
- **DOI**: 10.1016/J.CAG.2017.08.004
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1805.11970v1)
- **Published**: 2018-05-30 13:55:14+00:00
- **Updated**: 2018-05-30 13:55:14+00:00
- **Authors**: Rodrigo F. Berriel, Franco Schmidt Rossi, Alberto F. de Souza, Thiago Oliveira-Santos
- **Comment**: 13 pages, 13 figures, 3 videos, and GitHub with models
- **Journal**: Computers & Graphics, 2017, vol. 68, pp. 32-42
- **Summary**: Correctly identifying crosswalks is an essential task for the driving activity and mobility autonomy. Many crosswalk classification, detection and localization systems have been proposed in the literature over the years. These systems use different perspectives to tackle the crosswalk classification problem: satellite imagery, cockpit view (from the top of a car or behind the windshield), and pedestrian perspective. Most of the works in the literature are designed and evaluated using small and local datasets, i.e. datasets that present low diversity. Scaling to large datasets imposes a challenge for the annotation procedure. Moreover, there is still need for cross-database experiments in the literature because it is usually hard to collect the data in the same place and conditions of the final application. In this paper, we present a crosswalk classification system based on deep learning. For that, crowdsourcing platforms, such as OpenStreetMap and Google Street View, are exploited to enable automatic training via automatic acquisition and annotation of a large-scale database. Additionally, this work proposes a comparison study of models trained using fully-automatic data acquisition and annotation against models that were partially annotated. Cross-database experiments were also included in the experimentation to show that the proposed methods enable use with real world applications. Our results show that the model trained on the fully-automatic database achieved high overall accuracy (94.12%), and that a statistically significant improvement (to 96.30%) can be achieved by manually annotating a specific part of the database. Finally, the results of the cross-database experiments show that both models are robust to the many variations of image and scenarios, presenting a consistent behavior.



### Automatic generation of object shapes with desired functionalities
- **Arxiv ID**: http://arxiv.org/abs/1805.11984v2
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1805.11984v2)
- **Published**: 2018-05-30 14:00:40+00:00
- **Updated**: 2018-10-16 20:11:52+00:00
- **Authors**: Mihai Andries, Atabak Dehban, José Santos-Victor
- **Comment**: 12 pages, 9 figures, 28 references
- **Journal**: None
- **Summary**: 3D objects (artefacts) are made to fulfill functions. Designing an object often starts with defining a list of functionalities that it should provide, also known as functional requirements. Today, the design of 3D object models is still a slow and largely artisanal activity, with few Computer-Aided Design (CAD) tools existing to aid the exploration of the design solution space. To accelerate the design process, we introduce an algorithm for generating object shapes with desired functionalities. Following the concept of form follows function, we assume that existing object shapes were rationally chosen to provide desired functionalities. First, we use an artificial neural network to learn a function-to-form mapping by analysing a dataset of objects labeled with their functionalities. Then, we combine forms providing one or more desired functions, generating an object shape that is expected to provide all of them. Finally, we verify in simulation whether the generated object possesses the desired functionalities, by defining and executing functionality tests on it.



### Generalizing to Unseen Domains via Adversarial Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/1805.12018v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.12018v2)
- **Published**: 2018-05-30 15:03:27+00:00
- **Updated**: 2018-11-06 16:51:32+00:00
- **Authors**: Riccardo Volpi, Hongseok Namkoong, Ozan Sener, John Duchi, Vittorio Murino, Silvio Savarese
- **Comment**: Accepted to NIPS 2018 (camera ready)
- **Journal**: None
- **Summary**: We are concerned with learning models that generalize well to different \emph{unseen} domains. We consider a worst-case formulation over data distributions that are near the source domain in the feature space. Only using training data from a single source distribution, we propose an iterative procedure that augments the dataset with examples from a fictitious target domain that is "hard" under the current model. We show that our iterative scheme is an adaptive data augmentation method where we append adversarial examples at each iteration. For softmax losses, we show that our method is a data-dependent regularization scheme that behaves differently from classical regularizers that regularize towards zero (e.g., ridge or lasso). On digit recognition and semantic segmentation tasks, our method learns models improve performance across a range of a priori unknown target domains.



### Privacy Aware Offloading of Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1805.12024v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.12024v1)
- **Published**: 2018-05-30 15:10:20+00:00
- **Updated**: 2018-05-30 15:10:20+00:00
- **Authors**: Sam Leroux, Tim Verbelen, Pieter Simoens, Bart Dhoedt
- **Comment**: ICML 2018 Privacy in Machine Learning and Artificial Intelligence
  workshop
- **Journal**: None
- **Summary**: Deep neural networks require large amounts of resources which makes them hard to use on resource constrained devices such as Internet-of-things devices. Offloading the computations to the cloud can circumvent these constraints but introduces a privacy risk since the operator of the cloud is not necessarily trustworthy. We propose a technique that obfuscates the data before sending it to the remote computation node. The obfuscated data is unintelligible for a human eavesdropper but can still be classified with a high accuracy by a neural network trained on unobfuscated images.



### Robust Place Categorization with Deep Domain Generalization
- **Arxiv ID**: http://arxiv.org/abs/1805.12048v1
- **DOI**: 10.1109/LRA.2018.2809700
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1805.12048v1)
- **Published**: 2018-05-30 16:00:34+00:00
- **Updated**: 2018-05-30 16:00:34+00:00
- **Authors**: Massimiliano Mancini, Samuel Rota Bulò, Barbara Caputo, Elisa Ricci
- **Comment**: None
- **Journal**: None
- **Summary**: Traditional place categorization approaches in robot vision assume that training and test images have similar visual appearance. Therefore, any seasonal, illumination and environmental changes typically lead to severe degradation in performance. To cope with this problem, recent works have proposed to adopt domain adaptation techniques. While effective, these methods assume that some prior information about the scenario where the robot will operate is available at training time. Unfortunately, in many cases this assumption does not hold, as we often do not know where a robot will be deployed. To overcome this issue, in this paper we present an approach which aims at learning classification models able to generalize to unseen scenarios. Specifically, we propose a novel deep learning framework for domain generalization. Our method develops from the intuition that, given a set of different classification models associated to known domains (e.g. corresponding to multiple environments, robots), the best model for a new sample in the novel domain can be computed directly at test time by optimally combining the known models. To implement our idea, we exploit recent advances in deep domain adaptation and design a Convolutional Neural Network architecture with novel layers performing a weighted version of Batch Normalization. Our experiments, conducted on three common datasets for robot place categorization, confirm the validity of our contribution.



### Stochastic Deep Compressive Sensing for the Reconstruction of Diffusion Tensor Cardiac MRI
- **Arxiv ID**: http://arxiv.org/abs/1805.12064v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.12064v1)
- **Published**: 2018-05-30 16:31:04+00:00
- **Updated**: 2018-05-30 16:31:04+00:00
- **Authors**: Jo Schlemper, Guang Yang, Pedro Ferreira, Andrew Scott, Laura-Ann McGill, Zohya Khalique, Margarita Gorodezky, Malte Roehl, Jennifer Keegan, Dudley Pennell, David Firmin, Daniel Rueckert
- **Comment**: Accepted for MICCAI 2018
- **Journal**: None
- **Summary**: Understanding the structure of the heart at the microscopic scale of cardiomyocytes and their aggregates provides new insights into the mechanisms of heart disease and enables the investigation of effective therapeutics. Diffusion Tensor Cardiac Magnetic Resonance (DT-CMR) is a unique non-invasive technique that can resolve the microscopic structure, organisation, and integrity of the myocardium without the need for exogenous contrast agents. However, this technique suffers from relatively low signal-to-noise ratio (SNR) and frequent signal loss due to respiratory and cardiac motion. Current DT-CMR techniques rely on acquiring and averaging multiple signal acquisitions to improve the SNR. Moreover, in order to mitigate the influence of respiratory movement, patients are required to perform many breath holds which results in prolonged acquisition durations (e.g., ~30 mins using the existing technology). In this study, we propose a novel cascaded Convolutional Neural Networks (CNN) based compressive sensing (CS) technique and explore its applicability to improve DT-CMR acquisitions. Our simulation based studies have achieved high reconstruction fidelity and good agreement between DT-CMR parameters obtained with the proposed reconstruction and fully sampled ground truth. When compared to other state-of-the-art methods, our proposed deep cascaded CNN method and its stochastic variation demonstrated significant improvements. To the best of our knowledge, this is the first study using deep CNN based CS for the DT-CMR reconstruction. In addition, with relatively straightforward modifications to the acquisition scheme, our method can easily be translated into a method for online, at-the-scanner reconstruction enabling the deployment of accelerated DT-CMR in various clinical applications.



### A Robust and Effective Approach Towards Accurate Metastasis Detection and pN-stage Classification in Breast Cancer
- **Arxiv ID**: http://arxiv.org/abs/1805.12067v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.12067v1)
- **Published**: 2018-05-30 16:33:37+00:00
- **Updated**: 2018-05-30 16:33:37+00:00
- **Authors**: Byungjae Lee, Kyunghyun Paeng
- **Comment**: Accepted at MICCAI 2018
- **Journal**: None
- **Summary**: Predicting TNM stage is the major determinant of breast cancer prognosis and treatment. The essential part of TNM stage classification is whether the cancer has metastasized to the regional lymph nodes (N-stage). Pathologic N-stage (pN-stage) is commonly performed by pathologists detecting metastasis in histological slides. However, this diagnostic procedure is prone to misinterpretation and would normally require extensive time by pathologists because of the sheer volume of data that needs a thorough review. Automated detection of lymph node metastasis and pN-stage prediction has a great potential to reduce their workload and help the pathologist. Recent advances in convolutional neural networks (CNN) have shown significant improvements in histological slide analysis, but accuracy is not optimized because of the difficulty in the handling of gigapixel images. In this paper, we propose a robust method for metastasis detection and pN-stage classification in breast cancer from multiple gigapixel pathology images in an effective way. pN-stage is predicted by combining patch-level CNN based metastasis detector and slide-level lymph node classifier. The proposed framework achieves a state-of-the-art quadratic weighted kappa score of 0.9203 on the Camelyon17 dataset, outperforming the previous winning method of the Camelyon17 challenge.



### Automatic, fast and robust characterization of noise distributions for diffusion MRI
- **Arxiv ID**: http://arxiv.org/abs/1805.12071v2
- **DOI**: 10.1007/978-3-030-00928-1_35
- **Categories**: **cs.CV**, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/1805.12071v2)
- **Published**: 2018-05-30 16:38:25+00:00
- **Updated**: 2018-10-02 10:10:48+00:00
- **Authors**: Samuel St-Jean, Alberto De Luca, Max A. Viergever, Alexander Leemans
- **Comment**: v2: added publisher DOI statement, fixed text typo in appendix A2
- **Journal**: St-Jean S. et al. (2018) Automatic, Fast and Robust
  Characterization of Noise Distributions for Diffusion MRI. In: Medical Image
  Computing and Computer Assisted Intervention - MICCAI 2018. LNCS, vol 11070.
  Springer, Cham
- **Summary**: Knowledge of the noise distribution in magnitude diffusion MRI images is the centerpiece to quantify uncertainties arising from the acquisition process. The use of parallel imaging methods, the number of receiver coils and imaging filters applied by the scanner, amongst other factors, dictate the resulting signal distribution. Accurate estimation beyond textbook Rician or noncentral chi distributions often requires information about the acquisition process (e.g. coils sensitivity maps or reconstruction coefficients), which is not usually available. We introduce a new method where a change of variable naturally gives rise to a particular form of the gamma distribution for background signals. The first moments and maximum likelihood estimators of this gamma distribution explicitly depend on the number of coils, making it possible to estimate all unknown parameters using only the magnitude data. A rejection step is used to make the method automatic and robust to artifacts. Experiments on synthetic datasets show that the proposed method can reliably estimate both the degrees of freedom and the standard deviation. The worst case errors range from below 2% (spatially uniform noise) to approximately 10% (spatially variable noise). Repeated acquisitions of in vivo datasets show that the estimated parameters are stable and have lower variances than compared methods.



### CuisineNet: Food Attributes Classification using Multi-scale Convolution Network
- **Arxiv ID**: http://arxiv.org/abs/1805.12081v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.12081v2)
- **Published**: 2018-05-30 16:56:32+00:00
- **Updated**: 2018-06-08 16:44:15+00:00
- **Authors**: Md. Mostafa Kamal Sarker, Mohammed Jabreel, Hatem A. Rashwan, Syeda Furruka Banu, Antonio Moreno, Petia Radeva, Domenec Puig
- **Comment**: 8 pages, Submitted in CCIA 2018
- **Journal**: None
- **Summary**: Diversity of food and its attributes represents the culinary habits of peoples from different countries. Thus, this paper addresses the problem of identifying food culture of people around the world and its flavor by classifying two main food attributes, cuisine and flavor. A deep learning model based on multi-scale convotuional networks is proposed for extracting more accurate features from input images. The aggregation of multi-scale convolution layers with different kernel size is also used for weighting the features results from different scales. In addition, a joint loss function based on Negative Log Likelihood (NLL) is used to fit the model probability to multi labeled classes for multi-modal classification task. Furthermore, this work provides a new dataset for food attributes, so-called Yummly48K, extracted from the popular food website, Yummly. Our model is assessed on the constructed Yummly48K dataset. The experimental results show that our proposed method yields 65% and 62% average F1 score on validation and test set which outperforming the state-of-the-art models.



### Context-aware Cascade Attention-based RNN for Video Emotion Recognition
- **Arxiv ID**: http://arxiv.org/abs/1805.12098v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.12098v1)
- **Published**: 2018-05-30 17:31:46+00:00
- **Updated**: 2018-05-30 17:31:46+00:00
- **Authors**: Man-Chin Sun, Shih-Huan Hsu, Min-Chun Yang, Jen-Hsien Chien
- **Comment**: None
- **Journal**: None
- **Summary**: Emotion recognition can provide crucial information about the user in many applications when building human-computer interaction (HCI) systems. Most of current researches on visual emotion recognition are focusing on exploring facial features. However, context information including surrounding environment and human body can also provide extra clues to recognize emotion more accurately. Inspired by "sequence to sequence model" for neural machine translation, which models input and output sequences by an encoder and a decoder in recurrent neural network (RNN) architecture respectively, a novel architecture, "CACA-RNN", is proposed in this work. The proposed network consists of two RNNs in a cascaded architecture to process both context and facial information to perform video emotion classification. Results of the model were submitted to video emotion recognition sub-challenge in Multimodal Emotion Recognition Challenge (MEC2017). CACA-RNN outperforms the MEC2017 baseline (mAP of 21.7%): it achieved mAP of 45.51% on the testing set in the video only challenge.



### On Consensus-Optimality Trade-offs in Collaborative Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1805.12120v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.DC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1805.12120v1)
- **Published**: 2018-05-30 17:59:24+00:00
- **Updated**: 2018-05-30 17:59:24+00:00
- **Authors**: Zhanhong Jiang, Aditya Balu, Chinmay Hegde, Soumik Sarkar
- **Comment**: None
- **Journal**: None
- **Summary**: In distributed machine learning, where agents collaboratively learn from diverse private data sets, there is a fundamental tension between consensus and optimality. In this paper, we build on recent algorithmic progresses in distributed deep learning to explore various consensus-optimality trade-offs over a fixed communication topology. First, we propose the incremental consensus-based distributed SGD (i-CDSGD) algorithm, which involves multiple consensus steps (where each agent communicates information with its neighbors) within each SGD iteration. Second, we propose the generalized consensus-based distributed SGD (g-CDSGD) algorithm that enables us to navigate the full spectrum from complete consensus (all agents agree) to complete disagreement (each agent converges to individual model parameters). We analytically establish convergence of the proposed algorithms for strongly convex and nonconvex objective functions; we also analyze the momentum variants of the algorithms for the strongly convex case. We support our algorithms via numerical experiments, and demonstrate significant improvements over existing methods for collaborative deep learning.



### Robustness May Be at Odds with Accuracy
- **Arxiv ID**: http://arxiv.org/abs/1805.12152v5
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1805.12152v5)
- **Published**: 2018-05-30 18:00:32+00:00
- **Updated**: 2019-09-09 08:09:25+00:00
- **Authors**: Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, Aleksander Madry
- **Comment**: ICLR'19
- **Journal**: None
- **Summary**: We show that there may exist an inherent tension between the goal of adversarial robustness and that of standard generalization. Specifically, training robust models may not only be more resource-consuming, but also lead to a reduction of standard accuracy. We demonstrate that this trade-off between the standard accuracy of a model and its robustness to adversarial perturbations provably exists in a fairly simple and natural setting. These findings also corroborate a similar phenomenon observed empirically in more complex settings. Further, we argue that this phenomenon is a consequence of robust classifiers learning fundamentally different feature representations than standard classifiers. These differences, in particular, seem to result in unexpected benefits: the representations learned by robust models tend to align better with salient data characteristics and human perception.



### Supervised Mixed Norm Autoencoder for Kinship Verification in Unconstrained Videos
- **Arxiv ID**: http://arxiv.org/abs/1805.12167v1
- **DOI**: 10.1109/TIP.2018.2840880
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.12167v1)
- **Published**: 2018-05-30 18:27:05+00:00
- **Updated**: 2018-05-30 18:27:05+00:00
- **Authors**: Naman Kohli, Daksha Yadav, Mayank Vatsa, Richa Singh, Afzel Noore
- **Comment**: Accepted for publication in Transactions in Image Processing
- **Journal**: None
- **Summary**: Identifying kinship relations has garnered interest due to several applications such as organizing and tagging the enormous amount of videos being uploaded on the Internet. Existing research in kinship verification primarily focuses on kinship prediction with image pairs. In this research, we propose a new deep learning framework for kinship verification in unconstrained videos using a novel Supervised Mixed Norm regularization Autoencoder (SMNAE). This new autoencoder formulation introduces class-specific sparsity in the weight matrix. The proposed three-stage SMNAE based kinship verification framework utilizes the learned spatio-temporal representation in the video frames for verifying kinship in a pair of videos. A new kinship video (KIVI) database of more than 500 individuals with variations due to illumination, pose, occlusion, ethnicity, and expression is collected for this research. It comprises a total of 355 true kin video pairs with over 250,000 still frames. The effectiveness of the proposed framework is demonstrated on the KIVI database and six existing kinship databases. On the KIVI database, SMNAE yields video-based kinship verification accuracy of 83.18% which is at least 3.2% better than existing algorithms. The algorithm is also evaluated on six publicly available kinship databases and compared with best-reported results. It is observed that the proposed SMNAE consistently yields best results on all the databases



### Video Summarization by Learning from Unpaired Data
- **Arxiv ID**: http://arxiv.org/abs/1805.12174v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.12174v2)
- **Published**: 2018-05-30 18:48:25+00:00
- **Updated**: 2019-04-08 20:50:01+00:00
- **Authors**: Mrigank Rochan, Yang Wang
- **Comment**: Accepted to CVPR 2019
- **Journal**: None
- **Summary**: We consider the problem of video summarization. Given an input raw video, the goal is to select a small subset of key frames from the input video to create a shorter summary video that best describes the content of the original video. Most of the current state-of-the-art video summarization approaches use supervised learning and require labeled training data. Each training instance consists of a raw input video and its ground truth summary video curated by human annotators. However, it is very expensive and difficult to create such labeled training examples. To address this limitation, we propose a novel formulation to learn video summarization from unpaired data. We present an approach that learns to generate optimal video summaries using a set of raw videos ($V$) and a set of summary videos ($S$), where there exists no correspondence between $V$ and $S$. We argue that this type of data is much easier to collect. Our model aims to learn a mapping function $F : V \rightarrow S$ such that the distribution of resultant summary videos from $F(V)$ is similar to the distribution of $S$ with the help of an adversarial objective. In addition, we enforce a diversity constraint on $F(V)$ to ensure that the generated video summaries are visually diverse. Experimental results on two benchmark datasets indicate that our proposed approach significantly outperforms other alternative methods.



### Why do deep convolutional networks generalize so poorly to small image transformations?
- **Arxiv ID**: http://arxiv.org/abs/1805.12177v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.12177v4)
- **Published**: 2018-05-30 18:56:33+00:00
- **Updated**: 2019-12-31 13:40:12+00:00
- **Authors**: Aharon Azulay, Yair Weiss
- **Comment**: None
- **Journal**: JMLR 20(184) 1-25 2019
- **Summary**: Convolutional Neural Networks (CNNs) are commonly assumed to be invariant to small image transformations: either because of the convolutional architecture or because they were trained using data augmentation. Recently, several authors have shown that this is not the case: small translations or rescalings of the input image can drastically change the network's prediction. In this paper, we quantify this phenomena and ask why neither the convolutional architecture nor data augmentation are sufficient to achieve the desired invariance. Specifically, we show that the convolutional architecture does not give invariance since architectures ignore the classical sampling theorem, and data augmentation does not give invariance because the CNNs learn to be invariant to transformations only for images that are very similar to typical images from the training set. We discuss two possible solutions to this problem: (1) antialiasing the intermediate representations and (2) increasing data augmentation and show that they provide only a partial solution at best. Taken together, our results indicate that the problem of insuring invariance to small image transformations in neural networks while preserving high accuracy remains unsolved.



### Context Exploitation using Hierarchical Bayesian Models
- **Arxiv ID**: http://arxiv.org/abs/1805.12183v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1805.12183v1)
- **Published**: 2018-05-30 19:09:11+00:00
- **Updated**: 2018-05-30 19:09:11+00:00
- **Authors**: Christopher A. George, Pranab Banerjee, Kendra E. Moore
- **Comment**: 4 pages; 3 figures; 5 tables
- **Journal**: Proceedings of the National Fire Control Symposium, February 2018
- **Summary**: We consider the problem of how to improve automatic target recognition by fusing the naive sensor-level classification decisions with "intuition," or context, in a mathematically principled way. This is a general approach that is compatible with many definitions of context, but for specificity, we consider context as co-occurrence in imagery. In particular, we consider images that contain multiple objects identified at various confidence levels. We learn the patterns of co-occurrence in each context, then use these patterns as hyper-parameters for a Hierarchical Bayesian Model. The result is that low-confidence sensor classification decisions can be dramatically improved by fusing those readings with context. We further use hyperpriors to address the case where multiple contexts may be appropriate. We also consider the Bayesian Network, an alternative to the Hierarchical Bayesian Model, which is computationally more efficient but assumes that context and sensor readings are uncorrelated.



### Tiling and Stitching Segmentation Output for Remote Sensing: Basic Challenges and Recommendations
- **Arxiv ID**: http://arxiv.org/abs/1805.12219v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.12219v3)
- **Published**: 2018-05-30 20:34:07+00:00
- **Updated**: 2019-02-25 14:44:09+00:00
- **Authors**: Bohao Huang, Daniel Reichman, Leslie M. Collins, Kyle Bradbury, Jordan M. Malof
- **Comment**: None
- **Journal**: None
- **Summary**: In this work we consider the application of convolutional neural networks (CNNs) for pixel-wise labeling (a.k.a., semantic segmentation) of remote sensing imagery (e.g., aerial color or hyperspectral imagery). Remote sensing imagery is usually stored in the form of very large images, referred to as "tiles", which are too large to be segmented directly using most CNNs and their associated hardware. As a result, during label inference, smaller sub-images, called "patches", are processed individually and then "stitched" (concatenated) back together to create a tile-sized label map. This approach suffers from computational ineffiency and can result in discontinuities at output boundaries. We propose a simple alternative approach in which the input size of the CNN is dramatically increased only during label inference. This does not avoid stitching altogether, but substantially mitigates its limitations. We evaluate the performance of the proposed approach against a vonventional stitching approach using two popular segmentation CNN models and two large-scale remote sensing imagery datasets. The results suggest that the proposed approach substantially reduces label inference time, while also yielding modest overall label accuracy increases. This approach contributed to our wining entry (overall performance) in the INRIA building labeling competition.



### Collaborative Human-AI (CHAI): Evidence-Based Interpretable Melanoma Classification in Dermoscopic Images
- **Arxiv ID**: http://arxiv.org/abs/1805.12234v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.12234v3)
- **Published**: 2018-05-30 21:28:39+00:00
- **Updated**: 2018-08-01 18:01:48+00:00
- **Authors**: Noel C. F. Codella, Chung-Ching Lin, Allan Halpern, Michael Hind, Rogerio Feris, John R. Smith
- **Comment**: Presented at MICCAI 2018, Workshop on Interpretability of Machine
  Intelligence in Medical Image Computing (IMIMIC): https://imimic.bitbucket.io
- **Journal**: None
- **Summary**: Automated dermoscopic image analysis has witnessed rapid growth in diagnostic performance. Yet adoption faces resistance, in part, because no evidence is provided to support decisions. In this work, an approach for evidence-based classification is presented. A feature embedding is learned with CNNs, triplet-loss, and global average pooling, and used to classify via kNN search. Evidence is provided as both the discovered neighbors, as well as localized image regions most relevant to measuring distance between query and neighbors. To ensure that results are relevant in terms of both label accuracy and human visual similarity for any skill level, a novel hierarchical triplet logic is implemented to jointly learn an embedding according to disease labels and non-expert similarity. Results are improved over baselines trained on disease labels alone, as well as standard multiclass loss. Quantitative relevance of results, according to non-expert similarity, as well as localized image regions, are also significantly improved.



### Novel Video Prediction for Large-scale Scene using Optical Flow
- **Arxiv ID**: http://arxiv.org/abs/1805.12243v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.12243v1)
- **Published**: 2018-05-30 22:11:54+00:00
- **Updated**: 2018-05-30 22:11:54+00:00
- **Authors**: Henglai Wei, Xiaochuan Yin, Penghong Lin
- **Comment**: None
- **Journal**: None
- **Summary**: Making predictions of future frames is a critical challenge in autonomous driving research. Most of the existing methods for video prediction attempt to generate future frames in simple and fixed scenes. In this paper, we propose a novel and effective optical flow conditioned method for the task of video prediction with an application to complex urban scenes. In contrast with previous work, the prediction model only requires video sequences and optical flow sequences for training and testing. Our method uses the rich spatial-temporal features in video sequences. The method takes advantage of the motion information extracting from optical flow maps between neighbor images as well as previous images. Empirical evaluations on the KITTI dataset and the Cityscapes dataset demonstrate the effectiveness of our method.



### Multi-level 3D CNN for Learning Multi-scale Spatial Features
- **Arxiv ID**: http://arxiv.org/abs/1805.12254v2
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.12254v2)
- **Published**: 2018-05-30 22:53:28+00:00
- **Updated**: 2019-05-03 18:37:27+00:00
- **Authors**: Sambit Ghadai, Xian Lee, Aditya Balu, Soumik Sarkar, Adarsh Krishnamurthy
- **Comment**: CVPR 2019 workshop on Deep Learning for Geometric Shape Understanding
- **Journal**: None
- **Summary**: 3D object recognition accuracy can be improved by learning the multi-scale spatial features from 3D spatial geometric representations of objects such as point clouds, 3D models, surfaces, and RGB-D data. Current deep learning approaches learn such features either using structured data representations (voxel grids and octrees) or from unstructured representations (graphs and point clouds). Learning features from such structured representations is limited by the restriction on resolution and tree depth while unstructured representations creates a challenge due to non-uniformity among data samples. In this paper, we propose an end-to-end multi-level learning approach on a multi-level voxel grid to overcome these drawbacks. To demonstrate the utility of the proposed multi-level learning, we use a multi-level voxel representation of 3D objects to perform object recognition. The multi-level voxel representation consists of a coarse voxel grid that contains volumetric information of the 3D object. In addition, each voxel in the coarse grid that contains a portion of the object boundary is subdivided into multiple fine-level voxel grids. The performance of our multi-level learning algorithm for object recognition is comparable to dense voxel representations while using significantly lower memory.



### Rehabilitating the ColorChecker Dataset for Illuminant Estimation
- **Arxiv ID**: http://arxiv.org/abs/1805.12262v3
- **DOI**: 10.2352/ISSN.2169-2629.2018.26.350
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.12262v3)
- **Published**: 2018-05-30 23:41:17+00:00
- **Updated**: 2018-09-17 16:53:27+00:00
- **Authors**: Ghalia Hemrit, Graham D. Finlayson, Arjan Gijsenij, Peter Gehler, Simone Bianco, Brian Funt, Mark Drew, Lilong Shi
- **Comment**: 4 pages, 3 figures, 2 tables, Proceedings of the 26th Color and
  Imaging Conference
- **Journal**: Color and Imaging Conference, 2018
- **Summary**: In a previous work, it was shown that there is a curious problem with the benchmark ColorChecker dataset for illuminant estimation. To wit, this dataset has at least 3 different sets of ground-truths. Typically, for a single algorithm a single ground-truth is used. But then different algorithms, whose performance is measured with respect to different ground-truths, are compared against each other and then ranked. This makes no sense. We show in this paper that there are also errors in how each ground-truth set was calculated. As a result, all performance rankings based on the ColorChecker dataset - and there are scores of these - are inaccurate.   In this paper, we re-generate a new 'recommended' set of ground-truth based on the calculation methodology described by Shi and Funt. We then review the performance evaluation of a range of illuminant estimation algorithms. Compared with the legacy ground-truths, we find that the difference in how algorithms perform can be large, with many local rankings of algorithms being reversed.   Finally, we draw the readers attention to our new 'open' data repository which, we hope, will allow the ColorChecker set to be rehabilitated and once again to become a useful benchmark for illuminant estimation algorithms.



