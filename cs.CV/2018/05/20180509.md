# Arxiv Papers in cs.CV on 2018-05-09
### Attention-Aware Compositional Network for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1805.03344v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.03344v2)
- **Published**: 2018-05-09 01:43:10+00:00
- **Updated**: 2018-05-16 08:20:51+00:00
- **Authors**: Jing Xu, Rui Zhao, Feng Zhu, Huaming Wang, Wanli Ouyang
- **Comment**: Accepted at CVPR2018
- **Journal**: None
- **Summary**: Person re-identification (ReID) is to identify pedestrians observed from different camera views based on visual appearance. It is a challenging task due to large pose variations, complex background clutters and severe occlusions. Recently, human pose estimation by predicting joint locations was largely improved in accuracy. It is reasonable to use pose estimation results for handling pose variations and background clutters, and such attempts have obtained great improvement in ReID performance. However, we argue that the pose information was not well utilized and hasn't yet been fully exploited for person ReID.   In this work, we introduce a novel framework called Attention-Aware Compositional Network (AACN) for person ReID. AACN consists of two main components: Pose-guided Part Attention (PPA) and Attention-aware Feature Composition (AFC). PPA is learned and applied to mask out undesirable background features in pedestrian feature maps. Furthermore, pose-guided visibility scores are estimated for body parts to deal with part occlusion in the proposed AFC module. Extensive experiments with ablation analysis show the effectiveness of our method, and state-of-the-art results are achieved on several public datasets, including Market-1501, CUHK03, CUHK01, SenseReID, CUHK03-NP and DukeMTMC-reID.



### A Memristor based Unsupervised Neuromorphic System Towards Fast and Energy-Efficient GAN
- **Arxiv ID**: http://arxiv.org/abs/1806.01775v4
- **DOI**: None
- **Categories**: **cs.DC**, cs.CV, cs.ET, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1806.01775v4)
- **Published**: 2018-05-09 02:45:38+00:00
- **Updated**: 2019-09-08 08:46:12+00:00
- **Authors**: F. Liu, C. Liu, F. Bi
- **Comment**: 8 pages, 14 figures
- **Journal**: None
- **Summary**: Deep Learning has gained immense success in pushing today's artificial intelligence forward. To solve the challenge of limited labeled data in the supervised learning world, unsupervised learning has been proposed years ago while low accuracy hinters its realistic applications. Generative adversarial network (GAN) emerges as an unsupervised learning approach with promising accuracy and are under extensively study. However, the execution of GAN is extremely memory and computation intensive and results in ultra-low speed and high-power consumption. In this work, we proposed a holistic solution for fast and energy-efficient GAN computation through a memristor-based neuromorphic system. First, we exploited a hardware and software co-design approach to map the computation blocks in GAN efficiently. We also proposed an efficient data flow for optimal parallelism training and testing, depending on the computation correlations between different computing blocks. To compute the unique and complex loss of GAN, we developed a diff-block with optimized accuracy and performance. The experiment results on big data show that our design achieves 2.8x speedup and 6.1x energy-saving compared with the traditional GPU accelerator, as well as 5.5x speedup and 1.4x energy-saving compared with the previous FPGA-based accelerator.



### SPG-Net: Segmentation Prediction and Guidance Network for Image Inpainting
- **Arxiv ID**: http://arxiv.org/abs/1805.03356v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.03356v4)
- **Published**: 2018-05-09 03:02:06+00:00
- **Updated**: 2018-08-06 19:53:18+00:00
- **Authors**: Yuhang Song, Chao Yang, Yeji Shen, Peng Wang, Qin Huang, C. -C. Jay Kuo
- **Comment**: BMVC 2018 camera ready
- **Journal**: None
- **Summary**: In this paper, we focus on image inpainting task, aiming at recovering the missing area of an incomplete image given the context information. Recent development in deep generative models enables an efficient end-to-end framework for image synthesis and inpainting tasks, but existing methods based on generative models don't exploit the segmentation information to constrain the object shapes, which usually lead to blurry results on the boundary. To tackle this problem, we propose to introduce the semantic segmentation information, which disentangles the inter-class difference and intra-class variation for image inpainting. This leads to much clearer recovered boundary between semantically different regions and better texture within semantically consistent segments. Our model factorizes the image inpainting process into segmentation prediction (SP-Net) and segmentation guidance (SG-Net) as two steps, which predict the segmentation labels in the missing area first, and then generate segmentation guided inpainting results. Experiments on multiple public datasets show that our approach outperforms existing methods in optimizing the image inpainting quality, and the interactive segmentation guidance provides possibilities for multi-modal predictions of image inpainting.



### Anchor Cascade for Efficient Face Detection
- **Arxiv ID**: http://arxiv.org/abs/1805.03363v1
- **DOI**: 10.1109/TIP.2018.2886790
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.03363v1)
- **Published**: 2018-05-09 03:46:35+00:00
- **Updated**: 2018-05-09 03:46:35+00:00
- **Authors**: Baosheng Yu, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: Face detection is essential to facial analysis tasks such as facial reenactment and face recognition. Both cascade face detectors and anchor-based face detectors have translated shining demos into practice and received intensive attention from the community. However, cascade face detectors often suffer from a low detection accuracy, while anchor-based face detectors rely heavily on very large networks pre-trained on large scale image classification datasets such as ImageNet [1], which is not computationally efficient for both training and deployment. In this paper, we devise an efficient anchor-based cascade framework called anchor cascade. To improve the detection accuracy by exploring contextual information, we further propose a context pyramid maxout mechanism for anchor cascade. As a result, anchor cascade can train very efficient face detection models with a high detection accuracy. Specifically, comparing with a popular CNN-based cascade face detector MTCNN [2], our anchor cascade face detector greatly improves the detection accuracy, e.g., from 0.9435 to 0.9704 at 1k false positives on FDDB, while it still runs in comparable speed. Experimental results on two widely used face detection benchmarks, FDDB and WIDER FACE, demonstrate the effectiveness of the proposed framework.



### PSGAN: A Generative Adversarial Network for Remote Sensing Image Pan-Sharpening
- **Arxiv ID**: http://arxiv.org/abs/1805.03371v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.03371v4)
- **Published**: 2018-05-09 04:58:57+00:00
- **Updated**: 2020-12-20 15:57:25+00:00
- **Authors**: Qingjie Liu, Huanyu Zhou, Qizhi Xu, Xiangyu Liu, Yunhong Wang
- **Comment**: Accepted to TGRS
- **Journal**: None
- **Summary**: This paper addresses the problem of remote sensing image pan-sharpening from the perspective of generative adversarial learning. We propose a novel deep neural network based method named PSGAN. To the best of our knowledge, this is one of the first attempts at producing high-quality pan-sharpened images with GANs. The PSGAN consists of two components: a generative network (i.e., generator) and a discriminative network (i.e., discriminator). The generator is designed to accept panchromatic (PAN) and multispectral (MS) images as inputs and maps them to the desired high-resolution (HR) MS images and the discriminator implements the adversarial training strategy for generating higher fidelity pan-sharpened images. In this paper, we evaluate several architectures and designs, namely two-stream input, stacking input, batch normalization layer, and attention mechanism to find the optimal solution for pan-sharpening. Extensive experiments on QuickBird, GaoFen-2, and WorldView-2 satellite images demonstrate that the proposed PSGANs not only are effective in generating high-quality HR MS images and superior to state-of-the-art methods and also generalize well to full-scale images.



### New Techniques for Preserving Global Structure and Denoising with Low Information Loss in Single-Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1805.03383v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.03383v2)
- **Published**: 2018-05-09 05:59:10+00:00
- **Updated**: 2018-06-16 00:19:44+00:00
- **Authors**: Yijie Bei, Alex Damian, Shijia Hu, Sachit Menon, Nikhil Ravi, Cynthia Rudin
- **Comment**: 8 pages, CVPR workshop 2018
- **Journal**: None
- **Summary**: This work identifies and addresses two important technical challenges in single-image super-resolution: (1) how to upsample an image without magnifying noise and (2) how to preserve large scale structure when upsampling. We summarize the techniques we developed for our second place entry in Track 1 (Bicubic Downsampling), seventh place entry in Track 2 (Realistic Adverse Conditions), and seventh place entry in Track 3 (Realistic difficult) in the 2018 NTIRE Super-Resolution Challenge. Furthermore, we present new neural network architectures that specifically address the two challenges listed above: denoising and preservation of large-scale structure.



### Edit Probability for Scene Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/1805.03384v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.03384v1)
- **Published**: 2018-05-09 06:14:51+00:00
- **Updated**: 2018-05-09 06:14:51+00:00
- **Authors**: Fan Bai, Zhanzhan Cheng, Yi Niu, Shiliang Pu, Shuigeng Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: We consider the scene text recognition problem under the attention-based encoder-decoder framework, which is the state of the art. The existing methods usually employ a frame-wise maximal likelihood loss to optimize the models. When we train the model, the misalignment between the ground truth strings and the attention's output sequences of probability distribution, which is caused by missing or superfluous characters, will confuse and mislead the training process, and consequently make the training costly and degrade the recognition accuracy. To handle this problem, we propose a novel method called edit probability (EP) for scene text recognition. EP tries to effectively estimate the probability of generating a string from the output sequence of probability distribution conditioned on the input image, while considering the possible occurrences of missing/superfluous characters. The advantage lies in that the training process can focus on the missing, superfluous and unrecognized characters, and thus the impact of the misalignment problem can be alleviated or even overcome. We conduct extensive experiments on standard benchmarks, including the IIIT-5K, Street View Text and ICDAR datasets. Experimental results show that the EP can substantially boost scene text recognition performance.



### Deep Directional Statistics: Pose Estimation with Uncertainty Quantification
- **Arxiv ID**: http://arxiv.org/abs/1805.03430v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.03430v1)
- **Published**: 2018-05-09 09:22:09+00:00
- **Updated**: 2018-05-09 09:22:09+00:00
- **Authors**: Sergey Prokudin, Peter Gehler, Sebastian Nowozin
- **Comment**: None
- **Journal**: None
- **Summary**: Modern deep learning systems successfully solve many perception tasks such as object pose estimation when the input image is of high quality. However, in challenging imaging conditions such as on low-resolution images or when the image is corrupted by imaging artifacts, current systems degrade considerably in accuracy. While a loss in performance is unavoidable, we would like our models to quantify their uncertainty in order to achieve robustness against images of varying quality. Probabilistic deep learning models combine the expressive power of deep learning with uncertainty quantification. In this paper, we propose a novel probabilistic deep learning model for the task of angular regression. Our model uses von Mises distributions to predict a distribution over object pose angle. Whereas a single von Mises distribution is making strong assumptions about the shape of the distribution, we extend the basic model to predict a mixture of von Mises distributions. We show how to learn a mixture model using a finite and infinite number of mixture components. Our model allows for likelihood-based training and efficient inference at test time. We demonstrate on a number of challenging pose estimation datasets that our model produces calibrated probability predictions and competitive or superior point estimates compared to the current state-of-the-art.



### Robust Classification with Convolutional Prototype Learning
- **Arxiv ID**: http://arxiv.org/abs/1805.03438v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.03438v1)
- **Published**: 2018-05-09 09:55:45+00:00
- **Updated**: 2018-05-09 09:55:45+00:00
- **Authors**: Hong-Ming Yang, Xu-Yao Zhang, Fei Yin, Cheng-Lin Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have been widely used for image classification. Despite its high accuracies, CNN has been shown to be easily fooled by some adversarial examples, indicating that CNN is not robust enough for pattern classification. In this paper, we argue that the lack of robustness for CNN is caused by the softmax layer, which is a totally discriminative model and based on the assumption of closed world (i.e., with a fixed number of categories). To improve the robustness, we propose a novel learning framework called convolutional prototype learning (CPL). The advantage of using prototypes is that it can well handle the open world recognition problem and therefore improve the robustness. Under the framework of CPL, we design multiple classification criteria to train the network. Moreover, a prototype loss (PL) is proposed as a regularization to improve the intra-class compactness of the feature representation, which can be viewed as a generative model based on the Gaussian assumption of different classes. Experiments on several datasets demonstrate that CPL can achieve comparable or even better results than traditional CNN, and from the robustness perspective, CPL shows great advantages for both the rejection and incremental category learning tasks.



### Controlling the privacy loss with the input feature maps of the layers in convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/1805.03444v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.03444v4)
- **Published**: 2018-05-09 10:12:31+00:00
- **Updated**: 2018-12-05 12:03:32+00:00
- **Authors**: Woohyung Chun, Sung-Min Hong, Junho Huh, Inyup Kang
- **Comment**: 9 pages (8 pages for contents) and 5 figures
- **Journal**: None
- **Summary**: We propose the method to sanitize the privacy of the IFM(Input Feature Map)s that are fed into the layers of CNN(Convolutional Neural Network)s. The method introduces the degree of the sanitization that makes the application using a CNN be able to control the privacy loss represented as the ratio of the probabilistic accuracies for original IFM and sanitized IFM. For the sanitization of an IFM, the sample-and-hold based approximation scheme is devised to satisfy an application-specific degree of the sanitization. The scheme approximates an IFM by replacing all the samples in a window with the non-zero sample closest to the mean of the sampling window. It also removes the dependency on CNN configuration by unfolding multi-dimensional IFM tensors into one-dimensional streams to be approximated.



### Object Tracking with Correlation Filters using Selective Single Background Patch
- **Arxiv ID**: http://arxiv.org/abs/1805.03453v1
- **DOI**: None
- **Categories**: **cs.CV**, 65D19, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/1805.03453v1)
- **Published**: 2018-05-09 10:49:14+00:00
- **Updated**: 2018-05-09 10:49:14+00:00
- **Authors**: Lasitha Mekkayil, Hariharan Ramasangu
- **Comment**: 5 pages, 3 figures
- **Journal**: None
- **Summary**: Correlation filter plays a major role in improved tracking performance compared to existing trackers. The tracker uses the adaptive correlation response to predict the location of the target. Many varieties of correlation trackers were proposed recently with high accuracy and frame rates. The paper proposes a method to select a single background patch to have a better tracking performance. The paper also contributes a variant of correlation filter by modifying the filter with image restoration filters. The approach is validated using Object Tracking Benchmark sequences.



### Full 3D Reconstruction of Transparent Objects
- **Arxiv ID**: http://arxiv.org/abs/1805.03482v2
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1805.03482v2)
- **Published**: 2018-05-09 12:39:06+00:00
- **Updated**: 2018-05-14 13:40:00+00:00
- **Authors**: Bojian Wu, Yang Zhou, Yiming Qian, Minglun Gong, Hui Huang
- **Comment**: Accepted to SIGGRAPH 2018
- **Journal**: None
- **Summary**: Numerous techniques have been proposed for reconstructing 3D models for opaque objects in past decades. However, none of them can be directly applied to transparent objects. This paper presents a fully automatic approach for reconstructing complete 3D shapes of transparent objects. Through positioning an object on a turntable, its silhouettes and light refraction paths under different viewing directions are captured. Then, starting from an initial rough model generated from space carving, our algorithm progressively optimizes the model under three constraints: surface and refraction normal consistency, surface projection and silhouette consistency, and surface smoothness. Experimental results on both synthetic and real objects demonstrate that our method can successfully recover the complex shapes of transparent objects and faithfully reproduce their light refraction properties.



### Joint Action Unit localisation and intensity estimation through heatmap regression
- **Arxiv ID**: http://arxiv.org/abs/1805.03487v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.03487v2)
- **Published**: 2018-05-09 12:49:20+00:00
- **Updated**: 2018-07-20 14:19:16+00:00
- **Authors**: Enrique Sanchez-Lozano, Georgios Tzimiropoulos, Michel Valstar
- **Comment**: BMVC 2018. Code and model will be available to download from
  https://github.com/ESanchezLozano/Action-Units-Heatmaps
- **Journal**: None
- **Summary**: This paper proposes a supervised learning approach to jointly perform facial Action Unit (AU) localisation and intensity estimation. Contrary to previous works that try to learn an unsupervised representation of the Action Unit regions, we propose to directly and jointly estimate all AU intensities through heatmap regression, along with the location in the face where they cause visible changes. Our approach aims to learn a pixel-wise regression function returning a score per AU, which indicates an AU intensity at a given spatial location. Heatmap regression then generates an image, or channel, per AU, in which each pixel indicates the corresponding AU intensity. To generate the ground-truth heatmaps for a target AU, the facial landmarks are first estimated, and a 2D Gaussian is drawn around the points where the AU is known to cause changes. The amplitude and size of the Gaussian is determined by the intensity of the AU. We show that using a single Hourglass network suffices to attain new state of the art results, demonstrating the effectiveness of such a simple approach. The use of heatmap regression allows learning of a shared representation between AUs without the need to rely on latent representations, as these are implicitly learned from the data. We validate the proposed approach on the BP4D dataset, showing a modest improvement on recent, complex, techniques, as well as robustness against misalignment errors. Code for testing and models will be available to download from https://github.com/ESanchezLozano/Action-Units-Heatmaps.



### Rethinking Diversified and Discriminative Proposal Generation for Visual Grounding
- **Arxiv ID**: http://arxiv.org/abs/1805.03508v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.03508v1)
- **Published**: 2018-05-09 13:25:27+00:00
- **Updated**: 2018-05-09 13:25:27+00:00
- **Authors**: Zhou Yu, Jun Yu, Chenchao Xiang, Zhou Zhao, Qi Tian, Dacheng Tao
- **Comment**: Accepted in IJCAI 2018
- **Journal**: None
- **Summary**: Visual grounding aims to localize an object in an image referred to by a textual query phrase. Various visual grounding approaches have been proposed, and the problem can be modularized into a general framework: proposal generation, multi-modal feature representation, and proposal ranking. Of these three modules, most existing approaches focus on the latter two, with the importance of proposal generation generally neglected. In this paper, we rethink the problem of what properties make a good proposal generator. We introduce the diversity and discrimination simultaneously when generating proposals, and in doing so propose Diversified and Discriminative Proposal Networks model (DDPN). Based on the proposals generated by DDPN, we propose a high performance baseline model for visual grounding and evaluate it on four benchmark datasets. Experimental results demonstrate that our model delivers significant improvements on all the tested data-sets (e.g., 18.8\% improvement on ReferItGame and 8.2\% improvement on Flickr30k Entities over the existing state-of-the-arts respectively)



### Deep 2.5D Vehicle Classification with Sparse SfM Depth Prior for Automated Toll Systems
- **Arxiv ID**: http://arxiv.org/abs/1805.03511v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.03511v2)
- **Published**: 2018-05-09 13:28:52+00:00
- **Updated**: 2018-05-11 06:24:33+00:00
- **Authors**: Georg Waltner, Michael Maurer, Thomas Holzmann, Patrick Ruprecht, Michael Opitz, Horst Possegger, Friedrich Fraundorfer, Horst Bischof
- **Comment**: Submitted to the IEEE International Conference on Intelligent
  Transportation Systems 2018 (ITSC), 6 pages, 4 figures; changed format in
  compliance with adapted IEEE template
- **Journal**: None
- **Summary**: Automated toll systems rely on proper classification of the passing vehicles. This is especially difficult when the images used for classification only cover parts of the vehicle. To obtain information about the whole vehicle. we reconstruct the vehicle as 3D object and exploit this additional information within a Convolutional Neural Network (CNN). However, when using deep networks for 3D object classification, large amounts of dense 3D models are required for good accuracy, which are often neither available nor feasible to process due to memory requirements. Therefore, in our method we reproject the 3D object onto the image plane using the reconstructed points, lines or both. We utilize this sparse depth prior within an auxiliary network branch that acts as a regularizer during training. We show that this auxiliary regularizer helps to improve accuracy compared to 2D classification on a real-world dataset. Furthermore due to the design of the network, at test time only the 2D camera images are required for classification which enables the usage in portable computer vision systems.



### FlowFields++: Accurate Optical Flow Correspondences Meet Robust Interpolation
- **Arxiv ID**: http://arxiv.org/abs/1805.03517v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.03517v1)
- **Published**: 2018-05-09 13:40:37+00:00
- **Updated**: 2018-05-09 13:40:37+00:00
- **Authors**: René Schuster, Christian Bailer, Oliver Wasenmüller, Didier Stricker
- **Comment**: None
- **Journal**: None
- **Summary**: Optical Flow algorithms are of high importance for many applications. Recently, the Flow Field algorithm and its modifications have shown remarkable results, as they have been evaluated with top accuracy on different data sets. In our analysis of the algorithm we have found that it produces accurate sparse matches, but there is room for improvement in the interpolation. Thus, we propose in this paper FlowFields++, where we combine the accurate matches of Flow Fields with a robust interpolation. In addition, we propose improved variational optimization as post-processing. Our new algorithm is evaluated on the challenging KITTI and MPI Sintel data sets with public top results on both benchmarks.



### Phase retrieval for Fourier Ptychography under varying amount of measurements
- **Arxiv ID**: http://arxiv.org/abs/1805.03593v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.03593v1)
- **Published**: 2018-05-09 15:42:44+00:00
- **Updated**: 2018-05-09 15:42:44+00:00
- **Authors**: Lokesh Boominathan, Mayug Maniparambil, Honey Gupta, Rahul Baburajan, Kaushik Mitra
- **Comment**: Supplementary material attached after Reference section
- **Journal**: None
- **Summary**: Fourier Ptychography is a recently proposed imaging technique that yields high-resolution images by computationally transcending the diffraction blur of an optical system. At the crux of this method is the phase retrieval algorithm, which is used for computationally stitching together low-resolution images taken under varying illumination angles of a coherent light source. However, the traditional iterative phase retrieval technique relies heavily on the initialization and also need a good amount of overlap in the Fourier domain for the successively captured low-resolution images, thus increasing the acquisition time and data. We show that an auto-encoder based architecture can be adaptively trained for phase retrieval under both low overlap, where traditional techniques completely fail, and at higher levels of overlap. For the low overlap case we show that a supervised deep learning technique using an autoencoder generator is a good choice for solving the Fourier ptychography problem. And for the high overlap case, we show that optimizing the generator for reducing the forward model error is an appropriate choice. Using simulations for the challenging case of uncorrelated phase and amplitude, we show that our method outperforms many of the previously proposed Fourier ptychography phase retrieval techniques.



### Layered Optical Flow Estimation Using a Deep Neural Network with a Soft Mask
- **Arxiv ID**: http://arxiv.org/abs/1805.03596v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.03596v1)
- **Published**: 2018-05-09 15:45:48+00:00
- **Updated**: 2018-05-09 15:45:48+00:00
- **Authors**: Xi Zhang, Di Ma, Xu Ouyang, Shanshan Jiang, Lin Gan, Gady Agam
- **Comment**: None
- **Journal**: None
- **Summary**: Using a layered representation for motion estimation has the advantage of being able to cope with discontinuities and occlusions. In this paper, we learn to estimate optical flow by combining a layered motion representation with deep learning. Instead of pre-segmenting the image to layers, the proposed approach automatically generates a layered representation of optical flow using the proposed soft-mask module. The essential components of the soft-mask module are maxout and fuse operations, which enable a disjoint layered representation of optical flow and more accurate flow estimation. We show that by using masks the motion estimate results in a quadratic function of input features in the output layer. The proposed soft-mask module can be added to any existing optical flow estimation networks by replacing their flow output layer. In this work, we use FlowNet as the base network to which we add the soft-mask module. The resulting network is tested on three well-known benchmarks with both supervised and unsupervised flow estimation tasks. Evaluation results show that the proposed network achieve better results compared with the original FlowNet.



### Facade Segmentation in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1805.08634v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.08634v1)
- **Published**: 2018-05-09 16:21:31+00:00
- **Updated**: 2018-05-09 16:21:31+00:00
- **Authors**: John Femiani, Wamiq Reyaz Para, Niloy Mitra, Peter Wonka
- **Comment**: 16 pages, 7 figures
- **Journal**: None
- **Summary**: Urban facade segmentation from automatically acquired imagery, in contrast to traditional image segmentation, poses several unique challenges. 360-degree photospheres captured from vehicles are an effective way to capture a large number of images, but this data presents difficult-to-model warping and stitching artifacts. In addition, each pixel can belong to multiple facade elements, and different facade elements (e.g., window, balcony, sill, etc.) are correlated and vary wildly in their characteristics. In this paper, we propose three network architectures of varying complexity to achieve multilabel semantic segmentation of facade images while exploiting their unique characteristics. Specifically, we propose a MULTIFACSEGNET architecture to assign multiple labels to each pixel, a SEPARABLE architecture as a low-rank formulation that encourages extraction of rectangular elements, and a COMPATIBILITY network that simultaneously seeks segmentation across facade element types allowing the network to 'see' intermediate output probabilities of the various facade element classes. Our results on benchmark datasets show significant improvements over existing facade segmentation approaches for the typical facade elements. For example, on one commonly used dataset, the accuracy scores for window(the most important architectural element) increases from 0.91 to 0.97 percent compared to the best competing method, and comparable improvements on other element types.



### Fast and Accurate Tumor Segmentation of Histology Images using Persistent Homology and Deep Convolutional Features
- **Arxiv ID**: http://arxiv.org/abs/1805.03699v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.03699v1)
- **Published**: 2018-05-09 19:02:29+00:00
- **Updated**: 2018-05-09 19:02:29+00:00
- **Authors**: Talha Qaiser, Yee-Wah Tsang, Daiki Taniyama, Naoya Sakamoto, Kazuaki Nakane, David Epstein, Nasir Rajpoot
- **Comment**: None
- **Journal**: None
- **Summary**: Tumor segmentation in whole-slide images of histology slides is an important step towards computer-assisted diagnosis. In this work, we propose a tumor segmentation framework based on the novel concept of persistent homology profiles (PHPs). For a given image patch, the homology profiles are derived by efficient computation of persistent homology, which is an algebraic tool from homology theory. We propose an efficient way of computing topological persistence of an image, alternative to simplicial homology. The PHPs are devised to distinguish tumor regions from their normal counterparts by modeling the atypical characteristics of tumor nuclei. We propose two variants of our method for tumor segmentation: one that targets speed without compromising accuracy and the other that targets higher accuracy. The fast version is based on the selection of exemplar image patches from a convolution neural network (CNN) and patch classification by quantifying the divergence between the PHPs of exemplars and the input image patch. Detailed comparative evaluation shows that the proposed algorithm is significantly faster than competing algorithms while achieving comparable results. The accurate version combines the PHPs and high-level CNN features and employs a multi-stage ensemble strategy for image patch labeling. Experimental results demonstrate that the combination of PHPs and CNN features outperforms competing algorithms. This study is performed on two independently collected colorectal datasets containing adenoma, adenocarcinoma, signet and healthy cases. Collectively, the accurate tumor segmentation produces the highest average patch-level F1-score, as compared with competing algorithms, on malignant and healthy cases from both the datasets. Overall the proposed framework highlights the utility of persistent homology for histopathology image analysis.



### A Continuous, Full-scope, Spatio-temporal Tracking Metric based on KL-divergence
- **Arxiv ID**: http://arxiv.org/abs/1805.03707v3
- **DOI**: None
- **Categories**: **cs.CV**, I.4.0
- **Links**: [PDF](http://arxiv.org/pdf/1805.03707v3)
- **Published**: 2018-05-09 19:47:03+00:00
- **Updated**: 2019-03-01 17:49:33+00:00
- **Authors**: Terrence Adams
- **Comment**: 12 pages, 1 figure, 7 tables
- **Journal**: None
- **Summary**: A unified metric is given for the evaluation of object tracking systems. The metric is inspired by KL-divergence or relative entropy, which is commonly used to evaluate clustering techniques. Since tracking problems are fundamentally different from clustering, the components of KL-divergence are recast to handle various types of tracking errors (i.e., false alarms, missed detections, merges, splits). Scoring results are given on a standard tracking dataset (Oxford Town Centre Dataset), as well as several simulated scenarios. Also, this new metric is compared with several other metrics including the commonly used Multiple Object Tracking Accuracy metric. In the final section, advantages of this metric are given including the fact that it is continuous, parameter-less and comprehensive.



### Evaluating ResNeXt Model Architecture for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1805.08700v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.08700v1)
- **Published**: 2018-05-09 20:41:27+00:00
- **Updated**: 2018-05-09 20:41:27+00:00
- **Authors**: Saifuddin Hitawala
- **Comment**: 8 pages, 6 figures
- **Journal**: None
- **Summary**: In recent years, deep learning methods have been successfully applied to image classification tasks. Many such deep neural networks exist today that can easily differentiate cats from dogs. One such model is the ResNeXt model that uses a homogeneous, multi-branch architecture for image classification. This paper aims at implementing and evaluating the ResNeXt model architecture on subsets of the CIFAR-10 dataset. It also tweaks the original ResNeXt hyper-parameters such as cardinality, depth and base-width and compares the performance of the modified model with the original. Analysis of the experiments performed in this paper show that a slight decrease in depth or base-width does not affect the performance of the model much leading to comparable results.



