# Arxiv Papers in cs.CV on 2018-05-07
### Skeleton-Based Action Recognition with Spatial Reasoning and Temporal Stack Learning
- **Arxiv ID**: http://arxiv.org/abs/1805.02335v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.02335v2)
- **Published**: 2018-05-07 03:54:28+00:00
- **Updated**: 2018-12-03 03:09:36+00:00
- **Authors**: Chenyang Si, Ya Jing, Wei Wang, Liang Wang, Tieniu Tan
- **Comment**: ECCV 2018
- **Journal**: None
- **Summary**: Skeleton-based action recognition has made great progress recently, but many problems still remain unsolved. For example, most of the previous methods model the representations of skeleton sequences without abundant spatial structure information and detailed temporal dynamics features. In this paper, we propose a novel model with spatial reasoning and temporal stack learning (SR-TSL) for skeleton based action recognition, which consists of a spatial reasoning network (SRN) and a temporal stack learning network (TSLN). The SRN can capture the high-level spatial structural information within each frame by a residual graph neural network, while the TSLN can model the detailed temporal dynamics of skeleton sequences by a composition of multiple skip-clip LSTMs. During training, we propose a clip-based incremental loss to optimize the model. We perform extensive experiments on the SYSU 3D Human-Object Interaction dataset and NTU RGB+D dataset and verify the effectiveness of each network of our model. The comparison results illustrate that our approach achieves much better results than state-of-the-art methods.



### Sharp Attention Network via Adaptive Sampling for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1805.02336v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.02336v2)
- **Published**: 2018-05-07 03:54:40+00:00
- **Updated**: 2018-09-26 11:27:49+00:00
- **Authors**: Chen Shen, Guo-Jun Qi, Rongxin Jiang, Zhongming Jin, Hongwei Yong, Yaowu Chen, Xian-Sheng Hua
- **Comment**: accepted by IEEE Transactions on Circuits and Systems for Video
  Technology(T-CSVT)
- **Journal**: None
- **Summary**: In this paper, we present novel sharp attention networks by adaptively sampling feature maps from convolutional neural networks (CNNs) for person re-identification (re-ID) problem. Due to the introduction of sampling-based attention models, the proposed approach can adaptively generate sharper attention-aware feature masks. This greatly differs from the gating-based attention mechanism that relies soft gating functions to select the relevant features for person re-ID. In contrast, the proposed sampling-based attention mechanism allows us to effectively trim irrelevant features by enforcing the resultant feature masks to focus on the most discriminative features. It can produce sharper attentions that are more assertive in localizing subtle features relevant to re-identifying people across cameras. For this purpose, a differentiable Gumbel-Softmax sampler is employed to approximate the Bernoulli sampling to train the sharp attention networks. Extensive experimental evaluations demonstrate the superiority of this new sharp attention model for person re-ID over the other state-of-the-art methods on three challenging benchmarks including CUHK03, Market-1501, and DukeMTMC-reID.



### A Hierarchical Matcher using Local Classifier Chains
- **Arxiv ID**: http://arxiv.org/abs/1805.02339v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.02339v1)
- **Published**: 2018-05-07 04:29:19+00:00
- **Updated**: 2018-05-07 04:29:19+00:00
- **Authors**: Lingfeng Zhang, Ioannis A. Kakadiaris
- **Comment**: None
- **Journal**: None
- **Summary**: This paper focuses on improving the performance of current convolutional neural networks in visual recognition without changing the network architecture. A hierarchical matcher is proposed that builds chains of local binary neural networks after one global neural network over all the class labels, named as Local Classifier Chains based Convolutional Neural Network (LCC-CNN). The signature of each sample as two components: global component based on the global network; local component based on local binary networks. The local networks are built based on label pairs created by a similarity matrix and confusion matrix. During matching, each sample travels through one global network and a chain of local networks to obtain its final matching to avoid error propagation. The proposed matcher has been evaluated with image recognition, character recognition and face recognition datasets. The experimental results indicate that the proposed matcher achieves better performance when compared with methods using only a global deep network. Compared with the UR2D system, the accuracy is improved significantly by 1% and 0.17% on the UHDB31 dataset and the IJB-A dataset, respectively.



### Full explicit consistency constraints in uncalibrated multiple homography estimation
- **Arxiv ID**: http://arxiv.org/abs/1805.02352v7
- **DOI**: 10.1007/978-3-030-20887-5_41
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.02352v7)
- **Published**: 2018-05-07 05:51:12+00:00
- **Updated**: 2019-05-29 03:10:44+00:00
- **Authors**: Wojciech Chojnacki, Zygmunt L. Szpak
- **Comment**: 18 pages, 3 figures
- **Journal**: Lecture Notes in Computer Science, vol. 11361, pp. 656-675,
  Springer, Cham, 2019
- **Summary**: We reveal a complete set of constraints that need to be imposed on a set of 3-by-3 matrices to ensure that the matrices represent genuine homographies associated with multiple planes between two views. We also show how to exploit the constraints to obtain more accurate estimates of homography matrices between two views. Our study resolves a long-standing research question and provides a fresh perspective and a more in-depth understanding of the multiple homography estimation task.



### GAN Based Medical Image Registration
- **Arxiv ID**: http://arxiv.org/abs/1805.02369v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.02369v4)
- **Published**: 2018-05-07 07:03:04+00:00
- **Updated**: 2019-09-10 07:59:40+00:00
- **Authors**: Dwarikanath Mahapatra
- **Comment**: None
- **Journal**: None
- **Summary**: Conventional approaches to image registration consist of time consuming iterative methods. Most current deep learning (DL) based registration methods extract deep features to use in an iterative setting. We propose an end-to-end DL method for registering multimodal images. Our approach uses generative adversarial networks (GANs) that eliminates the need for time consuming iterative methods, and directly generates the registered image with the deformation field. Appropriate constraints in the GAN cost function produce accurately registered images in less than a second. Experiments demonstrate their accuracy for multimodal retinal and cardiac MR image registration.



### A Review on Facial Micro-Expressions Analysis: Datasets, Features and Metrics
- **Arxiv ID**: http://arxiv.org/abs/1805.02397v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.02397v1)
- **Published**: 2018-05-07 08:29:24+00:00
- **Updated**: 2018-05-07 08:29:24+00:00
- **Authors**: Walied Merghani, Adrian K. Davison, Moi Hoon Yap
- **Comment**: Preprint submitted to IEEE Transactions
- **Journal**: None
- **Summary**: Facial micro-expressions are very brief, spontaneous facial expressions that appear on the face of humans when they either deliberately or unconsciously conceal an emotion. Micro-expression has shorter duration than macro-expression, which makes it more challenging for human and machine. Over the past ten years, automatic micro-expressions recognition has attracted increasing attention from researchers in psychology, computer science, security, neuroscience and other related disciplines. The aim of this paper is to provide the insights of automatic micro-expressions and recommendations for future research. There has been a lot of datasets released over the last decade that facilitated the rapid growth in this field. However, comparison across different datasets is difficult due to the inconsistency in experiment protocol, features used and evaluation methods. To address these issues, we review the datasets, features and the performance metrics deployed in the literature. Relevant challenges such as the spatial temporal settings during data collection, emotional classes versus objective classes in data labelling, face regions in data analysis, standardisation of metrics and the requirements for real-world implementation are discussed. We conclude by proposing some promising future directions to advancing micro-expressions research.



### Unpaired Multi-Domain Image Generation via Regularized Conditional GANs
- **Arxiv ID**: http://arxiv.org/abs/1805.02456v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.02456v1)
- **Published**: 2018-05-07 11:52:28+00:00
- **Updated**: 2018-05-07 11:52:28+00:00
- **Authors**: Xudong Mao, Qing Li
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we study the problem of multi-domain image generation, the goal of which is to generate pairs of corresponding images from different domains. With the recent development in generative models, image generation has achieved great progress and has been applied to various computer vision tasks. However, multi-domain image generation may not achieve the desired performance due to the difficulty of learning the correspondence of different domain images, especially when the information of paired samples is not given. To tackle this problem, we propose Regularized Conditional GAN (RegCGAN) which is capable of learning to generate corresponding images in the absence of paired training data. RegCGAN is based on the conditional GAN, and we introduce two regularizers to guide the model to learn the corresponding semantics of different domains. We evaluate the proposed model on several tasks for which paired training data is not given, including the generation of edges and photos, the generation of faces with different attributes, etc. The experimental results show that our model can successfully generate corresponding images for all these tasks, while outperforms the baseline methods. We also introduce an approach of applying RegCGAN to unsupervised domain adaptation.



### Deep Ordinal Hashing with Spatial Attention
- **Arxiv ID**: http://arxiv.org/abs/1805.02459v1
- **DOI**: 10.1109/TIP.2018.2883522
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.02459v1)
- **Published**: 2018-05-07 11:59:55+00:00
- **Updated**: 2018-05-07 11:59:55+00:00
- **Authors**: Lu Jin, Xiangbo Shu, Kai Li, Zechao Li, Guo-Jun Qi, Jinhui Tang
- **Comment**: None
- **Journal**: None
- **Summary**: Hashing has attracted increasing research attentions in recent years due to its high efficiency of computation and storage in image retrieval. Recent works have demonstrated the superiority of simultaneous feature representations and hash functions learning with deep neural networks. However, most existing deep hashing methods directly learn the hash functions by encoding the global semantic information, while ignoring the local spatial information of images. The loss of local spatial structure makes the performance bottleneck of hash functions, therefore limiting its application for accurate similarity retrieval. In this work, we propose a novel Deep Ordinal Hashing (DOH) method, which learns ordinal representations by leveraging the ranking structure of feature space from both local and global views. In particular, to effectively build the ranking structure, we propose to learn the rank correlation space by exploiting the local spatial information from Fully Convolutional Network (FCN) and the global semantic information from the Convolutional Neural Network (CNN) simultaneously. More specifically, an effective spatial attention model is designed to capture the local spatial information by selectively learning well-specified locations closely related to target objects. In such hashing framework,the local spatial and global semantic nature of images are captured in an end-to-end ranking-to-hashing manner. Experimental results conducted on three widely-used datasets demonstrate that the proposed DOH method significantly outperforms the state-of-the-art hashing methods.



### Comparative evaluation of instrument segmentation and tracking methods in minimally invasive surgery
- **Arxiv ID**: http://arxiv.org/abs/1805.02475v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.02475v1)
- **Published**: 2018-05-07 12:39:21+00:00
- **Updated**: 2018-05-07 12:39:21+00:00
- **Authors**: Sebastian Bodenstedt, Max Allan, Anthony Agustinos, Xiaofei Du, Luis Garcia-Peraza-Herrera, Hannes Kenngott, Thomas Kurmann, Beat Müller-Stich, Sebastien Ourselin, Daniil Pakhomov, Raphael Sznitman, Marvin Teichmann, Martin Thoma, Tom Vercauteren, Sandrine Voros, Martin Wagner, Pamela Wochner, Lena Maier-Hein, Danail Stoyanov, Stefanie Speidel
- **Comment**: None
- **Journal**: None
- **Summary**: Intraoperative segmentation and tracking of minimally invasive instruments is a prerequisite for computer- and robotic-assisted surgery. Since additional hardware like tracking systems or the robot encoders are cumbersome and lack accuracy, surgical vision is evolving as promising techniques to segment and track the instruments using only the endoscopic images. However, what is missing so far are common image data sets for consistent evaluation and benchmarking of algorithms against each other. The paper presents a comparative validation study of different vision-based methods for instrument segmentation and tracking in the context of robotic as well as conventional laparoscopic surgery. The contribution of the paper is twofold: we introduce a comprehensive validation data set that was provided to the study participants and present the results of the comparative validation study. Based on the results of the validation study, we arrive at the conclusion that modern deep learning approaches outperform other methods in instrument segmentation tasks, but the results are still not perfect. Furthermore, we show that merging results from different methods actually significantly increases accuracy in comparison to the best stand-alone method. On the other hand, the results of the instrument tracking task show that this is still an open challenge, especially during challenging scenarios in conventional laparoscopic surgery.



### MEGAN: Mixture of Experts of Generative Adversarial Networks for Multimodal Image Generation
- **Arxiv ID**: http://arxiv.org/abs/1805.02481v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.02481v2)
- **Published**: 2018-05-07 12:49:04+00:00
- **Updated**: 2018-05-08 08:11:44+00:00
- **Authors**: David Keetae Park, Seungjoo Yoo, Hyojin Bahng, Jaegul Choo, Noseong Park
- **Comment**: 27th International Joint Conference on Artificial Intelligence (IJCAI
  2018)
- **Journal**: None
- **Summary**: Recently, generative adversarial networks (GANs) have shown promising performance in generating realistic images. However, they often struggle in learning complex underlying modalities in a given dataset, resulting in poor-quality generated images. To mitigate this problem, we present a novel approach called mixture of experts GAN (MEGAN), an ensemble approach of multiple generator networks. Each generator network in MEGAN specializes in generating images with a particular subset of modalities, e.g., an image class. Instead of incorporating a separate step of handcrafted clustering of multiple modalities, our proposed model is trained through an end-to-end learning of multiple generators via gating networks, which is responsible for choosing the appropriate generator network for a given condition. We adopt the categorical reparameterization trick for a categorical decision to be made in selecting a generator while maintaining the flow of the gradients. We demonstrate that individual generators learn different and salient subparts of the data and achieve a multiscale structural similarity (MS-SSIM) score of 0.2470 for CelebA and a competitive unsupervised inception score of 8.33 in CIFAR-10.



### Long-Term Human Motion Prediction by Modeling Motion Context and Enhancing Motion Dynamic
- **Arxiv ID**: http://arxiv.org/abs/1805.02513v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.02513v1)
- **Published**: 2018-05-07 13:23:17+00:00
- **Updated**: 2018-05-07 13:23:17+00:00
- **Authors**: Yongyi Tang, Lin Ma, Wei Liu, Weishi Zheng
- **Comment**: Accepted by IJCAI 2018
- **Journal**: None
- **Summary**: Human motion prediction aims at generating future frames of human motion based on an observed sequence of skeletons. Recent methods employ the latest hidden states of a recurrent neural network (RNN) to encode the historical skeletons, which can only address short-term prediction. In this work, we propose a motion context modeling by summarizing the historical human motion with respect to the current prediction. A modified highway unit (MHU) is proposed for efficiently eliminating motionless joints and estimating next pose given the motion context. Furthermore, we enhance the motion dynamic by minimizing the gram matrix loss for long-term motion prediction. Experimental results show that the proposed model can promisingly forecast the human future movements, which yields superior performances over related state-of-the-art approaches. Moreover, specifying the motion context with the activity labels enables our model to perform human motion transfer.



### Detecting Traffic Lights by Single Shot Detection
- **Arxiv ID**: http://arxiv.org/abs/1805.02523v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.02523v3)
- **Published**: 2018-05-07 13:37:17+00:00
- **Updated**: 2018-10-11 13:50:50+00:00
- **Authors**: Julian Müller, Klaus Dietmayer
- **Comment**: Submitted to International Conference on Intelligent Transportation
  Systems (ITSC2018)
- **Journal**: None
- **Summary**: Recent improvements in object detection are driven by the success of convolutional neural networks (CNN). They are able to learn rich features outperforming hand-crafted features. So far, research in traffic light detection mainly focused on hand-crafted features, such as color, shape or brightness of the traffic light bulb. This paper presents a deep learning approach for accurate traffic light detection in adapting a single shot detection (SSD) approach. SSD performs object proposals creation and classification using a single CNN. The original SSD struggles in detecting very small objects, which is essential for traffic light detection. By our adaptations it is possible to detect objects much smaller than ten pixels without increasing the input image size. We present an extensive evaluation on the DriveU Traffic Light Dataset (DTLD). We reach both, high accuracy and low false positive rates. The trained model is real-time capable with ten frames per second on a Nvidia Titan Xp. Code has been made available at https://github.com/julimueller/tl_ssd.



### Near-drowning Early Prediction Technique Using Novel Equations (NEPTUNE) for Swimming Pools
- **Arxiv ID**: http://arxiv.org/abs/1805.02530v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.02530v3)
- **Published**: 2018-05-07 13:51:54+00:00
- **Updated**: 2018-08-30 11:53:37+00:00
- **Authors**: Bhaskaran David Prakash
- **Comment**: None
- **Journal**: None
- **Summary**: Safety is a critical aspect in all swimming pools. This paper describes a near drowning early prediction technique using novel equations (NEPTUNE). NEPTUNE uses equations or rules that would be able to detect near drowning using at least 1 but not more than 5 seconds of video sequence with no false positives. The backbone of NEPTUNE encompasses a mix of statistical image processing to merge images for a video sequence followed by K means clustering to extract segments in the merged image and finally a revisit to statistical image processing to derive variables for every segment. These variables would be used by the equations to identify near-drowning. NEPTUNE has the potential to be integrated into a swimming pool camera system that would send an alarm to the lifeguards for early response so that the likelihood of recovery is high.



### A probabilistic framework for handwritten text line segmentation
- **Arxiv ID**: http://arxiv.org/abs/1805.02536v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1805.02536v2)
- **Published**: 2018-05-07 14:10:20+00:00
- **Updated**: 2018-05-15 08:50:13+00:00
- **Authors**: Francisco Cruz, Oriol Ramos Terrades
- **Comment**: 47 pages, 23 images
- **Journal**: None
- **Summary**: We successfully combine Expectation-Maximization algorithm and variational approaches for parameter learning and computing inference on Markov random felds. This is a general method that can be applied to many computer vision tasks. In this paper, we apply it to handwritten text line segmentation. We conduct several experiments that demonstrate that our method deal with common issues of this task, such as complex document layout or non-latin scripts. The obtained results prove that our method achieve state-of-the-art performance on different benchmark datasets without any particular fine tuning step.



### Trajectory Representation and Landmark Projection for Continuous-Time Structure from Motion
- **Arxiv ID**: http://arxiv.org/abs/1805.02543v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.02543v1)
- **Published**: 2018-05-07 14:29:30+00:00
- **Updated**: 2018-05-07 14:29:30+00:00
- **Authors**: Hannes Ovrén, Per-Erik Forssén
- **Comment**: Submitted to IJRR
- **Journal**: None
- **Summary**: This paper revisits the problem of continuous-time structure from motion, and introduces a number of extensions that improve convergence and efficiency. The formulation with a $\mathcal{C}^2$-continuous spline for the trajectory naturally incorporates inertial measurements, as derivatives of the sought trajectory. We analyse the behaviour of split interpolation on $\mathbb{SO}(3)$ and on $\mathbb{R}^3$, and a joint interpolation on $\mathbb{SE}(3)$, and show that the latter implicitly couples the direction of translation and rotation. Such an assumption can make good sense for a camera mounted on a robot arm, but not for hand-held or body-mounted cameras. Our experiments show that split interpolation on $\mathbb{SO}(3)$ and on $\mathbb{R}^3$ is preferable over $\mathbb{SE}(3)$ interpolation in all tested cases. Finally, we investigate the problem of landmark reprojection on rolling shutter cameras, and show that the tested reprojection methods give similar quality, while their computational load varies by a factor of 2.



### Relational Network for Skeleton-Based Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1805.02556v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.02556v4)
- **Published**: 2018-05-07 14:59:54+00:00
- **Updated**: 2019-04-11 12:36:13+00:00
- **Authors**: Wu Zheng, Lin Li, Zhaoxiang Zhang, Yan Huang, Liang Wang
- **Comment**: Accepted by International Conference on Multimedia and Expo(ICME)
  2019 as Oral
- **Journal**: None
- **Summary**: With the fast development of effective and low-cost human skeleton capture systems, skeleton-based action recognition has attracted much attention recently. Most existing methods use Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN) to extract spatio-temporal information embedded in the skeleton sequences for action recognition. However, these approaches are limited in the ability of relational modeling in a single skeleton, due to the loss of important structural information when converting the raw skeleton data to adapt to the input format of CNN or RNN. In this paper, we propose an Attentional Recurrent Relational Network-LSTM (ARRN-LSTM) to simultaneously model spatial configurations and temporal dynamics in skeletons for action recognition. We introduce the Recurrent Relational Network to learn the spatial features in a single skeleton, followed by a multi-layer LSTM to learn the temporal features in the skeleton sequences. Between the two modules, we design an adaptive attentional module to focus attention on the most discriminative parts in the single skeleton. To exploit the complementarity from different geometries in the skeleton for sufficient relational modeling, we design a two-stream architecture to learn the structural features among joints and lines simultaneously. Extensive experiments are conducted on several popular skeleton datasets and the results show that the proposed approach achieves better results than most mainstream methods.



### 30m resolution Global Annual Burned Area Mapping based on Landsat images and Google Earth Engine
- **Arxiv ID**: http://arxiv.org/abs/1805.02579v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.02579v1)
- **Published**: 2018-05-07 15:38:15+00:00
- **Updated**: 2018-05-07 15:38:15+00:00
- **Authors**: Tengfei Long, Zhaoming Zhang, Guojin He, Weili Jiao, Chao Tang, Bingfang Wu, Xiaomei Zhang, Guizhou Wang, Ranyu Yin
- **Comment**: None
- **Journal**: None
- **Summary**: Heretofore, global burned area (BA) products are only available at coarse spatial resolution, since most of the current global BA products are produced with the help of active fire detection or dense time-series change analysis, which requires very high temporal resolution. In this study, however, we focus on automated global burned area mapping approach based on Landsat images. By utilizing the huge catalog of satellite imagery as well as the high-performance computing capacity of Google Earth Engine, we proposed an automated pipeline for generating 30-meter resolution global-scale annual burned area map from time-series of Landsat images, and a novel 30-meter resolution global annual burned area map of 2015 (GABAM 2015) is released. GABAM 2015 consists of spatial extent of fires that occurred during 2015 and not of fires that occurred in previous years. Cross-comparison with recent Fire_cci version 5.0 BA product found a similar spatial distribution and a strong correlation ($R^2=0.74$) between the burned areas from the two products, although differences were found in specific land cover categories (particularly in agriculture land). Preliminary global validation showed the commission and omission error of GABAM 2015 are 13.17% and 30.13%, respectively.



### Label Refinery: Improving ImageNet Classification through Label Progression
- **Arxiv ID**: http://arxiv.org/abs/1805.02641v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.02641v1)
- **Published**: 2018-05-07 17:52:42+00:00
- **Updated**: 2018-05-07 17:52:42+00:00
- **Authors**: Hessam Bagherinezhad, Maxwell Horton, Mohammad Rastegari, Ali Farhadi
- **Comment**: None
- **Journal**: None
- **Summary**: Among the three main components (data, labels, and models) of any supervised learning system, data and models have been the main subjects of active research. However, studying labels and their properties has received very little attention. Current principles and paradigms of labeling impose several challenges to machine learning algorithms. Labels are often incomplete, ambiguous, and redundant. In this paper we study the effects of various properties of labels and introduce the Label Refinery: an iterative procedure that updates the ground truth labels after examining the entire dataset. We show significant gain using refined labels across a wide range of models. Using a Label Refinery improves the state-of-the-art top-1 accuracy of (1) AlexNet from 59.3 to 67.2, (2) MobileNet from 70.6 to 73.39, (3) MobileNet-0.25 from 50.6 to 55.59, (4) VGG19 from 72.7 to 75.46, and (5) Darknet19 from 72.9 to 74.47.



### Multichannel Distributed Local Pattern for Content Based Indexing and Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1805.02679v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.02679v1)
- **Published**: 2018-05-07 18:14:49+00:00
- **Updated**: 2018-05-07 18:14:49+00:00
- **Authors**: Sonakshi Mathur, Mallika Chaudhary, Hemant Verma, Murari Mandal, S. K. Vipparthi, Subrahmanyam Murala
- **Comment**: Accepted in INDICON-2017
- **Journal**: None
- **Summary**: A novel color feature descriptor, Multichannel Distributed Local Pattern (MDLP) is proposed in this manuscript. The MDLP combines the salient features of both local binary and local mesh patterns in the neighborhood. The multi-distance information computed by the MDLP aids in robust extraction of the texture arrangement. Further, MDLP features are extracted for each color channel of an image. The retrieval performance of the MDLP is evaluated on the three benchmark datasets for CBIR, namely Corel-5000, Corel-10000 and MIT-Color Vistex respectively. The proposed technique attains substantial improvement as compared to other state-of- the-art feature descriptors in terms of various evaluation parameters such as ARP and ARR on the respective databases.



### Image Super-Resolution via Dual-State Recurrent Networks
- **Arxiv ID**: http://arxiv.org/abs/1805.02704v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.02704v1)
- **Published**: 2018-05-07 19:23:17+00:00
- **Updated**: 2018-05-07 19:23:17+00:00
- **Authors**: Wei Han, Shiyu Chang, Ding Liu, Mo Yu, Michael Witbrock, Thomas S. Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Advances in image super-resolution (SR) have recently benefited significantly from rapid developments in deep neural networks. Inspired by these recent discoveries, we note that many state-of-the-art deep SR architectures can be reformulated as a single-state recurrent neural network (RNN) with finite unfoldings. In this paper, we explore new structures for SR based on this compact RNN view, leading us to a dual-state design, the Dual-State Recurrent Network (DSRN). Compared to its single state counterparts that operate at a fixed spatial resolution, DSRN exploits both low-resolution (LR) and high-resolution (HR) signals jointly. Recurrent signals are exchanged between these states in both directions (both LR to HR and HR to LR) via delayed feedback. Extensive quantitative and qualitative evaluations on benchmark datasets and on a recent challenge demonstrate that the proposed DSRN performs favorably against state-of-the-art algorithms in terms of both memory consumption and predictive accuracy.



### Synaptic Cleft Segmentation in Non-Isotropic Volume Electron Microscopy of the Complete Drosophila Brain
- **Arxiv ID**: http://arxiv.org/abs/1805.02718v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.02718v1)
- **Published**: 2018-05-07 19:48:19+00:00
- **Updated**: 2018-05-07 19:48:19+00:00
- **Authors**: Larissa Heinrich, Jan Funke, Constantin Pape, Juan Nunez-Iglesias, Stephan Saalfeld
- **Comment**: None
- **Journal**: None
- **Summary**: Neural circuit reconstruction at single synapse resolution is increasingly recognized as crucially important to decipher the function of biological nervous systems. Volume electron microscopy in serial transmission or scanning mode has been demonstrated to provide the necessary resolution to segment or trace all neurites and to annotate all synaptic connections.   Automatic annotation of synaptic connections has been done successfully in near isotropic electron microscopy of vertebrate model organisms. Results on non-isotropic data in insect models, however, are not yet on par with human annotation.   We designed a new 3D-U-Net architecture to optimally represent isotropic fields of view in non-isotropic data. We used regression on a signed distance transform of manually annotated synaptic clefts of the CREMI challenge dataset to train this model and observed significant improvement over the state of the art.   We developed open source software for optimized parallel prediction on very large volumetric datasets and applied our model to predict synaptic clefts in a 50 tera-voxels dataset of the complete Drosophila brain. Our model generalizes well to areas far away from where training data was available.



### Building Disease Detection Algorithms with Very Small Numbers of Positive Samples
- **Arxiv ID**: http://arxiv.org/abs/1805.02730v1
- **DOI**: 10.1007/978-3-319-66179-7_54
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1805.02730v1)
- **Published**: 2018-05-07 20:26:14+00:00
- **Updated**: 2018-05-07 20:26:14+00:00
- **Authors**: Ken C. L. Wong, Alexandros Karargyris, Tanveer Syeda-Mahmood, Mehdi Moradi
- **Comment**: None
- **Journal**: None
- **Summary**: Although deep learning can provide promising results in medical image analysis, the lack of very large annotated datasets confines its full potential. Furthermore, limited positive samples also create unbalanced datasets which limit the true positive rates of trained models. As unbalanced datasets are mostly unavoidable, it is greatly beneficial if we can extract useful knowledge from negative samples to improve classification accuracy on limited positive samples. To this end, we propose a new strategy for building medical image analysis pipelines that target disease detection. We train a discriminative segmentation model only on normal images to provide a source of knowledge to be transferred to a disease detection classifier. We show that using the feature maps of a trained segmentation network, deviations from normal anatomy can be learned by a two-class classification network on an extremely unbalanced training dataset with as little as one positive for 17 negative samples. We demonstrate that even though the segmentation network is only trained on normal cardiac computed tomography images, the resulting feature maps can be used to detect pericardial effusion and cardiac septal defects with two-class convolutional classification networks.



### Learning Optical Flow via Dilated Networks and Occlusion Reasoning
- **Arxiv ID**: http://arxiv.org/abs/1805.02733v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1805.02733v1)
- **Published**: 2018-05-07 20:31:56+00:00
- **Updated**: 2018-05-07 20:31:56+00:00
- **Authors**: Yi Zhu, Shawn Newsam
- **Comment**: Accepted at ICIP 2018
- **Journal**: None
- **Summary**: Despite the significant progress that has been made on estimating optical flow recently, most estimation methods, including classical and deep learning approaches, still have difficulty with multi-scale estimation, real-time computation, and/or occlusion reasoning. In this paper, we introduce dilated convolution and occlusion reasoning into unsupervised optical flow estimation to address these issues. The dilated convolution allows our network to avoid upsampling via deconvolution and the resulting gridding artifacts. Dilated convolution also results in a smaller memory footprint which speeds up interference. The occlusion reasoning prevents our network from learning incorrect deformations due to occluded image regions during training. Our proposed method outperforms state-of-the-art unsupervised approaches on the KITTI benchmark. We also demonstrate its generalization capability by applying it to action recognition in video.



### Detection of Paroxysmal Atrial Fibrillation using Attention-based Bidirectional Recurrent Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1805.09133v1
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1805.09133v1)
- **Published**: 2018-05-07 20:34:17+00:00
- **Updated**: 2018-05-07 20:34:17+00:00
- **Authors**: Supreeth P. Shashikumar, Amit J. Shah, Gari D. Clifford, Shamim Nemati
- **Comment**: Accepted to the 24th ACM SIGKDD International Conference on Knowledge
  Discovery and Data Mining (KDD 2018), London, UK, 2018
- **Journal**: None
- **Summary**: Detection of atrial fibrillation (AF), a type of cardiac arrhythmia, is difficult since many cases of AF are usually clinically silent and undiagnosed. In particular paroxysmal AF is a form of AF that occurs occasionally, and has a higher probability of being undetected. In this work, we present an attention based deep learning framework for detection of paroxysmal AF episodes from a sequence of windows. Time-frequency representation of 30 seconds recording windows, over a 10 minute data segment, are fed sequentially into a deep convolutional neural network for image-based feature extraction, which are then presented to a bidirectional recurrent neural network with an attention layer for AF detection. To demonstrate the effectiveness of the proposed framework for transient AF detection, we use a database of 24 hour Holter Electrocardiogram (ECG) recordings acquired from 2850 patients at the University of Virginia heart station. The algorithm achieves an AUC of 0.94 on the testing set, which exceeds the performance of baseline models. We also demonstrate the cross-domain generalizablity of the approach by adapting the learned model parameters from one recording modality (ECG) to another (photoplethysmogram) with improved AF detection performance. The proposed high accuracy, low false alarm algorithm for detecting paroxysmal AF has potential applications in long-term monitoring using wearable sensors.



### End-to-End Refinement Guided by Pre-trained Prototypical Classifier
- **Arxiv ID**: http://arxiv.org/abs/1805.08698v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1805.08698v2)
- **Published**: 2018-05-07 22:18:24+00:00
- **Updated**: 2018-12-11 16:51:58+00:00
- **Authors**: Junwen Bai, Zihang Lai, Runzhe Yang, Yexiang Xue, John Gregoire, Carla Gomes
- **Comment**: None
- **Journal**: None
- **Summary**: Many real-world tasks involve identifying patterns from data satisfying background or prior knowledge. In domains like materials discovery, due to the flaws and biases in raw experimental data, the identification of X-ray diffraction patterns (XRD) often requires a huge amount of manual work in finding refined phases that are similar to the ideal theoretical ones. Automatically refining the raw XRDs utilizing the simulated theoretical data is thus desirable. We propose imitation refinement, a novel approach to refine imperfect input patterns, guided by a pre-trained classifier incorporating prior knowledge from simulated theoretical data, such that the refined patterns imitate the ideal data. The classifier is trained on the ideal simulated data to classify patterns and learns an embedding space where each class is represented by a prototype. The refiner learns to refine the imperfect patterns with small modifications, such that their embeddings are closer to the corresponding prototypes. We show that the refiner can be trained in both supervised and unsupervised fashions. We further illustrate the effectiveness of the proposed approach both qualitatively and quantitatively in a digit refinement task and an X-ray diffraction pattern refinement task in materials discovery.



