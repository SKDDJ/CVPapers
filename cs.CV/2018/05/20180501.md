# Arxiv Papers in cs.CV on 2018-05-01
### Semantic Binary Segmentation using Convolutional Networks without Decoders
- **Arxiv ID**: http://arxiv.org/abs/1805.00138v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.00138v2)
- **Published**: 2018-05-01 00:10:12+00:00
- **Updated**: 2018-05-25 18:14:39+00:00
- **Authors**: Shubhra Aich, William van der Kamp, Ian Stavness
- **Comment**: CVPR 2018 DeepGlobe Workshop; Code repository:
  https://github.com/littleaich/deepglobe2018
- **Journal**: None
- **Summary**: In this paper, we propose an efficient architecture for semantic image segmentation using the depth-to-space (D2S) operation. Our D2S model is comprised of a standard CNN encoder followed by a depth-to-space reordering of the final convolutional feature maps. Our approach eliminates the decoder portion of traditional encoder-decoder segmentation models and reduces the amount of computation almost by half. As a participant of the DeepGlobe Road Extraction competition, we evaluate our models on the corresponding road segmentation dataset. Our highly efficient D2S models exhibit comparable performance to standard segmentation models with much lower computational cost.



### Dialog-based Interactive Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1805.00145v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1805.00145v3)
- **Published**: 2018-05-01 01:13:01+00:00
- **Updated**: 2018-12-20 22:13:05+00:00
- **Authors**: Xiaoxiao Guo, Hui Wu, Yu Cheng, Steven Rennie, Gerald Tesauro, Rogerio Schmidt Feris
- **Comment**: accepted at NeurIPS 2018
- **Journal**: None
- **Summary**: Existing methods for interactive image retrieval have demonstrated the merit of integrating user feedback, improving retrieval results. However, most current systems rely on restricted forms of user feedback, such as binary relevance responses, or feedback based on a fixed set of relative attributes, which limits their impact. In this paper, we introduce a new approach to interactive image search that enables users to provide feedback via natural language, allowing for more natural and effective interaction. We formulate the task of dialog-based interactive image retrieval as a reinforcement learning problem, and reward the dialog system for improving the rank of the target image during each dialog turn. To mitigate the cumbersome and costly process of collecting human-machine conversations as the dialog system learns, we train our system with a user simulator, which is itself trained to describe the differences between target and candidate images. The efficacy of our approach is demonstrated in a footwear retrieval application. Experiments on both simulated and real-world data show that 1) our proposed learning framework achieves better accuracy than other supervised and reinforcement learning baselines and 2) user feedback based on natural language rather than pre-specified attributes leads to more effective retrieval results, and a more natural and expressive communication interface.



### Fixation Data Analysis for High Resolution Satellite Images
- **Arxiv ID**: http://arxiv.org/abs/1805.00192v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.00192v1)
- **Published**: 2018-05-01 05:15:25+00:00
- **Updated**: 2018-05-01 05:15:25+00:00
- **Authors**: Ashu Sharma, Jayanta Kumar Ghosh, Saptrarshi Kolay
- **Comment**: Extended version is submitted to SPIE-2018 conference
- **Journal**: None
- **Summary**: The presented study is an eye tracking experiment for high-resolution satellite (HRS) images. The reported experiment explores the Area Of Interest (AOI) based analysis of eye fixation data for complex HRS images. The study reflects the requisite of reference data for bottom-up saliency-based segmentation and the struggle of eye tracking data analysis for complex satellite images. The intended fixation data analysis aims towards the reference data creation for bottom-up saliency-based segmentation of high-resolution satellite images. The analytical outcome of this experimental study provides a solution for AOI-based analysis for fixation data in the complex environment of satellite images and recommendations for reference data construction which is already an ongoing effort.



### Localization: A Missing Link in the Pipeline of Object Matching and Registration
- **Arxiv ID**: http://arxiv.org/abs/1805.00223v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.00223v2)
- **Published**: 2018-05-01 07:50:25+00:00
- **Updated**: 2019-01-11 12:01:54+00:00
- **Authors**: Deepak Mishra, Rajeev Ranjan, Santanu Chaudhury, Mukul Sarkar, Arvinder Singh Soin
- **Comment**: 11 pages, 6 figures
- **Journal**: None
- **Summary**: Image registration is a process of aligning two or more images of same objects using geometric transformation. Most of the existing approaches work on the assumption of location invariance. These approaches require object-centric images to perform matching. Further, in absence of intensity level symmetry between the corresponding points in two images, the learning based registration approaches rely on synthetic deformations, which often fail in real scenarios. To address these issues, a combination of convolutional neural networks (CNNs) to perform the desired registration is developed in this work. The complete objective is divided into three sub-objectives: object localization, segmentation and matching transformation. Object localization step establishes an initial correspondence between the images. A modified version of single shot multi-box detector is used for this purpose. The detected region is cropped to make the images object-centric. Subsequently, the objects are segmented and matched using a spatial transformer network employing thin plate spline deformation. Initial experiments on MNIST and Caltech-101 datasets show that the proposed model is able to produce accurate matching. Quantitative evaluation performed using dice coefficient (DC) and mean intersection over union (mIoU) show that proposed method results in the values of 79% and 66%, respectively for MNIST dataset and the values of 94% and 90%, respectively for Caltech-101 dataset. The proposed framework is extended to the registration of CT and US images, which is free from any data specific assumptions and has better generalization capability as compared to the existing rule-based/classical approaches.



### Learning to Sketch with Shortcut Cycle Consistency
- **Arxiv ID**: http://arxiv.org/abs/1805.00247v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.00247v1)
- **Published**: 2018-05-01 09:13:14+00:00
- **Updated**: 2018-05-01 09:13:14+00:00
- **Authors**: Jifei Song, Kaiyue Pang, Yi-Zhe Song, Tao Xiang, Timothy Hospedales
- **Comment**: To appear in CVPR2018
- **Journal**: None
- **Summary**: To see is to sketch -- free-hand sketching naturally builds ties between human and machine vision. In this paper, we present a novel approach for translating an object photo to a sketch, mimicking the human sketching process. This is an extremely challenging task because the photo and sketch domains differ significantly. Furthermore, human sketches exhibit various levels of sophistication and abstraction even when depicting the same object instance in a reference photo. This means that even if photo-sketch pairs are available, they only provide weak supervision signal to learn a translation model. Compared with existing supervised approaches that solve the problem of D(E(photo)) -> sketch, where E($\cdot$) and D($\cdot$) denote encoder and decoder respectively, we take advantage of the inverse problem (e.g., D(E(sketch)) -> photo), and combine with the unsupervised learning tasks of within-domain reconstruction, all within a multi-task learning framework. Compared with existing unsupervised approaches based on cycle consistency (i.e., D(E(D(E(photo)))) -> photo), we introduce a shortcut consistency enforced at the encoder bottleneck (e.g., D(E(photo)) -> photo) to exploit the additional self-supervision. Both qualitative and quantitative results show that the proposed model is superior to a number of state-of-the-art alternatives. We also show that the synthetic sketches can be used to train a better fine-grained sketch-based image retrieval (FG-SBIR) model, effectively alleviating the problem of sketch data scarcity.



### Conditional Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/1805.00251v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.00251v1)
- **Published**: 2018-05-01 09:23:07+00:00
- **Updated**: 2018-05-01 09:23:07+00:00
- **Authors**: Jianxin Lin, Yingce Xia, Tao Qin, Zhibo Chen, Tie-Yan Liu
- **Comment**: 9 pages, 9 figures, IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)
- **Journal**: None
- **Summary**: Image-to-image translation tasks have been widely investigated with Generative Adversarial Networks (GANs) and dual learning. However, existing models lack the ability to control the translated results in the target domain and their results usually lack of diversity in the sense that a fixed image usually leads to (almost) deterministic translation result. In this paper, we study a new problem, conditional image-to-image translation, which is to translate an image from the source domain to the target domain conditioned on a given image in the target domain. It requires that the generated image should inherit some domain-specific features of the conditional image from the target domain. Therefore, changing the conditional image in the target domain will lead to diverse translation results for a fixed input image from the source domain, and therefore the conditional input image helps to control the translation results. We tackle this problem with unpaired data based on GANs and dual learning. We twist two conditional translation models (one translation from A domain to B domain, and the other one from B domain to A domain) together for inputs combination and reconstruction while preserving domain independent features. We carry out experiments on men's faces from-to women's faces translation and edges to shoes&bags translations. The results demonstrate the effectiveness of our proposed method.



### Object Activity Scene Description, Construction and Recognition
- **Arxiv ID**: http://arxiv.org/abs/1805.00258v1
- **DOI**: 10.1109/TCYB.2019.2904901
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.00258v1)
- **Published**: 2018-05-01 09:56:43+00:00
- **Updated**: 2018-05-01 09:56:43+00:00
- **Authors**: Hui Feng, Shanshan Wang, Shuzhi Sam Ge
- **Comment**: 13 pages, 9 figures
- **Journal**: None
- **Summary**: Action recognition is a critical task for social robots to meaningfully engage with their environment. 3D human skeleton-based action recognition is an attractive research area in recent years. Although, the existing approaches are good at action recognition, it is a great challenge to recognize a group of actions in an activity scene. To tackle this problem, at first, we partition the scene into several primitive actions (PAs) based upon motion attention mechanism. Then, the primitive actions are described by the trajectory vectors of corresponding joints. After that, motivated by text classification based on word embedding, we employ convolution neural network (CNN) to recognize activity scenes by considering motion of joints as "word" of activity. The experimental results on the scenes of human activity dataset show the efficiency of the proposed approach.



### Fast and Efficient Depth Map Estimation from Light Fields
- **Arxiv ID**: http://arxiv.org/abs/1805.00264v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.00264v1)
- **Published**: 2018-05-01 10:20:35+00:00
- **Updated**: 2018-05-01 10:20:35+00:00
- **Authors**: Yuriy Anisimov, Didier Stricker
- **Comment**: International Conference on 3D Vision (3DV), Qingdao, China, October
  2017
- **Journal**: None
- **Summary**: The paper presents an algorithm for depth map estimation from the light field images in relatively small amount of time, using only single thread on CPU. The proposed method improves existing principle of line fitting in 4-dimensional light field space. Line fitting is based on color values comparison using kernel density estimation. Our method utilizes result of Semi-Global Matching (SGM) with Census transform-based matching cost as a border initialization for line fitting. It provides a significant reduction of computations needed to find the best depth match. With the suggested evaluation metric we show that proposed method is applicable for efficient depth map estimation while preserving low computational time compared to others.



### Versatile Auxiliary Classifier with Generative Adversarial Network (VAC+GAN)
- **Arxiv ID**: http://arxiv.org/abs/1805.00316v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1805.00316v3)
- **Published**: 2018-05-01 13:17:39+00:00
- **Updated**: 2018-06-18 23:47:57+00:00
- **Authors**: Shabab Bazrafkan, Hossein Javidnia, Peter Corcoran
- **Comment**: This paper will be uploaded as two separate manuscripts
- **Journal**: None
- **Summary**: One of the most interesting challenges in Artificial Intelligence is to train conditional generators which are able to provide labeled adversarial samples drawn from a specific distribution. In this work, a new framework is presented to train a deep conditional generator by placing a classifier in parallel with the discriminator and back propagate the classification error through the generator network. The method is versatile and is applicable to any variations of Generative Adversarial Network (GAN) implementation, and also gives superior results compared to similar methods.



### Sample-to-Sample Correspondence for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1805.00355v3
- **DOI**: 10.1016/j.engappai.2018.05.001
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.00355v3)
- **Published**: 2018-05-01 14:12:57+00:00
- **Updated**: 2018-12-04 06:27:40+00:00
- **Authors**: Debasmit Das, C. S. George Lee
- **Comment**: Final version appeared in Engineering Applications of Artificial
  Intelligence. Mostly, the related work in this version is different from the
  published version
- **Journal**: None
- **Summary**: The assumption that training and testing samples are generated from the same distribution does not always hold for real-world machine-learning applications. The procedure of tackling this discrepancy between the training (source) and testing (target) domains is known as domain adaptation. We propose an unsupervised version of domain adaptation that considers the presence of only unlabelled data in the target domain. Our approach centers on finding correspondences between samples of each domain. The correspondences are obtained by treating the source and target samples as graphs and using a convex criterion to match them. The criteria used are first-order and second-order similarities between the graphs as well as a class-based regularization. We have also developed a computationally efficient routine for the convex optimization, thus allowing the proposed method to be used widely. To verify the effectiveness of the proposed method, computer simulations were conducted on synthetic, image classification and sentiment classification datasets. Results validated that the proposed local sample-to-sample matching method out-performs traditional moment-matching methods and is competitive with respect to current local domain-adaptation methods.



### Collaborations on YouTube: From Unsupervised Detection to the Impact on Video and Channel Popularity
- **Arxiv ID**: http://arxiv.org/abs/1805.01887v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.SI
- **Links**: [PDF](http://arxiv.org/pdf/1805.01887v1)
- **Published**: 2018-05-01 14:38:18+00:00
- **Updated**: 2018-05-01 14:38:18+00:00
- **Authors**: Christian Koch, Moritz Lode, Denny Stohr, Amr Rizk, Ralf Steinmetz
- **Comment**: 28 pages, 21 figures
- **Journal**: None
- **Summary**: YouTube is one of the most popular platforms for streaming of user-generated video. Nowadays, professional YouTubers are organized in so called multi-channel networks (MCNs). These networks offer services such as brand deals, equipment, and strategic advice in exchange for a share of the YouTubers' revenue. A major strategy to gain more subscribers and, hence, revenue is collaborating with other YouTubers. Yet, collaborations on YouTube have not been studied in a detailed quantitative manner. This paper aims to close this gap with the following contributions. First, we collect a YouTube dataset covering video statistics over three months for 7,942 channels. Second, we design a framework for collaboration detection given a previously unknown number of persons featuring in YouTube videos. We denote this framework for the analysis of collaborations in YouTube videos using a Deep Neural Network (DNN) based approach as CATANA. Third, we analyze about 2.4 years of video content and use CATANA to answer research questions providing guidance for YouTubers and MCNs for efficient collaboration strategies. Thereby, we focus on (i) collaboration frequency and partner selectivity, (ii) the influence of MCNs on channel collaborations, (iii) collaborating channel types, and (iv) the impact of collaborations on video and channel popularity. Our results show that collaborations are in many cases significantly beneficial in terms of viewers and newly attracted subscribers for both collaborating channels, showing often more than 100% popularity growth compared with non-collaboration videos.



### Which Facial Expressions Can Reveal Your Gender? A Study With 3D Faces
- **Arxiv ID**: http://arxiv.org/abs/1805.00371v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.00371v1)
- **Published**: 2018-05-01 14:50:23+00:00
- **Updated**: 2018-05-01 14:50:23+00:00
- **Authors**: Baiqiang Xia
- **Comment**: 20 pages, single column, 7 figures
- **Journal**: None
- **Summary**: Human exhibit rich gender cues in both appearance and behavior. In computer vision domain, gender recognition from facial appearance have been extensively studied, while facial behavior based gender recognition studies remain rare. In this work, we first demonstrate that facial expressions influence the gender patterns presented in 3D face, and gender recognition performance increases when training and testing within the same expression. In further, we design experiments which directly extract the morphological changes resulted from facial expressions as features, for expression-based gender recognition. Experimental results demonstrate that gender can be recognized with considerable accuracy in Happy and Disgust expressions, while Surprise and Sad expressions do not convey much gender related information. This is the first work in the literature which investigates expression-based gender classification with 3D faces, and reveals the strength of gender patterns incorporated in different types of expressions, namely the Happy, the Disgust, the Surprise and the Sad expressions.



### Boosting Self-Supervised Learning via Knowledge Transfer
- **Arxiv ID**: http://arxiv.org/abs/1805.00385v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.00385v1)
- **Published**: 2018-05-01 15:08:30+00:00
- **Updated**: 2018-05-01 15:08:30+00:00
- **Authors**: Mehdi Noroozi, Ananth Vinjimoor, Paolo Favaro, Hamed Pirsiavash
- **Comment**: None
- **Journal**: None
- **Summary**: In self-supervised learning, one trains a model to solve a so-called pretext task on a dataset without the need for human annotation. The main objective, however, is to transfer this model to a target domain and task. Currently, the most effective transfer strategy is fine-tuning, which restricts one to use the same model or parts thereof for both pretext and target tasks. In this paper, we present a novel framework for self-supervised learning that overcomes limitations in designing and comparing different tasks, models, and data domains. In particular, our framework decouples the structure of the self-supervised model from the final task-specific fine-tuned model. This allows us to: 1) quantitatively assess previously incompatible models including handcrafted features; 2) show that deeper neural network models can learn better representations from the same pretext task; 3) transfer knowledge learned with a deep model to a shallower one and thus boost its learning. We use this framework to design a novel self-supervised task, which achieves state-of-the-art performance on the common benchmarks in PASCAL VOC 2007, ILSVRC12 and Places by a significant margin. Our learned features shrink the mAP gap between models trained via self-supervised learning and supervised learning from 5.9% to 2.6% in object detection on PASCAL VOC 2007.



### Robust Face Recognition with Deeply Normalized Depth Images
- **Arxiv ID**: http://arxiv.org/abs/1805.00406v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.00406v1)
- **Published**: 2018-05-01 16:02:50+00:00
- **Updated**: 2018-05-01 16:02:50+00:00
- **Authors**: Ziqing Feng, Qijun Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Depth information has been proven useful for face recognition. However, existing depth-image-based face recognition methods still suffer from noisy depth values and varying poses and expressions. In this paper, we propose a novel method for normalizing facial depth images to frontal pose and neutral expression and extracting robust features from the normalized depth images. The method is implemented via two deep convolutional neural networks (DCNN), normalization network ($Net_{N}$) and feature extraction network ($Net_{F}$). Given a facial depth image, $Net_{N}$ first converts it to an HHA image, from which the 3D face is reconstructed via a DCNN. $Net_{N}$ then generates a pose-and-expression normalized (PEN) depth image from the reconstructed 3D face. The PEN depth image is finally passed to $Net_{F}$, which extracts a robust feature representation via another DCNN for face recognition. Our preliminary evaluation results demonstrate the superiority of the proposed method in recognizing faces of arbitrary poses and expressions with depth images.



### Image Denoising via Collaborative Dual-Domain Patch Filtering
- **Arxiv ID**: http://arxiv.org/abs/1805.00472v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.00472v1)
- **Published**: 2018-05-01 17:01:21+00:00
- **Updated**: 2018-05-01 17:01:21+00:00
- **Authors**: Muzammil Behzad
- **Comment**: 14 pages, 14 figures, 4 tables, article pending
- **Journal**: None
- **Summary**: In this paper, we propose a novel image denoising algorithm exploiting features from both spatial as well as transformed domain. We implement intensity-invariance based improved grouping for collaborative support-agnostic sparse reconstruction. For collaboration firstly, we stack similar-structured patches via intensity-invariant correlation measure. The grouped patches collaborate to yield desirable sparse estimates for noise filtering. This is because similar patches share the same support in the transformed domain, such similar supports can be used as probabilities of active taps to refine the sparse estimates. This ultimately produces a very useful patch estimate thus increasing the quality of recovered image by discarding the noise-causing components. A region growing based spatially developed post-processor is then applied to further enhance the smooth regions by extracting the spatial domain features. We also extend our proposed method for denoising of color images. Comparison results with the state-of-the-art algorithms in terms of peak signal-to-noise ratio (PNSR) and structural similarity (SSIM) index from extensive experimentations via a broad range of scenarios demonstrate the superiority of our proposed algorithm.



### Adapting Mask-RCNN for Automatic Nucleus Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1805.00500v1
- **DOI**: 10.1007/978-3-030-17798-0
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1805.00500v1)
- **Published**: 2018-05-01 18:11:38+00:00
- **Updated**: 2018-05-01 18:11:38+00:00
- **Authors**: Jeremiah W. Johnson
- **Comment**: 7 pages, 3 figures
- **Journal**: Proceedings of the 2019 Computer Vision Conference, Vol. 2
- **Summary**: Automatic segmentation of microscopy images is an important task in medical image processing and analysis. Nucleus detection is an important example of this task. Mask-RCNN is a recently proposed state-of-the-art algorithm for object detection, object localization, and object instance segmentation of natural images. In this paper we demonstrate that Mask-RCNN can be used to perform highly effective and efficient automatic segmentations of a wide range of microscopy images of cell nuclei, for a variety of cells acquired under a variety of conditions.



### Adaptive View Planning for Aerial 3D Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1805.00506v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG
- **Links**: [PDF](http://arxiv.org/pdf/1805.00506v2)
- **Published**: 2018-05-01 18:28:23+00:00
- **Updated**: 2019-09-20 18:28:48+00:00
- **Authors**: Cheng Peng, Volkan Isler
- **Comment**: None
- **Journal**: The 2019 International Conference on Robotics and Automation
  (ICRA)
- **Summary**: With the proliferation of small aerial vehicles, acquiring close up aerial imagery for high quality reconstruction of complex scenes is gaining importance. We present an adaptive view planning method to collect such images in an automated fashion. We start by sampling a small set of views to build a coarse proxy to the scene. We then present (i)~a method that builds a view manifold for view selection, and (ii) an algorithm to select a sparse set of views. The vehicle then visits these viewpoints to cover the scene, and the procedure is repeated until reconstruction quality converges or a desired level of quality is achieved. The view manifold provides an effective efficiency/quality compromise between using the entire 6 degree of freedom pose space and using a single view hemisphere to select the views.   Our results show that, in contrast to existing "explore and exploit" methods which collect only two sets of views, reconstruction quality can be drastically improved by adding a third set. They also indicate that three rounds of data collection is sufficient even for very complex scenes. We compare our algorithm to existing methods in three challenging scenes. We require each algorithm to select the same number of views. Our algorithm generates views which produce the least reconstruction error.



### Weakly Supervised Attention Learning for Textual Phrases Grounding
- **Arxiv ID**: http://arxiv.org/abs/1805.00545v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.00545v1)
- **Published**: 2018-05-01 20:34:37+00:00
- **Updated**: 2018-05-01 20:34:37+00:00
- **Authors**: Zhiyuan Fang, Shu Kong, Tianshu Yu, Yezhou Yang
- **Comment**: 4 pages, 3 figures
- **Journal**: None
- **Summary**: Grounding textual phrases in visual content is a meaningful yet challenging problem with various potential applications such as image-text inference or text-driven multimedia interaction. Most of the current existing methods adopt the supervised learning mechanism which requires ground-truth at pixel level during training. However, fine-grained level ground-truth annotation is quite time-consuming and severely narrows the scope for more general applications. In this extended abstract, we explore methods to localize flexibly image regions from the top-down signal (in a form of one-hot label or natural languages) with a weakly supervised attention learning mechanism. In our model, two types of modules are utilized: a backbone module for visual feature capturing, and an attentive module generating maps based on regularized bilinear pooling. We construct the model in an end-to-end fashion which is trained by encouraging the spatial attentive map to shift and focus on the region that consists of the best matched visual features with the top-down signal. We demonstrate the preliminary yet promising results on a testbed that is synthesized with multi-label MNIST data.



### Generating Synthetic X-ray Images of a Person from the Surface Geometry
- **Arxiv ID**: http://arxiv.org/abs/1805.00553v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.00553v2)
- **Published**: 2018-05-01 21:07:30+00:00
- **Updated**: 2018-05-14 14:21:32+00:00
- **Authors**: Brian Teixeira, Vivek Singh, Terrence Chen, Kai Ma, Birgi Tamersoy, Yifan Wu, Elena Balashova, Dorin Comaniciu
- **Comment**: accepted for spotlight presentation at CVPR 2018
- **Journal**: None
- **Summary**: We present a novel framework that learns to predict human anatomy from body surface. Specifically, our approach generates a synthetic X-ray image of a person only from the person's surface geometry. Furthermore, the synthetic X-ray image is parametrized and can be manipulated by adjusting a set of body markers which are also generated during the X-ray image prediction. With the proposed framework, multiple synthetic X-ray images can easily be generated by varying surface geometry. By perturbing the parameters, several additional synthetic X-ray images can be generated from the same surface geometry. As a result, our approach offers a potential to overcome the training data barrier in the medical domain. This capability is achieved by learning a pair of networks - one learns to generate the full image from the partial image and a set of parameters, and the other learns to estimate the parameters given the full image. During training, the two networks are trained iteratively such that they would converge to a solution where the predicted parameters and the full image are consistent with each other. In addition to medical data enrichment, our framework can also be used for image completion as well as anomaly detection.



### Secure Face Matching Using Fully Homomorphic Encryption
- **Arxiv ID**: http://arxiv.org/abs/1805.00577v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.00577v2)
- **Published**: 2018-05-01 23:46:41+00:00
- **Updated**: 2018-07-13 19:19:00+00:00
- **Authors**: Vishnu Naresh Boddeti
- **Comment**: BTAS 2018
- **Journal**: None
- **Summary**: Face recognition technology has demonstrated tremendous progress over the past few years, primarily due to advances in representation learning. As we witness the widespread adoption of these systems, it is imperative to consider the security of face representations. In this paper, we explore the practicality of using a fully homomorphic encryption based framework to secure a database of face templates. This framework is designed to preserve the privacy of users and prevent information leakage from the templates, while maintaining their utility through template matching directly in the encrypted domain. Additionally, we also explore a batching and dimensionality reduction scheme to trade-off face matching accuracy and computational complexity. Experiments on benchmark face datasets (LFW, IJB-A, IJB-B, CASIA) indicate that secure face matching can be practically feasible (16 KB template size and 0.01 sec per match pair for 512-dimensional features from SphereFace) while exhibiting minimal loss in matching performance.



