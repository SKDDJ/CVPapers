# Arxiv Papers in cs.CV on 2018-05-11
### Retinal Vessel Segmentation Based on Conditional Deep Convolutional Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1805.04224v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.04224v1)
- **Published**: 2018-05-11 02:17:04+00:00
- **Updated**: 2018-05-11 02:17:04+00:00
- **Authors**: Yun Jiang, Ning Tan
- **Comment**: in Chinese
- **Journal**: None
- **Summary**: The segmentation of retinal vessels is of significance for doctors to diagnose the fundus diseases. However, existing methods have various problems in the segmentation of the retinal vessels, such as insufficient segmentation of retinal vessels, weak anti-noise interference ability, and sensitivity to lesions, etc. Aiming to the shortcomings of existed methods, this paper proposes the use of conditional deep convolutional generative adversarial networks to segment the retinal vessels. We mainly improve the network structure of the generator. The introduction of the residual module at the convolutional layer for residual learning makes the network structure sensitive to changes in the output, as to better adjust the weight of the generator. In order to reduce the number of parameters and calculations, using a small convolution to halve the number of channels in the input signature before using a large convolution kernel. By used skip connection to connect the output of the convolutional layer with the output of the deconvolution layer to avoid low-level information sharing. By verifying the method on the DRIVE and STARE datasets, the segmentation accuracy rate is 96.08% and 97.71%, the sensitivity reaches 82.74% and 85.34% respectively, and the F-measure reaches 82.08% and 85.02% respectively. The sensitivity is 4.82% and 2.4% higher than that of R2U-Net.



### Just-in-Time Reconstruction: Inpainting Sparse Maps using Single View Depth Predictors as Priors
- **Arxiv ID**: http://arxiv.org/abs/1805.04239v1
- **DOI**: 10.1109/ICRA.2018.8460549
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.04239v1)
- **Published**: 2018-05-11 04:08:59+00:00
- **Updated**: 2018-05-11 04:08:59+00:00
- **Authors**: Chamara Saroj Weerasekera, Thanuja Dharmasiri, Ravi Garg, Tom Drummond, Ian Reid
- **Comment**: ICRA 2018
- **Journal**: None
- **Summary**: We present ``just-in-time reconstruction" as real-time image-guided inpainting of a map with arbitrary scale and sparsity to generate a fully dense depth map for the image. In particular, our goal is to inpaint a sparse map --- obtained from either a monocular visual SLAM system or a sparse sensor --- using a single-view depth prediction network as a virtual depth sensor. We adopt a fairly standard approach to data fusion, to produce a fused depth map by performing inference over a novel fully-connected Conditional Random Field (CRF) which is parameterized by the input depth maps and their pixel-wise confidence weights. Crucially, we obtain the confidence weights that parameterize the CRF model in a data-dependent manner via Convolutional Neural Networks (CNNs) which are trained to model the conditional depth error distributions given each source of input depth map and the associated RGB image. Our CRF model penalises absolute depth error in its nodes and pairwise scale-invariant depth error in its edges, and the confidence-based fusion minimizes the impact of outlier input depth values on the fused result. We demonstrate the flexibility of our method by real-time inpainting of ORB-SLAM, Kinect, and LIDAR depth maps acquired both indoors and outdoors at arbitrary scale and varied amount of irregular sparsity.



### Reciprocal Attention Fusion for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1805.04247v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1805.04247v2)
- **Published**: 2018-05-11 06:13:56+00:00
- **Updated**: 2018-07-22 06:16:54+00:00
- **Authors**: Moshiur R Farazi, Salman H Khan
- **Comment**: To appear in the British Machine Vision Conference (BMVC), September
  2018
- **Journal**: Proceedings of the British Machine Vision Conference (250) 2018
- **Summary**: Existing attention mechanisms either attend to local image grid or object level features for Visual Question Answering (VQA). Motivated by the observation that questions can relate to both object instances and their parts, we propose a novel attention mechanism that jointly considers reciprocal relationships between the two levels of visual details. The bottom-up attention thus generated is further coalesced with the top-down information to only focus on the scene elements that are most relevant to a given question. Our design hierarchically fuses multi-modal information i.e., language, object- and gird-level features, through an efficient tensor decomposition scheme. The proposed model improves the state-of-the-art single model performances from 67.9% to 68.2% on VQAv1 and from 65.7% to 67.4% on VQAv2, demonstrating a significant boost.



### Adaptive Selection of Deep Learning Models on Embedded Systems
- **Arxiv ID**: http://arxiv.org/abs/1805.04252v1
- **DOI**: None
- **Categories**: **cs.PF**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1805.04252v1)
- **Published**: 2018-05-11 06:53:59+00:00
- **Updated**: 2018-05-11 06:53:59+00:00
- **Authors**: Ben Taylor, Vicent Sanz Marco, Willy Wolff, Yehia Elkhatib, Zheng Wang
- **Comment**: Accepted to be published at LCTES 2018
- **Journal**: None
- **Summary**: The recent ground-breaking advances in deep learning networks ( DNNs ) make them attractive for embedded systems. However, it can take a long time for DNNs to make an inference on resource-limited embedded devices. Offloading the computation into the cloud is often infeasible due to privacy concerns, high latency, or the lack of connectivity. As such, there is a critical need to find a way to effectively execute the DNN models locally on the devices. This paper presents an adaptive scheme to determine which DNN model to use for a given input, by considering the desired accuracy and inference time. Our approach employs machine learning to develop a predictive model to quickly select a pre-trained DNN to use for a given input and the optimization constraint. We achieve this by first training off-line a predictive model, and then use the learnt model to select a DNN model to use for new, unseen inputs. We apply our approach to the image classification task and evaluate it on a Jetson TX2 embedded deep learning platform using the ImageNet ILSVRC 2012 validation dataset. We consider a range of influential DNN models. Experimental results show that our approach achieves a 7.52% improvement in inference accuracy, and a 1.8x reduction in inference time over the most-capable single DNN model.



### Stingray Detection of Aerial Images Using Augmented Training Images Generated by A Conditional Generative Model
- **Arxiv ID**: http://arxiv.org/abs/1805.04262v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.04262v3)
- **Published**: 2018-05-11 07:29:23+00:00
- **Updated**: 2018-06-25 06:44:41+00:00
- **Authors**: Yi-Min Chou, Chien-Hung Chen, Keng-Hao Liu, Chu-Song Chen
- **Comment**: to appear in CVPR 2018 Workshop (CVPR 2018 Workshop and Challenge:
  Automated Analysis of Marine Video for Environmental Monitoring)
- **Journal**: None
- **Summary**: In this paper, we present an object detection method that tackles the stingray detection problem based on aerial images. In this problem, the images are aerially captured on a sea-surface area by using an Unmanned Aerial Vehicle (UAV), and the stingrays swimming under (but close to) the sea surface are the target we want to detect and locate. To this end, we use a deep object detection method, faster RCNN, to train a stingray detector based on a limited training set of images. To boost the performance, we develop a new generative approach, conditional GLO, to increase the training samples of stingray, which is an extension of the Generative Latent Optimization (GLO) approach. Unlike traditional data augmentation methods that generate new data only for image classification, our proposed method that mixes foreground and background together can generate new data for an object detection task, and thus improve the training efficacy of a CNN detector. Experimental results show that satisfiable performance can be obtained by using our approach on stingray detection in aerial images.



### Clinical evaluation of semi-automatic opensource algorithmic software segmentation of the mandibular bone: Practical feasibility and assessment of a new course of action
- **Arxiv ID**: http://arxiv.org/abs/1805.08604v1
- **DOI**: 10.1371/journal.pone.0196378
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.08604v1)
- **Published**: 2018-05-11 08:38:58+00:00
- **Updated**: 2018-05-11 08:38:58+00:00
- **Authors**: JÃ¼rgen Wallner, Kerstin Hochegger, Xiaojun Chen, Irene Mischak, Knut Reinbacher, Mauro Pau, Tomislav Zrnc, Katja Schwenzer-Zimmerer, Wolfgang Zemann, Dieter Schmalstieg, Jan Egger
- **Comment**: 26 pages
- **Journal**: Wallner J, et al. (2018) Clinical evaluation of semi-automatic
  open-source algorithmic software segmentation of the mandibular bone:
  Practical feasibility and assessment of a new course of action. PLoS ONE
  13(5):e0196378
- **Summary**: Computer assisted technologies based on algorithmic software segmentation are an increasing topic of interest in complex surgical cases. However - due to functional instability, time consuming software processes, personnel resources or licensed-based financial costs many segmentation processes are often outsourced from clinical centers to third parties and the industry. Therefore, the aim of this trial was to assess the practical feasibility of an easy available, functional stable and licensed-free segmentation approach to be used in the clinical practice. In this retrospective, randomized, controlled trail the accuracy and accordance of the open-source based segmentation algorithm GrowCut (GC) was assessed through the comparison to the manually generated ground truth of the same anatomy using 10 CT lower jaw data-sets from the clinical routine. Assessment parameters were the segmentation time, the volume, the voxel number, the Dice Score (DSC) and the Hausdorff distance (HD). Overall segmentation times were about one minute. Mean DSC values of over 85% and HD below 33.5 voxel could be achieved. Statistical differences between the assessment parameters were not significant (p<0.05) and correlation coefficients were close to the value one (r > 0.94). Complete functional stable and time saving segmentations with high accuracy and high positive correlation could be performed by the presented interactive open-source based approach. In the cranio-maxillofacial complex the used method could represent an algorithmic alternative for image-based segmentation in the clinical practice for e.g. surgical treatment planning or visualization of postoperative results and offers several advantages. Systematic comparisons to other segmentation approaches or with a greater data amount are areas of future works.



### Piecewise classifier mappings: Learning fine-grained learners for novel categories with few examples
- **Arxiv ID**: http://arxiv.org/abs/1805.04288v2
- **DOI**: 10.1109/TIP.2019.2924811
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.04288v2)
- **Published**: 2018-05-11 09:24:15+00:00
- **Updated**: 2019-06-21 08:03:07+00:00
- **Authors**: Xiu-Shen Wei, Peng Wang, Lingqiao Liu, Chunhua Shen, Jianxin Wu
- **Comment**: Accepted by IEEE TIP
- **Journal**: None
- **Summary**: Humans are capable of learning a new fine-grained concept with very little supervision, \emph{e.g.}, few exemplary images for a species of bird, yet our best deep learning systems need hundreds or thousands of labeled examples. In this paper, we try to reduce this gap by studying the fine-grained image recognition problem in a challenging few-shot learning setting, termed few-shot fine-grained recognition (FSFG). The task of FSFG requires the learning systems to build classifiers for novel fine-grained categories from few examples (only one or less than five). To solve this problem, we propose an end-to-end trainable deep network which is inspired by the state-of-the-art fine-grained recognition model and is tailored for the FSFG task.   Specifically, our network consists of a bilinear feature learning module and a classifier mapping module: while the former encodes the discriminative information of an exemplar image into a feature vector, the latter maps the intermediate feature into the decision boundary of the novel category. The key novelty of our model is a "piecewise mappings" function in the classifier mapping module, which generates the decision boundary via learning a set of more attainable sub-classifiers in a more parameter-economic way. We learn the exemplar-to-classifier mapping based on an auxiliary dataset in a meta-learning fashion, which is expected to be able to generalize to novel categories. By conducting comprehensive experiments on three fine-grained datasets, we demonstrate that the proposed method achieves superior performance over the competing baselines.



### Weakly and Semi Supervised Human Body Part Parsing via Pose-Guided Knowledge Transfer
- **Arxiv ID**: http://arxiv.org/abs/1805.04310v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.04310v1)
- **Published**: 2018-05-11 10:30:56+00:00
- **Updated**: 2018-05-11 10:30:56+00:00
- **Authors**: Hao-Shu Fang, Guansong Lu, Xiaolin Fang, Jianwen Xie, Yu-Wing Tai, Cewu Lu
- **Comment**: CVPR'18 spotlight
- **Journal**: None
- **Summary**: Human body part parsing, or human semantic part segmentation, is fundamental to many computer vision tasks. In conventional semantic segmentation methods, the ground truth segmentations are provided, and fully convolutional networks (FCN) are trained in an end-to-end scheme. Although these methods have demonstrated impressive results, their performance highly depends on the quantity and quality of training data. In this paper, we present a novel method to generate synthetic human part segmentation data using easily-obtained human keypoint annotations. Our key idea is to exploit the anatomical similarity among human to transfer the parsing results of a person to another person with similar pose. Using these estimated results as additional training data, our semi-supervised model outperforms its strong-supervised counterpart by 6 mIOU on the PASCAL-Person-Part dataset, and we achieve state-of-the-art human parsing results. Our approach is general and can be readily extended to other object/animal parsing task assuming that their anatomical similarity can be annotated by keypoints. The proposed model and accompanying source code are available at https://github.com/MVIG-SJTU/WSHP



### Exploiting Images for Video Recognition with Hierarchical Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1805.04384v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.04384v1)
- **Published**: 2018-05-11 13:20:04+00:00
- **Updated**: 2018-05-11 13:20:04+00:00
- **Authors**: Feiwu Yu, Xinxiao Wu, Yuchao Sun, Lixin Duan
- **Comment**: IJCAI 2018
- **Journal**: None
- **Summary**: Existing deep learning methods of video recognition usually require a large number of labeled videos for training. But for a new task, videos are often unlabeled and it is also time-consuming and labor-intensive to annotate them. Instead of human annotation, we try to make use of existing fully labeled images to help recognize those videos. However, due to the problem of domain shifts and heterogeneous feature representations, the performance of classifiers trained on images may be dramatically degraded for video recognition tasks. In this paper, we propose a novel method, called Hierarchical Generative Adversarial Networks (HiGAN), to enhance recognition in videos (i.e., target domain) by transferring knowledge from images (i.e., source domain). The HiGAN model consists of a \emph{low-level} conditional GAN and a \emph{high-level} conditional GAN. By taking advantage of these two-level adversarial learning, our method is capable of learning a domain-invariant feature representation of source images and target videos. Comprehensive experiments on two challenging video recognition datasets (i.e. UCF101 and HMDB51) demonstrate the effectiveness of the proposed method when compared with the existing state-of-the-art domain adaptation methods.



### Weakly Supervised Domain-Specific Color Naming Based on Attention
- **Arxiv ID**: http://arxiv.org/abs/1805.04385v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.04385v1)
- **Published**: 2018-05-11 13:21:37+00:00
- **Updated**: 2018-05-11 13:21:37+00:00
- **Authors**: Lu Yu, Yongmei Cheng, Joost van de Weijer
- **Comment**: Accepted at ICPR2018
- **Journal**: None
- **Summary**: The majority of existing color naming methods focuses on the eleven basic color terms of the English language. However, in many applications, different sets of color names are used for the accurate description of objects. Labeling data to learn these domain-specific color names is an expensive and laborious task. Therefore, in this article we aim to learn color names from weakly labeled data. For this purpose, we add an attention branch to the color naming network. The attention branch is used to modulate the pixel-wise color naming predictions of the network. In experiments, we illustrate that the attention branch correctly identifies the relevant regions. Furthermore, we show that our method obtains state-of-the-art results for pixel-wise and image-wise classification on the EBAY dataset and is able to learn color names for various domains.



### Iteratively Trained Interactive Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1805.04398v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.04398v1)
- **Published**: 2018-05-11 13:43:27+00:00
- **Updated**: 2018-05-11 13:43:27+00:00
- **Authors**: Sabarinath Mahadevan, Paul Voigtlaender, Bastian Leibe
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning requires large amounts of training data to be effective. For the task of object segmentation, manually labeling data is very expensive, and hence interactive methods are needed. Following recent approaches, we develop an interactive object segmentation system which uses user input in the form of clicks as the input to a convolutional network. While previous methods use heuristic click sampling strategies to emulate user clicks during training, we propose a new iterative training strategy. During training, we iteratively add clicks based on the errors of the currently predicted segmentation. We show that our iterative training strategy together with additional improvements to the network architecture results in improved results over the state-of-the-art.



### PAD-Net: Multi-Tasks Guided Prediction-and-Distillation Network for Simultaneous Depth Estimation and Scene Parsing
- **Arxiv ID**: http://arxiv.org/abs/1805.04409v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.04409v1)
- **Published**: 2018-05-11 14:12:17+00:00
- **Updated**: 2018-05-11 14:12:17+00:00
- **Authors**: Dan Xu, Wanli Ouyang, Xiaogang Wang, Nicu Sebe
- **Comment**: Accepted at CVPR 2018
- **Journal**: None
- **Summary**: Depth estimation and scene parsing are two particularly important tasks in visual scene understanding. In this paper we tackle the problem of simultaneous depth estimation and scene parsing in a joint CNN. The task can be typically treated as a deep multi-task learning problem [42]. Different from previous methods directly optimizing multiple tasks given the input training data, this paper proposes a novel multi-task guided prediction-and-distillation network (PAD-Net), which first predicts a set of intermediate auxiliary tasks ranging from low level to high level, and then the predictions from these intermediate auxiliary tasks are utilized as multi-modal input via our proposed multi-modal distillation modules for the final tasks. During the joint learning, the intermediate tasks not only act as supervision for learning more robust deep representations but also provide rich multi-modal information for improving the final tasks. Extensive experiments are conducted on two challenging datasets (i.e. NYUD-v2 and Cityscapes) for both the depth estimation and scene parsing tasks, demonstrating the effectiveness of the proposed approach.



### Novel Deep Learning Model for Traffic Sign Detection Using Capsule Networks
- **Arxiv ID**: http://arxiv.org/abs/1805.04424v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.04424v1)
- **Published**: 2018-05-11 14:34:15+00:00
- **Updated**: 2018-05-11 14:34:15+00:00
- **Authors**: Amara Dinesh Kumar
- **Comment**: 5 pages,3 figures
- **Journal**: International Journal of Pure and Applied Mathematics Volume 118
  No. 20 2018, 4543-4548 ISSN: 1314-3395
- **Summary**: Convolutional neural networks are the most widely used deep learning algorithms for traffic signal classification till date but they fail to capture pose, view, orientation of the images because of the intrinsic inability of max pooling layer.This paper proposes a novel method for Traffic sign detection using deep learning architecture called capsule networks that achieves outstanding performance on the German traffic sign dataset.Capsule network consists of capsules which are a group of neurons representing the instantiating parameters of an object like the pose and orientation by using the dynamic routing and route by agreement algorithms.unlike the previous approaches of manual feature extraction,multiple deep neural networks with many parameters,our method eliminates the manual effort and provides resistance to the spatial variances.CNNs can be fooled easily using various adversary attacks and capsule networks can overcome such attacks from the intruders and can offer more reliability in traffic sign detection for autonomous vehicles.Capsule network have achieved the state-of-the-art accuracy of 97.6% on German Traffic Sign Recognition Benchmark dataset (GTSRB).



### Non-Stationary Texture Synthesis by Adversarial Expansion
- **Arxiv ID**: http://arxiv.org/abs/1805.04487v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1805.04487v1)
- **Published**: 2018-05-11 16:46:52+00:00
- **Updated**: 2018-05-11 16:46:52+00:00
- **Authors**: Yang Zhou, Zhen Zhu, Xiang Bai, Dani Lischinski, Daniel Cohen-Or, Hui Huang
- **Comment**: Accepted to SIGGRAPH 2018
- **Journal**: None
- **Summary**: The real world exhibits an abundance of non-stationary textures. Examples include textures with large-scale structures, as well as spatially variant and inhomogeneous textures. While existing example-based texture synthesis methods can cope well with stationary textures, non-stationary textures still pose a considerable challenge, which remains unresolved. In this paper, we propose a new approach for example-based non-stationary texture synthesis. Our approach uses a generative adversarial network (GAN), trained to double the spatial extent of texture blocks extracted from a specific texture exemplar. Once trained, the fully convolutional generator is able to expand the size of the entire exemplar, as well as of any of its sub-blocks. We demonstrate that this conceptually simple approach is highly effective for capturing large-scale structures, as well as other non-stationary attributes of the input exemplar. As a result, it can cope with challenging textures, which, to our knowledge, no other existing method can handle.



### Augmented Skeleton Space Transfer for Depth-based Hand Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1805.04497v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.04497v1)
- **Published**: 2018-05-11 17:28:49+00:00
- **Updated**: 2018-05-11 17:28:49+00:00
- **Authors**: Seungryul Baek, Kwang In Kim, Tae-Kyun Kim
- **Comment**: Accepted to CVPR 2018
- **Journal**: None
- **Summary**: Crucial to the success of training a depth-based 3D hand pose estimator (HPE) is the availability of comprehensive datasets covering diverse camera perspectives, shapes, and pose variations. However, collecting such annotated datasets is challenging. We propose to complete existing databases by generating new database entries. The key idea is to synthesize data in the skeleton space (instead of doing so in the depth-map space) which enables an easy and intuitive way of manipulating data entries. Since the skeleton entries generated in this way do not have the corresponding depth map entries, we exploit them by training a separate hand pose generator (HPG) which synthesizes the depth map from the skeleton entries. By training the HPG and HPE in a single unified optimization framework enforcing that 1) the HPE agrees with the paired depth and skeleton entries; and 2) the HPG-HPE combination satisfies the cyclic consistency (both the input and the output of HPG-HPE are skeletons) observed via the newly generated unpaired skeletons, our algorithm constructs a HPE which is robust to variations that go beyond the coverage of the existing database. Our training algorithm adopts the generative adversarial networks (GAN) training process. As a by-product, we obtain a hand pose discriminator (HPD) that is capable of picking out realistic hand poses. Our algorithm exploits this capability to refine the initial skeleton estimates in testing, further improving the accuracy. We test our algorithm on four challenging benchmark datasets (ICVL, MSRA, NYU and Big Hand 2.2M datasets) and demonstrate that our approach outperforms or is on par with state-of-the-art methods quantitatively and qualitatively.



### A volumetric deep Convolutional Neural Network for simulation of mock dark matter halo catalogues
- **Arxiv ID**: http://arxiv.org/abs/1805.04537v2
- **DOI**: 10.1093/mnras/sty2949
- **Categories**: **astro-ph.CO**, astro-ph.IM, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1805.04537v2)
- **Published**: 2018-05-11 18:05:50+00:00
- **Updated**: 2018-11-19 15:43:53+00:00
- **Authors**: Philippe Berger, George Stein
- **Comment**: 12 pages, 8 figures, 1 table. Accepted to MNRAS
- **Journal**: Monthly Notices of the Royal Astronomical Society, Volume 482,
  Issue 3, p.2861-2871, 2019
- **Summary**: For modern large-scale structure survey techniques it has become standard practice to test data analysis pipelines on large suites of mock simulations, a task which is currently prohibitively expensive for full N-body simulations. Instead of calculating this costly gravitational evolution, we have trained a three-dimensional deep Convolutional Neural Network (CNN) to identify dark matter protohalos directly from the cosmological initial conditions. Training on halo catalogues from the Peak Patch semi-analytic code, we test various CNN architectures and find they generically achieve a Dice coefficient of ~92% in only 24 hours of training. We present a simple and fast geometric halo finding algorithm to extract halos from this powerful pixel-wise binary classifier and find that the predicted catalogues match the mass function and power spectra of the ground truth simulations to within ~10%. We investigate the effect of long-range tidal forces on an object-by-object basis and find that the network's predictions are consistent with the non-linear ellipsoidal collapse equations used explicitly by the Peak Patch algorithm.



### ContextNet: Exploring Context and Detail for Semantic Segmentation in Real-time
- **Arxiv ID**: http://arxiv.org/abs/1805.04554v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.04554v4)
- **Published**: 2018-05-11 18:52:45+00:00
- **Updated**: 2018-11-05 12:41:28+00:00
- **Authors**: Rudra P K Poudel, Ujwal Bonde, Stephan Liwicki, Christopher Zach
- **Comment**: Published as a conference paper at British Machine Vision Conference
  (BMVC), 2018
- **Journal**: None
- **Summary**: Modern deep learning architectures produce highly accurate results on many challenging semantic segmentation datasets. State-of-the-art methods are, however, not directly transferable to real-time applications or embedded devices, since naive adaptation of such systems to reduce computational cost (speed, memory and energy) causes a significant drop in accuracy. We propose ContextNet, a new deep neural network architecture which builds on factorized convolution, network compression and pyramid representation to produce competitive semantic segmentation in real-time with low memory requirement. ContextNet combines a deep network branch at low resolution that captures global context information efficiently with a shallow branch that focuses on high-resolution segmentation details. We analyse our network in a thorough ablation study and present results on the Cityscapes dataset, achieving 66.1% accuracy at 18.3 frames per second at full (1024x2048) resolution (41.9 fps with pipelined computations for streamed data).



### Classification of Protein Crystallization X-Ray Images Using Major Convolutional Neural Network Architectures
- **Arxiv ID**: http://arxiv.org/abs/1805.04563v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.04563v1)
- **Published**: 2018-05-11 19:15:21+00:00
- **Updated**: 2018-05-11 19:15:21+00:00
- **Authors**: Soheil Ghafurian, Peter Orth, Corey Strickland, Hua Su, Sangita Patel, Steven Soisson, Belma Dogdas
- **Comment**: None
- **Journal**: None
- **Summary**: The generation of protein crystals is necessary for the study of protein molecular function and structure. This is done empirically by processing large numbers of crystallization trials and inspecting them regularly in search of those with forming crystals. To avoid missing the hard-gained crystals, this visual inspection of the trial X-ray images is done manually as opposed to the existing less accurate machine learning methods. To achieve higher accuracy for automation, we applied some of the most successful convolutional neural networks (ResNet, Inception, VGG, and AlexNet) for 10-way classification of the X-ray images. We showed that substantial classification accuracy is gained by using such networks compared to two simpler ones previously proposed for this purpose. The best accuracy was obtained from ResNet (81.43%), which corresponds to a missed crystal rate of 5.9%. This rate could be lowered to less than 0.1% by using a top-3 classification strategy. Our dataset consisted of 486,000 internally annotated images, which was augmented to more than a million to address class imbalance. We also provide a label-wise analysis of the results, identifying the main sources of error and inaccuracy.



### Revisiting Dilated Convolution: A Simple Approach for Weakly- and Semi- Supervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1805.04574v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.04574v2)
- **Published**: 2018-05-11 19:53:41+00:00
- **Updated**: 2018-05-28 01:18:12+00:00
- **Authors**: Yunchao Wei, Huaxin Xiao, Honghui Shi, Zequn Jie, Jiashi Feng, Thomas S. Huang
- **Comment**: Accepted by CVPR 2018 as Spotlight
- **Journal**: None
- **Summary**: Despite the remarkable progress, weakly supervised segmentation approaches are still inferior to their fully supervised counterparts. We obverse the performance gap mainly comes from their limitation on learning to produce high-quality dense object localization maps from image-level supervision. To mitigate such a gap, we revisit the dilated convolution [1] and reveal how it can be utilized in a novel way to effectively overcome this critical limitation of weakly supervised segmentation approaches. Specifically, we find that varying dilation rates can effectively enlarge the receptive fields of convolutional kernels and more importantly transfer the surrounding discriminative information to non-discriminative object regions, promoting the emergence of these regions in the object localization maps. Then, we design a generic classification network equipped with convolutional blocks of different dilated rates. It can produce dense and reliable object localization maps and effectively benefit both weakly- and semi- supervised semantic segmentation. Despite the apparent simplicity, our proposed approach obtains superior performance over state-of-the-arts. In particular, it achieves 60.8% and 67.6% mIoU scores on Pascal VOC 2012 test set in weakly- (only image-level labels are available) and semi- (1,464 segmentation masks are available) supervised settings, which are the new state-of-the-arts.



### The Domain Transform Solver
- **Arxiv ID**: http://arxiv.org/abs/1805.04590v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.04590v1)
- **Published**: 2018-05-11 20:57:46+00:00
- **Updated**: 2018-05-11 20:57:46+00:00
- **Authors**: Akash Bapat, Jan-Michael Frahm
- **Comment**: None
- **Journal**: None
- **Summary**: We present a framework for edge-aware optimization that is an order of magnitude faster than the state of the art while having comparable performance. Our key insight is that the optimization can be formulated by leveraging properties of the domain transform, a method for edge-aware filtering that defines a distance-preserving 1D mapping of the input space. This enables our method to improve performance for a variety of problems including stereo, depth super-resolution, and render from defocus, while keeping the computational complexity linear in the number of pixels. Our method is highly parallelizable and adaptable, and it has demonstrable scalability with respect to image resolution.



### Joint Flow: Temporal Flow Fields for Multi Person Tracking
- **Arxiv ID**: http://arxiv.org/abs/1805.04596v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.04596v2)
- **Published**: 2018-05-11 21:27:08+00:00
- **Updated**: 2018-07-20 13:04:02+00:00
- **Authors**: Andreas Doering, Umar Iqbal, Juergen Gall
- **Comment**: Accepted to BMVC
- **Journal**: None
- **Summary**: In this work we propose an online multi person pose tracking approach which works on two consecutive frames $I_{t-1}$ and $I_t$. The general formulation of our temporal network allows to rely on any multi person pose estimation approach as spatial network. From the spatial network we extract image features and pose features for both frames. These features serve as input for our temporal model that predicts Temporal Flow Fields (TFF). These TFF are vector fields which indicate the direction in which each body joint is going to move from frame $I_{t-1}$ to frame $I_t$. This novel representation allows to formulate a similarity measure of detected joints. These similarities are used as binary potentials in a bipartite graph optimization problem in order to perform tracking of multiple poses. We show that these TFF can be learned by a relative small CNN network whilst achieving state-of-the-art multi person pose tracking results.



### Unsupervised Learning for Fast Probabilistic Diffeomorphic Registration
- **Arxiv ID**: http://arxiv.org/abs/1805.04605v2
- **DOI**: 10.1007/978-3-030-00928-1_82
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1805.04605v2)
- **Published**: 2018-05-11 22:12:01+00:00
- **Updated**: 2018-09-14 13:28:36+00:00
- **Authors**: Adrian V. Dalca, Guha Balakrishnan, John Guttag, Mert R. Sabuncu
- **Comment**: MICCAI 2018 (Oral Presentation). Proceedings: LNCS 11070, pp 729-738
- **Journal**: LNCS 11070, pp 729-738, Springer. 2018
- **Summary**: Traditional deformable registration techniques achieve impressive results and offer a rigorous theoretical treatment, but are computationally intensive since they solve an optimization problem for each image pair. Recently, learning-based methods have facilitated fast registration by learning spatial deformation functions. However, these approaches use restricted deformation models, require supervised labels, or do not guarantee a diffeomorphic (topology-preserving) registration. Furthermore, learning-based registration tools have not been derived from a probabilistic framework that can offer uncertainty estimates. In this paper, we present a probabilistic generative model and derive an unsupervised learning-based inference algorithm that makes use of recent developments in convolutional neural networks (CNNs). We demonstrate our method on a 3D brain registration task, and provide an empirical analysis of the algorithm. Our approach results in state of the art accuracy and very fast runtimes, while providing diffeomorphic guarantees and uncertainty estimates. Our implementation is available online at http://voxelmorph.csail.mit.edu .



