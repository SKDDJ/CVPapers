# Arxiv Papers in cs.CV on 2018-05-06
### A Counter-Forensic Method for CNN-Based Camera Model Identification
- **Arxiv ID**: http://arxiv.org/abs/1805.02131v1
- **DOI**: 10.1109/CVPRW.2017.230
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.02131v1)
- **Published**: 2018-05-06 01:32:08+00:00
- **Updated**: 2018-05-06 01:32:08+00:00
- **Authors**: David GÃ¼era, Yu Wang, Luca Bondi, Paolo Bestagini, Stefano Tubaro, Edward J. Delp
- **Comment**: Presented at the IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR), Workshop on Media Forensics
- **Journal**: None
- **Summary**: An increasing number of digital images are being shared and accessed through websites, media, and social applications. Many of these images have been modified and are not authentic. Recent advances in the use of deep convolutional neural networks (CNNs) have facilitated the task of analyzing the veracity and authenticity of largely distributed image datasets. We examine in this paper the problem of identifying the camera model or type that was used to take an image and that can be spoofed. Due to the linear nature of CNNs and the high-dimensionality of images, neural networks are vulnerable to attacks with adversarial examples. These examples are imperceptibly different from correctly classified images but are misclassified with high confidence by CNNs. In this paper, we describe a counter-forensic method capable of subtly altering images to change their estimated camera model when they are analyzed by any CNN-based camera model detector. Our method can use both the Fast Gradient Sign Method (FGSM) or the Jacobian-based Saliency Map Attack (JSMA) to craft these adversarial images and does not require direct access to the CNN. Our results show that even advanced deep learning architectures trained to analyze images and obtain camera model information are still vulnerable to our proposed method.



### An Image dehazing approach based on the airlight field estimation
- **Arxiv ID**: http://arxiv.org/abs/1805.02142v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.02142v1)
- **Published**: 2018-05-06 03:18:20+00:00
- **Updated**: 2018-05-06 03:18:20+00:00
- **Authors**: Lijun Zhang, Yongbin Gao, Yujin Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a scheme for single image haze removal based on the airlight field (ALF) estimation. Conventional image dehazing methods which are based on a physical model generally take the global atmospheric light as a constant. However, the constant-airlight assumption may be unsuitable for images with large sky regions, which causes unacceptable brightness imbalance and color distortion in recovery images. This paper models the atmospheric light as a field function, and presents a maximum a-priori (MAP) method for jointly estimating the airlight field, the transmission rate and the haze free image. We also introduce a valid haze-level prior for effective estimate of transmission. Evaluation on real world images shows that the proposed approach outperforms existing methods in single image dehazing, especially when the large sky region is included.



### Quantization Mimic: Towards Very Tiny CNN for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1805.02152v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.02152v3)
- **Published**: 2018-05-06 05:36:07+00:00
- **Updated**: 2018-09-13 09:03:58+00:00
- **Authors**: Yi Wei, Xinyu Pan, Hongwei Qin, Wanli Ouyang, Junjie Yan
- **Comment**: Accepted to ECCV 2018. Version for conference submission (polish and
  use theoretical size in Table 1)
- **Journal**: None
- **Summary**: In this paper, we propose a simple and general framework for training very tiny CNNs for object detection. Due to limited representation ability, it is challenging to train very tiny networks for complicated tasks like detection. To the best of our knowledge, our method, called Quantization Mimic, is the first one focusing on very tiny networks. We utilize two types of acceleration methods: mimic and quantization. Mimic improves the performance of a student network by transfering knowledge from a teacher network. Quantization converts a full-precision network to a quantized one without large degradation of performance. If the teacher network is quantized, the search scope of the student network will be smaller. Using this feature of the quantization, we propose Quantization Mimic. It first quantizes the large network, then mimic a quantized small network. The quantization operation can help student network to better match the feature maps from teacher network. To evaluate our approach, we carry out experiments on various popular CNNs including VGG and Resnet, as well as different detection frameworks including Faster R-CNN and R-FCN. Experiments on Pascal VOC and WIDER FACE verify that our Quantization Mimic algorithm can be applied on various settings and outperforms state-of-the-art model acceleration methods given limited computing resouces.



### Acceleration of RED via Vector Extrapolation
- **Arxiv ID**: http://arxiv.org/abs/1805.02158v2
- **DOI**: None
- **Categories**: **cs.CV**, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/1805.02158v2)
- **Published**: 2018-05-06 06:43:02+00:00
- **Updated**: 2019-04-01 21:00:07+00:00
- **Authors**: Tao Hong, Yaniv Romano, Michael Elad
- **Comment**: 5 figures, 2 tables
- **Journal**: None
- **Summary**: Models play an important role in inverse problems, serving as the prior for representing the original signal to be recovered. REgularization by Denoising (RED) is a recently introduced general framework for constructing such priors using state-of-the-art denoising algorithms. Using RED, solving inverse problems is shown to amount to an iterated denoising process. However, as the complexity of denoising algorithms is generally high, this might lead to an overall slow algorithm. In this paper, we suggest an accelerated technique based on vector extrapolation (VE) to speed-up existing RED solvers. Numerical experiments validate the obtained gain by VE, leading to a substantial savings in computations compared with the original fixed-point method.



### Multi-Scale Face Restoration with Sequential Gating Ensemble Network
- **Arxiv ID**: http://arxiv.org/abs/1805.02164v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.02164v1)
- **Published**: 2018-05-06 07:38:42+00:00
- **Updated**: 2018-05-06 07:38:42+00:00
- **Authors**: Jianxin Lin, Tiankuang Zhou, Zhibo Chen
- **Comment**: 8 pages, 7 figures, Thirty-Second AAAI Conference on Artificial
  Intelligence (AAAI-18)
- **Journal**: None
- **Summary**: Restoring face images from distortions is important in face recognition applications and is challenged by multiple scale issues, which is still not well-solved in research area. In this paper, we present a Sequential Gating Ensemble Network (SGEN) for multi-scale face restoration issue. We first employ the principle of ensemble learning into SGEN architecture design to reinforce predictive performance of the network. The SGEN aggregates multi-level base-encoders and base-decoders into the network, which enables the network to contain multiple scales of receptive field. Instead of combining these base-en/decoders directly with non-sequential operations, the SGEN takes base-en/decoders from different levels as sequential data. Specifically, the SGEN learns to sequentially extract high level information from base-encoders in bottom-up manner and restore low level information from base-decoders in top-down manner. Besides, we propose to realize bottom-up and top-down information combination and selection with Sequential Gating Unit (SGU). The SGU sequentially takes two inputs from different levels and decides the output based on one active input. Experiment results demonstrate that our SGEN is more effective at multi-scale human face restoration with more image details and less noise than state-of-the-art image restoration models. By using adversarial training, SGEN also produces more visually preferred results than other models through subjective evaluation.



### Joint CS-MRI Reconstruction and Segmentation with a Unified Deep Network
- **Arxiv ID**: http://arxiv.org/abs/1805.02165v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.02165v1)
- **Published**: 2018-05-06 07:45:32+00:00
- **Updated**: 2018-05-06 07:45:32+00:00
- **Authors**: Liyan Sun, Zhiwen Fan, Yue Huang, Xinghao Ding, John Paisley
- **Comment**: 8 pages, 6 figures, 3 tables
- **Journal**: None
- **Summary**: The need for fast acquisition and automatic analysis of MRI data is growing in the age of big data. Although compressed sensing magnetic resonance imaging (CS-MRI) has been studied to accelerate MRI by reducing k-space measurements, in current CS-MRI techniques MRI applications such as segmentation are overlooked when doing image reconstruction. In this paper, we test the utility of CS-MRI methods in automatic segmentation models and propose a unified deep neural network architecture called SegNetMRI which we apply to the combined CS-MRI reconstruction and segmentation problem. SegNetMRI is built upon a MRI reconstruction network with multiple cascaded blocks each containing an encoder-decoder unit and a data fidelity unit, and MRI segmentation networks having the same encoder-decoder structure. The two subnetworks are pre-trained and fine-tuned with shared reconstruction encoders. The outputs are merged into the final segmentation. Our experiments show that SegNetMRI can improve both the reconstruction and segmentation performance when using compressive measurements.



### An Interval Type-2 Fuzzy Approach to Automatic PDF Generation for Histogram Specification
- **Arxiv ID**: http://arxiv.org/abs/1805.02173v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.02173v1)
- **Published**: 2018-05-06 09:08:46+00:00
- **Updated**: 2018-05-06 09:08:46+00:00
- **Authors**: Vishal Agarwal, Diwanshu Jain, A. Vamshi Krishna Reddy, Frank Chung-Hoon Rhee
- **Comment**: None
- **Journal**: None
- **Summary**: Image enhancement plays an important role in several application in the field of computer vision and image processing. Histogram specification (HS) is one of the most widely used techniques for contrast enhancement of an image, which requires an appropriate probability density function for the transformation. In this paper, we propose a fuzzy method to find a suitable PDF automatically for histogram specification using interval type - 2 (IT2) fuzzy approach, based on the fuzzy membership values obtained from the histogram of input image. The proposed algorithm works in 5 stages which includes - symmetric Gaussian fitting on the histogram, extraction of IT2 fuzzy membership functions (MFs) and therefore, footprint of uncertainty (FOU), obtaining membership value (MV), generating PDF and application of HS. We have proposed 4 different methods to find membership values - point-wise method, center of weight method, area method, and karnik-mendel (KM) method. The framework is sensitive to local variations in the histogram and chooses the best PDF so as to improve contrast enhancement. Experimental validity of the methods used is illustrated by qualitative and quantitative analysis on several images using the image quality index - Average Information Content (AIC) or Entropy, and by comparison with the commonly used algorithms such as Histogram Equalization (HE), Recursive Mean-Separate Histogram Equalization (RMSHE) and Brightness Preserving Fuzzy Histogram Equalization (BPFHE). It has been found out that on an average, our algorithm improves the AIC index by 11.5% as compared to the index obtained by histogram equalisation.



### Reachability Analysis of Deep Neural Networks with Provable Guarantees
- **Arxiv ID**: http://arxiv.org/abs/1805.02242v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.02242v1)
- **Published**: 2018-05-06 16:33:52+00:00
- **Updated**: 2018-05-06 16:33:52+00:00
- **Authors**: Wenjie Ruan, Xiaowei Huang, Marta Kwiatkowska
- **Comment**: This is the long version of the conference paper accepted in
  IJCAI-2018. Github: https://github.com/TrustAI/DeepGO
- **Journal**: None
- **Summary**: Verifying correctness of deep neural networks (DNNs) is challenging. We study a generic reachability problem for feed-forward DNNs which, for a given set of inputs to the network and a Lipschitz-continuous function over its outputs, computes the lower and upper bound on the function values. Because the network and the function are Lipschitz continuous, all values in the interval between the lower and upper bound are reachable. We show how to obtain the safety verification problem, the output range analysis problem and a robustness measure by instantiating the reachability problem. We present a novel algorithm based on adaptive nested optimisation to solve the reachability problem. The technique has been implemented and evaluated on a range of DNNs, demonstrating its efficiency, scalability and ability to handle a broader class of networks than state-of-the-art verification approaches.



### Image Based Fashion Product Recommendation with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1805.08694v2
- **DOI**: 10.1007/978-3-030-13709-0_40
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.08694v2)
- **Published**: 2018-05-06 18:14:51+00:00
- **Updated**: 2018-07-17 21:05:33+00:00
- **Authors**: Hessel Tuinhof, Clemens Pirker, Markus Haltmeier
- **Comment**: None
- **Journal**: LOD: International Conference on Machine Learning, Optimization,
  and Data Science Machine Learning, Optimization, and Data Science 4th
  International Conference, LOD 2018, Volterra, Italy, September 13-16, 2018,
  Revised Selected Papers
- **Summary**: We develop a two-stage deep learning framework that recommends fashion images based on other input images of similar style. For that purpose, a neural network classifier is used as a data-driven, visually-aware feature extractor. The latter then serves as input for similarity-based recommendations using a ranking algorithm. Our approach is tested on the publicly available Fashion dataset. Initialization strategies using transfer learning from larger product databases are presented. Combined with more traditional content-based recommendation systems, our framework can help to increase robustness and performance, for example, by better matching a particular customer style.



### S4ND: Single-Shot Single-Scale Lung Nodule Detection
- **Arxiv ID**: http://arxiv.org/abs/1805.02279v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.02279v2)
- **Published**: 2018-05-06 21:32:14+00:00
- **Updated**: 2018-06-03 18:26:28+00:00
- **Authors**: Naji Khosravan, Ulas Bagci
- **Comment**: Accepted for publication at MICCAI 2018 (21st International
  Conference on Medical Image Computing and Computer Assisted Intervention)
- **Journal**: None
- **Summary**: The state of the art lung nodule detection studies rely on computationally expensive multi-stage frameworks to detect nodules from CT scans. To address this computational challenge and provide better performance, in this paper we propose S4ND, a new deep learning based method for lung nodule detection. Our approach uses a single feed forward pass of a single network for detection and provides better performance when compared to the current literature. The whole detection pipeline is designed as a single $3D$ Convolutional Neural Network (CNN) with dense connections, trained in an end-to-end manner. S4ND does not require any further post-processing or user guidance to refine detection results. Experimentally, we compared our network with the current state-of-the-art object detection network (SSD) in computer vision as well as the state-of-the-art published method for lung nodule detection (3D DCNN). We used publically available $888$ CT scans from LUNA challenge dataset and showed that the proposed method outperforms the current literature both in terms of efficiency and accuracy by achieving an average FROC-score of $0.897$. We also provide an in-depth analysis of our proposed network to shed light on the unclear paradigms of tiny object detection.



### SqueezeJet: High-level Synthesis Accelerator Design for Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1805.08695v1
- **DOI**: 10.1007/978-3-319-78890-6_5
- **Categories**: **cs.CV**, cs.AR
- **Links**: [PDF](http://arxiv.org/pdf/1805.08695v1)
- **Published**: 2018-05-06 21:56:33+00:00
- **Updated**: 2018-05-06 21:56:33+00:00
- **Authors**: Panagiotis G. Mousouliotis, Loukas P. Petrou
- **Comment**: The final publication is available at Springer via
  https://doi.org/10.1007/978-3-319-78890-6_5
- **Journal**: None
- **Summary**: Deep convolutional neural networks have dominated the pattern recognition scene by providing much more accurate solutions in computer vision problems such as object recognition and object detection. Most of these solutions come at a huge computational cost, requiring billions of multiply-accumulate operations and, thus, making their use quite challenging in real-time applications that run on embedded mobile (resource-power constrained) hardware. This work presents the architecture, the high-level synthesis design, and the implementation of SqueezeJet, an FPGA accelerator for the inference phase of the SqueezeNet DCNN architecture, which is designed specifically for use in embedded systems. Results show that SqueezeJet can achieve 15.16 times speed-up compared to the software implementation of SqueezeNet running on an embedded mobile processor with less than 1% drop in top-5 accuracy.



### DocFace: Matching ID Document Photos to Selfies
- **Arxiv ID**: http://arxiv.org/abs/1805.02283v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.02283v1)
- **Published**: 2018-05-06 22:17:35+00:00
- **Updated**: 2018-05-06 22:17:35+00:00
- **Authors**: Yichun Shi, Anil K. Jain
- **Comment**: None
- **Journal**: None
- **Summary**: Numerous activities in our daily life, including transactions, access to services and transportation, require us to verify who we are by showing our ID documents containing face images, e.g. passports and driver licenses. An automatic system for matching ID document photos to live face images in real time with high accuracy would speedup the verification process and remove the burden on human operators. In this paper, by employing the transfer learning technique, we propose a new method, DocFace, to train a domain-specific network for ID document photo matching without a large dataset. Compared with the baseline of applying existing methods for general face recognition to this problem, our method achieves considerable improvement. A cross validation on an ID-Selfie dataset shows that DocFace improves the TAR from 61.14% to 92.77% at FAR=0.1%. Experimental results also indicate that given more training data, a viable system for automatic ID document photo matching can be developed and deployed.



