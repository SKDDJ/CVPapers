# Arxiv Papers in cs.CV on 2018-05-03
### Multimodal Emotion Recognition for One-Minute-Gradual Emotion Challenge
- **Arxiv ID**: http://arxiv.org/abs/1805.01060v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1805.01060v1)
- **Published**: 2018-05-03 00:10:10+00:00
- **Updated**: 2018-05-03 00:10:10+00:00
- **Authors**: Ziqi Zheng, Chenjie Cao, Xingwei Chen, Guoqiang Xu
- **Comment**: None
- **Journal**: None
- **Summary**: The continuous dimensional emotion modelled by arousal and valence can depict complex changes of emotions. In this paper, we present our works on arousal and valence predictions for One-Minute-Gradual (OMG) Emotion Challenge. Multimodal representations are first extracted from videos using a variety of acoustic, video and textual models and support vector machine (SVM) is then used for fusion of multimodal signals to make final predictions. Our solution achieves Concordant Correlation Coefficient (CCC) scores of 0.397 and 0.520 on arousal and valence respectively for the validation dataset, which outperforms the baseline systems with the best CCC scores of 0.15 and 0.23 on arousal and valence by a large margin.



### Perceptually Optimized Generative Adversarial Network for Single Image Dehazing
- **Arxiv ID**: http://arxiv.org/abs/1805.01084v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.01084v1)
- **Published**: 2018-05-03 02:00:33+00:00
- **Updated**: 2018-05-03 02:00:33+00:00
- **Authors**: Yixin Du, Xin Li
- **Comment**: None
- **Journal**: None
- **Summary**: Existing approaches towards single image dehazing including both model-based and learning-based heavily rely on the estimation of so-called transmission maps. Despite its conceptual simplicity, using transmission maps as an intermediate step often makes it more difficult to optimize the perceptual quality of reconstructed images. To overcome this weakness, we propose a direct deep learning approach toward image dehazing bypassing the step of transmission map estimation and facilitating end-to-end perceptual optimization. Our technical contributions are mainly three-fold. First, based on the analogy between dehazing and denoising, we propose to directly learn a nonlinear mapping from the space of degraded images to that of haze-free ones via recursive deep residual learning; Second, inspired by the success of generative adversarial networks (GAN), we propose to optimize the perceptual quality of dehazed images by introducing a discriminator and a loss function adaptive to hazy conditions; Third, we propose to remove notorious halo-like artifacts at large scene depth discontinuities by a novel application of guided filtering. Extensive experimental results have shown that the subjective qualities of dehazed images by the proposed perceptually optimized GAN (POGAN) are often more favorable than those by existing state-of-the-art approaches especially when hazy condition varies.



### Detection of Unknown Anomalies in Streaming Videos with Generative Energy-based Boltzmann Models
- **Arxiv ID**: http://arxiv.org/abs/1805.01090v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.01090v2)
- **Published**: 2018-05-03 02:47:55+00:00
- **Updated**: 2018-09-29 07:11:20+00:00
- **Authors**: Hung Vu, Tu Dinh Nguyen, Dinh Phung
- **Comment**: This manuscript is under consideration at Pattern Recognition Letters
- **Journal**: None
- **Summary**: Abnormal event detection is one of the important objectives in research and practical applications of video surveillance. However, there are still three challenging problems for most anomaly detection systems in practical setting: limited labeled data, ambiguous definition of "abnormal" and expensive feature engineering steps. This paper introduces a unified detection framework to handle these challenges using energy-based models, which are powerful tools for unsupervised representation learning. Our proposed models are firstly trained on unlabeled raw pixels of image frames from an input video rather than hand-crafted visual features; and then identify the locations of abnormal objects based on the errors between the input video and its reconstruction produced by the models. To handle video stream, we develop an online version of our framework, wherein the model parameters are updated incrementally with the image frames arriving on the fly. Our experiments show that our detectors, using Restricted Boltzmann Machines (RBMs) and Deep Boltzmann Machines (DBMs) as core modules, achieve superior anomaly detection performance to unsupervised baselines and obtain accuracy comparable with the state-of-the-art approaches when evaluating at the pixel-level. More importantly, we discover that our system trained with DBMs is able to simultaneously perform scene clustering and scene reconstruction. This capacity not only distinguishes our method from other existing detectors but also offers a unique tool to investigate and understand how the model works.



### USAR: an Interactive User-specific Aesthetic Ranking Framework for Images
- **Arxiv ID**: http://arxiv.org/abs/1805.01091v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.01091v2)
- **Published**: 2018-05-03 02:55:49+00:00
- **Updated**: 2018-08-16 01:59:39+00:00
- **Authors**: Pei Lv, Meng Wang, Yongbo Xu, Ze Peng, Junyi Sun, Shimei Su, Bing Zhou, Mingliang Xu
- **Comment**: None
- **Journal**: None
- **Summary**: When assessing whether an image is of high or low quality, it is indispensable to take personal preference into account. Existing aesthetic models lay emphasis on hand-crafted features or deep features commonly shared by high quality images, but with limited or no consideration for personal preference and user interaction. To that end, we propose a novel and user-friendly aesthetic ranking framework via powerful deep neural network and a small amount of user interaction, which can automatically estimate and rank the aesthetic characteristics of images in accordance with users' preference. Our framework takes as input a series of photos that users prefer, and produces as output a reliable, user-specific aesthetic ranking model matching with users' preference. Considering the subjectivity of personal preference and the uncertainty of user's single selection, a unique and exclusive dataset will be constructed interactively to describe the preference of one individual by retrieving the most similar images with regard to those specified by users. Based on this unique user-specific dataset and sufficient well-designed aesthetic attributes, a customized aesthetic distribution model can be learned, which concatenates both personalized preference and aesthetic rules. We conduct extensive experiments and user studies on two large-scale public datasets, and demonstrate that our framework outperforms those work based on conventional aesthetic assessment or ranking model.



### The feasibility of automated identification of six algae types using neural networks and fluorescence-based spectral-morphological features
- **Arxiv ID**: http://arxiv.org/abs/1805.01093v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.01093v1)
- **Published**: 2018-05-03 03:07:37+00:00
- **Updated**: 2018-05-03 03:07:37+00:00
- **Authors**: Jason L. Deglint, Chao Jin, Angela Chao, Alexander Wong
- **Comment**: 13 pages
- **Journal**: None
- **Summary**: Harmful algae blooms (HABs), which produce lethal toxins, are a growing global concern since they negatively affect the quality of drinking water and have major negative impact on wildlife, the fishing industry, as well as tourism and recreational water use. In this study, we investigate the feasibility of leveraging machine learning and fluorescence-based spectral-morphological features to enable the identification of six different algae types in an automated fashion. More specifically, a custom multi-band fluorescence imaging microscope is used to capture fluorescence imaging data of a water sample at six different excitation wavelengths ranging from 405 nm - 530 nm. A number of morphological and spectral fluorescence features are then extracted from the isolated micro-organism imaging data, and used to train neural network classification models designed for the purpose of identification of the six algae types given an isolated micro-organism. Experimental results using three different neural network classification models showed that the use of either fluorescence-based spectral features or fluorescence-based spectral-morphological features to train neural network classification models led to statistically significant improvements in identification accuracy when compared to the use of morphological features (with average identification accuracies of 95.7%+/-3.5% and 96.1%+/-1.5%, respectively). These preliminary results are quite promising, given that the identification accuracy of human taxonomists are typically between the range of 67% and 83%, and thus illustrates the feasibility of leveraging machine learning and fluorescence-based spectral-morphological features as a viable method for automated identification of different algae types.



### MC-GAN: Multi-conditional Generative Adversarial Network for Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1805.01123v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.01123v5)
- **Published**: 2018-05-03 05:47:22+00:00
- **Updated**: 2018-08-15 08:35:43+00:00
- **Authors**: Hyojin Park, YoungJoon Yoo, Nojun Kwak
- **Comment**: BMVC 2018 accepted
- **Journal**: None
- **Summary**: In this paper, we introduce a new method for generating an object image from text attributes on a desired location, when the base image is given. One step further to the existing studies on text-to-image generation mainly focusing on the object's appearance, the proposed method aims to generate an object image preserving the given background information, which is the first attempt in this field. To tackle the problem, we propose a multi-conditional GAN (MC-GAN) which controls both the object and background information jointly. As a core component of MC-GAN, we propose a synthesis block which disentangles the object and background information in the training stage. This block enables MC-GAN to generate a realistic object image with the desired background by controlling the amount of the background information from the given base image using the foreground information from the text attributes. From the experiments with Caltech-200 bird and Oxford-102 flower datasets, we show that our model is able to generate photo-realistic images with a resolution of 128 x 128. The source code of MC-GAN is released.



### Visual Object Tracking: The Initialisation Problem
- **Arxiv ID**: http://arxiv.org/abs/1805.01146v2
- **DOI**: 10.1109/CRV.2018.00029
- **Categories**: **cs.CV**, I.4.8; I.4.6; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/1805.01146v2)
- **Published**: 2018-05-03 07:43:00+00:00
- **Updated**: 2018-05-22 12:35:05+00:00
- **Authors**: George De Ath, Richard Everson
- **Comment**: 15th Conference on Computer and Robot Vision (CRV 2018). Source code
  available at https://github.com/georgedeath/initialisation-problem
- **Journal**: None
- **Summary**: Model initialisation is an important component of object tracking. Tracking algorithms are generally provided with the first frame of a sequence and a bounding box (BB) indicating the location of the object. This BB may contain a large number of background pixels in addition to the object and can lead to parts-based tracking algorithms initialising their object models in background regions of the BB. In this paper, we tackle this as a missing labels problem, marking pixels sufficiently away from the BB as belonging to the background and learning the labels of the unknown pixels. Three techniques, One-Class SVM (OC-SVM), Sampled-Based Background Model (SBBM) (a novel background model based on pixel samples), and Learning Based Digital Matting (LBDM), are adapted to the problem. These are evaluated with leave-one-video-out cross-validation on the VOT2016 tracking benchmark. Our evaluation shows both OC-SVMs and SBBM are capable of providing a good level of segmentation accuracy but are too parameter-dependent to be used in real-world scenarios. We show that LBDM achieves significantly increased performance with parameters selected by cross validation and we show that it is robust to parameter variation.



### Superpixel-guided Two-view Deterministic Geometric Model Fitting
- **Arxiv ID**: http://arxiv.org/abs/1805.01158v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.01158v1)
- **Published**: 2018-05-03 08:16:27+00:00
- **Updated**: 2018-05-03 08:16:27+00:00
- **Authors**: Guobao Xiao, Hanzi Wang, Yan Yan, David Suter
- **Comment**: None
- **Journal**: International Journal of Computer Vision (IJCV),2018
- **Summary**: Geometric model fitting is a fundamental research topic in computer vision and it aims to fit and segment multiple-structure data. In this paper, we propose a novel superpixel-guided two-view geometric model fitting method (called SDF), which can obtain reliable and consistent results for real images. Specifically, SDF includes three main parts: a deterministic sampling algorithm, a model hypothesis updating strategy and a novel model selection algorithm. The proposed deterministic sampling algorithm generates a set of initial model hypotheses according to the prior information of superpixels. Then the proposed updating strategy further improves the quality of model hypotheses. After that, by analyzing the properties of the updated model hypotheses, the proposed model selection algorithm extends the conventional "fit-and-remove" framework to estimate model instances in multiple-structure data. The three parts are tightly coupled to boost the performance of SDF in both speed and accuracy, and SDF has the deterministic nature. Experimental results show that the proposed SDF has significant advantages over several state-of-the-art fitting methods when it is applied to real images with single-structure and multiple-structure data.



### IncepText: A New Inception-Text Module with Deformable PSROI Pooling for Multi-Oriented Scene Text Detection
- **Arxiv ID**: http://arxiv.org/abs/1805.01167v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.01167v2)
- **Published**: 2018-05-03 08:37:28+00:00
- **Updated**: 2018-05-08 03:01:23+00:00
- **Authors**: Qiangpeng Yang, Mengli Cheng, Wenmeng Zhou, Yan Chen, Minghui Qiu, Wei Lin, Wei Chu
- **Comment**: Accepted by IJCAI 2018
- **Journal**: None
- **Summary**: Incidental scene text detection, especially for multi-oriented text regions, is one of the most challenging tasks in many computer vision applications. Different from the common object detection task, scene text often suffers from a large variance of aspect ratio, scale, and orientation. To solve this problem, we propose a novel end-to-end scene text detector IncepText from an instance-aware segmentation perspective. We design a novel Inception-Text module and introduce deformable PSROI pooling to deal with multi-oriented text detection. Extensive experiments on ICDAR2015, RCTW-17, and MSRA-TD500 datasets demonstrate our method's superiority in terms of both effectiveness and efficiency. Our proposed method achieves 1st place result on ICDAR2015 challenge and the state-of-the-art performance on other datasets. Moreover, we have released our implementation as an OCR product which is available for public access.



### BirdNet: a 3D Object Detection Framework from LiDAR information
- **Arxiv ID**: http://arxiv.org/abs/1805.01195v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.01195v1)
- **Published**: 2018-05-03 09:59:45+00:00
- **Updated**: 2018-05-03 09:59:45+00:00
- **Authors**: Jorge Beltran, Carlos Guindel, Francisco Miguel Moreno, Daniel Cruzado, Fernando Garcia, Arturo de la Escalera
- **Comment**: Submittied to IEEE International Conference on Intelligent
  Transportation Systems 2018 (ITSC)
- **Journal**: None
- **Summary**: Understanding driving situations regardless the conditions of the traffic scene is a cornerstone on the path towards autonomous vehicles; however, despite common sensor setups already include complementary devices such as LiDAR or radar, most of the research on perception systems has traditionally focused on computer vision. We present a LiDAR-based 3D object detection pipeline entailing three stages. First, laser information is projected into a novel cell encoding for bird's eye view projection. Later, both object location on the plane and its heading are estimated through a convolutional neural network originally designed for image processing. Finally, 3D oriented detections are computed in a post-processing phase. Experiments on KITTI dataset show that the proposed framework achieves state-of-the-art results among comparable methods. Further tests with different LiDAR sensors in real scenarios assess the multi-device capabilities of the approach.



### Label Embedding with Partial Heterogeneous Contexts
- **Arxiv ID**: http://arxiv.org/abs/1805.01199v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.01199v2)
- **Published**: 2018-05-03 10:08:51+00:00
- **Updated**: 2018-12-24 03:10:45+00:00
- **Authors**: Yaxin Shi, Donna Xu, Yuangang Pan, Ivor W. Tsang, Shirui Pan
- **Comment**: 8 pages,3 figures
- **Journal**: None
- **Summary**: Label embedding plays an important role in many real-world applications. To enhance the label relatedness captured by the embeddings, multiple contexts can be adopted. However, these contexts are heterogeneous and often partially observed in practical tasks, imposing significant challenges to capture the overall relatedness among labels. In this paper, we propose a general Partial Heterogeneous Context Label Embedding (PHCLE) framework to address these challenges. Categorizing heterogeneous contexts into two groups, relational context and descriptive context, we design tailor-made matrix factorization formula to effectively exploit the label relatedness in each context. With a shared embedding principle across heterogeneous contexts, the label relatedness is selectively aligned in a shared space. Due to our elegant formulation, PHCLE overcomes the partial context problem and can nicely incorporate more contexts, which both cannot be tackled with existing multi-context label embedding methods. An effective alternative optimization algorithm is further derived to solve the sparse matrix factorization problem. Experimental results demonstrate that the label embeddings obtained with PHCLE achieve superb performance in image classification task and exhibit good interpretability in the downstream label similarity analysis and image understanding task.



### Semantic segmentation of mFISH images using convolutional networks
- **Arxiv ID**: http://arxiv.org/abs/1805.01220v1
- **DOI**: 10.1002/cyto.a.23375
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.01220v1)
- **Published**: 2018-05-03 11:04:47+00:00
- **Updated**: 2018-05-03 11:04:47+00:00
- **Authors**: Esteban Pardo, José Mário T Morgado, Norberto Malpica
- **Comment**: None
- **Journal**: None
- **Summary**: Multicolor in situ hybridization (mFISH) is a karyotyping technique used to detect major chromosomal alterations using fluorescent probes and imaging techniques. Manual interpretation of mFISH images is a time consuming step that can be automated using machine learning; in previous works, pixel or patch wise classification was employed, overlooking spatial information which can help identify chromosomes. In this work, we propose a fully convolutional semantic segmentation network for the interpretation of mFISH images, which uses both spatial and spectral information to classify each pixel in an end-to-end fashion. The semantic segmentation network developed was tested on samples extracted from a public dataset using cross validation. Despite having no labeling information of the image it was tested on our algorithm yielded an average correct classification ratio (CCR) of 87.41%. Previously, this level of accuracy was only achieved with state of the art algorithms when classifying pixels from the same image in which the classifier has been trained. These results provide evidence that fully convolutional semantic segmentation networks may be employed in the computer aided diagnosis of genetic diseases with improved performance over the current methods of image analysis.



### audEERING's approach to the One-Minute-Gradual Emotion Challenge
- **Arxiv ID**: http://arxiv.org/abs/1805.01222v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.SD, eess.AS, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.01222v1)
- **Published**: 2018-05-03 11:06:23+00:00
- **Updated**: 2018-05-03 11:06:23+00:00
- **Authors**: Andreas Triantafyllopoulos, Hesam Sagha, Florian Eyben, Björn Schuller
- **Comment**: 3 pages
- **Journal**: None
- **Summary**: This paper describes audEERING's submissions as well as additional evaluations for the One-Minute-Gradual (OMG) emotion recognition challenge. We provide the results for audio and video processing on subject (in)dependent evaluations. On the provided Development set, we achieved 0.343 Concordance Correlation Coefficient (CCC) for arousal (from audio) and .401 for valence (from video).



### Noise Invariant Frame Selection: A Simple Method to Address the Background Noise Problem for Text-independent Speaker Verification
- **Arxiv ID**: http://arxiv.org/abs/1805.01259v1
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, eess.AS, 68T10
- **Links**: [PDF](http://arxiv.org/pdf/1805.01259v1)
- **Published**: 2018-05-03 12:35:06+00:00
- **Updated**: 2018-05-03 12:35:06+00:00
- **Authors**: Siyang Song, Shuimei Zhang, Björn Schuller, Linlin Shen, Michel Valstar
- **Comment**: Paper accepted in IJCNN 2018
- **Journal**: None
- **Summary**: The performance of speaker-related systems usually degrades heavily in practical applications largely due to the presence of background noise. To improve the robustness of such systems in unknown noisy environments, this paper proposes a simple pre-processing method called Noise Invariant Frame Selection (NIFS). Based on several noisy constraints, it selects noise invariant frames from utterances to represent speakers. Experiments conducted on the TIMIT database showed that the NIFS can significantly improve the performance of Vector Quantization (VQ), Gaussian Mixture Model-Universal Background Model (GMM-UBM) and i-vector-based speaker verification systems in different unknown noisy environments with different SNRs, in comparison to their baselines. Meanwhile, the proposed NIFS-based speaker verification systems achieves similar performance when we change the constraints (hyper-parameters) or features, which indicates that it is robust and easy to reproduce. Since NIFS is designed as a general algorithm, it could be further applied to other similar tasks.



### Learning-Based Compressive MRI
- **Arxiv ID**: http://arxiv.org/abs/1805.01266v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1805.01266v1)
- **Published**: 2018-05-03 12:49:34+00:00
- **Updated**: 2018-05-03 12:49:34+00:00
- **Authors**: Baran Gözcü, Rabeeh Karimi Mahabadi, Yen-Huan Li, Efe Ilıcak, Tolga Çukur, Jonathan Scarlett, Volkan Cevher
- **Comment**: 13 pages, 6 figures. IEEE TMI (Transactions of Medical Imaging)
- **Journal**: None
- **Summary**: In the area of magnetic resonance imaging (MRI), an extensive range of non-linear reconstruction algorithms have been proposed that can be used with general Fourier subsampling patterns. However, the design of these subsampling patterns has typically been considered in isolation from the reconstruction rule and the anatomy under consideration. In this paper, we propose a learning-based framework for optimizing MRI subsampling patterns for a specific reconstruction rule and anatomy, considering both the noiseless and noisy settings. Our learning algorithm has access to a representative set of training signals, and searches for a sampling pattern that performs well on average for the signals in this set. We present a novel parameter-free greedy mask selection method, and show it to be effective for a variety of reconstruction rules and performance metrics. Moreover we also support our numerical findings by providing a rigorous justification of our framework via statistical learning theory.



### Multi-label Learning Based Deep Transfer Neural Network for Facial Attribute Classification
- **Arxiv ID**: http://arxiv.org/abs/1805.01282v1
- **DOI**: 10.1016/j.patcog.2018.03.018
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.01282v1)
- **Published**: 2018-05-03 13:13:50+00:00
- **Updated**: 2018-05-03 13:13:50+00:00
- **Authors**: Ni Zhuang, Yan Yan, Si Chen, Hanzi Wang, Chunhua Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Neural Network (DNN) has recently achieved outstanding performance in a variety of computer vision tasks, including facial attribute classification. The great success of classifying facial attributes with DNN often relies on a massive amount of labelled data. However, in real-world applications, labelled data are only provided for some commonly used attributes (such as age, gender); whereas, unlabelled data are available for other attributes (such as attraction, hairline). To address the above problem, we propose a novel deep transfer neural network method based on multi-label learning for facial attribute classification, termed FMTNet, which consists of three sub-networks: the Face detection Network (FNet), the Multi-label learning Network (MNet) and the Transfer learning Network (TNet). Firstly, based on the Faster Region-based Convolutional Neural Network (Faster R-CNN), FNet is fine-tuned for face detection. Then, MNet is fine-tuned by FNet to predict multiple attributes with labelled data, where an effective loss weight scheme is developed to explicitly exploit the correlation between facial attributes based on attribute grouping. Finally, based on MNet, TNet is trained by taking advantage of unsupervised domain adaptation for unlabelled facial attribute classification. The three sub-networks are tightly coupled to perform effective facial attribute classification. A distinguishing characteristic of the proposed FMTNet method is that the three sub-networks (FNet, MNet and TNet) are constructed in a similar network structure. Extensive experimental results on challenging face datasets demonstrate the effectiveness of our proposed method compared with several state-of-the-art methods.



### Multi-task Learning of Cascaded CNN for Facial Attribute Classification
- **Arxiv ID**: http://arxiv.org/abs/1805.01290v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.01290v1)
- **Published**: 2018-05-03 13:23:23+00:00
- **Updated**: 2018-05-03 13:23:23+00:00
- **Authors**: Ni Zhuang, Yan Yan, Si Chen, Hanzi Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, facial attribute classification (FAC) has attracted significant attention in the computer vision community. Great progress has been made along with the availability of challenging FAC datasets. However, conventional FAC methods usually firstly pre-process the input images (i.e., perform face detection and alignment) and then predict facial attributes. These methods ignore the inherent dependencies among these tasks (i.e., face detection, facial landmark localization and FAC). Moreover, some methods using convolutional neural network are trained based on the fixed loss weights without considering the differences between facial attributes. In order to address the above problems, we propose a novel multi-task learning of cas- caded convolutional neural network method, termed MCFA, for predicting multiple facial attributes simultaneously. Specifically, the proposed method takes advantage of three cascaded sub-networks (i.e., S_Net, M_Net and L_Net corresponding to the neural networks under different scales) to jointly train multiple tasks in a coarse-to-fine manner, which can achieve end-to-end optimization. Furthermore, the proposed method automatically assigns the loss weight to each facial attribute based on a novel dynamic weighting scheme, thus making the proposed method concentrate on predicting the more difficult facial attributes. Experimental results show that the proposed method outperforms several state-of-the-art FAC methods on the challenging CelebA and LFWA datasets.



### Facial Landmarks Localization using Cascaded Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1805.01760v3
- **DOI**: 10.1016/j.cviu.2021.103171
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.01760v3)
- **Published**: 2018-05-03 13:53:20+00:00
- **Updated**: 2021-07-19 18:05:52+00:00
- **Authors**: Shahar Mahpod, Rig Das, Emanuele Maiorana, Yosi Keller, Patrizio Campisi
- **Comment**: None
- **Journal**: None
- **Summary**: The accurate localization of facial landmarks is at the core of face analysis tasks, such as face recognition and facial expression analysis, to name a few. In this work, we propose a novel localization approach based on a deep learning architecture that utilizes cascaded subnetworks with convolutional neural network units. The cascaded units of the first subnetwork estimate heatmap-based encodings of the landmarks locations, while the cascaded units of the second subnetwork receive as input the output of the corresponding heatmap estimation units, and refine them through regression. The proposed scheme is experimentally shown to compare favorably with contemporary state-of-the-art schemes, especially when applied to images depicting challenging localization conditions.



### SdcNet: A Computation-Efficient CNN for Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/1805.01317v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1805.01317v2)
- **Published**: 2018-05-03 14:16:25+00:00
- **Updated**: 2018-05-05 00:36:26+00:00
- **Authors**: Yunlong Ma, Chunyan Wang
- **Comment**: 5 pages,3 figures, conference
- **Journal**: None
- **Summary**: Extracting features from a huge amount of data for object recognition is a challenging task. Convolution neural network can be used to meet the challenge, but it often requires a large number of computation resources. In this paper, a computation-efficient convolutional module, named SdcBlock, is proposed and based on it, the convolution network SdcNet is introduced for object recognition tasks. In the proposed module, optimized successive depthwise convolutions supported by appropriate data management is applied in order to generate vectors containing high density and more varieties of feature information. The hyperparameters can be easily adjusted to suit varieties of tasks under different computation restrictions without significantly jeopardizing the performance. The experiments have shown that SdcNet achieved an error rate of 5.60% in CIFAR-10 with only 55M Flops and also reduced further the error rate to 5.24% using a moderate volume of 103M Flops. The expected computation efficiency of the SdcNet has been confirmed.



### Evaluation of CNN-based Single-Image Depth Estimation Methods
- **Arxiv ID**: http://arxiv.org/abs/1805.01328v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.01328v1)
- **Published**: 2018-05-03 14:34:34+00:00
- **Updated**: 2018-05-03 14:34:34+00:00
- **Authors**: Tobias Koch, Lukas Liebel, Friedrich Fraundorfer, Marco Körner
- **Comment**: None
- **Journal**: None
- **Summary**: While an increasing interest in deep models for single-image depth estimation methods can be observed, established schemes for their evaluation are still limited. We propose a set of novel quality criteria, allowing for a more detailed analysis by focusing on specific characteristics of depth maps. In particular, we address the preservation of edges and planar regions, depth consistency, and absolute distance accuracy. In order to employ these metrics to evaluate and compare state-of-the-art single-image depth estimation approaches, we provide a new high-quality RGB-D dataset. We used a DSLR camera together with a laser scanner to acquire high-resolution images and highly accurate depth maps. Experimental results show the validity of our proposed evaluation protocol.



### SIPs: Succinct Interest Points from Unsupervised Inlierness Probability Learning
- **Arxiv ID**: http://arxiv.org/abs/1805.01358v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.01358v2)
- **Published**: 2018-05-03 15:11:30+00:00
- **Updated**: 2019-08-19 13:13:37+00:00
- **Authors**: Titus Cieslewski, Konstantinos G. Derpanis, Davide Scaramuzza
- **Comment**: 8 pages, 2p references, 1p supplementary material. Accepted for
  publication at the IEEE International Conference on 3D Vision (3DV), Qu\'ebec
  City, 2019. v2 contains significant changes VS v1
- **Journal**: IEEE International Conference on 3D Vision (3DV), Qu\'ebec City,
  2019
- **Summary**: A wide range of computer vision algorithms rely on identifying sparse interest points in images and establishing correspondences between them. However, only a subset of the initially identified interest points results in true correspondences (inliers). In this paper, we seek a detector that finds the minimum number of points that are likely to result in an application-dependent "sufficient" number of inliers k. To quantify this goal, we introduce the "k-succinctness" metric. Extracting a minimum number of interest points is attractive for many applications, because it can reduce computational load, memory, and data transmission. Alongside succinctness, we introduce an unsupervised training methodology for interest point detectors that is based on predicting the probability of a given pixel being an inlier. In comparison to previous learned detectors, our method requires the least amount of data pre-processing. Our detector and other state-of-the-art detectors are extensively evaluated with respect to succinctness on popular public datasets covering both indoor and outdoor scenes, and both wide and narrow baselines. In certain cases, our detector is able to obtain an equivalent amount of inliers with as little as 60% of the amount of points of other detectors. The code and trained networks are provided at https://github.com/uzh-rpg/sips2_open .



### Machine learning regression on hyperspectral data to estimate multiple water parameters
- **Arxiv ID**: http://arxiv.org/abs/1805.01361v2
- **DOI**: 10.1109/WHISPERS.2018.8747010
- **Categories**: **cs.CV**, q-bio.QM, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.01361v2)
- **Published**: 2018-05-03 15:13:02+00:00
- **Updated**: 2018-08-07 08:32:58+00:00
- **Authors**: Philipp M. Maier, Sina Keller
- **Comment**: This work has been accepted to the IEEE WHISPERS 2018 conference. (C)
  2018 IEEE
- **Journal**: None
- **Summary**: In this paper, we present a regression framework involving several machine learning models to estimate water parameters based on hyperspectral data. Measurements from a multi-sensor field campaign, conducted on the River Elbe, Germany, represent the benchmark dataset. It contains hyperspectral data and the five water parameters chlorophyll a, green algae, diatoms, CDOM and turbidity. We apply a PCA for the high-dimensional data as a possible preprocessing step. Then, we evaluate the performance of the regression framework with and without this preprocessing step. The regression results of the framework clearly reveal the potential of estimating water parameters based on hyperspectral data with machine learning. The proposed framework provides the basis for further investigations, such as adapting the framework to estimate water parameters of different inland waters.



### Framewise approach in multimodal emotion recognition in OMG challenge
- **Arxiv ID**: http://arxiv.org/abs/1805.01369v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1805.01369v1)
- **Published**: 2018-05-03 15:21:44+00:00
- **Updated**: 2018-05-03 15:21:44+00:00
- **Authors**: Grigoriy Sterling, Andrey Belyaev, Maxim Ryabov
- **Comment**: None
- **Journal**: None
- **Summary**: In this report we described our approach achieves $53\%$ of unweighted accuracy over $7$ emotions and $0.05$ and $0.09$ mean squared errors for arousal and valence in OMG emotion recognition challenge. Our results were obtained with ensemble of single modality models trained on voice and face data from video separately. We consider each stream as a sequence of frames. Next we estimated features from frames and handle it with recurrent neural network. As audio frame we mean short $0.4$ second spectrogram interval. For features estimation for face pictures we used own ResNet neural network pretrained on AffectNet database. Each short spectrogram was considered as a picture and processed by convolutional network too. As a base audio model we used ResNet pretrained in speaker recognition task. Predictions from both modalities were fused on decision level and improve single-channel approaches by a few percent



### Boosting Domain Adaptation by Discovering Latent Domains
- **Arxiv ID**: http://arxiv.org/abs/1805.01386v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.01386v1)
- **Published**: 2018-05-03 15:55:48+00:00
- **Updated**: 2018-05-03 15:55:48+00:00
- **Authors**: Massimiliano Mancini, Lorenzo Porzi, Samuel Rota Bulò, Barbara Caputo, Elisa Ricci
- **Comment**: CVPR 2018
- **Journal**: None
- **Summary**: Current Domain Adaptation (DA) methods based on deep architectures assume that the source samples arise from a single distribution. However, in practice, most datasets can be regarded as mixtures of multiple domains. In these cases exploiting single-source DA methods for learning target classifiers may lead to sub-optimal, if not poor, results. In addition, in many applications it is difficult to manually provide the domain labels for all source data points, i.e. latent domains should be automatically discovered. This paper introduces a novel Convolutional Neural Network (CNN) architecture which (i) automatically discovers latent domains in visual datasets and (ii) exploits this information to learn robust target classifiers. Our approach is based on the introduction of two main components, which can be embedded into any existing CNN architecture: (i) a side branch that automatically computes the assignment of a source sample to a latent domain and (ii) novel layers that exploit domain membership information to appropriately align the distribution of the CNN internal feature representations to a reference distribution. We test our approach on publicly-available datasets, showing that it outperforms state-of-the-art multi-source DA methods by a large margin.



### Dimensional emotion recognition using visual and textual cues
- **Arxiv ID**: http://arxiv.org/abs/1805.01416v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1805.01416v1)
- **Published**: 2018-05-03 16:42:20+00:00
- **Updated**: 2018-05-03 16:42:20+00:00
- **Authors**: Pedro M. Ferreira, Diogo Pernes, Kelwin Fernandes, Ana Rebelo, Jaime S. Cardoso
- **Comment**: None
- **Journal**: None
- **Summary**: This paper addresses the problem of automatic emotion recognition in the scope of the One-Minute Gradual-Emotional Behavior challenge (OMG-Emotion challenge). The underlying objective of the challenge is the automatic estimation of emotion expressions in the two-dimensional emotion representation space (i.e., arousal and valence). The adopted methodology is a weighted ensemble of several models from both video and text modalities. For video-based recognition, two different types of visual cues (i.e., face and facial landmarks) were considered to feed a multi-input deep neural network. Regarding the text modality, a sequential model based on a simple recurrent architecture was implemented. In addition, we also introduce a model based on high-level features in order to embed domain knowledge in the learning process. Experimental results on the OMG-Emotion validation set demonstrate the effectiveness of the implemented ensemble model as it clearly outperforms the current baseline methods.



### InceptB: A CNN Based Classification Approach for Recognizing Traditional Bengali Games
- **Arxiv ID**: http://arxiv.org/abs/1805.01442v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.01442v2)
- **Published**: 2018-05-03 17:35:45+00:00
- **Updated**: 2018-09-16 16:40:02+00:00
- **Authors**: Mohammad Shakirul Islam, Ferdouse Ahmed Foysal, Nafis Neehal, Enamul Karim, Syed Akhter Hossain
- **Comment**: 8 pages, 8 sections (including reference), 5 images, 2 tables
- **Journal**: None
- **Summary**: Sports activities are an integral part of our day to day life. Introducing autonomous decision making and predictive models to recognize and analyze different sports events and activities has become an emerging trend in computer vision arena. Albeit the advances and vivid applications of artificial intelligence and computer vision in recognizing different popular western games, there remains a very minimal amount of efforts in the application of computer vision in recognizing traditional Bangladeshi games. We, in this paper, have described a novel Deep Learning based approach for recognizing traditional Bengali games. We have retrained the final layer of the renowned Inception V3 architecture developed by Google for our classification approach. Our approach shows promising results with an average accuracy of 80% approximately in correctly recognizing among 5 traditional Bangladeshi sports events.



### A Multi-component CNN-RNN Approach for Dimensional Emotion Recognition in-the-wild
- **Arxiv ID**: http://arxiv.org/abs/1805.01452v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.HC, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.01452v5)
- **Published**: 2018-05-03 17:54:44+00:00
- **Updated**: 2019-12-13 23:32:41+00:00
- **Authors**: Dimitrios Kollias, Stefanos Zafeiriou
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents our approach to the One-Minute Gradual-Emotion Recognition (OMG-Emotion) Challenge, focusing on dimensional emotion recognition through visual analysis of the provided emotion videos. The approach is based on a Convolutional and Recurrent (CNN-RNN) deep neural architecture we have developed for the relevant large AffWild Emotion Database. We extended and adapted this architecture, by letting a combination of multiple features generated in the CNN component be explored by RNN subnets. Our target has been to obtain best performance on the OMG-Emotion visual validation data set, while learning the respective visual training data set. Extended experimentation has led to best architectures for the estimation of the values of the valence and arousal emotion dimensions over these data sets.



### RMDL: Random Multimodel Deep Learning for Classification
- **Arxiv ID**: http://arxiv.org/abs/1805.01890v2
- **DOI**: 10.1145/3206098.3206111
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.01890v2)
- **Published**: 2018-05-03 19:36:43+00:00
- **Updated**: 2018-05-31 16:08:33+00:00
- **Authors**: Kamran Kowsari, Mojtaba Heidarysafa, Donald E. Brown, Kiana Jafari Meimandi, Laura E. Barnes
- **Comment**: Best Paper award ACM ICISDM
- **Journal**: None
- **Summary**: The continually increasing number of complex datasets each year necessitates ever improving machine learning methods for robust and accurate categorization of these data. This paper introduces Random Multimodel Deep Learning (RMDL): a new ensemble, deep learning approach for classification. Deep learning models have achieved state-of-the-art results across many domains. RMDL solves the problem of finding the best deep learning structure and architecture while simultaneously improving robustness and accuracy through ensembles of deep learning architectures. RDML can accept as input a variety data to include text, video, images, and symbolic. This paper describes RMDL and shows test results for image and text data including MNIST, CIFAR-10, WOS, Reuters, IMDB, and 20newsgroup. These test results show that RDML produces consistently better performance than standard methods over a broad range of data types and classification problems.



### Dictionary Learning and Sparse Coding on Statistical Manifolds
- **Arxiv ID**: http://arxiv.org/abs/1805.02505v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.02505v1)
- **Published**: 2018-05-03 22:00:38+00:00
- **Updated**: 2018-05-03 22:00:38+00:00
- **Authors**: Rudrasis Chakraborty, Monami Banerjee, Baba C. Vemuri
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1604.06939
- **Journal**: None
- **Summary**: In this paper, we propose a novel information theoretic framework for dictionary learning (DL) and sparse coding (SC) on a statistical manifold (the manifold of probability distributions). Unlike the traditional DL and SC framework, our new formulation does not explicitly incorporate any sparsity inducing norm in the cost function being optimized but yet yields sparse codes. Our algorithm approximates the data points on the statistical manifold (which are probability distributions) by the weighted Kullback-Leibeler center/mean (KL-center) of the dictionary atoms. The KL-center is defined as the minimizer of the maximum KL-divergence between itself and members of the set whose center is being sought. Further, we prove that the weighted KL-center is a sparse combination of the dictionary atoms. This result also holds for the case when the KL-divergence is replaced by the well known Hellinger distance. From an applications perspective, we present an extension of the aforementioned framework to the manifold of symmetric positive definite matrices (which can be identified with the manifold of zero mean gaussian distributions), $\mathcal{P}_n$. We present experiments involving a variety of dictionary-based reconstruction and classification problems in Computer Vision. Performance of the proposed algorithm is demonstrated by comparing it to several state-of-the-art methods in terms of reconstruction and classification accuracy as well as sparsity of the chosen representation.



### Pixel-wise Attentional Gating for Parsimonious Pixel Labeling
- **Arxiv ID**: http://arxiv.org/abs/1805.01556v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.01556v2)
- **Published**: 2018-05-03 22:05:57+00:00
- **Updated**: 2018-12-18 22:46:32+00:00
- **Authors**: Shu Kong, Charless Fowlkes
- **Comment**: https://www.ics.uci.edu/~skong2/PAG.html
- **Journal**: None
- **Summary**: To achieve parsimonious inference in per-pixel labeling tasks with a limited computational budget, we propose a \emph{Pixel-wise Attentional Gating} unit (\emph{PAG}) that learns to selectively process a subset of spatial locations at each layer of a deep convolutional network. PAG is a generic, architecture-independent, problem-agnostic mechanism that can be readily "plugged in" to an existing model with fine-tuning. We utilize PAG in two ways: 1) learning spatially varying pooling fields that improve model performance without the extra computation cost associated with multi-scale pooling, and 2) learning a dynamic computation policy for each pixel to decrease total computation while maintaining accuracy.   We extensively evaluate PAG on a variety of per-pixel labeling tasks, including semantic segmentation, boundary detection, monocular depth and surface normal estimation. We demonstrate that PAG allows competitive or state-of-the-art performance on these tasks. Our experiments show that PAG learns dynamic spatial allocation of computation over the input image which provides better performance trade-offs compared to related approaches (e.g., truncating deep models or dynamically skipping whole layers). Generally, we observe PAG can reduce computation by $10\%$ without noticeable loss in accuracy and performance degrades gracefully when imposing stronger computational constraints.



