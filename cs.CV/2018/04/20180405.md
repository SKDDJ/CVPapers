# Arxiv Papers in cs.CV on 2018-04-05
### Unifying Bilateral Filtering and Adversarial Training for Robust Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1804.01635v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.01635v3)
- **Published**: 2018-04-05 00:40:25+00:00
- **Updated**: 2018-10-08 17:36:59+00:00
- **Authors**: Neale Ratzlaff, Li Fuxin
- **Comment**: 9 pages, 14 figures
- **Journal**: None
- **Summary**: Recent analysis of deep neural networks has revealed their vulnerability to carefully structured adversarial examples. Many effective algorithms exist to craft these adversarial examples, but performant defenses seem to be far away. In this work, we explore the use of edge-aware bilateral filtering as a projection back to the space of natural images. We show that bilateral filtering is an effective defense in multiple attack settings, where the strength of the adversary gradually increases. In the case of an adversary who has no knowledge of the defense, bilateral filtering can remove more than 90% of adversarial examples from a variety of different attacks. To evaluate against an adversary with complete knowledge of our defense, we adapt the bilateral filter as a trainable layer in a neural network and show that adding this layer makes ImageNet images significantly more robust to attacks. When trained under a framework of adversarial training, we show that the resulting model is hard to fool with even the best attack methods.



### A Pyramid CNN for Dense-Leaves Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1804.01646v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.01646v1)
- **Published**: 2018-04-05 01:38:42+00:00
- **Updated**: 2018-04-05 01:38:42+00:00
- **Authors**: Daniel D. Morris
- **Comment**: To appear in Computer and Robot Vision, Toronto, May 2018
- **Journal**: None
- **Summary**: Automatic detection and segmentation of overlapping leaves in dense foliage can be a difficult task, particularly for leaves with strong textures and high occlusions. We present Dense-Leaves, an image dataset with ground truth segmentation labels that can be used to train and quantify algorithms for leaf segmentation in the wild. We also propose a pyramid convolutional neural network with multi-scale predictions that detects and discriminates leaf boundaries from interior textures. Using these detected boundaries, closed-contour boundaries around individual leaves are estimated with a watershed-based algorithm. The result is an instance segmenter for dense leaves. Promising segmentation results for leaves in dense foliage are obtained.



### High-dimension Tensor Completion via Gradient-based Optimization Under Tensor-train Format
- **Arxiv ID**: http://arxiv.org/abs/1804.01983v3
- **DOI**: None
- **Categories**: **cs.NA**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1804.01983v3)
- **Published**: 2018-04-05 02:06:28+00:00
- **Updated**: 2018-11-30 03:21:55+00:00
- **Authors**: Longhao Yuan, Qibin Zhao, Lihua Gui, Jianting Cao
- **Comment**: None
- **Journal**: None
- **Summary**: Tensor train (TT) decomposition has drawn people's attention due to its powerful representation ability and performance stability in high-order tensors. In this paper, we propose a novel approach to recover the missing entries of incomplete data represented by higher-order tensors. We attempt to find the low-rank TT decomposition of the incomplete data which captures the latent features of the whole data and then reconstruct the missing entries. By applying gradient descent algorithms, tensor completion problem is efficiently solved by optimization models. We propose two TT-based algorithms: Tensor Train Weighted Optimization (TT-WOPT) and Tensor Train Stochastic Gradient Descent (TT-SGD) to optimize TT decomposition factors. In addition, a method named Visual Data Tensorization (VDT) is proposed to transform visual data into higher-order tensors, resulting in the performance improvement of our algorithms. The experiments in synthetic data and visual data show high efficiency and performance of our algorithms compared to the state-of-the-art completion algorithms, especially in high-order, high missing rate, and large-scale tensor completion situations.



### Review of Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1804.01653v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1804.01653v2)
- **Published**: 2018-04-05 02:23:59+00:00
- **Updated**: 2018-08-28 15:34:03+00:00
- **Authors**: Rong Zhang, Weiping Li, Tong Mo
- **Comment**: In Chinese. Have been published in the journal "Information and
  Control"
- **Journal**: None
- **Summary**: In recent years, China, the United States and other countries, Google and other high-tech companies have increased investment in artificial intelligence. Deep learning is one of the current artificial intelligence research's key areas. This paper analyzes and summarizes the latest progress and future research directions of deep learning. Firstly, three basic models of deep learning are outlined, including multilayer perceptrons, convolutional neural networks, and recurrent neural networks. On this basis, we further analyze the emerging new models of convolution neural networks and recurrent neural networks. This paper then summarizes deep learning's applications in many areas of artificial intelligence, including speech processing, computer vision, natural language processing and so on. Finally, this paper discusses the existing problems of deep learning and gives the corresponding possible solutions.



### Pixel2Mesh: Generating 3D Mesh Models from Single RGB Images
- **Arxiv ID**: http://arxiv.org/abs/1804.01654v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.01654v2)
- **Published**: 2018-04-05 02:24:03+00:00
- **Updated**: 2018-08-03 08:52:33+00:00
- **Authors**: Nanyang Wang, Yinda Zhang, Zhuwen Li, Yanwei Fu, Wei Liu, Yu-Gang Jiang
- **Comment**: None
- **Journal**: ECCV2018
- **Summary**: We propose an end-to-end deep learning architecture that produces a 3D shape in triangular mesh from a single color image. Limited by the nature of deep neural network, previous methods usually represent a 3D shape in volume or point cloud, and it is non-trivial to convert them to the more ready-to-use mesh model. Unlike the existing methods, our network represents 3D mesh in a graph-based convolutional neural network and produces correct geometry by progressively deforming an ellipsoid, leveraging perceptual features extracted from the input image. We adopt a coarse-to-fine strategy to make the whole deformation procedure stable, and define various of mesh related losses to capture properties of different levels to guarantee visually appealing and physically accurate 3D geometry. Extensive experiments show that our method not only qualitatively produces mesh model with better details, but also achieves higher 3D shape estimation accuracy compared to the state-of-the-art.



### Learning Strict Identity Mappings in Deep Residual Networks
- **Arxiv ID**: http://arxiv.org/abs/1804.01661v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.01661v5)
- **Published**: 2018-04-05 03:19:53+00:00
- **Updated**: 2019-06-16 00:03:19+00:00
- **Authors**: Xin Yu, Zhiding Yu, Srikumar Ramalingam
- **Comment**: Make title consistent with the CVPR version
- **Journal**: None
- **Summary**: A family of super deep networks, referred to as residual networks or ResNet, achieved record-beating performance in various visual tasks such as image recognition, object detection, and semantic segmentation. The ability to train very deep networks naturally pushed the researchers to use enormous resources to achieve the best performance. Consequently, in many applications super deep residual networks were employed for just a marginal improvement in performance. In this paper, we propose epsilon-ResNet that allows us to automatically discard redundant layers, which produces responses that are smaller than a threshold epsilon, with a marginal or no loss in performance. The epsilon-ResNet architecture can be achieved using a few additional rectified linear units in the original ResNet. Our method does not use any additional variables nor numerous trials like other hyper-parameter optimization techniques. The layer selection is achieved using a single training process and the evaluation is performed on CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets. In some instances, we achieve about 80% reduction in the number of parameters.



### Learning to Separate Object Sounds by Watching Unlabeled Video
- **Arxiv ID**: http://arxiv.org/abs/1804.01665v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1804.01665v2)
- **Published**: 2018-04-05 04:06:46+00:00
- **Updated**: 2018-07-26 04:46:24+00:00
- **Authors**: Ruohan Gao, Rogerio Feris, Kristen Grauman
- **Comment**: Published in ECCV 2018; Project Page:
  http://vision.cs.utexas.edu/projects/separating_object_sounds/
- **Journal**: None
- **Summary**: Perceiving a scene most fully requires all the senses. Yet modeling how objects look and sound is challenging: most natural scenes and events contain multiple objects, and the audio track mixes all the sound sources together. We propose to learn audio-visual object models from unlabeled video, then exploit the visual context to perform audio source separation in novel videos. Our approach relies on a deep multi-instance multi-label learning framework to disentangle the audio frequency bases that map to individual visual objects, even without observing/hearing those objects in isolation. We show how the recovered disentangled bases can be used to guide audio source separation to obtain better-separated, object-level sounds. Our work is the first to learn audio source separation from large-scale "in the wild" videos containing multiple audio sources per video. We obtain state-of-the-art results on visually-aided audio source separation and audio denoising. Our video results: http://vision.cs.utexas.edu/projects/separating_object_sounds/



### Cancelable Indexing Based on Low-rank Approximation of Correlation-invariant Random Filtering for Fast and Secure Biometric Identification
- **Arxiv ID**: http://arxiv.org/abs/1804.01670v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/1804.01670v1)
- **Published**: 2018-04-05 04:56:44+00:00
- **Updated**: 2018-04-05 04:56:44+00:00
- **Authors**: Takao Murakami, Tetsushi Ohki, Yosuke Kaga, Masakazu Fujio, Kenta Takahashi
- **Comment**: Accepted to Pattern Recognition Letters (Special Issue on Robustness,
  Security and Regulation Aspects in Current Biometric Systems), 2018
- **Journal**: None
- **Summary**: A cancelable biometric scheme called correlation-invariant random filtering (CIRF) is known as a promising template protection scheme. This scheme transforms a biometric feature represented as an image via the 2D number theoretic transform (NTT) and random filtering. CIRF has perfect secrecy in that the transformed feature leaks no information about the original feature. However, CIRF cannot be applied to large-scale biometric identification, since the 2D inverse NTT in the matching phase requires high computational time. Furthermore, existing biometric indexing schemes cannot be used in conjunction with template protection schemes to speed up biometric identification, since a biometric index leaks some information about the original feature. In this paper, we propose a novel indexing scheme called "cancelable indexing" to speed up CIRF without losing its security properties. The proposed scheme is based on fast computation of CIRF via low-rank approximation of biometric images and via a minimum spanning tree representation of low-rank matrices in the Fourier domain. We prove that the transformed index leaks no information about the original index and the original biometric feature (i.e., perfect secrecy), and thoroughly discuss the security of the proposed scheme. We also demonstrate that it significantly reduces the one-to-many matching time using a finger-vein dataset that includes six fingers from 505 subjects.



### Hallucinated-IQA: No-Reference Image Quality Assessment via Adversarial Learning
- **Arxiv ID**: http://arxiv.org/abs/1804.01681v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.01681v1)
- **Published**: 2018-04-05 06:33:21+00:00
- **Updated**: 2018-04-05 06:33:21+00:00
- **Authors**: Kwan-Yee Lin, Guanxiang Wang
- **Comment**: Accepted to CVPR2018
- **Journal**: None
- **Summary**: No-reference image quality assessment (NR-IQA) is a fundamental yet challenging task in low-level computer vision community. The difficulty is particularly pronounced for the limited information, for which the corresponding reference for comparison is typically absent. Although various feature extraction mechanisms have been leveraged from natural scene statistics to deep neural networks in previous methods, the performance bottleneck still exists. In this work, we propose a hallucination-guided quality regression network to address the issue. We firstly generate a hallucinated reference constrained on the distorted image, to compensate the absence of the true reference. Then, we pair the information of hallucinated reference with the distorted image, and forward them to the regressor to learn the perceptual discrepancy with the guidance of an implicit ranking relationship within the generator, and therefore produce the precise quality prediction. To demonstrate the effectiveness of our approach, comprehensive experiments are evaluated on four popular image quality assessment benchmarks. Our method significantly outperforms all the previous state-of-the-art methods by large margins. The code and model will be publicly available on the project page https://kwanyeelin.github.io/projects/HIQA/HIQA.html.



### Look into Person: Joint Body Parsing & Pose Estimation Network and A New Benchmark
- **Arxiv ID**: http://arxiv.org/abs/1804.01984v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.01984v1)
- **Published**: 2018-04-05 07:41:15+00:00
- **Updated**: 2018-04-05 07:41:15+00:00
- **Authors**: Xiaodan Liang, Ke Gong, Xiaohui Shen, Liang Lin
- **Comment**: We proposed the most comprehensive dataset around the world for
  human-centric analysis! (Accepted By T-PAMI 2018) The dataset, code and
  models are available at http://www.sysu-hcp.net/lip/ . arXiv admin note:
  substantial text overlap with arXiv:1703.05446
- **Journal**: None
- **Summary**: Human parsing and pose estimation have recently received considerable interest due to their substantial application potentials. However, the existing datasets have limited numbers of images and annotations and lack a variety of human appearances and coverage of challenging cases in unconstrained environments. In this paper, we introduce a new benchmark named "Look into Person (LIP)" that provides a significant advancement in terms of scalability, diversity, and difficulty, which are crucial for future developments in human-centric analysis. This comprehensive dataset contains over 50,000 elaborately annotated images with 19 semantic part labels and 16 body joints, which are captured from a broad range of viewpoints, occlusions, and background complexities. Using these rich annotations, we perform detailed analyses of the leading human parsing and pose estimation approaches, thereby obtaining insights into the successes and failures of these methods. To further explore and take advantage of the semantic correlation of these two tasks, we propose a novel joint human parsing and pose estimation network to explore efficient context modeling, which can simultaneously predict parsing and pose with extremely high quality. Furthermore, we simplify the network to solve human parsing by exploring a novel self-supervised structure-sensitive learning approach, which imposes human pose structures into the parsing results without resorting to extra supervision. The dataset, code and models are available at http://www.sysu-hcp.net/lip/.



### Markerless Inside-Out Tracking for Interventional Applications
- **Arxiv ID**: http://arxiv.org/abs/1804.01708v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.01708v3)
- **Published**: 2018-04-05 07:47:08+00:00
- **Updated**: 2018-06-12 09:35:31+00:00
- **Authors**: Benjamin Busam, Patrick Ruhkamp, Salvatore Virga, Beatrice Lentes, Julia Rackerseder, Nassir Navab, Christoph Hennersperger
- **Comment**: Medical Image Computing and Computer Assisted Interventions
- **Journal**: None
- **Summary**: Tracking of rotation and translation of medical instruments plays a substantial role in many modern interventions. Traditional external optical tracking systems are often subject to line-of-sight issues, in particular when the region of interest is difficult to access or the procedure allows only for limited rigid body markers. The introduction of inside-out tracking systems aims to overcome these issues. We propose a marker-less tracking system based on visual SLAM to enable tracking of instruments in an interventional scenario. To achieve this goal, we mount a miniature multi-modal (monocular, stereo, active depth) vision system on the object of interest and relocalize its pose within an adaptive map of the operating room. We compare state-of-the-art algorithmic pipelines and apply the idea to transrectal 3D Ultrasound (TRUS) compounding of the prostate. Obtained volumes are compared to reconstruction using a commercial optical tracking system as well as a robotic manipulator. Feature-based binocular SLAM is identified as the most promising method and is tested extensively in challenging clinical environment under severe occlusion and for the use case of prostate US biopsies.



### Finding beans in burgers: Deep semantic-visual embedding with localization
- **Arxiv ID**: http://arxiv.org/abs/1804.01720v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1804.01720v2)
- **Published**: 2018-04-05 08:13:37+00:00
- **Updated**: 2018-04-06 14:04:35+00:00
- **Authors**: Martin Engilberge, Louis Chevallier, Patrick Pérez, Matthieu Cord
- **Comment**: Accepted to CVPR2018
- **Journal**: None
- **Summary**: Several works have proposed to learn a two-path neural network that maps images and texts, respectively, to a same shared Euclidean space where geometry captures useful semantic relationships. Such a multi-modal embedding can be trained and used for various tasks, notably image captioning. In the present work, we introduce a new architecture of this type, with a visual path that leverages recent space-aware pooling mechanisms. Combined with a textual path which is jointly trained from scratch, our semantic-visual embedding offers a versatile model. Once trained under the supervision of captioned images, it yields new state-of-the-art performance on cross-modal retrieval. It also allows the localization of new concepts from the embedding space into any input image, delivering state-of-the-art result on the visual grounding of phrases.



### Identifying Cross-Depicted Historical Motifs
- **Arxiv ID**: http://arxiv.org/abs/1804.01728v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.01728v2)
- **Published**: 2018-04-05 08:28:00+00:00
- **Updated**: 2018-12-06 19:56:10+00:00
- **Authors**: Vinaychandran Pondenkandath, Michele Alberti, Nicole Eichenberger, Rolf Ingold, Marcus Liwicki
- **Comment**: 6 pages, 6 figures
- **Journal**: 16th International Conference on Frontiers in Handwriting
  Recognition (Vol. 16, pp. 333-338), IEEE, 2018
- **Summary**: Cross-depiction is the problem of identifying the same object even when it is depicted in a variety of manners. This is a common problem in handwritten historical documents image analysis, for instance when the same letter or motif is depicted in several different ways. It is a simple task for humans yet conventional heuristic computer vision methods struggle to cope with it. In this paper we address this problem using state-of-the-art deep learning techniques on a dataset of historical watermarks containing images created with different methods of reproduction, such as hand tracing, rubbing, and radiography. To study the robustness of deep learning based approaches to the cross-depiction problem, we measure their performance on two different tasks: classification and similarity rankings. For the former we achieve a classification accuracy of 96% using deep convolutional neural networks. For the latter we have a false positive rate at 95% true positive rate of 0.11. These results outperform state-of-the-art methods by a significant margin.



### Missing Slice Recovery for Tensors Using a Low-rank Model in Embedded Space
- **Arxiv ID**: http://arxiv.org/abs/1804.01736v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DS
- **Links**: [PDF](http://arxiv.org/pdf/1804.01736v1)
- **Published**: 2018-04-05 08:44:01+00:00
- **Updated**: 2018-04-05 08:44:01+00:00
- **Authors**: Tatsuya Yokota, Burak Erem, Seyhmus Guler, Simon K. Warfield, Hidekata Hontani
- **Comment**: accepted for CVPR2018
- **Journal**: None
- **Summary**: Let us consider a case where all of the elements in some continuous slices are missing in tensor data.   In this case, the nuclear-norm and total variation regularization methods usually fail to recover the missing elements.   The key problem is capturing some delay/shift-invariant structure.   In this study, we consider a low-rank model in an embedded space of a tensor.   For this purpose, we extend a delay embedding for a time series to a "multi-way delay-embedding transform" for a tensor, which takes a given incomplete tensor as the input and outputs a higher-order incomplete Hankel tensor.   The higher-order tensor is then recovered by Tucker-based low-rank tensor factorization.   Finally, an estimated tensor can be obtained by using the inverse multi-way delay embedding transform of the recovered higher-order tensor.   Our experiments showed that the proposed method successfully recovered missing slices for some color images and functional magnetic resonance images.



### Bringing Cartoons to Life: Towards Improved Cartoon Face Detection and Recognition Systems
- **Arxiv ID**: http://arxiv.org/abs/1804.01753v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.01753v2)
- **Published**: 2018-04-05 09:59:00+00:00
- **Updated**: 2018-07-06 10:00:23+00:00
- **Authors**: Saurav Jha, Nikhil Agarwal, Suneeta Agarwal
- **Comment**: 8 pages, 5 figures, 7 tables
- **Journal**: None
- **Summary**: Given the recent deep learning advancements in face detection and recognition techniques for human faces, this paper answers the question "how well would they work for cartoons'?" - a domain that remains largely unexplored until recently, mainly due to the unavailability of large scale datasets and the failure of traditional methods on these. Our work studies and extends multiple frameworks for the aforementioned tasks. For face detection, we incorporate the Multi-task Cascaded Convolutional Network (MTCNN) architecture and contrast it with conventional methods. For face recognition, our two-fold contributions include: (i) an inductive transfer learning approach combining the feature learning capability of the Inception v3 network and the feature recognizing capability of Support Vector Machines (SVMs), (ii) a proposed Hybrid Convolutional Neural Network (HCNN) framework trained over a fusion of pixel values and 15 manually located facial keypoints. All the methods are evaluated on the Cartoon Faces in the Wild (IIIT-CFW) database. We demonstrate that the HCNN model offers stability superior to that of Inception+SVM over larger input variations, and explore the plausible architectural principles. We show that the Inception+SVM model establishes a state-of-the-art F1 score on the task of gender recognition of cartoon faces. Further, we introduce a small database hosting location coordinates of 15 points on the cartoon faces belonging to 50 public figures of the IIIT-CFW database.



### Learning a Robust Society of Tracking Parts using Co-occurrence Constraints
- **Arxiv ID**: http://arxiv.org/abs/1804.01771v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.01771v2)
- **Published**: 2018-04-05 10:43:35+00:00
- **Updated**: 2018-11-08 09:40:46+00:00
- **Authors**: Elena Burceanu, Marius Leordeanu
- **Comment**: 17+3 pages, 5 figures, European Conference on Computer Vision (ECCV),
  Visual Object Tracking workshop
- **Journal**: None
- **Summary**: Object tracking is an essential problem in computer vision that has been researched for several decades. One of the main challenges in tracking is to adapt to object appearance changes over time and avoiding drifting to background clutter. We address this challenge by proposing a deep neural network composed of different parts, which functions as a society of tracking parts. They work in conjunction according to a certain policy and learn from each other in a robust manner, using co-occurrence constraints that ensure robust inference and learning. From a structural point of view, our network is composed of two main pathways. One pathway is more conservative. It carefully monitors a large set of simple tracker parts learned as linear filters over deep feature activation maps. It assigns the parts different roles. It promotes the reliable ones and removes the inconsistent ones. We learn these filters simultaneously in an efficient way, with a single closed-form formulation, for which we propose novel theoretical properties. The second pathway is more progressive. It is learned completely online and thus it is able to better model object appearance changes. In order to adapt in a robust manner, it is learned only on highly confident frames, which are decided using co-occurrences with the first pathway. Thus, our system has the full benefit of two main approaches in tracking. The larger set of simpler filter parts offers robustness, while the full deep network learned online provides adaptability to change. As shown in the experimental section, our approach achieves state of the art performance on the challenging VOT17 benchmark, outperforming the published methods both on the general EAO metric and in the number of fails, by a significant margin.



### End-to-End Saliency Mapping via Probability Distribution Prediction
- **Arxiv ID**: http://arxiv.org/abs/1804.01793v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1804.01793v1)
- **Published**: 2018-04-05 11:59:01+00:00
- **Updated**: 2018-04-05 11:59:01+00:00
- **Authors**: Saumya Jetley, Naila Murray, Eleonora Vig
- **Comment**: None
- **Journal**: Proceedings of IEEE Conference on Computer Vision and Pattern
  Recognition 2016
- **Summary**: Most saliency estimation methods aim to explicitly model low-level conspicuity cues such as edges or blobs and may additionally incorporate top-down cues using face or text detection. Data-driven methods for training saliency models using eye-fixation data are increasingly popular, particularly with the introduction of large-scale datasets and deep architectures. However, current methods in this latter paradigm use loss functions designed for classification or regression tasks whereas saliency estimation is evaluated on topographical maps. In this work, we introduce a new saliency map model which formulates a map as a generalized Bernoulli distribution. We then train a deep architecture to predict such maps using novel loss functions which pair the softmax activation function with measures designed to compute distances between probability distributions. We show in extensive experiments the effectiveness of such loss functions over standard ones on four public benchmark datasets, and demonstrate improved performance over state-of-the-art saliency methods.



### Guess Where? Actor-Supervision for Spatiotemporal Action Localization
- **Arxiv ID**: http://arxiv.org/abs/1804.01824v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.01824v1)
- **Published**: 2018-04-05 13:08:25+00:00
- **Updated**: 2018-04-05 13:08:25+00:00
- **Authors**: Victor Escorcia, Cuong D. Dao, Mihir Jain, Bernard Ghanem, Cees Snoek
- **Comment**: cvpr version
- **Journal**: None
- **Summary**: This paper addresses the problem of spatiotemporal localization of actions in videos. Compared to leading approaches, which all learn to localize based on carefully annotated boxes on training video frames, we adhere to a weakly-supervised solution that only requires a video class label. We introduce an actor-supervised architecture that exploits the inherent compositionality of actions in terms of actor transformations, to localize actions. We make two contributions. First, we propose actor proposals derived from a detector for human and non-human actors intended for images, which is linked over time by Siamese similarity matching to account for actor deformations. Second, we propose an actor-based attention mechanism that enables the localization of the actions from action class labels and actor proposals and is end-to-end trainable. Experiments on three human and non-human action datasets show actor supervision is state-of-the-art for weakly-supervised action localization and is even competitive to some fully-supervised alternatives.



### Large Scale Local Online Similarity/Distance Learning Framework based on Passive/Aggressive
- **Arxiv ID**: http://arxiv.org/abs/1804.01900v1
- **DOI**: 10.1142/S0218001421510174
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1804.01900v1)
- **Published**: 2018-04-05 15:11:11+00:00
- **Updated**: 2018-04-05 15:11:11+00:00
- **Authors**: Baida Hamdan, Davood Zabihzadeh, Monsefi Reza
- **Comment**: None
- **Journal**: None
- **Summary**: Similarity/Distance measures play a key role in many machine learning, pattern recognition, and data mining algorithms, which leads to the emergence of metric learning field. Many metric learning algorithms learn a global distance function from data that satisfy the constraints of the problem. However, in many real-world datasets that the discrimination power of features varies in the different regions of input space, a global metric is often unable to capture the complexity of the task. To address this challenge, local metric learning methods are proposed that learn multiple metrics across the different regions of input space. Some advantages of these methods are high flexibility and the ability to learn a nonlinear mapping but typically achieves at the expense of higher time requirement and overfitting problem. To overcome these challenges, this research presents an online multiple metric learning framework. Each metric in the proposed framework is composed of a global and a local component learned simultaneously. Adding a global component to a local metric efficiently reduce the problem of overfitting. The proposed framework is also scalable with both sample size and the dimension of input data. To the best of our knowledge, this is the first local online similarity/distance learning framework based on PA (Passive/Aggressive). In addition, for scalability with the dimension of input data, DRP (Dual Random Projection) is extended for local online learning in the present work. It enables our methods to be run efficiently on high-dimensional datasets, while maintains their predictive performance. The proposed framework provides a straightforward local extension to any global online similarity/distance learning algorithm based on PA.



### Towards radiologist-level cancer risk assessment in CT lung screening using deep learning
- **Arxiv ID**: http://arxiv.org/abs/1804.01901v2
- **DOI**: 10.1016/j.compmedimag.2021.101883
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.01901v2)
- **Published**: 2018-04-05 15:12:33+00:00
- **Updated**: 2019-04-11 21:38:42+00:00
- **Authors**: Stojan Trajanovski, Dimitrios Mavroeidis, Christine Leon Swisher, Binyam Gebrekidan Gebre, Bastiaan S. Veeling, Rafael Wiemker, Tobias Klinder, Amir Tahmasebi, Shawn M. Regis, Christoph Wald, Brady J. McKee, Sebastian Flacke, Heber MacMahon, Homer Pien
- **Comment**: Submitted for publication. 11 pages
- **Journal**: Computerized Medical Imaging and Graphics, Elsevier, 2021
- **Summary**: Importance: Lung cancer is the leading cause of cancer mortality in the US, responsible for more deaths than breast, prostate, colon and pancreas cancer combined and it has been recently demonstrated that low-dose computed tomography (CT) screening of the chest can significantly reduce this death rate.   Objective: To compare the performance of a deep learning model to state-of-the-art automated algorithms and radiologists as well as assessing the robustness of the algorithm in heterogeneous datasets.   Design, Setting, and Participants: Three low-dose CT lung cancer screening datasets from heterogeneous sources were used, including National Lung Screening Trial (NLST, n=3410), Lahey Hospital and Medical Center (LHMC, n=3174) data, Kaggle competition data (from both stages, n=1595+505) and the University of Chicago data (UCM, a subset of NLST, annotated by radiologists, n=197). Relevant works on automated methods for Lung Cancer malignancy estimation have used significantly less data in size and diversity. At the first stage, our framework employs a nodule detector; while in the second stage, we use both the image area around the nodules and nodule features as inputs to a neural network that estimates the malignancy risk for the entire CT scan. We trained our two-stage algorithm on a part of the NLST dataset, and validated it on the other datasets.   Results, Conclusions, and Relevance: The proposed deep learning model: (a) generalizes well across all three data sets, achieving AUC between 86% to 94%; (b) has better performance than the widely accepted PanCan Risk Model, achieving 11% better AUC score; (c) has improved performance compared to the state-of-the-art represented by the winners of the Kaggle Data Science Bowl 2017 competition on lung cancer screening; (d) has comparable performance to radiologists in estimating cancer risk at a patient level.



### Multi-level Activation for Segmentation of Hierarchically-nested Classes
- **Arxiv ID**: http://arxiv.org/abs/1804.01910v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.01910v2)
- **Published**: 2018-04-05 15:27:02+00:00
- **Updated**: 2018-08-21 09:59:59+00:00
- **Authors**: Marie Piraud, Anjany Sekuboyina, Bjoern H. Menze
- **Comment**: Accepted for the BioImage Computing 2018 workshop, ECCV conference
- **Journal**: None
- **Summary**: For many biological image segmentation tasks, including topological knowledge, such as the nesting of classes, can greatly improve results. However, most `out-of-the-box' CNN models are still blind to such prior information. In this paper, we propose a novel approach to encode this information, through a multi-level activation layer and three compatible losses. We benchmark all of them on nuclei segmentation in bright-field microscopy cell images from the 2018 Data Science Bowl challenge, offering an exemplary segmentation task with cells and nested subcellular structures. Our scheme greatly speeds up learning, and outperforms standard multi-class classification with soft-max activation and a previously proposed method stemming from it, improving the Dice score significantly (p-values<0.007). Our approach is conceptually simple, easy to implement and can be integrated in any CNN architecture. It can be generalized to a higher number of classes, with or without further relations of containment.



### Machine learning of neuroimaging to diagnose cognitive impairment and dementia: a systematic review and comparative analysis
- **Arxiv ID**: http://arxiv.org/abs/1804.01961v2
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1804.01961v2)
- **Published**: 2018-04-05 17:17:39+00:00
- **Updated**: 2018-04-11 22:01:51+00:00
- **Authors**: Enrico Pellegrini, Lucia Ballerini, Maria del C. Valdes Hernandez, Francesca M. Chappell, Victor González-Castro, Devasuda Anblagan, Samuel Danso, Susana Muñoz Maniega, Dominic Job, Cyril Pernet, Grant Mair, Tom MacGillivray, Emanuele Trucco, Joanna Wardlaw
- **Comment**: None
- **Journal**: None
- **Summary**: INTRODUCTION: Advanced machine learning methods might help to identify dementia risk from neuroimaging, but their accuracy to date is unclear.   METHODS: We systematically reviewed the literature, 2006 to late 2016, for machine learning studies differentiating healthy ageing through to dementia of various types, assessing study quality, and comparing accuracy at different disease boundaries.   RESULTS: Of 111 relevant studies, most assessed Alzheimer's disease (AD) vs healthy controls, used ADNI data, support vector machines and only T1-weighted sequences. Accuracy was highest for differentiating AD from healthy controls, and poor for differentiating healthy controls vs MCI vs AD, or MCI converters vs non-converters. Accuracy increased using combined data types, but not by data source, sample size or machine learning method.   DISCUSSION: Machine learning does not differentiate clinically-relevant disease categories yet. More diverse datasets, combinations of different types of data, and close clinical integration of machine learning would help to advance the field.



### Iris Recognition After Death
- **Arxiv ID**: http://arxiv.org/abs/1804.01962v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.01962v2)
- **Published**: 2018-04-05 17:19:26+00:00
- **Updated**: 2018-10-31 10:37:06+00:00
- **Authors**: Mateusz Trokielewicz, Adam Czajka, Piotr Maciejewicz
- **Comment**: Paper accepted for publication in the IEEE Transactions on
  Information Forensics and Security
- **Journal**: None
- **Summary**: This paper presents a comprehensive study of post-mortem human iris recognition carried out for 1,200 near-infrared and 1,787 visible-light samples collected from 37 deceased individuals kept in the mortuary conditions. We used four independent iris recognition methods (three commercial and one academic) to analyze genuine and impostor comparison scores and check the dynamics of iris quality decay over a period of up to 814 hours after death. This study shows that post-mortem iris recognition may be close-to-perfect approximately 5 to 7 hours after death and occasionally is still viable even 21 days after death. These conclusions contradict the statements present in past literature that the iris is unusable as a biometrics shortly after death, and show that the dynamics of post-mortem changes to the iris that are important for biometric identification are more moderate than previously hypothesized. The paper contains a thorough medical commentary that helps to understand which post-mortem metamorphoses of the eye may impact the performance of automatic iris recognition. We also show that post-mortem iris recognition works equally well for images taken in near-infrared and when the red channel of visible-light sample is used. However, cross-wavelength matching presents significantly worse performance. This paper conforms to reproducible research and the database used in this study is made publicly available to facilitate research of post-mortem iris recognition. To our knowledge, this paper offers the most comprehensive evaluation of post-mortem iris recognition and the largest database of post-mortem iris images.



### CBMV: A Coalesced Bidirectional Matching Volume for Disparity Estimation
- **Arxiv ID**: http://arxiv.org/abs/1804.01967v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.01967v1)
- **Published**: 2018-04-05 17:30:56+00:00
- **Updated**: 2018-04-05 17:30:56+00:00
- **Authors**: Konstantinos Batsos, Changjiang Cai, Philippos Mordohai
- **Comment**: Accepted to Computer Vision and Pattern Recognition (CVPR) 2018
- **Journal**: None
- **Summary**: Recently, there has been a paradigm shift in stereo matching with learning-based methods achieving the best results on all popular benchmarks. The success of these methods is due to the availability of training data with ground truth; training learning-based systems on these datasets has allowed them to surpass the accuracy of conventional approaches based on heuristics and assumptions. Many of these assumptions, however, had been validated extensively and hold for the majority of possible inputs. In this paper, we generate a matching volume leveraging both data with ground truth and conventional wisdom. We accomplish this by coalescing diverse evidence from a bidirectional matching process via random forest classifiers. We show that the resulting matching volume estimation method achieves similar accuracy to purely data-driven alternatives on benchmarks and that it generalizes to unseen data much better. In fact, the results we submitted to the KITTI and ETH3D benchmarks were generated using a classifier trained on the Middlebury 2014 dataset.



### Regularizing Deep Networks by Modeling and Predicting Label Structure
- **Arxiv ID**: http://arxiv.org/abs/1804.02009v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.02009v1)
- **Published**: 2018-04-05 18:17:18+00:00
- **Updated**: 2018-04-05 18:17:18+00:00
- **Authors**: Mohammadreza Mostajabi, Michael Maire, Gregory Shakhnarovich
- **Comment**: to appear at CVPR 2018
- **Journal**: None
- **Summary**: We construct custom regularization functions for use in supervised training of deep neural networks. Our technique is applicable when the ground-truth labels themselves exhibit internal structure; we derive a regularizer by learning an autoencoder over the set of annotations. Training thereby becomes a two-phase procedure. The first phase models labels with an autoencoder. The second phase trains the actual network of interest by attaching an auxiliary branch that must predict output via a hidden layer of the autoencoder. After training, we discard this auxiliary branch.   We experiment in the context of semantic segmentation, demonstrating this regularization strategy leads to consistent accuracy boosts over baselines, both when training from scratch, or in combination with ImageNet pretraining. Gains are also consistent over different choices of convolutional network architecture. As our regularizer is discarded after training, our method has zero cost at test time; the performance improvements are essentially free. We are simply able to learn better network weights by building an abstract model of the label space, and then training the network to understand this abstraction alongside the original task.



### A Multi-Layer Approach to Superpixel-based Higher-order Conditional Random Field for Semantic Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1804.02032v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.02032v1)
- **Published**: 2018-04-05 19:31:03+00:00
- **Updated**: 2018-04-05 19:31:03+00:00
- **Authors**: Li Sulimowicz, Ishfaq Ahmad, Alexander Aved
- **Comment**: 7pages, 6 figures, 3 tables, an updated version from original
  submission to ICME2018
- **Journal**: None
- **Summary**: Superpixel-based Higher-order Conditional random fields (SP-HO-CRFs) are known for their effectiveness in enforcing both short and long spatial contiguity for pixelwise labelling in computer vision. However, their higher-order potentials are usually too complex to learn and often incur a high computational cost in performing inference. We propose an new approximation approach to SP-HO-CRFs that resolves these problems. Our approach is a multi-layer CRF framework that inherits the simplicity from pairwise CRFs by formulating both the higher-order and pairwise cues into the same pairwise potentials in the first layer. Essentially, this approach provides accuracy enhancement on the basis of pairwise CRFs without training by reusing their pre-trained parameters and/or weights. The proposed multi-layer approach performs especially well in delineating the boundary details (boarders) of object categories such as "trees" and "bushes". Multiple sets of experiments conducted on dataset MSRC-21 and PASCAL VOC 2012 validate the effectiveness and efficiency of the proposed methods.



### Pedestrian-Synthesis-GAN: Generating Pedestrian Data in Real Scene and Beyond
- **Arxiv ID**: http://arxiv.org/abs/1804.02047v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1804.02047v2)
- **Published**: 2018-04-05 20:22:01+00:00
- **Updated**: 2018-04-14 07:19:28+00:00
- **Authors**: Xi Ouyang, Yu Cheng, Yifan Jiang, Chun-Liang Li, Pan Zhou
- **Comment**: v2.0,adding supplementary
- **Journal**: None
- **Summary**: State-of-the-art pedestrian detection models have achieved great success in many benchmarks. However, these models require lots of annotation information and the labeling process usually takes much time and efforts. In this paper, we propose a method to generate labeled pedestrian data and adapt them to support the training of pedestrian detectors. The proposed framework is built on the Generative Adversarial Network (GAN) with multiple discriminators, trying to synthesize realistic pedestrians and learn the background context simultaneously. To handle the pedestrians of different sizes, we adopt the Spatial Pyramid Pooling (SPP) layer in the discriminator. We conduct experiments on two benchmarks. The results show that our framework can smoothly synthesize pedestrians on background images of variations and different levels of details. To quantitatively evaluate our approach, we add the generated samples into training data of the baseline pedestrian detectors and show the synthetic images are able to improve the detectors' performance.



### FPAN: Fine-grained and Progressive Attention Localization Network for Data Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1804.02056v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.02056v1)
- **Published**: 2018-04-05 20:59:36+00:00
- **Updated**: 2018-04-05 20:59:36+00:00
- **Authors**: Sijia Chen, Bin Song, Jie Guo, Xiaojiang Du, Mohsen Guizani
- **Comment**: None
- **Journal**: None
- **Summary**: The Localization of the target object for data retrieval is a key issue in the Intelligent and Connected Transportation Systems (ICTS). However, due to lack of intelligence in the traditional transportation system, it can take tremendous resources to manually retrieve and locate the queried objects among a large number of images. In order to solve this issue, we propose an effective method to query-based object localization that uses artificial intelligence techniques to automatically locate the queried object in the complex background. The presented method is termed as Fine-grained and Progressive Attention Localization Network (FPAN), which uses an image and a queried object as input to accurately locate the target object in the image. Specifically, the fine-grained attention module is naturally embedded into each layer of the convolution neural network (CNN), thereby gradually suppressing the regions that are irrelevant to the queried object and eventually shrinking attention to the target area. We further employ top-down attentions fusion algorithm operated by a learnable cascade up-sampling structure to establish the connection between the attention map and the exact location of the queried object in the original image. Furthermore, the FPAN is trained by multi-task learning with box segmentation loss and cosine loss. At last, we conduct comprehensive experiments on both queried-based digit localization and object tracking with synthetic and benchmark datasets, respectively. The experimental results show that our algorithm is far superior to other algorithms in the synthesis datasets and outperforms most existing trackers on the OTB and VOT datasets.



### Closed-form detector for solid sub-pixel targets in multivariate t-distributed background clutter
- **Arxiv ID**: http://arxiv.org/abs/1804.02062v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/1804.02062v2)
- **Published**: 2018-04-05 21:28:51+00:00
- **Updated**: 2018-04-28 19:12:42+00:00
- **Authors**: James Theiler, Beate Zimmer, Amanda Ziemann
- **Comment**: 4 pages, 2 figures
- **Journal**: None
- **Summary**: The generalized likelihood ratio test (GLRT) is used to derive a detector for solid sub-pixel targets in hyperspectral imagery. A closed-form solution is obtained that optimizes the replacement target model when the background is a fat-tailed elliptically-contoured multivariate t-distribution. This generalizes GLRT-based detectors that have previously been derived for the replacement target model with Gaussian background, and for the additive target model with an elliptically-contoured background. Experiments with simulated hyperspectral data illustrate the performance of this detector in various parameter regimes.



### Noise-resistant Deep Learning for Object Classification in 3D Point Clouds Using a Point Pair Descriptor
- **Arxiv ID**: http://arxiv.org/abs/1804.02077v1
- **DOI**: 10.1109/LRA.2018.2792681
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1804.02077v1)
- **Published**: 2018-04-05 23:19:55+00:00
- **Updated**: 2018-04-05 23:19:55+00:00
- **Authors**: Dmytro Bobkov, Sili Chen, Ruiqing Jian, Muhammad Iqbal, Eckehard Steinbach
- **Comment**: 8 pages
- **Journal**: IEEE Robotics and Automation Letters 2018 Volume 3, Issue 2 IEEE
  Robotics and Automation Letters IEEE Robotics and Automation Letters
- **Summary**: Object retrieval and classification in point cloud data is challenged by noise, irregular sampling density and occlusion. To address this issue, we propose a point pair descriptor that is robust to noise and occlusion and achieves high retrieval accuracy. We further show how the proposed descriptor can be used in a 4D convolutional neural network for the task of object classification. We propose a novel 4D convolutional layer that is able to learn class-specific clusters in the descriptor histograms. Finally, we provide experimental validation on 3 benchmark datasets, which confirms the superiority of the proposed approach.



