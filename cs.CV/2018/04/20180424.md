# Arxiv Papers in cs.CV on 2018-04-24
### Face Recognition: Primates in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1804.08790v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.08790v1)
- **Published**: 2018-04-24 00:19:32+00:00
- **Updated**: 2018-04-24 00:19:32+00:00
- **Authors**: Debayan Deb, Susan Wiper, Alexandra Russo, Sixue Gong, Yichun Shi, Cori Tymoszek, Anil Jain
- **Comment**: 8 pages, 12 figures
- **Journal**: None
- **Summary**: We present a new method of primate face recognition, and evaluate this method on several endangered primates, including golden monkeys, lemurs, and chimpanzees. The three datasets contain a total of 11,637 images of 280 individual primates from 14 species. Primate face recognition performance is evaluated using two existing state-of-the-art open-source systems, (i) FaceNet and (ii) SphereFace, (iii) a lemur face recognition system from literature, and (iv) our new convolutional neural network (CNN) architecture called PrimNet. Three recognition scenarios are considered: verification (1:1 comparison), and both open-set and closed-set identification (1:N search). We demonstrate that PrimNet outperforms all of the other systems in all three scenarios for all primate species tested. Finally, we implement an Android application of this recognition system to assist primate researchers and conservationists in the wild for individual recognition of primates.



### Explaining hyperspectral imaging based plant disease identification: 3D CNN and saliency maps
- **Arxiv ID**: http://arxiv.org/abs/1804.08831v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.08831v1)
- **Published**: 2018-04-24 03:39:36+00:00
- **Updated**: 2018-04-24 03:39:36+00:00
- **Authors**: Koushik Nagasubramanian, Sarah Jones, Asheesh K. Singh, Arti Singh, Baskar Ganapathysubramanian, Soumik Sarkar
- **Comment**: None
- **Journal**: None
- **Summary**: Our overarching goal is to develop an accurate and explainable model for plant disease identification using hyperspectral data. Charcoal rot is a soil borne fungal disease that affects the yield of soybean crops worldwide. Hyperspectral images were captured at 240 different wavelengths in the range of 383 - 1032 nm. We developed a 3D Convolutional Neural Network model for soybean charcoal rot disease identification. Our model has classification accuracy of 95.73\% and an infected class F1 score of 0.87. We infer the trained model using saliency map and visualize the most sensitive pixel locations that enable classification. The sensitivity of individual wavelengths for classification was also determined using the saliency map visualization. We identify the most sensitive wavelength as 733 nm using the saliency map visualization. Since the most sensitive wavelength is in the Near Infrared Region(700 - 1000 nm) of the electromagnetic spectrum, which is also the commonly used spectrum region for determining the vegetation health of the plant, we were more confident in the predictions using our model.



### Matlab Implementation of Machine Vision Algorithm on Ballast Degradation Evaluation
- **Arxiv ID**: http://arxiv.org/abs/1804.08835v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.08835v1)
- **Published**: 2018-04-24 04:16:46+00:00
- **Updated**: 2018-04-24 04:16:46+00:00
- **Authors**: Zixu Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: America has a massive railway system. As of 2006, U.S. freight railroads have 140,490 route- miles of standard gauge, but maintaining such a huge system and eliminating any dangers, like reduced track stability and poor drainage, caused by railway ballast degradation require huge amount of labor. The traditional way to quantify the degradation of ballast is to use an index called Fouling Index (FI) through ballast sampling and sieve analysis. However, determining the FI values in lab is very time-consuming and laborious, but with the help of recent development in the field of computer vision, a novel method for a potential machine-vison based ballast inspection system can be employed that can hopefully replace the traditional mechanical method. The new machine-vision approach analyses the images of the in-service ballasts, and then utilizes image segmentation algorithm to get ballast segments. By comparing the segment results and their corresponding FI values, this novel method produces a machine-vision-based index that has the best-fit relation with FI. The implementation details of how this algorithm works are discussed in this report.



### Spatiotemporal Learning of Dynamic Gestures from 3D Point Cloud Data
- **Arxiv ID**: http://arxiv.org/abs/1804.08859v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.08859v1)
- **Published**: 2018-04-24 06:48:56+00:00
- **Updated**: 2018-04-24 06:48:56+00:00
- **Authors**: Joshua Owoyemi, Koichi Hashimoto
- **Comment**: Accepted to ICRA2018, 6 Pages
- **Journal**: None
- **Summary**: In this paper, we demonstrate an end-to-end spatiotemporal gesture learning approach for 3D point cloud data using a new gestures dataset of point clouds acquired from a 3D sensor. Nine classes of gestures were learned from gestures sample data. We mapped point cloud data into dense occupancy grids, then time steps of the occupancy grids are used as inputs into a 3D convolutional neural network which learns the spatiotemporal features in the data without explicit modeling of gesture dynamics. We also introduced a 3D region of interest jittering approach for point cloud data augmentation. This resulted in an increased classification accuracy of up to 10% when the augmented data is added to the original training data. The developed model is able to classify gestures from the dataset with 84.44% accuracy. We propose that point cloud data will be a more viable data type for scene understanding and motion recognition, as 3D sensors become ubiquitous in years to come.



### Learning to See the Invisible: End-to-End Trainable Amodal Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1804.08864v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.08864v1)
- **Published**: 2018-04-24 06:55:44+00:00
- **Updated**: 2018-04-24 06:55:44+00:00
- **Authors**: Patrick Follmann, Rebecca König, Philipp Härtinger, Michael Klostermann
- **Comment**: 14 pages, plus appendix
- **Journal**: None
- **Summary**: Semantic amodal segmentation is a recently proposed extension to instance-aware segmentation that includes the prediction of the invisible region of each object instance. We present the first all-in-one end-to-end trainable model for semantic amodal segmentation that predicts the amodal instance masks as well as their visible and invisible part in a single forward pass. In a detailed analysis, we provide experiments to show which architecture choices are beneficial for an all-in-one amodal segmentation model. On the COCO amodal dataset, our model outperforms the current baseline for amodal segmentation by a large margin. To further evaluate our model, we provide two new datasets with ground truth for semantic amodal segmentation, D2S amodal and COCOA cls. For both datasets, our model provides a strong baseline performance. Using special data augmentation techniques, we show that amodal segmentation on D2S amodal is possible with reasonable performance, even without providing amodal training data.



### Homocentric Hypersphere Feature Embedding for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1804.08866v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.08866v2)
- **Published**: 2018-04-24 07:09:58+00:00
- **Updated**: 2018-05-01 01:49:49+00:00
- **Authors**: Wangmeng Xiang, Jianqiang Huang, Xianbiao Qi, Xiansheng Hua, Lei Zhang
- **Comment**: 11 pages, 7 figures
- **Journal**: None
- **Summary**: Person re-identification (Person ReID) is a challenging task due to the large variations in camera viewpoint, lighting, resolution, and human pose. Recently, with the advancement of deep learning technologies, the performance of Person ReID has been improved swiftly. Feature extraction and feature matching are two crucial components in the training and deployment stages of Person ReID. However, many existing Person ReID methods have measure inconsistency between the training stage and the deployment stage, and they couple magnitude and orientation information of feature vectors in feature representation. Meanwhile, traditional triplet loss methods focus on samples within a mini-batch and lack knowledge of global feature distribution. To address these issues, we propose a novel homocentric hypersphere embedding scheme to decouple magnitude and orientation information for both feature and weight vectors, and reformulate classification loss and triplet loss to their angular versions and combine them into an angular discriminative loss. We evaluate our proposed method extensively on the widely used Person ReID benchmarks, including Market1501, CUHK03 and DukeMTMC-ReID. Our method demonstrates leading performance on all datasets.



### Assessment of Deep Convolutional Neural Networks for Road Surface Classification
- **Arxiv ID**: http://arxiv.org/abs/1804.08872v2
- **DOI**: 10.1109/ITSC.2018.8569396
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.08872v2)
- **Published**: 2018-04-24 07:20:45+00:00
- **Updated**: 2018-08-07 10:32:41+00:00
- **Authors**: Marcus Nolte, Nikita Kister, Markus Maurer
- **Comment**: None
- **Journal**: None
- **Summary**: When parameterizing vehicle control algorithms for stability or trajectory control, the road-tire friction coefficient is an essential model parameter when it comes to control performance. One major impact on the friction coefficient is the condition of the road surface. A camera-based, forward-looking classification of the road-surface helps enabling an early parametrization of vehicle control algorithms. In this paper, we train and compare two different Deep Convolutional Neural Network models, regarding their application for road friction estimation and describe the challenges for training the classifier in terms of available training data and the construction of suitable datasets.



### Mask-aware Photorealistic Face Attribute Manipulation
- **Arxiv ID**: http://arxiv.org/abs/1804.08882v1
- **DOI**: 10.1007/s41095-021-0219-7
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.08882v1)
- **Published**: 2018-04-24 08:03:11+00:00
- **Updated**: 2018-04-24 08:03:11+00:00
- **Authors**: Ruoqi Sun, Chen Huang, Jianping Shi, Lizhuang Ma
- **Comment**: 7 pages, 4 figures. Computational Visual Media(2021)
- **Journal**: Computer Visual Media(2021)
- **Summary**: The task of face attribute manipulation has found increasing applications, but still remains challenging with the requirement of editing the attributes of a face image while preserving its unique details. In this paper, we choose to combine the Variational AutoEncoder (VAE) and Generative Adversarial Network (GAN) for photorealistic image generation. We propose an effective method to modify a modest amount of pixels in the feature maps of an encoder, changing the attribute strength continuously without hindering global information. Our training objectives of VAE and GAN are reinforced by the supervision of face recognition loss and cycle consistency loss for faithful preservation of face details. Moreover, we generate facial masks to enforce background consistency, which allows our training to focus on manipulating the foreground face rather than background. Experimental results demonstrate our method, called Mask-Adversarial AutoEncoder (M-AAE), can generate high-quality images with changing attributes and outperforms prior methods in detail preservation.



### Segmentation of Scanning Tunneling Microscopy Images Using Variational Methods and Empirical Wavelets
- **Arxiv ID**: http://arxiv.org/abs/1804.08890v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.08890v1)
- **Published**: 2018-04-24 08:16:59+00:00
- **Updated**: 2018-04-24 08:16:59+00:00
- **Authors**: Bui Kevin, Fauman Jacob, Kes David, Torres Mandiola Leticia, Ciomaga Adina, Salazar Ricardo, Bertozzi L. Andrea, Gilles Jerome, Guttentag I. Andrew, Weiss S. Paul
- **Comment**: None
- **Journal**: None
- **Summary**: In the fields of nanoscience and nanotechnology, it is important to be able to functionalize surfaces chemically for a wide variety of applications. Scanning tunneling microscopes (STMs) are important instruments in this area used to measure the surface structure and chemistry with better than molecular resolution. Self-assembly is frequently used to create monolayers that redefine the surface chemistry in just a single-molecule-thick layer. Indeed, STM images reveal rich information about the structure of self-assembled monolayers since they convey chemical and physical properties of the studied material.   In order to assist in and to enhance the analysis of STM and other images, we propose and demonstrate an image-processing framework that produces two image segmentations: one is based on intensities (apparent heights in STM images) and the other is based on textural patterns. The proposed framework begins with a cartoon+texture decomposition, which separates an image into its cartoon and texture components. Afterward, the cartoon image is segmented by a modified multiphase version of the local Chan-Vese model, while the texture image is segmented by a combination of 2D empirical wavelet transform and a clustering algorithm. Overall, our proposed framework contains several new features, specifically in presenting a new application of cartoon+texture decomposition and of the empirical wavelet transforms and in developing a specialized framework to segment STM images and other data. To demonstrate the potential of our approach, we apply it to actual STM images of cyanide monolayers on Au\{111\} and present their corresponding segmentation results.



### Accurate 3-D Reconstruction with RGB-D Cameras using Depth Map Fusion and Pose Refinement
- **Arxiv ID**: http://arxiv.org/abs/1804.08912v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.08912v1)
- **Published**: 2018-04-24 09:12:04+00:00
- **Updated**: 2018-04-24 09:12:04+00:00
- **Authors**: Markus Ylimäki, Juho Kannala, Janne Heikkilä
- **Comment**: Accepted to ICPR 2018
- **Journal**: None
- **Summary**: Depth map fusion is an essential part in both stereo and RGB-D based 3-D reconstruction pipelines. Whether produced with a passive stereo reconstruction or using an active depth sensor, such as Microsoft Kinect, the depth maps have noise and may have poor initial registration. In this paper, we introduce a method which is capable of handling outliers, and especially, even significant registration errors. The proposed method first fuses a sequence of depth maps into a single non-redundant point cloud so that the redundant points are merged together by giving more weight to more certain measurements. Then, the original depth maps are re-registered to the fused point cloud to refine the original camera extrinsic parameters. The fusion is then performed again with the refined extrinsic parameters. This procedure is repeated until the result is satisfying or no significant changes happen between iterations. The method is robust to outliers and erroneous depth measurements as well as even significant depth map registration errors due to inaccurate initial camera poses.



### Mining Automatically Estimated Poses from Video Recordings of Top Athletes
- **Arxiv ID**: http://arxiv.org/abs/1804.08944v2
- **DOI**: 10.2478/ijcss-2018-0005
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1804.08944v2)
- **Published**: 2018-04-24 10:30:12+00:00
- **Updated**: 2018-04-27 12:43:27+00:00
- **Authors**: Rainer Lienhart, Moritz Einfalt, Dan Zecha
- **Comment**: Under review for the International Journal of Computer Science in
  Sport
- **Journal**: None
- **Summary**: Human pose detection systems based on state-of-the-art DNNs are on the go to be extended, adapted and re-trained to fit the application domain of specific sports. Therefore, plenty of noisy pose data will soon be available from videos recorded at a regular and frequent basis. This work is among the first to develop mining algorithms that can mine the expected abundance of noisy and annotation-free pose data from video recordings in individual sports. Using swimming as an example of a sport with dominant cyclic motion, we show how to determine unsupervised time-continuous cycle speeds and temporally striking poses as well as measure unsupervised cycle stability over time. Additionally, we use long jump as an example of a sport with a rigid phase-based motion to present a technique to automatically partition the temporally estimated pose sequences into their respective phases. This enables the extraction of performance relevant, pose-based metrics currently used by national professional sports associations. Experimental results prove the effectiveness of our mining algorithms, which can also be applied to other cycle-based or phase-based types of sport.



### Correlation Tracking via Joint Discrimination and Reliability Learning
- **Arxiv ID**: http://arxiv.org/abs/1804.08965v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.08965v1)
- **Published**: 2018-04-24 11:38:22+00:00
- **Updated**: 2018-04-24 11:38:22+00:00
- **Authors**: Chong Sun, Dong Wang, Huchuan Lu, Ming-Hsuan Yang
- **Comment**: To appear in CVPR2018
- **Journal**: None
- **Summary**: For visual tracking, an ideal filter learned by the correlation filter (CF) method should take both discrimination and reliability information. However, existing attempts usually focus on the former one while pay less attention to reliability learning. This may make the learned filter be dominated by the unexpected salient regions on the feature map, thereby resulting in model degradation. To address this issue, we propose a novel CF-based optimization problem to jointly model the discrimination and reliability information. First, we treat the filter as the element-wise product of a base filter and a reliability term. The base filter is aimed to learn the discrimination information between the target and backgrounds, and the reliability term encourages the final filter to focus on more reliable regions. Second, we introduce a local response consistency regular term to emphasize equal contributions of different regions and avoid the tracker being dominated by unreliable regions. The proposed optimization problem can be solved using the alternating direction method and speeded up in the Fourier domain. We conduct extensive experiments on the OTB-2013, OTB-2015 and VOT-2016 datasets to evaluate the proposed tracker. Experimental results show that our tracker performs favorably against other state-of-the-art trackers.



### FaceShop: Deep Sketch-based Face Image Editing
- **Arxiv ID**: http://arxiv.org/abs/1804.08972v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1804.08972v2)
- **Published**: 2018-04-24 12:03:45+00:00
- **Updated**: 2018-06-07 13:28:54+00:00
- **Authors**: Tiziano Portenier, Qiyang Hu, Attila Szabó, Siavash Arjomand Bigdeli, Paolo Favaro, Matthias Zwicker
- **Comment**: 13 pages, 20 figures
- **Journal**: None
- **Summary**: We present a novel system for sketch-based face image editing, enabling users to edit images intuitively by sketching a few strokes on a region of interest. Our interface features tools to express a desired image manipulation by providing both geometry and color constraints as user-drawn strokes. As an alternative to the direct user input, our proposed system naturally supports a copy-paste mode, which allows users to edit a given image region by using parts of another exemplar image without the need of hand-drawn sketching at all. The proposed interface runs in real-time and facilitates an interactive and iterative workflow to quickly express the intended edits. Our system is based on a novel sketch domain and a convolutional neural network trained end-to-end to automatically learn to render image regions corresponding to the input strokes. To achieve high quality and semantically consistent results we train our neural network on two simultaneous tasks, namely image completion and image translation. To the best of our knowledge, we are the first to combine these two tasks in a unified framework for interactive image editing. Our results show that the proposed sketch domain, network architecture, and training procedure generalize well to real user input and enable high quality synthesis results without additional post-processing.



### Infrared and visible image fusion using Latent Low-Rank Representation
- **Arxiv ID**: http://arxiv.org/abs/1804.08992v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.08992v5)
- **Published**: 2018-04-24 12:44:02+00:00
- **Updated**: 2022-01-29 06:25:24+00:00
- **Authors**: Hui Li, Xiao-Jun Wu
- **Comment**: 6 pages, 8 figures, 1 tables
- **Journal**: None
- **Summary**: Infrared and visible image fusion is an important problem in the field of image fusion which has been applied widely in many fields. To better preserve the useful information from source images, in this paper, we propose a novel image fusion method based on latent low-rank representation(LatLRR) which is simple and effective. Firstly, the source images are decomposed into low-rank parts(global structure) and salient parts(local structure) by LatLRR. Then, the low-rank parts are fused by weighted-average strategy to preserve more contour information. Then, the salient parts are simply fused by sum strategy which is a efficient operation in this fusion framework. Finally, the fused image is obtained by combining the fused low-rank part and the fused salient part. Compared with other fusion methods experimentally, the proposed method has better fusion performance than state-of-the-art fusion methods in both subjective and objective evaluation. The Code of our fusion method is available at https://github.com/hli1221/imagefusion\_Infrared\_visible\_latlrr



### An Anchor-Free Region Proposal Network for Faster R-CNN based Text Detection Approaches
- **Arxiv ID**: http://arxiv.org/abs/1804.09003v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.09003v1)
- **Published**: 2018-04-24 13:08:32+00:00
- **Updated**: 2018-04-24 13:08:32+00:00
- **Authors**: Zhuoyao Zhong, Lei Sun, Qiang Huo
- **Comment**: Technical report
- **Journal**: None
- **Summary**: The anchor mechanism of Faster R-CNN and SSD framework is considered not effective enough to scene text detection, which can be attributed to its IoU based matching criterion between anchors and ground-truth boxes. In order to better enclose scene text instances of various shapes, it requires to design anchors of various scales, aspect ratios and even orientations manually, which makes anchor-based methods sophisticated and inefficient. In this paper, we propose a novel anchor-free region proposal network (AF-RPN) to replace the original anchor-based RPN in the Faster R-CNN framework to address the above problem. Compared with a vanilla RPN and FPN-RPN, AF-RPN can get rid of complicated anchor design and achieve higher recall rate on large-scale COCO-Text dataset. Owing to the high-quality text proposals, our Faster R-CNN based two-stage text detection approach achieves state-of-the-art results on ICDAR-2017 MLT, ICDAR-2015 and ICDAR-2013 text detection benchmarks when using single-scale and single-model (ResNet50) testing only.



### Cubes3D: Neural Network based Optical Flow in Omnidirectional Image Scenes
- **Arxiv ID**: http://arxiv.org/abs/1804.09004v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.09004v2)
- **Published**: 2018-04-24 13:08:39+00:00
- **Updated**: 2018-05-22 07:01:30+00:00
- **Authors**: André Apitzsch, Roman Seidel, Gangolf Hirtz
- **Comment**: ICPRAI 2018
- **Journal**: International Journal on Pattern Recognition and Artificial
  Intelligence, Montreal, 2018, 164-169
- **Summary**: Optical flow estimation with convolutional neural networks (CNNs) has recently solved various tasks of computer vision successfully. In this paper we adapt a state-of-the-art approach for optical flow estimation to omnidirectional images. We investigate CNN architectures to determine high motion variations caused by the geometry of fish-eye images. Further we determine the qualitative influence of texture on the non-rigid object to the motion vectors. For evaluation of the results we create ground truth motion fields synthetically. The ground truth contains cubes with static background. We test variations of pre-trained FlowNet 2.0 architectures by indicating common error metrics. We generate competitive results for the motion of the foreground with inhomogeneous texture on the moving object.



### Developing a machine learning framework for estimating soil moisture with VNIR hyperspectral data
- **Arxiv ID**: http://arxiv.org/abs/1804.09046v4
- **DOI**: 10.5194/isprs-annals-IV-1-101-2018
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1804.09046v4)
- **Published**: 2018-04-24 13:52:35+00:00
- **Updated**: 2018-07-12 10:40:59+00:00
- **Authors**: Sina Keller, Felix M. Riese, Johanna Stötzer, Philipp M. Maier, Stefan Hinz
- **Comment**: Accepted at ISPRS TC I Midterm Symposium Karlsruhe (October 2018)
- **Journal**: ISPRS Ann. Photogramm. Remote Sens. Spatial Inf. Sci., IV-1,
  101-108, 2018
- **Summary**: In this paper, we investigate the potential of estimating the soil-moisture content based on VNIR hyperspectral data combined with LWIR data. Measurements from a multi-sensor field campaign represent the benchmark dataset which contains measured hyperspectral, LWIR, and soil-moisture data conducted on grassland site. We introduce a regression framework with three steps consisting of feature selection, preprocessing, and well-chosen regression models. The latter are mainly supervised machine learning models. An exception are the self-organizing maps which combine unsupervised and supervised learning. We analyze the impact of the distinct preprocessing methods on the regression results. Of all regression models, the extremely randomized trees model without preprocessing provides the best estimation performance. Our results reveal the potential of the respective regression framework combined with the VNIR hyperspectral data to estimate soil moisture measured under real-world conditions. In conclusion, the results of this paper provide a basis for further improvements in different research directions.



### ECO: Efficient Convolutional Network for Online Video Understanding
- **Arxiv ID**: http://arxiv.org/abs/1804.09066v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.IR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1804.09066v2)
- **Published**: 2018-04-24 14:30:56+00:00
- **Updated**: 2018-05-07 09:46:08+00:00
- **Authors**: Mohammadreza Zolfaghari, Kamaljeet Singh, Thomas Brox
- **Comment**: Submitted to ECCV 2018. 17 pages, 7 figures, Supplementary Material,
  https://github.com/mzolfaghari/ECO-efficient-video-understanding
- **Journal**: None
- **Summary**: The state of the art in video understanding suffers from two problems: (1) The major part of reasoning is performed locally in the video, therefore, it misses important relationships within actions that span several seconds. (2) While there are local methods with fast per-frame processing, the processing of the whole video is not efficient and hampers fast video retrieval or online classification of long-term activities. In this paper, we introduce a network architecture that takes long-term content into account and enables fast per-video processing at the same time. The architecture is based on merging long-term content already in the network rather than in a post-hoc fusion. Together with a sampling strategy, which exploits that neighboring frames are largely redundant, this yields high-quality action classification and video captioning at up to 230 videos per second, where each video can consist of a few hundred frames. The approach achieves competitive performance across all datasets while being 10x to 80x faster than state-of-the-art methods.



### Robust Video Content Alignment and Compensation for Clear Vision Through the Rain
- **Arxiv ID**: http://arxiv.org/abs/1804.09555v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.09555v1)
- **Published**: 2018-04-24 14:50:34+00:00
- **Updated**: 2018-04-24 14:50:34+00:00
- **Authors**: Jie Chen, Cheen-Hau Tan, Junhui Hou, Lap-Pui Chau, He Li
- **Comment**: arXiv admin note: text overlap with arXiv:1803.10433
- **Journal**: None
- **Summary**: Outdoor vision-based systems suffer from atmospheric turbulences, and rain is one of the worst factors for vision degradation. Current rain removal methods show limitations either for complex dynamic scenes, or under torrential rain with opaque occlusions. We propose a novel derain framework which applies superpixel (SP) segmentation to decompose the scene into depth consistent units. Alignment of scene contents are done at the SP level, which proves to be robust against rain occlusion interferences and fast camera motion. Two alignment output tensors, i.e., optimal temporal match tensor and sorted spatial-temporal match tensor, provide informative clues for the location of rain streaks and the occluded background contents. Different classical and novel methods such as Robust Principle Component Analysis and Convolutional Neural Networks are applied and compared for their respective advantages in efficiently exploiting the rich spatial-temporal features provided by the two tensors. Extensive evaluations show that advantage of up to 5dB is achieved on the scene restoration PSNR over state-of-the-art methods, and the advantage is especially obvious with highly complex and dynamic scenes. Visual evaluations show that the proposed framework is not only able to suppress heavy and opaque occluding rain streaks, but also large semi-transparent regional fluctuations and distortions.



### Human-level Performance On Automatic Head Biometrics In Fetal Ultrasound Using Fully Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1804.09102v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.09102v1)
- **Published**: 2018-04-24 15:40:59+00:00
- **Updated**: 2018-04-24 15:40:59+00:00
- **Authors**: Matthew Sinclair, Christian F. Baumgartner, Jacqueline Matthew, Wenjia Bai, Juan Cerrolaza Martinez, Yuanwei Li, Sandra Smith, Caroline L. Knight, Bernhard Kainz, Jo Hajnal, Andrew P. King, Daniel Rueckert
- **Comment**: EMBC 2018
- **Journal**: None
- **Summary**: Measurement of head biometrics from fetal ultrasonography images is of key importance in monitoring the healthy development of fetuses. However, the accurate measurement of relevant anatomical structures is subject to large inter-observer variability in the clinic. To address this issue, an automated method utilizing Fully Convolutional Networks (FCN) is proposed to determine measurements of fetal head circumference (HC) and biparietal diameter (BPD). An FCN was trained on approximately 2000 2D ultrasound images of the head with annotations provided by 45 different sonographers during routine screening examinations to perform semantic segmentation of the head. An ellipse is fitted to the resulting segmentation contours to mimic the annotation typically produced by a sonographer. The model's performance was compared with inter-observer variability, where two experts manually annotated 100 test images. Mean absolute model-expert error was slightly better than inter-observer error for HC (1.99mm vs 2.16mm), and comparable for BPD (0.61mm vs 0.59mm), as well as Dice coefficient (0.980 vs 0.980). Our results demonstrate that the model performs at a level similar to a human expert, and learns to produce accurate predictions from a large dataset annotated by many sonographers. Additionally, measurements are generated in near real-time at 15fps on a GPU, which could speed up clinical workflow for both skilled and trainee sonographers.



### Structure Aware SLAM using Quadrics and Planes
- **Arxiv ID**: http://arxiv.org/abs/1804.09111v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1804.09111v3)
- **Published**: 2018-04-24 15:58:57+00:00
- **Updated**: 2018-11-02 07:07:20+00:00
- **Authors**: Mehdi Hosseinzadeh, Yasir Latif, Trung Pham, Niko Suenderhauf, Ian Reid
- **Comment**: Accepted to ACCV 2018
- **Journal**: None
- **Summary**: Simultaneous Localization And Mapping (SLAM) is a fundamental problem in mobile robotics. While point-based SLAM methods provide accurate camera localization, the generated maps lack semantic information. On the other hand, state of the art object detection methods provide rich information about entities present in the scene from a single image. This work marries the two and proposes a method for representing generic objects as quadrics which allows object detections to be seamlessly integrated in a SLAM framework. For scene coverage, additional dominant planar structures are modeled as infinite planes. Experiments show that the proposed points-planes-quadrics representation can easily incorporate Manhattan and object affordance constraints, greatly improving camera localization and leading to semantically meaningful maps. The performance of our SLAM system is demonstrated in https://youtu.be/dR-rB9keF8M .



### Keep it Unreal: Bridging the Realism Gap for 2.5D Recognition with Geometry Priors Only
- **Arxiv ID**: http://arxiv.org/abs/1804.09113v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.09113v2)
- **Published**: 2018-04-24 16:02:59+00:00
- **Updated**: 2018-05-24 16:08:07+00:00
- **Authors**: Sergey Zakharov, Benjamin Planche, Ziyan Wu, Andreas Hutter, Harald Kosch, Slobodan Ilic
- **Comment**: 10 pages + supplemetary material + references. The first two authors
  contributed equally to this work
- **Journal**: None
- **Summary**: With the increasing availability of large databases of 3D CAD models, depth-based recognition methods can be trained on an uncountable number of synthetically rendered images. However, discrepancies with the real data acquired from various depth sensors still noticeably impede progress. Previous works adopted unsupervised approaches to generate more realistic depth data, but they all require real scans for training, even if unlabeled. This still represents a strong requirement, especially when considering real-life/industrial settings where real training images are hard or impossible to acquire, but texture-less 3D models are available. We thus propose a novel approach leveraging only CAD models to bridge the realism gap. Purely trained on synthetic data, playing against an extensive augmentation pipeline in an unsupervised manner, our generative adversarial network learns to effectively segment depth images and recover the clean synthetic-looking depth information even from partial occlusions. As our solution is not only fully decoupled from the real domains but also from the task-specific analytics, the pre-processed scans can be handed to any kind and number of recognition methods also trained on synthetic data. Through various experiments, we demonstrate how this simplifies their training and consistently enhances their performance, with results on par with the same methods trained on real data, and better than usual approaches doing the reverse mapping.



### No Metrics Are Perfect: Adversarial Reward Learning for Visual Storytelling
- **Arxiv ID**: http://arxiv.org/abs/1804.09160v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1804.09160v2)
- **Published**: 2018-04-24 17:41:24+00:00
- **Updated**: 2018-07-09 00:15:14+00:00
- **Authors**: Xin Wang, Wenhu Chen, Yuan-Fang Wang, William Yang Wang
- **Comment**: ACL 2018. 15 pages, 10 figures, 4 tables, with supplementary material
- **Journal**: None
- **Summary**: Though impressive results have been achieved in visual captioning, the task of generating abstract stories from photo streams is still a little-tapped problem. Different from captions, stories have more expressive language styles and contain many imaginary concepts that do not appear in the images. Thus it poses challenges to behavioral cloning algorithms. Furthermore, due to the limitations of automatic metrics on evaluating story quality, reinforcement learning methods with hand-crafted rewards also face difficulties in gaining an overall performance boost. Therefore, we propose an Adversarial REward Learning (AREL) framework to learn an implicit reward function from human demonstrations, and then optimize policy search with the learned reward function. Though automatic eval- uation indicates slight performance boost over state-of-the-art (SOTA) methods in cloning expert behaviors, human evaluation shows that our approach achieves significant improvement in generating more human-like stories than SOTA systems.



### MaskFusion: Real-Time Recognition, Tracking and Reconstruction of Multiple Moving Objects
- **Arxiv ID**: http://arxiv.org/abs/1804.09194v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1804.09194v2)
- **Published**: 2018-04-24 18:15:15+00:00
- **Updated**: 2018-10-22 17:47:27+00:00
- **Authors**: Martin Rünz, Maud Buffier, Lourdes Agapito
- **Comment**: Presented at IEEE International Symposium on Mixed and Augmented
  Reality (ISMAR) 2018
- **Journal**: None
- **Summary**: We present MaskFusion, a real-time, object-aware, semantic and dynamic RGB-D SLAM system that goes beyond traditional systems which output a purely geometric map of a static scene. MaskFusion recognizes, segments and assigns semantic class labels to different objects in the scene, while tracking and reconstructing them even when they move independently from the camera.   As an RGB-D camera scans a cluttered scene, image-based instance-level semantic segmentation creates semantic object masks that enable real-time object recognition and the creation of an object-level representation for the world map. Unlike previous recognition-based SLAM systems, MaskFusion does not require known models of the objects it can recognize, and can deal with multiple independent motions. MaskFusion takes full advantage of using instance-level semantic segmentation to enable semantic labels to be fused into an object-aware map, unlike recent semantics enabled SLAM systems that perform voxel-level semantic segmentation. We show augmented-reality applications that demonstrate the unique features of the map output by MaskFusion: instance-aware, semantic and dynamic.



### Automated Mouse Organ Segmentation: A Deep Learning Based Solution
- **Arxiv ID**: http://arxiv.org/abs/1804.09205v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.09205v2)
- **Published**: 2018-04-24 18:38:01+00:00
- **Updated**: 2018-07-16 19:52:50+00:00
- **Authors**: Naveen Ashish, Mi-Youn Brusniak
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: The analysis of animal cross section images, such as cross sections of laboratory mice, is critical in assessing the effect of experimental drugs such as the biodistribution of candidate compounds in preclinical drug development stage. Tissue distribution of radiolabeled candidate therapeutic compounds can be quantified using techniques like Quantitative Whole-Body Autoradiography (QWBA).QWBA relies, among other aspects, on the accurate segmentation or identification of key organs of interest in the animal cross section image such as the brain, spine, heart, liver and others. We present a deep learning based organ segmentation solution to this problem, using which we can achieve automated organ segmentation with high precision (dice coefficient in the 0.83-0.95 range depending on organ) for the key organs of interest.



### On the effectiveness of task granularity for transfer learning
- **Arxiv ID**: http://arxiv.org/abs/1804.09235v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.09235v2)
- **Published**: 2018-04-24 20:06:55+00:00
- **Updated**: 2018-11-29 03:59:19+00:00
- **Authors**: Farzaneh Mahdisoltani, Guillaume Berger, Waseem Gharbieh, David Fleet, Roland Memisevic
- **Comment**: None
- **Journal**: None
- **Summary**: We describe a DNN for video classification and captioning, trained end-to-end, with shared features, to solve tasks at different levels of granularity, exploring the link between granularity in a source task and the quality of learned features for transfer learning. For solving the new task domain in transfer learning, we freeze the trained encoder and fine-tune a neural net on the target domain. We train on the Something-Something dataset with over 220, 000 videos, and multiple levels of target granularity, including 50 action groups, 174 fine-grained action categories and captions. Classification and captioning with Something-Something are challenging because of the subtle differences between actions, applied to thousands of different object classes, and the diversity of captions penned by crowd actors. Our model performs better than existing classification baselines for SomethingSomething, with impressive fine-grained results. And it yields a strong baseline on the new Something-Something captioning task. Experiments reveal that training with more fine-grained tasks tends to produce better features for transfer learning.



### A Sparse Coding Multi-Scale Precise-Timing Machine Learning Algorithm for Neuromorphic Event-Based Sensors
- **Arxiv ID**: http://arxiv.org/abs/1804.09236v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.09236v1)
- **Published**: 2018-04-24 20:10:18+00:00
- **Updated**: 2018-04-24 20:10:18+00:00
- **Authors**: Germain Haessig, Ryad Benosman
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces an unsupervised compact architecture that can extract features and classify the contents of dynamic scenes from the temporal output of a neuromorphic asynchronous event-based camera. Event-based cameras are clock-less sensors where each pixel asynchronously reports intensity changes encoded in time at the microsecond precision. While this technology is gaining more attention, there is still a lack of methodology and understanding of their temporal properties. This paper introduces an unsupervised time-oriented event-based machine learning algorithm building on the concept of hierarchy of temporal descriptors called time surfaces. In this work we show that the use of sparse coding allows for a very compact yet efficient time-based machine learning that lowers both the computational cost and memory need. We show that we can represent visual scene temporal dynamics with a finite set of elementary time surfaces while providing similar recognition rates as an uncompressed version by storing the most representative time surfaces using clustering techniques. Experiments will illustrate the main optimizations and trade-offs to consider when implementing the method for online continuous vs. offline learning. We report results on the same previously published 36 class character recognition task and a 4 class canonical dynamic card pip task, achieving 100% accuracy on each.



### Real-Time Human Detection as an Edge Service Enabled by a Lightweight CNN
- **Arxiv ID**: http://arxiv.org/abs/1805.00330v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.00330v1)
- **Published**: 2018-04-24 22:02:10+00:00
- **Updated**: 2018-04-24 22:02:10+00:00
- **Authors**: Seyed Yahya Nikouei, Yu Chen, Sejun Song, Ronghua Xu, Baek-Young Choi, Timothy R. Faughnan
- **Comment**: to appear in the IEEE International Conference on Edge Computing
  (IEEE EDGE 2018), San Francisco, CA, USA, July 2, 2018
- **Journal**: None
- **Summary**: Edge computing allows more computing tasks to take place on the decentralized nodes at the edge of networks. Today many delay sensitive, mission-critical applications can leverage these edge devices to reduce the time delay or even to enable real time, online decision making thanks to their onsite presence. Human objects detection, behavior recognition and prediction in smart surveillance fall into that category, where a transition of a huge volume of video streaming data can take valuable time and place heavy pressure on communication networks. It is widely recognized that video processing and object detection are computing intensive and too expensive to be handled by resource limited edge devices. Inspired by the depthwise separable convolution and Single Shot Multi-Box Detector (SSD), a lightweight Convolutional Neural Network (LCNN) is introduced in this paper. By narrowing down the classifier's searching space to focus on human objects in surveillance video frames, the proposed LCNN algorithm is able to detect pedestrians with an affordable computation workload to an edge device. A prototype has been implemented on an edge node (Raspberry PI 3) using openCV libraries, and satisfactory performance is achieved using real world surveillance video streams. The experimental study has validated the design of LCNN and shown it is a promising approach to computing intensive applications at the edge.



### Smart Surveillance as an Edge Network Service: from Harr-Cascade, SVM to a Lightweight CNN
- **Arxiv ID**: http://arxiv.org/abs/1805.00331v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.00331v2)
- **Published**: 2018-04-24 22:09:18+00:00
- **Updated**: 2018-10-01 13:08:19+00:00
- **Authors**: Seyed Yahya Nikouei, Yu Chen, Sejun Song, Ronghua Xu, Baek-Young Choi, Timothy R. Faughnan
- **Comment**: 10-page version, accepted by the 4th IEEE International Conference on
  Collaboration and Internet Computing (IEEE CIC 2018), Oct 18 - 20, 2018.
  Philadelphia, Pennsylvania, USA
- **Journal**: None
- **Summary**: Edge computing efficiently extends the realm of information technology beyond the boundary defined by cloud computing paradigm. Performing computation near the source and destination, edge computing is promising to address the challenges in many delay-sensitive applications, like real-time human surveillance. Leveraging the ubiquitously connected cameras and smart mobile devices, it enables video analytics at the edge. In recent years, many smart video surveillance approaches are proposed for object detection and tracking by using Artificial Intelligence (AI) and Machine Learning (ML) algorithms. This work explores the feasibility of two popular human-objects detection schemes, Harr-Cascade and HOG feature extraction and SVM classifier, at the edge and introduces a lightweight Convolutional Neural Network (L-CNN) leveraging the depthwise separable convolution for less computation, for human detection. Single Board computers (SBC) are used as edge devices for tests and algorithms are validated using real-world campus surveillance video streams and open data sets. The experimental results are promising that the final algorithm is able to track humans with a decent accuracy at a resource consumption affordable by edge devices in real-time manner.



### Segmentation-Free Approaches for Handwritten Numeral String Recognition
- **Arxiv ID**: http://arxiv.org/abs/1804.09279v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.09279v3)
- **Published**: 2018-04-24 22:15:11+00:00
- **Updated**: 2018-04-28 03:06:52+00:00
- **Authors**: Andre G Hochuli, Luiz E S Oliveira, Alceu S Britto Jr, Robert Sabourin
- **Comment**: Paper accepted for publication on IJCNN 2018
- **Journal**: None
- **Summary**: This paper presents segmentation-free strategies for the recognition of handwritten numeral strings of unknown length. A synthetic dataset of touching numeral strings of sizes 2-, 3- and 4-digits was created to train end-to-end solutions based on Convolutional Neural Networks. A robust experimental protocol is used to show that the proposed segmentation-free methods may reach the state-of-the-art performance without suffering the heavy burden of over-segmentation based methods. In addition, they confirmed the importance of introducing contextual information in the design of end-to-end solutions, such as the proposed length classifier when recognizing numeral strings.



