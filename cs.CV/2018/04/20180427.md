# Arxiv Papers in cs.CV on 2018-04-27
### Adversarial Training of Variational Auto-encoders for High Fidelity Image Generation
- **Arxiv ID**: http://arxiv.org/abs/1804.10323v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.10323v1)
- **Published**: 2018-04-27 02:19:35+00:00
- **Updated**: 2018-04-27 02:19:35+00:00
- **Authors**: Salman H. Khan, Munawar Hayat, Nick Barnes
- **Comment**: None
- **Journal**: None
- **Summary**: Variational auto-encoders (VAEs) provide an attractive solution to image generation problem. However, they tend to produce blurred and over-smoothed images due to their dependence on pixel-wise reconstruction loss. This paper introduces a new approach to alleviate this problem in the VAE based generative models. Our model simultaneously learns to match the data, reconstruction loss and the latent distributions of real and fake images to improve the quality of generated samples. To compute the loss distributions, we introduce an auto-encoder based discriminator model which allows an adversarial learning procedure. The discriminator in our model also provides perceptual guidance to the VAE by matching the learned similarity metric of the real and fake samples in the latent space. To stabilize the overall training process, our model uses an error feedback approach to maintain the equilibrium between competing networks in the model. Our experiments show that the generated samples from our proposed model exhibit a diverse set of attributes and facial expressions and scale up to high-resolution images very well.



### Deep learning approach to Fourier ptychographic microscopy
- **Arxiv ID**: http://arxiv.org/abs/1805.00334v3
- **DOI**: 10.1364/OE.26.026470
- **Categories**: **cs.CV**, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/1805.00334v3)
- **Published**: 2018-04-27 02:53:25+00:00
- **Updated**: 2018-07-30 22:59:06+00:00
- **Authors**: Thanh Nguyen, Yujia Xue, Yunzhe Li, Lei Tian, George Nehmetallah
- **Comment**: None
- **Journal**: Opt. Express 26, 26470-26484 (2018)
- **Summary**: Convolutional neural networks (CNNs) have gained tremendous success in solving complex inverse problems. The aim of this work is to develop a novel CNN framework to reconstruct video sequence of dynamic live cells captured using a computational microscopy technique, Fourier ptychographic microscopy (FPM). The unique feature of the FPM is its capability to reconstruct images with both wide field-of-view (FOV) and high resolution, i.e. a large space-bandwidth-product (SBP), by taking a series of low resolution intensity images. For live cell imaging, a single FPM frame contains thousands of cell samples with different morphological features. Our idea is to fully exploit the statistical information provided by this large spatial ensemble so as to make predictions in a sequential measurement, without using any additional temporal dataset. Specifically, we show that it is possible to reconstruct high-SBP dynamic cell videos by a CNN trained only on the first FPM dataset captured at the beginning of a time-series experiment. Our CNN approach reconstructs a 12800X10800 pixels phase image using only ~25 seconds, a 50X speedup compared to the model-based FPM algorithm. In addition, the CNN further reduces the required number of images in each time frame by ~6X. Overall, this significantly improves the imaging throughput by reducing both the acquisition and computational times. The proposed CNN is based on the conditional generative adversarial network (cGAN) framework. Additionally, we also exploit transfer learning so that our pre-trained CNN can be further optimized to image other cell types. Our technique demonstrates a promising deep learning approach to continuously monitor large live-cell populations over an extended time and gather useful spatial and temporal information with sub-cellular resolution.



### Latent Fingerprint Recognition: Role of Texture Template
- **Arxiv ID**: http://arxiv.org/abs/1804.10337v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.10337v1)
- **Published**: 2018-04-27 04:26:03+00:00
- **Updated**: 2018-04-27 04:26:03+00:00
- **Authors**: Kai Cao, Anil K. Jain
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a texture template approach, consisting of a set of virtual minutiae, to improve the overall latent fingerprint recognition accuracy. To compensate for the lack of sufficient number of minutiae in poor quality latent prints, we generate a set of virtual minutiae. However, due to a large number of these regularly placed virtual minutiae, texture based template matching has a large computational requirement compared to matching true minutiae templates. To improve both the accuracy and efficiency of the texture template matching, we investigate: i) both original and enhanced fingerprint patches for training convolutional neural networks (ConvNets) to improve the distinctiveness of descriptors associated with each virtual minutiae, ii) smaller patches around virtual minutiae and a fast ConvNet architecture to speed up descriptor extraction, iii) reduce the descriptor length, iv) a modified hierarchical graph matching strategy to improve the matching speed, and v) extraction of multiple texture templates to boost the performance. Experiments on NIST SD27 latent database show that the above strategies can improve the matching speed from 11 ms (24 threads) per comparison (between a latent and a reference print) to only 7.7 ms (single thread) per comparison while improving the rank-1 accuracy by 8.9% against 10K gallery.



### Stacked U-Nets: A No-Frills Approach to Natural Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1804.10343v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1804.10343v1)
- **Published**: 2018-04-27 05:03:32+00:00
- **Updated**: 2018-04-27 05:03:32+00:00
- **Authors**: Sohil Shah, Pallabi Ghosh, Larry S Davis, Tom Goldstein
- **Comment**: The code is available at https://github.com/shahsohil/sunets
- **Journal**: None
- **Summary**: Many imaging tasks require global information about all pixels in an image. Conventional bottom-up classification networks globalize information by decreasing resolution; features are pooled and downsampled into a single output. But for semantic segmentation and object detection tasks, a network must provide higher-resolution pixel-level outputs. To globalize information while preserving resolution, many researchers propose the inclusion of sophisticated auxiliary blocks, but these come at the cost of a considerable increase in network size and computational cost. This paper proposes stacked u-nets (SUNets), which iteratively combine features from different resolution scales while maintaining resolution. SUNets leverage the information globalization power of u-nets in a deeper network architectures that is capable of handling the complexity of natural images. SUNets perform extremely well on semantic segmentation tasks using a small number of parameters.



### An Element Sensitive Saliency Model with Position Prior Learning for Web Pages
- **Arxiv ID**: http://arxiv.org/abs/1804.10361v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.10361v2)
- **Published**: 2018-04-27 07:04:11+00:00
- **Updated**: 2018-11-03 09:31:22+00:00
- **Authors**: Yujun Gu, Jie Chang, Ya Zhang, Yanfeng Wang
- **Comment**: Submitted to ICIAI-2019
- **Journal**: None
- **Summary**: Understanding human visual attention is important for multimedia applications. Many studies have attempted to learn from eye-tracking data and build computational saliency prediction models. However, limited efforts have been devoted to saliency prediction for Web pages, which are characterized by more diverse content elements and spatial layouts. In this paper, we propose a novel end-to-end deep generative saliency model for Web pages. To capture position biases introduced by page layouts, a Position Prior Learning sub-network is proposed, which models position biases as multivariate Gaussian distribution using variational auto-encoder. To model different elements of a Web page, a Multi Discriminative Region Detection (MDRD) branch and a Text Region Detection(TRD) branch are introduced, which target to extract discriminative localizations and "prominent" text regions likely to correspond to human attention, respectively. We validate the proposed model with FiWI, a public Web-page dataset, and shows that the proposed model outperforms the state-of-art models for Web-page saliency prediction.



### Online Convolutional Sparse Coding with Sample-Dependent Dictionary
- **Arxiv ID**: http://arxiv.org/abs/1804.10366v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.10366v2)
- **Published**: 2018-04-27 07:21:16+00:00
- **Updated**: 2018-06-07 08:24:16+00:00
- **Authors**: Yaqing Wang, Quanming Yao, James T. Kwok, Lionel M. Ni
- **Comment**: Accepted by ICML-2018
- **Journal**: None
- **Summary**: Convolutional sparse coding (CSC) has been popularly used for the learning of shift-invariant dictionaries in image and signal processing. However, existing methods have limited scalability. In this paper, instead of convolving with a dictionary shared by all samples, we propose the use of a sample-dependent dictionary in which filters are obtained as linear combinations of a small set of base filters learned from the data. This added flexibility allows a large number of sample-dependent patterns to be captured, while the resultant model can still be efficiently learned by online learning. Extensive experimental results show that the proposed method outperforms existing CSC algorithms with significantly reduced time and space requirements.



### dhSegment: A generic deep-learning approach for document segmentation
- **Arxiv ID**: http://arxiv.org/abs/1804.10371v2
- **DOI**: 10.1109/ICFHR-2018.2018.00011
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.10371v2)
- **Published**: 2018-04-27 07:53:53+00:00
- **Updated**: 2019-08-14 07:43:04+00:00
- **Authors**: Sofia Ares Oliveira, Benoit Seguin, Frederic Kaplan
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years there have been multiple successful attempts tackling document processing problems separately by designing task specific hand-tuned strategies. We argue that the diversity of historical document processing tasks prohibits to solve them one at a time and shows a need for designing generic approaches in order to handle the variability of historical series. In this paper, we address multiple tasks simultaneously such as page extraction, baseline extraction, layout analysis or multiple typologies of illustrations and photograph extraction. We propose an open-source implementation of a CNN-based pixel-wise predictor coupled with task dependent post-processing blocks. We show that a single CNN-architecture can be used across tasks with competitive results. Moreover most of the task-specific post-precessing steps can be decomposed in a small number of simple and standard reusable operations, adding to the flexibility of our approach.



### Automatic classification of trees using a UAV onboard camera and deep learning
- **Arxiv ID**: http://arxiv.org/abs/1804.10390v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1804.10390v1)
- **Published**: 2018-04-27 08:38:22+00:00
- **Updated**: 2018-04-27 08:38:22+00:00
- **Authors**: Masanori Onishi, Takeshi Ise
- **Comment**: 9 pages, 3 figures, 5 tables
- **Journal**: None
- **Summary**: Automatic classification of trees using remotely sensed data has been a dream of many scientists and land use managers. Recently, Unmanned aerial vehicles (UAV) has been expected to be an easy-to-use, cost-effective tool for remote sensing of forests, and deep learning has attracted attention for its ability concerning machine vision. In this study, using a commercially available UAV and a publicly available package for deep learning, we constructed a machine vision system for the automatic classification of trees. In our method, we segmented a UAV photography image of forest into individual tree crowns and carried out object-based deep learning. As a result, the system was able to classify 7 tree types at 89.0% accuracy. This performance is notable because we only used basic RGB images from a standard UAV. In contrast, most of previous studies used expensive hardware such as multispectral imagers to improve the performance. This result means that our method has the potential to classify individual trees in a cost-effective manner. This can be a usable tool for many forest researchers and managements.



### Open Set Domain Adaptation by Backpropagation
- **Arxiv ID**: http://arxiv.org/abs/1804.10427v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.10427v2)
- **Published**: 2018-04-27 10:16:15+00:00
- **Updated**: 2018-07-06 16:19:51+00:00
- **Authors**: Kuniaki Saito, Shohei Yamamoto, Yoshitaka Ushiku, Tatsuya Harada
- **Comment**: Accepted by ECCV2018
- **Journal**: None
- **Summary**: Numerous algorithms have been proposed for transferring knowledge from a label-rich domain (source) to a label-scarce domain (target). Almost all of them are proposed for a closed-set scenario, where the source and the target domain completely share the class of their samples. We call the shared class the \doublequote{known class.} However, in practice, when samples in target domain are not labeled, we cannot know whether the domains share the class. A target domain can contain samples of classes that are not shared by the source domain. We call such classes the \doublequote{unknown class} and algorithms that work well in the open set situation are very practical. However, most existing distribution matching methods for domain adaptation do not work well in this setting because unknown target samples should not be aligned with the source.   In this paper, we propose a method for an open set domain adaptation scenario which utilizes adversarial training. A classifier is trained to make a boundary between the source and the target samples whereas a generator is trained to make target samples far from the boundary. Thus, we assign two options to the feature generator: aligning them with source known samples or rejecting them as unknown target samples. This approach allows extracting features that separate unknown target samples from known target samples. Our method was extensively evaluated in domain adaptation setting and outperformed other methods with a large margin in most settings.



### Localized Traffic Sign Detection with Multi-scale Deconvolution Networks
- **Arxiv ID**: http://arxiv.org/abs/1804.10428v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.10428v2)
- **Published**: 2018-04-27 10:20:44+00:00
- **Updated**: 2018-05-03 10:21:49+00:00
- **Authors**: Songwen Pei, Fuwu Tang, Yanfei Ji, Jing Fan, Zhong Ning
- **Comment**: None
- **Journal**: None
- **Summary**: Autonomous driving is becoming a future practical lifestyle greatly driven by deep learning. Specifically, an effective traffic sign detection by deep learning plays a critical role for it. However, different countries have different sets of traffic signs, making localized traffic sign recognition model training a tedious and daunting task. To address the issues of taking amount of time to compute complicate algorithm and low ratio of detecting blurred and sub-pixel images of localized traffic signs, we propose Multi-Scale Deconvolution Networks (MDN), which flexibly combines multi-scale convolutional neural network with deconvolution sub-network, leading to efficient and reliable localized traffic sign recognition model training. It is demonstrated that the proposed MDN is effective compared with classical algorithms on the benchmarks of the localized traffic sign, such as Chinese Traffic Sign Dataset (CTSD), and the German Traffic Sign Benchmarks (GTSRB).



### Variational Regularization of Inverse Problems for Manifold-Valued Data
- **Arxiv ID**: http://arxiv.org/abs/1804.10432v1
- **DOI**: None
- **Categories**: **math.NA**, cs.CV, math.DG, 94A08, 68U10, 90C90, 53B99, 65K10
- **Links**: [PDF](http://arxiv.org/pdf/1804.10432v1)
- **Published**: 2018-04-27 10:40:51+00:00
- **Updated**: 2018-04-27 10:40:51+00:00
- **Authors**: Martin Storath, Andreas Weinmann
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we consider the variational regularization of manifold-valued data in the inverse problems setting. In particular, we consider TV and TGV regularization for manifold-valued data with indirect measurement operators. We provide results on the well-posedness and present algorithms for a numerical realization of these models in the manifold setup. Further, we provide experimental results for synthetic and real data to show the potential of the proposed schemes for applications.



### Customized Image Narrative Generation via Interactive Visual Question Generation and Answering
- **Arxiv ID**: http://arxiv.org/abs/1805.00460v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1805.00460v1)
- **Published**: 2018-04-27 11:27:45+00:00
- **Updated**: 2018-04-27 11:27:45+00:00
- **Authors**: Andrew Shin, Yoshitaka Ushiku, Tatsuya Harada
- **Comment**: To Appear at CVPR 2018 as spotlight presentation
- **Journal**: None
- **Summary**: Image description task has been invariably examined in a static manner with qualitative presumptions held to be universally applicable, regardless of the scope or target of the description. In practice, however, different viewers may pay attention to different aspects of the image, and yield different descriptions or interpretations under various contexts. Such diversity in perspectives is difficult to derive with conventional image description techniques. In this paper, we propose a customized image narrative generation task, in which the users are interactively engaged in the generation process by providing answers to the questions. We further attempt to learn the user's interest via repeating such interactive stages, and to automatically reflect the interest in descriptions for new images. Experimental results demonstrate that our model can generate a variety of descriptions from single image that cover a wider range of topics than conventional models, while being customizable to the target user of interaction.



### Bound and Conquer: Improving Triangulation by Enforcing Consistency
- **Arxiv ID**: http://arxiv.org/abs/1804.10448v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.10448v1)
- **Published**: 2018-04-27 11:40:23+00:00
- **Updated**: 2018-04-27 11:40:23+00:00
- **Authors**: Adam Scholefield, Alireza Ghasemi, Martin Vetterli
- **Comment**: 8 pages, 4 figures, Submitted to IEEE Transactions on Pattern
  Analysis and Machine Intelligence
- **Journal**: None
- **Summary**: We study the accuracy of triangulation in multi-camera systems with respect to the number of cameras. We show that, under certain conditions, the optimal achievable reconstruction error decays quadratically as more cameras are added to the system. Furthermore, we analyse the error decay-rate of major state-of-the-art algorithms with respect to the number of cameras. To this end, we introduce the notion of consistency for triangulation, and show that consistent reconstruction algorithms achieve the optimal quadratic decay, which is asymptotically faster than some other methods. Finally, we present simulations results supporting our findings. Our simulations have been implemented in MATLAB and the resulting code is available in the supplementary material.



### A generalizable approach for multi-view 3D human pose regression
- **Arxiv ID**: http://arxiv.org/abs/1804.10462v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.10462v2)
- **Published**: 2018-04-27 12:14:40+00:00
- **Updated**: 2019-10-08 15:15:51+00:00
- **Authors**: Abdolrahim Kadkhodamohammadi, Nicolas Padoy
- **Comment**: The supplementary video is available at https://youtu.be/Cx_kTRzqqzA
- **Journal**: None
- **Summary**: Despite the significant improvement in the performance of monocular pose estimation approaches and their ability to generalize to unseen environments, multi-view (MV) approaches are often lagging behind in terms of accuracy and are specific to certain datasets. This is mainly due to the fact that (1) contrary to real world single-view (SV) datasets, MV datasets are often captured in controlled environments to collect precise 3D annotations, which do not cover all real world challenges, and (2) the model parameters are learned for specific camera setups. To alleviate these problems, we propose a two-stage approach to detect and estimate 3D human poses, which separates SV pose detection from MV 3D pose estimation. This separation enables us to utilize each dataset for the right task, i.e. SV datasets for constructing robust pose detection models and MV datasets for constructing precise MV 3D regression models. In addition, our 3D regression approach only requires 3D pose data and its projections to the views for building the model, hence removing the need for collecting annotated data from the test setup. Our approach can therefore be easily generalized to a new environment by simply projecting 3D poses into 2D during training according to the camera setup used at test time. As 2D poses are collected at test time using a SV pose detector, which might generate inaccurate detections, we model its characteristics and incorporate this information during training. We demonstrate that incorporating the detector's characteristics is important to build a robust 3D regression model and that the resulting regression model generalizes well to new MV environments. Our evaluation results show that our approach achieves competitive results on the Human3.6M dataset and significantly improves results on a MV clinical dataset that is the first MV dataset generated from live surgery recordings.



### Disentangling Factors of Variation with Cycle-Consistent Variational Auto-Encoders
- **Arxiv ID**: http://arxiv.org/abs/1804.10469v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.10469v1)
- **Published**: 2018-04-27 12:37:35+00:00
- **Updated**: 2018-04-27 12:37:35+00:00
- **Authors**: Ananya Harsh Jha, Saket Anand, Maneesh Singh, V. S. R. Veeravasarapu
- **Comment**: None
- **Journal**: None
- **Summary**: Generative models that learn disentangled representations for different factors of variation in an image can be very useful for targeted data augmentation. By sampling from the disentangled latent subspace of interest, we can efficiently generate new data necessary for a particular task. Learning disentangled representations is a challenging problem, especially when certain factors of variation are difficult to label. In this paper, we introduce a novel architecture that disentangles the latent space into two complementary subspaces by using only weak supervision in form of pairwise similarity labels. Inspired by the recent success of cycle-consistent adversarial architectures, we use cycle-consistency in a variational auto-encoder framework. Our non-adversarial approach is in contrast with the recent works that combine adversarial training with auto-encoders to disentangle representations. We show compelling results of disentangled latent subspaces on three datasets and compare with recent works that leverage adversarial training.



### Interactive Medical Image Segmentation via Point-Based Interaction and Sequential Patch Learning
- **Arxiv ID**: http://arxiv.org/abs/1804.10481v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.10481v2)
- **Published**: 2018-04-27 13:03:42+00:00
- **Updated**: 2018-05-09 02:51:33+00:00
- **Authors**: Jinquan Sun, Yinghuan Shi, Yang Gao, Lei Wang, Luping Zhou, Wanqi Yang, Dinggang Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Due to low tissue contrast, irregular object appearance, and unpredictable location variation, segmenting the objects from different medical imaging modalities (e.g., CT, MR) is considered as an important yet challenging task. In this paper, we present a novel method for interactive medical image segmentation with the following merits. (1) Our design is fundamentally different from previous pure patch-based and image-based segmentation methods. We observe that during delineation, the physician repeatedly check the inside-outside intensity changing to determine the boundary, which indicates that comparison in an inside-outside manner is extremely important. Thus, we innovatively model our segmentation task as learning the representation of the bi-directional sequential patches, starting from (or ending in) the given central point of the object. This can be realized by our proposed ConvRNN network embedded with a gated memory propagation unit. (2) Unlike previous interactive methods (requiring bounding box or seed points), we only ask the physician to merely click on the rough central point of the object before segmentation, which could simultaneously enhance the performance and reduce the segmentation time. (3) We utilize our method in a multi-level framework for better performance. We systematically evaluate our method in three different segmentation tasks including CT kidney tumor, MR prostate, and PROMISE12 challenge, showing promising results compared with state-of-the-art methods. The code is available here: \href{https://github.com/sunalbert/Sequential-patch-based-segmentation}{Sequential-patch-based-segmentation}.



### Crossbar-Net: A Novel Convolutional Network for Kidney Tumor Segmentation in CT Images
- **Arxiv ID**: http://arxiv.org/abs/1804.10484v4
- **DOI**: 10.1109/TIP.2019.2905537
- **Categories**: **cs.CV**, 68T45, 68T05, I.2.10; I.4
- **Links**: [PDF](http://arxiv.org/pdf/1804.10484v4)
- **Published**: 2018-04-27 13:09:27+00:00
- **Updated**: 2020-04-03 01:50:39+00:00
- **Authors**: Qian Yu, Yinghuan Shi, Jinquan Sun, Yang Gao, Yakang Dai, Jianbing Zhu
- **Comment**: This paper includes 15 pages and 16 figures
- **Journal**: None
- **Summary**: Due to the irregular motion, similar appearance and diverse shape, accurate segmentation of kidney tumor in CT images is a difficult and challenging task. To this end, we present a novel automatic segmentation method, termed as Crossbar-Net, with the goal of accurate segmenting the kidney tumors. Firstly, considering that the traditional learning-based segmentation methods normally employ either whole images or squared patches as the training samples, we innovatively sample the orthogonal non-squared patches (namely crossbar patches), to fully cover the whole kidney tumors in either horizontal or vertical directions. These sampled crossbar patches could not only represent the detailed local information of kidney tumor as the traditional patches, but also describe the global appearance from either horizontal or vertical direction using contextual information. Secondly, with the obtained crossbar patches, we trained a convolutional neural network with two sub-models (i.e., horizontal sub-model and vertical sub-model) in a cascaded manner, to integrate the segmentation results from two directions (i.e., horizontal and vertical). This cascaded training strategy could effectively guarantee the consistency between sub-models, by feeding each other with the most difficult samples, for a better segmentation. In the experiment, we evaluate our method on a real CT kidney tumor dataset, collected from 94 different patients including 3,500 images. Compared with the state-of-the-art segmentation methods, the results demonstrate the superior results of our method on dice ratio score, true positive fraction, centroid distance and Hausdorff distance. Moreover, we have extended our crossbar-net to a different task: cardiac segmentation, showing the promising results for the better generalization.



### A matrix-free approach to parallel and memory-efficient deformable image registration
- **Arxiv ID**: http://arxiv.org/abs/1804.10541v1
- **DOI**: None
- **Categories**: **cs.CV**, 92C55, 65K10, 65Y05
- **Links**: [PDF](http://arxiv.org/pdf/1804.10541v1)
- **Published**: 2018-04-27 14:52:41+00:00
- **Updated**: 2018-04-27 14:52:41+00:00
- **Authors**: Lars König, Jan Rühaak, Alexander Derksen, Jan Lellmann
- **Comment**: Accepted for publication in SIAM Journal on Scientific Computing
  (SISC)
- **Journal**: None
- **Summary**: We present a novel computational approach to fast and memory-efficient deformable image registration. In the variational registration model, the computation of the objective function derivatives is the computationally most expensive operation, both in terms of runtime and memory requirements. In order to target this bottleneck, we analyze the matrix structure of gradient and Hessian computations for the case of the normalized gradient fields distance measure and curvature regularization. Based on this analysis, we derive equivalent matrix-free closed-form expressions for derivative computations, eliminating the need for storing intermediate results and the costs of sparse matrix arithmetic. This has further benefits: (1) matrix computations can be fully parallelized, (2) memory complexity for derivative computation is reduced from linear to constant, and (3) overall computation times are substantially reduced.   In comparison with an optimized matrix-based reference implementation, the CPU implementation achieves speedup factors between 3.1 and 9.7, and we are able to handle substantially higher resolutions. Using a GPU implementation, we achieve an additional speedup factor of up to 9.2.   Furthermore, we evaluated the approach on real-world medical datasets. On ten publicly available lung CT images from the DIR-Lab 4DCT dataset, we achieve the best mean landmark error of 0.93 mm compared to other submissions on the DIR-Lab website, with an average runtime of only 9.23 s. Complete non-rigid registration of full-size 3D thorax-abdomen CT volumes from oncological follow-up is achieved in 12.6 s. The experimental results show that the proposed matrix-free algorithm enables the use of variational registration models also in applications which were previously impractical due to memory or runtime restrictions.



### Human Motion Modeling using DVGANs
- **Arxiv ID**: http://arxiv.org/abs/1804.10652v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.10652v2)
- **Published**: 2018-04-27 19:13:34+00:00
- **Updated**: 2018-05-18 16:52:24+00:00
- **Authors**: Xiao Lin, Mohamed R. Amer
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel generative model for human motion modeling using Generative Adversarial Networks (GANs). We formulate the GAN discriminator using dense validation at each time-scale and perturb the discriminator input to make it translation invariant. Our model is capable of motion generation and completion. We show through our evaluations the resiliency to noise, generalization over actions, and generation of long diverse sequences. We evaluate our approach on Human 3.6M and CMU motion capture datasets using inception scores.



### Large-Scale Visual Relationship Understanding
- **Arxiv ID**: http://arxiv.org/abs/1804.10660v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.10660v4)
- **Published**: 2018-04-27 19:41:39+00:00
- **Updated**: 2019-08-16 20:58:18+00:00
- **Authors**: Ji Zhang, Yannis Kalantidis, Marcus Rohrbach, Manohar Paluri, Ahmed Elgammal, Mohamed Elhoseiny
- **Comment**: None
- **Journal**: None
- **Summary**: Large scale visual understanding is challenging, as it requires a model to handle the widely-spread and imbalanced distribution of <subject, relation, object> triples. In real-world scenarios with large numbers of objects and relations, some are seen very commonly while others are barely seen. We develop a new relationship detection model that embeds objects and relations into two vector spaces where both discriminative capability and semantic affinity are preserved. We learn both a visual and a semantic module that map features from the two modalities into a shared space, where matched pairs of features have to discriminate against those unmatched, but also maintain close distances to semantically similar ones. Benefiting from that, our model can achieve superior performance even when the visual entity categories scale up to more than 80,000, with extremely skewed class distribution. We demonstrate the efficacy of our model on a large and imbalanced benchmark based of Visual Genome that comprises 53,000+ objects and 29,000+ relations, a scale at which no previous work has ever been evaluated at. We show superiority of our model over carefully designed baselines on the original Visual Genome dataset with 80,000+ categories. We also show state-of-the-art performance on the VRD dataset and the scene graph dataset which is a subset of Visual Genome with 200 categories.



### Joint Shape Representation and Classification for Detecting PDAC
- **Arxiv ID**: http://arxiv.org/abs/1804.10684v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.10684v2)
- **Published**: 2018-04-27 20:38:36+00:00
- **Updated**: 2019-08-21 01:26:35+00:00
- **Authors**: Fengze Liu, Lingxi Xie, Yingda Xia, Elliot K. Fishman, Alan L. Yuille
- **Comment**: Accepted to MICCAI 2019 Workshop(MLMI)(8 pages, 3 figures)
- **Journal**: None
- **Summary**: We aim to detect pancreatic ductal adenocarcinoma (PDAC) in abdominal CT scans, which sheds light on early diagnosis of pancreatic cancer. This is a 3D volume classification task with little training data. We propose a two-stage framework, which first segments the pancreas into a binary mask, then compresses the mask into a shape vector and performs abnormality classification. Shape representation and classification are performed in a joint manner, both to exploit the knowledge that PDAC often changes the shape of the pancreas and to prevent over-fitting. Experiments are performed on 300 normal scans and 136 PDAC cases. We achieve a specificity of 90.2% (false alarm occurs on less than 1/10 normal cases) at a sensitivity of 80.2% (less than 1/5 PDAC cases are not detected), which show promise for clinical applications.



### Extracting textual overlays from social media videos using neural networks
- **Arxiv ID**: http://arxiv.org/abs/1804.10687v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.10687v2)
- **Published**: 2018-04-27 21:09:20+00:00
- **Updated**: 2018-05-01 17:35:47+00:00
- **Authors**: Adam Słucki, Tomasz Trzcinski, Adam Bielski, Paweł Cyrta
- **Comment**: International Conference on Computer Vision and Graphics (ICCVG) 2018
- **Journal**: None
- **Summary**: Textual overlays are often used in social media videos as people who watch them without the sound would otherwise miss essential information conveyed in the audio stream. This is why extraction of those overlays can serve as an important meta-data source, e.g. for content classification or retrieval tasks. In this work, we present a robust method for extracting textual overlays from videos that builds up on multiple neural network architectures. The proposed solution relies on several processing steps: keyframe extraction, text detection and text recognition. The main component of our system, i.e. the text recognition module, is inspired by a convolutional recurrent neural network architecture and we improve its performance using synthetically generated dataset of over 600,000 images with text prepared by authors specifically for this task. We also develop a filtering method that reduces the amount of overlapping text phrases using Levenshtein distance and further boosts system's performance. The final accuracy of our solution reaches over 80A% and is au pair with state-of-the-art methods.



### Reward Learning from Narrated Demonstrations
- **Arxiv ID**: http://arxiv.org/abs/1804.10692v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1804.10692v1)
- **Published**: 2018-04-27 21:26:08+00:00
- **Updated**: 2018-04-27 21:26:08+00:00
- **Authors**: Hsiao-Yu Fish Tung, Adam W. Harley, Liang-Kang Huang, Katerina Fragkiadaki
- **Comment**: The work has been accepted to Conference on Computer Vision and
  Pattern Recognition (CVPR) 2018
- **Journal**: None
- **Summary**: Humans effortlessly "program" one another by communicating goals and desires in natural language. In contrast, humans program robotic behaviours by indicating desired object locations and poses to be achieved, by providing RGB images of goal configurations, or supplying a demonstration to be imitated. None of these methods generalize across environment variations, and they convey the goal in awkward technical terms. This work proposes joint learning of natural language grounding and instructable behavioural policies reinforced by perceptual detectors of natural language expressions, grounded to the sensory inputs of the robotic agent. Our supervision is narrated visual demonstrations(NVD), which are visual demonstrations paired with verbal narration (as opposed to being silent). We introduce a dataset of NVD where teachers perform activities while describing them in detail. We map the teachers' descriptions to perceptual reward detectors, and use them to train corresponding behavioural policies in simulation.We empirically show that our instructable agents (i) learn visual reward detectors using a small number of examples by exploiting hard negative mined configurations from demonstration dynamics, (ii) develop pick-and place policies using learned visual reward detectors, (iii) benefit from object-factorized state representations that mimic the syntactic structure of natural language goal expressions, and (iv) can execute behaviours that involve novel objects in novel locations at test time, instructed by natural language.



### Extracting Lungs from CT Images using Fully Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1804.10704v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.10704v1)
- **Published**: 2018-04-27 22:27:06+00:00
- **Updated**: 2018-04-27 22:27:06+00:00
- **Authors**: Jeovane Honório Alves, Pedro Martins Moreira Neto, Lucas Ferrari de Oliveira
- **Comment**: Accepted for presentation at the International Joint Conference on
  Neural Networks (IJCNN) 2018
- **Journal**: None
- **Summary**: Analysis of cancer and other pathological diseases, like the interstitial lung diseases (ILDs), is usually possible through Computed Tomography (CT) scans. To aid this, a preprocessing step of segmentation is performed to reduce the area to be analyzed, segmenting the lungs and removing unimportant regions. Generally, complex methods are developed to extract the lung region, also using hand-made feature extractors to enhance segmentation. With the popularity of deep learning techniques and its automated feature learning, we propose a lung segmentation approach using fully convolutional networks (FCNs) combined with fully connected conditional random fields (CRF), employed in many state-of-the-art segmentation works. Aiming to develop a generalized approach, the publicly available datasets from University Hospitals of Geneva (HUG) and VESSEL12 challenge were studied, including many healthy and pathological CT scans for evaluation. Experiments using the dataset individually, its trained model on the other dataset and a combination of both datasets were employed. Dice scores of $98.67\%\pm0.94\%$ for the HUG-ILD dataset and $99.19\%\pm0.37\%$ for the VESSEL12 dataset were achieved, outperforming works in the former and obtaining similar state-of-the-art results in the latter dataset, showing the capability in using deep learning approaches.



