# Arxiv Papers in cs.CV on 2018-04-03
### Improved Fusion of Visual and Language Representations by Dense Symmetric Co-Attention for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1804.00775v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.00775v2)
- **Published**: 2018-04-03 01:24:23+00:00
- **Updated**: 2018-12-01 08:12:22+00:00
- **Authors**: Duy-Kien Nguyen, Takayuki Okatani
- **Comment**: In Proceeding of CVPR'2018
- **Journal**: None
- **Summary**: A key solution to visual question answering (VQA) exists in how to fuse visual and language features extracted from an input image and question. We show that an attention mechanism that enables dense, bi-directional interactions between the two modalities contributes to boost accuracy of prediction of answers. Specifically, we present a simple architecture that is fully symmetric between visual and language representations, in which each question word attends on image regions and each image region attends on question words. It can be stacked to form a hierarchy for multi-step interactions between an image-question pair. We show through experiments that the proposed architecture achieves a new state-of-the-art on VQA and VQA 2.0 despite its small size. We also present qualitative evaluation, demonstrating how the proposed attention mechanism can generate reasonable attention maps on images and questions, which leads to the correct answer prediction.



### 3D Interpreter Networks for Viewer-Centered Wireframe Modeling
- **Arxiv ID**: http://arxiv.org/abs/1804.00782v2
- **DOI**: 10.1007/s11263-018-1074-6
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1804.00782v2)
- **Published**: 2018-04-03 01:55:31+00:00
- **Updated**: 2019-08-09 23:14:56+00:00
- **Authors**: Jiajun Wu, Tianfan Xue, Joseph J. Lim, Yuandong Tian, Joshua B. Tenenbaum, Antonio Torralba, William T. Freeman
- **Comment**: Journal preprint of arXiv:1604.08685 (IJCV, 2018). The first two
  authors contributed equally to this work. Project page:
  http://3dinterpreter.csail.mit.edu
- **Journal**: International Journal of Computer Vision, Volume 126, Issue 9, pp
  1009-1026, 2018
- **Summary**: Understanding 3D object structure from a single image is an important but challenging task in computer vision, mostly due to the lack of 3D object annotations to real images. Previous research tackled this problem by either searching for a 3D shape that best explains 2D annotations, or training purely on synthetic data with ground truth 3D information. In this work, we propose 3D INterpreter Networks (3D-INN), an end-to-end trainable framework that sequentially estimates 2D keypoint heatmaps and 3D object skeletons and poses. Our system learns from both 2D-annotated real images and synthetic 3D data. This is made possible mainly by two technical innovations. First, heatmaps of 2D keypoints serve as an intermediate representation to connect real and synthetic data. 3D-INN is trained on real images to estimate 2D keypoint heatmaps from an input image; it then predicts 3D object structure from heatmaps using knowledge learned from synthetic 3D shapes. By doing so, 3D-INN benefits from the variation and abundance of synthetic 3D objects, without suffering from the domain difference between real and synthesized images, often due to imperfect rendering. Second, we propose a Projection Layer, mapping estimated 3D structure back to 2D. During training, it ensures 3D-INN to predict 3D structure whose projection is consistent with the 2D annotations to real images. Experiments show that the proposed system performs well on both 2D keypoint estimation and 3D structure recovery. We also demonstrate that the recovered 3D information has wide vision applications, such as image retrieval.



### Multi-Scale Spatially-Asymmetric Recalibration for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1804.00787v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.00787v1)
- **Published**: 2018-04-03 02:09:14+00:00
- **Updated**: 2018-04-03 02:09:14+00:00
- **Authors**: Yan Wang, Lingxi Xie, Siyuan Qiao, Ya Zhang, Wenjun Zhang, Alan L. Yuille
- **Comment**: 17 pages, 5 figures, submitted to ECCV 2018
- **Journal**: None
- **Summary**: Convolution is spatially-symmetric, i.e., the visual features are independent of its position in the image, which limits its ability to utilize contextual cues for visual recognition. This paper addresses this issue by introducing a recalibration process, which refers to the surrounding region of each neuron, computes an importance value and multiplies it to the original neural response. Our approach is named multi-scale spatially-asymmetric recalibration (MS-SAR), which extracts visual cues from surrounding regions at multiple scales, and designs a weighting scheme which is asymmetric in the spatial domain. MS-SAR is implemented in an efficient way, so that only small fractions of extra parameters and computations are required. We apply MS-SAR to several popular building blocks, including the residual block and the densely-connected block, and demonstrate its superior performance in both CIFAR and ILSVRC2012 classification tasks.



### Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1804.00792v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1804.00792v2)
- **Published**: 2018-04-03 02:24:31+00:00
- **Updated**: 2018-11-10 15:37:17+00:00
- **Authors**: Ali Shafahi, W. Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer, Tudor Dumitras, Tom Goldstein
- **Comment**: Presented at the NIPS 2018 conference. 11 pages, 4 figures, with a
  supplementary section of 7 pages, 7 figures. First two authors contributed
  equally
- **Journal**: None
- **Summary**: Data poisoning is an attack on machine learning models wherein the attacker adds examples to the training set to manipulate the behavior of the model at test time. This paper explores poisoning attacks on neural nets. The proposed attacks use "clean-labels"; they don't require the attacker to have any control over the labeling of training data. They are also targeted; they control the behavior of the classifier on a $\textit{specific}$ test instance without degrading overall classifier performance. For example, an attacker could add a seemingly innocuous image (that is properly labeled) to a training set for a face recognition engine, and control the identity of a chosen person at test time. Because the attacker does not need to control the labeling function, poisons could be entered into the training set simply by leaving them on the web and waiting for them to be scraped by a data collection bot.   We present an optimization-based method for crafting poisons, and show that just one single poison image can control classifier behavior when transfer learning is used. For full end-to-end training, we present a "watermarking" strategy that makes poisoning reliable using multiple ($\approx$50) poisoned training instances. We demonstrate our method by generating poisoned frog images from the CIFAR dataset and using them to manipulate image classifiers.



### Left-Right Comparative Recurrent Model for Stereo Matching
- **Arxiv ID**: http://arxiv.org/abs/1804.00796v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.00796v1)
- **Published**: 2018-04-03 02:50:26+00:00
- **Updated**: 2018-04-03 02:50:26+00:00
- **Authors**: Zequn Jie, Pengfei Wang, Yonggen Ling, Bo Zhao, Yunchao Wei, Jiashi Feng, Wei Liu
- **Comment**: Accepted by CVPR 2018
- **Journal**: None
- **Summary**: Leveraging the disparity information from both left and right views is crucial for stereo disparity estimation. Left-right consistency check is an effective way to enhance the disparity estimation by referring to the information from the opposite view. However, the conventional left-right consistency check is an isolated post-processing step and heavily hand-crafted. This paper proposes a novel left-right comparative recurrent model to perform left-right consistency checking jointly with disparity estimation. At each recurrent step, the model produces disparity results for both views, and then performs online left-right comparison to identify the mismatched regions which may probably contain erroneously labeled pixels. A soft attention mechanism is introduced, which employs the learned error maps for better guiding the model to selectively focus on refining the unreliable regions at the next recurrent step. In this way, the generated disparity maps are progressively improved by the proposed recurrent model. Extensive evaluations on KITTI 2015, Scene Flow and Middlebury benchmarks validate the effectiveness of our model, demonstrating that state-of-the-art stereo disparity estimation results can be achieved by this new model.



### Patch-based Face Recognition using a Hierarchical Multi-label Matcher
- **Arxiv ID**: http://arxiv.org/abs/1804.01417v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.01417v1)
- **Published**: 2018-04-03 04:00:37+00:00
- **Updated**: 2018-04-03 04:00:37+00:00
- **Authors**: Lingfeng Zhang, Pengfei Dou, Ioannis A Kakadiaris
- **Comment**: accepted in IVC: Biometrics in the Wild. arXiv admin note: text
  overlap with arXiv:1803.09359
- **Journal**: None
- **Summary**: This paper proposes a hierarchical multi-label matcher for patch-based face recognition. In signature generation, a face image is iteratively divided into multi-level patches. Two different types of patch divisions and signatures are introduced for 2D facial image and texture-lifted image, respectively. The matcher training consists of three steps. First, local classifiers are built to learn the local matching of each patch. Second, the hierarchical relationships defined between local patches are used to learn the global matching of each patch. Three ways are introduced to learn the global matching: majority voting, l1-regularized weighting, and decision rule. Last, the global matchings of different levels are combined as the final matching. Experimental results on different face recognition tasks demonstrate the effectiveness of the proposed matcher at the cost of gallery generalization. Compared with the UR2D system, the proposed matcher improves the Rank-1 accuracy significantly by 3% and 0.18% on the UHDB31 dataset and IJB-A dataset, respectively.



### End-to-End Dense Video Captioning with Masked Transformer
- **Arxiv ID**: http://arxiv.org/abs/1804.00819v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.00819v1)
- **Published**: 2018-04-03 04:11:00+00:00
- **Updated**: 2018-04-03 04:11:00+00:00
- **Authors**: Luowei Zhou, Yingbo Zhou, Jason J. Corso, Richard Socher, Caiming Xiong
- **Comment**: To appear at CVPR18
- **Journal**: None
- **Summary**: Dense video captioning aims to generate text descriptions for all events in an untrimmed video. This involves both detecting and describing events. Therefore, all previous methods on dense video captioning tackle this problem by building two models, i.e. an event proposal and a captioning model, for these two sub-problems. The models are either trained separately or in alternation. This prevents direct influence of the language description to the event proposal, which is important for generating accurate descriptions. To address this problem, we propose an end-to-end transformer model for dense video captioning. The encoder encodes the video into appropriate representations. The proposal decoder decodes from the encoding with different anchors to form video event proposals. The captioning decoder employs a masking network to restrict its attention to the proposal event over the encoding feature. This masking network converts the event proposal to a differentiable mask, which ensures the consistency between the proposal and captioning during training. In addition, our model employs a self-attention mechanism, which enables the use of efficient non-recurrent structure during encoding and leads to performance improvements. We demonstrate the effectiveness of this end-to-end model on ActivityNet Captions and YouCookII datasets, where we achieved 10.12 and 6.58 METEOR score, respectively.



### Prediction and Localization of Student Engagement in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1804.00858v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.00858v4)
- **Published**: 2018-04-03 07:42:15+00:00
- **Updated**: 2018-06-26 21:32:04+00:00
- **Authors**: Amanjot Kaur, Aamir Mustafa, Love Mehta, Abhinav Dhall
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we introduce a new dataset for student engagement detection and localization. Digital revolution has transformed the traditional teaching procedure and a result analysis of the student engagement in an e-learning environment would facilitate effective task accomplishment and learning. Well known social cues of engagement/disengagement can be inferred from facial expressions, body movements and gaze pattern. In this paper, student's response to various stimuli videos are recorded and important cues are extracted to estimate variations in engagement level. In this paper, we study the association of a subject's behavioral cues with his/her engagement level, as annotated by labelers. We then localize engaging/non-engaging parts in the stimuli videos using a deep multiple instance learning based framework, which can give useful insight into designing Massive Open Online Courses (MOOCs) video material. Recognizing the lack of any publicly available dataset in the domain of user engagement, a new `in the wild' dataset is created to study the subject engagement problem. The dataset contains 195 videos captured from 78 subjects which is about 16.5 hours of recording. We present detailed baseline results using different classifiers ranging from traditional machine learning to deep learning based approaches. The subject independent analysis is performed so that it can be generalized to new users. The problem of engagement prediction is modeled as a weakly supervised learning problem. The dataset is manually annotated by different labelers for four levels of engagement independently and the correlation studies between annotated and predicted labels of videos by different classifiers is reported. This dataset creation is an effort to facilitate research in various e-learning environments such as intelligent tutoring systems, MOOCs, and others.



### Unsupervised Semantic-based Aggregation of Deep Convolutional Features
- **Arxiv ID**: http://arxiv.org/abs/1804.01422v1
- **DOI**: 10.1109/TIP.2018.2867104
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.01422v1)
- **Published**: 2018-04-03 07:43:05+00:00
- **Updated**: 2018-04-03 07:43:05+00:00
- **Authors**: Jian Xu, Chunheng Wang, Chengzuo Qi, Cunzhao Shi, Baihua Xiao
- **Comment**: 10 pages. arXiv admin note: text overlap with arXiv:1705.01247
- **Journal**: None
- **Summary**: In this paper, we propose a simple but effective semantic-based aggregation (SBA) method. The proposed SBA utilizes the discriminative filters of deep convolutional layers as semantic detectors. Moreover, we propose the effective unsupervised strategy to select some semantic detectors to generate the "probabilistic proposals", which highlight certain discriminative pattern of objects and suppress the noise of background. The final global SBA representation could then be acquired by aggregating the regional representations weighted by the selected "probabilistic proposals" corresponding to various semantic content. Our unsupervised SBA is easy to generalize and achieves excellent performance on various tasks. We conduct comprehensive experiments and show that our unsupervised SBA outperforms the state-of-the-art unsupervised and supervised aggregation methods on image retrieval, place recognition and cloud classification.



### Generating Diverse and Accurate Visual Captions by Comparative Adversarial Learning
- **Arxiv ID**: http://arxiv.org/abs/1804.00861v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.00861v3)
- **Published**: 2018-04-03 08:06:33+00:00
- **Updated**: 2019-03-10 07:01:55+00:00
- **Authors**: Dianqi Li, Qiuyuan Huang, Xiaodong He, Lei Zhang, Ming-Ting Sun
- **Comment**: None
- **Journal**: None
- **Summary**: We study how to generate captions that are not only accurate in describing an image but also discriminative across different images. The problem is both fundamental and interesting, as most machine-generated captions, despite phenomenal research progresses in the past several years, are expressed in a very monotonic and featureless format. While such captions are normally accurate, they often lack important characteristics in human languages - distinctiveness for each caption and diversity for different images. To address this problem, we propose a novel conditional generative adversarial network for generating diverse captions across images. Instead of estimating the quality of a caption solely on one image, the proposed comparative adversarial learning framework better assesses the quality of captions by comparing a set of captions within the image-caption joint space. By contrasting with human-written captions and image-mismatched captions, the caption generator effectively exploits the inherent characteristics of human languages, and generates more discriminative captions. We show that our proposed network is capable of producing accurate and diverse captions across images.



### Deep Appearance Maps
- **Arxiv ID**: http://arxiv.org/abs/1804.00863v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1804.00863v3)
- **Published**: 2018-04-03 08:17:38+00:00
- **Updated**: 2019-10-29 06:31:33+00:00
- **Authors**: Maxim Maximov, Laura Leal-Taixé, Mario Fritz, Tobias Ritschel
- **Comment**: None
- **Journal**: ICCV 2019
- **Summary**: We propose a deep representation of appearance, i. e., the relation of color, surface orientation, viewer position, material and illumination. Previous approaches have useddeep learning to extract classic appearance representationsrelating to reflectance model parameters (e. g., Phong) orillumination (e. g., HDR environment maps). We suggest todirectly represent appearance itself as a network we call aDeep Appearance Map (DAM). This is a 4D generalizationover 2D reflectance maps, which held the view direction fixed. First, we show how a DAM can be learned from images or video frames and later be used to synthesize appearance, given new surface orientations and viewer positions. Second, we demonstrate how another network can be used to map from an image or video frames to a DAM network to reproduce this appearance, without using a lengthy optimization such as stochastic gradient descent (learning-to-learn). Finally, we show the example of an appearance estimation-and-segmentation task, mapping from an image showingmultiple materials to multiple deep appearance maps.



### Exploring Multi-Branch and High-Level Semantic Networks for Improving Pedestrian Detection
- **Arxiv ID**: http://arxiv.org/abs/1804.00872v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.00872v1)
- **Published**: 2018-04-03 08:52:50+00:00
- **Updated**: 2018-04-03 08:52:50+00:00
- **Authors**: Jiale Cao, Yanwei Pang, Xuelong Li
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: To better detect pedestrians of various scales, deep multi-scale methods usually detect pedestrians of different scales by different in-network layers. However, the semantic levels of features from different layers are usually inconsistent. In this paper, we propose a multi-branch and high-level semantic network by gradually splitting a base network into multiple different branches. As a result, the different branches have the same depth and the output features of different branches have similarly high-level semantics. Due to the difference of receptive fields, the different branches are suitable to detect pedestrians of different scales. Meanwhile, the multi-branch network does not introduce additional parameters by sharing convolutional weights of different branches. To further improve detection performance, skip-layer connections among different branches are used to add context to the branch of relatively small receptive filed, and dilated convolution is incorporated into part branches to enlarge the resolutions of output feature maps. When they are embedded into Faster RCNN architecture, the weighted scores of proposal generation network and proposal classification network are further proposed. Experiments on KITTI dataset, Caltech pedestrian dataset, and Citypersons dataset demonstrate the effectiveness of proposed method. On these pedestrian datasets, the proposed method achieves state-of-the-art detection performance. Moreover, experiments on COCO benchmark show the proposed method is also suitable for general object detection.



### CodeSLAM - Learning a Compact, Optimisable Representation for Dense Visual SLAM
- **Arxiv ID**: http://arxiv.org/abs/1804.00874v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1804.00874v2)
- **Published**: 2018-04-03 09:00:42+00:00
- **Updated**: 2019-04-14 11:54:05+00:00
- **Authors**: Michael Bloesch, Jan Czarnowski, Ronald Clark, Stefan Leutenegger, Andrew J. Davison
- **Comment**: Published in Proceedings of the IEEE Conference on Computer Vision
  and Pattern Recognition (CVPR 2018)
- **Journal**: None
- **Summary**: The representation of geometry in real-time 3D perception systems continues to be a critical research issue. Dense maps capture complete surface shape and can be augmented with semantic labels, but their high dimensionality makes them computationally costly to store and process, and unsuitable for rigorous probabilistic inference. Sparse feature-based representations avoid these problems, but capture only partial scene information and are mainly useful for localisation only.   We present a new compact but dense representation of scene geometry which is conditioned on the intensity data from a single image and generated from a code consisting of a small number of parameters. We are inspired by work both on learned depth from images, and auto-encoders. Our approach is suitable for use in a keyframe-based monocular dense SLAM system: While each keyframe with a code can produce a depth map, the code can be optimised efficiently jointly with pose variables and together with the codes of overlapping keyframes to attain global consistency. Conditioning the depth map on the image allows the code to only represent aspects of the local geometry which cannot directly be predicted from the image. We explain how to learn our code representation, and demonstrate its advantageous properties in monocular SLAM.



### Weakly Supervised Instance Segmentation using Class Peak Response
- **Arxiv ID**: http://arxiv.org/abs/1804.00880v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.00880v1)
- **Published**: 2018-04-03 09:29:30+00:00
- **Updated**: 2018-04-03 09:29:30+00:00
- **Authors**: Yanzhao Zhou, Yi Zhu, Qixiang Ye, Qiang Qiu, Jianbin Jiao
- **Comment**: Accepted in CVPR 2018 (Spotlight)
- **Journal**: None
- **Summary**: Weakly supervised instance segmentation with image-level labels, instead of expensive pixel-level masks, remains unexplored. In this paper, we tackle this challenging problem by exploiting class peak responses to enable a classification network for instance mask extraction. With image labels supervision only, CNN classifiers in a fully convolutional manner can produce class response maps, which specify classification confidence at each image location. We observed that local maximums, i.e., peaks, in a class response map typically correspond to strong visual cues residing inside each instance. Motivated by this, we first design a process to stimulate peaks to emerge from a class response map. The emerged peaks are then back-propagated and effectively mapped to highly informative regions of each object instance, such as instance boundaries. We refer to the above maps generated from class peak responses as Peak Response Maps (PRMs). PRMs provide a fine-detailed instance-level representation, which allows instance masks to be extracted even with some off-the-shelf methods. To the best of our knowledge, we for the first time report results for the challenging image-level supervised instance segmentation task. Extensive experiments show that our method also boosts weakly supervised pointwise localization as well as semantic segmentation performance, and reports state-of-the-art results on popular benchmarks, including PASCAL VOC 2012 and MS COCO.



### PhaseNet for Video Frame Interpolation
- **Arxiv ID**: http://arxiv.org/abs/1804.00884v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.00884v1)
- **Published**: 2018-04-03 09:41:36+00:00
- **Updated**: 2018-04-03 09:41:36+00:00
- **Authors**: Simone Meyer, Abdelaziz Djelouah, Brian McWilliams, Alexander Sorkine-Hornung, Markus Gross, Christopher Schroers
- **Comment**: CVPR 2018
- **Journal**: None
- **Summary**: Most approaches for video frame interpolation require accurate dense correspondences to synthesize an in-between frame. Therefore, they do not perform well in challenging scenarios with e.g. lighting changes or motion blur. Recent deep learning approaches that rely on kernels to represent motion can only alleviate these problems to some extent. In those cases, methods that use a per-pixel phase-based motion representation have been shown to work well. However, they are only applicable for a limited amount of motion. We propose a new approach, PhaseNet, that is designed to robustly handle challenging scenarios while also coping with larger motion. Our approach consists of a neural network decoder that directly estimates the phase decomposition of the intermediate frame. We show that this is superior to the hand-crafted heuristics previously used in phase-based methods and also compares favorably to recent deep learning based approaches for video frame interpolation on challenging datasets.



### Learning to Guide Decoding for Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/1804.00887v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.00887v1)
- **Published**: 2018-04-03 09:50:06+00:00
- **Updated**: 2018-04-03 09:50:06+00:00
- **Authors**: Wenhao Jiang, Lin Ma, Xinpeng Chen, Hanwang Zhang, Wei Liu
- **Comment**: AAAI-18
- **Journal**: None
- **Summary**: Recently, much advance has been made in image captioning, and an encoder-decoder framework has achieved outstanding performance for this task. In this paper, we propose an extension of the encoder-decoder framework by adding a component called guiding network. The guiding network models the attribute properties of input images, and its output is leveraged to compose the input of the decoder at each time step. The guiding network can be plugged into the current encoder-decoder framework and trained in an end-to-end manner. Hence, the guiding vector can be adaptively learned according to the signal from the decoder, making itself to embed information from both image and language. Additionally, discriminative supervision can be employed to further improve the quality of guidance. The advantages of our proposed approach are verified by experiments carried out on the MS COCO dataset.



### When will you do what? - Anticipating Temporal Occurrences of Activities
- **Arxiv ID**: http://arxiv.org/abs/1804.00892v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.00892v1)
- **Published**: 2018-04-03 09:59:46+00:00
- **Updated**: 2018-04-03 09:59:46+00:00
- **Authors**: Yazan Abu Farha, Alexander Richard, Juergen Gall
- **Comment**: CVPR 2018
- **Journal**: None
- **Summary**: Analyzing human actions in videos has gained increased attention recently. While most works focus on classifying and labeling observed video frames or anticipating the very recent future, making long-term predictions over more than just a few seconds is a task with many practical applications that has not yet been addressed. In this paper, we propose two methods to predict a considerably large amount of future actions and their durations. Both, a CNN and an RNN are trained to learn future video labels based on previously seen content. We show that our methods generate accurate predictions of the future even for long videos with a huge amount of different actions and can even deal with noisy or erroneous input information.



### Towards whole-body CT Bone Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1804.00908v1
- **DOI**: 10.1007/978-3-662-56537-7_59
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.00908v1)
- **Published**: 2018-04-03 11:07:50+00:00
- **Updated**: 2018-04-03 11:07:50+00:00
- **Authors**: André Klein, Jan Warszawski, Jens Hillengaß, Klaus H. Maier-Hein
- **Comment**: Accepted conference paper at BVM 2018
- **Journal**: None
- **Summary**: Bone segmentation from CT images is a task that has been worked on for decades. It is an important ingredient to several diagnostics or treatment planning approaches and relevant to various diseases. As high-quality manual and semi-automatic bone segmentation is very time-consuming, a reliable and fully automatic approach would be of great interest in many scenarios. In this publication, we propose a UNet inspired architecture to address the task using Deep Learning. We evaluated the approach on whole-body CT scans of patients suffering from multiple myeloma. As the disease decomposes the bone, an accurate segmentation is of utmost importance for the evaluation of bone density, disease staging and localization of focal lesions. The method was evaluated on an in-house data-set of 6000 2D image slices taken from 15 whole-body CT scans, achieving a dice score of 0.96 and an IOU of 0.94.



### Dynamic Video Segmentation Network
- **Arxiv ID**: http://arxiv.org/abs/1804.00931v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.00931v2)
- **Published**: 2018-04-03 12:36:14+00:00
- **Updated**: 2018-06-14 12:11:48+00:00
- **Authors**: Yu-Syuan Xu, Tsu-Jui Fu, Hsuan-Kung Yang, Chun-Yi Lee
- **Comment**: CVPR 2018
- **Journal**: None
- **Summary**: In this paper, we present a detailed design of dynamic video segmentation network (DVSNet) for fast and efficient semantic video segmentation. DVSNet consists of two convolutional neural networks: a segmentation network and a flow network. The former generates highly accurate semantic segmentations, but is deeper and slower. The latter is much faster than the former, but its output requires further processing to generate less accurate semantic segmentations. We explore the use of a decision network to adaptively assign different frame regions to different networks based on a metric called expected confidence score. Frame regions with a higher expected confidence score traverse the flow network. Frame regions with a lower expected confidence score have to pass through the segmentation network. We have extensively performed experiments on various configurations of DVSNet, and investigated a number of variants for the proposed decision network. The experimental results show that our DVSNet is able to achieve up to 70.4% mIoU at 19.8 fps on the Cityscape dataset. A high speed version of DVSNet is able to deliver an fps of 30.4 with 63.2% mIoU on the same dataset. DVSNet is also able to reduce up to 95% of the computational workloads.



### Unsupervised Learning of Sequence Representations by Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/1804.00946v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1804.00946v2)
- **Published**: 2018-04-03 13:12:45+00:00
- **Updated**: 2018-04-26 22:31:09+00:00
- **Authors**: Wenjie Pei, David M. J. Tax
- **Comment**: None
- **Journal**: None
- **Summary**: Sequence data is challenging for machine learning approaches, because the lengths of the sequences may vary between samples. In this paper, we present an unsupervised learning model for sequence data, called the Integrated Sequence Autoencoder (ISA), to learn a fixed-length vectorial representation by minimizing the reconstruction error. Specifically, we propose to integrate two classical mechanisms for sequence reconstruction which takes into account both the global silhouette information and the local temporal dependencies. Furthermore, we propose a stop feature that serves as a temporal stamp to guide the reconstruction process, which results in a higher-quality representation. The learned representation is able to effectively summarize not only the apparent features, but also the underlying and high-level style information. Take for example a speech sequence sample: our ISA model can not only recognize the spoken text (apparent feature), but can also discriminate the speaker who utters the audio (more high-level style). One promising application of the ISA model is that it can be readily used in the semi-supervised learning scenario, in which a large amount of unlabeled data is leveraged to extract high-quality sequence representations and thus to improve the performance of the subsequent supervised learning tasks on limited labeled data.



### Training VAEs Under Structured Residuals
- **Arxiv ID**: http://arxiv.org/abs/1804.01050v3
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1804.01050v3)
- **Published**: 2018-04-03 16:04:22+00:00
- **Updated**: 2018-07-31 16:53:19+00:00
- **Authors**: Garoe Dorta, Sara Vicente, Lourdes Agapito, Neill D. F. Campbell, Ivor Simpson
- **Comment**: Simplified training methodology, added more results
- **Journal**: None
- **Summary**: Variational auto-encoders (VAEs) are a popular and powerful deep generative model. Previous works on VAEs have assumed a factorized likelihood model, whereby the output uncertainty of each pixel is assumed to be independent. This approximation is clearly limited as demonstrated by observing a residual image from a VAE reconstruction, which often possess a high level of structure. This paper demonstrates a novel scheme to incorporate a structured Gaussian likelihood prediction network within the VAE that allows the residual correlations to be modeled. Our novel architecture, with minimal increase in complexity, incorporates the covariance matrix prediction within the VAE. We also propose a new mechanism for allowing structured uncertainty on color images. Furthermore, we provide a scheme for effectively training this model, and include some suggestions for improving performance in terms of efficiency or modeling longer range correlations.



### DOCK: Detecting Objects by transferring Common-sense Knowledge
- **Arxiv ID**: http://arxiv.org/abs/1804.01077v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1804.01077v2)
- **Published**: 2018-04-03 17:41:53+00:00
- **Updated**: 2018-07-31 06:42:30+00:00
- **Authors**: Krishna Kumar Singh, Santosh Divvala, Ali Farhadi, Yong Jae Lee
- **Comment**: None
- **Journal**: ECCV, 2018
- **Summary**: We present a scalable approach for Detecting Objects by transferring Common-sense Knowledge (DOCK) from source to target categories. In our setting, the training data for the source categories have bounding box annotations, while those for the target categories only have image-level annotations. Current state-of-the-art approaches focus on image-level visual or semantic similarity to adapt a detector trained on the source categories to the new target categories. In contrast, our key idea is to (i) use similarity not at the image-level, but rather at the region-level, and (ii) leverage richer common-sense (based on attribute, spatial, etc.) to guide the algorithm towards learning the correct detections. We acquire such common-sense cues automatically from readily-available knowledge bases without any extra human effort. On the challenging MS COCO dataset, we find that common-sense knowledge can substantially improve detection performance over existing transfer-learning baselines.



### Unsupervised Geometry-Aware Representation for 3D Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1804.01110v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1804.01110v1)
- **Published**: 2018-04-03 18:01:54+00:00
- **Updated**: 2018-04-03 18:01:54+00:00
- **Authors**: Helge Rhodin, Mathieu Salzmann, Pascal Fua
- **Comment**: None
- **Journal**: None
- **Summary**: Modern 3D human pose estimation techniques rely on deep networks, which require large amounts of training data. While weakly-supervised methods require less supervision, by utilizing 2D poses or multi-view imagery without annotations, they still need a sufficiently large set of samples with 3D annotations for learning to succeed.   In this paper, we propose to overcome this problem by learning a geometry-aware body representation from multi-view images without annotations. To this end, we use an encoder-decoder that predicts an image from one viewpoint given an image from another viewpoint. Because this representation encodes 3D geometry, using it in a semi-supervised setting makes it easier to learn a mapping from it to 3D human pose. As evidenced by our experiments, our approach significantly outperforms fully-supervised methods given the same amount of labeled data, and improves over other semi-supervised methods while using as little as 1% of the labeled data.



### Visual Object Categorization Based on Hierarchical Shape Motifs Learned From Noisy Point Cloud Decompositions
- **Arxiv ID**: http://arxiv.org/abs/1804.01117v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1804.01117v1)
- **Published**: 2018-04-03 18:21:31+00:00
- **Updated**: 2018-04-03 18:21:31+00:00
- **Authors**: Christian A. Mueller, Andreas Birk
- **Comment**: None
- **Journal**: None
- **Summary**: Object shape is a key cue that contributes to the semantic understanding of objects. In this work we focus on the categorization of real-world object point clouds to particular shape types. Therein surface description and representation of object shape structure have significant influence on shape categorization accuracy, when dealing with real-world scenes featuring noisy, partial and occluded object observations. An unsupervised hierarchical learning procedure is utilized here to symbolically describe surface characteristics on multiple semantic levels. Furthermore, a constellation model is proposed that hierarchically decomposes objects. The decompositions are described as constellations of symbols (shape motifs) in a gradual order, hence reflecting shape structure from local to global, i.e., from parts over groups of parts to entire objects. The combination of this multi-level description of surfaces and the hierarchical decomposition of shapes leads to a representation which allows to conceptualize shapes. An object discrimination has been observed in experiments with seven categories featuring instances with sensor noise, occlusions as well as inter-category and intra-category similarities. Experiments include the evaluation of the proposed description and shape decomposition approach, and comparisons to Fast Point Feature Histograms, a Vocabulary Tree and a neural network-based Deep Learning method. Furthermore, experiments are conducted with alternative datasets which analyze the generalization capability of the proposed approach.



### Synthesizing Programs for Images using Reinforced Adversarial Learning
- **Arxiv ID**: http://arxiv.org/abs/1804.01118v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1804.01118v1)
- **Published**: 2018-04-03 18:25:42+00:00
- **Updated**: 2018-04-03 18:25:42+00:00
- **Authors**: Yaroslav Ganin, Tejas Kulkarni, Igor Babuschkin, S. M. Ali Eslami, Oriol Vinyals
- **Comment**: 12 pages, 13 figures
- **Journal**: None
- **Summary**: Advances in deep generative networks have led to impressive results in recent years. Nevertheless, such models can often waste their capacity on the minutiae of datasets, presumably due to weak inductive biases in their decoders. This is where graphics engines may come in handy since they abstract away low-level details and represent images as high-level programs. Current methods that combine deep learning and renderers are limited by hand-crafted likelihood or distance functions, a need for large amounts of supervision, or difficulties in scaling their inference algorithms to richer datasets. To mitigate these issues, we present SPIRAL, an adversarially trained agent that generates a program which is executed by a graphics engine to interpret and sample images. The goal of this agent is to fool a discriminator network that distinguishes between real and rendered data, trained with a distributed reinforcement learning setup without any supervision. A surprising finding is that using the discriminator's output as a reward signal is the key to allow the agent to make meaningful progress at matching the desired output rendering. To the best of our knowledge, this is the first demonstration of an end-to-end, unsupervised and adversarial inverse graphics agent on challenging real world (MNIST, Omniglot, CelebA) and synthetic 3D datasets.



### A Modified Image Comparison Algorithm Using Histogram Features
- **Arxiv ID**: http://arxiv.org/abs/1804.01142v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.01142v1)
- **Published**: 2018-04-03 19:41:00+00:00
- **Updated**: 2018-04-03 19:41:00+00:00
- **Authors**: Anas M. Al-Oraiqat, Natalya S. Kostyukova
- **Comment**: 8 pages, 7 figures
- **Journal**: International Journal of advanced studies in Computer Science and
  Engineering, 2018
- **Summary**: This article discuss the problem of color image content comparison. Particularly, methods of image content comparison are analyzed, restrictions of color histogram are described and a modified method of images content comparison is proposed. This method uses the color histograms and considers color locations. Testing and analyzing of based and modified algorithms are performed. The modified method shows 97% average precision for a collection containing about 700 images without loss of the advantages of based method, i.e. scale and rotation invariant.



### Crystal Loss and Quality Pooling for Unconstrained Face Verification and Recognition
- **Arxiv ID**: http://arxiv.org/abs/1804.01159v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.01159v2)
- **Published**: 2018-04-03 20:30:25+00:00
- **Updated**: 2019-02-04 03:13:42+00:00
- **Authors**: Rajeev Ranjan, Ankan Bansal, Hongyu Xu, Swami Sankaranarayanan, Jun-Cheng Chen, Carlos D. Castillo, Rama Chellappa
- **Comment**: Previously portions of this work appeared in arXiv:1703.09507, which
  was a conference version. This version is an extended journal version of it
- **Journal**: None
- **Summary**: In recent years, the performance of face verification and recognition systems based on deep convolutional neural networks (DCNNs) has significantly improved. A typical pipeline for face verification includes training a deep network for subject classification with softmax loss, using the penultimate layer output as the feature descriptor, and generating a cosine similarity score given a pair of face images or videos. The softmax loss function does not optimize the features to have higher similarity score for positive pairs and lower similarity score for negative pairs, which leads to a performance gap. In this paper, we propose a new loss function, called Crystal Loss, that restricts the features to lie on a hypersphere of a fixed radius. The loss can be easily implemented using existing deep learning frameworks. We show that integrating this simple step in the training pipeline significantly improves the performance of face verification and recognition systems. We achieve state-of-the-art performance for face verification and recognition on challenging LFW, IJB-A, IJB-B and IJB-C datasets over a large range of false alarm rates (10-1 to 10-7).



### Towards Deep Learning based Hand Keypoints Detection for Rapid Sequential Movements from RGB Images
- **Arxiv ID**: http://arxiv.org/abs/1804.01174v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.01174v1)
- **Published**: 2018-04-03 21:28:16+00:00
- **Updated**: 2018-04-03 21:28:16+00:00
- **Authors**: Srujana Gattupalli, Ashwin Ramesh Babu, James Robert Brady, Fillia Makedon, Vassilis Athitsos
- **Comment**: None
- **Journal**: None
- **Summary**: Hand keypoints detection and pose estimation has numerous applications in computer vision, but it is still an unsolved problem in many aspects. An application of hand keypoints detection is in performing cognitive assessments of a subject by observing the performance of that subject in physical tasks involving rapid finger motion. As a part of this work, we introduce a novel hand key-points benchmark dataset that consists of hand gestures recorded specifically for cognitive behavior monitoring. We explore the state of the art methods in hand keypoint detection and we provide quantitative evaluations for the performance of these methods on our dataset. In future, these results and our dataset can serve as a useful benchmark for hand keypoint recognition for rapid finger movements.



### Looking at Hands in Autonomous Vehicles: A ConvNet Approach using Part Affinity Fields
- **Arxiv ID**: http://arxiv.org/abs/1804.01176v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1804.01176v1)
- **Published**: 2018-04-03 21:45:14+00:00
- **Updated**: 2018-04-03 21:45:14+00:00
- **Authors**: Kevan Yuen, Mohan M. Trivedi
- **Comment**: 11 pages, 8 figures, 1 table. Submitted to "IEEE Transactions on
  Intelligent Vehicles" (under review)
- **Journal**: None
- **Summary**: In the context of autonomous driving, where humans may need to take over in the event where the computer may issue a takeover request, a key step towards driving safety is the monitoring of the hands to ensure the driver is ready for such a request. This work, focuses on the first step of this process, which is to locate the hands. Such a system must work in real-time and under varying harsh lighting conditions. This paper introduces a fast ConvNet approach, based on the work of original work of OpenPose for full body joint estimation. The network is modified with fewer parameters and retrained using our own day-time naturalistic autonomous driving dataset to estimate joint and affinity heatmaps for driver & passenger's wrist and elbows, for a total of 8 joint classes and part affinity fields between each wrist-elbow pair. The approach runs real-time on real-world data at 40 fps on multiple drivers and passengers. The system is extensively evaluated both quantitatively and qualitatively, showing at least 95% detection performance on joint localization and arm-angle estimation.



