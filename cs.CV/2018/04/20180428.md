# Arxiv Papers in cs.CV on 2018-04-28
### Deep Learning based Inter-Modality Image Registration Supervised by Intra-Modality Similarity
- **Arxiv ID**: http://arxiv.org/abs/1804.10735v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.10735v1)
- **Published**: 2018-04-28 03:53:42+00:00
- **Updated**: 2018-04-28 03:53:42+00:00
- **Authors**: Xiaohuan Cao, Jianhua Yang, Li Wang, Zhong Xue, Qian Wang, Dinggang Shen
- **Comment**: 8 pages, 4 figures
- **Journal**: None
- **Summary**: Non-rigid inter-modality registration can facilitate accurate information fusion from different modalities, but it is challenging due to the very different image appearances across modalities. In this paper, we propose to train a non-rigid inter-modality image registration network, which can directly predict the transformation field from the input multimodal images, such as CT and MR images. In particular, the training of our inter-modality registration network is supervised by intra-modality similarity metric based on the available paired data, which is derived from a pre-aligned CT and MR dataset. Specifically, in the training stage, to register the input CT and MR images, their similarity is evaluated on the warped MR image and the MR image that is paired with the input CT. So that, the intra-modality similarity metric can be directly applied to measure whether the input CT and MR images are well registered. Moreover, we use the idea of dual-modality fashion, in which we measure the similarity on both CT modality and MR modality. In this way, the complementary anatomies in both modalities can be jointly considered to more accurately train the inter-modality registration network. In the testing stage, the trained inter-modality registration network can be directly applied to register the new multimodal images without any paired data. Experimental results have shown that, the proposed method can achieve promising accuracy and efficiency for the challenging non-rigid inter-modality registration task and also outperforms the state-of-the-art approaches.



### Precise Box Score: Extract More Information from Datasets to Improve the Performance of Face Detection
- **Arxiv ID**: http://arxiv.org/abs/1804.10743v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.10743v1)
- **Published**: 2018-04-28 05:11:19+00:00
- **Updated**: 2018-04-28 05:11:19+00:00
- **Authors**: Ce Qi, Xiaoping Chen, Pingyu Wang, Fei Su
- **Comment**: None
- **Journal**: None
- **Summary**: For the training of face detection network based on R-CNN framework, anchors are assigned to be positive samples if intersection-over-unions (IoUs) with ground-truth are higher than the first threshold(such as 0.7); and to be negative samples if their IoUs are lower than the second threshold(such as 0.3). And the face detection model is trained by the above labels. However, anchors with IoU between first threshold and second threshold are not used. We propose a novel training strategy, Precise Box Score(PBS), to train object detection models. The proposed training strategy uses the anchors with IoUs between the first and second threshold, which can consistently improve the performance of face detection. Our proposed training strategy extracts more information from datasets, making better utilization of existing datasets. What's more, we also introduce a simple but effective model compression method(SEMCM), which can boost the performance of face detectors further. Experimental results show that the performance of face detection network can consistently be improved based on our proposed scheme.



### Efficient Subpixel Refinement with Symbolic Linear Predictors
- **Arxiv ID**: http://arxiv.org/abs/1804.10750v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.10750v1)
- **Published**: 2018-04-28 06:31:05+00:00
- **Updated**: 2018-04-28 06:31:05+00:00
- **Authors**: Vincent Lui, Jonathon Geeves, Winston Yii, Tom Drummond
- **Comment**: IEEE/CVF International Conference on Computer Vision and Pattern
  Recognition 2018 (CVPR)
- **Journal**: None
- **Summary**: We present an efficient subpixel refinement method usinga learning-based approach called Linear Predictors. Two key ideas are shown in this paper. Firstly, we present a novel technique, called Symbolic Linear Predictors, which makes the learning step efficient for subpixel refinement. This makes our approach feasible for online applications without compromising accuracy, while taking advantage of the run-time efficiency of learning based approaches. Secondly, we show how Linear Predictors can be used to predict the expected alignment error, allowing us to use only the best keypoints in resource constrained applications. We show the efficiency and accuracy of our method through extensive experiments.



### Spiking Deep Residual Network
- **Arxiv ID**: http://arxiv.org/abs/1805.01352v2
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1805.01352v2)
- **Published**: 2018-04-28 06:44:13+00:00
- **Updated**: 2020-06-03 16:55:37+00:00
- **Authors**: Yangfan Hu, Huajin Tang, Gang Pan
- **Comment**: None
- **Journal**: None
- **Summary**: Spiking neural networks (SNNs) have received significant attention for their biological plausibility. SNNs theoretically have at least the same computational power as traditional artificial neural networks (ANNs). They possess potential of achieving energy-efficiency while keeping comparable performance to deep neural networks (DNNs). However, it is still a big challenge to train a very deep SNN. In this paper, we propose an efficient approach to build a spiking version of deep residual network (ResNet). ResNet is considered as a kind of the state-of-the-art convolutional neural networks (CNNs). We employ the idea of converting a trained ResNet to a network of spiking neurons, named Spiking ResNet (S-ResNet). We propose a shortcut conversion model to appropriately scale continuous-valued activations to match firing rates in SNN, and a compensation mechanism to reduce the error caused by discretisation. Experimental results demonstrate that, compared with the state-of-the-art SNN approaches, the proposed Spiking ResNet achieves the best performance on CIFAR-10, CIFAR-100, and ImageNet 2012. Our work is the first time to build a SNN deeper than 40, with comparable performance to ANNs on a large-scale dataset.



### Detect, Quantify, and Incorporate Dataset Bias: A Neuroimaging Analysis on 12,207 Individuals
- **Arxiv ID**: http://arxiv.org/abs/1804.10764v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1804.10764v1)
- **Published**: 2018-04-28 09:11:34+00:00
- **Updated**: 2018-04-28 09:11:34+00:00
- **Authors**: Christian Wachinger, Benjamin Gutierrez Becker, Anna Rieckmann
- **Comment**: None
- **Journal**: None
- **Summary**: Neuroimaging datasets keep growing in size to address increasingly complex medical questions. However, even the largest datasets today alone are too small for training complex models or for finding genome wide associations. A solution is to grow the sample size by merging data across several datasets. However, bias in datasets complicates this approach and includes additional sources of variation in the data instead. In this work, we combine 15 large neuroimaging datasets to study bias. First, we detect bias by demonstrating that scans can be correctly assigned to a dataset with 73.3% accuracy. Next, we introduce metrics to quantify the compatibility across datasets and to create embeddings of neuroimaging sites. Finally, we incorporate the presence of bias for the selection of a training set for predicting autism. For the quantification of the dataset bias, we introduce two metrics: the Bhattacharyya distance between datasets and the age prediction error. The presented embedding of neuroimaging sites provides an interesting new visualization about the similarity of different sites. This could be used to guide the merging of data sources, while limiting the introduction of unwanted variation. Finally, we demonstrate a clear performance increase when incorporating dataset bias for training set selection in autism prediction. Overall, we believe that the growing amount of neuroimaging data necessitates to incorporate data-driven methods for quantifying dataset bias in future analyses.



### Toward Designing Convergent Deep Operator Splitting Methods for Task-specific Nonconvex Optimization
- **Arxiv ID**: http://arxiv.org/abs/1804.10798v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.10798v1)
- **Published**: 2018-04-28 12:59:50+00:00
- **Updated**: 2018-04-28 12:59:50+00:00
- **Authors**: Risheng Liu, Shichao Cheng, Yi He, Xin Fan, Zhongxuan Luo
- **Comment**: None
- **Journal**: None
- **Summary**: Operator splitting methods have been successfully used in computational sciences, statistics, learning and vision areas to reduce complex problems into a series of simpler subproblems. However, prevalent splitting schemes are mostly established only based on the mathematical properties of some general optimization models. So it is a laborious process and often requires many iterations of ideation and validation to obtain practical and task-specific optimal solutions, especially for nonconvex problems in real-world scenarios. To break through the above limits, we introduce a new algorithmic framework, called Learnable Bregman Splitting (LBS), to perform deep-architecture-based operator splitting for nonconvex optimization based on specific task model. Thanks to the data-dependent (i.e., learnable) nature, our LBS can not only speed up the convergence, but also avoid unwanted trivial solutions for real-world tasks. Though with inexact deep iterations, we can still establish the global convergence and estimate the asymptotic convergence rate of LBS only by enforcing some fairly loose assumptions. Extensive experiments on different applications (e.g., image completion and deblurring) verify our theoretical results and show the superiority of LBS against existing methods.



### Remote Detection of Idling Cars Using Infrared Imaging and Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/1804.10805v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.10805v1)
- **Published**: 2018-04-28 14:01:58+00:00
- **Updated**: 2018-04-28 14:01:58+00:00
- **Authors**: Muhammet Bastan, Kim-Hui Yap, Lap-Pui Chau
- **Comment**: Neural Computing and Applications
- **Journal**: None
- **Summary**: Idling vehicles waste energy and pollute the environment through exhaust emission. In some countries, idling a vehicle for more than a predefined duration is prohibited and automatic idling vehicle detection is desirable for law enforcement. We propose the first automatic system to detect idling cars, using infrared (IR) imaging and deep networks.   We rely on the differences in spatio-temporal heat signatures of idling and stopped cars and monitor the car temperature with a long-wavelength IR camera. We formulate the idling car detection problem as spatio-temporal event detection in IR image sequences and employ deep networks for spatio-temporal modeling. We collected the first IR image sequence dataset for idling car detection. First, we detect the cars in each IR image using a convolutional neural network, which is pre-trained on regular RGB images and fine-tuned on IR images for higher accuracy. Then, we track the detected cars over time to identify the cars that are parked. Finally, we use the 3D spatio-temporal IR image volume of each parked car as input to convolutional and recurrent networks to classify them as idling or not. We carried out an extensive empirical evaluation of temporal and spatio-temporal modeling approaches with various convolutional and recurrent architectures. We present promising experimental results on our IR image sequence dataset.



### Learning Cross-Modal Deep Embeddings for Multi-Object Image Retrieval using Text and Sketch
- **Arxiv ID**: http://arxiv.org/abs/1804.10819v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.10819v1)
- **Published**: 2018-04-28 15:23:25+00:00
- **Updated**: 2018-04-28 15:23:25+00:00
- **Authors**: Sounak Dey, Anjan Dutta, Suman K. Ghosh, Ernest Valveny, Josep Lladós, Umapada Pal
- **Comment**: Accepted at ICPR 2018
- **Journal**: None
- **Summary**: In this work we introduce a cross modal image retrieval system that allows both text and sketch as input modalities for the query. A cross-modal deep network architecture is formulated to jointly model the sketch and text input modalities as well as the the image output modality, learning a common embedding between text and images and between sketches and images. In addition, an attention model is used to selectively focus the attention on the different objects of the image, allowing for retrieval with multiple objects in the query. Experiments show that the proposed method performs the best in both single and multiple object image retrieval in standard datasets.



### A Bimodal Learning Approach to Assist Multi-sensory Effects Synchronization
- **Arxiv ID**: http://arxiv.org/abs/1804.10822v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1804.10822v1)
- **Published**: 2018-04-28 15:37:41+00:00
- **Updated**: 2018-04-28 15:37:41+00:00
- **Authors**: Raphael Abreu, Joel dos Santos, Eduardo Bezerra
- **Comment**: None
- **Journal**: None
- **Summary**: In mulsemedia applications, traditional media content (text, image, audio, video, etc.) can be related to media objects that target other human senses (e.g., smell, haptics, taste). Such applications aim at bridging the virtual and real worlds through sensors and actuators. Actuators are responsible for the execution of sensory effects (e.g., wind, heat, light), which produce sensory stimulations on the users. In these applications sensory stimulation must happen in a timely manner regarding the other traditional media content being presented. For example, at the moment in which an explosion is presented in the audiovisual content, it may be adequate to activate actuators that produce heat and light. It is common to use some declarative multimedia authoring language to relate the timestamp in which each media object is to be presented to the execution of some sensory effect. One problem in this setting is that the synchronization of media objects and sensory effects is done manually by the author(s) of the application, a process which is time-consuming and error prone. In this paper, we present a bimodal neural network architecture to assist the synchronization task in mulsemedia applications. Our approach is based on the idea that audio and video signals can be used simultaneously to identify the timestamps in which some sensory effect should be executed. Our learning architecture combines audio and video signals for the prediction of scene components. For evaluation purposes, we construct a dataset based on Google's AudioSet. We provide experiments to validate our bimodal architecture. Our results show that the bimodal approach produces better results when compared to several variants of unimodal architectures.



### CRAM: Clued Recurrent Attention Model
- **Arxiv ID**: http://arxiv.org/abs/1804.10844v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.10844v1)
- **Published**: 2018-04-28 19:27:43+00:00
- **Updated**: 2018-04-28 19:27:43+00:00
- **Authors**: Minki Chung, Sungzoon Cho
- **Comment**: None
- **Journal**: None
- **Summary**: To overcome the poor scalability of convolutional neural network, recurrent attention model(RAM) selectively choose what and where to look on the image. By directing recurrent attention model how to look the image, RAM can be even more successful in that the given clue narrow down the scope of the possible focus zone. In this perspective, this work proposes clued recurrent attention model (CRAM) which add clue or constraint on the RAM better problem solving. CRAM follows encoder-decoder framework, encoder utilizes recurrent attention model with spatial transformer network and decoder which varies depending on the task. To ensure the performance, CRAM tackles two computer vision task. One is the image classification task, with clue given as the binary image saliency which indicates the approximate location of object. The other is the inpainting task, with clue given as binary mask which indicates the occluded part. In both tasks, CRAM shows better performance than existing methods showing the successful extension of RAM.



### Imbalanced Deep Learning by Minority Class Incremental Rectification
- **Arxiv ID**: http://arxiv.org/abs/1804.10851v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.10851v1)
- **Published**: 2018-04-28 22:36:19+00:00
- **Updated**: 2018-04-28 22:36:19+00:00
- **Authors**: Qi Dong, Shaogang Gong, Xiatian Zhu
- **Comment**: Accepted for IEEE Trans. Pattern Analysis and Machine Intelligence
- **Journal**: None
- **Summary**: Model learning from class imbalanced training data is a long-standing and significant challenge for machine learning. In particular, existing deep learning methods consider mostly either class balanced data or moderately imbalanced data in model training, and ignore the challenge of learning from significantly imbalanced training data. To address this problem, we formulate a class imbalanced deep learning model based on batch-wise incremental minority (sparsely sampled) class rectification by hard sample mining in majority (frequently sampled) classes during model training. This model is designed to minimise the dominant effect of majority classes by discovering sparsely sampled boundaries of minority classes in an iterative batch-wise learning process. To that end, we introduce a Class Rectification Loss (CRL) function that can be deployed readily in deep network architectures. Extensive experimental evaluations are conducted on three imbalanced person attribute benchmark datasets (CelebA, X-Domain, DeepFashion) and one balanced object category benchmark dataset (CIFAR-100). These experimental results demonstrate the performance advantages and model scalability of the proposed batch-wise incremental minority class rectification model over the existing state-of-the-art models for addressing the problem of imbalanced data learning.



### Evaluation of Feature Detector-Descriptor for Real Object Matching under Various Conditions of Ilumination and Affine Transformation
- **Arxiv ID**: http://arxiv.org/abs/1804.10855v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.10855v2)
- **Published**: 2018-04-28 23:55:45+00:00
- **Updated**: 2018-05-02 02:24:30+00:00
- **Authors**: Novanto Yudistira, Achmad Ridok, Ali Fauzi
- **Comment**: None
- **Journal**: None
- **Summary**: This study attempts to provide explanations, descriptions and evaluations of some most popular and current combinations of description and descriptor frameworks, namely SIFT, SURF, MSER, and BRISK for keypoint extractors and SIFT, SURF, BRISK, and FREAK for descriptors. Evaluations are made based on the number of matches of keypoints and repeatability in various image variations. It is used as the main parameter to assess how well combinations of algorithms are in matching objects with different variations. There are many papers that describe the comparison of detection and description features to detect objects in images under various conditions, but the combination of algorithms attached to them has not been much discussed. The problem domain is limited to different illumination levels and affine transformations from different perspectives. To evaluate the robustness of all combinations of algorithms, we use a stereo image matching case.



