# Arxiv Papers in cs.CV on 2018-04-12
### Learning Rigidity in Dynamic Scenes with a Moving Camera for 3D Motion Field Estimation
- **Arxiv ID**: http://arxiv.org/abs/1804.04259v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.04259v2)
- **Published**: 2018-04-12 00:01:43+00:00
- **Updated**: 2018-07-30 08:11:45+00:00
- **Authors**: Zhaoyang Lv, Kihwan Kim, Alejandro Troccoli, Deqing Sun, James M. Rehg, Jan Kautz
- **Comment**: This work is accepted at European Conference on Computer Vision 2018.
  Project page (with the video):
  http://research.nvidia.com/publication/2018-09_Learning-Rigidity-in The codes
  will be released at https://github.com/NVlabs/learningrigidity
- **Journal**: None
- **Summary**: Estimation of 3D motion in a dynamic scene from a temporal pair of images is a core task in many scene understanding problems. In real world applications, a dynamic scene is commonly captured by a moving camera (i.e., panning, tilting or hand-held), increasing the task complexity because the scene is observed from different view points. The main challenge is the disambiguation of the camera motion from scene motion, which becomes more difficult as the amount of rigidity observed decreases, even with successful estimation of 2D image correspondences. Compared to other state-of-the-art 3D scene flow estimation methods, in this paper we propose to \emph{learn} the rigidity of a scene in a supervised manner from a large collection of dynamic scene data, and directly infer a rigidity mask from two sequential images with depths. With the learned network, we show how we can effectively estimate camera motion and projected scene flow using computed 2D optical flow and the inferred rigidity mask. For training and testing the rigidity network, we also provide a new semi-synthetic dynamic scene dataset (synthetic foreground objects with a real background) and an evaluation split that accounts for the percentage of observed non-rigid pixels. Through our evaluation we show the proposed framework outperforms current state-of-the-art scene flow estimation methods in challenging dynamic scenes.



### VITAL: VIsual Tracking via Adversarial Learning
- **Arxiv ID**: http://arxiv.org/abs/1804.04273v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.04273v1)
- **Published**: 2018-04-12 01:44:12+00:00
- **Updated**: 2018-04-12 01:44:12+00:00
- **Authors**: Yibing Song, Chao Ma, Xiaohe Wu, Lijun Gong, Linchao Bao, Wangmeng Zuo, Chunhua Shen, Rynson Lau, Ming-Hsuan Yang
- **Comment**: Spotlight in CVPR 2018
- **Journal**: None
- **Summary**: The tracking-by-detection framework consists of two stages, i.e., drawing samples around the target object in the first stage and classifying each sample as the target object or as background in the second stage. The performance of existing trackers using deep classification networks is limited by two aspects. First, the positive samples in each frame are highly spatially overlapped, and they fail to capture rich appearance variations. Second, there exists extreme class imbalance between positive and negative samples. This paper presents the VITAL algorithm to address these two problems via adversarial learning. To augment positive samples, we use a generative network to randomly generate masks, which are applied to adaptively dropout input features to capture a variety of appearance changes. With the use of adversarial learning, our network identifies the mask that maintains the most robust features of the target objects over a long temporal span. In addition, to handle the issue of class imbalance, we propose a high-order cost sensitive loss to decrease the effect of easy negative samples to facilitate training the classification network. Extensive experiments on benchmark datasets demonstrate that the proposed tracker performs favorably against state-of-the-art approaches.



### Clustering via Boundary Erosion
- **Arxiv ID**: http://arxiv.org/abs/1804.04312v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.04312v2)
- **Published**: 2018-04-12 04:39:04+00:00
- **Updated**: 2018-04-13 03:23:59+00:00
- **Authors**: Cheng-Hao Deng, Wan-Lei Zhao
- **Comment**: 10 pages, 6 figures
- **Journal**: None
- **Summary**: Clustering analysis identifies samples as groups based on either their mutual closeness or homogeneity. In order to detect clusters in arbitrary shapes, a novel and generic solution based on boundary erosion is proposed. The clusters are assumed to be separated by relatively sparse regions. The samples are eroded sequentially according to their dynamic boundary densities. The erosion starts from low density regions, invading inwards, until all the samples are eroded out. By this manner, boundaries between different clusters become more and more apparent. It therefore offers a natural and powerful way to separate the clusters when the boundaries between them are hard to be drawn at once. With the sequential order of being eroded, the sequential boundary levels are produced, following which the clusters in arbitrary shapes are automatically reconstructed. As demonstrated across various clustering tasks, it is able to outperform most of the state-of-the-art algorithms and its performance is nearly perfect in some scenarios.



### A Large-scale Attribute Dataset for Zero-shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1804.04314v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.04314v2)
- **Published**: 2018-04-12 04:58:34+00:00
- **Updated**: 2018-05-16 04:13:44+00:00
- **Authors**: Bo Zhao, Yanwei Fu, Rui Liang, Jiahong Wu, Yonggang Wang, Yizhou Wang
- **Comment**: None
- **Journal**: Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR) Workshops, 2019, pp. 0-0
- **Summary**: Zero-Shot Learning (ZSL) has attracted huge research attention over the past few years; it aims to learn the new concepts that have never been seen before. In classical ZSL algorithms, attributes are introduced as the intermediate semantic representation to realize the knowledge transfer from seen classes to unseen classes. Previous ZSL algorithms are tested on several benchmark datasets annotated with attributes. However, these datasets are defective in terms of the image distribution and attribute diversity. In addition, we argue that the "co-occurrence bias problem" of existing datasets, which is caused by the biased co-occurrence of objects, significantly hinders models from correctly learning the concept. To overcome these problems, we propose a Large-scale Attribute Dataset (LAD). Our dataset has 78,017 images of 5 super-classes, 230 classes. The image number of LAD is larger than the sum of the four most popular attribute datasets. 359 attributes of visual, semantic and subjective properties are defined and annotated in instance-level. We analyze our dataset by conducting both supervised learning and zero-shot learning tasks. Seven state-of-the-art ZSL algorithms are tested on this new dataset. The experimental results reveal the challenge of implementing zero-shot learning on our dataset.



### Cross-Modal Retrieval with Implicit Concept Association
- **Arxiv ID**: http://arxiv.org/abs/1804.04318v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1804.04318v2)
- **Published**: 2018-04-12 05:10:33+00:00
- **Updated**: 2018-04-25 16:30:57+00:00
- **Authors**: Yale Song, Mohammad Soleymani
- **Comment**: None
- **Journal**: None
- **Summary**: Traditional cross-modal retrieval assumes explicit association of concepts across modalities, where there is no ambiguity in how the concepts are linked to each other, e.g., when we do the image search with a query "dogs", we expect to see dog images. In this paper, we consider a different setting for cross-modal retrieval where data from different modalities are implicitly linked via concepts that must be inferred by high-level reasoning; we call this setting implicit concept association. To foster future research in this setting, we present a new dataset containing 47K pairs of animated GIFs and sentences crawled from the web, in which the GIFs depict physical or emotional reactions to the scenarios described in the text (called "reaction GIFs"). We report on a user study showing that, despite the presence of implicit concept association, humans are able to identify video-sentence pairs with matching concepts, suggesting the feasibility of our task. Furthermore, we propose a novel visual-semantic embedding network based on multiple instance learning. Unlike traditional approaches, we compute multiple embeddings from each modality, each representing different concepts, and measure their similarity by considering all possible combinations of visual-semantic embeddings in the framework of multiple instance learning. We evaluate our approach on two video-sentence datasets with explicit and implicit concept association and report competitive results compared to existing approaches on cross-modal retrieval.



### STAIR Actions: A Video Dataset of Everyday Home Actions
- **Arxiv ID**: http://arxiv.org/abs/1804.04326v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1804.04326v3)
- **Published**: 2018-04-12 05:48:06+00:00
- **Updated**: 2018-04-16 05:40:42+00:00
- **Authors**: Yuya Yoshikawa, Jiaqing Lin, Akikazu Takeuchi
- **Comment**: STAIR Actions dataset can be downloaded from
  http://actions.stair.center
- **Journal**: None
- **Summary**: A new large-scale video dataset for human action recognition, called STAIR Actions is introduced. STAIR Actions contains 100 categories of action labels representing fine-grained everyday home actions so that it can be applied to research in various home tasks such as nursing, caring, and security. In STAIR Actions, each video has a single action label. Moreover, for each action category, there are around 1,000 videos that were obtained from YouTube or produced by crowdsource workers. The duration of each video is mostly five to six seconds. The total number of videos is 102,462. We explain how we constructed STAIR Actions and show the characteristics of STAIR Actions compared to existing datasets for human action recognition. Experiments with three major models for action recognition show that STAIR Actions can train large models and achieve good performance. STAIR Actions can be downloaded from http://actions.stair.center



### MelanoGANs: High Resolution Skin Lesion Synthesis with GANs
- **Arxiv ID**: http://arxiv.org/abs/1804.04338v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.04338v1)
- **Published**: 2018-04-12 06:18:31+00:00
- **Updated**: 2018-04-12 06:18:31+00:00
- **Authors**: Christoph Baur, Shadi Albarqouni, Nassir Navab
- **Comment**: None
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) have been successfully used to synthesize realistically looking images of faces, scenery and even medical images. Unfortunately, they usually require large training datasets, which are often scarce in the medical field, and to the best of our knowledge GANs have been only applied for medical image synthesis at fairly low resolution. However, many state-of-the-art machine learning models operate on high resolution data as such data carries indispensable, valuable information. In this work, we try to generate realistically looking high resolution images of skin lesions with GANs, using only a small training dataset of 2000 samples. The nature of the data allows us to do a direct comparison between the image statistics of the generated samples and the real dataset. We both quantitatively and qualitatively compare state-of-the-art GAN architectures such as DCGAN and LAPGAN against a modification of the latter for the task of image generation at a resolution of 256x256px. Our investigation shows that we can approximate the real data distribution with all of the models, but we notice major differences when visually rating sample realism, diversity and artifacts. In a set of use-case experiments on skin lesion classification, we further show that we can successfully tackle the problem of heavy class imbalance with the help of synthesized high resolution melanoma samples.



### Benchmark data and method for real-time people counting in cluttered scenes using depth sensors
- **Arxiv ID**: http://arxiv.org/abs/1804.04339v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.04339v2)
- **Published**: 2018-04-12 06:22:31+00:00
- **Updated**: 2018-10-28 07:45:37+00:00
- **Authors**: ShiJie Sun, Naveed Akhtar, HuanSheng Song, ChaoYang Zhang, JianXin Li, Ajmal Mian
- **Comment**: Submitted to a journal
- **Journal**: None
- **Summary**: Vision-based automatic counting of people has widespread applications in intelligent transportation systems, security, and logistics. However, there is currently no large-scale public dataset for benchmarking approaches on this problem. This work fills this gap by introducing the first real-world RGB-D People Counting DataSet (PCDS) containing over 4,500 videos recorded at the entrance doors of buses in normal and cluttered conditions. It also proposes an efficient method for counting people in real-world cluttered scenes related to public transportations using depth videos. The proposed method computes a point cloud from the depth video frame and re-projects it onto the ground plane to normalize the depth information. The resulting depth image is analyzed for identifying potential human heads. The human head proposals are meticulously refined using a 3D human model. The proposals in each frame of the continuous video stream are tracked to trace their trajectories. The trajectories are again refined to ascertain reliable counting. People are eventually counted by accumulating the head trajectories leaving the scene. To enable effective head and trajectory identification, we also propose two different compound features. A thorough evaluation on PCDS demonstrates that our technique is able to count people in cluttered scenes with high accuracy at 45 fps on a 1.7 GHz processor, and hence it can be deployed for effective real-time people counting for intelligent transportation systems.



### Zero-Shot Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1804.04340v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.04340v2)
- **Published**: 2018-04-12 06:23:11+00:00
- **Updated**: 2018-07-27 06:07:37+00:00
- **Authors**: Ankan Bansal, Karan Sikka, Gaurav Sharma, Rama Chellappa, Ajay Divakaran
- **Comment**: 17 pages. ECCV 2018
- **Journal**: None
- **Summary**: We introduce and tackle the problem of zero-shot object detection (ZSD), which aims to detect object classes which are not observed during training. We work with a challenging set of object classes, not restricting ourselves to similar and/or fine-grained categories as in prior works on zero-shot classification. We present a principled approach by first adapting visual-semantic embeddings for ZSD. We then discuss the problems associated with selecting a background class and motivate two background-aware approaches for learning robust detectors. One of these models uses a fixed background class and the other is based on iterative latent assignments. We also outline the challenge associated with using a limited number of training classes and propose a solution based on dense sampling of the semantic label space using auxiliary data with a large number of categories. We propose novel splits of two standard detection datasets - MSCOCO and VisualGenome, and present extensive empirical results in both the traditional and generalized zero-shot settings to highlight the benefits of the proposed methods. We provide useful insights into the algorithm and conclude by posing some open questions to encourage further research.



### A two-stage 3D Unet framework for multi-class segmentation on full resolution image
- **Arxiv ID**: http://arxiv.org/abs/1804.04341v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.04341v1)
- **Published**: 2018-04-12 06:31:58+00:00
- **Updated**: 2018-04-12 06:31:58+00:00
- **Authors**: Chengjia Wang, Tom MacGillivray, Gillian Macnaught, Guang Yang, David Newby
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolutional neural networks (CNNs) have been intensively used for multi-class segmentation of data from different modalities and achieved state-of-the-art performances. However, a common problem when dealing with large, high resolution 3D data is that the volumes input into the deep CNNs has to be either cropped or downsampled due to limited memory capacity of computing devices. These operations lead to loss of resolution and increment of class imbalance in the input data batches, which can downgrade the performances of segmentation algorithms. Inspired by the architecture of image super-resolution CNN (SRCNN) and self-normalization network (SNN), we developed a two-stage modified Unet framework that simultaneously learns to detect a ROI within the full volume and to classify voxels without losing the original resolution. Experiments on a variety of multi-modal volumes demonstrated that, when trained with a simply weighted dice coefficients and our customized learning procedure, this framework shows better segmentation performances than state-of-the-art Deep CNNs with advanced similarity metrics.



### A Recurrent CNN for Automatic Detection and Classification of Coronary Artery Plaque and Stenosis in Coronary CT Angiography
- **Arxiv ID**: http://arxiv.org/abs/1804.04360v4
- **DOI**: 10.1109/TMI.2018.2883807
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.04360v4)
- **Published**: 2018-04-12 07:42:03+00:00
- **Updated**: 2018-12-10 10:38:09+00:00
- **Authors**: Majd Zreik, Robbert W. van Hamersvelt, Jelmer M. Wolterink, Tim Leiner, Max A. Viergever, Ivana Isgum
- **Comment**: Published in IEEE Transactions on Medical Imaging, 2019
- **Journal**: None
- **Summary**: Various types of atherosclerotic plaque and varying grades of stenosis could lead to different management of patients with coronary artery disease. Therefore, it is crucial to detect and classify the type of coronary artery plaque, as well as to detect and determine the degree of coronary artery stenosis. This study includes retrospectively collected clinically obtained coronary CT angiography (CCTA) scans of 163 patients. To perform automatic analysis for coronary artery plaque and stenosis classification, a multi-task recurrent convolutional neural network is applied on multi-planar reformatted (MPR) images of the coronary arteries. First, a 3D convolutional neural network is utilized to extract features along the coronary artery. Subsequently, the extracted features are aggregated by a recurrent neural network that performs two simultaneous multi-class classification tasks. In the first task, the network detects and characterizes the type of the coronary artery plaque (no plaque, non-calcified, mixed, calcified). In the second task, the network detects and determines the anatomical significance of the coronary artery stenosis (no stenosis, non-significant i.e. <50% luminal narrowing, significant i.e. >50% luminal narrowing). For detection and classification of coronary plaque, the method achieved an accuracy of 0.77. For detection and classification of stenosis, the method achieved an accuracy of 0.80. The results demonstrate that automatic detection and classification of coronary artery plaque and stenosis are feasible. This may enable automated triage of patients to those without coronary plaque and those with coronary plaque and stenosis in need for further cardiovascular workup.



### Generative Adversarial Training for MRA Image Synthesis Using Multi-Contrast MRI
- **Arxiv ID**: http://arxiv.org/abs/1804.04366v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.04366v1)
- **Published**: 2018-04-12 08:11:17+00:00
- **Updated**: 2018-04-12 08:11:17+00:00
- **Authors**: Sahin Olut, Yusuf Huseyin Sahin, Ugur Demir, Gozde Unal
- **Comment**: None
- **Journal**: None
- **Summary**: Magnetic Resonance Angiography (MRA) has become an essential MR contrast for imaging and evaluation of vascular anatomy and related diseases. MRA acquisitions are typically ordered for vascular interventions, whereas in typical scenarios, MRA sequences can be absent in the patient scans. This motivates the need for a technique that generates inexistent MRA from existing MR multi-contrast, which could be a valuable tool in retrospective subject evaluations and imaging studies. In this paper, we present a generative adversarial network (GAN) based technique to generate MRA from T1-weighted and T2-weighted MRI images, for the first time to our knowledge. To better model the representation of vessels which the MRA inherently highlights, we design a loss term dedicated to a faithful reproduction of vascularities. To that end, we incorporate steerable filter responses of the generated and reference images inside a Huber function loss term. Extending the well- established generator-discriminator architecture based on the recent PatchGAN model with the addition of steerable filter loss, the proposed steerable GAN (sGAN) method is evaluated on the large public database IXI. Experimental results show that the sGAN outperforms the baseline GAN method in terms of an overlap score with similar PSNR values, while it leads to improved visual perceptual quality.



### Image Correction via Deep Reciprocating HDR Transformation
- **Arxiv ID**: http://arxiv.org/abs/1804.04371v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1804.04371v1)
- **Published**: 2018-04-12 08:23:04+00:00
- **Updated**: 2018-04-12 08:23:04+00:00
- **Authors**: Xin Yang, Ke Xu, Yibing Song, Qiang Zhang, Xiaopeng Wei, Rynson Lau
- **Comment**: in CVPR 2018
- **Journal**: None
- **Summary**: Image correction aims to adjust an input image into a visually pleasing one. Existing approaches are proposed mainly from the perspective of image pixel manipulation. They are not effective to recover the details in the under/over exposed regions. In this paper, we revisit the image formation procedure and notice that the missing details in these regions exist in the corresponding high dynamic range (HDR) data. These details are well perceived by the human eyes but diminished in the low dynamic range (LDR) domain because of the tone mapping process. Therefore, we formulate the image correction task as an HDR transformation process and propose a novel approach called Deep Reciprocating HDR Transformation (DRHT). Given an input LDR image, we first reconstruct the missing details in the HDR domain. We then perform tone mapping on the predicted HDR data to generate the output LDR image with the recovered details. To this end, we propose a united framework consisting of two CNNs for HDR reconstruction and tone mapping. They are integrated end-to-end for joint training and prediction. Experiments on the standard benchmarks demonstrate that the proposed method performs favorably against state-of-the-art image correction methods.



### Blood Vessel Geometry Synthesis using Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1804.04381v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.04381v1)
- **Published**: 2018-04-12 09:08:56+00:00
- **Updated**: 2018-04-12 09:08:56+00:00
- **Authors**: Jelmer M. Wolterink, Tim Leiner, Ivana Isgum
- **Comment**: Submitted to the 1st Conference on Medical Imaging with Deep Learning
  (MIDL2018), Amsterdam, The Netherlands
  (https://openreview.net/forum?id=SJ4N7isiG)
- **Journal**: None
- **Summary**: Computationally synthesized blood vessels can be used for training and evaluation of medical image analysis applications. We propose a deep generative model to synthesize blood vessel geometries, with an application to coronary arteries in cardiac CT angiography (CCTA).   In the proposed method, a Wasserstein generative adversarial network (GAN) consisting of a generator and a discriminator network is trained. While the generator tries to synthesize realistic blood vessel geometries, the discriminator tries to distinguish synthesized geometries from those of real blood vessels. Both real and synthesized blood vessel geometries are parametrized as 1D signals based on the central vessel axis. The generator can optionally be provided with an attribute vector to synthesize vessels with particular characteristics.   The GAN was optimized using a reference database with parametrizations of 4,412 real coronary artery geometries extracted from CCTA scans. After training, plausible coronary artery geometries could be synthesized based on random vectors sampled from a latent space. A qualitative analysis showed strong similarities between real and synthesized coronary arteries. A detailed analysis of the latent space showed that the diversity present in coronary artery anatomy was accurately captured by the generator.   Results show that Wasserstein generative adversarial networks can be used to synthesize blood vessel geometries.



### Iterative fully convolutional neural networks for automatic vertebra segmentation and identification
- **Arxiv ID**: http://arxiv.org/abs/1804.04383v3
- **DOI**: 10.1016/j.media.2019.02.005
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.04383v3)
- **Published**: 2018-04-12 09:10:55+00:00
- **Updated**: 2019-02-11 17:16:50+00:00
- **Authors**: Nikolas Lessmann, Bram van Ginneken, Pim A. de Jong, Ivana Išgum
- **Comment**: Accepted for publication in Medical Image Analysis
- **Journal**: Medical Image Analysis 53, pp. 142-155, 2019
- **Summary**: Precise segmentation and anatomical identification of the vertebrae provides the basis for automatic analysis of the spine, such as detection of vertebral compression fractures or other abnormalities. Most dedicated spine CT and MR scans as well as scans of the chest, abdomen or neck cover only part of the spine. Segmentation and identification should therefore not rely on the visibility of certain vertebrae or a certain number of vertebrae. We propose an iterative instance segmentation approach that uses a fully convolutional neural network to segment and label vertebrae one after the other, independently of the number of visible vertebrae. This instance-by-instance segmentation is enabled by combining the network with a memory component that retains information about already segmented vertebrae. The network iteratively analyzes image patches, using information from both image and memory to search for the next vertebra. To efficiently traverse the image, we include the prior knowledge that the vertebrae are always located next to each other, which is used to follow the vertebral column. This method was evaluated with five diverse datasets, including multiple modalities (CT and MR), various fields of view and coverages of different sections of the spine, and a particularly challenging set of low-dose chest CT scans. The proposed iterative segmentation method compares favorably with state-of-the-art methods and is fast, flexible and generalizable.



### MGGAN: Solving Mode Collapse using Manifold Guided Training
- **Arxiv ID**: http://arxiv.org/abs/1804.04391v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.04391v1)
- **Published**: 2018-04-12 09:27:25+00:00
- **Updated**: 2018-04-12 09:27:25+00:00
- **Authors**: Duhyeon Bang, Hyunjung Shim
- **Comment**: None
- **Journal**: None
- **Summary**: Mode collapse is a critical problem in training generative adversarial networks. To alleviate mode collapse, several recent studies introduce new objective functions, network architectures or alternative training schemes. However, their achievement is often the result of sacrificing the image quality. In this paper, we propose a new algorithm, namely a manifold guided generative adversarial network (MGGAN), which leverages a guidance network on existing GAN architecture to induce generator learning all modes of data distribution. Based on extensive evaluations, we show that our algorithm resolves mode collapse without losing image quality. In particular, we demonstrate that our algorithm is easily extendable to various existing GANs. Experimental analysis justifies that the proposed algorithm is an effective and efficient tool for training GANs.



### Multi-Label Wireless Interference Identification with Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1804.04395v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.04395v1)
- **Published**: 2018-04-12 09:31:32+00:00
- **Updated**: 2018-04-12 09:31:32+00:00
- **Authors**: Sergej Grunau, Dimitri Block, Uwe Meier
- **Comment**: Submitted to the 16th International Conference on Industrial
  Informatics (INDIN 2018)
- **Journal**: None
- **Summary**: The steadily growing use of license-free frequency bands require reliable coexistence management and therefore proper wireless interference identification (WII). In this work, we propose a WII approach based upon a deep convolutional neural network (CNN) which classifies multiple IEEE 802.15.1, IEEE 802.11 b/g and IEEE 802.15.4 interfering signals in the presence of a utilized signal. The generated multi-label dataset contains frequency- and time-limited sensing snapshots with the bandwidth of 10 MHz and duration of 12.8 $\mu$s, respectively. Each snapshot combines one utilized signal with up to multiple interfering signals. The approach shows promising results for same-technology interference with a classification accuracy of approximately 100 % for IEEE 802.15.1 and IEEE 802.15.4 signals. For IEEE 802.11 b/g signals the accuracy increases for cross-technology interference with at least 90 %.



### Social Anchor-Unit Graph Regularized Tensor Completion for Large-Scale Image Retagging
- **Arxiv ID**: http://arxiv.org/abs/1804.04397v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.04397v2)
- **Published**: 2018-04-12 09:40:30+00:00
- **Updated**: 2018-10-03 07:28:41+00:00
- **Authors**: Jinhui Tang, Xiangbo Shu, Zechao Li, Yu-Gang Jiang, Qi Tian
- **Comment**: None
- **Journal**: None
- **Summary**: Image retagging aims to improve tag quality of social images by refining their original tags or assigning new high-quality tags. Recent approaches simultaneously explore visual, user and tag information to improve the performance of image retagging by constructing and exploring an image-tag-user graph. However, such methods will become computationally infeasible with the rapidly increasing number of images, tags and users. It has been proven that Anchor Graph Regularization (AGR) can significantly accelerate large-scale graph learning model by exploring only a small number of anchor points. Inspired by this, we propose a novel Social anchor-Unit GrAph Regularized Tensor Completion (SUGAR-TC) method to effectively refine the tags of social images, which is insensitive to the scale of the applied data. First, we construct an anchor-unit graph across multiple domains (e.g., image and user domains) rather than traditional anchor graph in a single domain. Second, a tensor completion based on SUGAR is implemented on the original image-tag-user tensor to refine the tags of the anchor images. Third, we efficiently assign tags to non-anchor images by leveraging the relationship between the non-anchor images and the anchor units. Experimental results on a real-world social image database well demonstrate the effectiveness of SUGAR-TC, outperforming several related methods.



### Unsupervised Discovery of Object Landmarks as Structural Representations
- **Arxiv ID**: http://arxiv.org/abs/1804.04412v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.04412v1)
- **Published**: 2018-04-12 10:25:41+00:00
- **Updated**: 2018-04-12 10:25:41+00:00
- **Authors**: Yuting Zhang, Yijie Guo, Yixin Jin, Yijun Luo, Zhiyuan He, Honglak Lee
- **Comment**: 48 pages
- **Journal**: CVPR 2018
- **Summary**: Deep neural networks can model images with rich latent representations, but they cannot naturally conceptualize structures of object categories in a human-perceptible way. This paper addresses the problem of learning object structures in an image modeling process without supervision. We propose an autoencoding formulation to discover landmarks as explicit structural representations. The encoding module outputs landmark coordinates, whose validity is ensured by constraints that reflect the necessary properties for landmarks. The decoding module takes the landmarks as a part of the learnable input representations in an end-to-end differentiable framework. Our discovered landmarks are semantically meaningful and more predictive of manually annotated landmarks than those discovered by previous methods. The coordinates of our landmarks are also complementary features to pretrained deep-neural-network representations in recognizing visual attributes. In addition, the proposed method naturally creates an unsupervised, perceptible interface to manipulate object shapes and decode images with controllable structures. The project webpage is at http://ytzhang.net/projects/lmdis-rep



### Transformation on Computer-Generated Facial Image to Avoid Detection by Spoofing Detector
- **Arxiv ID**: http://arxiv.org/abs/1804.04418v1
- **DOI**: 10.1109/ICME.2018.8486579
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.04418v1)
- **Published**: 2018-04-12 10:48:20+00:00
- **Updated**: 2018-04-12 10:48:20+00:00
- **Authors**: Huy H. Nguyen, Ngoc-Dung T. Tieu, Hoang-Quoc Nguyen-Son, Junichi Yamagishi, Isao Echizen
- **Comment**: Accepted to be Published in Proceedings of the IEEE International
  Conference on Multimedia and Expo (ICME) 2018, San Diego, USA
- **Journal**: None
- **Summary**: Making computer-generated (CG) images more difficult to detect is an interesting problem in computer graphics and security. While most approaches focus on the image rendering phase, this paper presents a method based on increasing the naturalness of CG facial images from the perspective of spoofing detectors. The proposed method is implemented using a convolutional neural network (CNN) comprising two autoencoders and a transformer and is trained using a black-box discriminator without gradient information. Over 50% of the transformed CG images were not detected by three state-of-the-art spoofing detectors. This capability raises an alarm regarding the reliability of facial authentication systems, which are becoming widely used in daily life.



### Exploiting feature representations through similarity learning, post-ranking and ranking aggregation for person re-identification
- **Arxiv ID**: http://arxiv.org/abs/1804.04419v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.04419v1)
- **Published**: 2018-04-12 10:49:37+00:00
- **Updated**: 2018-04-12 10:49:37+00:00
- **Authors**: Julio C. S. Jacques Junior, Xavier Baró, Sergio Escalera
- **Comment**: Preprint submitted to Image and Vision Computing
- **Journal**: None
- **Summary**: Person re-identification has received special attention by the human analysis community in the last few years. To address the challenges in this field, many researchers have proposed different strategies, which basically exploit either cross-view invariant features or cross-view robust metrics. In this work, we propose to exploit a post-ranking approach and combine different feature representations through ranking aggregation. Spatial information, which potentially benefits the person matching, is represented using a 2D body model, from which color and texture information are extracted and combined. We also consider background/foreground information, automatically extracted via Deep Decompositional Network, and the usage of Convolutional Neural Network (CNN) features. To describe the matching between images we use the polynomial feature map, also taking into account local and global information. The Discriminant Context Information Analysis based post-ranking approach is used to improve initial ranking lists. Finally, the Stuart ranking aggregation method is employed to combine complementary ranking lists obtained from different feature representations. Experimental results demonstrated that we improve the state-of-the-art on VIPeR and PRID450s datasets, achieving 67.21% and 75.64% on top-1 rank recognition rate, respectively, as well as obtaining competitive results on CUHK01 dataset.



### Extraction of Airways using Graph Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1804.04436v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.04436v1)
- **Published**: 2018-04-12 11:36:57+00:00
- **Updated**: 2018-04-12 11:36:57+00:00
- **Authors**: Raghavendra Selvan, Thomas Kipf, Max Welling, Jesper H. Pedersen, Jens Petersen, Marleen de Bruijne
- **Comment**: Extended Abstract submitted to MIDL, 2018. 3 pages
- **Journal**: None
- **Summary**: We present extraction of tree structures, such as airways, from image data as a graph refinement task. To this end, we propose a graph auto-encoder model that uses an encoder based on graph neural networks (GNNs) to learn embeddings from input node features and a decoder to predict connections between nodes. Performance of the GNN model is compared with mean-field networks in their ability to extract airways from 3D chest CT scans.



### Pooling is neither necessary nor sufficient for appropriate deformation stability in CNNs
- **Arxiv ID**: http://arxiv.org/abs/1804.04438v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1804.04438v2)
- **Published**: 2018-04-12 11:44:05+00:00
- **Updated**: 2018-05-25 13:03:50+00:00
- **Authors**: Avraham Ruderman, Neil C. Rabinowitz, Ari S. Morcos, Daniel Zoran
- **Comment**: NIPS 2018 submission
- **Journal**: None
- **Summary**: Many of our core assumptions about how neural networks operate remain empirically untested. One common assumption is that convolutional neural networks need to be stable to small translations and deformations to solve image recognition tasks. For many years, this stability was baked into CNN architectures by incorporating interleaved pooling layers. Recently, however, interleaved pooling has largely been abandoned. This raises a number of questions: Are our intuitions about deformation stability right at all? Is it important? Is pooling necessary for deformation invariance? If not, how is deformation invariance achieved in its absence? In this work, we rigorously test these questions, and find that deformation stability in convolutional networks is more nuanced than it first appears: (1) Deformation invariance is not a binary property, but rather that different tasks require different degrees of deformation stability at different layers. (2) Deformation stability is not a fixed property of a network and is heavily adjusted over the course of training, largely through the smoothness of the convolutional filters. (3) Interleaved pooling layers are neither necessary nor sufficient for achieving the optimal form of deformation stability for natural image classification. (4) Pooling confers too much deformation stability for image classification at initialization, and during training, networks have to learn to counteract this inductive bias. Together, these findings provide new insights into the role of interleaved pooling and deformation invariance in CNNs, and demonstrate the importance of rigorous empirical testing of even our most basic assumptions about the working of neural networks.



### Distort-and-Recover: Color Enhancement using Deep Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/1804.04450v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.04450v2)
- **Published**: 2018-04-12 11:59:20+00:00
- **Updated**: 2018-04-16 01:48:00+00:00
- **Authors**: Jongchan Park, Joon-Young Lee, Donggeun Yoo, In So Kweon
- **Comment**: Accepted to CVPR 2018
- **Journal**: None
- **Summary**: Learning-based color enhancement approaches typically learn to map from input images to retouched images. Most of existing methods require expensive pairs of input-retouched images or produce results in a non-interpretable way. In this paper, we present a deep reinforcement learning (DRL) based method for color enhancement to explicitly model the step-wise nature of human retouching process. We cast a color enhancement process as a Markov Decision Process where actions are defined as global color adjustment operations. Then we train our agent to learn the optimal global enhancement sequence of the actions. In addition, we present a 'distort-and-recover' training scheme which only requires high-quality reference images for training instead of input and retouched image pairs. Given high-quality reference images, we distort the images' color distribution and form distorted-reference image pairs for training. Through extensive experiments, we show that our method produces decent enhancement results and our DRL approach is more suitable for the 'distort-and-recover' training scheme than previous supervised approaches. Supplementary material and code are available at https://sites.google.com/view/distort-and-recover/



### CubeNet: Equivariance to 3D Rotation and Translation
- **Arxiv ID**: http://arxiv.org/abs/1804.04458v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1804.04458v1)
- **Published**: 2018-04-12 12:14:18+00:00
- **Updated**: 2018-04-12 12:14:18+00:00
- **Authors**: Daniel Worrall, Gabriel Brostow
- **Comment**: Preprint
- **Journal**: None
- **Summary**: 3D Convolutional Neural Networks are sensitive to transformations applied to their input. This is a problem because a voxelized version of a 3D object, and its rotated clone, will look unrelated to each other after passing through to the last layer of a network. Instead, an idealized model would preserve a meaningful representation of the voxelized object, while explaining the pose-difference between the two inputs. An equivariant representation vector has two components: the invariant identity part, and a discernable encoding of the transformation. Models that can't explain pose-differences risk "diluting" the representation, in pursuit of optimizing a classification or regression loss function.   We introduce a Group Convolutional Neural Network with linear equivariance to translations and right angle rotations in three dimensions. We call this network CubeNet, reflecting its cube-like symmetry. By construction, this network helps preserve a 3D shape's global and local signature, as it is transformed through successive layers. We apply this network to a variety of 3D inference problems, achieving state-of-the-art on the ModelNet10 classification challenge, and comparable performance on the ISBI 2012 Connectome Segmentation Benchmark. To the best of our knowledge, this is the first 3D rotation equivariant CNN for voxel representations.



### PCN: Part and Context Information for Pedestrian Detection with CNNs
- **Arxiv ID**: http://arxiv.org/abs/1804.04483v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.04483v1)
- **Published**: 2018-04-12 12:59:59+00:00
- **Updated**: 2018-04-12 12:59:59+00:00
- **Authors**: Shiguang Wang, Jian Cheng, Haijun Liu, Ming Tang
- **Comment**: Accepted by British Machine Vision Conference(BMVC) 2017
- **Journal**: None
- **Summary**: Pedestrian detection has achieved great improvements in recent years, while complex occlusion handling is still one of the most important problems. To take advantage of the body parts and context information for pedestrian detection, we propose the part and context network (PCN) in this work. PCN specially utilizes two branches which detect the pedestrians through body parts semantic and context information, respectively. In the Part Branch, the semantic information of body parts can communicate with each other via recurrent neural networks. In the Context Branch, we adopt a local competition mechanism for adaptive context scale selection. By combining the outputs of all branches, we develop a strong complementary pedestrian detector with a lower miss rate and better localization accuracy, especially for occlusion pedestrian. Comprehensive evaluations on two challenging pedestrian detection datasets (i.e. Caltech and INRIA) well demonstrated the effectiveness of the proposed PCN.



### Deep Autoencoding Models for Unsupervised Anomaly Segmentation in Brain MR Images
- **Arxiv ID**: http://arxiv.org/abs/1804.04488v1
- **DOI**: 10.1007/978-3-030-11723-8_16
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.04488v1)
- **Published**: 2018-04-12 13:13:29+00:00
- **Updated**: 2018-04-12 13:13:29+00:00
- **Authors**: Christoph Baur, Benedikt Wiestler, Shadi Albarqouni, Nassir Navab
- **Comment**: None
- **Journal**: BrainLesion: Glioma, Multiple Sclerosis, Stroke and Traumatic
  Brain Injuries. BrainLes 2018
- **Summary**: Reliably modeling normality and differentiating abnormal appearances from normal cases is a very appealing approach for detecting pathologies in medical images. A plethora of such unsupervised anomaly detection approaches has been made in the medical domain, based on statistical methods, content-based retrieval, clustering and recently also deep learning. Previous approaches towards deep unsupervised anomaly detection model patches of normal anatomy with variants of Autoencoders or GANs, and detect anomalies either as outliers in the learned feature space or from large reconstruction errors. In contrast to these patch-based approaches, we show that deep spatial autoencoding models can be efficiently used to capture normal anatomical variability of entire 2D brain MR images. A variety of experiments on real MR data containing MS lesions corroborates our hypothesis that we can detect and even delineate anomalies in brain MR images by simply comparing input images to their reconstruction. Results show that constraints on the latent space and adversarial training can further improve the segmentation performance over standard deep representation learning.



### Simultaneous Fidelity and Regularization Learning for Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/1804.04522v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.04522v4)
- **Published**: 2018-04-12 14:04:13+00:00
- **Updated**: 2019-07-10 05:16:21+00:00
- **Authors**: Dongwei Ren, Wangmeng Zuo, David Zhang, Lei Zhang, Ming-Hsuan Yang
- **Comment**: The supplementary file is at
  https://csdwren.github.io/papers/sfarl_supp.pdf, and the source code is at
  https://github.com/csdwren/sfarl
- **Journal**: None
- **Summary**: Most existing non-blind restoration methods are based on the assumption that a precise degradation model is known. As the degradation process can only be partially known or inaccurately modeled, images may not be well restored. Rain streak removal and image deconvolution with inaccurate blur kernels are two representative examples of such tasks. For rain streak removal, although an input image can be decomposed into a scene layer and a rain streak layer, there exists no explicit formulation for modeling rain streaks and the composition with scene layer. For blind deconvolution, as estimation error of blur kernel is usually introduced, the subsequent non-blind deconvolution process does not restore the latent image well. In this paper, we propose a principled algorithm within the maximum a posterior framework to tackle image restoration with a partially known or inaccurate degradation model. Specifically, the residual caused by a partially known or inaccurate degradation model is spatially dependent and complexly distributed. With a training set of degraded and ground-truth image pairs, we parameterize and learn the fidelity term for a degradation model in a task-driven manner. Furthermore, the regularization term can also be learned along with the fidelity term, thereby forming a simultaneous fidelity and regularization learning model. Extensive experimental results demonstrate the effectiveness of the proposed model for image deconvolution with inaccurate blur kernels, deconvolution with multiple degradations and rain streak removal.



### SoccerNet: A Scalable Dataset for Action Spotting in Soccer Videos
- **Arxiv ID**: http://arxiv.org/abs/1804.04527v2
- **DOI**: 10.1109/CVPRW.2018.00223
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.04527v2)
- **Published**: 2018-04-12 14:19:50+00:00
- **Updated**: 2018-04-22 13:05:03+00:00
- **Authors**: Silvio Giancola, Mohieddine Amine, Tarek Dghaily, Bernard Ghanem
- **Comment**: CVPR Workshop on Computer Vision in Sports 2018
- **Journal**: None
- **Summary**: In this paper, we introduce SoccerNet, a benchmark for action spotting in soccer videos. The dataset is composed of 500 complete soccer games from six main European leagues, covering three seasons from 2014 to 2017 and a total duration of 764 hours. A total of 6,637 temporal annotations are automatically parsed from online match reports at a one minute resolution for three main classes of events (Goal, Yellow/Red Card, and Substitution). As such, the dataset is easily scalable. These annotations are manually refined to a one second resolution by anchoring them at a single timestamp following well-defined soccer rules. With an average of one event every 6.9 minutes, this dataset focuses on the problem of localizing very sparse events within long videos. We define the task of spotting as finding the anchors of soccer events in a video. Making use of recent developments in the realm of generic action recognition and detection in video, we provide strong baselines for detecting soccer events. We show that our best model for classifying temporal segments of length one minute reaches a mean Average Precision (mAP) of 67.8%. For the spotting task, our baseline reaches an Average-mAP of 49.7% for tolerances $\delta$ ranging from 5 to 60 seconds. Our dataset and models are available at https://silviogiancola.github.io/SoccerNet.



### Seed-Point Based Geometric Partitioning of Nuclei Clumps
- **Arxiv ID**: http://arxiv.org/abs/1804.04549v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.04549v1)
- **Published**: 2018-04-12 14:46:24+00:00
- **Updated**: 2018-04-12 14:46:24+00:00
- **Authors**: James Kapaldo
- **Comment**: None
- **Journal**: None
- **Summary**: When applying automatic analysis of fluorescence or histopathological images of cells, it is necessary to partition, or de-clump, partially overlapping cell nuclei. In this work, I describe a method of partitioning partially overlapping cell nuclei using a seed-point based geometric partitioning. The geometric partitioning creates two different types of cuts, cuts between two boundary vertices and cuts between one boundary vertex and a new vertex introduced to the boundary interior. The cuts are then ranked according to a scoring metric, and the highest scoring cuts are used. This method was tested on a set of 2420 clumps of nuclei and was found to produced better results than current popular analysis software.



### Trajectory Factory: Tracklet Cleaving and Re-connection by Deep Siamese Bi-GRU for Multiple Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/1804.04555v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.04555v1)
- **Published**: 2018-04-12 15:05:55+00:00
- **Updated**: 2018-04-12 15:05:55+00:00
- **Authors**: Cong Ma, Changshui Yang, Fan Yang, Yueqing Zhuang, Ziwei Zhang, Huizhu Jia, Xiaodong Xie
- **Comment**: 6 pages, 5 figures
- **Journal**: None
- **Summary**: Multi-Object Tracking (MOT) is a challenging task in the complex scene such as surveillance and autonomous driving. In this paper, we propose a novel tracklet processing method to cleave and re-connect tracklets on crowd or long-term occlusion by Siamese Bi-Gated Recurrent Unit (GRU). The tracklet generation utilizes object features extracted by CNN and RNN to create the high-confidence tracklet candidates in sparse scenario. Due to mis-tracking in the generation process, the tracklets from different objects are split into several sub-tracklets by a bidirectional GRU. After that, a Siamese GRU based tracklet re-connection method is applied to link the sub-tracklets which belong to the same object to form a whole trajectory. In addition, we extract the tracklet images from existing MOT datasets and propose a novel dataset to train our networks. The proposed dataset contains more than 95160 pedestrian images. It has 793 different persons in it. On average, there are 120 images for each person with positions and sizes. Experimental results demonstrate the advantages of our model over the state-of-the-art methods on MOT16.



### Towards integrating spatial localization in convolutional neural networks for brain image segmentation
- **Arxiv ID**: http://arxiv.org/abs/1804.04563v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1804.04563v1)
- **Published**: 2018-04-12 15:20:48+00:00
- **Updated**: 2018-04-12 15:20:48+00:00
- **Authors**: Pierre-Antoine Ganaye, Michaël Sdika, Hugues Benoit-Cattin
- **Comment**: 5 pages, 2 figures
- **Journal**: None
- **Summary**: Semantic segmentation is an established while rapidly evolving field in medical imaging. In this paper we focus on the segmentation of brain Magnetic Resonance Images (MRI) into cerebral structures using convolutional neural networks (CNN). CNNs achieve good performance by finding effective high dimensional image features describing the patch content only. In this work, we propose different ways to introduce spatial constraints into the network to further reduce prediction inconsistencies.   A patch based CNN architecture was trained, making use of multiple scales to gather contextual information. Spatial constraints were introduced within the CNN through a distance to landmarks feature or through the integration of a probability atlas. We demonstrate experimentally that using spatial information helps to reduce segmentation inconsistencies.



### Deformation Aware Image Compression
- **Arxiv ID**: http://arxiv.org/abs/1804.04593v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.04593v1)
- **Published**: 2018-04-12 16:04:40+00:00
- **Updated**: 2018-04-12 16:04:40+00:00
- **Authors**: Tamar Rott Shaham, Tomer Michaeli
- **Comment**: Conference on Computer Vision and Pattern Recognition (CVPR), 2018
- **Journal**: None
- **Summary**: Lossy compression algorithms aim to compactly encode images in a way which enables to restore them with minimal error. We show that a key limitation of existing algorithms is that they rely on error measures that are extremely sensitive to geometric deformations (e.g. SSD, SSIM). These force the encoder to invest many bits in describing the exact geometry of every fine detail in the image, which is obviously wasteful, because the human visual system is indifferent to small local translations. Motivated by this observation, we propose a deformation-insensitive error measure that can be easily incorporated into any existing compression scheme. As we show, optimal compression under our criterion involves slightly deforming the input image such that it becomes more "compressible". Surprisingly, while these small deformations are barely noticeable, they enable the CODEC to preserve details that are otherwise completely lost. Our technique uses the CODEC as a "black box", thus allowing simple integration with arbitrary compression methods. Extensive experiments, including user studies, confirm that our approach significantly improves the visual quality of many CODECs. These include JPEG, JPEG2000, WebP, BPG, and a recent deep-net method.



### Pix3D: Dataset and Methods for Single-Image 3D Shape Modeling
- **Arxiv ID**: http://arxiv.org/abs/1804.04610v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1804.04610v1)
- **Published**: 2018-04-12 16:30:39+00:00
- **Updated**: 2018-04-12 16:30:39+00:00
- **Authors**: Xingyuan Sun, Jiajun Wu, Xiuming Zhang, Zhoutong Zhang, Chengkai Zhang, Tianfan Xue, Joshua B. Tenenbaum, William T. Freeman
- **Comment**: CVPR 2018. The first two authors contributed equally to this work.
  Project page: http://pix3d.csail.mit.edu
- **Journal**: None
- **Summary**: We study 3D shape modeling from a single image and make contributions to it in three aspects. First, we present Pix3D, a large-scale benchmark of diverse image-shape pairs with pixel-level 2D-3D alignment. Pix3D has wide applications in shape-related tasks including reconstruction, retrieval, viewpoint estimation, etc. Building such a large-scale dataset, however, is highly challenging; existing datasets either contain only synthetic data, or lack precise alignment between 2D images and 3D shapes, or only have a small number of images. Second, we calibrate the evaluation criteria for 3D shape reconstruction through behavioral studies, and use them to objectively and systematically benchmark cutting-edge reconstruction algorithms on Pix3D. Third, we design a novel model that simultaneously performs 3D reconstruction and pose estimation; our multi-task learning approach achieves state-of-the-art performance on both tasks.



### An efficient CNN for spectral reconstruction from RGB images
- **Arxiv ID**: http://arxiv.org/abs/1804.04647v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.04647v1)
- **Published**: 2018-04-12 17:48:05+00:00
- **Updated**: 2018-04-12 17:48:05+00:00
- **Authors**: Yigit Baran Can, Radu Timofte
- **Comment**: Submitted to ICIP 2018
- **Journal**: None
- **Summary**: Recently, the example-based single image spectral reconstruction from RGB images task, aka, spectral super-resolution was approached by means of deep learning by Galliani et al. The proposed very deep convolutional neural network (CNN) achieved superior performance on recent large benchmarks. However, Aeschbacher et al showed that comparable performance can be achieved by shallow learning method based on A+, a method introduced for image super-resolution by Timofte et al. In this paper, we propose a moderately deep CNN model and substantially improve the reported performance on three spectral reconstruction standard benchmarks: ICVL, CAVE, and NUS.



### Cross-Domain Visual Recognition via Domain Adaptive Dictionary Learning
- **Arxiv ID**: http://arxiv.org/abs/1804.04687v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.04687v2)
- **Published**: 2018-04-12 18:48:17+00:00
- **Updated**: 2018-04-16 03:05:33+00:00
- **Authors**: Hongyu Xu, Jingjing Zheng, Azadeh Alavi, Rama Chellappa
- **Comment**: Submitted to IEEE TIP Journal
- **Journal**: None
- **Summary**: In real-world visual recognition problems, the assumption that the training data (source domain) and test data (target domain) are sampled from the same distribution is often violated. This is known as the domain adaptation problem. In this work, we propose a novel domain-adaptive dictionary learning framework for cross-domain visual recognition. Our method generates a set of intermediate domains. These intermediate domains form a smooth path and bridge the gap between the source and target domains. Specifically, we not only learn a common dictionary to encode the domain-shared features, but also learn a set of domain-specific dictionaries to model the domain shift. The separation of the common and domain-specific dictionaries enables us to learn more compact and reconstructive dictionaries for domain adaptation. These dictionaries are learned by alternating between domain-adaptive sparse coding and dictionary updating steps. Meanwhile, our approach gradually recovers the feature representations of both source and target data along the domain path. By aligning all the recovered domain data, we derive the final domain-adaptive features for cross-domain visual recognition. Extensive experiments on three public datasets demonstrates that our approach outperforms most state-of-the-art methods.



### A Variational U-Net for Conditional Appearance and Shape Generation
- **Arxiv ID**: http://arxiv.org/abs/1804.04694v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.04694v1)
- **Published**: 2018-04-12 19:05:57+00:00
- **Updated**: 2018-04-12 19:05:57+00:00
- **Authors**: Patrick Esser, Ekaterina Sutter, Björn Ommer
- **Comment**: CVPR 2018 (Spotlight). Project Page at
  https://compvis.github.io/vunet/
- **Journal**: None
- **Summary**: Deep generative models have demonstrated great performance in image synthesis. However, results deteriorate in case of spatial deformations, since they generate images of objects directly, rather than modeling the intricate interplay of their inherent shape and appearance. We present a conditional U-Net for shape-guided image generation, conditioned on the output of a variational autoencoder for appearance. The approach is trained end-to-end on images, without requiring samples of the same object with varying pose or appearance. Experiments show that the model enables conditional image generation and transfer. Therefore, either shape or appearance can be retained from a query image, while freely altering the other. Moreover, appearance can be sampled due to its stochastic latent representation, while preserving shape. In quantitative and qualitative experiments on COCO, DeepFashion, shoes, Market-1501 and handbags, the approach demonstrates significant improvements over the state-of-the-art.



### An Universal Image Attractiveness Ranking Framework
- **Arxiv ID**: http://arxiv.org/abs/1805.00309v3
- **DOI**: 10.1109/WACV.2019.00075
- **Categories**: **cs.CV**, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1805.00309v3)
- **Published**: 2018-04-12 21:10:37+00:00
- **Updated**: 2019-01-14 06:34:48+00:00
- **Authors**: Ning Ma, Alexey Volkov, Aleksandr Livshits, Pawel Pietrusinski, Houdong Hu, Mark Bolin
- **Comment**: Accepted by 2019 Winter Conference on Application of Computer Vision
  (WACV)
- **Journal**: 2019 IEEE Winter Conference on Applications of Computer Vision
  (WACV)
- **Summary**: We propose a new framework to rank image attractiveness using a novel pairwise deep network trained with a large set of side-by-side multi-labeled image pairs from a web image index. The judges only provide relative ranking between two images without the need to directly assign an absolute score, or rate any predefined image attribute, thus making the rating more intuitive and accurate. We investigate a deep attractiveness rank net (DARN), a combination of deep convolutional neural network and rank net, to directly learn an attractiveness score mean and variance for each image and the underlying criteria the judges use to label each pair. The extension of this model (DARN-V2) is able to adapt to individual judge's personal preference. We also show the attractiveness of search results are significantly improved by using this attractiveness information in a real commercial search engine. We evaluate our model against other state-of-the-art models on our side-by-side web test data and another public aesthetic data set. With much less judgments (1M vs 50M), our model outperforms on side-by-side labeled data, and is comparable on data labeled by absolute score.



### Multimodal Unsupervised Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/1804.04732v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1804.04732v2)
- **Published**: 2018-04-12 21:17:54+00:00
- **Updated**: 2018-08-14 18:44:12+00:00
- **Authors**: Xun Huang, Ming-Yu Liu, Serge Belongie, Jan Kautz
- **Comment**: Accepted by ECCV 2018
- **Journal**: None
- **Summary**: Unsupervised image-to-image translation is an important and challenging problem in computer vision. Given an image in the source domain, the goal is to learn the conditional distribution of corresponding images in the target domain, without seeing any pairs of corresponding images. While this conditional distribution is inherently multimodal, existing approaches make an overly simplified assumption, modeling it as a deterministic one-to-one mapping. As a result, they fail to generate diverse outputs from a given source domain image. To address this limitation, we propose a Multimodal Unsupervised Image-to-image Translation (MUNIT) framework. We assume that the image representation can be decomposed into a content code that is domain-invariant, and a style code that captures domain-specific properties. To translate an image to another domain, we recombine its content code with a random style code sampled from the style space of the target domain. We analyze the proposed framework and establish several theoretical results. Extensive experiments with comparisons to the state-of-the-art approaches further demonstrates the advantage of the proposed framework. Moreover, our framework allows users to control the style of translation outputs by providing an example style image. Code and pretrained models are available at https://github.com/nvlabs/MUNIT



