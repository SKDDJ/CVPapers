# Arxiv Papers in cs.CV on 2018-04-30
### Cycle-Consistent Deep Generative Hashing for Cross-Modal Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1804.11013v2
- **DOI**: 10.1109/TIP.2018.2878970
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.11013v2)
- **Published**: 2018-04-30 01:28:20+00:00
- **Updated**: 2018-10-29 03:01:33+00:00
- **Authors**: Lin Wu, Yang Wang, Ling Shao
- **Comment**: To appeared on IEEE Trans. Image Processing. arXiv admin note: text
  overlap with arXiv:1703.10593 by other authors
- **Journal**: None
- **Summary**: In this paper, we propose a novel deep generative approach to cross-modal retrieval to learn hash functions in the absence of paired training samples through the cycle consistency loss. Our proposed approach employs adversarial training scheme to lean a couple of hash functions enabling translation between modalities while assuming the underlying semantic relationship. To induce the hash codes with semantics to the input-output pair, cycle consistency loss is further proposed upon the adversarial training to strengthen the correlations between inputs and corresponding outputs. Our approach is generative to learn hash functions such that the learned hash codes can maximally correlate each input-output correspondence, meanwhile can also regenerate the inputs so as to minimize the information loss. The learning to hash embedding is thus performed to jointly optimize the parameters of the hash functions across modalities as well as the associated generative models. Extensive experiments on a variety of large-scale cross-modal data sets demonstrate that our proposed method achieves better retrieval results than the state-of-the-arts.



### Adversarial Image Registration with Application for MR and TRUS Image Fusion
- **Arxiv ID**: http://arxiv.org/abs/1804.11024v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.11024v2)
- **Published**: 2018-04-30 02:12:57+00:00
- **Updated**: 2018-10-01 22:04:36+00:00
- **Authors**: Pingkun Yan, Sheng Xu, Ardeshir R. Rastinehad, Brad J. Wood
- **Comment**: Presented at the workshop on MLMI 2018, LNCS, volume 11046, pages 197
  to 204
- **Journal**: None
- **Summary**: Robust and accurate alignment of multimodal medical images is a very challenging task, which however is very useful for many clinical applications. For example, magnetic resonance (MR) and transrectal ultrasound (TRUS) image registration is a critical component in MR-TRUS fusion guided prostate interventions. However, due to the huge difference between the image appearances and the large variation in image correspondence, MR-TRUS image registration is a very challenging problem. In this paper, an adversarial image registration (AIR) framework is proposed. By training two deep neural networks simultaneously, one being a generator and the other being a discriminator, we can obtain not only a network for image registration, but also a metric network which can help evaluate the quality of image registration. The developed AIR-net is then evaluated using clinical datasets acquired through image-fusion guided prostate biopsy procedures and promising results are demonstrated.



### Deep Co-attention based Comparators For Relative Representation Learning in Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1804.11027v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.11027v1)
- **Published**: 2018-04-30 02:35:46+00:00
- **Updated**: 2018-04-30 02:35:46+00:00
- **Authors**: Lin Wu, Yang Wang, Junbin Gao, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: Person re-identification (re-ID) requires rapid, flexible yet discriminant representations to quickly generalize to unseen observations on-the-fly and recognize the same identity across disjoint camera views. Recent effective methods are developed in a pair-wise similarity learning system to detect a fixed set of features from distinct regions which are mapped to their vector embeddings for the distance measuring. However, the most relevant and crucial parts of each image are detected independently without referring to the dependency conditioned on one and another. Also, these region based methods rely on spatial manipulation to position the local features in comparable similarity measuring. To combat these limitations, in this paper we introduce the Deep Co-attention based Comparators (DCCs) that fuse the co-dependent representations of the paired images so as to focus on the relevant parts of both images and produce their \textit{relative representations}. Given a pair of pedestrian images to be compared, the proposed model mimics the foveation of human eyes to detect distinct regions concurrent on both images, namely co-dependent features, and alternatively attend to relevant regions to fuse them into the similarity learning. Our comparator is capable of producing dynamic representations relative to a particular sample every time, and thus well-suited to the case of re-identifying pedestrians on-the-fly. We perform extensive experiments to provide the insights and demonstrate the effectiveness of the proposed DCCs in person re-ID. Moreover, our approach has achieved the state-of-the-art performance on three benchmark data sets: DukeMTMC-reID \cite{DukeMTMC}, CUHK03 \cite{FPNN}, and Market-1501 \cite{Market1501}.



### Towards Deeper Generative Architectures for GANs using Dense connections
- **Arxiv ID**: http://arxiv.org/abs/1804.11031v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.11031v2)
- **Published**: 2018-04-30 02:45:12+00:00
- **Updated**: 2018-11-12 08:49:08+00:00
- **Authors**: Samarth Tripathi, Renbo Tu
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present the result of adopting skip connections and dense layers, previously used in image classification tasks, in the Fisher GAN implementation. We have experimented with different numbers of layers and inserting these connections in different sections of the network. Our findings suggests that networks implemented with the connections produce better images than the baseline, and the number of connections added has only slight effect on the result.



### Machine Learning for Exam Triage
- **Arxiv ID**: http://arxiv.org/abs/1805.00503v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1805.00503v1)
- **Published**: 2018-04-30 03:49:22+00:00
- **Updated**: 2018-04-30 03:49:22+00:00
- **Authors**: Xinyu Guan, Jessica Lee, Peter Wu, Yue Wu
- **Comment**: None
- **Journal**: None
- **Summary**: In this project, we extend the state-of-the-art CheXNet (Rajpurkar et al. [2017]) by making use of the additional non-image features in the dataset. Our model produced better AUROC scores than the original CheXNet.



### OMG - Emotion Challenge Solution
- **Arxiv ID**: http://arxiv.org/abs/1805.00348v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.SD, eess.AS, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.00348v1)
- **Published**: 2018-04-30 10:50:30+00:00
- **Updated**: 2018-04-30 10:50:30+00:00
- **Authors**: Yuqi Cui, Xiao Zhang, Yang Wang, Chenfeng Guo, Dongrui Wu
- **Comment**: None
- **Journal**: None
- **Summary**: This short paper describes our solution to the 2018 IEEE World Congress on Computational Intelligence One-Minute Gradual-Emotional Behavior Challenge, whose goal was to estimate continuous arousal and valence values from short videos. We designed four base regression models using visual and audio features, and then used a spectral approach to fuse them to obtain improved performance.



### Investigations on End-to-End Audiovisual Fusion
- **Arxiv ID**: http://arxiv.org/abs/1804.11127v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1804.11127v1)
- **Published**: 2018-04-30 11:27:26+00:00
- **Updated**: 2018-04-30 11:27:26+00:00
- **Authors**: Michael Wand, Ngoc Thang Vu, Juergen Schmidhuber
- **Comment**: Published at ICASSP 2018
- **Journal**: Proceedings of the 2018 IEEE International Conference on
  Acoustics, Speech and Signal Processing, pages 3041 - 3045
- **Summary**: Audiovisual speech recognition (AVSR) is a method to alleviate the adverse effect of noise in the acoustic signal. Leveraging recent developments in deep neural network-based speech recognition, we present an AVSR neural network architecture which is trained end-to-end, without the need to separately model the process of decision fusion as in conventional (e.g. HMM-based) systems. The fusion system outperforms single-modality recognition under all noise conditions. Investigation of the saliency of the input features shows that the neural network automatically adapts to different noise levels in the acoustic signal.



### Evolution of Visual Odometry Techniques
- **Arxiv ID**: http://arxiv.org/abs/1804.11142v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.11142v1)
- **Published**: 2018-04-30 12:05:00+00:00
- **Updated**: 2018-04-30 12:05:00+00:00
- **Authors**: Shashi Poddar, Rahul Kottath, Vinod Karar
- **Comment**: 12 pages, 0 figure
- **Journal**: None
- **Summary**: With rapid advancements in the area of mobile robotics and industrial automation, a growing need has arisen towards accurate navigation and localization of moving objects. Camera based motion estimation is one such technique which is gaining huge popularity owing to its simplicity and use of limited resources in generating motion path. In this paper, an attempt is made to introduce this topic for beginners covering different aspects of vision based motion estimation task. The evolution of VO schemes over last few decades is discussed under two broad categories, that is, geometric and non-geometric approaches. The geometric approaches are further detailed under three different classes, that is, feature-based, appearance-based, and a hybrid of feature and appearance based schemes. The non-geometric approach is one of the recent paradigm shift from conventional pose estimation technique and is thus discussed in a separate section. Towards the end, a list of different datasets for visual odometry and allied research areas are provided for a ready reference.



### Cross-Modal Retrieval in the Cooking Context: Learning Semantic Text-Image Embeddings
- **Arxiv ID**: http://arxiv.org/abs/1804.11146v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1804.11146v1)
- **Published**: 2018-04-30 12:14:32+00:00
- **Updated**: 2018-04-30 12:14:32+00:00
- **Authors**: Micael Carvalho, Rémi Cadène, David Picard, Laure Soulier, Nicolas Thome, Matthieu Cord
- **Comment**: accepted at the 41st International ACM SIGIR Conference on Research
  and Development in Information Retrieval, 2018
- **Journal**: None
- **Summary**: Designing powerful tools that support cooking activities has rapidly gained popularity due to the massive amounts of available data, as well as recent advances in machine learning that are capable of analyzing them. In this paper, we propose a cross-modal retrieval model aligning visual and textual data (like pictures of dishes and their recipes) in a shared representation space. We describe an effective learning scheme, capable of tackling large-scale problems, and validate it on the Recipe1M dataset containing nearly 1 million picture-recipe pairs. We show the effectiveness of our approach regarding previous state-of-the-art models and present qualitative results over computational cooking use cases.



### Learning Explicit Deep Representations from Deep Kernel Networks
- **Arxiv ID**: http://arxiv.org/abs/1804.11159v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.11159v1)
- **Published**: 2018-04-30 12:42:02+00:00
- **Updated**: 2018-04-30 12:42:02+00:00
- **Authors**: Mingyuan Jiu, Hichem Sahbi
- **Comment**: None
- **Journal**: None
- **Summary**: Deep kernel learning aims at designing nonlinear combinations of multiple standard elementary kernels by training deep networks. This scheme has proven to be effective, but intractable when handling large-scale datasets especially when the depth of the trained networks increases; indeed, the complexity of evaluating these networks scales quadratically w.r.t. the size of training data and linearly w.r.t. the depth of the trained networks. In this paper, we address the issue of efficient computation in Deep Kernel Networks (DKNs) by designing effective maps in the underlying Reproducing Kernel Hilbert Spaces. Given a pretrained DKN, our method builds its associated Deep Map Network (DMN) whose inner product approximates the original network while being far more efficient. The design principle of our method is greedy and achieved layer-wise, by finding maps that approximate DKNs at different (input, intermediate and output) layers. This design also considers an extra fine-tuning step based on unsupervised learning, that further enhances the generalization ability of the trained DMNs. When plugged into SVMs, these DMNs turn out to be as accurate as the underlying DKNs while being at least an order of magnitude faster on large-scale datasets, as shown through extensive experiments on the challenging ImageCLEF and COREL5k benchmarks.



### Sketch-a-Classifier: Sketch-based Photo Classifier Generation
- **Arxiv ID**: http://arxiv.org/abs/1804.11182v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.11182v1)
- **Published**: 2018-04-30 13:38:31+00:00
- **Updated**: 2018-04-30 13:38:31+00:00
- **Authors**: Conghui Hu, Da Li, Yi-Zhe Song, Tao Xiang, Timothy M. Hospedales
- **Comment**: published in CVPR2018 as spotlight
- **Journal**: None
- **Summary**: Contemporary deep learning techniques have made image recognition a reasonably reliable technology. However training effective photo classifiers typically takes numerous examples which limits image recognition's scalability and applicability to scenarios where images may not be available. This has motivated investigation into zero-shot learning, which addresses the issue via knowledge transfer from other modalities such as text. In this paper we investigate an alternative approach of synthesizing image classifiers: almost directly from a user's imagination, via free-hand sketch. This approach doesn't require the category to be nameable or describable via attributes as per zero-shot learning. We achieve this via training a {model regression} network to map from {free-hand sketch} space to the space of photo classifiers. It turns out that this mapping can be learned in a category-agnostic way, allowing photo classifiers for new categories to be synthesized by user with no need for annotated training photos. {We also demonstrate that this modality of classifier generation can also be used to enhance the granularity of an existing photo classifier, or as a complement to name-based zero-shot learning.



### How convolutional neural network see the world - A survey of convolutional neural network visualization methods
- **Arxiv ID**: http://arxiv.org/abs/1804.11191v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.11191v2)
- **Published**: 2018-04-30 13:47:11+00:00
- **Updated**: 2018-05-31 20:12:43+00:00
- **Authors**: Zhuwei Qin, Fuxun Yu, Chenchen Liu, Xiang Chen
- **Comment**: 32 pages, 21 figures. Mathematical Foundations of Computing
- **Journal**: None
- **Summary**: Nowadays, the Convolutional Neural Networks (CNNs) have achieved impressive performance on many computer vision related tasks, such as object detection, image recognition, image retrieval, etc. These achievements benefit from the CNNs outstanding capability to learn the input features with deep layers of neuron structures and iterative training process. However, these learned features are hard to identify and interpret from a human vision perspective, causing a lack of understanding of the CNNs internal working mechanism. To improve the CNN interpretability, the CNN visualization is well utilized as a qualitative analysis method, which translates the internal features into visually perceptible patterns. And many CNN visualization works have been proposed in the literature to interpret the CNN in perspectives of network structure, operation, and semantic concept. In this paper, we expect to provide a comprehensive survey of several representative CNN visualization methods, including Activation Maximization, Network Inversion, Deconvolutional Neural Networks (DeconvNet), and Network Dissection based visualization. These methods are presented in terms of motivations, algorithms, and experiment results. Based on these visualization methods, we also discuss their practical applications to demonstrate the significance of the CNN interpretability in areas of network design, optimization, security enhancement, etc.



### An Anti-fraud System for Car Insurance Claim Based on Visual Evidence
- **Arxiv ID**: http://arxiv.org/abs/1804.11207v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.11207v1)
- **Published**: 2018-04-30 14:03:22+00:00
- **Updated**: 2018-04-30 14:03:22+00:00
- **Authors**: Pei Li, Bingyu Shen, Weishan Dong
- **Comment**: None
- **Journal**: None
- **Summary**: Automatically scene understanding using machine learning algorithms has been widely applied to different industries to reduce the cost of manual labor. Nowadays, insurance companies launch express vehicle insurance claim and settlement by allowing customers uploading pictures taken by mobile devices. This kind of insurance claim is treated as small claim and can be processed either manually or automatically in a quick fashion. However, due to the increasing amount of claims every day, system or people are likely to be fooled by repeated claims for identical case leading to big lost to insurance companies.Thus, an anti-fraud checking before processing the claim is necessary. We create the first data set of car damage images collected from internet and local parking lots. In addition, we proposed an approach to generate robust deep features by locating the damages accurately and efficiently in the images. The state-of-the-art real-time object detector YOLO \cite{redmon2016you}is modified to train and discover damage region as an important part of the pipeline. Both local and global deep features are extracted using VGG model\cite{Simonyan14c}, which are fused later for more robust system performance. Experiments show our approach is effective in preventing fraud claims as well as meet the requirement to speed up the insurance claim prepossessing.



### Decoupling Respiratory and Angular Variation in Rotational X-ray Scans Using a Prior Bilinear Model
- **Arxiv ID**: http://arxiv.org/abs/1804.11227v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.11227v3)
- **Published**: 2018-04-30 14:26:15+00:00
- **Updated**: 2018-11-05 09:23:29+00:00
- **Authors**: Tobias Geimer, Paul Keall, Katharina Breininger, Vincent Caillet, Michelle Dunbar, Christoph Bert, Andreas Maier
- **Comment**: None
- **Journal**: None
- **Summary**: Data-driven respiratory signal extraction from rotational X-ray scans is a challenge as angular effects overlap with respiration-induced change in the scene. In this paper, we use the linearity of the X-ray transform to propose a bilinear model based on a prior 4D scan to separate angular and respiratory variation. The bilinear estimation process is supported by a B-spline interpolation using prior knowledge about the trajectory angle. Consequently, extraction of respiratory features simplifies to a linear problem. Though the need for a prior 4D CT seems steep, our proposed use-case of driving a respiratory motion model in radiation therapy usually meets this requirement. We evaluate on DRRs of 5 patient 4D CTs in a leave-one-phase-out manner and achieve a mean estimation error of 3.01 % in the gray values for unseen viewing angles. We further demonstrate suitability of the extracted weights to drive a motion model for treatments with a continuously rotating gantry.



### Dilated Temporal Relational Adversarial Network for Generic Video Summarization
- **Arxiv ID**: http://arxiv.org/abs/1804.11228v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.11228v2)
- **Published**: 2018-04-30 14:27:24+00:00
- **Updated**: 2019-09-16 02:09:50+00:00
- **Authors**: Yujia Zhang, Michael Kampffmeyer, Xiaodan Liang, Dingwen Zhang, Min Tan, Eric P. Xing
- **Comment**: None
- **Journal**: None
- **Summary**: The large amount of videos popping up every day, make it more and more critical that key information within videos can be extracted and understood in a very short time. Video summarization, the task of finding the smallest subset of frames, which still conveys the whole story of a given video, is thus of great significance to improve efficiency of video understanding. We propose a novel Dilated Temporal Relational Generative Adversarial Network (DTR-GAN) to achieve frame-level video summarization. Given a video, it selects the set of key frames, which contain the most meaningful and compact information. Specifically, DTR-GAN learns a dilated temporal relational generator and a discriminator with three-player loss in an adversarial manner. A new dilated temporal relation (DTR) unit is introduced to enhance temporal representation capturing. The generator uses this unit to effectively exploit global multi-scale temporal context to select key frames and to complement the commonly used Bi-LSTM. To ensure that summaries capture enough key video representation from a global perspective rather than a trivial randomly shorten sequence, we present a discriminator that learns to enforce both the information completeness and compactness of summaries via a three-player loss. The loss includes the generated summary loss, the random summary loss, and the real summary (ground-truth) loss, which play important roles for better regularizing the learned model to obtain useful summaries. Comprehensive experiments on three public datasets show the effectiveness of the proposed approach.



### On the Feasibility of Real-Time 3D Hand Tracking using Edge GPGPU Acceleration
- **Arxiv ID**: http://arxiv.org/abs/1804.11256v1
- **DOI**: None
- **Categories**: **cs.DC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1804.11256v1)
- **Published**: 2018-04-30 15:00:40+00:00
- **Updated**: 2018-04-30 15:00:40+00:00
- **Authors**: Ammar Qammaz, Sokol Kosta, Nikolaos Kyriazis, Antonis Argyros
- **Comment**: 6 pages, 5 figures
- **Journal**: None
- **Summary**: This paper presents the case study of a non-intrusive porting of a monolithic C++ library for real-time 3D hand tracking, to the domain of edge-based computation. Towards a proof of concept, the case study considers a pair of workstations, a computationally powerful and a computationally weak one. By wrapping the C++ library in Java container and by capitalizing on a Java-based offloading infrastructure that supports both CPU and GPGPU computations, we are able to establish automatically the required server-client workflow that best addresses the resource allocation problem in the effort to execute from the weak workstation. As a result, the weak workstation can perform well at the task, despite lacking the sufficient hardware to do the required computations locally. This is achieved by offloading computations which rely on GPGPU, to the powerful workstation, across the network that connects them. We show the edge-based computation challenges associated with the information flow of the ported algorithm, demonstrate how we cope with them, and identify what needs to be improved for achieving even better performance.



### 4D Temporally Coherent Light-field Video
- **Arxiv ID**: http://arxiv.org/abs/1804.11276v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.11276v1)
- **Published**: 2018-04-30 15:33:13+00:00
- **Updated**: 2018-04-30 15:33:13+00:00
- **Authors**: Armin Mustafa, Marco Volino, Jean-yves Guillemaut, Adrian Hilton
- **Comment**: Published in 3D Vision (3DV) 2017
- **Journal**: None
- **Summary**: Light-field video has recently been used in virtual and augmented reality applications to increase realism and immersion. However, existing light-field methods are generally limited to static scenes due to the requirement to acquire a dense scene representation. The large amount of data and the absence of methods to infer temporal coherence pose major challenges in storage, compression and editing compared to conventional video. In this paper, we propose the first method to extract a spatio-temporally coherent light-field video representation. A novel method to obtain Epipolar Plane Images (EPIs) from a spare light-field camera array is proposed. EPIs are used to constrain scene flow estimation to obtain 4D temporally coherent representations of dynamic light-fields. Temporal coherence is achieved on a variety of light-field datasets. Evaluation of the proposed light-field scene flow against existing multi-view dense correspondence approaches demonstrates a significant improvement in accuracy of temporal coherence.



### Stack-U-Net: Refinement Network for Image Segmentation on the Example of Optic Disc and Cup
- **Arxiv ID**: http://arxiv.org/abs/1804.11294v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.11294v2)
- **Published**: 2018-04-30 16:15:36+00:00
- **Updated**: 2018-11-21 12:28:07+00:00
- **Authors**: Artem Sevastopolsky, Stepan Drapak, Konstantin Kiselev, Blake M. Snyder, Jeremy D. Keenan, Anastasia Georgievskaya
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we propose a special cascade network for image segmentation, which is based on the U-Net networks as building blocks and the idea of the iterative refinement. The model was mainly applied to achieve higher recognition quality for the task of finding borders of the optic disc and cup, which are relevant to the presence of glaucoma. Compared to a single U-Net and the state-of-the-art methods for the investigated tasks, very high segmentation quality has been achieved without a need for increasing the volume of datasets. Our experiments include comparison with the best-known methods on publicly available databases DRIONS-DB, RIM-ONE v.3, DRISHTI-GS, and evaluation on a private data set collected in collaboration with University of California San Francisco Medical School. The analysis of the architecture details is presented, and it is argued that the model can be employed for a broad scope of image segmentation problems of similar nature.



### Hybrid Forests for Left Ventricle Segmentation using only the first slice label
- **Arxiv ID**: http://arxiv.org/abs/1804.11317v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.11317v1)
- **Published**: 2018-04-30 16:43:29+00:00
- **Updated**: 2018-04-30 16:43:29+00:00
- **Authors**: Ismaël Koné, Lahsen Boulmane
- **Comment**: 5 pages, 4 figures, 3rd International Conference on Intelligent
  System and Computer Vision (ISCV 2018), Fez, Morocco
- **Journal**: None
- **Summary**: Machine learning models produce state-of-the-art results in many MRI images segmentation. However, most of these models are trained on very large datasets which come from experts manual labeling. This labeling process is very time consuming and costs experts work. Therefore finding a way to reduce this cost is on high demand. In this paper, we propose a segmentation method which exploits MRI images sequential structure to nearly drop out this labeling task. Only the first slice needs to be manually labeled to train the model which then infers the next slice's segmentation. Inference result is another datum used to train the model again. The updated model then infers the third slice and the same process is carried out until the last slice. The proposed model is an combination of two Random Forest algorithms: the classical one and a recent one namely Mondrian Forests. We applied our method on human left ventricle segmentation and results are very promising. This method can also be used to generate labels.



### On the iterative refinement of densely connected representation levels for semantic segmentation
- **Arxiv ID**: http://arxiv.org/abs/1804.11332v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.11332v1)
- **Published**: 2018-04-30 17:26:49+00:00
- **Updated**: 2018-04-30 17:26:49+00:00
- **Authors**: Arantxa Casanova, Guillem Cucurull, Michal Drozdzal, Adriana Romero, Yoshua Bengio
- **Comment**: None
- **Journal**: None
- **Summary**: State-of-the-art semantic segmentation approaches increase the receptive field of their models by using either a downsampling path composed of poolings/strided convolutions or successive dilated convolutions. However, it is not clear which operation leads to best results. In this paper, we systematically study the differences introduced by distinct receptive field enlargement methods and their impact on the performance of a novel architecture, called Fully Convolutional DenseResNet (FC-DRN). FC-DRN has a densely connected backbone composed of residual networks. Following standard image segmentation architectures, receptive field enlargement operations that change the representation level are interleaved among residual networks. This allows the model to exploit the benefits of both residual and dense connectivity patterns, namely: gradient flow, iterative refinement of representations, multi-scale feature combination and deep supervision. In order to highlight the potential of our model, we test it on the challenging CamVid urban scene understanding benchmark and make the following observations: 1) downsampling operations outperform dilations when the model is trained from scratch, 2) dilations are useful during the finetuning step of the model, 3) coarser representations require less refinement steps, and 4) ResNets (by model construction) are good regularizers, since they can reduce the model capacity when needed. Finally, we compare our architecture to alternative methods and report state-of-the-art result on the Camvid dataset, with at least twice fewer parameters.



### Ultra Power-Efficient CNN Domain Specific Accelerator with 9.3TOPS/Watt for Mobile and Embedded Applications
- **Arxiv ID**: http://arxiv.org/abs/1805.00361v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.00361v1)
- **Published**: 2018-04-30 17:36:14+00:00
- **Updated**: 2018-04-30 17:36:14+00:00
- **Authors**: Baohua Sun, Lin Yang, Patrick Dong, Wenhan Zhang, Jason Dong, Charles Young
- **Comment**: 9 pages, 10 Figures. Accepted by CVPR 2018 Efficient Deep Learning
  for Computer Vision workshop
- **Journal**: None
- **Summary**: Computer vision performances have been significantly improved in recent years by Convolutional Neural Networks(CNN). Currently, applications using CNN algorithms are deployed mainly on general purpose hardwares, such as CPUs, GPUs or FPGAs. However, power consumption, speed, accuracy, memory footprint, and die size should all be taken into consideration for mobile and embedded applications. Domain Specific Architecture (DSA) for CNN is the efficient and practical solution for CNN deployment and implementation. We designed and produced a 28nm Two-Dimensional CNN-DSA accelerator with an ultra power-efficient performance of 9.3TOPS/Watt and with all processing done in the internal memory instead of outside DRAM. It classifies 224x224 RGB image inputs at more than 140fps with peak power consumption at less than 300mW and an accuracy comparable to the VGG benchmark. The CNN-DSA accelerator is reconfigurable to support CNN model coefficients of various layer sizes and layer types, including convolution, depth-wise convolution, short-cut connections, max pooling, and ReLU. Furthermore, in order to better support real-world deployment for various application scenarios, especially with low-end mobile and embedded platforms and MCUs (Microcontroller Units), we also designed algorithms to fully utilize the CNN-DSA accelerator efficiently by reducing the dependency on external accelerator computation resources, including implementation of Fully-Connected (FC) layers within the accelerator and compression of extracted features from the CNN-DSA accelerator. Live demos with our CNN-DSA accelerator on mobile and embedded systems show its capabilities to be widely and practically applied in the real world.



### Adversarial Semantic Alignment for Improved Image Captions
- **Arxiv ID**: http://arxiv.org/abs/1805.00063v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.00063v3)
- **Published**: 2018-04-30 19:10:43+00:00
- **Updated**: 2019-06-06 18:41:03+00:00
- **Authors**: Pierre L. Dognin, Igor Melnyk, Youssef Mroueh, Jarret Ross, Tom Sercu
- **Comment**: Authors Equal Contribution, CVPR 2019
- **Journal**: None
- **Summary**: In this paper we study image captioning as a conditional GAN training, proposing both a context-aware LSTM captioner and co-attentive discriminator, which enforces semantic alignment between images and captions. We empirically focus on the viability of two training methods: Self-critical Sequence Training (SCST) and Gumbel Straight-Through (ST) and demonstrate that SCST shows more stable gradient behavior and improved results over Gumbel ST, even without accessing discriminator gradients directly. We also address the problem of automatic evaluation for captioning models and introduce a new semantic score, and show its correlation to human judgement. As an evaluation paradigm, we argue that an important criterion for a captioner is the ability to generalize to compositions of objects that do not usually co-occur together. To this end, we introduce a small captioned Out of Context (OOC) test set. The OOC set, combined with our semantic score, are the proposed new diagnosis tools for the captioning community. When evaluated on OOC and MS-COCO benchmarks, we show that SCST-based training has a strong performance in both semantic score and human evaluation, promising to be a valuable new approach for efficient discrete GAN training.



### MV-YOLO: Motion Vector-aided Tracking by Semantic Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1805.00107v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.00107v2)
- **Published**: 2018-04-30 21:33:43+00:00
- **Updated**: 2018-06-15 20:21:39+00:00
- **Authors**: Saeed Ranjbar Alvar, Ivan V. Bajić
- **Comment**: None
- **Journal**: None
- **Summary**: Object tracking is the cornerstone of many visual analytics systems. While considerable progress has been made in this area in recent years, robust, efficient, and accurate tracking in real-world video remains a challenge. In this paper, we present a hybrid tracker that leverages motion information from the compressed video stream and a general-purpose semantic object detector acting on decoded frames to construct a fast and efficient tracking engine. The proposed approach is compared with several well-known recent trackers on the OTB tracking dataset. The results indicate advantages of the proposed method in terms of speed and/or accuracy.Other desirable features of the proposed method are its simplicity and deployment efficiency, which stems from the fact that it reuses the resources and information that may already exist in the system for other reasons.



### A Canonical Image Set for Examining and Comparing Image Processing Algorithms
- **Arxiv ID**: http://arxiv.org/abs/1805.00116v1
- **DOI**: 10.18178/joig.6.2.137-144
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.00116v1)
- **Published**: 2018-04-30 22:21:56+00:00
- **Updated**: 2018-04-30 22:21:56+00:00
- **Authors**: Jeffrey Uhlmann
- **Comment**: None
- **Journal**: Journal of Image and Graphics, Vol. 6, No. 2, pp. 137-144,
  December 2018
- **Summary**: The purpose of this paper is to introduce a set of four test images containing features and structures that can facilitate effective examination and comparison of image processing algorithms. More specifically, the images are designed to more explicitly expose the characteristic properties of algorithms for image compression, virtual resolution adjustment, and enhancement. This set was developed at the Naval Research Laboratory (NRL) in the late 1990s as a more rigorous alternative to Lena and other images that have come into common use for purely ad hoc reasons with little or no rigorous consideration of their suitability. The increasing number of test images appearing in the literature not only makes it more difficult to compare results from different papers, it also introduces the potential for cherry-picking to influence results. The key contribution of this paper is the proposal to establish {\em some} canonical set to ensure that published results can be analyzed and compared in a rigorous way from one paper to another, and consideration of the four NRL images is proposed for this purpose.



### CrowdHuman: A Benchmark for Detecting Human in a Crowd
- **Arxiv ID**: http://arxiv.org/abs/1805.00123v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.00123v1)
- **Published**: 2018-04-30 22:49:54+00:00
- **Updated**: 2018-04-30 22:49:54+00:00
- **Authors**: Shuai Shao, Zijian Zhao, Boxun Li, Tete Xiao, Gang Yu, Xiangyu Zhang, Jian Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Human detection has witnessed impressive progress in recent years. However, the occlusion issue of detecting human in highly crowded environments is far from solved. To make matters worse, crowd scenarios are still under-represented in current human detection benchmarks. In this paper, we introduce a new dataset, called CrowdHuman, to better evaluate detectors in crowd scenarios. The CrowdHuman dataset is large, rich-annotated and contains high diversity. There are a total of $470K$ human instances from the train and validation subsets, and $~22.6$ persons per image, with various kinds of occlusions in the dataset. Each human instance is annotated with a head bounding-box, human visible-region bounding-box and human full-body bounding-box. Baseline performance of state-of-the-art detection frameworks on CrowdHuman is presented. The cross-dataset generalization results of CrowdHuman dataset demonstrate state-of-the-art performance on previous dataset including Caltech-USA, CityPersons, and Brainwash without bells and whistles. We hope our dataset will serve as a solid baseline and help promote future research in human detection tasks.



### The Effects of Image Pre- and Post-Processing, Wavelet Decomposition, and Local Binary Patterns on U-Nets for Skin Lesion Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1805.05239v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1805.05239v1)
- **Published**: 2018-04-30 23:23:15+00:00
- **Updated**: 2018-04-30 23:23:15+00:00
- **Authors**: Sara Ross-Howe, H. R. Tizhoosh
- **Comment**: Accepted for publication in proceedings of the IEEE World Congress on
  Computational Intelligence (IEEE WCCI), Rio de Janeiro, Brazil, 8-3 July,
  2018
- **Journal**: None
- **Summary**: Skin cancer is a widespread, global, and potentially deadly disease, which over the last three decades has afflicted more lives in the USA than all other forms of cancer combined. There have been a lot of promising recent works utilizing deep network architectures, such as FCNs, U-Nets, and ResNets, for developing automated skin lesion segmentation. This paper investigates various pre- and post-processing techniques for improving the performance of U-Nets as measured by the Jaccard Index. The dataset provided as part of the "2017 ISBI Challenges on Skin Lesion Analysis Towards Melanoma Detection" was used for this evaluation and the performance of the finalist competitors was the standard for comparison. The pre-processing techniques employed in the proposed system included contrast enhancement, artifact removal, and vignette correction. More advanced image transformations, such as local binary patterns and wavelet decomposition, were also employed to augment the raw grayscale images used as network input features. While the performance of the proposed system fell short of the winners of the challenge, it was determined that using wavelet decomposition as an early transformation step improved the overall performance of the system over pre- and post-processing steps alone.



### Deep Barcodes for Fast Retrieval of Histopathology Scans
- **Arxiv ID**: http://arxiv.org/abs/1805.08833v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1805.08833v1)
- **Published**: 2018-04-30 23:27:29+00:00
- **Updated**: 2018-04-30 23:27:29+00:00
- **Authors**: Meghana Dinesh Kumar, Morteza Babaie, Hamid Tizhoosh
- **Comment**: Accepted for publication in proceedings of the IEEE World Congress on
  Computational Intelligence (IEEE WCCI), Rio de Janeiro, Brazil, 8-3 July,
  2018
- **Journal**: None
- **Summary**: We investigate the concept of deep barcodes and propose two methods to generate them in order to expedite the process of classification and retrieval of histopathology images. Since binary search is computationally less expensive, in terms of both speed and storage, deep barcodes could be useful when dealing with big data retrieval. Our experiments use the dataset Kimia Path24 to test three pre-trained networks for image retrieval. The dataset consists of 27,055 training images in 24 different classes with large variability, and 1,325 test images for testing. Apart from the high-speed and efficiency, results show a surprising retrieval accuracy of 71.62% for deep barcodes, as compared to 68.91% for deep features and 68.53% for compressed deep features.



