# Arxiv Papers in cs.CV on 2018-04-13
### A Hybrid Model for Identity Obfuscation by Face Replacement
- **Arxiv ID**: http://arxiv.org/abs/1804.04779v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/1804.04779v2)
- **Published**: 2018-04-13 03:55:32+00:00
- **Updated**: 2018-07-24 10:31:03+00:00
- **Authors**: Qianru Sun, Ayush Tewari, Weipeng Xu, Mario Fritz, Christian Theobalt, Bernt Schiele
- **Comment**: ECCV'18, camera-ready version
- **Journal**: None
- **Summary**: As more and more personal photos are shared and tagged in social media, avoiding privacy risks such as unintended recognition becomes increasingly challenging. We propose a new hybrid approach to obfuscate identities in photos by head replacement. Our approach combines state of the art parametric face synthesis with latest advances in Generative Adversarial Networks (GAN) for data-driven image synthesis. On the one hand, the parametric part of our method gives us control over the facial parameters and allows for explicit manipulation of the identity. On the other hand, the data-driven aspects allow for adding fine details and overall realism as well as seamless blending into the scene context. In our experiments, we show highly realistic output of our system that improves over the previous state of the art in obfuscation rate while preserving a higher similarity to the original image content.



### FishEyeRecNet: A Multi-Context Collaborative Deep Network for Fisheye Image Rectification
- **Arxiv ID**: http://arxiv.org/abs/1804.04784v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.04784v1)
- **Published**: 2018-04-13 04:18:47+00:00
- **Updated**: 2018-04-13 04:18:47+00:00
- **Authors**: Xiaoqing Yin, Xinchao Wang, Jun Yu, Maojun Zhang, Pascal Fua, Dacheng Tao
- **Comment**: 16 pages, 5 figures
- **Journal**: None
- **Summary**: Images captured by fisheye lenses violate the pinhole camera assumption and suffer from distortions. Rectification of fisheye images is therefore a crucial preprocessing step for many computer vision applications. In this paper, we propose an end-to-end multi-context collaborative deep network for removing distortions from single fisheye images. In contrast to conventional approaches, which focus on extracting hand-crafted features from input images, our method learns high-level semantics and low-level appearance features simultaneously to estimate the distortion parameters. To facilitate training, we construct a synthesized dataset that covers various scenes and distortion parameter settings. Experiments on both synthesized and real-world datasets show that the proposed model significantly outperforms current state of the art methods. Our code and synthesized dataset will be made publicly available.



### Deep Motion Boundary Detection
- **Arxiv ID**: http://arxiv.org/abs/1804.04785v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.04785v1)
- **Published**: 2018-04-13 04:19:06+00:00
- **Updated**: 2018-04-13 04:19:06+00:00
- **Authors**: Xiaoqing Yin, Xiyang Dai, Xinchao Wang, Maojun Zhang, Dacheng Tao, Larry Davis
- **Comment**: 17 pages, 5 figures
- **Journal**: None
- **Summary**: Motion boundary detection is a crucial yet challenging problem. Prior methods focus on analyzing the gradients and distributions of optical flow fields, or use hand-crafted features for motion boundary learning. In this paper, we propose the first dedicated end-to-end deep learning approach for motion boundary detection, which we term as MoBoNet. We introduce a refinement network structure which takes source input images, initial forward and backward optical flows as well as corresponding warping errors as inputs and produces high-resolution motion boundaries. Furthermore, we show that the obtained motion boundaries, through a fusion sub-network we design, can in turn guide the optical flows for removing the artifacts. The proposed MoBoNet is generic and works with any optical flows. Our motion boundary detection and the refined optical flow estimation achieve results superior to the state of the art.



### Talking Face Generation by Conditional Recurrent Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/1804.04786v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.04786v3)
- **Published**: 2018-04-13 04:19:52+00:00
- **Updated**: 2019-07-25 20:40:22+00:00
- **Authors**: Yang Song, Jingwen Zhu, Dawei Li, Xiaolong Wang, Hairong Qi
- **Comment**: Project
  Page:http://web.eecs.utk.edu/~ysong18/projects/talkingface/talkingface.html
- **Journal**: None
- **Summary**: Given an arbitrary face image and an arbitrary speech clip, the proposed work attempts to generating the talking face video with accurate lip synchronization while maintaining smooth transition of both lip and facial movement over the entire video clip. Existing works either do not consider temporal dependency on face images across different video frames thus easily yielding noticeable/abrupt facial and lip movement or are only limited to the generation of talking face video for a specific person thus lacking generalization capacity. We propose a novel conditional video generation network where the audio input is treated as a condition for the recurrent adversarial network such that temporal dependency is incorporated to realize smooth transition for the lip and facial movement. In addition, we deploy a multi-task adversarial training scheme in the context of video generation to improve both photo-realism and the accuracy for lip synchronization. Finally, based on the phoneme distribution information extracted from the audio clip, we develop a sample selection method that effectively reduces the size of the training dataset without sacrificing the quality of the generated video. Extensive experiments on both controlled and uncontrolled datasets demonstrate the superiority of the proposed approach in terms of visual quality, lip sync accuracy, and smooth transition of lip and facial movement, as compared to the state-of-the-art.



### Precise Temporal Action Localization by Evolving Temporal Proposals
- **Arxiv ID**: http://arxiv.org/abs/1804.04803v1
- **DOI**: 10.1145/3206025.3206029
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.04803v1)
- **Published**: 2018-04-13 07:10:36+00:00
- **Updated**: 2018-04-13 07:10:36+00:00
- **Authors**: Haonan Qiu, Yingbin Zheng, Hao Ye, Yao Lu, Feng Wang, Liang He
- **Comment**: None
- **Journal**: ACM International Conference on Multimedia Retrieval (ICMR), 2018,
  pp. 388-396
- **Summary**: Locating actions in long untrimmed videos has been a challenging problem in video content analysis. The performances of existing action localization approaches remain unsatisfactory in precisely determining the beginning and the end of an action. Imitating the human perception procedure with observations and refinements, we propose a novel three-phase action localization framework. Our framework is embedded with an Actionness Network to generate initial proposals through frame-wise similarity grouping, and then a Refinement Network to conduct boundary adjustment on these proposals. Finally, the refined proposals are sent to a Localization Network for further fine-grained location regression. The whole process can be deemed as multi-stage refinement using a novel non-local pyramid feature under various temporal granularities. We evaluate our framework on THUMOS14 benchmark and obtain a significant improvement over the state-of-the-arts approaches. Specifically, the performance gain is remarkable under precise localization with high IoU thresholds. Our proposed framework achieves mAP@IoU=0.5 of 34.2%.



### Learning Deep Sketch Abstraction
- **Arxiv ID**: http://arxiv.org/abs/1804.04804v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.04804v1)
- **Published**: 2018-04-13 07:12:02+00:00
- **Updated**: 2018-04-13 07:12:02+00:00
- **Authors**: Umar Riaz Muhammad, Yongxin Yang, Yi-Zhe Song, Tao Xiang, Timothy M. Hospedales
- **Comment**: This paper is accepted at CVPR 2018 as poster
- **Journal**: None
- **Summary**: Human free-hand sketches have been studied in various contexts including sketch recognition, synthesis and fine-grained sketch-based image retrieval (FG-SBIR). A fundamental challenge for sketch analysis is to deal with drastically different human drawing styles, particularly in terms of abstraction level. In this work, we propose the first stroke-level sketch abstraction model based on the insight of sketch abstraction as a process of trading off between the recognizability of a sketch and the number of strokes used to draw it. Concretely, we train a model for abstract sketch generation through reinforcement learning of a stroke removal policy that learns to predict which strokes can be safely removed without affecting recognizability. We show that our abstraction model can be used for various sketch analysis tasks including: (1) modeling stroke saliency and understanding the decision of sketch recognition models, (2) synthesizing sketches of variable abstraction for a given category, or reference object instance in a photo, and (3) training a FG-SBIR model with photos only, bypassing the expensive photo-sketch pair collection step.



### Mutual Suppression Network for Video Prediction using Disentangled Features
- **Arxiv ID**: http://arxiv.org/abs/1804.04810v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.04810v2)
- **Published**: 2018-04-13 07:35:22+00:00
- **Updated**: 2019-07-14 04:55:49+00:00
- **Authors**: Jungbeom Lee, Jangho Lee, Sungmin Lee, Sungroh Yoon
- **Comment**: BMVC 2019 (Oral)
- **Journal**: None
- **Summary**: Video prediction has been considered a difficult problem because the video contains not only high-dimensional spatial information but also complex temporal information. Video prediction can be performed by finding features in recent frames, and using them to generate approximations to upcoming frames. We approach this problem by disentangling spatial and temporal features in videos. We introduce a mutual suppression network (MSnet) which are trained in an adversarial manner and then produces spatial features which are free of motion information, and motion features with no spatial information. MSnet then uses motion-guided connection within an encoder-decoder-based architecture to transform spatial features from a previous frame to the time of an upcoming frame. We show how MSnet can be used for video prediction using disentangled representations. We also carry out experiments to assess the effectiveness of our method to disentangle features. MSnet obtains better results than other recent video prediction methods even though it has simpler encoders.



### Offline and Online calibration of Mobile Robot and SLAM Device for Navigation
- **Arxiv ID**: http://arxiv.org/abs/1804.04817v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1804.04817v1)
- **Published**: 2018-04-13 07:48:44+00:00
- **Updated**: 2018-04-13 07:48:44+00:00
- **Authors**: Ryoichi Ishikawa, Takeshi Oishi, Katsushi Ikeuchi
- **Comment**: None
- **Journal**: None
- **Summary**: Robot navigation technology is required to accomplish difficult tasks in various environments. In navigation, it is necessary to know the information of the external environments and the state of the robot under the environment. On the other hand, various studies have been done on SLAM technology, which is also used for navigation, but also applied to devices for Mixed Reality and the like.   In this paper, we propose a robot-device calibration method for navigation with a device using SLAM technology on a robot. The calibration is performed by using the position and orientation information given by the robot and the device. In the calibration, the most efficient way of movement is clarified according to the restriction of the robot movement. Furthermore, we also show a method to dynamically correct the position and orientation of the robot so that the information of the external environment and the shape information of the robot maintain consistency in order to reduce the dynamic error occurring during navigation.   Our method can be easily used for various kinds of robots and localization with sufficient precision for navigation is possible with offline calibration and online position correction. In the experiments, we confirm the parameters obtained by two types of offline calibration according to the degree of freedom of robot movement and validate the effectiveness of online correction method by plotting localized position error during robot's intense movement. Finally, we show the demonstration of navigation using SLAM device.



### Spline Error Weighting for Robust Visual-Inertial Fusion
- **Arxiv ID**: http://arxiv.org/abs/1804.04820v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.04820v1)
- **Published**: 2018-04-13 07:54:10+00:00
- **Updated**: 2018-04-13 07:54:10+00:00
- **Authors**: Hannes Ovrén, Per-Erik Forssén
- **Comment**: To appear in CVPR 2018
- **Journal**: None
- **Summary**: In this paper we derive and test a probability-based weighting that can balance residuals of different types in spline fitting. In contrast to previous formulations, the proposed spline error weighting scheme also incorporates a prediction of the approximation error of the spline fit. We demonstrate the effectiveness of the prediction in a synthetic experiment, and apply it to visual-inertial fusion on rolling shutter cameras. This results in a method that can estimate 3D structure with metric scale on generic first-person videos. We also propose a quality measure for spline fitting, that can be used to automatically select the knot spacing. Experiments verify that the obtained trajectory quality corresponds well with the requested quality. Finally, by linearly scaling the weights, we show that the proposed spline error weighting minimizes the estimation errors on real sequences, in terms of scale and end-point errors.



### Learning Warped Guidance for Blind Face Restoration
- **Arxiv ID**: http://arxiv.org/abs/1804.04829v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.04829v2)
- **Published**: 2018-04-13 08:22:09+00:00
- **Updated**: 2018-04-16 05:24:47+00:00
- **Authors**: Xiaoming Li, Ming Liu, Yuting Ye, Wangmeng Zuo, Liang Lin, Ruigang Yang
- **Comment**: 25 pages, 14 figures and 1 table
- **Journal**: None
- **Summary**: This paper studies the problem of blind face restoration from an unconstrained blurry, noisy, low-resolution, or compressed image (i.e., degraded observation). For better recovery of fine facial details, we modify the problem setting by taking both the degraded observation and a high-quality guided image of the same identity as input to our guided face restoration network (GFRNet). However, the degraded observation and guided image generally are different in pose, illumination and expression, thereby making plain CNNs (e.g., U-Net) fail to recover fine and identity-aware facial details. To tackle this issue, our GFRNet model includes both a warping subnetwork (WarpNet) and a reconstruction subnetwork (RecNet). The WarpNet is introduced to predict flow field for warping the guided image to correct pose and expression (i.e., warped guidance), while the RecNet takes the degraded observation and warped guidance as input to produce the restoration result. Due to that the ground-truth flow field is unavailable, landmark loss together with total variation regularization are incorporated to guide the learning of WarpNet. Furthermore, to make the model applicable to blind restoration, our GFRNet is trained on the synthetic data with versatile settings on blur kernel, noise level, downsampling scale factor, and JPEG quality factor. Experiments show that our GFRNet not only performs favorably against the state-of-the-art image and face restoration methods, but also generates visually photo-realistic results on real degraded facial images.



### BodyNet: Volumetric Inference of 3D Human Body Shapes
- **Arxiv ID**: http://arxiv.org/abs/1804.04875v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.04875v3)
- **Published**: 2018-04-13 10:21:40+00:00
- **Updated**: 2018-08-18 11:14:43+00:00
- **Authors**: Gül Varol, Duygu Ceylan, Bryan Russell, Jimei Yang, Ersin Yumer, Ivan Laptev, Cordelia Schmid
- **Comment**: Appears in: European Conference on Computer Vision 2018 (ECCV 2018).
  27 pages
- **Journal**: None
- **Summary**: Human shape estimation is an important task for video editing, animation and fashion industry. Predicting 3D human body shape from natural images, however, is highly challenging due to factors such as variation in human bodies, clothing and viewpoint. Prior methods addressing this problem typically attempt to fit parametric body models with certain priors on pose and shape. In this work we argue for an alternative representation and propose BodyNet, a neural network for direct inference of volumetric body shape from a single image. BodyNet is an end-to-end trainable network that benefits from (i) a volumetric 3D loss, (ii) a multi-view re-projection loss, and (iii) intermediate supervision of 2D pose, 2D body part segmentation, and 3D pose. Each of them results in performance improvement as demonstrated by our experiments. To evaluate the method, we fit the SMPL model to our network output and show state-of-the-art results on the SURREAL and Unite the People datasets, outperforming recent approaches. Besides achieving state-of-the-art performance, our method also enables volumetric body-part segmentation.



### Group Anomaly Detection using Deep Generative Models
- **Arxiv ID**: http://arxiv.org/abs/1804.04876v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.04876v1)
- **Published**: 2018-04-13 10:33:03+00:00
- **Updated**: 2018-04-13 10:33:03+00:00
- **Authors**: Raghavendra Chalapathy, Edward Toth, Sanjay Chawla
- **Comment**: Submitted Under review to The European Conference on Machine Learning
  and Principles and Practice of Knowledge Discovery in Databases ECML-2018
  Conference Dublin, Ireland during the 10-14 September 2018
- **Journal**: None
- **Summary**: Unlike conventional anomaly detection research that focuses on point anomalies, our goal is to detect anomalous collections of individual data points. In particular, we perform group anomaly detection (GAD) with an emphasis on irregular group distributions (e.g. irregular mixtures of image pixels). GAD is an important task in detecting unusual and anomalous phenomena in real-world applications such as high energy particle physics, social media, and medical imaging. In this paper, we take a generative approach by proposing deep generative models: Adversarial autoencoder (AAE) and variational autoencoder (VAE) for group anomaly detection. Both AAE and VAE detect group anomalies using point-wise input data where group memberships are known a priori. We conduct extensive experiments to evaluate our models on real-world datasets. The empirical results demonstrate that our approach is effective and robust in detecting group anomalies.



### Learning to Exploit the Prior Network Knowledge for Weakly-Supervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1804.04882v2
- **DOI**: 10.1109/TIP.2019.2901393
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.04882v2)
- **Published**: 2018-04-13 10:52:07+00:00
- **Updated**: 2019-02-22 14:26:29+00:00
- **Authors**: Carolina Redondo-Cabrera, Marcos Baptista-Ríos, Roberto J. López-Sastre
- **Comment**: None
- **Journal**: IEEE Transactions on Image Processing, 2019
- **Summary**: Training a Convolutional Neural Network (CNN) for semantic segmentation typically requires to collect a large amount of accurate pixel-level annotations, a hard and expensive task. In contrast, simple image tags are easier to gather. With this paper we introduce a novel weakly-supervised semantic segmentation model able to learn from image labels, and just image labels. Our model uses the prior knowledge of a network trained for image recognition, employing these image annotations as an attention mechanism to identify semantic regions in the images. We then present a methodology that builds accurate class-specific segmentation masks from these regions, where neither external objectness nor saliency algorithms are required. We describe how to incorporate this mask generation strategy into a fully end-to-end trainable process where the network jointly learns to classify and segment images. Our experiments on PASCAL VOC 2012 dataset show that exploiting these generated class-specific masks in conjunction with our novel end-to-end learning process outperforms several recent weakly-supervised semantic segmentation methods that use image tags only, and even some models that leverage additional supervision or training data.



### Pose estimation of a single circle using default intrinsic calibration
- **Arxiv ID**: http://arxiv.org/abs/1804.04922v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.04922v1)
- **Published**: 2018-04-13 12:52:42+00:00
- **Updated**: 2018-04-13 12:52:42+00:00
- **Authors**: Mariyanayagam Damien, Gurdjos Pierre, Chambon Sylvie, Brunet Florent, Charvillat Vincent
- **Comment**: None
- **Journal**: None
- **Summary**: Circular markers are planar markers which offer great performances for detection and pose estimation. For an uncalibrated camera with an unknown focal length, at least the images of at least two coplanar circles are generally required to recover their poses. Unfortunately, detecting more than one ellipse in the image must be tricky and time-consuming, especially regarding concentric circles. On the other hand, when the camera is calibrated, one circle suffices but the solution is twofold and can hardly be disambiguated. Our contribution is to put beyond this limit by dealing with the uncalibrated case of a camera seeing one circle and discussing how to remove the ambiguity. We propose a new problem formulation that enables to show how to detect geometric configurations in which the ambiguity can be removed. Furthermore, we introduce the notion of default camera intrinsics and show, using intensive empirical works, the surprising observation that very approximate calibration can lead to accurate circle pose estimation.



### CNN-based Landmark Detection in Cardiac CTA Scans
- **Arxiv ID**: http://arxiv.org/abs/1804.04963v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.04963v1)
- **Published**: 2018-04-13 14:32:42+00:00
- **Updated**: 2018-04-13 14:32:42+00:00
- **Authors**: Julia M. H. Noothout, Bob D. de Vos, Jelmer M. Wolterink, Tim Leiner, Ivana Išgum
- **Comment**: This work was submitted to MIDL 2018 Conference
- **Journal**: None
- **Summary**: Fast and accurate anatomical landmark detection can benefit many medical image analysis methods. Here, we propose a method to automatically detect anatomical landmarks in medical images. Automatic landmark detection is performed with a patch-based fully convolutional neural network (FCNN) that combines regression and classification. For any given image patch, regression is used to predict the 3D displacement vector from the image patch to the landmark. Simultaneously, classification is used to identify patches that contain the landmark. Under the assumption that patches close to a landmark can determine the landmark location more precisely than patches farther from it, only those patches that contain the landmark according to classification are used to determine the landmark location. The landmark location is obtained by calculating the average landmark location using the computed 3D displacement vectors. The method is evaluated using detection of six clinically relevant landmarks in coronary CT angiography (CCTA) scans: the right and left ostium, the bifurcation of the left main coronary artery (LM) into the left anterior descending and the left circumflex artery, and the origin of the right, non-coronary, and left aortic valve commissure. The proposed method achieved an average Euclidean distance error of 2.19 mm and 2.88 mm for the right and left ostium respectively, 3.78 mm for the bifurcation of the LM, and 1.82 mm, 2.10 mm and 1.89 mm for the origin of the right, non-coronary, and left aortic valve commissure respectively, demonstrating accurate performance. The proposed combination of regression and classification can be used to accurately detect landmarks in CCTA scans.



### An efficient deep convolutional laplacian pyramid architecture for CS reconstruction at low sampling ratios
- **Arxiv ID**: http://arxiv.org/abs/1804.04970v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.04970v1)
- **Published**: 2018-04-13 14:42:20+00:00
- **Updated**: 2018-04-13 14:42:20+00:00
- **Authors**: Wenxue Cui, Heyao Xu, Xinwei Gao, Shengping Zhang, Feng Jiang, Debin Zhao
- **Comment**: 5 pages. Accepted by ICASSP2018
- **Journal**: None
- **Summary**: The compressed sensing (CS) has been successfully applied to image compression in the past few years as most image signals are sparse in a certain domain. Several CS reconstruction models have been proposed and obtained superior performance. However, these methods suffer from blocking artifacts or ringing effects at low sampling ratios in most cases. To address this problem, we propose a deep convolutional Laplacian Pyramid Compressed Sensing Network (LapCSNet) for CS, which consists of a sampling sub-network and a reconstruction sub-network. In the sampling sub-network, we utilize a convolutional layer to mimic the sampling operator. In contrast to the fixed sampling matrices used in traditional CS methods, the filters used in our convolutional layer are jointly optimized with the reconstruction sub-network. In the reconstruction sub-network, two branches are designed to reconstruct multi-scale residual images and muti-scale target images progressively using a Laplacian pyramid architecture. The proposed LapCSNet not only integrates multi-scale information to achieve better performance but also reduces computational cost dramatically. Experimental results on benchmark datasets demonstrate that the proposed method is capable of reconstructing more details and sharper edges against the state-of-the-arts methods.



### Convolutional Neural Networks for Skull-stripping in Brain MR Imaging using Consensus-based Silver standard Masks
- **Arxiv ID**: http://arxiv.org/abs/1804.04988v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.04988v1)
- **Published**: 2018-04-13 15:18:06+00:00
- **Updated**: 2018-04-13 15:18:06+00:00
- **Authors**: Oeslle Lucena, Roberto Souza, Leticia Rittner, Richard Frayne, Roberto Lotufo
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks (CNN) for medical imaging are constrained by the number of annotated data required in the training stage. Usually, manual annotation is considered to be the "gold standard". However, medical imaging datasets that include expert manual segmentation are scarce as this step is time-consuming, and therefore expensive. Moreover, single-rater manual annotation is most often used in data-driven approaches making the network optimal with respect to only that single expert. In this work, we propose a CNN for brain extraction in magnetic resonance (MR) imaging, that is fully trained with what we refer to as silver standard masks. Our method consists of 1) developing a dataset with "silver standard" masks as input, and implementing both 2) a tri-planar method using parallel 2D U-Net-based CNNs (referred to as CONSNet) and 3) an auto-context implementation of CONSNet. The term CONSNet refers to our integrated approach, i.e., training with silver standard masks and using a 2D U-Net-based architecture. Our results showed that we outperformed (i.e., larger Dice coefficients) the current state-of-the-art SS methods. Our use of silver standard masks reduced the cost of manual annotation, decreased inter-intra-rater variability, and avoided CNN segmentation super-specialization towards one specific manual annotation guideline that can occur when gold standard masks are used. Moreover, the usage of silver standard masks greatly enlarges the volume of input annotated data because we can relatively easily generate labels for unlabeled data. In addition, our method has the advantage that, once trained, it takes only a few seconds to process a typical brain image volume using modern hardware, such as a high-end graphics processing unit. In contrast, many of the other competitive methods have processing times in the order of minutes.



### Comparatives, Quantifiers, Proportions: A Multi-Task Model for the Learning of Quantities from Vision
- **Arxiv ID**: http://arxiv.org/abs/1804.05018v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/1804.05018v1)
- **Published**: 2018-04-13 16:36:52+00:00
- **Updated**: 2018-04-13 16:36:52+00:00
- **Authors**: Sandro Pezzelle, Ionut-Teodor Sorodoc, Raffaella Bernardi
- **Comment**: 12 pages (references included). To appear in the Proceedings of
  NAACL-HLT 2018
- **Journal**: Proceedings of NAACL-HLT 2018
- **Summary**: The present work investigates whether different quantification mechanisms (set comparison, vague quantification, and proportional estimation) can be jointly learned from visual scenes by a multi-task computational model. The motivation is that, in humans, these processes underlie the same cognitive, non-symbolic ability, which allows an automatic estimation and comparison of set magnitudes. We show that when information about lower-complexity tasks is available, the higher-level proportional task becomes more accurate than when performed in isolation. Moreover, the multi-task model is able to generalize to unseen combinations of target/non-target objects. Consistently with behavioral evidence showing the interference of absolute number in the proportional task, the multi-task model no longer works when asked to provide the number of target objects in the scene.



### Unsupervised Sparse Dirichlet-Net for Hyperspectral Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1804.05042v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.05042v3)
- **Published**: 2018-04-13 17:17:05+00:00
- **Updated**: 2018-07-15 12:53:39+00:00
- **Authors**: Ying Qu, Hairong Qi, Chiman Kwan
- **Comment**: Accepted by The IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR 2018, Spotlight)
- **Journal**: None
- **Summary**: In many computer vision applications, obtaining images of high resolution in both the spatial and spectral domains are equally important. However, due to hardware limitations, one can only expect to acquire images of high resolution in either the spatial or spectral domains. This paper focuses on hyperspectral image super-resolution (HSI-SR), where a hyperspectral image (HSI) with low spatial resolution (LR) but high spectral resolution is fused with a multispectral image (MSI) with high spatial resolution (HR) but low spectral resolution to obtain HR HSI. Existing deep learning-based solutions are all supervised that would need a large training set and the availability of HR HSI, which is unrealistic. Here, we make the first attempt to solving the HSI-SR problem using an unsupervised encoder-decoder architecture that carries the following uniquenesses. First, it is composed of two encoder-decoder networks, coupled through a shared decoder, in order to preserve the rich spectral information from the HSI network. Second, the network encourages the representations from both modalities to follow a sparse Dirichlet distribution which naturally incorporates the two physical constraints of HSI and MSI. Third, the angular difference between representations are minimized in order to reduce the spectral distortion. We refer to the proposed architecture as unsupervised Sparse Dirichlet-Net, or uSDN. Extensive experimental results demonstrate the superior performance of uSDN as compared to the state-of-the-art.



### I-HAZE: a dehazing benchmark with real hazy and haze-free indoor images
- **Arxiv ID**: http://arxiv.org/abs/1804.05091v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.05091v1)
- **Published**: 2018-04-13 19:01:39+00:00
- **Updated**: 2018-04-13 19:01:39+00:00
- **Authors**: Codruta O. Ancuti, Cosmin Ancuti, Radu Timofte, Christophe De Vleeschouwer
- **Comment**: None
- **Journal**: None
- **Summary**: Image dehazing has become an important computational imaging topic in the recent years. However, due to the lack of ground truth images, the comparison of dehazing methods is not straightforward, nor objective. To overcome this issue we introduce a new dataset -named I-HAZE- that contains 35 image pairs of hazy and corresponding haze-free (ground-truth) indoor images. Different from most of the existing dehazing databases, hazy images have been generated using real haze produced by a professional haze machine. For easy color calibration and improved assessment of dehazing algorithms, each scene include a MacBeth color checker. Moreover, since the images are captured in a controlled environment, both haze-free and hazy images are captured under the same illumination conditions. This represents an important advantage of the I-HAZE dataset that allows us to objectively compare the existing image dehazing techniques using traditional image quality metrics such as PSNR and SSIM.



### O-HAZE: a dehazing benchmark with real hazy and haze-free outdoor images
- **Arxiv ID**: http://arxiv.org/abs/1804.05101v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.05101v1)
- **Published**: 2018-04-13 19:58:17+00:00
- **Updated**: 2018-04-13 19:58:17+00:00
- **Authors**: Codruta O. Ancuti, Cosmin Ancuti, Radu Timofte, Christophe De Vleeschouwer
- **Comment**: arXiv admin note: text overlap with arXiv:1804.05091
- **Journal**: None
- **Summary**: Haze removal or dehazing is a challenging ill-posed problem that has drawn a significant attention in the last few years. Despite this growing interest, the scientific community is still lacking a reference dataset to evaluate objectively and quantitatively the performance of proposed dehazing methods. The few datasets that are currently considered, both for assessment and training of learning-based dehazing techniques, exclusively rely on synthetic hazy images. To address this limitation, we introduce the first outdoor scenes database (named O-HAZE) composed of pairs of real hazy and corresponding haze-free images. In practice, hazy images have been captured in presence of real haze, generated by professional haze machines, and OHAZE contains 45 different outdoor scenes depicting the same visual content recorded in haze-free and hazy conditions, under the same illumination parameters. To illustrate its usefulness, O-HAZE is used to compare a representative set of state-of-the-art dehazing techniques, using traditional image quality metrics such as PSNR, SSIM and CIEDE2000. This reveals the limitations of current techniques, and questions some of their underlying assumptions.



### Multilevel Language and Vision Integration for Text-to-Clip Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1804.05113v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.05113v3)
- **Published**: 2018-04-13 20:46:37+00:00
- **Updated**: 2018-12-25 08:29:56+00:00
- **Authors**: Huijuan Xu, Kun He, Bryan A. Plummer, Leonid Sigal, Stan Sclaroff, Kate Saenko
- **Comment**: AAAI 2019
- **Journal**: None
- **Summary**: We address the problem of text-based activity retrieval in video. Given a sentence describing an activity, our task is to retrieve matching clips from an untrimmed video. To capture the inherent structures present in both text and video, we introduce a multilevel model that integrates vision and language features earlier and more tightly than prior work. First, we inject text features early on when generating clip proposals, to help eliminate unlikely clips and thus speed up processing and boost performance. Second, to learn a fine-grained similarity metric for retrieval, we use visual features to modulate the processing of query sentences at the word level in a recurrent neural network. A multi-task loss is also employed by adding query re-generation as an auxiliary task. Our approach significantly outperforms prior work on two challenging benchmarks: Charades-STA and ActivityNet Captions.



### Towards Safe Autonomous Driving: Capture Uncertainty in the Deep Neural Network For Lidar 3D Vehicle Detection
- **Arxiv ID**: http://arxiv.org/abs/1804.05132v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1804.05132v2)
- **Published**: 2018-04-13 22:13:30+00:00
- **Updated**: 2018-09-07 08:10:46+00:00
- **Authors**: Di Feng, Lars Rosenbaum, Klaus Dietmayer
- **Comment**: Accepted to present in the 21st IEEE International Conference on
  Intelligent Transportation Systems (ITSC 2018)
- **Journal**: None
- **Summary**: To assure that an autonomous car is driving safely on public roads, its object detection module should not only work correctly, but show its prediction confidence as well. Previous object detectors driven by deep learning do not explicitly model uncertainties in the neural network. We tackle with this problem by presenting practical methods to capture uncertainties in a 3D vehicle detector for Lidar point clouds. The proposed probabilistic detector represents reliable epistemic uncertainty and aleatoric uncertainty in classification and localization tasks. Experimental results show that the epistemic uncertainty is related to the detection accuracy, whereas the aleatoric uncertainty is influenced by vehicle distance and occlusion. The results also show that we can improve the detection performance by 1%-5% by modeling the aleatoric uncertainty.



