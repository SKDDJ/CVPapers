# Arxiv Papers in cs.CV on 2018-04-29
### CRAFT: Complementary Recommendations Using Adversarial Feature Transformer
- **Arxiv ID**: http://arxiv.org/abs/1804.10871v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.10871v3)
- **Published**: 2018-04-29 04:52:06+00:00
- **Updated**: 2018-09-10 06:58:24+00:00
- **Authors**: Cong Phuoc Huynh, Arridhana Ciptadi, Ambrish Tyagi, Amit Agrawal
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Traditional approaches for complementary product recommendations rely on behavioral and non-visual data such as customer co-views or co-buys. However, certain domains such as fashion are primarily visual. We propose a framework that harnesses visual cues in an unsupervised manner to learn the distribution of co-occurring complementary items in real world images. Our model learns a non-linear transformation between the two manifolds of source and target complementary item categories (e.g., tops and bottoms in outfits). Given a large dataset of images containing instances of co-occurring object categories, we train a generative transformer network directly on the feature representation space by casting it as an adversarial optimization problem. Such a conditional generative model can produce multiple novel samples of complementary items (in the feature space) for a given query item. The final recommendations are selected from the closest real world examples to the synthesized complementary features. We apply our framework to the task of recommending complementary tops for a given bottom clothing item. The recommendations made by our system are diverse, and are favored by human experts over the baseline approaches.



### TreeSegNet: Adaptive Tree CNNs for Subdecimeter Aerial Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1804.10879v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.10879v2)
- **Published**: 2018-04-29 06:17:00+00:00
- **Updated**: 2018-08-25 16:48:14+00:00
- **Authors**: Kai Yue, Lei Yang, Ruirui Li, Wei Hu, Fan Zhang, Wei Li
- **Comment**: 40 pages; 16 figures; 6 tables
- **Journal**: None
- **Summary**: For the task of subdecimeter aerial imagery segmentation, fine-grained semantic segmentation results are usually difficult to obtain because of complex remote sensing content and optical conditions. Recently, convolutional neural networks (CNNs) have shown outstanding performance on this task. Although many deep neural network structures and techniques have been applied to improve the accuracy, few have paid attention to better differentiating the easily confused classes. In this paper, we propose TreeSegNet which adopts an adaptive network to increase the classification rate at the pixelwise level. Specifically, based on the infrastructure of DeepUNet, a Tree-CNN block in which each node represents a ResNeXt unit is constructed adaptively according to the confusion matrix and the proposed TreeCutting algorithm. By transporting feature maps through concatenating connections, the Tree-CNN block fuses multiscale features and learns best weights for the model. In experiments on the ISPRS 2D semantic labeling Potsdam dataset, the results obtained by TreeSegNet are better than those of other published state-of-the-art methods. Detailed comparison and analysis show that the improvement brought by the adaptive Tree-CNN block is significant.



### Local Learning with Deep and Handcrafted Features for Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/1804.10892v7
- **DOI**: 10.1109/ACCESS.2019.2917266
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.10892v7)
- **Published**: 2018-04-29 09:12:13+00:00
- **Updated**: 2020-03-12 18:17:04+00:00
- **Authors**: Mariana-Iuliana Georgescu, Radu Tudor Ionescu, Marius Popescu
- **Comment**: Accepted in IEEE Access
- **Journal**: in IEEE Access, vol. 7, pp. 64827-64836, 2019
- **Summary**: We present an approach that combines automatic features learned by convolutional neural networks (CNN) and handcrafted features computed by the bag-of-visual-words (BOVW) model in order to achieve state-of-the-art results in facial expression recognition. To obtain automatic features, we experiment with multiple CNN architectures, pre-trained models and training procedures, e.g. Dense-Sparse-Dense. After fusing the two types of features, we employ a local learning framework to predict the class label for each test image. The local learning framework is based on three steps. First, a k-nearest neighbors model is applied in order to select the nearest training samples for an input test image. Second, a one-versus-all Support Vector Machines (SVM) classifier is trained on the selected training samples. Finally, the SVM classifier is used to predict the class label only for the test image it was trained for. Although we have used local learning in combination with handcrafted features in our previous work, to the best of our knowledge, local learning has never been employed in combination with deep features. The experiments on the 2013 Facial Expression Recognition (FER) Challenge data set, the FER+ data set and the AffectNet data set demonstrate that our approach achieves state-of-the-art results. With a top accuracy of 75.42% on FER 2013, 87.76% on the FER+, 59.58% on AffectNet 8-way classification and 63.31% on AffectNet 7-way classification, we surpass the state-of-the-art methods by more than 1% on all data sets.



### Scalable Angular Discriminative Deep Metric Learning for Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1804.10899v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1804.10899v2)
- **Published**: 2018-04-29 09:40:46+00:00
- **Updated**: 2018-05-01 01:30:56+00:00
- **Authors**: Bowen Wu, Huaming Wu, Monica M. Y. Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: With the development of deep learning, Deep Metric Learning (DML) has achieved great improvements in face recognition. Specifically, the widely used softmax loss in the training process often bring large intra-class variations, and feature normalization is only exploited in the testing process to compute the pair similarities. To bridge the gap, we impose the intra-class cosine similarity between the features and weight vectors in softmax loss larger than a margin in the training step, and extend it from four aspects. First, we explore the effect of a hard sample mining strategy. To alleviate the human labor of adjusting the margin hyper-parameter, a self-adaptive margin updating strategy is proposed. Then, a normalized version is given to take full advantage of the cosine similarity constraint. Furthermore, we enhance the former constraint to force the intra-class cosine similarity larger than the mean inter-class cosine similarity with a margin in the exponential feature projection space. Extensive experiments on Labeled Face in the Wild (LFW), Youtube Faces (YTF) and IARPA Janus Benchmark A (IJB-A) datasets demonstrate that the proposed methods outperform the mainstream DML methods and approach the state-of-the-art performance.



### Unsupervised Cross-Modality Domain Adaptation of ConvNets for Biomedical Image Segmentations with Adversarial Loss
- **Arxiv ID**: http://arxiv.org/abs/1804.10916v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.10916v2)
- **Published**: 2018-04-29 12:19:53+00:00
- **Updated**: 2018-06-19 03:12:15+00:00
- **Authors**: Qi Dou, Cheng Ouyang, Cheng Chen, Hao Chen, Pheng-Ann Heng
- **Comment**: IJCAI 2018
- **Journal**: None
- **Summary**: Convolutional networks (ConvNets) have achieved great successes in various challenging vision tasks. However, the performance of ConvNets would degrade when encountering the domain shift. The domain adaptation is more significant while challenging in the field of biomedical image analysis, where cross-modality data have largely different distributions. Given that annotating the medical data is especially expensive, the supervised transfer learning approaches are not quite optimal. In this paper, we propose an unsupervised domain adaptation framework with adversarial learning for cross-modality biomedical image segmentations. Specifically, our model is based on a dilated fully convolutional network for pixel-wise prediction. Moreover, we build a plug-and-play domain adaptation module (DAM) to map the target input to features which are aligned with source domain feature space. A domain critic module (DCM) is set up for discriminating the feature space of both domains. We optimize the DAM and DCM via an adversarial loss without using any target domain label. Our proposed method is validated by adapting a ConvNet trained with MRI images to unpaired CT data for cardiac structures segmentations, and achieved very promising results.



### Deep Affect Prediction in-the-wild: Aff-Wild Database and Challenge, Deep Architectures, and Beyond
- **Arxiv ID**: http://arxiv.org/abs/1804.10938v5
- **DOI**: 10.1007/s11263-019-01158-4
- **Categories**: **cs.CV**, cs.AI, cs.HC, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1804.10938v5)
- **Published**: 2018-04-29 14:18:07+00:00
- **Updated**: 2019-02-01 12:39:52+00:00
- **Authors**: Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A. Nicolaou, Athanasios Papaioannou, Guoying Zhao, Bj√∂rn Schuller, Irene Kotsia, Stefanos Zafeiriou
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic understanding of human affect using visual signals is of great importance in everyday human-machine interactions. Appraising human emotional states, behaviors and reactions displayed in real-world settings, can be accomplished using latent continuous dimensions (e.g., the circumplex model of affect). Valence (i.e., how positive or negative is an emotion) & arousal (i.e., power of the activation of the emotion) constitute popular and effective affect representations. Nevertheless, the majority of collected datasets this far, although containing naturalistic emotional states, have been captured in highly controlled recording conditions. In this paper, we introduce the Aff-Wild benchmark for training and evaluating affect recognition algorithms. We also report on the results of the First Affect-in-the-wild Challenge that was organized in conjunction with CVPR 2017 on the Aff-Wild database and was the first ever challenge on the estimation of valence and arousal in-the-wild. Furthermore, we design and extensively train an end-to-end deep neural architecture which performs prediction of continuous emotion dimensions based on visual cues. The proposed deep learning architecture, AffWildNet, includes convolutional & recurrent neural network layers, exploiting the invariant properties of convolutional features, while also modeling temporal dynamics that arise in human behavior via the recurrent layers. The AffWildNet produced state-of-the-art results on the Aff-Wild Challenge. We then exploit the AffWild database for learning features, which can be used as priors for achieving best performances both for dimensional, as well as categorical emotion recognition, using the RECOLA, AFEW-VA and EmotiW datasets, compared to all other methods designed for the same goal. The database and emotion recognition models are available at http://ibug.doc.ic.ac.uk/resources/first-affect-wild-challenge.



### UNIQ: Uniform Noise Injection for Non-Uniform Quantization of Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1804.10969v3
- **DOI**: 10.1145/3444943
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1804.10969v3)
- **Published**: 2018-04-29 17:38:20+00:00
- **Updated**: 2018-10-02 20:19:13+00:00
- **Authors**: Chaim Baskin, Eli Schwartz, Evgenii Zheltonozhskii, Natan Liss, Raja Giryes, Alex M. Bronstein, Avi Mendelson
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel method for neural network quantization that emulates a non-uniform $k$-quantile quantizer, which adapts to the distribution of the quantized parameters. Our approach provides a novel alternative to the existing uniform quantization techniques for neural networks. We suggest to compare the results as a function of the bit-operations (BOPS) performed, assuming a look-up table availability for the non-uniform case. In this setup, we show the advantages of our strategy in the low computational budget regime. While the proposed solution is harder to implement in hardware, we believe it sets a basis for new alternatives to neural networks quantization.



### Matryoshka Networks: Predicting 3D Geometry via Nested Shape Layers
- **Arxiv ID**: http://arxiv.org/abs/1804.10975v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/1804.10975v1)
- **Published**: 2018-04-29 18:33:21+00:00
- **Updated**: 2018-04-29 18:33:21+00:00
- **Authors**: Stephan R. Richter, Stefan Roth
- **Comment**: Published at the Conference on Computer Vision and Pattern
  Recognition (CVPR 2018)
- **Journal**: None
- **Summary**: In this paper, we develop novel, efficient 2D encodings for 3D geometry, which enable reconstructing full 3D shapes from a single image at high resolution. The key idea is to pose 3D shape reconstruction as a 2D prediction problem. To that end, we first develop a simple baseline network that predicts entire voxel tubes at each pixel of a reference view. By leveraging well-proven architectures for 2D pixel-prediction tasks, we attain state-of-the-art results, clearly outperforming purely voxel-based approaches. We scale this baseline to higher resolutions by proposing a memory-efficient shape encoding, which recursively decomposes a 3D shape into nested shape layers, similar to the pieces of a Matryoshka doll. This allows reconstructing highly detailed shapes with complex topology, as demonstrated in extensive experiments; we clearly outperform previous octree-based approaches despite having a much simpler architecture using standard network components. Our Matryoshka networks further enable reconstructing shapes from IDs or shape similarity, as well as shape sampling.



### Semi-parametric Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1804.10992v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1804.10992v1)
- **Published**: 2018-04-29 21:20:43+00:00
- **Updated**: 2018-04-29 21:20:43+00:00
- **Authors**: Xiaojuan Qi, Qifeng Chen, Jiaya Jia, Vladlen Koltun
- **Comment**: Published at the Conference on Computer Vision and Pattern
  Recognition (CVPR 2018)
- **Journal**: None
- **Summary**: We present a semi-parametric approach to photographic image synthesis from semantic layouts. The approach combines the complementary strengths of parametric and nonparametric techniques. The nonparametric component is a memory bank of image segments constructed from a training set of images. Given a novel semantic layout at test time, the memory bank is used to retrieve photographic references that are provided as source material to a deep network. The synthesis is performed by a deep network that draws on the provided photographic material. Experiments on multiple semantic segmentation datasets show that the presented approach yields considerably more realistic images than recent purely parametric techniques. The results are shown in the supplementary video at https://youtu.be/U4Q98lenGLQ



