# Arxiv Papers in cs.CV on 2018-04-16
### Comparative study of motion detection methods for video surveillance systems
- **Arxiv ID**: http://arxiv.org/abs/1804.05459v1
- **DOI**: 10.1117/1.JEI.26.2.023025
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.05459v1)
- **Published**: 2018-04-16 00:51:30+00:00
- **Updated**: 2018-04-16 00:51:30+00:00
- **Authors**: Kamal Sehairi, Chouireb Fatima, Jean Meunier
- **Comment**: 69 pages, 18 figures, journal paper
- **Journal**: J. Electron. Imaging 26(2), 023025 (2017)
- **Summary**: The objective of this study is to compare several change detection methods for a mono static camera and identify the best method for different complex environments and backgrounds in indoor and outdoor scenes. To this end, we used the CDnet video dataset as a benchmark that consists of many challenging problems, ranging from basic simple scenes to complex scenes affected by bad weather and dynamic backgrounds. Twelve change detection methods, ranging from simple temporal differencing to more sophisticated methods, were tested and several performance metrics were used to precisely evaluate the results. Because most of the considered methods have not previously been evaluated on this recent large scale dataset, this work compares these methods to fill a lack in the literature, and thus this evaluation joins as complementary compared with the previous comparative evaluations. Our experimental results show that there is no perfect method for all challenging cases, each method performs well in certain cases and fails in others. However, this study enables the user to identify the most suitable method for his or her needs.



### Im2Struct: Recovering 3D Shape Structure from a Single RGB Image
- **Arxiv ID**: http://arxiv.org/abs/1804.05469v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.05469v1)
- **Published**: 2018-04-16 01:32:30+00:00
- **Updated**: 2018-04-16 01:32:30+00:00
- **Authors**: Chengjie Niu, Jun Li, Kai Xu
- **Comment**: CVPR 2018
- **Journal**: None
- **Summary**: We propose to recover 3D shape structures from single RGB images, where structure refers to shape parts represented by cuboids and part relations encompassing connectivity and symmetry. Given a single 2D image with an object depicted, our goal is automatically recover a cuboid structure of the object parts as well as their mutual relations. We develop a convolutional-recursive auto-encoder comprised of structure parsing of a 2D image followed by structure recovering of a cuboid hierarchy. The encoder is achieved by a multi-scale convolutional network trained with the task of shape contour estimation, thereby learning to discern object structures in various forms and scales. The decoder fuses the features of the structure parsing network and the original image, and recursively decodes a hierarchy of cuboids. Since the decoder network is learned to recover part relations including connectivity and symmetry explicitly, the plausibility and generality of part structure recovery can be ensured. The two networks are jointly trained using the training data of contour-mask and cuboid structure pairs. Such pairs are generated by rendering stock 3D CAD models coming with part segmentation. Our method achieves unprecedentedly faithful and detailed recovery of diverse 3D part structures from single-view 2D images. We demonstrate two applications of our method including structure-guided completion of 3D volumes reconstructed from single-view images and structure-aware interactive editing of 2D images.



### Composable Unpaired Image to Image Translation
- **Arxiv ID**: http://arxiv.org/abs/1804.05470v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1804.05470v1)
- **Published**: 2018-04-16 01:38:11+00:00
- **Updated**: 2018-04-16 01:38:11+00:00
- **Authors**: Laura Graesser, Anant Gupta
- **Comment**: None
- **Journal**: None
- **Summary**: There has been remarkable recent work in unpaired image-to-image translation. However, they're restricted to translation on single pairs of distributions, with some exceptions. In this study, we extend one of these works to a scalable multidistribution translation mechanism. Our translation models not only converts from one distribution to another but can be stacked to create composite translation functions. We show that this composite property makes it possible to generate images with characteristics not seen in the training set. We also propose a decoupled training mechanism to train multiple distributions separately, which we show, generates better samples than isolated joint training. Further, we do a qualitative and quantitative analysis to assess the plausibility of the samples. The code is made available at https://github.com/lgraesser/im2im2im.



### Optimizing Video Object Detection via a Scale-Time Lattice
- **Arxiv ID**: http://arxiv.org/abs/1804.05472v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.05472v1)
- **Published**: 2018-04-16 01:52:01+00:00
- **Updated**: 2018-04-16 01:52:01+00:00
- **Authors**: Kai Chen, Jiaqi Wang, Shuo Yang, Xingcheng Zhang, Yuanjun Xiong, Chen Change Loy, Dahua Lin
- **Comment**: Accepted to CVPR 2018. Project page:
  http://mmlab.ie.cuhk.edu.hk/projects/ST-Lattice/
- **Journal**: None
- **Summary**: High-performance object detection relies on expensive convolutional networks to compute features, often leading to significant challenges in applications, e.g. those that require detecting objects from video streams in real time. The key to this problem is to trade accuracy for efficiency in an effective way, i.e. reducing the computing cost while maintaining competitive performance. To seek a good balance, previous efforts usually focus on optimizing the model architectures. This paper explores an alternative approach, that is, to reallocate the computation over a scale-time space. The basic idea is to perform expensive detection sparsely and propagate the results across both scales and time with substantially cheaper networks, by exploiting the strong correlations among them. Specifically, we present a unified framework that integrates detection, temporal propagation, and across-scale refinement on a Scale-Time Lattice. On this framework, one can explore various strategies to balance performance and cost. Taking advantage of this flexibility, we further develop an adaptive scheme with the detector invoked on demand and thus obtain improved tradeoff. On ImageNet VID dataset, the proposed method can achieve a competitive mAP 79.6% at 20 fps, or 79.0% at 62 fps as a performance/speed tradeoff.



### Binary Matrix Factorization via Dictionary Learning
- **Arxiv ID**: http://arxiv.org/abs/1804.05482v2
- **DOI**: 10.1109/JSTSP.2018.2875674
- **Categories**: **stat.ML**, cs.CV, cs.IR, cs.IT, cs.LG, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/1804.05482v2)
- **Published**: 2018-04-16 02:36:24+00:00
- **Updated**: 2018-07-26 01:13:05+00:00
- **Authors**: Ignacio Ramirez
- **Comment**: submitted for review to IEEE JSTSP on April 15th, 2018
- **Journal**: None
- **Summary**: Matrix factorization is a key tool in data analysis; its applications include recommender systems, correlation analysis, signal processing, among others. Binary matrices are a particular case which has received significant attention for over thirty years, especially within the field of data mining. Dictionary learning refers to a family of methods for learning overcomplete basis (also called frames) in order to efficiently encode samples of a given type; this area, now also about twenty years old, was mostly developed within the signal processing field. In this work we propose two binary matrix factorization methods based on a binary adaptation of the dictionary learning paradigm to binary matrices. The proposed algorithms focus on speed and scalability; they work with binary factors combined with bit-wise operations and a few auxiliary integer ones. Furthermore, the methods are readily applicable to online binary matrix factorization. Another important issue in matrix factorization is the choice of rank for the factors; we address this model selection problem with an efficient method based on the Minimum Description Length principle. Our preliminary results show that the proposed methods are effective at producing interpretable factorizations of various data types of different nature.



### Review on Optical Image Hiding and Watermarking Techniques
- **Arxiv ID**: http://arxiv.org/abs/1804.05483v1
- **DOI**: 10.1016/j.optlastec.2018.08.011
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.05483v1)
- **Published**: 2018-04-16 02:43:38+00:00
- **Updated**: 2018-04-16 02:43:38+00:00
- **Authors**: Shuming Jiao, Changyuan Zhou, Yishi Shi, Wenbin Zou, Xia Li
- **Comment**: None
- **Journal**: None
- **Summary**: Information security is a critical issue in modern society and image watermarking can effectively prevent unauthorized information access. Optical image watermarking techniques generally have advantages of parallel high-speed processing and multi-dimensional capabilities compared with digital approaches. This paper provides a comprehensive review on the research works related to optical image hiding and watermarking techniques conducted in the past decade. The past research works are focused on two major aspects, various optical systems for image hiding and the methods for embedding optical system output into a host image. A summary of the state-of-the-art works is made from these two perspectives.



### Multi-modality Sensor Data Classification with Selective Attention
- **Arxiv ID**: http://arxiv.org/abs/1804.05493v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.05493v2)
- **Published**: 2018-04-16 03:40:41+00:00
- **Updated**: 2018-05-01 14:12:52+00:00
- **Authors**: Xiang Zhang, Lina Yao, Chaoran Huang, Sen Wang, Mingkui Tan, Guodong Long, Can Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Multimodal wearable sensor data classification plays an important role in ubiquitous computing and has a wide range of applications in scenarios from healthcare to entertainment. However, most existing work in this field employs domain-specific approaches and is thus ineffective in complex sit- uations where multi-modality sensor data are col- lected. Moreover, the wearable sensor data are less informative than the conventional data such as texts or images. In this paper, to improve the adapt- ability of such classification methods across differ- ent application domains, we turn this classification task into a game and apply a deep reinforcement learning scheme to deal with complex situations dynamically. Additionally, we introduce a selective attention mechanism into the reinforcement learn- ing scheme to focus on the crucial dimensions of the data. This mechanism helps to capture extra information from the signal and thus it is able to significantly improve the discriminative power of the classifier. We carry out several experiments on three wearable sensor datasets and demonstrate the competitive performance of the proposed approach compared to several state-of-the-art baselines.



### Learning Simple Thresholded Features with Sparse Support Recovery
- **Arxiv ID**: http://arxiv.org/abs/1804.05515v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1804.05515v2)
- **Published**: 2018-04-16 06:20:55+00:00
- **Updated**: 2019-02-18 22:10:17+00:00
- **Authors**: Hongyu Xu, Zhangyang Wang, Haichuan Yang, Ding Liu, Ji Liu
- **Comment**: Accepted by IEEE Transactions on Circuits and Systems for Video
  Technology (TCSVT)
- **Journal**: None
- **Summary**: The thresholded feature has recently emerged as an extremely efficient, yet rough empirical approximation, of the time-consuming sparse coding inference process. Such an approximation has not yet been rigorously examined, and standard dictionaries often lead to non-optimal performance when used for computing thresholded features. In this paper, we first present two theoretical recovery guarantees for the thresholded feature to exactly recover the nonzero support of the sparse code. Motivated by them, we then formulate the Dictionary Learning for Thresholded Features (DLTF) model, which learns an optimized dictionary for applying the thresholded feature. In particular, for the $(k, 2)$ norm involved, a novel proximal operator with log-linear time complexity $O(m\log m)$ is derived. We evaluate the performance of DLTF on a vast range of synthetic and real-data tasks, where DLTF demonstrates remarkable efficiency, effectiveness and robustness in all experiments. In addition, we briefly discuss the potential link between DLTF and deep learning building blocks.



### Dual CNN Models for Unsupervised Monocular Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/1804.06324v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.06324v4)
- **Published**: 2018-04-16 07:04:36+00:00
- **Updated**: 2019-11-06 09:10:51+00:00
- **Authors**: Vamshi Krishna Repala, Shiv Ram Dubey
- **Comment**: Accepted in 8th Pattern Recognition and Machine Intelligence
  Conference (PReMI) 2019
- **Journal**: None
- **Summary**: The unsupervised depth estimation is the recent trend by utilizing the binocular stereo images to get rid of depth map ground truth. In unsupervised depth computation, the disparity images are generated by training the CNN with an image reconstruction loss. In this paper, a dual CNN based model is presented for unsupervised depth estimation with 6 losses (DNM6) with individual CNN for each view to generate the corresponding disparity map. The proposed dual CNN model is also extended with 12 losses (DNM12) by utilizing the cross disparities. The presented DNM6 and DNM12 models are experimented over KITTI driving and Cityscapes urban database and compared with the recent state-of-the-art result of unsupervised depth estimation. The code is available at: https://github.com/ishmav16/Dual-CNN-Models-for-Unsupervised-Monocular-Depth-Estimation.



### LoST? Appearance-Invariant Place Recognition for Opposite Viewpoints using Visual Semantics
- **Arxiv ID**: http://arxiv.org/abs/1804.05526v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1804.05526v3)
- **Published**: 2018-04-16 07:26:52+00:00
- **Updated**: 2018-05-26 10:34:17+00:00
- **Authors**: Sourav Garg, Niko Suenderhauf, Michael Milford
- **Comment**: Accepted for Robotics: Science and Systems (RSS) 2018. Source code
  now available at https://github.com/oravus/lostX
- **Journal**: None
- **Summary**: Human visual scene understanding is so remarkable that we are able to recognize a revisited place when entering it from the opposite direction it was first visited, even in the presence of extreme variations in appearance. This capability is especially apparent during driving: a human driver can recognize where they are when travelling in the reverse direction along a route for the first time, without having to turn back and look. The difficulty of this problem exceeds any addressed in past appearance- and viewpoint-invariant visual place recognition (VPR) research, in part because large parts of the scene are not commonly observable from opposite directions. Consequently, as shown in this paper, the precision-recall performance of current state-of-the-art viewpoint- and appearance-invariant VPR techniques is orders of magnitude below what would be usable in a closed-loop system. Current engineered solutions predominantly rely on panoramic camera or LIDAR sensing setups; an eminently suitable engineering solution but one that is clearly very different to how humans navigate, which also has implications for how naturally humans could interact and communicate with the navigation system. In this paper we develop a suite of novel semantic- and appearance-based techniques to enable for the first time high performance place recognition in this challenging scenario. We first propose a novel Local Semantic Tensor (LoST) descriptor of images using the convolutional feature maps from a state-of-the-art dense semantic segmentation network. Then, to verify the spatial semantic arrangement of the top matching candidates, we develop a novel approach for mining semantically-salient keypoint correspondences.



### A Novel Low-cost FPGA-based Real-time Object Tracking System
- **Arxiv ID**: http://arxiv.org/abs/1804.05535v2
- **DOI**: 10.1109/ASICON.2017.8252560
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.05535v2)
- **Published**: 2018-04-16 08:04:34+00:00
- **Updated**: 2018-04-22 13:55:04+00:00
- **Authors**: Peng Gao, Ruyue Yuan, Zhicong Lin, Linsheng Zhang, Yan Zhang
- **Comment**: Accepted by ASICON 2017
- **Journal**: None
- **Summary**: In current visual object tracking system, the CPU or GPU-based visual object tracking systems have high computational cost and consume a prohibitive amount of power. Therefore, in this paper, to reduce the computational burden of the Camshift algorithm, we propose a novel visual object tracking algorithm by exploiting the properties of the binary classifier and Kalman predictor. Moreover, we present a low-cost FPGA-based real-time object tracking hardware architecture. Extensive evaluations on OTB benchmark demonstrate that the proposed system has extremely compelling real-time, stability and robustness. The evaluation results show that the accuracy of our algorithm is about 48%, and the average speed is about 309 frames per second.



### A Novel Parallel Ray-Casting Algorithm
- **Arxiv ID**: http://arxiv.org/abs/1804.05541v2
- **DOI**: 10.1109/ICCWAMTIP.2016.8079804
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1804.05541v2)
- **Published**: 2018-04-16 08:15:38+00:00
- **Updated**: 2018-04-22 13:40:00+00:00
- **Authors**: Yan Zhang, Peng Gao, Xiao-Qing Li
- **Comment**: Accepted by ICCWAMTIP 2016
- **Journal**: None
- **Summary**: The Ray-Casting algorithm is an important method for fast real-time surface display from 3D medical images. Based on the Ray-Casting algorithm, a novel parallel Ray-Casting algorithm is proposed in this paper. A novel operation is introduced and defined as a star operation, and star operations can be computed in parallel in the proposed algorithm compared with the serial chain of star operations in the Ray-Casting algorithm. The computation complexity of the proposed algorithm is reduced from $O(n)$ to $O(\log^n_2)$.



### Particle-based pedestrian path prediction using LSTM-MDL models
- **Arxiv ID**: http://arxiv.org/abs/1804.05546v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.05546v3)
- **Published**: 2018-04-16 08:27:24+00:00
- **Updated**: 2018-08-29 07:02:13+00:00
- **Authors**: Ronny Hug, Stefan Becker, Wolfgang Hübner, Michael Arens
- **Comment**: Accepted at ITSC 2018
- **Journal**: None
- **Summary**: Recurrent neural networks are able to learn complex long-term relationships from sequential data and output a pdf over the state space. Therefore, recurrent models are a natural choice to address path prediction tasks, where a trained model is used to generate future expectations from past observations. When applied to security applications, like predicting the path of pedestrians for risk assessment, a point-wise greedy (ML) evaluation of the output pdf is not feasible, since the environment often allows multiple choices. Therefore, a robust risk assessment has to take all options into account, even if they are overall not very likely.   Towards this end, a combination of particle filter sampling strategies and a LSTM-MDL model is proposed to address a multi-modal path prediction task. The capabilities and viability of the proposed approach are evaluated on several synthetic test conditions, yielding the counter-intuitive result that the simplest approach performs best. Further, the feasibility of the proposed approach is illustrated on several real world scenes.



### Semantic Single-Image Dehazing
- **Arxiv ID**: http://arxiv.org/abs/1804.05624v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.05624v2)
- **Published**: 2018-04-16 11:59:31+00:00
- **Updated**: 2018-04-18 03:39:52+00:00
- **Authors**: Ziang Cheng, Shaodi You, Viorela Ila, Hongdong Li
- **Comment**: None
- **Journal**: None
- **Summary**: Single-image haze-removal is challenging due to limited information contained in one single image. Previous solutions largely rely on handcrafted priors to compensate for this deficiency. Recent convolutional neural network (CNN) models have been used to learn haze-related priors but they ultimately work as advanced image filters. In this paper we propose a novel semantic ap- proach towards single image haze removal. Unlike existing methods, we infer color priors based on extracted semantic features. We argue that semantic context can be exploited to give informative cues for (a) learning color prior on clean image and (b) estimating ambient illumination. This design allowed our model to recover clean images from challenging cases with strong ambiguity, e.g. saturated illumination color and sky regions in image. In experiments, we validate our ap- proach upon synthetic and real hazy images, where our method showed superior performance over state-of-the-art approaches, suggesting semantic information facilitates the haze removal task.



### Direct Sparse Visual-Inertial Odometry using Dynamic Marginalization
- **Arxiv ID**: http://arxiv.org/abs/1804.05625v1
- **DOI**: 10.1109/ICRA.2018.8462905
- **Categories**: **cs.CV**, cs.RO, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/1804.05625v1)
- **Published**: 2018-04-16 12:00:56+00:00
- **Updated**: 2018-04-16 12:00:56+00:00
- **Authors**: Lukas von Stumberg, Vladyslav Usenko, Daniel Cremers
- **Comment**: None
- **Journal**: None
- **Summary**: We present VI-DSO, a novel approach for visual-inertial odometry, which jointly estimates camera poses and sparse scene geometry by minimizing photometric and IMU measurement errors in a combined energy functional. The visual part of the system performs a bundle-adjustment like optimization on a sparse set of points, but unlike key-point based systems it directly minimizes a photometric error. This makes it possible for the system to track not only corners, but any pixels with large enough intensity gradients. IMU information is accumulated between several frames using measurement preintegration, and is inserted into the optimization as an additional constraint between keyframes. We explicitly include scale and gravity direction into our model and jointly optimize them together with other variables such as poses. As the scale is often not immediately observable using IMU data this allows us to initialize our visual-inertial system with an arbitrary scale instead of having to delay the initialization until everything is observable. We perform partial marginalization of old variables so that updates can be computed in a reasonable time. In order to keep the system consistent we propose a novel strategy which we call "dynamic marginalization". This technique allows us to use partial marginalization even in cases where the initial scale estimate is far from the optimum. We evaluate our method on the challenging EuRoC dataset, showing that VI-DSO outperforms the state of the art.



### IterGANs: Iterative GANs to Learn and Control 3D Object Transformation
- **Arxiv ID**: http://arxiv.org/abs/1804.05651v2
- **DOI**: 10.1016/j.cviu.2019.102803
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.05651v2)
- **Published**: 2018-04-16 13:08:58+00:00
- **Updated**: 2019-09-04 07:03:52+00:00
- **Authors**: Ysbrand Galama, Thomas Mensink
- **Comment**: None
- **Journal**: None
- **Summary**: We are interested in learning visual representations which allow for 3D manipulations of visual objects based on a single 2D image. We cast this into an image-to-image transformation task, and propose Iterative Generative Adversarial Networks (IterGANs) which iteratively transform an input image into an output image. Our models learn a visual representation that can be used for objects seen in training, but also for never seen objects. Since object manipulation requires a full understanding of the geometry and appearance of the object, our IterGANs learn an implicit 3D model and a full appearance model of the object, which are both inferred from a single (test) image. Two advantages of IterGANs are that the intermediate generated images can be used for an additional supervision signal, even in an unsupervised fashion, and that the number of iterations can be used as a control signal to steer the transformation. Experiments on rotated objects and scenes show how IterGANs help with the generation process.



### Neural Kinematic Networks for Unsupervised Motion Retargetting
- **Arxiv ID**: http://arxiv.org/abs/1804.05653v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.05653v1)
- **Published**: 2018-04-16 13:15:50+00:00
- **Updated**: 2018-04-16 13:15:50+00:00
- **Authors**: Ruben Villegas, Jimei Yang, Duygu Ceylan, Honglak Lee
- **Comment**: IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
- **Journal**: None
- **Summary**: We propose a recurrent neural network architecture with a Forward Kinematics layer and cycle consistency based adversarial training objective for unsupervised motion retargetting. Our network captures the high-level properties of an input motion by the forward kinematics layer, and adapts them to a target character with different skeleton bone lengths (e.g., shorter, longer arms etc.). Collecting paired motion training sequences from different characters is expensive. Instead, our network utilizes cycle consistency to learn to solve the Inverse Kinematics problem in an unsupervised manner. Our method works online, i.e., it adapts the motion sequence on-the-fly as new frames are received. In our experiments, we use the Mixamo animation data to test our method for a variety of motions and characters and achieve state-of-the-art results. We also demonstrate motion retargetting from monocular human videos to 3D characters using an off-the-shelf 3D pose estimator.



### An Extended Beta-Elliptic Model and Fuzzy Elementary Perceptual Codes for Online Multilingual Writer Identification using Deep Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1804.05661v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.05661v4)
- **Published**: 2018-04-16 13:27:11+00:00
- **Updated**: 2018-11-10 11:28:36+00:00
- **Authors**: Thameur Dhieb, Sourour Njah, Houcine Boubaker, Wael Ouarda, Mounir Ben Ayed, Adel M. Alimi
- **Comment**: None
- **Journal**: None
- **Summary**: Actually, the ability to identify the documents authors provides more chances for using these documents for various purposes. In this paper, we present a new effective biometric writer identification system from online handwriting. The system consists of the preprocessing and the segmentation of online handwriting into a sequence of Beta strokes in a first step. Then, from each stroke, we extract a set of static and dynamic features from new proposed model that we called Extended Beta-Elliptic model and from the Fuzzy Elementary Perceptual Codes. Next, all the segments which are composed of N consecutive strokes are categorized into groups and subgroups according to their position and their geometric characteristics. Finally, Deep Neural Network is used as classifier. Experimental results reveal that the proposed system achieves interesting results as compared to those of the existing writer identification systems on Latin and Arabic scripts.



### Training convolutional neural networks with megapixel images
- **Arxiv ID**: http://arxiv.org/abs/1804.05712v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.05712v1)
- **Published**: 2018-04-16 14:52:22+00:00
- **Updated**: 2018-04-16 14:52:22+00:00
- **Authors**: Hans Pinckaers, Geert Litjens
- **Comment**: Submitted to MIDL 2018
- **Journal**: None
- **Summary**: To train deep convolutional neural networks, the input data and the intermediate activations need to be kept in memory to calculate the gradient descent step. Given the limited memory available in the current generation accelerator cards, this limits the maximum dimensions of the input data. We demonstrate a method to train convolutional neural networks holding only parts of the image in memory while giving equivalent results. We quantitatively compare this new way of training convolutional neural networks with conventional training. In addition, as a proof of concept, we train a convolutional neural network with 64 megapixel images, which requires 97% less memory than the conventional approach.



### Classifying magnetic resonance image modalities with convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/1804.05764v2
- **DOI**: 10.1117/12.2293943
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.05764v2)
- **Published**: 2018-04-16 16:08:39+00:00
- **Updated**: 2018-04-17 01:47:16+00:00
- **Authors**: Samuel Remedios, Dzung L. Pham, John A. Butman, Snehashis Roy
- **Comment**: Github: https://github.com/sremedios/phinet
- **Journal**: None
- **Summary**: Magnetic Resonance (MR) imaging allows the acquisition of images with different contrast properties depending on the acquisition protocol and the magnetic properties of tissues. Many MR brain image processing techniques, such as tissue segmentation, require multiple MR contrasts as inputs, and each contrast is treated differently. Thus it is advantageous to automate the identification of image contrasts for various purposes, such as facilitating image processing pipelines, and managing and maintaining large databases via content-based image retrieval (CBIR). Most automated CBIR techniques focus on a two-step process: extracting features from data and classifying the image based on these features. We present a novel 3D deep convolutional neural network (CNN)-based method for MR image contrast classification. The proposed CNN automatically identifies the MR contrast of an input brain image volume. Specifically, we explored three classification problems: (1) identify T1-weighted (T1-w), T2-weighted (T2-w), and fluid-attenuated inversion recovery (FLAIR) contrasts, (2) identify pre vs post-contrast T1, (3) identify pre vs post-contrast FLAIR. A total of 3418 image volumes acquired from multiple sites and multiple scanners were used. To evaluate each task, the proposed model was trained on 2137 images and tested on the remaining 1281 images. Results showed that image volumes were correctly classified with 97.57% accuracy.



### Multi-Modal Emotion recognition on IEMOCAP Dataset using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1804.05788v3
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1804.05788v3)
- **Published**: 2018-04-16 16:58:37+00:00
- **Updated**: 2019-11-06 20:10:26+00:00
- **Authors**: Samarth Tripathi, Sarthak Tripathi, Homayoon Beigi
- **Comment**: None
- **Journal**: None
- **Summary**: Emotion recognition has become an important field of research in Human Computer Interactions as we improve upon the techniques for modelling the various aspects of behaviour. With the advancement of technology our understanding of emotions are advancing, there is a growing need for automatic emotion recognition systems. One of the directions the research is heading is the use of Neural Networks which are adept at estimating complex functions that depend on a large number and diverse source of input data. In this paper we attempt to exploit this effectiveness of Neural networks to enable us to perform multimodal Emotion recognition on IEMOCAP dataset using data from Speech, Text, and Motion capture data from face expressions, rotation and hand movements. Prior research has concentrated on Emotion detection from Speech on the IEMOCAP dataset, but our approach is the first that uses the multiple modes of data offered by IEMOCAP for a more robust and accurate emotion detection.



### Materials for Masses: SVBRDF Acquisition with a Single Mobile Phone Image
- **Arxiv ID**: http://arxiv.org/abs/1804.05790v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.05790v1)
- **Published**: 2018-04-16 16:59:38+00:00
- **Updated**: 2018-04-16 16:59:38+00:00
- **Authors**: Zhengqin Li, Kalyan Sunkavalli, Manmohan Chandraker
- **Comment**: submitted to European Conference on Computer Vision
- **Journal**: None
- **Summary**: We propose a material acquisition approach to recover the spatially-varying BRDF and normal map of a near-planar surface from a single image captured by a handheld mobile phone camera. Our method images the surface under arbitrary environment lighting with the flash turned on, thereby avoiding shadows while simultaneously capturing high-frequency specular highlights. We train a CNN to regress an SVBRDF and surface normals from this image. Our network is trained using a large-scale SVBRDF dataset and designed to incorporate physical insights for material estimation, including an in-network rendering layer to model appearance and a material classifier to provide additional supervision during training. We refine the results from the network using a dense CRF module whose terms are designed specifically for our task. The framework is trained end-to-end and produces high quality results for a variety of materials. We provide extensive ablation studies to evaluate our network on both synthetic and real data, while demonstrating significant improvements in comparisons with prior works.



### Global Robustness Evaluation of Deep Neural Networks with Provable Guarantees for the $L_0$ Norm
- **Arxiv ID**: http://arxiv.org/abs/1804.05805v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1804.05805v2)
- **Published**: 2018-04-16 17:24:51+00:00
- **Updated**: 2018-11-20 16:57:22+00:00
- **Authors**: Wenjie Ruan, Min Wu, Youcheng Sun, Xiaowei Huang, Daniel Kroening, Marta Kwiatkowska
- **Comment**: 42 Pages, Github: https://github.com/TrustAI/L0-TRE
- **Journal**: None
- **Summary**: Deployment of deep neural networks (DNNs) in safety- or security-critical systems requires provable guarantees on their correct behaviour. A common requirement is robustness to adversarial perturbations in a neighbourhood around an input. In this paper we focus on the $L_0$ norm and aim to compute, for a trained DNN and an input, the maximal radius of a safe norm ball around the input within which there are no adversarial examples. Then we define global robustness as an expectation of the maximal safe radius over a test data set. We first show that the problem is NP-hard, and then propose an approximate approach to iteratively compute lower and upper bounds on the network's robustness. The approach is \emph{anytime}, i.e., it returns intermediate bounds and robustness estimates that are gradually, but strictly, improved as the computation proceeds; \emph{tensor-based}, i.e., the computation is conducted over a set of inputs simultaneously, instead of one by one, to enable efficient GPU computation; and has \emph{provable guarantees}, i.e., both the bounds and the robustness estimates can converge to their optimal values. Finally, we demonstrate the utility of the proposed approach in practice to compute tight bounds by applying and adapting the anytime algorithm to a set of challenging problems, including global robustness evaluation, competitive $L_0$ attacks, test case generation for DNNs, and local robustness evaluation on large-scale ImageNet DNNs. We release the code of all case studies via GitHub.



### ShapeShifter: Robust Physical Adversarial Attack on Faster R-CNN Object Detector
- **Arxiv ID**: http://arxiv.org/abs/1804.05810v3
- **DOI**: 10.1007/978-3-030-10925-7_4
- **Categories**: **cs.CV**, cs.CR, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1804.05810v3)
- **Published**: 2018-04-16 17:29:43+00:00
- **Updated**: 2019-05-01 03:41:44+00:00
- **Authors**: Shang-Tse Chen, Cory Cornelius, Jason Martin, Duen Horng Chau
- **Comment**: None
- **Journal**: Joint European Conference on Machine Learning and Knowledge
  Discovery in Databases, pp. 52-68, 2018
- **Summary**: Given the ability to directly manipulate image pixels in the digital input space, an adversary can easily generate imperceptible perturbations to fool a Deep Neural Network (DNN) image classifier, as demonstrated in prior work. In this work, we propose ShapeShifter, an attack that tackles the more challenging problem of crafting physical adversarial perturbations to fool image-based object detectors like Faster R-CNN. Attacking an object detector is more difficult than attacking an image classifier, as it needs to mislead the classification results in multiple bounding boxes with different scales. Extending the digital attack to the physical world adds another layer of difficulty, because it requires the perturbation to be robust enough to survive real-world distortions due to different viewing distances and angles, lighting conditions, and camera limitations. We show that the Expectation over Transformation technique, which was originally proposed to enhance the robustness of adversarial perturbations in image classification, can be successfully adapted to the object detection setting. ShapeShifter can generate adversarially perturbed stop signs that are consistently mis-detected by Faster R-CNN as other objects, posing a potential threat to autonomous vehicles and other safety-critical computer vision systems.



### DCAN: Dual Channel-wise Alignment Networks for Unsupervised Scene Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1804.05827v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.05827v1)
- **Published**: 2018-04-16 17:54:08+00:00
- **Updated**: 2018-04-16 17:54:08+00:00
- **Authors**: Zuxuan Wu, Xintong Han, Yen-Liang Lin, Mustafa Gkhan Uzunbas, Tom Goldstein, Ser Nam Lim, Larry S. Davis
- **Comment**: None
- **Journal**: None
- **Summary**: Harvesting dense pixel-level annotations to train deep neural networks for semantic segmentation is extremely expensive and unwieldy at scale. While learning from synthetic data where labels are readily available sounds promising, performance degrades significantly when testing on novel realistic data due to domain discrepancies. We present Dual Channel-wise Alignment Networks (DCAN), a simple yet effective approach to reduce domain shift at both pixel-level and feature-level. Exploring statistics in each channel of CNN feature maps, our framework performs channel-wise feature alignment, which preserves spatial structures and semantic information, in both an image generator and a segmentation network. In particular, given an image from the source domain and unlabeled samples from the target domain, the generator synthesizes new images on-the-fly to resemble samples from the target domain in appearance and the segmentation network further refines high-level features before predicting semantic maps, both of which leverage feature statistics of sampled images from the target domain. Unlike much recent and concurrent work relying on adversarial training, our framework is lightweight and easy to train. Extensive experiments on adapting models trained on synthetic segmentation benchmarks to real urban scenes demonstrate the effectiveness of the proposed framework.



### Towards High Performance Video Object Detection for Mobiles
- **Arxiv ID**: http://arxiv.org/abs/1804.05830v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.05830v1)
- **Published**: 2018-04-16 17:59:36+00:00
- **Updated**: 2018-04-16 17:59:36+00:00
- **Authors**: Xizhou Zhu, Jifeng Dai, Xingchi Zhu, Yichen Wei, Lu Yuan
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the recent success of video object detection on Desktop GPUs, its architecture is still far too heavy for mobiles. It is also unclear whether the key principles of sparse feature propagation and multi-frame feature aggregation apply at very limited computational resources. In this paper, we present a light weight network architecture for video object detection on mobiles. Light weight image object detector is applied on sparse key frames. A very small network, Light Flow, is designed for establishing correspondence across frames. A flow-guided GRU module is designed to effectively aggregate features on key frames. For non-key frames, sparse feature propagation is performed. The whole network can be trained end-to-end. The proposed system achieves 60.2% mAP score at speed of 25.6 fps on mobiles (e.g., HuaWei Mate 8).



### Fast inference of deep neural networks in FPGAs for particle physics
- **Arxiv ID**: http://arxiv.org/abs/1804.06913v3
- **DOI**: 10.1088/1748-0221/13/07/P07027
- **Categories**: **physics.ins-det**, cs.CV, hep-ex, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1804.06913v3)
- **Published**: 2018-04-16 18:00:02+00:00
- **Updated**: 2018-06-28 21:43:10+00:00
- **Authors**: Javier Duarte, Song Han, Philip Harris, Sergo Jindariani, Edward Kreinar, Benjamin Kreis, Jennifer Ngadiuba, Maurizio Pierini, Ryan Rivera, Nhan Tran, Zhenbin Wu
- **Comment**: 22 pages, 17 figures, 2 tables, JINST revision
- **Journal**: JINST 13 P07027 (2018)
- **Summary**: Recent results at the Large Hadron Collider (LHC) have pointed to enhanced physics capabilities through the improvement of the real-time event processing techniques. Machine learning methods are ubiquitous and have proven to be very powerful in LHC physics, and particle physics as a whole. However, exploration of the use of such techniques in low-latency, low-power FPGA hardware has only just begun. FPGA-based trigger and data acquisition (DAQ) systems have extremely low, sub-microsecond latency requirements that are unique to particle physics. We present a case study for neural network inference in FPGAs focusing on a classifier for jet substructure which would enable, among many other physics scenarios, searches for new dark sector particles and novel measurements of the Higgs boson. While we focus on a specific example, the lessons are far-reaching. We develop a package based on High-Level Synthesis (HLS) called hls4ml to build machine learning models in FPGAs. The use of HLS increases accessibility across a broad user community and allows for a drastic decrease in firmware development time. We map out FPGA resource usage and latency versus neural network hyperparameters to identify the problems in particle physics that would benefit from performing neural network inference with FPGAs. For our example jet substructure model, we fit well within the available resources of modern FPGAs with a latency on the scale of 100 ns.



### Egocentric 6-DoF Tracking of Small Handheld Objects
- **Arxiv ID**: http://arxiv.org/abs/1804.05870v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.05870v1)
- **Published**: 2018-04-16 18:08:51+00:00
- **Updated**: 2018-04-16 18:08:51+00:00
- **Authors**: Rohit Pandey, Pavel Pidlypenskyi, Shuoran Yang, Christine Kaeser-Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Virtual and augmented reality technologies have seen significant growth in the past few years. A key component of such systems is the ability to track the pose of head mounted displays and controllers in 3D space. We tackle the problem of efficient 6-DoF tracking of a handheld controller from egocentric camera perspectives. We collected the HMD Controller dataset which consist of over 540,000 stereo image pairs labelled with the full 6-DoF pose of the handheld controller. Our proposed SSD-AF-Stereo3D model achieves a mean average error of 33.5 millimeters in 3D keypoint prediction and is used in conjunction with an IMU sensor on the controller to enable 6-DoF tracking. We also present results on approaches for model based full 6-DoF tracking. All our models operate under the strict constraints of real time mobile CPU inference.



### M-PACT: An Open Source Platform for Repeatable Activity Classification Research
- **Arxiv ID**: http://arxiv.org/abs/1804.05879v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.05879v3)
- **Published**: 2018-04-16 18:20:14+00:00
- **Updated**: 2018-10-05 18:14:21+00:00
- **Authors**: Eric Hofesmann, Madan Ravi Ganesh, Jason J. Corso
- **Comment**: None
- **Journal**: None
- **Summary**: There are many hurdles that prevent the replication of existing work which hinders the development of new activity classification models. These hurdles include switching between multiple deep learning libraries and the development of boilerplate experimental pipelines. We present M-PACT to overcome existing issues by removing the need to develop boilerplate code which allows users to quickly prototype action classification models while leveraging existing state-of-the-art (SOTA) models available in the platform. M-PACT is the first to offer four SOTA activity classification models, I3D, C3D, ResNet50+LSTM, and TSN, under a single platform with reproducible competitive results. This platform allows for the generation of models and results over activity recognition datasets through the use of modular code, various preprocessing and neural network layers, and seamless data flow. In this paper, we present the system architecture, detail the functions of various modules, and describe the basic tools to develop a new model in M-PACT.



### Densely Connected High Order Residual Network for Single Frame Image Super Resolution
- **Arxiv ID**: http://arxiv.org/abs/1804.05902v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.05902v1)
- **Published**: 2018-04-16 19:25:33+00:00
- **Updated**: 2018-04-16 19:25:33+00:00
- **Authors**: Yiwen Huang, Ming Qin
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolutional neural networks (DCNN) have been widely adopted for research on super resolution recently, however previous work focused mainly on stacking as many layers as possible in their model, in this paper, we present a new perspective regarding to image restoration problems that we can construct the neural network model reflecting the physical significance of the image restoration process, that is, embedding the a priori knowledge of image restoration directly into the structure of our neural network model, we employed a symmetric non-linear colorspace, the sigmoidal transfer, to replace traditional transfers such as, sRGB, Rec.709, which are asymmetric non-linear colorspaces, we also propose a "reuse plus patch" method to deal with super resolution of different scaling factors, our proposed methods and model show generally superior performance over previous work even though our model was only roughly trained and could still be underfitting the training set.



### Tree Morphology for Phenotyping from Semantics-Based Mapping in Orchard Environments
- **Arxiv ID**: http://arxiv.org/abs/1804.05905v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1804.05905v1)
- **Published**: 2018-04-16 19:33:37+00:00
- **Updated**: 2018-04-16 19:33:37+00:00
- **Authors**: Wenbo Dong, Volkan Isler
- **Comment**: 10 pages, 13 figures
- **Journal**: None
- **Summary**: Measuring tree morphology for phenotyping is an essential but labor-intensive activity in horticulture. Researchers often rely on manual measurements which may not be accurate for example when measuring tree volume. Recent approaches on automating the measurement process rely on LIDAR measurements coupled with high-accuracy GPS. Usually each side of a row is reconstructed independently and then merged using GPS information. Such approaches have two disadvantages: (1) they rely on specialized and expensive equipment, and (2) since the reconstruction process does not simultaneously use information from both sides, side reconstructions may not be accurate. We also show that standard loop closure methods do not necessarily align tree trunks well. In this paper, we present a novel vision system that employs only an RGB-D camera to estimate morphological parameters. A semantics-based mapping algorithm merges the two-sides 3D models of tree rows, where integrated semantic information is obtained and refined by robust fitting algorithms. We focus on measuring tree height, canopy volume and trunk diameter from the optimized 3D model. Experiments conducted in real orchards quantitatively demonstrate the accuracy of our method.



### Segmentation of both Diseased and Healthy Skin from Clinical Photographs in a Primary Care Setting
- **Arxiv ID**: http://arxiv.org/abs/1804.05944v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.05944v2)
- **Published**: 2018-04-16 21:08:04+00:00
- **Updated**: 2018-04-18 01:38:05+00:00
- **Authors**: Noel C. F. Codella, Daren Anderson, Tyler Philips, Anthony Porto, Kevin Massey, Jane Snowdon, Rogerio Feris, John Smith
- **Comment**: Accepted to IEEE EMBC 2018
- **Journal**: None
- **Summary**: This work presents the first segmentation study of both diseased and healthy skin in standard camera photographs from a clinical environment. Challenges arise from varied lighting conditions, skin types, backgrounds, and pathological states. For study, 400 clinical photographs (with skin segmentation masks) representing various pathological states of skin are retrospectively collected from a primary care network. 100 images are used for training and fine-tuning, and 300 are used for evaluation. This distribution between training and test partitions is chosen to reflect the difficulty in amassing large quantities of labeled data in this domain. A deep learning approach is used, and 3 public segmentation datasets of healthy skin are collected to study the potential benefits of pre-training. Two variants of U-Net are evaluated: U-Net and Dense Residual U-Net. We find that Dense Residual U-Nets have a 7.8% improvement in Jaccard, compared to classical U-Net architectures (0.55 vs. 0.51 Jaccard), for direct transfer, where fine-tuning data is not utilized. However, U-Net outperforms Dense Residual U-Net for both direct training (0.83 vs. 0.80) and fine-tuning (0.89 vs. 0.88). The stark performance improvement with fine-tuning compared to direct transfer and direct training emphasizes both the need for adequate representative data of diseased skin, and the utility of other publicly available data sources for this task.



### A Fusion Framework for Camouflaged Moving Foreground Detection in the Wavelet Domain
- **Arxiv ID**: http://arxiv.org/abs/1804.05984v1
- **DOI**: 10.1109/TIP.2018.2828329
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.05984v1)
- **Published**: 2018-04-16 23:49:56+00:00
- **Updated**: 2018-04-16 23:49:56+00:00
- **Authors**: Shuai Li, Dinei Florencio, Wanqing Li, Yaqin Zhao, Chris Cook
- **Comment**: 13 pages, accepted by IEEE TIP
- **Journal**: None
- **Summary**: Detecting camouflaged moving foreground objects has been known to be difficult due to the similarity between the foreground objects and the background. Conventional methods cannot distinguish the foreground from background due to the small differences between them and thus suffer from under-detection of the camouflaged foreground objects. In this paper, we present a fusion framework to address this problem in the wavelet domain. We first show that the small differences in the image domain can be highlighted in certain wavelet bands. Then the likelihood of each wavelet coefficient being foreground is estimated by formulating foreground and background models for each wavelet band. The proposed framework effectively aggregates the likelihoods from different wavelet bands based on the characteristics of the wavelet transform. Experimental results demonstrated that the proposed method significantly outperformed existing methods in detecting camouflaged foreground objects. Specifically, the average F-measure for the proposed algorithm was 0.87, compared to 0.71 to 0.8 for the other state-of-the-art methods.



