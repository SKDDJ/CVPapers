# Arxiv Papers in cs.CV on 2018-04-20
### High Dynamic Range SLAM with Map-Aware Exposure Time Control
- **Arxiv ID**: http://arxiv.org/abs/1804.07427v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.07427v1)
- **Published**: 2018-04-20 02:26:09+00:00
- **Updated**: 2018-04-20 02:26:09+00:00
- **Authors**: Sergey V. Alexandrov, Johann Prankl, Michael Zillich, Markus Vincze
- **Comment**: 3DV 2017
- **Journal**: None
- **Summary**: The research in dense online 3D mapping is mostly focused on the geometrical accuracy and spatial extent of the reconstructions. Their color appearance is often neglected, leading to inconsistent colors and noticeable artifacts. We rectify this by extending a state-of-the-art SLAM system to accumulate colors in HDR space. We replace the simplistic pixel intensity averaging scheme with HDR color fusion rules tailored to the incremental nature of SLAM and a noise model suitable for off-the-shelf RGB-D cameras. Our main contribution is a map-aware exposure time controller. It makes decisions based on the global state of the map and predicted camera motion, attempting to maximize the information gain of each observation. We report a set of experiments demonstrating the improved texture quality and advantages of using the custom controller that is tightly integrated in the mapping loop.



### Calibration-free B0 correction of EPI data using structured low rank matrix recovery
- **Arxiv ID**: http://arxiv.org/abs/1804.07436v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.07436v1)
- **Published**: 2018-04-20 03:13:27+00:00
- **Updated**: 2018-04-20 03:13:27+00:00
- **Authors**: Arvind Balachandrasekaran, Merry Mani, Mathews Jacob
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a structured low rank algorithm for the calibration-free compensation of field inhomogeneity artifacts in Echo Planar Imaging (EPI) MRI data. We acquire the data using two EPI readouts that differ in echo-time (TE). Using time segmentation, we reformulate the field inhomogeneity compensation problem as the recovery of an image time series from highly undersampled Fourier measurements. The temporal profile at each pixel is modeled as a single exponential, which is exploited to fill in the missing entries. We show that the exponential behavior at each pixel, along with the spatial smoothness of the exponential parameters, can be exploited to derive a 3D annihilation relation in the Fourier domain. This relation translates to a low rank property on a structured multi-fold Toeplitz matrix, whose entries correspond to the measured k-space samples. We introduce a fast two-step algorithm for the completion of the Toeplitz matrix from the available samples. In the first step, we estimate the null space vectors of the Toeplitz matrix using only its fully sampled rows. The null space is then used to estimate the signal subspace, which facilitates the efficient recovery of the time series of images. We finally demonstrate the proposed approach on spherical MR phantom data and human data and show that the artifacts are significantly reduced. The proposed approach could potentially be used to compensate for time varying field map variations in dynamic applications such as functional MRI.



### Vision Meets Drones: A Challenge
- **Arxiv ID**: http://arxiv.org/abs/1804.07437v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.07437v2)
- **Published**: 2018-04-20 03:19:21+00:00
- **Updated**: 2018-04-23 02:49:46+00:00
- **Authors**: Pengfei Zhu, Longyin Wen, Xiao Bian, Haibin Ling, Qinghua Hu
- **Comment**: 11 pages, 11 figures
- **Journal**: None
- **Summary**: In this paper we present a large-scale visual object detection and tracking benchmark, named VisDrone2018, aiming at advancing visual understanding tasks on the drone platform. The images and video sequences in the benchmark were captured over various urban/suburban areas of 14 different cities across China from north to south. Specifically, VisDrone2018 consists of 263 video clips and 10,209 images (no overlap with video clips) with rich annotations, including object bounding boxes, object categories, occlusion, truncation ratios, etc. With intensive amount of effort, our benchmark has more than 2.5 million annotated instances in 179,264 images/video frames. Being the largest such dataset ever published, the benchmark enables extensive evaluation and investigation of visual analysis algorithms on the drone platform. In particular, we design four popular tasks with the benchmark, including object detection in images, object detection in videos, single object tracking, and multi-object tracking. All these tasks are extremely challenging in the proposed dataset due to factors such as occlusion, large scale and pose variation, and fast motion. We hope the benchmark largely boost the research and development in visual analysis on drone platforms.



### View Adaptive Neural Networks for High Performance Skeleton-based Human Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1804.07453v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.07453v3)
- **Published**: 2018-04-20 05:37:47+00:00
- **Updated**: 2019-09-03 08:08:52+00:00
- **Authors**: Pengfei Zhang, Cuiling Lan, Junliang Xing, Wenjun Zeng, Jianru Xue, Nanning Zheng
- **Comment**: Accepted in Transactions on Pattern Analysis and Machine Intelligence
  (TPAMI)
- **Journal**: None
- **Summary**: Skeleton-based human action recognition has recently attracted increasing attention thanks to the accessibility and the popularity of 3D skeleton data. One of the key challenges in skeleton-based action recognition lies in the large view variations when capturing data. In order to alleviate the effects of view variations, this paper introduces a novel view adaptation scheme, which automatically determines the virtual observation viewpoints in a learning based data driven manner. We design two view adaptive neural networks, i.e., VA-RNN based on RNN, and VA-CNN based on CNN. For each network, a novel view adaptation module learns and determines the most suitable observation viewpoints, and transforms the skeletons to those viewpoints for the end-to-end recognition with a main classification network. Ablation studies find that the proposed view adaptive models are capable of transforming the skeletons of various viewpoints to much more consistent virtual viewpoints which largely eliminates the viewpoint influence. In addition, we design a two-stream scheme (referred to as VA-fusion) that fuses the scores of the two networks to provide the fused prediction. Extensive experimental evaluations on five challenging benchmarks demonstrate that the effectiveness of the proposed view-adaptive networks and superior performance over state-of-the-art approaches. The source code is available at https://github.com/microsoft/View-Adaptive-Neural-Networks-for-Skeleton-based-Human-Action-Recognition.



### Generating a Fusion Image: One's Identity and Another's Shape
- **Arxiv ID**: http://arxiv.org/abs/1804.07455v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.07455v2)
- **Published**: 2018-04-20 06:00:31+00:00
- **Updated**: 2022-01-26 03:09:45+00:00
- **Authors**: Donggyu Joo, Doyeon Kim, Junmo Kim
- **Comment**: CVPR 2018
- **Journal**: None
- **Summary**: Generating a novel image by manipulating two input images is an interesting research problem in the study of generative adversarial networks (GANs). We propose a new GAN-based network that generates a fusion image with the identity of input image x and the shape of input image y. Our network can simultaneously train on more than two image datasets in an unsupervised manner. We define an identity loss LI to catch the identity of image x and a shape loss LS to get the shape of y. In addition, we propose a novel training method called Min-Patch training to focus the generator on crucial parts of an image, rather than its entirety. We show qualitative results on the VGG Youtube Pose dataset, Eye dataset (MPIIGaze and UnityEyes), and the Photo-Sketch-Cartoon dataset.



### A Complementary Tracking Model with Multiple Features
- **Arxiv ID**: http://arxiv.org/abs/1804.07459v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1804.07459v3)
- **Published**: 2018-04-20 06:23:37+00:00
- **Updated**: 2018-07-19 08:47:44+00:00
- **Authors**: Peng Gao, Yipeng Ma, Chao Li, Ke Song, Fei Wang, Liyi Xiao
- **Comment**: Accepted by IVPAI 2018
- **Journal**: None
- **Summary**: Discriminative Correlation Filters based tracking algorithms exploiting conventional handcrafted features have achieved impressive results both in terms of accuracy and robustness. Template handcrafted features have shown excellent performance, but they perform poorly when the appearance of target changes rapidly such as fast motions and fast deformations. In contrast, statistical handcrafted features are insensitive to fast states changes, but they yield inferior performance in the scenarios of illumination variations and background clutters. In this work, to achieve an efficient tracking performance, we propose a novel visual tracking algorithm, named MFCMT, based on a complementary ensemble model with multiple features, including Histogram of Oriented Gradients (HOGs), Color Names (CNs) and Color Histograms (CHs). Additionally, to improve tracking results and prevent targets drift, we introduce an effective fusion method by exploiting relative entropy to coalesce all basic response maps and get an optimal response. Furthermore, we suggest a simple but efficient update strategy to boost tracking performance. Comprehensive evaluations are conducted on two tracking benchmarks demonstrate and the experimental results demonstrate that our method is competitive with numerous state-of-the-art trackers. Our tracker achieves impressive performance with faster speed on these benchmarks.



### Accurate Deep Direct Geo-Localization from Ground Imagery and Phone-Grade GPS
- **Arxiv ID**: http://arxiv.org/abs/1804.07470v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.07470v1)
- **Published**: 2018-04-20 07:01:59+00:00
- **Updated**: 2018-04-20 07:01:59+00:00
- **Authors**: Shaohui Sun, Ramesh Sarukkai, Jack Kwok, Vinay Shet
- **Comment**: To appear in CVPR 2018 Workshops
- **Journal**: None
- **Summary**: One of the most critical topics in autonomous driving or ride-sharing technology is to accurately localize vehicles in the world frame. In addition to common multi-view camera systems, it usually also relies on industrial grade sensors, such as LiDAR, differential GPS, high precision IMU, and etc. In this paper, we develop an approach to provide an effective solution to this problem. We propose a method to train a geo-spatial deep neural network (CNN+LSTM) to predict accurate geo-locations (latitude and longitude) using only ordinary ground imagery and low accuracy phone-grade GPS. We evaluate our approach on the open dataset released during ACM Multimedia 2017 Grand Challenge. Having ground truth locations for training, we are able to reach nearly lane-level accuracy. We also evaluate the proposed method on our own collected images in San Francisco downtown area often described as "downtown canyon" where consumer GPS signals are extremely inaccurate. The results show the model can predict quality locations that suffice in real business applications, such as ride-sharing, only using phone-grade GPS. Unlike classic visual localization or recent PoseNet-like methods that may work well in indoor environments or small-scale outdoor environments, we avoid using a map or an SFM (structure-from-motion) model at all. More importantly, the proposed method can be scaled up without concerns over the potential failure of 3D reconstruction.



### Graph-based Hypothesis Generation for Parallax-tolerant Image Stitching
- **Arxiv ID**: http://arxiv.org/abs/1804.07492v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.07492v1)
- **Published**: 2018-04-20 08:45:08+00:00
- **Updated**: 2018-04-20 08:45:08+00:00
- **Authors**: Jing Chen, Nan Li, Tianli Liao
- **Comment**: 3 pages, 3 figures, 2 tables
- **Journal**: None
- **Summary**: The seam-driven approach has been proven fairly effective for parallax-tolerant image stitching, whose strategy is to search for an invisible seam from finite representative hypotheses of local alignment. In this paper, we propose a graph-based hypothesis generation and a seam-guided local alignment for improving the effectiveness and the efficiency of the seam-driven approach. The experiment demonstrates the significant reduction of number of hypotheses and the improved quality of naturalness of final stitching results, comparing to the state-of-the-art method SEAGULL.



### Residual-Guide Feature Fusion Network for Single Image Deraining
- **Arxiv ID**: http://arxiv.org/abs/1804.07493v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.07493v1)
- **Published**: 2018-04-20 08:46:40+00:00
- **Updated**: 2018-04-20 08:46:40+00:00
- **Authors**: Zhiwen Fan, Huafeng Wu, Xueyang Fu, Yue Hunag, Xinghao Ding
- **Comment**: None
- **Journal**: None
- **Summary**: Single image rain streaks removal is extremely important since rainy images adversely affect many computer vision systems. Deep learning based methods have found great success in image deraining tasks. In this paper, we propose a novel residual-guide feature fusion network, called ResGuideNet, for single image deraining that progressively predicts highquality reconstruction. Specifically, we propose a cascaded network and adopt residuals generated from shallower blocks to guide deeper blocks. By using this strategy, we can obtain a coarse to fine estimation of negative residual as the blocks go deeper. The outputs of different blocks are merged into the final reconstruction. We adopt recursive convolution to build each block and apply supervision to all intermediate results, which enable our model to achieve promising performance on synthetic and real-world data while using fewer parameters than previous required. ResGuideNet is detachable to meet different rainy conditions. For images with light rain streaks and limited computational resource at test time, we can obtain a decent performance even with several building blocks. Experiments validate that ResGuideNet can benefit other low- and high-level vision tasks.



### Domain and Geometry Agnostic CNNs for Left Atrium Segmentation in 3D Ultrasound
- **Arxiv ID**: http://arxiv.org/abs/1805.00357v1
- **DOI**: 10.1007/978-3-030-00937-3_72
- **Categories**: **cs.CV**, cs.CY, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1805.00357v1)
- **Published**: 2018-04-20 09:22:50+00:00
- **Updated**: 2018-04-20 09:22:50+00:00
- **Authors**: Markus A. Degel, Nassir Navab, Shadi Albarqouni
- **Comment**: None
- **Journal**: Medical Image Computing and Computer Assisted Intervention (MICCAI
  2018)
- **Summary**: Segmentation of the left atrium and deriving its size can help to predict and detect various cardiovascular conditions. Automation of this process in 3D Ultrasound image data is desirable, since manual delineations are time-consuming, challenging and observer-dependent. Convolutional neural networks have made improvements in computer vision and in medical image analysis. They have successfully been applied to segmentation tasks and were extended to work on volumetric data. In this paper we introduce a combined deep-learning based approach on volumetric segmentation in Ultrasound acquisitions with incorporation of prior knowledge about left atrial shape and imaging device. The results show, that including a shape prior helps the domain adaptation and the accuracy of segmentation is further increased with adversarial learning.



### An Approximate Shading Model with Detail Decomposition for Object Relighting
- **Arxiv ID**: http://arxiv.org/abs/1804.07514v1
- **DOI**: 10.1007/s11263-018-1090-6
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.07514v1)
- **Published**: 2018-04-20 09:29:39+00:00
- **Updated**: 2018-04-20 09:29:39+00:00
- **Authors**: Zicheng Liao, Kevin Karsch, Hongyi Zhang, David Forsyth
- **Comment**: None
- **Journal**: None
- **Summary**: We present an object relighting system that allows an artist to select an object from an image and insert it into a target scene. Through simple interactions, the system can adjust illumination on the inserted object so that it appears naturally in the scene. To support image-based relighting, we build object model from the image, and propose a \emph{perceptually-inspired} approximate shading model for the relighting. It decomposes the shading field into (a) a rough shape term that can be reshaded, (b) a parametric shading detail that encodes missing features from the first term, and (c) a geometric detail term that captures fine-scale material properties. With this decomposition, the shading model combines 3D rendering and image-based composition and allows more flexible compositing than image-based methods. Quantitative evaluation and a set of user studies suggest our method is a promising alternative to existing methods of object insertion.



### MobileFaceNets: Efficient CNNs for Accurate Real-Time Face Verification on Mobile Devices
- **Arxiv ID**: http://arxiv.org/abs/1804.07573v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1804.07573v4)
- **Published**: 2018-04-20 12:18:57+00:00
- **Updated**: 2018-06-15 02:50:58+00:00
- **Authors**: Sheng Chen, Yang Liu, Xiang Gao, Zhen Han
- **Comment**: Accepted as a conference paper at CCBR 2018. Camera-ready version
- **Journal**: None
- **Summary**: We present a class of extremely efficient CNN models, MobileFaceNets, which use less than 1 million parameters and are specifically tailored for high-accuracy real-time face verification on mobile and embedded devices. We first make a simple analysis on the weakness of common mobile networks for face verification. The weakness has been well overcome by our specifically designed MobileFaceNets. Under the same experimental conditions, our MobileFaceNets achieve significantly superior accuracy as well as more than 2 times actual speedup over MobileNetV2. After trained by ArcFace loss on the refined MS-Celeb-1M, our single MobileFaceNet of 4.0MB size achieves 99.55% accuracy on LFW and 92.59% TAR@FAR1e-6 on MegaFace, which is even comparable to state-of-the-art big CNN models of hundreds MB size. The fastest one of MobileFaceNets has an actual inference time of 18 milliseconds on a mobile phone. For face verification, MobileFaceNets achieve significantly improved efficiency over previous state-of-the-art mobile CNNs.



### Revisiting Small Batch Training for Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1804.07612v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1804.07612v1)
- **Published**: 2018-04-20 13:44:12+00:00
- **Updated**: 2018-04-20 13:44:12+00:00
- **Authors**: Dominic Masters, Carlo Luschi
- **Comment**: None
- **Journal**: None
- **Summary**: Modern deep neural network training is typically based on mini-batch stochastic gradient optimization. While the use of large mini-batches increases the available computational parallelism, small batch training has been shown to provide improved generalization performance and allows a significantly smaller memory footprint, which might also be exploited to improve machine throughput.   In this paper, we review common assumptions on learning rate scaling and training duration, as a basis for an experimental comparison of test performance for different mini-batch sizes. We adopt a learning rate that corresponds to a constant average weight update per gradient calculation (i.e., per unit cost of computation), and point out that this results in a variance of the weight updates that increases linearly with the mini-batch size $m$.   The collected experimental results for the CIFAR-10, CIFAR-100 and ImageNet datasets show that increasing the mini-batch size progressively reduces the range of learning rates that provide stable convergence and acceptable test performance. On the other hand, small mini-batch sizes provide more up-to-date gradient calculations, which yields more stable and reliable training. The best performance has been consistently obtained for mini-batch sizes between $m = 2$ and $m = 32$, which contrasts with recent work advocating the use of mini-batch sizes in the thousands.



### Super-resolution Ultrasound Localization Microscopy through Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1804.07661v2
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1804.07661v2)
- **Published**: 2018-04-20 15:12:05+00:00
- **Updated**: 2018-12-13 15:13:22+00:00
- **Authors**: Ruud J. G. van Sloun, Oren Solomon, Matthew Bruce, Zin Z. Khaing, Hessel Wijkstra, Yonina C. Eldar, Massimo Mischi
- **Comment**: None
- **Journal**: None
- **Summary**: Ultrasound localization microscopy has enabled super-resolution vascular imaging through precise localization of individual ultrasound contrast agents (microbubbles) across numerous imaging frames. However, analysis of high-density regions with significant overlaps among the microbubble point spread responses yields high localization errors, constraining the technique to low-concentration conditions. As such, long acquisition times are required to sufficiently cover the vascular bed. In this work, we present a fast and precise method for obtaining super-resolution vascular images from high-density contrast-enhanced ultrasound imaging data. This method, which we term Deep Ultrasound Localization Microscopy (Deep-ULM), exploits modern deep learning strategies and employs a convolutional neural network to perform localization microscopy in dense scenarios. This end-to-end fully convolutional neural network architecture is trained effectively using on-line synthesized data, enabling robust inference in-vivo under a wide variety of imaging conditions. We show that deep learning attains super-resolution with challenging contrast-agent densities, both in-silico as well as in-vivo. Deep-ULM is suitable for real-time applications, resolving about 70 high-resolution patches (128x128 pixels) per second on a standard PC. Exploiting GPU computation, this number increases to 1250 patches per second.



### Rethinking the Faster R-CNN Architecture for Temporal Action Localization
- **Arxiv ID**: http://arxiv.org/abs/1804.07667v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.07667v1)
- **Published**: 2018-04-20 15:22:07+00:00
- **Updated**: 2018-04-20 15:22:07+00:00
- **Authors**: Yu-Wei Chao, Sudheendra Vijayanarasimhan, Bryan Seybold, David A. Ross, Jia Deng, Rahul Sukthankar
- **Comment**: Accepted in CVPR 2018
- **Journal**: None
- **Summary**: We propose TAL-Net, an improved approach to temporal action localization in video that is inspired by the Faster R-CNN object detection framework. TAL-Net addresses three key shortcomings of existing approaches: (1) we improve receptive field alignment using a multi-scale architecture that can accommodate extreme variation in action durations; (2) we better exploit the temporal context of actions for both proposal generation and action classification by appropriately extending receptive fields; and (3) we explicitly consider multi-stream feature fusion and demonstrate that fusing motion late is important. We achieve state-of-the-art performance for both action proposal and localization on THUMOS'14 detection benchmark and competitive performance on ActivityNet challenge.



### Image Inpainting for Irregular Holes Using Partial Convolutions
- **Arxiv ID**: http://arxiv.org/abs/1804.07723v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.07723v2)
- **Published**: 2018-04-20 17:00:14+00:00
- **Updated**: 2018-12-15 22:22:34+00:00
- **Authors**: Guilin Liu, Fitsum A. Reda, Kevin J. Shih, Ting-Chun Wang, Andrew Tao, Bryan Catanzaro
- **Comment**: Update: camera-ready; L1 loss is size-averaged; code of partial conv
  layer: https://github.com/NVIDIA/partialconv. Published at ECCV 2018
- **Journal**: None
- **Summary**: Existing deep learning based image inpainting methods use a standard convolutional network over the corrupted image, using convolutional filter responses conditioned on both valid pixels as well as the substitute values in the masked holes (typically the mean value). This often leads to artifacts such as color discrepancy and blurriness. Post-processing is usually used to reduce such artifacts, but are expensive and may fail. We propose the use of partial convolutions, where the convolution is masked and renormalized to be conditioned on only valid pixels. We further include a mechanism to automatically generate an updated mask for the next layer as part of the forward pass. Our model outperforms other methods for irregular masks. We show qualitative and quantitative comparisons with other methods to validate our approach.



### ADef: an Iterative Algorithm to Construct Adversarial Deformations
- **Arxiv ID**: http://arxiv.org/abs/1804.07729v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1804.07729v3)
- **Published**: 2018-04-20 17:11:06+00:00
- **Updated**: 2019-01-11 15:42:59+00:00
- **Authors**: Rima Alaifari, Giovanni S. Alberti, Tandri Gauksson
- **Comment**: ICLR 2019 conference paper. 25 pages, 20 figures
- **Journal**: None
- **Summary**: While deep neural networks have proven to be a powerful tool for many recognition and classification tasks, their stability properties are still not well understood. In the past, image classifiers have been shown to be vulnerable to so-called adversarial attacks, which are created by additively perturbing the correctly classified image. In this paper, we propose the ADef algorithm to construct a different kind of adversarial attack created by iteratively applying small deformations to the image, found through a gradient descent step. We demonstrate our results on MNIST with convolutional neural networks and on ImageNet with Inception-v3 and ResNet-101.



### Synthesizing Images of Humans in Unseen Poses
- **Arxiv ID**: http://arxiv.org/abs/1804.07739v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.07739v1)
- **Published**: 2018-04-20 17:34:44+00:00
- **Updated**: 2018-04-20 17:34:44+00:00
- **Authors**: Guha Balakrishnan, Amy Zhao, Adrian V. Dalca, Fredo Durand, John Guttag
- **Comment**: CVPR 2018
- **Journal**: None
- **Summary**: We address the computational problem of novel human pose synthesis. Given an image of a person and a desired pose, we produce a depiction of that person in that pose, retaining the appearance of both the person and background. We present a modular generative neural network that synthesizes unseen poses using training pairs of images and poses taken from human action videos. Our network separates a scene into different body part and background layers, moves body parts to new locations and refines their appearances, and composites the new foreground with a hole-filled background. These subtasks, implemented with separate modules, are trained jointly using only a single target image as a supervised label. We use an adversarial discriminator to force our network to synthesize realistic details conditioned on pose. We demonstrate image synthesis results on three action classes: golf, yoga/workouts and tennis, and show that our method produces accurate results within action classes as well as across action classes. Given a sequence of desired poses, we also produce coherent videos of actions.



### A Deep Face Identification Network Enhanced by Facial Attributes Prediction
- **Arxiv ID**: http://arxiv.org/abs/1805.00324v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.00324v1)
- **Published**: 2018-04-20 20:43:52+00:00
- **Updated**: 2018-04-20 20:43:52+00:00
- **Authors**: Fariborz Taherkhani, Nasser M. Nasrabadi, Jeremy Dawson
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a new deep framework which predicts facial attributes and leverage it as a soft modality to improve face identification performance. Our model is an end to end framework which consists of a convolutional neural network (CNN) whose output is fanned out into two separate branches; the first branch predicts facial attributes while the second branch identifies face images. Contrary to the existing multi-task methods which only use a shared CNN feature space to train these two tasks jointly, we fuse the predicted attributes with the features from the face modality in order to improve the face identification performance. Experimental results show that our model brings benefits to both face identification as well as facial attribute prediction performance, especially in the case of identity facial attributes such as gender prediction. We tested our model on two standard datasets annotated by identities and face attributes. Experimental results indicate that the proposed model outperforms most of the current existing face identification and attribute prediction methods.



### An Aggregated Multicolumn Dilated Convolution Network for Perspective-Free Counting
- **Arxiv ID**: http://arxiv.org/abs/1804.07821v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.07821v1)
- **Published**: 2018-04-20 20:49:10+00:00
- **Updated**: 2018-04-20 20:49:10+00:00
- **Authors**: Diptodip Deb, Jonathan Ventura
- **Comment**: CVPR 2018 Workshop On Visual Understanding of Humans in Crowd Scene
- **Journal**: None
- **Summary**: We propose the use of dilated filters to construct an aggregation module in a multicolumn convolutional neural network for perspective-free counting. Counting is a common problem in computer vision (e.g. traffic on the street or pedestrians in a crowd). Modern approaches to the counting problem involve the production of a density map via regression whose integral is equal to the number of objects in the image. However, objects in the image can occur at different scales (e.g. due to perspective effects) which can make it difficult for a learning agent to learn the proper density map. While the use of multiple columns to extract multiscale information from images has been shown before, our approach aggregates the multiscale information gathered by the multicolumn convolutional neural network to improve performance. Our experiments show that our proposed network outperforms the state-of-the-art on many benchmark datasets, and also that using our aggregation module in combination with a higher number of columns is beneficial for multiscale counting.



### HandyNet: A One-stop Solution to Detect, Segment, Localize & Analyze Driver Hands
- **Arxiv ID**: http://arxiv.org/abs/1804.07834v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.07834v2)
- **Published**: 2018-04-20 21:38:32+00:00
- **Updated**: 2019-01-30 20:21:15+00:00
- **Authors**: Akshay Rangesh, Mohan M. Trivedi
- **Comment**: None
- **Journal**: None
- **Summary**: Tasks related to human hands have long been part of the computer vision community. Hands being the primary actuators for humans, convey a lot about activities and intents, in addition to being an alternative form of communication/interaction with other humans and machines. In this study, we focus on training a single feedforward convolutional neural network (CNN) capable of executing many hand related tasks that may be of use in autonomous and semi-autonomous vehicles of the future. The resulting network, which we refer to as HandyNet, is capable of detecting, segmenting and localizing (in 3D) driver hands inside a vehicle cabin. The network is additionally trained to identify handheld objects that the driver may be interacting with. To meet the data requirements to train such a network, we propose a method for cheap annotation based on chroma-keying, thereby bypassing weeks of human effort required to label such data. This process can generate thousands of labeled training samples in an efficient manner, and may be replicated in new environments with relative ease.



### ConnNet: A Long-Range Relation-Aware Pixel-Connectivity Network for Salient Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1804.07836v2
- **DOI**: 10.1109/TIP.2018.2886997
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.07836v2)
- **Published**: 2018-04-20 21:40:57+00:00
- **Updated**: 2019-02-12 11:35:27+00:00
- **Authors**: Michael Kampffmeyer, Nanqing Dong, Xiaodan Liang, Yujia Zhang, Eric P. Xing
- **Comment**: None
- **Journal**: IEEE Transactions on Image Processing (Volume: 28 , Issue: 5 , May
  2019)
- **Summary**: Salient segmentation aims to segment out attention-grabbing regions, a critical yet challenging task and the foundation of many high-level computer vision applications. It requires semantic-aware grouping of pixels into salient regions and benefits from the utilization of global multi-scale contexts to achieve good local reasoning. Previous works often address it as two-class segmentation problems utilizing complicated multi-step procedures including refinement networks and complex graphical models. We argue that semantic salient segmentation can instead be effectively resolved by reformulating it as a simple yet intuitive pixel-pair based connectivity prediction task. Following the intuition that salient objects can be naturally grouped via semantic-aware connectivity between neighboring pixels, we propose a pure Connectivity Net (ConnNet). ConnNet predicts connectivity probabilities of each pixel with its neighboring pixels by leveraging multi-level cascade contexts embedded in the image and long-range pixel relations. We investigate our approach on two tasks, namely salient object segmentation and salient instance-level segmentation, and illustrate that consistent improvements can be obtained by modeling these tasks as connectivity instead of binary segmentation tasks for a variety of network architectures. We achieve state-of-the-art performance, outperforming or being comparable to existing approaches while reducing inference time due to our less complex approach.



### Large Scale Automated Reading of Frontal and Lateral Chest X-Rays using Dual Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1804.07839v2
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1804.07839v2)
- **Published**: 2018-04-20 21:48:35+00:00
- **Updated**: 2018-04-24 21:13:59+00:00
- **Authors**: Jonathan Rubin, Deepan Sanghavi, Claire Zhao, Kathy Lee, Ashequl Qadir, Minnan Xu-Wilson
- **Comment**: First draft, under review
- **Journal**: None
- **Summary**: The MIMIC-CXR dataset is (to date) the largest released chest x-ray dataset consisting of 473,064 chest x-rays and 206,574 radiology reports collected from 63,478 patients. We present the results of training and evaluating a collection of deep convolutional neural networks on this dataset to recognize multiple common thorax diseases. To the best of our knowledge, this is the first work that trains CNNs for this task on such a large collection of chest x-ray images, which is over four times the size of the largest previously released chest x-ray corpus (ChestX-Ray14). We describe and evaluate individual CNN models trained on frontal and lateral CXR view types. In addition, we present a novel DualNet architecture that emulates routine clinical practice by simultaneously processing both frontal and lateral CXR images obtained from a radiological exam. Our DualNet architecture shows improved performance in recognizing findings in CXR images when compared to applying separate baseline frontal and lateral classifiers.



### DeepPET: A deep encoder-decoder network for directly solving the PET reconstruction inverse problem
- **Arxiv ID**: http://arxiv.org/abs/1804.07851v2
- **DOI**: 10.1016/j.media.2019.03.013
- **Categories**: **cs.CV**, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1804.07851v2)
- **Published**: 2018-04-20 22:44:00+00:00
- **Updated**: 2018-09-25 19:03:06+00:00
- **Authors**: Ida Häggström, C. Ross Schmidtlein, Gabriele Campanella, Thomas J. Fuchs
- **Comment**: None
- **Journal**: Medical Image Analysis 54, pp. 253-262, 2019
- **Summary**: Positron emission tomography (PET) is a cornerstone of modern radiology. The ability to detect cancer and metastases in whole body scans fundamentally changed cancer diagnosis and treatment. One of the main bottlenecks in the clinical application is the time it takes to reconstruct the anatomical image from the deluge of data in PET imaging. State-of-the art methods based on expectation maximization can take hours for a single patient and depend on manual fine-tuning. This results not only in financial burden for hospitals but more importantly leads to less efficient patient handling, evaluation, and ultimately diagnosis and treatment for patients. To overcome this problem we present a novel PET image reconstruction technique based on a deep convolutional encoder-decoder network, that takes PET sinogram data as input and directly outputs full PET images. Using realistic simulated data, we demonstrate that our network is able to reconstruct images >100 times faster, and with comparable image quality (in terms of root mean squared error) relative to conventional iterative reconstruction techniques.



### Occluded object reconstruction for first responders with augmented reality glasses using conditional generative adversarial networks
- **Arxiv ID**: http://arxiv.org/abs/1805.00322v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1805.00322v1)
- **Published**: 2018-04-20 23:56:10+00:00
- **Updated**: 2018-04-20 23:56:10+00:00
- **Authors**: Kyongsik Yun, Thomas Lu, Edward Chow
- **Comment**: SPIE 2018
- **Journal**: None
- **Summary**: Firefighters suffer a variety of life-threatening risks, including line-of-duty deaths, injuries, and exposures to hazardous substances. Support for reducing these risks is important. We built a partially occluded object reconstruction method on augmented reality glasses for first responders. We used a deep learning based on conditional generative adversarial networks to train associations between the various images of flammable and hazardous objects and their partially occluded counterparts. Our system then reconstructed an image of a new flammable object. Finally, the reconstructed image was superimposed on the input image to provide "transparency". The system imitates human learning about the laws of physics through experience by learning the shape of flammable objects and the flame characteristics.



