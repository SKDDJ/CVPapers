# Arxiv Papers in cs.CV on 2018-04-15
### Multi-level Semantic Feature Augmentation for One-shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1804.05298v4
- **DOI**: 10.1109/TIP.2019.2910052
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.05298v4)
- **Published**: 2018-04-15 02:44:54+00:00
- **Updated**: 2019-03-15 09:37:14+00:00
- **Authors**: Zitian Chen, Yanwei Fu, Yinda Zhang, Yu-Gang Jiang, Xiangyang Xue, Leonid Sigal
- **Comment**: The paper has been ACCEPTED for publication as a REGULAR paper in the
  IEEE Transactions on Image Processing
- **Journal**: None
- **Summary**: The ability to quickly recognize and learn new visual concepts from limited samples enables humans to swiftly adapt to new environments. This ability is enabled by semantic associations of novel concepts with those that have already been learned and stored in memory. Computers can start to ascertain similar abilities by utilizing a semantic concept space. A concept space is a high-dimensional semantic space in which similar abstract concepts appear close and dissimilar ones far apart. In this paper, we propose a novel approach to one-shot learning that builds on this idea. Our approach learns to map a novel sample instance to a concept, relates that concept to the existing ones in the concept space and generates new instances, by interpolating among the concepts, to help learning. Instead of synthesizing new image instance, we propose to directly synthesize instance features by leveraging semantics using a novel auto-encoder network we call dual TriNet. The encoder part of the TriNet learns to map multi-layer visual features of deep CNNs, that is, multi-level concepts, to a semantic vector. In semantic space, we search for related concepts, which are then projected back into the image feature spaces by the decoder portion of the TriNet. Two strategies in the semantic space are explored. Notably, this seemingly simple strategy results in complex augmented feature distributions in the image feature space, leading to substantially better performance.



### Head Mounted Pupil Tracking Using Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1805.00311v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1805.00311v2)
- **Published**: 2018-04-15 04:48:16+00:00
- **Updated**: 2018-07-17 01:44:01+00:00
- **Authors**: Yinheng Zhu, Wanli Chen, Xun Zhan, Zonglin Guo, Hongjian Shi, Ian G. Harris
- **Comment**: It's out of date and not STOA any more
- **Journal**: None
- **Summary**: Pupil tracking is an important branch of object tracking which require high precision. We investigate head mounted pupil tracking which is often more convenient and precise than remote pupil tracking, but also more challenging. When pupil tracking suffers from noise like bad illumination, detection precision dramatically decreases. Due to the appearance of head mounted recording device and public benchmark image datasets, head mounted tracking algorithms have become easier to design and evaluate. In this paper, we propose a robust head mounted pupil detection algorithm which uses a Convolutional Neural Network (CNN) to combine different features of pupil. Here we consider three features of pupil. Firstly, we use three pupil feature-based algorithms to find pupil center independently. Secondly, we use a CNN to evaluate the quality of each result. Finally, we select the best result as output. The experimental results show that our proposed algorithm performs better than the present state-of-art.



### Local Descriptors Optimized for Average Precision
- **Arxiv ID**: http://arxiv.org/abs/1804.05312v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.05312v2)
- **Published**: 2018-04-15 07:08:24+00:00
- **Updated**: 2018-04-17 19:59:23+00:00
- **Authors**: Kun He, Yan Lu, Stan Sclaroff
- **Comment**: 13 pages, 8 figures. IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR), 2018
- **Journal**: None
- **Summary**: Extraction of local feature descriptors is a vital stage in the solution pipelines for numerous computer vision tasks. Learning-based approaches improve performance in certain tasks, but still cannot replace handcrafted features in general. In this paper, we improve the learning of local feature descriptors by optimizing the performance of descriptor matching, which is a common stage that follows descriptor extraction in local feature based pipelines, and can be formulated as nearest neighbor retrieval. Specifically, we directly optimize a ranking-based retrieval performance metric, Average Precision, using deep neural networks. This general-purpose solution can also be viewed as a listwise learning to rank approach, which is advantageous compared to recent local ranking approaches. On standard benchmarks, descriptors learned with our formulation achieve state-of-the-art results in patch verification, patch retrieval, and image matching.



### Attention-Gated Networks for Improving Ultrasound Scan Plane Detection
- **Arxiv ID**: http://arxiv.org/abs/1804.05338v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.05338v1)
- **Published**: 2018-04-15 11:15:28+00:00
- **Updated**: 2018-04-15 11:15:28+00:00
- **Authors**: Jo Schlemper, Ozan Oktay, Liang Chen, Jacqueline Matthew, Caroline Knight, Bernhard Kainz, Ben Glocker, Daniel Rueckert
- **Comment**: Submitted to MIDL2018 (OpenReview:
  https://openreview.net/forum?id=BJtn7-3sM)
- **Journal**: None
- **Summary**: In this work, we apply an attention-gated network to real-time automated scan plane detection for fetal ultrasound screening. Scan plane detection in fetal ultrasound is a challenging problem due the poor image quality resulting in low interpretability for both clinicians and automated algorithms. To solve this, we propose incorporating self-gated soft-attention mechanisms. A soft-attention mechanism generates a gating signal that is end-to-end trainable, which allows the network to contextualise local information useful for prediction. The proposed attention mechanism is generic and it can be easily incorporated into any existing classification architectures, while only requiring a few additional parameters. We show that, when the base network has a high capacity, the incorporated attention mechanism can provide efficient object localisation while improving the overall performance. When the base network has a low capacity, the method greatly outperforms the baseline approach and significantly reduces false positives. Lastly, the generated attention maps allow us to understand the model's reasoning process, which can also be used for weakly supervised object localisation.



### SparseNet: A Sparse DenseNet for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1804.05340v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.05340v1)
- **Published**: 2018-04-15 11:29:30+00:00
- **Updated**: 2018-04-15 11:29:30+00:00
- **Authors**: Wenqi Liu, Kun Zeng
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks have made remarkable progresses on various computer vision tasks. Recent works have shown that depth, width and shortcut connections of networks are all vital to their performances. In this paper, we introduce a method to sparsify DenseNet which can reduce connections of a L-layer DenseNet from O(L^2) to O(L), and thus we can simultaneously increase depth, width and connections of neural networks in a more parameter-efficient and computation-efficient way. Moreover, an attention module is introduced to further boost our network's performance. We denote our network as SparseNet. We evaluate SparseNet on datasets of CIFAR(including CIFAR10 and CIFAR100) and SVHN. Experiments show that SparseNet can obtain improvements over the state-of-the-art on CIFAR10 and SVHN. Furthermore, while achieving comparable performances as DenseNet on these datasets, SparseNet is x2.6 smaller and x3.7 faster than the original DenseNet.



### A Sparse Non-negative Matrix Factorization Framework for Identifying Functional Units of Tongue Behavior from MRI
- **Arxiv ID**: http://arxiv.org/abs/1804.05370v3
- **DOI**: 10.1109/TMI.2018.2870939
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.05370v3)
- **Published**: 2018-04-15 15:32:11+00:00
- **Updated**: 2018-09-29 17:13:08+00:00
- **Authors**: Jonghye Woo, Jerry L. Prince, Maureen Stone, Fangxu Xing, Arnold Gomez, Jordan R. Green, Christopher J. Hartnick, Thomas J. Brady, Timothy G. Reese, Van J. Wedeen, Georges El Fakhri
- **Comment**: Accepted at IEEE TMI (https://ieeexplore.ieee.org/document/8467354)
- **Journal**: None
- **Summary**: Muscle coordination patterns of lingual behaviors are synergies generated by deforming local muscle groups in a variety of ways. Functional units are functional muscle groups of local structural elements within the tongue that compress, expand, and move in a cohesive and consistent manner. Identifying the functional units using tagged-Magnetic Resonance Imaging (MRI) sheds light on the mechanisms of normal and pathological muscle coordination patterns, yielding improvement in surgical planning, treatment, or rehabilitation procedures. Here, to mine this information, we propose a matrix factorization and probabilistic graphical model framework to produce building blocks and their associated weighting map using motion quantities extracted from tagged-MRI. Our tagged-MRI imaging and accurate voxel-level tracking provide previously unavailable internal tongue motion patterns, thus revealing the inner workings of the tongue during speech or other lingual behaviors. We then employ spectral clustering on the weighting map to identify the cohesive regions defined by the tongue motion that may involve multiple or undocumented regions. To evaluate our method, we perform a series of experiments. We first use two-dimensional images and synthetic data to demonstrate the accuracy of our method. We then use three-dimensional synthetic and \textit{in vivo} tongue motion data using protrusion and simple speech tasks to identify subject-specific and data-driven functional units of the tongue in localized regions.



### Detecting Concrete Abnormality Using Time-series Thermal Imaging and Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/1804.05406v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1804.05406v2)
- **Published**: 2018-04-15 18:57:11+00:00
- **Updated**: 2019-04-10 17:02:02+00:00
- **Authors**: Chongsheng Cheng, Zhigang Shen
- **Comment**: Accepted as Poster for 98th Annual Meeting of Transportation Research
  Board (TRB)
- **Journal**: None
- **Summary**: Nondestructive detecting defects (NDD) in concrete structures have been explored for decades. Although limited successes were reported, major limitations still exist. The major limitations are the high noises to signal ratio created from the environmental factors, such as cloud, shadow, water, surface texture etc. and the decision making still relies on the engineering judgment of interpretation of image content. Time-series approach, such as principle component thermography approach has been experimented with some improved results. Recent progress in image processing using machine learning approach made it possible for detecting defects thermal features in more quantitative ways. In this paper, we provide a procedure to represent the thermal feature in the time domain by principal component analysis and regress the prediction of detection by two schemes of supervised learning models. Three independent experiments were conducted in a similar laboratory setup but varied in conditions to illustrate the performance and generalization of models. Results showed the effectiveness for the detection purpose with appropriate tuning for parameters. Future studies will focus on implementing more sophisticated structured models to handle more realistic cases under natural conditions.



### FDMO: Feature Assisted Direct Monocular Odometry
- **Arxiv ID**: http://arxiv.org/abs/1804.05422v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1804.05422v1)
- **Published**: 2018-04-15 20:24:23+00:00
- **Updated**: 2018-04-15 20:24:23+00:00
- **Authors**: Georges Younes, Daniel Asmar, John Zelek
- **Comment**: None
- **Journal**: None
- **Summary**: Visual Odometry (VO) can be categorized as being either direct or feature based. When the system is calibrated photometrically, and images are captured at high rates, direct methods have shown to outperform feature-based ones in terms of accuracy and processing time; they are also more robust to failure in feature-deprived environments. On the downside, Direct methods rely on heuristic motion models to seed the estimation of camera motion between frames; in the event that these models are violated (e.g., erratic motion), Direct methods easily fail. This paper proposes a novel system entitled FDMO (Feature assisted Direct Monocular Odometry), which complements the advantages of both direct and featured based techniques. FDMO bootstraps indirect feature tracking upon the sub-pixel accurate localized direct keyframes only when failure modes (e.g., large baselines) of direct tracking occur. Control returns back to direct odometry when these conditions are no longer violated. Efficiencies are introduced to help FDMO perform in real time. FDMO shows significant drift (alignment, rotation & scale) reduction when compared to DSO & ORB SLAM when evaluated using the TumMono and EuroC datasets.



### White matter fiber analysis using kernel dictionary learning and sparsity priors
- **Arxiv ID**: http://arxiv.org/abs/1804.05427v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.05427v1)
- **Published**: 2018-04-15 20:48:04+00:00
- **Updated**: 2018-04-15 20:48:04+00:00
- **Authors**: Kuldeep Kumar, Kaleem Siddiqi, Christian Desrosiers
- **Comment**: None
- **Journal**: None
- **Summary**: Diffusion magnetic resonance imaging, a non-invasive tool to infer white matter fiber connections, produces a large number of streamlines containing a wealth of information on structural connectivity. The size of these tractography outputs makes further analyses complex, creating a need for methods to group streamlines into meaningful bundles. In this work, we address this by proposing a set of kernel dictionary learning and sparsity priors based methods. Proposed frameworks include L-0 norm, group sparsity, as well as manifold regularization prior. The proposed methods allow streamlines to be assigned to more than one bundle, making it more robust to overlapping bundles and inter-subject variations. We evaluate the performance of our method on a labeled set and data from Human Connectome Project. Results highlight the ability of our method to group streamlines into plausible bundles and illustrate the impact of sparsity priors on the performance of the proposed methods.



### Weighted Low-Rank Approximation of Matrices and Background Modeling
- **Arxiv ID**: http://arxiv.org/abs/1804.06252v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NA, math.NA, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/1804.06252v1)
- **Published**: 2018-04-15 21:43:08+00:00
- **Updated**: 2018-04-15 21:43:08+00:00
- **Authors**: Aritra Dutta, Xin Li, Peter Richtarik
- **Comment**: arXiv admin note: text overlap with arXiv:1707.00281
- **Journal**: None
- **Summary**: We primarily study a special a weighted low-rank approximation of matrices and then apply it to solve the background modeling problem. We propose two algorithms for this purpose: one operates in the batch mode on the entire data and the other one operates in the batch-incremental mode on the data and naturally captures more background variations and computationally more effective. Moreover, we propose a robust technique that learns the background frame indices from the data and does not require any training frames. We demonstrate through extensive experiments that by inserting a simple weight in the Frobenius norm, it can be made robust to the outliers similar to the $\ell_1$ norm. Our methods match or outperform several state-of-the-art online and batch background modeling methods in virtually all quantitative and qualitative measures.



### Watch, Listen, and Describe: Globally and Locally Aligned Cross-Modal Attentions for Video Captioning
- **Arxiv ID**: http://arxiv.org/abs/1804.05448v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1804.05448v1)
- **Published**: 2018-04-15 23:04:57+00:00
- **Updated**: 2018-04-15 23:04:57+00:00
- **Authors**: Xin Wang, Yuan-Fang Wang, William Yang Wang
- **Comment**: NAACL 2018
- **Journal**: None
- **Summary**: A major challenge for video captioning is to combine audio and visual cues. Existing multi-modal fusion methods have shown encouraging results in video understanding. However, the temporal structures of multiple modalities at different granularities are rarely explored, and how to selectively fuse the multi-modal representations at different levels of details remains uncharted. In this paper, we propose a novel hierarchically aligned cross-modal attention (HACA) framework to learn and selectively fuse both global and local temporal dynamics of different modalities. Furthermore, for the first time, we validate the superior performance of the deep audio features on the video captioning task. Finally, our HACA model significantly outperforms the previous best systems and achieves new state-of-the-art results on the widely used MSR-VTT dataset.



