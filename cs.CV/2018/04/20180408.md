# Arxiv Papers in cs.CV on 2018-04-08
### Dimensionality's Blessing: Clustering Images by Underlying Distribution
- **Arxiv ID**: http://arxiv.org/abs/1804.02624v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.02624v1)
- **Published**: 2018-04-08 03:52:09+00:00
- **Updated**: 2018-04-08 03:52:09+00:00
- **Authors**: Wen-Yan Lin, Siying Liu, Jian-Huang Lai, Yasuyuki Matsushita
- **Comment**: Accepted in CVPR 2018
- **Journal**: None
- **Summary**: Many high dimensional vector distances tend to a constant. This is typically considered a negative "contrast-loss" phenomenon that hinders clustering and other machine learning techniques. We reinterpret "contrast-loss" as a blessing. Re-deriving "contrast-loss" using the law of large numbers, we show it results in a distribution's instances concentrating on a thin "hyper-shell". The hollow center means apparently chaotically overlapping distributions are actually intrinsically separable. We use this to develop distribution-clustering, an elegant algorithm for grouping of data points by their (unknown) underlying distribution. Distribution-clustering, creates notably clean clusters from raw unlabeled data, estimates the number of clusters for itself and is inherently robust to "outliers" which form their own clusters. This enables trawling for patterns in unorganized data and may be the key to enabling machine intelligence.



### OATM: Occlusion Aware Template Matching by Consensus Set Maximization
- **Arxiv ID**: http://arxiv.org/abs/1804.02638v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.02638v1)
- **Published**: 2018-04-08 07:19:55+00:00
- **Updated**: 2018-04-08 07:19:55+00:00
- **Authors**: Simon Korman, Mark Milam, Stefano Soatto
- **Comment**: to appear at cvpr 2018
- **Journal**: None
- **Summary**: We present a novel approach to template matching that is efficient, can handle partial occlusions, and comes with provable performance guarantees. A key component of the method is a reduction that transforms the problem of searching a nearest neighbor among $N$ high-dimensional vectors, to searching neighbors among two sets of order $\sqrt{N}$ vectors, which can be found efficiently using range search techniques. This allows for a quadratic improvement in search complexity, and makes the method scalable in handling large search spaces. The second contribution is a hashing scheme based on consensus set maximization, which allows us to handle occlusions. The resulting scheme can be seen as a randomized hypothesize-and-test algorithm, which is equipped with guarantees regarding the number of iterations required for obtaining an optimal solution with high probability. The predicted matching rates are validated empirically and the algorithm shows a significant improvement over the state-of-the-art in both speed and robustness to occlusions.



### A Clonal Selection Algorithm with Levenshtein Distance based Image Similarity in Multidimensional Subjective Tourist Information and Discovery of Cryptic Spots by Interactive GHSOM
- **Arxiv ID**: http://arxiv.org/abs/1804.05669v1
- **DOI**: 10.1109/SMC.2013.357
- **Categories**: **cs.IR**, cs.CV, cs.SI, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1804.05669v1)
- **Published**: 2018-04-08 08:18:16+00:00
- **Updated**: 2018-04-08 08:18:16+00:00
- **Authors**: Takumi Ichimura, Shin Kamada
- **Comment**: 6 pages, 9 figures, Proc. of 2013 IEEE International Conference on
  Systems, Man, and Cybernetics (IEEE SMC 2013). arXiv admin note: substantial
  text overlap with arXiv:1804.03993, arXiv:1804.02628, arXiv:1804.02816
- **Journal**: None
- **Summary**: Mobile Phone based Participatory Sensing (MPPS) system involves a community of users sending personal information and participating in autonomous sensing through their mobile phones. Sensed data can also be obtained from external sensing devices that can communicate wirelessly to the phone. Our developed tourist subjective data collection system with Android smartphone can determine the filtering rules to provide the important information of sightseeing spot. The rules are automatically generated by Interactive Growing Hierarchical SOM. However, the filtering rules related to photograph were not generated, because the extraction of the specified characteristics from images cannot be realized. We propose the effective method of the Levenshtein distance to deduce the spatial proximity of image viewpoints and thus determine the specified pattern in which images should be processed. To verify the proposed method, some experiments to classify the subjective data with images are executed by Interactive GHSOM and Clonal Selection Algorithm with Immunological Memory Cells in this paper.



### The Monge-Kantorovich Optimal Transport Distance for Image Comparison
- **Arxiv ID**: http://arxiv.org/abs/1804.03531v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.03531v1)
- **Published**: 2018-04-08 10:27:33+00:00
- **Updated**: 2018-04-08 10:27:33+00:00
- **Authors**: Michael Snow, Jan Van lent
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1612.00181
- **Journal**: None
- **Summary**: This paper focuses on the Monge-Kantorovich formulation of the optimal transport problem and the associated $L^2$ Wasserstein distance. We use the $L^2$ Wasserstein distance in the Nearest Neighbour (NN) machine learning architecture to demonstrate the potential power of the optimal transport distance for image comparison. We compare the Wasserstein distance to other established distances - including the partial differential equation (PDE) formulation of the optimal transport problem - and demonstrate that on the well known MNIST optical character recognition dataset, it achieves excellent results.



### Anticipating Traffic Accidents with Adaptive Loss and Large-scale Incident DB
- **Arxiv ID**: http://arxiv.org/abs/1804.02675v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.02675v1)
- **Published**: 2018-04-08 11:49:30+00:00
- **Updated**: 2018-04-08 11:49:30+00:00
- **Authors**: Tomoyuki Suzuki, Hirokatsu Kataoka, Yoshimitsu Aoki, Yutaka Satoh
- **Comment**: Accepted to CVPR 2018
- **Journal**: None
- **Summary**: In this paper, we propose a novel approach for traffic accident anticipation through (i) Adaptive Loss for Early Anticipation (AdaLEA) and (ii) a large-scale self-annotated incident database for anticipation. The proposed AdaLEA allows a model to gradually learn an earlier anticipation as training progresses. The loss function adaptively assigns penalty weights depending on how early the model can an- ticipate a traffic accident at each epoch. Additionally, we construct a Near-miss Incident DataBase for anticipation. This database contains an enormous number of traffic near- miss incident videos and annotations for detail evaluation of two tasks, risk anticipation and risk-factor anticipation. In our experimental results, we found our proposal achieved the highest scores for risk anticipation (+6.6% better on mean average precision (mAP) and 2.36 sec earlier than previous work on the average time-to-collision (ATTC)) and risk-factor anticipation (+4.3% better on mAP and 0.70 sec earlier than previous work on ATTC).



### Supervised Convolutional Sparse Coding
- **Arxiv ID**: http://arxiv.org/abs/1804.02678v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.02678v1)
- **Published**: 2018-04-08 12:05:12+00:00
- **Updated**: 2018-04-08 12:05:12+00:00
- **Authors**: Lama Affara, Bernard Ghanem, Peter Wonka
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Sparse Coding (CSC) is a well-established image representation model especially suited for image restoration tasks. In this work, we extend the applicability of this model by proposing a supervised approach to convolutional sparse coding, which aims at learning discriminative dictionaries instead of purely reconstructive ones. We incorporate a supervised regularization term into the traditional unsupervised CSC objective to encourage the final dictionary elements to be discriminative. Experimental results show that using supervised convolutional learning results in two key advantages. First, we learn more semantically relevant filters in the dictionary and second, we achieve improved image reconstruction on unseen data.



### Learning-based Video Motion Magnification
- **Arxiv ID**: http://arxiv.org/abs/1804.02684v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1804.02684v3)
- **Published**: 2018-04-08 12:57:23+00:00
- **Updated**: 2018-08-01 03:26:46+00:00
- **Authors**: Tae-Hyun Oh, Ronnachai Jaroensri, Changil Kim, Mohamed Elgharib, Fr√©do Durand, William T. Freeman, Wojciech Matusik
- **Comment**: Accepted as ECCV 2018 Oral. The 1st and 2nd authors equally
  contributed. Video result: https://youtu.be/GrMLeEcSNzY , Project page:
  http://people.csail.mit.edu/tiam/deepmag/ Some bibliography information was
  fixed
- **Journal**: None
- **Summary**: Video motion magnification techniques allow us to see small motions previously invisible to the naked eyes, such as those of vibrating airplane wings, or swaying buildings under the influence of the wind. Because the motion is small, the magnification results are prone to noise or excessive blurring. The state of the art relies on hand-designed filters to extract representations that may not be optimal. In this paper, we seek to learn the filters directly from examples using deep convolutional neural networks. To make training tractable, we carefully design a synthetic dataset that captures small motion well, and use two-frame input for training. We show that the learned filters achieve high-quality results on real videos, with less ringing artifacts and better noise characteristics than previous methods. While our model is not trained with temporal filters, we found that the temporal filters can be used with our extracted representations up to a moderate magnification, enabling a frequency-based motion selection. Finally, we analyze the learned filters and show that they behave similarly to the derivative filters used in previous works. Our code, trained model, and datasets will be available online.



### Fast Single Image Rain Removal via a Deep Decomposition-Composition Network
- **Arxiv ID**: http://arxiv.org/abs/1804.02688v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.02688v1)
- **Published**: 2018-04-08 13:14:09+00:00
- **Updated**: 2018-04-08 13:14:09+00:00
- **Authors**: Siyuan LI, Wenqi Ren, Jiawan Zhang, Jinke Yu, Xiaojie Guo
- **Comment**: None
- **Journal**: None
- **Summary**: Rain effect in images typically is annoying for many multimedia and computer vision tasks. For removing rain effect from a single image, deep leaning techniques have been attracting considerable attentions. This paper designs a novel multi-task leaning architecture in an end-to-end manner to reduce the mapping range from input to output and boost the performance. Concretely, a decomposition net is built to split rain images into clean background and rain layers. Different from previous architectures, our model consists of, besides a component representing the desired clean image, an extra component for the rain layer. During the training phase, we further employ a composition structure to reproduce the input by the separated clean image and rain information for improving the quality of decomposition. Experimental results on both synthetic and real images are conducted to reveal the high-quality recovery by our design, and show its superiority over other state-of-the-art methods. Furthermore, our design is also applicable to other layer decomposition tasks like dust removal. More importantly, our method only requires about 50ms, significantly faster than the competitors, to process a testing image in VGA resolution on a GTX 1080 GPU, making it attractive for practical use.



### Detecting Multi-Oriented Text with Corner-based Region Proposals
- **Arxiv ID**: http://arxiv.org/abs/1804.02690v2
- **DOI**: 10.1016/j.neucom.2019.01.013
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.02690v2)
- **Published**: 2018-04-08 13:36:03+00:00
- **Updated**: 2019-06-03 00:50:50+00:00
- **Authors**: Linjie Deng, Yanxiang Gong, Yi Lin, Jingwen Shuai, Xiaoguang Tu, Yuefei Zhang, Zheng Ma, Mei Xie
- **Comment**: None
- **Journal**: Neurocomputing, Volume 334, 21 March 2019, Pages 134-142
- **Summary**: Previous approaches for scene text detection usually rely on manually defined sliding windows. This work presents an intuitive two-stage region-based method to detect multi-oriented text without any prior knowledge regarding the textual shape. In the first stage, we estimate the possible locations of text instances by detecting and linking corners instead of shifting a set of default anchors. The quadrilateral proposals are geometry adaptive, which allows our method to cope with various text aspect ratios and orientations. In the second stage, we design a new pooling layer named Dual-RoI Pooling which embeds data augmentation inside the region-wise subnetwork for more robust classification and regression over these proposals. Experimental results on public benchmarks confirm that the proposed method is capable of achieving comparable performance with state-of-the-art methods. The code is publicly available at https://github.com/xhzdeng/crpn



### Personalized Classifier for Food Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/1804.04600v1
- **DOI**: 10.1109/TMM.2018.2814339
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1804.04600v1)
- **Published**: 2018-04-08 14:08:13+00:00
- **Updated**: 2018-04-08 14:08:13+00:00
- **Authors**: Shota Horiguchi, Sosuke Amano, Makoto Ogawa, Kiyoharu Aizawa
- **Comment**: Accepted to IEEE Transaction on Multimedia.
  http://ieeexplore.ieee.org/document/8316919/
- **Journal**: IEEE Transactions on Multimedia 20.10 (2018): 2836-2848
- **Summary**: Currently, food image recognition tasks are evaluated against fixed datasets. However, in real-world conditions, there are cases in which the number of samples in each class continues to increase and samples from novel classes appear. In particular, dynamic datasets in which each individual user creates samples and continues the updating process often have content that varies considerably between different users, and the number of samples per person is very limited. A single classifier common to all users cannot handle such dynamic data. Bridging the gap between the laboratory environment and the real world has not yet been accomplished on a large scale. Personalizing a classifier incrementally for each user is a promising way to do this. In this paper, we address the personalization problem, which involves adapting to the user's domain incrementally using a very limited number of samples. We propose a simple yet effective personalization framework which is a combination of the nearest class mean classifier and the 1-nearest neighbor classifier based on deep features. To conduct realistic experiments, we made use of a new dataset of daily food images collected by a food-logging application. Experimental results show that our proposed method significantly outperforms existing methods.



### Ordinal Pooling Networks: For Preserving Information over Shrinking Feature Maps
- **Arxiv ID**: http://arxiv.org/abs/1804.02702v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1804.02702v2)
- **Published**: 2018-04-08 15:00:46+00:00
- **Updated**: 2018-04-15 18:02:14+00:00
- **Authors**: Ashwani Kumar
- **Comment**: 9 pages with 4 figures
- **Journal**: None
- **Summary**: In the framework of convolutional neural networks that lie at the heart of deep learning, downsampling is often performed with a max-pooling operation that only retains the element with maximum activation, while completely discarding the information contained in other elements in a pooling region. To address this issue, a novel pooling scheme, Ordinal Pooling Network (OPN), is introduced in this work. OPN rearranges all the elements of a pooling region in a sequence and assigns different weights to these elements based upon their orders in the sequence, where the weights are learned via the gradient-based optimisation. The results of our small-scale experiments on image classification task demonstrate that this scheme leads to a consistent improvement in the accuracy over max-pooling operation. This improvement is expected to increase in deeper networks, where several layers of pooling become necessary.



### Image Segmentation using Sparse Subset Selection
- **Arxiv ID**: http://arxiv.org/abs/1804.02721v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.02721v1)
- **Published**: 2018-04-08 17:12:23+00:00
- **Updated**: 2018-04-08 17:12:23+00:00
- **Authors**: Fariba Zohrizadeh, Mohsen Kheirandishfard, Farhad Kamangar
- **Comment**: IEEE Winter Conference on Applications of Computer Vision (WACV),
  2018
- **Journal**: None
- **Summary**: In this paper, we present a new image segmentation method based on the concept of sparse subset selection. Starting with an over-segmentation, we adopt local spectral histogram features to encode the visual information of the small segments into high-dimensional vectors, called superpixel features. Then, the superpixel features are fed into a novel convex model which efficiently leverages the features to group the superpixels into a proper number of coherent regions. Our model automatically determines the optimal number of coherent regions and superpixels assignment to shape final segments. To solve our model, we propose a numerical algorithm based on the alternating direction method of multipliers (ADMM), whose iterations consist of two highly parallelizable sub-problems. We show each sub-problem enjoys closed-form solution which makes the ADMM iterations computationally very efficient. Extensive experiments on benchmark image segmentation datasets demonstrate that our proposed method in combination with an over-segmentation can provide high quality and competitive results compared to the existing state-of-the-art methods.



### Facial Aging and Rejuvenation by Conditional Multi-Adversarial Autoencoder with Ordinal Regression
- **Arxiv ID**: http://arxiv.org/abs/1804.02740v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.02740v1)
- **Published**: 2018-04-08 18:55:38+00:00
- **Updated**: 2018-04-08 18:55:38+00:00
- **Authors**: Haiping Zhu, Qi Zhou, Junping Zhang, James Z. Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Facial aging and facial rejuvenation analyze a given face photograph to predict a future look or estimate a past look of the person. To achieve this, it is critical to preserve human identity and the corresponding aging progression and regression with high accuracy. However, existing methods cannot simultaneously handle these two objectives well. We propose a novel generative adversarial network based approach, named the Conditional Multi-Adversarial AutoEncoder with Ordinal Regression (CMAAE-OR). It utilizes an age estimation technique to control the aging accuracy and takes a high-level feature representation to preserve personalized identity. Specifically, the face is first mapped to a latent vector through a convolutional encoder. The latent vector is then projected onto the face manifold conditional on the age through a deconvolutional generator. The latent vector preserves personalized face features and the age controls facial aging and rejuvenation. A discriminator and an ordinal regression are imposed on the encoder and the generator in tandem, making the generated face images to be more photorealistic while simultaneously exhibiting desirable aging effects. Besides, a high-level feature representation is utilized to preserve personalized identity of the generated face. Experiments on two benchmark datasets demonstrate appealing performance of the proposed method over the state-of-the-art.



### Direct Estimation of Pharmacokinetic Parameters from DCE-MRI using Deep CNN with Forward Physical Model Loss
- **Arxiv ID**: http://arxiv.org/abs/1804.02745v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.02745v2)
- **Published**: 2018-04-08 19:36:39+00:00
- **Updated**: 2018-06-12 11:34:14+00:00
- **Authors**: Cagdas Ulas, Giles Tetteh, Michael J. Thrippleton, Paul A. Armitage, Stephen D. Makin, Joanna M. Wardlaw, Mike E. Davies, Bjoern H. Menze
- **Comment**: Accepted at MICCAI 2018. 9 pages, 6 figures
- **Journal**: None
- **Summary**: Dynamic contrast-enhanced (DCE) MRI is an evolving imaging technique that provides a quantitative measure of pharmacokinetic (PK) parameters in body tissues, in which series of T1-weighted images are collected following the administration of a paramagnetic contrast agent. Unfortunately, in many applications, conventional clinical DCE-MRI suffers from low spatiotemporal resolution and insufficient volume coverage. In this paper, we propose a novel deep learning based approach to directly estimate the PK parameters from undersampled DCE-MRI data. Specifically, we design a custom loss function where we incorporate a forward physical model that relates the PK parameters to corrupted image-time series obtained due to subsampling in k-space. This allows the network to directly exploit the knowledge of true contrast agent kinetics in the training phase, and hence provide more accurate restoration of PK parameters. Experiments on clinical brain DCE datasets demonstrate the efficacy of our approach in terms of fidelity of PK parameter reconstruction and significantly faster parameter inference compared to a model-based iterative reconstruction method.



### Scaling Egocentric Vision: The EPIC-KITCHENS Dataset
- **Arxiv ID**: http://arxiv.org/abs/1804.02748v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.02748v2)
- **Published**: 2018-04-08 20:07:13+00:00
- **Updated**: 2018-07-31 09:05:07+00:00
- **Authors**: Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, Michael Wray
- **Comment**: European Conference on Computer Vision (ECCV) 2018 Dataset and
  Project page: http://epic-kitchens.github.io
- **Journal**: None
- **Summary**: First-person vision is gaining interest as it offers a unique viewpoint on people's interaction with objects, their attention, and even intention. However, progress in this challenging domain has been relatively slow due to the lack of sufficiently large datasets. In this paper, we introduce EPIC-KITCHENS, a large-scale egocentric video benchmark recorded by 32 participants in their native kitchen environments. Our videos depict nonscripted daily activities: we simply asked each participant to start recording every time they entered their kitchen. Recording took place in 4 cities (in North America and Europe) by participants belonging to 10 different nationalities, resulting in highly diverse cooking styles. Our dataset features 55 hours of video consisting of 11.5M frames, which we densely labeled for a total of 39.6K action segments and 454.3K object bounding boxes. Our annotation is unique in that we had the participants narrate their own videos (after recording), thus reflecting true intention, and we crowd-sourced ground-truths based on these. We describe our object, action and anticipation challenges, and evaluate several baselines over two test splits, seen and unseen kitchens. Dataset and Project page: http://epic-kitchens.github.io



### DeepASL: Kinetic Model Incorporated Loss for Denoising Arterial Spin Labeled MRI via Deep Residual Learning
- **Arxiv ID**: http://arxiv.org/abs/1804.02755v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.02755v2)
- **Published**: 2018-04-08 20:27:44+00:00
- **Updated**: 2018-06-12 11:34:02+00:00
- **Authors**: Cagdas Ulas, Giles Tetteh, Stephan Kaczmarz, Christine Preibisch, Bjoern H. Menze
- **Comment**: Accepted at MICCAI 2018. 9 pages, 4 figures, 1 table
- **Journal**: None
- **Summary**: Arterial spin labeling (ASL) allows to quantify the cerebral blood flow (CBF) by magnetic labeling of the arterial blood water. ASL is increasingly used in clinical studies due to its noninvasiveness, repeatability and benefits in quantification. However, ASL suffers from an inherently low-signal-to-noise ratio (SNR) requiring repeated measurements of control/spin-labeled (C/L) pairs to achieve a reasonable image quality, which in return increases motion sensitivity. This leads to clinically prolonged scanning times increasing the risk of motion artifacts. Thus, there is an immense need of advanced imaging and processing techniques in ASL. In this paper, we propose a novel deep learning based approach to improve the perfusion-weighted image quality obtained from a subset of all available pairwise C/L subtractions. Specifically, we train a deep fully convolutional network (FCN) to learn a mapping from noisy perfusion-weighted image and its subtraction (residual) from the clean image. Additionally, we incorporate the CBF estimation model in the loss function during training, which enables the network to produce high quality images while simultaneously enforcing the CBF estimates to be as close as reference CBF values. Extensive experiments on synthetic and clinical ASL datasets demonstrate the effectiveness of our method in terms of improved ASL image quality, accurate CBF parameter estimation and considerably small computation time during testing.



### Expressway visibility estimation based on image entropy and piecewise stationary time series analysis
- **Arxiv ID**: http://arxiv.org/abs/1804.04601v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.04601v1)
- **Published**: 2018-04-08 21:46:05+00:00
- **Updated**: 2018-04-08 21:46:05+00:00
- **Authors**: Xiaogang Cheng, Guoqing Liu, Anders Hedman, Kun Wang, Haibo Li
- **Comment**: None
- **Journal**: None
- **Summary**: Vision-based methods for visibility estimation can play a critical role in reducing traffic accidents caused by fog and haze. To overcome the disadvantages of current visibility estimation methods, we present a novel data-driven approach based on Gaussian image entropy and piecewise stationary time series analysis (SPEV). This is the first time that Gaussian image entropy is used for estimating atmospheric visibility. To lessen the impact of landscape and sunshine illuminance on visibility estimation, we used region of interest (ROI) analysis and took into account relative ratios of image entropy, to improve estimation accuracy. We assume fog and haze cause blurred images and that fog and haze can be considered as a piecewise stationary signal. We used piecewise stationary time series analysis to construct the piecewise causal relationship between image entropy and visibility. To obtain a real-world visibility measure during fog and haze, a subjective assessment was established through a study with 36 subjects who performed visibility observations. Finally, a total of two million videos were used for training the SPEV model and validate its effectiveness. The videos were collected from the constantly foggy and hazy Tongqi expressway in Jiangsu, China. The contrast model of visibility estimation was used for algorithm performance comparison, and the validation results of the SPEV model were encouraging as 99.14% of the relative errors were less than 10%.



### YOLOv3: An Incremental Improvement
- **Arxiv ID**: http://arxiv.org/abs/1804.02767v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.02767v1)
- **Published**: 2018-04-08 22:27:57+00:00
- **Updated**: 2018-04-08 22:27:57+00:00
- **Authors**: Joseph Redmon, Ali Farhadi
- **Comment**: Tech Report
- **Journal**: None
- **Summary**: We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always, all the code is online at https://pjreddie.com/yolo/



### Estimating Depth from RGB and Sparse Sensing
- **Arxiv ID**: http://arxiv.org/abs/1804.02771v2
- **DOI**: 10.1007/978-3-030-01225-0_11
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.02771v2)
- **Published**: 2018-04-08 22:46:10+00:00
- **Updated**: 2018-12-09 22:28:40+00:00
- **Authors**: Zhao Chen, Vijay Badrinarayanan, Gilad Drozdov, Andrew Rabinovich
- **Comment**: European Conference on Computer Vision (ECCV) 2018. Updated to
  camera-ready version with additional experiments
- **Journal**: In: European Conference on Computer Vision. pp. 176-192. Springer
  (2018)
- **Summary**: We present a deep model that can accurately produce dense depth maps given an RGB image with known depth at a very sparse set of pixels. The model works simultaneously for both indoor/outdoor scenes and produces state-of-the-art dense depth maps at nearly real-time speeds on both the NYUv2 and KITTI datasets. We surpass the state-of-the-art for monocular depth estimation even with depth values for only 1 out of every ~10000 image pixels, and we outperform other sparse-to-dense depth methods at all sparsity levels. With depth values for 1/256 of the image pixels, we achieve a mean absolute error of less than 1% of actual depth on indoor scenes, comparable to the performance of consumer-grade depth sensor hardware. Our experiments demonstrate that it would indeed be possible to efficiently transform sparse depth measurements obtained using e.g. lower-power depth sensors or SLAM systems into high-quality dense depth maps.



