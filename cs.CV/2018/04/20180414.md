# Arxiv Papers in cs.CV on 2018-04-14
### HyperFusion-Net: Densely Reflective Fusion for Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1804.05142v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.05142v1)
- **Published**: 2018-04-14 01:06:04+00:00
- **Updated**: 2018-04-14 01:06:04+00:00
- **Authors**: Pingping Zhang, Huchuan Lu, Chunhua Shen
- **Comment**: Submmited to ECCV 2018, 16 pages, including 6 figures and 4 tables.
  arXiv admin note: text overlap with arXiv:1802.06527
- **Journal**: None
- **Summary**: Salient object detection (SOD), which aims to find the most important region of interest and segment the relevant object/item in that area, is an important yet challenging vision task. This problem is inspired by the fact that human seems to perceive main scene elements with high priorities. Thus, accurate detection of salient objects in complex scenes is critical for human-computer interaction. In this paper, we present a novel feature learning framework for SOD, in which we cast the SOD as a pixel-wise classification problem. The proposed framework utilizes a densely hierarchical feature fusion network, named HyperFusion-Net, automatically predicts the most important area and segments the associated objects in an end-to-end manner. Specifically, inspired by the human perception system and image reflection separation, we first decompose input images into reflective image pairs by content-preserving transforms. Then, the complementary information of reflective image pairs is jointly extracted by an interweaved convolutional neural network (ICNN) and hierarchically combined with a hyper-dense fusion mechanism. Based on the fused multi-scale features, our method finally achieves a promising way of predicting SOD. As shown in our extensive experiments, the proposed method consistently outperforms other state-of-the-art methods on seven public datasets with a large margin.



### Road Segmentation Using CNN with GRU
- **Arxiv ID**: http://arxiv.org/abs/1804.05164v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1804.05164v1)
- **Published**: 2018-04-14 04:27:47+00:00
- **Updated**: 2018-04-14 04:27:47+00:00
- **Authors**: Yecheng Lyu, Xinming Huang
- **Comment**: submitted to IV2018
- **Journal**: None
- **Summary**: This paper presents an accurate and fast algorithm for road segmentation using convolutional neural network (CNN) and gated recurrent units (GRU). For autonomous vehicles, road segmentation is a fundamental task that can provide the drivable area for path planning. The existing deep neural network based segmentation algorithms usually take a very deep encoder-decoder structure to fuse pixels, which requires heavy computations, large memory and long processing time. Hereby, a CNN-GRU network model is proposed and trained to perform road segmentation using data captured by the front camera of a vehicle. GRU network obtains a long spatial sequence with lower computational complexity, comparing to traditional encoder-decoder architecture. The proposed road detector is evaluated on the KITTI road benchmark and achieves high accuracy for road segmentation at real-time processing speed.



### On the Limitation of MagNet Defense against $L_1$-based Adversarial Examples
- **Arxiv ID**: http://arxiv.org/abs/1805.00310v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.00310v2)
- **Published**: 2018-04-14 05:44:51+00:00
- **Updated**: 2018-05-09 15:37:54+00:00
- **Authors**: Pei-Hsuan Lu, Pin-Yu Chen, Kang-Cheng Chen, Chia-Mu Yu
- **Comment**: Accepted to IEEE/IFIP International Conference on Dependable and
  Systems and Networks (DSN) 2018 Workshop on Dependable and Secure Machine
  Learning
- **Journal**: None
- **Summary**: In recent years, defending adversarial perturbations to natural examples in order to build robust machine learning models trained by deep neural networks (DNNs) has become an emerging research field in the conjunction of deep learning and security. In particular, MagNet consisting of an adversary detector and a data reformer is by far one of the strongest defenses in the black-box oblivious attack setting, where the attacker aims to craft transferable adversarial examples from an undefended DNN model to bypass an unknown defense module deployed on the same DNN model. Under this setting, MagNet can successfully defend a variety of attacks in DNNs, including the high-confidence adversarial examples generated by the Carlini and Wagner's attack based on the $L_2$ distortion metric. However, in this paper, under the same attack setting we show that adversarial examples crafted based on the $L_1$ distortion metric can easily bypass MagNet and mislead the target DNN image classifiers on MNIST and CIFAR-10. We also provide explanations on why the considered approach can yield adversarial examples with superior attack performance and conduct extensive experiments on variants of MagNet to verify its lack of robustness to $L_1$ distortion based attacks. Notably, our results substantially weaken the assumption of effective threat models on MagNet that require knowing the deployed defense technique when attacking DNNs (i.e., the gray-box attack setting).



### LiDAR and Camera Calibration using Motion Estimated by Sensor Fusion Odometry
- **Arxiv ID**: http://arxiv.org/abs/1804.05178v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1804.05178v1)
- **Published**: 2018-04-14 06:53:07+00:00
- **Updated**: 2018-04-14 06:53:07+00:00
- **Authors**: Ryoichi Ishikawa, Takeshi Oishi, Katsushi Ikeuchi
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a method of targetless and automatic Camera-LiDAR calibration. Our approach is an extension of hand-eye calibration framework to 2D-3D calibration. By using the sensor fusion odometry method, the scaled camera motions are calculated with high accuracy. In addition to this, we clarify the suitable motion for this calibration method.   The proposed method only requires the three-dimensional point cloud and the camera image and does not need other information such as reflectance of LiDAR and to give initial extrinsic parameter. In the experiments, we demonstrate our method using several sensor configurations in indoor and outdoor scenes to verify the effectiveness. The accuracy of our method achieves more than other comparable state-of-the-art methods.



### Select, Attend, and Transfer: Light, Learnable Skip Connections
- **Arxiv ID**: http://arxiv.org/abs/1804.05181v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.05181v3)
- **Published**: 2018-04-14 07:30:15+00:00
- **Updated**: 2018-05-03 00:15:10+00:00
- **Authors**: Saeid Asgari Taghanaki, Aicha Bentaieb, Anmol Sharma, S. Kevin Zhou, Yefeng Zheng, Bogdan Georgescu, Puneet Sharma, Sasa Grbic, Zhoubing Xu, Dorin Comaniciu, Ghassan Hamarneh
- **Comment**: None
- **Journal**: None
- **Summary**: Skip connections in deep networks have improved both segmentation and classification performance by facilitating the training of deeper network architectures, and reducing the risks for vanishing gradients. They equip encoder-decoder-like networks with richer feature representations, but at the cost of higher memory usage, computation, and possibly resulting in transferring non-discriminative feature maps. In this paper, we focus on improving skip connections used in segmentation networks (e.g., U-Net, V-Net, and The One Hundred Layers Tiramisu (DensNet) architectures). We propose light, learnable skip connections which learn to first select the most discriminative channels and then attend to the most discriminative regions of the selected feature maps. The output of the proposed skip connections is a unique feature map which not only reduces the memory usage and network parameters to a high extent, but also improves segmentation accuracy. We evaluate the proposed method on three different 2D and volumetric datasets and demonstrate that the proposed light, learnable skip connections can outperform the traditional heavy skip connections in terms of segmentation accuracy, memory usage, and number of network parameters.



### Motion-based Object Segmentation based on Dense RGB-D Scene Flow
- **Arxiv ID**: http://arxiv.org/abs/1804.05195v2
- **DOI**: 10.1109/LRA.2018.2856525
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1804.05195v2)
- **Published**: 2018-04-14 09:33:40+00:00
- **Updated**: 2018-07-24 08:49:35+00:00
- **Authors**: Lin Shao, Parth Shah, Vikranth Dwaracherla, Jeannette Bohg
- **Comment**: Accepted to IEEE Robotics and Automation Letters and selected by
  IROS'18 Program Committee for presentation at the Conference
- **Journal**: None
- **Summary**: Given two consecutive RGB-D images, we propose a model that estimates a dense 3D motion field, also known as scene flow. We take advantage of the fact that in robot manipulation scenarios, scenes often consist of a set of rigidly moving objects. Our model jointly estimates (i) the segmentation of the scene into an unknown but finite number of objects, (ii) the motion trajectories of these objects and (iii) the object scene flow. We employ an hourglass, deep neural network architecture. In the encoding stage, the RGB and depth images undergo spatial compression and correlation. In the decoding stage, the model outputs three images containing a per-pixel estimate of the corresponding object center as well as object translation and rotation. This forms the basis for inferring the object segmentation and final object scene flow. To evaluate our model, we generated a new and challenging, large-scale, synthetic dataset that is specifically targeted at robotic manipulation: It contains a large number of scenes with a very diverse set of simultaneously moving 3D objects and is recorded with a simulated, static RGB-D camera. In quantitative experiments, we show that we outperform state-of-the-art scene flow and motion-segmentation methods on this data set. In qualitative experiments, we show how our learned model transfers to challenging real-world scenes, visually generating better results than existing methods.



### Beyond Trade-off: Accelerate FCN-based Face Detector with Higher Accuracy
- **Arxiv ID**: http://arxiv.org/abs/1804.05197v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.05197v2)
- **Published**: 2018-04-14 09:38:16+00:00
- **Updated**: 2018-06-02 10:56:10+00:00
- **Authors**: Guanglu Song, Yu Liu, Ming Jiang, Yujie Wang, Junjie Yan, Biao Leng
- **Comment**: Accepted by CVPR2018
- **Journal**: None
- **Summary**: Fully convolutional neural network (FCN) has been dominating the game of face detection task for a few years with its congenital capability of sliding-window-searching with shared kernels, which boiled down all the redundant calculation, and most recent state-of-the-art methods such as Faster-RCNN, SSD, YOLO and FPN use FCN as their backbone. So here comes one question: Can we find a universal strategy to further accelerate FCN with higher accuracy, so could accelerate all the recent FCN-based methods? To analyze this, we decompose the face searching space into two orthogonal directions, `scale' and `spatial'. Only a few coordinates in the space expanded by the two base vectors indicate foreground. So if FCN could ignore most of the other points, the searching space and false alarm should be significantly boiled down. Based on this philosophy, a novel method named scale estimation and spatial attention proposal ($S^2AP$) is proposed to pay attention to some specific scales and valid locations in the image pyramid. Furthermore, we adopt a masked-convolution operation based on the attention result to accelerate FCN calculation. Experiments show that FCN-based method RPN can be accelerated by about $4\times$ with the help of $S^2AP$ and masked-FCN and at the same time it can also achieve the state-of-the-art on FDDB, AFW and MALF face detection benchmarks as well.



### ClassiNet -- Predicting Missing Features for Short-Text Classification
- **Arxiv ID**: http://arxiv.org/abs/1804.05260v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1804.05260v1)
- **Published**: 2018-04-14 18:24:06+00:00
- **Updated**: 2018-04-14 18:24:06+00:00
- **Authors**: Danushka Bollegala, Vincent Atanasov, Takanori Maehara, Ken-ichi Kawarabayashi
- **Comment**: Accepted to ACM TKDD
- **Journal**: None
- **Summary**: The fundamental problem in short-text classification is \emph{feature sparseness} -- the lack of feature overlap between a trained model and a test instance to be classified. We propose \emph{ClassiNet} -- a network of classifiers trained for predicting missing features in a given instance, to overcome the feature sparseness problem. Using a set of unlabeled training instances, we first learn binary classifiers as feature predictors for predicting whether a particular feature occurs in a given instance. Next, each feature predictor is represented as a vertex $v_i$ in the ClassiNet where a one-to-one correspondence exists between feature predictors and vertices. The weight of the directed edge $e_{ij}$ connecting a vertex $v_i$ to a vertex $v_j$ represents the conditional probability that given $v_i$ exists in an instance, $v_j$ also exists in the same instance. We show that ClassiNets generalize word co-occurrence graphs by considering implicit co-occurrences between features. We extract numerous features from the trained ClassiNet to overcome feature sparseness. In particular, for a given instance $\vec{x}$, we find similar features from ClassiNet that did not appear in $\vec{x}$, and append those features in the representation of $\vec{x}$. Moreover, we propose a method based on graph propagation to find features that are indirectly related to a given short-text. We evaluate ClassiNets on several benchmark datasets for short-text classification. Our experimental results show that by using ClassiNet, we can statistically significantly improve the accuracy in short-text classification tasks, without having to use any external resources such as thesauri for finding related features.



### Physics-driven Fire Modeling from Multi-view Images
- **Arxiv ID**: http://arxiv.org/abs/1804.05261v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1804.05261v1)
- **Published**: 2018-04-14 18:28:51+00:00
- **Updated**: 2018-04-14 18:28:51+00:00
- **Authors**: Garoe Dorta, Luca Benedetti, Dmitry Kit, Yong-Liang Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Fire effects are widely used in various computer graphics applications such as visual effects and video games. Modeling the shape and appearance of fire phenomenon is challenging as the underlying effects are driven by complex laws of physics. State-of-the-art fire modeling techniques rely on sophisticated physical simulations which require intensive parameter tuning, or use simplifications which produce physically invalid results. In this paper, we present a novel method of reconstructing physically valid fire models from multi-view stereo images. Our method, for the first time, provides plausible estimation of physical properties (e.g., temperature, density) of a fire volume using RGB cameras. This allows for a number of novel phenomena such as global fire illumination effects. The effectiveness and usefulness of our method are tested by generating fire models from a variety of input data, and applying the reconstructed fire models for realistic illumination of virtual scenes.



### Fusion of hyperspectral and ground penetrating radar to estimate soil moisture
- **Arxiv ID**: http://arxiv.org/abs/1804.05273v3
- **DOI**: 10.1109/WHISPERS.2018.8747076
- **Categories**: **cs.CV**, physics.geo-ph
- **Links**: [PDF](http://arxiv.org/pdf/1804.05273v3)
- **Published**: 2018-04-14 20:51:54+00:00
- **Updated**: 2018-08-07 13:22:06+00:00
- **Authors**: Felix M. Riese, Sina Keller
- **Comment**: This work has been accepted to the IEEE WHISPERS 2018 conference. (C)
  2018 IEEE
- **Journal**: None
- **Summary**: In this contribution, we investigate the potential of hyperspectral data combined with either simulated ground penetrating radar (GPR) or simulated (sensor-like) soil-moisture data to estimate soil moisture. We propose two simulation approaches to extend a given multi-sensor dataset which contains sparse GPR data. In the first approach, simulated GPR data is generated either by an interpolation along the time axis or by a machine learning model. The second approach includes the simulation of soil-moisture along the GPR profile. The soil-moisture estimation is improved significantly by the fusion of hyperspectral and GPR data. In contrast, the combination of simulated, sensor-like soil-moisture values and hyperspectral data achieves the worst regression performance. In conclusion, the estimation of soil moisture with hyperspectral and GPR data engages further investigations.



### Horizontal Pyramid Matching for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1804.05275v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.05275v4)
- **Published**: 2018-04-14 20:53:40+00:00
- **Updated**: 2018-11-10 03:58:35+00:00
- **Authors**: Yang Fu, Yunchao Wei, Yuqian Zhou, Honghui Shi, Gao Huang, Xinchao Wang, Zhiqiang Yao, Thomas Huang
- **Comment**: Accepted by AAAI 2019
- **Journal**: None
- **Summary**: Despite the remarkable recent progress, person re-identification (Re-ID) approaches are still suffering from the failure cases where the discriminative body parts are missing. To mitigate such cases, we propose a simple yet effective Horizontal Pyramid Matching (HPM) approach to fully exploit various partial information of a given person, so that correct person candidates can be still identified even even some key parts are missing. Within the HPM, we make the following contributions to produce a more robust feature representation for the Re-ID task: 1) we learn to classify using partial feature representations at different horizontal pyramid scales, which successfully enhance the discriminative capabilities of various person parts; 2) we exploit average and max pooling strategies to account for person-specific discriminative information in a global-local manner. To validate the effectiveness of the proposed HPM, extensive experiments are conducted on three popular benchmarks, including Market-1501, DukeMTMC-ReID and CUHK03. In particular, we achieve mAP scores of 83.1%, 74.5% and 59.7% on these benchmarks, which are the new state-of-the-arts. Our code is available on Github



### On the Selection of Anchors and Targets for Video Hyperlinking
- **Arxiv ID**: http://arxiv.org/abs/1804.05286v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.05286v1)
- **Published**: 2018-04-14 22:53:39+00:00
- **Updated**: 2018-04-14 22:53:39+00:00
- **Authors**: Zhi-Qi Cheng, Hao Zhang, Xiao Wu, Chong-Wah Ngo
- **Comment**: ACM International Conference on Multimedia Retrieval (ICMR), 2017.
  (Oral)
- **Journal**: None
- **Summary**: A problem not well understood in video hyperlinking is what qualifies a fragment as an anchor or target. Ideally, anchors provide good starting points for navigation, and targets supplement anchors with additional details while not distracting users with irrelevant, false and redundant information. The problem is not trivial for intertwining relationship between data characteristics and user expectation. Imagine that in a large dataset, there are clusters of fragments spreading over the feature space. The nature of each cluster can be described by its size (implying popularity) and structure (implying complexity). A principle way of hyperlinking can be carried out by picking centers of clusters as anchors and from there reach out to targets within or outside of clusters with consideration of neighborhood complexity. The question is which fragments should be selected either as anchors or targets, in one way to reflect the rich content of a dataset, and meanwhile to minimize the risk of frustrating user experience. This paper provides some insights to this question from the perspective of hubness and local intrinsic dimensionality, which are two statistical properties in assessing the popularity and complexity of data space. Based these properties, two novel algorithms are proposed for low-risk automatic selection of anchors and targets.



### Video2Shop: Exact Matching Clothes in Videos to Online Shopping Images
- **Arxiv ID**: http://arxiv.org/abs/1804.05287v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.05287v2)
- **Published**: 2018-04-14 22:59:44+00:00
- **Updated**: 2018-12-04 16:50:02+00:00
- **Authors**: Zhi-Qi Cheng, Xiao Wu, Yang Liu, Xian-Sheng Hua
- **Comment**: IEEE International Conference on Computer Vision and Pattern
  Recognition (CVPR), 2017
- **Journal**: None
- **Summary**: In recent years, both online retail and video hosting service are exponentially growing. In this paper, we explore a new cross-domain task, Video2Shop, targeting for matching clothes appeared in videos to the exact same items in online shops. A novel deep neural network, called AsymNet, is proposed to explore this problem. For the image side, well-established methods are used to detect and extract features for clothing patches with arbitrary sizes. For the video side, deep visual features are extracted from detected object regions in each frame, and further fed into a Long Short-Term Memory (LSTM) framework for sequence modeling, which captures the temporal dynamics in videos. To conduct exact matching between videos and online shopping images, LSTM hidden states, representing the video, and image features, which represent static object images, are jointly modeled under the similarity network with reconfigurable deep tree structure. Moreover, an approximate training method is proposed to achieve the efficiency when training. Extensive experiments conducted on a large cross-domain dataset have demonstrated the effectiveness and efficiency of the proposed AsymNet, which outperforms the state-of-the-art methods.



