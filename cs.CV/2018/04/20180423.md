# Arxiv Papers in cs.CV on 2018-04-23
### syGlass: Interactive Exploration of Multidimensional Images Using Virtual Reality Head-mounted Displays
- **Arxiv ID**: http://arxiv.org/abs/1804.08197v4
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1804.08197v4)
- **Published**: 2018-04-23 00:04:54+00:00
- **Updated**: 2018-08-22 01:48:32+00:00
- **Authors**: Stanislav Pidhorskyi, Michael Morehead, Quinn Jones, George Spirou, Gianfranco Doretto
- **Comment**: None
- **Journal**: None
- **Summary**: The quest for deeper understanding of biological systems has driven the acquisition of increasingly larger multidimensional image datasets. Inspecting and manipulating data of this complexity is very challenging in traditional visualization systems. We developed syGlass, a software package capable of visualizing large scale volumetric data with inexpensive virtual reality head-mounted display technology. This allows leveraging stereoscopic vision to significantly improve perception of complex 3D structures, and provides immersive interaction with data directly in 3D. We accomplished this by developing highly optimized data flow and volume rendering pipelines, tested on datasets up to 16TB in size, as well as tools available in a virtual reality GUI to support advanced data exploration, annotation, and cataloguing.



### High Performance Visual Tracking with Circular and Structural Operators
- **Arxiv ID**: http://arxiv.org/abs/1804.08208v3
- **DOI**: 10.1016/j.knosys.2018.08.008
- **Categories**: **cs.CV**, cs.AI, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1804.08208v3)
- **Published**: 2018-04-23 01:08:43+00:00
- **Updated**: 2018-10-13 07:06:14+00:00
- **Authors**: Peng Gao, Yipeng Ma, Ke Song, Chao Li, Fei Wang, Liyi Xiao, Yan Zhang
- **Comment**: Accepted to Knowledge-Based SYSTEMS
- **Journal**: None
- **Summary**: In this paper, a novel circular and structural operator tracker (CSOT) is proposed for high performance visual tracking, it not only possesses the powerful discriminative capability of SOSVM but also efficiently inherits the superior computational efficiency of DCF. Based on the proposed circular and structural operators, a set of primal confidence score maps can be obtained by circular correlating feature maps with their corresponding structural correlation filters. Furthermore, an implicit interpolation is applied to convert the multi-resolution feature maps to the continuous domain and make all primal confidence score maps have the same spatial resolution. Then, we exploit an efficient ensemble post-processor based on relative entropy, which can coalesce primal confidence score maps and create an optimal confidence score map for more accurate localization. The target is localized on the peak of the optimal confidence score map. Besides, we introduce a collaborative optimization strategy to update circular and structural operators by iteratively training structural correlation filters, which significantly reduces computational complexity and improves robustness. Experimental results demonstrate that our approach achieves state-of-the-art performance in mean AUC scores of 71.5% and 69.4% on the OTB-2013 and OTB-2015 benchmarks respectively, and obtains a third-best expected average overlap (EAO) score of 29.8% on the VOT-2017 benchmark.



### Multi-scale prediction for robust hand detection and classification
- **Arxiv ID**: http://arxiv.org/abs/1804.08220v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.08220v1)
- **Published**: 2018-04-23 02:02:14+00:00
- **Updated**: 2018-04-23 02:02:14+00:00
- **Authors**: Ding Lu, Yong Wang, Robert Laganiere, Xinbin Luo, Shan Fu
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a multi-scale Fully Convolutional Networks (MSP-RFCN) to robustly detect and classify human hands under various challenging conditions. In our approach, the input image is passed through the proposed network to generate score maps, based on multi-scale predictions. The network has been specifically designed to deal with small objects. It uses an architecture based on region proposals generated at multiple scales. Our method is evaluated on challenging hand datasets, namely the Vision for Intelligent Vehicles and Applications (VIVA) Challenge and the Oxford hand dataset. It is compared against recent hand detection algorithms. The experimental results demonstrate that our proposed method achieves state-of-the-art detection for hands of various sizes.



### Constructing Locally Dense Point Clouds Using OpenSfM and ORB-SLAM2
- **Arxiv ID**: http://arxiv.org/abs/1804.08243v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.08243v1)
- **Published**: 2018-04-23 05:07:51+00:00
- **Updated**: 2018-04-23 05:07:51+00:00
- **Authors**: Fouad Amer, Zixu Zhao, Siwei Tang, Wilfredo Torres
- **Comment**: None
- **Journal**: None
- **Summary**: This paper aims at finding a method to register two different point clouds constructed by ORB-SLAM2 and OpenSfM. To do this, we post some tags with unique textures in the scene and take videos and photos of that area. Then we take short videos of only the tags to extract their features. By matching the ORB feature of the tags with their corresponding features in the scene, it is then possible to localize the position of these tags both in point clouds constructed by ORB-SLAM2 and OpenSfM. Thus, the best transformation matrix between two point clouds can be calculated, and the two point clouds can be aligned.



### Memory Attention Networks for Skeleton-based Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1804.08254v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.08254v2)
- **Published**: 2018-04-23 06:32:27+00:00
- **Updated**: 2018-05-03 08:20:42+00:00
- **Authors**: Chunyu Xie, Ce Li, Baochang Zhang, Chen Chen, Jungong Han, Changqing Zou, Jianzhuang Liu
- **Comment**: Accepted by IJCAI 2018
- **Journal**: None
- **Summary**: Skeleton-based action recognition task is entangled with complex spatio-temporal variations of skeleton joints, and remains challenging for Recurrent Neural Networks (RNNs). In this work, we propose a temporal-then-spatial recalibration scheme to alleviate such complex variations, resulting in an end-to-end Memory Attention Networks (MANs) which consist of a Temporal Attention Recalibration Module (TARM) and a Spatio-Temporal Convolution Module (STCM). Specifically, the TARM is deployed in a residual learning module that employs a novel attention learning network to recalibrate the temporal attention of frames in a skeleton sequence. The STCM treats the attention calibrated skeleton joint sequences as images and leverages the Convolution Neural Networks (CNNs) to further model the spatial and temporal information of skeleton data. These two modules (TARM and STCM) seamlessly form a single network architecture that can be trained in an end-to-end fashion. MANs significantly boost the performance of skeleton-based action recognition and achieve the best results on four challenging benchmark datasets: NTU RGB+D, HDM05, SYSU-3D and UT-Kinect.



### Progressive refinement: a method of coarse-to-fine image parsing using stacked network
- **Arxiv ID**: http://arxiv.org/abs/1804.08256v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1804.08256v1)
- **Published**: 2018-04-23 06:33:53+00:00
- **Updated**: 2018-04-23 06:33:53+00:00
- **Authors**: Jiagao Hu, Zhengxing Sun, Yunhan Sun, Jinlong Shi
- **Comment**: Accepted for presentation in an ORAL session at ICME 2018
- **Journal**: None
- **Summary**: To parse images into fine-grained semantic parts, the complex fine-grained elements will put it in trouble when using off-the-shelf semantic segmentation networks. In this paper, for image parsing task, we propose to parse images from coarse to fine with progressively refined semantic classes. It is achieved by stacking the segmentation layers in a segmentation network several times. The former segmentation module parses images at a coarser-grained level, and the result will be feed to the following one to provide effective contextual clues for the finer-grained parsing. To recover the details of small structures, we add skip connections from shallow layers of the network to fine-grained parsing modules. As for the network training, we merge classes in groundtruth to get coarse-to-fine label maps, and train the stacked network with these hierarchical supervision end-to-end. Our coarse-to-fine stacked framework can be injected into many advanced neural networks to improve the parsing results. Extensive evaluations on several public datasets including face parsing and human parsing well demonstrate the superiority of our method.



### To Create What You Tell: Generating Videos from Captions
- **Arxiv ID**: http://arxiv.org/abs/1804.08264v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.08264v1)
- **Published**: 2018-04-23 07:07:20+00:00
- **Updated**: 2018-04-23 07:07:20+00:00
- **Authors**: Yingwei Pan, Zhaofan Qiu, Ting Yao, Houqiang Li, Tao Mei
- **Comment**: ACM MM 2017 Brave New Idea
- **Journal**: None
- **Summary**: We are creating multimedia contents everyday and everywhere. While automatic content generation has played a fundamental challenge to multimedia community for decades, recent advances of deep learning have made this problem feasible. For example, the Generative Adversarial Networks (GANs) is a rewarding approach to synthesize images. Nevertheless, it is not trivial when capitalizing on GANs to generate videos. The difficulty originates from the intrinsic structure where a video is a sequence of visually coherent and semantically dependent frames. This motivates us to explore semantic and temporal coherence in designing GANs to generate videos. In this paper, we present a novel Temporal GANs conditioning on Captions, namely TGANs-C, in which the input to the generator network is a concatenation of a latent noise vector and caption embedding, and then is transformed into a frame sequence with 3D spatio-temporal convolutions. Unlike the naive discriminator which only judges pairs as fake or real, our discriminator additionally notes whether the video matches the correct caption. In particular, the discriminator network consists of three discriminators: video discriminator classifying realistic videos from generated ones and optimizes video-caption matching, frame discriminator discriminating between real and fake frames and aligning frames with the conditioning caption, and motion discriminator emphasizing the philosophy that the adjacent frames in the generated videos should be smoothly connected as in real ones. We qualitatively demonstrate the capability of our TGANs-C to generate plausible videos conditioning on the given captions on two synthetic datasets (SBMG and TBMG) and one real-world dataset (MSVD). Moreover, quantitative experiments on MSVD are performed to validate our proposal via Generative Adversarial Metric and human study.



### Jointly Localizing and Describing Events for Dense Video Captioning
- **Arxiv ID**: http://arxiv.org/abs/1804.08274v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.08274v1)
- **Published**: 2018-04-23 08:18:35+00:00
- **Updated**: 2018-04-23 08:18:35+00:00
- **Authors**: Yehao Li, Ting Yao, Yingwei Pan, Hongyang Chao, Tao Mei
- **Comment**: CVPR 2018 Spotlight, Rank 1 in ActivityNet Captions Challenge 2017
- **Journal**: None
- **Summary**: Automatically describing a video with natural language is regarded as a fundamental challenge in computer vision. The problem nevertheless is not trivial especially when a video contains multiple events to be worthy of mention, which often happens in real videos. A valid question is how to temporally localize and then describe events, which is known as "dense video captioning." In this paper, we present a novel framework for dense video captioning that unifies the localization of temporal event proposals and sentence generation of each proposal, by jointly training them in an end-to-end manner. To combine these two worlds, we integrate a new design, namely descriptiveness regression, into a single shot detection structure to infer the descriptive complexity of each detected proposal via sentence generation. This in turn adjusts the temporal locations of each event proposal. Our model differs from existing dense video captioning methods since we propose a joint and global optimization of detection and captioning, and the framework uniquely capitalizes on an attribute-augmented video captioning architecture. Extensive experiments are conducted on ActivityNet Captions dataset and our framework shows clear improvements when compared to the state-of-the-art techniques. More remarkably, we obtain a new record: METEOR of 12.96% on ActivityNet Captions official test set.



### Deep Semantic Hashing with Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1804.08275v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1804.08275v1)
- **Published**: 2018-04-23 08:19:55+00:00
- **Updated**: 2018-04-23 08:19:55+00:00
- **Authors**: Zhaofan Qiu, Yingwei Pan, Ting Yao, Tao Mei
- **Comment**: SIGIR 2017 Oral
- **Journal**: None
- **Summary**: Hashing has been a widely-adopted technique for nearest neighbor search in large-scale image retrieval tasks. Recent research has shown that leveraging supervised information can lead to high quality hashing. However, the cost of annotating data is often an obstacle when applying supervised hashing to a new domain. Moreover, the results can suffer from the robustness problem as the data at training and test stage could come from similar but different distributions. This paper studies the exploration of generating synthetic data through semi-supervised generative adversarial networks (GANs), which leverages largely unlabeled and limited labeled training data to produce highly compelling data with intrinsic invariance and global coherence, for better understanding statistical structures of natural data. We demonstrate that the above two limitations can be well mitigated by applying the synthetic data for hashing. Specifically, a novel deep semantic hashing with GANs (DSH-GANs) is presented, which mainly consists of four components: a deep convolution neural networks (CNN) for learning image representations, an adversary stream to distinguish synthetic images from real ones, a hash stream for encoding image representations to hash codes and a classification stream. The whole architecture is trained end-to-end by jointly optimizing three losses, i.e., adversarial loss to correct label of synthetic or real for each sample, triplet ranking loss to preserve the relative similarity ordering in the input real-synthetic triplets and classification loss to classify each sample accurately. Extensive experiments conducted on both CIFAR-10 and NUS-WIDE image benchmarks validate the capability of exploiting synthetic images for hashing. Our framework also achieves superior results when compared to state-of-the-art deep hash models.



### Memory Matching Networks for One-Shot Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/1804.08281v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.08281v1)
- **Published**: 2018-04-23 08:31:26+00:00
- **Updated**: 2018-04-23 08:31:26+00:00
- **Authors**: Qi Cai, Yingwei Pan, Ting Yao, Chenggang Yan, Tao Mei
- **Comment**: CVPR 2018
- **Journal**: None
- **Summary**: In this paper, we introduce the new ideas of augmenting Convolutional Neural Networks (CNNs) with Memory and learning to learn the network parameters for the unlabelled images on the fly in one-shot learning. Specifically, we present Memory Matching Networks (MM-Net) --- a novel deep architecture that explores the training procedure, following the philosophy that training and test conditions must match. Technically, MM-Net writes the features of a set of labelled images (support set) into memory and reads from memory when performing inference to holistically leverage the knowledge in the set. Meanwhile, a Contextual Learner employs the memory slots in a sequential manner to predict the parameters of CNNs for unlabelled images. The whole architecture is trained by once showing only a few examples per class and switching the learning from minibatch to minibatch, which is tailored for one-shot learning when presented with a few examples of new categories at test time. Unlike the conventional one-shot learning approaches, our MM-Net could output one unified model irrespective of the number of shots and categories. Extensive experiments are conducted on two public datasets, i.e., Omniglot and \emph{mini}ImageNet, and superior results are reported when compared to state-of-the-art approaches. More remarkably, our MM-Net improves one-shot accuracy on Omniglot from 98.95% to 99.28% and from 49.21% to 53.37% on \emph{mini}ImageNet.



### Fully Convolutional Adaptation Networks for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1804.08286v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.08286v1)
- **Published**: 2018-04-23 08:44:52+00:00
- **Updated**: 2018-04-23 08:44:52+00:00
- **Authors**: Yiheng Zhang, Zhaofan Qiu, Ting Yao, Dong Liu, Tao Mei
- **Comment**: CVPR 2018, Rank 1 in Segmentation Track of Visual Domain Adaptation
  Challenge 2017
- **Journal**: None
- **Summary**: The recent advances in deep neural networks have convincingly demonstrated high capability in learning vision models on large datasets. Nevertheless, collecting expert labeled datasets especially with pixel-level annotations is an extremely expensive process. An appealing alternative is to render synthetic data (e.g., computer games) and generate ground truth automatically. However, simply applying the models learnt on synthetic images may lead to high generalization error on real images due to domain shift. In this paper, we facilitate this issue from the perspectives of both visual appearance-level and representation-level domain adaptation. The former adapts source-domain images to appear as if drawn from the "style" in the target domain and the latter attempts to learn domain-invariant representations. Specifically, we present Fully Convolutional Adaptation Networks (FCAN), a novel deep architecture for semantic segmentation which combines Appearance Adaptation Networks (AAN) and Representation Adaptation Networks (RAN). AAN learns a transformation from one domain to the other in the pixel space and RAN is optimized in an adversarial learning manner to maximally fool the domain discriminator with the learnt source and target representations. Extensive experiments are conducted on the transfer from GTA5 (game videos) to Cityscapes (urban street scenes) on semantic segmentation and our proposal achieves superior results when comparing to state-of-the-art unsupervised adaptation techniques. More remarkably, we obtain a new record: mIoU of 47.5% on BDDS (drive-cam videos) in an unsupervised setting.



### MVTec D2S: Densely Segmented Supermarket Dataset
- **Arxiv ID**: http://arxiv.org/abs/1804.08292v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.08292v2)
- **Published**: 2018-04-23 09:01:26+00:00
- **Updated**: 2018-07-25 15:50:26+00:00
- **Authors**: Patrick Follmann, Tobias Böttger, Philipp Härtinger, Rebecca König, Markus Ulrich
- **Comment**: accepted to ECCV 2018
- **Journal**: None
- **Summary**: We introduce the Densely Segmented Supermarket (D2S) dataset, a novel benchmark for instance-aware semantic segmentation in an industrial domain. It contains 21,000 high-resolution images with pixel-wise labels of all object instances. The objects comprise groceries and everyday products from 60 categories. The benchmark is designed such that it resembles the real-world setting of an automatic checkout, inventory, or warehouse system. The training images only contain objects of a single class on a homogeneous background, while the validation and test sets are much more complex and diverse. To further benchmark the robustness of instance segmentation methods, the scenes are acquired with different lightings, rotations, and backgrounds. We ensure that there are no ambiguities in the labels and that every instance is labeled comprehensively. The annotations are pixel-precise and allow using crops of single instances for articial data augmentation. The dataset covers several challenges highly relevant in the field, such as a limited amount of training data and a high diversity in the test and validation sets. The evaluation of state-of-the-art object detection and instance segmentation methods on D2S reveals significant room for improvement.



### Deep cross-domain building extraction for selective depth estimation from oblique aerial imagery
- **Arxiv ID**: http://arxiv.org/abs/1804.08302v3
- **DOI**: 10.5194/isprs-annals-IV-1-125-2018
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1804.08302v3)
- **Published**: 2018-04-23 09:22:55+00:00
- **Updated**: 2019-09-21 20:24:52+00:00
- **Authors**: Boitumelo Ruf, Laurenz Thiel, Martin Weinmann
- **Comment**: Accepted in the ISPRS Annals of the Photogrammetry, Remote Sensing
  and Spatial Information Science
- **Journal**: ISPRS Ann. Photogramm. Remote Sens. Spatial Inf. Sci., IV-1,
  125-132, 2018
- **Summary**: With the technological advancements of aerial imagery and accurate 3d reconstruction of urban environments, more and more attention has been paid to the automated analyses of urban areas. In our work, we examine two important aspects that allow live analysis of building structures in city models given oblique aerial imagery, namely automatic building extraction with convolutional neural networks (CNNs) and selective real-time depth estimation from aerial imagery. We use transfer learning to train the Faster R-CNN method for real-time deep object detection, by combining a large ground-based dataset for urban scene understanding with a smaller number of images from an aerial dataset. We achieve an average precision (AP) of about 80% for the task of building extraction on a selected evaluation dataset. Our evaluation focuses on both dataset-specific learning and transfer learning. Furthermore, we present an algorithm that allows for multi-view depth estimation from aerial imagery in real-time. We adopt the semi-global matching (SGM) optimization strategy to preserve sharp edges at object boundaries. In combination with the Faster R-CNN, it allows a selective reconstruction of buildings, identified with regions of interest (RoIs), from oblique aerial imagery.



### Taskonomy: Disentangling Task Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/1804.08328v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.NE, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1804.08328v1)
- **Published**: 2018-04-23 10:46:28+00:00
- **Updated**: 2018-04-23 10:46:28+00:00
- **Authors**: Amir Zamir, Alexander Sax, William Shen, Leonidas Guibas, Jitendra Malik, Silvio Savarese
- **Comment**: CVPR 2018 (Oral). See project website and live demos at
  http://taskonomy.vision/
- **Journal**: None
- **Summary**: Do visual tasks have a relationship, or are they unrelated? For instance, could having surface normals simplify estimating the depth of an image? Intuition answers these questions positively, implying existence of a structure among visual tasks. Knowing this structure has notable values; it is the concept underlying transfer learning and provides a principled way for identifying redundancies across tasks, e.g., to seamlessly reuse supervision among related tasks or solve many tasks in one system without piling up the complexity.   We proposes a fully computational approach for modeling the structure of space of visual tasks. This is done via finding (first and higher-order) transfer learning dependencies across a dictionary of twenty six 2D, 2.5D, 3D, and semantic tasks in a latent space. The product is a computational taxonomic map for task transfer learning. We study the consequences of this structure, e.g. nontrivial emerged relationships, and exploit them to reduce the demand for labeled data. For example, we show that the total number of labeled datapoints needed for solving a set of 10 tasks can be reduced by roughly 2/3 (compared to training independently) while keeping the performance nearly the same. We provide a set of tools for computing and probing this taxonomical structure including a solver that users can employ to devise efficient supervision policies for their use cases.



### Deep Facial Expression Recognition: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1804.08348v2
- **DOI**: 10.1109/TAFFC.2020.2981446
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.08348v2)
- **Published**: 2018-04-23 11:40:09+00:00
- **Updated**: 2018-10-22 02:26:49+00:00
- **Authors**: Shan Li, Weihong Deng
- **Comment**: None
- **Journal**: IEEE Transactions on Affective Computing 2020
- **Summary**: With the transition of facial expression recognition (FER) from laboratory-controlled to challenging in-the-wild conditions and the recent success of deep learning techniques in various fields, deep neural networks have increasingly been leveraged to learn discriminative representations for automatic FER. Recent deep FER systems generally focus on two important issues: overfitting caused by a lack of sufficient training data and expression-unrelated variations, such as illumination, head pose and identity bias. In this paper, we provide a comprehensive survey on deep FER, including datasets and algorithms that provide insights into these intrinsic problems. First, we describe the standard pipeline of a deep FER system with the related background knowledge and suggestions of applicable implementations for each stage. We then introduce the available datasets that are widely used in the literature and provide accepted data selection and evaluation principles for these datasets. For the state of the art in deep FER, we review existing novel deep neural networks and related training strategies that are designed for FER based on both static images and dynamic image sequences, and discuss their advantages and limitations. Competitive performances on widely used benchmarks are also summarized in this section. We then extend our survey to additional related issues and application scenarios. Finally, we review the remaining challenges and corresponding opportunities in this field as well as future directions for the design of robust deep FER systems.



### Multi-focus Image Fusion using dictionary learning and Low-Rank Representation
- **Arxiv ID**: http://arxiv.org/abs/1804.08355v2
- **DOI**: 10.1007/978-3-319-71607-7_59
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.08355v2)
- **Published**: 2018-04-23 11:57:44+00:00
- **Updated**: 2018-12-18 08:32:20+00:00
- **Authors**: Hui Li, Xiao-Jun Wu
- **Comment**: 12 pages, 5 figures, 2 tables. The 9th International Conference on
  Image and Graphics (ICIG 2017, Oral)
- **Journal**: None
- **Summary**: Among the representation learning, the low-rank representation (LRR) is one of the hot research topics in many fields, especially in image processing and pattern recognition. Although LRR can capture the global structure, the ability of local structure preservation is limited because LRR lacks dictionary learning. In this paper, we propose a novel multi-focus image fusion method based on dictionary learning and LRR to get a better performance in both global and local structure. Firstly, the source images are divided into several patches by sliding window technique. Then, the patches are classified according to the Histogram of Oriented Gradient (HOG) features. And the sub-dictionaries of each class are learned by K-singular value decomposition (K-SVD) algorithm. Secondly, a global dictionary is constructed by combining these sub-dictionaries. Then, we use the global dictionary in LRR to obtain the LRR coefficients vector for each patch. Finally, the l_1-norm and choose-max fuse strategy for each coefficients vector is adopted to reconstruct fused image from the fused LRR coefficients and the global dictionary. Experimental results demonstrate that the proposed method can obtain state-of-the-art performance in both qualitative and quantitative evaluations compared with serval classical methods and novel methods.The Code of our fusion method is available at https://github.com/hli1221/imagefusion_dllrr



### DenseFuse: A Fusion Approach to Infrared and Visible Images
- **Arxiv ID**: http://arxiv.org/abs/1804.08361v9
- **DOI**: 10.1109/TIP.2018.2887342
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.08361v9)
- **Published**: 2018-04-23 12:20:11+00:00
- **Updated**: 2019-01-20 05:46:09+00:00
- **Authors**: Hui Li, Xiao-Jun Wu
- **Comment**: 10 pages, 11 figures, 2 tables. Accepted by IEEE Transactions on
  Image Processing
- **Journal**: None
- **Summary**: In this paper, we present a novel deep learning architecture for infrared and visible images fusion problem. In contrast to conventional convolutional networks, our encoding network is combined by convolutional layers, fusion layer and dense block in which the output of each layer is connected to every other layer. We attempt to use this architecture to get more useful features from source images in encoding process. And two fusion layers(fusion strategies) are designed to fuse these features. Finally, the fused image is reconstructed by decoder. Compared with existing fusion methods, the proposed fusion method achieves state-of-the-art performance in objective and subjective assessment. Code and pre-trained models are available at https://github.com/hli1221/imagefusion_densefuse



### VLocNet++: Deep Multitask Learning for Semantic Visual Localization and Odometry
- **Arxiv ID**: http://arxiv.org/abs/1804.08366v6
- **DOI**: 10.1109/LRA.2018.2869640
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1804.08366v6)
- **Published**: 2018-04-23 12:30:16+00:00
- **Updated**: 2018-10-11 15:36:09+00:00
- **Authors**: Noha Radwan, Abhinav Valada, Wolfram Burgard
- **Comment**: Demo and dataset available at http://deeploc.cs.uni-freiburg.de
- **Journal**: IEEE Robotics and Automation Letters (RA-L), 3(4):4407-4414, 2018
- **Summary**: Semantic understanding and localization are fundamental enablers of robot autonomy that have for the most part been tackled as disjoint problems. While deep learning has enabled recent breakthroughs across a wide spectrum of scene understanding tasks, its applicability to state estimation tasks has been limited due to the direct formulation that renders it incapable of encoding scene-specific constrains. In this work, we propose the VLocNet++ architecture that employs a multitask learning approach to exploit the inter-task relationship between learning semantics, regressing 6-DoF global pose and odometry, for the mutual benefit of each of these tasks. Our network overcomes the aforementioned limitation by simultaneously embedding geometric and semantic knowledge of the world into the pose regression network. We propose a novel adaptive weighted fusion layer to aggregate motion-specific temporal information and to fuse semantic features into the localization stream based on region activations. Furthermore, we propose a self-supervised warping technique that uses the relative motion to warp intermediate network representations in the segmentation stream for learning consistent semantics. Finally, we introduce a first-of-a-kind urban outdoor localization dataset with pixel-level semantic labels and multiple loops for training deep networks. Extensive experiments on the challenging Microsoft 7-Scenes benchmark and our DeepLoc dataset demonstrate that our approach exceeds the state-of-the-art outperforming local feature-based methods while simultaneously performing multiple tasks and exhibiting substantial robustness in challenging scenarios.



### Convolutional capsule network for classification of breast cancer histology images
- **Arxiv ID**: http://arxiv.org/abs/1804.08376v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1804.08376v1)
- **Published**: 2018-04-23 12:48:49+00:00
- **Updated**: 2018-04-23 12:48:49+00:00
- **Authors**: Tomas Iesmantas, Robertas Alzbutas
- **Comment**: Submitted to ICIAR 2018
- **Journal**: None
- **Summary**: Automatization of the diagnosis of any kind of disease is of great importance and it's gaining speed as more and more deep learning solutions are applied to different problems. One of such computer aided systems could be a decision support too able to accurately differentiate between different types of breast cancer histological images - normal tissue or carcinoma. In this paper authors present a deep learning solution, based on convolutional capsule network for classification of four types of images of breast tissue biopsy when hematoxylin and eusin staining is applied. The cross-validation accuracy was achieved to be 0.87 with equaly high sensitivity.



### BrainSlug: Transparent Acceleration of Deep Learning Through Depth-First Parallelism
- **Arxiv ID**: http://arxiv.org/abs/1804.08378v1
- **DOI**: None
- **Categories**: **cs.DC**, cs.AI, cs.CV, cs.NE, cs.PF
- **Links**: [PDF](http://arxiv.org/pdf/1804.08378v1)
- **Published**: 2018-04-23 12:49:04+00:00
- **Updated**: 2018-04-23 12:49:04+00:00
- **Authors**: Nicolas Weber, Florian Schmidt, Mathias Niepert, Felipe Huici
- **Comment**: Technical Report, 13 pages
- **Journal**: None
- **Summary**: Neural network frameworks such as PyTorch and TensorFlow are the workhorses of numerous machine learning applications ranging from object recognition to machine translation. While these frameworks are versatile and straightforward to use, the training of and inference in deep neural networks is resource (energy, compute, and memory) intensive. In contrast to recent works focusing on algorithmic enhancements, we introduce BrainSlug, a framework that transparently accelerates neural network workloads by changing the default layer-by-layer processing to a depth-first approach, reducing the amount of data required by the computations and thus improving the performance of the available hardware caches. BrainSlug achieves performance improvements of up to 41.1% on CPUs and 35.7% on GPUs. These optimizations come at zero cost to the user as they do not require hardware changes and only need tiny adjustments to the software.



### STAN: Spatio-Temporal Adversarial Networks for Abnormal Event Detection
- **Arxiv ID**: http://arxiv.org/abs/1804.08381v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.08381v1)
- **Published**: 2018-04-23 12:50:26+00:00
- **Updated**: 2018-04-23 12:50:26+00:00
- **Authors**: Sangmin Lee, Hak Gu Kim, Yong Man Ro
- **Comment**: ICASSP 2018
- **Journal**: None
- **Summary**: In this paper, we propose a novel abnormal event detection method with spatio-temporal adversarial networks (STAN). We devise a spatio-temporal generator which synthesizes an inter-frame by considering spatio-temporal characteristics with bidirectional ConvLSTM. A proposed spatio-temporal discriminator determines whether an input sequence is real-normal or not with 3D convolutional layers. These two networks are trained in an adversarial way to effectively encode spatio-temporal features of normal patterns. After the learning, the generator and the discriminator can be independently used as detectors, and deviations from the learned normal patterns are detected as abnormalities. Experimental results show that the proposed method achieved competitive performance compared to the state-of-the-art methods. Further, for the interpretation, we visualize the location of abnormal events detected by the proposed networks using a generator loss and discriminator gradients.



### Abdominal multi-organ segmentation with organ-attention networks and statistical fusion
- **Arxiv ID**: http://arxiv.org/abs/1804.08414v1
- **DOI**: 10.1016/j.media.2019.04.005
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.08414v1)
- **Published**: 2018-04-23 13:38:34+00:00
- **Updated**: 2018-04-23 13:38:34+00:00
- **Authors**: Yan Wang, Yuyin Zhou, Wei Shen, Seyoun Park, Elliot K. Fishman, Alan L. Yuille
- **Comment**: 21 pages, 11 figures
- **Journal**: Medical Image Analysis, 2019
- **Summary**: Accurate and robust segmentation of abdominal organs on CT is essential for many clinical applications such as computer-aided diagnosis and computer-aided surgery. But this task is challenging due to the weak boundaries of organs, the complexity of the background, and the variable sizes of different organs. To address these challenges, we introduce a novel framework for multi-organ segmentation by using organ-attention networks with reverse connections (OAN-RCs) which are applied to 2D views, of the 3D CT volume, and output estimates which are combined by statistical fusion exploiting structural similarity. OAN is a two-stage deep convolutional network, where deep network features from the first stage are combined with the original image, in a second stage, to reduce the complex background and enhance the discriminative information for the target organs. RCs are added to the first stage to give the lower layers semantic information thereby enabling them to adapt to the sizes of different organs. Our networks are trained on 2D views enabling us to use holistic information and allowing efficient computation. To compensate for the limited cross-sectional information of the original 3D volumetric CT, multi-sectional images are reconstructed from the three different 2D view directions. Then we combine the segmentation results from the different views using statistical fusion, with a novel term relating the structural similarity of the 2D views to the original 3D structure. To train the network and evaluate results, 13 structures were manually annotated by four human raters and confirmed by a senior expert on 236 normal cases. We tested our algorithm and computed Dice-Sorensen similarity coefficients and surface distances for evaluating our estimates of the 13 structures. Our experiments show that the proposed approach outperforms 2D- and 3D-patch based state-of-the-art methods.



### Efficient Pose Tracking from Natural Features in Standard Web Browsers
- **Arxiv ID**: http://arxiv.org/abs/1804.08424v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, cs.NI
- **Links**: [PDF](http://arxiv.org/pdf/1804.08424v1)
- **Published**: 2018-04-23 13:46:01+00:00
- **Updated**: 2018-04-23 13:46:01+00:00
- **Authors**: Fabian Göttl, Philipp Gagel, Jens Grubert
- **Comment**: None
- **Journal**: None
- **Summary**: Computer Vision-based natural feature tracking is at the core of modern Augmented Reality applications. Still, Web-based Augmented Reality typically relies on location-based sensing (using GPS and orientation sensors) or marker-based approaches to solve the pose estimation problem.   We present an implementation and evaluation of an efficient natural feature tracking pipeline for standard Web browsers using HTML5 and WebAssembly. Our system can track image targets at real-time frame rates tablet PCs (up to 60 Hz) and smartphones (up to 25 Hz).



### Decorrelated Batch Normalization
- **Arxiv ID**: http://arxiv.org/abs/1804.08450v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1804.08450v1)
- **Published**: 2018-04-23 14:06:50+00:00
- **Updated**: 2018-04-23 14:06:50+00:00
- **Authors**: Lei Huang, Dawei Yang, Bo Lang, Jia Deng
- **Comment**: Accepted to CVPR 2018. Code available at
  https://github.com/umich-vl/DecorrelatedBN
- **Journal**: None
- **Summary**: Batch Normalization (BN) is capable of accelerating the training of deep models by centering and scaling activations within mini-batches. In this work, we propose Decorrelated Batch Normalization (DBN), which not just centers and scales activations but whitens them. We explore multiple whitening techniques, and find that PCA whitening causes a problem we call stochastic axis swapping, which is detrimental to learning. We show that ZCA whitening does not suffer from this problem, permitting successful learning. DBN retains the desirable qualities of BN and further improves BN's optimization efficiency and generalization ability. We design comprehensive experiments to show that DBN can improve the performance of BN on multilayer perceptrons and convolutional neural networks. Furthermore, we consistently improve the accuracy of residual networks on CIFAR-10, CIFAR-100, and ImageNet.



### Attention Based Natural Language Grounding by Navigating Virtual Environment
- **Arxiv ID**: http://arxiv.org/abs/1804.08454v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1804.08454v2)
- **Published**: 2018-04-23 14:11:17+00:00
- **Updated**: 2018-12-21 19:00:54+00:00
- **Authors**: Akilesh B, Abhishek Sinha, Mausoom Sarkar, Balaji Krishnamurthy
- **Comment**: Accepted at WACV 2019. Also at NeurIPS 2017 workshop on
  Visually-Grounded Interaction and Language (ViGIL)
- **Journal**: None
- **Summary**: In this work, we focus on the problem of grounding language by training an agent to follow a set of natural language instructions and navigate to a target object in an environment. The agent receives visual information through raw pixels and a natural language instruction telling what task needs to be achieved and is trained in an end-to-end way. We develop an attention mechanism for multi-modal fusion of visual and textual modalities that allows the agent to learn to complete the task and achieve language grounding. Our experimental results show that our attention mechanism outperforms the existing multi-modal fusion mechanisms proposed for both 2D and 3D environments in order to solve the above-mentioned task in terms of both speed and success rate. We show that the learnt textual representations are semantically meaningful as they follow vector arithmetic in the embedding space. The effectiveness of our attention approach over the contemporary fusion mechanisms is also highlighted from the textual embeddings learnt by the different approaches. We also show that our model generalizes effectively to unseen scenarios and exhibit zero-shot generalization capabilities both in 2D and 3D environments. The code for our 2D environment as well as the models that we developed for both 2D and 3D are available at https://github.com/rl-lang-grounding/rl-lang-ground.



### Joint Enhancement and Denoising Method via Sequential Decomposition
- **Arxiv ID**: http://arxiv.org/abs/1804.08468v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.08468v3)
- **Published**: 2018-04-23 14:31:21+00:00
- **Updated**: 2018-04-28 13:55:01+00:00
- **Authors**: Xutong Ren, Mading Li, Wen-Huang Cheng, Jiaying Liu
- **Comment**: Accepted by ISCAS 2018
- **Journal**: None
- **Summary**: Many low-light enhancement methods ignore intensive noise in original images. As a result, they often simultaneously enhance the noise as well. Furthermore, extra denoising procedures adopted by most methods ruin the details. In this paper, we introduce a joint low-light enhancement and denoising strategy, aimed at obtaining well-enhanced low-light images while getting rid of the inherent noise issue simultaneously. The proposed method performs Retinex model based decomposition in a successive sequence, which sequentially estimates a piece-wise smoothed illumination and a noise-suppressed reflectance. After getting the illumination and reflectance map, we adjust the illumination layer and generate our enhancement result. In this noise-suppressed sequential decomposition process we enforce the spatial smoothness on each component and skillfully make use of weight matrices to suppress the noise and improve the contrast. Results of extensive experiments demonstrate the effectiveness and practicability of our method. It performs well for a wide variety of images, and achieves better or comparable quality compared with the state-of-the-art methods.



### Beyond Narrative Description: Generating Poetry from Images by Multi-Adversarial Training
- **Arxiv ID**: http://arxiv.org/abs/1804.08473v4
- **DOI**: 10.1145/3240508.3240587
- **Categories**: **cs.CV**, cs.AI, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1804.08473v4)
- **Published**: 2018-04-23 14:35:59+00:00
- **Updated**: 2018-10-10 03:23:38+00:00
- **Authors**: Bei Liu, Jianlong Fu, Makoto P. Kato, Masatoshi Yoshikawa
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic generation of natural language from images has attracted extensive attention. In this paper, we take one step further to investigate generation of poetic language (with multiple lines) to an image for automatic poetry creation. This task involves multiple challenges, including discovering poetic clues from the image (e.g., hope from green), and generating poems to satisfy both relevance to the image and poeticness in language level. To solve the above challenges, we formulate the task of poem generation into two correlated sub-tasks by multi-adversarial training via policy gradient, through which the cross-modal relevance and poetic language style can be ensured. To extract poetic clues from images, we propose to learn a deep coupled visual-poetic embedding, in which the poetic representation from objects, sentiments and scenes in an image can be jointly learned. Two discriminative networks are further introduced to guide the poem generation, including a multi-modal discriminator and a poem-style discriminator. To facilitate the research, we have released two poem datasets by human annotators with two distinct properties: 1) the first human annotated image-to-poem pair dataset (with 8,292 pairs in total), and 2) to-date the largest public English poem corpus dataset (with 92,265 different poems in total). Extensive experiments are conducted with 8K images, among which 1.5K image are randomly picked for evaluation. Both objective and subjective evaluations show the superior performances against the state-of-the-art methods for poem generation from images. Turing test carried out with over 500 human subjects, among which 30 evaluators are poetry experts, demonstrates the effectiveness of our approach.



### Object Counts! Bringing Explicit Detections Back into Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/1805.00314v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1805.00314v1)
- **Published**: 2018-04-23 14:51:46+00:00
- **Updated**: 2018-04-23 14:51:46+00:00
- **Authors**: Josiah Wang, Pranava Madhyastha, Lucia Specia
- **Comment**: Please cite: In Proceedings of 2018 Conference of the North American
  Chapter of the Association for Computational Linguistics (NAACL 2018)
- **Journal**: None
- **Summary**: The use of explicit object detectors as an intermediate step to image captioning - which used to constitute an essential stage in early work - is often bypassed in the currently dominant end-to-end approaches, where the language model is conditioned directly on a mid-level image embedding. We argue that explicit detections provide rich semantic information, and can thus be used as an interpretable representation to better understand why end-to-end image captioning systems work well. We provide an in-depth analysis of end-to-end image captioning by exploring a variety of cues that can be derived from such object detections. Our study reveals that end-to-end image captioning systems rely on matching image representations to generate captions, and that encoding the frequency, size and position of objects are complementary and all play a role in forming a good image representation. It also reveals that different object categories contribute in different ways towards image captioning.



### ALIGNet: Partial-Shape Agnostic Alignment via Unsupervised Learning
- **Arxiv ID**: http://arxiv.org/abs/1804.08497v2
- **DOI**: None
- **Categories**: **cs.GR**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1804.08497v2)
- **Published**: 2018-04-23 15:17:26+00:00
- **Updated**: 2018-10-30 22:26:39+00:00
- **Authors**: Rana Hanocka, Noa Fish, Zhenhua Wang, Raja Giryes, Shachar Fleishman, Daniel Cohen-Or
- **Comment**: To be presented at SIGGRAPH Asia 2018
- **Journal**: None
- **Summary**: The process of aligning a pair of shapes is a fundamental operation in computer graphics. Traditional approaches rely heavily on matching corresponding points or features to guide the alignment, a paradigm that falters when significant shape portions are missing. These techniques generally do not incorporate prior knowledge about expected shape characteristics, which can help compensate for any misleading cues left by inaccuracies exhibited in the input shapes. We present an approach based on a deep neural network, leveraging shape datasets to learn a shape-aware prior for source-to-target alignment that is robust to shape incompleteness. In the absence of ground truth alignments for supervision, we train a network on the task of shape alignment using incomplete shapes generated from full shapes for self-supervision. Our network, called ALIGNet, is trained to warp complete source shapes to incomplete targets, as if the target shapes were complete, thus essentially rendering the alignment partial-shape agnostic. We aim for the network to develop specialized expertise over the common characteristics of the shapes in each dataset, thereby achieving a higher-level understanding of the expected shape space to which a local approach would be oblivious. We constrain ALIGNet through an anisotropic total variation identity regularization to promote piecewise smooth deformation fields, facilitating both partial-shape agnosticism and post-deformation applications. We demonstrate that ALIGNet learns to align geometrically distinct shapes, and is able to infer plausible mappings even when the target shape is significantly incomplete. We show that our network learns the common expected characteristics of shape collections, without over-fitting or memorization, enabling it to produce plausible deformations on unseen data during test time.



### Person Identification from Partial Gait Cycle Using Fully Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1804.08506v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.08506v1)
- **Published**: 2018-04-23 15:27:20+00:00
- **Updated**: 2018-04-23 15:27:20+00:00
- **Authors**: Maryam Babaee, Linwei Li, Gerhard Rigoll
- **Comment**: None
- **Journal**: None
- **Summary**: Gait as a biometric property for person identification plays a key role in video surveillance and security applications. In gait recognition, normally, gait feature such as Gait Energy Image (GEI) is extracted from one full gait cycle. However in many circumstances, such a full gait cycle might not be available due to occlusion. Thus, the GEI is not complete giving rise to a degrading in gait-based person identification rate. In this paper, we address this issue by proposing a novel method to identify individuals from gait feature when a few (or even single) frame(s) is available. To do so, we propose a deep learning-based approach to transform incomplete GEI to the corresponding complete GEI obtained from a full gait cycle. More precisely, this transformation is done gradually by training several auto encoders independently and then combining these as a uniform model. Experimental results on two public gait datasets, namely OULP and Casia-B demonstrate the validity of the proposed method in dealing with very incomplete gait cycles.



### A New Channel Boosted Convolutional Neural Network using Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/1804.08528v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.08528v5)
- **Published**: 2018-04-23 16:02:35+00:00
- **Updated**: 2020-07-04 19:56:44+00:00
- **Authors**: Asifullah Khan, Anabia Sohail, Amna Ali
- **Comment**: 24 Pages, 5 Figures, 1 Table
- **Journal**: None
- **Summary**: We present a novel architectural enhancement of Channel Boosting in a deep convolutional neural network (CNN). This idea of Channel Boosting exploits both the channel dimension of CNN (learning from multiple input channels) and Transfer learning (TL). TL is utilized at two different stages; channel generation and channel exploitation. In the proposed methodology, a deep CNN is boosted by various channels available through TL from already trained Deep Neural Networks, in addition to its original channel. The deep architecture of CNN then exploits the original and boosted channels down the stream for learning discriminative patterns. Churn prediction in telecom is a challenging task due to the high dimensionality and imbalanced nature of the data. Therefore, churn prediction data is used to evaluate the performance of the proposed Channel Boosted CNN (CB CNN). In the first phase, informative discriminative features are being extracted using a stacked autoencoder, and then in the second phase, these features are combined with the original features to form Channel Boosted images. Finally, the knowledge gained by a pretrained CNN is exploited by employing TL. The results are promising and show the ability of the Channel Boosting concept in learning complex classification problems by discerning even minute differences in churners and nonchurners. The proposed work validates the concept observed from the evolution of recent CNN architectures that the innovative restructuring of a CNN architecture may increase the networks representative capacity.



### VectorDefense: Vectorization as a Defense to Adversarial Examples
- **Arxiv ID**: http://arxiv.org/abs/1804.08529v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1804.08529v1)
- **Published**: 2018-04-23 16:04:55+00:00
- **Updated**: 2018-04-23 16:04:55+00:00
- **Authors**: Vishaal Munusamy Kabilan, Brandon Morris, Anh Nguyen
- **Comment**: 17 pages, 14 figures
- **Journal**: None
- **Summary**: Training deep neural networks on images represented as grids of pixels has brought to light an interesting phenomenon known as adversarial examples. Inspired by how humans reconstruct abstract concepts, we attempt to codify the input bitmap image into a set of compact, interpretable elements to avoid being fooled by the adversarial structures. We take the first step in this direction by experimenting with image vectorization as an input transformation step to map the adversarial examples back into the natural manifold of MNIST handwritten digits. We compare our method vs. state-of-the-art input transformations and further discuss the trade-offs between a hand-designed and a learned transformation defense.



### Light-weight Head Pose Invariant Gaze Tracking
- **Arxiv ID**: http://arxiv.org/abs/1804.08572v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.08572v1)
- **Published**: 2018-04-23 17:12:11+00:00
- **Updated**: 2018-04-23 17:12:11+00:00
- **Authors**: Rajeev Ranjan, Shalini De Mello, Jan Kautz
- **Comment**: 9 pages, IEEE Conference on Computer Vision and Pattern Recognition
  Workshop
- **Journal**: None
- **Summary**: Unconstrained remote gaze tracking using off-the-shelf cameras is a challenging problem. Recently, promising algorithms for appearance-based gaze estimation using convolutional neural networks (CNN) have been proposed. Improving their robustness to various confounding factors including variable head pose, subject identity, illumination and image quality remain open problems. In this work, we study the effect of variable head pose on machine learning regressors trained to estimate gaze direction. We propose a novel branched CNN architecture that improves the robustness of gaze classifiers to variable head pose, without increasing computational cost. We also present various procedures to effectively train our gaze network including transfer learning from the more closely related task of object viewpoint estimation and from a large high-fidelity synthetic gaze dataset, which enable our ten times faster gaze network to achieve competitive accuracy to its current state-of-the-art direct competitor.



### Large Scale Scene Text Verification with Guided Attention
- **Arxiv ID**: http://arxiv.org/abs/1804.08588v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.08588v2)
- **Published**: 2018-04-23 17:30:49+00:00
- **Updated**: 2018-11-19 01:01:52+00:00
- **Authors**: Dafang He, Yeqing Li, Alexander Gorban, Derrall Heath, Julian Ibarz, Qian Yu, Daniel Kifer, C. Lee Giles
- **Comment**: 18 pages, ACCV 2019
- **Journal**: None
- **Summary**: Many tasks are related to determining if a particular text string exists in an image. In this work, we propose a new framework that learns this task in an end-to-end way. The framework takes an image and a text string as input and then outputs the probability of the text string being present in the image. This is the first end-to-end framework that learns such relationships between text and images in scene text area. The framework does not require explicit scene text detection or recognition and thus no bounding box annotations are needed for it. It is also the first work in scene text area that tackles suh a weakly labeled problem. Based on this framework, we developed a model called Guided Attention. Our designed model achieves much better results than several state-of-the-art scene text reading based solutions for a challenging Street View Business Matching task. The task tries to find correct business names for storefront images and the dataset we collected for it is substantially larger, and more challenging than existing scene text dataset. This new real-world task provides a new perspective for studying scene text related problems. We also demonstrate the uniqueness of our task via a comparison between our problem and a typical Visual Question Answering problem.



### Black-box Adversarial Attacks with Limited Queries and Information
- **Arxiv ID**: http://arxiv.org/abs/1804.08598v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1804.08598v3)
- **Published**: 2018-04-23 17:46:34+00:00
- **Updated**: 2018-07-11 13:51:00+00:00
- **Authors**: Andrew Ilyas, Logan Engstrom, Anish Athalye, Jessy Lin
- **Comment**: ICML 2018. This supercedes the previous paper "Query-efficient
  Black-box adversarial examples."
- **Journal**: None
- **Summary**: Current neural network-based classifiers are susceptible to adversarial examples even in the black-box setting, where the attacker only has query access to the model. In practice, the threat model for real-world systems is often more restrictive than the typical black-box model where the adversary can observe the full output of the network on arbitrarily many chosen inputs. We define three realistic threat models that more accurately characterize many real-world classifiers: the query-limited setting, the partial-information setting, and the label-only setting. We develop new attacks that fool classifiers under these more restrictive threat models, where previous methods would be impractical or ineffective. We demonstrate that our methods are effective against an ImageNet classifier under our proposed threat models. We also demonstrate a targeted black-box attack against a commercial classifier, overcoming the challenges of limited query access, partial information, and other practical issues to break the Google Cloud Vision API.



### Zero-Shot Visual Imitation
- **Arxiv ID**: http://arxiv.org/abs/1804.08606v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1804.08606v1)
- **Published**: 2018-04-23 17:58:26+00:00
- **Updated**: 2018-04-23 17:58:26+00:00
- **Authors**: Deepak Pathak, Parsa Mahmoudieh, Guanghao Luo, Pulkit Agrawal, Dian Chen, Yide Shentu, Evan Shelhamer, Jitendra Malik, Alexei A. Efros, Trevor Darrell
- **Comment**: Oral presentation at ICLR 2018. Website at
  https://pathak22.github.io/zeroshot-imitation/
- **Journal**: None
- **Summary**: The current dominant paradigm for imitation learning relies on strong supervision of expert actions to learn both 'what' and 'how' to imitate. We pursue an alternative paradigm wherein an agent first explores the world without any expert supervision and then distills its experience into a goal-conditioned skill policy with a novel forward consistency loss. In our framework, the role of the expert is only to communicate the goals (i.e., what to imitate) during inference. The learned policy is then employed to mimic the expert (i.e., how to imitate) after seeing just a sequence of images demonstrating the desired task. Our method is 'zero-shot' in the sense that the agent never has access to expert actions during training or for the task demonstration at inference. We evaluate our zero-shot imitator in two real-world settings: complex rope manipulation with a Baxter robot and navigation in previously unseen office environments with a TurtleBot. Through further experiments in VizDoom simulation, we provide evidence that better mechanisms for exploration lead to learning a more capable policy which in turn improves end task performance. Videos, models, and more details are available at https://pathak22.github.io/zeroshot-imitation/



### Rendition: Reclaiming what a black box takes away
- **Arxiv ID**: http://arxiv.org/abs/1804.08651v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.08651v1)
- **Published**: 2018-04-23 18:23:00+00:00
- **Updated**: 2018-04-23 18:23:00+00:00
- **Authors**: Peyman Milanfar
- **Comment**: None
- **Journal**: None
- **Summary**: The premise of our work is deceptively familiar: A black box $f(\cdot)$ has altered an image $\mathbf{x} \rightarrow f(\mathbf{x})$. Recover the image $\mathbf{x}$. This black box might be any number of simple or complicated things: a linear or non-linear filter, some app on your phone, etc. The latter is a good canonical example for the problem we address: Given only "the app" and an image produced by the app, find the image that was fed to the app. You can run the given image (or any other image) through the app as many times as you like, but you can not look inside the (code for the) app to see how it works. At first blush, the problem sounds a lot like a standard inverse problem, but it is not in the following sense: While we have access to the black box $f(\cdot)$ and can run any image through it and observe the output, we do not know how the block box alters the image. Therefore we have no explicit form or model of $f(\cdot)$. Nor are we necessarily interested in the internal workings of the black box. We are simply happy to reverse its effect on a particular image, to whatever extent possible. This is what we call the "rendition" (rather than restoration) problem, as it does not fit the mold of an inverse problem (blind or otherwise). We describe general conditions under which rendition is possible, and provide a remarkably simple algorithm that works for both contractive and expansive black box operators. The principal and novel take-away message from our work is this surprising fact: One simple algorithm can reliably undo a wide class of (not too violent) image distortions.   A higher quality pdf of this paper is available at http://www.milanfar.org



### Fingerprint Match in Box
- **Arxiv ID**: http://arxiv.org/abs/1804.08659v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.08659v1)
- **Published**: 2018-04-23 18:38:39+00:00
- **Updated**: 2018-04-23 18:38:39+00:00
- **Authors**: Joshua J. Engelsma, Kai Cao, Anil K. Jain
- **Comment**: None
- **Journal**: None
- **Summary**: We open source fingerprint Match in Box, a complete end-to-end fingerprint recognition system embedded within a 4 inch cube. Match in Box stands in contrast to a typical bulky and expensive proprietary fingerprint recognition system which requires sending a fingerprint image to an external host for processing and subsequent spoof detection and matching. In particular, Match in Box is a first of a kind, portable, low-cost, and easy-to-assemble fingerprint reader with an enrollment database embedded within the reader's memory and open source fingerprint spoof detector, feature extractor, and matcher all running on the reader's internal vision processing unit (VPU). An onboard touch screen and rechargeable battery pack make this device extremely portable and ideal for applying both fingerprint authentication (1:1 comparison) and fingerprint identification (1:N search) to applications (vaccination tracking, food and benefit distribution programs, human trafficking prevention) in rural communities, especially in developing countries. We also show that Match in Box is suited for capturing neonate fingerprints due to its high resolution (1900 ppi) cameras.



### DeepDIVA: A Highly-Functional Python Framework for Reproducible Experiments
- **Arxiv ID**: http://arxiv.org/abs/1805.00329v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.00329v1)
- **Published**: 2018-04-23 20:00:42+00:00
- **Updated**: 2018-04-23 20:00:42+00:00
- **Authors**: Michele Alberti, Vinaychandran Pondenkandath, Marcel Würsch, Rolf Ingold, Marcus Liwicki
- **Comment**: Submitted at the 16th International Conference on Frontiers in
  Handwriting Recognition (ICFHR), 6 pages, 6 Figures
- **Journal**: None
- **Summary**: We introduce DeepDIVA: an infrastructure designed to enable quick and intuitive setup of reproducible experiments with a large range of useful analysis functionality. Reproducing scientific results can be a frustrating experience, not only in document image analysis but in machine learning in general. Using DeepDIVA a researcher can either reproduce a given experiment with a very limited amount of information or share their own experiments with others. Moreover, the framework offers a large range of functions, such as boilerplate code, keeping track of experiments, hyper-parameter optimization, and visualization of data and results. To demonstrate the effectiveness of this framework, this paper presents case studies in the area of handwritten document analysis where researchers benefit from the integrated functionality. DeepDIVA is implemented in Python and uses the deep learning framework PyTorch. It is completely open source, and accessible as Web Service through DIVAServices.



### Discovering Style Trends through Deep Visually Aware Latent Item Embeddings
- **Arxiv ID**: http://arxiv.org/abs/1804.08704v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.08704v1)
- **Published**: 2018-04-23 20:07:28+00:00
- **Updated**: 2018-04-23 20:07:28+00:00
- **Authors**: Murium Iqbal, Adair Kovac, Kamelia Aryafar
- **Comment**: CVPR Workshops Accepted Paper
- **Journal**: None
- **Summary**: In this paper, we explore Latent Dirichlet Allocation (LDA) and Polylingual Latent Dirichlet Allocation (PolyLDA), as a means to discover trending styles in Overstock from deep visual semantic features transferred from a pretrained convolutional neural network and text-based item attributes. To utilize deep visual semantic features in conjunction with LDA, we develop a method for creating a bag of words representation of unrolled image vectors. By viewing the channels within the convolutional layers of a Resnet-50 as being representative of a word, we can index these activations to create visual documents. We then train LDA over these documents to discover the latent style in the images. We also incorporate text-based data with PolyLDA, where each representation is viewed as an independent language attempting to describe the same style. The resulting topics are shown to be excellent indicators of visual style across our platform.



### Siamese Generative Adversarial Privatizer for Biometric Data
- **Arxiv ID**: http://arxiv.org/abs/1804.08757v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.08757v3)
- **Published**: 2018-04-23 21:57:28+00:00
- **Updated**: 2018-10-08 07:23:51+00:00
- **Authors**: Witold Oleszkiewicz, Peter Kairouz, Karol Piczak, Ram Rajagopal, Tomasz Trzcinski
- **Comment**: Paper accepted to ACCV 2018 (Asian Conference on Computer Vision)
- **Journal**: None
- **Summary**: State-of-the-art machine learning algorithms can be fooled by carefully crafted adversarial examples. As such, adversarial examples present a concrete problem in AI safety. In this work we turn the tables and ask the following question: can we harness the power of adversarial examples to prevent malicious adversaries from learning identifying information from data while allowing non-malicious entities to benefit from the utility of the same data? For instance, can we use adversarial examples to anonymize biometric dataset of faces while retaining usefulness of this data for other purposes, such as emotion recognition? To address this question, we propose a simple yet effective method, called Siamese Generative Adversarial Privatizer (SGAP), that exploits the properties of a Siamese neural network to find discriminative features that convey identifying information. When coupled with a generative model, our approach is able to correctly locate and disguise identifying information, while minimally reducing the utility of the privatized dataset. Extensive evaluation on a biometric dataset of fingerprints and cartoon faces confirms usefulness of our simple yet effective method.



### Switchable Temporal Propagation Network
- **Arxiv ID**: http://arxiv.org/abs/1804.08758v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.08758v2)
- **Published**: 2018-04-23 22:03:41+00:00
- **Updated**: 2018-05-04 17:55:41+00:00
- **Authors**: Sifei Liu, Guangyu Zhong, Shalini De Mello, Jinwei Gu, Varun Jampani, Ming-Hsuan Yang, Jan Kautz
- **Comment**: None
- **Journal**: None
- **Summary**: Videos contain highly redundant information between frames. Such redundancy has been extensively studied in video compression and encoding, but is less explored for more advanced video processing. In this paper, we propose a learnable unified framework for propagating a variety of visual properties of video images, including but not limited to color, high dynamic range (HDR), and segmentation information, where the properties are available for only a few key-frames. Our approach is based on a temporal propagation network (TPN), which models the transition-related affinity between a pair of frames in a purely data-driven manner. We theoretically prove two essential factors for TPN: (a) by regularizing the global transformation matrix as orthogonal, the "style energy" of the property can be well preserved during propagation; (b) such regularization can be achieved by the proposed switchable TPN with bi-directional training on pairs of frames. We apply the switchable TPN to three tasks: colorizing a gray-scale video based on a few color key-frames, generating an HDR video from a low dynamic range (LDR) video and a few HDR frames, and propagating a segmentation mask from the first frame in videos. Experimental results show that our approach is significantly more accurate and efficient than the state-of-the-art methods.



