# Arxiv Papers in cs.CV on 2018-04-17
### Neural Compatibility Modeling with Attentive Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/1805.00313v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1805.00313v1)
- **Published**: 2018-04-17 01:26:48+00:00
- **Updated**: 2018-04-17 01:26:48+00:00
- **Authors**: Xuemeng Song, Fuli Feng, Xianjing Han, Xin Yang, Wei Liu, Liqiang Nie
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, the booming fashion sector and its huge potential benefits have attracted tremendous attention from many research communities. In particular, increasing research efforts have been dedicated to the complementary clothing matching as matching clothes to make a suitable outfit has become a daily headache for many people, especially those who do not have the sense of aesthetics. Thanks to the remarkable success of neural networks in various applications such as image classification and speech recognition, the researchers are enabled to adopt the data-driven learning methods to analyze fashion items. Nevertheless, existing studies overlook the rich valuable knowledge (rules) accumulated in fashion domain, especially the rules regarding clothing matching. Towards this end, in this work, we shed light on complementary clothing matching by integrating the advanced deep neural networks and the rich fashion domain knowledge. Considering that the rules can be fuzzy and different rules may have different confidence levels to different samples, we present a neural compatibility modeling scheme with attentive knowledge distillation based on the teacher-student network scheme. Extensive experiments on the real-world dataset show the superiority of our model over several state-of-the-art baselines. Based upon the comparisons, we observe certain fashion insights that add value to the fashion matching study. As a byproduct, we released the codes, and involved parameters to benefit other researchers.



### Geometry-aware Deep Network for Single-Image Novel View Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1804.06008v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.06008v1)
- **Published**: 2018-04-17 01:31:25+00:00
- **Updated**: 2018-04-17 01:31:25+00:00
- **Authors**: Miaomiao Liu, Xuming He, Mathieu Salzmann
- **Comment**: CVPR 2018
- **Journal**: None
- **Summary**: This paper tackles the problem of novel view synthesis from a single image. In particular, we target real-world scenes with rich geometric structure, a challenging task due to the large appearance variations of such scenes and the lack of simple 3D models to represent them. Modern, learning-based approaches mostly focus on appearance to synthesize novel views and thus tend to generate predictions that are inconsistent with the underlying scene structure. By contrast, in this paper, we propose to exploit the 3D geometry of the scene to synthesize a novel view. Specifically, we approximate a real-world scene by a fixed number of planes, and learn to predict a set of homographies and their corresponding region masks to transform the input image into a novel view. To this end, we develop a new region-aware geometric transform network that performs these multiple tasks in a common framework. Our results on the outdoor KITTI and the indoor ScanNet datasets demonstrate the effectiveness of our network in generating high quality synthetic views that respect the scene geometry, thus outperforming the state-of-the-art methods.



### DoubleFusion: Real-time Capture of Human Performances with Inner Body Shapes from a Single Depth Sensor
- **Arxiv ID**: http://arxiv.org/abs/1804.06023v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.06023v1)
- **Published**: 2018-04-17 03:09:01+00:00
- **Updated**: 2018-04-17 03:09:01+00:00
- **Authors**: Tao Yu, Zerong Zheng, Kaiwen Guo, Jianhui Zhao, Qionghai Dai, Hao Li, Gerard Pons-Moll, Yebin Liu
- **Comment**: IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
  2018
- **Journal**: None
- **Summary**: We propose DoubleFusion, a new real-time system that combines volumetric dynamic reconstruction with data-driven template fitting to simultaneously reconstruct detailed geometry, non-rigid motion and the inner human body shape from a single depth camera. One of the key contributions of this method is a double layer representation consisting of a complete parametric body shape inside, and a gradually fused outer surface layer. A pre-defined node graph on the body surface parameterizes the non-rigid deformations near the body, and a free-form dynamically changing graph parameterizes the outer surface layer far from the body, which allows more general reconstruction. We further propose a joint motion tracking method based on the double layer representation to enable robust and fast motion tracking performance. Moreover, the inner body shape is optimized online and forced to fit inside the outer surface layer. Overall, our method enables increasingly denoised, detailed and complete surface reconstructions, fast motion tracking performance and plausible inner body shape reconstruction in real-time. In particular, experiments show improved fast motion tracking and loop closure performance on more challenging scenarios.



### Learning to Color from Language
- **Arxiv ID**: http://arxiv.org/abs/1804.06026v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1804.06026v1)
- **Published**: 2018-04-17 03:22:00+00:00
- **Updated**: 2018-04-17 03:22:00+00:00
- **Authors**: Varun Manjunatha, Mohit Iyyer, Jordan Boyd-Graber, Larry Davis
- **Comment**: 6 pages
- **Journal**: North American Chapter of the Association for Computational
  Linguistics (NAACL), 2018
- **Summary**: Automatic colorization is the process of adding color to greyscale images. We condition this process on language, allowing end users to manipulate a colorized image by feeding in different captions. We present two different architectures for language-conditioned colorization, both of which produce more accurate and plausible colorizations than a language-agnostic version. Through this language-based framework, we can dramatically alter colorizations by manipulating descriptive color words in captions.



### Pixels, voxels, and views: A study of shape representations for single view 3D object shape prediction
- **Arxiv ID**: http://arxiv.org/abs/1804.06032v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.06032v2)
- **Published**: 2018-04-17 03:32:00+00:00
- **Updated**: 2018-06-12 03:12:21+00:00
- **Authors**: Daeyun Shin, Charless C. Fowlkes, Derek Hoiem
- **Comment**: CVPR 2018
- **Journal**: None
- **Summary**: The goal of this paper is to compare surface-based and volumetric 3D object shape representations, as well as viewer-centered and object-centered reference frames for single-view 3D shape prediction. We propose a new algorithm for predicting depth maps from multiple viewpoints, with a single depth or RGB image as input. By modifying the network and the way models are evaluated, we can directly compare the merits of voxels vs. surfaces and viewer-centered vs. object-centered for familiar vs. unfamiliar objects, as predicted from RGB or depth images. Among our findings, we show that surface-based methods outperform voxel representations for objects from novel classes and produce higher resolution outputs. We also find that using viewer-centered coordinates is advantageous for novel objects, while object-centered representations are better for more familiar objects. Interestingly, the coordinate frame significantly affects the shape representation learned, with object-centered placing more importance on implicitly recognizing the object category and viewer-centered producing shape representations with less dependence on category recognition.



### Real-Time Rotation-Invariant Face Detection with Progressive Calibration Networks
- **Arxiv ID**: http://arxiv.org/abs/1804.06039v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.06039v3)
- **Published**: 2018-04-17 04:27:14+00:00
- **Updated**: 2022-03-18 22:42:29+00:00
- **Authors**: Xuepeng Shi, Shiguang Shan, Meina Kan, Shuzhe Wu, Xilin Chen
- **Comment**: Accepted to CVPR 2018. Code: https://github.com/Rock-100/FaceKit
- **Journal**: None
- **Summary**: Rotation-invariant face detection, i.e. detecting faces with arbitrary rotation-in-plane (RIP) angles, is widely required in unconstrained applications but still remains as a challenging task, due to the large variations of face appearances. Most existing methods compromise with speed or accuracy to handle the large RIP variations. To address this problem more efficiently, we propose Progressive Calibration Networks (PCN) to perform rotation-invariant face detection in a coarse-to-fine manner. PCN consists of three stages, each of which not only distinguishes the faces from non-faces, but also calibrates the RIP orientation of each face candidate to upright progressively. By dividing the calibration process into several progressive steps and only predicting coarse orientations in early stages, PCN can achieve precise and fast calibration. By performing binary classification of face vs. non-face with gradually decreasing RIP ranges, PCN can accurately detect faces with full $360^{\circ}$ RIP angles. Such designs lead to a real-time rotation-invariant face detector. The experiments on multi-oriented FDDB and a challenging subset of WIDER FACE containing rotated faces in the wild show that our PCN achieves quite promising performance.



### Iterative Residual Image Deconvolution
- **Arxiv ID**: http://arxiv.org/abs/1804.06042v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.06042v2)
- **Published**: 2018-04-17 05:07:28+00:00
- **Updated**: 2018-11-04 16:36:01+00:00
- **Authors**: Li Si-Yao, Dongwei Ren, Furong Zhao, Zijian Hu, Junfeng Li, Qian Yin
- **Comment**: rejected by AAAI 2019
- **Journal**: None
- **Summary**: Image deblurring, a.k.a. image deconvolution, recovers a clear image from pixel superposition caused by blur degradation. Few deep convolutional neural networks (CNN) succeed in addressing this task. In this paper, we first demonstrate that the minimum-mean-square-error (MMSE) solution to image deblurring can be interestingly unfolded into a series of residual components. Based on this analysis, we propose a novel iterative residual deconvolution (IRD) algorithm. Further, IRD motivates us to take one step forward to design an explicable and effective CNN architecture for image deconvolution. Specifically, a sequence of residual CNN units are deployed, whose intermediate outputs are then concatenated and integrated, resulting in concatenated residual convolutional network (CRCNet). The experimental results demonstrate that proposed CRCNet not only achieves better quantitative metrics but also recovers more visually plausible texture details compared with state-of-the-art methods.



### Co-occurrence Feature Learning from Skeleton Data for Action Recognition and Detection with Hierarchical Aggregation
- **Arxiv ID**: http://arxiv.org/abs/1804.06055v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.06055v1)
- **Published**: 2018-04-17 06:00:13+00:00
- **Updated**: 2018-04-17 06:00:13+00:00
- **Authors**: Chao Li, Qiaoyong Zhong, Di Xie, Shiliang Pu
- **Comment**: IJCAI18 oral
- **Journal**: None
- **Summary**: Skeleton-based human action recognition has recently drawn increasing attentions with the availability of large-scale skeleton datasets. The most crucial factors for this task lie in two aspects: the intra-frame representation for joint co-occurrences and the inter-frame representation for skeletons' temporal evolutions. In this paper we propose an end-to-end convolutional co-occurrence feature learning framework. The co-occurrence features are learned with a hierarchical methodology, in which different levels of contextual information are aggregated gradually. Firstly point-level information of each joint is encoded independently. Then they are assembled into semantic representation in both spatial and temporal domains. Specifically, we introduce a global spatial aggregation scheme, which is able to learn superior joint co-occurrence features over local aggregation. Besides, raw skeleton coordinates as well as their temporal difference are integrated with a two-stream paradigm. Experiments show that our approach consistently outperforms other state-of-the-arts on action recognition and detection benchmarks like NTU RGB+D, SBU Kinect Interaction and PKU-MMD.



### Improving Deep Binary Embedding Networks by Order-aware Reweighting of Triplets
- **Arxiv ID**: http://arxiv.org/abs/1804.06061v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.06061v1)
- **Published**: 2018-04-17 06:29:40+00:00
- **Updated**: 2018-04-17 06:29:40+00:00
- **Authors**: Jikai Chen, Hanjiang Lai, Libing Geng, Yan Pan
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we focus on triplet-based deep binary embedding networks for image retrieval task. The triplet loss has been shown to be most effective for the ranking problem. However, most of the previous works treat the triplets equally or select the hard triplets based on the loss. Such strategies do not consider the order relations, which is important for retrieval task. To this end, we propose an order-aware reweighting method to effectively train the triplet-based deep networks, which up-weights the important triplets and down-weights the uninformative triplets. First, we present the order-aware weighting factors to indicate the importance of the triplets, which depend on the rank order of binary codes. Then, we reshape the triplet loss to the squared triplet loss such that the loss function will put more weights on the important triplets. Extensive evaluations on four benchmark datasets show that the proposed method achieves significant performance compared with the state-of-the-art baselines.



### Cross-Domain Adversarial Auto-Encoder
- **Arxiv ID**: http://arxiv.org/abs/1804.06078v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.IT, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/1804.06078v1)
- **Published**: 2018-04-17 07:12:58+00:00
- **Updated**: 2018-04-17 07:12:58+00:00
- **Authors**: Haodi Hou, Jing Huo, Yang Gao
- **Comment**: Under review as a conference paper of KDD 2018
- **Journal**: None
- **Summary**: In this paper, we propose the Cross-Domain Adversarial Auto-Encoder (CDAAE) to address the problem of cross-domain image inference, generation and transformation. We make the assumption that images from different domains share the same latent code space for content, while having separate latent code space for style. The proposed framework can map cross-domain data to a latent code vector consisting of a content part and a style part. The latent code vector is matched with a prior distribution so that we can generate meaningful samples from any part of the prior space. Consequently, given a sample of one domain, our framework can generate various samples of the other domain with the same content of the input. This makes the proposed framework different from the current work of cross-domain transformation. Besides, the proposed framework can be trained with both labeled and unlabeled data, which makes it also suitable for domain adaptation. Experimental results on data sets SVHN, MNIST and CASIA show the proposed framework achieved visually appealing performance for image generation task. Besides, we also demonstrate the proposed method achieved superior results for domain adaptation. Code of our experiments is available in https://github.com/luckycallor/CDAAE.



### Sparse Unsupervised Capsules Generalize Better
- **Arxiv ID**: http://arxiv.org/abs/1804.06094v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.06094v1)
- **Published**: 2018-04-17 08:03:42+00:00
- **Updated**: 2018-04-17 08:03:42+00:00
- **Authors**: David Rawlinson, Abdelrahman Ahmed, Gideon Kowadlo
- **Comment**: None
- **Journal**: None
- **Summary**: We show that unsupervised training of latent capsule layers using only the reconstruction loss, without masking to select the correct output class, causes a loss of equivariances and other desirable capsule qualities. This implies that supervised capsules networks can't be very deep. Unsupervised sparsening of latent capsule layer activity both restores these qualities and appears to generalize better than supervised masking, while potentially enabling deeper capsules networks. We train a sparse, unsupervised capsules network of similar geometry to Sabour et al (2017) on MNIST, and then test classification accuracy on affNIST using an SVM layer. Accuracy is improved from benchmark 79% to 90%.



### Human Motion Capture Using a Drone
- **Arxiv ID**: http://arxiv.org/abs/1804.06112v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1804.06112v1)
- **Published**: 2018-04-17 08:57:40+00:00
- **Updated**: 2018-04-17 08:57:40+00:00
- **Authors**: Xiaowei Zhou, Sikang Liu, Georgios Pavlakos, Vijay Kumar, Kostas Daniilidis
- **Comment**: In International Conference on Robotics and Automation (ICRA) 2018
- **Journal**: None
- **Summary**: Current motion capture (MoCap) systems generally require markers and multiple calibrated cameras, which can be used only in constrained environments. In this work we introduce a drone-based system for 3D human MoCap. The system only needs an autonomously flying drone with an on-board RGB camera and is usable in various indoor and outdoor environments. A reconstruction algorithm is developed to recover full-body motion from the video recorded by the drone. We argue that, besides the capability of tracking a moving subject, a flying drone also provides fast varying viewpoints, which is beneficial for motion reconstruction. We evaluate the accuracy of the proposed system using our new DroCap dataset and also demonstrate its applicability for MoCap in the wild using a consumer drone.



### A Support Tensor Train Machine
- **Arxiv ID**: http://arxiv.org/abs/1804.06114v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NA, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1804.06114v1)
- **Published**: 2018-04-17 08:59:13+00:00
- **Updated**: 2018-04-17 08:59:13+00:00
- **Authors**: Cong Chen, Kim Batselier, Ching-Yun Ko, Ngai Wong
- **Comment**: 7 pages
- **Journal**: None
- **Summary**: There has been growing interest in extending traditional vector-based machine learning techniques to their tensor forms. An example is the support tensor machine (STM) that utilizes a rank-one tensor to capture the data structure, thereby alleviating the overfitting and curse of dimensionality problems in the conventional support vector machine (SVM). However, the expressive power of a rank-one tensor is restrictive for many real-world data. To overcome this limitation, we introduce a support tensor train machine (STTM) by replacing the rank-one tensor in an STM with a tensor train. Experiments validate and confirm the superiority of an STTM over the SVM and STM.



### The TUM VI Benchmark for Evaluating Visual-Inertial Odometry
- **Arxiv ID**: http://arxiv.org/abs/1804.06120v3
- **DOI**: 10.1109/IROS.2018.8593419
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1804.06120v3)
- **Published**: 2018-04-17 09:11:23+00:00
- **Updated**: 2020-03-09 13:42:21+00:00
- **Authors**: David Schubert, Thore Goll, Nikolaus Demmel, Vladyslav Usenko, Jörg Stückler, Daniel Cremers
- **Comment**: Updates compared to previous version include additional evaluations
  and DOI
- **Journal**: None
- **Summary**: Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024x1024 resolution at 20 Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200 Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120 Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data is publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.



### Automatic Assessment of Artistic Quality of Photos
- **Arxiv ID**: http://arxiv.org/abs/1804.06124v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.06124v1)
- **Published**: 2018-04-17 09:35:16+00:00
- **Updated**: 2018-04-17 09:35:16+00:00
- **Authors**: Ashish Verma, Kranthi Koukuntla, Rohit Varma, Snehasis Mukherjee
- **Comment**: Submitted to a journal 7 months back, waiting for their response
- **Journal**: None
- **Summary**: This paper proposes a technique to assess the aesthetic quality of photographs. The goal of the study is to predict whether a given photograph is captured by professional photographers, or by common people, based on a measurement of artistic quality of the photograph. We propose a Multi-Layer-Perceptron based system to analyze some low, mid and high level image features and find their effectiveness to measure artistic quality of the image and produce a measurement of the artistic quality of the image on a scale of 10. We validate the proposed system on a large dataset, containing images downloaded from the internet. The dataset contains some images captured by professional photographers and the rest of the images captured by common people. The proposed measurement of artistic quality of images provides higher value of photo quality for the images captured by professional photographers, compared to the values provided for the other images.



### Fast and Accurate Tensor Completion with Total Variation Regularized Tensor Trains
- **Arxiv ID**: http://arxiv.org/abs/1804.06128v3
- **DOI**: None
- **Categories**: **cs.NA**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1804.06128v3)
- **Published**: 2018-04-17 09:37:11+00:00
- **Updated**: 2018-11-13 07:59:54+00:00
- **Authors**: Ching-Yun Ko, Kim Batselier, Wenjian Yu, Ngai Wong
- **Comment**: 13 pages. Source code and supplemental materials are available via:
  https://github.com/IRENEKO/TTC Updates 11/13: included more comparisons and
  experimental results
- **Journal**: None
- **Summary**: We propose a new tensor completion method based on tensor trains. The to-be-completed tensor is modeled as a low-rank tensor train, where we use the known tensor entries and their coordinates to update the tensor train. A novel tensor train initialization procedure is proposed specifically for image and video completion, which is demonstrated to ensure fast convergence of the completion algorithm. The tensor train framework is also shown to easily accommodate Total Variation and Tikhonov regularization due to their low-rank tensor train representations. Image and video inpainting experiments verify the superiority of the proposed scheme in terms of both speed and scalability, where a speedup of up to 155X is observed compared to state-of-the-art tensor completion methods at a similar accuracy. Moreover, we demonstrate the proposed scheme is especially advantageous over existing algorithms when only tiny portions (say, 1%) of the to-be-completed images/videos are known.



### IGCV$2$: Interleaved Structured Sparse Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1804.06202v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.06202v1)
- **Published**: 2018-04-17 12:36:36+00:00
- **Updated**: 2018-04-17 12:36:36+00:00
- **Authors**: Guotian Xie, Jingdong Wang, Ting Zhang, Jianhuang Lai, Richang Hong, Guo-Jun Qi
- **Comment**: Accepted by CVPR 2018
- **Journal**: None
- **Summary**: In this paper, we study the problem of designing efficient convolutional neural network architectures with the interest in eliminating the redundancy in convolution kernels. In addition to structured sparse kernels, low-rank kernels and the product of low-rank kernels, the product of structured sparse kernels, which is a framework for interpreting the recently-developed interleaved group convolutions (IGC) and its variants (e.g., Xception), has been attracting increasing interests.   Motivated by the observation that the convolutions contained in a group convolution in IGC can be further decomposed in the same manner, we present a modularized building block, {IGCV$2$:} interleaved structured sparse convolutions. It generalizes interleaved group convolutions, which is composed of two structured sparse kernels, to the product of more structured sparse kernels, further eliminating the redundancy. We present the complementary condition and the balance condition to guide the design of structured sparse kernels, obtaining a balance among three aspects: model size, computation complexity and classification accuracy. Experimental results demonstrate the advantage on the balance among these three aspects compared to interleaved group convolutions and Xception, and competitive performance compared to other state-of-the-art architecture design methods.



### Simple Baselines for Human Pose Estimation and Tracking
- **Arxiv ID**: http://arxiv.org/abs/1804.06208v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.06208v2)
- **Published**: 2018-04-17 12:55:23+00:00
- **Updated**: 2018-08-21 06:54:53+00:00
- **Authors**: Bin Xiao, Haiping Wu, Yichen Wei
- **Comment**: Accepted by ECCV 2018
- **Journal**: None
- **Summary**: There has been significant progress on pose estimation and increasing interests on pose tracking in recent years. At the same time, the overall algorithm and system complexity increases as well, making the algorithm analysis and comparison more difficult. This work provides simple and effective baseline methods. They are helpful for inspiring and evaluating new ideas for the field. State-of-the-art results are achieved on challenging benchmarks. The code will be available at https://github.com/leoxiaobin/pose.pytorch.



### DetNet: A Backbone network for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1804.06215v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.06215v2)
- **Published**: 2018-04-17 13:09:13+00:00
- **Updated**: 2018-04-19 06:36:36+00:00
- **Authors**: Zeming Li, Chao Peng, Gang Yu, Xiangyu Zhang, Yangdong Deng, Jian Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Recent CNN based object detectors, no matter one-stage methods like YOLO, SSD, and RetinaNe or two-stage detectors like Faster R-CNN, R-FCN and FPN are usually trying to directly finetune from ImageNet pre-trained models designed for image classification. There has been little work discussing on the backbone feature extractor specifically designed for the object detection. More importantly, there are several differences between the tasks of image classification and object detection. 1. Recent object detectors like FPN and RetinaNet usually involve extra stages against the task of image classification to handle the objects with various scales. 2. Object detection not only needs to recognize the category of the object instances but also spatially locate the position. Large downsampling factor brings large valid receptive field, which is good for image classification but compromises the object location ability. Due to the gap between the image classification and object detection, we propose DetNet in this paper, which is a novel backbone network specifically designed for object detection. Moreover, DetNet includes the extra stages against traditional backbone network for image classification, while maintains high spatial resolution in deeper layers. Without any bells and whistles, state-of-the-art results have been obtained for both object detection and instance segmentation on the MSCOCO benchmark based on our DetNet~(4.8G FLOPs) backbone. The code will be released for the reproduction.



### A Saliency-based Convolutional Neural Network for Table and Chart Detection in Digitized Documents
- **Arxiv ID**: http://arxiv.org/abs/1804.06236v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.06236v1)
- **Published**: 2018-04-17 13:39:29+00:00
- **Updated**: 2018-04-17 13:39:29+00:00
- **Authors**: I. Kavasidis, S. Palazzo, C. Spampinato, C. Pino, D. Giordano, D. Giuffrida, P. Messina
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Convolutional Neural Networks (DCNNs) have recently been applied successfully to a variety of vision and multimedia tasks, thus driving development of novel solutions in several application domains. Document analysis is a particularly promising area for DCNNs: indeed, the number of available digital documents has reached unprecedented levels, and humans are no longer able to discover and retrieve all the information contained in these documents without the help of automation. Under this scenario, DCNNs offers a viable solution to automate the information extraction process from digital documents. Within the realm of information extraction from documents, detection of tables and charts is particularly needed as they contain a visual summary of the most valuable information contained in a document. For a complete automation of visual information extraction process from tables and charts, it is necessary to develop techniques that localize them and identify precisely their boundaries. In this paper we aim at solving the table/chart detection task through an approach that combines deep convolutional neural networks, graphical models and saliency concepts. In particular, we propose a saliency-based fully-convolutional neural network performing multi-scale reasoning on visual cues followed by a fully-connected conditional random field (CRF) for localizing tables and charts in digital/digitized documents. Performance analysis carried out on an extended version of ICDAR 2013 (with annotated charts as well as tables) shows that our approach yields promising results, outperforming existing models.



### Vortex Pooling: Improving Context Representation in Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1804.06242v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.06242v2)
- **Published**: 2018-04-17 13:44:51+00:00
- **Updated**: 2018-04-22 14:09:33+00:00
- **Authors**: Chen-Wei Xie, Hong-Yu Zhou, Jianxin Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic segmentation is a fundamental task in computer vision, which can be considered as a per-pixel classification problem. Recently, although fully convolutional neural network (FCN) based approaches have made remarkable progress in such task, aggregating local and contextual information in convolutional feature maps is still a challenging problem. In this paper, we argue that, when predicting the category of a given pixel, the regions close to the target are more important than those far from it. To tackle this problem, we then propose an effective yet efficient approach named Vortex Pooling to effectively utilize contextual information. Empirical studies are also provided to validate the effectiveness of the proposed method. To be specific, our approach outperforms the previous state-of-the-art model named DeepLab v3 by 1.5% on the PASCAL VOC 2012 val set and 0.6% on the test set by replacing the Atrous Spatial Pyramid Pooling (ASPP) module in DeepLab v3 with the proposed Vortex Pooling. Moreover, our model (10.13FPS) shares similar computation cost with DeepLab v3 (10.37 FPS).



### PM-GANs: Discriminative Representation Learning for Action Recognition Using Partial-modalities
- **Arxiv ID**: http://arxiv.org/abs/1804.06248v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.06248v1)
- **Published**: 2018-04-17 13:48:18+00:00
- **Updated**: 2018-04-17 13:48:18+00:00
- **Authors**: Lan Wang, Chenqiang Gao, Luyu Yang, Yue Zhao, Wangmeng Zuo, Deyu Meng
- **Comment**: None
- **Journal**: None
- **Summary**: Data of different modalities generally convey complimentary but heterogeneous information, and a more discriminative representation is often preferred by combining multiple data modalities like the RGB and infrared features. However in reality, obtaining both data channels is challenging due to many limitations. For example, the RGB surveillance cameras are often restricted from private spaces, which is in conflict with the need of abnormal activity detection for personal security. As a result, using partial data channels to build a full representation of multi-modalities is clearly desired. In this paper, we propose a novel Partial-modal Generative Adversarial Networks (PM-GANs) that learns a full-modal representation using data from only partial modalities. The full representation is achieved by a generated representation in place of the missing data channel. Extensive experiments are conducted to verify the performance of our proposed method on action recognition, compared with four state-of-the-art methods. Meanwhile, a new Infrared-Visible Dataset for action recognition is introduced, and will be the first publicly available action dataset that contains paired infrared and visible spectrum.



### Temporal Coherent and Graph Optimized Manifold Ranking for Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/1804.06253v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.06253v1)
- **Published**: 2018-04-17 13:50:24+00:00
- **Updated**: 2018-04-17 13:50:24+00:00
- **Authors**: Bo Jiang, Doudou Lin, Bin Luo, Jin Tang
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, weighted patch representation has been widely studied for alleviating the impact of background information included in bounding box to improve visual tracking results. However, existing weighted patch representation models generally exploit spatial structure information among patches in each frame separately which ignore (1) unary featureof each patch and (2) temporal correlation among patches in different frames. To address this problem, we propose a novel unified temporal coherence and graph optimized ranking model for weighted patch representation in visual tracking problem. There are three main contributions of this paper. First, we propose to employ a flexible graph ranking for patch weight computation which exploits both structure information among patches and unary feature of each patch simultaneously. Second, we propose a new more discriminative ranking model by further considering the temporal correlation among patches in different frames. Third, a neighborhood preserved, low-rank graph is learned and incorporated to build a unified optimized ranking model. Experiments on two benchmark datasets show the benefits of our model.



### Synthetic data generation for Indic handwritten text recognition
- **Arxiv ID**: http://arxiv.org/abs/1804.06254v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.06254v1)
- **Published**: 2018-04-17 13:52:59+00:00
- **Updated**: 2018-04-17 13:52:59+00:00
- **Authors**: Partha Pratim Roy, Akash Mohta, Bidyut B. Chaudhuri
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a novel approach to generate synthetic dataset for handwritten word recognition systems. It is difficult to recognize handwritten scripts for which sufficient training data is not readily available or it may be expensive to collect such data. Hence, it becomes hard to train recognition systems owing to lack of proper dataset. To overcome such problems, synthetic data could be used to create or expand the existing training dataset to improve recognition performance. Any available digital data from online newspaper and such sources can be used to generate synthetic data. In this paper, we propose to add distortion/deformation to digital data in such a way that the underlying pattern is preserved, so that the image so produced bears a close similarity to actual handwritten samples. The images thus produced can be used independently to train the system or be combined with natural handwritten data to augment the original dataset and improve the recognition system. We experimented using synthetic data to improve the recognition accuracy of isolated characters and words. The framework is tested on 2 Indic scripts - Devanagari (Hindi) and Bengali (Bangla), for numeral, character and word recognition. We have obtained encouraging results from the experiment. Finally, the experiment with Latin text verifies the utility of the approach.



### Network Signatures from Image Representation of Adjacency Matrices: Deep/Transfer Learning for Subgraph Classification
- **Arxiv ID**: http://arxiv.org/abs/1804.06275v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.SI
- **Links**: [PDF](http://arxiv.org/pdf/1804.06275v1)
- **Published**: 2018-04-17 14:14:26+00:00
- **Updated**: 2018-04-17 14:14:26+00:00
- **Authors**: Kshiteesh Hegde, Malik Magdon-Ismail, Ram Ramanathan, Bishal Thapa
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel subgraph image representation for classification of network fragments with the targets being their parent networks. The graph image representation is based on 2D image embeddings of adjacency matrices. We use this image representation in two modes. First, as the input to a machine learning algorithm. Second, as the input to a pure transfer learner. Our conclusions from several datasets are that (a) deep learning using our structured image features performs the best compared to benchmark graph kernel and classical features based methods; and, (b) pure transfer learning works effectively with minimum interference from the user and is robust against small data.



### PlaneNet: Piece-wise Planar Reconstruction from a Single RGB Image
- **Arxiv ID**: http://arxiv.org/abs/1804.06278v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.06278v1)
- **Published**: 2018-04-17 14:18:33+00:00
- **Updated**: 2018-04-17 14:18:33+00:00
- **Authors**: Chen Liu, Jimei Yang, Duygu Ceylan, Ersin Yumer, Yasutaka Furukawa
- **Comment**: CVPR 2018
- **Journal**: None
- **Summary**: This paper proposes a deep neural network (DNN) for piece-wise planar depthmap reconstruction from a single RGB image. While DNNs have brought remarkable progress to single-image depth prediction, piece-wise planar depthmap reconstruction requires a structured geometry representation, and has been a difficult task to master even for DNNs. The proposed end-to-end DNN learns to directly infer a set of plane parameters and corresponding plane segmentation masks from a single RGB image. We have generated more than 50,000 piece-wise planar depthmaps for training and testing from ScanNet, a large-scale RGBD video database. Our qualitative and quantitative evaluations demonstrate that the proposed approach outperforms baseline methods in terms of both plane segmentation and depth estimation accuracy. To the best of our knowledge, this paper presents the first end-to-end neural architecture for piece-wise planar reconstruction from a single RGB image. Code and data are available at https://github.com/art-programmer/PlaneNet.



### Efficient Solvers for Sparse Subspace Clustering
- **Arxiv ID**: http://arxiv.org/abs/1804.06291v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.06291v2)
- **Published**: 2018-04-17 14:41:52+00:00
- **Updated**: 2020-02-20 03:19:41+00:00
- **Authors**: Farhad Pourkamali-Anaraki, James Folberth, Stephen Becker
- **Comment**: This paper is accepted for publication in Signal Processing
- **Journal**: None
- **Summary**: Sparse subspace clustering (SSC) clusters $n$ points that lie near a union of low-dimensional subspaces. The SSC model expresses each point as a linear or affine combination of the other points, using either $\ell_1$ or $\ell_0$ regularization. Using $\ell_1$ regularization results in a convex problem but requires $O(n^2)$ storage, and is typically solved by the alternating direction method of multipliers which takes $O(n^3)$ flops. The $\ell_0$ model is non-convex but only needs memory linear in $n$, and is solved via orthogonal matching pursuit and cannot handle the case of affine subspaces. This paper shows that a proximal gradient framework can solve SSC, covering both $\ell_1$ and $\ell_0$ models, and both linear and affine constraints. For both $\ell_1$ and $\ell_0$, algorithms to compute the proximity operator in the presence of affine constraints have not been presented in the SSC literature, so we derive an exact and efficient algorithm that solves the $\ell_1$ case with just $O(n^2)$ flops. In the $\ell_0$ case, our algorithm retains the low-memory overhead, and is the first algorithm to solve the SSC-$\ell_0$ model with affine constraints. Experiments show our algorithms do not rely on sensitive regularization parameters, and they are less sensitive to sparsity misspecification and high noise.



### PredRNN++: Towards A Resolution of the Deep-in-Time Dilemma in Spatiotemporal Predictive Learning
- **Arxiv ID**: http://arxiv.org/abs/1804.06300v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1804.06300v2)
- **Published**: 2018-04-17 14:52:19+00:00
- **Updated**: 2018-11-19 02:17:17+00:00
- **Authors**: Yunbo Wang, Zhifeng Gao, Mingsheng Long, Jianmin Wang, Philip S. Yu
- **Comment**: None
- **Journal**: None
- **Summary**: We present PredRNN++, an improved recurrent network for video predictive learning. In pursuit of a greater spatiotemporal modeling capability, our approach increases the transition depth between adjacent states by leveraging a novel recurrent unit, which is named Causal LSTM for re-organizing the spatial and temporal memories in a cascaded mechanism. However, there is still a dilemma in video predictive learning: increasingly deep-in-time models have been designed for capturing complex variations, while introducing more difficulties in the gradient back-propagation. To alleviate this undesirable effect, we propose a Gradient Highway architecture, which provides alternative shorter routes for gradient flows from outputs back to long-range inputs. This architecture works seamlessly with causal LSTMs, enabling PredRNN++ to capture short-term and long-term dependencies adaptively. We assess our model on both synthetic and real video datasets, showing its ability to ease the vanishing gradient problem and yield state-of-the-art prediction results even in a difficult objects occlusion scenario.



### Three-Dimensional GPU-Accelerated Active Contours for Automated Localization of Cells in Large Images
- **Arxiv ID**: http://arxiv.org/abs/1804.06304v1
- **DOI**: 10.1371/journal.pone.0215843
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1804.06304v1)
- **Published**: 2018-04-17 15:01:11+00:00
- **Updated**: 2018-04-17 15:01:11+00:00
- **Authors**: Mahsa Lotfollahi, Sebastian Berisha, Leila Saadatifard, Laura Montier, Jokubas Ziburkus, David Mayerich
- **Comment**: None
- **Journal**: None
- **Summary**: Cell segmentation in microscopy is a challenging problem, since cells are often asymmetric and densely packed. This becomes particularly challenging for extremely large images, since manual intervention and processing time can make segmentation intractable. In this paper, we present an efficient and highly parallel formulation for symmetric three-dimensional (3D) contour evolution that extends previous work on fast two-dimensional active contours. We provide a formulation for optimization on 3D images, as well as a strategy for accelerating computation on consumer graphics hardware. The proposed software takes advantage of Monte-Carlo sampling schemes in order to speed up convergence and reduce thread divergence. Experimental results show that this method provides superior performance for large 2D and 3D cell segmentation tasks when compared to existing methods on large 3D brain images.



### Training a Binary Weight Object Detector by Knowledge Transfer for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1804.06332v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.06332v2)
- **Published**: 2018-04-17 15:53:44+00:00
- **Updated**: 2019-05-25 17:03:05+00:00
- **Authors**: Jiaolong Xu, Peng Wang, Heng Yang, Antonio M. López
- **Comment**: Accepted by ICRA 2019
- **Journal**: None
- **Summary**: Autonomous driving has harsh requirements of small model size and energy efficiency, in order to enable the embedded system to achieve real-time on-board object detection. Recent deep convolutional neural network based object detectors have achieved state-of-the-art accuracy. However, such models are trained with numerous parameters and their high computational costs and large storage prohibit the deployment to memory and computation resource limited systems. Low-precision neural networks are popular techniques for reducing the computation requirements and memory footprint. Among them, binary weight neural network (BWN) is the extreme case which quantizes the float-point into just $1$ bit. BWNs are difficult to train and suffer from accuracy deprecation due to the extreme low-bit representation. To address this problem, we propose a knowledge transfer (KT) method to aid the training of BWN using a full-precision teacher network. We built DarkNet- and MobileNet-based binary weight YOLO-v2 detectors and conduct experiments on KITTI benchmark for car, pedestrian and cyclist detection. The experimental results show that the proposed method maintains high detection accuracy while reducing the model size of DarkNet-YOLO from 257 MB to 8.8 MB and MobileNet-YOLO from 193 MB to 7.9 MB.



### Not-so-supervised: a survey of semi-supervised, multi-instance, and transfer learning in medical image analysis
- **Arxiv ID**: http://arxiv.org/abs/1804.06353v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.06353v2)
- **Published**: 2018-04-17 16:25:31+00:00
- **Updated**: 2018-09-14 07:34:46+00:00
- **Authors**: Veronika Cheplygina, Marleen de Bruijne, Josien P. W. Pluim
- **Comment**: Submitted to Medical Image Analysis
- **Journal**: None
- **Summary**: Machine learning (ML) algorithms have made a tremendous impact in the field of medical imaging. While medical imaging datasets have been growing in size, a challenge for supervised ML algorithms that is frequently mentioned is the lack of annotated data. As a result, various methods which can learn with less/other types of supervision, have been proposed. We review semi-supervised, multiple instance, and transfer learning in medical imaging, both in diagnosis/detection or segmentation tasks. We also discuss connections between these learning scenarios, and opportunities for future research.



### DGPose: Deep Generative Models for Human Body Analysis
- **Arxiv ID**: http://arxiv.org/abs/1804.06364v2
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1804.06364v2)
- **Published**: 2018-04-17 16:43:35+00:00
- **Updated**: 2020-02-14 19:48:00+00:00
- **Authors**: Rodrigo de Bem, Arnab Ghosh, Thalaiyasingam Ajanthan, Ondrej Miksik, Adnane Boukhayma, N. Siddharth, Philip Torr
- **Comment**: IJCV 2020 special issue on 'Generating Realistic Visual Data of Human
  Behavior' preprint. Keywords: deep generative models, semi-supervised
  learning, human pose estimation, variational autoencoders, generative
  adversarial networks
- **Journal**: None
- **Summary**: Deep generative modelling for human body analysis is an emerging problem with many interesting applications. However, the latent space learned by such approaches is typically not interpretable, resulting in less flexibility. In this work, we present deep generative models for human body analysis in which the body pose and the visual appearance are disentangled. Such a disentanglement allows independent manipulation of pose and appearance, and hence enables applications such as pose-transfer without specific training for such a task. Our proposed models, the Conditional-DGPose and the Semi-DGPose, have different characteristics. In the first, body pose labels are taken as conditioners, from a fully-supervised training set. In the second, our structured semi-supervised approach allows for pose estimation to be performed by the model itself and relaxes the need for labelled data. Therefore, the Semi-DGPose aims for the joint understanding and generation of people in images. It is not only capable of mapping images to interpretable latent representations but also able to map these representations back to the image space. We compare our models with relevant baselines, the ClothNet-Body and the Pose Guided Person Generation networks, demonstrating their merits on the Human3.6M, ChictopiaPlus and DeepFashion benchmarks.



### Im2Avatar: Colorful 3D Reconstruction from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/1804.06375v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.06375v1)
- **Published**: 2018-04-17 17:02:20+00:00
- **Updated**: 2018-04-17 17:02:20+00:00
- **Authors**: Yongbin Sun, Ziwei Liu, Yue Wang, Sanjay E. Sarma
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Existing works on single-image 3D reconstruction mainly focus on shape recovery. In this work, we study a new problem, that is, simultaneously recovering 3D shape and surface color from a single image, namely "colorful 3D reconstruction". This problem is both challenging and intriguing because the ability to infer textured 3D model from a single image is at the core of visual understanding. Here, we propose an end-to-end trainable framework, Colorful Voxel Network (CVN), to tackle this problem. Conditioned on a single 2D input, CVN learns to decompose shape and surface color information of a 3D object into a 3D shape branch and a surface color branch, respectively. Specifically, for the shape recovery, we generate a shape volume with the state of its voxels indicating occupancy. For the surface color recovery, we combine the strength of appearance hallucination and geometric projection by concurrently learning a regressed color volume and a 2D-to-3D flow volume, which are then fused into a blended color volume. The final textured 3D model is obtained by sampling color from the blended color volume at the positions of occupied voxels in the shape volume. To handle the severe sparse volume representations, a novel loss function, Mean Squared False Cross-Entropy Loss (MSFCEL), is designed. Extensive experiments demonstrate that our approach achieves significant improvement over baselines, and shows great generalization across diverse object categories and arbitrary viewpoints.



### Deep Object Co-Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1804.06423v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.06423v2)
- **Published**: 2018-04-17 18:24:51+00:00
- **Updated**: 2019-05-28 17:05:49+00:00
- **Authors**: Weihao Li, Omid Hosseini Jafari, Carsten Rother
- **Comment**: Accepted at ACCV 2018
- **Journal**: None
- **Summary**: This work presents a deep object co-segmentation (DOCS) approach for segmenting common objects of the same class within a pair of images. This means that the method learns to ignore common, or uncommon, background stuff and focuses on objects. If multiple object classes are presented in the image pair, they are jointly extracted as foreground. To address this task, we propose a CNN-based Siamese encoder-decoder architecture. The encoder extracts high-level semantic features of the foreground objects, a mutual correlation layer detects the common objects, and finally, the decoder generates the output foreground masks for each image. To train our model, we compile a large object co-segmentation dataset consisting of image pairs from the PASCAL VOC dataset with common objects masks. We evaluate our approach on commonly used datasets for co-segmentation tasks and observe that our approach consistently outperforms competing methods, for both seen and unseen object classes.



### Vision Based Dynamic Offside Line Marker for Soccer Games
- **Arxiv ID**: http://arxiv.org/abs/1804.06438v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.06438v1)
- **Published**: 2018-04-17 19:00:01+00:00
- **Updated**: 2018-04-17 19:00:01+00:00
- **Authors**: Karthik Muthuraman, Pranav Joshi, Suraj Kiran Raman
- **Comment**: None
- **Journal**: None
- **Summary**: Offside detection in soccer has emerged as one of the most important decisions with an average of 50 offside decisions every game. False detections and rash calls adversely affect game conditions and in many cases drastically change the outcome of the game. The human eye has finite precision and can only discern a limited amount of detail in a given instance. Current offside decisions are made manually by sideline referees and tend to remain controversial in many games. This calls for automated offside detection techniques in order to assist accurate refereeing. In this work, we have explicitly used computer vision and image processing techniques like Hough transform, color similarity (quantization), graph connected components, and vanishing point ideas to identify the probable offside regions.   Keywords: Hough transform, connected components, KLT tracking, color similarity.



### Deep Multimodal Subspace Clustering Networks
- **Arxiv ID**: http://arxiv.org/abs/1804.06498v3
- **DOI**: 10.1109/JSTSP.2018.2875385
- **Categories**: **cs.LG**, cs.CV, stat.ML, 68T45, 62H30, I.5.3; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/1804.06498v3)
- **Published**: 2018-04-17 23:04:33+00:00
- **Updated**: 2019-01-04 20:19:14+00:00
- **Authors**: Mahdi Abavisani, Vishal M. Patel
- **Comment**: None
- **Journal**: IEEE Journal of Selected Topics in Signal Processing, vol. 12, no.
  6, pp. 1601-1614, Dec. 2018
- **Summary**: We present convolutional neural network (CNN) based approaches for unsupervised multimodal subspace clustering. The proposed framework consists of three main stages - multimodal encoder, self-expressive layer, and multimodal decoder. The encoder takes multimodal data as input and fuses them to a latent space representation. The self-expressive layer is responsible for enforcing the self-expressiveness property and acquiring an affinity matrix corresponding to the data points. The decoder reconstructs the original input data. The network uses the distance between the decoder's reconstruction and the original input in its training. We investigate early, late and intermediate fusion techniques and propose three different encoders corresponding to them for spatial fusion. The self-expressive layers and multimodal decoders are essentially the same for different spatial fusion-based approaches. In addition to various spatial fusion-based methods, an affinity fusion-based network is also proposed in which the self-expressive layer corresponding to different modalities is enforced to be the same. Extensive experiments on three datasets show that the proposed methods significantly outperform the state-of-the-art multimodal subspace clustering methods.



### Learning how to be robust: Deep polynomial regression
- **Arxiv ID**: http://arxiv.org/abs/1804.06504v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.06504v2)
- **Published**: 2018-04-17 23:36:12+00:00
- **Updated**: 2018-05-23 10:21:03+00:00
- **Authors**: Juan-Manuel Perez-Rua, Tomas Crivelli, Patrick Bouthemy, Patrick Perez
- **Comment**: 18 pages, conference
- **Journal**: None
- **Summary**: Polynomial regression is a recurrent problem with a large number of applications. In computer vision it often appears in motion analysis. Whatever the application, standard methods for regression of polynomial models tend to deliver biased results when the input data is heavily contaminated by outliers. Moreover, the problem is even harder when outliers have strong structure. Departing from problem-tailored heuristics for robust estimation of parametric models, we explore deep convolutional neural networks. Our work aims to find a generic approach for training deep regression models without the explicit need of supervised annotation. We bypass the need for a tailored loss function on the regression parameters by attaching to our model a differentiable hard-wired decoder corresponding to the polynomial operation at hand. We demonstrate the value of our findings by comparing with standard robust regression methods. Furthermore, we demonstrate how to use such models for a real computer vision problem, i.e., video stabilization. The qualitative and quantitative experiments show that neural networks are able to learn robustness for general polynomial regression, with results that well overpass scores of traditional robust estimation methods.



### Complementary Attributes: A New Clue to Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1804.06505v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.06505v2)
- **Published**: 2018-04-17 23:48:21+00:00
- **Updated**: 2019-07-22 07:56:03+00:00
- **Authors**: Xiaofeng Xu, Ivor W. Tsang, Chuancai Liu
- **Comment**: Accepted by IEEE TRANSACTIONS ON CYBERNETICS
- **Journal**: None
- **Summary**: Zero-shot learning (ZSL) aims to recognize unseen objects using disjoint seen objects via sharing attributes. The generalization performance of ZSL is governed by the attributes, which transfer semantic information from seen classes to unseen classes. To take full advantage of the knowledge transferred by attributes, in this paper, we introduce the notion of complementary attributes (CA), as a supplement to the original attributes, to enhance the semantic representation ability. Theoretical analyses demonstrate that complementary attributes can improve the PAC-style generalization bound of original ZSL model. Since the proposed CA focuses on enhancing the semantic representation, CA can be easily applied to any existing attribute-based ZSL methods, including the label-embedding strategy based ZSL (LEZSL) and the probability-prediction strategy based ZSL (PPZSL). In PPZSL, there is a strong assumption that all the attributes are independent of each other, which is arguably unrealistic in practice. To solve this problem, a novel rank aggregation framework is proposed to circumvent the assumption. Extensive experiments on five ZSL benchmark datasets and the large-scale ImageNet dataset demonstrate that the proposed complementary attributes and rank aggregation can significantly and robustly improve existing ZSL methods and achieve the state-of-the-art performance.



