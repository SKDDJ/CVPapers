# Arxiv Papers in cs.CV on 2018-04-21
### 3D Human Pose Estimation on a Configurable Bed from a Pressure Image
- **Arxiv ID**: http://arxiv.org/abs/1804.07873v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1804.07873v2)
- **Published**: 2018-04-21 01:15:44+00:00
- **Updated**: 2018-08-29 22:30:32+00:00
- **Authors**: Henry M. Clever, Ariel Kapusta, Daehyung Park, Zackory Erickson, Yash Chitalia, Charles C. Kemp
- **Comment**: 8 pages, 10 figures
- **Journal**: None
- **Summary**: Robots have the potential to assist people in bed, such as in healthcare settings, yet bedding materials like sheets and blankets can make observation of the human body difficult for robots. A pressure-sensing mat on a bed can provide pressure images that are relatively insensitive to bedding materials. However, prior work on estimating human pose from pressure images has been restricted to 2D pose estimates and flat beds. In this work, we present two convolutional neural networks to estimate the 3D joint positions of a person in a configurable bed from a single pressure image. The first network directly outputs 3D joint positions, while the second outputs a kinematic model that includes estimated joint angles and limb lengths. We evaluated our networks on data from 17 human participants with two bed configurations: supine and seated. Our networks achieved a mean joint position error of 77 mm when tested with data from people outside the training set, outperforming several baselines. We also present a simple mechanical model that provides insight into ambiguity associated with limbs raised off of the pressure mat, and demonstrate that Monte Carlo dropout can be used to estimate pose confidence in these situations. Finally, we provide a demonstration in which a mobile manipulator uses our network's estimated kinematic model to reach a location on a person's body in spite of the person being seated in a bed and covered by a blanket.



### Learning to Refine Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1804.07909v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.07909v1)
- **Published**: 2018-04-21 07:46:13+00:00
- **Updated**: 2018-04-21 07:46:13+00:00
- **Authors**: Mihai Fieraru, Anna Khoreva, Leonid Pishchulin, Bernt Schiele
- **Comment**: To appear in CVPRW (2018). Workshop: Visual Understanding of Humans
  in Crowd Scene and the 2nd Look Into Person Challenge (VUHCS-LIP)
- **Journal**: None
- **Summary**: Multi-person pose estimation in images and videos is an important yet challenging task with many applications. Despite the large improvements in human pose estimation enabled by the development of convolutional neural networks, there still exist a lot of difficult cases where even the state-of-the-art models fail to correctly localize all body joints. This motivates the need for an additional refinement step that addresses these challenging cases and can be easily applied on top of any existing method. In this work, we introduce a pose refinement network (PoseRefiner) which takes as input both the image and a given pose estimate and learns to directly predict a refined pose by jointly reasoning about the input-output space. In order for the network to learn to refine incorrect body joint predictions, we employ a novel data augmentation scheme for training, where we model "hard" human pose cases. We evaluate our approach on four popular large-scale pose estimation benchmarks such as MPII Single- and Multi-Person Pose Estimation, PoseTrack Pose Estimation, and PoseTrack Pose Tracking, and report systematic improvement over the state of the art.



### Multi-view registration of unordered range scans by fast correspondence propagation of multi-scale descriptors
- **Arxiv ID**: http://arxiv.org/abs/1804.07926v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1804.07926v1)
- **Published**: 2018-04-21 09:36:33+00:00
- **Updated**: 2018-04-21 09:36:33+00:00
- **Authors**: Jihua Zhu, Siyu Xu, Zutao Jiang, Shanmin Pang, Jun Wang, Zhongyu Li
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a global approach for the multi-view registration of unordered range scans. As the basis of multi-view registration, pair-wise registration is very pivotal. Therefore, we first select a good descriptor and accelerate its correspondence propagation for the pair-wise registration. Then, we design an effective rule to judge the reliability of pair-wise registration results. Subsequently, we propose a model augmentation method, which can utilize reliable results of pair-wise registration to augment the model shape. Finally, multi-view registration can be accomplished by operating the pair-wise registration and judgment, and model augmentation alternately. Experimental results on public available data sets show, that this approach can automatically achieve the multi-view registration of unordered range scans with good accuracy and effectiveness.



### Multi-Modal Coreference Resolution with the Correlation between Space Structures
- **Arxiv ID**: http://arxiv.org/abs/1804.08010v2
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1804.08010v2)
- **Published**: 2018-04-21 19:15:19+00:00
- **Updated**: 2018-09-01 12:33:13+00:00
- **Authors**: Qibin Zheng, Xingchun Diao, Jianjun Cao, Xiaolei Zhou, Yi Liu, Hongmei Li
- **Comment**: 9 pages, 6 figures
- **Journal**: None
- **Summary**: Multi-modal data is becoming more common in big data background. Finding the semantically similar objects from different modality is one of the heart problems of multi-modal learning. Most of the current methods try to learn the inter-modal correlation with extrinsic supervised information, while intrinsic structural information of each modality is neglected. The performance of these methods heavily depends on the richness of training samples. However, obtaining the multi-modal training samples is still a labor and cost intensive work. In this paper, we bring a extrinsic correlation between the space structures of each modalities in coreference resolution. With this correlation, a semi-supervised learning model for multi-modal coreference resolution is proposed. We firstly extract high-level features of images and text, then compute the distances of each object from some reference points to build the space structure of each modality. With a shared reference point set, the space structures of each modality are correlated. We employ the correlation to build a commonly shared space that the semantic distance between multi-modal objects can be computed directly. The experiments on two multi-modal datasets show that our model performs better than the existing methods with insufficient training data.



### ShapeStacks: Learning Vision-Based Physical Intuition for Generalised Object Stacking
- **Arxiv ID**: http://arxiv.org/abs/1804.08018v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.08018v2)
- **Published**: 2018-04-21 19:52:10+00:00
- **Updated**: 2018-07-06 14:00:34+00:00
- **Authors**: Oliver Groth, Fabian B. Fuchs, Ingmar Posner, Andrea Vedaldi
- **Comment**: revised version to appear at ECCV 2018
- **Journal**: None
- **Summary**: Physical intuition is pivotal for intelligent agents to perform complex tasks. In this paper we investigate the passive acquisition of an intuitive understanding of physical principles as well as the active utilisation of this intuition in the context of generalised object stacking. To this end, we provide: a simulation-based dataset featuring 20,000 stack configurations composed of a variety of elementary geometric primitives richly annotated regarding semantics and structural stability. We train visual classifiers for binary stability prediction on the ShapeStacks data and scrutinise their learned physical intuition. Due to the richness of the training data our approach also generalises favourably to real-world scenarios achieving state-of-the-art stability prediction on a publicly available benchmark of block towers. We then leverage the physical intuition learned by our model to actively construct stable stacks and observe the emergence of an intuitive notion of stackability - an inherent object affordance - induced by the active stacking task. Our approach performs well even in challenging conditions where it considerably exceeds the stack height observed during training or in cases where initially unstable structures must be stabilised via counterbalancing.



### Synthesized Texture Quality Assessment via Multi-scale Spatial and Statistical Texture Attributes of Image and Gradient Magnitude Coefficients
- **Arxiv ID**: http://arxiv.org/abs/1804.08020v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.08020v2)
- **Published**: 2018-04-21 20:18:45+00:00
- **Updated**: 2018-04-26 20:54:38+00:00
- **Authors**: S. Alireza Golestaneh, Lina Karam
- **Comment**: None
- **Journal**: None
- **Summary**: Perceptual quality assessment for synthesized textures is a challenging task. In this paper, we propose a training-free reduced-reference (RR) objective quality assessment method that quantifies the perceived quality of synthesized textures. The proposed reduced-reference synthesized texture quality assessment metric is based on measuring the spatial and statistical attributes of the texture image using both image- and gradient-based wavelet coefficients at multiple scales. Performance evaluations on two synthesized texture databases demonstrate that our proposed RR synthesized texture quality metric significantly outperforms both full-reference and RR state-of-the-art quality metrics in predicting the perceived visual quality of the synthesized textures.



### Angiodysplasia Detection and Localization Using Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1804.08024v1
- **DOI**: 10.1109/ICMLA.2018.00098
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.08024v1)
- **Published**: 2018-04-21 20:38:32+00:00
- **Updated**: 2018-04-21 20:38:32+00:00
- **Authors**: Alexey Shvets, Vladimir Iglovikov, Alexander Rakhlin, Alexandr A. Kalinin
- **Comment**: 12 pages, 6 figures
- **Journal**: 2018 17th IEEE International Conference on Machine Learning and
  Applications (ICMLA)
- **Summary**: Accurate detection and localization for angiodysplasia lesions is an important problem in early stage diagnostics of gastrointestinal bleeding and anemia. Gold-standard for angiodysplasia detection and localization is performed using wireless capsule endoscopy. This pill-like device is able to produce thousand of high enough resolution images during one passage through gastrointestinal tract. In this paper we present our winning solution for MICCAI 2017 Endoscopic Vision SubChallenge: Angiodysplasia Detection and Localization its further improvements over the state-of-the-art results using several novel deep neural network architectures. It address the binary segmentation problem, where every pixel in an image is labeled as an angiodysplasia lesions or background. Then, we analyze connected component of each predicted mask. Based on the analysis we developed a classifier that predict angiodysplasia lesions (binary variable) and a detector for their localization (center of a component). In this setting, our approach outperforms other methods in every task subcategory for angiodysplasia detection and localization thereby providing state-of-the-art results for these problems. The source code for our solution is made publicly available at https://github.com/ternaus/angiodysplasia-segmentatio



### Learning Myelin Content in Multiple Sclerosis from Multimodal MRI through Adversarial Training
- **Arxiv ID**: http://arxiv.org/abs/1804.08039v2
- **DOI**: 10.1007/978-3-030-00931-1_59
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.08039v2)
- **Published**: 2018-04-21 22:23:51+00:00
- **Updated**: 2018-06-08 09:49:59+00:00
- **Authors**: Wen Wei, Emilie Poirion, Benedetta Bodini, Stanley Durrleman, Nicholas Ayache, Bruno Stankoff, Olivier Colliot
- **Comment**: Accepted by MICCAI2018
- **Journal**: None
- **Summary**: Multiple sclerosis (MS) is a demyelinating disease of the central nervous system (CNS). A reliable measure of the tissue myelin content is therefore essential for the understanding of the physiopathology of MS, tracking progression and assessing treatment efficacy. Positron emission tomography (PET) with $[^{11} \mbox{C}] \mbox{PIB}$ has been proposed as a promising biomarker for measuring myelin content changes in-vivo in MS. However, PET imaging is expensive and invasive due to the injection of a radioactive tracer. On the contrary, magnetic resonance imaging (MRI) is a non-invasive, widely available technique, but existing MRI sequences do not provide, to date, a reliable, specific, or direct marker of either demyelination or remyelination. In this work, we therefore propose Sketcher-Refiner Generative Adversarial Networks (GANs) with specifically designed adversarial loss functions to predict the PET-derived myelin content map from a combination of MRI modalities. The prediction problem is solved by a sketch-refinement process in which the sketcher generates the preliminary anatomical and physiological information and the refiner refines and generates images reflecting the tissue myelin content in the human brain. We evaluated the ability of our method to predict myelin content at both global and voxel-wise levels. The evaluation results show that the demyelination in lesion regions and myelin content in normal-appearing white matter (NAWM) can be well predicted by our method. The method has the potential to become a useful tool for clinical management of patients with MS.



### Study of Residual Networks for Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/1805.00325v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.00325v1)
- **Published**: 2018-04-21 23:04:53+00:00
- **Updated**: 2018-04-21 23:04:53+00:00
- **Authors**: Mohammad Sadegh Ebrahimi, Hossein Karkeh Abadi
- **Comment**: 6 pages, 9 figures
- **Journal**: None
- **Summary**: Deep neural networks demonstrate to have a high performance on image classification tasks while being more difficult to train. Due to the complexity and vanishing gradient problem, it normally takes a lot of time and more computational power to train deeper neural networks. Deep residual networks (ResNets) can make the training process faster and attain more accuracy compared to their equivalent neural networks. ResNets achieve this improvement by adding a simple skip connection parallel to the layers of convolutional neural networks. In this project we first design a ResNet model that can perform the image classification task on the Tiny ImageNet dataset with a high accuracy, then we compare the performance of this ResNet model with its equivalent Convolutional Network (ConvNet). Our findings illustrate that ResNets are more prone to overfitting despite their higher accuracy. Several methods to prevent overfitting such as adding dropout layers and stochastic augmentation of the training dataset has been studied in this work.



### Bridgeout: stochastic bridge regularization for deep neural networks
- **Arxiv ID**: http://arxiv.org/abs/1804.08042v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1804.08042v1)
- **Published**: 2018-04-21 23:27:24+00:00
- **Updated**: 2018-04-21 23:27:24+00:00
- **Authors**: Najeeb Khan, Jawad Shah, Ian Stavness
- **Comment**: None
- **Journal**: None
- **Summary**: A major challenge in training deep neural networks is overfitting, i.e. inferior performance on unseen test examples compared to performance on training examples. To reduce overfitting, stochastic regularization methods have shown superior performance compared to deterministic weight penalties on a number of image recognition tasks. Stochastic methods such as Dropout and Shakeout, in expectation, are equivalent to imposing a ridge and elastic-net penalty on the model parameters, respectively. However, the choice of the norm of weight penalty is problem dependent and is not restricted to $\{L_1,L_2\}$. Therefore, in this paper we propose the Bridgeout stochastic regularization technique and prove that it is equivalent to an $L_q$ penalty on the weights, where the norm $q$ can be learned as a hyperparameter from data. Experimental results show that Bridgeout results in sparse model weights, improved gradients and superior classification performance compared to Dropout and Shakeout on synthetic and real datasets.



### First Impressions: A Survey on Vision-Based Apparent Personality Trait Analysis
- **Arxiv ID**: http://arxiv.org/abs/1804.08046v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.08046v3)
- **Published**: 2018-04-21 23:53:18+00:00
- **Updated**: 2019-07-17 08:48:31+00:00
- **Authors**: Julio C. S. Jacques Junior, Yağmur Güçlütürk, Marc Pérez, Umut Güçlü, Carlos Andujar, Xavier Baró, Hugo Jair Escalante, Isabelle Guyon, Marcel A. J. van Gerven, Rob van Lier, Sergio Escalera
- **Comment**: Accepted on IEEE Transactions on Affective Computing (TAC)
- **Journal**: None
- **Summary**: Personality analysis has been widely studied in psychology, neuropsychology, and signal processing fields, among others. From the past few years, it also became an attractive research area in visual computing. From the computational point of view, by far speech and text have been the most considered cues of information for analyzing personality. However, recently there has been an increasing interest from the computer vision community in analyzing personality from visual data. Recent computer vision approaches are able to accurately analyze human faces, body postures and behaviors, and use these information to infer apparent personality traits. Because of the overwhelming research interest in this topic, and of the potential impact that this sort of methods could have in society, we present in this paper an up-to-date review of existing vision-based approaches for apparent personality trait recognition. We describe seminal and cutting edge works on the subject, discussing and comparing their distinctive features and limitations. Future venues of research in the field are identified and discussed. Furthermore, aspects on the subjectivity in data labeling/evaluation, as well as current datasets and challenges organized to push the research on the field are reviewed.



