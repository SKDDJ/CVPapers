# Arxiv Papers in cs.CV on 2018-04-01
### Recognizing Challenging Handwritten Annotations with Fully Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1804.00236v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1804.00236v2)
- **Published**: 2018-04-01 00:56:02+00:00
- **Updated**: 2018-06-22 12:40:23+00:00
- **Authors**: Andreas KÃ¶lsch, Ashutosh Mishra, Saurabh Varshneya, Muhammad Zeshan Afzal, Marcus Liwicki
- **Comment**: None
- **Journal**: 16th International Conference on Frontiers in Handwriting
  Recognition 2018
- **Summary**: This paper introduces a very challenging dataset of historic German documents and evaluates Fully Convolutional Neural Network (FCNN) based methods to locate handwritten annotations of any kind in these documents. The handwritten annotations can appear in form of underlines and text by using various writing instruments, e.g., the use of pencils makes the data more challenging. We train and evaluate various end-to-end semantic segmentation approaches and report the results. The task is to classify the pixels of documents into two classes: background and handwritten annotation. The best model achieves a mean Intersection over Union (IoU) score of 95.6% on the test documents of the presented dataset. We also present a comparison of different strategies used for data augmentation and training on our presented dataset. For evaluation, we use the Layout Analysis Evaluator for the ICDAR 2017 Competition on Layout Analysis for Challenging Medieval Manuscripts.



### Graph Correspondence Transfer for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1804.00242v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.00242v1)
- **Published**: 2018-04-01 01:39:17+00:00
- **Updated**: 2018-04-01 01:39:17+00:00
- **Authors**: Qin Zhou, Heng Fan, Shibao Zheng, Hang Su, Xinzhe Li, Shuang Wu, Haibin Ling
- **Comment**: Accepted to AAAI'18 (Oral). The code is available at
  http://www.dabi.temple.edu/~hbling/code/gct.htm
- **Journal**: None
- **Summary**: In this paper, we propose a graph correspondence transfer (GCT) approach for person re-identification. Unlike existing methods, the GCT model formulates person re-identification as an off-line graph matching and on-line correspondence transferring problem. In specific, during training, the GCT model aims to learn off-line a set of correspondence templates from positive training pairs with various pose-pair configurations via patch-wise graph matching. During testing, for each pair of test samples, we select a few training pairs with the most similar pose-pair configurations as references, and transfer the correspondences of these references to test pair for feature distance calculation. The matching score is derived by aggregating distances from different references. For each probe image, the gallery image with the highest matching score is the re-identifying result. Compared to existing algorithms, our GCT can handle spatial misalignment caused by large variations in view angles and human poses owing to the benefits of patch-wise graph matching. Extensive experiments on five benchmarks including VIPeR, Road, PRID450S, 3DPES and CUHK01 evidence the superior performance of GCT model over other state-of-the-art methods.



### SampleAhead: Online Classifier-Sampler Communication for Learning from Synthesized Data
- **Arxiv ID**: http://arxiv.org/abs/1804.00248v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.00248v2)
- **Published**: 2018-04-01 02:12:41+00:00
- **Updated**: 2018-07-28 00:42:40+00:00
- **Authors**: Qi Chen, Weichao Qiu, Yi Zhang, Lingxi Xie, Alan Yuille
- **Comment**: BMVC 2018 Oral
- **Journal**: None
- **Summary**: State-of-the-art techniques of artificial intelligence, in particular deep learning, are mostly data-driven. However, collecting and manually labeling a large scale dataset is both difficult and expensive. A promising alternative is to introduce synthesized training data, so that the dataset size can be significantly enlarged with little human labor. But, this raises an important problem in active vision: given an {\bf infinite} data space, how to effectively sample a {\bf finite} subset to train a visual classifier? This paper presents an approach for learning from synthesized data effectively. The motivation is straightforward -- increasing the probability of seeing difficult training data. We introduce a module named {\bf SampleAhead} to formulate the learning process into an online communication between a {\em classifier} and a {\em sampler}, and update them iteratively. In each round, we adjust the sampling distribution according to the classification results, and train the classifier using the data sampled from the updated distribution. Experiments are performed by introducing synthesized images rendered from ShapeNet models to assist PASCAL3D+ classification. Our approach enjoys higher classification accuracy, especially in the scenario of a limited number of training samples. This demonstrates its efficiency in exploring the infinite data space.



### One-Two-One Networks for Compression Artifacts Reduction in Remote Sensing
- **Arxiv ID**: http://arxiv.org/abs/1804.00256v1
- **DOI**: 10.1016/j.isprsjprs.2018.01.003
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.00256v1)
- **Published**: 2018-04-01 04:44:13+00:00
- **Updated**: 2018-04-01 04:44:13+00:00
- **Authors**: Baochang Zhang, Jiaxin Gu, Chen Chen, Jungong Han, Xiangbo Su, Xianbin Cao, Jianzhuang Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Compression artifacts reduction (CAR) is a challenging problem in the field of remote sensing. Most recent deep learning based methods have demonstrated superior performance over the previous hand-crafted methods. In this paper, we propose an end-to-end one-two-one (OTO) network, to combine different deep models, i.e., summation and difference models, to solve the CAR problem. Particularly, the difference model motivated by the Laplacian pyramid is designed to obtain the high frequency information, while the summation model aggregates the low frequency information. We provide an in-depth investigation into our OTO architecture based on the Taylor expansion, which shows that these two kinds of information can be fused in a nonlinear scheme to gain more capacity of handling complicated image compression artifacts, especially the blocking effect in compression. Extensive experiments are conducted to demonstrate the superior performance of the OTO networks, as compared to the state-of-the-arts on remote sensing datasets and other benchmark datasets.



### Real-time Progressive 3D Semantic Segmentation for Indoor Scene
- **Arxiv ID**: http://arxiv.org/abs/1804.00257v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.00257v5)
- **Published**: 2018-04-01 05:09:08+00:00
- **Updated**: 2019-04-05 14:09:02+00:00
- **Authors**: Quang-Hieu Pham, Binh-Son Hua, Duc Thanh Nguyen, Sai-Kit Yeung
- **Comment**: WACV 2019. More information at https://pqhieu.github.io/wacv19.html
- **Journal**: None
- **Summary**: The widespread adoption of autonomous systems such as drones and assistant robots has created a need for real-time high-quality semantic scene segmentation. In this paper, we propose an efficient yet robust technique for on-the-fly dense reconstruction and semantic segmentation of 3D indoor scenes. To guarantee (near) real-time performance, our method is built atop an efficient super-voxel clustering method and a conditional random field with higher-order constraints from structural and object cues, enabling progressive dense semantic segmentation without any precomputation. We extensively evaluate our method on different indoor scenes including kitchens, offices, and bedrooms in the SceneNN and ScanNet datasets and show that our technique consistently produces state-of-the-art segmentation results in both qualitative and quantitative experiments.



### EarthMapper: A Tool Box for the Semantic Segmentation of Remote Sensing Imagery
- **Arxiv ID**: http://arxiv.org/abs/1804.00292v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1804.00292v1)
- **Published**: 2018-04-01 12:44:20+00:00
- **Updated**: 2018-04-01 12:44:20+00:00
- **Authors**: Ronald Kemker, Utsav B. Gewali, Christopher Kanan
- **Comment**: 5 pages
- **Journal**: None
- **Summary**: Deep learning continues to push state-of-the-art performance for the semantic segmentation of color (i.e., RGB) imagery; however, the lack of annotated data for many remote sensing sensors (i.e. hyperspectral imagery (HSI)) prevents researchers from taking advantage of this recent success. Since generating sensor specific datasets is time intensive and cost prohibitive, remote sensing researchers have embraced deep unsupervised feature extraction. Although these methods have pushed state-of-the-art performance on current HSI benchmarks, many of these tools are not readily accessible to many researchers. In this letter, we introduce a software pipeline, which we call EarthMapper, for the semantic segmentation of non-RGB remote sensing imagery. It includes self-taught spatial-spectral feature extraction, various standard and deep learning classifiers, and undirected graphical models for post-processing. We evaluated EarthMapper on the Indian Pines and Pavia University datasets and have released this code for public use.



### Differential Attention for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1804.00298v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.00298v2)
- **Published**: 2018-04-01 13:52:55+00:00
- **Updated**: 2018-04-03 06:30:19+00:00
- **Authors**: Badri Patro, Vinay P. Namboodiri
- **Comment**: Accepted to CVPR 2018
- **Journal**: None
- **Summary**: In this paper we aim to answer questions based on images when provided with a dataset of question-answer pairs for a number of images during training. A number of methods have focused on solving this problem by using image based attention. This is done by focusing on a specific part of the image while answering the question. Humans also do so when solving this problem. However, the regions that the previous systems focus on are not correlated with the regions that humans focus on. The accuracy is limited due to this drawback. In this paper, we propose to solve this problem by using an exemplar based method. We obtain one or more supporting and opposing exemplars to obtain a differential attention region. This differential attention is closer to human attention than other image based attention methods. It also helps in obtaining improved accuracy when answering questions. The method is evaluated on challenging benchmark datasets. We perform better than other image based attention methods and are competitive with other state of the art methods that focus on both image and questions.



### Fully automatic detection and segmentation of abdominal aortic thrombus in post-operative CTA images using deep convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/1804.00304v1
- **DOI**: 10.1016/j.media.2018.03.010
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.00304v1)
- **Published**: 2018-04-01 15:26:32+00:00
- **Updated**: 2018-04-01 15:26:32+00:00
- **Authors**: Karen LÃ³pez-Linares, Nerea Aranjuelo, Luis Kabongo, Gregory Maclair, Nerea Lete, Mario Ceresa, Ainhoa GarcÃ­a-Familiar, IvÃ¡n MacÃ­a, Miguel A. GonzÃ¡lez Ballester
- **Comment**: None
- **Journal**: None
- **Summary**: Computerized Tomography Angiography (CTA) based follow-up of Abdominal Aortic Aneurysms (AAA) treated with Endovascular Aneurysm Repair (EVAR) is essential to evaluate the progress of the patient and detect complications. In this context, accurate quantification of post-operative thrombus volume is required. However, a proper evaluation is hindered by the lack of automatic, robust and reproducible thrombus segmentation algorithms. We propose a new fully automatic approach based on Deep Convolutional Neural Networks (DCNN) for robust and reproducible thrombus region of interest detection and subsequent fine thrombus segmentation. The DetecNet detection network is adapted to perform region of interest extraction from a complete CTA and a new segmentation network architecture, based on Fully Convolutional Networks and a Holistically-Nested Edge Detection Network, is presented. These networks are trained, validated and tested in 13 post-operative CTA volumes of different patients using a 4-fold cross-validation approach to provide more robustness to the results. Our pipeline achieves a Dice score of more than 82% for post-operative thrombus segmentation and provides a mean relative volume difference between ground truth and automatic segmentation that lays within the experienced human observer variance without the need of human intervention in most common cases.



### Robust Fruit Counting: Combining Deep Learning, Tracking, and Structure from Motion
- **Arxiv ID**: http://arxiv.org/abs/1804.00307v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.00307v2)
- **Published**: 2018-04-01 15:44:58+00:00
- **Updated**: 2018-08-02 04:35:07+00:00
- **Authors**: Xu Liu, Steven W. Chen, Shreyas Aditya, Nivedha Sivakumar, Sandeep Dcunha, Chao Qu, Camillo J. Taylor, Jnaneshwar Das, Vijay Kumar
- **Comment**: Accepted in IROS 2018 (2018 IEEE/RSJ International Conference on
  Intelligent Robots and Systems)
- **Journal**: None
- **Summary**: We present a novel fruit counting pipeline that combines deep segmentation, frame to frame tracking, and 3D localization to accurately count visible fruits across a sequence of images. Our pipeline works on image streams from a monocular camera, both in natural light, as well as with controlled illumination at night. We first train a Fully Convolutional Network (FCN) and segment video frame images into fruit and non-fruit pixels. We then track fruits across frames using the Hungarian Algorithm where the objective cost is determined from a Kalman Filter corrected Kanade-Lucas-Tomasi (KLT) Tracker. In order to correct the estimated count from tracking process, we combine tracking results with a Structure from Motion (SfM) algorithm to calculate relative 3D locations and size estimates to reject outliers and double counted fruit tracks. We evaluate our algorithm by comparing with ground-truth human-annotated visual counts. Our results demonstrate that our pipeline is able to accurately and reliably count fruits across image sequences, and the correction step can significantly improve the counting accuracy and robustness. Although discussed in the context of fruit counting, our work can extend to detection, tracking, and counting of a variety of other stationary features of interest such as leaf-spots, wilt, and blossom.



### Seeing Voices and Hearing Faces: Cross-modal biometric matching
- **Arxiv ID**: http://arxiv.org/abs/1804.00326v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.00326v2)
- **Published**: 2018-04-01 18:02:41+00:00
- **Updated**: 2018-04-03 10:55:59+00:00
- **Authors**: Arsha Nagrani, Samuel Albanie, Andrew Zisserman
- **Comment**: To appear in: IEEE Computer Vision and Pattern Recognition (CVPR),
  2018
- **Journal**: None
- **Summary**: We introduce a seemingly impossible task: given only an audio clip of someone speaking, decide which of two face images is the speaker. In this paper we study this, and a number of related cross-modal tasks, aimed at answering the question: how much can we infer from the voice about the face and vice versa? We study this task "in the wild", employing the datasets that are now publicly available for face recognition from static images (VGGFace) and speaker identification from audio (VoxCeleb). These provide training and testing scenarios for both static and dynamic testing of cross-modal matching. We make the following contributions: (i) we introduce CNN architectures for both binary and multi-way cross-modal face and audio matching, (ii) we compare dynamic testing (where video information is available, but the audio is not from the same video) with static testing (where only a single still image is available), and (iii) we use human testing as a baseline to calibrate the difficulty of the task. We show that a CNN can indeed be trained to solve this task in both the static and dynamic scenarios, and is even well above chance on 10-way classification of the face given the voice. The CNN matches human performance on easy examples (e.g. different gender across faces) but exceeds human performance on more challenging examples (e.g. faces with the same gender, age and nationality).



### Unsupervised Correlation Analysis
- **Arxiv ID**: http://arxiv.org/abs/1804.00347v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1804.00347v1)
- **Published**: 2018-04-01 21:14:06+00:00
- **Updated**: 2018-04-01 21:14:06+00:00
- **Authors**: Yedid Hoshen, Lior Wolf
- **Comment**: CVPR 2018
- **Journal**: None
- **Summary**: Linking between two data sources is a basic building block in numerous computer vision problems. In this paper, we set to answer a fundamental cognitive question: are prior correspondences necessary for linking between different domains?   One of the most popular methods for linking between domains is Canonical Correlation Analysis (CCA). All current CCA algorithms require correspondences between the views. We introduce a new method Unsupervised Correlation Analysis (UCA), which requires no prior correspondences between the two domains. The correlation maximization term in CCA is replaced by a combination of a reconstruction term (similar to autoencoders), full cycle loss, orthogonality and multiple domain confusion terms. Due to lack of supervision, the optimization leads to multiple alternative solutions with similar scores and we therefore introduce a consensus-based mechanism that is often able to recover the desired solution. Remarkably, this suffices in order to link remote domains such as text and images. We also present results on well accepted CCA benchmarks, showing that performance far exceeds other unsupervised baselines, and approaches supervised performance in some cases.



### Artificial Intelligence and its Role in Near Future
- **Arxiv ID**: http://arxiv.org/abs/1804.01396v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1804.01396v1)
- **Published**: 2018-04-01 23:12:30+00:00
- **Updated**: 2018-04-01 23:12:30+00:00
- **Authors**: Jahanzaib Shabbir, Tarique Anwer
- **Comment**: None
- **Journal**: None
- **Summary**: AI technology has a long history which is actively and constantly changing and growing. It focuses on intelligent agents, which contain devices that perceive the environment and based on which takes actions in order to maximize goal success chances. In this paper, we will explain the modern AI basics and various representative applications of AI. In the context of the modern digitalized world, AI is the property of machines, computer programs, and systems to perform the intellectual and creative functions of a person, independently find ways to solve problems, be able to draw conclusions and make decisions. Most artificial intelligence systems have the ability to learn, which allows people to improve their performance over time. The recent research on AI tools, including machine learning, deep learning and predictive analysis intended toward increasing the planning, learning, reasoning, thinking and action taking ability. Based on which, the proposed research intends towards exploring on how the human intelligence differs from the artificial intelligence. Moreover, we critically analyze what AI of today is capable of doing, why it still cannot reach human intelligence and what are the open challenges existing in front of AI to reach and outperform human level of intelligence. Furthermore, it will explore the future predictions for artificial intelligence and based on which potential solution will be recommended to solve it within next decades.



