# Arxiv Papers in cs.CV on 2018-04-06
### Performance Evaluation of 3D Correspondence Grouping Algorithms
- **Arxiv ID**: http://arxiv.org/abs/1804.02085v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1804.02085v1)
- **Published**: 2018-04-06 00:10:18+00:00
- **Updated**: 2018-04-06 00:10:18+00:00
- **Authors**: Jiaqi Yang, Ke Xian, Yang Xiao, Zhiguo Cao
- **Comment**: Accepted to 3DV 2017, (Spotlight)
- **Journal**: None
- **Summary**: This paper presents a thorough evaluation of several widely-used 3D correspondence grouping algorithms, motived by their significance in vision tasks relying on correct feature correspondences. A good correspondence grouping algorithm is desired to retrieve as many as inliers from initial feature matches, giving a rise in both precision and recall. Towards this rule, we deploy the experiments on three benchmarks respectively addressing shape retrieval, 3D object recognition and point cloud registration scenarios. The variety in application context brings a rich category of nuisances including noise, varying point densities, clutter, occlusion and partial overlaps. It also results to different ratios of inliers and correspondence distributions for comprehensive evaluation. Based on the quantitative outcomes, we give a summarization of the merits/demerits of the evaluated algorithms from both performance and efficiency perspectives.



### Question Type Guided Attention in Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1804.02088v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.02088v2)
- **Published**: 2018-04-06 00:28:57+00:00
- **Updated**: 2018-07-18 20:43:42+00:00
- **Authors**: Yang Shi, Tommaso Furlanello, Sheng Zha, Animashree Anandkumar
- **Comment**: None
- **Journal**: None
- **Summary**: Visual Question Answering (VQA) requires integration of feature maps with drastically different structures and focus of the correct regions. Image descriptors have structures at multiple spatial scales, while lexical inputs inherently follow a temporal sequence and naturally cluster into semantically different question types. A lot of previous works use complex models to extract feature representations but neglect to use high-level information summary such as question types in learning. In this work, we propose Question Type-guided Attention (QTA). It utilizes the information of question type to dynamically balance between bottom-up and top-down visual features, respectively extracted from ResNet and Faster R-CNN networks. We experiment with multiple VQA architectures with extensive input ablation studies over the TDIUC dataset and show that QTA systematically improves the performance by more than 5% across multiple question type categories such as "Activity Recognition", "Utility" and "Counting" on TDIUC dataset. By adding QTA on the state-of-art model MCB, we achieve 3% improvement for overall accuracy. Finally, we propose a multi-task extension to predict question types which generalizes QTA to applications that lack of question type, with minimal performance loss.



### Impact of ultrasound image reconstruction method on breast lesion classification with neural transfer learning
- **Arxiv ID**: http://arxiv.org/abs/1804.02119v1
- **DOI**: None
- **Categories**: **cs.CV**, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1804.02119v1)
- **Published**: 2018-04-06 03:07:09+00:00
- **Updated**: 2018-04-06 03:07:09+00:00
- **Authors**: Michal Byra, Tomasz Sznajder, Danijel Korzinek, Hanna Piotrzkowska-Wroblewska, Katarzyna Dobruch-Sobczak, Andrzej Nowicki, Krzysztof Marasek
- **Comment**: 6 pages, 5 figures, 3 tables
- **Journal**: None
- **Summary**: Deep learning algorithms, especially convolutional neural networks, have become a methodology of choice in medical image analysis. However, recent studies in computer vision show that even a small modification of input image intensities may cause a deep learning model to classify the image differently. In medical imaging, the distribution of image intensities is related to applied image reconstruction algorithm. In this paper we investigate the impact of ultrasound image reconstruction method on breast lesion classification with neural transfer learning. Due to high dynamic range raw ultrasonic signals are commonly compressed in order to reconstruct B-mode images. Based on raw data acquired from breast lesions, we reconstruct B-mode images using different compression levels. Next, transfer learning is applied for classification. Differently reconstructed images are employed for training and evaluation. We show that the modification of the reconstruction algorithm leads to decrease of classification performance. As a remedy, we propose a method of data augmentation. We show that the augmentation of the training set with differently reconstructed B-mode images leads to a more robust and efficient classification. Our study suggests that it is important to take into account image reconstruction algorithms implemented in medical scanners during development of computer aided diagnosis systems.



### Motion Segmentation by Exploiting Complementary Geometric Models
- **Arxiv ID**: http://arxiv.org/abs/1804.02142v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.02142v1)
- **Published**: 2018-04-06 06:04:45+00:00
- **Updated**: 2018-04-06 06:04:45+00:00
- **Authors**: Xun Xu, Loong-Fah Cheong, Zhuwen Li
- **Comment**: CVPR 2018
- **Journal**: None
- **Summary**: Many real-world sequences cannot be conveniently categorized as general or degenerate; in such cases, imposing a false dichotomy in using the fundamental matrix or homography model for motion segmentation would lead to difficulty. Even when we are confronted with a general scene-motion, the fundamental matrix approach as a model for motion segmentation still suffers from several defects, which we discuss in this paper. The full potential of the fundamental matrix approach could only be realized if we judiciously harness information from the simpler homography model. From these considerations, we propose a multi-view spectral clustering framework that synergistically combines multiple models together. We show that the performance can be substantially improved in this way. We perform extensive testing on existing motion segmentation datasets, achieving state-of-the-art performance on all of them; we also put forth a more realistic and challenging dataset adapted from the KITTI benchmark, containing real-world effects such as strong perspectives and strong forward translations not seen in the traditional datasets.



### Adaptive Quantile Sparse Image (AQuaSI) Prior for Inverse Imaging Problems
- **Arxiv ID**: http://arxiv.org/abs/1804.02152v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.02152v2)
- **Published**: 2018-04-06 07:18:54+00:00
- **Updated**: 2020-02-21 16:18:03+00:00
- **Authors**: Franziska Schirrmacher, Thomas KÃ¶hler, Christian Riess
- **Comment**: None
- **Journal**: None
- **Summary**: Inverse problems play a central role for many classical computer vision and image processing tasks. Many inverse problems are ill-posed, and hence require a prior to regularize the solution space. However, many of the existing priors, like total variation, are based on ad-hoc assumptions that have difficulties to represent the actual distribution of natural images. Thus, a key challenge in research on image processing is to find better suited priors to represent natural images.   In this work, we propose the Adaptive Quantile Sparse Image (AQuaSI) prior. It is based on a quantile filter, can be used as a joint filter on guidance data, and be readily plugged into a wide range of numerical optimization algorithms. We demonstrate the efficacy of the proposed prior in joint RGB/depth upsampling, on RGB/NIR image restoration, and in a comparison with related regularization by denoising approaches.



### OpenSeqSLAM2.0: An Open Source Toolbox for Visual Place Recognition Under Changing Conditions
- **Arxiv ID**: http://arxiv.org/abs/1804.02156v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1804.02156v2)
- **Published**: 2018-04-06 07:37:45+00:00
- **Updated**: 2018-04-11 06:25:05+00:00
- **Authors**: Ben Talbot, Sourav Garg, Michael Milford
- **Comment**: 8 pages, Submitted to IROS 2018 (2018 IEEE/RSJ International
  Conference on Intelligent Robots and Systems), see
  http://www.michaelmilford.com/seqslam for access to the software
- **Journal**: None
- **Summary**: Visually recognising a traversed route - regardless of whether seen during the day or night, in clear or inclement conditions, or in summer or winter - is an important capability for navigating robots. Since SeqSLAM was introduced in 2012, a large body of work has followed exploring how robotic systems can use the algorithm to meet the challenges posed by navigation in changing environmental conditions. The following paper describes OpenSeqSLAM2.0, a fully open source toolbox for visual place recognition under changing conditions. Beyond the benefits of open access to the source code, OpenSeqSLAM2.0 provides a number of tools to facilitate exploration of the visual place recognition problem and interactive parameter tuning. Using the new open source platform, it is shown for the first time how comprehensive parameter characterisations provide new insights into many of the system components previously presented in ad hoc ways and provide users with a guide to what system component options should be used under what circumstances and why.



### Monocular Semantic Occupancy Grid Mapping with Convolutional Variational Encoder-Decoder Networks
- **Arxiv ID**: http://arxiv.org/abs/1804.02176v3
- **DOI**: 10.1109/LRA.2019.2891028
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1804.02176v3)
- **Published**: 2018-04-06 09:38:35+00:00
- **Updated**: 2018-12-31 18:23:05+00:00
- **Authors**: Chenyang Lu, Marinus Jacobus Gerardus van de Molengraft, Gijs Dubbelman
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we research and evaluate end-to-end learning of monocular semantic-metric occupancy grid mapping from weak binocular ground truth. The network learns to predict four classes, as well as a camera to bird's eye view mapping. At the core, it utilizes a variational encoder-decoder network that encodes the front-view visual information of the driving scene and subsequently decodes it into a 2-D top-view Cartesian coordinate system. The evaluations on Cityscapes show that the end-to-end learning of semantic-metric occupancy grids outperforms the deterministic mapping approach with flat-plane assumption by more than 12% mean IoU. Furthermore, we show that the variational sampling with a relatively small embedding vector brings robustness against vehicle dynamic perturbations, and generalizability for unseen KITTI data. Our network achieves real-time inference rates of approx. 35 Hz for an input image with a resolution of 256x512 pixels and an output map with 64x64 occupancy grid cells using a Titan V GPU.



### Learn To Pay Attention
- **Arxiv ID**: http://arxiv.org/abs/1804.02391v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1804.02391v2)
- **Published**: 2018-04-06 10:47:26+00:00
- **Updated**: 2018-04-26 14:33:56+00:00
- **Authors**: Saumya Jetley, Nicholas A. Lord, Namhoon Lee, Philip H. S. Torr
- **Comment**: International Conference on Learning Representations 2018
- **Journal**: None
- **Summary**: We propose an end-to-end-trainable attention module for convolutional neural network (CNN) architectures built for image classification. The module takes as input the 2D feature vector maps which form the intermediate representations of the input image at different stages in the CNN pipeline, and outputs a 2D matrix of scores for each map. Standard CNN architectures are modified through the incorporation of this module, and trained under the constraint that a convex combination of the intermediate 2D feature vectors, as parameterised by the score matrices, must \textit{alone} be used for classification. Incentivised to amplify the relevant and suppress the irrelevant or misleading, the scores thus assume the role of attention values. Our experimental observations provide clear evidence to this effect: the learned attention maps neatly highlight the regions of interest while suppressing background clutter. Consequently, the proposed function is able to bootstrap standard CNN architectures for the task of image classification, demonstrating superior generalisation over 6 unseen benchmark datasets. When binarised, our attention maps outperform other CNN-based attention maps, traditional saliency maps, and top object proposals for weakly supervised segmentation as demonstrated on the Object Discovery dataset. We also demonstrate improved robustness against the fast gradient sign method of adversarial attack.



### Mix and match networks: encoder-decoder alignment for zero-pair image translation
- **Arxiv ID**: http://arxiv.org/abs/1804.02199v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1804.02199v1)
- **Published**: 2018-04-06 10:53:07+00:00
- **Updated**: 2018-04-06 10:53:07+00:00
- **Authors**: Yaxing Wang, Joost van de Weijer, Luis Herranz
- **Comment**: Accepted CVPR 2018
- **Journal**: None
- **Summary**: We address the problem of image translation between domains or modalities for which no direct paired data is available (i.e. zero-pair translation). We propose mix and match networks, based on multiple encoders and decoders aligned in such a way that other encoder-decoder pairs can be composed at test time to perform unseen image translation tasks between domains or modalities for which explicit paired samples were not seen during training. We study the impact of autoencoders, side information and losses in improving the alignment and transferability of trained pairwise translation models to unseen translations. We show our approach is scalable and can perform colorization and style transfer between unseen combinations of domains. We evaluate our system in a challenging cross-modal setting where semantic segmentation is estimated from depth images, without explicit access to any depth-semantic segmentation training pairs. Our model outperforms baselines based on pix2pix and CycleGAN models.



### Ensemble Manifold Segmentation for Model Distillation and Semi-supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/1804.02201v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.02201v1)
- **Published**: 2018-04-06 10:55:16+00:00
- **Updated**: 2018-04-06 10:55:16+00:00
- **Authors**: Dengxin Dai, Wen Li, Till Kroeger, Luc Van Gool
- **Comment**: None
- **Journal**: None
- **Summary**: Manifold theory has been the central concept of many learning methods. However, learning modern CNNs with manifold structures has not raised due attention, mainly because of the inconvenience of imposing manifold structures onto the architecture of the CNNs. In this paper we present ManifoldNet, a novel method to encourage learning of manifold-aware representations. Our approach segments the input manifold into a set of fragments. By assigning the corresponding segmentation id as a pseudo label to every sample, we convert the problem of preserving the local manifold structure into a point-wise classification task. Due to its unsupervised nature, the segmentation tends to be noisy. We mitigate this by introducing ensemble manifold segmentation (EMS). EMS accounts for the manifold structure by dividing the training data into an ensemble of classification training sets that contain samples of local proximity. CNNs are trained on these ensembles under a multi-task learning framework to conform to the manifold. ManifoldNet can be trained with only the pseudo labels or together with task-specific labels. We evaluate ManifoldNet on two different tasks: network imitation (distillation) and semi-supervised learning. Our experiments show that the manifold structures are effectively utilized for both unsupervised and semi-supervised learning.



### Automatic Prediction of Building Age from Photographs
- **Arxiv ID**: http://arxiv.org/abs/1804.02205v2
- **DOI**: 10.1145/3206025.3206060
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.02205v2)
- **Published**: 2018-04-06 11:06:43+00:00
- **Updated**: 2018-04-19 17:45:38+00:00
- **Authors**: Matthias Zeppelzauer, Miroslav Despotovic, Muntaha Sakeena, David Koch, Mario DÃ¶ller
- **Comment**: Preprint of paper to appear in ACM International Conference on
  Multimedia Retrieval (ICMR) 2018 Conference
- **Journal**: None
- **Summary**: We present a first method for the automated age estimation of buildings from unconstrained photographs. To this end, we propose a two-stage approach that firstly learns characteristic visual patterns for different building epochs at patch-level and then globally aggregates patch-level age estimates over the building. We compile evaluation datasets from different sources and perform an detailed evaluation of our approach, its sensitivity to parameters, and the capabilities of the employed deep networks to learn characteristic visual age-related patterns. Results show that our approach is able to estimate building age at a surprisingly high level that even outperforms human evaluators and thereby sets a new performance baseline. This work represents a first step towards the automated assessment of building parameters for automated price prediction.



### Telepresence System based on Simulated Holographic Display
- **Arxiv ID**: http://arxiv.org/abs/1804.02343v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.02343v1)
- **Published**: 2018-04-06 16:14:34+00:00
- **Updated**: 2018-04-06 16:14:34+00:00
- **Authors**: Diana-Margarita CÃ³rdova-Esparza, Juan Terven, Hugo JimÃ©nez-HernÃ¡ndez, Ana Herrera-Navarro, Alberto VÃ¡zquez-Cervantes Juan-M. GarcÃ­a-Huerta
- **Comment**: None
- **Journal**: None
- **Summary**: We present a telepresence system based on a custom-made simulated holographic display that produces a full 3D model of the remote participants using commodity depth sensors. Our display is composed of a video projector and a quadrangular pyramid made of acrylic, that allows the user to experience an omnidirectional visualization of a remote person without the need for head-mounted displays. To obtain a precise representation of the participants, we fuse together multiple views extracted using a deep background subtraction method. Our system represents an attempt to democratize high-fidelity 3D telepresence using off-the-shelf components.



### Cross-Domain Image Matching with Deep Feature Maps
- **Arxiv ID**: http://arxiv.org/abs/1804.02367v2
- **DOI**: 10.1007/s11263-018-01143-3
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.02367v2)
- **Published**: 2018-04-06 17:35:43+00:00
- **Updated**: 2018-10-01 04:16:17+00:00
- **Authors**: Bailey Kong, James Supancic, Deva Ramanan, Charless C. Fowlkes
- **Comment**: None
- **Journal**: None
- **Summary**: We investigate the problem of automatically determining what type of shoe left an impression found at a crime scene. This recognition problem is made difficult by the variability in types of crime scene evidence (ranging from traces of dust or oil on hard surfaces to impressions made in soil) and the lack of comprehensive databases of shoe outsole tread patterns. We find that mid-level features extracted by pre-trained convolutional neural nets are surprisingly effective descriptors for this specialized domains. However, the choice of similarity measure for matching exemplars to a query image is essential to good performance. For matching multi-channel deep features, we propose the use of multi-channel normalized cross-correlation and analyze its effectiveness. Our proposed metric significantly improves performance in matching crime scene shoeprints to laboratory test impressions. We also show its effectiveness in other cross-domain image retrieval problems: matching facade images to segmentation labels and aerial photos to map images. Finally, we introduce a discriminatively trained variant and fine-tune our system through our proposed metric, obtaining state-of-the-art performance.



### EPINET: A Fully-Convolutional Neural Network Using Epipolar Geometry for Depth from Light Field Images
- **Arxiv ID**: http://arxiv.org/abs/1804.02379v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.02379v1)
- **Published**: 2018-04-06 17:52:38+00:00
- **Updated**: 2018-04-06 17:52:38+00:00
- **Authors**: Changha Shin, Hae-Gon Jeon, Youngjin Yoon, In So Kweon, Seon Joo Kim
- **Comment**: Accepted to CVPR 2018, Total 10 pages
- **Journal**: None
- **Summary**: Light field cameras capture both the spatial and the angular properties of light rays in space. Due to its property, one can compute the depth from light fields in uncontrolled lighting environments, which is a big advantage over active sensing devices. Depth computed from light fields can be used for many applications including 3D modelling and refocusing. However, light field images from hand-held cameras have very narrow baselines with noise, making the depth estimation difficult. any approaches have been proposed to overcome these limitations for the light field depth estimation, but there is a clear trade-off between the accuracy and the speed in these methods. In this paper, we introduce a fast and accurate light field depth estimation method based on a fully-convolutional neural network. Our network is designed by considering the light field geometry and we also overcome the lack of training data by proposing light field specific data augmentation methods. We achieved the top rank in the HCI 4D Light Field Benchmark on most metrics, and we also demonstrate the effectiveness of the proposed method on real-world light-field images.



### Image Segmentation Using Subspace Representation and Sparse Decomposition
- **Arxiv ID**: http://arxiv.org/abs/1804.02419v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.02419v1)
- **Published**: 2018-04-06 18:36:53+00:00
- **Updated**: 2018-04-06 18:36:53+00:00
- **Authors**: Shervin Minaee
- **Comment**: PhD Dissertation, NYU, 2018
- **Journal**: None
- **Summary**: Image foreground extraction is a classical problem in image processing and vision, with a large range of applications. In this dissertation, we focus on the extraction of text and graphics in mixed-content images, and design novel approaches for various aspects of this problem.   We first propose a sparse decomposition framework, which models the background by a subspace containing smooth basis vectors, and foreground as a sparse and connected component. We then formulate an optimization framework to solve this problem, by adding suitable regularizations to the cost function to promote the desired characteristics of each component. We present two techniques to solve the proposed optimization problem, one based on alternating direction method of multipliers (ADMM), and the other one based on robust regression. Promising results are obtained for screen content image segmentation using the proposed algorithm.   We then propose a robust subspace learning algorithm for the representation of the background component using training images that could contain both background and foreground components, as well as noise. With the learnt subspace for the background, we can further improve the segmentation results, compared to using a fixed subspace. Lastly, we investigate a different class of signal/image decomposition problem, where only one signal component is active at each signal element. In this case, besides estimating each component, we need to find their supports, which can be specified by a binary mask. We propose a mixed-integer programming problem, that jointly estimates the two components and their supports through an alternating optimization scheme. We show the application of this algorithm on various problems, including image segmentation, video motion segmentation, and also separation of text from textured images.



### Extracting Scientific Figures with Distantly Supervised Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1804.02445v2
- **DOI**: 10.1145/3197026.3197040
- **Categories**: **cs.DL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1804.02445v2)
- **Published**: 2018-04-06 20:22:47+00:00
- **Updated**: 2018-05-30 19:13:53+00:00
- **Authors**: Noah Siegel, Nicholas Lourie, Russell Power, Waleed Ammar
- **Comment**: 10 pages, 5 figures, paper accepted at JCDL 2018
- **Journal**: None
- **Summary**: Non-textual components such as charts, diagrams and tables provide key information in many scientific documents, but the lack of large labeled datasets has impeded the development of data-driven methods for scientific figure extraction. In this paper, we induce high-quality training labels for the task of figure extraction in a large number of scientific documents, with no human intervention. To accomplish this we leverage the auxiliary data provided in two large web collections of scientific documents (arXiv and PubMed) to locate figures and their associated captions in the rasterized PDF. We share the resulting dataset of over 5.5 million induced labels---4,000 times larger than the previous largest figure extraction dataset---with an average precision of 96.8%, to enable the development of modern data-driven methods for this task. We use this dataset to train a deep neural network for end-to-end figure detection, yielding a model that can be more easily extended to new domains compared to previous work. The model was successfully deployed in Semantic Scholar, a large-scale academic search engine, and used to extract figures in 13 million scientific documents.



### Deep Person Detection in 2D Range Data
- **Arxiv ID**: http://arxiv.org/abs/1804.02463v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1804.02463v1)
- **Published**: 2018-04-06 21:39:55+00:00
- **Updated**: 2018-04-06 21:39:55+00:00
- **Authors**: Lucas Beyer, Alexander Hermans, Timm Linder, Kai O. Arras, Bastian Leibe
- **Comment**: None
- **Journal**: None
- **Summary**: Detecting humans is a key skill for mobile robots and intelligent vehicles in a large variety of applications. While the problem is well studied for certain sensory modalities such as image data, few works exist that address this detection task using 2D range data. However, a widespread sensory setup for many mobile robots in service and domestic applications contains a horizontally mounted 2D laser scanner. Detecting people from 2D range data is challenging due to the speed and dynamics of human leg motion and the high levels of occlusion and self-occlusion particularly in crowds of people. While previous approaches mostly relied on handcrafted features, we recently developed the deep learning based wheelchair and walker detector DROW. In this paper, we show the generalization to people, including small modifications that significantly boost DROW's performance. Additionally, by providing a small, fully online temporal window in our network, we further boost our score. We extend the DROW dataset with person annotations, making this the largest dataset of person annotations in 2D range data, recorded during several days in a real-world environment with high diversity. Extensive experiments with three current baseline methods indicate it is a challenging dataset, on which our improved DROW detector beats the current state-of-the-art.



### Visual Tracking Using Sparse Coding and Earth Mover's Distance
- **Arxiv ID**: http://arxiv.org/abs/1804.02470v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1804.02470v1)
- **Published**: 2018-04-06 22:07:47+00:00
- **Updated**: 2018-04-06 22:07:47+00:00
- **Authors**: Gang Yao, Ashwin Dani
- **Comment**: None
- **Journal**: None
- **Summary**: An efficient iterative Earth Mover's Distance (iEMD) algorithm for visual tracking is proposed in this paper. The Earth Mover's Distance (EMD) is used as the similarity measure to search for the optimal template candidates in feature-spatial space in a video sequence. The computation of the EMD is formulated as the transportation problem from linear programming. The efficiency of the EMD optimization problem limits its use for visual tracking. To alleviate this problem, a transportation-simplex method is used for EMD optimization and a monotonically convergent iterative optimization algorithm is developed. The local sparse representation is used as the appearance models for the iEMD tracker. The maximum-alignment-pooling method is used for constructing a sparse coding histogram which reduces the computational complexity of the EMD optimization. The template update algorithm based on the EMD is also presented. The iEMD tracking algorithm assumes small inter-frame movement in order to guarantee convergence. When the camera is mounted on a moving robot, e.g., a flying quadcopter, the camera could experience a sudden and rapid motion leading to large inter-frame movements. To ensure that the tracking algorithm converges, a gyro-aided extension of the iEMD tracker is presented, where synchronized gyroscope information is utilized to compensate for the rotation of the camera. The iEMD algorithm's performance is evaluated using eight publicly available datasets. The performance of the iEMD algorithm is compared with seven state-of-the-art tracking algorithms based on relative percentage overlap. The robustness of this algorithm for large inter-frame displacements is also illustrated.



