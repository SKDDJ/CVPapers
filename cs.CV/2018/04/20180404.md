# Arxiv Papers in cs.CV on 2018-04-04
### An integration of fast alignment and maximum-likelihood methods for electron subtomogram averaging and classification
- **Arxiv ID**: http://arxiv.org/abs/1804.01203v1
- **DOI**: 10.1093/bioinformatics/bty267
- **Categories**: **q-bio.QM**, cs.CV, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/1804.01203v1)
- **Published**: 2018-04-04 01:16:20+00:00
- **Updated**: 2018-04-04 01:16:20+00:00
- **Authors**: Yixiu Zhao, Xiangrui Zeng, Qiang Guo, Min Xu
- **Comment**: 17 pages
- **Journal**: Intelligent Systems for Molecular Biology (ISMB) 2018,
  Bioinformatics
- **Summary**: Motivation: Cellular Electron CryoTomography (CECT) is an emerging 3D imaging technique that visualizes subcellular organization of single cells at submolecular resolution and in near-native state. CECT captures large numbers of macromolecular complexes of highly diverse structures and abundances. However, the structural complexity and imaging limits complicate the systematic de novo structural recovery and recognition of these macromolecular complexes. Efficient and accurate reference-free subtomogram averaging and classification represent the most critical tasks for such analysis. Existing subtomogram alignment based methods are prone to the missing wedge effects and low signal-to-noise ratio (SNR). Moreover, existing maximum-likelihood based methods rely on integration operations, which are in principle computationally infeasible for accurate calculation.   Results: Built on existing works, we propose an integrated method, Fast Alignment Maximum Likelihood method (FAML), which uses fast subtomogram alignment to sample sub-optimal rigid transformations. The transformations are then used to approximate integrals for maximum-likelihood update of subtomogram averages through expectation-maximization algorithm. Our tests on simulated and experimental subtomograms showed that, compared to our previously developed fast alignment method (FA), FAML is significantly more robust to noise and missing wedge effects with moderate increases of computation cost.Besides, FAML performs well with significantly fewer input subtomograms when the FA method fails. Therefore, FAML can serve as a key component for improved construction of initial structural models from macromolecules captured by CECT.



### A Segmentation-aware Deep Fusion Network for Compressed Sensing MRI
- **Arxiv ID**: http://arxiv.org/abs/1804.01210v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.01210v1)
- **Published**: 2018-04-04 02:10:58+00:00
- **Updated**: 2018-04-04 02:10:58+00:00
- **Authors**: Zhiwen Fan, Liyan Sun, Xinghao Ding, Yue Huang, Congbo Cai, John Paisley
- **Comment**: None
- **Journal**: None
- **Summary**: Compressed sensing MRI is a classic inverse problem in the field of computational imaging, accelerating the MR imaging by measuring less k-space data. The deep neural network models provide the stronger representation ability and faster reconstruction compared with "shallow" optimization-based methods. However, in the existing deep-based CS-MRI models, the high-level semantic supervision information from massive segmentation-labels in MRI dataset is overlooked. In this paper, we proposed a segmentation-aware deep fusion network called SADFN for compressed sensing MRI. The multilayer feature aggregation (MLFA) method is introduced here to fuse all the features from different layers in the segmentation network. Then, the aggregated feature maps containing semantic information are provided to each layer in the reconstruction network with a feature fusion strategy. This guarantees the reconstruction network is aware of the different regions in the image it reconstructs, simplifying the function mapping. We prove the utility of the cross-layer and cross-task information fusion strategy by comparative study. Extensive experiments on brain segmentation benchmark MRBrainS validated that the proposed SADFN model achieves state-of-the-art accuracy in compressed sensing MRI. This paper provides a novel approach to guide the low-level visual task using the information from mid- or high-level task.



### Self-Supervised Adversarial Hashing Networks for Cross-Modal Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1804.01223v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.01223v1)
- **Published**: 2018-04-04 03:03:47+00:00
- **Updated**: 2018-04-04 03:03:47+00:00
- **Authors**: Chao Li, Cheng Deng, Ning Li, Wei Liu, Xinbo Gao, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: Thanks to the success of deep learning, cross-modal retrieval has made significant progress recently. However, there still remains a crucial bottleneck: how to bridge the modality gap to further enhance the retrieval accuracy. In this paper, we propose a self-supervised adversarial hashing (\textbf{SSAH}) approach, which lies among the early attempts to incorporate adversarial learning into cross-modal hashing in a self-supervised fashion. The primary contribution of this work is that two adversarial networks are leveraged to maximize the semantic correlation and consistency of the representations between different modalities. In addition, we harness a self-supervised semantic network to discover high-level semantic information in the form of multi-label annotations. Such information guides the feature learning process and preserves the modality relationships in both the common semantic space and the Hamming space. Extensive experiments carried out on three benchmark datasets validate that the proposed SSAH surpasses the state-of-the-art methods.



### Discriminative Cross-View Binary Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/1804.01233v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.01233v1)
- **Published**: 2018-04-04 04:17:52+00:00
- **Updated**: 2018-04-04 04:17:52+00:00
- **Authors**: Liu Liu, Hairong Qi
- **Comment**: Published in WACV2018. Code will be available soon
- **Journal**: WACV2018
- **Summary**: Learning compact representation is vital and challenging for large scale multimedia data. Cross-view/cross-modal hashing for effective binary representation learning has received significant attention with exponentially growing availability of multimedia content. Most existing cross-view hashing algorithms emphasize the similarities in individual views, which are then connected via cross-view similarities. In this work, we focus on the exploitation of the discriminative information from different views, and propose an end-to-end method to learn semantic-preserving and discriminative binary representation, dubbed Discriminative Cross-View Hashing (DCVH), in light of learning multitasking binary representation for various tasks including cross-view retrieval, image-to-image retrieval, and image annotation/tagging. The proposed DCVH has the following key components. First, it uses convolutional neural network (CNN) based nonlinear hashing functions and multilabel classification for both images and texts simultaneously. Such hashing functions achieve effective continuous relaxation during training without explicit quantization loss by using Direct Binary Embedding (DBE) layers. Second, we propose an effective view alignment via Hamming distance minimization, which is efficiently accomplished by bit-wise XOR operation. Extensive experiments on two image-text benchmark datasets demonstrate that DCVH outperforms state-of-the-art cross-view hashing algorithms as well as single-view image hashing algorithms. In addition, DCVH can provide competitive performance for image annotation/tagging.



### Building Efficient CNN Architecture for Offline Handwritten Chinese Character Recognition
- **Arxiv ID**: http://arxiv.org/abs/1804.01259v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.01259v2)
- **Published**: 2018-04-04 06:58:03+00:00
- **Updated**: 2018-04-08 02:11:01+00:00
- **Authors**: Zhiyuan Li, Nanjun Teng, Min Jin, Huaxiang Lu
- **Comment**: 7 pages, 2 figures
- **Journal**: None
- **Summary**: Deep convolutional networks based methods have brought great breakthrough in images classification, which provides an end-to-end solution for handwritten Chinese character recognition(HCCR) problem through learning discriminative features automatically. Nevertheless, state-of-the-art CNNs appear to incur huge computation cost, and require the storage of a large number of parameters especially in fully connected layers, which is difficult to deploy such networks into alternative hardware device with the limit of computation amount. To solve the storage problem, we propose a novel technique called Global Weighted Arverage Pooling for reducing the parameters in fully connected layer without loss in accuracy. Besides, we implement a cascaded model in single CNN by adding mid output layer to complete recognition as early as possible, which reduces average inference time significantly. Experiments were performed on the ICDAR-2013 offline HCCR dataset, and it is found that the proposed approach only needs 6.9ms for classfying a chracter image on average, and achieves the state-of-the-art accuracy of 97.1% while requiring only 3.3MB for storage.



### Gaussian Process Uncertainty in Age Estimation as a Measure of Brain Abnormality
- **Arxiv ID**: http://arxiv.org/abs/1804.01296v1
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.NC, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1804.01296v1)
- **Published**: 2018-04-04 08:38:07+00:00
- **Updated**: 2018-04-04 08:38:07+00:00
- **Authors**: Benjamin Gutierrez Becker, Tassilo Klein, Christian Wachinger
- **Comment**: Paper accepted in Neuroimage
- **Journal**: None
- **Summary**: Multivariate regression models for age estimation are a powerful tool for assessing abnormal brain morphology associated to neuropathology. Age prediction models are built on cohorts of healthy subjects and are built to reflect normal aging patterns. The application of these multivariate models to diseased subjects usually results in high prediction errors, under the hypothesis that neuropathology presents a similar degenerative pattern as that of accelerated aging. In this work, we propose an alternative to the idea that pathology follows a similar trajectory than normal aging. Instead, we propose the use of metrics which measure deviations from the mean aging trajectory. We propose to measure these deviations using two different metrics: uncertainty in a Gaussian process regression model and a newly proposed age weighted uncertainty measure. Consequently, our approach assumes that pathologic brain patterns are different to those of normal aging. We present results for subjects with autism, mild cognitive impairment and Alzheimer's disease to highlight the versatility of the approach to different diseases and age ranges. We evaluate volume, thickness, and VBM features for quantifying brain morphology. Our evaluations are performed on a large number of images obtained from a variety of publicly available neuroimaging databases. Across all features, our uncertainty based measurements yield a better separation between diseased subjects and healthy individuals than the prediction error. Finally, we illustrate differences in the disease pattern to normal aging, supporting the application of uncertainty as a measure of neuropathology.



### A Unifying Contrast Maximization Framework for Event Cameras, with Applications to Motion, Depth, and Optical Flow Estimation
- **Arxiv ID**: http://arxiv.org/abs/1804.01306v1
- **DOI**: 10.1109/CVPR.2018.00407
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1804.01306v1)
- **Published**: 2018-04-04 08:59:57+00:00
- **Updated**: 2018-04-04 08:59:57+00:00
- **Authors**: Guillermo Gallego, Henri Rebecq, Davide Scaramuzza
- **Comment**: 16 pages, 16 figures. Video: https://youtu.be/KFMZFhi-9Aw
- **Journal**: IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
  Salt Lake City, 2018
- **Summary**: We present a unifying framework to solve several computer vision problems with event cameras: motion, depth and optical flow estimation. The main idea of our framework is to find the point trajectories on the image plane that are best aligned with the event data by maximizing an objective function: the contrast of an image of warped events. Our method implicitly handles data association between the events, and therefore, does not rely on additional appearance information about the scene. In addition to accurately recovering the motion parameters of the problem, our framework produces motion-corrected edge-like images with high dynamic range that can be used for further scene analysis. The proposed method is not only simple, but more importantly, it is, to the best of our knowledge, the first method that can be successfully applied to such a diverse set of important vision tasks with event cameras.



### Btrfly Net: Vertebrae Labelling with Energy-based Adversarial Learning of Local Spine Prior
- **Arxiv ID**: http://arxiv.org/abs/1804.01307v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.01307v2)
- **Published**: 2018-04-04 09:00:33+00:00
- **Updated**: 2018-11-13 16:20:52+00:00
- **Authors**: Anjany Sekuboyina, Markus Rempfler, Jan Kukačka, Giles Tetteh, Alexander Valentinitsch, Jan S. Kirschke, Bjoern H. Menze
- **Comment**: Published as conference paper in Medical Image Computing and Computer
  Assisted Intervention - MICCAI 2018
- **Journal**: None
- **Summary**: Robust localisation and identification of vertebrae is essential for automated spine analysis. The contribution of this work to the task is two-fold: (1) Inspired by the human expert, we hypothesise that a sagittal and coronal reformation of the spine contain sufficient information for labelling the vertebrae. Thereby, we propose a butterfly-shaped network architecture (termed Btrfly Net) that efficiently combines the information across reformations. (2) Underpinning the Btrfly net, we present an energy-based adversarial training regime that encodes local spine structure as an anatomical prior into the network, thereby enabling it to achieve state-of-art performance in all standard metrics on a benchmark dataset of 302 scans without any post-processing during inference.



### Event-based Vision meets Deep Learning on Steering Prediction for Self-driving Cars
- **Arxiv ID**: http://arxiv.org/abs/1804.01310v1
- **DOI**: 10.1109/CVPR.2018.00568
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1804.01310v1)
- **Published**: 2018-04-04 09:05:41+00:00
- **Updated**: 2018-04-04 09:05:41+00:00
- **Authors**: Ana I. Maqueda, Antonio Loquercio, Guillermo Gallego, Narciso Garcia, Davide Scaramuzza
- **Comment**: 9 pages, 8 figures, 6 tables. Video: https://youtu.be/_r_bsjkJTHA
- **Journal**: IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
  Salt Lake City, 2018
- **Summary**: Event cameras are bio-inspired vision sensors that naturally capture the dynamics of a scene, filtering out redundant information. This paper presents a deep neural network approach that unlocks the potential of event cameras on a challenging motion-estimation task: prediction of a vehicle's steering angle. To make the best out of this sensor-algorithm combination, we adapt state-of-the-art convolutional architectures to the output of event sensors and extensively evaluate the performance of our approach on a publicly available large scale event-camera dataset (~1000 km). We present qualitative and quantitative explanations of why event cameras allow robust steering prediction even in cases where traditional cameras fail, e.g. challenging illumination conditions and fast motion. Finally, we demonstrate the advantages of leveraging transfer learning from traditional to event-based vision, and show that our approach outperforms state-of-the-art algorithms based on standard cameras.



### A Multi-Stage Multi-Task Neural Network for Aerial Scene Interpretation and Geolocalization
- **Arxiv ID**: http://arxiv.org/abs/1804.01322v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.01322v1)
- **Published**: 2018-04-04 09:42:47+00:00
- **Updated**: 2018-04-04 09:42:47+00:00
- **Authors**: Alina Marcu, Dragos Costea, Emil Slusanschi, Marius Leordeanu
- **Comment**: 23 pages, 11 figures. Under review at the 15th European Conference on
  Computer Vision (ECCV 2018)
- **Journal**: None
- **Summary**: Semantic segmentation and vision-based geolocalization in aerial images are challenging tasks in computer vision. Due to the advent of deep convolutional nets and the availability of relatively low cost UAVs, they are currently generating a growing attention in the field. We propose a novel multi-task multi-stage neural network that is able to handle the two problems at the same time, in a single forward pass. The first stage of our network predicts pixelwise class labels, while the second stage provides a precise location using two branches. One branch uses a regression network, while the other is used to predict a location map trained as a segmentation task. From a structural point of view, our architecture uses encoder-decoder modules at each stage, having the same encoder structure re-used. Furthermore, its size is limited to be tractable on an embedded GPU. We achieve commercial GPS-level localization accuracy from satellite images with spatial resolution of 1 square meter per pixel in a city-wide area of interest. On the task of semantic segmentation, we obtain state-of-the-art results on two challenging datasets, the Inria Aerial Image Labeling dataset and Massachusetts Buildings.



### Generative Visual Rationales
- **Arxiv ID**: http://arxiv.org/abs/1804.04539v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.04539v1)
- **Published**: 2018-04-04 10:00:24+00:00
- **Updated**: 2018-04-04 10:00:24+00:00
- **Authors**: Jarrel Seah, Jennifer Tang, Andy Kitchen, Jonathan Seah
- **Comment**: None
- **Journal**: None
- **Summary**: Interpretability and small labelled datasets are key issues in the practical application of deep learning, particularly in areas such as medicine. In this paper, we present a semi-supervised technique that addresses both these issues by leveraging large unlabelled datasets to encode and decode images into a dense latent representation. Using chest radiography as an example, we apply this encoder to other labelled datasets and apply simple models to the latent vectors to learn algorithms to identify heart failure.   For each prediction, we generate visual rationales by optimizing a latent representation to minimize the prediction of disease while constrained by a similarity measure in image space. Decoding the resultant latent representation produces an image without apparent disease. The difference between the original decoding and the altered image forms an interpretable visual rationale for the algorithm's prediction on that image. We also apply our method to the MNIST dataset and compare the generated rationales to other techniques described in the literature.



### Normalized Cut Loss for Weakly-supervised CNN Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1804.01346v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.01346v1)
- **Published**: 2018-04-04 11:18:21+00:00
- **Updated**: 2018-04-04 11:18:21+00:00
- **Authors**: Meng Tang, Abdelaziz Djelouah, Federico Perazzi, Yuri Boykov, Christopher Schroers
- **Comment**: Accepted at CVPR 2018
- **Journal**: None
- **Summary**: Most recent semantic segmentation methods train deep convolutional neural networks with fully annotated masks requiring pixel-accuracy for good quality training. Common weakly-supervised approaches generate full masks from partial input (e.g. scribbles or seeds) using standard interactive segmentation methods as preprocessing. But, errors in such masks result in poorer training since standard loss functions (e.g. cross-entropy) do not distinguish seeds from potentially mislabeled other pixels. Inspired by the general ideas in semi-supervised learning, we address these problems via a new principled loss function evaluating network output with criteria standard in "shallow" segmentation, e.g. normalized cut. Unlike prior work, the cross entropy part of our loss evaluates only seeds where labels are known while normalized cut softly evaluates consistency of all pixels. We focus on normalized cut loss where dense Gaussian kernel is efficiently implemented in linear time by fast Bilateral filtering. Our normalized cut loss approach to segmentation brings the quality of weakly-supervised training significantly closer to fully supervised methods.



### Boosting Handwriting Text Recognition in Small Databases with Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/1804.01527v1
- **DOI**: 10.1109/ICFHR-2018.2018.00081
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1804.01527v1)
- **Published**: 2018-04-04 11:20:28+00:00
- **Updated**: 2018-04-04 11:20:28+00:00
- **Authors**: José Carlos Aradillas, Juan José Murillo-Fuentes, Pablo M. Olmos
- **Comment**: ICFHR 2018 Conference
- **Journal**: None
- **Summary**: In this paper we deal with the offline handwriting text recognition (HTR) problem with reduced training datasets. Recent HTR solutions based on artificial neural networks exhibit remarkable solutions in referenced databases. These deep learning neural networks are composed of both convolutional (CNN) and long short-term memory recurrent units (LSTM). In addition, connectionist temporal classification (CTC) is the key to avoid segmentation at character level, greatly facilitating the labeling task. One of the main drawbacks of the CNNLSTM-CTC (CLC) solutions is that they need a considerable part of the text to be transcribed for every type of calligraphy, typically in the order of a few thousands of lines. Furthermore, in some scenarios the text to transcribe is not that long, e.g. in the Washington database. The CLC typically overfits for this reduced number of training samples. Our proposal is based on the transfer learning (TL) from the parameters learned with a bigger database. We first investigate, for a reduced and fixed number of training samples, 350 lines, how the learning from a large database, the IAM, can be transferred to the learning of the CLC of a reduced database, Washington. We focus on which layers of the network could be not re-trained. We conclude that the best solution is to re-train the whole CLC parameters initialized to the values obtained after the training of the CLC from the larger database. We also investigate results when the training size is further reduced. The differences in the CER are more remarkable when training with just 350 lines, a CER of 3.3% is achieved with TL while we have a CER of 18.2% when training from scratch. As a byproduct, the learning times are quite reduced. Similar good results are obtained from the Parzival database when trained with this reduced number of lines and this new approach.



### Fine-grained Video Attractiveness Prediction Using Multimodal Deep Learning on a Large Real-world Dataset
- **Arxiv ID**: http://arxiv.org/abs/1804.01373v2
- **DOI**: 10.1145/3184558.3186584
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.01373v2)
- **Published**: 2018-04-04 12:44:43+00:00
- **Updated**: 2018-04-07 01:45:29+00:00
- **Authors**: Xinpeng Chen, Jingyuan Chen, Lin Ma, Jian Yao, Wei Liu, Jiebo Luo, Tong Zhang
- **Comment**: Accepted by WWW 2018 The Big Web Track
- **Journal**: None
- **Summary**: Nowadays, billions of videos are online ready to be viewed and shared. Among an enormous volume of videos, some popular ones are widely viewed by online users while the majority attract little attention. Furthermore, within each video, different segments may attract significantly different numbers of views. This phenomenon leads to a challenging yet important problem, namely fine-grained video attractiveness prediction. However, one major obstacle for such a challenging problem is that no suitable benchmark dataset currently exists. To this end, we construct the first fine-grained video attractiveness dataset, which is collected from one of the most popular video websites in the world. In total, the constructed FVAD consists of 1,019 drama episodes with 780.6 hours covering different categories and a wide variety of video contents. Apart from the large amount of videos, hundreds of millions of user behaviors during watching videos are also included, such as "view counts", "fast-forward", "fast-rewind", and so on, where "view counts" reflects the video attractiveness while other engagements capture the interactions between the viewers and videos. First, we demonstrate that video attractiveness and different engagements present different relationships. Second, FVAD provides us an opportunity to study the fine-grained video attractiveness prediction problem. We design different sequential models to perform video attractiveness prediction by relying solely on video contents. The sequential models exploit the multimodal relationships between visual and audio components of the video contents at different levels. Experimental results demonstrate the effectiveness of our proposed sequential models with different visual and audio representations, the necessity of incorporating the two modalities, and the complementary behaviors of the sequential prediction models at different levels.



### SketchMate: Deep Hashing for Million-Scale Human Sketch Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1804.01401v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.01401v1)
- **Published**: 2018-04-04 13:39:26+00:00
- **Updated**: 2018-04-04 13:39:26+00:00
- **Authors**: Peng Xu, Yongye Huang, Tongtong Yuan, Kaiyue Pang, Yi-Zhe Song, Tao Xiang, Timothy M. Hospedales, Zhanyu Ma, Jun Guo
- **Comment**: Accepted by CVPR2018
- **Journal**: None
- **Summary**: We propose a deep hashing framework for sketch retrieval that, for the first time, works on a multi-million scale human sketch dataset. Leveraging on this large dataset, we explore a few sketch-specific traits that were otherwise under-studied in prior literature. Instead of following the conventional sketch recognition task, we introduce the novel problem of sketch hashing retrieval which is not only more challenging, but also offers a better testbed for large-scale sketch analysis, since: (i) more fine-grained sketch feature learning is required to accommodate the large variations in style and abstraction, and (ii) a compact binary code needs to be learned at the same time to enable efficient retrieval. Key to our network design is the embedding of unique characteristics of human sketch, where (i) a two-branch CNN-RNN architecture is adapted to explore the temporal ordering of strokes, and (ii) a novel hashing loss is specifically designed to accommodate both the temporal and abstract traits of sketches. By working with a 3.8M sketch dataset, we show that state-of-the-art hashing models specifically engineered for static images fail to perform well on temporal sketch data. Our network on the other hand not only offers the best retrieval performance on various code sizes, but also yields the best generalization performance under a zero-shot setting and when re-purposed for sketch recognition. Such superior performances effectively demonstrate the benefit of our sketch-specific design.



### Layout-induced Video Representation for Recognizing Agent-in-Place Actions
- **Arxiv ID**: http://arxiv.org/abs/1804.01429v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.01429v3)
- **Published**: 2018-04-04 14:25:04+00:00
- **Updated**: 2019-04-01 04:46:05+00:00
- **Authors**: Ruichi Yu, Hongcheng Wang, Ang Li, Jingxiao Zheng, Vlad I. Morariu, Larry S. Davis
- **Comment**: None
- **Journal**: None
- **Summary**: We address the recognition of agent-in-place actions, which are associated with agents who perform them and places where they occur, in the context of outdoor home surveillance. We introduce a representation of the geometry and topology of scene layouts so that a network can generalize from the layouts observed in the training set to unseen layouts in the test set. This Layout-Induced Video Representation (LIVR) abstracts away low-level appearance variance and encodes geometric and topological relationships of places in a specific scene layout. LIVR partitions the semantic features of a video clip into different places to force the network to learn place-based feature descriptions; to predict the confidence of each action, LIVR aggregates features from the place associated with an action and its adjacent places on the scene layout. We introduce the Agent-in-Place Action dataset to show that our method allows neural network models to generalize significantly better to unseen scenes.



### Learning Discriminative Features with Multiple Granularities for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1804.01438v3
- **DOI**: 10.1145/3240508.3240552
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.01438v3)
- **Published**: 2018-04-04 14:36:01+00:00
- **Updated**: 2018-08-14 06:43:29+00:00
- **Authors**: Guanshuo Wang, Yufeng Yuan, Xiong Chen, Jiwei Li, Xi Zhou
- **Comment**: 9 pages, 5 figures. To appear in ACM Multimedia 2018
- **Journal**: None
- **Summary**: The combination of global and partial features has been an essential solution to improve discriminative performances in person re-identification (Re-ID) tasks. Previous part-based methods mainly focus on locating regions with specific pre-defined semantics to learn local representations, which increases learning difficulty but not efficient or robust to scenarios with large variances. In this paper, we propose an end-to-end feature learning strategy integrating discriminative information with various granularities. We carefully design the Multiple Granularity Network (MGN), a multi-branch deep network architecture consisting of one branch for global feature representations and two branches for local feature representations. Instead of learning on semantic regions, we uniformly partition the images into several stripes, and vary the number of parts in different local branches to obtain local feature representations with multiple granularities. Comprehensive experiments implemented on the mainstream evaluation datasets including Market-1501, DukeMTMC-reid and CUHK03 indicate that our method has robustly achieved state-of-the-art performances and outperformed any existing approaches by a large margin. For example, on Market-1501 dataset in single query mode, we achieve a state-of-the-art result of Rank-1/mAP=96.6%/94.2% after re-ranking.



### Improving Classification Rate of Schizophrenia Using a Multimodal Multi-Layer Perceptron Model with Structural and Functional MR
- **Arxiv ID**: http://arxiv.org/abs/1804.04591v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.04591v1)
- **Published**: 2018-04-04 14:46:10+00:00
- **Updated**: 2018-04-04 14:46:10+00:00
- **Authors**: Alvaro Ulloa, Sergey Plis, Vince Calhoun
- **Comment**: None
- **Journal**: None
- **Summary**: The wide variety of brain imaging technologies allows us to exploit information inherent to different data modalities. The richness of multimodal datasets may increase predictive power and reveal latent variables that otherwise would have not been found. However, the analysis of multimodal data is often conducted by assuming linear interactions which impact the accuracy of the results. We propose the use of a multimodal multi-layer perceptron model to enhance the predictive power of structural and functional magnetic resonance imaging (sMRI and fMRI) combined.   We also use a synthetic data generator to pre-train each modality input layers, alleviating the effects of the small sample size that is often the case for brain imaging modalities. The proposed model improved the average and uncertainty of the area under the ROC curve to 0.850+-0.051 compared to the best results on individual modalities (0.741+-0.075 for sMRI, and 0.833+-0.050 for fMRI).



### Jointly Discovering Visual Objects and Spoken Words from Raw Sensory Input
- **Arxiv ID**: http://arxiv.org/abs/1804.01452v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/1804.01452v1)
- **Published**: 2018-04-04 15:03:08+00:00
- **Updated**: 2018-04-04 15:03:08+00:00
- **Authors**: David Harwath, Adrià Recasens, Dídac Surís, Galen Chuang, Antonio Torralba, James Glass
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we explore neural network models that learn to associate segments of spoken audio captions with the semantically relevant portions of natural images that they refer to. We demonstrate that these audio-visual associative localizations emerge from network-internal representations learned as a by-product of training to perform an image-audio retrieval task. Our models operate directly on the image pixels and speech waveform, and do not rely on any conventional supervision in the form of labels, segmentations, or alignments between the modalities during training. We perform analysis using the Places 205 and ADE20k datasets demonstrating that our models implicitly learn semantically-coupled object and word detectors.



### Density Adaptive Point Set Registration
- **Arxiv ID**: http://arxiv.org/abs/1804.01495v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.01495v2)
- **Published**: 2018-04-04 16:28:50+00:00
- **Updated**: 2018-10-23 08:48:09+00:00
- **Authors**: Felix Järemo Lawin, Martin Danelljan, Fahad Shahbaz Khan, Per-Erik Forssén, Michael Felsberg
- **Comment**: CVPR 2018 (Oral)
- **Journal**: The IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR) 2018
- **Summary**: Probabilistic methods for point set registration have demonstrated competitive results in recent years. These techniques estimate a probability distribution model of the point clouds. While such a representation has shown promise, it is highly sensitive to variations in the density of 3D points. This fundamental problem is primarily caused by changes in the sensor location across point sets. We revisit the foundations of the probabilistic registration paradigm. Contrary to previous works, we model the underlying structure of the scene as a latent probability distribution, and thereby induce invariance to point set density changes. Both the probabilistic model of the scene and the registration parameters are inferred by minimizing the Kullback-Leibler divergence in an Expectation Maximization based framework. Our density-adaptive registration successfully handles severe density variations commonly encountered in terrestrial Lidar applications. We perform extensive experiments on several challenging real-world Lidar datasets. The results demonstrate that our approach outperforms state-of-the-art probabilistic methods for multi-view registration, without the need of re-sampling. Code is available at https://github.com/felja633/DARE.



### The Tsetlin Machine -- A Game Theoretic Bandit Driven Approach to Optimal Pattern Recognition with Propositional Logic
- **Arxiv ID**: http://arxiv.org/abs/1804.01508v15
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1804.01508v15)
- **Published**: 2018-04-04 16:52:34+00:00
- **Updated**: 2021-01-02 00:51:08+00:00
- **Authors**: Ole-Christoffer Granmo
- **Comment**: 42 pages, 14 figures, further formalizing key concepts
- **Journal**: None
- **Summary**: Although simple individually, artificial neurons provide state-of-the-art performance when interconnected in deep networks. Arguably, the Tsetlin Automaton is an even simpler and more versatile learning mechanism, capable of solving the multi-armed bandit problem. Merely by means of a single integer as memory, it learns the optimal action in stochastic environments through increment and decrement operations. In this paper, we introduce the Tsetlin Machine, which solves complex pattern recognition problems with propositional formulas, composed by a collective of Tsetlin Automata. To eliminate the longstanding problem of vanishing signal-to-noise ratio, the Tsetlin Machine orchestrates the automata using a novel game. Further, both inputs, patterns, and outputs are expressed as bits, while recognition and learning rely on bit manipulation, simplifying computation. Our theoretical analysis establishes that the Nash equilibria of the game align with the propositional formulas that provide optimal pattern recognition accuracy. This translates to learning without local optima, only global ones. In five benchmarks, the Tsetlin Machine provides competitive accuracy compared with SVMs, Decision Trees, Random Forests, Naive Bayes Classifier, Logistic Regression, and Neural Networks. We further demonstrate how the propositional formulas facilitate interpretation. In conclusion, we believe the combination of high accuracy, interpretability, and computational simplicity makes the Tsetlin Machine a promising tool for a wide range of domains.



### Stochastic Adversarial Video Prediction
- **Arxiv ID**: http://arxiv.org/abs/1804.01523v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1804.01523v1)
- **Published**: 2018-04-04 17:55:40+00:00
- **Updated**: 2018-04-04 17:55:40+00:00
- **Authors**: Alex X. Lee, Richard Zhang, Frederik Ebert, Pieter Abbeel, Chelsea Finn, Sergey Levine
- **Comment**: Website: https://alexlee-gk.github.io/video_prediction/
- **Journal**: None
- **Summary**: Being able to predict what may happen in the future requires an in-depth understanding of the physical and causal rules that govern the world. A model that is able to do so has a number of appealing applications, from robotic planning to representation learning. However, learning to predict raw future observations, such as frames in a video, is exceedingly challenging -- the ambiguous nature of the problem can cause a naively designed model to average together possible futures into a single, blurry prediction. Recently, this has been addressed by two distinct approaches: (a) latent variational variable models that explicitly model underlying stochasticity and (b) adversarially-trained models that aim to produce naturalistic images. However, a standard latent variable model can struggle to produce realistic results, and a standard adversarially-trained model underutilizes latent variables and fails to produce diverse predictions. We show that these distinct methods are in fact complementary. Combining the two produces predictions that look more realistic to human raters and better cover the range of possible futures. Our method outperforms prior and concurrent work in these aspects.



### Self-supervised Learning of Geometrically Stable Features Through Probabilistic Introspection
- **Arxiv ID**: http://arxiv.org/abs/1804.01552v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.01552v1)
- **Published**: 2018-04-04 18:15:17+00:00
- **Updated**: 2018-04-04 18:15:17+00:00
- **Authors**: David Novotny, Samuel Albanie, Diane Larlus, Andrea Vedaldi
- **Comment**: In 2018 IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR 2018)
- **Journal**: None
- **Summary**: Self-supervision can dramatically cut back the amount of manually-labelled data required to train deep neural networks. While self-supervision has usually been considered for tasks such as image classification, in this paper we aim at extending it to geometry-oriented tasks such as semantic matching and part detection. We do so by building on several recent ideas in unsupervised landmark detection. Our approach learns dense distinctive visual descriptors from an unlabelled dataset of images using synthetic image transformations. It does so by means of a robust probabilistic formulation that can introspectively determine which image regions are likely to result in stable image matching. We show empirically that a network pre-trained in this manner requires significantly less supervision to learn semantic object parts compared to numerous pre-training alternatives. We also show that the pre-trained representation is excellent for semantic object matching.



### Semi-Supervised Deep Metrics for Image Registration
- **Arxiv ID**: http://arxiv.org/abs/1804.01565v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.01565v1)
- **Published**: 2018-04-04 18:51:51+00:00
- **Updated**: 2018-04-04 18:51:51+00:00
- **Authors**: Alireza Sedghi, Jie Luo, Alireza Mehrtash, Steve Pieper, Clare M. Tempany, Tina Kapur, Parvin Mousavi, William M. Wells III
- **Comment**: Under Review for MICCAI 2018
- **Journal**: None
- **Summary**: Deep metrics have been shown effective as similarity measures in multi-modal image registration; however, the metrics are currently constructed from aligned image pairs in the training data. In this paper, we propose a strategy for learning such metrics from roughly aligned training data. Symmetrizing the data corrects bias in the metric that results from misalignment in the data (at the expense of increased variance), while random perturbations to the data, i.e. dithering, ensures that the metric has a single mode, and is amenable to registration by optimization. Evaluation is performed on the task of registration on separate unseen test image pairs. The results demonstrate the feasibility of learning a useful deep metric from substantially misaligned training data, in some cases the results are significantly better than from Mutual Information. Data augmentation via dithering is, therefore, an effective strategy for discharging the need for well-aligned training data; this brings deep metric registration from the realm of supervised to semi-supervised machine learning.



### Accelerated Optimization in the PDE Framework: Formulations for the Manifold of Diffeomorphisms
- **Arxiv ID**: http://arxiv.org/abs/1804.02307v2
- **DOI**: None
- **Categories**: **math.OC**, cs.CV, cs.NA
- **Links**: [PDF](http://arxiv.org/pdf/1804.02307v2)
- **Published**: 2018-04-04 19:58:03+00:00
- **Updated**: 2018-05-23 21:38:57+00:00
- **Authors**: Ganesh Sundaramoorthi, Anthony Yezzi
- **Comment**: None
- **Journal**: None
- **Summary**: We consider the problem of optimization of cost functionals on the infinite-dimensional manifold of diffeomorphisms. We present a new class of optimization methods, valid for any optimization problem setup on the space of diffeomorphisms by generalizing Nesterov accelerated optimization to the manifold of diffeomorphisms. While our framework is general for infinite dimensional manifolds, we specifically treat the case of diffeomorphisms, motivated by optical flow problems in computer vision. This is accomplished by building on a recent variational approach to a general class of accelerated optimization methods by Wibisono, Wilson and Jordan, which applies in finite dimensions. We generalize that approach to infinite dimensional manifolds. We derive the surprisingly simple continuum evolution equations, which are partial differential equations, for accelerated gradient descent, and relate it to simple mechanical principles from fluid mechanics. Our approach has natural connections to the optimal mass transport problem. This is because one can think of our approach as an evolution of an infinite number of particles endowed with mass (represented with a mass density) that moves in an energy landscape. The mass evolves with the optimization variable, and endows the particles with dynamics. This is different than the finite dimensional case where only a single particle moves and hence the dynamics does not depend on the mass. We derive the theory, compute the PDEs for accelerated optimization, and illustrate the behavior of these new accelerated optimization schemes.



### StainGAN: Stain Style Transfer for Digital Histological Images
- **Arxiv ID**: http://arxiv.org/abs/1804.01601v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.01601v1)
- **Published**: 2018-04-04 20:34:53+00:00
- **Updated**: 2018-04-04 20:34:53+00:00
- **Authors**: M Tarek Shaban, Christoph Baur, Nassir Navab, Shadi Albarqouni
- **Comment**: Submitted to MICCAI 2018
- **Journal**: None
- **Summary**: Digitized Histological diagnosis is in increasing demand. However, color variations due to various factors are imposing obstacles to the diagnosis process. The problem of stain color variations is a well-defined problem with many proposed solutions. Most of these solutions are highly dependent on a reference template slide. We propose a deep-learning solution inspired by CycleGANs that is trained end-to-end, eliminating the need for an expert to pick a representative reference slide. Our approach showed superior results quantitatively and qualitatively against the state of the art methods (10% improvement visually using SSIM). We further validated our method on a clinical use-case, namely Breast Cancer tumor classification, showing 12% increase in AUC. The code will be made publicly available.



### Learnable Exposure Fusion for Dynamic Scenes
- **Arxiv ID**: http://arxiv.org/abs/1804.01611v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.01611v1)
- **Published**: 2018-04-04 21:36:31+00:00
- **Updated**: 2018-04-04 21:36:31+00:00
- **Authors**: Fahd Bouzaraa, Ibrahim Halfaoui, Onay Urfalioglu
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we focus on Exposure Fusion (EF) [ExposFusi2] for dynamic scenes. The task is to fuse multiple images obtained by exposure bracketing to create an image which comprises a high level of details. Typically, such images are not possible to obtain directly from a camera due to hardware limitations, e.g., a limited dynamic range of the sensor. A major problem of such tasks is that the images may not be spatially aligned due to scene motion or camera motion. It is known that the required alignment by image registration problems is ill-posed. In this case, the images to be aligned vary in their intensity range, which makes the problem even more difficult.   To address the mentioned problems, we propose an end-to-end \emph{Convolutional Neural Network} (CNN) based approach to learn to estimate exposure fusion from $2$ and $3$ Low Dynamic Range (LDR) images depicting different scene contents. To the best of our knowledge, no efficient and robust CNN-based end-to-end approach can be found in the literature for this kind of problem. The idea is to create a dataset with perfectly aligned LDR images to obtain ground-truth exposure fusion images. At the same time, we obtain additional LDR images with some motion, having the same exposure fusion ground-truth as the perfectly aligned LDR images. This way, we can train an end-to-end CNN having misaligned LDR input images, but with a proper ground truth exposure fusion image. We propose a specific CNN-architecture to solve this problem. In various experiments, we show that the proposed approach yields excellent results.



### Image Generation from Scene Graphs
- **Arxiv ID**: http://arxiv.org/abs/1804.01622v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1804.01622v1)
- **Published**: 2018-04-04 22:59:08+00:00
- **Updated**: 2018-04-04 22:59:08+00:00
- **Authors**: Justin Johnson, Agrim Gupta, Li Fei-Fei
- **Comment**: To appear at CVPR 2018
- **Journal**: None
- **Summary**: To truly understand the visual world our models should be able not only to recognize images but also generate them. To this end, there has been exciting recent progress on generating images from natural language descriptions. These methods give stunning results on limited domains such as descriptions of birds or flowers, but struggle to faithfully reproduce complex sentences with many objects and relationships. To overcome this limitation we propose a method for generating images from scene graphs, enabling explicitly reasoning about objects and their relationships. Our model uses graph convolution to process input graphs, computes a scene layout by predicting bounding boxes and segmentation masks for objects, and converts the layout to an image with a cascaded refinement network. The network is trained adversarially against a pair of discriminators to ensure realistic outputs. We validate our approach on Visual Genome and COCO-Stuff, where qualitative results, ablations, and user studies demonstrate our method's ability to generate complex images with multiple objects.



### Evaluation of Object Trackers in Distorted Surveillance Videos
- **Arxiv ID**: http://arxiv.org/abs/1804.01624v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.01624v1)
- **Published**: 2018-04-04 23:34:17+00:00
- **Updated**: 2018-04-04 23:34:17+00:00
- **Authors**: Roger Gomez Nieto, H. D. Benitez-Restrepo, Ivan Mauricio Cabezas
- **Comment**: 5 pages, 8 figures, presented in SPSWSIVA 2018
- **Journal**: None
- **Summary**: Object tracking in realistic scenarios is a difficult problem affected by various image factors such as occlusion, clutter, confusion, object shape, unstable speed, and zooming. While these conditions do affect tracking performance, there is no clear distinction between the scene dependent challenges like occlusion, clutter, etc., and the challenges imposed by traditional notions of impairments from capture, compression, processing, and transmission. This paper is concerned with the latter interpretation of quality as it affects video tracking performance. In this work we aim to evaluate two state-of-the-art trackers (STRUCK and TLD) systematically and experimentally in surveillance videos affected by in-capture distortions such as under-exposure and defocus. We evaluate these trackers with the area under curve (AUC) values of success plots and precision curves. In spite of the fact that STRUCK and TLD have ranked high in video tracking surveys. This study concludes that incapture distortions severely affect the performance of these trackers. For this reason, the design and construction of a robust tracker with respect to these distortions remains an open question that can be answered by creating algorithms that makes use of perceptual features to compensate the degradations provided by these distortions.



