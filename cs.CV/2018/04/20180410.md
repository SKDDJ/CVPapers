# Arxiv Papers in cs.CV on 2018-04-10
### Recurrent Neural Networks for Person Re-identification Revisited
- **Arxiv ID**: http://arxiv.org/abs/1804.03281v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.03281v1)
- **Published**: 2018-04-10 00:14:37+00:00
- **Updated**: 2018-04-10 00:14:37+00:00
- **Authors**: Jean-Baptiste Boin, Andre Araujo, Bernd Girod
- **Comment**: None
- **Journal**: None
- **Summary**: The task of person re-identification has recently received rising attention due to the high performance achieved by new methods based on deep learning. In particular, in the context of video-based re-identification, many state-of-the-art works have explored the use of Recurrent Neural Networks (RNNs) to process input sequences. In this work, we revisit this tool by deriving an approximation which reveals the small effect of recurrent connections, leading to a much simpler feed-forward architecture. Using the same parameters as the recurrent version, our proposed feed-forward architecture obtains very similar accuracy. More importantly, our model can be combined with a new training process to significantly improve re-identification performance. Our experiments demonstrate that the proposed models converge substantially faster than recurrent ones, with accuracy improvements by up to 5% on two datasets. The performance achieved is better or on par with other RNN-based person re-identification techniques.



### Understanding Humans in Crowded Scenes: Deep Nested Adversarial Learning and A New Benchmark for Multi-Human Parsing
- **Arxiv ID**: http://arxiv.org/abs/1804.03287v3
- **DOI**: 10.13140/RG.2.2.23242.67523
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.03287v3)
- **Published**: 2018-04-10 00:41:26+00:00
- **Updated**: 2018-07-06 05:49:40+00:00
- **Authors**: Jian Zhao, Jianshu Li, Yu Cheng, Li Zhou, Terence Sim, Shuicheng Yan, Jiashi Feng
- **Comment**: The first three authors are with equal contributions
- **Journal**: None
- **Summary**: Despite the noticeable progress in perceptual tasks like detection, instance segmentation and human parsing, computers still perform unsatisfactorily on visually understanding humans in crowded scenes, such as group behavior analysis, person re-identification and autonomous driving, etc. To this end, models need to comprehensively perceive the semantic information and the differences between instances in a multi-human image, which is recently defined as the multi-human parsing task. In this paper, we present a new large-scale database "Multi-Human Parsing (MHP)" for algorithm development and evaluation, and advances the state-of-the-art in understanding humans in crowded scenes. MHP contains 25,403 elaborately annotated images with 58 fine-grained semantic category labels, involving 2-26 persons per image and captured in real-world scenes from various viewpoints, poses, occlusion, interactions and background. We further propose a novel deep Nested Adversarial Network (NAN) model for multi-human parsing. NAN consists of three Generative Adversarial Network (GAN)-like sub-nets, respectively performing semantic saliency prediction, instance-agnostic parsing and instance-aware clustering. These sub-nets form a nested structure and are carefully designed to learn jointly in an end-to-end way. NAN consistently outperforms existing state-of-the-art solutions on our MHP and several other datasets, and serves as a strong baseline to drive the future research for multi-human parsing.



### Pilot Comparative Study of Different Deep Features for Palmprint Identification in Low-Quality Images
- **Arxiv ID**: http://arxiv.org/abs/1804.04602v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.04602v1)
- **Published**: 2018-04-10 01:13:33+00:00
- **Updated**: 2018-04-10 01:13:33+00:00
- **Authors**: A. S. Tarawneh, D. Chetverikov, A. B. Hassanat
- **Comment**: 5 pages, 5 figures, Ninth Hungarian Conference on Computer Graphics
  and Geometry, Budapest, 2018
- **Journal**: Ninth Hungarian Conference on Computer Graphics and Geometry,
  Budapest, 2018
- **Summary**: Deep Convolutional Neural Networks (CNNs) are widespread, efficient tools of visual recognition. In this paper, we present a comparative study of three popular pre-trained CNN models: AlexNet, VGG-16 and VGG-19. We address the problem of palmprint identification in low-quality imagery and apply Support Vector Machines (SVMs) with all of the compared models. For the comparison, we use the MOHI palmprint image database whose images are characterized by low contrast, shadows, and varying illumination, scale, translation and rotation. Another, high-quality database called COEP is also considered to study the recognition gap between high-quality and low-quality imagery. Our experiments show that the deeper pre-trained CNN models, e.g., VGG-16 and VGG-19, tend to extract highly distinguishable features that recognize low-quality palmprints more efficiently than the less deep networks such as AlexNet. Furthermore, our experiments on the two databases using various models demonstrate that the features extracted from lower-level fully connected layers provide higher recognition rates than higher-layer features. Our results indicate that different pre-trained models can be efficiently used in touchless identification systems with low-quality palmprint images.



### A Systematic DNN Weight Pruning Framework using Alternating Direction Method of Multipliers
- **Arxiv ID**: http://arxiv.org/abs/1804.03294v3
- **DOI**: 10.1007/978-3-030-01237-3_12
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1804.03294v3)
- **Published**: 2018-04-10 01:14:51+00:00
- **Updated**: 2018-07-25 16:52:02+00:00
- **Authors**: Tianyun Zhang, Shaokai Ye, Kaiqi Zhang, Jian Tang, Wujie Wen, Makan Fardad, Yanzhi Wang
- **Comment**: None
- **Journal**: ECCV 2018, pp 191-207
- **Summary**: Weight pruning methods for deep neural networks (DNNs) have been investigated recently, but prior work in this area is mainly heuristic, iterative pruning, thereby lacking guarantees on the weight reduction ratio and convergence time. To mitigate these limitations, we present a systematic weight pruning framework of DNNs using the alternating direction method of multipliers (ADMM). We first formulate the weight pruning problem of DNNs as a nonconvex optimization problem with combinatorial constraints specifying the sparsity requirements, and then adopt the ADMM framework for systematic weight pruning. By using ADMM, the original nonconvex optimization problem is decomposed into two subproblems that are solved iteratively. One of these subproblems can be solved using stochastic gradient descent, the other can be solved analytically. Besides, our method achieves a fast convergence rate.   The weight pruning results are very promising and consistently outperform the prior work. On the LeNet-5 model for the MNIST data set, we achieve 71.2 times weight reduction without accuracy loss. On the AlexNet model for the ImageNet data set, we achieve 21 times weight reduction without accuracy loss. When we focus on the convolutional layer pruning for computation reductions, we can reduce the total computation by five times compared with the prior work (achieving a total of 13.4 times weight reduction in convolutional layers). Our models and codes are released at https://github.com/KaiqiZhang/admm-pruning



### Crafting a Toolchain for Image Restoration by Deep Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/1804.03312v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.03312v1)
- **Published**: 2018-04-10 02:30:40+00:00
- **Updated**: 2018-04-10 02:30:40+00:00
- **Authors**: Ke Yu, Chao Dong, Liang Lin, Chen Change Loy
- **Comment**: To appear at CVPR 2018 (Spotlight). Project page:
  http://mmlab.ie.cuhk.edu.hk/projects/RL-Restore/
- **Journal**: None
- **Summary**: We investigate a novel approach for image restoration by reinforcement learning. Unlike existing studies that mostly train a single large network for a specialized task, we prepare a toolbox consisting of small-scale convolutional networks of different complexities and specialized in different tasks. Our method, RL-Restore, then learns a policy to select appropriate tools from the toolbox to progressively restore the quality of a corrupted image. We formulate a step-wise reward function proportional to how well the image is restored at each step to learn the action policy. We also devise a joint learning scheme to train the agent and tools for better performance in handling uncertainty. In comparison to conventional human-designed networks, RL-Restore is capable of restoring images corrupted with complex and unknown distortions in a more parameter-efficient manner using the dynamically formed toolchain.



### Cortex Neural Network: learning with Neural Network groups
- **Arxiv ID**: http://arxiv.org/abs/1804.03313v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1804.03313v1)
- **Published**: 2018-04-10 02:33:47+00:00
- **Updated**: 2018-04-10 02:33:47+00:00
- **Authors**: Liyao Gao
- **Comment**: None
- **Journal**: None
- **Summary**: Neural Network has been successfully applied to many real-world problems, such as image recognition and machine translation. However, for the current architecture of neural networks, it is hard to perform complex cognitive tasks, for example, to process the image and audio inputs together. Cortex, as an important architecture in the brain, is important for animals to perform the complex cognitive task. We view the architecture of Cortex in the brain as a missing part in the design of the current artificial neural network. In this paper, we purpose Cortex Neural Network (CrtxNN). The Cortex Neural Network is an upper architecture of neural networks which motivated from cerebral cortex in the brain to handle different tasks in the same learning system. It is able to identify different tasks and solve them with different methods. In our implementation, the Cortex Neural Network is able to process different cognitive tasks and perform reflection to get a higher accuracy. We provide a series of experiments to examine the capability of the cortex architecture on traditional neural networks. Our experiments proved its ability on the Cortex Neural Network can reach accuracy by 98.32% on MNIST and 62% on CIFAR10 at the same time, which can promisingly reduce the loss by 40%.



### Outline Objects using Deep Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/1804.04603v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1804.04603v2)
- **Published**: 2018-04-10 03:25:31+00:00
- **Updated**: 2018-04-20 07:10:44+00:00
- **Authors**: Zhenxin Wang, Sayan Sarcar, Jingxin Liu, Yilin Zheng, Xiangshi Ren
- **Comment**: None
- **Journal**: None
- **Summary**: Image segmentation needs both local boundary position information and global object context information. The performance of the recent state-of-the-art method, fully convolutional networks, reaches a bottleneck due to the neural network limit after balancing between the two types of information simultaneously in an end-to-end training style. To overcome this problem, we divide the semantic image segmentation into temporal subtasks. First, we find a possible pixel position of some object boundary; then trace the boundary at steps within a limited length until the whole object is outlined. We present the first deep reinforcement learning approach to semantic image segmentation, called DeepOutline, which outperforms other algorithms in Coco detection leaderboard in the middle and large size person category in Coco val2017 dataset. Meanwhile, it provides an insight into a divide and conquer way by reinforcement learning on computer vision problems.



### On the Robustness of the CVPR 2018 White-Box Adversarial Example Defenses
- **Arxiv ID**: http://arxiv.org/abs/1804.03286v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1804.03286v1)
- **Published**: 2018-04-10 04:54:29+00:00
- **Updated**: 2018-04-10 04:54:29+00:00
- **Authors**: Anish Athalye, Nicholas Carlini
- **Comment**: None
- **Journal**: None
- **Summary**: Neural networks are known to be vulnerable to adversarial examples. In this note, we evaluate the two white-box defenses that appeared at CVPR 2018 and find they are ineffective: when applying existing techniques, we can reduce the accuracy of the defended models to 0%.



### Modular Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1804.03343v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.03343v1)
- **Published**: 2018-04-10 05:28:00+00:00
- **Updated**: 2018-04-10 05:28:00+00:00
- **Authors**: Bo Zhao, Bo Chang, Zequn Jie, Leonid Sigal
- **Comment**: None
- **Journal**: None
- **Summary**: Existing methods for multi-domain image-to-image translation (or generation) attempt to directly map an input image (or a random vector) to an image in one of the output domains. However, most existing methods have limited scalability and robustness, since they require building independent models for each pair of domains in question. This leads to two significant shortcomings: (1) the need to train exponential number of pairwise models, and (2) the inability to leverage data from other domains when training a particular pairwise mapping. Inspired by recent work on module networks, this paper proposes ModularGAN for multi-domain image generation and image-to-image translation. ModularGAN consists of several reusable and composable modules that carry on different functions (e.g., encoding, decoding, transformations). These modules can be trained simultaneously, leveraging data from all domains, and then combined to construct specific GAN networks at test time, according to the specific image translation task. This leads to ModularGAN's superior flexibility of generating (or translating to) an image in any desired domain. Experimental results demonstrate that our model not only presents compelling perceptual results but also outperforms state-of-the-art methods on multi-domain facial attribute transfer.



### Mean Field Network based Graph Refinement with application to Airway Tree Extraction
- **Arxiv ID**: http://arxiv.org/abs/1804.03348v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.03348v1)
- **Published**: 2018-04-10 05:52:22+00:00
- **Updated**: 2018-04-10 05:52:22+00:00
- **Authors**: Raghavendra Selvan, Max Welling, Jesper H. Pedersen, Jens Petersen, Marleen de Bruijne
- **Comment**: 10 pages. Preprint
- **Journal**: None
- **Summary**: We present tree extraction in 3D images as a graph refinement task, of obtaining a subgraph from an over-complete input graph. To this end, we formulate an approximate Bayesian inference framework on undirected graphs using mean field approximation (MFA). Mean field networks are used for inference based on the interpretation that iterations of MFA can be seen as feed-forward operations in a neural network. This allows us to learn the model parameters from training data using back-propagation algorithm. We demonstrate usefulness of the model to extract airway trees from 3D chest CT data. We first obtain probability images using a voxel classifier that distinguishes airways from background and use Bayesian smoothing to model individual airway branches. This yields us joint Gaussian density estimates of position, orientation and scale as node features of the input graph. Performance of the method is compared with two methods: the first uses probability images from a trained voxel classifier with region growing, which is similar to one of the best performing methods at EXACT'09 airway challenge, and the second method is based on Bayesian smoothing on these probability images. Using centerline distance as error measure the presented method shows significant improvement compared to these two methods.



### Reference-Conditioned Super-Resolution by Neural Texture Transfer
- **Arxiv ID**: http://arxiv.org/abs/1804.03360v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.03360v1)
- **Published**: 2018-04-10 06:30:44+00:00
- **Updated**: 2018-04-10 06:30:44+00:00
- **Authors**: Zhifei Zhang, Zhaowen Wang, Zhe Lin, Hairong Qi
- **Comment**: Project Page:
  http://web.eecs.utk.edu/~zzhang61/project_page/SRNTT/SRNTT.html
- **Journal**: None
- **Summary**: With the recent advancement in deep learning, we have witnessed a great progress in single image super-resolution. However, due to the significant information loss of the image downscaling process, it has become extremely challenging to further advance the state-of-the-art, especially for large upscaling factors. This paper explores a new research direction in super resolution, called reference-conditioned super-resolution, in which a reference image containing desired high-resolution texture details is provided besides the low-resolution image. We focus on transferring the high-resolution texture from reference images to the super-resolution process without the constraint of content similarity between reference and target images, which is a key difference from previous example-based methods. Inspired by recent work on image stylization, we address the problem via neural texture transfer. We design an end-to-end trainable deep model which generates detail enriched results by adaptively fusing the content from the low-resolution image with the texture patterns from the reference image. We create a benchmark dataset for the general research of reference-based super-resolution, which contains reference images paired with low-resolution inputs with varying degrees of similarity. Both objective and subjective evaluations demonstrate the great potential of using reference images as well as the superiority of our results over other state-of-the-art methods.



### Learning Deep Gradient Descent Optimization for Image Deconvolution
- **Arxiv ID**: http://arxiv.org/abs/1804.03368v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.03368v2)
- **Published**: 2018-04-10 06:58:12+00:00
- **Updated**: 2020-02-17 06:09:24+00:00
- **Authors**: Dong Gong, Zhen Zhang, Qinfeng Shi, Anton van den Hengel, Chunhua Shen, Yanning Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: As an integral component of blind image deblurring, non-blind deconvolution removes image blur with a given blur kernel, which is essential but difficult due to the ill-posed nature of the inverse problem. The predominant approach is based on optimization subject to regularization functions that are either manually designed, or learned from examples. Existing learning based methods have shown superior restoration quality but are not practical enough due to their restricted and static model design. They solely focus on learning a prior and require to know the noise level for deconvolution. We address the gap between the optimization-based and learning-based approaches by learning a universal gradient descent optimizer. We propose a Recurrent Gradient Descent Network (RGDN) by systematically incorporating deep neural networks into a fully parameterized gradient descent scheme. A hyper-parameter-free update unit shared across steps is used to generate updates from the current estimates, based on a convolutional neural network. By training on diverse examples, the Recurrent Gradient Descent Network learns an implicit image prior and a universal update rule through recursive supervision. The learned optimizer can be repeatedly used to improve the quality of diverse degenerated observations. The proposed method possesses strong interpretability and high generalization. Extensive experiments on synthetic benchmarks and challenging real-world images demonstrate that the proposed deep optimization method is effective and robust to produce favorable results as well as practical for real-world image deblurring applications.



### Discovery and usage of joint attention in images
- **Arxiv ID**: http://arxiv.org/abs/1804.04604v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1804.04604v1)
- **Published**: 2018-04-10 07:04:19+00:00
- **Updated**: 2018-04-10 07:04:19+00:00
- **Authors**: Daniel Harari, Joshua B. Tenenbaum, Shimon Ullman
- **Comment**: 6 pages, 3 figures
- **Journal**: None
- **Summary**: Joint visual attention is characterized by two or more individuals looking at a common target at the same time. The ability to identify joint attention in scenes, the people involved, and their common target, is fundamental to the understanding of social interactions, including others' intentions and goals. In this work we deal with the extraction of joint attention events, and the use of such events for image descriptions. The work makes two novel contributions. First, our extraction algorithm is the first which identifies joint visual attention in single static images. It computes 3D gaze direction, identifies the gaze target by combining gaze direction with a 3D depth map computed for the image, and identifies the common gaze target. Second, we use a human study to demonstrate the sensitivity of humans to joint attention, suggesting that the detection of such a configuration in an image can be useful for understanding the image, including the goals of the agents and their joint activity, and therefore can contribute to image captioning and related tasks.



### Loss Rank Mining: A General Hard Example Mining Method for Real-time Detectors
- **Arxiv ID**: http://arxiv.org/abs/1804.04606v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.04606v1)
- **Published**: 2018-04-10 07:43:16+00:00
- **Updated**: 2018-04-10 07:43:16+00:00
- **Authors**: Hao Yu, Zhaoning Zhang, Zheng Qin, Hao Wu, Dongsheng Li, Jun Zhao, Xicheng Lu
- **Comment**: 8 pages, 6 figures
- **Journal**: None
- **Summary**: Modern object detectors usually suffer from low accuracy issues, as foregrounds always drown in tons of backgrounds and become hard examples during training. Compared with those proposal-based ones, real-time detectors are in far more serious trouble since they renounce the use of region-proposing stage which is used to filter a majority of backgrounds for achieving real-time rates. Though foregrounds as hard examples are in urgent need of being mined from tons of backgrounds, a considerable number of state-of-the-art real-time detectors, like YOLO series, have yet to profit from existing hard example mining methods, as using these methods need detectors fit series of prerequisites. In this paper, we propose a general hard example mining method named Loss Rank Mining (LRM) to fill the gap. LRM is a general method for real-time detectors, as it utilizes the final feature map which exists in all real-time detectors to mine hard examples. By using LRM, some elements representing easy examples in final feature map are filtered and detectors are forced to concentrate on hard examples during training. Extensive experiments validate the effectiveness of our method. With our method, the improvements of YOLOv2 detector on auto-driving related dataset KITTI and more general dataset PASCAL VOC are over 5% and 2% mAP, respectively. In addition, LRM is the first hard example mining strategy which could fit YOLOv2 perfectly and make it better applied in series of real scenarios where both real-time rates and accurate detection are strongly demanded.



### Learning Pose Specific Representations by Predicting Different Views
- **Arxiv ID**: http://arxiv.org/abs/1804.03390v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, I.2.6; I.2.10; I.4.5; I.4.8; I.4.10; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/1804.03390v2)
- **Published**: 2018-04-10 08:22:23+00:00
- **Updated**: 2018-05-23 15:02:46+00:00
- **Authors**: Georg Poier, David Schinagl, Horst Bischof
- **Comment**: CVPR 2018 (Spotlight); Project Page at
  https://poier.github.io/PreView/
- **Journal**: None
- **Summary**: The labeled data required to learn pose estimation for articulated objects is difficult to provide in the desired quantity, realism, density, and accuracy. To address this issue, we develop a method to learn representations, which are very specific for articulated poses, without the need for labeled training data. We exploit the observation that the object pose of a known object is predictive for the appearance in any known view. That is, given only the pose and shape parameters of a hand, the hand's appearance from any viewpoint can be approximated. To exploit this observation, we train a model that -- given input from one view -- estimates a latent representation, which is trained to be predictive for the appearance of the object when captured from another viewpoint. Thus, the only necessary supervision is the second view. The training process of this model reveals an implicit pose representation in the latent space. Importantly, at test time the pose representation can be inferred using only a single view. In qualitative and quantitative experiments we show that the learned representations capture detailed pose information. Moreover, when training the proposed method jointly with labeled and unlabeled data, it consistently surpasses the performance of its fully supervised counterpart, while reducing the amount of needed labeled samples by at least one order of magnitude.



### Roto-Translation Covariant Convolutional Networks for Medical Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/1804.03393v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, math.GR
- **Links**: [PDF](http://arxiv.org/pdf/1804.03393v3)
- **Published**: 2018-04-10 08:23:44+00:00
- **Updated**: 2018-06-11 17:53:31+00:00
- **Authors**: Erik J Bekkers, Maxime W Lafarge, Mitko Veta, Koen AJ Eppenhof, Josien PW Pluim, Remco Duits
- **Comment**: 8 pages, 2 figures, 1 table, accepted at MICCAI 2018
- **Journal**: None
- **Summary**: We propose a framework for rotation and translation covariant deep learning using $SE(2)$ group convolutions. The group product of the special Euclidean motion group $SE(2)$ describes how a concatenation of two roto-translations results in a net roto-translation. We encode this geometric structure into convolutional neural networks (CNNs) via $SE(2)$ group convolutional layers, which fit into the standard 2D CNN framework, and which allow to generically deal with rotated input samples without the need for data augmentation.   We introduce three layers: a lifting layer which lifts a 2D (vector valued) image to an $SE(2)$-image, i.e., 3D (vector valued) data whose domain is $SE(2)$; a group convolution layer from and to an $SE(2)$-image; and a projection layer from an $SE(2)$-image to a 2D image. The lifting and group convolution layers are $SE(2)$ covariant (the output roto-translates with the input). The final projection layer, a maximum intensity projection over rotations, makes the full CNN rotation invariant.   We show with three different problems in histopathology, retinal imaging, and electron microscopy that with the proposed group CNNs, state-of-the-art performance can be achieved, without the need for data augmentation by rotation and with increased performance compared to standard CNNs that do rely on augmentation.



### A Fast Hierarchically Preconditioned Eigensolver Based On Multiresolution Matrix Decomposition
- **Arxiv ID**: http://arxiv.org/abs/1804.03415v2
- **DOI**: None
- **Categories**: **math.NA**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1804.03415v2)
- **Published**: 2018-04-10 09:30:37+00:00
- **Updated**: 2018-06-27 15:06:57+00:00
- **Authors**: Thomas Y. Hou, De Huang, Ka Chun Lam, Ziyun Zhang
- **Comment**: 46 pages, 11 figures, 10 tables
- **Journal**: None
- **Summary**: In this paper we propose a new iterative method to hierarchically compute a relatively large number of leftmost eigenpairs of a sparse symmetric positive matrix under the multiresolution operator compression framework. We exploit the well-conditioned property of every decomposition components by integrating the multiresolution framework into the Implicitly restarted Lanczos method. We achieve this combination by proposing an extension-refinement iterative scheme, in which the intrinsic idea is to decompose the target spectrum into several segments such that the corresponding eigenproblem in each segment is well-conditioned. Theoretical analysis and numerical illustration are also reported to illustrate the efficiency and effectiveness of this algorithm.



### Graphical Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1804.03429v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1804.03429v2)
- **Published**: 2018-04-10 10:12:38+00:00
- **Updated**: 2018-12-12 08:20:54+00:00
- **Authors**: Chongxuan Li, Max Welling, Jun Zhu, Bo Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: We propose Graphical Generative Adversarial Networks (Graphical-GAN) to model structured data. Graphical-GAN conjoins the power of Bayesian networks on compactly representing the dependency structures among random variables and that of generative adversarial networks on learning expressive dependency functions. We introduce a structured recognition model to infer the posterior distribution of latent variables given observations. We generalize the Expectation Propagation (EP) algorithm to learn the generative model and recognition model jointly. Finally, we present two important instances of Graphical-GAN, i.e. Gaussian Mixture GAN (GMGAN) and State Space GAN (SSGAN), which can successfully learn the discrete and temporal structures on visual datasets, respectively.



### RSGAN: Face Swapping and Editing using Face and Hair Representation in Latent Spaces
- **Arxiv ID**: http://arxiv.org/abs/1804.03447v2
- **DOI**: 10.1145/3230744.3230818
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1804.03447v2)
- **Published**: 2018-04-10 10:54:34+00:00
- **Updated**: 2018-04-18 06:44:06+00:00
- **Authors**: Ryota Natsume, Tatsuya Yatagawa, Shigeo Morishima
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present an integrated system for automatically generating and editing face images through face swapping, attribute-based editing, and random face parts synthesis. The proposed system is based on a deep neural network that variationally learns the face and hair regions with large-scale face image datasets. Different from conventional variational methods, the proposed network represents the latent spaces individually for faces and hairs. We refer to the proposed network as region-separative generative adversarial network (RSGAN). The proposed network independently handles face and hair appearances in the latent spaces, and then, face swapping is achieved by replacing the latent-space representations of the faces, and reconstruct the entire face image with them. This approach in the latent space robustly performs face swapping even for images which the previous methods result in failure due to inappropriate fitting or the 3D morphable models. In addition, the proposed system can further edit face-swapped images with the same network by manipulating visual attributes or by composing them with randomly generated face or hair parts.



### Universal features of mountain ridge networks on Earth
- **Arxiv ID**: http://arxiv.org/abs/1804.03457v3
- **DOI**: 10.1093/comnet/cnz017
- **Categories**: **physics.geo-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1804.03457v3)
- **Published**: 2018-04-10 11:19:55+00:00
- **Updated**: 2019-05-17 13:18:06+00:00
- **Authors**: Rafał Rak, Jarosław Kwapień, Paweł Oświęcimka, Paweł Zięba, Stanisław Drożdż
- **Comment**: to appear in Journal of Complex Networks
- **Journal**: Journal of Complex Networks 8, cnz017 (2020)
- **Summary**: Compared to the heavily studied surface drainage systems, the mountain ridge systems have been a subject of less attention even on the empirical level, despite the fact that their structure is richer. To reduce this deficiency, we analyze different mountain ranges by means of a network approach and grasp some essential features of the ridge branching structure. We also employ a fractal analysis as it is especially suitable for describing properties of rough objects and surfaces. As our approach differs from typical analyses that are carried out in geophysics, we believe that it can initialize a research direction that will allow to shed more light on the processes that are responsible for landscape formation and will contribute to the network theory by indicating a need for the construction of new models of the network growth as no existing model can properly describe the ridge formation. We also believe that certain features of our study can offer help in the cartographic generalization. Specifically, we study structure of the ridge networks based on the empirical elevation data collected by SRTM. We consider mountain ranges from different geological periods and geographical locations. For each mountain range, we construct a simple topographic network representation (the ridge junctions are nodes) and a ridge representation (the ridges are nodes and the junctions are edges) and calculate the parameters characterizing their topology. We observe that the topographic networks inherit the fractal structure of the mountain ranges but do not show any other complex features. In contrast, the ridge networks, while lacking the proper fractality, reveal the power-law degree distributions with the exponent $1.6\le \beta \le 1.7$. By taking into account the fact that the analyzed mountains differ in many properties, these values seem to be universal for the earthly mountainous terrain.



### Exploring Disentangled Feature Representation Beyond Face Identification
- **Arxiv ID**: http://arxiv.org/abs/1804.03487v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1804.03487v1)
- **Published**: 2018-04-10 12:59:53+00:00
- **Updated**: 2018-04-10 12:59:53+00:00
- **Authors**: Yu Liu, Fangyin Wei, Jing Shao, Lu Sheng, Junjie Yan, Xiaogang Wang
- **Comment**: Accepted by CVPR 2018
- **Journal**: None
- **Summary**: This paper proposes learning disentangled but complementary face features with minimal supervision by face identification. Specifically, we construct an identity Distilling and Dispelling Autoencoder (D2AE) framework that adversarially learns the identity-distilled features for identity verification and the identity-dispelled features to fool the verification system. Thanks to the design of two-stream cues, the learned disentangled features represent not only the identity or attribute but the complete input image. Comprehensive evaluations further demonstrate that the proposed features not only maintain state-of-the-art identity verification performance on LFW, but also acquire competitive discriminative power for face attribute recognition on CelebA and LFWA. Moreover, the proposed system is ready to semantically control the face generation/editing based on various identities and attributes in an unsupervised manner.



### PointNetVLAD: Deep Point Cloud Based Retrieval for Large-Scale Place Recognition
- **Arxiv ID**: http://arxiv.org/abs/1804.03492v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.03492v3)
- **Published**: 2018-04-10 13:06:56+00:00
- **Updated**: 2018-05-16 08:47:33+00:00
- **Authors**: Mikaela Angelina Uy, Gim Hee Lee
- **Comment**: CVPR 2018, 11 pages, 10 figures
- **Journal**: None
- **Summary**: Unlike its image based counterpart, point cloud based retrieval for place recognition has remained as an unexplored and unsolved problem. This is largely due to the difficulty in extracting local feature descriptors from a point cloud that can subsequently be encoded into a global descriptor for the retrieval task. In this paper, we propose the PointNetVLAD where we leverage on the recent success of deep networks to solve point cloud based retrieval for place recognition. Specifically, our PointNetVLAD is a combination/modification of the existing PointNet and NetVLAD, which allows end-to-end training and inference to extract the global descriptor from a given 3D point cloud. Furthermore, we propose the "lazy triplet and quadruplet" loss functions that can achieve more discriminative and generalizable global descriptors to tackle the retrieval task. We create benchmark datasets for point cloud based retrieval for place recognition, and the experimental results on these datasets show the feasibility of our PointNetVLAD. Our code and the link for the benchmark dataset downloads are available in our project website. http://github.com/mikacuy/pointnetvlad/



### A real-time and unsupervised face Re-Identification system for Human-Robot Interaction
- **Arxiv ID**: http://arxiv.org/abs/1804.03547v3
- **DOI**: 10.1016/j.patrec.2018.04.009
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.03547v3)
- **Published**: 2018-04-10 14:07:45+00:00
- **Updated**: 2022-03-24 11:04:37+00:00
- **Authors**: Yujiang Wang, Jie Shen, Stavros Petridis, Maja Pantic
- **Comment**: Code implementation in Python is available at:
  https://github.com/ibug-group/face_reid
- **Journal**: Pattern Recognition Letters, Volume 128, 1 December 2019, Pages
  559-568
- **Summary**: In the context of Human-Robot Interaction (HRI), face Re-Identification (face Re-ID) aims to verify if certain detected faces have already been observed by robots. The ability of distinguishing between different users is crucial in social robots as it will enable the robot to tailor the interaction strategy toward the users' individual preferences. So far face recognition research has achieved great success, however little attention has been paid to the realistic applications of Face Re-ID in social robots. In this paper, we present an effective and unsupervised face Re-ID system which simultaneously re-identifies multiple faces for HRI. This Re-ID system employs Deep Convolutional Neural Networks to extract features, and an online clustering algorithm to determine the face's ID. Its performance is evaluated on two datasets: the TERESA video dataset collected by the TERESA robot, and the YouTube Face Dataset (YTF Dataset). We demonstrate that the optimised combination of techniques achieves an overall 93.55% accuracy on TERESA dataset and an overall 90.41% accuracy on YTF dataset. We have implemented the proposed method into a software module in the HCI^2 Framework for it to be further integrated into the TERESA robot, and has achieved real-time performance at 10~26 Frames per second.



### Two Stream 3D Semantic Scene Completion
- **Arxiv ID**: http://arxiv.org/abs/1804.03550v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.03550v4)
- **Published**: 2018-04-10 14:10:26+00:00
- **Updated**: 2019-05-15 14:36:17+00:00
- **Authors**: Martin Garbade, Yueh-Tung Chen, Johann Sawatzky, Juergen Gall
- **Comment**: None
- **Journal**: None
- **Summary**: Inferring the 3D geometry and the semantic meaning of surfaces, which are occluded, is a very challenging task. Recently, a first end-to-end learning approach has been proposed that completes a scene from a single depth image. The approach voxelizes the scene and predicts for each voxel if it is occupied and, if it is occupied, the semantic class label. In this work, we propose a two stream approach that leverages depth information and semantic information, which is inferred from the RGB image, for this task. The approach constructs an incomplete 3D semantic tensor, which uses a compact three-channel encoding for the inferred semantic information, and uses a 3D CNN to infer the complete 3D semantic tensor. In our experimental evaluation, we show that the proposed two stream approach substantially outperforms the state-of-the-art for semantic scene completion.



### Evaluation of the visual odometry methods for semi-dense real-time
- **Arxiv ID**: http://arxiv.org/abs/1804.03558v2
- **DOI**: 10.5121/acij
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.03558v2)
- **Published**: 2018-04-10 14:28:08+00:00
- **Updated**: 2018-04-12 08:14:58+00:00
- **Authors**: Haidara Gaoussou, Peng Dewei
- **Comment**: 14 pages, 6 figures
- **Journal**: Advanced Computing: An International Journal (ACIJ), Vol.9, No.2,
  March 2018
- **Summary**: Recent decades have witnessed a significant increase in the use of visual odometry(VO) in the computer vision area. It has also been used in varieties of robotic applications, for example on the Mars Exploration Rovers. This paper, firstly, discusses two popular existing visual odometry approaches, namely LSD-SLAM and ORB-SLAM2 to improve the performance metrics of visual SLAM systems using Umeyama Method. We carefully evaluate the methods referred to above on three different well-known KITTI datasets, EuRoC MAV dataset, and TUM RGB-D dataset to obtain the best results and graphically compare the results to evaluation metrics from different visual odometry approaches. Secondly, we propose an approach running in real-time with a stereo camera, which combines an existing feature-based (indirect) method and an existing feature-less (direct) method matching with accurate semidense direct image alignment and reconstructing an accurate 3D environment directly on pixels that have image gradient. Keywords VO, performance metrics, Umeyama Method, feature-based method, feature-less method & semi-dense real-time.



### Large Field and High Resolution: Detecting Needle in Haystack
- **Arxiv ID**: http://arxiv.org/abs/1804.03576v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.03576v1)
- **Published**: 2018-04-10 14:56:21+00:00
- **Updated**: 2018-04-10 14:56:21+00:00
- **Authors**: Hadar Gorodissky, Daniel Harari, Shimon Ullman
- **Comment**: 15 pages, 7 figures
- **Journal**: None
- **Summary**: The growing use of convolutional neural networks (CNN) for a broad range of visual tasks, including tasks involving fine details, raises the problem of applying such networks to a large field of view, since the amount of computations increases significantly with the number of pixels. To deal effectively with this difficulty, we develop and compare methods of using CNNs for the task of small target localization in natural images, given a limited "budget" of samples to form an image. Inspired in part by human vision, we develop and compare variable sampling schemes, with peak resolution at the center and decreasing resolution with eccentricity, applied iteratively by re-centering the image at the previous predicted target location. The results indicate that variable resolution models significantly outperform constant resolution models. Surprisingly, variable resolution models and in particular multi-channel models, outperform the optimal, "budget-free" full-resolution model, using only 5\% of the samples.



### Classification of Point Cloud Scenes with Multiscale Voxel Deep Network
- **Arxiv ID**: http://arxiv.org/abs/1804.03583v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.03583v1)
- **Published**: 2018-04-10 15:14:11+00:00
- **Updated**: 2018-04-10 15:14:11+00:00
- **Authors**: Xavier Roynard, Jean-Emmanuel Deschaud, François Goulette
- **Comment**: preprint
- **Journal**: None
- **Summary**: In this article we describe a new convolutional neural network (CNN) to classify 3D point clouds of urban or indoor scenes. Solutions are given to the problems encountered working on scene point clouds, and a network is described that allows for point classification using only the position of points in a multi-scale neighborhood.   On the reduced-8 Semantic3D benchmark [Hackel et al., 2017], this network, ranked second, beats the state of the art of point classification methods (those not using a regularization step).



### Geometrical analysis of polynomial lens distortion models
- **Arxiv ID**: http://arxiv.org/abs/1804.03584v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.03584v2)
- **Published**: 2018-04-10 15:16:05+00:00
- **Updated**: 2018-07-30 10:18:19+00:00
- **Authors**: José I. Ronda, Antonio Valdés
- **Comment**: Accepted in the Journal of Mathematical Imaging and Vision
- **Journal**: None
- **Summary**: Polynomial functions are a usual choice to model the nonlinearity of lenses. Typically, these models are obtained through physical analysis of the lens system or on purely empirical grounds. The aim of this work is to facilitate an alternative approach to the selection or design of these models based on establishing a priori the desired geometrical properties of the distortion functions. With this purpose we obtain all the possible isotropic linear models and also those that are formed by functions with symmetry with respect to some axis. In this way, the classical models (decentering, thin prism distortion) are found to be particular instances of the family of models found by geometric considerations. These results allow to find generalizations of the most usually employed models while preserving the desired geometrical properties. Our results also provide a better understanding of the geometric properties of the models employed in the most usual computer vision software libraries.



### A Deep Information Sharing Network for Multi-contrast Compressed Sensing MRI Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1804.03596v1
- **DOI**: 10.1109/TIP.2019.2925288
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.03596v1)
- **Published**: 2018-04-10 15:43:48+00:00
- **Updated**: 2018-04-10 15:43:48+00:00
- **Authors**: Liyan Sun, Zhiwen Fan, Yue Huang, Xinghao Ding, John Paisley
- **Comment**: 13 pages, 16 figures, 3 tables
- **Journal**: None
- **Summary**: In multi-contrast magnetic resonance imaging (MRI), compressed sensing theory can accelerate imaging by sampling fewer measurements within each contrast. The conventional optimization-based models suffer several limitations: strict assumption of shared sparse support, time-consuming optimization and "shallow" models with difficulties in encoding the rich patterns hiding in massive MRI data. In this paper, we propose the first deep learning model for multi-contrast MRI reconstruction. We achieve information sharing through feature sharing units, which significantly reduces the number of parameters. The feature sharing unit is combined with a data fidelity unit to comprise an inference block. These inference blocks are cascaded with dense connections, which allows for information transmission across different depths of the network efficiently. Our extensive experiments on various multi-contrast MRI datasets show that proposed model outperforms both state-of-the-art single-contrast and multi-contrast MRI methods in accuracy and efficiency. We show the improved reconstruction quality can bring great benefits for the later medical image analysis stage. Furthermore, the robustness of the proposed model to the non-registration environment shows its potential in real MRI applications.



### Imagine This! Scripts to Compositions to Videos
- **Arxiv ID**: http://arxiv.org/abs/1804.03608v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1804.03608v1)
- **Published**: 2018-04-10 15:59:45+00:00
- **Updated**: 2018-04-10 15:59:45+00:00
- **Authors**: Tanmay Gupta, Dustin Schwenk, Ali Farhadi, Derek Hoiem, Aniruddha Kembhavi
- **Comment**: Supplementary material included
- **Journal**: None
- **Summary**: Imagining a scene described in natural language with realistic layout and appearance of entities is the ultimate test of spatial, visual, and semantic world knowledge. Towards this goal, we present the Composition, Retrieval, and Fusion Network (CRAFT), a model capable of learning this knowledge from video-caption data and applying it while generating videos from novel captions. CRAFT explicitly predicts a temporal-layout of mentioned entities (characters and objects), retrieves spatio-temporal entity segments from a video database and fuses them to generate scene videos. Our contributions include sequential training of components of CRAFT while jointly modeling layout and appearances, and losses that encourage learning compositional representations for retrieval. We evaluate CRAFT on semantic fidelity to caption, composition consistency, and visual quality. CRAFT outperforms direct pixel generation approaches and generalizes well to unseen captions and to unseen video databases with no text annotations. We demonstrate CRAFT on FLINTSTONES, a new richly annotated video-caption dataset with over 25000 videos. For a glimpse of videos generated by CRAFT, see https://youtu.be/688Vv86n0z8.



### Looking to Listen at the Cocktail Party: A Speaker-Independent Audio-Visual Model for Speech Separation
- **Arxiv ID**: http://arxiv.org/abs/1804.03619v2
- **DOI**: 10.1145/3197517.3201357
- **Categories**: **cs.SD**, cs.CV, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1804.03619v2)
- **Published**: 2018-04-10 16:28:59+00:00
- **Updated**: 2018-08-09 21:22:37+00:00
- **Authors**: Ariel Ephrat, Inbar Mosseri, Oran Lang, Tali Dekel, Kevin Wilson, Avinatan Hassidim, William T. Freeman, Michael Rubinstein
- **Comment**: Accepted to SIGGRAPH 2018. Project webpage:
  https://looking-to-listen.github.io
- **Journal**: ACM Trans. Graph. 37(4): 112:1-112:11 (2018)
- **Summary**: We present a joint audio-visual model for isolating a single speech signal from a mixture of sounds such as other speakers and background noise. Solving this task using only audio as input is extremely challenging and does not provide an association of the separated speech signals with speakers in the video. In this paper, we present a deep network-based model that incorporates both visual and auditory signals to solve this task. The visual features are used to "focus" the audio on desired speakers in a scene and to improve the speech separation quality. To train our joint audio-visual model, we introduce AVSpeech, a new dataset comprised of thousands of hours of video segments from the Web. We demonstrate the applicability of our method to classic speech separation tasks, as well as real-world scenarios involving heated interviews, noisy bars, and screaming children, only requiring the user to specify the face of the person in the video whose speech they want to isolate. Our method shows clear advantage over state-of-the-art audio-only speech separation in cases of mixed speech. In addition, our model, which is speaker-independent (trained once, applicable to any speaker), produces better results than recent audio-visual speech separation methods that are speaker-dependent (require training a separate model for each speaker of interest).



### Audio-Visual Scene Analysis with Self-Supervised Multisensory Features
- **Arxiv ID**: http://arxiv.org/abs/1804.03641v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1804.03641v2)
- **Published**: 2018-04-10 17:36:50+00:00
- **Updated**: 2018-10-09 07:15:29+00:00
- **Authors**: Andrew Owens, Alexei A. Efros
- **Comment**: None
- **Journal**: None
- **Summary**: The thud of a bouncing ball, the onset of speech as lips open -- when visual and audio events occur together, it suggests that there might be a common, underlying event that produced both signals. In this paper, we argue that the visual and audio components of a video signal should be modeled jointly using a fused multisensory representation. We propose to learn such a representation in a self-supervised way, by training a neural network to predict whether video frames and audio are temporally aligned. We use this learned representation for three applications: (a) sound source localization, i.e. visualizing the source of sound in a video; (b) audio-visual action recognition; and (c) on/off-screen audio source separation, e.g. removing the off-screen translator's voice from a foreign official's speech. Code, models, and video results are available on our webpage: http://andrewowens.com/multisensory



### Semi-supervised Adversarial Learning to Generate Photorealistic Face Images of New Identities from 3D Morphable Model
- **Arxiv ID**: http://arxiv.org/abs/1804.03675v1
- **DOI**: 10.1007/978-3-030-01252-6_14
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.03675v1)
- **Published**: 2018-04-10 18:18:30+00:00
- **Updated**: 2018-04-10 18:18:30+00:00
- **Authors**: Baris Gecer, Binod Bhattarai, Josef Kittler, Tae-Kyun Kim
- **Comment**: None
- **Journal**: In Proceedings of the European conference on computer vision
  (ECCV), 2018, pp. 217-234
- **Summary**: We propose a novel end-to-end semi-supervised adversarial framework to generate photorealistic face images of new identities with wide ranges of expressions, poses, and illuminations conditioned by a 3D morphable model. Previous adversarial style-transfer methods either supervise their networks with large volume of paired data or use unpaired data with a highly under-constrained two-way generative framework in an unsupervised fashion. We introduce pairwise adversarial supervision to constrain two-way domain adaptation by a small number of paired real and synthetic images for training along with the large volume of unpaired data. Extensive qualitative and quantitative experiments are performed to validate our idea. Generated face images of new identities contain pose, lighting and expression diversity and qualitative results show that they are highly constraint by the synthetic input image while adding photorealism and retaining identity information. We combine face images generated by the proposed method with the real data set to train face recognition algorithms. We evaluated the model on two challenging data sets: LFW and IJB-A. We observe that the generated images from our framework consistently improves over the performance of deep face recognition network trained with Oxford VGG Face dataset and achieves comparable results to the state-of-the-art.



### French Word Recognition through a Quick Survey on Recurrent Neural Networks Using Long-Short Term Memory RNN-LSTM
- **Arxiv ID**: http://arxiv.org/abs/1804.03683v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.03683v1)
- **Published**: 2018-04-10 18:41:14+00:00
- **Updated**: 2018-04-10 18:41:14+00:00
- **Authors**: Saman Sarraf
- **Comment**: None
- **Journal**: American Scientific Research Journal for Engineering, Technology,
  and Sciences (ASRJETS) (2018) Volume 39, No 1, pp 250-267
- **Summary**: Optical character recognition (OCR) is a fundamental problem in computer vision. Research studies have shown significant progress in classifying printed characters using deep learning-based methods and topologies. Among current algorithms, recurrent neural networks with long-short term memory blocks called RNN-LSTM have provided the highest performance in terms of accuracy rate. Using the top 5,000 French words collected from the internet including all signs and accents, RNN-LSTM models were trained and tested for several cases. Six fonts were used to generate OCR samples and an additional dataset that included all samples from these six fonts was prepared for training and testing purposes. The trained RNN-LSTM models were tested and achieved the accuracy rates of 99.98798% and 99.91889% for edit distance and sequence error, respectively. An accurate preprocessing followed by height normalization (standardization methods in deep learning) enabled the RNN-LSTM model to be trained in the most efficient way. This machine learning work also revealed the robustness of RNN-LSTM topology to recognize printed characters.



### Unsupervised and semi-supervised learning with Categorical Generative Adversarial Networks assisted by Wasserstein distance for dermoscopy image Classification
- **Arxiv ID**: http://arxiv.org/abs/1804.03700v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.03700v1)
- **Published**: 2018-04-10 19:53:53+00:00
- **Updated**: 2018-04-10 19:53:53+00:00
- **Authors**: Xin Yi, Ekta Walia, Paul Babyn
- **Comment**: None
- **Journal**: None
- **Summary**: Melanoma is a curable aggressive skin cancer if detected early. Typically, the diagnosis involves initial screening with subsequent biopsy and histopathological examination if necessary. Computer aided diagnosis offers an objective score that is independent of clinical experience and the potential to lower the workload of a dermatologist. In the recent past, success of deep learning algorithms in the field of general computer vision has motivated successful application of supervised deep learning methods in computer aided melanoma recognition. However, large quantities of labeled images are required to make further improvements on the supervised method. A good annotation generally requires clinical and histological confirmation, which requires significant effort. In an attempt to alleviate this constraint, we propose to use categorical generative adversarial network to automatically learn the feature representation of dermoscopy images in an unsupervised and semi-supervised manner. Thorough experiments on ISIC 2016 skin lesion chal- lenge demonstrate that the proposed feature learning method has achieved an average precision score of 0.424 with only 140 labeled images. Moreover, the proposed method is also capable of generating real-world like dermoscopy images.



### Graph Matching with Anchor Nodes: A Learning Approach
- **Arxiv ID**: http://arxiv.org/abs/1804.03715v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.03715v1)
- **Published**: 2018-04-10 20:49:50+00:00
- **Updated**: 2018-04-10 20:49:50+00:00
- **Authors**: Nan Hu, Raif M. Rustamov, Leonidas Guibas
- **Comment**: final version for CVPR2013
- **Journal**: None
- **Summary**: In this paper, we consider the weighted graph matching problem with partially disclosed correspondences between a number of anchor nodes. Our construction exploits recently introduced node signatures based on graph Laplacians, namely the Laplacian family signature (LFS) on the nodes, and the pairwise heat kernel map on the edges. In this paper, without assuming an explicit form of parametric dependence nor a distance metric between node signatures, we formulate an optimization problem which incorporates the knowledge of anchor nodes. Solving this problem gives us an optimized proximity measure specific to the graphs under consideration. Using this as a first order compatibility term, we then set up an integer quadratic program (IQP) to solve for a near optimal graph matching. Our experiments demonstrate the superior performance of our approach on randomly generated graphs and on two widely-used image sequences, when compared with other existing signature and adjacency matrix based graph matching methods.



