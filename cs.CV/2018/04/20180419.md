# Arxiv Papers in cs.CV on 2018-04-19
### Disentangling Controllable and Uncontrollable Factors of Variation by Interacting with the World
- **Arxiv ID**: http://arxiv.org/abs/1804.06955v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.06955v2)
- **Published**: 2018-04-19 00:53:34+00:00
- **Updated**: 2018-05-22 00:10:17+00:00
- **Authors**: Yoshihide Sawada
- **Comment**: Revised version
- **Journal**: None
- **Summary**: We introduce a method to disentangle controllable and uncontrollable factors of variation by interacting with the world. Disentanglement leads to good representations and is important when applying deep neural networks (DNNs) in fields where explanations are required. This study attempts to improve an existing reinforcement learning (RL) approach to disentangle controllable and uncontrollable factors of variation, because the method lacks a mechanism to represent uncontrollable obstacles. To address this problem, we train two DNNs simultaneously: one that represents the controllable object and another that represents uncontrollable obstacles. For stable training, we applied a pretraining approach using a model robust against uncontrollable obstacles. Simulation experiments demonstrate that the proposed model can disentangle independently controllable and uncontrollable factors without annotated data.



### A-CCNN: adaptive ccnn for density estimation and crowd counting
- **Arxiv ID**: http://arxiv.org/abs/1804.06958v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.06958v2)
- **Published**: 2018-04-19 01:01:16+00:00
- **Updated**: 2018-04-20 03:57:26+00:00
- **Authors**: Saeed Amirgholipour Kasmani, Xiangjian He, Wenjing Jia, Dadong Wang, Michelle Zeibots
- **Comment**: 5 pages, 2 figures
- **Journal**: None
- **Summary**: Crowd counting, for estimating the number of people in a crowd using vision-based computer techniques, has attracted much interest in the research community. Although many attempts have been reported, real-world problems, such as huge variation in subjects' sizes in images and serious occlusion among people, make it still a challenging problem. In this paper, we propose an Adaptive Counting Convolutional Neural Network (A-CCNN) and consider the scale variation of objects in a frame adaptively so as to improve the accuracy of counting. Our method takes advantages of contextual information to provide more accurate and adaptive density maps and crowd counting in a scene. Extensively experimental evaluation is conducted using different benchmark datasets for object-counting and shows that the proposed approach is effective and outperforms state-of-the-art approaches.



### Adversarial Complementary Learning for Weakly Supervised Object Localization
- **Arxiv ID**: http://arxiv.org/abs/1804.06962v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.06962v1)
- **Published**: 2018-04-19 01:17:40+00:00
- **Updated**: 2018-04-19 01:17:40+00:00
- **Authors**: Xiaolin Zhang, Yunchao Wei, Jiashi Feng, Yi Yang, Thomas Huang
- **Comment**: CVPR 2018 Accepted
- **Journal**: None
- **Summary**: In this work, we propose Adversarial Complementary Learning (ACoL) to automatically localize integral objects of semantic interest with weak supervision. We first mathematically prove that class localization maps can be obtained by directly selecting the class-specific feature maps of the last convolutional layer, which paves a simple way to identify object regions. We then present a simple network architecture including two parallel-classifiers for object localization. Specifically, we leverage one classification branch to dynamically localize some discriminative object regions during the forward pass. Although it is usually responsive to sparse parts of the target objects, this classifier can drive the counterpart classifier to discover new and complementary object regions by erasing its discovered regions from the feature maps. With such an adversarial learning, the two parallel-classifiers are forced to leverage complementary object regions for classification and can finally generate integral object localization together. The merits of ACoL are mainly two-fold: 1) it can be trained in an end-to-end manner; 2) dynamically erasing enables the counterpart classifier to discover complementary object regions more effectively. We demonstrate the superiority of our ACoL approach in a variety of experiments. In particular, the Top-1 localization error rate on the ILSVRC dataset is 45.14%, which is the new state-of-the-art.



### GNAS: A Greedy Neural Architecture Search Method for Multi-Attribute Learning
- **Arxiv ID**: http://arxiv.org/abs/1804.06964v2
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1804.06964v2)
- **Published**: 2018-04-19 01:29:00+00:00
- **Updated**: 2018-08-01 21:45:17+00:00
- **Authors**: Siyu Huang, Xi Li, Zhi-Qi Cheng, Zhongfei Zhang, Alexander Hauptmann
- **Comment**: ACM MM 2018 (Oral)
- **Journal**: None
- **Summary**: A key problem in deep multi-attribute learning is to effectively discover the inter-attribute correlation structures. Typically, the conventional deep multi-attribute learning approaches follow the pipeline of manually designing the network architectures based on task-specific expertise prior knowledge and careful network tunings, leading to the inflexibility for various complicated scenarios in practice. Motivated by addressing this problem, we propose an efficient greedy neural architecture search approach (GNAS) to automatically discover the optimal tree-like deep architecture for multi-attribute learning. In a greedy manner, GNAS divides the optimization of global architecture into the optimizations of individual connections step by step. By iteratively updating the local architectures, the global tree-like architecture gets converged where the bottom layers are shared across relevant attributes and the branches in top layers more encode attribute-specific features. Experiments on three benchmark multi-attribute datasets show the effectiveness and compactness of neural architectures derived by GNAS, and also demonstrate the efficiency of GNAS in searching neural architectures.



### Infrared and Visible Image Fusion using a Deep Learning Framework
- **Arxiv ID**: http://arxiv.org/abs/1804.06992v4
- **DOI**: 10.1109/ICPR.2018.8546006
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.06992v4)
- **Published**: 2018-04-19 04:30:08+00:00
- **Updated**: 2018-12-18 08:36:54+00:00
- **Authors**: Hui Li, Xiao-Jun Wu, Josef Kittler
- **Comment**: 6 pages, 6 figures, 2 tables, ICPR 2018(accepted)
- **Journal**: None
- **Summary**: In recent years, deep learning has become a very active research tool which is used in many image processing fields. In this paper, we propose an effective image fusion method using a deep learning framework to generate a single image which contains all the features from infrared and visible images. First, the source images are decomposed into base parts and detail content. Then the base parts are fused by weighted-averaging. For the detail content, we use a deep learning network to extract multi-layer features. Using these features, we use l_1-norm and weighted-average strategy to generate several candidates of the fused detail content. Once we get these candidates, the max selection strategy is used to get final fused detail content. Finally, the fused image will be reconstructed by combining the fused base part and detail content. The experimental results demonstrate that our proposed method achieves state-of-the-art performance in both objective assessment and visual quality. The Code of our fusion method is available at https://github.com/hli1221/imagefusion_deeplearning



### PURE: Scalable Phase Unwrapping with Spatial Redundant Arcs
- **Arxiv ID**: http://arxiv.org/abs/1805.00321v2
- **DOI**: None
- **Categories**: **cs.OH**, cs.CV, cs.DS
- **Links**: [PDF](http://arxiv.org/pdf/1805.00321v2)
- **Published**: 2018-04-19 06:05:07+00:00
- **Updated**: 2018-05-03 04:13:51+00:00
- **Authors**: Ravi Lanka
- **Comment**: None
- **Journal**: None
- **Summary**: Phase unwrapping is a key problem in many coherent imaging systems, such as synthetic aperture radar (SAR) interferometry. A general formulation for redundant integration of finite differences for phase unwrapping (Costantini et al., 2010) was shown to produce a more reliable solution by exploiting redundant differential estimates. However, this technique requires a commercial linear programming solver for large-scale problems. For a linear cost function, we propose a method based on Dual Decomposition that breaks the given problem defined over a non-planar graph into tractable sub-problems over planar subgraphs. We also propose a decomposition technique that exploits the underlying graph structure for solving the sub-problems efficiently and guarantees asymptotic convergence to the globally optimal solution. The experimental results demonstrate that the proposed approach is comparable to the existing state-of-the-art methods in terms of the estimate with a better runtime and memory footprint.



### Large Margin Structured Convolution Operator for Thermal Infrared Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/1804.07006v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1804.07006v2)
- **Published**: 2018-04-19 06:12:02+00:00
- **Updated**: 2018-07-19 08:21:20+00:00
- **Authors**: Peng Gao, Yipeng Ma, Ke Song, Chao Li, Fei Wang, Liyi Xiao
- **Comment**: Accepted as contributed paper in ICPR'18
- **Journal**: None
- **Summary**: Compared with visible object tracking, thermal infrared (TIR) object tracking can track an arbitrary target in total darkness since it cannot be influenced by illumination variations. However, there are many unwanted attributes that constrain the potentials of TIR tracking, such as the absence of visual color patterns and low resolutions. Recently, structured output support vector machine (SOSVM) and discriminative correlation filter (DCF) have been successfully applied to visible object tracking, respectively. Motivated by these, in this paper, we propose a large margin structured convolution operator (LMSCO) to achieve efficient TIR object tracking. To improve the tracking performance, we employ the spatial regularization and implicit interpolation to obtain continuous deep feature maps, including deep appearance features and deep motion features, of the TIR targets. Finally, a collaborative optimization strategy is exploited to significantly update the operators. Our approach not only inherits the advantage of the strong discriminative capability of SOSVM but also achieves accurate and robust tracking with higher-dimensional features and more dense samples. To the best of our knowledge, we are the first to incorporate the advantages of DCF and SOSVM for TIR object tracking. Comprehensive evaluations on two thermal infrared tracking benchmarks, i.e. VOT-TIR2015 and VOT-TIR2016, clearly demonstrate that our LMSCO tracker achieves impressive results and outperforms most state-of-the-art trackers in terms of accuracy and robustness with sufficient frame rate.



### CANDID: Robust Change Dynamics and Deterministic Update Policy for Dynamic Background Subtraction
- **Arxiv ID**: http://arxiv.org/abs/1804.07008v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.07008v1)
- **Published**: 2018-04-19 06:17:19+00:00
- **Updated**: 2018-04-19 06:17:19+00:00
- **Authors**: Murari Mandal, Prafulla Saxena, Santosh Kumar Vipparthi, Subrahmanyam Murala
- **Comment**: Accepted in ICPR-2018
- **Journal**: None
- **Summary**: Background subtraction in video provides the preliminary information which is essential for many computer vision applications. In this paper, we propose a sequence of approaches named CANDID to handle the change detection problem in challenging video scenarios. The CANDID adaptively initializes the pixel-level distance threshold and update rate. These parameters are updated by computing the change dynamics at a location. Further, the background model is maintained by formulating a deterministic update policy. The performance of the proposed method is evaluated over various challenging scenarios such as dynamic background and extreme weather conditions. The qualitative and quantitative measures of the proposed method outperform the existing state-of-the-art approaches.



### To Find Where You Talk: Temporal Sentence Localization in Video with Attention Based Location Regression
- **Arxiv ID**: http://arxiv.org/abs/1804.07014v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.07014v4)
- **Published**: 2018-04-19 06:48:40+00:00
- **Updated**: 2018-11-03 06:48:06+00:00
- **Authors**: Yitian Yuan, Tao Mei, Wenwu Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Given an untrimmed video and a sentence description, temporal sentence localization aims to automatically determine the start and end points of the described sentence within the video. The problem is challenging as it needs the understanding of both video and sentence. Existing research predominantly employs a costly "scan and localize" framework, neglecting the global video context and the specific details within sentences which play as critical issues for this problem. In this paper, we propose a novel Attention Based Location Regression (ABLR) approach to solve the temporal sentence localization from a global perspective. Specifically, to preserve the context information, ABLR first encodes both video and sentence via Bidirectional LSTM networks. Then, a multi-modal co-attention mechanism is introduced to generate not only video attention which reflects the global video structure, but also sentence attention which highlights the crucial details for temporal localization. Finally, a novel attention based location regression network is designed to predict the temporal coordinates of sentence query from the previous attention. ABLR is jointly trained in an end-to-end manner. Comprehensive experiments on ActivityNet Captions and TACoS datasets demonstrate both the effectiveness and the efficiency of the proposed ABLR approach.



### VH-HFCN based Parking Slot and Lane Markings Segmentation on Panoramic Surround View
- **Arxiv ID**: http://arxiv.org/abs/1804.07027v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1804.07027v2)
- **Published**: 2018-04-19 07:56:54+00:00
- **Updated**: 2018-05-07 02:20:04+00:00
- **Authors**: Yan Wu, Tao Yang, Junqiao Zhao, Linting Guan, Wei Jiang
- **Comment**: 6 pages, 7 figures
- **Journal**: None
- **Summary**: The automatic parking is being massively developed by car manufacturers and providers. Until now, there are two problems with the automatic parking. First, there is no openly-available segmentation labels of parking slot on panoramic surround view (PSV) dataset. Second, how to detect parking slot and road structure robustly. Therefore, in this paper, we build up a public PSV dataset. At the same time, we proposed a highly fused convolutional network (HFCN) based segmentation method for parking slot and lane markings based on the PSV dataset. A surround-view image is made of four calibrated images captured from four fisheye cameras. We collect and label more than 4,200 surround view images for this task, which contain various illuminated scenes of different types of parking slots. A VH-HFCN network is proposed, which adopts an HFCN as the base, with an extra efficient VH-stage for better segmenting various markings. The VH-stage consists of two independent linear convolution paths with vertical and horizontal convolution kernels respectively. This modification enables the network to robustly and precisely extract linear features. We evaluated our model on the PSV dataset and the results showed outstanding performance in ground markings segmentation. Based on the segmented markings, parking slots and lanes are acquired by skeletonization, hough line transform and line arrangement.



### Reconstruction of Simulation-Based Physical Field by Reconstruction Neural Network Method
- **Arxiv ID**: http://arxiv.org/abs/1805.00528v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1805.00528v3)
- **Published**: 2018-04-19 08:17:08+00:00
- **Updated**: 2018-09-05 02:32:12+00:00
- **Authors**: Yu Li, Hu Wang, Kangjia Mo, Tao Zeng
- **Comment**: None
- **Journal**: None
- **Summary**: A variety of modeling techniques have been developed in the past decade to reduce the computational expense and improve the accuracy of modeling. In this study, a new framework of modeling is suggested. Compared with other popular methods, a distinctive characteristic is "from image based model to analysis based model (e.g. stress, strain, and deformation)". In such a framework, a reconstruction neural network (ReConNN) model designed for simulation-based physical field's reconstruction is proposed. The ReConNN contains two submodels that are convolutional neural network (CNN) and generative adversarial net-work (GAN). The CNN is employed to construct the mapping between contour images of physical field and objective function. Subsequently, the GAN is utilized to generate more images which are similar to the existing contour images. Finally, Lagrange polynomial is applied to complete the reconstruction. However, the existing CNN models are commonly applied to the classification tasks, which seem to be difficult to handle with regression tasks of images. Meanwhile, the existing GAN architectures are insufficient to generate high-accuracy "pseudo contour images". Therefore, a ReConNN model based on a Convolution in Convolution (CIC) and a Convolutional AutoEncoder based on Wasserstein Generative Adversarial Network (WGAN-CAE) is suggested. To evaluate the performance of the proposed model representatively, a classical topology optimization procedure is considered. Then the ReConNN is utilized to the reconstruction of heat transfer process of a pin fin heat sink. It demonstrates that the proposed ReConNN model is proved to be a potential capability to reconstruct physical field for multidisciplinary, such as structural optimization.



### Analyzing Solar Irradiance Variation From GPS and Cameras
- **Arxiv ID**: http://arxiv.org/abs/1804.07629v1
- **DOI**: None
- **Categories**: **astro-ph.IM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1804.07629v1)
- **Published**: 2018-04-19 09:04:07+00:00
- **Updated**: 2018-04-19 09:04:07+00:00
- **Authors**: Shilpa Manandhar, Soumyabrata Dev, Yee Hui Lee, Yu Song Meng
- **Comment**: Published in IEEE AP-S Symposium on Antennas and Propagation and
  USNC-URSI Radio Science Meeting, 2018
- **Journal**: None
- **Summary**: The total amount of solar irradiance falling on the earth's surface is an important area of study amongst the photo-voltaic (PV) engineers and remote sensing analysts. The received solar irradiance impacts the total amount of generated solar energy. However, this generation is often hindered by the high degree of solar irradiance variability. In this paper, we study the main factors behind such variability with the assistance of Global Positioning System (GPS) and ground-based, high-resolution sky cameras. This analysis will also be helpful for understanding cloud phenomenon and other events in the earth's atmosphere.



### Inherent Brain Segmentation Quality Control from Fully ConvNet Monte Carlo Sampling
- **Arxiv ID**: http://arxiv.org/abs/1804.07046v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.07046v2)
- **Published**: 2018-04-19 09:16:35+00:00
- **Updated**: 2018-06-08 15:40:39+00:00
- **Authors**: Abhijit Guha Roy, Sailesh Conjeti, Nassir Navab, Christian Wachinger
- **Comment**: Accepted at MICCAI 2018
- **Journal**: None
- **Summary**: We introduce inherent measures for effective quality control of brain segmentation based on a Bayesian fully convolutional neural network, using model uncertainty. Monte Carlo samples from the posterior distribution are efficiently generated using dropout at test time. Based on these samples, we introduce next to a voxel-wise uncertainty map also three metrics for structure-wise uncertainty. We then incorporate these structure-wise uncertainty in group analyses as a measure of confidence in the observation. Our results show that the metrics are highly correlated to segmentation accuracy and therefore present an inherent measure of segmentation quality. Furthermore, group analysis with uncertainty results in effect sizes closer to that of manual annotations. The introduced uncertainty metrics can not only be very useful in translation to clinical practice but also provide automated quality control and group analyses in processing large data repositories.



### Predicting resonant properties of plasmonic structures by deep learning
- **Arxiv ID**: http://arxiv.org/abs/1805.00312v1
- **DOI**: None
- **Categories**: **cs.CV**, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/1805.00312v1)
- **Published**: 2018-04-19 09:25:35+00:00
- **Updated**: 2018-04-19 09:25:35+00:00
- **Authors**: Iman Sajedian, Jeonghyun Kim, Junsuk Rho
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning can be used to extract meaningful results from images. In this paper, we used convolutional neural networks combined with recurrent neural networks on images of plasmonic structures and extract absorption data form them. To provide the required data for the model we did 100,000 simulations with similar setups and random structures. By designing a deep network we could find a model that could predict the absorption of any structure with similar setup. We used convolutional neural networks to get the spatial information from the images and we used recurrent neural networks to help the model find the relationship between the spatial information obtained from convolutional neural network model. With this design we could reach a very low loss in predicting the absorption compared to the results obtained from numerical simulation in a very short time.



### Now you see me: evaluating performance in long-term visual tracking
- **Arxiv ID**: http://arxiv.org/abs/1804.07056v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.07056v1)
- **Published**: 2018-04-19 09:41:58+00:00
- **Updated**: 2018-04-19 09:41:58+00:00
- **Authors**: Alan Lukežič, Luka Čehovin Zajc, Tomáš Vojíř, Jiří Matas, Matej Kristan
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new long-term tracking performance evaluation methodology and present a new challenging dataset of carefully selected sequences with many target disappearances. We perform an extensive evaluation of six long-term and nine short-term state-of-the-art trackers, using new performance measures, suitable for evaluating long-term tracking - tracking precision, recall and F-score. The evaluation shows that a good model update strategy and the capability of image-wide re-detection are critical for long-term tracking performance. We integrated the methodology in the VOT toolkit to automate experimental analysis and benchmarking and to facilitate the development of long-term trackers.



### Attacking Convolutional Neural Network using Differential Evolution
- **Arxiv ID**: http://arxiv.org/abs/1804.07062v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.07062v1)
- **Published**: 2018-04-19 10:05:52+00:00
- **Updated**: 2018-04-19 10:05:52+00:00
- **Authors**: Jiawei Su, Danilo Vasconcellos Vargas, Kouichi Sakurai
- **Comment**: None
- **Journal**: None
- **Summary**: The output of Convolutional Neural Networks (CNN) has been shown to be discontinuous which can make the CNN image classifier vulnerable to small well-tuned artificial perturbations. That is, images modified by adding such perturbations(i.e. adversarial perturbations) that make little difference to human eyes, can completely alter the CNN classification results. In this paper, we propose a practical attack using differential evolution(DE) for generating effective adversarial perturbations. We comprehensively evaluate the effectiveness of different types of DEs for conducting the attack on different network structures. The proposed method is a black-box attack which only requires the miracle feedback of the target CNN systems. The results show that under strict constraints which simultaneously control the number of pixels changed and overall perturbation strength, attacking can achieve 72.29%, 78.24% and 61.28% non-targeted attack success rates, with 88.68%, 99.85% and 73.07% confidence on average, on three common types of CNNs. The attack only requires modifying 5 pixels with 20.44, 14.76 and 22.98 pixel values distortion. Thus, the result shows that the current DNNs are also vulnerable to such simpler black-box attacks even under very limited attack conditions.



### Detecting Regions of Maximal Divergence for Spatio-Temporal Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/1804.07091v2
- **DOI**: 10.1109/TPAMI.2018.2823766
- **Categories**: **stat.ML**, cs.CV, cs.LG, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/1804.07091v2)
- **Published**: 2018-04-19 11:23:07+00:00
- **Updated**: 2019-07-23 07:23:39+00:00
- **Authors**: Björn Barz, Erik Rodner, Yanira Guanche Garcia, Joachim Denzler
- **Comment**: Accepted by TPAMI. Examples and code:
  https://cvjena.github.io/libmaxdiv/
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence,
  vol. 41, no. 5, pp. 1088-1101, 1 May 2019
- **Summary**: Automatic detection of anomalies in space- and time-varying measurements is an important tool in several fields, e.g., fraud detection, climate analysis, or healthcare monitoring. We present an algorithm for detecting anomalous regions in multivariate spatio-temporal time-series, which allows for spotting the interesting parts in large amounts of data, including video and text data. In opposition to existing techniques for detecting isolated anomalous data points, we propose the "Maximally Divergent Intervals" (MDI) framework for unsupervised detection of coherent spatial regions and time intervals characterized by a high Kullback-Leibler divergence compared with all other data given. In this regard, we define an unbiased Kullback-Leibler divergence that allows for ranking regions of different size and show how to enable the algorithm to run on large-scale data sets in reasonable time using an interval proposal technique. Experiments on both synthetic and real data from various domains, such as climate analysis, video surveillance, and text forensics, demonstrate that our method is widely applicable and a valuable tool for finding interesting events in different types of data.



### Part-Aligned Bilinear Representations for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1804.07094v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.07094v1)
- **Published**: 2018-04-19 11:35:19+00:00
- **Updated**: 2018-04-19 11:35:19+00:00
- **Authors**: Yumin Suh, Jingdong Wang, Siyu Tang, Tao Mei, Kyoung Mu Lee
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel network that learns a part-aligned representation for person re-identification. It handles the body part misalignment problem, that is, body parts are misaligned across human detections due to pose/viewpoint change and unreliable detection. Our model consists of a two-stream network (one stream for appearance map extraction and the other one for body part map extraction) and a bilinear-pooling layer that generates and spatially pools a part-aligned map. Each local feature of the part-aligned map is obtained by a bilinear mapping of the corresponding local appearance and body part descriptors. Our new representation leads to a robust image matching similarity, which is equivalent to an aggregation of the local similarities of the corresponding body parts combined with the weighted appearance similarity. This part-aligned representation reduces the part misalignment problem significantly. Our approach is also advantageous over other pose-guided representations (e.g., extracting representations over the bounding box of each body part) by learning part descriptors optimal for person re-identification. For training the network, our approach does not require any part annotation on the person re-identification dataset. Instead, we simply initialize the part sub-stream using a pre-trained sub-network of an existing pose estimation network, and train the whole network to minimize the re-identification loss. We validate the effectiveness of our approach by demonstrating its superiority over the state-of-the-art methods on the standard benchmark datasets, including Market-1501, CUHK03, CUHK01 and DukeMTMC, and standard video dataset MARS.



### Unsupervised Prostate Cancer Detection on H&E using Convolutional Adversarial Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/1804.07098v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1804.07098v1)
- **Published**: 2018-04-19 11:40:23+00:00
- **Updated**: 2018-04-19 11:40:23+00:00
- **Authors**: Wouter Bulten, Geert Litjens
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an unsupervised method using self-clustering convolutional adversarial autoencoders to classify prostate tissue as tumor or non-tumor without any labeled training data. The clustering method is integrated into the training of the autoencoder and requires only little post-processing. Our network trains on hematoxylin and eosin (H&E) input patches and we tested two different reconstruction targets, H&E and immunohistochemistry (IHC). We show that antibody-driven feature learning using IHC helps the network to learn relevant features for the clustering task. Our network achieves a F1 score of 0.62 using only a small set of validation labels to assign classes to clusters.



### Estimation of Tissue Oxygen Saturation from RGB Images based on Pixel-level Image Translation
- **Arxiv ID**: http://arxiv.org/abs/1804.07116v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.07116v1)
- **Published**: 2018-04-19 12:41:21+00:00
- **Updated**: 2018-04-19 12:41:21+00:00
- **Authors**: Qing-Biao Li, Xiao-Yun Zhou, Jianyu Lin, Jian-Qing Zheng, Neil T. Clancy, Daniel S. Elson
- **Comment**: None
- **Journal**: None
- **Summary**: Intra-operative measurement of tissue oxygen saturation (StO2) has been widely explored by pulse oximetry or hyperspectral imaging (HSI) to assess the function and viability of tissue. In this paper we propose a pixel- level image-to-image translation approach based on conditional Generative Adversarial Networks (cGAN) to estimate tissue oxygen saturation (StO2) directly from RGB images. The real-time performance and non-reliance on additional hardware, enable a seamless integration of the proposed method into surgical and diagnostic workflows with standard endoscope systems. For validation, RGB images and StO2 ground truth were simulated and estimated from HSI images collected by a liquid crystal tuneable filter (LCTF) endoscope for three tissue types (porcine bowel, lamb uterus and rabbit uterus). The result show that the proposed method can achieve visually identical images with comparable accuracy.



### Instance Selection Improves Geometric Mean Accuracy: A Study on Imbalanced Data Classification
- **Arxiv ID**: http://arxiv.org/abs/1804.07155v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML, 62H30
- **Links**: [PDF](http://arxiv.org/pdf/1804.07155v1)
- **Published**: 2018-04-19 13:32:50+00:00
- **Updated**: 2018-04-19 13:32:50+00:00
- **Authors**: Ludmila I. Kuncheva, Álvar Arnaiz-González, José-Francisco Díez-Pastor, Iain A. D. Gunn
- **Comment**: 11 pages, 7 figures
- **Journal**: None
- **Summary**: A natural way of handling imbalanced data is to attempt to equalise the class frequencies and train the classifier of choice on balanced data. For two-class imbalanced problems, the classification success is typically measured by the geometric mean (GM) of the true positive and true negative rates. Here we prove that GM can be improved upon by instance selection, and give the theoretical conditions for such an improvement. We demonstrate that GM is non-monotonic with respect to the number of retained instances, which discourages systematic instance selection. We also show that balancing the distribution frequencies is inferior to a direct maximisation of GM. To verify our theoretical findings, we carried out an experimental study of 12 instance selection methods for imbalanced data, using 66 standard benchmark data sets. The results reveal possible room for new instance selection methods for imbalanced data.



### Unsupervised Probabilistic Deformation Modeling for Robust Diffeomorphic Registration
- **Arxiv ID**: http://arxiv.org/abs/1804.07172v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.07172v2)
- **Published**: 2018-04-19 13:53:04+00:00
- **Updated**: 2018-07-20 13:03:25+00:00
- **Authors**: Julian Krebs, Tommaso Mansi, Boris Mailhé, Nicholas Ayache, Hervé Delingette
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a deformable registration algorithm based on unsupervised learning of a low-dimensional probabilistic parameterization of deformations. We model registration in a probabilistic and generative fashion, by applying a conditional variational autoencoder (CVAE) network. This model enables to also generate normal or pathological deformations of any new image based on the probabilistic latent space. Most recent learning-based registration algorithms use supervised labels or deformation models, that miss important properties such as diffeomorphism and sufficiently regular deformation fields. In this work, we constrain transformations to be diffeomorphic by using a differentiable exponentiation layer with a symmetric loss function. We evaluated our method on 330 cardiac MR sequences and demonstrate robust intra-subject registration results comparable to two state-of-the-art methods but with more regular deformation fields compared to a recent learning-based algorithm. Our method reached a mean DICE score of 78.3% and a mean Hausdorff distance of 7.9mm. In two preliminary experiments, we illustrate the model's abilities to transport pathological deformations to healthy subjects and to cluster five diseases in the unsupervised deformation encoding space with a classification performance of 70%.



### Recognizing Birds from Sound - The 2018 BirdCLEF Baseline System
- **Arxiv ID**: http://arxiv.org/abs/1804.07177v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.07177v1)
- **Published**: 2018-04-19 14:01:01+00:00
- **Updated**: 2018-04-19 14:01:01+00:00
- **Authors**: Stefan Kahl, Thomas Wilhelm-Stein, Holger Klinck, Danny Kowerko, Maximilian Eibl
- **Comment**: The repository and a continuative tutorial can be found here:
  https://github.com/kahst/BirdCLEF-Baseline
- **Journal**: None
- **Summary**: Reliable identification of bird species in recorded audio files would be a transformative tool for researchers, conservation biologists, and birders. In recent years, artificial neural networks have greatly improved the detection quality of machine learning systems for bird species recognition. We present a baseline system using convolutional neural networks. We publish our code base as reference for participants in the 2018 LifeCLEF bird identification task and discuss our experiments and potential improvements.



### Motion Fused Frames: Data Level Fusion Strategy for Hand Gesture Recognition
- **Arxiv ID**: http://arxiv.org/abs/1804.07187v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.07187v2)
- **Published**: 2018-04-19 14:20:50+00:00
- **Updated**: 2018-04-26 08:12:39+00:00
- **Authors**: Okan Köpüklü, Neslihan Köse, Gerhard Rigoll
- **Comment**: Accepted to CVPR 2018 as workshop paper
- **Journal**: None
- **Summary**: Acquiring spatio-temporal states of an action is the most crucial step for action classification. In this paper, we propose a data level fusion strategy, Motion Fused Frames (MFFs), designed to fuse motion information into static images as better representatives of spatio-temporal states of an action. MFFs can be used as input to any deep learning architecture with very little modification on the network. We evaluate MFFs on hand gesture recognition tasks using three video datasets - Jester, ChaLearn LAP IsoGD and NVIDIA Dynamic Hand Gesture Datasets - which require capturing long-term temporal relations of hand movements. Our approach obtains very competitive performance on Jester and ChaLearn benchmarks with the classification accuracies of 96.28% and 57.4%, respectively, while achieving state-of-the-art performance with 84.7% accuracy on NVIDIA benchmark.



### Multi-view Hybrid Embedding: A Divide-and-Conquer Approach
- **Arxiv ID**: http://arxiv.org/abs/1804.07237v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1804.07237v2)
- **Published**: 2018-04-19 15:38:15+00:00
- **Updated**: 2019-01-21 10:46:35+00:00
- **Authors**: Jiamiao Xu, Shujian Yu, Xinge You, Mengjun Leng, Xiao-Yuan Jing, C. L. Philip Chen
- **Comment**: This paper has been accepted by IEEE Transactions on Cybernetics
- **Journal**: None
- **Summary**: We present a novel cross-view classification algorithm where the gallery and probe data come from different views. A popular approach to tackle this problem is the multi-view subspace learning (MvSL) that aims to learn a latent subspace shared by multi-view data. Despite promising results obtained on some applications, the performance of existing methods deteriorates dramatically when the multi-view data is sampled from nonlinear manifolds or suffers from heavy outliers. To circumvent this drawback, motivated by the Divide-and-Conquer strategy, we propose Multi-view Hybrid Embedding (MvHE), a unique method of dividing the problem of cross-view classification into three subproblems and building one model for each subproblem. Specifically, the first model is designed to remove view discrepancy, whereas the second and third models attempt to discover the intrinsic nonlinear structure and to increase discriminability in intra-view and inter-view samples respectively. The kernel extension is conducted to further boost the representation power of MvHE. Extensive experiments are conducted on four benchmark datasets. Our methods demonstrate overwhelming advantages against the state-of-the-art MvSL based cross-view classification approaches in terms of classification accuracy and robustness.



### Weakly Supervised Representation Learning for Unsynchronized Audio-Visual Events
- **Arxiv ID**: http://arxiv.org/abs/1804.07345v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1804.07345v2)
- **Published**: 2018-04-19 19:33:11+00:00
- **Updated**: 2018-07-09 16:34:42+00:00
- **Authors**: Sanjeel Parekh, Slim Essid, Alexey Ozerov, Ngoc Q. K. Duong, Patrick Pérez, Gaël Richard
- **Comment**: None
- **Journal**: None
- **Summary**: Audio-visual representation learning is an important task from the perspective of designing machines with the ability to understand complex events. To this end, we propose a novel multimodal framework that instantiates multiple instance learning. We show that the learnt representations are useful for classifying events and localizing their characteristic audio-visual elements. The system is trained using only video-level event labels without any timing information. An important feature of our method is its capacity to learn from unsynchronized audio-visual events. We achieve state-of-the-art results on a large-scale dataset of weakly-labeled audio event videos. Visualizations of localized visual regions and audio segments substantiate our system's efficacy, especially when dealing with noisy situations where modality-specific cues appear asynchronously.



### Sampling-free Uncertainty Estimation in Gated Recurrent Units with Exponential Families
- **Arxiv ID**: http://arxiv.org/abs/1804.07351v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1804.07351v2)
- **Published**: 2018-04-19 19:40:47+00:00
- **Updated**: 2018-09-02 20:00:32+00:00
- **Authors**: Seong Jae Hwang, Ronak Mehta, Hyunwoo J. Kim, Vikas Singh
- **Comment**: Version 2
- **Journal**: None
- **Summary**: There has recently been a concerted effort to derive mechanisms in vision and machine learning systems to offer uncertainty estimates of the predictions they make. Clearly, there are enormous benefits to a system that is not only accurate but also has a sense for when it is not sure. Existing proposals center around Bayesian interpretations of modern deep architectures -- these are effective but can often be computationally demanding. We show how classical ideas in the literature on exponential families on probabilistic networks provide an excellent starting point to derive uncertainty estimates in Gated Recurrent Units (GRU). Our proposal directly quantifies uncertainty deterministically, without the need for costly sampling-based estimation. We demonstrate how our model can be used to quantitatively and qualitatively measure uncertainty in unsupervised image sequence prediction. To our knowledge, this is the first result describing sampling-free uncertainty estimation for powerful sequential models such as GRUs.



### Unsupervised Representation Adversarial Learning Network: from Reconstruction to Generation
- **Arxiv ID**: http://arxiv.org/abs/1804.07353v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1804.07353v2)
- **Published**: 2018-04-19 19:42:22+00:00
- **Updated**: 2019-04-06 16:44:38+00:00
- **Authors**: Yuqian Zhou, Kuangxiao Gu, Thomas Huang
- **Comment**: None
- **Journal**: None
- **Summary**: A good representation for arbitrarily complicated data should have the capability of semantic generation, clustering and reconstruction. Previous research has already achieved impressive performance on either one. This paper aims at learning a disentangled representation effective for all of them in an unsupervised way. To achieve all the three tasks together, we learn the forward and inverse mapping between data and representation on the basis of a symmetric adversarial process. In theory, we minimize the upper bound of the two conditional entropy loss between the latent variables and the observations together to achieve the cycle consistency. The newly proposed RepGAN is tested on MNIST, fashionMNIST, CelebA, and SVHN datasets to perform unsupervised classification, generation and reconstruction tasks. The result demonstrates that RepGAN is able to learn a useful and competitive representation. To the author's knowledge, our work is the first one to achieve both a high unsupervised classification accuracy and low reconstruction error on MNIST. Codes are available at https://github.com/yzhouas/RepGAN-tensorflow.



### Survey of Face Detection on Low-quality Images
- **Arxiv ID**: http://arxiv.org/abs/1804.07362v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.07362v1)
- **Published**: 2018-04-19 20:13:36+00:00
- **Updated**: 2018-04-19 20:13:36+00:00
- **Authors**: Yuqian Zhou, Ding Liu, Thomas Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Face detection is a well-explored problem. Many challenges on face detectors like extreme pose, illumination, low resolution and small scales are studied in the previous work. However, previous proposed models are mostly trained and tested on good-quality images which are not always the case for practical applications like surveillance systems. In this paper, we first review the current state-of-the-art face detectors and their performance on benchmark dataset FDDB, and compare the design protocols of the algorithms. Secondly, we investigate their performance degradation while testing on low-quality images with different levels of blur, noise, and contrast. Our results demonstrate that both hand-crafted and deep-learning based face detectors are not robust enough for low-quality images. It inspires researchers to produce more robust design for face detection in the wild.



### Video based Contextual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1804.07399v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1804.07399v1)
- **Published**: 2018-04-19 23:06:00+00:00
- **Updated**: 2018-04-19 23:06:00+00:00
- **Authors**: Akash Ganesan, Divyansh Pal, Karthik Muthuraman, Shubham Dash
- **Comment**: None
- **Journal**: None
- **Summary**: The primary aim of this project is to build a contextual Question-Answering model for videos. The current methodologies provide a robust model for image based Question-Answering, but we are aim to generalize this approach to be videos. We propose a graphical representation of video which is able to handle several types of queries across the whole video. For example, if a frame has an image of a man and a cat sitting, it should be able to handle queries like, where is the cat sitting with respect to the man? or ,what is the man holding in his hand?. It should be able to answer queries relating to temporal relationships also.



