# Arxiv Papers in cs.CV on 2018-04-09
### Occluded Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1804.02792v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1804.02792v3)
- **Published**: 2018-04-09 01:56:53+00:00
- **Updated**: 2018-04-20 14:22:34+00:00
- **Authors**: Jiaxuan Zhuo, Zeyu Chen, Jianhuang Lai, Guangcong Wang
- **Comment**: 6 pages, 7 figures, IEEE International Conference of Multimedia and
  Expo 2018
- **Journal**: None
- **Summary**: Person re-identification (re-id) suffers from a serious occlusion problem when applied to crowded public places. In this paper, we propose to retrieve a full-body person image by using a person image with occlusions. This differs significantly from the conventional person re-id problem where it is assumed that person images are detected without any occlusion. We thus call this new problem the occluded person re-identitification. To address this new problem, we propose a novel Attention Framework of Person Body (AFPB) based on deep learning, consisting of 1) an Occlusion Simulator (OS) which automatically generates artificial occlusions for full-body person images, and 2) multi-task losses that force the neural network not only to discriminate a person's identity but also to determine whether a sample is from the occluded data distribution or the full-body data distribution. Experiments on a new occluded person re-id dataset and three existing benchmarks modified to include full-body person images and occluded person images show the superiority of the proposed method.



### A Novel Multi-Task Tensor Correlation Neural Network for Facial Attribute Prediction
- **Arxiv ID**: http://arxiv.org/abs/1804.02810v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.02810v1)
- **Published**: 2018-04-09 04:20:40+00:00
- **Updated**: 2018-04-09 04:20:40+00:00
- **Authors**: Mingxing Duan, Kenli Li, Qi Tian
- **Comment**: Submitted to ACM Multimedia 2018
- **Journal**: None
- **Summary**: Face multi-attribute prediction benefits substantially from multi-task learning (MTL), which learns multiple face attributes simultaneously to achieve shared or mutually related representations of different attributes. The most widely used MTL convolutional neural network is heuristically or empirically designed by sharing all of the convolutional layers and splitting at the fully connected layers for task-specific losses. However, it is improper to view all low and mid-level features for different attributes as being the same, especially when these attributes are only loosely related. In this paper, we propose a novel multi-attribute tensor correlation neural network (MTCN) for face attribute prediction. The structure shares the information in low-level features (e.g., the first two convolutional layers) but splits that in high-level features (e.g., from the third convolutional layer to the fully connected layer). At the same time, during high-level feature extraction, each subnetwork (e.g., Age-Net, Gender-Net, ..., and Smile-Net) excavates closely related features from other networks to enhance its features. Then, we project the features of the C9 layers of the fine-tuned subnetworks into a highly correlated space by using a novel tensor correlation analysis algorithm (NTCCA). The final face attribute prediction is made based on the correlation matrix. Experimental results on benchmarks with multiple face attributes (CelebA and LFWA) show that the proposed approach has superior performance compared to state-of-the-art methods.



### Recovering Realistic Texture in Image Super-resolution by Deep Spatial Feature Transform
- **Arxiv ID**: http://arxiv.org/abs/1804.02815v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.02815v1)
- **Published**: 2018-04-09 04:57:06+00:00
- **Updated**: 2018-04-09 04:57:06+00:00
- **Authors**: Xintao Wang, Ke Yu, Chao Dong, Chen Change Loy
- **Comment**: This work is accepted in CVPR 2018. Our project page is
  http://mmlab.ie.cuhk.edu.hk/projects/SFTGAN/
- **Journal**: None
- **Summary**: Despite that convolutional neural networks (CNN) have recently demonstrated high-quality reconstruction for single-image super-resolution (SR), recovering natural and realistic texture remains a challenging problem. In this paper, we show that it is possible to recover textures faithful to semantic classes. In particular, we only need to modulate features of a few intermediate layers in a single network conditioned on semantic segmentation probability maps. This is made possible through a novel Spatial Feature Transform (SFT) layer that generates affine transformation parameters for spatial-wise feature modulation. SFT layers can be trained end-to-end together with the SR network using the same loss function. During testing, it accepts an input image of arbitrary size and generates a high-resolution image with just a single forward pass conditioned on the categorical priors. Our final results show that an SR network equipped with SFT can generate more realistic and visually pleasing textures in comparison to state-of-the-art SRGAN and EnhanceNet.



### A Generation Method of Immunological Memory in Clonal Selection Algorithm by using Restricted Boltzmann Machines
- **Arxiv ID**: http://arxiv.org/abs/1804.02816v1
- **DOI**: 10.1109/SMC.2015.465
- **Categories**: **cs.NE**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1804.02816v1)
- **Published**: 2018-04-09 05:14:26+00:00
- **Updated**: 2018-04-09 05:14:26+00:00
- **Authors**: Shin Kamada, Takumi Ichimura
- **Comment**: 6 pages, 10 figures, Proc. of 2015 IEEE International Conference on
  Systems, Man, and Cybernetics(IEEE SMC 2015)
- **Journal**: None
- **Summary**: Recently, a high technique of image processing is required to extract the image features in real time. In our research, the tourist subject data are collected from the Mobile Phone based Participatory Sensing (MPPS) system. Each record consists of image files with GPS, geographic location name, user's numerical evaluation, and comments written in natural language at sightseeing spots where a user really visits. In our previous research, the famous landmarks in sightseeing spot can be detected by Clonal Selection Algorithm with Immunological Memory Cell (CSAIM). However, some landmarks was not detected correctly by the previous method because they didn't have enough amount of information for the feature extraction. In order to improve the weakness, we propose the generation method of immunological memory by Restricted Boltzmann Machines. To verify the effectiveness of the method, some experiments for classification of the subjective data are executed by using machine learning tools for Deep Learning.



### Composing photomosaic images using clustering based evolutionary programming
- **Arxiv ID**: http://arxiv.org/abs/1804.02827v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1804.02827v1)
- **Published**: 2018-04-09 05:57:24+00:00
- **Updated**: 2018-04-09 05:57:24+00:00
- **Authors**: Yaodong He, Jianfeng Zhou, Shiu Yin Yuen
- **Comment**: None
- **Journal**: None
- **Summary**: Photomosaic images are a type of images consisting of various tiny images. A complete form can be seen clearly by viewing it from a long distance. Small tiny images which replace blocks of the original image can be seen clearly by viewing it from a short distance. In the past, many algorithms have been proposed trying to automatically compose photomosaic images. Most of these algorithms are designed with greedy algorithms to match the blocks with the tiny images. To obtain a better visual sense and satisfy some commercial requirements, a constraint that a tiny image should not be repeatedly used many times is usually added. With the constraint, the photomosaic problem becomes a combinatorial optimization problem. Evolutionary algorithms imitating the process of natural selection are popular and powerful in combinatorial optimization problems. However, little work has been done on applying evolutionary algorithms to photomosaic problem. In this paper, we present an algorithm called clustering based evolutionary programming to deal with the problem. We give prior knowledge to the optimization algorithm which makes the optimization process converges faster. In our experiment, the proposed algorithm is compared with the state of the art algorithms and software. The results indicate that our algorithm performs the best.



### Photometric Stereo in Participating Media Considering Shape-Dependent Forward Scatter
- **Arxiv ID**: http://arxiv.org/abs/1804.02836v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.02836v2)
- **Published**: 2018-04-09 06:25:20+00:00
- **Updated**: 2018-04-10 00:39:12+00:00
- **Authors**: Yuki Fujimura, Masaaki Iiyama, Atsushi Hashimoto, Michihiko Minoh
- **Comment**: 9 pages, accepted to CVPR 2018
- **Journal**: None
- **Summary**: Images captured in participating media such as murky water, fog, or smoke are degraded by scattered light. Thus, the use of traditional three-dimensional (3D) reconstruction techniques in such environments is difficult. In this paper, we propose a photometric stereo method for participating media. The proposed method differs from previous studies with respect to modeling shape-dependent forward scatter. In the proposed model, forward scatter is described as an analytical form using lookup tables and is represented by spatially-variant kernels. We also propose an approximation of a large-scale dense matrix as a sparse matrix, which enables the removal of forward scatter. Experiments with real and synthesized data demonstrate that the proposed method improves 3D reconstruction in participating media.



### Viewpoint-aware Video Summarization
- **Arxiv ID**: http://arxiv.org/abs/1804.02843v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.02843v2)
- **Published**: 2018-04-09 06:49:48+00:00
- **Updated**: 2018-04-10 13:38:10+00:00
- **Authors**: Atsushi Kanehira, Luc Van Gool, Yoshitaka Ushiku, Tatsuya Harada
- **Comment**: to appear at CVPR 2018
- **Journal**: None
- **Summary**: This paper introduces a novel variant of video summarization, namely building a summary that depends on the particular aspect of a video the viewer focuses on. We refer to this as $\textit{viewpoint}$. To infer what the desired $\textit{viewpoint}$ may be, we assume that several other videos are available, especially groups of videos, e.g., as folders on a person's phone or laptop. The semantic similarity between videos in a group vs. the dissimilarity between groups is used to produce $\textit{viewpoint}$-specific summaries. For considering similarity as well as avoiding redundancy, output summary should be (A) diverse, (B) representative of videos in the same group, and (C) discriminative against videos in the different groups. To satisfy these requirements (A)-(C) simultaneously, we proposed a novel video summarization method from multiple groups of videos. Inspired by Fisher's discriminant criteria, it selects summary by optimizing the combination of three terms (a) inner-summary, (b) inner-group, and (c) between-group variances defined on the feature representation of summary, which can simply represent (A)-(C). Moreover, we developed a novel dataset to investigate how well the generated summary reflects the underlying $\textit{viewpoint}$. Quantitative and qualitative experiments conducted on the dataset demonstrate the effectiveness of proposed method.



### Semantic Edge Detection with Diverse Deep Supervision
- **Arxiv ID**: http://arxiv.org/abs/1804.02864v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.02864v5)
- **Published**: 2018-04-09 08:28:08+00:00
- **Updated**: 2021-10-27 12:28:21+00:00
- **Authors**: Yun Liu, Ming-Ming Cheng, Deng-Ping Fan, Le Zhang, JiaWang Bian, Dacheng Tao
- **Comment**: International Journal of Computer Vision
- **Journal**: None
- **Summary**: Semantic edge detection (SED), which aims at jointly extracting edges as well as their category information, has far-reaching applications in domains such as semantic segmentation, object proposal generation, and object recognition. SED naturally requires achieving two distinct supervision targets: locating fine detailed edges and identifying high-level semantics. Our motivation comes from the hypothesis that such distinct targets prevent state-of-the-art SED methods from effectively using deep supervision to improve results. To this end, we propose a novel fully convolutional neural network using diverse deep supervision (DDS) within a multi-task framework where bottom layers aim at generating category-agnostic edges, while top layers are responsible for the detection of category-aware semantic edges. To overcome the hypothesized supervision challenge, a novel information converter unit is introduced, whose effectiveness has been extensively evaluated on SBD and Cityscapes datasets.



### Variational 3D-PIV with Sparse Descriptors
- **Arxiv ID**: http://arxiv.org/abs/1804.02872v1
- **DOI**: 10.1088/1361-6501/aab5a0
- **Categories**: **cs.CV**, physics.flu-dyn
- **Links**: [PDF](http://arxiv.org/pdf/1804.02872v1)
- **Published**: 2018-04-09 09:09:25+00:00
- **Updated**: 2018-04-09 09:09:25+00:00
- **Authors**: Katrin Lasinger, Christoph Vogel, Thomas Pock, Konrad Schindler
- **Comment**: to be published in Measurement Science and Technology
- **Journal**: None
- **Summary**: 3D Particle Imaging Velocimetry (3D-PIV) aim to recover the flow field in a volume of fluid, which has been seeded with tracer particles and observed from multiple camera viewpoints. The first step of 3D-PIV is to reconstruct the 3D locations of the tracer particles from synchronous views of the volume. We propose a new method for iterative particle reconstruction (IPR), in which the locations and intensities of all particles are inferred in one joint energy minimization. The energy function is designed to penalize deviations between the reconstructed 3D particles and the image evidence, while at the same time aiming for a sparse set of particles. We find that the new method, without any post-processing, achieves significantly cleaner particle volumes than a conventional, tomographic MART reconstruction, and can handle a wide range of particle densities. The second step of 3D-PIV is to then recover the dense motion field from two consecutive particle reconstructions. We propose a variational model, which makes it possible to directly include physical properties, such as incompressibility and viscosity, in the estimation of the motion field. To further exploit the sparse nature of the input data, we propose a novel, compact descriptor of the local particle layout. Hence, we avoid the memory-intensive storage of high-resolution intensity volumes. Our framework is generic and allows for a variety of different data costs (correlation measures) and regularizers. We quantitatively evaluate it with both the sum of squared differences (SSD) and the normalized cross-correlation (NCC), respectively with both a hard and a soft version of the incompressibility constraint.



### A Fully Progressive Approach to Single-Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1804.02900v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.02900v2)
- **Published**: 2018-04-09 10:28:03+00:00
- **Updated**: 2018-04-10 14:22:14+00:00
- **Authors**: Yifan Wang, Federico Perazzi, Brian McWilliams, Alexander Sorkine-Hornung, Olga Sorkine-Hornung, Christopher Schroers
- **Comment**: None
- **Journal**: None
- **Summary**: Recent deep learning approaches to single image super-resolution have achieved impressive results in terms of traditional error measures and perceptual quality. However, in each case it remains challenging to achieve high quality results for large upsampling factors. To this end, we propose a method (ProSR) that is progressive both in architecture and training: the network upsamples an image in intermediate steps, while the learning process is organized from easy to hard, as is done in curriculum learning. To obtain more photorealistic results, we design a generative adversarial network (GAN), named ProGanSR, that follows the same progressive multi-scale design principle. This not only allows to scale well to high upsampling factors (e.g., 8x) but constitutes a principled multi-scale approach that increases the reconstruction quality for all upsampling factors simultaneously. In particular ProSR ranks 2nd in terms of SSIM and 4th in terms of PSNR in the NTIRE2018 SISR challenge [34]. Compared to the top-ranking team, our model is marginally lower, but runs 5 times faster.



### Bringing Alive Blurred Moments
- **Arxiv ID**: http://arxiv.org/abs/1804.02913v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.02913v2)
- **Published**: 2018-04-09 11:14:32+00:00
- **Updated**: 2019-03-09 21:58:00+00:00
- **Authors**: Kuldeep Purohit, Anshul Shah, A. N. Rajagopalan
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: We present a solution for the goal of extracting a video from a single motion blurred image to sequentially reconstruct the clear views of a scene as beheld by the camera during the time of exposure. We first learn motion representation from sharp videos in an unsupervised manner through training of a convolutional recurrent video autoencoder network that performs a surrogate task of video reconstruction. Once trained, it is employed for guided training of a motion encoder for blurred images. This network extracts embedded motion information from the blurred image to generate a sharp video in conjunction with the trained recurrent video decoder. As an intermediate step, we also design an efficient architecture that enables real-time single image deblurring and outperforms competing methods across all factors: accuracy, speed, and compactness. Experiments on real scenes and standard datasets demonstrate the superiority of our framework over the state-of-the-art and its ability to generate a plausible sequence of temporally consistent sharp frames.



### Distribution-Aware Binarization of Neural Networks for Sketch Recognition
- **Arxiv ID**: http://arxiv.org/abs/1804.02941v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.02941v1)
- **Published**: 2018-04-09 12:31:07+00:00
- **Updated**: 2018-04-09 12:31:07+00:00
- **Authors**: Ameya Prabhu, Vishal Batchu, Sri Aurobindo Munagala, Rohit Gajawada, Anoop Namboodiri
- **Comment**: Accepted at WACV '18 (Oral)
- **Journal**: None
- **Summary**: Deep neural networks are highly effective at a range of computational tasks. However, they tend to be computationally expensive, especially in vision-related problems, and also have large memory requirements. One of the most effective methods to achieve significant improvements in computational/spatial efficiency is to binarize the weights and activations in a network. However, naive binarization results in accuracy drops when applied to networks for most tasks. In this work, we present a highly generalized, distribution-aware approach to binarizing deep networks that allows us to retain the advantages of a binarized network, while reducing accuracy drops. We also develop efficient implementations for our proposed approach across different architectures. We present a theoretical analysis of the technique to show the effective representational power of the resulting layers, and explore the forms of data they model best. Experiments on popular datasets show that our technique offers better accuracies than naive binarization, while retaining the same benefits that binarization provides - with respect to run-time compression, reduction of computational costs, and power consumption.



### Abdominal Aortic Aneurysm Segmentation with a Small Number of Training Subjects
- **Arxiv ID**: http://arxiv.org/abs/1804.02943v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.02943v1)
- **Published**: 2018-04-09 12:37:45+00:00
- **Updated**: 2018-04-09 12:37:45+00:00
- **Authors**: Jian-Qing Zheng, Xiao-Yun Zhou, Qing-Biao Li, Celia Riga, Guang-Zhong Yang
- **Comment**: 2 pages, 2 figures
- **Journal**: None
- **Summary**: Pre-operative Abdominal Aortic Aneurysm (AAA) 3D shape is critical for customized stent-graft design in Fenestrated Endovascular Aortic Repair (FEVAR). Traditional segmentation approaches implement expert-designed feature extractors while recent deep neural networks extract features automatically with multiple non-linear modules. Usually, a large training dataset is essential for applying deep learning on AAA segmentation. In this paper, the AAA was segmented using U-net with a small number (two) of training subjects. Firstly, Computed Tomography Angiography (CTA) slices were augmented with gray value variation and translation to avoid the overfitting caused by the small number of training subjects. Then, U-net was trained to segment the AAA. Dice Similarity Coefficients (DSCs) over 0.8 were achieved on the testing subjects. The PLZ, DLZ and aortic branches are all reconstructed reasonably, which will facilitate stent graft customization and help shape instantiation for intra-operative surgery navigation in FEVAR.



### Generative Adversarial Networks for Extreme Learned Image Compression
- **Arxiv ID**: http://arxiv.org/abs/1804.02958v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1804.02958v3)
- **Published**: 2018-04-09 13:13:29+00:00
- **Updated**: 2019-08-18 13:02:02+00:00
- **Authors**: Eirikur Agustsson, Michael Tschannen, Fabian Mentzer, Radu Timofte, Luc Van Gool
- **Comment**: E. Agustsson, M. Tschannen, and F. Mentzer contributed equally to
  this work. ICCV 2019 camera ready version
- **Journal**: None
- **Summary**: We present a learned image compression system based on GANs, operating at extremely low bitrates. Our proposed framework combines an encoder, decoder/generator and a multi-scale discriminator, which we train jointly for a generative learned compression objective. The model synthesizes details it cannot afford to store, obtaining visually pleasing results at bitrates where previous methods fail and show strong artifacts. Furthermore, if a semantic label map of the original image is available, our method can fully synthesize unimportant regions in the decoded image such as streets and trees from the label map, proportionally reducing the storage cost. A user study confirms that for low bitrates, our approach is preferred to state-of-the-art methods, even when they use more than double the bits.



### HyperDense-Net: A hyper-densely connected CNN for multi-modal image segmentation
- **Arxiv ID**: http://arxiv.org/abs/1804.02967v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.02967v2)
- **Published**: 2018-04-09 13:26:13+00:00
- **Updated**: 2019-03-02 01:21:07+00:00
- **Authors**: Jose Dolz, Karthik Gopinath, Jing Yuan, Herve Lombaert, Christian Desrosiers, Ismail Ben Ayed
- **Comment**: Paper accepted at IEEE TMI in October 2018. Last version of this
  paper updates the reference to the IEEE TMI paper which compares the
  submissions to the iSEG 2017 MICCAI Challenge
- **Journal**: None
- **Summary**: Recently, dense connections have attracted substantial attention in computer vision because they facilitate gradient flow and implicit deep supervision during training. Particularly, DenseNet, which connects each layer to every other layer in a feed-forward fashion, has shown impressive performances in natural image classification tasks. We propose HyperDenseNet, a 3D fully convolutional neural network that extends the definition of dense connectivity to multi-modal segmentation problems. Each imaging modality has a path, and dense connections occur not only between the pairs of layers within the same path, but also between those across different paths. This contrasts with the existing multi-modal CNN approaches, in which modeling several modalities relies entirely on a single joint layer (or level of abstraction) for fusion, typically either at the input or at the output of the network. Therefore, the proposed network has total freedom to learn more complex combinations between the modalities, within and in-between all the levels of abstraction, which increases significantly the learning representation. We report extensive evaluations over two different and highly competitive multi-modal brain tissue segmentation challenges, iSEG 2017 and MRBrainS 2013, with the former focusing on 6-month infant data and the latter on adult images. HyperDenseNet yielded significant improvements over many state-of-the-art segmentation networks, ranking at the top on both benchmarks. We further provide a comprehensive experimental analysis of features re-use, which confirms the importance of hyper-dense connections in multi-modal representation learning. Our code is publicly available at https://www.github.com/josedolz/HyperDenseNet.



### Face Sketch Synthesis Style Similarity:A New Structure Co-occurrence Texture Measure
- **Arxiv ID**: http://arxiv.org/abs/1804.02975v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.02975v1)
- **Published**: 2018-04-09 13:44:42+00:00
- **Updated**: 2018-04-09 13:44:42+00:00
- **Authors**: Deng-Ping Fan, ShengChuan Zhang, Yu-Huan Wu, Ming-Ming Cheng, Bo Ren, Rongrong Ji, Paul L Rosin
- **Comment**: 9pages, 15 figures, conference
- **Journal**: None
- **Summary**: Existing face sketch synthesis (FSS) similarity measures are sensitive to slight image degradation (e.g., noise, blur). However, human perception of the similarity of two sketches will consider both structure and texture as essential factors and is not sensitive to slight ("pixel-level") mismatches. Consequently, the use of existing similarity measures can lead to better algorithms receiving a lower score than worse algorithms. This unreliable evaluation has significantly hindered the development of the FSS field. To solve this problem, we propose a novel and robust style similarity measure called Scoot-measure (Structure CO-Occurrence Texture Measure), which simultaneously evaluates "block-level" spatial structure and co-occurrence texture statistics. In addition, we further propose 4 new meta-measures and create 2 new datasets to perform a comprehensive evaluation of several widely-used FSS measures on two large databases. Experimental results demonstrate that our measure not only provides a reliable evaluation but also achieves significantly improved performance. Specifically, the study indicated a higher degree (78.8%) of correlation between our measure and human judgment than the best prior measure (58.6%). Our code will be made available.



### Multi-views Fusion CNN for Left Ventricular Volumes Estimation on Cardiac MR Images
- **Arxiv ID**: http://arxiv.org/abs/1804.03008v1
- **DOI**: 10.1109/TBME.2017.2762762
- **Categories**: **cs.CV**, 68T10
- **Links**: [PDF](http://arxiv.org/pdf/1804.03008v1)
- **Published**: 2018-04-09 14:14:02+00:00
- **Updated**: 2018-04-09 14:14:02+00:00
- **Authors**: Gongning Luo, Suyu Dong, Kuanquan Wang, Wangmeng Zuo, Shaodong Cao, Henggui Zhang
- **Comment**: to appear on Transactions on Biomedical Engineering
- **Journal**: None
- **Summary**: Left ventricular (LV) volumes estimation is a critical procedure for cardiac disease diagnosis. The objective of this paper is to address direct LV volumes prediction task. Methods: In this paper, we propose a direct volumes prediction method based on the end-to-end deep convolutional neural networks (CNN). We study the end-to-end LV volumes prediction method in items of the data preprocessing, networks structure, and multi-views fusion strategy. The main contributions of this paper are the following aspects. First, we propose a new data preprocessing method on cardiac magnetic resonance (CMR). Second, we propose a new networks structure for end-to-end LV volumes estimation. Third, we explore the representational capacity of different slices, and propose a fusion strategy to improve the prediction accuracy. Results: The evaluation results show that the proposed method outperforms other state-of-the-art LV volumes estimation methods on the open accessible benchmark datasets. The clinical indexes derived from the predicted volumes agree well with the ground truth (EDV: R2=0.974, RMSE=9.6ml; ESV: R2=0.976, RMSE=7.1ml; EF: R2=0.828, RMSE =4.71%). Conclusion: Experimental results prove that the proposed method may be useful for LV volumes prediction task. Significance: The proposed method not only has application potential for cardiac diseases screening for large-scale CMR data, but also can be extended to other medical image research fields



### Learning at the Ends: From Hand to Tool Affordances in Humanoid Robots
- **Arxiv ID**: http://arxiv.org/abs/1804.03022v1
- **DOI**: 10.1109/DEVLRN.2017.8329826
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1804.03022v1)
- **Published**: 2018-04-09 14:28:15+00:00
- **Updated**: 2018-04-09 14:28:15+00:00
- **Authors**: Giovanni Saponaro, Pedro Vicente, Atabak Dehban, Lorenzo Jamone, Alexandre Bernardino, José Santos-Victor
- **Comment**: dataset available at htts://vislab.isr.tecnico.ulisboa.pt/, IEEE
  International Conference on Development and Learning and on Epigenetic
  Robotics (ICDL-EpiRob 2017)
- **Journal**: None
- **Summary**: One of the open challenges in designing robots that operate successfully in the unpredictable human environment is how to make them able to predict what actions they can perform on objects, and what their effects will be, i.e., the ability to perceive object affordances. Since modeling all the possible world interactions is unfeasible, learning from experience is required, posing the challenge of collecting a large amount of experiences (i.e., training data). Typically, a manipulative robot operates on external objects by using its own hands (or similar end-effectors), but in some cases the use of tools may be desirable, nevertheless, it is reasonable to assume that while a robot can collect many sensorimotor experiences using its own hands, this cannot happen for all possible human-made tools.   Therefore, in this paper we investigate the developmental transition from hand to tool affordances: what sensorimotor skills that a robot has acquired with its bare hands can be employed for tool use? By employing a visual and motor imagination mechanism to represent different hand postures compactly, we propose a probabilistic model to learn hand affordances, and we show how this model can generalize to estimate the affordances of previously unseen tools, ultimately supporting planning, decision-making and tool selection tasks in humanoid robots. We present experimental results with the iCub humanoid robot, and we publicly release the collected sensorimotor data in the form of a hand posture affordances dataset.



### Approximate k-NN Graph Construction: a Generic Online Approach
- **Arxiv ID**: http://arxiv.org/abs/1804.03032v5
- **DOI**: None
- **Categories**: **cs.IR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1804.03032v5)
- **Published**: 2018-04-09 14:49:19+00:00
- **Updated**: 2020-09-17 06:08:09+00:00
- **Authors**: Wan-Lei Zhao, Hui Wang, Chong-Wah Ngo
- **Comment**: 12 pages, 7 figures
- **Journal**: None
- **Summary**: Nearest neighbor search and k-nearest neighbor graph construction are two fundamental issues arise from many disciplines such as multimedia information retrieval, data-mining and machine learning. They become more and more imminent given the big data emerge in various fields in recent years. In this paper, a simple but effective solution both for approximate k-nearest neighbor search and approximate k-nearest neighbor graph construction is presented. These two issues are addressed jointly in our solution. On the one hand, the approximate k-nearest neighbor graph construction is treated as a search task. Each sample along with its k-nearest neighbors are joined into the k-nearest neighbor graph by performing the nearest neighbor search sequentially on the graph under construction. On the other hand, the built k-nearest neighbor graph is used to support k-nearest neighbor search. Since the graph is built online, the dynamic update on the graph, which is not possible from most of the existing solutions, is supported. This solution is feasible for various distance measures. Its effectiveness both as k-nearest neighbor construction and k-nearest neighbor search approaches is verified across different types of data in different scales, various dimensions and under different metrics.



### 3D Fluid Flow Estimation with Integrated Particle Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1804.03037v3
- **DOI**: 10.1007/s11263-019-01261-6
- **Categories**: **cs.CV**, physics.flu-dyn
- **Links**: [PDF](http://arxiv.org/pdf/1804.03037v3)
- **Published**: 2018-04-09 14:54:35+00:00
- **Updated**: 2019-11-21 19:12:58+00:00
- **Authors**: Katrin Lasinger, Christoph Vogel, Thomas Pock, Konrad Schindler
- **Comment**: To appear in International Journal of Computer Vision (IJCV)
- **Journal**: None
- **Summary**: The standard approach to densely reconstruct the motion in a volume of fluid is to inject high-contrast tracer particles and record their motion with multiple high-speed cameras. Almost all existing work processes the acquired multi-view video in two separate steps, utilizing either a pure Eulerian or pure Lagrangian approach. Eulerian methods perform a voxel-based reconstruction of particles per time step, followed by 3D motion estimation, with some form of dense matching between the precomputed voxel grids from different time steps. In this sequential procedure, the first step cannot use temporal consistency considerations to support the reconstruction, while the second step has no access to the original, high-resolution image data. Alternatively, Lagrangian methods reconstruct an explicit, sparse set of particles and track the individual particles over time. Physical constraints can only be incorporated in a post-processing step when interpolating the particle tracks to a dense motion field. We show, for the first time, how to jointly reconstruct both the individual tracer particles and a dense 3D fluid motion field from the image data, using an integrated energy minimization. Our hybrid Lagrangian/Eulerian model reconstructs individual particles, and at the same time recovers a dense 3D motion field in the entire domain. Making particles explicit greatly reduces the memory consumption and allows one to use the high-res input images for matching. Whereas the dense motion field makes it possible to include physical a-priori constraints and account for the incompressibility and viscosity of the fluid. The method exhibits greatly (~70%) improved results over our recently published baseline with two separate steps for 3D reconstruction and motion estimation. Our results with only two time steps are comparable to those of sota tracking-based methods that require much longer sequences.



### Robust fusion algorithms for unsupervised change detection between multi-band optical images - A comprehensive case study
- **Arxiv ID**: http://arxiv.org/abs/1804.03068v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.data-an
- **Links**: [PDF](http://arxiv.org/pdf/1804.03068v1)
- **Published**: 2018-04-09 15:57:22+00:00
- **Updated**: 2018-04-09 15:57:22+00:00
- **Authors**: Vinicius Ferraris, Nicolas Dobigeon, Marie Chabert
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised change detection techniques are generally constrained to two multi-band optical images acquired at different times through sensors sharing the same spatial and spectral resolution. This scenario is suitable for a straight comparison of homologous pixels such as pixel-wise differencing. However, in some specific cases such as emergency situations, the only available images may be those acquired through different kinds of sensors with different resolutions. Recently some change detection techniques dealing with images with different spatial and spectral resolutions, have been proposed. Nevertheless, they are focused on a specific scenario where one image has a high spatial and low spectral resolution while the other has a low spatial and high spectral resolution. This paper addresses the problem of detecting changes between any two multi-band optical images disregarding their spatial and spectral resolution disparities. We propose a method that effectively uses the available information by modeling the two observed images as spatially and spectrally degraded versions of two (unobserved) latent images characterized by the same high spatial and high spectral resolutions. Covering the same scene, the latent images are expected to be globally similar except for possible changes in spatially sparse locations. Thus, the change detection task is envisioned through a robust fusion task which enforces the differences between the estimated latent images to be spatially sparse. We show that this robust fusion can be formulated as an inverse problem which is iteratively solved using an alternate minimization strategy. The proposed framework is implemented for an exhaustive list of applicative scenarios and applied to real multi-band optical images. A comparison with state-of-the-art change detection methods evidences the accuracy of the proposed robust fusion-based strategy.



### Binge Watching: Scaling Affordance Learning from Sitcoms
- **Arxiv ID**: http://arxiv.org/abs/1804.03080v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.03080v1)
- **Published**: 2018-04-09 16:14:05+00:00
- **Updated**: 2018-04-09 16:14:05+00:00
- **Authors**: Xiaolong Wang, Rohit Girdhar, Abhinav Gupta
- **Comment**: CVPR 2017, project page:
  http://www.cs.cmu.edu/~xiaolonw/affordance.html
- **Journal**: None
- **Summary**: In recent years, there has been a renewed interest in jointly modeling perception and action. At the core of this investigation is the idea of modeling affordances(Affordances are opportunities of interaction in the scene. In other words, it represents what actions can the object be used for). However, when it comes to predicting affordances, even the state of the art approaches still do not use any ConvNets. Why is that? Unlike semantic or 3D tasks, there still does not exist any large-scale dataset for affordances. In this paper, we tackle the challenge of creating one of the biggest dataset for learning affordances. We use seven sitcoms to extract a diverse set of scenes and how actors interact with different objects in the scenes. Our dataset consists of more than 10K scenes and 28K ways humans can interact with these 10K images. We also propose a two-step approach to predict affordances in a new scene. In the first step, given a location in the scene we classify which of the 30 pose classes is the likely affordance pose. Given the pose class and the scene, we then use a Variational Autoencoder (VAE) to extract the scale and deformation of the pose. The VAE allows us to sample the distribution of possible poses at test time. Finally, we show the importance of large-scale data in learning a generalizable and robust model of affordances.



### Attribute-Centered Loss for Soft-Biometrics Guided Face Sketch-Photo Recognition
- **Arxiv ID**: http://arxiv.org/abs/1804.03082v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.03082v1)
- **Published**: 2018-04-09 16:15:50+00:00
- **Updated**: 2018-04-09 16:15:50+00:00
- **Authors**: Hadi Kazemi, Sobhan Soleymani, Ali Dabouei, Mehdi Iranmanesh, Nasser M. Nasrabadi
- **Comment**: Accepted as a conference paper on CVPRW 2018
- **Journal**: None
- **Summary**: Face sketches are able to capture the spatial topology of a face while lacking some facial attributes such as race, skin, or hair color. Existing sketch-photo recognition approaches have mostly ignored the importance of facial attributes. In this paper, we propose a new loss function, called attribute-centered loss, to train a Deep Coupled Convolutional Neural Network (DCCNN) for the facial attribute guided sketch to photo matching. Specifically, an attribute-centered loss is proposed which learns several distinct centers, in a shared embedding space, for photos and sketches with different combinations of attributes. The DCCNN simultaneously is trained to map photos and pairs of testified attributes and corresponding forensic sketches around their associated centers, while preserving the spatial topology information. Importantly, the centers learn to keep a relative distance from each other, related to their number of contradictory attributes. Extensive experiments are performed on composite (E-PRIP) and semi-forensic (IIIT-D Semi-forensic) databases. The proposed method significantly outperforms the state-of-the-art.



### Markerless tracking of user-defined features with deep learning
- **Arxiv ID**: http://arxiv.org/abs/1804.03142v1
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.NC, q-bio.QM, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1804.03142v1)
- **Published**: 2018-04-09 17:10:39+00:00
- **Updated**: 2018-04-09 17:10:39+00:00
- **Authors**: Alexander Mathis, Pranav Mamidanna, Taiga Abe, Kevin M. Cury, Venkatesh N. Murthy, Mackenzie W. Mathis, Matthias Bethge
- **Comment**: Videos at http://www.mousemotorlab.org/deeplabcut
- **Journal**: Nature Neuroscience, Technical Report, published: 20 August 2018
- **Summary**: Quantifying behavior is crucial for many applications in neuroscience. Videography provides easy methods for the observation and recording of animal behavior in diverse settings, yet extracting particular aspects of a behavior for further analysis can be highly time consuming. In motor control studies, humans or other animals are often marked with reflective markers to assist with computer-based tracking, yet markers are intrusive (especially for smaller animals), and the number and location of the markers must be determined a priori. Here, we present a highly efficient method for markerless tracking based on transfer learning with deep neural networks that achieves excellent results with minimal training data. We demonstrate the versatility of this framework by tracking various body parts in a broad collection of experimental settings: mice odor trail-tracking, egg-laying behavior in drosophila, and mouse hand articulation in a skilled forelimb task. For example, during the skilled reaching behavior, individual joints can be automatically tracked (and a confidence score is reported). Remarkably, even when a small number of frames are labeled ($\approx 200$), the algorithm achieves excellent tracking performance on test frames that is comparable to human accuracy.



### AMNet: Memorability Estimation with Attention
- **Arxiv ID**: http://arxiv.org/abs/1804.03115v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1804.03115v1)
- **Published**: 2018-04-09 17:28:00+00:00
- **Updated**: 2018-04-09 17:28:00+00:00
- **Authors**: Jiri Fajtl, Vasileios Argyriou, Dorothy Monekosso, Paolo Remagnino
- **Comment**: To appear at CVPR 2018
- **Journal**: None
- **Summary**: In this paper we present the design and evaluation of an end-to-end trainable, deep neural network with a visual attention mechanism for memorability estimation in still images. We analyze the suitability of transfer learning of deep models from image classification to the memorability task. Further on we study the impact of the attention mechanism on the memorability estimation and evaluate our network on the SUN Memorability and the LaMem datasets. Our network outperforms the existing state of the art models on both datasets in terms of the Spearman's rank correlation as well as the mean squared error, closely matching human consistency.



### Blazingly Fast Video Object Segmentation with Pixel-Wise Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/1804.03131v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.03131v1)
- **Published**: 2018-04-09 17:54:35+00:00
- **Updated**: 2018-04-09 17:54:35+00:00
- **Authors**: Yuhua Chen, Jordi Pont-Tuset, Alberto Montes, Luc Van Gool
- **Comment**: Accepted to CVPR 2018
- **Journal**: None
- **Summary**: This paper tackles the problem of video object segmentation, given some user annotation which indicates the object of interest. The problem is formulated as pixel-wise retrieval in a learned embedding space: we embed pixels of the same object instance into the vicinity of each other, using a fully convolutional network trained by a modified triplet loss as the embedding model. Then the annotated pixels are set as reference and the rest of the pixels are classified using a nearest-neighbor approach. The proposed method supports different kinds of user input such as segmentation mask in the first frame (semi-supervised scenario), or a sparse set of clicked points (interactive scenario). In the semi-supervised scenario, we achieve results competitive with the state of the art but at a fraction of computation cost (275 milliseconds per frame). In the interactive scenario where the user is able to refine their input iteratively, the proposed method provides instant response to each input, and reaches comparable quality to competing methods with much less interaction.



### The Sound of Pixels
- **Arxiv ID**: http://arxiv.org/abs/1804.03160v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1804.03160v4)
- **Published**: 2018-04-09 18:00:36+00:00
- **Updated**: 2018-10-14 01:09:15+00:00
- **Authors**: Hang Zhao, Chuang Gan, Andrew Rouditchenko, Carl Vondrick, Josh McDermott, Antonio Torralba
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce PixelPlayer, a system that, by leveraging large amounts of unlabeled videos, learns to locate image regions which produce sounds and separate the input sounds into a set of components that represents the sound from each pixel. Our approach capitalizes on the natural synchronization of the visual and audio modalities to learn models that jointly parse sounds and images, without requiring additional manual supervision. Experimental results on a newly collected MUSIC dataset show that our proposed Mix-and-Separate framework outperforms several baselines on source separation. Qualitative results suggest our model learns to ground sounds in vision, enabling applications such as independently adjusting the volume of sound sources.



### Improving Confidence Estimates for Unfamiliar Examples
- **Arxiv ID**: http://arxiv.org/abs/1804.03166v6
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1804.03166v6)
- **Published**: 2018-04-09 18:08:14+00:00
- **Updated**: 2020-09-07 18:42:19+00:00
- **Authors**: Zhizhong Li, Derek Hoiem
- **Comment**: Published in CVPR 2020 (oral). ERRATA: (1) a previous version (v3)
  included erroneous results for $T$-scaling, where novel samples are
  mistakenly included in the validation set for calibration. Please disregard
  those results. (2) Previous versions (v4, v5) incorrectly stated that Adam
  was used. In fact, we used SGD
- **Journal**: None
- **Summary**: Intuitively, unfamiliarity should lead to lack of confidence. In reality, current algorithms often make highly confident yet wrong predictions when faced with relevant but unfamiliar examples. A classifier we trained to recognize gender is 12 times more likely to be wrong with a 99% confident prediction if presented with a subject from a different age group than those seen during training. In this paper, we compare and evaluate several methods to improve confidence estimates for unfamiliar and familiar samples. We propose a testing methodology of splitting unfamiliar and familiar samples by attribute (age, breed, subcategory) or sampling (similar datasets collected by different people at different times). We evaluate methods including confidence calibration, ensembles, distillation, and a Bayesian model and use several metrics to analyze label, likelihood, and calibration error. While all methods reduce over-confident errors, the ensemble of calibrated models performs best overall, and T-scaling performs best among the approaches with fastest inference. Our code is available at https://github.com/lizhitwo/ConfidenceEstimates .   $\color{red}{\text{Please see UPDATED ERRATA.}}$



### Studying the Effects of Deep Brain Stimulation and Medication on the Dynamics of STN-LFP Signals for Human Behavior Analysis
- **Arxiv ID**: http://arxiv.org/abs/1804.03190v1
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1804.03190v1)
- **Published**: 2018-04-09 19:10:31+00:00
- **Updated**: 2018-04-09 19:10:31+00:00
- **Authors**: Hosein M. Golshan, Adam O. Hebb, Joshua Nedrud, Mohammad H. Mahoor
- **Comment**: 40th IEEE International Conference on Engineering in Medicine and
  Biology (IEEE EMBC), Honolulu, Hawaii, July 17-21, 2018
- **Journal**: None
- **Summary**: This paper presents the results of our recent work on studying the effects of deep brain stimulation (DBS) and medication on the dynamics of brain local field potential (LFP) signals used for behavior analysis of patients with Parkinson s disease (PD). DBS is a technique used to alleviate the severe symptoms of PD when pharmacotherapy is not very effective. Behavior recognition from the LFP signals recorded from the subthalamic nucleus (STN) has application in developing closed-loop DBS systems, where the stimulation pulse is adaptively generated according to subjects performing behavior. Most of the existing studies on behavior recognition that use STN-LFPs are based on the DBS being off. This paper discovers how the performance and accuracy of automated behavior recognition from the LFP signals are affected under different paradigms of stimulation on/off. We first study the notion of beta power suppression in LFP signals under different scenarios (stimulation on/off and medication on/off). Afterward, we explore the accuracy of support vector machines in predicting human actions (button press and reach) using the spectrogram of STN-LFP signals. Our experiments on the recorded LFP signals of three subjects confirm that the beta power is suppressed significantly when the patients take medication (p-value<0.002) or stimulation (p-value<0.0003). The results also show that we can classify different behaviors with a reasonable accuracy of 85% even when the high-amplitude stimulation is applied.



### An ADMM-Based Universal Framework for Adversarial Attacks on Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1804.03193v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1804.03193v1)
- **Published**: 2018-04-09 19:23:01+00:00
- **Updated**: 2018-04-09 19:23:01+00:00
- **Authors**: Pu Zhao, Sijia Liu, Yanzhi Wang, Xue Lin
- **Comment**: 9 pages, 3 figures, in submission
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) are known vulnerable to adversarial attacks. That is, adversarial examples, obtained by adding delicately crafted distortions onto original legal inputs, can mislead a DNN to classify them as any target labels. In a successful adversarial attack, the targeted mis-classification should be achieved with the minimal distortion added. In the literature, the added distortions are usually measured by L0, L1, L2, and L infinity norms, namely, L0, L1, L2, and L infinity attacks, respectively. However, there lacks a versatile framework for all types of adversarial attacks.   This work for the first time unifies the methods of generating adversarial examples by leveraging ADMM (Alternating Direction Method of Multipliers), an operator splitting optimization approach, such that L0, L1, L2, and L infinity attacks can be effectively implemented by this general framework with little modifications. Comparing with the state-of-the-art attacks in each category, our ADMM-based attacks are so far the strongest, achieving both the 100% attack success rate and the minimal distortion.



### Assessment of Breast Cancer Histology using Densely Connected Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1804.04595v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.04595v1)
- **Published**: 2018-04-09 19:40:02+00:00
- **Updated**: 2018-04-09 19:40:02+00:00
- **Authors**: Matthias Kohl, Christoph Walz, Florian Ludwig, Stefan Braunewell, Maximilian Baust
- **Comment**: None
- **Journal**: None
- **Summary**: Breast cancer is the most frequently diagnosed cancer and leading cause of cancer-related death among females worldwide. In this article, we investigate the applicability of densely connected convolutional neural networks to the problems of histology image classification and whole slide image segmentation in the area of computer-aided diagnoses for breast cancer. To this end, we study various approaches for transfer learning and apply them to the data set from the 2018 grand challenge on breast cancer histology images (BACH).



### Exploiting Partial Structural Symmetry For Patient-Specific Image Augmentation in Trauma Interventions
- **Arxiv ID**: http://arxiv.org/abs/1804.03224v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.03224v1)
- **Published**: 2018-04-09 20:31:08+00:00
- **Updated**: 2018-04-09 20:31:08+00:00
- **Authors**: Javad Fotouhi, Mathias Unberath, Giacomo Taylor, Arash Ghaani Farashahi, Bastian Bier, Russell H. Taylor, Greg M. Osgood, M. D., Mehran Armand, Nassir Navab
- **Comment**: JF, MU, and GT have contributed equally
- **Journal**: None
- **Summary**: In unilateral pelvic fracture reductions, surgeons attempt to reconstruct the bone fragments such that bilateral symmetry in the bony anatomy is restored. We propose to exploit this "structurally symmetric" nature of the pelvic bone, and provide intra-operative image augmentation to assist the surgeon in repairing dislocated fragments. The main challenge is to automatically estimate the desired plane of symmetry within the patient's pre-operative CT. We propose to estimate this plane using a non-linear optimization strategy, by minimizing Tukey's biweight robust estimator, relying on the partial symmetry of the anatomy. Moreover, a regularization term is designed to enforce the similarity of bone density histograms on both sides of this plane, relying on the biological fact that, even if injured, the dislocated bone segments remain within the body. The experimental results demonstrate the performance of the proposed method in estimating this "plane of partial symmetry" using CT images of both healthy and injured anatomy. Examples of unilateral pelvic fractures are used to show how intra-operative X-ray images could be augmented with the forward-projections of the mirrored anatomy, acting as objective road-map for fracture reduction procedures.



### NetAdapt: Platform-Aware Neural Network Adaptation for Mobile Applications
- **Arxiv ID**: http://arxiv.org/abs/1804.03230v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.03230v2)
- **Published**: 2018-04-09 20:45:26+00:00
- **Updated**: 2018-09-28 19:20:16+00:00
- **Authors**: Tien-Ju Yang, Andrew Howard, Bo Chen, Xiao Zhang, Alec Go, Mark Sandler, Vivienne Sze, Hartwig Adam
- **Comment**: Accepted by ECCV 2018
- **Journal**: None
- **Summary**: This work proposes an algorithm, called NetAdapt, that automatically adapts a pre-trained deep neural network to a mobile platform given a resource budget. While many existing algorithms simplify networks based on the number of MACs or weights, optimizing those indirect metrics may not necessarily reduce the direct metrics, such as latency and energy consumption. To solve this problem, NetAdapt incorporates direct metrics into its adaptation algorithm. These direct metrics are evaluated using empirical measurements, so that detailed knowledge of the platform and toolchain is not required. NetAdapt automatically and progressively simplifies a pre-trained network until the resource budget is met while maximizing the accuracy. Experiment results show that NetAdapt achieves better accuracy versus latency trade-offs on both mobile CPU and mobile GPU, compared with the state-of-the-art automated network simplification algorithms. For image classification on the ImageNet dataset, NetAdapt achieves up to a 1.7$\times$ speedup in measured inference latency with equal or higher accuracy on MobileNets (V1&V2).



### Fine-grained Activity Recognition in Baseball Videos
- **Arxiv ID**: http://arxiv.org/abs/1804.03247v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.03247v1)
- **Published**: 2018-04-09 21:32:36+00:00
- **Updated**: 2018-04-09 21:32:36+00:00
- **Authors**: AJ Piergiovanni, Michael S. Ryoo
- **Comment**: CVPR Workshop on Computer Vision in Sports
- **Journal**: CVPR Workshop on Computer Vision in Sports 2018
- **Summary**: In this paper, we introduce a challenging new dataset, MLB-YouTube, designed for fine-grained activity detection. The dataset contains two settings: segmented video classification as well as activity detection in continuous videos. We experimentally compare various recognition approaches capturing temporal structure in activity videos, by classifying segmented videos and extending those approaches to continuous videos. We also compare models on the extremely difficult task of predicting pitch speed and pitch type from broadcast baseball videos. We find that learning temporal structure is valuable for fine-grained activity recognition.



### Towards Deep Cellular Phenotyping in Placental Histology
- **Arxiv ID**: http://arxiv.org/abs/1804.03270v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.03270v2)
- **Published**: 2018-04-09 23:11:10+00:00
- **Updated**: 2018-05-25 18:40:30+00:00
- **Authors**: Michael Ferlaino, Craig A. Glastonbury, Carolina Motta-Mejia, Manu Vatish, Ingrid Granne, Stephen Kennedy, Cecilia M. Lindgren, Christoffer Nellåker
- **Comment**: Updated MRC funding material. Corrected typo that suggested
  ensembling and Inception accuracy were the same (updated to reflect the fact
  the ensemble model is 1% better than previously reported)
- **Journal**: None
- **Summary**: The placenta is a complex organ, playing multiple roles during fetal development. Very little is known about the association between placental morphological abnormalities and fetal physiology. In this work, we present an open sourced, computationally tractable deep learning pipeline to analyse placenta histology at the level of the cell. By utilising two deep Convolutional Neural Network architectures and transfer learning, we can robustly localise and classify placental cells within five classes with an accuracy of 89%. Furthermore, we learn deep embeddings encoding phenotypic knowledge that is capable of both stratifying five distinct cell populations and learn intraclass phenotypic variance. We envisage that the automation of this pipeline to population scale studies of placenta histology has the potential to improve our understanding of basic cellular placental biology and its variations, particularly its role in predicting adverse birth outcomes.



