# Arxiv Papers in cs.CV on 2018-04-02
### End-to-End Detection and Re-identification Integrated Net for Person Search
- **Arxiv ID**: http://arxiv.org/abs/1804.00376v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.00376v1)
- **Published**: 2018-04-02 02:34:35+00:00
- **Updated**: 2018-04-02 02:34:35+00:00
- **Authors**: Zhenwei He, Lei Zhang, Wei Jia
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a pedestrian detection and re-identification (re-id) integration net (I-Net) in an end-to-end learning framework. The I-Net is used in real-world video surveillance scenarios, where the target person needs to be searched in the whole scene videos, while the annotations of pedestrian bounding boxes are unavailable. By comparing to the OIM which is a work for joint detection and re-id, we have three distinct contributions. First, we introduce a Siamese architecture of I-Net instead of 1 stream, such that a verification task can be implemented. Second, we propose a novel on-line pairing loss (OLP) and hard example priority softmax loss (HEP), such that only the hard negatives are posed much attention in loss computation. Third, an on-line dictionary for negative samples storage is designed in I-Net without recording the positive samples. We show our result on person search datasets, the gap between detection and re-identification is narrowed. The superior performance can be achieved.



### Attention-based Ensemble for Deep Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/1804.00382v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.00382v2)
- **Published**: 2018-04-02 03:23:06+00:00
- **Updated**: 2018-08-31 09:12:37+00:00
- **Authors**: Wonsik Kim, Bhavya Goyal, Kunal Chawla, Jungmin Lee, Keunjoo Kwon
- **Comment**: ECCV 2018 camera-ready
- **Journal**: None
- **Summary**: Deep metric learning aims to learn an embedding function, modeled as deep neural network. This embedding function usually puts semantically similar images close while dissimilar images far from each other in the learned embedding space. Recently, ensemble has been applied to deep metric learning to yield state-of-the-art results. As one important aspect of ensemble, the learners should be diverse in their feature embeddings. To this end, we propose an attention-based ensemble, which uses multiple attention masks, so that each learner can attend to different parts of the object. We also propose a divergence loss, which encourages diversity among the learners. The proposed method is applied to the standard benchmarks of deep metric learning and experimental results show that it outperforms the state-of-the-art methods by a significant margin on image retrieval tasks.



### Low-Latency Video Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1804.00389v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.00389v1)
- **Published**: 2018-04-02 03:47:51+00:00
- **Updated**: 2018-04-02 03:47:51+00:00
- **Authors**: Yule Li, Jianping Shi, Dahua Lin
- **Comment**: Accepted by CVPR 2018 as Spotlight
- **Journal**: None
- **Summary**: Recent years have seen remarkable progress in semantic segmentation. Yet, it remains a challenging task to apply segmentation techniques to video-based applications. Specifically, the high throughput of video streams, the sheer cost of running fully convolutional networks, together with the low-latency requirements in many real-world applications, e.g. autonomous driving, present a significant challenge to the design of the video segmentation framework. To tackle this combined challenge, we develop a framework for video semantic segmentation, which incorporates two novel components: (1) a feature propagation module that adaptively fuses features over time via spatially variant convolution, thus reducing the cost of per-frame computation; and (2) an adaptive scheduler that dynamically allocate computation based on accuracy prediction. Both components work together to ensure low latency while maintaining high segmentation quality. On both Cityscapes and CamVid, the proposed framework obtained competitive performance compared to the state of the art, while substantially reducing the latency, from 360 ms to 119 ms.



### Bridging the Gap Between 2D and 3D Organ Segmentation with Volumetric Fusion Net
- **Arxiv ID**: http://arxiv.org/abs/1804.00392v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.00392v2)
- **Published**: 2018-04-02 03:57:14+00:00
- **Updated**: 2018-06-09 15:46:44+00:00
- **Authors**: Yingda Xia, Lingxi Xie, Fengze Liu, Zhuotun Zhu, Elliot K. Fishman, Alan L. Yuille
- **Comment**: 8 pages, 2 figures, accepted to MICCAI 2018
- **Journal**: None
- **Summary**: There has been a debate on whether to use 2D or 3D deep neural networks for volumetric organ segmentation. Both 2D and 3D models have their advantages and disadvantages. In this paper, we present an alternative framework, which trains 2D networks on different viewpoints for segmentation, and builds a 3D Volumetric Fusion Net (VFN) to fuse the 2D segmentation results. VFN is relatively shallow and contains much fewer parameters than most 3D networks, making our framework more efficient at integrating 3D information for segmentation. We train and test the segmentation and fusion modules individually, and propose a novel strategy, named cross-cross-augmentation, to make full use of the limited training data. We evaluate our framework on several challenging abdominal organs, and verify its superiority in segmentation accuracy and stability over existing 2D and 3D approaches.



### Generative Spatiotemporal Modeling Of Neutrophil Behavior
- **Arxiv ID**: http://arxiv.org/abs/1804.00393v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.00393v1)
- **Published**: 2018-04-02 04:07:11+00:00
- **Updated**: 2018-04-02 04:07:11+00:00
- **Authors**: Narita Pandhe, Balazs Rada, Shannon Quinn
- **Comment**: 4 pages, Accepted to 2018 IEEE International Symposium on Biomedical
  Imaging
- **Journal**: None
- **Summary**: Cell motion and appearance have a strong correlation with cell cycle and disease progression. Many contemporary efforts in machine learning utilize spatio-temporal models to predict a cell's physical state and, consequently, the advancement of disease. Alternatively, generative models learn the underlying distribution of the data, creating holistic representations that can be used in learning. In this work, we propose an aggregate model that combine Generative Adversarial Networks (GANs) and Autoregressive (AR) models to predict cell motion and appearance in human neutrophils imaged by differential interference contrast (DIC) microscopy. We bifurcate the task of learning cell statistics by leveraging GANs for the spatial component and AR models for the temporal component. The aggregate model learned results offer a promising computational environment for studying changes in organellar shape, quantity, and spatial distribution over large sequences.



### SyncGAN: Synchronize the Latent Space of Cross-modal Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1804.00410v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.00410v1)
- **Published**: 2018-04-02 06:27:50+00:00
- **Updated**: 2018-04-02 06:27:50+00:00
- **Authors**: Wen-Cheng Chen, Chien-Wen Chen, Min-Chun Hu
- **Comment**: 9 pages, Part of this work is accepted by IEEE International
  Conference on Multimedia Expo 2018
- **Journal**: None
- **Summary**: Generative adversarial network (GAN) has achieved impressive success on cross-domain generation, but it faces difficulty in cross-modal generation due to the lack of a common distribution between heterogeneous data. Most existing methods of conditional based cross-modal GANs adopt the strategy of one-directional transfer and have achieved preliminary success on text-to-image transfer. Instead of learning the transfer between different modalities, we aim to learn a synchronous latent space representing the cross-modal common concept. A novel network component named synchronizer is proposed in this work to judge whether the paired data is synchronous/corresponding or not, which can constrain the latent space of generators in the GANs. Our GAN model, named as SyncGAN, can successfully generate synchronous data (e.g., a pair of image and sound) from identical random noise. For transforming data from one modality to another, we recover the latent code by inverting the mappings of a generator and use it to generate data of different modality. In addition, the proposed model can achieve semi-supervised learning, which makes our model more flexible for practical applications.



### End-to-End Learning of Motion Representation for Video Understanding
- **Arxiv ID**: http://arxiv.org/abs/1804.00413v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.00413v1)
- **Published**: 2018-04-02 06:40:37+00:00
- **Updated**: 2018-04-02 06:40:37+00:00
- **Authors**: Lijie Fan, Wenbing Huang, Chuang Gan, Stefano Ermon, Boqing Gong, Junzhou Huang
- **Comment**: CVPR 2018 spotlight. The first two authors contributed equally to
  this paper
- **Journal**: None
- **Summary**: Despite the recent success of end-to-end learned representations, hand-crafted optical flow features are still widely used in video analysis tasks. To fill this gap, we propose TVNet, a novel end-to-end trainable neural network, to learn optical-flow-like features from data. TVNet subsumes a specific optical flow solver, the TV-L1 method, and is initialized by unfolding its optimization iterations as neural layers. TVNet can therefore be used directly without any extra learning. Moreover, it can be naturally concatenated with other task-specific networks to formulate an end-to-end architecture, thus making our method more efficient than current multi-stage approaches by avoiding the need to pre-compute and store features on disk. Finally, the parameters of the TVNet can be further fine-tuned by end-to-end training. This enables TVNet to learn richer and task-specific patterns beyond exact optical flow. Extensive experiments on two action recognition benchmarks verify the effectiveness of the proposed approach. Our TVNet achieves better accuracies than all compared methods, while being competitive with the fastest counterpart in terms of features extraction time.



### Face Alignment in Full Pose Range: A 3D Total Solution
- **Arxiv ID**: http://arxiv.org/abs/1804.01005v1
- **DOI**: 10.1109/TPAMI.2017.2778152
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.01005v1)
- **Published**: 2018-04-02 07:49:19+00:00
- **Updated**: 2018-04-02 07:49:19+00:00
- **Authors**: Xiangyu Zhu, Xiaoming Liu, Zhen Lei, Stan Z. Li
- **Comment**: Published by IEEE TPAMI in 28 November 2017. arXiv admin note: text
  overlap with arXiv:1511.07212
- **Journal**: None
- **Summary**: Face alignment, which fits a face model to an image and extracts the semantic meanings of facial pixels, has been an important topic in the computer vision community. However, most algorithms are designed for faces in small to medium poses (yaw angle is smaller than 45 degrees), which lack the ability to align faces in large poses up to 90 degrees. The challenges are three-fold. Firstly, the commonly used landmark face model assumes that all the landmarks are visible and is therefore not suitable for large poses. Secondly, the face appearance varies more drastically across large poses, from the frontal view to the profile view. Thirdly, labelling landmarks in large poses is extremely challenging since the invisible landmarks have to be guessed. In this paper, we propose to tackle these three challenges in an new alignment framework termed 3D Dense Face Alignment (3DDFA), in which a dense 3D Morphable Model (3DMM) is fitted to the image via Cascaded Convolutional Neural Networks. We also utilize 3D information to synthesize face images in profile views to provide abundant samples for training. Experiments on the challenging AFLW database show that the proposed approach achieves significant improvements over the state-of-the-art methods.



### Multi-scale Location-aware Kernel Representation for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1804.00428v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.00428v1)
- **Published**: 2018-04-02 08:27:40+00:00
- **Updated**: 2018-04-02 08:27:40+00:00
- **Authors**: Hao Wang, Qilong Wang, Mingqi Gao, Peihua Li, Wangmeng Zuo
- **Comment**: Accepted by CVPR2018
- **Journal**: None
- **Summary**: Although Faster R-CNN and its variants have shown promising performance in object detection, they only exploit simple first-order representation of object proposals for final classification and regression. Recent classification methods demonstrate that the integration of high-order statistics into deep convolutional neural networks can achieve impressive improvement, but their goal is to model whole images by discarding location information so that they cannot be directly adopted to object detection. In this paper, we make an attempt to exploit high-order statistics in object detection, aiming at generating more discriminative representations for proposals to enhance the performance of detectors. To this end, we propose a novel Multi-scale Location-aware Kernel Representation (MLKP) to capture high-order statistics of deep features in proposals. Our MLKP can be efficiently computed on a modified multi-scale feature map using a low-dimensional polynomial kernel approximation.Moreover, different from existing orderless global representations based on high-order statistics, our proposed MLKP is location retentive and sensitive so that it can be flexibly adopted to object detection. Through integrating into Faster R-CNN schema, the proposed MLKP achieves very competitive performance with state-of-the-art methods, and improves Faster R-CNN by 4.9% (mAP), 4.7% (mAP) and 5.0% (AP at IOU=[0.5:0.05:0.95]) on PASCAL VOC 2007, VOC 2012 and MS COCO benchmarks, respectively. Code is available at: https://github.com/Hwang64/MLKP.



### A Vehicle Detection Approach using Deep Learning Methodologies
- **Arxiv ID**: http://arxiv.org/abs/1804.00429v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.00429v1)
- **Published**: 2018-04-02 08:34:38+00:00
- **Updated**: 2018-04-02 08:34:38+00:00
- **Authors**: Abdullah Asim Yilmaz, Mehmet Serdar Guzel, Iman Askerbeyli, Erkan Bostanci
- **Comment**: 7 pages, 8 Figures, 1 table
- **Journal**: None
- **Summary**: The purpose of this study is to successfully train our vehicle detector using R-CNN, Faster R-CNN deep learning methods on a sample vehicle data sets and to optimize the success rate of the trained detector by providing efficient results for vehicle detection by testing the trained vehicle detector on the test data. The working method consists of six main stages. These are respectively; loading the data set, the design of the convolutional neural network, configuration of training options, training of the Faster R-CNN object detector and evaluation of trained detector. In addition, in the scope of the study, Faster R-CNN, R-CNN deep learning methods were mentioned and experimental analysis comparisons were made with the results obtained from vehicle detection.



### Deep Residual Learning for Accelerated MRI using Magnitude and Phase Networks
- **Arxiv ID**: http://arxiv.org/abs/1804.00432v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1804.00432v1)
- **Published**: 2018-04-02 09:08:02+00:00
- **Updated**: 2018-04-02 09:08:02+00:00
- **Authors**: Dongwook Lee, Jaejun Yoo, Sungho Tak, Jong Chul Ye
- **Comment**: This paper will appear in IEEE Trans. Biomedical Engineering, Special
  Section on Deep Learning
- **Journal**: None
- **Summary**: Accelerated magnetic resonance (MR) scan acquisition with compressed sensing (CS) and parallel imaging is a powerful method to reduce MR imaging scan time. However, many reconstruction algorithms have high computational costs. To address this, we investigate deep residual learning networks to remove aliasing artifacts from artifact corrupted images. The proposed deep residual learning networks are composed of magnitude and phase networks that are separately trained. If both phase and magnitude information are available, the proposed algorithm can work as an iterative k-space interpolation algorithm using framelet representation. When only magnitude data is available, the proposed approach works as an image domain post-processing algorithm. Even with strong coherent aliasing artifacts, the proposed network successfully learned and removed the aliasing artifacts, whereas current parallel and CS reconstruction methods were unable to remove these artifacts. Comparisons using single and multiple coil show that the proposed residual network provides good reconstruction results with orders of magnitude faster computational time than existing compressed sensing methods. The proposed deep learning framework may have a great potential for accelerated MR reconstruction by generating accurate results immediately.



### SINet: A Scale-insensitive Convolutional Neural Network for Fast Vehicle Detection
- **Arxiv ID**: http://arxiv.org/abs/1804.00433v2
- **DOI**: 10.1109/TITS.2018.2838132
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.00433v2)
- **Published**: 2018-04-02 09:27:09+00:00
- **Updated**: 2018-05-16 09:05:29+00:00
- **Authors**: Xiaowei Hu, Xuemiao Xu, Yongjie Xiao, Hao Chen, Shengfeng He, Jing Qin, Pheng-Ann Heng
- **Comment**: Accepted by IEEE Transactions on Intelligent Transportation Systems
  (T-ITS)
- **Journal**: IEEE Transactions on Intelligent Transportation Systems, vol. 20,
  no. 3, pp. 1010-1019, 2019
- **Summary**: Vision-based vehicle detection approaches achieve incredible success in recent years with the development of deep convolutional neural network (CNN). However, existing CNN based algorithms suffer from the problem that the convolutional features are scale-sensitive in object detection task but it is common that traffic images and videos contain vehicles with a large variance of scales. In this paper, we delve into the source of scale sensitivity, and reveal two key issues: 1) existing RoI pooling destroys the structure of small scale objects, 2) the large intra-class distance for a large variance of scales exceeds the representation capability of a single network. Based on these findings, we present a scale-insensitive convolutional neural network (SINet) for fast detecting vehicles with a large variance of scales. First, we present a context-aware RoI pooling to maintain the contextual information and original structure of small scale objects. Second, we present a multi-branch decision network to minimize the intra-class distance of features. These lightweight techniques bring zero extra time complexity but prominent detection accuracy improvement. The proposed techniques can be equipped with any deep network architectures and keep them trained end-to-end. Our SINet achieves state-of-the-art performance in terms of accuracy and speed (up to 37 FPS) on the KITTI benchmark and a new highway dataset, which contains a large variance of scales and extremely small objects.



### Exploring to learn visual saliency: The RL-IAC approach
- **Arxiv ID**: http://arxiv.org/abs/1804.00435v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1804.00435v1)
- **Published**: 2018-04-02 09:39:22+00:00
- **Updated**: 2018-04-02 09:39:22+00:00
- **Authors**: Celine Craye, Timothee Lesort, David Filliat, Jean-Francois Goudou
- **Comment**: None
- **Journal**: None
- **Summary**: The problem of object localization and recognition on autonomous mobile robots is still an active topic. In this context, we tackle the problem of learning a model of visual saliency directly on a robot. This model, learned and improved on-the-fly during the robot's exploration provides an efficient tool for localizing relevant objects within their environment. The proposed approach includes two intertwined components. On the one hand, we describe a method for learning and incrementally updating a model of visual saliency from a depth-based object detector. This model of saliency can also be exploited to produce bounding box proposals around objects of interest. On the other hand, we investigate an autonomous exploration technique to efficiently learn such a saliency model. The proposed exploration, called Reinforcement Learning-Intelligent Adaptive Curiosity (RL-IAC) is able to drive the robot's exploration so that samples selected by the robot are likely to improve the current model of saliency. We then demonstrate that such a saliency model learned directly on a robot outperforms several state-of-the-art saliency techniques, and that RL-IAC can drastically decrease the required time for learning a reliable saliency model.



### Fixed-sized representation learning from Offline Handwritten Signatures of different sizes
- **Arxiv ID**: http://arxiv.org/abs/1804.00448v2
- **DOI**: 10.1007/s10032-018-0301-6
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1804.00448v2)
- **Published**: 2018-04-02 11:07:01+00:00
- **Updated**: 2018-04-24 17:06:46+00:00
- **Authors**: Luiz G. Hafemann, Robert Sabourin, Luiz S. Oliveira
- **Comment**: This is a pre-print of an article published in the International
  Journal on Document Analysis and Recognition
- **Journal**: None
- **Summary**: Methods for learning feature representations for Offline Handwritten Signature Verification have been successfully proposed in recent literature, using Deep Convolutional Neural Networks to learn representations from signature pixels. Such methods reported large performance improvements compared to handcrafted feature extractors. However, they also introduced an important constraint: the inputs to the neural networks must have a fixed size, while signatures vary significantly in size between different users. In this paper we propose addressing this issue by learning a fixed-sized representation from variable-sized signatures by modifying the network architecture, using Spatial Pyramid Pooling. We also investigate the impact of the resolution of the images used for training, and the impact of adapting (fine-tuning) the representations to new operating conditions (different acquisition protocols, such as writing instruments and scan resolution). On the GPDS dataset, we achieve results comparable with the state-of-the-art, while removing the constraint of having a maximum size for the signatures to be processed. We also show that using higher resolutions (300 or 600dpi) can improve performance when skilled forgeries from a subset of users are available for feature learning, but lower resolutions (around 100dpi) can be used if only genuine signatures are used. Lastly, we show that fine-tuning can improve performance when the operating conditions change.



### Regional Priority Based Anomaly Detection using Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/1804.00492v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1804.00492v1)
- **Published**: 2018-04-02 13:49:01+00:00
- **Updated**: 2018-04-02 13:49:01+00:00
- **Authors**: Shruti Mittal, Dattaraj Rao
- **Comment**: 5 pages, 5 figures
- **Journal**: None
- **Summary**: In the recent times, autoencoders, besides being used for compression, have been proven quite useful even for regenerating similar images or help in image denoising. They have also been explored for anomaly detection in a few cases. However, due to location invariance property of convolutional neural network, autoencoders tend to learn from or search for learned features in the complete image. This creates issues when all the items in the image are not equally important and their location matters. For such cases, a semi supervised solution - regional priority based autoencoder (RPAE) has been proposed. In this model, similar to object detection models, a region proposal network identifies the relevant areas in the images as belonging to one of the predefined categories and then those bounding boxes are fed into appropriate decoder based on the category they belong to. Finally, the error scores from all the decoders are combined based on their importance to provide total reconstruction error.



### Multilayer Complex Network Descriptors for Color-Texture Characterization
- **Arxiv ID**: http://arxiv.org/abs/1804.00501v1
- **DOI**: 10.1016/j.ins.2019.02.060
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.00501v1)
- **Published**: 2018-04-02 13:55:43+00:00
- **Updated**: 2018-04-02 13:55:43+00:00
- **Authors**: Leonardo F S Scabini, Rayner H M Condori, Wesley N Gonçalves, Odemir M Bruno
- **Comment**: 20 pages, 7 figures and 4 tables
- **Journal**: None
- **Summary**: A new method based on complex networks is proposed for color-texture analysis. The proposal consists on modeling the image as a multilayer complex network where each color channel is a layer, and each pixel (in each color channel) is represented as a network vertex. The network dynamic evolution is accessed using a set of modeling parameters (radii and thresholds), and new characterization techniques are introduced to capt information regarding within and between color channel spatial interaction. An automatic and adaptive approach for threshold selection is also proposed. We conduct classification experiments on 5 well-known datasets: Vistex, Usptex, Outex13, CURet and MBT. Results among various literature methods are compared, including deep convolutional neural networks with pre-trained architectures. The proposed method presented the highest overall performance over the 5 datasets, with 97.7 of mean accuracy against 97.0 achieved by the ResNet convolutional neural network with 50 layers.



### Average Biased ReLU Based CNN Descriptor for Improved Face Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1804.02051v2
- **DOI**: 10.1007/s11042-020-10269-x
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.02051v2)
- **Published**: 2018-04-02 15:03:02+00:00
- **Updated**: 2022-01-23 04:15:21+00:00
- **Authors**: Shiv Ram Dubey, Soumendu Chakraborty
- **Comment**: Published by Multimedia Tools and Applications, Springer
- **Journal**: None
- **Summary**: The convolutional neural networks (CNN), including AlexNet, GoogleNet, VGGNet, etc. extract features for many computer vision problems which are very discriminative. The trained CNN model over one dataset performs reasonably well whereas on another dataset of similar type the hand-designed feature descriptor outperforms the same trained CNN model. The Rectified Linear Unit (ReLU) layer discards some values in order to introduce the non-linearity. In this paper, it is proposed that the discriminative ability of deep image representation using trained model can be improved by Average Biased ReLU (AB-ReLU) at the last few layers. Basically, AB-ReLU improves the discriminative ability in two ways: 1) it exploits some of the discriminative and discarded negative information of ReLU and 2) it also neglects the irrelevant and positive information used in ReLU. The VGGFace model trained in MatConvNet over the VGG-Face dataset is used as the feature descriptor for face retrieval over other face datasets. The proposed approach is tested over six challenging, unconstrained and robust face datasets (PubFig, LFW, PaSC, AR, FERET and ExtYale) and also on a large scale face dataset (PolyUNIR) in retrieval framework. It is observed that the AB-ReLU outperforms the ReLU when used with a pre-trained VGGFace model over the face datasets. The validation error by training the network after replacing all ReLUs with AB-ReLUs is also observed to be favorable over each dataset. The AB-ReLU even outperforms the state-of-the-art activation functions, such as Sigmoid, ReLU, Leaky ReLU and Flexible ReLU over all seven face datasets.



### Learning Intrinsic Image Decomposition from Watching the World
- **Arxiv ID**: http://arxiv.org/abs/1804.00582v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.00582v1)
- **Published**: 2018-04-02 15:06:11+00:00
- **Updated**: 2018-04-02 15:06:11+00:00
- **Authors**: Zhengqi Li, Noah Snavely
- **Comment**: CVPR, 2018
- **Journal**: None
- **Summary**: Single-view intrinsic image decomposition is a highly ill-posed problem, and so a promising approach is to learn from large amounts of data. However, it is difficult to collect ground truth training data at scale for intrinsic images. In this paper, we explore a different approach to learning intrinsic images: observing image sequences over time depicting the same scene under changing illumination, and learning single-view decompositions that are consistent with these changes. This approach allows us to learn without ground truth decompositions, and to instead exploit information available from multiple images when training. Our trained model can then be applied at test time to single views. We describe a new learning framework based on this idea, including new loss functions that can be efficiently evaluated over entire sequences. While prior learning-based methods achieve good performance on specific benchmarks, we show that our approach generalizes well to several diverse datasets, including MIT intrinsic images, Intrinsic Images in the Wild and Shading Annotations in the Wild.



### Learning Descriptor Networks for 3D Shape Synthesis and Analysis
- **Arxiv ID**: http://arxiv.org/abs/1804.00586v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.00586v1)
- **Published**: 2018-04-02 15:15:34+00:00
- **Updated**: 2018-04-02 15:15:34+00:00
- **Authors**: Jianwen Xie, Zilong Zheng, Ruiqi Gao, Wenguan Wang, Song-Chun Zhu, Ying Nian Wu
- **Comment**: CVPR 2018
- **Journal**: None
- **Summary**: This paper proposes a 3D shape descriptor network, which is a deep convolutional energy-based model, for modeling volumetric shape patterns. The maximum likelihood training of the model follows an "analysis by synthesis" scheme and can be interpreted as a mode seeking and mode shifting process. The model can synthesize 3D shape patterns by sampling from the probability distribution via MCMC such as Langevin dynamics. The model can be used to train a 3D generator network via MCMC teaching. The conditional version of the 3D shape descriptor net can be used for 3D object recovery and 3D object super-resolution. Experiments demonstrate that the proposed model can generate realistic 3D shape patterns and can be useful for 3D shape analysis.



### MegaDepth: Learning Single-View Depth Prediction from Internet Photos
- **Arxiv ID**: http://arxiv.org/abs/1804.00607v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.00607v4)
- **Published**: 2018-04-02 16:03:34+00:00
- **Updated**: 2018-11-28 01:12:43+00:00
- **Authors**: Zhengqi Li, Noah Snavely
- **Comment**: updated paper for 'MegaDepth: Learning Single-View Depth Prediction
  from Internet Photos', CVPR, 2018
- **Journal**: None
- **Summary**: Single-view depth prediction is a fundamental problem in computer vision. Recently, deep learning methods have led to significant progress, but such methods are limited by the available training data. Current datasets based on 3D sensors have key limitations, including indoor-only images (NYU), small numbers of training examples (Make3D), and sparse sampling (KITTI). We propose to use multi-view Internet photo collections, a virtually unlimited data source, to generate training data via modern structure-from-motion and multi-view stereo (MVS) methods, and present a large depth dataset called MegaDepth based on this idea. Data derived from MVS comes with its own challenges, including noise and unreconstructable objects. We address these challenges with new data cleaning methods, as well as automatically augmenting our data with ordinal depth relations generated using semantic segmentation. We validate the use of large amounts of Internet data by showing that models trained on MegaDepth exhibit strong generalization-not only to novel scenes, but also to other diverse datasets including Make3D, KITTI, and DIW, even when no images from those datasets are seen during training.



### Updating the generator in PPGN-h with gradients flowing through the encoder
- **Arxiv ID**: http://arxiv.org/abs/1804.00630v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.00630v1)
- **Published**: 2018-04-02 17:22:00+00:00
- **Updated**: 2018-04-02 17:22:00+00:00
- **Authors**: Hesam Pakdaman
- **Comment**: Master's thesis, 7 pages
- **Journal**: None
- **Summary**: The Generative Adversarial Network framework has shown success in implicitly modeling data distributions and is able to generate realistic samples. Its architecture is comprised of a generator, which produces fake data that superficially seem to belong to the real data distribution, and a discriminator which is to distinguish fake from genuine samples. The Noiseless Joint Plug & Play model offers an extension to the framework by simultaneously training autoencoders. This model uses a pre-trained encoder as a feature extractor, feeding the generator with global information. Using the Plug & Play network as baseline, we design a new model by adding discriminators to the Plug & Play architecture. These additional discriminators are trained to discern real and fake latent codes, which are the output of the encoder using genuine and generated inputs, respectively. We proceed to investigate whether this approach is viable. Experiments conducted for the MNIST manifold show that this indeed is the case.



### 3D Registration of Curves and Surfaces using Local Differential Information
- **Arxiv ID**: http://arxiv.org/abs/1804.00637v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.00637v1)
- **Published**: 2018-04-02 17:33:15+00:00
- **Updated**: 2018-04-02 17:33:15+00:00
- **Authors**: Carolina Raposo, Joao P. Barreto
- **Comment**: CVPR 2018
- **Journal**: None
- **Summary**: This article presents for the first time a global method for registering 3D curves with 3D surfaces without requiring an initialization. The algorithm works with 2-tuples point+vector that consist in pairs of points augmented with the information of their tangents or normals. A closed-form solution for determining the alignment transformation from a pair of matching 2-tuples is proposed. In addition, the set of necessary conditions for two 2-tuples to match is derived. This allows fast search of correspondences that are used in an hypothesise-and-test framework for accomplishing global registration. Comparative experiments demonstrate that the proposed algorithm is the first effective solution for curve vs surface registration, with the method achieving accurate alignment in situations of small overlap and large percentage of outliers in a fraction of a second. The proposed framework is extended to the cases of curve vs curve and surface vs surface registration, with the former being particularly relevant since it is also a largely unsolved problem.



### Universal Planning Networks
- **Arxiv ID**: http://arxiv.org/abs/1804.00645v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1804.00645v2)
- **Published**: 2018-04-02 17:51:53+00:00
- **Updated**: 2018-04-04 17:36:36+00:00
- **Authors**: Aravind Srinivas, Allan Jabri, Pieter Abbeel, Sergey Levine, Chelsea Finn
- **Comment**: Videos available at https://sites.google.com/view/upn-public/home
- **Journal**: None
- **Summary**: A key challenge in complex visuomotor control is learning abstract representations that are effective for specifying goals, planning, and generalization. To this end, we introduce universal planning networks (UPN). UPNs embed differentiable planning within a goal-directed policy. This planning computation unrolls a forward model in a latent space and infers an optimal action plan through gradient descent trajectory optimization. The plan-by-gradient-descent process and its underlying representations are learned end-to-end to directly optimize a supervised imitation learning objective. We find that the representations learned are not only effective for goal-directed visual imitation via gradient-based trajectory optimization, but can also provide a metric for specifying goals using images. The learned representations can be leveraged to specify distance-based rewards to reach new target states for model-free reinforcement learning, resulting in substantially more effective learning when solving new tasks described via image-based goals. We were able to achieve successful transfer of visuomotor planning strategies across robots with significantly different morphologies and actuation capabilities.



### DeepMVS: Learning Multi-view Stereopsis
- **Arxiv ID**: http://arxiv.org/abs/1804.00650v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.00650v1)
- **Published**: 2018-04-02 17:58:45+00:00
- **Updated**: 2018-04-02 17:58:45+00:00
- **Authors**: Po-Han Huang, Kevin Matzen, Johannes Kopf, Narendra Ahuja, Jia-Bin Huang
- **Comment**: CVPR 2018. Project page: https://phuang17.github.io/DeepMVS/ Code:
  https://github.com/phuang17/DeepMVS
- **Journal**: None
- **Summary**: We present DeepMVS, a deep convolutional neural network (ConvNet) for multi-view stereo reconstruction. Taking an arbitrary number of posed images as input, we first produce a set of plane-sweep volumes and use the proposed DeepMVS network to predict high-quality disparity maps. The key contributions that enable these results are (1) supervised pretraining on a photorealistic synthetic dataset, (2) an effective method for aggregating information across a set of unordered images, and (3) integrating multi-layer feature activations from the pre-trained VGG-19 network. We validate the efficacy of DeepMVS using the ETH3D Benchmark. Our results show that DeepMVS compares favorably against state-of-the-art conventional MVS algorithms and other ConvNet based methods, particularly for near-textureless regions and thin structures.



### Interactive Hand Pose Estimation: Boosting accuracy in localizing extended finger joints
- **Arxiv ID**: http://arxiv.org/abs/1804.00651v2
- **DOI**: 10.2352/ISSN.2470-1173.2018.2.VIPC-251
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1804.00651v2)
- **Published**: 2018-04-02 17:59:38+00:00
- **Updated**: 2018-07-25 15:31:40+00:00
- **Authors**: Cairong Zhang, Guijin Wang, Hengkai Guo, Xinghao Chen, Fei Qiao, Huazhong Yang
- **Comment**: Original publication available on
  https://doi.org/10.2352/ISSN.2470-1173.2018.2.VIPC-251
- **Journal**: Electronic Imaging, Visual Information Processing and
  Communication IX (2018), pp. 251-1-251-6(6)
- **Summary**: Accurate 3D hand pose estimation plays an important role in Human Machine Interaction (HMI). In the reality of HMI, joints in fingers stretching out, especially corresponding fingertips, are much more important than other joints. We propose a novel method to refine stretching-out finger joint locations after obtaining rough hand pose estimation. It first detects which fingers are stretching out, then neighbor pixels of certain joint vote for its new location based on random forests. The algorithm is tested on two public datasets: MSRA15 and ICVL. After the refinement stage of stretching-out fingers, errors of predicted HMI finger joint locations are significantly reduced. Mean error of all fingertips reduces around 5mm (relatively more than 20%). Stretching-out fingertip locations are even more precise, which in MSRA15 reduces 10.51mm (relatively 41.4%).



### Hierarchical Novelty Detection for Visual Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/1804.00722v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1804.00722v2)
- **Published**: 2018-04-02 20:36:43+00:00
- **Updated**: 2018-06-15 10:18:15+00:00
- **Authors**: Kibok Lee, Kimin Lee, Kyle Min, Yuting Zhang, Jinwoo Shin, Honglak Lee
- **Comment**: CVPR 2018
- **Journal**: None
- **Summary**: Deep neural networks have achieved impressive success in large-scale visual object recognition tasks with a predefined set of classes. However, recognizing objects of novel classes unseen during training still remains challenging. The problem of detecting such novel classes has been addressed in the literature, but most prior works have focused on providing simple binary or regressive decisions, e.g., the output would be "known," "novel," or corresponding confidence intervals. In this paper, we study more informative novelty detection schemes based on a hierarchical classification framework. For an object of a novel class, we aim for finding its closest super class in the hierarchical taxonomy of known classes. To this end, we propose two different approaches termed top-down and flatten methods, and their combination as well. The essential ingredients of our methods are confidence-calibrated classifiers, data relabeling, and the leave-one-out strategy for modeling novel classes under the hierarchical taxonomy. Furthermore, our method can generate a hierarchical embedding that leads to improved generalized zero-shot learning performance in combination with other commonly-used semantic embeddings.



### Confidence from Invariance to Image Transformations
- **Arxiv ID**: http://arxiv.org/abs/1804.00657v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.00657v1)
- **Published**: 2018-04-02 20:38:52+00:00
- **Updated**: 2018-04-02 20:38:52+00:00
- **Authors**: Yuval Bahat, Gregory Shakhnarovich
- **Comment**: None
- **Journal**: None
- **Summary**: We develop a technique for automatically detecting the classification errors of a pre-trained visual classifier. Our method is agnostic to the form of the classifier, requiring access only to classifier responses to a set of inputs. We train a parametric binary classifier (error/correct) on a representation derived from a set of classifier responses generated from multiple copies of the same input, each subject to a different natural image transformation. Thus, we establish a measure of confidence in classifier's decision by analyzing the invariance of its decision under various transformations. In experiments with multiple data sets (STL-10,CIFAR-100,ImageNet) and classifiers, we demonstrate new state of the art for the error detection task. In addition, we apply our technique to novelty detection scenarios, where we also demonstrate state of the art results.



### Forecasting Future Humphrey Visual Fields Using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1804.04543v1
- **DOI**: 10.1371/journal.pone.0214875
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1804.04543v1)
- **Published**: 2018-04-02 21:05:22+00:00
- **Updated**: 2018-04-02 21:05:22+00:00
- **Authors**: Joanne C. Wen, Cecilia S. Lee, Pearse A. Keane, Sa Xiao, Yue Wu, Ariel Rokem, Philip P. Chen, Aaron Y. Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Purpose: To determine if deep learning networks could be trained to forecast a future 24-2 Humphrey Visual Field (HVF).   Participants: All patients who obtained a HVF 24-2 at the University of Washington.   Methods: All datapoints from consecutive 24-2 HVFs from 1998 to 2018 were extracted from a University of Washington database. Ten-fold cross validation with a held out test set was used to develop the three main phases of model development: model architecture selection, dataset combination selection, and time-interval model training with transfer learning, to train a deep learning artificial neural network capable of generating a point-wise visual field prediction.   Results: More than 1.7 million perimetry points were extracted to the hundredth decibel from 32,443 24-2 HVFs. The best performing model with 20 million trainable parameters, CascadeNet-5, was selected. The overall MAE for the test set was 2.47 dB (95% CI: 2.45 dB to 2.48 dB). The 100 fully trained models were able to successfully predict progressive field loss in glaucomatous eyes up to 5.5 years in the future with a correlation of 0.92 between the MD of predicted and actual future HVF (p < 2.2 x 10 -16 ) and an average difference of 0.41 dB.   Conclusions: Using unfiltered real-world datasets, deep learning networks show an impressive ability to not only learn spatio-temporal HVF changes but also to generate predictions for future HVFs up to 5.5 years, given only a single HVF.



