# Arxiv Papers in cs.CV on 2018-04-22
### Decoupled Networks
- **Arxiv ID**: http://arxiv.org/abs/1804.08071v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1804.08071v1)
- **Published**: 2018-04-22 05:26:08+00:00
- **Updated**: 2018-04-22 05:26:08+00:00
- **Authors**: Weiyang Liu, Zhen Liu, Zhiding Yu, Bo Dai, Rongmei Lin, Yisen Wang, James M. Rehg, Le Song
- **Comment**: CVPR 2018 (Spotlight)
- **Journal**: None
- **Summary**: Inner product-based convolution has been a central component of convolutional neural networks (CNNs) and the key to learning visual representations. Inspired by the observation that CNN-learned features are naturally decoupled with the norm of features corresponding to the intra-class variation and the angle corresponding to the semantic difference, we propose a generic decoupled learning framework which models the intra-class variation and semantic difference independently. Specifically, we first reparametrize the inner product to a decoupled form and then generalize it to the decoupled convolution operator which serves as the building block of our decoupled networks. We present several effective instances of the decoupled convolution operator. Each decoupled operator is well motivated and has an intuitive geometric interpretation. Based on these decoupled operators, we further propose to directly learn the operator from data. Extensive experiments show that such decoupled reparameterization renders significant performance gain with easier convergence and stronger robustness.



### Anchor-based Nearest Class Mean Loss for Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1804.08087v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.08087v1)
- **Published**: 2018-04-22 08:49:59+00:00
- **Updated**: 2018-04-22 08:49:59+00:00
- **Authors**: Fusheng Hao, Jun Cheng, Lei Wang, Xinchao Wang, Jianzhong Cao, Xiping Hu, Dapeng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: Discriminative features are critical for machine learning applications. Most existing deep learning approaches, however, rely on convolutional neural networks (CNNs) for learning features, whose discriminant power is not explicitly enforced. In this paper, we propose a novel approach to train deep CNNs by imposing the intra-class compactness and the inter-class separability, so as to enhance the learned features' discriminant power. To this end, we introduce anchors, which are predefined vectors regarded as the centers for each class and fixed during training. Discriminative features are obtained by constraining the deep CNNs to map training samples to the corresponding anchors as close as possible. We propose two principles to select the anchors, and measure the proximity of two points using the Euclidean and cosine distance metric functions, which results in two novel loss functions. These loss functions require no sample pairs or triplets and can be efficiently optimized by batch stochastic gradient descent. We test the proposed method on three benchmark image classification datasets and demonstrate its promising results.



### Matching Fingerphotos to Slap Fingerprint Images
- **Arxiv ID**: http://arxiv.org/abs/1804.08122v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.08122v1)
- **Published**: 2018-04-22 15:21:42+00:00
- **Updated**: 2018-04-22 15:21:42+00:00
- **Authors**: Debayan Deb, Tarang Chugh, Joshua Engelsma, Kai Cao, Neeta Nain, Jake Kendall, Anil K. Jain
- **Comment**: 9 pages, 16 figures, 5 tables, conference
- **Journal**: None
- **Summary**: We address the problem of comparing fingerphotos, fingerprint images from a commodity smartphone camera, with the corresponding legacy slap contact-based fingerprint images. Development of robust versions of these technologies would enable the use of the billions of standard Android phones as biometric readers through a simple software download, dramatically lowering the cost and complexity of deployment relative to using a separate fingerprint reader. Two fingerphoto apps running on Android phones and an optical slap reader were utilized for fingerprint collection of 309 subjects who primarily work as construction workers, farmers, and domestic helpers. Experimental results show that a True Accept Rate (TAR) of 95.79 at a False Accept Rate (FAR) of 0.1% can be achieved in matching fingerphotos to slaps (two thumbs and two index fingers) using a COTS fingerprint matcher. By comparison, a baseline TAR of 98.55% at 0.1% FAR is achieved when matching fingerprint images from two different contact-based optical readers. We also report the usability of the two smartphone apps, in terms of failure to acquire rate and fingerprint acquisition time. Our results show that fingerphotos are promising to authenticate individuals (against a national ID database) for banking, welfare distribution, and healthcare applications in developing countries.



### Micro-Net: A unified model for segmentation of various objects in microscopy images
- **Arxiv ID**: http://arxiv.org/abs/1804.08145v2
- **DOI**: 10.1016/j.media.2018.12.003
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.08145v2)
- **Published**: 2018-04-22 17:40:32+00:00
- **Updated**: 2019-01-22 20:26:05+00:00
- **Authors**: Shan E Ahmed Raza, Linda Cheung, Muhammad Shaban, Simon Graham, David Epstein, Stella Pelengaris, Michael Khan, Nasir M. Rajpoot
- **Comment**: None
- **Journal**: Medical Image Analysis. 52 (2019) 160-173
- **Summary**: Object segmentation and structure localization are important steps in automated image analysis pipelines for microscopy images. We present a convolution neural network (CNN) based deep learning architecture for segmentation of objects in microscopy images. The proposed network can be used to segment cells, nuclei and glands in fluorescence microscopy and histology images after slight tuning of input parameters. The network trains at multiple resolutions of the input image, connects the intermediate layers for better localization and context and generates the output using multi-resolution deconvolution filters. The extra convolutional layers which bypass the max-pooling operation allow the network to train for variable input intensities and object size and make it robust to noisy data. We compare our results on publicly available data sets and show that the proposed network outperforms recent deep learning algorithms.



### I Know How You Feel: Emotion Recognition with Facial Landmarks
- **Arxiv ID**: http://arxiv.org/abs/1805.00326v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1805.00326v2)
- **Published**: 2018-04-22 19:06:50+00:00
- **Updated**: 2018-10-23 18:29:02+00:00
- **Authors**: Ivona Tautkute, Tomasz Trzcinski, Adam Bielski
- **Comment**: CVPRW 2018, The IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR) Workshops 2018
- **Journal**: None
- **Summary**: Classification of human emotions remains an important and challenging task for many computer vision algorithms, especially in the era of humanoid robots which coexist with humans in their everyday life. Currently proposed methods for emotion recognition solve this task using multi-layered convolutional networks that do not explicitly infer any facial features in the classification phase. In this work, we postulate a fundamentally different approach to solve emotion recognition task that relies on incorporating facial landmarks as a part of the classification loss function. To that end, we extend a recently proposed Deep Alignment Network (DAN), that achieves state-of-the-art results in the recent facial landmark recognition challenge, with a term related to facial features. Thanks to this simple modification, our model called EmotionalDAN is able to outperform state-of-the-art emotion classification methods on two challenging benchmark dataset by up to 5%.



### A Deep Convolutional Neural Network for Lung Cancer Diagnostic
- **Arxiv ID**: http://arxiv.org/abs/1804.08170v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.08170v1)
- **Published**: 2018-04-22 21:00:28+00:00
- **Updated**: 2018-04-22 21:00:28+00:00
- **Authors**: Mehdi Fatan Serj, Bahram Lavi, Gabriela Hoff, Domenec Puig Valls
- **Comment**: 10 pages, 5 figures, 2 tables
- **Journal**: None
- **Summary**: In this paper, we examine the strength of deep learning technique for diagnosing lung cancer on medical image analysis problem. Convolutional neural networks (CNNs) models become popular among the pattern recognition and computer vision research area because of their promising outcome on generating high-level image representations. We propose a new deep learning architecture for learning high-level image representation to achieve high classification accuracy with low variance in medical image binary classification tasks. We aim to learn discriminant compact features at beginning of our deep convolutional neural network. We evaluate our model on Kaggle Data Science Bowl 2017 (KDSB17) data set, and compare it with some related works proposed in the Kaggle competition.



### Large Receptive Field Networks for High-Scale Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1804.08181v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.08181v1)
- **Published**: 2018-04-22 21:50:51+00:00
- **Updated**: 2018-04-22 21:50:51+00:00
- **Authors**: George Seif, Dimitrios Androutsos
- **Comment**: Accepted as a conference paper at CVPR2018 in the NTIRE Workshop
  http://www.vision.ee.ethz.ch/en/ntire18/
- **Journal**: None
- **Summary**: Convolutional Neural Networks have been the backbone of recent rapid progress in Single-Image Super-Resolution. However, existing networks are very deep with many network parameters, thus having a large memory footprint and being challenging to train. We propose Large Receptive Field Networks which strive to directly expand the receptive field of Super-Resolution networks without increasing depth or parameter count. In particular, we use two different methods to expand the network receptive field: 1-D separable kernels and atrous convolutions. We conduct considerable experiments to study the performance of various arrangement schemes of the 1-D separable kernels and atrous convolution in terms of accuracy (PSNR / SSIM), parameter count, and speed, while focusing on the more challenging high upscaling factors. Extensive benchmark evaluations demonstrate the effectiveness of our approach.



