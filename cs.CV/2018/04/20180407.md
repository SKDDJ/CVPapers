# Arxiv Papers in cs.CV on 2018-04-07
### MVSNet: Depth Inference for Unstructured Multi-view Stereo
- **Arxiv ID**: http://arxiv.org/abs/1804.02505v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.02505v2)
- **Published**: 2018-04-07 03:57:00+00:00
- **Updated**: 2018-07-17 12:44:13+00:00
- **Authors**: Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, Long Quan
- **Comment**: Accepted to European Conference on Computer Vision (ECCV 2018)
- **Journal**: None
- **Summary**: We present an end-to-end deep learning architecture for depth map inference from multi-view images. In the network, we first extract deep visual image features, and then build the 3D cost volume upon the reference camera frustum via the differentiable homography warping. Next, we apply 3D convolutions to regularize and regress the initial depth map, which is then refined with the reference image to generate the final output. Our framework flexibly adapts arbitrary N-view inputs using a variance-based cost metric that maps multiple features into one cost feature. The proposed MVSNet is demonstrated on the large-scale indoor DTU dataset. With simple post-processing, our method not only significantly outperforms previous state-of-the-arts, but also is several times faster in runtime. We also evaluate MVSNet on the complex outdoor Tanks and Temples dataset, where our method ranks first before April 18, 2018 without any fine-tuning, showing the strong generalization ability of MVSNet.



### Evolution leads to a diversity of motion-detection neuronal circuits
- **Arxiv ID**: http://arxiv.org/abs/1804.02508v2
- **DOI**: None
- **Categories**: **q-bio.PE**, cs.CV, cs.NE, nlin.AO
- **Links**: [PDF](http://arxiv.org/pdf/1804.02508v2)
- **Published**: 2018-04-07 04:26:21+00:00
- **Updated**: 2018-06-05 14:06:08+00:00
- **Authors**: Ali Tehrani-Saleh, Thomas LaBar, Christoph Adami
- **Comment**: 8 pages, 8 figures, Artificial Life Conference (2018), to appear
- **Journal**: None
- **Summary**: A central goal of evolutionary biology is to explain the origins and distribution of diversity across life. Beyond species or genetic diversity, we also observe diversity in the circuits (genetic or otherwise) underlying complex functional traits. However, while the theory behind the origins and maintenance of genetic and species diversity has been studied for decades, theory concerning the origin of diverse functional circuits is still in its infancy. It is not known how many different circuit structures can implement any given function, which evolutionary factors lead to different circuits, and whether the evolution of a particular circuit was due to adaptive or non-adaptive processes. Here, we use digital experimental evolution to study the diversity of neural circuits that encode motion detection in digital (artificial) brains. We find that evolution leads to an enormous diversity of potential neural architectures encoding motion detection circuits, even for circuits encoding the exact same function. Evolved circuits vary in both redundancy and complexity (as previously found in genetic circuits) suggesting that similar evolutionary principles underlie circuit formation using any substrate. We also show that a simple (designed) motion detection circuit that is optimally-adapted gains in complexity when evolved further, and that selection for mutational robustness led this gain in complexity.



### Learning a Text-Video Embedding from Incomplete and Heterogeneous Data
- **Arxiv ID**: http://arxiv.org/abs/1804.02516v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.02516v2)
- **Published**: 2018-04-07 06:59:45+00:00
- **Updated**: 2020-01-16 13:18:58+00:00
- **Authors**: Antoine Miech, Ivan Laptev, Josef Sivic
- **Comment**: The paper had a major update in January 2020 after a bug we found in
  the codebase that affected many results
- **Journal**: None
- **Summary**: Joint understanding of video and language is an active research area with many applications. Prior work in this domain typically relies on learning text-video embeddings. One difficulty with this approach, however, is the lack of large-scale annotated video-caption datasets for training. To address this issue, we aim at learning text-video embeddings from heterogeneous data sources. To this end, we propose a Mixture-of-Embedding-Experts (MEE) model with ability to handle missing input modalities during training. As a result, our framework can learn improved text-video embeddings simultaneously from image and video datasets. We also show the generalization of MEE to other input modalities such as face descriptors. We evaluate our method on the task of video retrieval and report results for the MPII Movie Description and MSR-VTT datasets. The proposed MEE model demonstrates significant improvements and outperforms previously reported methods on both text-to-video and video-to-text retrieval tasks. Code is available at: https://github.com/antoine77340/Mixture-of-Embedding-Experts



### Statistical transformer networks: learning shape and appearance models via self supervision
- **Arxiv ID**: http://arxiv.org/abs/1804.02541v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1804.02541v1)
- **Published**: 2018-04-07 10:18:15+00:00
- **Updated**: 2018-04-07 10:18:15+00:00
- **Authors**: Anil Bas, William A. P. Smith
- **Comment**: None
- **Journal**: None
- **Summary**: We generalise Spatial Transformer Networks (STN) by replacing the parametric transformation of a fixed, regular sampling grid with a deformable, statistical shape model which is itself learnt. We call this a Statistical Transformer Network (StaTN). By training a network containing a StaTN end-to-end for a particular task, the network learns the optimal nonrigid alignment of the input data for the task. Moreover, the statistical shape model is learnt with no direct supervision (such as landmarks) and can be reused for other tasks. Besides training for a specific task, we also show that a StaTN can learn a shape model using generic loss functions. This includes a loss inspired by the minimum description length principle in which an appearance model is also learnt from scratch. In this configuration, our model learns an active appearance model and a means to fit the model from scratch with no supervision at all, even identity labels.



### Not quite unreasonable effectiveness of machine learning algorithms
- **Arxiv ID**: http://arxiv.org/abs/1804.02543v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1804.02543v1)
- **Published**: 2018-04-07 10:24:04+00:00
- **Updated**: 2018-04-07 10:24:04+00:00
- **Authors**: Egor Illarionov, Roman Khudorozhkov
- **Comment**: None
- **Journal**: None
- **Summary**: State-of-the-art machine learning algorithms demonstrate close to absolute performance in selected challenges. We provide arguments that the reason can be in low variability of the samples and high effectiveness in learning typical patterns. Due to this fact, standard performance metrics do not reveal model capacity and new metrics are required for the better understanding of state-of-the-art.



### Efficient No-Reference Quality Assessment and Classification Model for Contrast Distorted Images
- **Arxiv ID**: http://arxiv.org/abs/1804.02554v1
- **DOI**: 10.1109/TBC.2018.2818402
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.02554v1)
- **Published**: 2018-04-07 12:52:33+00:00
- **Updated**: 2018-04-07 12:52:33+00:00
- **Authors**: Hossein Ziaei Nafchi, Mohamed Cheriet
- **Comment**: 6 pages, 4 figures, 4 tables
- **Journal**: None
- **Summary**: In this paper, an efficient Minkowski Distance based Metric (MDM) for no-reference (NR) quality assessment of contrast distorted images is proposed. It is shown that higher orders of Minkowski distance and entropy provide accurate quality prediction for the contrast distorted images. The proposed metric performs predictions by extracting only three features from the distorted images followed by a regression analysis. Furthermore, the proposed features are able to classify type of the contrast distorted images with a high accuracy. Experimental results on four datasets CSIQ, TID2013, CCID2014, and SIQAD show that the proposed metric with a very low complexity provides better quality predictions than the state-of-the-art NR metrics. The MATLAB source code of the proposed metric is available to public at http://www.synchromedia.ca/system/files/MDM.zip.



### Drive Video Analysis for the Detection of Traffic Near-Miss Incidents
- **Arxiv ID**: http://arxiv.org/abs/1804.02555v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1804.02555v1)
- **Published**: 2018-04-07 12:56:40+00:00
- **Updated**: 2018-04-07 12:56:40+00:00
- **Authors**: Hirokatsu Kataoka, Teppei Suzuki, Shoko Oikawa, Yasuhiro Matsui, Yutaka Satoh
- **Comment**: Accepted to ICRA 2018
- **Journal**: None
- **Summary**: Because of their recent introduction, self-driving cars and advanced driver assistance system (ADAS) equipped vehicles have had little opportunity to learn, the dangerous traffic (including near-miss incident) scenarios that provide normal drivers with strong motivation to drive safely. Accordingly, as a means of providing learning depth, this paper presents a novel traffic database that contains information on a large number of traffic near-miss incidents that were obtained by mounting driving recorders in more than 100 taxis over the course of a decade. The study makes the following two main contributions: (i) In order to assist automated systems in detecting near-miss incidents based on database instances, we created a large-scale traffic near-miss incident database (NIDB) that consists of video clip of dangerous events captured by monocular driving recorders. (ii) To illustrate the applicability of NIDB traffic near-miss incidents, we provide two primary database-related improvements: parameter fine-tuning using various near-miss scenes from NIDB, and foreground/background separation into motion representation. Then, using our new database in conjunction with a monocular driving recorder, we developed a near-miss recognition method that provides automated systems with a performance level that is comparable to a human-level understanding of near-miss incidents (64.5% vs. 68.4% at near-miss recognition, 61.3% vs. 78.7% at near-miss detection).



### Application of Superpixels to Segment Several Landmarks in Running Rodents
- **Arxiv ID**: http://arxiv.org/abs/1804.02574v1
- **DOI**: 10.1134/S1054661818030082
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.02574v1)
- **Published**: 2018-04-07 16:46:15+00:00
- **Updated**: 2018-04-07 16:46:15+00:00
- **Authors**: Omid Haji Maghsoudi, Annie Vahedipour, Benjamin Robertson, Andrew Spence
- **Comment**: This paper has been accepted by the Journal of Pattern Recognition
  and Image Analysis (PRIA), being published by July 2018
- **Journal**: None
- **Summary**: Examining locomotion has improved our basic understanding of motor control and aided in treating motor impairment. Mice and rats are the model system of choice for basic neuroscience studies of human disease. High frame rates are needed to quantify the kinematics of running rodents, due to their high stride frequency. Manual tracking, especially for multiple body landmarks, becomes extremely time-consuming. To overcome these limitations, we proposed the use of superpixels based image segmentation as superpixels utilized both spatial and color information for segmentation. We segmented some parts of body and tested the success of segmentation as a function of color space and SLIC segment size. We used a simple merging function to connect the segmented regions considered as neighbor and having the same intensity value range. In addition, 28 features were extracted, and t-SNE was used to demonstrate how much the methods are capable to differentiate the regions. Finally, we compared the segmented regions to a manually outlined region. The results showed for segmentation, using the RGB image was slightly better compared to the hue channel. For merg- ing and classification, however, the hue representation was better as it captures the relevant color information in a single channel.



### POL-LWIR Vehicle Detection: Convolutional Neural Networks Meet Polarised Infrared Sensors
- **Arxiv ID**: http://arxiv.org/abs/1804.02576v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.02576v1)
- **Published**: 2018-04-07 17:09:34+00:00
- **Updated**: 2018-04-07 17:09:34+00:00
- **Authors**: Marcel Sheeny, Andrew Wallace, Mehryar Emambakhsh, Sen Wang, Barry Connor
- **Comment**: Computer Vision and Pattern Recognition Workshop 2018
- **Journal**: None
- **Summary**: For vehicle autonomy, driver assistance and situational awareness, it is necessary to operate at day and night, and in all weather conditions. In particular, long wave infrared (LWIR) sensors that receive predominantly emitted radiation have the capability to operate at night as well as during the day. In this work, we employ a polarised LWIR (POL-LWIR) camera to acquire data from a mobile vehicle, to compare and contrast four different convolutional neural network (CNN) configurations to detect other vehicles in video sequences. We evaluate two distinct and promising approaches, two-stage detection (Faster-RCNN) and one-stage detection (SSD), in four different configurations. We also employ two different image decompositions: the first based on the polarisation ellipse and the second on the Stokes parameters themselves. To evaluate our approach, the experimental trials were quantified by mean average precision (mAP) and processing time, showing a clear trade-off between the two factors. For example, the best mAP result of 80.94% was achieved using Faster-RCNN, but at a frame rate of 6.4 fps. In contrast, MobileNet SSD achieved only 64.51% mAP, but at 53.4 fps.



### Semi-Supervised Multi-Organ Segmentation via Deep Multi-Planar Co-Training
- **Arxiv ID**: http://arxiv.org/abs/1804.02586v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.02586v3)
- **Published**: 2018-04-07 20:09:58+00:00
- **Updated**: 2018-11-19 23:24:36+00:00
- **Authors**: Yuyin Zhou, Yan Wang, Peng Tang, Song Bai, Wei Shen, Elliot K. Fishman, Alan L. Yuille
- **Comment**: accepted by WACV2019
- **Journal**: None
- **Summary**: In multi-organ segmentation of abdominal CT scans, most existing fully supervised deep learning algorithms require lots of voxel-wise annotations, which are usually difficult, expensive, and slow to obtain. In comparison, massive unlabeled 3D CT volumes are usually easily accessible. Current mainstream works to address the semi-supervised biomedical image segmentation problem are mostly graph-based. By contrast, deep network based semi-supervised learning methods have not drawn much attention in this field. In this work, we propose Deep Multi-Planar Co-Training (DMPCT), whose contributions can be divided into two folds: 1) The deep model is learned in a co-training style which can mine consensus information from multiple planes like the sagittal, coronal, and axial planes; 2) Multi-planar fusion is applied to generate more reliable pseudo-labels, which alleviates the errors occurring in the pseudo-labels and thus can help to train better segmentation networks. Experiments are done on our newly collected large dataset with 100 unlabeled cases as well as 210 labeled cases where 16 anatomical structures are manually annotated by four radiologists and confirmed by a senior expert. The results suggest that DMPCT significantly outperforms the fully supervised method by more than 4% especially when only a small set of annotations is used.



### Estimation of Camera Locations in Highly Corrupted Scenarios: All About that Base, No Shape Trouble
- **Arxiv ID**: http://arxiv.org/abs/1804.02591v1
- **DOI**: 10.1109/CVPR.2018.00303
- **Categories**: **cs.CV**, cs.IT, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/1804.02591v1)
- **Published**: 2018-04-07 20:48:58+00:00
- **Updated**: 2018-04-07 20:48:58+00:00
- **Authors**: Yunpeng Shi, Gilad Lerman
- **Comment**: To appear in the CVPR 2018 proceedings
- **Journal**: CVPR, 2018, pp. 2868-2876
- **Summary**: We propose a strategy for improving camera location estimation in structure from motion. Our setting assumes highly corrupted pairwise directions (i.e., normalized relative location vectors), so there is a clear room for improving current state-of-the-art solutions for this problem. Our strategy identifies severely corrupted pairwise directions by using a geometric consistency condition. It then selects a cleaner set of pairwise directions as a preprocessing step for common solvers. We theoretically guarantee the successful performance of a basic version of our strategy under a synthetic corruption model. Numerical results on artificial and real data demonstrate the significant improvement obtained by our strategy.



### Training Multi-organ Segmentation Networks with Sample Selection by Relaxed Upper Confident Bound
- **Arxiv ID**: http://arxiv.org/abs/1804.02595v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.02595v1)
- **Published**: 2018-04-07 21:52:10+00:00
- **Updated**: 2018-04-07 21:52:10+00:00
- **Authors**: Yan Wang, Yuyin Zhou, Peng Tang, Wei Shen, Elliot K. Fishman, Alan L. Yuille
- **Comment**: Submitted to MICCAI 2018
- **Journal**: None
- **Summary**: Deep convolutional neural networks (CNNs), especially fully convolutional networks, have been widely applied to automatic medical image segmentation problems, e.g., multi-organ segmentation. Existing CNN-based segmentation methods mainly focus on looking for increasingly powerful network architectures, but pay less attention to data sampling strategies for training networks more effectively. In this paper, we present a simple but effective sample selection method for training multi-organ segmentation networks. Sample selection exhibits an exploitation-exploration strategy, i.e., exploiting hard samples and exploring less frequently visited samples. Based on the fact that very hard samples might have annotation errors, we propose a new sample selection policy, named Relaxed Upper Confident Bound (RUCB). Compared with other sample selection policies, e.g., Upper Confident Bound (UCB), it exploits a range of hard samples rather than being stuck with a small set of very hard ones, which mitigates the influence of annotation errors during training. We apply this new sample selection policy to training a multi-organ segmentation network on a dataset containing 120 abdominal CT scans and show that it boosts segmentation performance significantly.



### Real-world Noisy Image Denoising: A New Benchmark
- **Arxiv ID**: http://arxiv.org/abs/1804.02603v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.02603v1)
- **Published**: 2018-04-07 23:54:21+00:00
- **Updated**: 2018-04-07 23:54:21+00:00
- **Authors**: Jun Xu, Hui Li, Zhetong Liang, David Zhang, Lei Zhang
- **Comment**: 13 pages, 8 figures, 8 tables. arXiv admin note: text overlap with
  arXiv:1707.01313 by other authors
- **Journal**: None
- **Summary**: Most of previous image denoising methods focus on additive white Gaussian noise (AWGN). However,the real-world noisy image denoising problem with the advancing of the computer vision techiniques. In order to promote the study on this problem while implementing the concurrent real-world image denoising datasets, we construct a new benchmark dataset which contains comprehensive real-world noisy images of different natural scenes. These images are captured by different cameras under different camera settings. We evaluate the different denoising methods on our new dataset as well as previous datasets. Extensive experimental results demonstrate that the recently proposed methods designed specifically for realistic noise removal based on sparse or low rank theories achieve better denoising performance and are more robust than other competing methods, and the newly proposed dataset is more challenging. The constructed dataset of real photographs is publicly available at \url{https://github.com/csjunxu/PolyUDataset} for researchers to investigate new real-world image denoising methods. We will add more analysis on the noise statistics in the real photographs of our new dataset in the next version of this article.



