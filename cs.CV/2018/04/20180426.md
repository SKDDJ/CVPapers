# Arxiv Papers in cs.CV on 2018-04-26
### Generative Model for Heterogeneous Inference
- **Arxiv ID**: http://arxiv.org/abs/1804.09858v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1804.09858v1)
- **Published**: 2018-04-26 02:28:34+00:00
- **Updated**: 2018-04-26 02:28:34+00:00
- **Authors**: Honggang Zhou, Yunchun Li, Hailong Yang, Wei Li, Jie Jia
- **Comment**: None
- **Journal**: None
- **Summary**: Generative models (GMs) such as Generative Adversary Network (GAN) and Variational Auto-Encoder (VAE) have thrived these years and achieved high quality results in generating new samples. Especially in Computer Vision, GMs have been used in image inpainting, denoising and completion, which can be treated as the inference from observed pixels to corrupted pixels. However, images are hierarchically structured which are quite different from many real-world inference scenarios with non-hierarchical features. These inference scenarios contain heterogeneous stochastic variables and irregular mutual dependences. Traditionally they are modeled by Bayesian Network (BN). However, the learning and inference of BN model are NP-hard thus the number of stochastic variables in BN is highly constrained. In this paper, we adapt typical GMs to enable heterogeneous learning and inference in polynomial time.We also propose an extended autoregressive (EAR) model and an EAR with adversary loss (EARA) model and give theoretical results on their effectiveness. Experiments on several BN datasets show that our proposed EAR model achieves the best performance in most cases compared to other GMs. Except for black box analysis, we've also done a serial of experiments on Markov border inference of GMs for white box analysis and give theoretical results.



### Competitive Learning Enriches Learning Representation and Accelerates the Fine-tuning of CNNs
- **Arxiv ID**: http://arxiv.org/abs/1804.09859v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1804.09859v1)
- **Published**: 2018-04-26 02:28:48+00:00
- **Updated**: 2018-04-26 02:28:48+00:00
- **Authors**: Takashi Shinozaki
- **Comment**: Appeared at NIPS 2017 Workshop: Deep Learning: Bridging Theory and
  Practice
- **Journal**: None
- **Summary**: In this study, we propose the integration of competitive learning into convolutional neural networks (CNNs) to improve the representation learning and efficiency of fine-tuning. Conventional CNNs use back propagation learning, and it enables powerful representation learning by a discrimination task. However, it requires huge amount of labeled data, and acquisition of labeled data is much harder than that of unlabeled data. Thus, efficient use of unlabeled data is getting crucial for DNNs. To address the problem, we introduce unsupervised competitive learning into the convolutional layer, and utilize unlabeled data for effective representation learning. The results of validation experiments using a toy model demonstrated that strong representation learning effectively extracted bases of images into convolutional filters using unlabeled data, and accelerated the speed of the fine-tuning of subsequent supervised back propagation learning. The leverage was more apparent when the number of filters was sufficiently large, and, in such a case, the error rate steeply decreased in the initial phase of fine-tuning. Thus, the proposed method enlarged the number of filters in CNNs, and enabled a more detailed and generalized representation. It could provide a possibility of not only deep but broad neural networks.



### Accelerator-Aware Pruning for Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1804.09862v3
- **DOI**: 10.1109/TCSVT.2019.2911674
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1804.09862v3)
- **Published**: 2018-04-26 02:35:04+00:00
- **Updated**: 2020-09-05 07:22:14+00:00
- **Authors**: Hyeong-Ju Kang
- **Comment**: 11 pages, 9 figures
- **Journal**: IEEE Transactions on Circuits and Systems for Video Technology,
  vol. 30, no. 7, pp. 2093-2103, Jul. 2020
- **Summary**: Convolutional neural networks have shown tremendous performance capabilities in computer vision tasks, but their excessive amounts of weight storage and arithmetic operations prevent them from being adopted in embedded environments. One of the solutions involves pruning, where certain unimportant weights are forced to have a value of zero. Many pruning schemes have been proposed, but these have mainly focused on the number of pruned weights. Previous pruning schemes scarcely considered ASIC or FPGA accelerator architectures. When these pruned networks are run on accelerators, the lack of consideration of the architecture causes some inefficiency problems, including internal buffer misalignments and load imbalances. This paper proposes a new pruning scheme that reflects accelerator architectures. In the proposed scheme, pruning is performed so that the same number of weights remain for each weight group corresponding to activations fetched simultaneously. In this way, the pruning scheme resolves the inefficiency problems, doubling the accelerator performance. Even with this constraint, the proposed pruning scheme reached a pruning ratio similar to that of previous unconstrained pruning schemes, not only on AlexNet and VGG16 but also on state-of-the-art very deep networks such as ResNet. Furthermore, the proposed scheme demonstrated a comparable pruning ratio on compact networks such as MobileNet and on slimmed networks that were already pruned in a channel-wise manner. In addition to improving the efficiency of previous sparse accelerators, it will be also shown that the proposed pruning scheme can be used to reduce the logic complexity of sparse accelerators.The pruned models are publicly available at https://github.com/HyeongjuKang/accelerator-aware-pruning.



### Prospects for Theranostics in Neurosurgical Imaging: Empowering Confocal Laser Endomicroscopy Diagnostics via Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1804.09873v2
- **DOI**: 10.3389/fonc.2018.00240
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.09873v2)
- **Published**: 2018-04-26 03:33:37+00:00
- **Updated**: 2018-08-18 06:42:49+00:00
- **Authors**: Mohammadhassan Izadyyazdanabadi, Evgenii Belykh, Michael Mooney, Jennifer Eschbacher, Peter Nakaji, Yezhou Yang, Mark C. Preul
- **Comment**: See the final version published in Frontiers in Oncology here:
  https://www.frontiersin.org/articles/10.3389/fonc.2018.00240/full
- **Journal**: None
- **Summary**: Confocal laser endomicroscopy (CLE) is an advanced optical fluorescence imaging technology that has the potential to increase intraoperative precision, extend resection, and tailor surgery for malignant invasive brain tumors because of its subcellular dimension resolution. Despite its promising diagnostic potential, interpreting the gray tone fluorescence images can be difficult for untrained users. In this review, we provide a detailed description of bioinformatical analysis methodology of CLE images that begins to assist the neurosurgeon and pathologist to rapidly connect on-the-fly intraoperative imaging, pathology, and surgical observation into a conclusionary system within the concept of theranostics. We present an overview and discuss deep learning models for automatic detection of the diagnostic CLE images and discuss various training regimes and ensemble modeling effect on the power of deep learning predictive models. Two major approaches reviewed in this paper include the models that can automatically classify CLE images into diagnostic/nondiagnostic, glioma/nonglioma, tumor/injury/normal categories and models that can localize histological features on the CLE images using weakly supervised methods. We also briefly review advances in the deep learning approaches used for CLE image analysis in other organs. Significant advances in speed and precision of automated diagnostic frame selection would augment the diagnostic potential of CLE, improve operative workflow and integration into brain tumor surgery. Such technology and bioinformatics analytics lend themselves to improved precision, personalization, and theranostics in brain tumor treatment.



### Boosting LiDAR-based Semantic Labeling by Cross-Modal Training Data Generation
- **Arxiv ID**: http://arxiv.org/abs/1804.09915v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.09915v1)
- **Published**: 2018-04-26 07:18:30+00:00
- **Updated**: 2018-04-26 07:18:30+00:00
- **Authors**: Florian Piewak, Peter Pinggera, Manuel Schäfer, David Peter, Beate Schwarz, Nick Schneider, David Pfeiffer, Markus Enzweiler, Marius Zöllner
- **Comment**: None
- **Journal**: None
- **Summary**: Mobile robots and autonomous vehicles rely on multi-modal sensor setups to perceive and understand their surroundings. Aside from cameras, LiDAR sensors represent a central component of state-of-the-art perception systems. In addition to accurate spatial perception, a comprehensive semantic understanding of the environment is essential for efficient and safe operation. In this paper we present a novel deep neural network architecture called LiLaNet for point-wise, multi-class semantic labeling of semi-dense LiDAR data. The network utilizes virtual image projections of the 3D point clouds for efficient inference. Further, we propose an automated process for large-scale cross-modal training data generation called Autolabeling, in order to boost semantic labeling performance while keeping the manual annotation effort low. The effectiveness of the proposed network architecture as well as the automated data generation process is demonstrated on a manually annotated ground truth dataset. LiLaNet is shown to significantly outperform current state-of-the-art CNN architectures for LiDAR data. Applying our automatically generated large-scale training data yields a boost of up to 14 percentage points compared to networks trained on manually annotated data only.



### Pay Attention to Virality: understanding popularity of social media videos with the attention mechanism
- **Arxiv ID**: http://arxiv.org/abs/1804.09949v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1804.09949v1)
- **Published**: 2018-04-26 09:06:06+00:00
- **Updated**: 2018-04-26 09:06:06+00:00
- **Authors**: Adam Bielski, Tomasz Trzcinski
- **Comment**: Conference on Computer Vision and Pattern Recognition Workshops
  (CVPRW), CVPR 2018 workshop on Visual Understanding of Subjective Attributes
  of Data (V-USAD)
- **Journal**: None
- **Summary**: Predicting popularity of social media videos before they are published is a challenging task, mainly due to the complexity of content distribution network as well as the number of factors that play part in this process. As solving this task provides tremendous help for media content creators, many successful methods were proposed to solve this problem with machine learning. In this work, we change the viewpoint and postulate that it is not only the predicted popularity that matters, but also, maybe even more importantly, understanding of how individual parts influence the final popularity score. To that end, we propose to combine the Grad-CAM visualization method with a soft attention mechanism. Our preliminary results show that this approach allows for more intuitive interpretation of the content impact on video popularity, while achieving competitive results in terms of prediction accuracy.



### Near-Lossless Deep Feature Compression for Collaborative Intelligence
- **Arxiv ID**: http://arxiv.org/abs/1804.09963v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1804.09963v2)
- **Published**: 2018-04-26 09:52:42+00:00
- **Updated**: 2018-06-15 21:42:37+00:00
- **Authors**: Hyomin Choi, Ivan V. Bajic
- **Comment**: None
- **Journal**: None
- **Summary**: Collaborative intelligence is a new paradigm for efficient deployment of deep neural networks across the mobile-cloud infrastructure. By dividing the network between the mobile and the cloud, it is possible to distribute the computational workload such that the overall energy and/or latency of the system is minimized. However, this necessitates sending deep feature data from the mobile to the cloud in order to perform inference. In this work, we examine the differences between the deep feature data and natural image data, and propose a simple and effective near-lossless deep feature compressor. The proposed method achieves up to 5% bit rate reduction compared to HEVC-Intra and even more against other popular image codecs. Finally, we suggest an approach for reconstructing the input image from compressed deep features in the cloud, that could serve to supplement the inference performed by the deep model.



### Recommending Outfits from Personal Closet
- **Arxiv ID**: http://arxiv.org/abs/1804.09979v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.09979v1)
- **Published**: 2018-04-26 10:30:06+00:00
- **Updated**: 2018-04-26 10:30:06+00:00
- **Authors**: Pongsate Tangseng, Kota Yamaguchi, Takayuki Okatani
- **Comment**: None
- **Journal**: None
- **Summary**: We consider grading a fashion outfit for recommendation, where we assume that users have a closet of items and we aim at producing a score for an arbitrary combination of items in the closet. The challenge in outfit grading is that the input to the system is a bag of item pictures that are unordered and vary in size. We build a deep neural network-based system that can take variable-length items and predict a score. We collect a large number of outfits from a popular fashion sharing website, Polyvore, and evaluate the performance of our grading system. We compare our model with a random-choice baseline, both on the traditional classification evaluation and on people's judgment using a crowdsourcing platform. With over 84% in classification accuracy and 91% matching ratio to human annotators, our model can reliably grade the quality of an outfit. We also build an outfit recommender on top of our grader to demonstrate the practical application of our model for a personal closet assistant.



### Link and code: Fast indexing with graphs and compact regression codes
- **Arxiv ID**: http://arxiv.org/abs/1804.09996v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.DB, cs.DS, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1804.09996v2)
- **Published**: 2018-04-26 11:24:42+00:00
- **Updated**: 2018-04-27 10:01:51+00:00
- **Authors**: Matthijs Douze, Alexandre Sablayrolles, Hervé Jégou
- **Comment**: None
- **Journal**: None
- **Summary**: Similarity search approaches based on graph walks have recently attained outstanding speed-accuracy trade-offs, taking aside the memory requirements. In this paper, we revisit these approaches by considering, additionally, the memory constraint required to index billions of images on a single server. This leads us to propose a method based both on graph traversal and compact representations. We encode the indexed vectors using quantization and exploit the graph structure to refine the similarity estimation.   In essence, our method takes the best of these two worlds: the search strategy is based on nested graphs, thereby providing high precision with a relatively small set of comparisons. At the same time it offers a significant memory compression. As a result, our approach outperforms the state of the art on operating points considering 64-128 bytes per vector, as demonstrated by our results on two billion-scale public benchmarks.



### Force Estimation from OCT Volumes using 3D CNNs
- **Arxiv ID**: http://arxiv.org/abs/1804.10002v1
- **DOI**: 10.1007/s11548-018-1777-8
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.10002v1)
- **Published**: 2018-04-26 11:46:39+00:00
- **Updated**: 2018-04-26 11:46:39+00:00
- **Authors**: Nils Gessert, Jens Beringhoff, Christoph Otte, Alexander Schlaefer
- **Comment**: Published in the International Journal of Computer Assisted Radiology
  and Surgery
- **Journal**: None
- **Summary**: \textit{Purpose} Estimating the interaction forces of instruments and tissue is of interest, particularly to provide haptic feedback during robot assisted minimally invasive interventions. Different approaches based on external and integrated force sensors have been proposed. These are hampered by friction, sensor size, and sterilizability. We investigate a novel approach to estimate the force vector directly from optical coherence tomography image volumes.   \textit{Methods} We introduce a novel Siamese 3D CNN architecture. The network takes an undeformed reference volume and a deformed sample volume as an input and outputs the three components of the force vector. We employ a deep residual architecture with bottlenecks for increased efficiency. We compare the Siamese approach to methods using difference volumes and two-dimensional projections. Data was generated using a robotic setup to obtain ground truth force vectors for silicon tissue phantoms as well as porcine tissue.   \textit{Results} Our method achieves a mean average error of 7.7 +- 4.3 mN when estimating the force vector. Our novel Siamese 3D CNN architecture outperforms single-path methods that achieve a mean average error of 11.59 +- 6.7 mN. Moreover, the use of volume data leads to significantly higher performance compared to processing only surface information which achieves a mean average error of 24.38 +- 22.0 mN. Based on the tissue dataset, our methods shows good generalization in between different subjects.   \textit{Conclusions} We propose a novel image-based force estimation method using optical coherence tomography. We illustrate that capturing the deformation of subsurface structures substantially improves force estimation. Our approach can provide accurate force estimates in surgical setups when using intraoperative optical coherence tomography.



### Joint Deformable Registration of Large EM Image Volumes: A Matrix Solver Approach
- **Arxiv ID**: http://arxiv.org/abs/1804.10019v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.10019v1)
- **Published**: 2018-04-26 12:35:58+00:00
- **Updated**: 2018-04-26 12:35:58+00:00
- **Authors**: Khaled Khairy, Gennady Denisov, Stephan Saalfeld
- **Comment**: None
- **Journal**: None
- **Summary**: Large electron microscopy image datasets for connectomics are typically composed of thousands to millions of partially overlapping two-dimensional images (tiles), which must be registered into a coherent volume prior to further analysis. A common registration strategy is to find matching features between neighboring and overlapping image pairs, followed by a numerical estimation of optimal image deformation using a so-called solver program.   Existing solvers are inadequate for large data volumes, and inefficient for small-scale image registration.   In this work, an efficient and accurate matrix-based solver method is presented. A linear system is constructed that combines minimization of feature-pair square distances with explicit constraints in a regularization term. In absence of reliable priors for regularization, we show how to construct a rigid-model approximation to use as prior. The linear system is solved using available computer programs, whose performance on typical registration tasks we briefly compare, and to which future scale-up is delegated. Our method is applied to the joint alignment of 2.67 million images, with more than 200 million point-pairs and has been used for successfully aligning the first full adult fruit fly brain.



### Deep Keyframe Detection in Human Action Videos
- **Arxiv ID**: http://arxiv.org/abs/1804.10021v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.10021v1)
- **Published**: 2018-04-26 12:41:05+00:00
- **Updated**: 2018-04-26 12:41:05+00:00
- **Authors**: Xiang Yan, Syed Zulqarnain Gilani, Hanlin Qin, Mingtao Feng, Liang Zhang, Ajmal Mian
- **Comment**: None
- **Journal**: None
- **Summary**: Detecting representative frames in videos based on human actions is quite challenging because of the combined factors of human pose in action and the background. This paper addresses this problem and formulates the key frame detection as one of finding the video frames that optimally maximally contribute to differentiating the underlying action category from all other categories. To this end, we introduce a deep two-stream ConvNet for key frame detection in videos that learns to directly predict the location of key frames. Our key idea is to automatically generate labeled data for the CNN learning using a supervised linear discriminant method. While the training data is generated taking many different human action videos into account, the trained CNN can predict the importance of frames from a single video. We specify a new ConvNet framework, consisting of a summarizer and discriminator. The summarizer is a two-stream ConvNet aimed at, first, capturing the appearance and motion features of video frames, and then encoding the obtained appearance and motion features for video representation. The discriminator is a fitting function aimed at distinguishing between the key frames and others in the video. We conduct experiments on a challenging human action dataset UCF101 and show that our method can detect key frames with high accuracy.



### Better and Faster: Knowledge Transfer from Multiple Self-supervised Learning Tasks via Graph Distillation for Video Classification
- **Arxiv ID**: http://arxiv.org/abs/1804.10069v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.10069v1)
- **Published**: 2018-04-26 13:57:17+00:00
- **Updated**: 2018-04-26 13:57:17+00:00
- **Authors**: Chenrui Zhang, Yuxin Peng
- **Comment**: 7 pages, accepted by International Joint Conference on Artificial
  Intelligence (IJCAI) 2018
- **Journal**: None
- **Summary**: Video representation learning is a vital problem for classification task. Recently, a promising unsupervised paradigm termed self-supervised learning has emerged, which explores inherent supervisory signals implied in massive data for feature learning via solving auxiliary tasks. However, existing methods in this regard suffer from two limitations when extended to video classification. First, they focus only on a single task, whereas ignoring complementarity among different task-specific features and thus resulting in suboptimal video representation. Second, high computational and memory cost hinders their application in real-world scenarios. In this paper, we propose a graph-based distillation framework to address these problems: (1) We propose logits graph and representation graph to transfer knowledge from multiple self-supervised tasks, where the former distills classifier-level knowledge by solving a multi-distribution joint matching problem, and the latter distills internal feature knowledge from pairwise ensembled representations with tackling the challenge of heterogeneity among different features; (2) The proposal that adopts a teacher-student framework can reduce the redundancy of knowledge learnt from teachers dramatically, leading to a lighter student model that solves classification task more efficiently. Experimental results on 3 video datasets validate that our proposal not only helps learn better video representation but also compress model for faster inference.



### Visual Data Synthesis via GAN for Zero-Shot Video Classification
- **Arxiv ID**: http://arxiv.org/abs/1804.10073v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.10073v1)
- **Published**: 2018-04-26 14:10:41+00:00
- **Updated**: 2018-04-26 14:10:41+00:00
- **Authors**: Chenrui Zhang, Yuxin Peng
- **Comment**: 7 pages, accepted by International Joint Conference on Artificial
  Intelligence (IJCAI) 2018
- **Journal**: None
- **Summary**: Zero-Shot Learning (ZSL) in video classification is a promising research direction, which aims to tackle the challenge from explosive growth of video categories. Most existing methods exploit seen-to-unseen correlation via learning a projection between visual and semantic spaces. However, such projection-based paradigms cannot fully utilize the discriminative information implied in data distribution, and commonly suffer from the information degradation issue caused by "heterogeneity gap". In this paper, we propose a visual data synthesis framework via GAN to address these problems. Specifically, both semantic knowledge and visual distribution are leveraged to synthesize video feature of unseen categories, and ZSL can be turned into typical supervised problem with the synthetic features. First, we propose multi-level semantic inference to boost video feature synthesis, which captures the discriminative information implied in joint visual-semantic distribution via feature-level and label-level semantic inference. Second, we propose Matching-aware Mutual Information Correlation to overcome information degradation issue, which captures seen-to-unseen correlation in matched and mismatched visual-semantic pairs by mutual information, providing the zero-shot synthesis procedure with robust guidance signals. Experimental results on four video datasets demonstrate that our approach can improve the zero-shot video classification performance significantly.



### Domain Adaptation through Synthesis for Unsupervised Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1804.10094v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.10094v1)
- **Published**: 2018-04-26 14:45:24+00:00
- **Updated**: 2018-04-26 14:45:24+00:00
- **Authors**: Slawomir Bak, Peter Carr, Jean-Francois Lalonde
- **Comment**: None
- **Journal**: None
- **Summary**: Drastic variations in illumination across surveillance cameras make the person re-identification problem extremely challenging. Current large scale re-identification datasets have a significant number of training subjects, but lack diversity in lighting conditions. As a result, a trained model requires fine-tuning to become effective under an unseen illumination condition. To alleviate this problem, we introduce a new synthetic dataset that contains hundreds of illumination conditions. Specifically, we use 100 virtual humans illuminated with multiple HDR environment maps which accurately model realistic indoor and outdoor lighting. To achieve better accuracy in unseen illumination conditions we propose a novel domain adaptation technique that takes advantage of our synthetic data and performs fine-tuning in a completely unsupervised way. Our approach yields significantly higher accuracy than semi-supervised and unsupervised state-of-the-art methods, and is very competitive with supervised techniques.



### Visual Estimation of Building Condition with Patch-level ConvNets
- **Arxiv ID**: http://arxiv.org/abs/1804.10113v1
- **DOI**: 10.1145/3210499.3210526
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.10113v1)
- **Published**: 2018-04-26 15:33:57+00:00
- **Updated**: 2018-04-26 15:33:57+00:00
- **Authors**: David Koch, Miroslav Despotovic, Muntaha Sakeena, Mario Döller, Matthias Zeppelzauer
- **Comment**: To appear in: Workshop on Multimedia for Real Estate Tech, ICMR 2018,
  Yokohama, Japan
- **Journal**: None
- **Summary**: The condition of a building is an important factor for real estate valuation. Currently, the estimation of condition is determined by real estate appraisers which makes it subjective to a certain degree. We propose a novel vision-based approach for the assessment of the building condition from exterior views of the building. To this end, we develop a multi-scale patch-based pattern extraction approach and combine it with convolutional neural networks to estimate building condition from visual clues. Our evaluation shows that visually estimated building condition can serve as a proxy for condition estimates by appraisers.



### IamNN: Iterative and Adaptive Mobile Neural Network for Efficient Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1804.10123v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1804.10123v1)
- **Published**: 2018-04-26 15:57:00+00:00
- **Updated**: 2018-04-26 15:57:00+00:00
- **Authors**: Sam Leroux, Pavlo Molchanov, Pieter Simoens, Bart Dhoedt, Thomas Breuel, Jan Kautz
- **Comment**: ICLR 2018 Workshop track
- **Journal**: None
- **Summary**: Deep residual networks (ResNets) made a recent breakthrough in deep learning. The core idea of ResNets is to have shortcut connections between layers that allow the network to be much deeper while still being easy to optimize avoiding vanishing gradients. These shortcut connections have interesting side-effects that make ResNets behave differently from other typical network architectures. In this work we use these properties to design a network based on a ResNet but with parameter sharing and with adaptive computation time. The resulting network is much smaller than the original network and can adapt the computational cost to the complexity of the input image.



### Detection-Tracking for Efficient Person Analysis: The DetTA Pipeline
- **Arxiv ID**: http://arxiv.org/abs/1804.10134v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1804.10134v2)
- **Published**: 2018-04-26 16:04:30+00:00
- **Updated**: 2018-07-28 10:33:47+00:00
- **Authors**: Stefan Breuers, Lucas Beyer, Umer Rafi, Bastian Leibe
- **Comment**: Code available at: https://github.com/sbreuers/detta
- **Journal**: None
- **Summary**: In the past decade many robots were deployed in the wild, and people detection and tracking is an important component of such deployments. On top of that, one often needs to run modules which analyze persons and extract higher level attributes such as age and gender, or dynamic information like gaze and pose. The latter ones are especially necessary for building a reactive, social robot-person interaction.   In this paper, we combine those components in a fully modular detection-tracking-analysis pipeline, called DetTA. We investigate the benefits of such an integration on the example of head and skeleton pose, by using the consistent track ID for a temporal filtering of the analysis modules' observations, showing a slight improvement in a challenging real-world scenario. We also study the potential of a so-called "free-flight" mode, where the analysis of a person attribute only relies on the filter's predictions for certain frames. Here, our study shows that this boosts the runtime dramatically, while the prediction quality remains stable. This insight is especially important for reducing power consumption and sharing precious (GPU-)memory when running many analysis components on a mobile platform, especially so in the era of expensive deep learning methods.



### Two-Stream Binocular Network: Accurate Near Field Finger Detection Based On Binocular Images
- **Arxiv ID**: http://arxiv.org/abs/1804.10160v1
- **DOI**: 10.1109/VCIP.2017.8305146
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.10160v1)
- **Published**: 2018-04-26 16:36:36+00:00
- **Updated**: 2018-04-26 16:36:36+00:00
- **Authors**: Yi Wei, Guijin Wang, Cairong Zhang, Hengkai Guo, Xinghao Chen, Huazhong Yang
- **Comment**: Published in: Visual Communications and Image Processing (VCIP), 2017
  IEEE. Original IEEE publication available on
  https://ieeexplore.ieee.org/abstract/document/8305146/. Dataset available on
  https://sites.google.com/view/thuhand17
- **Journal**: Visual Communications and Image Processing (VCIP), 2017 IEEE
  (2017) 1-4
- **Summary**: Fingertip detection plays an important role in human computer interaction. Previous works transform binocular images into depth images. Then depth-based hand pose estimation methods are used to predict 3D positions of fingertips. Different from previous works, we propose a new framework, named Two-Stream Binocular Network (TSBnet) to detect fingertips from binocular images directly. TSBnet first shares convolutional layers for low level features of right and left images. Then it extracts high level features in two-stream convolutional networks separately. Further, we add a new layer: binocular distance measurement layer to improve performance of our model. To verify our scheme, we build a binocular hand image dataset, containing about 117k pairs of images in training set and 10k pairs of images in test set. Our methods achieve an average error of 10.9mm on our test set, outperforming previous work by 5.9mm (relatively 35.1%).



### Machine Learning pipeline for discovering neuroimaging-based biomarkers in neurology and psychiatry
- **Arxiv ID**: http://arxiv.org/abs/1804.10163v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/1804.10163v1)
- **Published**: 2018-04-26 16:42:38+00:00
- **Updated**: 2018-04-26 16:42:38+00:00
- **Authors**: Alexander Bernstein, Evgeny Burnaev, Ekaterina Kondratyeva, Svetlana Sushchinskaya, Maxim Sharaev, Alexander Andreev, Alexey Artemov, Renat Akzhigitov
- **Comment**: 20 pages, 2 figures
- **Journal**: None
- **Summary**: We consider a problem of diagnostic pattern recognition/classification from neuroimaging data. We propose a common data analysis pipeline for neuroimaging-based diagnostic classification problems using various ML algorithms and processing toolboxes for brain imaging. We illustrate the pipeline application by discovering new biomarkers for diagnostics of epilepsy and depression based on clinical and MRI/fMRI data for patients and healthy volunteers.



### fMRI: preprocessing, classification and pattern recognition
- **Arxiv ID**: http://arxiv.org/abs/1804.10167v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/1804.10167v1)
- **Published**: 2018-04-26 16:48:52+00:00
- **Updated**: 2018-04-26 16:48:52+00:00
- **Authors**: Maxim Sharaev, Alexander Andreev, Alexey Artemov, Alexander Bernstein, Evgeny Burnaev, Ekaterina Kondratyeva, Svetlana Sushchinskaya, Renat Akzhigitov
- **Comment**: 20 pages, 1 figure
- **Journal**: None
- **Summary**: As machine learning continues to gain momentum in the neuroscience community, we witness the emergence of novel applications such as diagnostics, characterization, and treatment outcome prediction for psychiatric and neurological disorders, for instance, epilepsy and depression. Systematic research into these mental disorders increasingly involves drawing clinical conclusions on the basis of data-driven approaches; to this end, structural and functional neuroimaging serve as key source modalities. Identification of informative neuroimaging markers requires establishing a comprehensive preparation pipeline for data which may be severely corrupted by artifactual signal fluctuations. In this work, we review a large body of literature to provide ample evidence for the advantages of pattern recognition approaches in clinical applications, overview advanced graph-based pattern recognition approaches, and propose a noise-aware neuroimaging data processing pipeline. To demonstrate the effectiveness of our approach, we provide results from a pilot study, which show a significant improvement in classification accuracy, indicating a promising research direction.



### Capsule networks for low-data transfer learning
- **Arxiv ID**: http://arxiv.org/abs/1804.10172v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.10172v1)
- **Published**: 2018-04-26 17:01:12+00:00
- **Updated**: 2018-04-26 17:01:12+00:00
- **Authors**: Andrew Gritsevskiy, Maksym Korablyov
- **Comment**: 11 pages, 10 figures
- **Journal**: None
- **Summary**: We propose a capsule network-based architecture for generalizing learning to new data with few examples. Using both generative and non-generative capsule networks with intermediate routing, we are able to generalize to new information over 25 times faster than a similar convolutional neural network. We train the networks on the multiMNIST dataset lacking one digit. After the networks reach their maximum accuracy, we inject 1-100 examples of the missing digit into the training set, and measure the number of batches needed to return to a comparable level of accuracy. We then discuss the improvement in low-data transfer learning that capsule networks bring, and propose future directions for capsule research.



### Network Transplanting
- **Arxiv ID**: http://arxiv.org/abs/1804.10272v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1804.10272v2)
- **Published**: 2018-04-26 20:25:36+00:00
- **Updated**: 2018-12-18 05:18:29+00:00
- **Authors**: Quanshi Zhang, Yu Yang, Qian Yu, Ying Nian Wu
- **Comment**: None
- **Journal**: None
- **Summary**: This paper focuses on a new task, i.e., transplanting a category-and-task-specific neural network to a generic, modular network without strong supervision. We design an functionally interpretable structure for the generic network. Like building LEGO blocks, we teach the generic network a new category by directly transplanting the module corresponding to the category from a pre-trained network with a few or even without sample annotations. Our method incrementally adds new categories to the generic network but does not affect representations of existing categories. In this way, our method breaks the typical bottleneck of learning a net for massive tasks and categories, i.e. the requirement of collecting samples for all tasks and categories at the same time before the learning begins. Thus, we use a new distillation algorithm, namely back-distillation, to overcome specific challenges of network transplanting. Our method without training samples even outperformed the baseline with 100 training samples.



### Pushing the Limits of Unconstrained Face Detection: a Challenge Dataset and Baseline Results
- **Arxiv ID**: http://arxiv.org/abs/1804.10275v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.10275v3)
- **Published**: 2018-04-26 20:44:06+00:00
- **Updated**: 2018-08-08 15:13:27+00:00
- **Authors**: Hajime Nada, Vishwanath A. Sindagi, He Zhang, Vishal M. Patel
- **Comment**: Accepted in BTAS'2018
- **Journal**: None
- **Summary**: Face detection has witnessed immense progress in the last few years, with new milestones being surpassed every year. While many challenges such as large variations in scale, pose, appearance are successfully addressed, there still exist several issues which are not specifically captured by existing methods or datasets. In this work, we identify the next set of challenges that requires attention from the research community and collect a new dataset of face images that involve these issues such as weather-based degradations, motion blur, focus blur and several others. We demonstrate that there is a considerable gap in the performance of state-of-the-art detectors and real-world requirements. Hence, in an attempt to fuel further research in unconstrained face detection, we present a new annotated Unconstrained Face Detection Dataset (UFDD) with several challenges and benchmark recent methods. Additionally, we provide an in-depth analysis of the results and failure cases of these methods. The dataset as well as baseline results will be made publicly available in due time. The UFDD dataset as well as baseline results are available at: www.ufdd.info/



