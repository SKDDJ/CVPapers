# Arxiv Papers in cs.CV on 2020-03-27
### Local Facial Makeup Transfer via Disentangled Representation
- **Arxiv ID**: http://arxiv.org/abs/2003.12065v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.12065v2)
- **Published**: 2020-03-27 00:25:13+00:00
- **Updated**: 2020-06-21 01:22:02+00:00
- **Authors**: Zhaoyang Sun, Wenxuan Liu, Feng Liu, Ryan Wen Liu, Shengwu Xiong
- **Comment**: There's something wrong with the experiment. It's not complete
- **Journal**: None
- **Summary**: Facial makeup transfer aims to render a non-makeup face image in an arbitrary given makeup one while preserving face identity. The most advanced method separates makeup style information from face images to realize makeup transfer. However, makeup style includes several semantic clear local styles which are still entangled together. In this paper, we propose a novel unified adversarial disentangling network to further decompose face images into four independent components, i.e., personal identity, lips makeup style, eyes makeup style and face makeup style. Owing to the further disentangling of makeup style, our method can not only control the degree of global makeup style, but also flexibly regulate the degree of local makeup styles which any other approaches can't do. For makeup removal, different from other methods which regard makeup removal as the reverse process of makeup, we integrate the makeup transfer with the makeup removal into one uniform framework and obtain multiple makeup removal results. Extensive experiments have demonstrated that our approach can produce more realistic and accurate makeup transfer results compared to the state-of-the-art methods.



### HERS: Homomorphically Encrypted Representation Search
- **Arxiv ID**: http://arxiv.org/abs/2003.12197v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.12197v3)
- **Published**: 2020-03-27 01:10:54+00:00
- **Updated**: 2022-06-18 19:26:45+00:00
- **Authors**: Joshua J. Engelsma, Anil K. Jain, Vishnu Naresh Boddeti
- **Comment**: Published in the Trustworthy Biometrics Special Issue of IEEE
  Transactions on Biometrics, Behavior, and Identity Science 2021
- **Journal**: None
- **Summary**: We present a method to search for a probe (or query) image representation against a large gallery in the encrypted domain. We require that the probe and gallery images be represented in terms of a fixed-length representation, which is typical for representations obtained from learned networks. Our encryption scheme is agnostic to how the fixed-length representation is obtained and can therefore be applied to any fixed-length representation in any application domain. Our method, dubbed HERS (Homomorphically Encrypted Representation Search), operates by (i) compressing the representation towards its estimated intrinsic dimensionality with minimal loss of accuracy (ii) encrypting the compressed representation using the proposed fully homomorphic encryption scheme, and (iii) efficiently searching against a gallery of encrypted representations directly in the encrypted domain, without decrypting them. Numerical results on large galleries of face, fingerprint, and object datasets such as ImageNet show that, for the first time, accurate and fast image search within the encrypted domain is feasible at scale (500 seconds; $275\times$ speed up over state-of-the-art for encrypted search against a gallery of 100 million). Code is available at https://github.com/human-analysis/hers-encrypted-image-search



### Interval Neural Networks as Instability Detectors for Image Reconstructions
- **Arxiv ID**: http://arxiv.org/abs/2003.13471v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML, I.5.1; I.4.5; J.3; I.2.m
- **Links**: [PDF](http://arxiv.org/pdf/2003.13471v1)
- **Published**: 2020-03-27 01:34:16+00:00
- **Updated**: 2020-03-27 01:34:16+00:00
- **Authors**: Jan Macdonald, Maximilian März, Luis Oala, Wojciech Samek
- **Comment**: JM, MM and LO contributed equally
- **Journal**: None
- **Summary**: This work investigates the detection of instabilities that may occur when utilizing deep learning models for image reconstruction tasks. Although neural networks often empirically outperform traditional reconstruction methods, their usage for sensitive medical applications remains controversial. Indeed, in a recent series of works, it has been demonstrated that deep learning approaches are susceptible to various types of instabilities, caused for instance by adversarial noise or out-of-distribution features. It is argued that this phenomenon can be observed regardless of the underlying architecture and that there is no easy remedy. Based on this insight, the present work demonstrates on two use cases how uncertainty quantification methods can be employed as instability detectors. In particular, it is shown that the recently proposed Interval Neural Networks are highly effective in revealing instabilities of reconstructions. Such an ability is crucial to ensure a safe use of deep learning-based methods for medical image reconstruction.



### Multi-Granularity Reference-Aided Attentive Feature Aggregation for Video-based Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2003.12224v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.12224v1)
- **Published**: 2020-03-27 03:49:21+00:00
- **Updated**: 2020-03-27 03:49:21+00:00
- **Authors**: Zhizheng Zhang, Cuiling Lan, Wenjun Zeng, Zhibo Chen
- **Comment**: Accepted by CVPR2020
- **Journal**: None
- **Summary**: Video-based person re-identification (reID) aims at matching the same person across video clips. It is a challenging task due to the existence of redundancy among frames, newly revealed appearance, occlusion, and motion blurs. In this paper, we propose an attentive feature aggregation module, namely Multi-Granularity Reference-aided Attentive Feature Aggregation (MG-RAFA), to delicately aggregate spatio-temporal features into a discriminative video-level feature representation. In order to determine the contribution/importance of a spatial-temporal feature node, we propose to learn the attention from a global view with convolutional operations. Specifically, we stack its relations, i.e., pairwise correlations with respect to a representative set of reference feature nodes (S-RFNs) that represents global video information, together with the feature itself to infer the attention. Moreover, to exploit the semantics of different levels, we propose to learn multi-granularity attentions based on the relations captured at different granularities. Extensive ablation studies demonstrate the effectiveness of our attentive feature aggregation module MG-RAFA. Our framework achieves the state-of-the-art performance on three benchmark datasets.



### Learning to Optimize Non-Rigid Tracking
- **Arxiv ID**: http://arxiv.org/abs/2003.12230v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.12230v1)
- **Published**: 2020-03-27 04:40:57+00:00
- **Updated**: 2020-03-27 04:40:57+00:00
- **Authors**: Yang Li, Aljaž Božič, Tianwei Zhang, Yanli Ji, Tatsuya Harada, Matthias Nießner
- **Comment**: Accepted to CVPR'2020 (oral)
- **Journal**: None
- **Summary**: One of the widespread solutions for non-rigid tracking has a nested-loop structure: with Gauss-Newton to minimize a tracking objective in the outer loop, and Preconditioned Conjugate Gradient (PCG) to solve a sparse linear system in the inner loop. In this paper, we employ learnable optimizations to improve tracking robustness and speed up solver convergence. First, we upgrade the tracking objective by integrating an alignment data term on deep features which are learned end-to-end through CNN. The new tracking objective can capture the global deformation which helps Gauss-Newton to jump over local minimum, leading to robust tracking on large non-rigid motions. Second, we bridge the gap between the preconditioning technique and learning method by introducing a ConditionNet which is trained to generate a preconditioner such that PCG can converge within a small number of steps. Experimental results indicate that the proposed learning method converges faster than the original PCG by a large margin.



### Towards Discriminability and Diversity: Batch Nuclear-norm Maximization under Label Insufficient Situations
- **Arxiv ID**: http://arxiv.org/abs/2003.12237v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.12237v1)
- **Published**: 2020-03-27 05:04:24+00:00
- **Updated**: 2020-03-27 05:04:24+00:00
- **Authors**: Shuhao Cui, Shuhui Wang, Junbao Zhuo, Liang Li, Qingming Huang, Qi Tian
- **Comment**: Accepted to CVPR 2020 as Oral
- **Journal**: None
- **Summary**: The learning of the deep networks largely relies on the data with human-annotated labels. In some label insufficient situations, the performance degrades on the decision boundary with high data density. A common solution is to directly minimize the Shannon Entropy, but the side effect caused by entropy minimization, i.e., reduction of the prediction diversity, is mostly ignored. To address this issue, we reinvestigate the structure of classification output matrix of a randomly selected data batch. We find by theoretical analysis that the prediction discriminability and diversity could be separately measured by the Frobenius-norm and rank of the batch output matrix. Besides, the nuclear-norm is an upperbound of the Frobenius-norm, and a convex approximation of the matrix rank. Accordingly, to improve both discriminability and diversity, we propose Batch Nuclear-norm Maximization (BNM) on the output matrix. BNM could boost the learning under typical label insufficient learning scenarios, such as semi-supervised learning, domain adaptation and open domain recognition. On these tasks, extensive experimental results show that BNM outperforms competitors and works well with existing well-known methods. The code is available at https://github.com/cuishuhao/BNM.



### MiLeNAS: Efficient Neural Architecture Search via Mixed-Level Reformulation
- **Arxiv ID**: http://arxiv.org/abs/2003.12238v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.12238v1)
- **Published**: 2020-03-27 05:06:54+00:00
- **Updated**: 2020-03-27 05:06:54+00:00
- **Authors**: Chaoyang He, Haishan Ye, Li Shen, Tong Zhang
- **Comment**: This paper is published in CVPR 2020 (IEEE/CVF Conference on Computer
  Vision and Pattern Recognition 2020)
- **Journal**: None
- **Summary**: Many recently proposed methods for Neural Architecture Search (NAS) can be formulated as bilevel optimization. For efficient implementation, its solution requires approximations of second-order methods. In this paper, we demonstrate that gradient errors caused by such approximations lead to suboptimality, in the sense that the optimization procedure fails to converge to a (locally) optimal solution. To remedy this, this paper proposes \mldas, a mixed-level reformulation for NAS that can be optimized efficiently and reliably. It is shown that even when using a simple first-order method on the mixed-level formulation, \mldas\ can achieve a lower validation error for NAS problems. Consequently, architectures obtained by our method achieve consistently higher accuracies than those obtained from bilevel optimization. Moreover, \mldas\ proposes a framework beyond DARTS. It is upgraded via model size-based search and early stopping strategies to complete the search process in around 5 hours. Extensive experiments within the convolutional architecture search space validate the effectiveness of our approach.



### Dynamic Region-Aware Convolution
- **Arxiv ID**: http://arxiv.org/abs/2003.12243v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.12243v3)
- **Published**: 2020-03-27 05:49:57+00:00
- **Updated**: 2021-03-15 16:28:46+00:00
- **Authors**: Jin Chen, Xijun Wang, Zichao Guo, Xiangyu Zhang, Jian Sun
- **Comment**: Accepted at CVPR 2021
- **Journal**: None
- **Summary**: We propose a new convolution called Dynamic Region-Aware Convolution (DRConv), which can automatically assign multiple filters to corresponding spatial regions where features have similar representation. In this way, DRConv outperforms standard convolution in modeling semantic variations. Standard convolutional layer can increase the number of filers to extract more visual elements but results in high computational cost. More gracefully, our DRConv transfers the increasing channel-wise filters to spatial dimension with learnable instructor, which not only improve representation ability of convolution, but also maintains computational cost and the translation-invariance as standard convolution dose. DRConv is an effective and elegant method for handling complex and variable spatial information distribution. It can substitute standard convolution in any existing networks for its plug-and-play property, especially to power convolution layers in efficient networks. We evaluate DRConv on a wide range of models (MobileNet series, ShuffleNetV2, etc.) and tasks (Classification, Face Recognition, Detection and Segmentation). On ImageNet classification, DRConv-based ShuffleNetV2-0.5x achieves state-of-the-art performance of 67.1% at 46M multiply-adds level with 6.3% relative improvement.



### One-Shot GAN Generated Fake Face Detection
- **Arxiv ID**: http://arxiv.org/abs/2003.12244v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.12244v1)
- **Published**: 2020-03-27 05:51:14+00:00
- **Updated**: 2020-03-27 05:51:14+00:00
- **Authors**: Hadi Mansourifar, Weidong Shi
- **Comment**: None
- **Journal**: None
- **Summary**: Fake face detection is a significant challenge for intelligent systems as generative models become more powerful every single day. As the quality of fake faces increases, the trained models become more and more inefficient to detect the novel fake faces, since the corresponding training data is considered outdated. In this case, robust One-Shot learning methods is more compatible with the requirements of changeable training data. In this paper, we propose a universal One-Shot GAN generated fake face detection method which can be used in significantly different areas of anomaly detection. The proposed method is based on extracting out-of-context objects from faces via scene understanding models. To do so, we use state of the art scene understanding and object detection methods as a pre-processing tool to detect the weird objects in the face. Second, we create a bag of words given all the detected out-of-context objects per all training data. This way, we transform each image into a sparse vector where each feature represents the confidence score related to each detected object in the image. Our experiments show that, we can discriminate fake faces from real ones in terms of out-of-context features. It means that, different sets of objects are detected in fake faces comparing to real ones when we analyze them with scene understanding and object detection models. We prove that, the proposed method can outperform previous methods based on our experiments on Style-GAN generated fake faces.



### A Comprehensive Review for Breast Histopathology Image Analysis Using Classical and Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2003.12255v2
- **DOI**: 10.1109/ACCESS.2020.2993788
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.12255v2)
- **Published**: 2020-03-27 06:53:41+00:00
- **Updated**: 2020-06-18 12:48:38+00:00
- **Authors**: Xiaomin Zhou, Chen Li, Md Mamunur Rahaman, Yudong Yao, Shiliang Ai, Changhao Sun, Xiaoyan Li, Qian Wang, Tao Jiang
- **Comment**: 25 pages,19 figures
- **Journal**: IEEE Access, Vol.8, page:90931-90956, 2020
- **Summary**: Breast cancer is one of the most common and deadliest cancers among women. Since histopathological images contain sufficient phenotypic information, they play an indispensable role in the diagnosis and treatment of breast cancers. To improve the accuracy and objectivity of Breast Histopathological Image Analysis (BHIA), Artificial Neural Network (ANN) approaches are widely used in the segmentation and classification tasks of breast histopathological images. In this review, we present a comprehensive overview of the BHIA techniques based on ANNs. First of all, we categorize the BHIA systems into classical and deep neural networks for in-depth investigation. Then, the relevant studies based on BHIA systems are presented. After that, we analyze the existing models to discover the most suitable algorithms. Finally, publicly accessible datasets, along with their download links, are provided for the convenience of future researchers.



### Weakly Supervised Dataset Collection for Robust Person Detection
- **Arxiv ID**: http://arxiv.org/abs/2003.12263v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.12263v2)
- **Published**: 2020-03-27 07:36:59+00:00
- **Updated**: 2020-05-01 07:35:26+00:00
- **Authors**: Munetaka Minoguchi, Ken Okayama, Yutaka Satoh, Hirokatsu Kataoka
- **Comment**: Project page:
  https://github.com/cvpaperchallenge/FashionCultureDataBase_DLoader The paper
  is under consideration at Pattern Recognition Letters
- **Journal**: None
- **Summary**: To construct an algorithm that can provide robust person detection, we present a dataset with over 8 million images that was produced in a weakly supervised manner. Through labor-intensive human annotation, the person detection research community has produced relatively small datasets containing on the order of 100,000 images, such as the EuroCity Persons dataset, which includes 240,000 bounding boxes. Therefore, we have collected 8.7 million images of persons based on a two-step collection process, namely person detection with an existing detector and data refinement for false positive suppression. According to the experimental results, the Weakly Supervised Person Dataset (WSPD) is simple yet effective for person detection pre-training. In the context of pre-trained person detection algorithms, our WSPD pre-trained model has 13.38 and 6.38% better accuracy than the same model trained on the fully supervised ImageNet and EuroCity Persons datasets, respectively, when verified with the Caltech Pedestrian.



### Controllable Person Image Synthesis with Attribute-Decomposed GAN
- **Arxiv ID**: http://arxiv.org/abs/2003.12267v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.12267v4)
- **Published**: 2020-03-27 07:47:06+00:00
- **Updated**: 2020-07-19 05:32:31+00:00
- **Authors**: Yifang Men, Yiming Mao, Yuning Jiang, Wei-Ying Ma, Zhouhui Lian
- **Comment**: Accepted by CVPR 2020 (Oral). Project Page:
  https://menyifang.github.io/projects/ADGAN/ADGAN.html
- **Journal**: None
- **Summary**: This paper introduces the Attribute-Decomposed GAN, a novel generative model for controllable person image synthesis, which can produce realistic person images with desired human attributes (e.g., pose, head, upper clothes and pants) provided in various source inputs. The core idea of the proposed model is to embed human attributes into the latent space as independent codes and thus achieve flexible and continuous control of attributes via mixing and interpolation operations in explicit style representations. Specifically, a new architecture consisting of two encoding pathways with style block connections is proposed to decompose the original hard mapping into multiple more accessible subtasks. In source pathway, we further extract component layouts with an off-the-shelf human parser and feed them into a shared global texture encoder for decomposed latent codes. This strategy allows for the synthesis of more realistic output images and automatic separation of un-annotated attributes. Experimental results demonstrate the proposed method's superiority over the state of the art in pose transfer and its effectiveness in the brand-new task of component attribute transfer.



### Applications of the Streaming Networks
- **Arxiv ID**: http://arxiv.org/abs/2004.11805v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.11805v1)
- **Published**: 2020-03-27 08:13:17+00:00
- **Updated**: 2020-03-27 08:13:17+00:00
- **Authors**: Sergey Tarasenko, Fumihiko Takahashi
- **Comment**: 4 pages, 6 figures
- **Journal**: None
- **Summary**: Most recently Streaming Networks (STnets) have been introduced as a mechanism of robust noise-corrupted images classification. STnets is a family of convolutional neural networks, which consists of multiple neural networks (streams), which have different inputs and their outputs are concatenated and fed into a single joint classifier. The original paper has illustrated how STnets can successfully classify images from Cifar10, EuroSat and UCmerced datasets, when images were corrupted with various levels of random zero noise. In this paper, we demonstrate that STnets are capable of high accuracy classification of images corrupted with Gaussian noise, fog, snow, etc. (Cifar10 corrupted dataset) and low light images (subset of Carvana dataset). We also introduce a new type of STnets called Hybrid STnets. Thus, we illustrate that STnets is a universal tool of image classification when original training dataset is corrupted with noise or other transformations, which lead to information loss from original images.



### Towards Accurate Scene Text Recognition with Semantic Reasoning Networks
- **Arxiv ID**: http://arxiv.org/abs/2003.12294v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.12294v1)
- **Published**: 2020-03-27 09:19:25+00:00
- **Updated**: 2020-03-27 09:19:25+00:00
- **Authors**: Deli Yu, Xuan Li, Chengquan Zhang, Junyu Han, Jingtuo Liu, Errui Ding
- **Comment**: Accepted to CVPR2020
- **Journal**: None
- **Summary**: Scene text image contains two levels of contents: visual texture and semantic information. Although the previous scene text recognition methods have made great progress over the past few years, the research on mining semantic information to assist text recognition attracts less attention, only RNN-like structures are explored to implicitly model semantic information. However, we observe that RNN based methods have some obvious shortcomings, such as time-dependent decoding manner and one-way serial transmission of semantic context, which greatly limit the help of semantic information and the computation efficiency. To mitigate these limitations, we propose a novel end-to-end trainable framework named semantic reasoning network (SRN) for accurate scene text recognition, where a global semantic reasoning module (GSRM) is introduced to capture global semantic context through multi-way parallel transmission. The state-of-the-art results on 7 public benchmarks, including regular text, irregular text and non-Latin long text, verify the effectiveness and robustness of the proposed method. In addition, the speed of SRN has significant advantages over the RNN based methods, demonstrating its value in practical use.



### Generalizable Model-agnostic Semantic Segmentation via Target-specific Normalization
- **Arxiv ID**: http://arxiv.org/abs/2003.12296v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.12296v2)
- **Published**: 2020-03-27 09:25:19+00:00
- **Updated**: 2021-08-31 06:43:50+00:00
- **Authors**: Jian Zhang, Lei Qi, Yinghuan Shi, Yang Gao
- **Comment**: Accepted by Pattern Recognition (PR)
- **Journal**: None
- **Summary**: Semantic segmentation in a supervised learning manner has achieved significant progress in recent years. However, its performance usually drops dramatically due to the data-distribution discrepancy between seen and unseen domains when we directly deploy the trained model to segment the images of unseen (or new coming) domains. To this end, we propose a novel domain generalization framework for the generalizable semantic segmentation task, which enhances the generalization ability of the model from two different views, including the training paradigm and the test strategy. Concretely, we exploit the model-agnostic learning to simulate the domain shift problem, which deals with the domain generalization from the training scheme perspective. Besides, considering the data-distribution discrepancy between seen source and unseen target domains, we develop the target-specific normalization scheme to enhance the generalization ability. Furthermore, when images come one by one in the test stage, we design the image-based memory bank (Image Bank in short) with style-based selection policy to select similar images to obtain more accurate statistics of normalization. Extensive experiments highlight that the proposed method produces state-of-the-art performance for the domain generalization of semantic segmentation on multiple benchmark segmentation datasets, i.e., Cityscapes, Mapillary.



### CurlingNet: Compositional Learning between Images and Text for Fashion IQ Data
- **Arxiv ID**: http://arxiv.org/abs/2003.12299v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.12299v2)
- **Published**: 2020-03-27 09:36:32+00:00
- **Updated**: 2020-03-30 04:35:16+00:00
- **Authors**: Youngjae Yu, Seunghwan Lee, Yuncheol Choi, Gunhee Kim
- **Comment**: 4 pages, 4 figures, ICCV 2019 Linguistics Meets image and video
  retrieval workshop, Fashion IQ challenge
- **Journal**: None
- **Summary**: We present an approach named CurlingNet that can measure the semantic distance of composition of image-text embedding. In order to learn an effective image-text composition for the data in the fashion domain, our model proposes two key components as follows. First, the Delivery makes the transition of a source image in an embedding space. Second, the Sweeping emphasizes query-related components of fashion images in the embedding space. We utilize a channel-wise gating mechanism to make it possible. Our single model outperforms previous state-of-the-art image-text composition models including TIRG and FiLM. We participate in the first fashion-IQ challenge in ICCV 2019, for which ensemble of our model achieves one of the best performances.



### Lightweight Photometric Stereo for Facial Details Recovery
- **Arxiv ID**: http://arxiv.org/abs/2003.12307v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.12307v1)
- **Published**: 2020-03-27 10:10:00+00:00
- **Updated**: 2020-03-27 10:10:00+00:00
- **Authors**: Xueying Wang, Yudong Guo, Bailin Deng, Juyong Zhang
- **Comment**: Accepted to CVPR2020. The source code is available
  https://github.com/Juyong/FacePSNet
- **Journal**: None
- **Summary**: Recently, 3D face reconstruction from a single image has achieved great success with the help of deep learning and shape prior knowledge, but they often fail to produce accurate geometry details. On the other hand, photometric stereo methods can recover reliable geometry details, but require dense inputs and need to solve a complex optimization problem. In this paper, we present a lightweight strategy that only requires sparse inputs or even a single image to recover high-fidelity face shapes with images captured under near-field lights. To this end, we construct a dataset containing 84 different subjects with 29 expressions under 3 different lights. Data augmentation is applied to enrich the data in terms of diversity in identity, lighting, expression, etc. With this constructed dataset, we propose a novel neural network specially designed for photometric stereo based 3D face reconstruction. Extensive experiments and comparisons demonstrate that our method can generate high-quality reconstruction results with one to three facial images captured under near-field lights. Our full framework is available at https://github.com/Juyong/FacePSNet.



### An Investigation into the Stochasticity of Batch Whitening
- **Arxiv ID**: http://arxiv.org/abs/2003.12327v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.12327v1)
- **Published**: 2020-03-27 11:06:32+00:00
- **Updated**: 2020-03-27 11:06:32+00:00
- **Authors**: Lei Huang, Lei Zhao, Yi Zhou, Fan Zhu, Li Liu, Ling Shao
- **Comment**: Accepted to CVPR 2020. The Code is available at
  https://github.com/huangleiBuaa/StochasticityBW
- **Journal**: None
- **Summary**: Batch Normalization (BN) is extensively employed in various network architectures by performing standardization within mini-batches.   A full understanding of the process has been a central target in the deep learning communities.   Unlike existing works, which usually only analyze the standardization operation, this paper investigates the more general Batch Whitening (BW). Our work originates from the observation that while various whitening transformations equivalently improve the conditioning, they show significantly different behaviors in discriminative scenarios and training Generative Adversarial Networks (GANs).   We attribute this phenomenon to the stochasticity that BW introduces.   We quantitatively investigate the stochasticity of different whitening transformations and show that it correlates well with the optimization behaviors during training.   We also investigate how stochasticity relates to the estimation of population statistics during inference.   Based on our analysis, we provide a framework for designing and comparing BW algorithms in different scenarios.   Our proposed BW algorithm improves the residual networks by a significant margin on ImageNet classification.   Besides, we show that the stochasticity of BW can improve the GAN's performance with, however, the sacrifice of the training stability.



### Viral Pneumonia Screening on Chest X-ray Images Using Confidence-Aware Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2003.12338v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.12338v4)
- **Published**: 2020-03-27 11:32:18+00:00
- **Updated**: 2020-12-02 00:05:40+00:00
- **Authors**: Jianpeng Zhang, Yutong Xie, Guansong Pang, Zhibin Liao, Johan Verjans, Wenxin Li, Zongji Sun, Jian He, Yi Li, Chunhua Shen, Yong Xia
- **Comment**: Accepted to IEEE Trans. Medical Imaging. 12 pages
- **Journal**: None
- **Summary**: Cluster of viral pneumonia occurrences during a short period of time may be a harbinger of an outbreak or pandemic, like SARS, MERS, and recent COVID-19. Rapid and accurate detection of viral pneumonia using chest X-ray can be significantly useful in large-scale screening and epidemic prevention, particularly when other chest imaging modalities are less available. Viral pneumonia often have diverse causes and exhibit notably different visual appearances on X-ray images. The evolution of viruses and the emergence of novel mutated viruses further result in substantial dataset shift, which greatly limits the performance of classification approaches. In this paper, we formulate the task of differentiating viral pneumonia from non-viral pneumonia and healthy controls into an one-class classification-based anomaly detection problem, and thus propose the confidence-aware anomaly detection (CAAD) model, which consists of a shared feature extractor, an anomaly detection module, and a confidence prediction module. If the anomaly score produced by the anomaly detection module is large enough or the confidence score estimated by the confidence prediction module is small enough, we accept the input as an anomaly case (i.e., viral pneumonia). The major advantage of our approach over binary classification is that we avoid modeling individual viral pneumonia classes explicitly and treat all known viral pneumonia cases as anomalies to reinforce the one-class model. The proposed model outperforms binary classification models on the clinical X-VIRAL dataset that contains 5,977 viral pneumonia (no COVID-19) cases, 18,619 non-viral pneumonia cases, and 18,774 healthy controls.



### Introducing Pose Consistency and Warp-Alignment for Self-Supervised 6D Object Pose Estimation in Color Images
- **Arxiv ID**: http://arxiv.org/abs/2003.12344v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.12344v2)
- **Published**: 2020-03-27 11:53:38+00:00
- **Updated**: 2020-10-16 09:49:53+00:00
- **Authors**: Juil Sock, Guillermo Garcia-Hernando, Anil Armagan, Tae-Kyun Kim
- **Comment**: Accepted to 3DV'2020 as Oral
- **Journal**: None
- **Summary**: Most successful approaches to estimate the 6D pose of an object typically train a neural network by supervising the learning with annotated poses in real world images. These annotations are generally expensive to obtain and a common workaround is to generate and train on synthetic scenes, with the drawback of limited generalisation when the model is deployed in the real world. In this work, a two-stage 6D object pose estimator framework that can be applied on top of existing neural-network-based approaches and that does not require pose annotations on real images is proposed. The first self-supervised stage enforces the pose consistency between rendered predictions and real input images, narrowing the gap between the two domains. The second stage fine-tunes the previously trained model by enforcing the photometric consistency between pairs of different object views, where one image is warped and aligned to match the view of the other and thus enabling their comparison. In the absence of both real image annotations and depth information, applying the proposed framework on top of two recent approaches results in state-of-the-art performance when compared to methods trained only on synthetic data, domain adaptation baselines and a concurrent self-supervised approach on LINEMOD, LINEMOD OCCLUSION and HomebrewedDB datasets.



### Convolutional Spiking Neural Networks for Spatio-Temporal Feature Extraction
- **Arxiv ID**: http://arxiv.org/abs/2003.12346v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.12346v2)
- **Published**: 2020-03-27 11:58:51+00:00
- **Updated**: 2021-01-18 00:23:29+00:00
- **Authors**: Ali Samadzadeh, Fatemeh Sadat Tabatabaei Far, Ali Javadi, Ahmad Nickabadi, Morteza Haghir Chehreghani
- **Comment**: 10 pages, 7 figures, 2 tables
- **Journal**: None
- **Summary**: Spiking neural networks (SNNs) can be used in low-power and embedded systems (such as emerging neuromorphic chips) due to their event-based nature. Also, they have the advantage of low computation cost in contrast to conventional artificial neural networks (ANNs), while preserving ANN's properties. However, temporal coding in layers of convolutional spiking neural networks and other types of SNNs has yet to be studied. In this paper, we provide insight into spatio-temporal feature extraction of convolutional SNNs in experiments designed to exploit this property. The shallow convolutional SNN outperforms state-of-the-art spatio-temporal feature extractor methods such as C3D, ConvLstm, and similar networks. Furthermore, we present a new deep spiking architecture to tackle real-world problems (in particular classification tasks) which achieved superior performance compared to other SNN methods on NMNIST (99.6%), DVS-CIFAR10 (69.2%) and DVS-Gesture (96.7%) and ANN methods on UCF-101 (42.1%) and HMDB-51 (21.5%) datasets. It is also worth noting that the training process is implemented based on variation of spatio-temporal backpropagation explained in the paper.



### Enhanced Self-Perception in Mixed Reality: Egocentric Arm Segmentation and Database with Automatic Labelling
- **Arxiv ID**: http://arxiv.org/abs/2003.12352v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.12352v1)
- **Published**: 2020-03-27 12:09:27+00:00
- **Updated**: 2020-03-27 12:09:27+00:00
- **Authors**: Ester Gonzalez-Sosa, Pablo Perez, Ruben Tolosana, Redouane Kachach, Alvaro Villegas
- **Comment**: None
- **Journal**: None
- **Summary**: In this study, we focus on the egocentric segmentation of arms to improve self-perception in Augmented Virtuality (AV). The main contributions of this work are: i) a comprehensive survey of segmentation algorithms for AV; ii) an Egocentric Arm Segmentation Dataset, composed of more than 10, 000 images, comprising variations of skin color, and gender, among others. We provide all details required for the automated generation of groundtruth and semi-synthetic images; iii) the use of deep learning for the first time for segmenting arms in AV; iv) to showcase the usefulness of this database, we report results on different real egocentric hand datasets, including GTEA Gaze+, EDSH, EgoHands, Ego Youtube Hands, THU-Read, TEgO, FPAB, and Ego Gesture, which allow for direct comparisons with existing approaches utilizing color or depth. Results confirm the suitability of the EgoArm dataset for this task, achieving improvement up to 40% with respect to the original network, depending on the particular dataset. Results also suggest that, while approaches based on color or depth can work in controlled conditions (lack of occlusion, uniform lighting, only objects of interest in the near range, controlled background, etc.), egocentric segmentation based on deep learning is more robust in real AV applications.



### Deep-n-Cheap: An Automated Search Framework for Low Complexity Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2004.00974v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.00974v3)
- **Published**: 2020-03-27 13:00:21+00:00
- **Updated**: 2020-09-05 21:24:04+00:00
- **Authors**: Sourya Dey, Saikrishna C. Kanala, Keith M. Chugg, Peter A. Beerel
- **Comment**: Accepted as a conference paper at ACML 2020
- **Journal**: None
- **Summary**: We present Deep-n-Cheap -- an open-source AutoML framework to search for deep learning models. This search includes both architecture and training hyperparameters, and supports convolutional neural networks and multi-layer perceptrons. Our framework is targeted for deployment on both benchmark and custom datasets, and as a result, offers a greater degree of search space customizability as compared to a more limited search over only pre-existing models from literature. We also introduce the technique of 'search transfer', which demonstrates the generalization capabilities of the models found by our framework to multiple datasets.   Deep-n-Cheap includes a user-customizable complexity penalty which trades off performance with training time or number of parameters. Specifically, our framework results in models offering performance comparable to state-of-the-art while taking 1-2 orders of magnitude less time to train than models from other AutoML and model search frameworks. Additionally, this work investigates and develops various insights regarding the search process. In particular, we show the superiority of a greedy strategy and justify our choice of Bayesian optimization as the primary search methodology over random / grid search.



### Modeling 3D Shapes by Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2003.12397v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.12397v3)
- **Published**: 2020-03-27 13:05:39+00:00
- **Updated**: 2020-09-17 04:45:27+00:00
- **Authors**: Cheng Lin, Tingxiang Fan, Wenping Wang, Matthias Nießner
- **Comment**: Accepted to ECCV 2020; Video: https://youtu.be/w5e9g_lvbyE
- **Journal**: None
- **Summary**: We explore how to enable machines to model 3D shapes like human modelers using deep reinforcement learning (RL). In 3D modeling software like Maya, a modeler usually creates a mesh model in two steps: (1) approximating the shape using a set of primitives; (2) editing the meshes of the primitives to create detailed geometry. Inspired by such artist-based modeling, we propose a two-step neural framework based on RL to learn 3D modeling policies. By taking actions and collecting rewards in an interactive environment, the agents first learn to parse a target shape into primitives and then to edit the geometry. To effectively train the modeling agents, we introduce a novel training algorithm that combines heuristic policy, imitation learning and reinforcement learning. Our experiments show that the agents can learn good policies to produce regular and structure-aware mesh models, which demonstrates the feasibility and effectiveness of the proposed RL framework.



### Learning Implicit Surface Light Fields
- **Arxiv ID**: http://arxiv.org/abs/2003.12406v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.12406v1)
- **Published**: 2020-03-27 13:17:45+00:00
- **Updated**: 2020-03-27 13:17:45+00:00
- **Authors**: Michael Oechsle, Michael Niemeyer, Lars Mescheder, Thilo Strauss, Andreas Geiger
- **Comment**: None
- **Journal**: None
- **Summary**: Implicit representations of 3D objects have recently achieved impressive results on learning-based 3D reconstruction tasks. While existing works use simple texture models to represent object appearance, photo-realistic image synthesis requires reasoning about the complex interplay of light, geometry and surface properties. In this work, we propose a novel implicit representation for capturing the visual appearance of an object in terms of its surface light field. In contrast to existing representations, our implicit model represents surface light fields in a continuous fashion and independent of the geometry. Moreover, we condition the surface light field with respect to the location and color of a small light source. Compared to traditional surface light field models, this allows us to manipulate the light source and relight the object using environment maps. We further demonstrate the capabilities of our model to predict the visual appearance of an unseen object from a single real RGB image and corresponding 3D shape information. As evidenced by our experiments, our model is able to infer rich visual appearance including shadows and specular reflections. Finally, we show that the proposed representation can be embedded into a variational auto-encoder for generating novel appearances that conform to the specified illumination conditions.



### Weakly-Supervised Action Localization by Generative Attention Modeling
- **Arxiv ID**: http://arxiv.org/abs/2003.12424v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.12424v2)
- **Published**: 2020-03-27 14:02:56+00:00
- **Updated**: 2020-03-30 14:36:48+00:00
- **Authors**: Baifeng Shi, Qi Dai, Yadong Mu, Jingdong Wang
- **Comment**: CVPR2020. Code is available at
  https://github.com/bfshi/DGAM-Weakly-Supervised-Action-Localization
- **Journal**: None
- **Summary**: Weakly-supervised temporal action localization is a problem of learning an action localization model with only video-level action labeling available. The general framework largely relies on the classification activation, which employs an attention model to identify the action-related frames and then categorizes them into different classes. Such method results in the action-context confusion issue: context frames near action clips tend to be recognized as action frames themselves, since they are closely related to the specific classes. To solve the problem, in this paper we propose to model the class-agnostic frame-wise probability conditioned on the frame attention using conditional Variational Auto-Encoder (VAE). With the observation that the context exhibits notable difference from the action at representation level, a probabilistic model, i.e., conditional VAE, is learned to model the likelihood of each frame given the attention. By maximizing the conditional probability with respect to the attention, the action and non-action frames are well separated. Experiments on THUMOS14 and ActivityNet1.2 demonstrate advantage of our method and effectiveness in handling action-context confusion problem. Code is now available on GitHub.



### Augmenting Colonoscopy using Extended and Directional CycleGAN for Lossy Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2003.12473v3
- **DOI**: 10.1109/CVPR42600.2020.00475
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.12473v3)
- **Published**: 2020-03-27 15:34:17+00:00
- **Updated**: 2021-08-26 09:31:33+00:00
- **Authors**: Shawn Mathew, Saad Nadeem, Sruti Kumari, Arie Kaufman
- **Comment**: CVPR 2020. **First two authors contributed equally to this work
- **Journal**: None
- **Summary**: Colorectal cancer screening modalities, such as optical colonoscopy (OC) and virtual colonoscopy (VC), are critical for diagnosing and ultimately removing polyps (precursors of colon cancer). The non-invasive VC is normally used to inspect a 3D reconstructed colon (from CT scans) for polyps and if found, the OC procedure is performed to physically traverse the colon via endoscope and remove these polyps. In this paper, we present a deep learning framework, Extended and Directional CycleGAN, for lossy unpaired image-to-image translation between OC and VC to augment OC video sequences with scale-consistent depth information from VC, and augment VC with patient-specific textures, color and specular highlights from OC (e.g, for realistic polyp synthesis). Both OC and VC contain structural information, but it is obscured in OC by additional patient-specific texture and specular highlights, hence making the translation from OC to VC lossy. The existing CycleGAN approaches do not handle lossy transformations. To address this shortcoming, we introduce an extended cycle consistency loss, which compares the geometric structures from OC in the VC domain. This loss removes the need for the CycleGAN to embed OC information in the VC domain. To handle a stronger removal of the textures and lighting, a Directional Discriminator is introduced to differentiate the direction of translation (by creating paired information for the discriminator), as opposed to the standard CycleGAN which is direction-agnostic. Combining the extended cycle consistency loss and the Directional Discriminator, we show state-of-the-art results on scale-consistent depth inference for phantom, textured VC and for real polyp and normal colon video sequences. We also present results for realistic pendunculated and flat polyp synthesis from bumps introduced in 3D VC models. Code/models: https://github.com/nadeemlab/CEP.



### Hybrid Models for Open Set Recognition
- **Arxiv ID**: http://arxiv.org/abs/2003.12506v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.12506v2)
- **Published**: 2020-03-27 16:14:27+00:00
- **Updated**: 2020-08-04 01:06:26+00:00
- **Authors**: Hongjie Zhang, Ang Li, Jie Guo, Yanwen Guo
- **Comment**: 16 pages, 4 figures
- **Journal**: None
- **Summary**: Open set recognition requires a classifier to detect samples not belonging to any of the classes in its training set. Existing methods fit a probability distribution to the training samples on their embedding space and detect outliers according to this distribution. The embedding space is often obtained from a discriminative classifier. However, such discriminative representation focuses only on known classes, which may not be critical for distinguishing the unknown classes. We argue that the representation space should be jointly learned from the inlier classifier and the density estimator (served as an outlier detector). We propose the OpenHybrid framework, which is composed of an encoder to encode the input data into a joint embedding space, a classifier to classify samples to inlier classes, and a flow-based density estimator to detect whether a sample belongs to the unknown category. A typical problem of existing flow-based models is that they may assign a higher likelihood to outliers. However, we empirically observe that such an issue does not occur in our experiments when learning a joint representation for discriminative and generative components. Experiments on standard open set benchmarks also reveal that an end-to-end trained OpenHybrid model significantly outperforms state-of-the-art methods and flow-based baselines.



### Assessing Image Quality Issues for Real-World Problems
- **Arxiv ID**: http://arxiv.org/abs/2003.12511v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.12511v2)
- **Published**: 2020-03-27 16:21:44+00:00
- **Updated**: 2020-03-30 16:47:09+00:00
- **Authors**: Tai-Yin Chiu, Yinan Zhao, Danna Gurari
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a new large-scale dataset that links the assessment of image quality issues to two practical vision tasks: image captioning and visual question answering. First, we identify for 39,181 images taken by people who are blind whether each is sufficient quality to recognize the content as well as what quality flaws are observed from six options. These labels serve as a critical foundation for us to make the following contributions: (1) a new problem and algorithms for deciding whether an image is insufficient quality to recognize the content and so not captionable, (2) a new problem and algorithms for deciding which of six quality flaws an image contains, (3) a new problem and algorithms for deciding whether a visual question is unanswerable due to unrecognizable content versus the content of interest being missing from the field of view, and (4) a novel application of more efficiently creating a large-scale image captioning dataset by automatically deciding whether an image is insufficient quality and so should not be captioned. We publicly-share our datasets and code to facilitate future extensions of this work: https://vizwiz.org.



### DA-NAS: Data Adapted Pruning for Efficient Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2003.12563v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.12563v1)
- **Published**: 2020-03-27 17:55:21+00:00
- **Updated**: 2020-03-27 17:55:21+00:00
- **Authors**: Xiyang Dai, Dongdong Chen, Mengchen Liu, Yinpeng Chen, Lu Yuan
- **Comment**: None
- **Journal**: None
- **Summary**: Efficient search is a core issue in Neural Architecture Search (NAS). It is difficult for conventional NAS algorithms to directly search the architectures on large-scale tasks like ImageNet. In general, the cost of GPU hours for NAS grows with regard to training dataset size and candidate set size. One common way is searching on a smaller proxy dataset (e.g., CIFAR-10) and then transferring to the target task (e.g., ImageNet). These architectures optimized on proxy data are not guaranteed to be optimal on the target task. Another common way is learning with a smaller candidate set, which may require expert knowledge and indeed betrays the essence of NAS. In this paper, we present DA-NAS that can directly search the architecture for large-scale target tasks while allowing a large candidate set in a more efficient manner. Our method is based on an interesting observation that the learning speed for blocks in deep neural networks is related to the difficulty of recognizing distinct categories. We carefully design a progressive data adapted pruning strategy for efficient architecture search. It will quickly trim low performed blocks on a subset of target dataset (e.g., easy classes), and then gradually find the best blocks on the whole target dataset. At this time, the original candidate set becomes as compact as possible, providing a faster search in the target task. Experiments on ImageNet verify the effectiveness of our approach. It is 2x faster than previous methods while the accuracy is currently state-of-the-art, at 76.2% under small FLOPs constraint. It supports an argument search space (i.e., more candidate blocks) to efficiently search the best-performing architecture.



### Probabilistic Regression for Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/2003.12565v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.12565v1)
- **Published**: 2020-03-27 17:58:37+00:00
- **Updated**: 2020-03-27 17:58:37+00:00
- **Authors**: Martin Danelljan, Luc Van Gool, Radu Timofte
- **Comment**: CVPR 2020. Includes appendix
- **Journal**: None
- **Summary**: Visual tracking is fundamentally the problem of regressing the state of the target in each video frame. While significant progress has been achieved, trackers are still prone to failures and inaccuracies. It is therefore crucial to represent the uncertainty in the target estimation. Although current prominent paradigms rely on estimating a state-dependent confidence score, this value lacks a clear probabilistic interpretation, complicating its use.   In this work, we therefore propose a probabilistic regression formulation and apply it to tracking. Our network predicts the conditional probability density of the target state given an input image. Crucially, our formulation is capable of modeling label noise stemming from inaccurate annotations and ambiguities in the task. The regression network is trained by minimizing the Kullback-Leibler divergence. When applied for tracking, our formulation not only allows a probabilistic representation of the output, but also substantially improves the performance. Our tracker sets a new state-of-the-art on six datasets, achieving 59.8% AUC on LaSOT and 75.8% Success on TrackingNet. The code and models are available at https://github.com/visionml/pytracking.



### GAN-based Priors for Quantifying Uncertainty
- **Arxiv ID**: http://arxiv.org/abs/2003.12597v1
- **DOI**: 10.13140/RG.2.2.28806.32322
- **Categories**: **stat.ML**, cs.CV, cs.LG, physics.comp-ph
- **Links**: [PDF](http://arxiv.org/pdf/2003.12597v1)
- **Published**: 2020-03-27 18:52:54+00:00
- **Updated**: 2020-03-27 18:52:54+00:00
- **Authors**: Dhruv V. Patel, Assad A. Oberai
- **Comment**: None
- **Journal**: None
- **Summary**: Bayesian inference is used extensively to quantify the uncertainty in an inferred field given the measurement of a related field when the two are linked by a mathematical model. Despite its many applications, Bayesian inference faces challenges when inferring fields that have discrete representations of large dimension, and/or have prior distributions that are difficult to characterize mathematically. In this work we demonstrate how the approximate distribution learned by a deep generative adversarial network (GAN) may be used as a prior in a Bayesian update to address both these challenges. We demonstrate the efficacy of this approach on two distinct, and remarkably broad, classes of problems. The first class leads to supervised learning algorithms for image classification with superior out of distribution detection and accuracy, and for image inpainting with built-in variance estimation. The second class leads to unsupervised learning algorithms for image denoising and for solving physics-driven inverse problems.



### Source Printer Identification from Document Images Acquired using Smartphone
- **Arxiv ID**: http://arxiv.org/abs/2003.12602v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.12602v1)
- **Published**: 2020-03-27 18:59:32+00:00
- **Updated**: 2020-03-27 18:59:32+00:00
- **Authors**: Sharad Joshi, Suraj Saxena, Nitin Khanna
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Vast volumes of printed documents continue to be used for various important as well as trivial applications. Such applications often rely on the information provided in the form of printed text documents whose integrity verification poses a challenge due to time constraints and lack of resources. Source printer identification provides essential information about the origin and integrity of a printed document in a fast and cost-effective manner. Even when fraudulent documents are identified, information about their origin can help stop future frauds. If a smartphone camera replaces scanner for the document acquisition process, document forensics would be more economical, user-friendly, and even faster in many applications where remote and distributed analysis is beneficial. Building on existing methods, we propose to learn a single CNN model from the fusion of letter images and their printer-specific noise residuals. In the absence of any publicly available dataset, we created a new dataset consisting of 2250 document images of text documents printed by eighteen printers and acquired by a smartphone camera at five acquisition settings. The proposed method achieves 98.42% document classification accuracy using images of letter 'e' under a 5x2 cross-validation approach. Further, when tested using about half a million letters of all types, it achieves 90.33% and 98.01% letter and document classification accuracies, respectively, thus highlighting the ability to learn a discriminative model without dependence on a single letter type. Also, classification accuracies are encouraging under various acquisition settings, including low illumination and change in angle between the document and camera planes.



### Image compression optimized for 3D reconstruction by utilizing deep neural networks
- **Arxiv ID**: http://arxiv.org/abs/2003.12618v2
- **DOI**: 10.1016/j.jvcir.2021.103208
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.12618v2)
- **Published**: 2020-03-27 19:55:30+00:00
- **Updated**: 2021-07-24 17:18:35+00:00
- **Authors**: Alex Golts, Yoav Y. Schechner
- **Comment**: None
- **Journal**: None
- **Summary**: Computer vision tasks are often expected to be executed on compressed images. Classical image compression standards like JPEG 2000 are widely used. However, they do not account for the specific end-task at hand. Motivated by works on recurrent neural network (RNN)-based image compression and three-dimensional (3D) reconstruction, we propose unified network architectures to solve both tasks jointly. These joint models provide image compression tailored for the specific task of 3D reconstruction. Images compressed by our proposed models, yield 3D reconstruction performance superior as compared to using JPEG 2000 compression. Our models significantly extend the range of compression rates for which 3D reconstruction is possible. We also show that this can be done highly efficiently at almost no additional cost to obtain compression on top of the computation already required for performing the 3D reconstruction task.



### Acceleration of Convolutional Neural Network Using FFT-Based Split Convolutions
- **Arxiv ID**: http://arxiv.org/abs/2003.12621v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.12621v2)
- **Published**: 2020-03-27 20:16:57+00:00
- **Updated**: 2020-04-03 21:14:18+00:00
- **Authors**: Kamran Chitsaz, Mohsen Hajabdollahi, Nader Karimi, Shadrokh Samavi, Shahram Shirani
- **Comment**: 5 pages, 4 figures
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have a large number of variables and hence suffer from a complexity problem for their implementation. Different methods and techniques have developed to alleviate the problem of CNN's complexity, such as quantization, pruning, etc. Among the different simplification methods, computation in the Fourier domain is regarded as a new paradigm for the acceleration of CNNs. Recent studies on Fast Fourier Transform (FFT) based CNN aiming at simplifying the computations required for FFT. However, there is a lot of space for working on the reduction of the computational complexity of FFT. In this paper, a new method for CNN processing in the FFT domain is proposed, which is based on input splitting. There are problems in the computation of FFT using small kernels in situations such as CNN. Splitting can be considered as an effective solution for such issues aroused by small kernels. Using splitting redundancy, such as overlap-and-add, is reduced and, efficiency is increased. Hardware implementation of the proposed FFT method, as well as different analyses of the complexity, are performed to demonstrate the proper performance of the proposed method.



### SceneCAD: Predicting Object Alignments and Layouts in RGB-D Scans
- **Arxiv ID**: http://arxiv.org/abs/2003.12622v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.12622v1)
- **Published**: 2020-03-27 20:17:00+00:00
- **Updated**: 2020-03-27 20:17:00+00:00
- **Authors**: Armen Avetisyan, Tatiana Khanova, Christopher Choy, Denver Dash, Angela Dai, Matthias Nießner
- **Comment**: Video here https://youtu.be/F0DpggYByh0
- **Journal**: None
- **Summary**: We present a novel approach to reconstructing lightweight, CAD-based representations of scanned 3D environments from commodity RGB-D sensors. Our key idea is to jointly optimize for both CAD model alignments as well as layout estimations of the scanned scene, explicitly modeling inter-relationships between objects-to-objects and objects-to-layout. Since object arrangement and scene layout are intrinsically coupled, we show that treating the problem jointly significantly helps to produce globally-consistent representations of a scene. Object CAD models are aligned to the scene by establishing dense correspondences between geometry, and we introduce a hierarchical layout prediction approach to estimate layout planes from corners and edges of the scene.To this end, we propose a message-passing graph neural network to model the inter-relationships between objects and layout, guiding generation of a globally object alignment in a scene. By considering the global scene layout, we achieve significantly improved CAD alignments compared to state-of-the-art methods, improving from 41.83% to 58.41% alignment accuracy on SUNCG and from 50.05% to 61.24% on ScanNet, respectively. The resulting CAD-based representations makes our method well-suited for applications in content creation such as augmented- or virtual reality.



### On the Evaluation of Prohibited Item Classification and Detection in Volumetric 3D Computed Tomography Baggage Security Screening Imagery
- **Arxiv ID**: http://arxiv.org/abs/2003.12625v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.12625v1)
- **Published**: 2020-03-27 20:17:58+00:00
- **Updated**: 2020-03-27 20:17:58+00:00
- **Authors**: Qian Wang, Neelanjan Bhowmik, Toby P. Breckon
- **Comment**: Accepted to IJCNN 2020
- **Journal**: None
- **Summary**: X-ray Computed Tomography (CT) based 3D imaging is widely used in airports for aviation security screening whilst prior work on prohibited item detection focuses primarily on 2D X-ray imagery. In this paper, we aim to evaluate the possibility of extending the automatic prohibited item detection from 2D X-ray imagery to volumetric 3D CT baggage security screening imagery. To these ends, we take advantage of 3D Convolutional Neural Neworks (CNN) and popular object detection frameworks such as RetinaNet and Faster R-CNN in our work. As the first attempt to use 3D CNN for volumetric 3D CT baggage security screening, we first evaluate different CNN architectures on the classification of isolated prohibited item volumes and compare against traditional methods which use hand-crafted features. Subsequently, we evaluate object detection performance of different architectures on volumetric 3D CT baggage images. The results of our experiments on Bottle and Handgun datasets demonstrate that 3D CNN models can achieve comparable performance (98% true positive rate and 1.5% false positive rate) to traditional methods but require significantly less time for inference (0.014s per volume). Furthermore, the extended 3D object detection models achieve promising performance in detecting prohibited items within volumetric 3D CT baggage imagery with 76% mAP for bottles and 88% mAP for handguns, which shows both the challenge and promise of such threat detection within 3D CT X-ray security imagery.



### MCFlow: Monte Carlo Flow Models for Data Imputation
- **Arxiv ID**: http://arxiv.org/abs/2003.12628v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.12628v1)
- **Published**: 2020-03-27 20:33:52+00:00
- **Updated**: 2020-03-27 20:33:52+00:00
- **Authors**: Trevor W. Richardson, Wencheng Wu, Lei Lin, Beilei Xu, Edgar A. Bernal
- **Comment**: None
- **Journal**: 2020 Computer Vision and Pattern Recognition (CVPR)
- **Summary**: We consider the topic of data imputation, a foundational task in machine learning that addresses issues with missing data. To that end, we propose MCFlow, a deep framework for imputation that leverages normalizing flow generative models and Monte Carlo sampling. We address the causality dilemma that arises when training models with incomplete data by introducing an iterative learning scheme which alternately updates the density estimate and the values of the missing entries in the training data. We provide extensive empirical validation of the effectiveness of the proposed method on standard multivariate and image datasets, and benchmark its performance against state-of-the-art alternatives. We demonstrate that MCFlow is superior to competing methods in terms of the quality of the imputed data, as well as with regards to its ability to preserve the semantic structure of the data.



### Detection and Description of Change in Visual Streams
- **Arxiv ID**: http://arxiv.org/abs/2003.12633v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.12633v2)
- **Published**: 2020-03-27 20:49:38+00:00
- **Updated**: 2020-04-09 20:32:19+00:00
- **Authors**: Davis Gilton, Ruotian Luo, Rebecca Willett, Greg Shakhnarovich
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a framework for the analysis of changes in visual streams: ordered sequences of images, possibly separated by significant time gaps. We propose a new approach to incorporating unlabeled data into training to generate natural language descriptions of change. We also develop a framework for estimating the time of change in visual stream. We use learned representations for change evidence and consistency of perceived change, and combine these in a regularized graph cut based change detector. Experimental evaluation on visual stream datasets, which we release as part of our contribution, shows that representation learning driven by natural language descriptions significantly improves change detection accuracy, compared to methods that do not rely on language.



### Combining Visible and Infrared Spectrum Imagery using Machine Learning for Small Unmanned Aerial System Detection
- **Arxiv ID**: http://arxiv.org/abs/2003.12638v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.12638v2)
- **Published**: 2020-03-27 21:06:14+00:00
- **Updated**: 2020-04-02 19:08:02+00:00
- **Authors**: Vinicius G. Goecks, Grayson Woods, John Valasek
- **Comment**: Project page: https://sites.google.com/view/tamudrone-spie2020/
- **Journal**: None
- **Summary**: Advances in machine learning and deep neural networks for object detection, coupled with lower cost and power requirements of cameras, led to promising vision-based solutions for sUAS detection. However, solely relying on the visible spectrum has previously led to reliability issues in low contrast scenarios such as sUAS flying below the treeline and against bright sources of light. Alternatively, due to the relatively high heat signatures emitted from sUAS during flight, a long-wave infrared (LWIR) sensor is able to produce images that clearly contrast the sUAS from its background. However, compared to widely available visible spectrum sensors, LWIR sensors have lower resolution and may produce more false positives when exposed to birds or other heat sources. This research work proposes combining the advantages of the LWIR and visible spectrum sensors using machine learning for vision-based detection of sUAS. Utilizing the heightened background contrast from the LWIR sensor combined and synchronized with the relatively increased resolution of the visible spectrum sensor, a deep learning model was trained to detect the sUAS through previously difficult environments. More specifically, the approach demonstrated effective detection of multiple sUAS flying above and below the treeline, in the presence of heat sources, and glare from the sun. Our approach achieved a detection rate of 71.2 +- 8.3%, improving by 69% when compared to LWIR and by 30.4% when visible spectrum alone, and achieved false alarm rate of 2.7 +- 2.6%, decreasing by 74.1% and by 47.1% when compared to LWIR and visible spectrum alone, respectively, on average, for single and multiple drone scenarios, controlled for the same confidence metric of the machine learning object detector of at least 50%. Videos of the solution's performance can be seen at https://sites.google.com/view/tamudrone-spie2020/.



### Deep 3D Capture: Geometry and Reflectance from Sparse Multi-View Images
- **Arxiv ID**: http://arxiv.org/abs/2003.12642v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2003.12642v2)
- **Published**: 2020-03-27 21:28:54+00:00
- **Updated**: 2020-07-04 07:48:28+00:00
- **Authors**: Sai Bi, Zexiang Xu, Kalyan Sunkavalli, David Kriegman, Ravi Ramamoorthi
- **Comment**: Accepted to CVPR 2020
- **Journal**: None
- **Summary**: We introduce a novel learning-based method to reconstruct the high-quality geometry and complex, spatially-varying BRDF of an arbitrary object from a sparse set of only six images captured by wide-baseline cameras under collocated point lighting. We first estimate per-view depth maps using a deep multi-view stereo network; these depth maps are used to coarsely align the different views. We propose a novel multi-view reflectance estimation network architecture that is trained to pool features from these coarsely aligned images and predict per-view spatially-varying diffuse albedo, surface normals, specular roughness and specular albedo. We do this by jointly optimizing the latent space of our multi-view reflectance network to minimize the photometric error between images rendered with our predictions and the input images. While previous state-of-the-art methods fail on such sparse acquisition setups, we demonstrate, via extensive experiments on synthetic and real data, that our method produces high-quality reconstructions that can be used to render photorealistic images.



### Designing Color Filters that Make Cameras MoreColorimetric
- **Arxiv ID**: http://arxiv.org/abs/2003.12645v1
- **DOI**: 10.1109/TIP.2020.3038523
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.12645v1)
- **Published**: 2020-03-27 21:41:30+00:00
- **Updated**: 2020-03-27 21:41:30+00:00
- **Authors**: Graham D. Finlayson, Yuteng Zhu
- **Comment**: 13 pages, 5 figures, 3 algorithms, journal
- **Journal**: None
- **Summary**: When we place a colored filter in front of a camera the effective camera response functions are equal to the given camera spectral sensitivities multiplied by the filter spectral transmittance. In this paper, we solve for the filter which returns the modified sensitivities as close to being a linear transformation from the color matching functions of human visual system as possible. When this linearity condition - sometimes called the Luther condition - is approximately met, the `camera+filter' system can be used for accurate color measurement. Then, we reformulate our filter design optimisation for making the sensor responses as close to the CIEXYZ tristimulus values as possible given the knowledge of real measured surfaces and illuminants spectra data. This data-driven method in turn is extended to incorporate constraints on the filter (smoothness and bounded transmission). Also, because how the optimisation is initialised is shown to impact on the performance of the solved-for filters, a multi-initialisation optimisation is developed.   Experiments demonstrate that, by taking pictures through our optimised color filters we can make cameras significantly more colorimetric.



### Deep CG2Real: Synthetic-to-Real Translation via Image Disentanglement
- **Arxiv ID**: http://arxiv.org/abs/2003.12649v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.12649v1)
- **Published**: 2020-03-27 21:45:41+00:00
- **Updated**: 2020-03-27 21:45:41+00:00
- **Authors**: Sai Bi, Kalyan Sunkavalli, Federico Perazzi, Eli Shechtman, Vladimir Kim, Ravi Ramamoorthi
- **Comment**: Accepted to ICCV 2019
- **Journal**: None
- **Summary**: We present a method to improve the visual realism of low-quality, synthetic images, e.g. OpenGL renderings. Training an unpaired synthetic-to-real translation network in image space is severely under-constrained and produces visible artifacts. Instead, we propose a semi-supervised approach that operates on the disentangled shading and albedo layers of the image. Our two-stage pipeline first learns to predict accurate shading in a supervised fashion using physically-based renderings as targets, and further increases the realism of the textures and shading with an improved CycleGAN network. Extensive evaluations on the SUNCG indoor scene dataset demonstrate that our approach yields more realistic images compared to other state-of-the-art approaches. Furthermore, networks trained on our generated "real" images predict more accurate depth and normals than domain adaptation approaches, suggesting that improving the visual realism of the images can be more effective than imposing task-specific losses.



### Automatic Generation of Chinese Handwriting via Fonts Style Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2004.03339v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.03339v1)
- **Published**: 2020-03-27 23:34:01+00:00
- **Updated**: 2020-03-27 23:34:01+00:00
- **Authors**: Fenxi Xiao, Bo Huang, Xia Wu
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose and end-to-end deep Chinese font generation system. This system can generate new style fonts by interpolation of latent style-related embeding variables that could achieve smooth transition between different style. Our method is simpler and more effective than other methods, which will help to improve the font design efficiency



