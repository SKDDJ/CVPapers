# Arxiv Papers in cs.CV on 2020-03-30
### Space-Time-Aware Multi-Resolution Video Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2003.13170v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.13170v1)
- **Published**: 2020-03-30 00:33:17+00:00
- **Updated**: 2020-03-30 00:33:17+00:00
- **Authors**: Muhammad Haris, Greg Shakhnarovich, Norimichi Ukita
- **Comment**: To appear in CVPR2020
- **Journal**: None
- **Summary**: We consider the problem of space-time super-resolution (ST-SR): increasing spatial resolution of video frames and simultaneously interpolating frames to increase the frame rate. Modern approaches handle these axes one at a time. In contrast, our proposed model called STARnet super-resolves jointly in space and time. This allows us to leverage mutually informative relationships between time and space: higher resolution can provide more detailed information about motion, and higher frame-rate can provide better pixel alignment. The components of our model that generate latent low- and high-resolution representations during ST-SR can be used to finetune a specialized mechanism for just spatial or just temporal super-resolution. Experimental results demonstrate that STARnet improves the performances of space-time, spatial, and temporal video super-resolution by substantial margins on publicly available datasets.



### Gradually Vanishing Bridge for Adversarial Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2003.13183v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.13183v1)
- **Published**: 2020-03-30 01:36:13+00:00
- **Updated**: 2020-03-30 01:36:13+00:00
- **Authors**: Shuhao Cui, Shuhui Wang, Junbao Zhuo, Chi Su, Qingming Huang, Qi Tian
- **Comment**: CVPR2020
- **Journal**: None
- **Summary**: In unsupervised domain adaptation, rich domain-specific characteristics bring great challenge to learn domain-invariant representations. However, domain discrepancy is considered to be directly minimized in existing solutions, which is difficult to achieve in practice. Some methods alleviate the difficulty by explicitly modeling domain-invariant and domain-specific parts in the representations, but the adverse influence of the explicit construction lies in the residual domain-specific characteristics in the constructed domain-invariant representations. In this paper, we equip adversarial domain adaptation with Gradually Vanishing Bridge (GVB) mechanism on both generator and discriminator. On the generator, GVB could not only reduce the overall transfer difficulty, but also reduce the influence of the residual domain-specific characteristics in domain-invariant representations. On the discriminator, GVB contributes to enhance the discriminating ability, and balance the adversarial training process. Experiments on three challenging datasets show that our GVB methods outperform strong competitors, and cooperate well with other adversarial methods. The code is available at https://github.com/cuishuhao/GVB.



### Incremental Learning In Online Scenario
- **Arxiv ID**: http://arxiv.org/abs/2003.13191v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.13191v2)
- **Published**: 2020-03-30 02:24:26+00:00
- **Updated**: 2021-04-19 00:50:31+00:00
- **Authors**: Jiangpeng He, Runyu Mao, Zeman Shao, Fengqing Zhu
- **Comment**: Accepted paper at CVPR 2020
- **Journal**: None
- **Summary**: Modern deep learning approaches have achieved great success in many vision applications by training a model using all available task-specific data. However, there are two major obstacles making it challenging to implement for real life applications: (1) Learning new classes makes the trained model quickly forget old classes knowledge, which is referred to as catastrophic forgetting. (2) As new observations of old classes come sequentially over time, the distribution may change in unforeseen way, making the performance degrade dramatically on future data, which is referred to as concept drift. Current state-of-the-art incremental learning methods require a long time to train the model whenever new classes are added and none of them takes into consideration the new observations of old classes. In this paper, we propose an incremental learning framework that can work in the challenging online learning scenario and handle both new classes data and new observations of old classes. We address problem (1) in online mode by introducing a modified cross-distillation loss together with a two-step learning technique. Our method outperforms the results obtained from current state-of-the-art offline incremental learning methods on the CIFAR-100 and ImageNet-1000 (ILSVRC 2012) datasets under the same experiment protocol but in online scenario. We also provide a simple yet effective method to mitigate problem (2) by updating exemplar set using the feature of each new observation of old classes and demonstrate a real life application of online food image classification based on our complete framework using the Food-101 dataset.



### Adversarial Feature Hallucination Networks for Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2003.13193v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.13193v2)
- **Published**: 2020-03-30 02:43:16+00:00
- **Updated**: 2020-10-27 19:16:50+00:00
- **Authors**: Kai Li, Yulun Zhang, Kunpeng Li, Yun Fu
- **Comment**: Correct some typos
- **Journal**: None
- **Summary**: The recent flourish of deep learning in various tasks is largely accredited to the rich and accessible labeled data. Nonetheless, massive supervision remains a luxury for many real applications, boosting great interest in label-scarce techniques such as few-shot learning (FSL), which aims to learn concept of new classes with a few labeled samples. A natural approach to FSL is data augmentation and many recent works have proved the feasibility by proposing various data synthesis models. However, these models fail to well secure the discriminability and diversity of the synthesized data and thus often produce undesirable results. In this paper, we propose Adversarial Feature Hallucination Networks (AFHN) which is based on conditional Wasserstein Generative Adversarial networks (cWGAN) and hallucinates diverse and discriminative features conditioned on the few labeled samples. Two novel regularizers, i.e., the classification regularizer and the anti-collapse regularizer, are incorporated into AFHN to encourage discriminability and diversity of the synthesized features, respectively. Ablation study verifies the effectiveness of the proposed cWGAN based feature hallucination framework and the proposed regularizers. Comparative results on three common benchmark datasets substantiate the superiority of AFHN to existing data augmentation based FSL approaches and other state-of-the-art ones.



### Density-Aware Graph for Deep Semi-Supervised Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/2003.13194v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.13194v1)
- **Published**: 2020-03-30 02:52:40+00:00
- **Updated**: 2020-03-30 02:52:40+00:00
- **Authors**: Suichan Li, Bin Liu, Dongdong Chen, Qi Chu, Lu Yuan, Nenghai Yu
- **Comment**: Accepted by CVPR2020
- **Journal**: None
- **Summary**: Semi-supervised learning (SSL) has been extensively studied to improve the generalization ability of deep neural networks for visual recognition. To involve the unlabelled data, most existing SSL methods are based on common density-based cluster assumption: samples lying in the same high-density region are likely to belong to the same class, including the methods performing consistency regularization or generating pseudo-labels for the unlabelled images. Despite their impressive performance, we argue three limitations exist: 1) Though the density information is demonstrated to be an important clue, they all use it in an implicit way and have not exploited it in depth. 2) For feature learning, they often learn the feature embedding based on the single data sample and ignore the neighborhood information. 3) For label-propagation based pseudo-label generation, it is often done offline and difficult to be end-to-end trained with feature learning. Motivated by these limitations, this paper proposes to solve the SSL problem by building a novel density-aware graph, based on which the neighborhood information can be easily leveraged and the feature learning and label propagation can also be trained in an end-to-end way. Specifically, we first propose a new Density-aware Neighborhood Aggregation(DNA) module to learn more discriminative features by incorporating the neighborhood information in a density-aware manner. Then a novel Density-ascending Path based Label Propagation(DPLP) module is proposed to generate the pseudo-labels for unlabeled samples more efficiently according to the feature distribution characterized by density. Finally, the DNA module and DPLP module evolve and improve each other end-to-end.



### Cross-Domain Document Object Detection: Benchmark Suite and Method
- **Arxiv ID**: http://arxiv.org/abs/2003.13197v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.13197v1)
- **Published**: 2020-03-30 03:04:51+00:00
- **Updated**: 2020-03-30 03:04:51+00:00
- **Authors**: Kai Li, Curtis Wigington, Chris Tensmeyer, Handong Zhao, Nikolaos Barmpalios, Vlad I. Morariu, Varun Manjunatha, Tong Sun, Yun Fu
- **Comment**: To appear in CVPR 2020
- **Journal**: None
- **Summary**: Decomposing images of document pages into high-level semantic regions (e.g., figures, tables, paragraphs), document object detection (DOD) is fundamental for downstream tasks like intelligent document editing and understanding. DOD remains a challenging problem as document objects vary significantly in layout, size, aspect ratio, texture, etc. An additional challenge arises in practice because large labeled training datasets are only available for domains that differ from the target domain. We investigate cross-domain DOD, where the goal is to learn a detector for the target domain using labeled data from the source domain and only unlabeled data from the target domain. Documents from the two domains may vary significantly in layout, language, and genre. We establish a benchmark suite consisting of different types of PDF document datasets that can be utilized for cross-domain DOD model training and evaluation. For each dataset, we provide the page images, bounding box annotations, PDF files, and the rendering layers extracted from the PDF files. Moreover, we propose a novel cross-domain DOD model which builds upon the standard detection model and addresses domain shifts by incorporating three novel alignment modules: Feature Pyramid Alignment (FPA) module, Region Alignment (RA) module and Rendering Layer alignment (RLA) module. Extensive experiments on the benchmark suite substantiate the efficacy of the three proposed modules and the proposed method significantly outperforms the baseline methods. The project page is at \url{https://github.com/kailigo/cddod}.



### InterBERT: Vision-and-Language Interaction for Multi-modal Pretraining
- **Arxiv ID**: http://arxiv.org/abs/2003.13198v4
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.13198v4)
- **Published**: 2020-03-30 03:13:22+00:00
- **Updated**: 2021-04-22 11:20:26+00:00
- **Authors**: Junyang Lin, An Yang, Yichang Zhang, Jie Liu, Jingren Zhou, Hongxia Yang
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: Multi-modal pretraining for learning high-level multi-modal representation is a further step towards deep learning and artificial intelligence. In this work, we propose a novel model, namely InterBERT (BERT for Interaction), which is the first model of our series of multimodal pretraining methods M6 (MultiModality-to-MultiModality Multitask Mega-transformer). The model owns strong capability of modeling interaction between the information flows of different modalities. The single-stream interaction module is capable of effectively processing information of multiple modalilties, and the two-stream module on top preserves the independence of each modality to avoid performance downgrade in single-modal tasks. We pretrain the model with three pretraining tasks, including masked segment modeling (MSM), masked region modeling (MRM) and image-text matching (ITM); and finetune the model on a series of vision-and-language downstream tasks. Experimental results demonstrate that InterBERT outperforms a series of strong baselines, including the most recent multi-modal pretraining methods, and the analysis shows that MSM and MRM are effective for pretraining and our method can achieve performances comparable to BERT in single-modal tasks. Besides, we propose a large-scale dataset for multi-modal pretraining in Chinese, and we develop the Chinese InterBERT which is the first Chinese multi-modal pretrained model. We pretrain the Chinese InterBERT on our proposed dataset of 3.1M image-text pairs from the mobile Taobao, the largest Chinese e-commerce platform. We finetune the model for text-based image retrieval, and recently we deployed the model online for topic-based recommendation.



### Learning to Learn Single Domain Generalization
- **Arxiv ID**: http://arxiv.org/abs/2003.13216v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.13216v1)
- **Published**: 2020-03-30 04:39:53+00:00
- **Updated**: 2020-03-30 04:39:53+00:00
- **Authors**: Fengchun Qiao, Long Zhao, Xi Peng
- **Comment**: In CVPR 2020 (13 pages including supplementary material). The source
  code and pre-trained models are publicly available at:
  https://github.com/joffery/M-ADA
- **Journal**: None
- **Summary**: We are concerned with a worst-case scenario in model generalization, in the sense that a model aims to perform well on many unseen domains while there is only one single domain available for training. We propose a new method named adversarial domain augmentation to solve this Out-of-Distribution (OOD) generalization problem. The key idea is to leverage adversarial training to create "fictitious" yet "challenging" populations, from which a model can learn to generalize with theoretical guarantees. To facilitate fast and desirable domain augmentation, we cast the model training in a meta-learning scheme and use a Wasserstein Auto-Encoder (WAE) to relax the widely used worst-case constraint. Detailed theoretical analysis is provided to testify our formulation, while extensive experiments on multiple benchmark datasets indicate its superior performance in tackling single domain generalization.



### Learning Memory-guided Normality for Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2003.13228v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.13228v1)
- **Published**: 2020-03-30 05:30:09+00:00
- **Updated**: 2020-03-30 05:30:09+00:00
- **Authors**: Hyunjong Park, Jongyoun Noh, Bumsub Ham
- **Comment**: Accepted to CVPR 2020
- **Journal**: None
- **Summary**: We address the problem of anomaly detection, that is, detecting anomalous events in a video sequence. Anomaly detection methods based on convolutional neural networks (CNNs) typically leverage proxy tasks, such as reconstructing input video frames, to learn models describing normality without seeing anomalous samples at training time, and quantify the extent of abnormalities using the reconstruction error at test time. The main drawbacks of these approaches are that they do not consider the diversity of normal patterns explicitly, and the powerful representation capacity of CNNs allows to reconstruct abnormal video frames. To address this problem, we present an unsupervised learning approach to anomaly detection that considers the diversity of normal patterns explicitly, while lessening the representation capacity of CNNs. To this end, we propose to use a memory module with a new update scheme where items in the memory record prototypical patterns of normal data. We also present novel feature compactness and separateness losses to train the memory, boosting the discriminative power of both memory items and deeply learned features from normal data. Experimental results on standard benchmarks demonstrate the effectiveness and efficiency of our approach, which outperforms the state of the art.



### FusedProp: Towards Efficient Training of Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2004.03335v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.03335v1)
- **Published**: 2020-03-30 06:46:29+00:00
- **Updated**: 2020-03-30 06:46:29+00:00
- **Authors**: Zachary Polizzi, Chuan-Yung Tsai
- **Comment**: source code available at https://github.com/zplizzi/fusedprop
- **Journal**: None
- **Summary**: Generative adversarial networks (GANs) are capable of generating strikingly realistic samples but state-of-the-art GANs can be extremely computationally expensive to train. In this paper, we propose the fused propagation (FusedProp) algorithm which can be used to efficiently train the discriminator and the generator of common GANs simultaneously using only one forward and one backward propagation. We show that FusedProp achieves 1.49 times the training speed compared to the conventional training of GANs, although further studies are required to improve its stability. By reporting our preliminary results and open-sourcing our implementation, we hope to accelerate future research on the training of GANs.



### MetaFuse: A Pre-trained Fusion Model for Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2003.13239v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.13239v1)
- **Published**: 2020-03-30 06:54:50+00:00
- **Updated**: 2020-03-30 06:54:50+00:00
- **Authors**: Rongchang Xie, Chunyu Wang, Yizhou Wang
- **Comment**: Accepted to CVPR2020
- **Journal**: None
- **Summary**: Cross view feature fusion is the key to address the occlusion problem in human pose estimation. The current fusion methods need to train a separate model for every pair of cameras making them difficult to scale. In this work, we introduce MetaFuse, a pre-trained fusion model learned from a large number of cameras in the Panoptic dataset. The model can be efficiently adapted or finetuned for a new pair of cameras using a small number of labeled images. The strong adaptation power of MetaFuse is due in large part to the proposed factorization of the original fusion model into two parts (1) a generic fusion model shared by all cameras, and (2) lightweight camera-dependent transformations. Furthermore, the generic model is learned from many cameras by a meta-learning style algorithm to maximize its adaptation capability to various camera poses. We observe in experiments that MetaFuse finetuned on the public datasets outperforms the state-of-the-arts by a large margin which validates its value in practice.



### Physical Model Guided Deep Image Deraining
- **Arxiv ID**: http://arxiv.org/abs/2003.13242v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.13242v1)
- **Published**: 2020-03-30 07:08:13+00:00
- **Updated**: 2020-03-30 07:08:13+00:00
- **Authors**: Honghe Zhu, Cong Wang, Yajie Zhang, Zhixun Su, Guohui Zhao
- **Comment**: IEEE ICME2020
- **Journal**: None
- **Summary**: Single image deraining is an urgent task because the degraded rainy image makes many computer vision systems fail to work, such as video surveillance and autonomous driving.   So, deraining becomes important and an effective deraining algorithm is needed.   In this paper, we propose a novel network based on physical model guided learning for single image deraining, which consists of three sub-networks: rain streaks network, rain-free network, and guide-learning network.   The concatenation of rain streaks and rain-free image that are estimated by rain streaks network, rain-free network, respectively, is input to the guide-learning network to guide further learning and the direct sum of the two estimated images is constrained with the input rainy image based on the physical model of rainy image.   Moreover, we further develop the Multi-Scale Residual Block (MSRB) to better utilize multi-scale information and it is proved to boost the deraining performance.   Quantitative and qualitative experimental results demonstrate that the proposed method outperforms the state-of-the-art deraining methods.   The source code will be available at \url{https://supercong94.wixsite.com/supercong94}.



### Memory Aggregation Networks for Efficient Interactive Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2003.13246v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.13246v1)
- **Published**: 2020-03-30 07:25:26+00:00
- **Updated**: 2020-03-30 07:25:26+00:00
- **Authors**: Jiaxu Miao, Yunchao Wei, Yi Yang
- **Comment**: Accepted to CVPR 2020. 10 pages, 9 figures
- **Journal**: None
- **Summary**: Interactive video object segmentation (iVOS) aims at efficiently harvesting high-quality segmentation masks of the target object in a video with user interactions. Most previous state-of-the-arts tackle the iVOS with two independent networks for conducting user interaction and temporal propagation, respectively, leading to inefficiencies during the inference stage. In this work, we propose a unified framework, named Memory Aggregation Networks (MA-Net), to address the challenging iVOS in a more efficient way. Our MA-Net integrates the interaction and the propagation operations into a single network, which significantly promotes the efficiency of iVOS in the scheme of multi-round interactions. More importantly, we propose a simple yet effective memory aggregation mechanism to record the informative knowledge from the previous interaction rounds, improving the robustness in discovering challenging objects of interest greatly. We conduct extensive experiments on the validation set of DAVIS Challenge 2018 benchmark. In particular, our MA-Net achieves the J@60 score of 76.1% without any bells and whistles, outperforming the state-of-the-arts with more than 2.7%.



### Optimizing Geometry Compression using Quantum Annealing
- **Arxiv ID**: http://arxiv.org/abs/2003.13253v1
- **DOI**: 10.1109/GLOCOMW.2018.8644358
- **Categories**: **quant-ph**, cs.CG, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.13253v1)
- **Published**: 2020-03-30 07:56:34+00:00
- **Updated**: 2020-03-30 07:56:34+00:00
- **Authors**: Sebastian Feld, Markus Friedrich, Claudia Linnhoff-Popien
- **Comment**: 6 pages, 3 figures
- **Journal**: 2018 IEEE Globecom Workshops (GC Wkshps), Abu Dhabi, United Arab
  Emirates, 2018, pp. 1-6
- **Summary**: The compression of geometry data is an important aspect of bandwidth-efficient data transfer for distributed 3d computer vision applications. We propose a quantum-enabled lossy 3d point cloud compression pipeline based on the constructive solid geometry (CSG) model representation. Key parts of the pipeline are mapped to NP-complete problems for which an efficient Ising formulation suitable for the execution on a Quantum Annealer exists. We describe existing Ising formulations for the maximum clique search problem and the smallest exact cover problem, both of which are important building blocks of the proposed compression pipeline. Additionally, we discuss the properties of the overall pipeline regarding result optimality and described Ising formulations.



### TapLab: A Fast Framework for Semantic Video Segmentation Tapping into Compressed-Domain Knowledge
- **Arxiv ID**: http://arxiv.org/abs/2003.13260v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.13260v3)
- **Published**: 2020-03-30 08:13:47+00:00
- **Updated**: 2020-08-18 06:52:41+00:00
- **Authors**: Junyi Feng, Songyuan Li, Xi Li, Fei Wu, Qi Tian, Ming-Hsuan Yang, Haibin Ling
- **Comment**: Accepted to TPAMI
- **Journal**: None
- **Summary**: Real-time semantic video segmentation is a challenging task due to the strict requirements of inference speed. Recent approaches mainly devote great efforts to reducing the model size for high efficiency. In this paper, we rethink this problem from a different viewpoint: using knowledge contained in compressed videos. We propose a simple and effective framework, dubbed TapLab, to tap into resources from the compressed domain. Specifically, we design a fast feature warping module using motion vectors for acceleration. To reduce the noise introduced by motion vectors, we design a residual-guided correction module and a residual-guided frame selection module using residuals. TapLab significantly reduces redundant computations of the state-of-the-art fast semantic image segmentation models, running 3 to 10 times faster with controllable accuracy degradation. The experimental results show that TapLab achieves 70.6% mIoU on the Cityscapes dataset at 99.8 FPS with a single GPU card for the 1024x2048 videos. A high-speed version even reaches the speed of 160+ FPS. Codes will be available soon at https://github.com/Sixkplus/TapLab.



### Domain-aware Visual Bias Eliminating for Generalized Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2003.13261v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.13261v2)
- **Published**: 2020-03-30 08:17:04+00:00
- **Updated**: 2020-04-10 07:12:53+00:00
- **Authors**: Shaobo Min, Hantao Yao, Hongtao Xie, Chaoqun Wang, Zheng-Jun Zha, Yongdong Zhang
- **Comment**: Accepted by CVPR2020
- **Journal**: None
- **Summary**: Recent methods focus on learning a unified semantic-aligned visual representation to transfer knowledge between two domains, while ignoring the effect of semantic-free visual representation in alleviating the biased recognition problem. In this paper, we propose a novel Domain-aware Visual Bias Eliminating (DVBE) network that constructs two complementary visual representations, i.e., semantic-free and semantic-aligned, to treat seen and unseen domains separately. Specifically, we explore cross-attentive second-order visual statistics to compact the semantic-free representation, and design an adaptive margin Softmax to maximize inter-class divergences. Thus, the semantic-free representation becomes discriminative enough to not only predict seen class accurately but also filter out unseen images, i.e., domain detection, based on the predicted class entropy. For unseen images, we automatically search an optimal semantic-visual alignment architecture, rather than manual designs, to predict unseen classes. With accurate domain detection, the biased recognition problem towards the seen domain is significantly reduced. Experiments on five benchmarks for classification and segmentation show that DVBE outperforms existing methods by averaged 5.7% improvement.



### Towards Palmprint Verification On Smartphones
- **Arxiv ID**: http://arxiv.org/abs/2003.13266v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.13266v2)
- **Published**: 2020-03-30 08:31:03+00:00
- **Updated**: 2020-08-03 04:08:04+00:00
- **Authors**: Yingyi Zhang, Lin Zhang, Ruixin Zhang, Shaoxin Li, Jilin Li, Feiyue Huang
- **Comment**: None
- **Journal**: None
- **Summary**: With the rapid development of mobile devices, smartphones have gradually become an indispensable part of people's lives. Meanwhile, biometric authentication has been corroborated to be an effective method for establishing a person's identity with high confidence. Hence, recently, biometric technologies for smartphones have also become increasingly sophisticated and popular. But it is noteworthy that the application potential of palmprints for smartphones is seriously underestimated. Studies in the past two decades have shown that palmprints have outstanding merits in uniqueness and permanence, and have high user acceptance. However, currently, studies specializing in palmprint verification for smartphones are still quite sporadic, especially when compared to face- or fingerprint-oriented ones. In this paper, aiming to fill the aforementioned research gap, we conducted a thorough study of palmprint verification on smartphones and our contributions are twofold. First, to facilitate the study of palmprint verification on smartphones, we established an annotated palmprint dataset named MPD, which was collected by multi-brand smartphones in two separate sessions with various backgrounds and illumination conditions. As the largest dataset in this field, MPD contains 16,000 palm images collected from 200 subjects. Second, we built a DCNN-based palmprint verification system named DeepMPV+ for smartphones. In DeepMPV+, two key steps, ROI extraction and ROI matching, are both formulated as learning problems and then solved naturally by modern DCNN models. The efficiency and efficacy of DeepMPV+ have been corroborated by extensive experiments. To make our results fully reproducible, the labeled dataset and the relevant source codes have been made publicly available at https://cslinzhang.github.io/MobilePalmPrint/.



### Architecture Disentanglement for Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2003.13268v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.13268v2)
- **Published**: 2020-03-30 08:34:33+00:00
- **Updated**: 2021-03-24 03:03:54+00:00
- **Authors**: Jie Hu, Liujuan Cao, Qixiang Ye, Tong Tong, ShengChuan Zhang, Ke Li, Feiyue Huang, Rongrong Ji, Ling Shao
- **Comment**: None
- **Journal**: None
- **Summary**: Understanding the inner workings of deep neural networks (DNNs) is essential to provide trustworthy artificial intelligence techniques for practical applications. Existing studies typically involve linking semantic concepts to units or layers of DNNs, but fail to explain the inference process. In this paper, we introduce neural architecture disentanglement (NAD) to fill the gap. Specifically, NAD learns to disentangle a pre-trained DNN into sub-architectures according to independent tasks, forming information flows that describe the inference processes. We investigate whether, where, and how the disentanglement occurs through experiments conducted with handcrafted and automatically-searched network architectures, on both object-based and scene-based datasets. Based on the experimental results, we present three new findings that provide fresh insights into the inner logic of DNNs. First, DNNs can be divided into sub-architectures for independent tasks. Second, deeper layers do not always correspond to higher semantics. Third, the connection type in a DNN affects how the information flows across layers, leading to different disentanglement behaviors. With NAD, we further explain why DNNs sometimes give wrong predictions. Experimental results show that misclassified images have a high probability of being assigned to task sub-architectures similar to the correct ones. Code will be available at: https://github.com/hujiecpp/NAD.



### Multi-Objective Matrix Normalization for Fine-grained Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/2003.13272v2
- **DOI**: 10.1109/TIP.2020.2977457
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.13272v2)
- **Published**: 2020-03-30 08:40:35+00:00
- **Updated**: 2020-04-10 07:33:42+00:00
- **Authors**: Shaobo Min, Hantao Yao, Hongtao Xie, Zheng-Jun Zha, Yongdong Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Bilinear pooling achieves great success in fine-grained visual recognition (FGVC). Recent methods have shown that the matrix power normalization can stabilize the second-order information in bilinear features, but some problems, e.g., redundant information and over-fitting, remain to be resolved. In this paper, we propose an efficient Multi-Objective Matrix Normalization (MOMN) method that can simultaneously normalize a bilinear representation in terms of square-root, low-rank, and sparsity. These three regularizers can not only stabilize the second-order information, but also compact the bilinear features and promote model generalization. In MOMN, a core challenge is how to jointly optimize three non-smooth regularizers of different convex properties. To this end, MOMN first formulates them into an augmented Lagrange formula with approximated regularizer constraints. Then, auxiliary variables are introduced to relax different constraints, which allow each regularizer to be solved alternately. Finally, several updating strategies based on gradient descent are designed to obtain consistent convergence and efficient implementation. Consequently, MOMN is implemented with only matrix multiplication, which is well-compatible with GPU acceleration, and the normalized bilinear features are stabilized and discriminative. Experiments on five public benchmarks for FGVC demonstrate that the proposed MOMN is superior to existing normalization-based methods in terms of both accuracy and efficiency. The code is available: https://github.com/mboboGO/MOMN.



### Adversarial Domain Adaptation with Prototype-Based Normalized Output Conditioner
- **Arxiv ID**: http://arxiv.org/abs/2003.13274v4
- **DOI**: 10.1109/TIP.2021.3124674
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.13274v4)
- **Published**: 2020-03-30 08:50:32+00:00
- **Updated**: 2021-12-18 07:30:37+00:00
- **Authors**: Dapeng Hu, Jian Liang, Qibin Hou, Hanshu Yan, Yunpeng Chen
- **Comment**: Published at IEEE transactions on image processing 2021
- **Journal**: None
- **Summary**: In this work, we attempt to address unsupervised domain adaptation by devising simple and compact conditional domain adversarial training methods. We first revisit the simple concatenation conditioning strategy where features are concatenated with output predictions as the input of the discriminator. We find the concatenation strategy suffers from the weak conditioning strength. We further demonstrate that enlarging the norm of concatenated predictions can effectively energize the conditional domain alignment. Thus we improve concatenation conditioning by normalizing the output predictions to have the same norm of features, and term the derived method as Normalized OutpUt coNditioner~(NOUN). However, conditioning on raw output predictions for domain alignment, NOUN suffers from inaccurate predictions of the target domain. To this end, we propose to condition the cross-domain feature alignment in the prototype space rather than in the output space. Combining the novel prototype-based conditioning with NOUN, we term the enhanced method as PROtotype-based Normalized OutpUt coNditioner~(PRONOUN). Experiments on both object recognition and semantic segmentation show that NOUN can effectively align the multi-modal structures across domains and even outperform state-of-the-art domain adversarial training methods. Together with prototype-based conditioning, PRONOUN further improves the adaptation performance over NOUN on multiple object recognition benchmarks for UDA.



### Unsupervised Model Personalization while Preserving Privacy and Scalability: An Open Problem
- **Arxiv ID**: http://arxiv.org/abs/2003.13296v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2003.13296v1)
- **Published**: 2020-03-30 09:35:12+00:00
- **Updated**: 2020-03-30 09:35:12+00:00
- **Authors**: Matthias De Lange, Xu Jia, Sarah Parisot, Ales Leonardis, Gregory Slabaugh, Tinne Tuytelaars
- **Comment**: CVPR 2020
- **Journal**: Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR), June 2020
- **Summary**: This work investigates the task of unsupervised model personalization, adapted to continually evolving, unlabeled local user images. We consider the practical scenario where a high capacity server interacts with a myriad of resource-limited edge devices, imposing strong requirements on scalability and local data privacy. We aim to address this challenge within the continual learning paradigm and provide a novel Dual User-Adaptation framework (DUA) to explore the problem. This framework flexibly disentangles user-adaptation into model personalization on the server and local data regularization on the user device, with desirable properties regarding scalability and privacy constraints. First, on the server, we introduce incremental learning of task-specific expert models, subsequently aggregated using a concealed unsupervised user prior. Aggregation avoids retraining, whereas the user prior conceals sensitive raw user data, and grants unsupervised adaptation. Second, local user-adaptation incorporates a domain adaptation point of view, adapting regularizing batch normalization parameters to the user data. We explore various empirical user configurations with different priors in categories and a tenfold of transforms for MIT Indoor Scene recognition, and classify numbers in a combined MNIST and SVHN setup. Extensive experiments yield promising results for data-driven local adaptation and elicit user priors for server adaptation to depend on the model rather than user data. Hence, although user-adaptation remains a challenging open problem, the DUA framework formalizes a principled foundation for personalizing both on server and user device, while maintaining privacy and scalability.



### Real-Time Fruit Recognition and Grasping Estimation for Autonomous Apple Harvesting
- **Arxiv ID**: http://arxiv.org/abs/2003.13298v2
- **DOI**: 10.3390/s19204599
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.13298v2)
- **Published**: 2020-03-30 09:37:55+00:00
- **Updated**: 2020-04-05 12:07:21+00:00
- **Authors**: Hanwen Kang, Chao Chen
- **Comment**: None
- **Journal**: None
- **Summary**: In this research, a fully neural network based visual perception framework for autonomous apple harvesting is proposed. The proposed framework includes a multi-function neural network for fruit recognition and a Pointnet grasp estimation to determine the proper grasp pose to guide the robotic execution. Fruit recognition takes raw input of RGB images from the RGB-D camera to perform fruit detection and instance segmentation, and Pointnet grasp estimation take point cloud of each fruit as input and output the prediction of grasp pose for each of fruits. The proposed framework is validated by using RGB-D images collected from laboratory and orchard environments, a robotic grasping test in a controlled environment is also included in the experiments. Experimental shows that the proposed framework can accurately localise and estimate the grasp pose for robotic grasping.



### Active stereo vision three-dimensional reconstruction by RGB dot pattern projection and ray intersection
- **Arxiv ID**: http://arxiv.org/abs/2003.13322v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.13322v2)
- **Published**: 2020-03-30 10:13:28+00:00
- **Updated**: 2020-03-31 01:44:41+00:00
- **Authors**: Yongcan Shuang, Zhenzhou Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Active stereo vision is important in reconstructing objects without obvious textures. However, it is still very challenging to extract and match the projected patterns from two camera views automatically and robustly. In this paper, we propose a new pattern extraction method and a new stereo vision matching method based on our novel structured light pattern. Instead of using the widely used 2D disparity to calculate the depths of the objects, we use the ray intersection to compute the 3D shapes directly. Experimental results showed that the proposed approach could reconstruct the 3D shape of the object significantly more robustly than state of the art methods that include the widely used disparity based active stereo vision method, the time of flight method and the structured light method. In addition, experimental results also showed that the proposed approach could reconstruct the 3D motions of the dynamic shapes robustly.



### PointGMM: a Neural GMM Network for Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2003.13326v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.GR, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.13326v1)
- **Published**: 2020-03-30 10:34:59+00:00
- **Updated**: 2020-03-30 10:34:59+00:00
- **Authors**: Amir Hertz, Rana Hanocka, Raja Giryes, Daniel Cohen-Or
- **Comment**: CVPR 2020 -- final version
- **Journal**: None
- **Summary**: Point clouds are a popular representation for 3D shapes. However, they encode a particular sampling without accounting for shape priors or non-local information. We advocate for the use of a hierarchical Gaussian mixture model (hGMM), which is a compact, adaptive and lightweight representation that probabilistically defines the underlying 3D surface. We present PointGMM, a neural network that learns to generate hGMMs which are characteristic of the shape class, and also coincide with the input point cloud. PointGMM is trained over a collection of shapes to learn a class-specific prior. The hierarchical representation has two main advantages: (i) coarse-to-fine learning, which avoids converging to poor local-minima; and (ii) (an unsupervised) consistent partitioning of the input shape. We show that as a generative model, PointGMM learns a meaningful latent space which enables generating consistent interpolations between existing shapes, as well as synthesizing novel shapes. We also present a novel framework for rigid registration using PointGMM, that learns to disentangle orientation from structure of an input shape.



### Strip Pooling: Rethinking Spatial Pooling for Scene Parsing
- **Arxiv ID**: http://arxiv.org/abs/2003.13328v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.13328v1)
- **Published**: 2020-03-30 10:40:11+00:00
- **Updated**: 2020-03-30 10:40:11+00:00
- **Authors**: Qibin Hou, Li Zhang, Ming-Ming Cheng, Jiashi Feng
- **Comment**: Published as a CVPR2020 paper
- **Journal**: None
- **Summary**: Spatial pooling has been proven highly effective in capturing long-range contextual information for pixel-wise prediction tasks, such as scene parsing. In this paper, beyond conventional spatial pooling that usually has a regular shape of NxN, we rethink the formulation of spatial pooling by introducing a new pooling strategy, called strip pooling, which considers a long but narrow kernel, i.e., 1xN or Nx1. Based on strip pooling, we further investigate spatial pooling architecture design by 1) introducing a new strip pooling module that enables backbone networks to efficiently model long-range dependencies, 2) presenting a novel building block with diverse spatial pooling as a core, and 3) systematically comparing the performance of the proposed strip pooling and conventional spatial pooling techniques. Both novel pooling-based designs are lightweight and can serve as an efficient plug-and-play module in existing scene parsing networks. Extensive experiments on popular benchmarks (e.g., ADE20K and Cityscapes) demonstrate that our simple approach establishes new state-of-the-art results. Code is made available at https://github.com/Andrew-Qibin/SPNet.



### Context Based Emotion Recognition using EMOTIC Dataset
- **Arxiv ID**: http://arxiv.org/abs/2003.13401v1
- **DOI**: 10.1109/TPAMI.2019.2916866
- **Categories**: **cs.CV**, cs.LG, I.4.8; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2003.13401v1)
- **Published**: 2020-03-30 12:38:50+00:00
- **Updated**: 2020-03-30 12:38:50+00:00
- **Authors**: Ronak Kosti, Jose M. Alvarez, Adria Recasens, Agata Lapedriza
- **Comment**: None
- **Journal**: None
- **Summary**: In our everyday lives and social interactions we often try to perceive the emotional states of people. There has been a lot of research in providing machines with a similar capacity of recognizing emotions. From a computer vision perspective, most of the previous efforts have been focusing in analyzing the facial expressions and, in some cases, also the body pose. Some of these methods work remarkably well in specific settings. However, their performance is limited in natural, unconstrained environments. Psychological studies show that the scene context, in addition to facial expression and body pose, provides important information to our perception of people's emotions. However, the processing of the context for automatic emotion recognition has not been explored in depth, partly due to the lack of proper data. In this paper we present EMOTIC, a dataset of images of people in a diverse set of natural situations, annotated with their apparent emotion. The EMOTIC dataset combines two different types of emotion representation: (1) a set of 26 discrete categories, and (2) the continuous dimensions Valence, Arousal, and Dominance. We also present a detailed statistical and algorithmic analysis of the dataset along with annotators' agreement analysis. Using the EMOTIC dataset we train different CNN models for emotion recognition, combining the information of the bounding box containing the person with the contextual information extracted from the scene. Our results show how scene context provides important information to automatically recognize emotional states and motivate further research in this direction. Dataset and code is open-sourced and available at: https://github.com/rkosti/emotic and link for the peer-reviewed published article: https://ieeexplore.ieee.org/document/8713881



### Predicting Semantic Map Representations from Images using Pyramid Occupancy Networks
- **Arxiv ID**: http://arxiv.org/abs/2003.13402v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.13402v1)
- **Published**: 2020-03-30 12:39:44+00:00
- **Updated**: 2020-03-30 12:39:44+00:00
- **Authors**: Thomas Roddick, Roberto Cipolla
- **Comment**: None
- **Journal**: None
- **Summary**: Autonomous vehicles commonly rely on highly detailed birds-eye-view maps of their environment, which capture both static elements of the scene such as road layout as well as dynamic elements such as other cars and pedestrians. Generating these map representations on the fly is a complex multi-stage process which incorporates many important vision-based elements, including ground plane estimation, road segmentation and 3D object detection. In this work we present a simple, unified approach for estimating maps directly from monocular images using a single end-to-end deep learning architecture. For the maps themselves we adopt a semantic Bayesian occupancy grid framework, allowing us to trivially accumulate information over multiple cameras and timesteps. We demonstrate the effectiveness of our approach by evaluating against several challenging baselines on the NuScenes and Argoverse datasets, and show that we are able to achieve a relative improvement of 9.1% and 22.3% respectively compared to the best-performing existing method.



### Same Features, Different Day: Weakly Supervised Feature Learning for Seasonal Invariance
- **Arxiv ID**: http://arxiv.org/abs/2003.13431v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.13431v1)
- **Published**: 2020-03-30 12:56:44+00:00
- **Updated**: 2020-03-30 12:56:44+00:00
- **Authors**: Jaime Spencer, Richard Bowden, Simon Hadfield
- **Comment**: None
- **Journal**: None
- **Summary**: "Like night and day" is a commonly used expression to imply that two things are completely different. Unfortunately, this tends to be the case for current visual feature representations of the same scene across varying seasons or times of day. The aim of this paper is to provide a dense feature representation that can be used to perform localization, sparse matching or image retrieval, regardless of the current seasonal or temporal appearance.   Recently, there have been several proposed methodologies for deep learning dense feature representations. These methods make use of ground truth pixel-wise correspondences between pairs of images and focus on the spatial properties of the features. As such, they don't address temporal or seasonal variation. Furthermore, obtaining the required pixel-wise correspondence data to train in cross-seasonal environments is highly complex in most scenarios.   We propose Deja-Vu, a weakly supervised approach to learning season invariant features that does not require pixel-wise ground truth data. The proposed system only requires coarse labels indicating if two images correspond to the same location or not. From these labels, the network is trained to produce "similar" dense feature maps for corresponding locations despite environmental changes. Code will be made available at: https://github.com/jspenmar/DejaVu_Features



### Computer Aided Detection for Pulmonary Embolism Challenge (CAD-PE)
- **Arxiv ID**: http://arxiv.org/abs/2003.13440v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.13440v1)
- **Published**: 2020-03-30 13:05:07+00:00
- **Updated**: 2020-03-30 13:05:07+00:00
- **Authors**: Germán González, Daniel Jimenez-Carretero, Sara Rodríguez-López, Carlos Cano-Espinosa, Miguel Cazorla, Tanya Agarwal, Vinit Agarwal, Nima Tajbakhsh, Michael B. Gotway, Jianming Liang, Mojtaba Masoudi, Noushin Eftekhari, Mahdi Saadatmand, Hamid-Reza Pourreza, Patricia Fraga-Rivas, Eduardo Fraile, Frank J. Rybicki, Ara Kassarjian, Raúl San José Estépar, Maria J. Ledesma-Carbayo
- **Comment**: 8 pages, 3 figures
- **Journal**: None
- **Summary**: Rationale: Computer aided detection (CAD) algorithms for Pulmonary Embolism (PE) algorithms have been shown to increase radiologists' sensitivity with a small increase in specificity. However, CAD for PE has not been adopted into clinical practice, likely because of the high number of false positives current CAD software produces. Objective: To generate a database of annotated computed tomography pulmonary angiographies, use it to compare the sensitivity and false positive rate of current algorithms and to develop new methods that improve such metrics. Methods: 91 Computed tomography pulmonary angiography scans were annotated by at least one radiologist by segmenting all pulmonary emboli visible on the study. 20 annotated CTPAs were open to the public in the form of a medical image analysis challenge. 20 more were kept for evaluation purposes. 51 were made available post-challenge. 8 submissions, 6 of them novel, were evaluated on the 20 evaluation CTPAs. Performance was measured as per embolus sensitivity vs. false positives per scan curve. Results: The best algorithms achieved a per-embolus sensitivity of 75% at 2 false positives per scan (fps) or of 70% at 1 fps, outperforming the state of the art. Deep learning approaches outperformed traditional machine learning ones, and their performance improved with the number of training cases. Significance: Through this work and challenge we have improved the state-of-the art of computer aided detection algorithms for pulmonary embolism. An open database and an evaluation benchmark for such algorithms have been generated, easing the development of further improvements. Implications on clinical practice will need further research.



### DeFeat-Net: General Monocular Depth via Simultaneous Unsupervised Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2003.13446v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.13446v1)
- **Published**: 2020-03-30 13:10:32+00:00
- **Updated**: 2020-03-30 13:10:32+00:00
- **Authors**: Jaime Spencer, Richard Bowden, Simon Hadfield
- **Comment**: None
- **Journal**: None
- **Summary**: In the current monocular depth research, the dominant approach is to employ unsupervised training on large datasets, driven by warped photometric consistency. Such approaches lack robustness and are unable to generalize to challenging domains such as nighttime scenes or adverse weather conditions where assumptions about photometric consistency break down.   We propose DeFeat-Net (Depth & Feature network), an approach to simultaneously learn a cross-domain dense feature representation, alongside a robust depth-estimation framework based on warped feature consistency. The resulting feature representation is learned in an unsupervised manner with no explicit ground-truth correspondences required.   We show that within a single domain, our technique is comparable to both the current state of the art in monocular depth estimation and supervised feature representation learning. However, by simultaneously learning features, depth and motion, our technique is able to generalize to challenging domains, allowing DeFeat-Net to outperform the current state-of-the-art with around 10% reduction in all error measures on more challenging sequences such as nighttime driving.



### RPM-Net: Robust Point Matching using Learned Features
- **Arxiv ID**: http://arxiv.org/abs/2003.13479v1
- **DOI**: 10.1109/CVPR42600.2020.01184
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.13479v1)
- **Published**: 2020-03-30 13:45:27+00:00
- **Updated**: 2020-03-30 13:45:27+00:00
- **Authors**: Zi Jian Yew, Gim Hee Lee
- **Comment**: 10 pages, 4 figures. To appear in CVPR2020
- **Journal**: None
- **Summary**: Iterative Closest Point (ICP) solves the rigid point cloud registration problem iteratively in two steps: (1) make hard assignments of spatially closest point correspondences, and then (2) find the least-squares rigid transformation. The hard assignments of closest point correspondences based on spatial distances are sensitive to the initial rigid transformation and noisy/outlier points, which often cause ICP to converge to wrong local minima. In this paper, we propose the RPM-Net -- a less sensitive to initialization and more robust deep learning-based approach for rigid point cloud registration. To this end, our network uses the differentiable Sinkhorn layer and annealing to get soft assignments of point correspondences from hybrid features learned from both spatial coordinates and local geometry. To further improve registration performance, we introduce a secondary network to predict optimal annealing parameters. Unlike some existing methods, our RPM-Net handles missing correspondences and point clouds with partial visibility. Experimental results show that our RPM-Net achieves state-of-the-art performance compared to existing non-deep learning and recent deep learning methods. Our source code is available at the project website https://github.com/yewzijian/RPMNet .



### Faster than FAST: GPU-Accelerated Frontend for High-Speed VIO
- **Arxiv ID**: http://arxiv.org/abs/2003.13493v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.13493v3)
- **Published**: 2020-03-30 14:16:23+00:00
- **Updated**: 2020-08-03 09:22:13+00:00
- **Authors**: Balazs Nagy, Philipp Foehn, Davide Scaramuzza
- **Comment**: IEEE International Conference on Intelligent Robots and Systems
  (IROS), 2020. Open-source implementation available at
  https://github.com/uzh-rpg/vilib
- **Journal**: None
- **Summary**: The recent introduction of powerful embedded graphics processing units (GPUs) has allowed for unforeseen improvements in real-time computer vision applications. It has enabled algorithms to run onboard, well above the standard video rates, yielding not only higher information processing capability, but also reduced latency. This work focuses on the applicability of efficient low-level, GPU hardware-specific instructions to improve on existing computer vision algorithms in the field of visual-inertial odometry (VIO). While most steps of a VIO pipeline work on visual features, they rely on image data for detection and tracking, of which both steps are well suited for parallelization. Especially non-maxima suppression and the subsequent feature selection are prominent contributors to the overall image processing latency. Our work first revisits the problem of non-maxima suppression for feature detection specifically on GPUs, and proposes a solution that selects local response maxima, imposes spatial feature distribution, and extracts features simultaneously. Our second contribution introduces an enhanced FAST feature detector that applies the aforementioned non-maxima suppression method. Finally, we compare our method to other state-of-the-art CPU and GPU implementations, where we always outperform all of them in feature tracking and detection, resulting in over 1000fps throughput on an embedded Jetson TX2 platform. Additionally, we demonstrate our work integrated in a VIO pipeline achieving a metric state estimation at ~200fps.



### An Open-source Tool for Hyperspectral Image Augmentation in Tensorflow
- **Arxiv ID**: http://arxiv.org/abs/2003.13502v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.13502v2)
- **Published**: 2020-03-30 14:28:12+00:00
- **Updated**: 2020-07-09 17:25:26+00:00
- **Authors**: Mohamed Abdelhack
- **Comment**: None
- **Journal**: None
- **Summary**: Satellite imagery allows a plethora of applications ranging from weather forecasting to land surveying. The rapid development of computer vision systems could open new horizons to the utilization of satellite data due to the abundance of large volumes of data. However, current state-of-the-art computer vision systems mainly cater to applications that mainly involve natural images. While useful, those images exhibit a different distribution from satellite images in addition to having more spectral channels. This allows the use of pretrained deep learning models only in a subset of spectral channels that are equivalent to natural images thus discarding valuable information from other spectral channels. This calls for research effort to optimize deep learning models for satellite imagery to enable the assessment of their utility in the domain of remote sensing. Tensorflow tool allows for rapid prototyping and testing of deep learning models, however, its built-in image generator is designed to handle a maximum of four spectral channels. This manuscript introduces an open-source tool that allows the implementation of image augmentation for hyperspectral images in Tensorflow. Given how accessible and easy-to-use Tensorflow is, this tool would provide many researchers with the means to implement, test, and deploy deep learning models for remote sensing applications.



### Improved Gradient based Adversarial Attacks for Quantized Networks
- **Arxiv ID**: http://arxiv.org/abs/2003.13511v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.13511v2)
- **Published**: 2020-03-30 14:34:08+00:00
- **Updated**: 2021-12-29 17:22:40+00:00
- **Authors**: Kartik Gupta, Thalaiyasingam Ajanthan
- **Comment**: AAAI 2022
- **Journal**: None
- **Summary**: Neural network quantization has become increasingly popular due to efficient memory consumption and faster computation resulting from bitwise operations on the quantized networks. Even though they exhibit excellent generalization capabilities, their robustness properties are not well-understood. In this work, we systematically study the robustness of quantized networks against gradient based adversarial attacks and demonstrate that these quantized models suffer from gradient vanishing issues and show a fake sense of robustness. By attributing gradient vanishing to poor forward-backward signal propagation in the trained network, we introduce a simple temperature scaling approach to mitigate this issue while preserving the decision boundary. Despite being a simple modification to existing gradient based adversarial attacks, experiments on multiple image classification datasets with multiple network architectures demonstrate that our temperature scaled attacks obtain near-perfect success rate on quantized networks while outperforming original attacks on adversarially trained models as well as floating-point networks. Code is available at https://github.com/kartikgupta-at-anu/attack-bnn.



### LayoutMP3D: Layout Annotation of Matterport3D
- **Arxiv ID**: http://arxiv.org/abs/2003.13516v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.13516v1)
- **Published**: 2020-03-30 14:40:56+00:00
- **Updated**: 2020-03-30 14:40:56+00:00
- **Authors**: Fu-En Wang, Yu-Hsuan Yeh, Min Sun, Wei-Chen Chiu, Yi-Hsuan Tsai
- **Comment**: Annotation is available at https://github.com/fuenwang/LayoutMP3D
- **Journal**: None
- **Summary**: Inferring the information of 3D layout from a single equirectangular panorama is crucial for numerous applications of virtual reality or robotics (e.g., scene understanding and navigation). To achieve this, several datasets are collected for the task of 360 layout estimation. To facilitate the learning algorithms for autonomous systems in indoor scenarios, we consider the Matterport3D dataset with their originally provided depth map ground truths and further release our annotations for layout ground truths from a subset of Matterport3D. As Matterport3D contains accurate depth ground truths from time-of-flight (ToF) sensors, our dataset provides both the layout and depth information, which enables the opportunity to explore the environment by integrating both cues.



### OCmst: One-class Novelty Detection using Convolutional Neural Network and Minimum Spanning Trees
- **Arxiv ID**: http://arxiv.org/abs/2003.13524v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.13524v1)
- **Published**: 2020-03-30 14:55:39+00:00
- **Updated**: 2020-03-30 14:55:39+00:00
- **Authors**: Riccardo La Grassa, Ignazio Gallo, Nicola Landro
- **Comment**: 16 pages
- **Journal**: None
- **Summary**: We present a novel model called One Class Minimum Spanning Tree (OCmst) for novelty detection problem that uses a Convolutional Neural Network (CNN) as deep feature extractor and graph-based model based on Minimum Spanning Tree (MST). In a novelty detection scenario, the training data is no polluted by outliers (abnormal class) and the goal is to recognize if a test instance belongs to the normal class or to the abnormal class. Our approach uses the deep features from CNN to feed a pair of MSTs built starting from each test instance. To cut down the computational time we use a parameter $\gamma$ to specify the size of the MST's starting to the neighbours from the test instance. To prove the effectiveness of the proposed approach we conducted experiments on two publicly available datasets, well-known in literature and we achieved the state-of-the-art results on CIFAR10 dataset.



### Improving out-of-distribution generalization via multi-task self-supervised pretraining
- **Arxiv ID**: http://arxiv.org/abs/2003.13525v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.13525v1)
- **Published**: 2020-03-30 14:55:53+00:00
- **Updated**: 2020-03-30 14:55:53+00:00
- **Authors**: Isabela Albuquerque, Nikhil Naik, Junnan Li, Nitish Keskar, Richard Socher
- **Comment**: None
- **Journal**: None
- **Summary**: Self-supervised feature representations have been shown to be useful for supervised classification, few-shot learning, and adversarial robustness. We show that features obtained using self-supervised learning are comparable to, or better than, supervised learning for domain generalization in computer vision. We introduce a new self-supervised pretext task of predicting responses to Gabor filter banks and demonstrate that multi-task learning of compatible pretext tasks improves domain generalization performance as compared to training individual tasks alone. Features learnt through self-supervision obtain better generalization to unseen domains when compared to their supervised counterpart when there is a larger domain shift between training and test distributions and even show better localization ability for objects of interest. Self-supervised feature representations can also be combined with other domain generalization methods to further boost performance.



### SiTGRU: Single-Tunnelled Gated Recurrent Unit for Abnormality Detection
- **Arxiv ID**: http://arxiv.org/abs/2003.13528v1
- **DOI**: 10.1016/j.ins.2020.03.034
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.13528v1)
- **Published**: 2020-03-30 14:58:13+00:00
- **Updated**: 2020-03-30 14:58:13+00:00
- **Authors**: Habtamu Fanta, Zhiwen Shao, Lizhuang Ma
- **Comment**: 14 pages, 11 figures, 13 tables, this paper is accepted on Journal of
  Information Sciences
- **Journal**: Journal of Information Sciences 524 (2020) 15-32
- **Summary**: Abnormality detection is a challenging task due to the dependence on a specific context and the unconstrained variability of practical scenarios. In recent years, it has benefited from the powerful features learnt by deep neural networks, and handcrafted features specialized for abnormality detectors. However, these approaches with large complexity still have limitations in handling long term sequential data (e.g., videos), and their learnt features do not thoroughly capture useful information. Recurrent Neural Networks (RNNs) have been shown to be capable of robustly dealing with temporal data in long term sequences. In this paper, we propose a novel version of Gated Recurrent Unit (GRU), called Single Tunnelled GRU for abnormality detection. Particularly, the Single Tunnelled GRU discards the heavy weighted reset gate from GRU cells that overlooks the importance of past content by only favouring current input to obtain an optimized single gated cell model. Moreover, we substitute the hyperbolic tangent activation in standard GRUs with sigmoid activation, as the former suffers from performance loss in deeper networks. Empirical results show that our proposed optimized GRU model outperforms standard GRU and Long Short Term Memory (LSTM) networks on most metrics for detection and generalization tasks on CUHK Avenue and UCSD datasets. The model is also computationally efficient with reduced training and testing time over standard RNNs.



### Super Resolution for Root Imaging
- **Arxiv ID**: http://arxiv.org/abs/2003.13537v2
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2003.13537v2)
- **Published**: 2020-03-30 15:11:15+00:00
- **Updated**: 2020-05-05 12:23:36+00:00
- **Authors**: Jose F. Ruiz-Munoz, Jyothier K. Nimmagadda, Tyler G. Dowd, James E. Baciak, Alina Zare
- **Comment**: Under review. Submitted to Applications in Plant Sciences (APPS)
- **Journal**: None
- **Summary**: High-resolution cameras have become very helpful for plant phenotyping by providing a mechanism for tasks such as target versus background discrimination, and the measurement and analysis of fine-above-ground plant attributes. However, the acquisition of high-resolution (HR) imagery of plant roots is more challenging than above-ground data collection. Thus, an effective super-resolution (SR) algorithm is desired for overcoming resolution limitations of sensors, reducing storage space requirements, and boosting the performance of later analysis, such as automatic segmentation. We propose a SR framework for enhancing images of plant roots by using convolutional neural networks (CNNs). We compare three alternatives for training the SR model: i) training with non-plant-root images, ii) training with plant-root images, and iii) pretraining the model with non-plant-root images and fine-tuning with plant-root images. We demonstrate on a collection of publicly available datasets that the SR models outperform the basic bicubic interpolation even when trained with non-root datasets. Also, our segmentation experiments show that high performance on this task can be achieved independently of the SNR. Therefore, we conclude that the quality of the image enhancement depends on the application.



### Rethinking Depthwise Separable Convolutions: How Intra-Kernel Correlations Lead to Improved MobileNets
- **Arxiv ID**: http://arxiv.org/abs/2003.13549v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.13549v3)
- **Published**: 2020-03-30 15:23:27+00:00
- **Updated**: 2020-07-13 14:57:16+00:00
- **Authors**: Daniel Haase, Manuel Amthor
- **Comment**: Published at CVPR 2020. Code and models are available under
  https://github.com/zeiss-microscopy/BSConv
- **Journal**: The IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR), 2020, pp. 14600-14609
- **Summary**: We introduce blueprint separable convolutions (BSConv) as highly efficient building blocks for CNNs. They are motivated by quantitative analyses of kernel properties from trained models, which show the dominance of correlations along the depth axis. Based on our findings, we formulate a theoretical foundation from which we derive efficient implementations using only standard layers. Moreover, our approach provides a thorough theoretical derivation, interpretation, and justification for the application of depthwise separable convolutions (DSCs) in general, which have become the basis of many modern network architectures. Ultimately, we reveal that DSC-based architectures such as MobileNets implicitly rely on cross-kernel correlations, while our BSConv formulation is based on intra-kernel correlations and thus allows for a more efficient separation of regular convolutions. Extensive experiments on large-scale and fine-grained classification datasets show that BSConvs clearly and consistently improve MobileNets and other DSC-based architectures without introducing any further complexity. For fine-grained datasets, we achieve an improvement of up to 13.7 percentage points. In addition, if used as drop-in replacement for standard architectures such as ResNets, BSConv variants also outperform their vanilla counterparts by up to 9.5 percentage points on ImageNet. Code and models are available under https://github.com/zeiss-microscopy/BSConv.



### BVI-DVC: A Training Database for Deep Video Compression
- **Arxiv ID**: http://arxiv.org/abs/2003.13552v2
- **DOI**: 10.1109/TMM.2021.3108943
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.13552v2)
- **Published**: 2020-03-30 15:26:16+00:00
- **Updated**: 2020-10-08 10:24:30+00:00
- **Authors**: Di Ma, Fan Zhang, David R. Bull
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning methods are increasingly being applied in the optimisation of video compression algorithms and can achieve significantly enhanced coding gains, compared to conventional approaches. Such approaches often employ Convolutional Neural Networks (CNNs) which are trained on databases with relatively limited content coverage. In this paper, a new extensive and representative video database, BVI-DVC, is presented for training CNN-based video compression systems, with specific emphasis on machine learning tools that enhance conventional coding architectures, including spatial resolution and bit depth up-sampling, post-processing and in-loop filtering. BVI-DVC contains 800 sequences at various spatial resolutions from 270p to 2160p and has been evaluated on ten existing network architectures for four different coding tools. Experimental results show that this database produces significant improvements in terms of coding gains over three existing (commonly used) image/video training databases under the same training and evaluation configurations. The overall additional coding improvements by using the proposed database for all tested coding modules and CNN architectures are up to 10.3% based on the assessment of PSNR and 8.1% based on VMAF.



### Squeezed Deep 6DoF Object Detection Using Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2003.13586v3
- **DOI**: 10.1109/IJCNN48605.2020.9207459
- **Categories**: **cs.CV**, cs.LG, eess.IV, 68-04
- **Links**: [PDF](http://arxiv.org/pdf/2003.13586v3)
- **Published**: 2020-03-30 16:03:03+00:00
- **Updated**: 2020-05-29 22:41:21+00:00
- **Authors**: Heitor Felix, Walber M. Rodrigues, David Macêdo, Francisco Simões, Adriano L. I. Oliveira, Veronica Teichrieb, Cleber Zanchettin
- **Comment**: This paper was accepted by 2020 International Joint Conference on
  Neural Networks (IJCNN)
- **Journal**: 2020 International Joint Conference on Neural Networks (IJCNN)
- **Summary**: The detection of objects considering a 6DoF pose is a common requirement to build virtual and augmented reality applications. It is usually a complex task which requires real-time processing and high precision results for adequate user experience. Recently, different deep learning techniques have been proposed to detect objects in 6DoF in RGB images. However, they rely on high complexity networks, requiring a computational power that prevents them from working on mobile devices. In this paper, we propose an approach to reduce the complexity of 6DoF detection networks while maintaining accuracy. We used Knowledge Distillation to teach portables Convolutional Neural Networks (CNN) to learn from a real-time 6DoF detection CNN. The proposed method allows real-time applications using only RGB images while decreasing the hardware requirements. We used the LINEMOD dataset to evaluate the proposed method, and the experimental results show that the proposed method reduces the memory requirement by almost 99\% in comparison to the original architecture with the cost of reducing half the accuracy in one of the metrics. Code is available at https://github.com/heitorcfelix/singleshot6Dpose.



### How Not to Give a FLOP: Combining Regularization and Pruning for Efficient Inference
- **Arxiv ID**: http://arxiv.org/abs/2003.13593v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.13593v2)
- **Published**: 2020-03-30 16:20:46+00:00
- **Updated**: 2020-04-09 11:21:07+00:00
- **Authors**: Tai Vu, Emily Wen, Roy Nehoran
- **Comment**: Citations added, typos fixed
- **Journal**: None
- **Summary**: The challenge of speeding up deep learning models during the deployment phase has been a large, expensive bottleneck in the modern tech industry. In this paper, we examine the use of both regularization and pruning for reduced computational complexity and more efficient inference in Deep Neural Networks (DNNs). In particular, we apply mixup and cutout regularizations and soft filter pruning to the ResNet architecture, focusing on minimizing floating-point operations (FLOPs). Furthermore, by using regularization in conjunction with network pruning, we show that such a combination makes a substantial improvement over each of the two techniques individually.



### Speech2Action: Cross-modal Supervision for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2003.13594v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.13594v1)
- **Published**: 2020-03-30 16:22:39+00:00
- **Updated**: 2020-03-30 16:22:39+00:00
- **Authors**: Arsha Nagrani, Chen Sun, David Ross, Rahul Sukthankar, Cordelia Schmid, Andrew Zisserman
- **Comment**: Accepted to CVPR 2020
- **Journal**: None
- **Summary**: Is it possible to guess human action from dialogue alone? In this work we investigate the link between spoken words and actions in movies. We note that movie screenplays describe actions, as well as contain the speech of characters and hence can be used to learn this correlation with no additional supervision. We train a BERT-based Speech2Action classifier on over a thousand movie screenplays, to predict action labels from transcribed speech segments. We then apply this model to the speech segments of a large unlabelled movie corpus (188M speech segments from 288K movies). Using the predictions of this model, we obtain weak action labels for over 800K video clips. By training on these video clips, we demonstrate superior action recognition performance on standard action recognition benchmarks, without using a single manually labelled action example.



### Laplacian Denoising Autoencoder
- **Arxiv ID**: http://arxiv.org/abs/2003.13623v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.13623v1)
- **Published**: 2020-03-30 16:52:39+00:00
- **Updated**: 2020-03-30 16:52:39+00:00
- **Authors**: Jianbo Jiao, Linchao Bao, Yunchao Wei, Shengfeng He, Honghui Shi, Rynson Lau, Thomas S. Huang
- **Comment**: None
- **Journal**: None
- **Summary**: While deep neural networks have been shown to perform remarkably well in many machine learning tasks, labeling a large amount of ground truth data for supervised training is usually very costly to scale. Therefore, learning robust representations with unlabeled data is critical in relieving human effort and vital for many downstream tasks. Recent advances in unsupervised and self-supervised learning approaches for visual data have benefited greatly from domain knowledge. Here we are interested in a more generic unsupervised learning framework that can be easily generalized to other domains. In this paper, we propose to learn data representations with a novel type of denoising autoencoder, where the noisy input data is generated by corrupting latent clean data in the gradient domain. This can be naturally generalized to span multiple scales with a Laplacian pyramid representation of the input data. In this way, the agent learns more robust representations that exploit the underlying data structures across multiple scales. Experiments on several visual benchmarks demonstrate that better representations can be learned with the proposed approach, compared to its counterpart with single-scale corruption and other approaches. Furthermore, we also demonstrate that the learned representations perform well when transferring to other downstream vision tasks.



### Streaming Networks: Increase Noise Robustness and Filter Diversity via Hard-wired and Input-induced Sparsity
- **Arxiv ID**: http://arxiv.org/abs/2004.03334v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.03334v2)
- **Published**: 2020-03-30 16:58:23+00:00
- **Updated**: 2020-04-09 03:50:07+00:00
- **Authors**: Sergey Tarasenko, Fumihiko Takahashi
- **Comment**: 17 pages, 37 figures. arXiv admin note: text overlap with
  arXiv:1910.11107
- **Journal**: None
- **Summary**: The CNNs have achieved a state-of-the-art performance in many applications. Recent studies illustrate that CNN's recognition accuracy drops drastically if images are noise corrupted. We focus on the problem of robust recognition accuracy of noise-corrupted images. We introduce a novel network architecture called Streaming Networks. Each stream is taking a certain intensity slice of the original image as an input, and stream parameters are trained independently. We use network capacity, hard-wired and input-induced sparsity as the dimensions for experiments. The results indicate that only the presence of both hard-wired and input-induces sparsity enables robust noisy image recognition. Streaming Nets is the only architecture which has both types of sparsity and exhibits higher robustness to noise. Finally, to illustrate increase in filter diversity we illustrate that a distribution of filter weights of the first conv layer gradually approaches uniform distribution as the degree of hard-wired and domain-induced sparsity and capacities increases.



### TResNet: High Performance GPU-Dedicated Architecture
- **Arxiv ID**: http://arxiv.org/abs/2003.13630v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.13630v3)
- **Published**: 2020-03-30 17:04:47+00:00
- **Updated**: 2020-08-27 05:36:43+00:00
- **Authors**: Tal Ridnik, Hussam Lawen, Asaf Noy, Emanuel Ben Baruch, Gilad Sharir, Itamar Friedman
- **Comment**: 11 pages, 5 figures
- **Journal**: None
- **Summary**: Many deep learning models, developed in recent years, reach higher ImageNet accuracy than ResNet50, with fewer or comparable FLOPS count. While FLOPs are often seen as a proxy for network efficiency, when measuring actual GPU training and inference throughput, vanilla ResNet50 is usually significantly faster than its recent competitors, offering better throughput-accuracy trade-off.   In this work, we introduce a series of architecture modifications that aim to boost neural networks' accuracy, while retaining their GPU training and inference efficiency. We first demonstrate and discuss the bottlenecks induced by FLOPs-optimizations. We then suggest alternative designs that better utilize GPU structure and assets. Finally, we introduce a new family of GPU-dedicated models, called TResNet, which achieve better accuracy and efficiency than previous ConvNets.   Using a TResNet model, with similar GPU throughput to ResNet50, we reach 80.8 top-1 accuracy on ImageNet. Our TResNet models also transfer well and achieve state-of-the-art accuracy on competitive single-label classification datasets such as Stanford cars (96.0%), CIFAR-10 (99.0%), CIFAR-100 (91.5%) and Oxford-Flowers (99.1%). They also perform well on multi-label classification and object detection tasks. Implementation is available at: https://github.com/mrT23/TResNet.



### Supervised and Unsupervised Detections for Multiple Object Tracking in Traffic Scenes: A Comparative Study
- **Arxiv ID**: http://arxiv.org/abs/2003.13644v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.13644v1)
- **Published**: 2020-03-30 17:27:04+00:00
- **Updated**: 2020-03-30 17:27:04+00:00
- **Authors**: Hui-Lee Ooi, Guillaume-Alexandre Bilodeau, Nicolas Saunier
- **Comment**: Accepted for ICIAR 2020
- **Journal**: None
- **Summary**: In this paper, we propose a multiple object tracker, called MF-Tracker, that integrates multiple classical features (spatial distances and colours) and modern features (detection labels and re-identification features) in its tracking framework. Since our tracker can work with detections coming either from unsupervised and supervised object detectors, we also investigated the impact of supervised and unsupervised detection inputs in our method and for tracking road users in general. We also compared our results with existing methods that were applied on the UA-Detrac and the UrbanTracker datasets. Results show that our proposed method is performing very well in both datasets with different inputs (MOTA ranging from 0:3491 to 0:5805 for unsupervised inputs on the UrbanTracker dataset and an average MOTA of 0:7638 for supervised inputs on the UA Detrac dataset) under different circumstances. A well-trained supervised object detector can give better results in challenging scenarios. However, in simpler scenarios, if good training data is not available, unsupervised method can perform well and can be a good alternative.



### Weakly-supervised land classification for coastal zone based on deep convolutional neural networks by incorporating dual-polarimetric characteristics into training dataset
- **Arxiv ID**: http://arxiv.org/abs/2003.13648v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.13648v1)
- **Published**: 2020-03-30 17:32:49+00:00
- **Updated**: 2020-03-30 17:32:49+00:00
- **Authors**: Sheng Sun, Armando Marino, Wenze Shui, Zhongwen Hu
- **Comment**: None
- **Journal**: None
- **Summary**: In this work we explore the performance of DCNNs on semantic segmentation using spaceborne polarimetric synthetic aperture radar (PolSAR) datasets. The semantic segmentation task using PolSAR data can be categorized as weakly supervised learning when the characteristics of SAR data and data annotating procedures are factored in. Datasets are initially analyzed for selecting feasible pre-training images. Then the differences between spaceborne and airborne datasets are examined in terms of spatial resolution and viewing geometry. In this study we used two dual-polarimetric images acquired by TerraSAR-X DLR. A novel method to produce training dataset with more supervised information is developed. Specifically, a series of typical classified images as well as intensity images serve as training datasets. A field survey is conducted for an area of about 20 square kilometers to obtain a ground truth dataset used for accuracy evaluation. Several transfer learning strategies are made for aforementioned training datasets which will be combined in a practicable order. Three DCNN models, including SegNet, U-Net, and LinkNet, are implemented next.



### Plug-and-Play Algorithms for Large-scale Snapshot Compressive Imaging
- **Arxiv ID**: http://arxiv.org/abs/2003.13654v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.13654v2)
- **Published**: 2020-03-30 17:41:12+00:00
- **Updated**: 2020-07-17 21:10:04+00:00
- **Authors**: Xin Yuan, Yang Liu, Jinli Suo, Qionghai Dai
- **Comment**: CVPR 2020. Corrected a proof of convergence in previous version
- **Journal**: None
- **Summary**: Snapshot compressive imaging (SCI) aims to capture the high-dimensional (usually 3D) images using a 2D sensor (detector) in a single snapshot. Though enjoying the advantages of low-bandwidth, low-power and low-cost, applying SCI to large-scale problems (HD or UHD videos) in our daily life is still challenging. The bottleneck lies in the reconstruction algorithms; they are either too slow (iterative optimization algorithms) or not flexible to the encoding process (deep learning based end-to-end networks). In this paper, we develop fast and flexible algorithms for SCI based on the plug-and-play (PnP) framework. In addition to the widely used PnP-ADMM method, we further propose the PnP-GAP (generalized alternating projection) algorithm with a lower computational workload and prove the convergence of PnP-GAP under the SCI hardware constraints. By employing deep denoising priors, we first time show that PnP can recover a UHD color video ($3840\times 1644\times 48$ with PNSR above 30dB) from a snapshot 2D measurement. Extensive results on both simulation and real datasets verify the superiority of our proposed algorithm. The code is available at https://github.com/liuyang12/PnP-SCI.



### Exploiting Deep Generative Prior for Versatile Image Restoration and Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2003.13659v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.13659v4)
- **Published**: 2020-03-30 17:45:07+00:00
- **Updated**: 2020-07-20 10:06:24+00:00
- **Authors**: Xingang Pan, Xiaohang Zhan, Bo Dai, Dahua Lin, Chen Change Loy, Ping Luo
- **Comment**: Accepted to ECCV2020 as oral. 1) Precise GAN-inversion by
  discriminator-guided generator finetuning. 2) A versatile way for
  high-quality image restoration and manipulation. Code:
  https://github.com/XingangPan/deep-generative-prior
- **Journal**: None
- **Summary**: Learning a good image prior is a long-term goal for image restoration and manipulation. While existing methods like deep image prior (DIP) capture low-level image statistics, there are still gaps toward an image prior that captures rich image semantics including color, spatial coherence, textures, and high-level concepts. This work presents an effective way to exploit the image prior captured by a generative adversarial network (GAN) trained on large-scale natural images. As shown in Fig.1, the deep generative prior (DGP) provides compelling results to restore missing semantics, e.g., color, patch, resolution, of various degraded images. It also enables diverse image manipulation including random jittering, image morphing, and category transfer. Such highly flexible restoration and manipulation are made possible through relaxing the assumption of existing GAN-inversion methods, which tend to fix the generator. Notably, we allow the generator to be fine-tuned on-the-fly in a progressive manner regularized by feature distance obtained by the discriminator in GAN. We show that these easy-to-implement and practical changes help preserve the reconstruction to remain in the manifold of nature image, and thus lead to more precise and faithful reconstruction for real images. Code is available at https://github.com/XingangPan/deep-generative-prior.



### Designing Network Design Spaces
- **Arxiv ID**: http://arxiv.org/abs/2003.13678v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.13678v1)
- **Published**: 2020-03-30 17:57:47+00:00
- **Updated**: 2020-03-30 17:57:47+00:00
- **Authors**: Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, Piotr Dollár
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: In this work, we present a new network design paradigm. Our goal is to help advance the understanding of network design and discover design principles that generalize across settings. Instead of focusing on designing individual network instances, we design network design spaces that parametrize populations of networks. The overall process is analogous to classic manual design of networks, but elevated to the design space level. Using our methodology we explore the structure aspect of network design and arrive at a low-dimensional design space consisting of simple, regular networks that we call RegNet. The core insight of the RegNet parametrization is surprisingly simple: widths and depths of good networks can be explained by a quantized linear function. We analyze the RegNet design space and arrive at interesting findings that do not match the current practice of network design. The RegNet design space provides simple and fast networks that work well across a wide range of flop regimes. Under comparable training settings and flops, the RegNet models outperform the popular EfficientNet models while being up to 5x faster on GPUs.



### DHP: Differentiable Meta Pruning via HyperNetworks
- **Arxiv ID**: http://arxiv.org/abs/2003.13683v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.13683v3)
- **Published**: 2020-03-30 17:59:18+00:00
- **Updated**: 2020-08-01 10:59:30+00:00
- **Authors**: Yawei Li, Shuhang Gu, Kai Zhang, Luc Van Gool, Radu Timofte
- **Comment**: ECCV camera-ready. Code is available at
  https://github.com/ofsoundof/dhp
- **Journal**: None
- **Summary**: Network pruning has been the driving force for the acceleration of neural networks and the alleviation of model storage/transmission burden. With the advent of AutoML and neural architecture search (NAS), pruning has become topical with automatic mechanism and searching based architecture optimization. Yet, current automatic designs rely on either reinforcement learning or evolutionary algorithm. Due to the non-differentiability of those algorithms, the pruning algorithm needs a long searching stage before reaching the convergence.   To circumvent this problem, this paper introduces a differentiable pruning method via hypernetworks for automatic network pruning. The specifically designed hypernetworks take latent vectors as input and generate the weight parameters of the backbone network. The latent vectors control the output channels of the convolutional layers in the backbone network and act as a handle for the pruning of the layers. By enforcing $\ell_1$ sparsity regularization to the latent vectors and utilizing proximal gradient solver, sparse latent vectors can be obtained. Passing the sparsified latent vectors through the hypernetworks, the corresponding slices of the generated weight parameters can be removed, achieving the effect of network pruning. The latent vectors of all the layers are pruned together, resulting in an automatic layer configuration. Extensive experiments are conducted on various networks for image classification, single image super-resolution, and denoising. And the experimental results validate the proposed method.



### Certifiable Relative Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2003.13732v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.13732v2)
- **Published**: 2020-03-30 18:26:04+00:00
- **Updated**: 2021-02-19 09:45:53+00:00
- **Authors**: Mercedes Garcia-Salguero, Jesus Briales, Javier Gonzalez-Jimenez
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we present the first fast optimality certifier for the non-minimal version of the Relative Pose problem for calibrated cameras from epipolar constraints. The proposed certifier is based on Lagrangian duality and relies on a novel closed-form expression for dual points. We also leverage an efficient solver that performs local optimization on the manifold of the original problem's non-convex domain. The optimality of the solution is then checked via our novel fast certifier. The extensive conducted experiments demonstrate that, despite its simplicity, this certifiable solver performs excellently on synthetic data, repeatedly attaining the (certified \textit{a posteriori}) optimal solution and shows a satisfactory performance on real data.



### Non-dimensional Star-Identification
- **Arxiv ID**: http://arxiv.org/abs/2003.13736v2
- **DOI**: 10.3390/s20092697
- **Categories**: **astro-ph.IM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.13736v2)
- **Published**: 2020-03-30 18:33:20+00:00
- **Updated**: 2020-05-14 15:32:00+00:00
- **Authors**: Carl Leake, David Arnas, Daniele Mortari
- **Comment**: 17 pages, 10 figures, 4 tables
- **Journal**: Sensors. 2020; 20(9):2697
- **Summary**: This study introduces a new "Non-Dimensional" star identification algorithm to reliably identify the stars observed by a wide field-of-view star tracker when the focal length and optical axis offset values are known with poor accuracy. This algorithm is particularly suited to complement nominal lost-in-space algorithms, which may identify stars incorrectly when the focal length and/or optical axis offset deviate from their nominal operational ranges. These deviations may be caused, for example, by launch vibrations or thermal variations in orbit. The algorithm performance is compared in terms of accuracy, speed, and robustness to the Pyramid algorithm. These comparisons highlight the clear advantages that a combined approach of these methodologies provides.



### Combining detection and tracking for human pose estimation in videos
- **Arxiv ID**: http://arxiv.org/abs/2003.13743v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.13743v1)
- **Published**: 2020-03-30 18:45:31+00:00
- **Updated**: 2020-03-30 18:45:31+00:00
- **Authors**: Manchen Wang, Joseph Tighe, Davide Modolo
- **Comment**: Accepted to CVPR 2020 as oral
- **Journal**: None
- **Summary**: We propose a novel top-down approach that tackles the problem of multi-person human pose estimation and tracking in videos. In contrast to existing top-down approaches, our method is not limited by the performance of its person detector and can predict the poses of person instances not localized. It achieves this capability by propagating known person locations forward and backward in time and searching for poses in those regions. Our approach consists of three components: (i) a Clip Tracking Network that performs body joint detection and tracking simultaneously on small video clips; (ii) a Video Tracking Pipeline that merges the fixed-length tracklets produced by the Clip Tracking Network to arbitrary length tracks; and (iii) a Spatial-Temporal Merging procedure that refines the joint locations based on spatial and temporal smoothing terms. Thanks to the precision of our Clip Tracking Network and our merging procedure, our approach produces very accurate joint predictions and can fix common mistakes on hard scenarios like heavily entangled people. Our approach achieves state-of-the-art results on both joint detection and tracking, on both the PoseTrack 2017 and 2018 datasets, and against all top-down and bottom-down approaches.



### PointAR: Efficient Lighting Estimation for Mobile Augmented Reality
- **Arxiv ID**: http://arxiv.org/abs/2004.00006v4
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.00006v4)
- **Published**: 2020-03-30 19:13:26+00:00
- **Updated**: 2020-07-17 20:13:40+00:00
- **Authors**: Yiqin Zhao, Tian Guo
- **Comment**: Accepted to 16th European Conference On Computer Vision (ECCV'20)
- **Journal**: None
- **Summary**: We propose an efficient lighting estimation pipeline that is suitable to run on modern mobile devices, with comparable resource complexities to state-of-the-art mobile deep learning models. Our pipeline, PointAR, takes a single RGB-D image captured from the mobile camera and a 2D location in that image, and estimates 2nd order spherical harmonics coefficients. This estimated spherical harmonics coefficients can be directly utilized by rendering engines for supporting spatially variant indoor lighting, in the context of augmented reality. Our key insight is to formulate the lighting estimation as a point cloud-based learning problem directly from point clouds, which is in part inspired by the Monte Carlo integration leveraged by real-time spherical harmonics lighting. While existing approaches estimate lighting information with complex deep learning pipelines, our method focuses on reducing the computational complexity. Through both quantitative and qualitative experiments, we demonstrate that PointAR achieves lower lighting estimation errors compared to state-of-the-art methods. Further, our method requires an order of magnitude lower resource, comparable to that of mobile-specific DNNs.



### Understanding the impact of mistakes on background regions in crowd counting
- **Arxiv ID**: http://arxiv.org/abs/2003.13759v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.13759v1)
- **Published**: 2020-03-30 19:16:18+00:00
- **Updated**: 2020-03-30 19:16:18+00:00
- **Authors**: Davide Modolo, Bing Shuai, Rahul Rama Varior, Joseph Tighe
- **Comment**: None
- **Journal**: None
- **Summary**: Every crowd counting researcher has likely observed their model output wrong positive predictions on image regions not containing any person. But how often do these mistakes happen? Are our models negatively affected by this? In this paper we analyze this problem in depth. In order to understand its magnitude, we present an extensive analysis on five of the most important crowd counting datasets. We present this analysis in two parts. First, we quantify the number of mistakes made by popular crowd counting approaches. Our results show that (i) mistakes on background are substantial and they are responsible for 18-49% of the total error, (ii) models do not generalize well to different kinds of backgrounds and perform poorly on completely background images, and (iii) models make many more mistakes than those captured by the standard Mean Absolute Error (MAE) metric, as counting on background compensates considerably for misses on foreground. And second, we quantify the performance change gained by helping the model better deal with this problem. We enrich a typical crowd counting network with a segmentation branch trained to suppress background predictions. This simple addition (i) reduces background error by 10-83%, (ii) reduces foreground error by up to 26% and (iii) improves overall crowd counting performance up to 20%. When compared against the literature, this simple technique achieves very competitive results on all datasets, on par with the state-of-the-art, showing the importance of tackling the background problem.



### Measuring Generalisation to Unseen Viewpoints, Articulations, Shapes and Objects for 3D Hand Pose Estimation under Hand-Object Interaction
- **Arxiv ID**: http://arxiv.org/abs/2003.13764v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.13764v2)
- **Published**: 2020-03-30 19:28:13+00:00
- **Updated**: 2020-09-10 15:35:17+00:00
- **Authors**: Anil Armagan, Guillermo Garcia-Hernando, Seungryul Baek, Shreyas Hampali, Mahdi Rad, Zhaohui Zhang, Shipeng Xie, MingXiu Chen, Boshen Zhang, Fu Xiong, Yang Xiao, Zhiguo Cao, Junsong Yuan, Pengfei Ren, Weiting Huang, Haifeng Sun, Marek Hrúz, Jakub Kanis, Zdeněk Krňoul, Qingfu Wan, Shile Li, Linlin Yang, Dongheui Lee, Angela Yao, Weiguo Zhou, Sijia Mei, Yunhui Liu, Adrian Spurr, Umar Iqbal, Pavlo Molchanov, Philippe Weinzaepfel, Romain Brégier, Grégory Rogez, Vincent Lepetit, Tae-Kyun Kim
- **Comment**: European Conference on Computer Vision (ECCV), 2020
- **Journal**: None
- **Summary**: We study how well different types of approaches generalise in the task of 3D hand pose estimation under single hand scenarios and hand-object interaction. We show that the accuracy of state-of-the-art methods can drop, and that they fail mostly on poses absent from the training set. Unfortunately, since the space of hand poses is highly dimensional, it is inherently not feasible to cover the whole space densely, despite recent efforts in collecting large-scale training datasets. This sampling problem is even more severe when hands are interacting with objects and/or inputs are RGB rather than depth images, as RGB images also vary with lighting conditions and colors. To address these issues, we designed a public challenge (HANDS'19) to evaluate the abilities of current 3D hand pose estimators (HPEs) to interpolate and extrapolate the poses of a training set. More exactly, HANDS'19 is designed (a) to evaluate the influence of both depth and color modalities on 3D hand pose estimation, under the presence or absence of objects; (b) to assess the generalisation abilities w.r.t. four main axes: shapes, articulations, viewpoints, and objects; (c) to explore the use of a synthetic hand model to fill the gaps of current datasets. Through the challenge, the overall accuracy has dramatically improved over the baseline, especially on extrapolation tasks, from 27mm to 13mm mean joint error. Our analyses highlight the impacts of: Data pre-processing, ensemble approaches, the use of a parametric 3D hand model (MANO), and different HPE methods/backbones.



### Domain Balancing: Face Recognition on Long-Tailed Domains
- **Arxiv ID**: http://arxiv.org/abs/2003.13791v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.13791v1)
- **Published**: 2020-03-30 20:16:31+00:00
- **Updated**: 2020-03-30 20:16:31+00:00
- **Authors**: Dong Cao, Xiangyu Zhu, Xingyu Huang, Jianzhu Guo, Zhen Lei
- **Comment**: Accepted to CVPR2020
- **Journal**: None
- **Summary**: Long-tailed problem has been an important topic in face recognition task. However, existing methods only concentrate on the long-tailed distribution of classes. Differently, we devote to the long-tailed domain distribution problem, which refers to the fact that a small number of domains frequently appear while other domains far less existing. The key challenge of the problem is that domain labels are too complicated (related to race, age, pose, illumination, etc.) and inaccessible in real applications. In this paper, we propose a novel Domain Balancing (DB) mechanism to handle this problem. Specifically, we first propose a Domain Frequency Indicator (DFI) to judge whether a sample is from head domains or tail domains. Secondly, we formulate a light-weighted Residual Balancing Mapping (RBM) block to balance the domain distribution by adjusting the network according to DFI. Finally, we propose a Domain Balancing Margin (DBM) in the loss function to further optimize the feature space of the tail domains to improve generalization. Extensive analysis and experiments on several face recognition benchmarks demonstrate that the proposed method effectively enhances the generalization capacities and achieves superior performance.



### When to Use Convolutional Neural Networks for Inverse Problems
- **Arxiv ID**: http://arxiv.org/abs/2003.13820v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.13820v1)
- **Published**: 2020-03-30 21:08:14+00:00
- **Updated**: 2020-03-30 21:08:14+00:00
- **Authors**: Nathaniel Chodosh, Simon Lucey
- **Comment**: CVPR 2020 Poster
- **Journal**: None
- **Summary**: Reconstruction tasks in computer vision aim fundamentally to recover an undetermined signal from a set of noisy measurements. Examples include super-resolution, image denoising, and non-rigid structure from motion, all of which have seen recent advancements through deep learning. However, earlier work made extensive use of sparse signal reconstruction frameworks (e.g convolutional sparse coding). While this work was ultimately surpassed by deep learning, it rested on a much more developed theoretical framework. Recent work by Papyan et. al provides a bridge between the two approaches by showing how a convolutional neural network (CNN) can be viewed as an approximate solution to a convolutional sparse coding (CSC) problem. In this work we argue that for some types of inverse problems the CNN approximation breaks down leading to poor performance. We argue that for these types of problems the CSC approach should be used instead and validate this argument with empirical evidence. Specifically we identify JPEG artifact reduction and non-rigid trajectory reconstruction as challenging inverse problems for CNNs and demonstrate state of the art performance on them using a CSC method. Furthermore, we offer some practical improvements to this model and its application, and also show how insights from the CSC model can be used to make CNNs effective in tasks where their naive application fails.



### Co-occurrence of deep convolutional features for image search
- **Arxiv ID**: http://arxiv.org/abs/2003.13827v2
- **DOI**: 10.1016/j.imavis.2020.103909
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.13827v2)
- **Published**: 2020-03-30 21:27:22+00:00
- **Updated**: 2021-06-10 16:47:42+00:00
- **Authors**: J. I. Forcen, Miguel Pagola, Edurne Barrenechea, Humberto Bustince
- **Comment**: None
- **Journal**: None
- **Summary**: Image search can be tackled using deep features from pre-trained Convolutional Neural Networks (CNN). The feature map from the last convolutional layer of a CNN encodes descriptive information from which a discriminative global descriptor can be obtained. We propose a new representation of co-occurrences from deep convolutional features to extract additional relevant information from this last convolutional layer. Combining this co-occurrence map with the feature map, we achieve an improved image representation. We present two different methods to get the co-occurrence representation, the first one based on direct aggregation of activations, and the second one, based on a trainable co-occurrence representation. The image descriptors derived from our methodology improve the performance in very well-known image retrieval datasets as we prove in the experiments.



### Sign Language Transformers: Joint End-to-end Sign Language Recognition and Translation
- **Arxiv ID**: http://arxiv.org/abs/2003.13830v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.13830v1)
- **Published**: 2020-03-30 21:35:09+00:00
- **Updated**: 2020-03-30 21:35:09+00:00
- **Authors**: Necati Cihan Camgoz, Oscar Koller, Simon Hadfield, Richard Bowden
- **Comment**: None
- **Journal**: None
- **Summary**: Prior work on Sign Language Translation has shown that having a mid-level sign gloss representation (effectively recognizing the individual signs) improves the translation performance drastically. In fact, the current state-of-the-art in translation requires gloss level tokenization in order to work. We introduce a novel transformer based architecture that jointly learns Continuous Sign Language Recognition and Translation while being trainable in an end-to-end manner. This is achieved by using a Connectionist Temporal Classification (CTC) loss to bind the recognition and translation problems into a single unified architecture. This joint approach does not require any ground-truth timing information, simultaneously solving two co-dependant sequence-to-sequence learning problems and leads to significant performance gains.   We evaluate the recognition and translation performances of our approaches on the challenging RWTH-PHOENIX-Weather-2014T (PHOENIX14T) dataset. We report state-of-the-art sign language recognition and translation results achieved by our Sign Language Transformers. Our translation networks outperform both sign video to spoken language and gloss to spoken language translation models, in some cases more than doubling the performance (9.58 vs. 21.80 BLEU-4 Score). We also share new baseline translation results using transformer networks for several other text-to-text sign language translation tasks.



### Label-Efficient Learning on Point Clouds using Approximate Convex Decompositions
- **Arxiv ID**: http://arxiv.org/abs/2003.13834v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.13834v2)
- **Published**: 2020-03-30 21:44:43+00:00
- **Updated**: 2020-08-04 21:01:08+00:00
- **Authors**: Matheus Gadelha, Aruni RoyChowdhury, Gopal Sharma, Evangelos Kalogerakis, Liangliang Cao, Erik Learned-Miller, Rui Wang, Subhransu Maji
- **Comment**: First two authors had equal contribution. ECCV'20 version. 19 pages,
  5 figures
- **Journal**: 16th European Conference on Computer Vision (ECCV 2020)
- **Summary**: The problems of shape classification and part segmentation from 3D point clouds have garnered increasing attention in the last few years. Both of these problems, however, suffer from relatively small training sets, creating the need for statistically efficient methods to learn 3D shape representations. In this paper, we investigate the use of Approximate Convex Decompositions (ACD) as a self-supervisory signal for label-efficient learning of point cloud representations. We show that using ACD to approximate ground truth segmentation provides excellent self-supervision for learning 3D point cloud representations that are highly effective on downstream tasks. We report improvements over the state-of-the-art for unsupervised representation learning on the ModelNet40 shape classification dataset and significant gains in few-shot part segmentation on the ShapeNetPart dataset.Code available at https://github.com/matheusgadelha/PointCloudLearningACD



### ActGAN: Flexible and Efficient One-shot Face Reenactment
- **Arxiv ID**: http://arxiv.org/abs/2003.13840v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.13840v1)
- **Published**: 2020-03-30 22:03:16+00:00
- **Updated**: 2020-03-30 22:03:16+00:00
- **Authors**: Ivan Kosarevych, Marian Petruk, Markian Kostiv, Orest Kupyn, Mykola Maksymenko, Volodymyr Budzan
- **Comment**: accepted by IWBF2020
- **Journal**: None
- **Summary**: This paper introduces ActGAN - a novel end-to-end generative adversarial network (GAN) for one-shot face reenactment. Given two images, the goal is to transfer the facial expression of the source actor onto a target person in a photo-realistic fashion. While existing methods require target identity to be predefined, we address this problem by introducing a "many-to-many" approach, which allows arbitrary persons both for source and target without additional retraining. To this end, we employ the Feature Pyramid Network (FPN) as a core generator building block - the first application of FPN in face reenactment, producing finer results. We also introduce a solution to preserve a person's identity between synthesized and target person by adopting the state-of-the-art approach in deep face recognition domain. The architecture readily supports reenactment in different scenarios: "many-to-many", "one-to-one", "one-to-another" in terms of expression accuracy, identity preservation, and overall image quality. We demonstrate that ActGAN achieves competitive performance against recent works concerning visual quality.



### AvatarMe: Realistically Renderable 3D Facial Reconstruction "in-the-wild"
- **Arxiv ID**: http://arxiv.org/abs/2003.13845v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, I.2.10; I.3.7; I.4.1
- **Links**: [PDF](http://arxiv.org/pdf/2003.13845v1)
- **Published**: 2020-03-30 22:17:54+00:00
- **Updated**: 2020-03-30 22:17:54+00:00
- **Authors**: Alexandros Lattas, Stylianos Moschoglou, Baris Gecer, Stylianos Ploumpis, Vasileios Triantafyllou, Abhijeet Ghosh, Stefanos Zafeiriou
- **Comment**: Accepted to CVPR2020. Project page: github.com/lattas/AvatarMe with
  high resolution results, data and more. 10 pages, 9 figures
- **Journal**: None
- **Summary**: Over the last years, with the advent of Generative Adversarial Networks (GANs), many face analysis tasks have accomplished astounding performance, with applications including, but not limited to, face generation and 3D face reconstruction from a single "in-the-wild" image. Nevertheless, to the best of our knowledge, there is no method which can produce high-resolution photorealistic 3D faces from "in-the-wild" images and this can be attributed to the: (a) scarcity of available data for training, and (b) lack of robust methodologies that can successfully be applied on very high-resolution data. In this paper, we introduce AvatarMe, the first method that is able to reconstruct photorealistic 3D faces from a single "in-the-wild" image with an increasing level of detail. To achieve this, we capture a large dataset of facial shape and reflectance and build on a state-of-the-art 3D texture and shape reconstruction method and successively refine its results, while generating the per-pixel diffuse and specular components that are required for realistic rendering. As we demonstrate in a series of qualitative and quantitative experiments, AvatarMe outperforms the existing arts by a significant margin and reconstructs authentic, 4K by 6K-resolution 3D faces from a single low-resolution image that, for the first time, bridges the uncanny valley.



### Can Deep Learning Recognize Subtle Human Activities?
- **Arxiv ID**: http://arxiv.org/abs/2003.13852v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.13852v1)
- **Published**: 2020-03-30 22:45:43+00:00
- **Updated**: 2020-03-30 22:45:43+00:00
- **Authors**: Vincent Jacquot, Zhuofan Ying, Gabriel Kreiman
- **Comment**: poster at CVPR 2020, includes supplementary figures
- **Journal**: None
- **Summary**: Deep Learning has driven recent and exciting progress in computer vision, instilling the belief that these algorithms could solve any visual task. Yet, datasets commonly used to train and test computer vision algorithms have pervasive confounding factors. Such biases make it difficult to truly estimate the performance of those algorithms and how well computer vision models can extrapolate outside the distribution in which they were trained. In this work, we propose a new action classification challenge that is performed well by humans, but poorly by state-of-the-art Deep Learning models. As a proof-of-principle, we consider three exemplary tasks: drinking, reading, and sitting. The best accuracies reached using state-of-the-art computer vision models were 61.7%, 62.8%, and 76.8%, respectively, while human participants scored above 90% accuracy on the three tasks. We propose a rigorous method to reduce confounds when creating datasets, and when comparing human versus computer vision performance. Source code and datasets are publicly available.



### Semi-supervised Learning for Few-shot Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2003.13853v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.13853v2)
- **Published**: 2020-03-30 22:46:49+00:00
- **Updated**: 2020-04-02 09:09:19+00:00
- **Authors**: Yaxing Wang, Salman Khan, Abel Gonzalez-Garcia, Joost van de Weijer, Fahad Shahbaz Khan
- **Comment**: CVPR2020
- **Journal**: None
- **Summary**: In the last few years, unpaired image-to-image translation has witnessed remarkable progress. Although the latest methods are able to generate realistic images, they crucially rely on a large number of labeled images. Recently, some methods have tackled the challenging setting of few-shot image-to-image translation, reducing the labeled data requirements for the target domain during inference. In this work, we go one step further and reduce the amount of required labeled data also from the source domain during training. To do so, we propose applying semi-supervised learning via a noise-tolerant pseudo-labeling procedure. We also apply a cycle consistency constraint to further exploit the information from unlabeled images, either from the same dataset or external. Additionally, we propose several structural modifications to facilitate the image translation task under these circumstances. Our semi-supervised method for few-shot image translation, called SEMIT, achieves excellent results on four different datasets using as little as 10% of the source labels, and matches the performance of the main fully-supervised competitor using only 20% labeled data. Our code and models are made public at: https://github.com/yaxingwang/SEMIT.



### COVID-CT-Dataset: A CT Scan Dataset about COVID-19
- **Arxiv ID**: http://arxiv.org/abs/2003.13865v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.13865v3)
- **Published**: 2020-03-30 23:27:24+00:00
- **Updated**: 2020-06-17 20:14:22+00:00
- **Authors**: Xingyi Yang, Xuehai He, Jinyu Zhao, Yichen Zhang, Shanghang Zhang, Pengtao Xie
- **Comment**: None
- **Journal**: None
- **Summary**: During the outbreak time of COVID-19, computed tomography (CT) is a useful manner for diagnosing COVID-19 patients. Due to privacy issues, publicly available COVID-19 CT datasets are highly difficult to obtain, which hinders the research and development of AI-powered diagnosis methods of COVID-19 based on CTs. To address this issue, we build an open-sourced dataset -- COVID-CT, which contains 349 COVID-19 CT images from 216 patients and 463 non-COVID-19 CTs. The utility of this dataset is confirmed by a senior radiologist who has been diagnosing and treating COVID-19 patients since the outbreak of this pandemic. We also perform experimental studies which further demonstrate that this dataset is useful for developing AI-based diagnosis models of COVID-19. Using this dataset, we develop diagnosis methods based on multi-task learning and self-supervised learning, that achieve an F1 of 0.90, an AUC of 0.98, and an accuracy of 0.89. According to the senior radiologist, models with such performance are good enough for clinical usage. The data and code are available at https://github.com/UCSD-AI4H/COVID-CT



### Dataless Model Selection with the Deep Frame Potential
- **Arxiv ID**: http://arxiv.org/abs/2003.13866v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.13866v1)
- **Published**: 2020-03-30 23:27:25+00:00
- **Updated**: 2020-03-30 23:27:25+00:00
- **Authors**: Calvin Murdock, Simon Lucey
- **Comment**: Oral presentation at the Conference on Computer Vision and Pattern
  Recognition (CVPR), 2020
- **Journal**: None
- **Summary**: Choosing a deep neural network architecture is a fundamental problem in applications that require balancing performance and parameter efficiency. Standard approaches rely on ad-hoc engineering or computationally expensive validation on a specific dataset. We instead attempt to quantify networks by their intrinsic capacity for unique and robust representations, enabling efficient architecture comparisons without requiring any data. Building upon theoretical connections between deep learning and sparse approximation, we propose the deep frame potential: a measure of coherence that is approximately related to representation stability but has minimizers that depend only on network structure. This provides a framework for jointly quantifying the contributions of architectural hyper-parameters such as depth, width, and skip connections. We validate its use as a criterion for model selection and demonstrate correlation with generalization error on a variety of common residual and densely connected network architectures.



### 3D-MPA: Multi Proposal Aggregation for 3D Semantic Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2003.13867v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.13867v1)
- **Published**: 2020-03-30 23:28:50+00:00
- **Updated**: 2020-03-30 23:28:50+00:00
- **Authors**: Francis Engelmann, Martin Bokeloh, Alireza Fathi, Bastian Leibe, Matthias Nießner
- **Comment**: CVPR2020, Video: https://youtu.be/ifL8yTbRFDk Project Page:
  https://www.vision.rwth-aachen.de/3d_instance_segmentation/
- **Journal**: None
- **Summary**: We present 3D-MPA, a method for instance segmentation on 3D point clouds. Given an input point cloud, we propose an object-centric approach where each point votes for its object center. We sample object proposals from the predicted object centers. Then, we learn proposal features from grouped point features that voted for the same object center. A graph convolutional network introduces inter-proposal relations, providing higher-level feature learning in addition to the lower-level point features. Each proposal comprises a semantic label, a set of associated points over which we define a foreground-background mask, an objectness score and aggregation features. Previous works usually perform non-maximum-suppression (NMS) over proposals to obtain the final object detections or semantic instances. However, NMS can discard potentially correct predictions. Instead, our approach keeps all proposals and groups them together based on the learned aggregation features. We show that grouping proposals improves over NMS and outperforms previous state-of-the-art methods on the tasks of 3D object detection and semantic instance segmentation on the ScanNetV2 benchmark and the S3DIS dataset.



### Lesion Conditional Image Generation for Improved Segmentation of Intracranial Hemorrhage from CT Images
- **Arxiv ID**: http://arxiv.org/abs/2003.13868v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.13868v1)
- **Published**: 2020-03-30 23:32:54+00:00
- **Updated**: 2020-03-30 23:32:54+00:00
- **Authors**: Manohar Karki, Junghwan Cho, Seokhwan Ko
- **Comment**: None
- **Journal**: None
- **Summary**: Data augmentation can effectively resolve a scarcity of images when training machine-learning algorithms. It can make them more robust to unseen images. We present a lesion conditional Generative Adversarial Network LcGAN to generate synthetic Computed Tomography (CT) images for data augmentation. A lesion conditional image (segmented mask) is an input to both the generator and the discriminator of the LcGAN during training. The trained model generates contextual CT images based on input masks. We quantify the quality of the images by using a fully convolutional network (FCN) score and blurriness. We also train another classification network to select better synthetic images. These synthetic CT images are then augmented to our hemorrhagic lesion segmentation network. By applying this augmentation method on 2.5%, 10% and 25% of original data, segmentation improved by 12.8%, 6% and 1.6% respectively.



### RetinaTrack: Online Single Stage Joint Detection and Tracking
- **Arxiv ID**: http://arxiv.org/abs/2003.13870v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.13870v1)
- **Published**: 2020-03-30 23:46:29+00:00
- **Updated**: 2020-03-30 23:46:29+00:00
- **Authors**: Zhichao Lu, Vivek Rathod, Ronny Votel, Jonathan Huang
- **Comment**: Accepted to CVPR 2020
- **Journal**: None
- **Summary**: Traditionally multi-object tracking and object detection are performed using separate systems with most prior works focusing exclusively on one of these aspects over the other. Tracking systems clearly benefit from having access to accurate detections, however and there is ample evidence in literature that detectors can benefit from tracking which, for example, can help to smooth predictions over time. In this paper we focus on the tracking-by-detection paradigm for autonomous driving where both tasks are mission critical. We propose a conceptually simple and efficient joint model of detection and tracking, called RetinaTrack, which modifies the popular single stage RetinaNet approach such that it is amenable to instance-level embedding training. We show, via evaluations on the Waymo Open Dataset, that we outperform a recent state of the art tracking algorithm while requiring significantly less computation. We believe that our simple yet effective approach can serve as a strong baseline for future work in this area.



### A Low-cost Fault Corrector for Deep Neural Networks through Range Restriction
- **Arxiv ID**: http://arxiv.org/abs/2003.13874v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.13874v4)
- **Published**: 2020-03-30 23:53:55+00:00
- **Updated**: 2021-03-29 01:47:53+00:00
- **Authors**: Zitao Chen, Guanpeng Li, Karthik Pattabiraman
- **Comment**: 13 pages, 12 figures
- **Journal**: None
- **Summary**: The adoption of deep neural networks (DNNs) in safety-critical domains has engendered serious reliability concerns. A prominent example is hardware transient faults that are growing in frequency due to the progressive technology scaling, and can lead to failures in DNNs.   This work proposes Ranger, a low-cost fault corrector, which directly rectifies the faulty output due to transient faults without re-computation. DNNs are inherently resilient to benign faults (which will not cause output corruption), but not to critical faults (which can result in erroneous output). Ranger is an automated transformation to selectively restrict the value ranges in DNNs, which reduces the large deviations caused by critical faults and transforms them to benign faults that can be tolerated by the inherent resilience of the DNNs. Our evaluation on 8 DNNs demonstrates Ranger significantly increases the error resilience of the DNNs (by 3x to 50x), with no loss in accuracy, and with negligible overheads.



