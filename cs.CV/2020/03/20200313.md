# Arxiv Papers in cs.CV on 2020-03-13
### An Adversarial Objective for Scalable Exploration
- **Arxiv ID**: http://arxiv.org/abs/2003.06082v4
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.06082v4)
- **Published**: 2020-03-13 02:03:05+00:00
- **Updated**: 2020-11-11 18:39:43+00:00
- **Authors**: Bernadette Bucher, Karl Schmeckpeper, Nikolai Matni, Kostas Daniilidis
- **Comment**: Additional visualizations of our results are available on our website
  at https://sites.google.com/view/action-for-better-prediction . Bernadette
  Bucher and Karl Schmeckpeper contributed equally
- **Journal**: None
- **Summary**: Model-based curiosity combines active learning approaches to optimal sampling with the information gain based incentives for exploration presented in the curiosity literature. Existing model-based curiosity methods look to approximate prediction uncertainty with approaches which struggle to scale to many prediction-planning pipelines used in robotics tasks. We address these scalability issues with an adversarial curiosity method minimizing a score given by a discriminator network. This discriminator is optimized jointly with a prediction model and enables our active learning approach to sample sequences of observations and actions which result in predictions considered the least realistic by the discriminator. We demonstrate progressively increasing advantages as compute is restricted of our adversarial curiosity approach over leading model-based exploration strategies in simulated environments. We further demonstrate the ability of our adversarial curiosity method to scale to a robotic manipulation prediction-planning pipeline where we improve sample efficiency and prediction performance for a domain transfer problem.



### BigGAN-based Bayesian reconstruction of natural images from human brain activity
- **Arxiv ID**: http://arxiv.org/abs/2003.06105v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2003.06105v1)
- **Published**: 2020-03-13 04:32:11+00:00
- **Updated**: 2020-03-13 04:32:11+00:00
- **Authors**: Kai Qiao, Jian Chen, Linyuan Wang, Chi Zhang, Li Tong, Bin Yan
- **Comment**: brain decoding; visual reconstruction; generative adversarial network
  (GAN); Bayesian framework. under review in Neuroscience of Elsevier
- **Journal**: None
- **Summary**: In the visual decoding domain, visually reconstructing presented images given the corresponding human brain activity monitored by functional magnetic resonance imaging (fMRI) is difficult, especially when reconstructing viewed natural images. Visual reconstruction is a conditional image generation on fMRI data and thus generative adversarial network (GAN) for natural image generation is recently introduced for this task. Although GAN-based methods have greatly improved, the fidelity and naturalness of reconstruction are still unsatisfactory due to the small number of fMRI data samples and the instability of GAN training. In this study, we proposed a new GAN-based Bayesian visual reconstruction method (GAN-BVRM) that includes a classifier to decode categories from fMRI data, a pre-trained conditional generator to generate natural images of specified categories, and a set of encoding models and evaluator to evaluate generated images. GAN-BVRM employs the pre-trained generator of the prevailing BigGAN to generate masses of natural images, and selects the images that best matches with the corresponding brain activity through the encoding models as the reconstruction of the image stimuli. In this process, the semantic and detailed contents of reconstruction are controlled by decoded categories and encoding models, respectively. GAN-BVRM used the Bayesian manner to avoid contradiction between naturalness and fidelity from current GAN-based methods and thus can improve the advantages of GAN. Experimental results revealed that GAN-BVRM improves the fidelity and naturalness, that is, the reconstruction is natural and similar to the presented image stimuli.



### A Spatial-Temporal Attentive Network with Spatial Continuity for Trajectory Prediction
- **Arxiv ID**: http://arxiv.org/abs/2003.06107v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.06107v3)
- **Published**: 2020-03-13 04:35:50+00:00
- **Updated**: 2021-10-14 11:56:46+00:00
- **Authors**: Beihao Xia, Conghao Wang, Qinmu Peng, Xinge You, Dacheng Tao
- **Comment**: bad model settings and unclear results
- **Journal**: None
- **Summary**: It remains challenging to automatically predict the multi-agent trajectory due to multiple interactions including agent to agent interaction and scene to agent interaction. Although recent methods have achieved promising performance, most of them just consider spatial influence of the interactions and ignore the fact that temporal influence always accompanies spatial influence. Moreover, those methods based on scene information always require extra segmented scene images to generate multiple socially acceptable trajectories. To solve these limitations, we propose a novel model named spatial-temporal attentive network with spatial continuity (STAN-SC). First, spatial-temporal attention mechanism is presented to explore the most useful and important information. Second, we conduct a joint feature sequence based on the sequence and instant state information to make the generative trajectories keep spatial continuity. Experiments are performed on the two widely used ETH-UCY datasets and demonstrate that the proposed model achieves state-of-the-art prediction accuracy and handles more complex scenarios.



### A High-Performance Object Proposals based on Horizontal High Frequency Signal
- **Arxiv ID**: http://arxiv.org/abs/2003.06124v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.06124v2)
- **Published**: 2020-03-13 05:41:17+00:00
- **Updated**: 2020-05-14 03:12:32+00:00
- **Authors**: Jiang Chao, Liang Huawei, Wang Zhiling
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, the use of object proposal as a preprocessing step for target detection to improve computational efficiency has become an effective method. Good object proposal methods should have high object detection recall rate and low computational cost, as well as good localization quality and repeatability. However, it is difficult for current advanced algorithms to achieve a good balance in the above performance. For this problem, we propose a class-independent object proposal algorithm BIHL. It combines the advantages of window scoring and superpixel merging, which not only improves the localization quality but also speeds up the computational efficiency. The experimental results on the VOC2007 data set show that when the IOU is 0.5 and 10,000 budget proposals, our method can achieve the highest detection recall and an mean average best overlap of 79.5%, and the computational efficiency is nearly three times faster than the current fastest method. Moreover, our method is the method with the highest average repeatability among the methods that achieve good repeatability to various disturbances.



### Semantic Segmentation of highly class imbalanced fully labelled 3D volumetric biomedical images and unsupervised Domain Adaptation of the pre-trained Segmentation Network to segment another fully unlabelled Biomedical 3D Image stack
- **Arxiv ID**: http://arxiv.org/abs/2004.02748v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.02748v1)
- **Published**: 2020-03-13 06:01:18+00:00
- **Updated**: 2020-03-13 06:01:18+00:00
- **Authors**: Shreya Roy, Anirban Chakraborty
- **Comment**: 6 pages and 6 figures. Submitting to ICPR2020
- **Journal**: None
- **Summary**: The goal of our work is to perform pixel label semantic segmentation on 3D biomedical volumetric data. Manual annotation is always difficult for a large bio-medical dataset. So, we consider two cases where one dataset is fully labeled and the other dataset is assumed to be fully unlabelled. We first perform Semantic Segmentation on the fully labeled isotropic biomedical source data (FIBSEM) and try to incorporate the the trained model for segmenting the target unlabelled dataset(SNEMI3D)which shares some similarities with the source dataset in the context of different types of cellular bodies and other cellular components. Although, the cellular components vary in size and shape. So in this paper, we have proposed a novel approach in the context of unsupervised domain adaptation while classifying each pixel of the target volumetric data into cell boundary and cell body. Also, we have proposed a novel approach to giving non-uniform weights to different pixels in the training images while performing the pixel-level semantic segmentation in the presence of the corresponding pixel-wise label map along with the training original images in the source domain. We have used the Entropy Map or a Distance Transform matrix retrieved from the given ground truth label map which has helped to overcome the class imbalance problem in the medical image data where the cell boundaries are extremely thin and hence, extremely prone to be misclassified as non-boundary.



### Dual Temporal Memory Network for Efficient Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2003.06125v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.06125v1)
- **Published**: 2020-03-13 06:07:45+00:00
- **Updated**: 2020-03-13 06:07:45+00:00
- **Authors**: Kaihua Zhang, Long Wang, Dong Liu, Bo Liu, Qingshan Liu, Zhu Li
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: Video Object Segmentation (VOS) is typically formulated in a semi-supervised setting. Given the ground-truth segmentation mask on the first frame, the task of VOS is to track and segment the single or multiple objects of interests in the rest frames of the video at the pixel level. One of the fundamental challenges in VOS is how to make the most use of the temporal information to boost the performance. We present an end-to-end network which stores short- and long-term video sequence information preceding the current frame as the temporal memories to address the temporal modeling in VOS. Our network consists of two temporal sub-networks including a short-term memory sub-network and a long-term memory sub-network. The short-term memory sub-network models the fine-grained spatial-temporal interactions between local regions across neighboring frames in video via a graph-based learning framework, which can well preserve the visual consistency of local regions over time. The long-term memory sub-network models the long-range evolution of object via a Simplified-Gated Recurrent Unit (S-GRU), making the segmentation be robust against occlusions and drift errors. In our experiments, we show that our proposed method achieves a favorable and competitive performance on three frequently-used VOS datasets, including DAVIS 2016, DAVIS 2017 and Youtube-VOS in terms of both speed and accuracy.



### LIBRE: The Multiple 3D LiDAR Dataset
- **Arxiv ID**: http://arxiv.org/abs/2003.06129v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.06129v2)
- **Published**: 2020-03-13 06:17:39+00:00
- **Updated**: 2020-06-24 17:00:54+00:00
- **Authors**: Alexander Carballo, Jacob Lambert, Abraham Monrroy-Cano, David Robert Wong, Patiphon Narksri, Yuki Kitsukawa, Eijiro Takeuchi, Shinpei Kato, Kazuya Takeda
- **Comment**: Accepted for oral presentation at IEEE Intelligent Vehicles Symposium
  (IV2020), https://2020.ieee-iv.org/ LIBRE dataset available at
  https://sites.google.com/g.sp.m.is.nagoya-u.ac.jp/libre-dataset/ Reference
  video available at https://youtu.be/rWyecoCtKcQ
- **Journal**: None
- **Summary**: In this work, we present LIBRE: LiDAR Benchmarking and Reference, a first-of-its-kind dataset featuring 10 different LiDAR sensors, covering a range of manufacturers, models, and laser configurations. Data captured independently from each sensor includes three different environments and configurations: static targets, where objects were placed at known distances and measured from a fixed position within a controlled environment; adverse weather, where static obstacles were measured from a moving vehicle, captured in a weather chamber where LiDARs were exposed to different conditions (fog, rain, strong light); and finally, dynamic traffic, where dynamic objects were captured from a vehicle driven on public urban roads, multiple times at different times of the day, and including supporting sensors such as cameras, infrared imaging, and odometry devices. LIBRE will contribute to the research community to (1) provide a means for a fair comparison of currently available LiDARs, and (2) facilitate the improvement of existing self-driving vehicles and robotics-related software, in terms of development and tuning of LiDAR-based perception algorithms.



### Partial Weight Adaptation for Robust DNN Inference
- **Arxiv ID**: http://arxiv.org/abs/2003.06131v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.06131v1)
- **Published**: 2020-03-13 06:25:45+00:00
- **Updated**: 2020-03-13 06:25:45+00:00
- **Authors**: Xiufeng Xie, Kyu-Han Kim
- **Comment**: To appear in CVPR 2020
- **Journal**: None
- **Summary**: Mainstream video analytics uses a pre-trained DNN model with an assumption that inference input and training data follow the same probability distribution. However, this assumption does not always hold in the wild: autonomous vehicles may capture video with varying brightness; unstable wireless bandwidth calls for adaptive bitrate streaming of video; and, inference servers may serve inputs from heterogeneous IoT devices/cameras. In such situations, the level of input distortion changes rapidly, thus reshaping the probability distribution of the input.   We present GearNN, an adaptive inference architecture that accommodates heterogeneous DNN inputs. GearNN employs an optimization algorithm to identify a small set of "distortion-sensitive" DNN parameters, given a memory budget. Based on the distortion level of the input, GearNN then adapts only the distortion-sensitive parameters, while reusing the rest of constant parameters across all input qualities. In our evaluation of DNN inference with dynamic input distortions, GearNN improves the accuracy (mIoU) by an average of 18.12% over a DNN trained with the undistorted dataset and 4.84% over stability training from Google, with only 1.8% extra memory overhead.



### Is There Tradeoff between Spatial and Temporal in Video Super-Resolution?
- **Arxiv ID**: http://arxiv.org/abs/2003.06141v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.06141v1)
- **Published**: 2020-03-13 07:49:05+00:00
- **Updated**: 2020-03-13 07:49:05+00:00
- **Authors**: Haochen Zhang, Dong Liu, Zhiwei Xiong
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances of deep learning lead to great success of image and video super-resolution (SR) methods that are based on convolutional neural networks (CNN). For video SR, advanced algorithms have been proposed to exploit the temporal correlation between low-resolution (LR) video frames, and/or to super-resolve a frame with multiple LR frames. These methods pursue higher quality of super-resolved frames, where the quality is usually measured frame by frame in e.g. PSNR. However, frame-wise quality may not reveal the consistency between frames. If an algorithm is applied to each frame independently (which is the case of most previous methods), the algorithm may cause temporal inconsistency, which can be observed as flickering. It is a natural requirement to improve both frame-wise fidelity and between-frame consistency, which are termed spatial quality and temporal quality, respectively. Then we may ask, is a method optimized for spatial quality also optimized for temporal quality? Can we optimize the two quality metrics jointly?



### PointINS: Point-based Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2003.06148v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.06148v2)
- **Published**: 2020-03-13 08:24:58+00:00
- **Updated**: 2021-07-01 09:07:06+00:00
- **Authors**: Lu Qi, Yi Wang, Yukang Chen, Yingcong Chen, Xiangyu Zhang, Jian Sun, Jiaya Jia
- **Comment**: Accepted by IEEE Transactions on Pattern Analysis and Machine
  Intelligence
- **Journal**: None
- **Summary**: In this paper, we explore the mask representation in instance segmentation with Point-of-Interest (PoI) features. Differentiating multiple potential instances within a single PoI feature is challenging because learning a high-dimensional mask feature for each instance using vanilla convolution demands a heavy computing burden. To address this challenge, we propose an instance-aware convolution. It decomposes this mask representation learning task into two tractable modules as instance-aware weights and instance-agnostic features. The former is to parametrize convolution for producing mask features corresponding to different instances, improving mask learning efficiency by avoiding employing several independent convolutions. Meanwhile, the latter serves as mask templates in a single point. Together, instance-aware mask features are computed by convolving the template with dynamic weights, used for the mask prediction. Along with instance-aware convolution, we propose PointINS, a simple and practical instance segmentation approach, building upon dense one-stage detectors. Through extensive experiments, we evaluated the effectiveness of our framework built upon RetinaNet and FCOS. PointINS in ResNet101 backbone achieves a 38.3 mask mean average precision (mAP) on COCO dataset, outperforming existing point-based methods by a large margin. It gives a comparable performance to the region-based Mask R-CNN with faster inference.



### Gimme Signals: Discriminative signal encoding for multimodal activity recognition
- **Arxiv ID**: http://arxiv.org/abs/2003.06156v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2003.06156v2)
- **Published**: 2020-03-13 08:58:15+00:00
- **Updated**: 2020-04-09 13:10:04+00:00
- **Authors**: Raphael Memmesheimer, Nick Theisen, Dietrich Paulus
- **Comment**: 8 pages, 4 figures, 4 tables
- **Journal**: None
- **Summary**: We present a simple, yet effective and flexible method for action recognition supporting multiple sensor modalities. Multivariate signal sequences are encoded in an image and are then classified using a recently proposed EfficientNet CNN architecture. Our focus was to find an approach that generalizes well across different sensor modalities without specific adaptions while still achieving good results. We apply our method to 4 action recognition datasets containing skeleton sequences, inertial and motion capturing measurements as well as \wifi fingerprints that range up to 120 action classes. Our method defines the current best CNN-based approach on the NTU RGB+D 120 dataset, lifts the state of the art on the ARIL Wi-Fi dataset by +6.78%, improves the UTD-MHAD inertial baseline by +14.4%, the UTD-MHAD skeleton baseline by 1.13% and achieves 96.11% on the Simitate motion capturing data (80/20 split). We further demonstrate experiments on both, modality fusion on a signal level and signal reduction to prevent the representation from overloading.



### Random smooth gray value transformations for cross modality learning with gray value invariant networks
- **Arxiv ID**: http://arxiv.org/abs/2003.06158v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.06158v1)
- **Published**: 2020-03-13 09:01:08+00:00
- **Updated**: 2020-03-13 09:01:08+00:00
- **Authors**: Nikolas Lessmann, Bram van Ginneken
- **Comment**: None
- **Journal**: None
- **Summary**: Random transformations are commonly used for augmentation of the training data with the goal of reducing the uniformity of the training samples. These transformations normally aim at variations that can be expected in images from the same modality. Here, we propose a simple method for transforming the gray values of an image with the goal of reducing cross modality differences. This approach enables segmentation of the lumbar vertebral bodies in CT images using a network trained exclusively with MR images. The source code is made available at https://github.com/nlessmann/rsgt



### Adaptive Graph Convolutional Network with Attention Graph Clustering for Co-saliency Detection
- **Arxiv ID**: http://arxiv.org/abs/2003.06167v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.06167v1)
- **Published**: 2020-03-13 09:35:59+00:00
- **Updated**: 2020-03-13 09:35:59+00:00
- **Authors**: Kaihua Zhang, Tengpeng Li, Shiwen Shen, Bo Liu, Jin Chen, Qingshan Liu
- **Comment**: CVPR2020
- **Journal**: None
- **Summary**: Co-saliency detection aims to discover the common and salient foregrounds from a group of relevant images. For this task, we present a novel adaptive graph convolutional network with attention graph clustering (GCAGC). Three major contributions have been made, and are experimentally shown to have substantial practical merits. First, we propose a graph convolutional network design to extract information cues to characterize the intra- and interimage correspondence. Second, we develop an attention graph clustering algorithm to discriminate the common objects from all the salient foreground objects in an unsupervised fashion. Third, we present a unified framework with encoder-decoder structure to jointly train and optimize the graph convolutional network, attention graph cluster, and co-saliency detection decoder in an end-to-end manner. We evaluate our proposed GCAGC method on three cosaliency detection benchmark datasets (iCoseg, Cosal2015 and COCO-SEG). Our GCAGC method obtains significant improvements over the state-of-the-arts on most of them.



### Towards a Framework for Visual Intelligence in Service Robotics: Epistemic Requirements and Gap Analysis
- **Arxiv ID**: http://arxiv.org/abs/2003.06171v1
- **DOI**: 10.24963/kr.2020/93
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.06171v1)
- **Published**: 2020-03-13 09:41:05+00:00
- **Updated**: 2020-03-13 09:41:05+00:00
- **Authors**: Agnese Chiatti, Enrico Motta, Enrico Daga
- **Comment**: None
- **Journal**: In Proceedings of the 17th International Conference on Principles
  of Knowledge Representation and Reasoning (KR 2020), Special Session on KR
  and Robotics
- **Summary**: A key capability required by service robots operating in real-world, dynamic environments is that of Visual Intelligence, i.e., the ability to use their vision system, reasoning components and background knowledge to make sense of their environment. In this paper, we analyze the epistemic requirements for Visual Intelligence, both in a top-down fashion, using existing frameworks for human-like Visual Intelligence in the literature, and from the bottom up, based on the errors emerging from object recognition trials in a real-world robotic scenario. Finally, we use these requirements to evaluate current knowledge bases for Service Robotics and to identify gaps in the support they provide for Visual Intelligence. These gaps provide the basis of a research agenda for developing more effective knowledge representations for Visual Intelligence.



### Joint COCO and Mapillary Workshop at ICCV 2019 Keypoint Detection Challenge Track Technical Report: Distribution-Aware Coordinate Representation for Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2003.07232v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.07232v1)
- **Published**: 2020-03-13 10:22:36+00:00
- **Updated**: 2020-03-13 10:22:36+00:00
- **Authors**: Hanbin Dai, Liangbo Zhou, Feng Zhang, Zhengyu Zhang, Hong Hu, Xiatian Zhu, Mao Ye
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1910.06278
- **Journal**: None
- **Summary**: In this paper, we focus on the coordinate representation in human pose estimation. While being the standard choice, heatmap based representation has not been systematically investigated. We found that the process of coordinate decoding (i.e. transforming the predicted heatmaps to the coordinates) is surprisingly significant for human pose estimation performance, which nevertheless was not recognised before. In light of the discovered importance, we further probe the design limitations of the standard coordinate decoding method and propose a principled distribution-aware decoding method. Meanwhile, we improve the standard coordinate encoding process (i.e. transforming ground-truth coordinates to heatmaps) by generating accurate heatmap distributions for unbiased model training. Taking them together, we formulate a novel Distribution-Aware coordinate Representation for Keypoint (DARK) method. Serving as a model-agnostic plug-in, DARK significantly improves the performance of a variety of state-of-the-art human pose estimation models. Extensive experiments show that DARK yields the best results on COCO keypoint detection challenge, validating the usefulness and effectiveness of our novel coordinate representation idea. The project page containing more details is at https://ilovepose.github.io/coco



### On the effectiveness of convolutional autoencoders on image-based personalized recommender systems
- **Arxiv ID**: http://arxiv.org/abs/2003.06205v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.IR, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.06205v1)
- **Published**: 2020-03-13 11:19:02+00:00
- **Updated**: 2020-03-13 11:19:02+00:00
- **Authors**: E. Blanco-Mallo, B. Remeseiro, V. Bolón-Canedo, A. Alonso-Betanzos
- **Comment**: None
- **Journal**: None
- **Summary**: Recommender systems (RS) are increasingly present in our daily lives, especially since the advent of Big Data, which allows for storing all kinds of information about users' preferences. Personalized RS are successfully applied in platforms such as Netflix, Amazon or YouTube. However, they are missing in gastronomic platforms such as TripAdvisor, where moreover we can find millions of images tagged with users' tastes. This paper explores the potential of using those images as sources of information for modeling users' tastes and proposes an image-based classification system to obtain personalized recommendations, using a convolutional autoencoder as feature extractor. The proposed architecture will be applied to TripAdvisor data, using users' reviews that can be defined as a triad composed by a user, a restaurant, and an image of it taken by the user. Since the dataset is highly unbalanced, the use of data augmentation on the minority class is also considered in the experimentation. Results on data from three cities of different sizes (Santiago de Compostela, Barcelona and New York) demonstrate the effectiveness of using a convolutional autoencoder as feature extractor, instead of the standard deep features computed with convolutional neural networks.



### Pyramidal Edge-maps and Attention based Guided Thermal Super-resolution
- **Arxiv ID**: http://arxiv.org/abs/2003.06216v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.06216v2)
- **Published**: 2020-03-13 12:11:26+00:00
- **Updated**: 2020-09-30 05:50:14+00:00
- **Authors**: Honey Gupta, Kaushik Mitra
- **Comment**: None
- **Journal**: None
- **Summary**: Guided super-resolution (GSR) of thermal images using visible range images is challenging because of the difference in the spectral-range between the images. This in turn means that there is significant texture-mismatch between the images, which manifests as blur and ghosting artifacts in the super-resolved thermal image. To tackle this, we propose a novel algorithm for GSR based on pyramidal edge-maps extracted from the visible image. Our proposed network has two sub-networks. The first sub-network super-resolves the low-resolution thermal image while the second obtains edge-maps from the visible image at a growing perceptual scale and integrates them into the super-resolution sub-network with the help of attention-based fusion. Extraction and integration of multi-level edges allows the super-resolution network to process texture-to-object level information progressively, enabling more straightforward identification of overlapping edges between the input images. Extensive experiments show that our model outperforms the state-of-the-art GSR methods, both quantitatively and qualitatively.



### Semantic Pyramid for Image Generation
- **Arxiv ID**: http://arxiv.org/abs/2003.06221v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.06221v2)
- **Published**: 2020-03-13 12:23:37+00:00
- **Updated**: 2020-03-16 14:31:49+00:00
- **Authors**: Assaf Shocher, Yossi Gandelsman, Inbar Mosseri, Michal Yarom, Michal Irani, William T. Freeman, Tali Dekel
- **Comment**: None
- **Journal**: IEEE Conference on Computer Vision and Pattern Recognition, 2020.
  CVPR 2020
- **Summary**: We present a novel GAN-based model that utilizes the space of deep features learned by a pre-trained classification model. Inspired by classical image pyramid representations, we construct our model as a Semantic Generation Pyramid -- a hierarchical framework which leverages the continuum of semantic information encapsulated in such deep features; this ranges from low level information contained in fine features to high level, semantic information contained in deeper features. More specifically, given a set of features extracted from a reference image, our model generates diverse image samples, each with matching features at each semantic level of the classification model. We demonstrate that our model results in a versatile and flexible framework that can be used in various classic and novel image generation tasks. These include: generating images with a controllable extent of semantic similarity to a reference image, and different manipulation tasks such as semantically-controlled inpainting and compositing; all achieved with the same model, with no further training.



### Fusion-Aware Point Convolution for Online Semantic 3D Scene Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2003.06233v4
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.06233v4)
- **Published**: 2020-03-13 12:32:24+00:00
- **Updated**: 2022-01-13 13:05:34+00:00
- **Authors**: Jiazhao Zhang, Chenyang Zhu, Lintao Zheng, Kai Xu
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: Online semantic 3D segmentation in company with real-time RGB-D reconstruction poses special challenges such as how to perform 3D convolution directly over the progressively fused 3D geometric data, and how to smartly fuse information from frame to frame. We propose a novel fusion-aware 3D point convolution which operates directly on the geometric surface being reconstructed and exploits effectively the inter-frame correlation for high quality 3D feature learning. This is enabled by a dedicated dynamic data structure which organizes the online acquired point cloud with global-local trees. Globally, we compile the online reconstructed 3D points into an incrementally growing coordinate interval tree, enabling fast point insertion and neighborhood query. Locally, we maintain the neighborhood information for each point using an octree whose construction benefits from the fast query of the global tree.Both levels of trees update dynamically and help the 3D convolution effectively exploits the temporal coherence for effective information fusion across RGB-D frames.



### What Information Does a ResNet Compress?
- **Arxiv ID**: http://arxiv.org/abs/2003.06254v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.06254v1)
- **Published**: 2020-03-13 13:02:11+00:00
- **Updated**: 2020-03-13 13:02:11+00:00
- **Authors**: Luke Nicholas Darlow, Amos Storkey
- **Comment**: 10 pages + appendices; submitted to ICLR 2019
- **Journal**: None
- **Summary**: The information bottleneck principle (Shwartz-Ziv & Tishby, 2017) suggests that SGD-based training of deep neural networks results in optimally compressed hidden layers, from an information theoretic perspective. However, this claim was established on toy data. The goal of the work we present here is to test whether the information bottleneck principle is applicable to a realistic setting using a larger and deeper convolutional architecture, a ResNet model. We trained PixelCNN++ models as inverse representation decoders to measure the mutual information between hidden layers of a ResNet and input image data, when trained for (1) classification and (2) autoencoding. We find that two stages of learning happen for both training regimes, and that compression does occur, even for an autoencoder. Sampling images by conditioning on hidden layers' activations offers an intuitive visualisation to understand what a ResNets learns to forget.



### Belief Propagation Reloaded: Learning BP-Layers for Labeling Problems
- **Arxiv ID**: http://arxiv.org/abs/2003.06258v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.06258v1)
- **Published**: 2020-03-13 13:11:35+00:00
- **Updated**: 2020-03-13 13:11:35+00:00
- **Authors**: Patrick Knöbelreiter, Christian Sormann, Alexander Shekhovtsov, Friedrich Fraundorfer, Thomas Pock
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: It has been proposed by many researchers that combining deep neural networks with graphical models can create more efficient and better regularized composite models. The main difficulties in implementing this in practice are associated with a discrepancy in suitable learning objectives as well as with the necessity of approximations for the inference. In this work we take one of the simplest inference methods, a truncated max-product Belief Propagation, and add what is necessary to make it a proper component of a deep learning model: We connect it to learning formulations with losses on marginals and compute the backprop operation. This BP-Layer can be used as the final or an intermediate block in convolutional neural networks (CNNs), allowing us to design a hierarchical model composing BP inference and CNNs at different scale levels. The model is applicable to a range of dense prediction problems, is well-trainable and provides parameter-efficient and robust solutions in stereo, optical flow and semantic segmentation.



### Neural Network Tracking of Moving Objects with Unknown Equations of Motion
- **Arxiv ID**: http://arxiv.org/abs/2003.08362v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.08362v1)
- **Published**: 2020-03-13 13:16:14+00:00
- **Updated**: 2020-03-13 13:16:14+00:00
- **Authors**: Boaz Fish, Ben Zion Bobrovsky
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we present a Neural Network design that can be used to track the location of a moving object within a given range based on the object's noisy coordinates measurement. A function commonly performed by the KLMn filter, our goal is to show that our method outperforms the Kalman filter in certain scenarios.



### Automatic Lesion Detection System (ALDS) for Skin Cancer Classification Using SVM and Neural Classifiers
- **Arxiv ID**: http://arxiv.org/abs/2003.06276v1
- **DOI**: 10.1109/BIBE.2016.53
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.06276v1)
- **Published**: 2020-03-13 13:31:35+00:00
- **Updated**: 2020-03-13 13:31:35+00:00
- **Authors**: Muhammad Ali Farooq, Muhammad Aatif Mobeen Azhar, Rana Hammad Raza
- **Comment**: None
- **Journal**: None
- **Summary**: Technology aided platforms provide reliable tools in almost every field these days. These tools being supported by computational power are significant for applications that need sensitive and precise data analysis. One such important application in the medical field is Automatic Lesion Detection System (ALDS) for skin cancer classification. Computer aided diagnosis helps physicians and dermatologists to obtain a second opinion for proper analysis and treatment of skin cancer. Precise segmentation of the cancerous mole along with surrounding area is essential for proper analysis and diagnosis. This paper is focused towards the development of improved ALDS framework based on probabilistic approach that initially utilizes active contours and watershed merged mask for segmenting out the mole and later SVM and Neural Classifier are applied for the classification of the segmented mole. After lesion segmentation, the selected features are classified to ascertain that whether the case under consideration is melanoma or non-melanoma. The approach is tested for varying datasets and comparative analysis is performed that reflects the effectiveness of the proposed system.



### Harmonizing Transferability and Discriminability for Adapting Object Detectors
- **Arxiv ID**: http://arxiv.org/abs/2003.06297v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.06297v1)
- **Published**: 2020-03-13 13:47:48+00:00
- **Updated**: 2020-03-13 13:47:48+00:00
- **Authors**: Chaoqi Chen, Zebiao Zheng, Xinghao Ding, Yue Huang, Qi Dou
- **Comment**: Accepted by CVPR 2020
- **Journal**: None
- **Summary**: Recent advances in adaptive object detection have achieved compelling results in virtue of adversarial feature adaptation to mitigate the distributional shifts along the detection pipeline. Whilst adversarial adaptation significantly enhances the transferability of feature representations, the feature discriminability of object detectors remains less investigated. Moreover, transferability and discriminability may come at a contradiction in adversarial adaptation given the complex combinations of objects and the differentiated scene layouts between domains. In this paper, we propose a Hierarchical Transferability Calibration Network (HTCN) that hierarchically (local-region/image/instance) calibrates the transferability of feature representations for harmonizing transferability and discriminability. The proposed model consists of three components: (1) Importance Weighted Adversarial Training with input Interpolation (IWAT-I), which strengthens the global discriminability by re-weighting the interpolated image-level features; (2) Context-aware Instance-Level Alignment (CILA) module, which enhances the local discriminability by capturing the underlying complementary effect between the instance-level feature and the global context information for the instance-level feature alignment; (3) local feature masks that calibrate the local transferability to provide semantic guidance for the following discriminative pattern alignment. Experimental results show that HTCN significantly outperforms the state-of-the-art methods on benchmark datasets.



### DHOG: Deep Hierarchical Object Grouping
- **Arxiv ID**: http://arxiv.org/abs/2003.08821v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.08821v1)
- **Published**: 2020-03-13 14:11:48+00:00
- **Updated**: 2020-03-13 14:11:48+00:00
- **Authors**: Luke Nicholas Darlow, Amos Storkey
- **Comment**: 15 pages, submitted to ECCV 2020
- **Journal**: None
- **Summary**: Recently, a number of competitive methods have tackled unsupervised representation learning by maximising the mutual information between the representations produced from augmentations. The resulting representations are then invariant to stochastic augmentation strategies, and can be used for downstream tasks such as clustering or classification. Yet data augmentations preserve many properties of an image and so there is potential for a suboptimal choice of representation that relies on matching easy-to-find features in the data. We demonstrate that greedy or local methods of maximising mutual information (such as stochastic gradient optimisation) discover local optima of the mutual information criterion; the resulting representations are also less-ideally suited to complex downstream tasks. Earlier work has not specifically identified or addressed this issue. We introduce deep hierarchical object grouping (DHOG) that computes a number of distinct discrete representations of images in a hierarchical order, eventually generating representations that better optimise the mutual information objective. We also find that these representations align better with the downstream task of grouping into underlying object classes. We tested DHOG on unsupervised clustering, which is a natural downstream test as the target representation is a discrete labelling of the data. We achieved new state-of-the-art results on the three main benchmarks without any prefiltering or Sobel-edge detection that proved necessary for many previous methods to work. We obtain accuracy improvements of: 4.3% on CIFAR-10, 1.5% on CIFAR-100-20, and 7.2% on SVHN.



### Extending Maps with Semantic and Contextual Object Information for Robot Navigation: a Learning-Based Framework using Visual and Depth Cues
- **Arxiv ID**: http://arxiv.org/abs/2003.06336v1
- **DOI**: 10.1007/s10846-019-01136-5
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2003.06336v1)
- **Published**: 2020-03-13 15:05:23+00:00
- **Updated**: 2020-03-13 15:05:23+00:00
- **Authors**: Renato Martins, Dhiego Bersan, Mario F. M. Campos, Erickson R. Nascimento
- **Comment**: Preprint version of the article to appear at Journal of Intelligent &
  Robotic Systems (2020)
- **Journal**: None
- **Summary**: This paper addresses the problem of building augmented metric representations of scenes with semantic information from RGB-D images. We propose a complete framework to create an enhanced map representation of the environment with object-level information to be used in several applications such as human-robot interaction, assistive robotics, visual navigation, or in manipulation tasks. Our formulation leverages a CNN-based object detector (Yolo) with a 3D model-based segmentation technique to perform instance semantic segmentation, and to localize, identify, and track different classes of objects in the scene. The tracking and positioning of semantic classes is done with a dictionary of Kalman filters in order to combine sensor measurements over time and then providing more accurate maps. The formulation is designed to identify and to disregard dynamic objects in order to obtain a medium-term invariant map representation. The proposed method was evaluated with collected and publicly available RGB-D data sequences acquired in different indoor scenes. Experimental results show the potential of the technique to produce augmented semantic maps containing several objects (notably doors). We also provide to the community a dataset composed of annotated object classes (doors, fire extinguishers, benches, water fountains) and their positioning, as well as the source code as ROS packages.



### Advanced Deep Learning Methodologies for Skin Cancer Classification in Prodromal Stages
- **Arxiv ID**: http://arxiv.org/abs/2003.06356v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.06356v1)
- **Published**: 2020-03-13 16:07:00+00:00
- **Updated**: 2020-03-13 16:07:00+00:00
- **Authors**: Muhammad Ali Farooq, Asma Khatoon, Viktor Varkarakis, Peter Corcoran
- **Comment**: Paper Published in AICS 2019
- **Journal**: None
- **Summary**: Technology-assisted platforms provide reliable solutions in almost every field these days. One such important application in the medical field is the skin cancer classification in preliminary stages that need sensitive and precise data analysis. For the proposed study the Kaggle skin cancer dataset is utilized. The proposed study consists of two main phases. In the first phase, the images are preprocessed to remove the clutters thus producing a refined version of training images. To achieve that, a sharpening filter is applied followed by a hair removal algorithm. Different image quality measurement metrics including Peak Signal to Noise (PSNR), Mean Square Error (MSE), Maximum Absolute Squared Deviation (MXERR) and Energy Ratio/ Ratio of Squared Norms (L2RAT) are used to compare the overall image quality before and after applying preprocessing operations. The results from the aforementioned image quality metrics prove that image quality is not compromised however it is upgraded by applying the preprocessing operations. The second phase of the proposed research work incorporates deep learning methodologies that play an imperative role in accurate, precise and robust classification of the lesion mole. This has been reflected by using two state of the art deep learning models: Inception-v3 and MobileNet. The experimental results demonstrate notable improvement in train and validation accuracy by using the refined version of images of both the networks, however, the Inception-v3 network was able to achieve better validation accuracy thus it was finally selected to evaluate it on test data. The final test accuracy using state of art Inception-v3 network was 86%.



### Probabilistic Future Prediction for Video Scene Understanding
- **Arxiv ID**: http://arxiv.org/abs/2003.06409v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2003.06409v2)
- **Published**: 2020-03-13 17:48:21+00:00
- **Updated**: 2020-07-17 10:07:40+00:00
- **Authors**: Anthony Hu, Fergal Cotter, Nikhil Mohan, Corina Gurau, Alex Kendall
- **Comment**: Accepted as a conference paper at ECCV 2020
- **Journal**: None
- **Summary**: We present a novel deep learning architecture for probabilistic future prediction from video. We predict the future semantics, geometry and motion of complex real-world urban scenes and use this representation to control an autonomous vehicle. This work is the first to jointly predict ego-motion, static scene, and the motion of dynamic agents in a probabilistic manner, which allows sampling consistent, highly probable futures from a compact latent space. Our model learns a representation from RGB video with a spatio-temporal convolutional module. The learned representation can be explicitly decoded to future semantic segmentation, depth, and optical flow, in addition to being an input to a learnt driving policy. To model the stochasticity of the future, we introduce a conditional variational approach which minimises the divergence between the present distribution (what could happen given what we have seen) and the future distribution (what we observe actually happens). During inference, diverse futures are generated by sampling from the present distribution.



### Sparse Graphical Memory for Robust Planning
- **Arxiv ID**: http://arxiv.org/abs/2003.06417v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.06417v3)
- **Published**: 2020-03-13 17:59:32+00:00
- **Updated**: 2020-11-12 21:37:49+00:00
- **Authors**: Scott Emmons, Ajay Jain, Michael Laskin, Thanard Kurutach, Pieter Abbeel, Deepak Pathak
- **Comment**: Accepted at NeurIPS 2020. Video and code at
  https://mishalaskin.github.io/sgm/
- **Journal**: None
- **Summary**: To operate effectively in the real world, agents should be able to act from high-dimensional raw sensory input such as images and achieve diverse goals across long time-horizons. Current deep reinforcement and imitation learning methods can learn directly from high-dimensional inputs but do not scale well to long-horizon tasks. In contrast, classical graphical methods like A* search are able to solve long-horizon tasks, but assume that the state space is abstracted away from raw sensory input. Recent works have attempted to combine the strengths of deep learning and classical planning; however, dominant methods in this domain are still quite brittle and scale poorly with the size of the environment. We introduce Sparse Graphical Memory (SGM), a new data structure that stores states and feasible transitions in a sparse memory. SGM aggregates states according to a novel two-way consistency objective, adapting classic state aggregation criteria to goal-conditioned RL: two states are redundant when they are interchangeable both as goals and as starting states. Theoretically, we prove that merging nodes according to two-way consistency leads to an increase in shortest path lengths that scales only linearly with the merging threshold. Experimentally, we show that SGM significantly outperforms current state of the art methods on long horizon, sparse-reward visual navigation tasks. Project video and code are available at https://mishalaskin.github.io/sgm/



### Learning Unbiased Representations via Mutual Information Backpropagation
- **Arxiv ID**: http://arxiv.org/abs/2003.06430v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.06430v1)
- **Published**: 2020-03-13 18:06:31+00:00
- **Updated**: 2020-03-13 18:06:31+00:00
- **Authors**: Ruggero Ragonesi, Riccardo Volpi, Jacopo Cavazza, Vittorio Murino
- **Comment**: Code publicly available at https://github.com/rugrag/learn-unbiased
- **Journal**: None
- **Summary**: We are interested in learning data-driven representations that can generalize well, even when trained on inherently biased data. In particular, we face the case where some attributes (bias) of the data, if learned by the model, can severely compromise its generalization properties. We tackle this problem through the lens of information theory, leveraging recent findings for a differentiable estimation of mutual information. We propose a novel end-to-end optimization strategy, which simultaneously estimates and minimizes the mutual information between the learned representation and the data attributes. When applied on standard benchmarks, our model shows comparable or superior classification performance with respect to state-of-the-art approaches. Moreover, our method is general enough to be applicable to the problem of ``algorithmic fairness'', with competitive results.



### A Neural Architecture for Detecting Confusion in Eye-tracking Data
- **Arxiv ID**: http://arxiv.org/abs/2003.06434v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.06434v1)
- **Published**: 2020-03-13 18:20:39+00:00
- **Updated**: 2020-03-13 18:20:39+00:00
- **Authors**: Shane Sims, Cristina Conati
- **Comment**: None
- **Journal**: None
- **Summary**: Encouraged by the success of deep learning in a variety of domains, we investigate a novel application of its methods on the effectiveness of detecting user confusion in eye-tracking data. We introduce an architecture that uses RNN and CNN sub-models in parallel to take advantage of the temporal and visuospatial aspects of our data. Experiments with a dataset of user interactions with the ValueChart visualization tool show that our model outperforms an existing model based on Random Forests resulting in a 22% improvement in combined sensitivity & specificity.



### Mutual Information Maximization for Effective Lip Reading
- **Arxiv ID**: http://arxiv.org/abs/2003.06439v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.06439v1)
- **Published**: 2020-03-13 18:47:42+00:00
- **Updated**: 2020-03-13 18:47:42+00:00
- **Authors**: Xing Zhao, Shuang Yang, Shiguang Shan, Xilin Chen
- **Comment**: 8 pages, Accepted in the 15th IEEE International Conference on
  Automatic Face and Gesture Recognition (FG 2020)
- **Journal**: None
- **Summary**: Lip reading has received an increasing research interest in recent years due to the rapid development of deep learning and its widespread potential applications. One key point to obtain good performance for the lip reading task depends heavily on how effective the representation can be to capture the lip movement information and meanwhile to resist the noises resulted from the change of pose, lighting conditions, speaker's appearance and so on. Towards this target, we propose to introduce the mutual information constraints on both the local feature's level and the global sequence's level to enhance the relations of the features with the speech content. On the one hand, we constraint the features generated at each time step to enable them carry a strong relation with the speech content by imposing the local mutual information maximization constraint (LMIM), leading to improvements over the model's ability to discover fine-grained lip movements and the fine-grained differences among words with similar pronunciation, such as ``spend'' and ``spending''. On the other hand, we introduce the mutual information maximization constraint on the global sequence's level (GMIM), to make the model be able to pay more attention to discriminate key frames related with the speech content, and less to various noises appeared in the speaking process. By combining these two advantages together, the proposed method is expected to be both discriminative and robust for effective lip reading. To verify this method, we evaluate on two large-scale benchmark. We perform a detailed analysis and comparison on several aspects, including the comparison of the LMIM and GMIM with the baseline, the visualization of the learned representation and so on. The results not only prove the effectiveness of the proposed method but also report new state-of-the-art performance on both the two benchmarks.



### The GraphNet Zoo: An All-in-One Graph Based Deep Semi-Supervised Framework for Medical Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2003.06451v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.06451v2)
- **Published**: 2020-03-13 19:18:21+00:00
- **Updated**: 2020-06-26 18:19:25+00:00
- **Authors**: Marianne de Vriendt, Philip Sellars, Angelica I Aviles-Rivero
- **Comment**: None
- **Journal**: None
- **Summary**: We consider the problem of classifying a medical image dataset when we have a limited amounts of labels. This is very common yet challenging setting as labelled data is expensive, time consuming to collect and may require expert knowledge. The current classification go-to of deep supervised learning is unable to cope with such a problem setup. However, using semi-supervised learning, one can produce accurate classifications using a significantly reduced amount of labelled data. Therefore, semi-supervised learning is perfectly suited for medical image classification. However, there has almost been no uptake of semi-supervised methods in the medical domain. In this work, we propose an all-in-one framework for deep semi-supervised classification focusing on graph based approaches, which up to our knowledge it is the first time that an approach with minimal labels has been shown to such an unprecedented scale with medical data. We introduce the concept of hybrid models by defining a classifier as a combination between an energy-based model and a deep net. Our energy functional is built on the Dirichlet energy based on the graph p-Laplacian. Our framework includes energies based on the $\ell_1$ and $\ell_2$ norms. We then connected this energy model to a deep net to generate a much richer feature space to construct a stronger graph. Our framework can be set to be adapted to any complex dataset. We demonstrate, through extensive numerical comparisons, that our approach readily compete with fully-supervised state-of-the-art techniques for the applications of Malaria Cells, Mammograms and Chest X-ray classification whilst using only 20% of labels.



### GeoDA: a geometric framework for black-box adversarial attacks
- **Arxiv ID**: http://arxiv.org/abs/2003.06468v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.06468v1)
- **Published**: 2020-03-13 20:03:01+00:00
- **Updated**: 2020-03-13 20:03:01+00:00
- **Authors**: Ali Rahmati, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard, Huaiyu Dai
- **Comment**: In Proceedings of IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR), 2020
- **Journal**: None
- **Summary**: Adversarial examples are known as carefully perturbed images fooling image classifiers. We propose a geometric framework to generate adversarial examples in one of the most challenging black-box settings where the adversary can only generate a small number of queries, each of them returning the top-$1$ label of the classifier. Our framework is based on the observation that the decision boundary of deep networks usually has a small mean curvature in the vicinity of data samples. We propose an effective iterative algorithm to generate query-efficient black-box perturbations with small $\ell_p$ norms for $p \ge 1$, which is confirmed via experimental evaluations on state-of-the-art natural image classifiers. Moreover, for $p=2$, we theoretically show that our algorithm actually converges to the minimal $\ell_2$-perturbation when the curvature of the decision boundary is bounded. We also obtain the optimal distribution of the queries over the iterations of the algorithm. Finally, experimental results confirm that our principled black-box attack algorithm performs better than state-of-the-art algorithms as it generates smaller perturbations with a reduced number of queries.



### Inducing Optimal Attribute Representations for Conditional GANs
- **Arxiv ID**: http://arxiv.org/abs/2003.06472v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.06472v2)
- **Published**: 2020-03-13 20:24:07+00:00
- **Updated**: 2020-09-08 13:36:11+00:00
- **Authors**: Binod Bhattarai, Tae-Kyun Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Conditional GANs are widely used in translating an image from one category to another. Meaningful conditions to GANs provide greater flexibility and control over the nature of the target domain synthetic data. Existing conditional GANs commonly encode target domain label information as hard-coded categorical vectors in the form of 0s and 1s. The major drawbacks of such representations are inability to encode the high-order semantic information of target categories and their relative dependencies. We propose a novel end-to-end learning framework with Graph Convolutional Networks to learn the attribute representations to condition on the generator. The GAN losses, i.e. the discriminator and attribute classification losses, are fed back to the Graph resulting in the synthetic images that are more natural and clearer in attributes. Moreover, prior-arts are given priorities to condition on the generator side, not on the discriminator side of GANs. We apply the conditions to the discriminator side as well via multi-task learning. We enhanced the four state-of-the art cGANs architectures: Stargan, Stargan-JNT, AttGAN and STGAN. Our extensive qualitative and quantitative evaluations on challenging face attributes manipulation data set, CelebA, LFWA, and RaFD, show that the cGANs enhanced by our methods outperform by a large margin, compared to their counter-parts and other conditioning methods, in terms of both target attributes recognition rates and quality measures such as PSNR and SSIM.



### Self-supervised Single-view 3D Reconstruction via Semantic Consistency
- **Arxiv ID**: http://arxiv.org/abs/2003.06473v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.06473v1)
- **Published**: 2020-03-13 20:29:01+00:00
- **Updated**: 2020-03-13 20:29:01+00:00
- **Authors**: Xueting Li, Sifei Liu, Kihwan Kim, Shalini De Mello, Varun Jampani, Ming-Hsuan Yang, Jan Kautz
- **Comment**: Codes and other resources will be released at:
  https://sites.google.com/view/unsup-mesh/home
- **Journal**: None
- **Summary**: We learn a self-supervised, single-view 3D reconstruction model that predicts the 3D mesh shape, texture and camera pose of a target object with a collection of 2D images and silhouettes. The proposed method does not necessitate 3D supervision, manually annotated keypoints, multi-view images of an object or a prior 3D template. The key insight of our work is that objects can be represented as a collection of deformable parts, and each part is semantically coherent across different instances of the same category (e.g., wings on birds and wheels on cars). Therefore, by leveraging self-supervisedly learned part segmentation of a large collection of category-specific images, we can effectively enforce semantic consistency between the reconstructed meshes and the original images. This significantly reduces ambiguities during joint prediction of shape and camera pose of an object, along with texture. To the best of our knowledge, we are the first to try and solve the single-view reconstruction problem without a category-specific template mesh or semantic keypoints. Thus our model can easily generalize to various object categories without such labels, e.g., horses, penguins, etc. Through a variety of experiments on several categories of deformable and rigid objects, we demonstrate that our unsupervised method performs comparably if not better than existing category-specific reconstruction methods learned with supervision.



### Recurrent convolutional neural networks for mandible segmentation from computed tomography
- **Arxiv ID**: http://arxiv.org/abs/2003.06486v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.06486v1)
- **Published**: 2020-03-13 21:11:28+00:00
- **Updated**: 2020-03-13 21:11:28+00:00
- **Authors**: Bingjiang Qiu, Jiapan Guo, Joep Kraeima, Haye H. Glas, Ronald J. H. Borra, Max J. H. Witjes, Peter M. A. van Ooijen
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, accurate mandible segmentation in CT scans based on deep learning methods has attracted much attention. However, there still exist two major challenges, namely, metal artifacts among mandibles and large variations in shape or size among individuals. To address these two challenges, we propose a recurrent segmentation convolutional neural network (RSegCNN) that embeds segmentation convolutional neural network (SegCNN) into the recurrent neural network (RNN) for robust and accurate segmentation of the mandible. Such a design of the system takes into account the similarity and continuity of the mandible shapes captured in adjacent image slices in CT scans. The RSegCNN infers the mandible information based on the recurrent structure with the embedded encoder-decoder segmentation (SegCNN) components. The recurrent structure guides the system to exploit relevant and important information from adjacent slices, while the SegCNN component focuses on the mandible shapes from a single CT slice. We conducted extensive experiments to evaluate the proposed RSegCNN on two head and neck CT datasets. The experimental results show that the RSegCNN is significantly better than the state-of-the-art models for accurate mandible segmentation.



### Explainable Deep Classification Models for Domain Generalization
- **Arxiv ID**: http://arxiv.org/abs/2003.06498v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.06498v1)
- **Published**: 2020-03-13 22:22:15+00:00
- **Updated**: 2020-03-13 22:22:15+00:00
- **Authors**: Andrea Zunino, Sarah Adel Bargal, Riccardo Volpi, Mehrnoosh Sameki, Jianming Zhang, Stan Sclaroff, Vittorio Murino, Kate Saenko
- **Comment**: None
- **Journal**: None
- **Summary**: Conventionally, AI models are thought to trade off explainability for lower accuracy. We develop a training strategy that not only leads to a more explainable AI system for object classification, but as a consequence, suffers no perceptible accuracy degradation. Explanations are defined as regions of visual evidence upon which a deep classification network makes a decision. This is represented in the form of a saliency map conveying how much each pixel contributed to the network's decision. Our training strategy enforces a periodic saliency-based feedback to encourage the model to focus on the image regions that directly correspond to the ground-truth object. We quantify explainability using an automated metric, and using human judgement. We propose explainability as a means for bridging the visual-semantic gap between different domains where model explanations are used as a means of disentagling domain specific information from otherwise relevant features. We demonstrate that this leads to improved generalization to new domains without hindering performance on the original domain.



### A Privacy-Preserving-Oriented DNN Pruning and Mobile Acceleration Framework
- **Arxiv ID**: http://arxiv.org/abs/2003.06513v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.06513v2)
- **Published**: 2020-03-13 23:52:03+00:00
- **Updated**: 2020-09-17 00:45:39+00:00
- **Authors**: Yifan Gong, Zheng Zhan, Zhengang Li, Wei Niu, Xiaolong Ma, Wenhao Wang, Bin Ren, Caiwen Ding, Xue Lin, Xiaolin Xu, Yanzhi Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Weight pruning of deep neural networks (DNNs) has been proposed to satisfy the limited storage and computing capability of mobile edge devices. However, previous pruning methods mainly focus on reducing the model size and/or improving performance without considering the privacy of user data. To mitigate this concern, we propose a privacy-preserving-oriented pruning and mobile acceleration framework that does not require the private training dataset. At the algorithm level of the proposed framework, a systematic weight pruning technique based on the alternating direction method of multipliers (ADMM) is designed to iteratively solve the pattern-based pruning problem for each layer with randomly generated synthetic data. In addition, corresponding optimizations at the compiler level are leveraged for inference accelerations on devices. With the proposed framework, users could avoid the time-consuming pruning process for non-experts and directly benefit from compressed models. Experimental results show that the proposed framework outperforms three state-of-art end-to-end DNN frameworks, i.e., TensorFlow-Lite, TVM, and MNN, with speedup up to 4.2X, 2.5X, and 2.0X, respectively, with almost no accuracy loss, while preserving data privacy.



### Evaluation of Cross-View Matching to Improve Ground Vehicle Localization with Aerial Perception
- **Arxiv ID**: http://arxiv.org/abs/2003.06515v4
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.06515v4)
- **Published**: 2020-03-13 23:59:07+00:00
- **Updated**: 2020-11-15 22:20:51+00:00
- **Authors**: Deeksha Dixit, Surabhi Verma, Pratap Tokekar
- **Comment**: 8 pages, 12 figures, submitted to IEEE Robotics and Automation
  Letters (RA-L) with the International Conference on Robotics and Automation
  (ICRA 2021) presentation option
- **Journal**: None
- **Summary**: Cross-view matching refers to the problem of finding the closest match for a given query ground view image to one from a database of aerial images. If the aerial images are geotagged, then the closest matching aerial image can be used to localize the query ground view image. Due to the recent success of deep learning methods, several cross-view matching techniques have been proposed. These approaches perform well for the matching of isolated query images. However, their evaluation over a trajectory is limited. In this paper, we evaluate cross-view matching for the task of localizing a ground vehicle over a longer trajectory. We treat these cross-view matches as sensor measurements that are fused using a particle filter. We evaluate the performance of this method using a city-wide dataset collected in a photorealistic simulation by varying four parameters: height of aerial images, the pitch of the aerial camera mount, FOV of the ground camera, and the methodology of fusing cross-view measurements in the particle filter. We also report the results obtained using our pipeline on a real-world dataset collected using Google Street View and satellite view APIs.



