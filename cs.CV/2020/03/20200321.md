# Arxiv Papers in cs.CV on 2020-03-21
### Towards a MEMS-based Adaptive LIDAR
- **Arxiv ID**: http://arxiv.org/abs/2003.09545v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.09545v2)
- **Published**: 2020-03-21 01:11:50+00:00
- **Updated**: 2020-10-16 05:12:19+00:00
- **Authors**: Francesco Pittaluga, Zaid Tasneem, Justin Folden, Brevin Tilmon, Ayan Chakrabarti, Sanjeev J. Koppal
- **Comment**: 14 pages, 5 figures, project site:
  https://www.fpittaluga.com/adaptivelidar, to be published in International
  Conference on 3D Vision 2020
- **Journal**: None
- **Summary**: We present a proof-of-concept LIDAR design that allows adaptive real-time measurements according to dynamically specified measurement patterns. We describe our optical setup and calibration, which enables fast sparse depth measurements using a scanning MEMS (micro-electro-mechanical) mirror. We validate the efficacy of our prototype LIDAR design by testing on over 75 static and dynamic scenes spanning a range of environments. We show CNN-based depth-map completion experiments which demonstrate that our sensor can realize adaptive depth sensing for dynamic scenes.



### Adversarial Continual Learning
- **Arxiv ID**: http://arxiv.org/abs/2003.09553v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.09553v2)
- **Published**: 2020-03-21 02:08:17+00:00
- **Updated**: 2020-07-21 15:42:20+00:00
- **Authors**: Sayna Ebrahimi, Franziska Meier, Roberto Calandra, Trevor Darrell, Marcus Rohrbach
- **Comment**: Accepted at ECCV 2020
- **Journal**: None
- **Summary**: Continual learning aims to learn new tasks without forgetting previously learned ones. We hypothesize that representations learned to solve each task in a sequence have a shared structure while containing some task-specific properties. We show that shared features are significantly less prone to forgetting and propose a novel hybrid continual learning framework that learns a disjoint representation for task-invariant and task-specific features required to solve a sequence of tasks. Our model combines architecture growth to prevent forgetting of task-specific skills and an experience replay approach to preserve shared skills. We demonstrate our hybrid approach is effective in avoiding forgetting and show it is superior to both architecture-based and memory-based approaches on class incrementally learning of a single dataset as well as a sequence of multiple datasets in image classification. Our code is available at \url{https://github.com/facebookresearch/Adversarial-Continual-Learning}.



### Appearance Fusion of Multiple Cues for Video Co-localization
- **Arxiv ID**: http://arxiv.org/abs/2003.09556v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.09556v2)
- **Published**: 2020-03-21 02:26:36+00:00
- **Updated**: 2020-07-18 04:53:03+00:00
- **Authors**: Koteswar Rao Jerripothula
- **Comment**: 17 Pages and 8 figures. Submitted to ACCV20
- **Journal**: None
- **Summary**: This work addresses the joint object discovery problem in videos while utilizing multiple object-related cues. In contrast to the usual spatial fusion approach, a novel appearance fusion approach is presented here. Specifically, this paper proposes an effective fusion process of different GMMs derived from multiple cues into one GMM. Much the same as any fusion strategy, this approach also needs some guidance. The proposed method relies on reliability and consensus phenomenon for guidance. As a case study, we pursue the "video co-localization" object discovery problem to propose our methodology. Our experiments on YouTube Objects and YouTube Co-localization datasets demonstrate that the proposed method of appearance fusion undoubtedly has an advantage over both the spatial fusion strategy and the current state-of-the-art video co-localization methods.



### Non-Adversarial Video Synthesis with Learned Priors
- **Arxiv ID**: http://arxiv.org/abs/2003.09565v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.09565v3)
- **Published**: 2020-03-21 02:57:33+00:00
- **Updated**: 2020-04-17 20:54:58+00:00
- **Authors**: Abhishek Aich, Akash Gupta, Rameswar Panda, Rakib Hyder, M. Salman Asif, Amit K. Roy-Chowdhury
- **Comment**: Accepted to CVPR 2020
- **Journal**: None
- **Summary**: Most of the existing works in video synthesis focus on generating videos using adversarial learning. Despite their success, these methods often require input reference frame or fail to generate diverse videos from the given data distribution, with little to no uniformity in the quality of videos that can be generated. Different from these methods, we focus on the problem of generating videos from latent noise vectors, without any reference input frames. To this end, we develop a novel approach that jointly optimizes the input latent space, the weights of a recurrent neural network and a generator through non-adversarial learning. Optimizing for the input latent space along with the network weights allows us to generate videos in a controlled environment, i.e., we can faithfully generate all videos the model has seen during the learning process as well as new unseen videos. Extensive experiments on three challenging and diverse datasets well demonstrate that our approach generates superior quality videos compared to the existing state-of-the-art methods.



### Palm-GAN: Generating Realistic Palmprint Images Using Total-Variation Regularized GAN
- **Arxiv ID**: http://arxiv.org/abs/2003.10834v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.10834v1)
- **Published**: 2020-03-21 03:24:36+00:00
- **Updated**: 2020-03-21 03:24:36+00:00
- **Authors**: Shervin Minaee, Mehdi Minaei, Amirali Abdolrashidi
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1812.10482,
  arXiv:1812.04822
- **Journal**: None
- **Summary**: Generating realistic palmprint (more generally biometric) images has always been an interesting and, at the same time, challenging problem. Classical statistical models fail to generate realistic-looking palmprint images, as they are not powerful enough to capture the complicated texture representation of palmprint images. In this work, we present a deep learning framework based on generative adversarial networks (GAN), which is able to generate realistic palmprint images. To help the model learn more realistic images, we proposed to add a suitable regularization to the loss function, which imposes the line connectivity of generated palmprint images. This is very desirable for palmprints, as the principal lines in palm are usually connected. We apply this framework to a popular palmprint databases, and generate images which look very realistic, and similar to the samples in this database. Through experimental results, we show that the generated palmprint images look very realistic, have a good diversity, and are able to capture different parts of the prior distribution. We also report the Frechet Inception distance (FID) of the proposed model, and show that our model is able to achieve really good quantitative performance in terms of FID score.



### Monocular Real-time Hand Shape and Motion Capture using Multi-modal Data
- **Arxiv ID**: http://arxiv.org/abs/2003.09572v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.09572v3)
- **Published**: 2020-03-21 03:51:54+00:00
- **Updated**: 2022-03-11 13:39:43+00:00
- **Authors**: Yuxiao Zhou, Marc Habermann, Weipeng Xu, Ikhsanul Habibie, Christian Theobalt, Feng Xu
- **Comment**: Accepted by CVPR 2020
- **Journal**: None
- **Summary**: We present a novel method for monocular hand shape and pose estimation at unprecedented runtime performance of 100fps and at state-of-the-art accuracy. This is enabled by a new learning based architecture designed such that it can make use of all the sources of available hand training data: image data with either 2D or 3D annotations, as well as stand-alone 3D animations without corresponding image data. It features a 3D hand joint detection module and an inverse kinematics module which regresses not only 3D joint positions but also maps them to joint rotations in a single feed-forward pass. This output makes the method more directly usable for applications in computer vision and graphics compared to only regressing 3D joint positions. We demonstrate that our architectural design leads to a significant quantitative and qualitative improvement over the state of the art on several challenging benchmarks. Our model is publicly available for future research.



### Who2com: Collaborative Perception via Learnable Handshake Communication
- **Arxiv ID**: http://arxiv.org/abs/2003.09575v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MA, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2003.09575v1)
- **Published**: 2020-03-21 04:16:22+00:00
- **Updated**: 2020-03-21 04:16:22+00:00
- **Authors**: Yen-Cheng Liu, Junjiao Tian, Chih-Yao Ma, Nathan Glaser, Chia-Wen Kuo, Zsolt Kira
- **Comment**: Accepted to ICRA 2020
- **Journal**: None
- **Summary**: In this paper, we propose the problem of collaborative perception, where robots can combine their local observations with those of neighboring agents in a learnable way to improve accuracy on a perception task. Unlike existing work in robotics and multi-agent reinforcement learning, we formulate the problem as one where learned information must be shared across a set of agents in a bandwidth-sensitive manner to optimize for scene understanding tasks such as semantic segmentation. Inspired by networking communication protocols, we propose a multi-stage handshake communication mechanism where the neural network can learn to compress relevant information needed for each stage. Specifically, a target agent with degraded sensor data sends a compressed request, the other agents respond with matching scores, and the target agent determines who to connect with (i.e., receive information from). We additionally develop the AirSim-CP dataset and metrics based on the AirSim simulator where a group of aerial robots perceive diverse landscapes, such as roads, grasslands, buildings, etc. We show that for the semantic segmentation task, our handshake communication method significantly improves accuracy by approximately 20% over decentralized baselines, and is comparable to centralized ones using a quarter of the bandwidth.



### End-to-end Autonomous Driving Perception with Sequential Latent Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2003.12464v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.12464v2)
- **Published**: 2020-03-21 05:37:44+00:00
- **Updated**: 2020-10-09 03:40:42+00:00
- **Authors**: Jianyu Chen, Zhuo Xu, Masayoshi Tomizuka
- **Comment**: 8 pages, 10 figures, 2 tables
- **Journal**: None
- **Summary**: Current autonomous driving systems are composed of a perception system and a decision system. Both of them are divided into multiple subsystems built up with lots of human heuristics. An end-to-end approach might clean up the system and avoid huge efforts of human engineering, as well as obtain better performance with increasing data and computation resources. Compared to the decision system, the perception system is more suitable to be designed in an end-to-end framework, since it does not require online driving exploration. In this paper, we propose a novel end-to-end approach for autonomous driving perception. A latent space is introduced to capture all relevant features useful for perception, which is learned through sequential latent representation learning. The learned end-to-end perception model is able to solve the detection, tracking, localization and mapping problems altogether with only minimum human engineering efforts and without storing any maps online. The proposed method is evaluated in a realistic urban driving simulator, with both camera image and lidar point cloud as sensor inputs. The codes and videos of this work are available at our github repo and project website.



### Topological Sweep for Multi-Target Detection of Geostationary Space Objects
- **Arxiv ID**: http://arxiv.org/abs/2003.09583v3
- **DOI**: 10.1109/TSP.2020.3021232
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.09583v3)
- **Published**: 2020-03-21 06:00:41+00:00
- **Updated**: 2020-09-01 06:06:10+00:00
- **Authors**: Daqi Liu, Bo Chen, Tat-Jun Chin, Mark Rutten
- **Comment**: 12 pages, 12 figures, accepted to IEEE Transactions on Signal
  Processing
- **Journal**: None
- **Summary**: Conducting surveillance of the Earth's orbit is a key task towards achieving space situational awareness (SSA). Our work focuses on the optical detection of man-made objects (e.g., satellites, space debris) in Geostationary orbit (GEO), which is home to major space assets such as telecommunications and navigational satellites. GEO object detection is challenging due to the distance of the targets, which appear as small dim points among a clutter of bright stars. In this paper, we propose a novel multi-target detection technique based on topological sweep, to find GEO objects from a short sequence of optical images. Our topological sweep technique exploits the geometric duality that underpins the approximately linear trajectory of target objects across the input sequence, to extract the targets from significant clutter and noise. Unlike standard multi-target methods, our algorithm deterministically solves a combinatorial problem to ensure high-recall rates without requiring accurate initializations. The usage of geometric duality also yields an algorithm that is computationally efficient and suitable for online processing.



### Single-shot autofocusing of microscopy images using deep learning
- **Arxiv ID**: http://arxiv.org/abs/2003.09585v2
- **DOI**: 10.1021/acsphotonics.0c01774
- **Categories**: **eess.IV**, cs.CV, cs.LG, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/2003.09585v2)
- **Published**: 2020-03-21 06:07:27+00:00
- **Updated**: 2021-01-22 06:20:14+00:00
- **Authors**: Yilin Luo, Luzhe Huang, Yair Rivenson, Aydogan Ozcan
- **Comment**: 27 pages, 8 figures, 9 supplementary figures, 2 supplementary tables
- **Journal**: ACS Photonics (2021)
- **Summary**: We demonstrate a deep learning-based offline autofocusing method, termed Deep-R, that is trained to rapidly and blindly autofocus a single-shot microscopy image of a specimen that is acquired at an arbitrary out-of-focus plane. We illustrate the efficacy of Deep-R using various tissue sections that were imaged using fluorescence and brightfield microscopy modalities and demonstrate snapshot autofocusing under different scenarios, such as a uniform axial defocus as well as a sample tilt within the field-of-view. Our results reveal that Deep-R is significantly faster when compared with standard online algorithmic autofocusing methods. This deep learning-based blind autofocusing framework opens up new opportunities for rapid microscopic imaging of large sample areas, also reducing the photon dose on the sample.



### Cooling-Shrinking Attack: Blinding the Tracker with Imperceptible Noises
- **Arxiv ID**: http://arxiv.org/abs/2003.09595v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.09595v1)
- **Published**: 2020-03-21 07:13:40+00:00
- **Updated**: 2020-03-21 07:13:40+00:00
- **Authors**: Bin Yan, Dong Wang, Huchuan Lu, Xiaoyun Yang
- **Comment**: CVPR2020
- **Journal**: None
- **Summary**: Adversarial attack of CNN aims at deceiving models to misbehave by adding imperceptible perturbations to images. This feature facilitates to understand neural networks deeply and to improve the robustness of deep learning models. Although several works have focused on attacking image classifiers and object detectors, an effective and efficient method for attacking single object trackers of any target in a model-free way remains lacking. In this paper, a cooling-shrinking attack method is proposed to deceive state-of-the-art SiameseRPN-based trackers. An effective and efficient perturbation generator is trained with a carefully designed adversarial loss, which can simultaneously cool hot regions where the target exists on the heatmaps and force the predicted bounding box to shrink, making the tracked target invisible to trackers. Numerous experiments on OTB100, VOT2018, and LaSOT datasets show that our method can effectively fool the state-of-the-art SiameseRPN++ tracker by adding small perturbations to the template or the search regions. Besides, our method has good transferability and is able to deceive other top-performance trackers such as DaSiamRPN, DaSiamRPN-UpdateNet, and DiMP. The source codes are available at https://github.com/MasterBin-IIAU/CSA.



### A level set representation method for N-dimensional convex shape and applications
- **Arxiv ID**: http://arxiv.org/abs/2003.09600v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.09600v1)
- **Published**: 2020-03-21 07:37:44+00:00
- **Updated**: 2020-03-21 07:37:44+00:00
- **Authors**: Lingfeng li, Shousheng Luo, Xue-Cheng Tai, Jiang Yang
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we present a new efficient method for convex shape representation, which is regardless of the dimension of the concerned objects, using level-set approaches. Convexity prior is very useful for object completion in computer vision. It is a very challenging task to design an efficient method for high dimensional convex objects representation. In this paper, we prove that the convexity of the considered object is equivalent to the convexity of the associated signed distance function. Then, the second order condition of convex functions is used to characterize the shape convexity equivalently. We apply this new method to two applications: object segmentation with convexity prior and convex hull problem (especially with outliers). For both applications, the involved problems can be written as a general optimization problem with three constraints. Efficient algorithm based on alternating direction method of multipliers is presented for the optimization problem. Numerical experiments are conducted to verify the effectiveness and efficiency of the proposed representation method and algorithm.



### Temporally Coherent Embeddings for Self-Supervised Video Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2004.02753v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/2004.02753v5)
- **Published**: 2020-03-21 12:25:50+00:00
- **Updated**: 2020-11-17 04:21:35+00:00
- **Authors**: Joshua Knights, Ben Harwood, Daniel Ward, Anthony Vanderkop, Olivia Mackenzie-Ross, Peyman Moghadam
- **Comment**: Accepted at ICPR 2020. Project page:
  https://csiro-robotics.github.io/TCE-Webpage/
- **Journal**: None
- **Summary**: This paper presents TCE: Temporally Coherent Embeddings for self-supervised video representation learning. The proposed method exploits inherent structure of unlabeled video data to explicitly enforce temporal coherency in the embedding space, rather than indirectly learning it through ranking or predictive proxy tasks. In the same way that high-level visual information in the world changes smoothly, we believe that nearby frames in learned representations will benefit from demonstrating similar properties. Using this assumption, we train our TCE model to encode videos such that adjacent frames exist close to each other and videos are separated from one another. Using TCE we learn robust representations from large quantities of unlabeled video data. We thoroughly analyse and evaluate our self-supervised learned TCE models on a downstream task of video action recognition using multiple challenging benchmarks (Kinetics400, UCF101, HMDB51). With a simple but effective 2D-CNN backbone and only RGB stream inputs, TCE pre-trained representations outperform all previous selfsupervised 2D-CNN and 3D-CNN pre-trained on UCF101. The code and pre-trained models for this paper can be downloaded at: https://github.com/csiro-robotics/TCE



### BiCANet: Bi-directional Contextual Aggregating Network for Image Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2003.09669v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.09669v1)
- **Published**: 2020-03-21 14:23:04+00:00
- **Updated**: 2020-03-21 14:23:04+00:00
- **Authors**: Quan Zhou, Dechun Cong, Bin Kang, Xiaofu Wu, Baoyu Zheng, Huimin Lu, Longin Jan Latecki
- **Comment**: None
- **Journal**: None
- **Summary**: Exploring contextual information in convolution neural networks (CNNs) has gained substantial attention in recent years for semantic segmentation. This paper introduces a Bi-directional Contextual Aggregating Network, called BiCANet, for semantic segmentation. Unlike previous approaches that encode context in feature space, BiCANet aggregates contextual cues from a categorical perspective, which is mainly consist of three parts: contextual condensed projection block (CCPB), bi-directional context interaction block (BCIB), and muti-scale contextual fusion block (MCFB). More specifically, CCPB learns a category-based mapping through a split-transform-merge architecture, which condenses contextual cues with different receptive fields from intermediate layer. BCIB, on the other hand, employs dense skipped-connections to enhance the class-level context exchanging. Finally, MCFB integrates multi-scale contextual cues by investigating short- and long-ranged spatial dependencies. To evaluate BiCANet, we have conducted extensive experiments on three semantic segmentation datasets: PASCAL VOC 2012, Cityscapes, and ADE20K. The experimental results demonstrate that BiCANet outperforms recent state-of-the-art networks without any postprocess techniques. Particularly, BiCANet achieves the mIoU score of 86.7%, 82.4% and 38.66% on PASCAL VOC 2012, Cityscapes and ADE20K testset, respectively.



### On Information Plane Analyses of Neural Network Classifiers -- A Review
- **Arxiv ID**: http://arxiv.org/abs/2003.09671v3
- **DOI**: 10.1109/TNNLS.2021.3089037
- **Categories**: **cs.LG**, cs.CV, cs.IT, math.IT, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.09671v3)
- **Published**: 2020-03-21 14:43:45+00:00
- **Updated**: 2021-06-10 15:06:30+00:00
- **Authors**: Bernhard C. Geiger
- **Comment**: 12 pages, 3 figures; accepted for publication in IEEE Transactions on
  Neural Networks and Learning Systems. (c) 2021 IEEE
- **Journal**: IEEE Trans. Neural Networks and Learning Systems 33(12):7039-7051
- **Summary**: We review the current literature concerned with information plane analyses of neural network classifiers. While the underlying information bottleneck theory and the claim that information-theoretic compression is causally linked to generalization are plausible, empirical evidence was found to be both supporting and conflicting. We review this evidence together with a detailed analysis of how the respective information quantities were estimated. Our survey suggests that compression visualized in information planes is not necessarily information-theoretic, but is rather often compatible with geometric compression of the latent representations. This insight gives the information plane a renewed justification.   Aside from this, we shed light on the problem of estimating mutual information in deterministic neural networks and its consequences. Specifically, we argue that even in feed-forward neural networks the data processing inequality need not hold for estimates of mutual information. Similarly, while a fitting phase, in which the mutual information between the latent representation and the target increases, is necessary (but not sufficient) for good classification performance, depending on the specifics of mutual information estimation such a fitting phase need not be visible in the information plane.



### Geometrically Mappable Image Features
- **Arxiv ID**: http://arxiv.org/abs/2003.09682v1
- **DOI**: 10.1109/LRA.2020.2970616
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.09682v1)
- **Published**: 2020-03-21 15:36:38+00:00
- **Updated**: 2020-03-21 15:36:38+00:00
- **Authors**: Janine Thoma, Danda Pani Paudel, Ajad Chhatkuli, Luc Van Gool
- **Comment**: Implementation available at
  https://github.com/janinethoma/geometrically_mappable
- **Journal**: IEEE Robotics and Automation Letters 5, no. 2 (2020): 2062-2069
- **Summary**: Vision-based localization of an agent in a map is an important problem in robotics and computer vision. In that context, localization by learning matchable image features is gaining popularity due to recent advances in machine learning. Features that uniquely describe the visual contents of images have a wide range of applications, including image retrieval and understanding. In this work, we propose a method that learns image features targeted for image-retrieval-based localization. Retrieval-based localization has several benefits, such as easy maintenance and quick computation. However, the state-of-the-art features only provide visual similarity scores which do not explicitly reveal the geometric distance between query and retrieved images. Knowing this distance is highly desirable for accurate localization, especially when the reference images are sparsely distributed in the scene. Therefore, we propose a novel loss function for learning image features which are both visually representative and geometrically relatable. This is achieved by guiding the learning process such that the feature and geometric distances between images are directly proportional. In our experiments we show that our features not only offer significantly better localization accuracy, but also allow to estimate the trajectory of a query sequence in absence of the reference images.



### Multi-Task Learning Enhanced Single Image De-Raining
- **Arxiv ID**: http://arxiv.org/abs/2003.09689v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.09689v2)
- **Published**: 2020-03-21 16:19:56+00:00
- **Updated**: 2020-05-11 13:23:06+00:00
- **Authors**: Yulong Fan, Rong Chen, Bo Li
- **Comment**: 15 pages, 16 figures
- **Journal**: None
- **Summary**: Rain removal in images is an important task in computer vision filed and attracting attentions of more and more people. In this paper, we address a non-trivial issue of removing visual effect of rain streak from a single image. Differing from existing work, our method combines various semantic constraint task in a proposed multi-task regression model for rain removal. These tasks reinforce the model's capabilities from the content, edge-aware, and local texture similarity respectively. To further improve the performance of multi-task learning, we also present two simple but powerful dynamic weighting algorithms. The proposed multi-task enhanced network (MENET) is a powerful convolutional neural network based on U-Net for rain removal research, with a specific focus on utilize multiple tasks constraints and exploit the synergy among them to facilitate the model's rain removal capacity. It is noteworthy that the adaptive weighting scheme has further resulted in improved network capability. We conduct several experiments on synthetic and real rain images, and achieve superior rain removal performance over several selected state-of-the-art (SOTA) approaches. The overall effect of our method is impressive, even in the decomposition of heavy rain and rain streak accumulation.The source code and some results can be found at:https://github.com/SumiHui/MENET.



### Cross-modal Deep Face Normals with Deactivable Skip Connections
- **Arxiv ID**: http://arxiv.org/abs/2003.09691v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.09691v2)
- **Published**: 2020-03-21 16:26:59+00:00
- **Updated**: 2020-03-30 13:54:14+00:00
- **Authors**: Victoria Fernandez Abrevaya, Adnane Boukhayma, Philip H. S. Torr, Edmond Boyer
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: We present an approach for estimating surface normals from in-the-wild color images of faces. While data-driven strategies have been proposed for single face images, limited available ground truth data makes this problem difficult. To alleviate this issue, we propose a method that can leverage all available image and normal data, whether paired or not, thanks to a novel cross-modal learning architecture. In particular, we enable additional training with single modality data, either color or normal, by using two encoder-decoder networks with a shared latent space. The proposed architecture also enables face details to be transferred between the image and normal domains, given paired data, through skip connections between the image encoder and normal decoder. Core to our approach is a novel module that we call deactivable skip connections, which allows integrating both the auto-encoded and image-to-normal branches within the same architecture that can be trained end-to-end. This allows learning of a rich latent space that can accurately capture the normal information. We compare against state-of-the-art methods and show that our approach can achieve significant improvements, both quantitative and qualitative, with natural face images.



### Video-based Person Re-Identification using Gated Convolutional Recurrent Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2003.09717v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.09717v1)
- **Published**: 2020-03-21 18:15:37+00:00
- **Updated**: 2020-03-21 18:15:37+00:00
- **Authors**: Yang Feng, Yu Wang, Jiebo Luo
- **Comment**: This work was done in 2017
- **Journal**: None
- **Summary**: Deep neural networks have been successfully applied to solving the video-based person re-identification problem with impressive results reported. The existing networks for person re-id are designed to extract discriminative features that preserve the identity information. Usually, whole video frames are fed into the neural networks and all the regions in a frame are equally treated. This may be a suboptimal choice because many regions, e.g., background regions in the video, are not related to the person. Furthermore, the person of interest may be occluded by another person or something else. These unrelated regions may hinder person re-identification. In this paper, we introduce a novel gating mechanism to deep neural networks. Our gating mechanism will learn which regions are helpful for person re-identification and let these regions pass the gate. The unrelated background regions or occluding regions are filtered out by the gate. In each frame, the color channels and optical flow channels provide quite different information. To better leverage such information, we generate one gate using the color channels and another gate using the optical flow channels. These two gates are combined to provide a more reliable gate with a novel fusion method. Experimental results on two major datasets demonstrate the performance improvements due to the proposed gating mechanism.



### Learning 3D Part Assembly from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/2003.09754v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2003.09754v2)
- **Published**: 2020-03-21 21:19:28+00:00
- **Updated**: 2020-03-24 17:44:20+00:00
- **Authors**: Yichen Li, Kaichun Mo, Lin Shao, Minhyuk Sung, Leonidas Guibas
- **Comment**: None
- **Journal**: None
- **Summary**: Autonomous assembly is a crucial capability for robots in many applications. For this task, several problems such as obstacle avoidance, motion planning, and actuator control have been extensively studied in robotics. However, when it comes to task specification, the space of possibilities remains underexplored. Towards this end, we introduce a novel problem, single-image-guided 3D part assembly, along with a learningbased solution. We study this problem in the setting of furniture assembly from a given complete set of parts and a single image depicting the entire assembled object. Multiple challenges exist in this setting, including handling ambiguity among parts (e.g., slats in a chair back and leg stretchers) and 3D pose prediction for parts and part subassemblies, whether visible or occluded. We address these issues by proposing a two-module pipeline that leverages strong 2D-3D correspondences and assembly-oriented graph message-passing to infer part relationships. In experiments with a PartNet-based synthetic benchmark, we demonstrate the effectiveness of our framework as compared with three baseline approaches.



### Monocular Depth Prediction through Continuous 3D Loss
- **Arxiv ID**: http://arxiv.org/abs/2003.09763v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.09763v2)
- **Published**: 2020-03-21 22:47:12+00:00
- **Updated**: 2020-08-08 20:36:41+00:00
- **Authors**: Minghan Zhu, Maani Ghaffari, Yuanxin Zhong, Pingping Lu, Zhong Cao, Ryan M. Eustice, Huei Peng
- **Comment**: 8 pages, 4 figures. Accepted by IROS 2020
- **Journal**: None
- **Summary**: This paper reports a new continuous 3D loss function for learning depth from monocular images. The dense depth prediction from a monocular image is supervised using sparse LIDAR points, which enables us to leverage available open source datasets with camera-LIDAR sensor suites during training. Currently, accurate and affordable range sensor is not readily available. Stereo cameras and LIDARs measure depth either inaccurately or sparsely/costly. In contrast to the current point-to-point loss evaluation approach, the proposed 3D loss treats point clouds as continuous objects; therefore, it compensates for the lack of dense ground truth depth due to LIDAR's sparsity measurements. We applied the proposed loss in three state-of-the-art monocular depth prediction approaches DORN, BTS, and Monodepth2. Experimental evaluation shows that the proposed loss improves the depth prediction accuracy and produces point-clouds with more consistent 3D geometric structures compared with all tested baselines, implying the benefit of the proposed loss on general depth prediction networks. A video demo of this work is available at https://youtu.be/5HL8BjSAY4Y.



### Lifespan Age Transformation Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2003.09764v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.09764v2)
- **Published**: 2020-03-21 22:48:06+00:00
- **Updated**: 2020-07-24 12:08:55+00:00
- **Authors**: Roy Or-El, Soumyadip Sengupta, Ohad Fried, Eli Shechtman, Ira Kemelmacher-Shlizerman
- **Comment**: ECCV 2020 Camera-Ready version. Main Changes: 1. Added Ethics & Bias
  statement in the supplementary material 2. Comparison figures to PyGAN [46]
  and S2GAN [13] were removed due to copyright issues. These figures can be
  found in the project's webpage (link is provided in the paper). 3. Added
  links to the code and dataset (Github)
- **Journal**: None
- **Summary**: We address the problem of single photo age progression and regression-the prediction of how a person might look in the future, or how they looked in the past. Most existing aging methods are limited to changing the texture, overlooking transformations in head shape that occur during the human aging and growth process. This limits the applicability of previous methods to aging of adults to slightly older adults, and application of those methods to photos of children does not produce quality results. We propose a novel multi-domain image-to-image generative adversarial network architecture, whose learned latent space models a continuous bi-directional aging process. The network is trained on the FFHQ dataset, which we labeled for ages, gender, and semantic segmentation. Fixed age classes are used as anchors to approximate continuous age transformation. Our framework can predict a full head portrait for ages 0-70 from a single photo, modifying both texture and shape of the head. We demonstrate results on a wide variety of photos and datasets, and show significant improvement over the state of the art.



