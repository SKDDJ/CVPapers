# Arxiv Papers in cs.CV on 2020-03-01
### Understanding the Intrinsic Robustness of Image Distributions using Conditional Generative Models
- **Arxiv ID**: http://arxiv.org/abs/2003.00378v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.00378v1)
- **Published**: 2020-03-01 01:45:04+00:00
- **Updated**: 2020-03-01 01:45:04+00:00
- **Authors**: Xiao Zhang, Jinghui Chen, Quanquan Gu, David Evans
- **Comment**: 14 pages, 2 figures, 5 tables, AISTATS final paper reformatted for
  readability
- **Journal**: None
- **Summary**: Starting with Gilmer et al. (2018), several works have demonstrated the inevitability of adversarial examples based on different assumptions about the underlying input probability space. It remains unclear, however, whether these results apply to natural image distributions. In this work, we assume the underlying data distribution is captured by some conditional generative model, and prove intrinsic robustness bounds for a general class of classifiers, which solves an open problem in Fawzi et al. (2018). Building upon the state-of-the-art conditional generative models, we study the intrinsic robustness of two common image benchmarks under $\ell_2$ perturbations, and show the existence of a large gap between the robustness limits implied by our theory and the adversarial robustness achieved by current state-of-the-art robust models. Code for all our experiments is available at https://github.com/xiaozhanguva/Intrinsic-Rob.



### Unblind Your Apps: Predicting Natural-Language Labels for Mobile GUI Components by Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2003.00380v2
- **DOI**: 10.1145/3377811.3380327
- **Categories**: **cs.HC**, cs.CV, cs.SE, D.2; H.5.0
- **Links**: [PDF](http://arxiv.org/pdf/2003.00380v2)
- **Published**: 2020-03-01 02:31:26+00:00
- **Updated**: 2020-07-02 11:38:28+00:00
- **Authors**: Jieshan Chen, Chunyang Chen, Zhenchang Xing, Xiwei Xu, Liming Zhu, Guoqiang Li, Jinshui Wang
- **Comment**: Accepted to 42nd International Conference on Software Engineering
- **Journal**: None
- **Summary**: According to the World Health Organization(WHO), it is estimated that approximately 1.3 billion people live with some forms of vision impairment globally, of whom 36 million are blind. Due to their disability, engaging these minority into the society is a challenging problem. The recent rise of smart mobile phones provides a new solution by enabling blind users' convenient access to the information and service for understanding the world. Users with vision impairment can adopt the screen reader embedded in the mobile operating systems to read the content of each screen within the app, and use gestures to interact with the phone. However, the prerequisite of using screen readers is that developers have to add natural-language labels to the image-based components when they are developing the app. Unfortunately, more than 77% apps have issues of missing labels, according to our analysis of 10,408 Android apps. Most of these issues are caused by developers' lack of awareness and knowledge in considering the minority. And even if developers want to add the labels to UI components, they may not come up with concise and clear description as most of them are of no visual issues. To overcome these challenges, we develop a deep-learning based model, called LabelDroid, to automatically predict the labels of image-based buttons by learning from large-scale commercial apps in Google Play. The experimental results show that our model can make accurate predictions and the generated labels are of higher quality than that from real Android developers.



### Say As You Wish: Fine-grained Control of Image Caption Generation with Abstract Scene Graphs
- **Arxiv ID**: http://arxiv.org/abs/2003.00387v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2003.00387v1)
- **Published**: 2020-03-01 03:34:07+00:00
- **Updated**: 2020-03-01 03:34:07+00:00
- **Authors**: Shizhe Chen, Qin Jin, Peng Wang, Qi Wu
- **Comment**: To be appeared in CVPR 2020
- **Journal**: None
- **Summary**: Humans are able to describe image contents with coarse to fine details as they wish. However, most image captioning models are intention-agnostic which can not generate diverse descriptions according to different user intentions initiatively. In this work, we propose the Abstract Scene Graph (ASG) structure to represent user intention in fine-grained level and control what and how detailed the generated description should be. The ASG is a directed graph consisting of three types of \textbf{abstract nodes} (object, attribute, relationship) grounded in the image without any concrete semantic labels. Thus it is easy to obtain either manually or automatically. From the ASG, we propose a novel ASG2Caption model, which is able to recognise user intentions and semantics in the graph, and therefore generate desired captions according to the graph structure. Our model achieves better controllability conditioning on ASGs than carefully designed baselines on both VisualGenome and MSCOCO datasets. It also significantly improves the caption diversity via automatically sampling diverse ASGs as control signals.



### Joint Wasserstein Distribution Matching
- **Arxiv ID**: http://arxiv.org/abs/2003.00389v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.00389v1)
- **Published**: 2020-03-01 03:39:00+00:00
- **Updated**: 2020-03-01 03:39:00+00:00
- **Authors**: JieZhang Cao, Langyuan Mo, Qing Du, Yong Guo, Peilin Zhao, Junzhou Huang, Mingkui Tan
- **Comment**: This paper is accepted by Chinese Journal of Computers in 2020
- **Journal**: None
- **Summary**: Joint distribution matching (JDM) problem, which aims to learn bidirectional mappings to match joint distributions of two domains, occurs in many machine learning and computer vision applications. This problem, however, is very difficult due to two critical challenges: (i) it is often difficult to exploit sufficient information from the joint distribution to conduct the matching; (ii) this problem is hard to formulate and optimize. In this paper, relying on optimal transport theory, we propose to address JDM problem by minimizing the Wasserstein distance of the joint distributions in two domains. However, the resultant optimization problem is still intractable. We then propose an important theorem to reduce the intractable problem into a simple optimization problem, and develop a novel method (called Joint Wasserstein Distribution Matching (JWDM)) to solve it. In the experiments, we apply our method to unsupervised image translation and cross-domain video synthesis. Both qualitative and quantitative comparisons demonstrate the superior performance of our method over several state-of-the-arts.



### Fine-grained Video-Text Retrieval with Hierarchical Graph Reasoning
- **Arxiv ID**: http://arxiv.org/abs/2003.00392v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2003.00392v1)
- **Published**: 2020-03-01 03:44:19+00:00
- **Updated**: 2020-03-01 03:44:19+00:00
- **Authors**: Shizhe Chen, Yida Zhao, Qin Jin, Qi Wu
- **Comment**: To be appeared in CVPR 2020
- **Journal**: None
- **Summary**: Cross-modal retrieval between videos and texts has attracted growing attentions due to the rapid emergence of videos on the web. The current dominant approach for this problem is to learn a joint embedding space to measure cross-modal similarities. However, simple joint embeddings are insufficient to represent complicated visual and textual details, such as scenes, objects, actions and their compositions. To improve fine-grained video-text retrieval, we propose a Hierarchical Graph Reasoning (HGR) model, which decomposes video-text matching into global-to-local levels. To be specific, the model disentangles texts into hierarchical semantic graph including three levels of events, actions, entities and relationships across levels. Attention-based graph reasoning is utilized to generate hierarchical textual embeddings, which can guide the learning of diverse and hierarchical video representations. The HGR model aggregates matchings from different video-text levels to capture both global and local details. Experimental results on three video-text datasets demonstrate the advantages of our model. Such hierarchical decomposition also enables better generalization across datasets and improves the ability to distinguish fine-grained semantic differences.



### Deep Active Learning for Biased Datasets via Fisher Kernel Self-Supervision
- **Arxiv ID**: http://arxiv.org/abs/2003.00393v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2003.00393v1)
- **Published**: 2020-03-01 03:56:32+00:00
- **Updated**: 2020-03-01 03:56:32+00:00
- **Authors**: Denis Gudovskiy, Alec Hodgkinson, Takuya Yamaguchi, Sotaro Tsukizawa
- **Comment**: Accepted to CVPR 2020. Preprint
- **Journal**: None
- **Summary**: Active learning (AL) aims to minimize labeling efforts for data-demanding deep neural networks (DNNs) by selecting the most representative data points for annotation. However, currently used methods are ill-equipped to deal with biased data. The main motivation of this paper is to consider a realistic setting for pool-based semi-supervised AL, where the unlabeled collection of train data is biased. We theoretically derive an optimal acquisition function for AL in this setting. It can be formulated as distribution shift minimization between unlabeled train data and weakly-labeled validation dataset. To implement such acquisition function, we propose a low-complexity method for feature density matching using self-supervised Fisher kernel (FK) as well as several novel pseudo-label estimators. Our FK-based method outperforms state-of-the-art methods on MNIST, SVHN, and ImageNet classification while requiring only 1/10th of processing. The conducted experiments show at least 40% drop in labeling efforts for the biased class-imbalanced data compared to existing methods.



### Intelligent Home 3D: Automatic 3D-House Design from Linguistic Descriptions Only
- **Arxiv ID**: http://arxiv.org/abs/2003.00397v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.00397v1)
- **Published**: 2020-03-01 04:28:48+00:00
- **Updated**: 2020-03-01 04:28:48+00:00
- **Authors**: Qi Chen, Qi Wu, Rui Tang, Yuhan Wang, Shuai Wang, Mingkui Tan
- **Comment**: To appear in CVPR2020
- **Journal**: None
- **Summary**: Home design is a complex task that normally requires architects to finish with their professional skills and tools. It will be fascinating that if one can produce a house plan intuitively without knowing much knowledge about home design and experience of using complex designing tools, for example, via natural language. In this paper, we formulate it as a language conditioned visual content generation problem that is further divided into a floor plan generation and an interior texture (such as floor and wall) synthesis task. The only control signal of the generation process is the linguistic expression given by users that describe the house details. To this end, we propose a House Plan Generative Model (HPGM) that first translates the language input to a structural graph representation and then predicts the layout of rooms with a Graph Conditioned Layout Prediction Network (GC LPN) and generates the interior texture with a Language Conditioned Texture GAN (LCT-GAN). With some post-processing, the final product of this task is a 3D house model. To train and evaluate our model, we build the first Text-to-3D House Model dataset.



### Why is the Mahalanobis Distance Effective for Anomaly Detection?
- **Arxiv ID**: http://arxiv.org/abs/2003.00402v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.00402v2)
- **Published**: 2020-03-01 04:48:36+00:00
- **Updated**: 2020-04-30 11:42:33+00:00
- **Authors**: Ryo Kamoi, Kei Kobayashi
- **Comment**: None
- **Journal**: None
- **Summary**: The Mahalanobis distance-based confidence score, a recently proposed anomaly detection method for pre-trained neural classifiers, achieves state-of-the-art performance on both out-of-distribution (OoD) and adversarial examples detection. This work analyzes why this method exhibits such strong performance in practical settings while imposing an implausible assumption; namely, that class conditional distributions of pre-trained features have tied covariance. Although the Mahalanobis distance-based method is claimed to be motivated by classification prediction confidence, we find that its superior performance stems from information not useful for classification. This suggests that the reason the Mahalanobis confidence score works so well is mistaken, and makes use of different information from ODIN, another popular OoD detection method based on prediction confidence. This perspective motivates us to combine these two methods, and the combined detector exhibits improved performance and robustness. These findings provide insight into the behavior of neural classifiers in response to anomalous inputs.



### Cops-Ref: A new Dataset and Task on Compositional Referring Expression Comprehension
- **Arxiv ID**: http://arxiv.org/abs/2003.00403v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.00403v1)
- **Published**: 2020-03-01 04:59:38+00:00
- **Updated**: 2020-03-01 04:59:38+00:00
- **Authors**: Zhenfang Chen, Peng Wang, Lin Ma, Kwan-Yee K. Wong, Qi Wu
- **Comment**: To appear in CVPR2020
- **Journal**: None
- **Summary**: Referring expression comprehension (REF) aims at identifying a particular object in a scene by a natural language expression. It requires joint reasoning over the textual and visual domains to solve the problem. Some popular referring expression datasets, however, fail to provide an ideal test bed for evaluating the reasoning ability of the models, mainly because 1) their expressions typically describe only some simple distinctive properties of the object and 2) their images contain limited distracting information. To bridge the gap, we propose a new dataset for visual reasoning in context of referring expression comprehension with two main features. First, we design a novel expression engine rendering various reasoning logics that can be flexibly combined with rich visual properties to generate expressions with varying compositionality. Second, to better exploit the full reasoning chain embodied in an expression, we propose a new test setting by adding additional distracting images containing objects sharing similar properties with the referent, thus minimising the success rate of reasoning-free cross-domain alignment. We evaluate several state-of-the-art REF models, but find none of them can achieve promising performance. A proposed modular hard mining strategy performs the best but still leaves substantial room for improvement. We hope this new dataset and task can serve as a benchmark for deeper visual reasoning analysis and foster the research on referring expression comprehension.



### FMT:Fusing Multi-task Convolutional Neural Network for Person Search
- **Arxiv ID**: http://arxiv.org/abs/2003.00406v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.00406v1)
- **Published**: 2020-03-01 05:20:47+00:00
- **Updated**: 2020-03-01 05:20:47+00:00
- **Authors**: Sulan Zhai, Shunqiang Liu, Xiao Wang, Jin Tang
- **Comment**: Published on Multimedia Tools and Applications
- **Journal**: None
- **Summary**: Person search is to detect all persons and identify the query persons from detected persons in the image without proposals and bounding boxes, which is different from person re-identification. In this paper, we propose a fusing multi-task convolutional neural network(FMT-CNN) to tackle the correlation and heterogeneity of detection and re-identification with a single convolutional neural network. We focus on how the interplay of person detection and person re-identification affects the overall performance. We employ person labels in region proposal network to produce features for person re-identification and person detection network, which can improve the accuracy of detection and re-identification simultaneously. We also use a multiple loss to train our re-identification network. Experiment results on CUHK-SYSU Person Search dataset show that the performance of our proposed method is superior to state-of-the-art approaches in both mAP and top-1.



### PF-Net: Point Fractal Network for 3D Point Cloud Completion
- **Arxiv ID**: http://arxiv.org/abs/2003.00410v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2003.00410v1)
- **Published**: 2020-03-01 05:40:21+00:00
- **Updated**: 2020-03-01 05:40:21+00:00
- **Authors**: Zitian Huang, Yikuan Yu, Jiawen Xu, Feng Ni, Xinyi Le
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a Point Fractal Network (PF-Net), a novel learning-based approach for precise and high-fidelity point cloud completion. Unlike existing point cloud completion networks, which generate the overall shape of the point cloud from the incomplete point cloud and always change existing points and encounter noise and geometrical loss, PF-Net preserves the spatial arrangements of the incomplete point cloud and can figure out the detailed geometrical structure of the missing region(s) in the prediction. To succeed at this task, PF-Net estimates the missing point cloud hierarchically by utilizing a feature-points-based multi-scale generating network. Further, we add up multi-stage completion loss and adversarial loss to generate more realistic missing region(s). The adversarial loss can better tackle multiple modes in the prediction. Our experiments demonstrate the effectiveness of our method for several challenging point cloud completion tasks.



### Towards Automatic Face-to-Face Translation
- **Arxiv ID**: http://arxiv.org/abs/2003.00418v1
- **DOI**: 10.1145/3343031.3351066
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MM, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/2003.00418v1)
- **Published**: 2020-03-01 06:42:43+00:00
- **Updated**: 2020-03-01 06:42:43+00:00
- **Authors**: Prajwal K R, Rudrabha Mukhopadhyay, Jerin Philip, Abhishek Jha, Vinay Namboodiri, C. V. Jawahar
- **Comment**: 9 pages (including references), 5 figures, Published in ACM
  Multimedia, 2019
- **Journal**: MM '19: Proceedings of the 27th ACM International Conference on
  Multimedia; October 2019; Pages 1428-1436
- **Summary**: In light of the recent breakthroughs in automatic machine translation systems, we propose a novel approach that we term as "Face-to-Face Translation". As today's digital communication becomes increasingly visual, we argue that there is a need for systems that can automatically translate a video of a person speaking in language A into a target language B with realistic lip synchronization. In this work, we create an automatic pipeline for this problem and demonstrate its impact on multiple real-world applications. First, we build a working speech-to-speech translation system by bringing together multiple existing modules from speech and language. We then move towards "Face-to-Face Translation" by incorporating a novel visual module, LipGAN for generating realistic talking faces from the translated audio. Quantitative evaluation of LipGAN on the standard LRW test set shows that it significantly outperforms existing approaches across all standard metrics. We also subject our Face-to-Face Translation pipeline, to multiple human evaluations and show that it can significantly improve the overall user experience for consuming and interacting with multimodal content across languages. Code, models and demo video are made publicly available.   Demo video: https://www.youtube.com/watch?v=aHG6Oei8jF0   Code and models: https://github.com/Rudrabha/LipGAN



### Learning When and Where to Zoom with Deep Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2003.00425v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.00425v2)
- **Published**: 2020-03-01 07:16:46+00:00
- **Updated**: 2020-04-20 18:25:16+00:00
- **Authors**: Burak Uzkent, Stefano Ermon
- **Comment**: To appear in CVPR 2020 as an Oral Presentation. The code can be found
  at https://github.com/ermongroup/PatchDrop
- **Journal**: None
- **Summary**: While high resolution images contain semantically more useful information than their lower resolution counterparts, processing them is computationally more expensive, and in some applications, e.g. remote sensing, they can be much more expensive to acquire. For these reasons, it is desirable to develop an automatic method to selectively use high resolution data when necessary while maintaining accuracy and reducing acquisition/run-time cost. In this direction, we propose PatchDrop a reinforcement learning approach to dynamically identify when and where to use/acquire high resolution data conditioned on the paired, cheap, low resolution images. We conduct experiments on CIFAR10, CIFAR100, ImageNet and fMoW datasets where we use significantly less high resolution data while maintaining similar accuracy to models which use full high resolution images.



### Deep Learning for Content-based Personalized Viewport Prediction of 360-Degree VR Videos
- **Arxiv ID**: http://arxiv.org/abs/2003.00429v1
- **DOI**: 10.1109/LNET.2020.2977124
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.00429v1)
- **Published**: 2020-03-01 07:31:50+00:00
- **Updated**: 2020-03-01 07:31:50+00:00
- **Authors**: Xinwei Chen, Ali Taleb Zadeh Kasgari, Walid Saad
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, the problem of head movement prediction for virtual reality videos is studied. In the considered model, a deep learning network is introduced to leverage position data as well as video frame content to predict future head movement. For optimizing data input into this neural network, data sample rate, reduced data, and long-period prediction length are also explored for this model. Simulation results show that the proposed approach yields 16.1\% improvement in terms of prediction accuracy compared to a baseline approach that relies only on the position data.



### STC-Flow: Spatio-temporal Context-aware Optical Flow Estimation
- **Arxiv ID**: http://arxiv.org/abs/2003.00434v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.00434v2)
- **Published**: 2020-03-01 08:18:57+00:00
- **Updated**: 2020-11-03 09:11:06+00:00
- **Authors**: Xiaolin Song, Yuyang Zhao, Jingyu Yang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a spatio-temporal contextual network, STC-Flow, for optical flow estimation. Unlike previous optical flow estimation approaches with local pyramid feature extraction and multi-level correlation, we propose a contextual relation exploration architecture by capturing rich long-range dependencies in spatial and temporal dimensions. Specifically, STC-Flow contains three key context modules - pyramidal spatial context module, temporal context correlation module and recurrent residual contextual upsampling module, to build the relationship in each stage of feature extraction, correlation, and flow reconstruction, respectively. Experimental results indicate that the proposed scheme achieves the state-of-the-art performance of two-frame based methods on the Sintel dataset and the KITTI 2012/2015 datasets.



### Learning from Suspected Target: Bootstrapping Performance for Breast Cancer Detection in Mammography
- **Arxiv ID**: http://arxiv.org/abs/2003.01109v1
- **DOI**: 10.1007/978-3-030-32226-7_52
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.01109v1)
- **Published**: 2020-03-01 09:04:24+00:00
- **Updated**: 2020-03-01 09:04:24+00:00
- **Authors**: Li Xiao, Cheng Zhu, Junjun Liu, Chunlong Luo, Peifang Liu, Yi Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning object detection algorithm has been widely used in medical image analysis. Currently all the object detection tasks are based on the data annotated with object classes and their bounding boxes. On the other hand, medical images such as mammography usually contain normal regions or objects that are similar to the lesion region, and may be misclassified in the testing stage if they are not taken care of. In this paper, we address such problem by introducing a novel top likelihood loss together with a new sampling procedure to select and train the suspected target regions, as well as proposing a similarity loss to further identify suspected targets from targets. Mean average precision (mAP) according to the predicted targets and specificity, sensitivity, accuracy, AUC values according to classification of patients are adopted for performance comparisons. We firstly test our proposed method on a private dense mammogram dataset. Results show that our proposed method greatly reduce the false positive rate and the specificity is increased by 0.25 on detecting mass type cancer. It is worth mention that dense breast typically has a higher risk for developing breast cancers and also are harder for cancer detection in diagnosis, and our method outperforms a reported result from performance of radiologists. Our method is also validated on the public Digital Database for Screening Mammography (DDSM) dataset, brings significant improvement on mass type cancer detection and outperforms the most state-of-the-art work.



### Environment-agnostic Multitask Learning for Natural Language Grounded Navigation
- **Arxiv ID**: http://arxiv.org/abs/2003.00443v5
- **DOI**: None
- **Categories**: **cs.AI**, cs.CL, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2003.00443v5)
- **Published**: 2020-03-01 09:06:31+00:00
- **Updated**: 2020-07-21 02:54:38+00:00
- **Authors**: Xin Eric Wang, Vihan Jain, Eugene Ie, William Yang Wang, Zornitsa Kozareva, Sujith Ravi
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: Recent research efforts enable study for natural language grounded navigation in photo-realistic environments, e.g., following natural language instructions or dialog. However, existing methods tend to overfit training data in seen environments and fail to generalize well in previously unseen environments. To close the gap between seen and unseen environments, we aim at learning a generalized navigation model from two novel perspectives: (1) we introduce a multitask navigation model that can be seamlessly trained on both Vision-Language Navigation (VLN) and Navigation from Dialog History (NDH) tasks, which benefits from richer natural language guidance and effectively transfers knowledge across tasks; (2) we propose to learn environment-agnostic representations for the navigation policy that are invariant among the environments seen during training, thus generalizing better on unseen environments. Extensive experiments show that environment-agnostic multitask learning significantly reduces the performance gap between seen and unseen environments, and the navigation agent trained so outperforms baselines on unseen environments by 16% (relative measure on success rate) on VLN and 120% (goal progress) on NDH. Our submission to the CVDN leaderboard establishes a new state-of-the-art for the NDH task on the holdout test set. Code is available at https://github.com/google-research/valan.



### Dimensionality reduction to maximize prediction generalization capability
- **Arxiv ID**: http://arxiv.org/abs/2003.00470v2
- **DOI**: 10.1038/s42256-021-00306-1
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.00470v2)
- **Published**: 2020-03-01 12:04:59+00:00
- **Updated**: 2022-01-20 08:48:42+00:00
- **Authors**: Takuya Isomura, Taro Toyoizumi
- **Comment**: None
- **Journal**: Nature Machine Intelligence 3, 434-446 (2021)
- **Summary**: Generalization of time series prediction remains an important open issue in machine learning, wherein earlier methods have either large generalization error or local minima. We develop an analytically solvable, unsupervised learning scheme that extracts the most informative components for predicting future inputs, termed predictive principal component analysis (PredPCA). Our scheme can effectively remove unpredictable noise and minimize test prediction error through convex optimization. Mathematical analyses demonstrate that, provided with sufficient training samples and sufficiently high-dimensional observations, PredPCA can asymptotically identify hidden states, system parameters, and dimensionalities of canonical nonlinear generative processes, with a global convergence guarantee. We demonstrate the performance of PredPCA using sequential visual inputs comprising hand-digits, rotating 3D objects, and natural scenes. It reliably estimates distinct hidden states and predicts future outcomes of previously unseen test input data, based exclusively on noisy observations. The simple architecture and low computational cost of PredPCA are highly desirable for neuromorphic hardware.



### State-Aware Tracker for Real-Time Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2003.00482v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.00482v1)
- **Published**: 2020-03-01 12:48:20+00:00
- **Updated**: 2020-03-01 12:48:20+00:00
- **Authors**: Xi Chen, Zuoxin Li, Ye Yuan, Gang Yu, Jianxin Shen, Donglian Qi
- **Comment**: Accepted by CVPR2020
- **Journal**: None
- **Summary**: In this work, we address the task of semi-supervised video object segmentation(VOS) and explore how to make efficient use of video property to tackle the challenge of semi-supervision. We propose a novel pipeline called State-Aware Tracker(SAT), which can produce accurate segmentation results with real-time speed. For higher efficiency, SAT takes advantage of the inter-frame consistency and deals with each target object as a tracklet. For more stable and robust performance over video sequences, SAT gets awareness for each state and makes self-adaptation via two feedback loops. One loop assists SAT in generating more stable tracklets. The other loop helps to construct a more robust and holistic target representation. SAT achieves a promising result of 72.3% J&F mean with 39 FPS on DAVIS2017-Val dataset, which shows a decent trade-off between efficiency and accuracy. Code will be released at github.com/MegviiDetection/video_analyst.



### PointASNL: Robust Point Clouds Processing using Nonlocal Neural Networks with Adaptive Sampling
- **Arxiv ID**: http://arxiv.org/abs/2003.00492v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.00492v3)
- **Published**: 2020-03-01 14:04:08+00:00
- **Updated**: 2020-05-05 07:46:18+00:00
- **Authors**: Xu Yan, Chaoda Zheng, Zhen Li, Sheng Wang, Shuguang Cui
- **Comment**: To appear in CVPR 2020. Also seen in
  http://kaldir.vc.in.tum.de/scannet_benchmark/
- **Journal**: None
- **Summary**: Raw point clouds data inevitably contains outliers or noise through acquisition from 3D sensors or reconstruction algorithms. In this paper, we present a novel end-to-end network for robust point clouds processing, named PointASNL, which can deal with point clouds with noise effectively. The key component in our approach is the adaptive sampling (AS) module. It first re-weights the neighbors around the initial sampled points from farthest point sampling (FPS), and then adaptively adjusts the sampled points beyond the entire point cloud. Our AS module can not only benefit the feature learning of point clouds, but also ease the biased effect of outliers. To further capture the neighbor and long-range dependencies of the sampled point, we proposed a local-nonlocal (L-NL) module inspired by the nonlocal operation. Such L-NL module enables the learning process insensitive to noise. Extensive experiments verify the robustness and superiority of our approach in point clouds processing tasks regardless of synthesis data, indoor data, and outdoor data with or without noise. Specifically, PointASNL achieves state-of-the-art robust performance for classification and segmentation tasks on all datasets, and significantly outperforms previous methods on real-world outdoor SemanticKITTI dataset with considerate noise. Our code is released through https://github.com/yanx27/PointASNL.



### MonoPair: Monocular 3D Object Detection Using Pairwise Spatial Relationships
- **Arxiv ID**: http://arxiv.org/abs/2003.00504v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2003.00504v1)
- **Published**: 2020-03-01 15:37:48+00:00
- **Updated**: 2020-03-01 15:37:48+00:00
- **Authors**: Yongjian Chen, Lei Tai, Kai Sun, Mingyang Li
- **Comment**: CVPR 2020 accepted
- **Journal**: None
- **Summary**: Monocular 3D object detection is an essential component in autonomous driving while challenging to solve, especially for those occluded samples which are only partially visible. Most detectors consider each 3D object as an independent training target, inevitably resulting in a lack of useful information for occluded samples. To this end, we propose a novel method to improve the monocular 3D object detection by considering the relationship of paired samples. This allows us to encode spatial constraints for partially-occluded objects from their adjacent neighbors. Specifically, the proposed detector computes uncertainty-aware predictions for object locations and 3D distances for the adjacent object pairs, which are subsequently jointly optimized by nonlinear least squares. Finally, the one-stage uncertainty-aware prediction structure and the post-optimization module are dedicatedly integrated for ensuring the run-time efficiency. Experiments demonstrate that our method yields the best performance on KITTI 3D detection benchmark, by outperforming state-of-the-art competitors by wide margins, especially for the hard samples.



### Deep Attention Aware Feature Learning for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2003.00517v1
- **DOI**: 10.1016/j.patcog.2022.108567
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.00517v1)
- **Published**: 2020-03-01 16:27:14+00:00
- **Updated**: 2020-03-01 16:27:14+00:00
- **Authors**: Yifan Chen, Han Wang, Xiaolu Sun, Bin Fan, Chu Tang
- **Comment**: None
- **Journal**: None
- **Summary**: Visual attention has proven to be effective in improving the performance of person re-identification. Most existing methods apply visual attention heuristically by learning an additional attention map to re-weight the feature maps for person re-identification. However, this kind of methods inevitably increase the model complexity and inference time. In this paper, we propose to incorporate the attention learning as additional objectives in a person ReID network without changing the original structure, thus maintain the same inference time and model size. Two kinds of attentions have been considered to make the learned feature maps being aware of the person and related body parts respectively. Globally, a holistic attention branch (HAB) makes the feature maps obtained by backbone focus on persons so as to alleviate the influence of background. Locally, a partial attention branch (PAB) makes the extracted features be decoupled into several groups and be separately responsible for different body parts (i.e., keypoints), thus increasing the robustness to pose variation and partial occlusion. These two kinds of attentions are universal and can be incorporated into existing ReID networks. We have tested its performance on two typical networks (TriNet and Bag of Tricks) and observed significant performance improvement on five widely used datasets.



### ZoomNet: Part-Aware Adaptive Zooming Neural Network for 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2003.00529v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.00529v1)
- **Published**: 2020-03-01 17:18:08+00:00
- **Updated**: 2020-03-01 17:18:08+00:00
- **Authors**: Zhenbo Xu, Wei Zhang, Xiaoqing Ye, Xiao Tan, Wei Yang, Shilei Wen, Errui Ding, Ajin Meng, Liusheng Huang
- **Comment**: Accpeted by AAAI 2020 as Oral presentation; The github page will be
  updated in March,2020
- **Journal**: None
- **Summary**: 3D object detection is an essential task in autonomous driving and robotics. Though great progress has been made, challenges remain in estimating 3D pose for distant and occluded objects. In this paper, we present a novel framework named ZoomNet for stereo imagery-based 3D detection. The pipeline of ZoomNet begins with an ordinary 2D object detection model which is used to obtain pairs of left-right bounding boxes. To further exploit the abundant texture cues in RGB images for more accurate disparity estimation, we introduce a conceptually straight-forward module -- adaptive zooming, which simultaneously resizes 2D instance bounding boxes to a unified resolution and adjusts the camera intrinsic parameters accordingly. In this way, we are able to estimate higher-quality disparity maps from the resized box images then construct dense point clouds for both nearby and distant objects. Moreover, we introduce to learn part locations as complementary features to improve the resistance against occlusion and put forward the 3D fitting score to better estimate the 3D detection quality. Extensive experiments on the popular KITTI 3D detection dataset indicate ZoomNet surpasses all previous state-of-the-art methods by large margins (improved by 9.4% on APbv (IoU=0.7) over pseudo-LiDAR). Ablation study also demonstrates that our adaptive zooming strategy brings an improvement of over 10% on AP3d (IoU=0.7). In addition, since the official KITTI benchmark lacks fine-grained annotations like pixel-wise part locations, we also present our KFG dataset by augmenting KITTI with detailed instance-wise annotations including pixel-wise part location, pixel-wise disparity, etc.. Both the KFG dataset and our codes will be publicly available at https://github.com/detectRecog/ZoomNet.



### 3DCFS: Fast and Robust Joint 3D Semantic-Instance Segmentation via Coupled Feature Selection
- **Arxiv ID**: http://arxiv.org/abs/2003.00535v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.00535v1)
- **Published**: 2020-03-01 17:48:17+00:00
- **Updated**: 2020-03-01 17:48:17+00:00
- **Authors**: Liang Du, Jingang Tan, Xiangyang Xue, Lili Chen, Hongkai Wen, Jianfeng Feng, Jiamao Li, Xiaolin Zhang
- **Comment**: icra 2020
- **Journal**: None
- **Summary**: We propose a novel fast and robust 3D point clouds segmentation framework via coupled feature selection, named 3DCFS, that jointly performs semantic and instance segmentation. Inspired by the human scene perception process, we design a novel coupled feature selection module, named CFSM, that adaptively selects and fuses the reciprocal semantic and instance features from two tasks in a coupled manner. To further boost the performance of the instance segmentation task in our 3DCFS, we investigate a loss function that helps the model learn to balance the magnitudes of the output embedding dimensions during training, which makes calculating the Euclidean distance more reliable and enhances the generalizability of the model. Extensive experiments demonstrate that our 3DCFS outperforms state-of-the-art methods on benchmark datasets in terms of accuracy, speed and computational cost.



### Soft-Root-Sign Activation Function
- **Arxiv ID**: http://arxiv.org/abs/2003.00547v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.00547v1)
- **Published**: 2020-03-01 18:38:11+00:00
- **Updated**: 2020-03-01 18:38:11+00:00
- **Authors**: Yuan Zhou, Dandan Li, Shuwei Huo, Sun-Yuan Kung
- **Comment**: None
- **Journal**: None
- **Summary**: The choice of activation function in deep networks has a significant effect on the training dynamics and task performance. At present, the most effective and widely-used activation function is ReLU. However, because of the non-zero mean, negative missing and unbounded output, ReLU is at a potential disadvantage during optimization. To this end, we introduce a novel activation function to manage to overcome the above three challenges. The proposed nonlinearity, namely "Soft-Root-Sign" (SRS), is smooth, non-monotonic, and bounded. Notably, the bounded property of SRS distinguishes itself from most state-of-the-art activation functions. In contrast to ReLU, SRS can adaptively adjust the output by a pair of independent trainable parameters to capture negative information and provide zero-mean property, which leading not only to better generalization performance, but also to faster learning speed. It also avoids and rectifies the output distribution to be scattered in the non-negative real number space, making it more compatible with batch normalization (BN) and less sensitive to initialization. In experiments, we evaluated SRS on deep networks applied to a variety of tasks, including image classification, machine translation and generative modelling. Our SRS matches or exceeds models with ReLU and other state-of-the-art nonlinearities, showing that the proposed activation function is generalized and can achieve high performance across tasks. Ablation study further verified the compatibility with BN and self-adaptability for different initialization.



### The Sloop System for Individual Animal Identification with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2003.00559v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, 68T45, I.4.9; I.4.10
- **Links**: [PDF](http://arxiv.org/pdf/2003.00559v1)
- **Published**: 2020-03-01 19:08:06+00:00
- **Updated**: 2020-03-01 19:08:06+00:00
- **Authors**: Kshitij Bakliwal, Sai Ravela
- **Comment**: To appear in WACV 2020 Workshop on Deep Learning for
  Re-Identification
- **Journal**: None
- **Summary**: The MIT Sloop system indexes and retrieves photographs from databases of non-stationary animal population distributions. To do this, it adaptively represents and matches generic visual feature representations using sparse relevance feedback from experts and crowds. Here, we describe the Sloop system and its application, then compare its approach to a standard deep learning formulation. We then show that priming with amplitude and deformation features requires very shallow networks to produce superior recognition results. Results suggest that relevance feedback, which enables Sloop's high-recall performance may also be essential for deep learning approaches to individual identification to deliver comparable results.



### Shape retrieval of non-rigid 3d human models
- **Arxiv ID**: http://arxiv.org/abs/2003.08763v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.08763v1)
- **Published**: 2020-03-01 20:03:16+00:00
- **Updated**: 2020-03-01 20:03:16+00:00
- **Authors**: David Pickup, Xianfang Sun, Paul L Rosin, Ralph R Martin, Z Cheng, Zhouhui Lian, Masaki Aono, A Ben Hamza, A Bronstein, M Bronstein, S Bu, Umberto Castellani, S Cheng, Valeria Garro, Andrea Giachetti, Afzal Godil, Luca Isaia, J Han, Henry Johan, L Lai, Bo Li, C Li, Haisheng Li, Roee Litman, X Liu, Z Liu, Yijuan Lu, L Sun, G Tam, Atsushi Tatsuma, J Ye
- **Comment**: International Journal of Computer Vision, 2016
- **Journal**: None
- **Summary**: 3D models of humans are commonly used within computer graphics and vision, and so the ability to distinguish between body shapes is an important shape retrieval problem. We extend our recent paper which provided a benchmark for testing non-rigid 3D shape retrieval algorithms on 3D human models. This benchmark provided a far stricter challenge than previous shape benchmarks. We have added 145 new models for use as a separate training set, in order to standardise the training data used and provide a fairer comparison. We have also included experiments with the FAUST dataset of human scans. All participants of the previous benchmark study have taken part in the new tests reported here, many providing updated results using the new data. In addition, further participants have also taken part, and we provide extra analysis of the retrieval results. A total of 25 different shape retrieval methods.



### FLIC: Fast Lidar Image Clustering
- **Arxiv ID**: http://arxiv.org/abs/2003.00575v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2003.00575v2)
- **Published**: 2020-03-01 20:21:31+00:00
- **Updated**: 2020-12-08 14:11:20+00:00
- **Authors**: Frederik Hasecke, Lukas Hahn, Anton Kummert
- **Comment**: 9 pages, 10 figures, accepted to appear in ICPRAM 2021
- **Journal**: None
- **Summary**: Lidar sensors are widely used in various applications, ranging from scientific fields over industrial use to integration in consumer products. With an ever growing number of different driver assistance systems, they have been introduced to automotive series production in recent years and are considered an important building block for the practical realisation of autonomous driving. However, due to the potentially large amount of Lidar points per scan, tailored algorithms are required to identify objects (e.g. pedestrians or vehicles) with high precision in a very short time. In this work, we propose an algorithmic approach for real-time instance segmentation of Lidar sensor data. We show how our method leverages the properties of the Euclidean distance to retain three-dimensional measurement information, while being narrowed down to a two-dimensional representation for fast computation. We further introduce what we call "skip connections", to make our approach robust against over-segmentation and improve assignment in cases of partial occlusion. Through detailed evaluation on public data and comparison with established methods, we show how these aspects enable state-of-the-art performance and runtime on a single CPU core.



### Rethinking Fully Convolutional Networks for the Analysis of Photoluminescence Wafer Images
- **Arxiv ID**: http://arxiv.org/abs/2003.00594v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.00594v1)
- **Published**: 2020-03-01 21:31:22+00:00
- **Updated**: 2020-03-01 21:31:22+00:00
- **Authors**: Maike Lorena Stern, Hans Lindberg, Klaus Meyer-Wegener
- **Comment**: None
- **Journal**: None
- **Summary**: The manufacturing of light-emitting diodes is a complex semiconductor-manufacturing process, interspersed with different measurements. Among the employed measurements, photoluminescence imaging has several advantages, namely being a non-destructive, fast and thus cost-effective measurement. On a photoluminescence measurement image of an LED wafer, every pixel corresponds to an LED chip's brightness after photo-excitation, revealing chip performance information. However, generating a chip-fine defect map of the LED wafer, based on photoluminescence images, proves challenging for multiple reasons: on the one hand, the measured brightness values vary from image to image, in addition to local spots of differing brightness. On the other hand, certain defect structures may assume multiple shapes, sizes and brightness gradients, where salient brightness values may correspond to defective LED chips, measurement artefacts or non-defective structures. In this work, we revisit the creation of chip-fine defect maps using fully convolutional networks and show that the problem of segmenting objects at multiple scales can be improved by the incorporation of densely connected convolutional blocks and atrous spatial pyramid pooling modules. We also share implementation details and our experiences with training networks with small datasets of measurement images. The proposed architecture significantly improves the segmentation accuracy of highly variable defect structures over our previous version.



### 3D Point Cloud Processing and Learning for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2003.00601v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2003.00601v1)
- **Published**: 2020-03-01 22:13:46+00:00
- **Updated**: 2020-03-01 22:13:46+00:00
- **Authors**: Siheng Chen, Baoan Liu, Chen Feng, Carlos Vallespi-Gonzalez, Carl Wellington
- **Comment**: IEEE Signal Processing Magazine, Special Issue on Autonomous Driving
- **Journal**: None
- **Summary**: We present a review of 3D point cloud processing and learning for autonomous driving. As one of the most important sensors in autonomous vehicles, light detection and ranging (LiDAR) sensors collect 3D point clouds that precisely record the external surfaces of objects and scenes. The tools for 3D point cloud processing and learning are critical to the map creation, localization, and perception modules in an autonomous vehicle. While much attention has been paid to data collected from cameras, such as images and videos, an increasing number of researchers have recognized the importance and significance of LiDAR in autonomous driving and have proposed processing and learning algorithms to exploit 3D point clouds. We review the recent progress in this research area and summarize what has been tried and what is needed for practical and safe autonomous vehicles. We also offer perspectives on open issues that are needed to be solved in the future.



