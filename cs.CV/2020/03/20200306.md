# Arxiv Papers in cs.CV on 2020-03-06
### Likelihood Regret: An Out-of-Distribution Detection Score For Variational Auto-encoder
- **Arxiv ID**: http://arxiv.org/abs/2003.02977v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.02977v3)
- **Published**: 2020-03-06 00:30:38+00:00
- **Updated**: 2020-10-10 21:58:14+00:00
- **Authors**: Zhisheng Xiao, Qing Yan, Yali Amit
- **Comment**: NeurIPS 2020
- **Journal**: None
- **Summary**: Deep probabilistic generative models enable modeling the likelihoods of very high dimensional data. An important application of generative modeling should be the ability to detect out-of-distribution (OOD) samples by setting a threshold on the likelihood. However, some recent studies show that probabilistic generative models can, in some cases, assign higher likelihoods on certain types of OOD samples, making the OOD detection rules based on likelihood threshold problematic. To address this issue, several OOD detection methods have been proposed for deep generative models. In this paper, we make the observation that many of these methods fail when applied to generative models based on Variational Auto-encoders (VAE). As an alternative, we propose Likelihood Regret, an efficient OOD score for VAEs. We benchmark our proposed method over existing approaches, and empirical results suggest that our method obtains the best overall OOD detection performances when applied to VAEs.



### Neural networks approach for mammography diagnosis using wavelets features
- **Arxiv ID**: http://arxiv.org/abs/2003.03000v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.03000v1)
- **Published**: 2020-03-06 02:10:47+00:00
- **Updated**: 2020-03-06 02:10:47+00:00
- **Authors**: Essam A. Rashed, and Mohamed G. Awad
- **Comment**: Reprint
- **Journal**: First Canadian Student Conference on Biomedical Computing, 2006
- **Summary**: A supervised diagnosis system for digital mammogram is developed. The diagnosis processes are done by transforming the data of the images into a feature vector using wavelets multilevel decomposition. This vector is used as the feature tailored toward separating different mammogram classes. The suggested model consists of artificial neural networks designed for classifying mammograms according to tumor type and risk level. Results are enhanced from our previous study by extracting feature vectors using multilevel decompositions instead of one level of decomposition. Radiologist-labeled images were used to evaluate the diagnosis system. Results are very promising and show possible guide for future work.



### Unifying Graph Embedding Features with Graph Convolutional Networks for Skeleton-based Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2003.03007v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.03007v2)
- **Published**: 2020-03-06 02:31:26+00:00
- **Updated**: 2022-10-11 12:32:17+00:00
- **Authors**: Dong Yang, Monica Mengqi Li, Hong Fu, Jicong Fan, Zhao Zhang, Howard Leung
- **Comment**: None
- **Journal**: None
- **Summary**: Combining skeleton structure with graph convolutional networks has achieved remarkable performance in human action recognition. Since current research focuses on designing basic graph for representing skeleton data, these embedding features contain basic topological information, which cannot learn more systematic perspectives from skeleton data. In this paper, we overcome this limitation by proposing a novel framework, which unifies 15 graph embedding features into the graph convolutional network for human action recognition, aiming to best take advantage of graph information to distinguish key joints, bones, and body parts in human action, instead of being exclusive to a single feature or domain. Additionally, we fully investigate how to find the best graph features of skeleton structure for improving human action recognition. Besides, the topological information of the skeleton sequence is explored to further enhance the performance in a multi-stream framework. Moreover, the unified graph features are extracted by the adaptive methods on the training process, which further yields improvements. Our model is validated by three large-scale datasets, namely NTU-RGB+D, Kinetics and SYSU-3D, and outperforms the state-of-the-art methods. Overall, our work unified graph embedding features to promotes systematic research on human action recognition.



### Modeling User Behaviors in Machine Operation Tasks for Adaptive Guidance
- **Arxiv ID**: http://arxiv.org/abs/2003.03025v5
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.03025v5)
- **Published**: 2020-03-06 04:05:08+00:00
- **Updated**: 2020-09-16 06:57:52+00:00
- **Authors**: Chen Long-fei, Yuichi Nakamura, Kazuaki Kondo
- **Comment**: None
- **Journal**: None
- **Summary**: An adaptive guidance system that supports equipment operators requires a comprehensive model, which involves a variety of user behaviors that considers different skill and knowledge levels, as well as rapid-changing task situations. In the present paper, we introduced a novel method for modeling operational tasks, aiming to integrate visual operation records provided by users with diverse experience levels and personal characteristics. For this purpose, we investigated the relationships between user behavior patterns that could be visually observed and their skill levels under machine operation conditions. We considered 144 samples of two sewing tasks performed by 12 operators using a head-mounted RGB-D camera and a static gaze tracker. Behavioral features, such as the operator's gaze and head movements, hand interactions, and hotspots, were observed with significant behavioral trends resulting from continuous user skill improvement. We used a two-step method to model the diversity of user behavior: prototype selection and experience integration based on skill ranking. The experimental results showed that several features could serve as appropriate indices for user skill evaluation, as well as providing valuable clues for revealing personal behavioral characteristics. The integration of user records with different skills and operational habits allowed developing a rich, inclusive task model that could be used flexibly to adapt to diverse user-specific needs.



### DA4AD: End-to-End Deep Attention-based Visual Localization for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2003.03026v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2003.03026v2)
- **Published**: 2020-03-06 04:34:39+00:00
- **Updated**: 2020-07-13 17:31:33+00:00
- **Authors**: Yao Zhou, Guowei Wan, Shenhua Hou, Li Yu, Gang Wang, Xiaofei Rui, Shiyu Song
- **Comment**: 19 pages, 4 figures, Accepted by ECCV 2020
- **Journal**: None
- **Summary**: We present a visual localization framework based on novel deep attention aware features for autonomous driving that achieves centimeter level localization accuracy. Conventional approaches to the visual localization problem rely on handcrafted features or human-made objects on the road. They are known to be either prone to unstable matching caused by severe appearance or lighting changes, or too scarce to deliver constant and robust localization results in challenging scenarios. In this work, we seek to exploit the deep attention mechanism to search for salient, distinctive and stable features that are good for long-term matching in the scene through a novel end-to-end deep neural network. Furthermore, our learned feature descriptors are demonstrated to be competent to establish robust matches and therefore successfully estimate the optimal camera poses with high precision. We comprehensively validate the effectiveness of our method using a freshly collected dataset with high-quality ground truth trajectories and hardware synchronization between sensors. Results demonstrate that our method achieves a competitive localization accuracy when compared to the LiDAR-based localization solutions under various challenging circumstances, leading to a potential low-cost localization solution for autonomous driving.



### Clean-Label Backdoor Attacks on Video Recognition Models
- **Arxiv ID**: http://arxiv.org/abs/2003.03030v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.03030v2)
- **Published**: 2020-03-06 04:51:48+00:00
- **Updated**: 2020-06-16 12:13:20+00:00
- **Authors**: Shihao Zhao, Xingjun Ma, Xiang Zheng, James Bailey, Jingjing Chen, Yu-Gang Jiang
- **Comment**: CVPR2020
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) are vulnerable to backdoor attacks which can hide backdoor triggers in DNNs by poisoning training data. A backdoored model behaves normally on clean test images, yet consistently predicts a particular target class for any test examples that contain the trigger pattern. As such, backdoor attacks are hard to detect, and have raised severe security concerns in real-world applications. Thus far, backdoor research has mostly been conducted in the image domain with image classification models. In this paper, we show that existing image backdoor attacks are far less effective on videos, and outline 4 strict conditions where existing attacks are likely to fail: 1) scenarios with more input dimensions (eg. videos), 2) scenarios with high resolution, 3) scenarios with a large number of classes and few examples per class (a "sparse dataset"), and 4) attacks with access to correct labels (eg. clean-label attacks). We propose the use of a universal adversarial trigger as the backdoor trigger to attack video recognition models, a situation where backdoor attacks are likely to be challenged by the above 4 strict conditions. We show on benchmark video datasets that our proposed backdoor attack can manipulate state-of-the-art video models with high success rates by poisoning only a small proportion of training data (without changing the labels). We also show that our proposed backdoor attack is resistant to state-of-the-art backdoor defense/detection methods, and can even be applied to improve image backdoor attacks. Our proposed video backdoor attack not only serves as a strong baseline for improving the robustness of video models, but also provides a new perspective for more understanding more powerful backdoor attacks.



### DeProCams: Simultaneous Relighting, Compensation and Shape Reconstruction for Projector-Camera Systems
- **Arxiv ID**: http://arxiv.org/abs/2003.03040v2
- **DOI**: 10.1109/TVCG.2021.3067771
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2003.03040v2)
- **Published**: 2020-03-06 05:49:16+00:00
- **Updated**: 2021-01-24 09:19:14+00:00
- **Authors**: Bingyao Huang, Haibin Ling
- **Comment**: Source code and supplementary material at:
  https://github.com/BingyaoHuang/DeProCams
- **Journal**: None
- **Summary**: Image-based relighting, projector compensation and depth/normal reconstruction are three important tasks of projector-camera systems (ProCams) and spatial augmented reality (SAR). Although they share a similar pipeline of finding projector-camera image mappings, in tradition, they are addressed independently, sometimes with different prerequisites, devices and sampling images. In practice, this may be cumbersome for SAR applications to address them one-by-one. In this paper, we propose a novel end-to-end trainable model named DeProCams to explicitly learn the photometric and geometric mappings of ProCams, and once trained, DeProCams can be applied simultaneously to the three tasks. DeProCams explicitly decomposes the projector-camera image mappings into three subprocesses: shading attributes estimation, rough direct light estimation and photorealistic neural rendering. A particular challenge addressed by DeProCams is occlusion, for which we exploit epipolar constraint and propose a novel differentiable projector direct light mask. Thus, it can be learned end-to-end along with the other modules. Afterwards, to improve convergence, we apply photometric and geometric constraints such that the intermediate results are plausible. In our experiments, DeProCams shows clear advantages over previous arts with promising quality and meanwhile being fully differentiable. Moreover, by solving the three tasks in a unified model, DeProCams waives the need for additional optical devices, radiometric calibrations and structured light.



### GeoConv: Geodesic Guided Convolution for Facial Action Unit Recognition
- **Arxiv ID**: http://arxiv.org/abs/2003.03055v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.03055v1)
- **Published**: 2020-03-06 07:05:46+00:00
- **Updated**: 2020-03-06 07:05:46+00:00
- **Authors**: Yuedong Chen, Guoxian Song, Zhiwen Shao, Jianfei Cai, Tat-Jen Cham, Jianming Zheng
- **Comment**: 16 pages, 3 figures
- **Journal**: None
- **Summary**: Automatic facial action unit (AU) recognition has attracted great attention but still remains a challenging task, as subtle changes of local facial muscles are difficult to thoroughly capture. Most existing AU recognition approaches leverage geometry information in a straightforward 2D or 3D manner, which either ignore 3D manifold information or suffer from high computational costs. In this paper, we propose a novel geodesic guided convolution (GeoConv) for AU recognition by embedding 3D manifold information into 2D convolutions. Specifically, the kernel of GeoConv is weighted by our introduced geodesic weights, which are negatively correlated to geodesic distances on a coarsely reconstructed 3D face model. Moreover, based on GeoConv, we further develop an end-to-end trainable framework named GeoCNN for AU recognition. Extensive experiments on BP4D and DISFA benchmarks show that our approach significantly outperforms the state-of-the-art AU recognition methods.



### CNN-based Repetitive self-revised learning for photos' aesthetics imbalanced classification
- **Arxiv ID**: http://arxiv.org/abs/2003.03081v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.03081v4)
- **Published**: 2020-03-06 08:54:53+00:00
- **Updated**: 2020-03-27 06:51:43+00:00
- **Authors**: Ying Dai
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1909.08213
- **Journal**: None
- **Summary**: Aesthetic assessment is subjective, and the distribution of the aesthetic levels is imbalanced. In order to realize the auto-assessment of photo aesthetics, we focus on using repetitive self-revised learning (RSRL) to train the CNN-based aesthetics classification network by imbalanced data set. As RSRL, the network is trained repetitively by dropping out the low likelihood photo samples at the middle levels of aesthetics from the training data set based on the previously trained network. Further, the retained two networks are used in extracting highlight regions of the photos related with the aesthetic assessment. Experimental results show that the CNN-based repetitive self-revised learning is effective for improving the performances of the imbalanced classification.



### StereoNeuroBayesSLAM: A Neurobiologically Inspired Stereo Visual SLAM System Based on Direct Sparse Method
- **Arxiv ID**: http://arxiv.org/abs/2003.03091v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, eess.IV, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2003.03091v1)
- **Published**: 2020-03-06 09:07:50+00:00
- **Updated**: 2020-03-06 09:07:50+00:00
- **Authors**: Taiping Zeng, Xiaoli Li, Bailu Si
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a neurobiologically inspired visual simultaneous localization and mapping (SLAM) system based on direction sparse method to real-time build cognitive maps of large-scale environments from a moving stereo camera. The core SLAM system mainly comprises a Bayesian attractor network, which utilizes neural responses of head direction (HD) cells in the hippocampus and grid cells in the medial entorhinal cortex (MEC) to represent the head direction and the position of the robot in the environment, respectively. Direct sparse method is employed to accurately and robustly estimate velocity information from a stereo camera. Input rotational and translational velocities are integrated by the HD cell and grid cell networks, respectively. We demonstrated our neurobiologically inspired stereo visual SLAM system on the KITTI odometry benchmark datasets. Our proposed SLAM system is robust to real-time build a coherent semi-metric topological map from a stereo camera. Qualitative evaluation on cognitive maps shows that our proposed neurobiologically inspired stereo visual SLAM system outperforms our previous brain-inspired algorithms and the neurobiologically inspired monocular visual SLAM system both in terms of tracking accuracy and robustness, which is closer to the traditional state-of-the-art one.



### Show, Edit and Tell: A Framework for Editing Image Captions
- **Arxiv ID**: http://arxiv.org/abs/2003.03107v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.03107v1)
- **Published**: 2020-03-06 09:52:17+00:00
- **Updated**: 2020-03-06 09:52:17+00:00
- **Authors**: Fawaz Sammani, Luke Melas-Kyriazi
- **Comment**: Accepted to CVPR 2020
- **Journal**: None
- **Summary**: Most image captioning frameworks generate captions directly from images, learning a mapping from visual features to natural language. However, editing existing captions can be easier than generating new ones from scratch. Intuitively, when editing captions, a model is not required to learn information that is already present in the caption (i.e. sentence structure), enabling it to focus on fixing details (e.g. replacing repetitive words). This paper proposes a novel approach to image captioning based on iterative adaptive refinement of an existing caption. Specifically, our caption-editing model consisting of two sub-modules: (1) EditNet, a language module with an adaptive copy mechanism (Copy-LSTM) and a Selective Copy Memory Attention mechanism (SCMA), and (2) DCNet, an LSTM-based denoising auto-encoder. These components enable our model to directly copy from and modify existing captions. Experiments demonstrate that our new approach achieves state-of-art performance on the MS COCO dataset both with and without sequence-level training.



### Meta-SVDD: Probabilistic Meta-Learning for One-Class Classification in Cancer Histology Images
- **Arxiv ID**: http://arxiv.org/abs/2003.03109v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.03109v1)
- **Published**: 2020-03-06 09:59:57+00:00
- **Updated**: 2020-03-06 09:59:57+00:00
- **Authors**: Jevgenij Gamper, Brandon Chan, Yee Wah Tsang, David Snead, Nasir Rajpoot
- **Comment**: Accepted to Medical Imaging meets NeurIPS Workshop, Conference on
  Neural Information Processing Systems 2019, Vancouver
- **Journal**: None
- **Summary**: To train a robust deep learning model, one usually needs a balanced set of categories in the training data. The data acquired in a medical domain, however, frequently contains an abundance of healthy patients, versus a small variety of positive, abnormal cases. Moreover, the annotation of a positive sample requires time consuming input from medical domain experts. This scenario would suggest a promise for one-class classification type approaches. In this work we propose a general one-class classification model for histology, that is meta-trained on multiple histology datasets simultaneously, and can be applied to new tasks without expensive re-training. This model could be easily used by pathology domain experts, and potentially be used for screening purposes.



### Pixel-Level Self-Paced Learning for Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2003.03113v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.03113v2)
- **Published**: 2020-03-06 10:04:50+00:00
- **Updated**: 2020-03-09 04:32:26+00:00
- **Authors**: Wei. Lin, Junyu. Gao, Qi. Wang, Xuelong. Li
- **Comment**: 5 pages, 5 figures. Accepted by ICASSP 2020, Source code:
  https://github.com/Elin24/PSPL
- **Journal**: None
- **Summary**: Recently, lots of deep networks are proposed to improve the quality of predicted super-resolution (SR) images, due to its widespread use in several image-based fields. However, with these networks being constructed deeper and deeper, they also cost much longer time for training, which may guide the learners to local optimization. To tackle this problem, this paper designs a training strategy named Pixel-level Self-Paced Learning (PSPL) to accelerate the convergence velocity of SISR models. PSPL imitating self-paced learning gives each pixel in the predicted SR image and its corresponding pixel in ground truth an attention weight, to guide the model to a better region in parameter space. Extensive experiments proved that PSPL could speed up the training of SISR models, and prompt several existing models to obtain new better results. Furthermore, the source code is available at https://github.com/Elin24/PSPL.



### Bundle Adjustment on a Graph Processor
- **Arxiv ID**: http://arxiv.org/abs/2003.03134v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/2003.03134v2)
- **Published**: 2020-03-06 11:05:55+00:00
- **Updated**: 2020-03-30 16:59:24+00:00
- **Authors**: Joseph Ortiz, Mark Pupilli, Stefan Leutenegger, Andrew J. Davison
- **Comment**: Published in Proceedings of the IEEE Conference on Computer Vision
  and Pattern Recognition (CVPR 2020). Video:
  https://www.youtube.com/watch?v=TqeN8aQNgd0
- **Journal**: None
- **Summary**: Graph processors such as Graphcore's Intelligence Processing Unit (IPU) are part of the major new wave of novel computer architecture for AI, and have a general design with massively parallel computation, distributed on-chip memory and very high inter-core communication bandwidth which allows breakthrough performance for message passing algorithms on arbitrary graphs. We show for the first time that the classical computer vision problem of bundle adjustment (BA) can be solved extremely fast on a graph processor using Gaussian Belief Propagation. Our simple but fully parallel implementation uses the 1216 cores on a single IPU chip to, for instance, solve a real BA problem with 125 keyframes and 1919 points in under 40ms, compared to 1450ms for the Ceres CPU library. Further code optimisation will surely increase this difference on static problems, but we argue that the real promise of graph processing is for flexible in-place optimisation of general, dynamically changing factor graphs representing Spatial AI problems. We give indications of this with experiments showing the ability of GBP to efficiently solve incremental SLAM problems, and deal with robust cost functions and different types of factors.



### Knowledge graph based methods for record linkage
- **Arxiv ID**: http://arxiv.org/abs/2003.03136v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.03136v1)
- **Published**: 2020-03-06 11:09:44+00:00
- **Updated**: 2020-03-06 11:09:44+00:00
- **Authors**: B. Gautam, O. Ramos Terrades, J. M. Pujades, M. Valls
- **Comment**: the paper is under consideration at Pattern Recognition Letters
- **Journal**: None
- **Summary**: Nowadays, it is common in Historical Demography the use of individual-level data as a consequence of a predominant life-course approach for the understanding of the demographic behaviour, family transition, mobility, etc. Record linkage advance is key in these disciplines since it allows to increase the volume and the data complexity to be analyzed. However, current methods are constrained to link data coming from the same kind of sources. Knowledge graph are flexible semantic representations, which allow to encode data variability and semantic relations in a structured manner.   In this paper we propose the knowledge graph use to tackle record linkage task. The proposed method, named {\bf WERL}, takes advantage of the main knowledge graph properties and learns embedding vectors to encode census information. These embeddings are properly weighted to maximize the record linkage performance. We have evaluated this method on benchmark data sets and we have compared it to related methods with stimulating and satisfactory results.



### Demographic Bias in Presentation Attack Detection of Iris Recognition Systems
- **Arxiv ID**: http://arxiv.org/abs/2003.03151v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.03151v2)
- **Published**: 2020-03-06 12:16:19+00:00
- **Updated**: 2020-07-03 10:02:30+00:00
- **Authors**: Meiling Fang, Naser Damer, Florian Kirchbuchner, Arjan Kuijper
- **Comment**: accepted for publication at EUSIPCO2020
- **Journal**: None
- **Summary**: With the widespread use of biometric systems, the demographic bias problem raises more attention. Although many studies addressed bias issues in biometric verification, there are no works that analyze the bias in presentation attack detection (PAD) decisions. Hence, we investigate and analyze the demographic bias in iris PAD algorithms in this paper. To enable a clear discussion, we adapt the notions of differential performance and differential outcome to the PAD problem. We study the bias in iris PAD using three baselines (hand-crafted, transfer-learning, and training from scratch) using the NDCLD-2013 database. The experimental results point out that female users will be significantly less protected by the PAD, in comparison to males.



### D3Feat: Joint Learning of Dense Detection and Description of 3D Local Features
- **Arxiv ID**: http://arxiv.org/abs/2003.03164v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.03164v1)
- **Published**: 2020-03-06 12:51:09+00:00
- **Updated**: 2020-03-06 12:51:09+00:00
- **Authors**: Xuyang Bai, Zixin Luo, Lei Zhou, Hongbo Fu, Long Quan, Chiew-Lan Tai
- **Comment**: Accepted to CVPR 2020, supplementary materials included
- **Journal**: None
- **Summary**: A successful point cloud registration often lies on robust establishment of sparse matches through discriminative 3D local features. Despite the fast evolution of learning-based 3D feature descriptors, little attention has been drawn to the learning of 3D feature detectors, even less for a joint learning of the two tasks. In this paper, we leverage a 3D fully convolutional network for 3D point clouds, and propose a novel and practical learning mechanism that densely predicts both a detection score and a description feature for each 3D point. In particular, we propose a keypoint selection strategy that overcomes the inherent density variations of 3D point clouds, and further propose a self-supervised detector loss guided by the on-the-fly feature matching results during training. Finally, our method achieves state-of-the-art results in both indoor and outdoor scenarios, evaluated on 3DMatch and KITTI datasets, and shows its strong generalization ability on the ETH dataset. Towards practical use, we show that by adopting a reliable feature detector, sampling a smaller number of features is sufficient to achieve accurate and fast point cloud alignment.[code release](https://github.com/XuyangBai/D3Feat)



### When Deep Learning Meets Data Alignment: A Review on Deep Registration Networks (DRNs)
- **Arxiv ID**: http://arxiv.org/abs/2003.03167v2
- **DOI**: 10.3390/app10217524
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.03167v2)
- **Published**: 2020-03-06 12:56:19+00:00
- **Updated**: 2020-10-28 12:02:07+00:00
- **Authors**: Victor Villena-Martinez, Sergiu Oprea, Marcelo Saval-Calvo, Jorge Azorin-Lopez, Andres Fuster-Guillo, Robert B. Fisher
- **Comment**: Published in Applied Sciences
- **Journal**: Appl. Sci. 2020, 10(21), 7524
- **Summary**: Registration is the process that computes the transformation that aligns sets of data. Commonly, a registration process can be divided into four main steps: target selection, feature extraction, feature matching, and transform computation for the alignment. The accuracy of the result depends on multiple factors, the most significant are the quantity of input data, the presence of noise, outliers and occlusions, the quality of the extracted features, real-time requirements and the type of transformation, especially those ones defined by multiple parameters, like non-rigid deformations. Recent advancements in machine learning could be a turning point in these issues, particularly with the development of deep learning (DL) techniques, which are helping to improve multiple computer vision problems through an abstract understanding of the input data. In this paper, a review of deep learning-based registration methods is presented. We classify the different papers proposing a framework extracted from the traditional registration pipeline to analyse the new learning-based proposal strengths. Deep Registration Networks (DRNs) try to solve the alignment task either replacing part of the traditional pipeline with a network or fully solving the registration problem. The main conclusions extracted are, on the one hand, 1) learning-based registration techniques cannot always be clearly classified in the traditional pipeline. 2) These approaches allow more complex inputs like conceptual models as well as the traditional 3D datasets. 3) In spite of the generality of learning, the current proposals are still ad hoc solutions. Finally, 4) this is a young topic that still requires a large effort to reach general solutions able to cope with the problems that affect traditional approaches.



### Noise Estimation Using Density Estimation for Self-Supervised Multimodal Learning
- **Arxiv ID**: http://arxiv.org/abs/2003.03186v3
- **DOI**: None
- **Categories**: **cs.CV**, I.2.10; I.4; I.5
- **Links**: [PDF](http://arxiv.org/pdf/2003.03186v3)
- **Published**: 2020-03-06 13:25:12+00:00
- **Updated**: 2020-12-10 14:26:22+00:00
- **Authors**: Elad Amrani, Rami Ben-Ari, Daniel Rotman, Alex Bronstein
- **Comment**: Accepted to AAAI 2021
- **Journal**: None
- **Summary**: One of the key factors of enabling machine learning models to comprehend and solve real-world tasks is to leverage multimodal data. Unfortunately, annotation of multimodal data is challenging and expensive. Recently, self-supervised multimodal methods that combine vision and language were proposed to learn multimodal representations without annotation. However, these methods often choose to ignore the presence of high levels of noise and thus yield sub-optimal results. In this work, we show that the problem of noise estimation for multimodal data can be reduced to a multimodal density estimation task. Using multimodal density estimation, we propose a noise estimation building block for multimodal representation learning that is based strictly on the inherent correlation between different modalities. We demonstrate how our noise estimation can be broadly integrated and achieves comparable results to state-of-the-art performance on five different benchmark datasets for two challenging multimodal tasks: Video Question Answering and Text-To-Video Retrieval. Furthermore, we provide a theoretical probabilistic error bound substantiating our empirical results and analyze failure cases. Code: https://github.com/elad-amrani/ssml.



### Can We Read Speech Beyond the Lips? Rethinking RoI Selection for Deep Visual Speech Recognition
- **Arxiv ID**: http://arxiv.org/abs/2003.03206v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.03206v2)
- **Published**: 2020-03-06 13:52:46+00:00
- **Updated**: 2020-03-09 06:06:20+00:00
- **Authors**: Yuanhang Zhang, Shuang Yang, Jingyun Xiao, Shiguang Shan, Xilin Chen
- **Comment**: 8 pages; accepted in the 15th IEEE International Conference on
  Automatic Face and Gesture Recognition (FG 2020)
- **Journal**: None
- **Summary**: Recent advances in deep learning have heightened interest among researchers in the field of visual speech recognition (VSR). Currently, most existing methods equate VSR with automatic lip reading, which attempts to recognise speech by analysing lip motion. However, human experience and psychological studies suggest that we do not always fix our gaze at each other's lips during a face-to-face conversation, but rather scan the whole face repetitively. This inspires us to revisit a fundamental yet somehow overlooked problem: can VSR models benefit from reading extraoral facial regions, i.e. beyond the lips? In this paper, we perform a comprehensive study to evaluate the effects of different facial regions with state-of-the-art VSR models, including the mouth, the whole face, the upper face, and even the cheeks. Experiments are conducted on both word-level and sentence-level benchmarks with different characteristics. We find that despite the complex variations of the data, incorporating information from extraoral facial regions, even the upper face, consistently benefits VSR performance. Furthermore, we introduce a simple yet effective method based on Cutout to learn more discriminative features for face-based VSR, hoping to maximise the utility of information encoded in different facial regions. Our experiments show obvious improvements over existing state-of-the-art methods that use only the lip region as inputs, a result we believe would probably provide the VSR community with some new and exciting insights.



### Diverse and Admissible Trajectory Forecasting through Multimodal Context Understanding
- **Arxiv ID**: http://arxiv.org/abs/2003.03212v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.03212v4)
- **Published**: 2020-03-06 13:59:39+00:00
- **Updated**: 2020-08-31 13:57:26+00:00
- **Authors**: Seong Hyeon Park, Gyubok Lee, Manoj Bhat, Jimin Seo, Minseok Kang, Jonathan Francis, Ashwin R. Jadhav, Paul Pu Liang, Louis-Philippe Morency
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: Multi-agent trajectory forecasting in autonomous driving requires an agent to accurately anticipate the behaviors of the surrounding vehicles and pedestrians, for safe and reliable decision-making. Due to partial observability in these dynamical scenes, directly obtaining the posterior distribution over future agent trajectories remains a challenging problem. In realistic embodied environments, each agent's future trajectories should be both diverse since multiple plausible sequences of actions can be used to reach its intended goals, and admissible since they must obey physical constraints and stay in drivable areas. In this paper, we propose a model that synthesizes multiple input signals from the multimodal world|the environment's scene context and interactions between multiple surrounding agents|to best model all diverse and admissible trajectories. We compare our model with strong baselines and ablations across two public datasets and show a significant performance improvement over previous state-of-the-art methods. Lastly, we offer new metrics incorporating admissibility criteria to further study and evaluate the diversity of predictions. Codes are at: https://github.com/kami93/CMU-DATF.



### Anysize GAN: A solution to the image-warping problem
- **Arxiv ID**: http://arxiv.org/abs/2003.03233v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.03233v2)
- **Published**: 2020-03-06 14:18:42+00:00
- **Updated**: 2020-07-08 21:19:38+00:00
- **Authors**: Connah Kendrick, David Gillespie, Moi Hoon Yap
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new type of General Adversarial Network (GAN) to resolve a common issue with Deep Learning. We develop a novel architecture that can be applied to existing latent vector based GAN structures that allows them to generate on-the-fly images of any size. Existing GAN for image generation requires uniform images of matching dimensions. However, publicly available datasets, such as ImageNet contain thousands of different sizes. Resizing image causes deformations and changing the image data, whereas as our network does not require this preprocessing step. We make significant changes to the standard data loading techniques to enable any size image to be loaded for training. We also modify the network in two ways, by adding multiple inputs and a novel dynamic resizing layer. Finally we make adjustments to the discriminator to work on multiple resolutions. These changes can allow multiple resolution datasets to be trained on without any resizing, if memory allows. We validate our results on the ISIC 2019 skin lesion dataset. We demonstrate our method can successfully generate realistic images at different sizes without issue, preserving and understanding spatial relationships, while maintaining feature relationships. We will release the source codes upon paper acceptance.



### Automated detection of corrosion in used nuclear fuel dry storage canisters using residual neural networks
- **Arxiv ID**: http://arxiv.org/abs/2003.03241v3
- **DOI**: None
- **Categories**: **cs.CV**, stat.AP, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.03241v3)
- **Published**: 2020-03-06 14:42:07+00:00
- **Updated**: 2020-07-13 16:06:36+00:00
- **Authors**: Theodore Papamarkou, Hayley Guy, Bryce Kroencke, Jordan Miller, Preston Robinette, Daniel Schultz, Jacob Hinkle, Laura Pullum, Catherine Schuman, Jeremy Renshaw, Stylianos Chatzidakis
- **Comment**: None
- **Journal**: None
- **Summary**: Nondestructive evaluation methods play an important role in ensuring component integrity and safety in many industries. Operator fatigue can play a critical role in the reliability of such methods. This is important for inspecting high value assets or assets with a high consequence of failure, such as aerospace and nuclear components. Recent advances in convolution neural networks can support and automate these inspection efforts. This paper proposes using residual neural networks (ResNets) for real-time detection of corrosion, including iron oxide discoloration, pitting and stress corrosion cracking, in dry storage stainless steel canisters housing used nuclear fuel. The proposed approach crops nuclear canister images into smaller tiles, trains a ResNet on these tiles, and classifies images as corroded or intact using the per-image count of tiles predicted as corroded by the ResNet. The results demonstrate that such a deep learning approach allows to detect the locus of corrosion via smaller tiles, and at the same time to infer with high accuracy whether an image comes from a corroded canister. Thereby, the proposed approach holds promise to automate and speed up nuclear fuel canister inspections, to minimize inspection costs, and to partially replace human-conducted onsite inspections, thus reducing radiation doses to personnel.



### Traffic Signs Detection and Recognition System using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2003.03256v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.03256v1)
- **Published**: 2020-03-06 14:54:40+00:00
- **Updated**: 2020-03-06 14:54:40+00:00
- **Authors**: Pavly Salah Zaki, Marco Magdy William, Bolis Karam Soliman, Kerolos Gamal Alexsan, Keroles Khalil, Magdy El-Moursy
- **Comment**: 7 pages, 14 figures, 10 tables
- **Journal**: None
- **Summary**: With the rapid development of technology, automobiles have become an essential asset in our day-to-day lives. One of the more important researches is Traffic Signs Recognition (TSR) systems. This paper describes an approach for efficiently detecting and recognizing traffic signs in real-time, taking into account the various weather, illumination and visibility challenges through the means of transfer learning. We tackle the traffic sign detection problem using the state-of-the-art of multi-object detection systems such as Faster Recurrent Convolutional Neural Networks (F-RCNN) and Single Shot Multi- Box Detector (SSD) combined with various feature extractors such as MobileNet v1 and Inception v2, and also Tiny-YOLOv2. However, the focus of this paper is going to be F-RCNN Inception v2 and Tiny YOLO v2 as they achieved the best results. The aforementioned models were fine-tuned on the German Traffic Signs Detection Benchmark (GTSDB) dataset. These models were tested on the host PC as well as Raspberry Pi 3 Model B+ and the TASS PreScan simulation. We will discuss the results of all the models in the conclusion section.



### Spherical formulation of moving object geometric constraints for monocular fisheye cameras
- **Arxiv ID**: http://arxiv.org/abs/2003.03262v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.03262v1)
- **Published**: 2020-03-06 14:59:38+00:00
- **Updated**: 2020-03-06 14:59:38+00:00
- **Authors**: Letizia Mariotti, Ciaran Hughes
- **Comment**: 8 pages, 9 figures, 2 tables Conference ITSC 2019
- **Journal**: 2019 IEEE Intelligent Transportation Systems Conference (ITSC),
  Auckland, New Zealand, 2019, pp. 816-823
- **Summary**: In this paper, we introduce a moving object detection algorithm for fisheye cameras used in autonomous driving. We reformulate the three commonly used constraints in rectilinear images (epipolar, positive depth and positive height constraints) to spherical coordinates which is invariant to specific camera configuration once the calibration is known. One of the main challenging use case in autonomous driving is to detect parallel moving objects which suffer from motion-parallax ambiguity. To alleviate this, we formulate an additional fourth constraint, called the anti-parallel constraint, which aids the detection of objects with motion that mirrors the ego-vehicle possible. We analyze the proposed algorithm in different scenarios and demonstrate that it works effectively operating directly on fisheye images.



### A Hybrid Approach for Tracking Individual Players in Broadcast Match Videos
- **Arxiv ID**: http://arxiv.org/abs/2003.03271v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.03271v2)
- **Published**: 2020-03-06 15:16:23+00:00
- **Updated**: 2020-03-10 13:09:14+00:00
- **Authors**: Roberto L. Castro, Diego Andrade, Basilio Fraguela
- **Comment**: Comments: 25 pages, LaTeX; typos corrected
- **Journal**: None
- **Summary**: Tracking people in a video sequence is a challenging task that has been approached from many perspectives. This task becomes even more complicated when the person to track is a player in a broadcasted sport event, the reasons being the existence of difficulties such as frequent camera movements or switches, total and partial occlusions between players, and blurry frames due to the codification algorithm of the video. This paper introduces a player tracking solution which is both fast and accurate. This allows to track a player precisely in real-time. The approach combines several models that are executed concurrently in a relatively modest hardware, and whose accuracy has been validated against hand-labeled broadcast video sequences. Regarding the accuracy, the tests show that the area under curve (AUC) of our approach is around 0.6, which is similar to generic state of the art solutions. As for performance, our proposal can process high definition videos (1920x1080 px) at 80 fps.



### Explaining Away Attacks Against Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2003.05748v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.05748v1)
- **Published**: 2020-03-06 15:32:30+00:00
- **Updated**: 2020-03-06 15:32:30+00:00
- **Authors**: Sean Saito, Jin Wang
- **Comment**: 2 pages, 2 figures; Accepted at MLSys 2020 First Workshop on Secure
  and Resilient Autonomy
- **Journal**: None
- **Summary**: We investigate the problem of identifying adversarial attacks on image-based neural networks. We present intriguing experimental results showing significant discrepancies between the explanations generated for the predictions of a model on clean and adversarial data. Utilizing this intuition, we propose a framework which can identify whether a given input is adversarial based on the explanations given by the model. Code for our experiments can be found here: https://github.com/seansaito/Explaining-Away-Attacks-Against-Neural-Networks.



### Probability Weighted Compact Feature for Domain Adaptive Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2003.03293v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.03293v1)
- **Published**: 2020-03-06 16:11:37+00:00
- **Updated**: 2020-03-06 16:11:37+00:00
- **Authors**: Fuxiang Huang, Lei Zhang, Yang Yang, Xichuan Zhou
- **Comment**: Accepted by CVPR 2020; The source code is available at
  https://github.com/fuxianghuang1/PWCF
- **Journal**: None
- **Summary**: Domain adaptive image retrieval includes single-domain retrieval and cross-domain retrieval. Most of the existing image retrieval methods only focus on single-domain retrieval, which assumes that the distributions of retrieval databases and queries are similar. However, in practical application, the discrepancies between retrieval databases often taken in ideal illumination/pose/background/camera conditions and queries usually obtained in uncontrolled conditions are very large. In this paper, considering the practical application, we focus on challenging cross-domain retrieval. To address the problem, we propose an effective method named Probability Weighted Compact Feature Learning (PWCF), which provides inter-domain correlation guidance to promote cross-domain retrieval accuracy and learns a series of compact binary codes to improve the retrieval speed. First, we derive our loss function through the Maximum A Posteriori Estimation (MAP): Bayesian Perspective (BP) induced focal-triplet loss, BP induced quantization loss and BP induced classification loss. Second, we propose a common manifold structure between domains to explore the potential correlation across domains. Considering the original feature representation is biased due to the inter-domain discrepancy, the manifold structure is difficult to be constructed. Therefore, we propose a new feature named Histogram Feature of Neighbors (HFON) from the sample statistics perspective. Extensive experiments on various benchmark databases validate that our method outperforms many state-of-the-art image retrieval methods for domain adaptive image retrieval. The source code is available at https://github.com/fuxianghuang1/PWCF



### Heterogeneity Loss to Handle Intersubject and Intrasubject Variability in Cancer
- **Arxiv ID**: http://arxiv.org/abs/2003.03295v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.03295v2)
- **Published**: 2020-03-06 16:16:23+00:00
- **Updated**: 2020-03-19 02:01:11+00:00
- **Authors**: Shubham Goswami, Suril Mehta, Dhruva Sahrawat, Anubha Gupta, Ritu Gupta
- **Comment**: Accepted in ICLR 2020 workshop
  AI4AH(https://sites.google.com/view/ai4ah-iclr2020)
- **Journal**: None
- **Summary**: Developing nations lack adequate number of hospitals with modern equipment and skilled doctors. Hence, a significant proportion of these nations' population, particularly in rural areas, is not able to avail specialized and timely healthcare facilities. In recent years, deep learning (DL) models, a class of artificial intelligence (AI) methods, have shown impressive results in medical domain. These AI methods can provide immense support to developing nations as affordable healthcare solutions. This work is focused on one such application of blood cancer diagnosis. However, there are some challenges to DL models in cancer research because of the unavailability of a large data for adequate training and the difficulty of capturing heterogeneity in data at different levels ranging from acquisition characteristics, session, to subject-level (within subjects and across subjects). These challenges render DL models prone to overfitting and hence, models lack generalization on prospective subjects' data. In this work, we address these problems in the application of B-cell Acute Lymphoblastic Leukemia (B-ALL) diagnosis using deep learning. We propose heterogeneity loss that captures subject-level heterogeneity, thereby, forcing the neural network to learn subject-independent features. We also propose an unorthodox ensemble strategy that helps us in providing improved classification over models trained on 7-folds giving a weighted-$F_1$ score of 95.26% on unseen (test) subjects' data that are, so far, the best results on the C-NMC 2019 dataset for B-ALL classification.



### Captioning Images with Novel Objects via Online Vocabulary Expansion
- **Arxiv ID**: http://arxiv.org/abs/2003.03305v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.03305v1)
- **Published**: 2020-03-06 16:34:15+00:00
- **Updated**: 2020-03-06 16:34:15+00:00
- **Authors**: Mikihiro Tanaka, Tatsuya Harada
- **Comment**: None
- **Journal**: None
- **Summary**: In this study, we introduce a low cost method for generating descriptions from images containing novel objects. Generally, constructing a model, which can explain images with novel objects, is costly because of the following: (1) collecting a large amount of data for each category, and (2) retraining the entire system. If humans see a small number of novel objects, they are able to estimate their properties by associating their appearance with known objects. Accordingly, we propose a method that can explain images with novel objects without retraining using the word embeddings of the objects estimated from only a small number of image features of the objects. The method can be integrated with general image-captioning models. The experimental results show the effectiveness of our approach.



### Scalable Uncertainty for Computer Vision with Functional Variational Inference
- **Arxiv ID**: http://arxiv.org/abs/2003.03396v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.03396v1)
- **Published**: 2020-03-06 19:09:42+00:00
- **Updated**: 2020-03-06 19:09:42+00:00
- **Authors**: Eduardo D C Carvalho, Ronald Clark, Andrea Nicastro, Paul H J Kelly
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: As Deep Learning continues to yield successful applications in Computer Vision, the ability to quantify all forms of uncertainty is a paramount requirement for its safe and reliable deployment in the real-world. In this work, we leverage the formulation of variational inference in function space, where we associate Gaussian Processes (GPs) to both Bayesian CNN priors and variational family. Since GPs are fully determined by their mean and covariance functions, we are able to obtain predictive uncertainty estimates at the cost of a single forward pass through any chosen CNN architecture and for any supervised learning task. By leveraging the structure of the induced covariance matrices, we propose numerically efficient algorithms which enable fast training in the context of high-dimensional tasks such as depth estimation and semantic segmentation. Additionally, we provide sufficient conditions for constructing regression loss functions whose probabilistic counterparts are compatible with aleatoric uncertainty quantification.



### Toward Enabling a Reliable Quality Monitoring System for Additive Manufacturing Process using Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2003.08749v1
- **DOI**: None
- **Categories**: **cs.CV**, cond-mat.mtrl-sci, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.08749v1)
- **Published**: 2020-03-06 20:49:20+00:00
- **Updated**: 2020-03-06 20:49:20+00:00
- **Authors**: Yaser Banadaki, Nariman Razaviarab, Hadi Fekrmandi, Safura Sharifi
- **Comment**: None
- **Journal**: None
- **Summary**: Additive Manufacturing (AM) is a crucial component of the smart industry. In this paper, we propose an automated quality grading system for the AM process using a deep convolutional neural network (CNN) model. The CNN model is trained offline using the images of the internal and surface defects in the layer-by-layer deposition of materials and tested online by studying the performance of detecting and classifying the failure in AM process at different extruder speeds and temperatures. The model demonstrates the accuracy of 94% and specificity of 96%, as well as above 75% in three classifier measures of the Fscore, the sensitivity, and precision for classifying the quality of the printing process in five grades in real-time. The proposed online model adds an automated, consistent, and non-contact quality control signal to the AM process that eliminates the manual inspection of parts after they are entirely built. The quality monitoring signal can also be used by the machine to suggest remedial actions by adjusting the parameters in real-time. The proposed quality predictive model serves as a proof-of-concept for any type of AM machines to produce reliable parts with fewer quality hiccups while limiting the waste of both time and materials.



### Semi-Supervised StyleGAN for Disentanglement Learning
- **Arxiv ID**: http://arxiv.org/abs/2003.03461v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.03461v3)
- **Published**: 2020-03-06 22:54:46+00:00
- **Updated**: 2020-11-25 23:06:53+00:00
- **Authors**: Weili Nie, Tero Karras, Animesh Garg, Shoubhik Debnath, Anjul Patney, Ankit B. Patel, Anima Anandkumar
- **Comment**: ICML 2020, 21 pages. Project page:
  https://sites.google.com/nvidia.com/semi-stylegan
- **Journal**: None
- **Summary**: Disentanglement learning is crucial for obtaining disentangled representations and controllable generation. Current disentanglement methods face several inherent limitations: difficulty with high-resolution images, primarily focusing on learning disentangled representations, and non-identifiability due to the unsupervised setting. To alleviate these limitations, we design new architectures and loss functions based on StyleGAN (Karras et al., 2019), for semi-supervised high-resolution disentanglement learning. We create two complex high-resolution synthetic datasets for systematic testing. We investigate the impact of limited supervision and find that using only 0.25%~2.5% of labeled data is sufficient for good disentanglement on both synthetic and real datasets. We propose new metrics to quantify generator controllability, and observe there may exist a crucial trade-off between disentangled representation learning and controllable generation. We also consider semantic fine-grained image editing to achieve better generalization to unseen images.



