# Arxiv Papers in cs.CV on 2020-03-14
### Leveraging Vision and Kinematics Data to Improve Realism of Biomechanic Soft-tissue Simulation for Robotic Surgery
- **Arxiv ID**: http://arxiv.org/abs/2003.06518v1
- **DOI**: 10.1007/s11548-020-02139-6
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.06518v1)
- **Published**: 2020-03-14 00:16:08+00:00
- **Updated**: 2020-03-14 00:16:08+00:00
- **Authors**: Jie Ying Wu, Peter Kazanzides, Mathias Unberath
- **Comment**: 12 pages, 4 figures, to be published in IJCARS IPCAI special edition
  2020
- **Journal**: None
- **Summary**: Purpose Surgical simulations play an increasingly important role in surgeon education and developing algorithms that enable robots to perform surgical subtasks. To model anatomy, Finite Element Method (FEM) simulations have been held as the gold standard for calculating accurate soft-tissue deformation. Unfortunately, their accuracy is highly dependent on the simulation parameters, which can be difficult to obtain.   Methods In this work, we investigate how live data acquired during any robotic endoscopic surgical procedure may be used to correct for inaccurate FEM simulation results. Since FEMs are calculated from initial parameters and cannot directly incorporate observations, we propose to add a correction factor that accounts for the discrepancy between simulation and observations. We train a network to predict this correction factor.   Results To evaluate our method, we use an open-source da Vinci Surgical System to probe a soft-tissue phantom and replay the interaction in simulation. We train the network to correct for the difference between the predicted mesh position and the measured point cloud. This results in 15-30% improvement in the mean distance, demonstrating the effectiveness of our approach across a large range of simulation parameters.   Conclusion We show a first step towards a framework that synergistically combines the benefits of model-based simulation and real-time observations. It corrects discrepancies between simulation and the scene that results from inaccurate modeling parameters. This can provide a more accurate simulation environment for surgeons and better data with which to train algorithms.



### Symmetry Detection of Occluded Point Cloud Using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2003.06520v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2003.06520v1)
- **Published**: 2020-03-14 00:23:58+00:00
- **Updated**: 2020-03-14 00:23:58+00:00
- **Authors**: Zhelun Wu, Hongyan Jiang, Siyun He
- **Comment**: None
- **Journal**: None
- **Summary**: Symmetry detection has been a classical problem in computer graphics, many of which using traditional geometric methods. In recent years, however, we have witnessed the arising deep learning changed the landscape of computer graphics. In this paper, we aim to solve the symmetry detection of the occluded point cloud in a deep-learning fashion. To the best of our knowledge, we are the first to utilize deep learning to tackle such a problem. In such a deep learning framework, double supervisions: points on the symmetry plane and normal vectors are employed to help us pinpoint the symmetry plane. We conducted experiments on the YCB- video dataset and demonstrate the efficacy of our method.



### Instant recovery of shape from spectrum via latent space connections
- **Arxiv ID**: http://arxiv.org/abs/2003.06523v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.06523v4)
- **Published**: 2020-03-14 00:48:34+00:00
- **Updated**: 2020-11-04 21:53:40+00:00
- **Authors**: Riccardo Marin, Arianna Rampini, Umberto Castellani, Emanuele Rodolà, Maks Ovsjanikov, Simone Melzi
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce the first learning-based method for recovering shapes from Laplacian spectra. Given an auto-encoder, our model takes the form of a cycle-consistent module to map latent vectors to sequences of eigenvalues. This module provides an efficient and effective linkage between spectrum and geometry of a given shape. Our data-driven approach replaces the need for ad-hoc regularizers required by prior methods, while providing more accurate results at a fraction of the computational cost. Our learning model applies without modifications across different dimensions (2D and 3D shapes alike), representations (meshes, contours and point clouds), as well as across different shape classes, and admits arbitrary resolution of the input spectrum without affecting complexity. The increased flexibility allows us to provide a proxy to differentiable eigendecomposition and to address notoriously difficult tasks in 3D vision and geometry processing within a unified framework, including shape generation from spectrum, mesh super-resolution, shape exploration, style transfer, spectrum estimation from point clouds, segmentation transfer and point-to-point matching.



### Measuring and improving the quality of visual explanations
- **Arxiv ID**: http://arxiv.org/abs/2003.08774v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.08774v2)
- **Published**: 2020-03-14 00:52:00+00:00
- **Updated**: 2020-03-20 10:00:58+00:00
- **Authors**: Agnieszka Grabska-Barwińska
- **Comment**: None
- **Journal**: None
- **Summary**: The ability of to explain neural network decisions goes hand in hand with their safe deployment. Several methods have been proposed to highlight features important for a given network decision. However, there is no consensus on how to measure effectiveness of these methods. We propose a new procedure for evaluating explanations. We use it to investigate visual explanations extracted from a range of possible sources in a neural network. We quantify the benefit of combining these sources and challenge a recent appeal for taking bias parameters into account. We support our conclusions with a general assessment of the impact of bias parameters in ImageNet classifiers



### Boundary Guidance Hierarchical Network for Real-Time Tongue Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2003.06529v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.06529v1)
- **Published**: 2020-03-14 01:25:46+00:00
- **Updated**: 2020-03-14 01:25:46+00:00
- **Authors**: Xinyi Zeng, Qian Zhang, Jia Chen, Guixu Zhang, Aimin Zhou, Yiqin Wang
- **Comment**: 10 pages, 8 figures
- **Journal**: None
- **Summary**: Automated tongue image segmentation in tongue images is a challenging task for two reasons: 1) there are many pathological details on the tongue surface, which affect the extraction of the boundary; 2) the shapes of the tongues captured from various persons (with different diseases) are quite different. To deal with the challenge, a novel end-to-end Boundary Guidance Hierarchical Network (BGHNet) with a new hybrid loss is proposed in this paper. In the new approach, firstly Context Feature Encoder Module (CFEM) is built upon the bottomup pathway to confront with the shrinkage of the receptive field. Secondly, a novel hierarchical recurrent feature fusion module (HRFFM) is adopt to progressively and hierarchically refine object maps to recover image details by integrating local context information. Finally, the proposed hybrid loss in a four hierarchy-pixel, patch, map and boundary guides the network to effectively segment the tongue regions and accurate tongue boundaries. BGHNet is applied to a set of tongue images. The experimental results suggest that the proposed approach can achieve the latest tongue segmentation performance. And in the meantime, the lightweight network contains only 15.45M parameters and performs only 11.22GFLOPS.



### Towards Causality-Aware Inferring: A Sequential Discriminative Approach for Medical Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/2003.06534v5
- **DOI**: 10.1109/TPAMI.2023.3292363
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.06534v5)
- **Published**: 2020-03-14 02:05:54+00:00
- **Updated**: 2023-07-03 08:57:37+00:00
- **Authors**: Junfan Lin, Keze Wang, Ziliang Chen, Xiaodan Liang, Liang Lin
- **Comment**: To appear in TPAMI 2023. In the experiments, our trained agent
  achieves the new state-of-the-art under various experimental settings and
  possesses the advantage of sample-efficiency and robustness compared to other
  existing MDA methods
- **Journal**: None
- **Summary**: Medical diagnosis assistant (MDA) aims to build an interactive diagnostic agent to sequentially inquire about symptoms for discriminating diseases. However, since the dialogue records used to build a patient simulator are collected passively, the data might be deteriorated by some task-unrelated biases, such as the preference of the collectors. These biases might hinder the diagnostic agent to capture transportable knowledge from the simulator. This work attempts to address these critical issues in MDA by taking advantage of the causal diagram to identify and resolve two representative non-causal biases, i.e., (i) default-answer bias and (ii) distributional inquiry bias. Specifically, Bias (i) originates from the patient simulator which tries to answer the unrecorded inquiries with some biased default answers. Consequently, the diagnostic agents cannot fully demonstrate their advantages due to the biased answers. To eliminate this bias and inspired by the propensity score matching technique with causal diagram, we propose a propensity-based patient simulator to effectively answer unrecorded inquiry by drawing knowledge from the other records; Bias (ii) inherently comes along with the passively collected data, and is one of the key obstacles for training the agent towards "learning how" rather than "remembering what". For example, within the distribution of training data, if a symptom is highly coupled with a certain disease, the agent might learn to only inquire about that symptom to discriminate that disease, thus might not generalize to the out-of-distribution cases. To this end, we propose a progressive assurance agent, which includes the dual processes accounting for symptom inquiry and disease diagnosis respectively. The inquiry process is driven by the diagnosis process in a top-down manner to inquire about symptoms for enhancing diagnostic confidence.



### An End-to-End Geometric Deficiency Elimination Algorithm for 3D Meshes
- **Arxiv ID**: http://arxiv.org/abs/2003.06535v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.06535v2)
- **Published**: 2020-03-14 02:31:58+00:00
- **Updated**: 2020-09-03 02:55:59+00:00
- **Authors**: Bingtao Ma, Hongsen Liu, Liangliang Nan, Yang Cong
- **Comment**: None
- **Journal**: None
- **Summary**: The 3D mesh is an important representation of geometric data. In the generation of mesh data, geometric deficiencies (e.g., duplicate elements, degenerate faces, isolated vertices, self-intersection, and inner faces) are unavoidable and may violate the topology structure of an object. In this paper, we propose an effective and efficient geometric deficiency elimination algorithm for 3D meshes. Specifically, duplicate elements can be eliminated by assessing the occurrence times of vertices or faces; degenerate faces can be removed according to the outer product of two edges; since isolated vertices do not appear in any face vertices, they can be deleted directly; self-intersecting faces are detected using an AABB tree and remeshed afterward; by simulating whether multiple random rays that shoot from a face can reach infinity, we can judge whether the surface is an inner face, then decide to delete it or not. Experiments on ModelNet40 dataset illustrate that our method can eliminate the deficiencies of the 3D mesh thoroughly.



### OccuSeg: Occupancy-aware 3D Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2003.06537v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.06537v3)
- **Published**: 2020-03-14 02:48:55+00:00
- **Updated**: 2020-04-28 07:29:53+00:00
- **Authors**: Lei Han, Tian Zheng, Lan Xu, Lu Fang
- **Comment**: CVPR 2020, video this https URL https://youtu.be/co7y6LQ7Kqc
- **Journal**: None
- **Summary**: 3D instance segmentation, with a variety of applications in robotics and augmented reality, is in large demands these days. Unlike 2D images that are projective observations of the environment, 3D models provide metric reconstruction of the scenes without occlusion or scale ambiguity. In this paper, we define "3D occupancy size", as the number of voxels occupied by each instance. It owns advantages of robustness in prediction, on which basis, OccuSeg, an occupancy-aware 3D instance segmentation scheme is proposed. Our multi-task learning produces both occupancy signal and embedding representations, where the training of spatial and feature embeddings varies with their difference in scale-aware. Our clustering scheme benefits from the reliable comparison between the predicted occupancy size and the clustered occupancy size, which encourages hard samples being correctly clustered and avoids over segmentation. The proposed approach achieves state-of-the-art performance on 3 real-world datasets, i.e. ScanNetV2, S3DIS and SceneNN, while maintaining high efficiency.



### Dynamic Divide-and-Conquer Adversarial Training for Robust Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2003.06555v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.06555v2)
- **Published**: 2020-03-14 05:06:49+00:00
- **Updated**: 2021-08-16 15:00:16+00:00
- **Authors**: Xiaogang Xu, Hengshuang Zhao, Jiaya Jia
- **Comment**: ICCV2021
- **Journal**: None
- **Summary**: Adversarial training is promising for improving robustness of deep neural networks towards adversarial perturbations, especially on the classification task. The effect of this type of training on semantic segmentation, contrarily, just commences. We make the initial attempt to explore the defense strategy on semantic segmentation by formulating a general adversarial training procedure that can perform decently on both adversarial and clean samples. We propose a dynamic divide-and-conquer adversarial training (DDC-AT) strategy to enhance the defense effect, by setting additional branches in the target model during training, and dealing with pixels with diverse properties towards adversarial perturbation. Our dynamical division mechanism divides pixels into multiple branches automatically. Note all these additional branches can be abandoned during inference and thus leave no extra parameter and computation cost. Extensive experiments with various segmentation models are conducted on PASCAL VOC 2012 and Cityscapes datasets, in which DDC-AT yields satisfying performance under both white- and black-box attack.



### On the benefits of defining vicinal distributions in latent space
- **Arxiv ID**: http://arxiv.org/abs/2003.06566v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.06566v4)
- **Published**: 2020-03-14 06:45:26+00:00
- **Updated**: 2021-10-18 09:20:57+00:00
- **Authors**: Puneet Mangla, Vedant Singh, Shreyas Jayant Havaldar, Vineeth N Balasubramanian
- **Comment**: Accepted at Elsevier Pattern Recognition Letters (2021), Best Paper
  Award at CVPR 2021 Workshop on Adversarial Machine Learning in Real-World
  Computer Vision (AML-CV), Also accepted at ICLR 2021 Workshops on
  Robust-Reliable Machine Learning (Oral) and Generalization beyond the
  training distribution (Abstract)
- **Journal**: None
- **Summary**: The vicinal risk minimization (VRM) principle is an empirical risk minimization (ERM) variant that replaces Dirac masses with vicinal functions. There is strong numerical and theoretical evidence showing that VRM outperforms ERM in terms of generalization if appropriate vicinal functions are chosen. Mixup Training (MT), a popular choice of vicinal distribution, improves the generalization performance of models by introducing globally linear behavior in between training examples. Apart from generalization, recent works have shown that mixup trained models are relatively robust to input perturbations/corruptions and at the same time are calibrated better than their non-mixup counterparts. In this work, we investigate the benefits of defining these vicinal distributions like mixup in latent space of generative models rather than in input space itself. We propose a new approach - \textit{VarMixup (Variational Mixup)} - to better sample mixup images by using the latent manifold underlying the data. Our empirical studies on CIFAR-10, CIFAR-100, and Tiny-ImageNet demonstrate that models trained by performing mixup in the latent manifold learned by VAEs are inherently more robust to various input corruptions/perturbations, are significantly better calibrated, and exhibit more local-linear loss landscapes.



### AutoSTR: Efficient Backbone Search for Scene Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/2003.06567v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.06567v2)
- **Published**: 2020-03-14 06:51:04+00:00
- **Updated**: 2020-07-16 16:36:10+00:00
- **Authors**: Hui Zhang, Quanming Yao, Mingkun Yang, Yongchao Xu, Xiang Bai
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: Scene text recognition (STR) is very challenging due to the diversity of text instances and the complexity of scenes. The community has paid increasing attention to boost the performance by improving the pre-processing image module, like rectification and deblurring, or the sequence translator. However, another critical module, i.e., the feature sequence extractor, has not been extensively explored. In this work, inspired by the success of neural architecture search (NAS), which can identify better architectures than human-designed ones, we propose automated STR (AutoSTR) to search data-dependent backbones to boost text recognition performance. First, we design a domain-specific search space for STR, which contains both choices on operations and constraints on the downsampling path. Then, we propose a two-step search algorithm, which decouples operations and downsampling path, for an efficient search in the given space. Experiments demonstrate that, by searching data-dependent backbones, AutoSTR can outperform the state-of-the-art approaches on standard benchmarks with much fewer FLOPS and model parameters.



### Counterfactual Samples Synthesizing for Robust Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2003.06576v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2003.06576v1)
- **Published**: 2020-03-14 08:34:31+00:00
- **Updated**: 2020-03-14 08:34:31+00:00
- **Authors**: Long Chen, Xin Yan, Jun Xiao, Hanwang Zhang, Shiliang Pu, Yueting Zhuang
- **Comment**: Appear in CVPR 2020; Codes in https://github.com/yanxinzju/CSS-VQA
- **Journal**: None
- **Summary**: Despite Visual Question Answering (VQA) has realized impressive progress over the last few years, today's VQA models tend to capture superficial linguistic correlations in the train set and fail to generalize to the test set with different QA distributions. To reduce the language biases, several recent works introduce an auxiliary question-only model to regularize the training of targeted VQA model, and achieve dominating performance on VQA-CP. However, since the complexity of design, current methods are unable to equip the ensemble-based models with two indispensable characteristics of an ideal VQA model: 1) visual-explainable: the model should rely on the right visual regions when making decisions. 2) question-sensitive: the model should be sensitive to the linguistic variations in question. To this end, we propose a model-agnostic Counterfactual Samples Synthesizing (CSS) training scheme. The CSS generates numerous counterfactual training samples by masking critical objects in images or words in questions, and assigning different ground-truth answers. After training with the complementary samples (ie, the original and generated samples), the VQA models are forced to focus on all critical objects and words, which significantly improves both visual-explainable and question-sensitive abilities. In return, the performance of these models is further boosted. Extensive ablations have shown the effectiveness of CSS. Particularly, by building on top of the model LMH, we achieve a record-breaking performance of 58.95% on VQA-CP v2, with 6.5% gains.



### From W-Net to CDGAN: Bi-temporal Change Detection via Deep Learning Techniques
- **Arxiv ID**: http://arxiv.org/abs/2003.06583v1
- **DOI**: 10.1109/TGRS.2019.2948659
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.06583v1)
- **Published**: 2020-03-14 09:24:08+00:00
- **Updated**: 2020-03-14 09:24:08+00:00
- **Authors**: Bin Hou, Qingjie Liu, Heng Wang, Yunhong Wang
- **Comment**: Accept to TGRS
- **Journal**: None
- **Summary**: Traditional change detection methods usually follow the image differencing, change feature extraction and classification framework, and their performance is limited by such simple image domain differencing and also the hand-crafted features. Recently, the success of deep convolutional neural networks (CNNs) has widely spread across the whole field of computer vision for their powerful representation abilities. In this paper, we therefore address the remote sensing image change detection problem with deep learning techniques. We firstly propose an end-to-end dual-branch architecture, termed as the W-Net, with each branch taking as input one of the two bi-temporal images as in the traditional change detection models. In this way, CNN features with more powerful representative abilities can be obtained to boost the final detection performance. Also, W-Net performs differencing in the feature domain rather than in the traditional image domain, which greatly alleviates loss of useful information for determining the changes. Furthermore, by reformulating change detection as an image translation problem, we apply the recently popular Generative Adversarial Network (GAN) in which our W-Net serves as the Generator, leading to a new GAN architecture for change detection which we call CDGAN. To train our networks and also facilitate future research, we construct a large scale dataset by collecting images from Google Earth and provide carefully manually annotated ground truths. Experiments show that our proposed methods can provide fine-grained change detection results superior to the existing state-of-the-art baselines.



### Image-to-image Neural Network for Addition and Subtraction of a Pair of Not Very Large Numbers
- **Arxiv ID**: http://arxiv.org/abs/2003.06592v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.06592v1)
- **Published**: 2020-03-14 09:59:17+00:00
- **Updated**: 2020-03-14 09:59:17+00:00
- **Authors**: Vladimir Ivashkin
- **Comment**: None
- **Journal**: None
- **Summary**: Looking back at the history of calculators, one can see that they become less functional and more computationally expensive over time. A modern calculator runs on a personal computer and is drawn at 60 fps only to help us click a few digits with a mouse pointer. A search engine is often used as a calculator, which means that nowadays we need the Internet just to add two numbers. In this paper, we propose to go further and train a convolutional neural network that takes an image of a simple mathematical expression and generates an image of an answer. This neural calculator works only with pairs of double-digit numbers and supports only addition and subtraction. Also, sometimes it makes mistakes. We promise that the proposed calculator is a small step for man, but one giant leap for mankind.



### Brain MRI-based 3D Convolutional Neural Networks for Classification of Schizophrenia and Controls
- **Arxiv ID**: http://arxiv.org/abs/2003.08818v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.08818v1)
- **Published**: 2020-03-14 10:05:21+00:00
- **Updated**: 2020-03-14 10:05:21+00:00
- **Authors**: Mengjiao Hu, Kang Sim, Juan Helen Zhou, Xudong Jiang, Cuntai Guan
- **Comment**: 4 PAGES
- **Journal**: None
- **Summary**: Convolutional Neural Network (CNN) has been successfully applied on classification of both natural images and medical images but not yet been applied to differentiating patients with schizophrenia from healthy controls. Given the subtle, mixed, and sparsely distributed brain atrophy patterns of schizophrenia, the capability of automatic feature learning makes CNN a powerful tool for classifying schizophrenia from controls as it removes the subjectivity in selecting relevant spatial features. To examine the feasibility of applying CNN to classification of schizophrenia and controls based on structural Magnetic Resonance Imaging (MRI), we built 3D CNN models with different architectures and compared their performance with a handcrafted feature-based machine learning approach. Support vector machine (SVM) was used as classifier and Voxel-based Morphometry (VBM) was used as feature for handcrafted feature-based machine learning. 3D CNN models with sequential architecture, inception module and residual module were trained from scratch. CNN models achieved higher cross-validation accuracy than handcrafted feature-based machine learning. Moreover, testing on an independent dataset, 3D CNN models greatly outperformed handcrafted feature-based machine learning. This study underscored the potential of CNN for identifying patients with schizophrenia using 3D brain MR images and paved the way for imaging-based individual-level diagnosis and prognosis in psychiatric disorders.



### Collaborative Motion Prediction via Neural Motion Message Passing
- **Arxiv ID**: http://arxiv.org/abs/2003.06594v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.06594v1)
- **Published**: 2020-03-14 10:12:54+00:00
- **Updated**: 2020-03-14 10:12:54+00:00
- **Authors**: Yue Hu, Siheng Chen, Ya Zhang, Xiao Gu
- **Comment**: Accepted by CVPR 2020 Oral
- **Journal**: None
- **Summary**: Motion prediction is essential and challenging for autonomous vehicles and social robots. One challenge of motion prediction is to model the interaction among traffic actors, which could cooperate with each other to avoid collisions or form groups. To address this challenge, we propose neural motion message passing (NMMP) to explicitly model the interaction and learn representations for directed interactions between actors. Based on the proposed NMMP, we design the motion prediction systems for two settings: the pedestrian setting and the joint pedestrian and vehicle setting. Both systems share a common pattern: we use an individual branch to model the behavior of a single actor and an interactive branch to model the interaction between actors, while with different wrappers to handle the varied input formats and characteristics. The experimental results show that both systems outperform the previous state-of-the-art methods on several existing benchmarks. Besides, we provide interpretability for interaction learning.



### Learn to Augment: Joint Data Augmentation and Network Optimization for Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/2003.06606v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.06606v1)
- **Published**: 2020-03-14 11:18:22+00:00
- **Updated**: 2020-03-14 11:18:22+00:00
- **Authors**: Canjie Luo, Yuanzhi Zhu, Lianwen Jin, Yongpan Wang
- **Comment**: Accepted to Proc. IEEE Conf. Comp. Vis. Pattern Recogn. (CVPR) 2020
- **Journal**: None
- **Summary**: Handwritten text and scene text suffer from various shapes and distorted patterns. Thus training a robust recognition model requires a large amount of data to cover diversity as much as possible. In contrast to data collection and annotation, data augmentation is a low cost way. In this paper, we propose a new method for text image augmentation. Different from traditional augmentation methods such as rotation, scaling and perspective transformation, our proposed augmentation method is designed to learn proper and efficient data augmentation which is more effective and specific for training a robust recognizer. By using a set of custom fiducial points, the proposed augmentation method is flexible and controllable. Furthermore, we bridge the gap between the isolated processes of data augmentation and network optimization by joint learning. An agent network learns from the output of the recognition network and controls the fiducial points to generate more proper training samples for the recognition network. Extensive experiments on various benchmarks, including regular scene text, irregular scene text and handwritten text, show that the proposed augmentation and the joint learning methods significantly boost the performance of the recognition networks. A general toolkit for geometric augmentation is available.



### Medical Image Enhancement Using Histogram Processing and Feature Extraction for Cancer Classification
- **Arxiv ID**: http://arxiv.org/abs/2003.06615v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.06615v1)
- **Published**: 2020-03-14 12:11:23+00:00
- **Updated**: 2020-03-14 12:11:23+00:00
- **Authors**: Sakshi Patel, Bharath K P, Rajesh Kumar Muthu
- **Comment**: None
- **Journal**: None
- **Summary**: MRI (Magnetic Resonance Imaging) is a technique used to analyze and diagnose the problem defined by images like cancer or tumor in a brain. Physicians require good contrast images for better treatment purpose as it contains maximum information of the disease. MRI images are low contrast images which make diagnoses difficult; hence better localization of image pixels is required. Histogram Equalization techniques help to enhance the image so that it gives an improved visual quality and a well defined problem. The contrast and brightness is enhanced in such a way that it does not lose its original information and the brightness is preserved. We compare the different equalization techniques in this paper; the techniques are critically studied and elaborated. They are also tabulated to compare various parameters present in the image. In addition we have also segmented and extracted the tumor part out of the brain using K-means algorithm. For classification and feature extraction the method used is Support Vector Machine (SVM). The main goal of this research work is to help the medical field with a light of image processing.



### Monocular Depth Estimation Based On Deep Learning: An Overview
- **Arxiv ID**: http://arxiv.org/abs/2003.06620v2
- **DOI**: 10.1007/s11431-020-1582-8
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.06620v2)
- **Published**: 2020-03-14 12:35:34+00:00
- **Updated**: 2020-07-03 11:41:20+00:00
- **Authors**: Chaoqiang Zhao, Qiyu Sun, Chongzhen Zhang, Yang Tang, Feng Qian
- **Comment**: 14 pages, 4 figures
- **Journal**: None
- **Summary**: Depth information is important for autonomous systems to perceive environments and estimate their own state. Traditional depth estimation methods, like structure from motion and stereo vision matching, are built on feature correspondences of multiple viewpoints. Meanwhile, the predicted depth maps are sparse. Inferring depth information from a single image (monocular depth estimation) is an ill-posed problem. With the rapid development of deep neural networks, monocular depth estimation based on deep learning has been widely studied recently and achieved promising performance in accuracy. Meanwhile, dense depth maps are estimated from single images by deep neural networks in an end-to-end manner. In order to improve the accuracy of depth estimation, different kinds of network frameworks, loss functions and training strategies are proposed subsequently. Therefore, we survey the current monocular depth estimation methods based on deep learning in this review. Initially, we conclude several widely used datasets and evaluation indicators in deep learning-based depth estimation. Furthermore, we review some representative existing methods according to different training manners: supervised, unsupervised and semi-supervised. Finally, we discuss the challenges and provide some ideas for future researches in monocular depth estimation.



### Rapid Whole Slide Imaging via Learning-based Two-shot Virtual Autofocusing
- **Arxiv ID**: http://arxiv.org/abs/2003.06630v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.06630v1)
- **Published**: 2020-03-14 13:40:33+00:00
- **Updated**: 2020-03-14 13:40:33+00:00
- **Authors**: Qiang Li, Xianming Liu, Kaige Han, Cheng Guo, Xiangyang Ji, Xiaolin Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Whole slide imaging (WSI) is an emerging technology for digital pathology. The process of autofocusing is the main influence of the performance of WSI. Traditional autofocusing methods either are time-consuming due to repetitive mechanical motions, or require additional hardware and thus are not compatible to current WSI systems. In this paper, we propose the concept of \textit{virtual autofocusing}, which does not rely on mechanical adjustment to conduct refocusing but instead recovers in-focus images in an offline learning-based manner. With the initial focal position, we only perform two-shot imaging, in contrast traditional methods commonly need to conduct as many as 21 times image shooting in each tile scanning. Considering that the two captured out-of-focus images retain pieces of partial information about the underlying in-focus image, we propose a U-Net-inspired deep neural network based approach for fusing them into a recovered in-focus image. The proposed scheme is fast in tissue slides scanning, enabling a high-throughput generation of digital pathology images. Experimental results demonstrate that our scheme achieves satisfactory refocusing performance.



### Non-Local Part-Aware Point Cloud Denoising
- **Arxiv ID**: http://arxiv.org/abs/2003.06631v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.06631v1)
- **Published**: 2020-03-14 13:51:50+00:00
- **Updated**: 2020-03-14 13:51:50+00:00
- **Authors**: Chao Huang, Ruihui Li, Xianzhi Li, Chi-Wing Fu
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a novel non-local part-aware deep neural network to denoise point clouds by exploring the inherent non-local self-similarity in 3D objects and scenes. Different from existing works that explore small local patches, we design the non-local learning unit (NLU) customized with a graph attention module to adaptively capture non-local semantically-related features over the entire point cloud. To enhance the denoising performance, we cascade a series of NLUs to progressively distill the noise features from the noisy inputs. Further, besides the conventional surface reconstruction loss, we formulate a semantic part loss to regularize the predictions towards the relevant parts and enable denoising in a part-aware manner. Lastly, we performed extensive experiments to evaluate our method, both quantitatively and qualitatively, and demonstrate its superiority over the state-of-the-arts on both synthetic and real-scanned noisy inputs.



### Functional Data Analysis and Visualisation of Three-dimensional Surface Shape
- **Arxiv ID**: http://arxiv.org/abs/2003.08817v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ME
- **Links**: [PDF](http://arxiv.org/pdf/2003.08817v1)
- **Published**: 2020-03-14 13:54:24+00:00
- **Updated**: 2020-03-14 13:54:24+00:00
- **Authors**: Stanislav Katina, Liberty Vittert, Adrian W. Bowman
- **Comment**: None
- **Journal**: None
- **Summary**: The advent of high resolution imaging has made data on surface shape widespread. Methods for the analysis of shape based on landmarks are well established but high resolution data require a functional approach. The starting point is a systematic and consistent description of each surface shape. Three innovative forms of analysis are then introduced. The first uses surface integration to address issues of registration, principal component analysis and the measurement of asymmetry, all in functional form. Computational issues are handled through discrete approximations to integrals, based in this case on appropriate surface area weighted sums. The second innovation is to focus on sub-spaces where interesting behaviour such as group differences are exhibited, rather than on individual principal components. The third innovation concerns the comparison of individual shapes with a relevant control set, where the concept of a normal range is extended to the highly multivariate setting of surface shape. This has particularly strong applications to medical contexts where the assessment of individual patients is very important. All of these ideas are developed and illustrated in the important context of human facial shape, with a strong emphasis on the effective visual communication of effects of interest.



### Large-Scale Optimal Transport via Adversarial Training with Cycle-Consistency
- **Arxiv ID**: http://arxiv.org/abs/2003.06635v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.06635v1)
- **Published**: 2020-03-14 14:06:46+00:00
- **Updated**: 2020-03-14 14:06:46+00:00
- **Authors**: Guansong Lu, Zhiming Zhou, Jian Shen, Cheng Chen, Weinan Zhang, Yong Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in large-scale optimal transport have greatly extended its application scenarios in machine learning. However, existing methods either not explicitly learn the transport map or do not support general cost function. In this paper, we propose an end-to-end approach for large-scale optimal transport, which directly solves the transport map and is compatible with general cost function. It models the transport map via stochastic neural networks and enforces the constraint on the marginal distributions via adversarial training. The proposed framework can be further extended towards learning Monge map or optimal bijection via adopting cycle-consistency constraint(s). We verify the effectiveness of the proposed method and demonstrate its superior performance against existing methods with large-scale real-world applications, including domain adaptation, image-to-image translation, and color transfer.



### Fast Depth Estimation for View Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2003.06637v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.06637v1)
- **Published**: 2020-03-14 14:10:42+00:00
- **Updated**: 2020-03-14 14:10:42+00:00
- **Authors**: Nantheera Anantrasirichai, Majid Geravand, David Braendler, David R. Bull
- **Comment**: 5 pages
- **Journal**: None
- **Summary**: Disparity/depth estimation from sequences of stereo images is an important element in 3D vision. Owing to occlusions, imperfect settings and homogeneous luminance, accurate estimate of depth remains a challenging problem. Targetting view synthesis, we propose a novel learning-based framework making use of dilated convolution, densely connected convolutional modules, compact decoder and skip connections. The network is shallow but dense, so it is fast and accurate. Two additional contributions -- a non-linear adjustment of the depth resolution and the introduction of a projection loss, lead to reduction of estimation error by up to 20% and 25% respectively. The results show that our network outperforms state-of-the-art methods with an average improvement in accuracy of depth estimation and view synthesis by approximately 45% and 34% respectively. Where our method generates comparable quality of estimated depth, it performs 10 times faster than those methods.



### Investigating Generalization in Neural Networks under Optimally Evolved Training Perturbations
- **Arxiv ID**: http://arxiv.org/abs/2003.06646v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.06646v1)
- **Published**: 2020-03-14 14:38:07+00:00
- **Updated**: 2020-03-14 14:38:07+00:00
- **Authors**: Subhajit Chaudhury, Toshihiko Yamasaki
- **Comment**: Accepted at IEEE ICASSP 2020
- **Journal**: None
- **Summary**: In this paper, we study the generalization properties of neural networks under input perturbations and show that minimal training data corruption by a few pixel modifications can cause drastic overfitting. We propose an evolutionary algorithm to search for optimal pixel perturbations using novel cost function inspired from literature in domain adaptation that explicitly maximizes the generalization gap and domain divergence between clean and corrupted images. Our method outperforms previous pixel-based data distribution shift methods on state-of-the-art Convolutional Neural Networks (CNNs) architectures. Interestingly, we find that the choice of optimization plays an important role in generalization robustness due to the empirical observation that SGD is resilient to such training data corruption unlike adaptive optimization techniques (ADAM). Our source code is available at https://github.com/subhajitchaudhury/evo-shift.



### Structured Domain Adaptation with Online Relation Regularization for Unsupervised Person Re-ID
- **Arxiv ID**: http://arxiv.org/abs/2003.06650v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.06650v3)
- **Published**: 2020-03-14 14:45:18+00:00
- **Updated**: 2022-05-05 13:58:36+00:00
- **Authors**: Yixiao Ge, Feng Zhu, Dapeng Chen, Rui Zhao, Xiaogang Wang, Hongsheng Li
- **Comment**: Accepted by IEEE Transactions on Neural Networks and Learning Systems
  (TNNLS)
- **Journal**: None
- **Summary**: Unsupervised domain adaptation (UDA) aims at adapting the model trained on a labeled source-domain dataset to an unlabeled target-domain dataset. The task of UDA on open-set person re-identification (re-ID) is even more challenging as the identities (classes) do not have overlap between the two domains. One major research direction was based on domain translation, which, however, has fallen out of favor in recent years due to inferior performance compared to pseudo-label-based methods. We argue that the domain translation has great potential on exploiting the valuable source-domain data but existing methods did not provide proper regularization on the translation process. Specifically, previous methods only focus on maintaining the identities of the translated images while ignoring the inter-sample relations during translation. To tackle the challenges, we propose an end-to-end structured domain adaptation framework with an online relation-consistency regularization term. During training, the person feature encoder is optimized to model inter-sample relations on-the-fly for supervising relation-consistency domain translation, which in turn, improves the encoder with informative translated images. The encoder can be further improved with pseudo labels, where the source-to-target translated images with ground-truth identities and target-domain images with pseudo identities are jointly used for training. In the experiments, our proposed framework is shown to achieve state-of-the-art performance on multiple UDA tasks of person re-ID. With the synthetic-to-real translated images from our structured domain-translation network, we achieved second place in the Visual Domain Adaptation Challenge (VisDA) in 2020.



### Interactive Neural Style Transfer with Artists
- **Arxiv ID**: http://arxiv.org/abs/2003.06659v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.06659v1)
- **Published**: 2020-03-14 15:27:44+00:00
- **Updated**: 2020-03-14 15:27:44+00:00
- **Authors**: Thomas Kerdreux, Louis Thiry, Erwan Kerdreux
- **Comment**: None
- **Journal**: None
- **Summary**: We present interactive painting processes in which a painter and various neural style transfer algorithms interact on a real canvas. Understanding what these algorithms' outputs achieve is then paramount to describe the creative agency in our interactive experiments. We gather a set of paired painting-pictures images and present a new evaluation methodology based on the predictivity of neural style transfer algorithms. We point some algorithms' instabilities and show that they can be used to enlarge the diversity and pleasing oddity of the images synthesized by the numerous existing neural style transfer algorithms. This diversity of images was perceived as a source of inspiration for human painters, portraying the machine as a computational catalyst.



### TAFSSL: Task-Adaptive Feature Sub-Space Learning for few-shot classification
- **Arxiv ID**: http://arxiv.org/abs/2003.06670v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.06670v1)
- **Published**: 2020-03-14 16:59:17+00:00
- **Updated**: 2020-03-14 16:59:17+00:00
- **Authors**: Moshe Lichtenstein, Prasanna Sattigeri, Rogerio Feris, Raja Giryes, Leonid Karlinsky
- **Comment**: None
- **Journal**: None
- **Summary**: The field of Few-Shot Learning (FSL), or learning from very few (typically $1$ or $5$) examples per novel class (unseen during training), has received a lot of attention and significant performance advances in the recent literature. While number of techniques have been proposed for FSL, several factors have emerged as most important for FSL performance, awarding SOTA even to the simplest of techniques. These are: the backbone architecture (bigger is better), type of pre-training on the base classes (meta-training vs regular multi-class, currently regular wins), quantity and diversity of the base classes set (the more the merrier, resulting in richer and better adaptive features), and the use of self-supervised tasks during pre-training (serving as a proxy for increasing the diversity of the base set). In this paper we propose yet another simple technique that is important for the few shot learning performance - a search for a compact feature sub-space that is discriminative for a given few-shot test task. We show that the Task-Adaptive Feature Sub-Space Learning (TAFSSL) can significantly boost the performance in FSL scenarios when some additional unlabeled data accompanies the novel few-shot task, be it either the set of unlabeled queries (transductive FSL) or some additional set of unlabeled data samples (semi-supervised FSL). Specifically, we show that on the challenging miniImageNet and tieredImageNet benchmarks, TAFSSL can improve the current state-of-the-art in both transductive and semi-supervised FSL settings by more than $5\%$, while increasing the benefit of using unlabeled data in FSL to above $10\%$ performance gain.



### EmotiCon: Context-Aware Multimodal Emotion Recognition using Frege's Principle
- **Arxiv ID**: http://arxiv.org/abs/2003.06692v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.06692v1)
- **Published**: 2020-03-14 19:55:21+00:00
- **Updated**: 2020-03-14 19:55:21+00:00
- **Authors**: Trisha Mittal, Pooja Guhan, Uttaran Bhattacharya, Rohan Chandra, Aniket Bera, Dinesh Manocha
- **Comment**: None
- **Journal**: None
- **Summary**: We present EmotiCon, a learning-based algorithm for context-aware perceived human emotion recognition from videos and images. Motivated by Frege's Context Principle from psychology, our approach combines three interpretations of context for emotion recognition. Our first interpretation is based on using multiple modalities(e.g. faces and gaits) for emotion recognition. For the second interpretation, we gather semantic context from the input image and use a self-attention-based CNN to encode this information. Finally, we use depth maps to model the third interpretation related to socio-dynamic interactions and proximity among agents. We demonstrate the efficiency of our network through experiments on EMOTIC, a benchmark dataset. We report an Average Precision (AP) score of 35.48 across 26 classes, which is an improvement of 7-8 over prior methods. We also introduce a new dataset, GroupWalk, which is a collection of videos captured in multiple real-world settings of people walking. We report an AP of 65.83 across 4 categories on GroupWalk, which is also an improvement over prior methods.



### Identifying Individual Dogs in Social Media Images
- **Arxiv ID**: http://arxiv.org/abs/2003.06705v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.06705v1)
- **Published**: 2020-03-14 21:11:02+00:00
- **Updated**: 2020-03-14 21:11:02+00:00
- **Authors**: Djordje Batic, Dubravko Culibrk
- **Comment**: Presented at BMVC 2019: Workshop on Visual AI and Entrepreneurship,
  Cardiff, UK
- **Journal**: None
- **Summary**: We present the results of an initial study focused on developing a visual AI solution able to recognize individual dogs in unconstrained (wild) images occurring on social media.   The work described here is part of joint project done with Pet2Net, a social network focused on pets and their owners. In order to detect and recognize individual dogs we combine transfer learning and object detection approaches on Inception v3 and SSD Inception v2 architectures respectively and evaluate the proposed pipeline using a new data set containing real data that the users uploaded to Pet2Net platform. We show that it can achieve 94.59% accuracy in identifying individual dogs. Our approach has been designed with simplicity in mind and the goal of easy deployment on all the images uploaded to Pet2Net platform.   A purely visual approach to identifying dogs in images, will enhance Pet2Net features aimed at finding lost dogs, as well as form the basis of future work focused on identifying social relationships between dogs, which cannot be inferred from other data collected by the platform.



### Emotions Don't Lie: An Audio-Visual Deepfake Detection Method Using Affective Cues
- **Arxiv ID**: http://arxiv.org/abs/2003.06711v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/2003.06711v3)
- **Published**: 2020-03-14 22:07:26+00:00
- **Updated**: 2020-08-01 20:43:34+00:00
- **Authors**: Trisha Mittal, Uttaran Bhattacharya, Rohan Chandra, Aniket Bera, Dinesh Manocha
- **Comment**: Accepted to ACMMM-2020
- **Journal**: None
- **Summary**: We present a learning-based method for detecting real and fake deepfake multimedia content. To maximize information for learning, we extract and analyze the similarity between the two audio and visual modalities from within the same video. Additionally, we extract and compare affective cues corresponding to perceived emotion from the two modalities within a video to infer whether the input video is "real" or "fake". We propose a deep learning network, inspired by the Siamese network architecture and the triplet loss. To validate our model, we report the AUC metric on two large-scale deepfake detection datasets, DeepFake-TIMIT Dataset and DFDC. We compare our approach with several SOTA deepfake detection methods and report per-video AUC of 84.4% on the DFDC and 96.6% on the DF-TIMIT datasets, respectively. To the best of our knowledge, ours is the first approach that simultaneously exploits audio and video modalities and also perceived emotions from the two modalities for deepfake detection.



### Class Conditional Alignment for Partial Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2003.06722v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.06722v1)
- **Published**: 2020-03-14 23:51:57+00:00
- **Updated**: 2020-03-14 23:51:57+00:00
- **Authors**: Mohsen Kheirandishfard, Fariba Zohrizadeh, Farhad Kamangar
- **Comment**: None
- **Journal**: None
- **Summary**: Adversarial adaptation models have demonstrated significant progress towards transferring knowledge from a labeled source dataset to an unlabeled target dataset. Partial domain adaptation (PDA) investigates the scenarios in which the source domain is large and diverse, and the target label space is a subset of the source label space. The main purpose of PDA is to identify the shared classes between the domains and promote learning transferable knowledge from these classes. In this paper, we propose a multi-class adversarial architecture for PDA. The proposed approach jointly aligns the marginal and class-conditional distributions in the shared label space by minimaxing a novel multi-class adversarial loss function. Furthermore, we incorporate effective regularization terms to encourage selecting the most relevant subset of source domain classes. In the absence of target labels, the proposed approach is able to effectively learn domain-invariant feature representations, which in turn can enhance the classification performance in the target domain. Comprehensive experiments on three benchmark datasets Office-31, Office-Home, and Caltech-Office corroborate the effectiveness of the proposed approach in addressing different partial transfer learning tasks.



