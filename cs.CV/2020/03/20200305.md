# Arxiv Papers in cs.CV on 2020-03-05
### Plant Disease Detection from Images
- **Arxiv ID**: http://arxiv.org/abs/2003.05379v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.05379v1)
- **Published**: 2020-03-05 02:17:36+00:00
- **Updated**: 2020-03-05 02:17:36+00:00
- **Authors**: Anjaneya Teja Sarma Kalvakolanu
- **Comment**: None
- **Journal**: None
- **Summary**: Plant disease detection is a huge problem and often require professional help to detect the disease. This research focuses on creating a deep learning model that detects the type of disease that affected the plant from the images of the leaves of the plants. The deep learning is done with the help of Convolutional Neural Network by performing transfer learning. The model is created using transfer learning and is experimented with both resnet 34 and resnet 50 to demonstrate that discriminative learning gives better results. This method achieved state of art results for the dataset used. The main goal is to lower the professional help to detect the plant diseases and make this model accessible to as many people as possible.



### Who Make Drivers Stop? Towards Driver-centric Risk Assessment: Risk Object Identification via Causal Inference
- **Arxiv ID**: http://arxiv.org/abs/2003.02425v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2003.02425v2)
- **Published**: 2020-03-05 04:14:35+00:00
- **Updated**: 2020-08-03 04:28:21+00:00
- **Authors**: Chengxi Li, Stanley H. Chan, Yi-Ting Chen
- **Comment**: Accepted to the International Conference on Intelligent Robots and
  Systems (IROS) 2020
- **Journal**: None
- **Summary**: A significant amount of people die in road accidents due to driver errors. To reduce fatalities, developing intelligent driving systems assisting drivers to identify potential risks is in an urgent need. Risky situations are generally defined based on collision prediction in the existing works. However, collision is only a source of potential risks, and a more generic definition is required. In this work, we propose a novel driver-centric definition of risk, i.e., objects influencing drivers' behavior are risky. A new task called risk object identification is introduced. We formulate the task as the cause-effect problem and present a novel two-stage risk object identification framework based on causal inference with the proposed object-level manipulable driving model. We demonstrate favorable performance on risk object identification compared with strong baselines on the Honda Research Institute Driving Dataset (HDD). Our framework achieves a substantial average performance boost over a strong baseline by 7.5%.



### Team O2AS at the World Robot Summit 2018: An Approach to Robotic Kitting and Assembly Tasks using General Purpose Grippers and Tools
- **Arxiv ID**: http://arxiv.org/abs/2003.02427v1
- **DOI**: 10.1080/01691864.2020.1734481
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.02427v1)
- **Published**: 2020-03-05 04:34:09+00:00
- **Updated**: 2020-03-05 04:34:09+00:00
- **Authors**: Felix von Drigalski, Chisato Nakashima, Yoshiya Shibata, Yoshinori Konishi, Joshua C. Triyonoputro, Kaidi Nie, Damien Petit, Toshio Ueshiba, Ryuichi Takase, Yukiyasu Domae, Taku Yoshioka, Yoshihisa Ijiri, Ixchel G. Ramirez-Alpizar, Weiwei Wan, Kensuke Harada
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a versatile robotic system for kitting and assembly tasks which uses no jigs or commercial tool changers. Instead of specialized end effectors, it uses its two-finger grippers to grasp and hold tools to perform subtasks such as screwing and suctioning. A third gripper is used as a precision picking and centering tool, and uses in-built passive compliance to compensate for small position errors and uncertainty. A novel grasp point detection for bin picking is described for the kitting task, using a single depth map. Using the proposed system we competed in the Assembly Challenge of the Industrial Robotics Category of the World Robot Challenge at the World Robot Summit 2018, obtaining 4th place and the SICE award for lean design and versatile tool use. We show the effectiveness of our approach through experiments performed during the competition.



### Drone-based RGB-Infrared Cross-Modality Vehicle Detection via Uncertainty-Aware Learning
- **Arxiv ID**: http://arxiv.org/abs/2003.02437v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.02437v2)
- **Published**: 2020-03-05 05:29:44+00:00
- **Updated**: 2021-10-14 06:38:19+00:00
- **Authors**: Yiming Sun, Bing Cao, Pengfei Zhu, Qinghua Hu
- **Comment**: None
- **Journal**: None
- **Summary**: Drone-based vehicle detection aims at finding the vehicle locations and categories in an aerial image. It empowers smart city traffic management and disaster rescue. Researchers have made mount of efforts in this area and achieved considerable progress. Nevertheless, it is still a challenge when the objects are hard to distinguish, especially in low light conditions. To tackle this problem, we construct a large-scale drone-based RGB-Infrared vehicle detection dataset, termed DroneVehicle. Our DroneVehicle collects 28, 439 RGB-Infrared image pairs, covering urban roads, residential areas, parking lots, and other scenarios from day to night. Due to the great gap between RGB and infrared images, cross-modal images provide both effective information and redundant information. To address this dilemma, we further propose an uncertainty-aware cross-modality vehicle detection (UA-CMDet) framework to extract complementary information from cross-modal images, which can significantly improve the detection performance in low light conditions. An uncertainty-aware module (UAM) is designed to quantify the uncertainty weights of each modality, which is calculated by the cross-modal Intersection over Union (IoU) and the RGB illumination value. Furthermore, we design an illumination-aware cross-modal non-maximum suppression algorithm to better integrate the modal-specific information in the inference phase. Extensive experiments on the DroneVehicle dataset demonstrate the flexibility and effectiveness of the proposed method for crossmodality vehicle detection. The dataset can be download from https://github.com/VisDrone/DroneVehicle.



### Harnessing Multi-View Perspective of Light Fields for Low-Light Imaging
- **Arxiv ID**: http://arxiv.org/abs/2003.02438v2
- **DOI**: 10.1109/TIP.2020.3045617
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.02438v2)
- **Published**: 2020-03-05 05:32:44+00:00
- **Updated**: 2020-12-08 11:16:25+00:00
- **Authors**: Mohit Lamba, Kranthi Kumar, Kaushik Mitra
- **Comment**: Visit our project page at https://mohitlamba94.github.io/L3Fnet/ to
  download the dataset and the code
- **Journal**: None
- **Summary**: Light Field (LF) offers unique advantages such as post-capture refocusing and depth estimation, but low-light conditions limit these capabilities. To restore low-light LFs we should harness the geometric cues present in different LF views, which is not possible using single-frame low-light enhancement techniques. We, therefore, propose a deep neural network for Low-Light Light Field (L3F) restoration, which we refer to as L3Fnet. The proposed L3Fnet not only performs the necessary visual enhancement of each LF view but also preserves the epipolar geometry across views. We achieve this by adopting a two-stage architecture for L3Fnet. Stage-I looks at all the LF views to encode the LF geometry. This encoded information is then used in Stage-II to reconstruct each LF view. To facilitate learning-based techniques for low-light LF imaging, we collected a comprehensive LF dataset of various scenes. For each scene, we captured four LFs, one with near-optimal exposure and ISO settings and the others at different levels of low-light conditions varying from low to extreme low-light settings. The effectiveness of the proposed L3Fnet is supported by both visual and numerical comparisons on this dataset. To further analyze the performance of low-light reconstruction methods, we also propose an L3F-wild dataset that contains LF captured late at night with almost zero lux values. No ground truth is available in this dataset. To perform well on the L3F-wild dataset, any method must adapt to the light level of the captured scene. To do this we propose a novel pre-processing block that makes L3Fnet robust to various degrees of low-light conditions. Lastly, we show that L3Fnet can also be used for low-light enhancement of single-frame images, despite it being engineered for LF data. We do so by converting the single-frame DSLR image into a form suitable to L3Fnet, which we call as pseudo-LF.



### End-to-End Trainable One-Stage Parking Slot Detection Integrating Global and Local Information
- **Arxiv ID**: http://arxiv.org/abs/2003.02445v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.02445v1)
- **Published**: 2020-03-05 05:57:20+00:00
- **Updated**: 2020-03-05 05:57:20+00:00
- **Authors**: Jae Kyu Suhr, Ho Gi Jung
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes an end-to-end trainable one-stage parking slot detection method for around view monitor (AVM) images. The proposed method simultaneously acquires global information (entrance, type, and occupancy of parking slot) and local information (location and orientation of junction) by using a convolutional neural network (CNN), and integrates them to detect parking slots with their properties. This method divides an AVM image into a grid and performs a CNN-based feature extraction. For each cell of the grid, the global and local information of the parking slot is obtained by applying convolution filters to the extracted feature map. Final detection results are produced by integrating the global and local information of the parking slot through non-maximum suppression (NMS). Since the proposed method obtains most of the information of the parking slot using a fully convolutional network without a region proposal stage, it is an end-to-end trainable one-stage detector. In experiments, this method was quantitatively evaluated using the public dataset and outperforms previous methods by showing both recall and precision of 99.77%, type classification accuracy of 100%, and occupancy classification accuracy of 99.31% while processing 60 frames per second.



### Cluster Pruning: An Efficient Filter Pruning Method for Edge AI Vision Applications
- **Arxiv ID**: http://arxiv.org/abs/2003.02449v1
- **DOI**: 10.1109/JSTSP.2020.2971418
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.02449v1)
- **Published**: 2020-03-05 06:20:09+00:00
- **Updated**: 2020-03-05 06:20:09+00:00
- **Authors**: Chinthaka Gamanayake, Lahiru Jayasinghe, Benny Ng, Chau Yuen
- **Comment**: None
- **Journal**: J-STSP-CDNN-00206-2019
- **Summary**: Even though the Convolutional Neural Networks (CNN) has shown superior results in the field of computer vision, it is still a challenging task to implement computer vision algorithms in real-time at the edge, especially using a low-cost IoT device due to high memory consumption and computation complexities in a CNN. Network compression methodologies such as weight pruning, filter pruning, and quantization are used to overcome the above mentioned problem. Even though filter pruning methodology has shown better performances compared to other techniques, irregularity of the number of filters pruned across different layers of a CNN might not comply with majority of the neural computing hardware architectures. In this paper, a novel greedy approach called cluster pruning has been proposed, which provides a structured way of removing filters in a CNN by considering the importance of filters and the underlying hardware architecture. The proposed methodology is compared with the conventional filter pruning algorithm on Pascal-VOC open dataset, and Head-Counting dataset, which is our own dataset developed to detect and count people entering a room. We benchmark our proposed method on three hardware architectures, namely CPU, GPU, and Intel Movidius Neural Computer Stick (NCS) using the popular SSD-MobileNet and SSD-SqueezeNet neural network architectures used for edge-AI vision applications. Results demonstrate that our method outperforms the conventional filter pruning methodology, using both datasets on above mentioned hardware architectures. Furthermore, a low cost IoT hardware setup consisting of an Intel Movidius-NCS is proposed to deploy an edge-AI application using our proposed pruning methodology.



### Fake Generated Painting Detection via Frequency Analysis
- **Arxiv ID**: http://arxiv.org/abs/2003.02467v2
- **DOI**: 10.1109/ICIP40778.2020.9190892
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.02467v2)
- **Published**: 2020-03-05 07:33:07+00:00
- **Updated**: 2020-06-19 08:17:37+00:00
- **Authors**: Yong Bai, Yuanfang Guo, Jinjie Wei, Lin Lu, Rui Wang, Yunhong Wang
- **Comment**: 5 pages, 6 figures, accepted by ICIP 2020
- **Journal**: None
- **Summary**: With the development of deep neural networks, digital fake paintings can be generated by various style transfer algorithms.To detect the fake generated paintings, we analyze the fake generated and real paintings in Fourier frequency domain and observe statistical differences and artifacts. Based on our observations, we propose Fake Generated Painting Detection via Frequency Analysis (FGPD-FA) by extracting three types of features in frequency domain. Besides, we also propose a digital fake painting detection database for assessing the proposed method. Experimental results demonstrate the excellence of the proposed method in different testing conditions.



### Cumulant-free closed-form formulas for some common (dis)similarities between densities of an exponential family
- **Arxiv ID**: http://arxiv.org/abs/2003.02469v3
- **DOI**: None
- **Categories**: **math.ST**, cs.CV, cs.IT, cs.LG, math.IT, stat.TH
- **Links**: [PDF](http://arxiv.org/pdf/2003.02469v3)
- **Published**: 2020-03-05 07:46:22+00:00
- **Updated**: 2020-04-07 04:11:00+00:00
- **Authors**: Frank Nielsen, Richard Nock
- **Comment**: 33 pages
- **Journal**: None
- **Summary**: It is well-known that the Bhattacharyya, Hellinger, Kullback-Leibler, $\alpha$-divergences, and Jeffreys' divergences between densities belonging to a same exponential family have generic closed-form formulas relying on the strictly convex and real-analytic cumulant function characterizing the exponential family. In this work, we report (dis)similarity formulas which bypass the explicit use of the cumulant function and highlight the role of quasi-arithmetic means and their multivariate mean operator extensions. In practice, these cumulant-free formulas are handy when implementing these (dis)similarities using legacy Application Programming Interfaces (APIs) since our method requires only to partially factorize the densities canonically of the considered exponential family.



### Adversarial Vertex Mixup: Toward Better Adversarially Robust Generalization
- **Arxiv ID**: http://arxiv.org/abs/2003.02484v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.02484v3)
- **Published**: 2020-03-05 08:47:46+00:00
- **Updated**: 2020-07-27 12:26:13+00:00
- **Authors**: Saehyung Lee, Hyungyu Lee, Sungroh Yoon
- **Comment**: To appear in CVPR 2020 (Oral)
- **Journal**: None
- **Summary**: Adversarial examples cause neural networks to produce incorrect outputs with high confidence. Although adversarial training is one of the most effective forms of defense against adversarial examples, unfortunately, a large gap exists between test accuracy and training accuracy in adversarial training. In this paper, we identify Adversarial Feature Overfitting (AFO), which may cause poor adversarially robust generalization, and we show that adversarial training can overshoot the optimal point in terms of robust generalization, leading to AFO in our simple Gaussian model. Considering these theoretical results, we present soft labeling as a solution to the AFO problem. Furthermore, we propose Adversarial Vertex mixup (AVmixup), a soft-labeled data augmentation approach for improving adversarially robust generalization. We complement our theoretical analysis with experiments on CIFAR10, CIFAR100, SVHN, and Tiny ImageNet, and show that AVmixup significantly improves the robust generalization performance and that it reduces the trade-off between standard accuracy and adversarial robustness.



### Demographic Bias in Biometrics: A Survey on an Emerging Challenge
- **Arxiv ID**: http://arxiv.org/abs/2003.02488v2
- **DOI**: 10.1109/TTS.2020.2992344
- **Categories**: **cs.CY**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.02488v2)
- **Published**: 2020-03-05 09:07:59+00:00
- **Updated**: 2020-04-14 08:18:24+00:00
- **Authors**: P. Drozdowski, C. Rathgeb, A. Dantcheva, N. Damer, C. Busch
- **Comment**: 15 pages, 3 figures, 3 tables. Submitted to IEEE Transactions on
  Technology and Society. Update after first round of peer review
- **Journal**: IEEE Transactions on Technology and Society 1, no. 2 (2020):
  89-103
- **Summary**: Systems incorporating biometric technologies have become ubiquitous in personal, commercial, and governmental identity management applications. Both cooperative (e.g. access control) and non-cooperative (e.g. surveillance and forensics) systems have benefited from biometrics. Such systems rely on the uniqueness of certain biological or behavioural characteristics of human beings, which enable for individuals to be reliably recognised using automated algorithms.   Recently, however, there has been a wave of public and academic concerns regarding the existence of systemic bias in automated decision systems (including biometrics). Most prominently, face recognition algorithms have often been labelled as "racist" or "biased" by the media, non-governmental organisations, and researchers alike.   The main contributions of this article are: (1) an overview of the topic of algorithmic bias in the context of biometrics, (2) a comprehensive survey of the existing literature on biometric bias estimation and mitigation, (3) a discussion of the pertinent technical and social matters, and (4) an outline of the remaining challenges and future work items, both from technological and social points of view.



### Detecting Attended Visual Targets in Video
- **Arxiv ID**: http://arxiv.org/abs/2003.02501v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.02501v2)
- **Published**: 2020-03-05 09:29:48+00:00
- **Updated**: 2020-03-30 23:38:48+00:00
- **Authors**: Eunji Chong, Yongxin Wang, Nataniel Ruiz, James M. Rehg
- **Comment**: Accepted to CVPR 2020
- **Journal**: None
- **Summary**: We address the problem of detecting attention targets in video. Our goal is to identify where each person in each frame of a video is looking, and correctly handle the case where the gaze target is out-of-frame. Our novel architecture models the dynamic interaction between the scene and head features and infers time-varying attention targets. We introduce a new annotated dataset, VideoAttentionTarget, containing complex and dynamic patterns of real-world gaze behavior. Our experiments show that our model can effectively infer dynamic attention in videos. In addition, we apply our predicted attention maps to two social gaze behavior recognition tasks, and show that the resulting classifiers significantly outperform existing methods. We achieve state-of-the-art performance on three datasets: GazeFollow (static images), VideoAttentionTarget (videos), and VideoCoAtt (videos), and obtain the first results for automatically classifying clinically-relevant gaze behavior without wearable cameras or eye trackers.



### Hierarchical Modes Exploring in Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2003.08752v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08752v1)
- **Published**: 2020-03-05 10:43:50+00:00
- **Updated**: 2020-03-05 10:43:50+00:00
- **Authors**: Mengxiao Hu, Jinlong Li, Maolin Hu, Tao Hu
- **Comment**: None
- **Journal**: None
- **Summary**: In conditional Generative Adversarial Networks (cGANs), when two different initial noises are concatenated with the same conditional information, the distance between their outputs is relatively smaller, which makes minor modes likely to collapse into large modes. To prevent this happen, we proposed a hierarchical mode exploring method to alleviate mode collapse in cGANs by introducing a diversity measurement into the objective function as the regularization term. We also introduced the Expected Ratios of Expansion (ERE) into the regularization term, by minimizing the sum of differences between the real change of distance and ERE, we can control the diversity of generated images w.r.t specific-level features. We validated the proposed algorithm on four conditional image synthesis tasks including categorical generation, paired and un-paired image translation and text-to-image generation. Both qualitative and quantitative results show that the proposed method is effective in alleviating the mode collapse problem in cGANs, and can control the diversity of output images w.r.t specific-level features.



### A Balanced and Uncertainty-aware Approach for Partial Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2003.02541v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.02541v2)
- **Published**: 2020-03-05 11:37:06+00:00
- **Updated**: 2020-07-16 11:04:44+00:00
- **Authors**: Jian Liang, Yunbo Wang, Dapeng Hu, Ran He, Jiashi Feng
- **Comment**: Accepted to ECCV 2020
- **Journal**: None
- **Summary**: This work addresses the unsupervised domain adaptation problem, especially in the case of class labels in the target domain being only a subset of those in the source domain. Such a partial transfer setting is realistic but challenging and existing methods always suffer from two key problems, negative transfer and uncertainty propagation. In this paper, we build on domain adversarial learning and propose a novel domain adaptation method BA$^3$US with two new techniques termed Balanced Adversarial Alignment (BAA) and Adaptive Uncertainty Suppression (AUS), respectively. On one hand, negative transfer results in misclassification of target samples to the classes only present in the source domain. To address this issue, BAA pursues the balance between label distributions across domains in a fairly simple manner. Specifically, it randomly leverages a few source samples to augment the smaller target domain during domain alignment so that classes in different domains are symmetric. On the other hand, a source sample would be denoted as uncertain if there is an incorrect class that has a relatively high prediction score, and such uncertainty easily propagates to unlabeled target data around it during alignment, which severely deteriorates adaptation performance. Thus we present AUS that emphasizes uncertain samples and exploits an adaptive weighted complement entropy objective to encourage incorrect classes to have uniform and low prediction scores. Experimental results on multiple benchmarks demonstrate our BA$^3$US surpasses state-of-the-arts for partial domain adaptation tasks. Code is available at \url{https://github.com/tim-learn/BA3US}.



### Embedding Expansion: Augmentation in Embedding Space for Deep Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/2003.02546v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.02546v3)
- **Published**: 2020-03-05 11:43:17+00:00
- **Updated**: 2020-04-23 06:13:11+00:00
- **Authors**: Byungsoo Ko, Geonmo Gu
- **Comment**: Accepted by CVPR 2020
- **Journal**: None
- **Summary**: Learning the distance metric between pairs of samples has been studied for image retrieval and clustering. With the remarkable success of pair-based metric learning losses, recent works have proposed the use of generated synthetic points on metric learning losses for augmentation and generalization. However, these methods require additional generative networks along with the main network, which can lead to a larger model size, slower training speed, and harder optimization. Meanwhile, post-processing techniques, such as query expansion and database augmentation, have proposed the combination of feature points to obtain additional semantic information. In this paper, inspired by query expansion and database augmentation, we propose an augmentation method in an embedding space for pair-based metric learning losses, called embedding expansion. The proposed method generates synthetic points containing augmented information by a combination of feature points and performs hard negative pair mining to learn with the most informative feature representations. Because of its simplicity and flexibility, it can be used for existing metric learning losses without affecting model size, training speed, or optimization difficulty. Finally, the combination of embedding expansion and representative metric learning losses outperforms the state-of-the-art losses and previous sample generation methods in both image retrieval and clustering tasks. The implementation is publicly available.



### GANwriting: Content-Conditioned Generation of Styled Handwritten Word Images
- **Arxiv ID**: http://arxiv.org/abs/2003.02567v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.02567v2)
- **Published**: 2020-03-05 12:37:29+00:00
- **Updated**: 2020-07-21 19:40:15+00:00
- **Authors**: Lei Kang, Pau Riba, Yaxing Wang, Marçal Rusiñol, Alicia Fornés, Mauricio Villegas
- **Comment**: Accepted to ECCV2020
- **Journal**: None
- **Summary**: Although current image generation methods have reached impressive quality levels, they are still unable to produce plausible yet diverse images of handwritten words. On the contrary, when writing by hand, a great variability is observed across different writers, and even when analyzing words scribbled by the same individual, involuntary variations are conspicuous. In this work, we take a step closer to producing realistic and varied artificially rendered handwritten words. We propose a novel method that is able to produce credible handwritten word images by conditioning the generative process with both calligraphic style features and textual content. Our generator is guided by three complementary learning objectives: to produce realistic images, to imitate a certain handwriting style and to convey a specific textual content. Our model is unconstrained to any predefined vocabulary, being able to render whatever input word. Given a sample writer, it is also able to mimic its calligraphic features in a few-shot setup. We significantly advance over prior art and demonstrate with qualitative, quantitative and human-based evaluations the realistic aspect of our synthetically produced images.



### MarginDistillation: distillation for margin-based softmax
- **Arxiv ID**: http://arxiv.org/abs/2003.02586v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.02586v1)
- **Published**: 2020-03-05 13:03:23+00:00
- **Updated**: 2020-03-05 13:03:23+00:00
- **Authors**: David Svitov, Sergey Alyamkin
- **Comment**: None
- **Journal**: None
- **Summary**: The usage of convolutional neural networks (CNNs) in conjunction with a margin-based softmax approach demonstrates a state-of-the-art performance for the face recognition problem. Recently, lightweight neural network models trained with the margin-based softmax have been introduced for the face identification task for edge devices. In this paper, we propose a novel distillation method for lightweight neural network architectures that outperforms other known methods for the face recognition task on LFW, AgeDB-30 and Megaface datasets. The idea of the proposed method is to use class centers from the teacher network for the student network. Then the student network is trained to get the same angles between the class centers and the face embeddings, predicted by the teacher network.



### AI outperformed every dermatologist: Improved dermoscopic melanoma diagnosis through customizing batch logic and loss function in an optimized Deep CNN architecture
- **Arxiv ID**: http://arxiv.org/abs/2003.02597v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.02597v2)
- **Published**: 2020-03-05 13:19:13+00:00
- **Updated**: 2020-08-28 17:11:08+00:00
- **Authors**: Cong Tri Pham, Mai Chi Luong, Dung Van Hoang, Antoine Doucet
- **Comment**: We are submitting the article in the journal and waiting for the
  review result, so we want to temporarily delete the article. When the article
  is officially accepted, it will be resubmitted
- **Journal**: None
- **Summary**: Melanoma, one of most dangerous types of skin cancer, re-sults in a very high mortality rate. Early detection and resection are two key points for a successful cure. Recent research has used artificial intelligence to classify melanoma and nevus and to compare the assessment of these algorithms to that of dermatologists. However, an imbalance of sensitivity and specificity measures affected the performance of existing models. This study proposes a method using deep convolutional neural networks aiming to detect melanoma as a binary classification problem. It involves 3 key features, namely customized batch logic, customized loss function and reformed fully connected layers. The training dataset is kept up to date including 17,302 images of melanoma and nevus; this is the largest dataset by far. The model performance is compared to that of 157 dermatologists from 12 university hospitals in Germany based on MClass-D dataset. The model outperformed all 157 dermatologists and achieved state-of-the-art performance with AUC at 94.4% with sensitivity of 85.0% and specificity of 95.0% using a prediction threshold of 0.5 on the MClass-D dataset of 100 dermoscopic images. Moreover, a threshold of 0.40858 showed the most balanced measure compared to other researches, and is promisingly application to medical diagnosis, with sensitivity of 90.0% and specificity of 93.8%.



### Learning the sense of touch in simulation: a sim-to-real strategy for vision-based tactile sensing
- **Arxiv ID**: http://arxiv.org/abs/2003.02640v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.02640v1)
- **Published**: 2020-03-05 14:17:45+00:00
- **Updated**: 2020-03-05 14:17:45+00:00
- **Authors**: Carmelo Sferrazza, Thomas Bi, Raffaello D'Andrea
- **Comment**: This work has been submitted to the 2020 IEEE/RSJ International
  Conference on Intelligent Robots and Systems (IROS) for possible publication.
  Accompanying video: https://youtu.be/dDTga9PgWS0
- **Journal**: None
- **Summary**: Data-driven approaches to tactile sensing aim to overcome the complexity of accurately modeling contact with soft materials. However, their widespread adoption is impaired by concerns about data efficiency and the capability to generalize when applied to various tasks. This paper focuses on both these aspects with regard to a vision-based tactile sensor, which aims to reconstruct the distribution of the three-dimensional contact forces applied on its soft surface. Accurate models for the soft materials and the camera projection, derived via state-of-the-art techniques in the respective domains, are employed to generate a dataset in simulation. A strategy is proposed to train a tailored deep neural network entirely from the simulation data. The resulting learning architecture is directly transferable across multiple tactile sensors without further training and yields accurate predictions on real data, while showing promising generalization capabilities to unseen contact conditions.



### SketchyCOCO: Image Generation from Freehand Scene Sketches
- **Arxiv ID**: http://arxiv.org/abs/2003.02683v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.02683v5)
- **Published**: 2020-03-05 14:54:10+00:00
- **Updated**: 2020-04-07 10:15:39+00:00
- **Authors**: Chengying Gao, Qi Liu, Qi Xu, Limin Wang, Jianzhuang Liu, Changqing Zou
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce the first method for automatic image generation from scene-level freehand sketches. Our model allows for controllable image generation by specifying the synthesis goal via freehand sketches. The key contribution is an attribute vector bridged Generative Adversarial Network called EdgeGAN, which supports high visual-quality object-level image content generation without using freehand sketches as training data. We have built a large-scale composite dataset called SketchyCOCO to support and evaluate the solution. We validate our approach on the tasks of both object-level and scene-level image generation on SketchyCOCO. Through quantitative, qualitative results, human evaluation and ablation studies, we demonstrate the method's capacity to generate realistic complex scene-level images from various freehand sketches.



### Self-Supervised Visual Learning by Variable Playback Speeds Prediction of a Video
- **Arxiv ID**: http://arxiv.org/abs/2003.02692v2
- **DOI**: 10.1109/ACCESS.2021.3084840
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.02692v2)
- **Published**: 2020-03-05 15:01:08+00:00
- **Updated**: 2021-05-21 02:39:54+00:00
- **Authors**: Hyeon Cho, Taehoon Kim, Hyung Jin Chang, Wonjun Hwang
- **Comment**: Accepted by IEEE Access on May 19, 2021
- **Journal**: None
- **Summary**: We propose a self-supervised visual learning method by predicting the variable playback speeds of a video. Without semantic labels, we learn the spatio-temporal visual representation of the video by leveraging the variations in the visual appearance according to different playback speeds under the assumption of temporal coherence. To learn the spatio-temporal visual variations in the entire video, we have not only predicted a single playback speed but also generated clips of various playback speeds and directions with randomized starting points. Hence the visual representation can be successfully learned from the meta information (playback speeds and directions) of the video. We also propose a new layer dependable temporal group normalization method that can be applied to 3D convolutional networks to improve the representation learning performance where we divide the temporal features into several groups and normalize each one using the different corresponding parameters. We validate the effectiveness of our method by fine-tuning it to the action recognition and video retrieval tasks on UCF-101 and HMDB-51.



### Unique Class Group Based Multi-Label Balancing Optimizer for Action Unit Detection
- **Arxiv ID**: http://arxiv.org/abs/2003.08751v1
- **DOI**: 10.1109/FG47880.2020.00101
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.08751v1)
- **Published**: 2020-03-05 15:34:46+00:00
- **Updated**: 2020-03-05 15:34:46+00:00
- **Authors**: Ines Rieger, Jaspar Pahl, Dominik Seuss
- **Comment**: Accepted at the 15th IEEE International Conference on Automatic Face
  and Gesture Recognition 2020, Workshop "Affect Recognition in-the-wild:
  Uni/Multi-Modal Analysis & VA-AU-Expression Challenges". arXiv admin note:
  substantial text overlap with arXiv:2002.03238
- **Journal**: Proceedings of the 15th IEEE International Conference on Automatic
  Face and Gesture Recognition (FG 2020) 619-623
- **Summary**: Balancing methods for single-label data cannot be applied to multi-label problems as they would also resample the samples with high occurrences. We propose to reformulate this problem as an optimization problem in order to balance multi-label data. We apply this balancing algorithm to training datasets for detecting isolated facial movements, so-called Action Units. Several Action Units can describe combined emotions or physical states such as pain. As datasets in this area are limited and mostly imbalanced, we show how optimized balancing and then augmentation can improve Action Unit detection. At the IEEE Conference on Face and Gesture Recognition 2020, we ranked third in the Affective Behavior Analysis in-the-wild (ABAW) challenge for the Action Unit detection task.



### Search Space of Adversarial Perturbations against Image Filters
- **Arxiv ID**: http://arxiv.org/abs/2003.02750v1
- **DOI**: 10.14569/IJACSA.2020.0110102
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.02750v1)
- **Published**: 2020-03-05 16:40:06+00:00
- **Updated**: 2020-03-05 16:40:06+00:00
- **Authors**: Dang Duy Thang, Toshihiro Matsui
- **Comment**: None
- **Journal**: Published in International Journal of Advanced Computer Science
  and Applications(IJACSA), Volume 11 Issue 1, 2020
- **Summary**: The superiority of deep learning performance is threatened by safety issues for itself. Recent findings have shown that deep learning systems are very weak to adversarial examples, an attack form that was altered by the attacker's intent to deceive the deep learning system. There are many proposed defensive methods to protect deep learning systems against adversarial examples. However, there is still a lack of principal strategies to deceive those defensive methods. Any time a particular countermeasure is proposed, a new powerful adversarial attack will be invented to deceive that countermeasure. In this study, we focus on investigating the ability to create adversarial patterns in search space against defensive methods that use image filters. Experimental results conducted on the ImageNet dataset with image classification tasks showed the correlation between the search space of adversarial perturbation and filters. These findings open a new direction for building stronger offensive methods towards deep learning systems.



### Combating noisy labels by agreement: A joint training method with co-regularization
- **Arxiv ID**: http://arxiv.org/abs/2003.02752v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.02752v3)
- **Published**: 2020-03-05 16:42:41+00:00
- **Updated**: 2020-04-22 17:06:32+00:00
- **Authors**: Hongxin Wei, Lei Feng, Xiangyu Chen, Bo An
- **Comment**: Accepted by CVPR 2020; Code is available at:
  https://github.com/hongxin001/JoCoR. arXiv admin note: text overlap with
  arXiv:1901.04215 by other authors
- **Journal**: None
- **Summary**: Deep Learning with noisy labels is a practically challenging problem in weakly supervised learning. The state-of-the-art approaches "Decoupling" and "Co-teaching+" claim that the "disagreement" strategy is crucial for alleviating the problem of learning with noisy labels. In this paper, we start from a different perspective and propose a robust learning paradigm called JoCoR, which aims to reduce the diversity of two networks during training. Specifically, we first use two networks to make predictions on the same mini-batch data and calculate a joint loss with Co-Regularization for each training example. Then we select small-loss examples to update the parameters of both two networks simultaneously. Trained by the joint loss, these two networks would be more and more similar due to the effect of Co-Regularization. Extensive experimental results on corrupted data from benchmark datasets including MNIST, CIFAR-10, CIFAR-100 and Clothing1M demonstrate that JoCoR is superior to many state-of-the-art approaches for learning with noisy labels.



### Event-Based Angular Velocity Regression with Spiking Networks
- **Arxiv ID**: http://arxiv.org/abs/2003.02790v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2003.02790v1)
- **Published**: 2020-03-05 17:37:16+00:00
- **Updated**: 2020-03-05 17:37:16+00:00
- **Authors**: Mathias Gehrig, Sumit Bam Shrestha, Daniel Mouritzen, Davide Scaramuzza
- **Comment**: None
- **Journal**: IEEE International Conference on Robotics and Automation (ICRA),
  Paris, 2020
- **Summary**: Spiking Neural Networks (SNNs) are bio-inspired networks that process information conveyed as temporal spikes rather than numeric values. A spiking neuron of an SNN only produces a spike whenever a significant number of spikes occur within a short period of time. Due to their spike-based computational model, SNNs can process output from event-based, asynchronous sensors without any pre-processing at extremely lower power unlike standard artificial neural networks. This is possible due to specialized neuromorphic hardware that implements the highly-parallelizable concept of SNNs in silicon. Yet, SNNs have not enjoyed the same rise of popularity as artificial neural networks. This not only stems from the fact that their input format is rather unconventional but also due to the challenges in training spiking networks. Despite their temporal nature and recent algorithmic advances, they have been mostly evaluated on classification problems. We propose, for the first time, a temporal regression problem of numerical values given events from an event camera. We specifically investigate the prediction of the 3-DOF angular velocity of a rotating event camera with an SNN. The difficulty of this problem arises from the prediction of angular velocities continuously in time directly from irregular, asynchronous event-based input. Directly utilising the output of event cameras without any pre-processing ensures that we inherit all the benefits that they provide over conventional cameras. That is high-temporal resolution, high-dynamic range and no motion blur. To assess the performance of SNNs on this task, we introduce a synthetic event camera dataset generated from real-world panoramic images and show that we can successfully train an SNN to perform angular velocity regression.



### A Neuro-AI Interface for Evaluating Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2003.03193v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.03193v2)
- **Published**: 2020-03-05 17:53:43+00:00
- **Updated**: 2020-04-06 10:42:02+00:00
- **Authors**: Zhengwei Wang, Qi She, Alan F. Smeaton, Tomas E. Ward, Graham Healy
- **Comment**: Accepted by ICLR 2020 Workshop Bridging AI and Cognitive Science
  (BAICS). arXiv admin note: substantial text overlap with arXiv:1905.04243
- **Journal**: None
- **Summary**: Generative adversarial networks (GANs) are increasingly attracting attention in the computer vision, natural language processing, speech synthesis and similar domains. However, evaluating the performance of GANs is still an open and challenging problem. Existing evaluation metrics primarily measure the dissimilarity between real and generated images using automated statistical methods. They often require large sample sizes for evaluation and do not directly reflect human perception of image quality. In this work, we introduce an evaluation metric called Neuroscore, for evaluating the performance of GANs, that more directly reflects psychoperceptual image quality through the utilization of brain signals. Our results show that Neuroscore has superior performance to the current evaluation metrics in that: (1) It is more consistent with human judgment; (2) The evaluation process needs much smaller numbers of samples; and (3) It is able to rank the quality of images on a per GAN basis. A convolutional neural network (CNN) based neuro-AI interface is proposed to predict Neuroscore from GAN-generated images directly without the need for neural responses. Importantly, we show that including neural responses during the training phase of the network can significantly improve the prediction capability of the proposed model. Codes and data can be referred at this link: https://github.com/villawang/Neuro-AI-Interface.



### Generalizable semi-supervised learning method to estimate mass from sparsely annotated images
- **Arxiv ID**: http://arxiv.org/abs/2003.03192v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.03192v1)
- **Published**: 2020-03-05 18:13:07+00:00
- **Updated**: 2020-03-05 18:13:07+00:00
- **Authors**: Muhammad K. A. Hamdan, Diane T. Rover, Matthew J. Darr, John Just
- **Comment**: 22 pages, 21 figures, Computers and Electronics in Agriculture. arXiv
  admin note: text overlap with arXiv:1908.04387
- **Journal**: None
- **Summary**: Mass flow estimation is of great importance to several industries, and it can be quite challenging to obtain accurate estimates due to limitation in expense or general infeasibility. In the context of agricultural applications, yield monitoring is a key component to precision agriculture and mass flow is the critical factor to measure. Measuring mass flow allows for field productivity analysis, cost minimization, and adjustments to machine efficiency. Methods such as volume or force-impact have been used to measure mass flow; however, these methods are limited in application and accuracy. In this work, we use deep learning to develop and test a vision system that can accurately estimate the mass of sugarcane while running in real-time on a sugarcane harvester during operation. The deep learning algorithm that is used to estimate mass flow is trained using very sparsely annotated images (semi-supervised) using only final load weights (aggregated weights over a certain period of time). The deep neural network (DNN) succeeds in capturing the mass of sugarcane accurately and surpasses older volumetric-based methods, despite highly varying lighting and material colors in the images. The deep neural network is initially trained to predict mass on laboratory data (bamboo) and then transfer learning is utilized to apply the same methods to estimate mass of sugarcane. Using a vision system with a relatively lightweight deep neural network we are able to estimate mass of bamboo with an average error of 4.5% and 5.9% for a select season of sugarcane.



### Feature Extraction for Hyperspectral Imagery: The Evolution from Shallow to Deep (Overview and Toolbox)
- **Arxiv ID**: http://arxiv.org/abs/2003.02822v4
- **DOI**: 10.1109/MGRS.2020.2979764
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.02822v4)
- **Published**: 2020-03-05 18:45:22+00:00
- **Updated**: 2020-07-29 20:38:54+00:00
- **Authors**: Behnood Rasti, Danfeng Hong, Renlong Hang, Pedram Ghamisi, Xudong Kang, Jocelyn Chanussot, Jon Atli Benediktsson
- **Comment**: None
- **Journal**: IEEE Geoscience and Remote Sensing Magazine, 2020
- **Summary**: Hyperspectral images provide detailed spectral information through hundreds of (narrow) spectral channels (also known as dimensionality or bands) with continuous spectral information that can accurately classify diverse materials of interest. The increased dimensionality of such data makes it possible to significantly improve data information content but provides a challenge to the conventional techniques (the so-called curse of dimensionality) for accurate analysis of hyperspectral images. Feature extraction, as a vibrant field of research in the hyperspectral community, evolved through decades of research to address this issue and extract informative features suitable for data representation and classification. The advances in feature extraction have been inspired by two fields of research, including the popularization of image and signal processing as well as machine (deep) learning, leading to two types of feature extraction approaches named shallow and deep techniques. This article outlines the advances in feature extraction approaches for hyperspectral imagery by providing a technical overview of the state-of-the-art techniques, providing useful entry points for researchers at different levels, including students, researchers, and senior researchers, willing to explore novel investigations on this challenging topic. In more detail, this paper provides a bird's eye view over shallow (both supervised and unsupervised) and deep feature extraction approaches specifically dedicated to the topic of hyperspectral feature extraction and its application on hyperspectral image classification. Additionally, this paper compares 15 advanced techniques with an emphasis on their methodological foundations in terms of classification accuracies. Furthermore, the codes and libraries are shared at https://github.com/BehnoodRasti/HyFTech-Hyperspectral-Shallow-Deep-Feature-Extraction-Toolbox.



### Action Segmentation with Joint Self-Supervised Temporal Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2003.02824v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.02824v3)
- **Published**: 2020-03-05 18:52:33+00:00
- **Updated**: 2020-03-18 21:09:07+00:00
- **Authors**: Min-Hung Chen, Baopu Li, Yingze Bao, Ghassan AlRegib, Zsolt Kira
- **Comment**: CVPR 2020. Source code: https://github.com/cmhungsteve/SSTDA
- **Journal**: None
- **Summary**: Despite the recent progress of fully-supervised action segmentation techniques, the performance is still not fully satisfactory. One main challenge is the problem of spatiotemporal variations (e.g. different people may perform the same activity in various ways). Therefore, we exploit unlabeled videos to address this problem by reformulating the action segmentation task as a cross-domain problem with domain discrepancy caused by spatio-temporal variations. To reduce the discrepancy, we propose Self-Supervised Temporal Domain Adaptation (SSTDA), which contains two self-supervised auxiliary tasks (binary and sequential domain prediction) to jointly align cross-domain feature spaces embedded with local and global temporal dynamics, achieving better performance than other Domain Adaptation (DA) approaches. On three challenging benchmark datasets (GTEA, 50Salads, and Breakfast), SSTDA outperforms the current state-of-the-art method by large margins (e.g. for the F1@25 score, from 59.6% to 69.1% on Breakfast, from 73.4% to 81.5% on 50Salads, and from 83.6% to 89.1% on GTEA), and requires only 65% of the labeled training data for comparable performance, demonstrating the usefulness of adapting to unlabeled target videos across variations. The source code is available at https://github.com/cmhungsteve/SSTDA.



### Optimizing JPEG Quantization for Classification Networks
- **Arxiv ID**: http://arxiv.org/abs/2003.02874v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.PF, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.02874v1)
- **Published**: 2020-03-05 19:13:06+00:00
- **Updated**: 2020-03-05 19:13:06+00:00
- **Authors**: Zhijing Li, Christopher De Sa, Adrian Sampson
- **Comment**: 6 pages, 13 figures, Resource-Constrained Machine Learning (ReCoML)
  Workshop of MLSys 2020 Conference, Austin, TX, USA, 2020
- **Journal**: None
- **Summary**: Deep learning for computer vision depends on lossy image compression: it reduces the storage required for training and test data and lowers transfer costs in deployment. Mainstream datasets and imaging pipelines all rely on standard JPEG compression. In JPEG, the degree of quantization of frequency coefficients controls the lossiness: an 8 by 8 quantization table (Q-table) decides both the quality of the encoded image and the compression ratio. While a long history of work has sought better Q-tables, existing work either seeks to minimize image distortion or to optimize for models of the human visual system. This work asks whether JPEG Q-tables exist that are "better" for specific vision networks and can offer better quality--size trade-offs than ones designed for human perception or minimal distortion. We reconstruct an ImageNet test set with higher resolution to explore the effect of JPEG compression under novel Q-tables. We attempt several approaches to tune a Q-table for a vision task. We find that a simple sorted random sampling method can exceed the performance of the standard JPEG Q-table. We also use hyper-parameter tuning techniques including bounded random search, Bayesian optimization, and composite heuristic optimization methods. The new Q-tables we obtained can improve the compression rate by 10% to 200% when the accuracy is fixed, or improve accuracy up to $2\%$ at the same compression rate.



### Segmentation of Satellite Imagery using U-Net Models for Land Cover Classification
- **Arxiv ID**: http://arxiv.org/abs/2003.02899v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2003.02899v1)
- **Published**: 2020-03-05 20:07:48+00:00
- **Updated**: 2020-03-05 20:07:48+00:00
- **Authors**: Priit Ulmas, Innar Liiv
- **Comment**: Submitted to IEEE Access; 11 pages, 6 figures
- **Journal**: None
- **Summary**: The focus of this paper is using a convolutional machine learning model with a modified U-Net structure for creating land cover classification mapping based on satellite imagery. The aim of the research is to train and test convolutional models for automatic land cover mapping and to assess their usability in increasing land cover mapping accuracy and change detection. To solve these tasks, authors prepared a dataset and trained machine learning models for land cover classification and semantic segmentation from satellite images. The results were analysed on three different land classification levels. BigEarthNet satellite image archive was selected for the research as one of two main datasets. This novel and recent dataset was published in 2019 and includes Sentinel-2 satellite photos from 10 European countries made in 2017 and 2018. As a second dataset the authors composed an original set containing a Sentinel-2 image and a CORINE land cover map of Estonia. The developed classification model shows a high overall F\textsubscript{1} score of 0.749 on multiclass land cover classification with 43 possible image labels. The model also highlights noisy data in the BigEarthNet dataset, where images seem to have incorrect labels. The segmentation models offer a solution for generating automatic land cover mappings based on Sentinel-2 satellite images and show a high IoU score for land cover classes such as forests, inland waters and arable land. The models show a capability of increasing the accuracy of existing land classification maps and in land cover change detection.



### Longevity Associated Geometry Identified in Satellite Images: Sidewalks, Driveways and Hiking Trails
- **Arxiv ID**: http://arxiv.org/abs/2003.08750v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.08750v1)
- **Published**: 2020-03-05 20:23:11+00:00
- **Updated**: 2020-03-05 20:23:11+00:00
- **Authors**: Joshua J. Levy, Rebecca M. Lebeaux, Anne G. Hoen, Brock C. Christensen, Louis J. Vaickus, Todd A. MacKenzie
- **Comment**: None
- **Journal**: None
- **Summary**: Importance: Following a century of increase, life expectancy in the United States has stagnated and begun to decline in recent decades. Using satellite images and street view images prior work has demonstrated associations of the built environment with income, education, access to care and health factors such as obesity. However, assessment of learned image feature relationships with variation in crude mortality rate across the United States has been lacking.   Objective: Investigate prediction of county-level mortality rates in the U.S. using satellite images.   Design: Satellite images were extracted with the Google Static Maps application programming interface for 430 counties representing approximately 68.9% of the US population. A convolutional neural network was trained using crude mortality rates for each county in 2015 to predict mortality. Learned image features were interpreted using Shapley Additive Feature Explanations, clustered, and compared to mortality and its associated covariate predictors.   Main Outcomes and Measures: County mortality was predicted using satellite images.   Results: Predicted mortality from satellite images in a held-out test set of counties was strongly correlated to the true crude mortality rate (Pearson r=0.72). Learned image features were clustered, and we identified 10 clusters that were associated with education, income, geographical region, race and age.   Conclusion and Relevance: The application of deep learning techniques to remotely-sensed features of the built environment can serve as a useful predictor of mortality in the United States. Tools that are able to identify image features associated with health-related outcomes can inform targeted public health interventions.



### Generating Embroidery Patterns Using Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2003.02909v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.02909v1)
- **Published**: 2020-03-05 20:32:40+00:00
- **Updated**: 2020-03-05 20:32:40+00:00
- **Authors**: Mohammad Akif Beg, Jia Yuan Yu
- **Comment**: None
- **Journal**: None
- **Summary**: In many scenarios in computer vision, machine learning, and computer graphics, there is a requirement to learn the mapping from an image of one domain to an image of another domain, called Image-to-image translation. For example, style transfer, object transfiguration, visually altering the appearance of weather conditions in an image, changing the appearance of a day image into a night image or vice versa, photo enhancement, to name a few. In this paper, we propose two machine learning techniques to solve the embroidery image-to-image translation. Our goal is to generate a preview image which looks similar to an embroidered image, from a user-uploaded image. Our techniques are modifications of two existing techniques, neural style transfer, and cycle-consistent generative-adversarial network. Neural style transfer renders the semantic content of an image from one domain in the style of a different image in another domain, whereas a cycle-consistent generative adversarial network learns the mapping from an input image to output image without any paired training data, and also learn a loss function to train this mapping. Furthermore, the techniques we propose are independent of any embroidery attributes, such as elevation of the image, light-source, start, and endpoints of a stitch, type of stitch used, fabric type, etc. Given the user image, our techniques can generate a preview image which looks similar to an embroidered image. We train and test our propose techniques on an embroidery dataset which consist of simple 2D images. To do so, we prepare an unpaired embroidery dataset with more than 8000 user-uploaded images along with embroidered images. Empirical results show that these techniques successfully generate an approximate preview of an embroidered version of a user image, which can help users in decision making.



### A deep learning-facilitated radiomics solution for the prediction of lung lesion shrinkage in non-small cell lung cancer trials
- **Arxiv ID**: http://arxiv.org/abs/2003.02943v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.02943v1)
- **Published**: 2020-03-05 21:49:42+00:00
- **Updated**: 2020-03-05 21:49:42+00:00
- **Authors**: Antong Chen, Jennifer Saouaf, Bo Zhou, Randolph Crawford, Jianda Yuan, Junshui Ma, Richard Baumgartner, Shubing Wang, Gregory Goldmacher
- **Comment**: Accepted by International Symposium on Biomedical Imaging (ISBI) 2020
- **Journal**: None
- **Summary**: Herein we propose a deep learning-based approach for the prediction of lung lesion response based on radiomic features extracted from clinical CT scans of patients in non-small cell lung cancer trials. The approach starts with the classification of lung lesions from the set of primary and metastatic lesions at various anatomic locations. Focusing on the lung lesions, we perform automatic segmentation to extract their 3D volumes. Radiomic features are then extracted from the lesion on the pre-treatment scan and the first follow-up scan to predict which lesions will shrink at least 30% in diameter during treatment (either Pembrolizumab or combinations of chemotherapy and Pembrolizumab), which is defined as a partial response by the Response Evaluation Criteria In Solid Tumors (RECIST) guidelines. A 5-fold cross validation on the training set led to an AUC of 0.84 +/- 0.03, and the prediction on the testing dataset reached AUC of 0.73 +/- 0.02 for the outcome of 30% diameter shrinkage.



### Metric-Scale Truncation-Robust Heatmaps for 3D Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2003.02953v1
- **DOI**: 10.1109/FG47880.2020.00108
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.02953v1)
- **Published**: 2020-03-05 22:38:13+00:00
- **Updated**: 2020-03-05 22:38:13+00:00
- **Authors**: István Sárándi, Timm Linder, Kai O. Arras, Bastian Leibe
- **Comment**: Accepted for publication at the 2020 IEEE Conference on Automatic
  Face and Gesture Recognition (FG)
- **Journal**: None
- **Summary**: Heatmap representations have formed the basis of 2D human pose estimation systems for many years, but their generalizations for 3D pose have only recently been considered. This includes 2.5D volumetric heatmaps, whose X and Y axes correspond to image space and the Z axis to metric depth around the subject. To obtain metric-scale predictions, these methods must include a separate, explicit post-processing step to resolve scale ambiguity. Further, they cannot encode body joint positions outside of the image boundaries, leading to incomplete pose estimates in case of image truncation. We address these limitations by proposing metric-scale truncation-robust (MeTRo) volumetric heatmaps, whose dimensions are defined in metric 3D space near the subject, instead of being aligned with image space. We train a fully-convolutional network to estimate such heatmaps from monocular RGB in an end-to-end manner. This reinterpretation of the heatmap dimensions allows us to estimate complete metric-scale poses without test-time knowledge of the focal length or person distance and without relying on anthropometric heuristics in post-processing. Furthermore, as the image space is decoupled from the heatmap space, the network can learn to reason about joints beyond the image boundary. Using ResNet-50 without any additional learned layers, we obtain state-of-the-art results on the Human3.6M and MPI-INF-3DHP benchmarks. As our method is simple and fast, it can become a useful component for real-time top-down multi-person pose estimation systems. We make our code publicly available to facilitate further research (see https://vision.rwth-aachen.de/metro-pose3d).



### From Perspective X-ray Imaging to Parallax-Robust Orthographic Stitching
- **Arxiv ID**: http://arxiv.org/abs/2003.02959v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.02959v1)
- **Published**: 2020-03-05 23:16:48+00:00
- **Updated**: 2020-03-05 23:16:48+00:00
- **Authors**: Javad Fotouhi, Xingtong Liu, Mehran Armand, Nassir Navab, Mathias Unberath
- **Comment**: None
- **Journal**: None
- **Summary**: Stitching images acquired under perspective projective geometry is a relevant topic in computer vision with multiple applications ranging from smartphone panoramas to the construction of digital maps. Image stitching is an equally prominent challenge in medical imaging, where the limited field-of-view captured by single images prohibits holistic analysis of patient anatomy. The barrier that prevents straight-forward mosaicing of 2D images is depth mismatch due to parallax. In this work, we leverage the Fourier slice theorem to aggregate information from multiple transmission images in parallax-free domains using fundamental principles of X-ray image formation. The semantics of the stitched image are restored using a novel deep learning strategy that exploits similarity measures designed around frequency, as well as dense and sparse spatial image content. Our pipeline, not only stitches images, but also provides orthographic reconstruction that enables metric measurements of clinically relevant quantities directly on the 2D image plane.



### Forgetting Outside the Box: Scrubbing Deep Networks of Information Accessible from Input-Output Observations
- **Arxiv ID**: http://arxiv.org/abs/2003.02960v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.IT, math.IT, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.02960v3)
- **Published**: 2020-03-05 23:17:35+00:00
- **Updated**: 2020-10-29 02:23:28+00:00
- **Authors**: Aditya Golatkar, Alessandro Achille, Stefano Soatto
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: We describe a procedure for removing dependency on a cohort of training data from a trained deep network that improves upon and generalizes previous methods to different readout functions and can be extended to ensure forgetting in the activations of the network. We introduce a new bound on how much information can be extracted per query about the forgotten cohort from a black-box network for which only the input-output behavior is observed. The proposed forgetting procedure has a deterministic part derived from the differential equations of a linearized version of the model, and a stochastic part that ensures information destruction by adding noise tailored to the geometry of the loss landscape. We exploit the connections between the activation and weight dynamics of a DNN inspired by Neural Tangent Kernels to compute the information in the activations.



