# Arxiv Papers in cs.CV on 2020-03-02
### PSF--NET: A Non-parametric Point Spread Function Model for Ground Based Optical Telescopes
- **Arxiv ID**: http://arxiv.org/abs/2003.00615v1
- **DOI**: 10.3847/1538-3881/ab7b79
- **Categories**: **astro-ph.IM**, astro-ph.SR, cs.CV, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/2003.00615v1)
- **Published**: 2020-03-02 00:17:25+00:00
- **Updated**: 2020-03-02 00:17:25+00:00
- **Authors**: Peng Jia, Xuebo Wu, Yi Huang, Bojun Cai, Dongmei Cai
- **Comment**: Accepted by AJ. The complete code can be downloaded at
  DOI:10.12149/101014
- **Journal**: None
- **Summary**: Ground based optical telescopes are seriously affected by atmospheric turbulence induced aberrations. Understanding properties of these aberrations is important both for instruments design and image restoration methods development. Because the point spread function can reflect performance of the whole optic system, it is appropriate to use the point spread function to describe atmospheric turbulence induced aberrations. Assuming point spread functions induced by the atmospheric turbulence with the same profile belong to the same manifold space, we propose a non-parametric point spread function -- PSF-NET. The PSF-NET has a cycle convolutional neural network structure and is a statistical representation of the manifold space of PSFs induced by the atmospheric turbulence with the same profile. Testing the PSF-NET with simulated and real observation data, we find that a well trained PSF--NET can restore any short exposure images blurred by atmospheric turbulence with the same profile. Besides, we further use the impulse response of the PSF-NET, which can be viewed as the statistical mean PSF, to analyze interpretation properties of the PSF-NET. We find that variations of statistical mean PSFs are caused by variations of the atmospheric turbulence profile: as the difference of the atmospheric turbulence profile increases, the difference between statistical mean PSFs also increases. The PSF-NET proposed in this paper provides a new way to analyze atmospheric turbulence induced aberrations, which would be benefit to develop new observation methods for ground based optical telescopes.



### Extremely Dense Point Correspondences using a Learned Feature Descriptor
- **Arxiv ID**: http://arxiv.org/abs/2003.00619v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.00619v2)
- **Published**: 2020-03-02 00:44:04+00:00
- **Updated**: 2020-03-27 17:52:40+00:00
- **Authors**: Xingtong Liu, Yiping Zheng, Benjamin Killeen, Masaru Ishii, Gregory D. Hager, Russell H. Taylor, Mathias Unberath
- **Comment**: The work has been accepted for publication in CVPR 2020
- **Journal**: None
- **Summary**: High-quality 3D reconstructions from endoscopy video play an important role in many clinical applications, including surgical navigation where they enable direct video-CT registration. While many methods exist for general multi-view 3D reconstruction, these methods often fail to deliver satisfactory performance on endoscopic video. Part of the reason is that local descriptors that establish pair-wise point correspondences, and thus drive reconstruction, struggle when confronted with the texture-scarce surface of anatomy. Learning-based dense descriptors usually have larger receptive fields enabling the encoding of global information, which can be used to disambiguate matches. In this work, we present an effective self-supervised training scheme and novel loss design for dense descriptor learning. In direct comparison to recent local and dense descriptors on an in-house sinus endoscopy dataset, we demonstrate that our proposed dense descriptor can generalize to unseen patients and scopes, thereby largely improving the performance of Structure from Motion (SfM) in terms of model density and completeness. We also evaluate our method on a public dense optical flow dataset and a small-scale SfM public dataset to further demonstrate the effectiveness and generality of our method. The source code is available at https://github.com/lppllppl920/DenseDescriptorLearning-Pytorch.



### Unsupervised Domain Adaptation for Mammogram Image Classification: A Promising Tool for Model Generalization
- **Arxiv ID**: http://arxiv.org/abs/2003.01111v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2003.01111v1)
- **Published**: 2020-03-02 02:42:43+00:00
- **Updated**: 2020-03-02 02:42:43+00:00
- **Authors**: Yu Zhang, Gongbo Liang, Nathan Jacobs, Xiaoqin Wang
- **Comment**: Accepted by C-MIMI 2019 as a scientific abstract
- **Journal**: None
- **Summary**: Generalization is one of the key challenges in the clinical validation and application of deep learning models to medical images. Studies have shown that such models trained on publicly available datasets often do not work well on real-world clinical data due to the differences in patient population and image device configurations. Also, manually annotating clinical images is expensive. In this work, we propose an unsupervised domain adaptation (UDA) method using Cycle-GAN to improve the generalization ability of the model without using any additional manual annotations.



### Matching Neuromorphic Events and Color Images via Adversarial Learning
- **Arxiv ID**: http://arxiv.org/abs/2003.00636v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.00636v1)
- **Published**: 2020-03-02 02:48:56+00:00
- **Updated**: 2020-03-02 02:48:56+00:00
- **Authors**: Fang Xu, Shijie Lin, Wen Yang, Lei Yu, Dengxin Dai, Gui-song Xia
- **Comment**: None
- **Journal**: None
- **Summary**: The event camera has appealing properties: high dynamic range, low latency, low power consumption and low memory usage, and thus provides complementariness to conventional frame-based cameras. It only captures the dynamics of a scene and is able to capture almost "continuous" motion. However, different from frame-based camera that reflects the whole appearance as scenes are, the event camera casts away the detailed characteristics of objects, such as texture and color. To take advantages of both modalities, the event camera and frame-based camera are combined together for various machine vision tasks. Then the cross-modal matching between neuromorphic events and color images plays a vital and essential role. In this paper, we propose the Event-Based Image Retrieval (EBIR) problem to exploit the cross-modal matching task. Given an event stream depicting a particular object as query, the aim is to retrieve color images containing the same object. This problem is challenging because there exists a large modality gap between neuromorphic events and color images. We address the EBIR problem by proposing neuromorphic Events-Color image Feature Learning (ECFL). Particularly, the adversarial learning is employed to jointly model neuromorphic events and color images into a common embedding space. We also contribute to the community N-UKbench and EC180 dataset to promote the development of EBIR problem. Extensive experiments on our datasets show that the proposed method is superior in learning effective modality-invariant representation to link two different modalities.



### A Novel Recurrent Encoder-Decoder Structure for Large-Scale Multi-view Stereo Reconstruction from An Open Aerial Dataset
- **Arxiv ID**: http://arxiv.org/abs/2003.00637v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.00637v3)
- **Published**: 2020-03-02 03:04:13+00:00
- **Updated**: 2020-03-16 04:27:33+00:00
- **Authors**: Jin Liu, Shunping Ji
- **Comment**: None
- **Journal**: None
- **Summary**: A great deal of research has demonstrated recently that multi-view stereo (MVS) matching can be solved with deep learning methods. However, these efforts were focused on close-range objects and only a very few of the deep learning-based methods were specifically designed for large-scale 3D urban reconstruction due to the lack of multi-view aerial image benchmarks. In this paper, we present a synthetic aerial dataset, called the WHU dataset, we created for MVS tasks, which, to our knowledge, is the first large-scale multi-view aerial dataset. It was generated from a highly accurate 3D digital surface model produced from thousands of real aerial images with precise camera parameters. We also introduce in this paper a novel network, called RED-Net, for wide-range depth inference, which we developed from a recurrent encoder-decoder structure to regularize cost maps across depths and a 2D fully convolutional network as framework. RED-Net's low memory requirements and high performance make it suitable for large-scale and highly accurate 3D Earth surface reconstruction. Our experiments confirmed that not only did our method exceed the current state-of-the-art MVS methods by more than 50% mean absolute error (MAE) with less memory and computational cost, but its efficiency as well. It outperformed one of the best commercial software programs based on conventional methods, improving their efficiency 16 times over. Moreover, we proved that our RED-Net model pre-trained on the synthetic WHU dataset can be efficiently transferred to very different multi-view aerial image datasets without any fine-tuning. Dataset are available at http://gpcv.whu.edu.cn/data.



### VAE/WGAN-Based Image Representation Learning For Pose-Preserving Seamless Identity Replacement In Facial Images
- **Arxiv ID**: http://arxiv.org/abs/2003.00641v1
- **DOI**: 10.1109/MLSP.2019.8918926
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.00641v1)
- **Published**: 2020-03-02 03:35:59+00:00
- **Updated**: 2020-03-02 03:35:59+00:00
- **Authors**: Hiroki Kawai, Jiawei Chen, Prakash Ishwar, Janusz Konrad
- **Comment**: 6 pages, 5 figures, 2019 IEEE 29th International Workshop on Machine
  Learning for Signal Processing (MLSP)
- **Journal**: 2019 IEEE 29th International Workshop on Machine Learning for
  Signal Processing (MLSP)
- **Summary**: We present a novel variational generative adversarial network (VGAN) based on Wasserstein loss to learn a latent representation from a face image that is invariant to identity but preserves head-pose information. This facilitates synthesis of a realistic face image with the same head pose as a given input image, but with a different identity. One application of this network is in privacy-sensitive scenarios; after identity replacement in an image, utility, such as head pose, can still be recovered. Extensive experimental validation on synthetic and real human-face image datasets performed under 3 threat scenarios confirms the ability of the proposed network to preserve head pose of the input image, mask the input identity, and synthesize a good-quality realistic face image of a desired identity. We also show that our network can be used to perform pose-preserving identity morphing and identity-preserving pose morphing. The proposed method improves over a recent state-of-the-art method in terms of quantitative metrics as well as synthesized image quality.



### Global Context-Aware Progressive Aggregation Network for Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2003.00651v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.00651v1)
- **Published**: 2020-03-02 04:26:10+00:00
- **Updated**: 2020-03-02 04:26:10+00:00
- **Authors**: Zuyao Chen, Qianqian Xu, Runmin Cong, Qingming Huang
- **Comment**: None
- **Journal**: AAAI 2020
- **Summary**: Deep convolutional neural networks have achieved competitive performance in salient object detection, in which how to learn effective and comprehensive features plays a critical role. Most of the previous works mainly adopted multiple level feature integration yet ignored the gap between different features. Besides, there also exists a dilution process of high-level features as they passed on the top-down pathway. To remedy these issues, we propose a novel network named GCPANet to effectively integrate low-level appearance features, high-level semantic features, and global context features through some progressive context-aware Feature Interweaved Aggregation (FIA) modules and generate the saliency map in a supervised way. Moreover, a Head Attention (HA) module is used to reduce information redundancy and enhance the top layers features by leveraging the spatial and channel-wise attention, and the Self Refinement (SR) module is utilized to further refine and heighten the input features. Furthermore, we design the Global Context Flow (GCF) module to generate the global context information at different stages, which aims to learn the relationship among different salient regions and alleviate the dilution effect of high-level features. Experimental results on six benchmark datasets demonstrate that the proposed approach outperforms the state-of-the-art methods both quantitatively and qualitatively.



### MVP: Unified Motion and Visual Self-Supervised Learning for Large-Scale Robotic Navigation
- **Arxiv ID**: http://arxiv.org/abs/2003.00667v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.00667v1)
- **Published**: 2020-03-02 05:19:52+00:00
- **Updated**: 2020-03-02 05:19:52+00:00
- **Authors**: Marvin Chancán, Michael Milford
- **Comment**: Under review at IROS 2020
- **Journal**: None
- **Summary**: Autonomous navigation emerges from both motion and local visual perception in real-world environments. However, most successful robotic motion estimation methods (e.g. VO, SLAM, SfM) and vision systems (e.g. CNN, visual place recognition-VPR) are often separately used for mapping and localization tasks. Conversely, recent reinforcement learning (RL) based methods for visual navigation rely on the quality of GPS data reception, which may not be reliable when directly using it as ground truth across multiple, month-spaced traversals in large environments. In this paper, we propose a novel motion and visual perception approach, dubbed MVP, that unifies these two sensor modalities for large-scale, target-driven navigation tasks. Our MVP-based method can learn faster, and is more accurate and robust to both extreme environmental changes and poor GPS data than corresponding vision-only navigation methods. MVP temporally incorporates compact image representations, obtained using VPR, with optimized motion estimation data, including but not limited to those from VO or optimized radar odometry (RO), to efficiently learn self-supervised navigation policies via RL. We evaluate our method on two large real-world datasets, Oxford Robotcar and Nordland Railway, over a range of weather (e.g. overcast, night, snow, sun, rain, clouds) and seasonal (e.g. winter, spring, fall, summer) conditions using the new CityLearn framework; an interactive environment for efficiently training navigation agents. Our experimental results, on traversals of the Oxford RobotCar dataset with no GPS data, show that MVP can achieve 53% and 93% navigation success rate using VO and RO, respectively, compared to 7% for a vision-only method. We additionally report a trade-off between the RL success rate and the motion estimation precision.



### IntrA: 3D Intracranial Aneurysm Dataset for Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2003.02920v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.02920v2)
- **Published**: 2020-03-02 05:21:53+00:00
- **Updated**: 2020-04-06 08:09:59+00:00
- **Authors**: Xi Yang, Ding Xia, Taichi Kin, Takeo Igarashi
- **Comment**: Accepted by cvpr2020, camera-ready version will be uploaded later
- **Journal**: None
- **Summary**: Medicine is an important application area for deep learning models. Research in this field is a combination of medical expertise and data science knowledge. In this paper, instead of 2D medical images, we introduce an open-access 3D intracranial aneurysm dataset, IntrA, that makes the application of points-based and mesh-based classification and segmentation models available. Our dataset can be used to diagnose intracranial aneurysms and to extract the neck for a clipping operation in medicine and other areas of deep learning, such as normal estimation and surface reconstruction. We provide a large-scale benchmark of classification and part segmentation by testing state-of-the-art networks. We also discuss the performance of each method and demonstrate the challenges of our dataset. The published dataset can be accessed here: https://github.com/intra3d2019/IntrA.



### SketchGNN: Semantic Sketch Segmentation with Graph Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2003.00678v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.00678v2)
- **Published**: 2020-03-02 05:48:55+00:00
- **Updated**: 2021-03-25 09:05:02+00:00
- **Authors**: Lumin Yang, Jiajie Zhuang, Hongbo Fu, Xiangzhi Wei, Kun Zhou, Youyi Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce SketchGNN, a convolutional graph neural network for semantic segmentation and labeling of freehand vector sketches. We treat an input stroke-based sketch as a graph, with nodes representing the sampled points along input strokes and edges encoding the stroke structure information. To predict the per-node labels, our SketchGNN uses graph convolution and a static-dynamic branching network architecture to extract the features at three levels, i.e., point-level, stroke-level, and sketch-level. SketchGNN significantly improves the accuracy of the state-of-the-art methods for semantic sketch segmentation (by 11.2% in the pixel-based metric and 18.2% in the component-based metric over a large-scale challenging SPG dataset) and has magnitudes fewer parameters than both image-based and sequence-based methods.



### Hybrid Deep Learning for Detecting Lung Diseases from X-ray Images
- **Arxiv ID**: http://arxiv.org/abs/2003.00682v3
- **DOI**: 10.1016/j.imu.2020.100391
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.00682v3)
- **Published**: 2020-03-02 06:07:30+00:00
- **Updated**: 2020-07-01 17:31:27+00:00
- **Authors**: Subrato Bharati, Prajoy Podder, M. Rubaiyat Hossain Mondal
- **Comment**: 13 figures
- **Journal**: Informatics in Medicine Unlocked, 2020
- **Summary**: Lung disease is common throughout the world. These include chronic obstructive pulmonary disease, pneumonia, asthma, tuberculosis, fibrosis, etc. Timely diagnosis of lung disease is essential. Many image processing and machine learning models have been developed for this purpose. Different forms of existing deep learning techniques including convolutional neural network (CNN), vanilla neural network, visual geometry group based neural network (VGG), and capsule network are applied for lung disease prediction.The basic CNN has poor performance for rotated, tilted, or other abnormal image orientation. Therefore, we propose a new hybrid deep learning framework by combining VGG, data augmentation and spatial transformer network (STN) with CNN. This new hybrid method is termed here as VGG Data STN with CNN (VDSNet). As implementation tools, Jupyter Notebook, Tensorflow, and Keras are used. The new model is applied to NIH chest X-ray image dataset collected from Kaggle repository. Full and sample versions of the dataset are considered. For both full and sample datasets, VDSNet outperforms existing methods in terms of a number of metrics including precision, recall, F0.5 score and validation accuracy. For the case of full dataset, VDSNet exhibits a validation accuracy of 73%, while vanilla gray, vanilla RGB, hybrid CNN and VGG, and modified capsule network have accuracy values of 67.8%, 69%, 69.5%, 60.5% and 63.8%, respectively. When sample dataset rather than full dataset is used, VDSNet requires much lower training time at the expense of a slightly lower validation accuracy. Hence, the proposed VDSNet framework will simplify the detection of lung disease for experts as well as for doctors.



### Deep Image Spatial Transformation for Person Image Generation
- **Arxiv ID**: http://arxiv.org/abs/2003.00696v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2003.00696v2)
- **Published**: 2020-03-02 07:31:00+00:00
- **Updated**: 2020-03-18 09:42:02+00:00
- **Authors**: Yurui Ren, Xiaoming Yu, Junming Chen, Thomas H. Li, Ge Li
- **Comment**: None
- **Journal**: None
- **Summary**: Pose-guided person image generation is to transform a source person image to a target pose. This task requires spatial manipulations of source data. However, Convolutional Neural Networks are limited by the lack of ability to spatially transform the inputs. In this paper, we propose a differentiable global-flow local-attention framework to reassemble the inputs at the feature level. Specifically, our model first calculates the global correlations between sources and targets to predict flow fields. Then, the flowed local patch pairs are extracted from the feature maps to calculate the local attention coefficients. Finally, we warp the source features using a content-aware sampling method with the obtained local attention coefficients. The results of both subjective and objective experiments demonstrate the superiority of our model. Besides, additional results in video animation and view synthesis show that our model is applicable to other tasks requiring spatial transformation. Our source code is available at https://github.com/RenYurui/Global-Flow-Local-Attention.



### Relational Deep Feature Learning for Heterogeneous Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2003.00697v3
- **DOI**: 10.1109/TIFS.2020.3013186
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.00697v3)
- **Published**: 2020-03-02 07:35:23+00:00
- **Updated**: 2020-07-14 11:06:22+00:00
- **Authors**: MyeongAh Cho, Taeoh Kim, Ig-Jae Kim, Kyungjae Lee, Sangyoun Lee
- **Comment**: None
- **Journal**: IEEE Transactions on Information Forensics and Security, vol. 16,
  pp. 376-388, 2021
- **Summary**: Heterogeneous Face Recognition (HFR) is a task that matches faces across two different domains such as visible light (VIS), near-infrared (NIR), or the sketch domain. Due to the lack of databases, HFR methods usually exploit the pre-trained features on a large-scale visual database that contain general facial information. However, these pre-trained features cause performance degradation due to the texture discrepancy with the visual domain. With this motivation, we propose a graph-structured module called Relational Graph Module (RGM) that extracts global relational information in addition to general facial features. Because each identity's relational information between intra-facial parts is similar in any modality, the modeling relationship between features can help cross-domain matching. Through the RGM, relation propagation diminishes texture dependency without losing its advantages from the pre-trained features. Furthermore, the RGM captures global facial geometrics from locally correlated convolutional features to identify long-range relationships. In addition, we propose a Node Attention Unit (NAU) that performs node-wise recalibration to concentrate on the more informative nodes arising from relation-based propagation. Furthermore, we suggest a novel conditional-margin loss function (C-softmax) for the efficient projection learning of the embedding vector in HFR. The proposed method outperforms other state-of-the-art methods on five HFR databases. Furthermore, we demonstrate performance improvement on three backbones because our module can be plugged into any pre-trained face recognition backbone to overcome the limitations of a small HFR database.



### GPU-Accelerated Mobile Multi-view Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/2003.00706v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.PF, I.4.0; I.4.8; I.4.9; I.3.3; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2003.00706v1)
- **Published**: 2020-03-02 08:20:47+00:00
- **Updated**: 2020-03-02 08:20:47+00:00
- **Authors**: Puneet Kohli, Saravana Gunaseelan, Jason Orozco, Yiwen Hua, Edward Li, Nicolas Dahlquist
- **Comment**: 6 pages, 5 figures
- **Journal**: None
- **Summary**: An estimated 60% of smartphones sold in 2018 were equipped with multiple rear cameras, enabling a wide variety of 3D-enabled applications such as 3D Photos. The success of 3D Photo platforms (Facebook 3D Photo, Holopix, etc) depend on a steady influx of user generated content. These platforms must provide simple image manipulation tools to facilitate content creation, akin to traditional photo platforms. Artistic neural style transfer, propelled by recent advancements in GPU technology, is one such tool for enhancing traditional photos. However, naively extrapolating single-view neural style transfer to the multi-view scenario produces visually inconsistent results and is prohibitively slow on mobile devices. We present a GPU-accelerated multi-view style transfer pipeline which enforces style consistency between views with on-demand performance on mobile platforms. Our pipeline is modular and creates high quality depth and parallax effects from a stereoscopic image pair.



### Unbiased Mean Teacher for Cross-domain Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2003.00707v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.00707v2)
- **Published**: 2020-03-02 08:20:55+00:00
- **Updated**: 2021-06-23 00:53:18+00:00
- **Authors**: Jinhong Deng, Wen Li, Yuhua Chen, Lixin Duan
- **Comment**: Accepted by CVPR2021
- **Journal**: None
- **Summary**: Cross-domain object detection is challenging, because object detection model is often vulnerable to data variance, especially to the considerable domain shift between two distinctive domains. In this paper, we propose a new Unbiased Mean Teacher (UMT) model for cross-domain object detection. We reveal that there often exists a considerable model bias for the simple mean teacher (MT) model in cross-domain scenarios, and eliminate the model bias with several simple yet highly effective strategies. In particular, for the teacher model, we propose a cross-domain distillation method for MT to maximally exploit the expertise of the teacher model. Moreover, for the student model, we alleviate its bias by augmenting training samples with pixel-level adaptation. Finally, for the teaching process, we employ an out-of-distribution estimation strategy to select samples that most fit the current model to further enhance the cross-domain distillation process. By tackling the model bias issue with these strategies, our UMT model achieves mAPs of 44.1%, 58.1%, 41.7%, and 43.1% on benchmark datasets Clipart1k, Watercolor2k, Foggy Cityscapes, and Cityscapes, respectively, which outperforms the existing state-of-the-art results in notable margins. Our implementation is available at https://github.com/kinredon/umt.



### Learned Enrichment of Top-View Grid Maps Improves Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2003.00710v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.00710v2)
- **Published**: 2020-03-02 08:27:54+00:00
- **Updated**: 2020-03-09 11:57:30+00:00
- **Authors**: Sascha Wirges, Ye Yang, Sven Richter, Haohao Hu, Christoph Stiller
- **Comment**: 6 pages, 6 figures, 4 tables
- **Journal**: None
- **Summary**: We propose an object detector for top-view grid maps which is additionally trained to generate an enriched version of its input. Our goal in the joint model is to improve generalization by regularizing towards structural knowledge in form of a map fused from multiple adjacent range sensor measurements. This training data can be generated in an automatic fashion, thus does not require manual annotations. We present an evidential framework to generate training data, investigate different model architectures and show that predicting enriched inputs as an additional task can improve object detection performance.



### A-TVSNet: Aggregated Two-View Stereo Network for Multi-View Stereo Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2003.00711v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.00711v1)
- **Published**: 2020-03-02 08:29:35+00:00
- **Updated**: 2020-03-02 08:29:35+00:00
- **Authors**: Sizhang Dai, Weibing Huang
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a learning-based network for depth map estimation from multi-view stereo (MVS) images. Our proposed network consists of three sub-networks: 1) a base network for initial depth map estimation from an unstructured stereo image pair, 2) a novel refinement network that leverages both photometric and geometric information, and 3) an attentional multi-view aggregation framework that enables efficient information exchange and integration among different stereo image pairs. The proposed network, called A-TVSNet, is evaluated on various MVS datasets and shows the ability to produce high quality depth map that outperforms competing approaches. Our code is available at https://github.com/daiszh/A-TVSNet.



### Towards Unconstrained Palmprint Recognition on Consumer Devices: a Literature Review
- **Arxiv ID**: http://arxiv.org/abs/2003.00737v1
- **DOI**: 10.1109/ACCESS.2020.2992219
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.00737v1)
- **Published**: 2020-03-02 09:53:43+00:00
- **Updated**: 2020-03-02 09:53:43+00:00
- **Authors**: Adrian-S. Ungureanu, Saqib Salahuddin, Peter Corcoran
- **Comment**: None
- **Journal**: None
- **Summary**: As a biometric palmprints have been largely under-utilized, but they offer some advantages over fingerprints and facial biometrics. Recent improvements in imaging capabilities on handheld and wearable consumer devices have re-awakened interest in the use fo palmprints. The aim of this paper is to provide a comprehensive review of state-of-the-art methods for palmprint recognition including Region of Interest extraction methods, feature extraction approaches and matching algorithms along with overview of available palmprint datasets in order to understand the latest trends and research dynamics in the palmprint recognition field.



### Long Short-Term Sample Distillation
- **Arxiv ID**: http://arxiv.org/abs/2003.00739v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2003.00739v1)
- **Published**: 2020-03-02 10:03:14+00:00
- **Updated**: 2020-03-02 10:03:14+00:00
- **Authors**: Liang Jiang, Zujie Wen, Zhongping Liang, Yafang Wang, Gerard de Melo, Zhe Li, Liangzhuang Ma, Jiaxing Zhang, Xiaolong Li, Yuan Qi
- **Comment**: published as a conference paper at AAAI 2020
- **Journal**: None
- **Summary**: In the past decade, there has been substantial progress at training increasingly deep neural networks. Recent advances within the teacher--student training paradigm have established that information about past training updates show promise as a source of guidance during subsequent training steps. Based on this notion, in this paper, we propose Long Short-Term Sample Distillation, a novel training policy that simultaneously leverages multiple phases of the previous training process to guide the later training updates to a neural network, while efficiently proceeding in just one single generation pass. With Long Short-Term Sample Distillation, the supervision signal for each sample is decomposed into two parts: a long-term signal and a short-term one. The long-term teacher draws on snapshots from several epochs ago in order to provide steadfast guidance and to guarantee teacher--student differences, while the short-term one yields more up-to-date cues with the goal of enabling higher-quality updates. Moreover, the teachers for each sample are unique, such that, overall, the model learns from a very diverse set of teachers. Comprehensive experimental results across a range of vision and NLP tasks demonstrate the effectiveness of this new training method.



### A Question-Centric Model for Visual Question Answering in Medical Imaging
- **Arxiv ID**: http://arxiv.org/abs/2003.08760v1
- **DOI**: 10.1109/TMI.2020.2978284
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.08760v1)
- **Published**: 2020-03-02 10:16:16+00:00
- **Updated**: 2020-03-02 10:16:16+00:00
- **Authors**: Minh H. Vu, Tommy Löfstedt, Tufve Nyholm, Raphael Sznitman
- **Comment**: Accepted at IEEE Transactions on Medical Imaging
- **Journal**: None
- **Summary**: Deep learning methods have proven extremely effective at performing a variety of medical image analysis tasks. With their potential use in clinical routine, their lack of transparency has however been one of their few weak points, raising concerns regarding their behavior and failure modes. While most research to infer model behavior has focused on indirect strategies that estimate prediction uncertainties and visualize model support in the input image space, the ability to explicitly query a prediction model regarding its image content offers a more direct way to determine the behavior of trained models. To this end, we present a novel Visual Question Answering approach that allows an image to be queried by means of a written question. Experiments on a variety of medical and natural image datasets show that by fusing image and question features in a novel way, the proposed approach achieves an equal or higher accuracy compared to current methods.



### Evolutionary Image Transition and Painting Using Random Walks
- **Arxiv ID**: http://arxiv.org/abs/2003.01517v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.01517v1)
- **Published**: 2020-03-02 10:28:24+00:00
- **Updated**: 2020-03-02 10:28:24+00:00
- **Authors**: Aneta Neumann, Bradley Alexander, Frank Neumann
- **Comment**: Accepted for the Evolutionary Computation Journal (MIT Press). arXiv
  admin note: text overlap with arXiv:1604.06187
- **Journal**: None
- **Summary**: We present a study demonstrating how random walk algorithms can be used for evolutionary image transition. We design different mutation operators based on uniform and biased random walks and study how their combination with a baseline mutation operator can lead to interesting image transition processes in terms of visual effects and artistic features. Using feature-based analysis we investigate the evolutionary image transition behaviour with respect to different features and evaluate the images constructed during the image transition process. Afterwards, we investigate how modifications of our biased random walk approaches can be used for evolutionary image painting. We introduce an evolutionary image painting approach whose underlying biased random walk can be controlled by a parameter influencing the bias of the random walk and thereby creating different artistic painting effects.



### Learning Depth With Very Sparse Supervision
- **Arxiv ID**: http://arxiv.org/abs/2003.00752v2
- **DOI**: 10.1109/LRA.2020.3009067
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2003.00752v2)
- **Published**: 2020-03-02 10:44:13+00:00
- **Updated**: 2020-07-16 10:01:55+00:00
- **Authors**: Antonio Loquercio, Alexey Dosovitskiy, Davide Scaramuzza
- **Comment**: Accepted for Publication at the IEEE Robotics and Automation Letters
  (RA-L) 2020, and International Conference on Intelligent Robots and Systems
  (IROS) 2020
- **Journal**: IEEE Robotics and Automation Letters (RA-L) 2020
- **Summary**: Motivated by the astonishing capabilities of natural intelligent agents and inspired by theories from psychology, this paper explores the idea that perception gets coupled to 3D properties of the world via interaction with the environment. Existing works for depth estimation require either massive amounts of annotated training data or some form of hard-coded geometrical constraint. This paper explores a new approach to learning depth perception requiring neither of those. Specifically, we train a specialized global-local network architecture with what would be available to a robot interacting with the environment: from extremely sparse depth measurements down to even a single pixel per image. From a pair of consecutive images, our proposed network outputs a latent representation of the observer's motion between the images and a dense depth map. Experiments on several datasets show that, when ground truth is available even for just one of the image pixels, the proposed network can learn monocular dense depth estimation up to 22.5% more accurately than state-of-the-art approaches. We believe that this work, despite its scientific interest, lays the foundations to learn depth from extremely sparse supervision, which can be valuable to all robotic systems acting under severe bandwidth or sensing constraints.



### Unsupervised Learning of Depth, Optical Flow and Pose with Occlusion from 3D Geometry
- **Arxiv ID**: http://arxiv.org/abs/2003.00766v3
- **DOI**: 10.1109/TITS.2020.3010418
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.00766v3)
- **Published**: 2020-03-02 11:18:13+00:00
- **Updated**: 2020-08-20 05:26:00+00:00
- **Authors**: Guangming Wang, Chi Zhang, Hesheng Wang, Jingchuan Wang, Yong Wang, Xinlei Wang
- **Comment**: Published in: IEEE Transactions on Intelligent Transportation
  Systems. DOI: 10.1109/TITS.2020.3010418
- **Journal**: IEEE Transactions on Intelligent Transportation Systems, 2020
- **Summary**: In autonomous driving, monocular sequences contain lots of information. Monocular depth estimation, camera ego-motion estimation and optical flow estimation in consecutive frames are high-profile concerns recently. By analyzing tasks above, pixels in the middle frame are modeled into three parts: the rigid region, the non-rigid region, and the occluded region. In joint unsupervised training of depth and pose, we can segment the occluded region explicitly. The occlusion information is used in unsupervised learning of depth, pose and optical flow, as the image reconstructed by depth-pose and optical flow will be invalid in occluded regions. A less-than-mean mask is designed to further exclude the mismatched pixels interfered with by motion or illumination change in the training of depth and pose networks. This method is also used to exclude some trivial mismatched pixels in the training of the optical flow network. Maximum normalization is proposed for depth smoothness term to restrain depth degradation in textureless regions. In the occluded region, as depth and camera motion can provide more reliable motion estimation, they can be used to instruct unsupervised learning of optical flow. Our experiments in KITTI dataset demonstrate that the model based on three regions, full and explicit segmentation of the occlusion region, the rigid region, and the non-rigid region with corresponding unsupervised losses can improve performance on three tasks significantly. The source code is available at: https://github.com/guangmingw/DOPlearning.



### Convolutional Sparse Support Estimator Network (CSEN) From energy efficient support estimation to learning-aided Compressive Sensing
- **Arxiv ID**: http://arxiv.org/abs/2003.00768v2
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.00768v2)
- **Published**: 2020-03-02 11:18:35+00:00
- **Updated**: 2020-04-05 19:41:06+00:00
- **Authors**: Mehmet Yamac, Mete Ahishali, Serkan Kiranyaz, Moncef Gabbouj
- **Comment**: None
- **Journal**: None
- **Summary**: Support estimation (SE) of a sparse signal refers to finding the location indices of the non-zero elements in a sparse representation. Most of the traditional approaches dealing with SE problem are iterative algorithms based on greedy methods or optimization techniques. Indeed, a vast majority of them use sparse signal recovery techniques to obtain support sets instead of directly mapping the non-zero locations from denser measurements (e.g., Compressively Sensed Measurements). This study proposes a novel approach for learning such a mapping from a training set. To accomplish this objective, the Convolutional Support Estimator Networks (CSENs), each with a compact configuration, are designed. The proposed CSEN can be a crucial tool for the following scenarios: (i) Real-time and low-cost support estimation can be applied in any mobile and low-power edge device for anomaly localization, simultaneous face recognition, etc. (ii) CSEN's output can directly be used as "prior information" which improves the performance of sparse signal recovery algorithms. The results over the benchmark datasets show that state-of-the-art performance levels can be achieved by the proposed approach with a significantly reduced computational complexity.



### Identity Recognition in Intelligent Cars with Behavioral Data and LSTM-ResNet Classifier
- **Arxiv ID**: http://arxiv.org/abs/2003.00770v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T01 (Primary) 68T10 (Secondary)
- **Links**: [PDF](http://arxiv.org/pdf/2003.00770v1)
- **Published**: 2020-03-02 11:24:05+00:00
- **Updated**: 2020-03-02 11:24:05+00:00
- **Authors**: Michael Hammann, Maximilian Kraus, Sina Shafaei, Alois Knoll
- **Comment**: 12 Pages, 3 Figures
- **Journal**: None
- **Summary**: Identity recognition in a car cabin is a critical task nowadays and offers a great field of applications ranging from personalizing intelligent cars to suit drivers physical and behavioral needs to increasing safety and security. However, the performance and applicability of published approaches are still not suitable for use in series cars and need to be improved. In this paper, we investigate Human Identity Recognition in a car cabin with Time Series Classification (TSC) and deep neural networks. We use gas and brake pedal pressure as input to our models. This data is easily collectable during driving in everyday situations. Since our classifiers have very little memory requirements and do not require any input data preproccesing, we were able to train on one Intel i5-3210M processor only. Our classification approach is based on a combination of LSTM and ResNet. The network trained on a subset of NUDrive outperforms the ResNet and LSTM models trained solely by 35.9 % and 53.85 % accuracy respectively. We reach a final accuracy of 79.49 % on a 10-drivers subset of NUDrive and 96.90 % on a 5-drivers subset of UTDrive.



### Addressing target shift in zero-shot learning using grouped adversarial learning
- **Arxiv ID**: http://arxiv.org/abs/2003.00845v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.00845v2)
- **Published**: 2020-03-02 13:00:27+00:00
- **Updated**: 2020-06-16 11:38:50+00:00
- **Authors**: Saneem Ahmed Chemmengath, Soumava Paul, Samarth Bharadwaj, Suranjana Samanta, Karthik Sankaranarayanan
- **Comment**: Under submission at Neurips 2020
- **Journal**: None
- **Summary**: Zero-shot learning (ZSL) algorithms typically work by exploiting attribute correlations to be able to make predictions in unseen classes. However, these correlations do not remain intact at test time in most practical settings and the resulting change in these correlations lead to adverse effects on zero-shot learning performance. In this paper, we present a new paradigm for ZSL that: (i) utilizes the class-attribute mapping of unseen classes to estimate the change in target distribution (target shift), and (ii) propose a novel technique called grouped Adversarial Learning (gAL) to reduce negative effects of this shift. Our approach is widely applicable for several existing ZSL algorithms, including those with implicit attribute predictions. We apply the proposed technique ($g$AL) on three popular ZSL algorithms: ALE, SJE, and DEVISE, and show performance improvements on 4 popular ZSL datasets: AwA2, aPY, CUB and SUN. We obtain SOTA results on SUN and aPY datasets and achieve comparable results on AwA2.



### Learning to Deblur and Generate High Frame Rate Video with an Event Camera
- **Arxiv ID**: http://arxiv.org/abs/2003.00847v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.00847v2)
- **Published**: 2020-03-02 13:02:05+00:00
- **Updated**: 2020-03-20 04:09:55+00:00
- **Authors**: Chen Haoyu, Teng Minggui, Shi Boxin, Wang YIzhou, Huang Tiejun
- **Comment**: None
- **Journal**: None
- **Summary**: Event cameras are bio-inspired cameras which can measure the change of intensity asynchronously with high temporal resolution. One of the event cameras' advantages is that they do not suffer from motion blur when recording high-speed scenes. In this paper, we formulate the deblurring task on traditional cameras directed by events to be a residual learning one, and we propose corresponding network architectures for effective learning of deblurring and high frame rate video generation tasks. We first train a modified U-Net network to restore a sharp image from a blurry image using corresponding events. Then we train another similar network with different downsampling blocks to generate high frame rate video using the restored sharp image and events. Experiment results show that our method can restore sharper images and videos than state-of-the-art methods.



### Multi-View Learning for Vision-and-Language Navigation
- **Arxiv ID**: http://arxiv.org/abs/2003.00857v3
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.00857v3)
- **Published**: 2020-03-02 13:07:46+00:00
- **Updated**: 2020-03-09 21:15:55+00:00
- **Authors**: Qiaolin Xia, Xiujun Li, Chunyuan Li, Yonatan Bisk, Zhifang Sui, Jianfeng Gao, Yejin Choi, Noah A. Smith
- **Comment**: 16 pages, 8 figures
- **Journal**: None
- **Summary**: Learning to navigate in a visual environment following natural language instructions is a challenging task because natural language instructions are highly variable, ambiguous, and under-specified. In this paper, we present a novel training paradigm, Learn from EveryOne (LEO), which leverages multiple instructions (as different views) for the same trajectory to resolve language ambiguity and improve generalization. By sharing parameters across instructions, our approach learns more effectively from limited training data and generalizes better in unseen environments. On the recent Room-to-Room (R2R) benchmark dataset, LEO achieves 16% improvement (absolute) over a greedy agent as the base agent (25.3% $\rightarrow$ 41.4%) in Success Rate weighted by Path Length (SPL). Further, LEO is complementary to most existing models for vision-and-language navigation, allowing for easy integration with the existing techniques, leading to LEO+, which creates the new state of the art, pushing the R2R benchmark to 62% (9% absolute improvement).



### Learning Texture Invariant Representation for Domain Adaptation of Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2003.00867v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.00867v2)
- **Published**: 2020-03-02 13:11:54+00:00
- **Updated**: 2020-03-30 06:56:11+00:00
- **Authors**: Myeongjin Kim, Hyeran Byun
- **Comment**: 2020 CVPR
- **Journal**: None
- **Summary**: Since annotating pixel-level labels for semantic segmentation is laborious, leveraging synthetic data is an attractive solution. However, due to the domain gap between synthetic domain and real domain, it is challenging for a model trained with synthetic data to generalize to real data. In this paper, considering the fundamental difference between the two domains as the texture, we propose a method to adapt to the texture of the target domain. First, we diversity the texture of synthetic images using a style transfer algorithm. The various textures of generated images prevent a segmentation model from overfitting to one specific (synthetic) texture. Then, we fine-tune the model with self-training to get direct supervision of the target texture. Our results achieve state-of-the-art performance and we analyze the properties of the model trained on the stylized dataset with extensive experiments.



### 3D Object Detection From LiDAR Data Using Distance Dependent Feature Extraction
- **Arxiv ID**: http://arxiv.org/abs/2003.00888v2
- **DOI**: 10.5220/0009330402890300
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.00888v2)
- **Published**: 2020-03-02 13:16:35+00:00
- **Updated**: 2020-03-03 07:47:20+00:00
- **Authors**: Guus Engels, Nerea Aranjuelo, Ignacio Arganda-Carreras, Marcos Nieto, Oihana Otaegui
- **Comment**: 10 pages, 8 figures, 6th International Conference on Vehicle
  Technology and Intelligent Transport Systems (VEHITS 2020)
- **Journal**: None
- **Summary**: This paper presents a new approach to 3D object detection that leverages the properties of the data obtained by a LiDAR sensor. State-of-the-art detectors use neural network architectures based on assumptions valid for camera images. However, point clouds obtained from LiDAR are fundamentally different. Most detectors use shared filter kernels to extract features which do not take into account the range dependent nature of the point cloud features. To show this, different detectors are trained on two splits of the KITTI dataset: close range (objects up to 25 meters from LiDAR) and long-range. Top view images are generated from point clouds as input for the networks. Combined results outperform the baseline network trained on the full dataset with a single backbone. Additional research compares the effect of using different input features when converting the point cloud to image. The results indicate that the network focuses on the shape and structure of the objects, rather than exact values of the input. This work proposes an improvement for 3D object detectors by taking into account the properties of LiDAR point clouds over distance. Results show that training separate networks for close-range and long-range objects boosts performance for all KITTI benchmark difficulties.



### Gated Fusion Network for Degraded Image Super Resolution
- **Arxiv ID**: http://arxiv.org/abs/2003.00893v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.00893v2)
- **Published**: 2020-03-02 13:28:32+00:00
- **Updated**: 2020-03-04 10:47:16+00:00
- **Authors**: Xinyi Zhang, Hang Dong, Zhe Hu, Wei-Sheng Lai, Fei Wang, Ming-Hsuan Yang
- **Comment**: Accepted by IJCV. The code will be publicly available at
  https://github.com/BookerDeWitt/GFN-IJCV
- **Journal**: None
- **Summary**: Single image super resolution aims to enhance image quality with respect to spatial content, which is a fundamental task in computer vision. In this work, we address the task of single frame super resolution with the presence of image degradation, e.g., blur, haze, or rain streaks. Due to the limitations of frame capturing and formation processes, image degradation is inevitable, and the artifacts would be exacerbated by super resolution methods. To address this problem, we propose a dual-branch convolutional neural network to extract base features and recovered features separately. The base features contain local and global information of the input image. On the other hand, the recovered features focus on the degraded regions and are used to remove the degradation. Those features are then fused through a recursive gate module to obtain sharp features for super resolution. By decomposing the feature extraction step into two task-independent streams, the dual-branch model can facilitate the training process by avoiding learning the mixed degradation all-in-one and thus enhance the final high-resolution prediction results. We evaluate the proposed method in three degradation scenarios. Experiments on these scenarios demonstrate that the proposed method performs more efficiently and favorably against the state-of-the-art approaches on benchmark datasets.



### Weakly-supervised Object Localization for Few-shot Learning and Fine-grained Few-shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2003.00874v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML, 68T30, 68T10 (Primary), I.2.4; I.5.3
- **Links**: [PDF](http://arxiv.org/pdf/2003.00874v3)
- **Published**: 2020-03-02 14:07:05+00:00
- **Updated**: 2020-12-12 02:50:57+00:00
- **Authors**: Xiaojian He, Jinfu Lin, Junming Shen
- **Comment**: 8 pages, 6 figures, 6 tables
- **Journal**: None
- **Summary**: Few-shot learning (FSL) aims to learn novel visual categories from very few samples, which is a challenging problem in real-world applications. Many methods of few-shot classification work well on general images to learn global representation. However, they can not deal with fine-grained categories well at the same time due to a lack of subtle and local information. We argue that localization is an efficient approach because it directly provides the discriminative regions, which is critical for both general classification and fine-grained classification in a low data regime. In this paper, we propose a Self-Attention Based Complementary Module (SAC Module) to fulfill the weakly-supervised object localization, and more importantly produce the activated masks for selecting discriminative deep descriptors for few-shot classification. Based on each selected deep descriptor, Semantic Alignment Module (SAM) calculates the semantic alignment distance between the query and support images to boost classification performance. Extensive experiments show our method outperforms the state-of-the-art methods on benchmark datasets under various settings, especially on the fine-grained few-shot tasks. Besides, our method achieves superior performance over previous methods when training the model on miniImageNet and evaluating it on the different datasets, demonstrating its superior generalization capacity. Extra visualization shows the proposed method can localize the key objects more interval.



### Always Look on the Bright Side of the Field: Merging Pose and Contextual Data to Estimate Orientation of Soccer Players
- **Arxiv ID**: http://arxiv.org/abs/2003.00943v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.00943v2)
- **Published**: 2020-03-02 14:42:51+00:00
- **Updated**: 2020-05-18 15:32:43+00:00
- **Authors**: Adrià Arbués-Sangüesa, Adrián Martín, Javier Fernández, Carlos Rodríguez, Gloria Haro, Coloma Ballester
- **Comment**: Article accepted in the International Conference on Image Processing
  (ICIP 2020); Appendix was not included in the original manuscript
- **Journal**: None
- **Summary**: Although orientation has proven to be a key skill of soccer players in order to succeed in a broad spectrum of plays, body orientation is a yet-little-explored area in sports analytics' research. Despite being an inherently ambiguous concept, player orientation can be defined as the projection (2D) of the normal vector placed in the center of the upper-torso of players (3D). This research presents a novel technique to obtain player orientation from monocular video recordings by mapping pose parts (shoulders and hips) in a 2D field by combining OpenPose with a super-resolution network, and merging the obtained estimation with contextual information (ball position). Results have been validated with players-held EPTS devices, obtaining a median error of 27 degrees/player. Moreover, three novel types of orientation maps are proposed in order to make raw orientation data easy to visualize and understand, thus allowing further analysis at team- or player-level.



### DriverMHG: A Multi-Modal Dataset for Dynamic Recognition of Driver Micro Hand Gestures and a Real-Time Recognition Framework
- **Arxiv ID**: http://arxiv.org/abs/2003.00951v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.00951v2)
- **Published**: 2020-03-02 14:54:19+00:00
- **Updated**: 2021-10-19 13:38:48+00:00
- **Authors**: Okan Köpüklü, Thomas Ledwon, Yao Rong, Neslihan Kose, Gerhard Rigoll
- **Comment**: Accepted to IEEE International Conference on Automatic Face and
  Gesture Recognition (FG 2020)
- **Journal**: None
- **Summary**: The use of hand gestures provides a natural alternative to cumbersome interface devices for Human-Computer Interaction (HCI) systems. However, real-time recognition of dynamic micro hand gestures from video streams is challenging for in-vehicle scenarios since (i) the gestures should be performed naturally without distracting the driver, (ii) micro hand gestures occur within very short time intervals at spatially constrained areas, (iii) the performed gesture should be recognized only once, and (iv) the entire architecture should be designed lightweight as it will be deployed to an embedded system. In this work, we propose an HCI system for dynamic recognition of driver micro hand gestures, which can have a crucial impact in automotive sector especially for safety related issues. For this purpose, we initially collected a dataset named Driver Micro Hand Gestures (DriverMHG), which consists of RGB, depth and infrared modalities. The challenges for dynamic recognition of micro hand gestures have been addressed by proposing a lightweight convolutional neural network (CNN) based architecture which operates online efficiently with a sliding window approach. For the CNN model, several 3-dimensional resource efficient networks are applied and their performances are analyzed. Online recognition of gestures has been performed with 3D-MobileNetV2, which provided the best offline accuracy among the applied networks with similar computational complexities. The final architecture is deployed on a driver simulator operating in real-time. We make DriverMHG dataset and our source code publicly available.



### Evaluating Temporal Queries Over Video Feeds
- **Arxiv ID**: http://arxiv.org/abs/2003.00953v3
- **DOI**: None
- **Categories**: **cs.DB**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.00953v3)
- **Published**: 2020-03-02 14:55:57+00:00
- **Updated**: 2020-03-05 22:22:46+00:00
- **Authors**: Yueting Chen, Xiaohui Yu, Nick Koudas
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in Computer Vision and Deep Learning made possible the efficient extraction of a schema from frames of streaming video. As such, a stream of objects and their associated classes along with unique object identifiers derived via object tracking can be generated, providing unique objects as they are captured across frames. In this paper we initiate a study of temporal queries involving objects and their co-occurrences in video feeds. For example, queries that identify video segments during which the same two red cars and the same two humans appear jointly for five minutes are of interest to many applications ranging from law enforcement to security and safety. We take the first step and define such queries in a way that they incorporate certain physical aspects of video capture such as object occlusion. We present an architecture consisting of three layers, namely object detection/tracking, intermediate data generation and query evaluation. We propose two techniques,MFS and SSG, to organize all detected objects in the intermediate data generation layer, which effectively, given the queries, minimizes the number of objects and frames that have to be considered during query evaluation. We also introduce an algorithm called State Traversal (ST) that processes incoming frames against the SSG and efficiently prunes objects and frames unrelated to query evaluation, while maintaining all states required for succinct query evaluation. We present the results of a thorough experimental evaluation utilizing both real and synthetic data establishing the trade-offs between MFS and SSG. We stress various parameters of interest in our evaluation and demonstrate that the proposed query evaluation methodology coupled with the proposed algorithms is capable to evaluate temporal queries over video feeds efficiently, achieving orders of magnitude performance benefits.



### Plug & Play Convolutional Regression Tracker for Video Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2003.00981v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.00981v1)
- **Published**: 2020-03-02 15:57:55+00:00
- **Updated**: 2020-03-02 15:57:55+00:00
- **Authors**: Ye Lyu, Michael Ying Yang, George Vosselman, Gui-Song Xia
- **Comment**: None
- **Journal**: None
- **Summary**: Video object detection targets to simultaneously localize the bounding boxes of the objects and identify their classes in a given video. One challenge for video object detection is to consistently detect all objects across the whole video. As the appearance of objects may deteriorate in some frames, features or detections from the other frames are commonly used to enhance the prediction. In this paper, we propose a Plug & Play scale-adaptive convolutional regression tracker for the video object detection task, which could be easily and compatibly implanted into the current state-of-the-art detection networks. As the tracker reuses the features from the detector, it is a very light-weighted increment to the detection network. The whole network performs at the speed close to a standard object detector. With our new video object detection pipeline design, image object detectors can be easily turned into efficient video object detectors without modifying any parameters. The performance is evaluated on the large-scale ImageNet VID dataset. Our Plug & Play design improves mAP score for the image detector by around 5% with only little speed drop.



### Constrained Nonnegative Matrix Factorization for Blind Hyperspectral Unmixing incorporating Endmember Independence
- **Arxiv ID**: http://arxiv.org/abs/2003.01041v5
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.01041v5)
- **Published**: 2020-03-02 17:20:04+00:00
- **Updated**: 2021-08-07 04:42:34+00:00
- **Authors**: E. M. M. B. Ekanayake, H. M. H. K. Weerasooriya, D. Y. L. Ranasinghe, S. Herath, B. Rathnayake, G. M. R. I. Godaliyadda, M. P. B. Ekanayake, H. M. V. R. Herath
- **Comment**: 18 pages, 16 figures
- **Journal**: None
- **Summary**: Hyperspectral unmixing (HU) has become an important technique in exploiting hyperspectral data since it decomposes a mixed pixel into a collection of endmembers weighted by fractional abundances. The endmembers of a hyperspectral image (HSI) are more likely to be generated by independent sources and be mixed in a macroscopic degree before arriving at the sensor element of the imaging spectrometer as mixed spectra. Over the past few decades, many attempts have focused on imposing auxiliary constraints on the conventional nonnegative matrix factorization (NMF) framework in order to effectively unmix these mixed spectra. As a promising step toward finding an optimum constraint to extract endmembers, this paper presents a novel blind HU algorithm, referred to as Kurtosis-based Smooth Nonnegative Matrix Factorization (KbSNMF) which incorporates a novel constraint based on the statistical independence of the probability density functions of endmember spectra. Imposing this constraint on the conventional NMF framework promotes the extraction of independent endmembers while further enhancing the parts-based representation of data. Experiments conducted on diverse synthetic HSI datasets (with numerous numbers of endmembers, spectral bands, pixels, and noise levels) and three standard real HSI datasets demonstrate the validity of the proposed KbSNMF algorithm compared to several state-of-the-art NMF-based HU baselines. The proposed algorithm exhibits superior performance especially in terms of extracting endmember spectra from hyperspectral data; therefore, it could uplift the performance of recent deep learning HU methods which utilize the endmember spectra as supervisory input data for abundance extraction.



### D3VO: Deep Depth, Deep Pose and Deep Uncertainty for Monocular Visual Odometry
- **Arxiv ID**: http://arxiv.org/abs/2003.01060v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2003.01060v2)
- **Published**: 2020-03-02 17:47:13+00:00
- **Updated**: 2020-03-28 21:08:41+00:00
- **Authors**: Nan Yang, Lukas von Stumberg, Rui Wang, Daniel Cremers
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: We propose D3VO as a novel framework for monocular visual odometry that exploits deep networks on three levels -- deep depth, pose and uncertainty estimation. We first propose a novel self-supervised monocular depth estimation network trained on stereo videos without any external supervision. In particular, it aligns the training image pairs into similar lighting condition with predictive brightness transformation parameters. Besides, we model the photometric uncertainties of pixels on the input images, which improves the depth estimation accuracy and provides a learned weighting function for the photometric residuals in direct (feature-less) visual odometry. Evaluation results show that the proposed network outperforms state-of-the-art self-supervised depth estimation networks. D3VO tightly incorporates the predicted depth, pose and uncertainty into a direct visual odometry method to boost both the front-end tracking as well as the back-end non-linear optimization. We evaluate D3VO in terms of monocular visual odometry on both the KITTI odometry benchmark and the EuRoC MAV dataset.The results show that D3VO outperforms state-of-the-art traditional monocular VO methods by a large margin. It also achieves comparable results to state-of-the-art stereo/LiDAR odometry on KITTI and to the state-of-the-art visual-inertial odometry on EuRoC MAV, while using only a single camera.



### Unlimited Resolution Image Generation with R2D2-GANs
- **Arxiv ID**: http://arxiv.org/abs/2003.01063v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.01063v1)
- **Published**: 2020-03-02 17:49:32+00:00
- **Updated**: 2020-03-02 17:49:32+00:00
- **Authors**: Marija Jegorova, Antti Ilari Karjalainen, Jose Vazquez, Timothy M. Hospedales
- **Comment**: Accepted to 2020 IEEE OCEANS (Singapore)
- **Journal**: None
- **Summary**: In this paper we present a novel simulation technique for generating high quality images of any predefined resolution. This method can be used to synthesize sonar scans of size equivalent to those collected during a full-length mission, with across track resolutions of any chosen magnitude. In essence, our model extends Generative Adversarial Networks (GANs) based architecture into a conditional recursive setting, that facilitates the continuity of the generated images. The data produced is continuous, realistically-looking, and can also be generated at least two times faster than the real speed of acquisition for the sonars with higher resolutions, such as EdgeTech. The seabed topography can be fully controlled by the user. The visual assessment tests demonstrate that humans cannot distinguish the simulated images from real. Moreover, experimental results suggest that in the absence of real data the autonomous recognition systems can benefit greatly from training with the synthetic data, produced by the R2D2-GANs.



### Robot Calligraphy using Pseudospectral Optimal Control in Conjunction with a Novel Dynamic Brush Model
- **Arxiv ID**: http://arxiv.org/abs/2003.01565v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.01565v3)
- **Published**: 2020-03-02 18:26:04+00:00
- **Updated**: 2020-09-17 18:01:02+00:00
- **Authors**: Sen Wang, Jiaqi Chen, Xuanliang Deng, Seth Hutchinson, Frank Dellaert
- **Comment**: Update to arXiv:1911.08002 mistakenly submitted as new article
- **Journal**: None
- **Summary**: Chinese calligraphy is a unique art form with great artistic value but difficult to master. In this paper, we formulate the calligraphy writing problem as a trajectory optimization problem, and propose an improved virtual brush model for simulating the real writing process. Our approach is inspired by pseudospectral optimal control in that we parameterize the actuator trajectory for each stroke as a Chebyshev polynomial. The proposed dynamic virtual brush model plays a key role in formulating the objective function to be optimized. Our approach shows excellent performance in drawing aesthetically pleasing characters, and does so much more efficiently than previous work, opening up the possibility to achieve real-time closed-loop control.



### Learn2Perturb: an End-to-end Feature Perturbation Learning to Improve Adversarial Robustness
- **Arxiv ID**: http://arxiv.org/abs/2003.01090v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.01090v2)
- **Published**: 2020-03-02 18:27:35+00:00
- **Updated**: 2020-03-03 16:51:46+00:00
- **Authors**: Ahmadreza Jeddi, Mohammad Javad Shafiee, Michelle Karg, Christian Scharfenberger, Alexander Wong
- **Comment**: 13 pages, 6 figures To be published in proceedings of IEEE conference
  on Computer Vision and Pattern Recognition (CVPR 2020)
- **Journal**: None
- **Summary**: While deep neural networks have been achieving state-of-the-art performance across a wide variety of applications, their vulnerability to adversarial attacks limits their widespread deployment for safety-critical applications. Alongside other adversarial defense approaches being investigated, there has been a very recent interest in improving adversarial robustness in deep neural networks through the introduction of perturbations during the training process. However, such methods leverage fixed, pre-defined perturbations and require significant hyper-parameter tuning that makes them very difficult to leverage in a general fashion. In this study, we introduce Learn2Perturb, an end-to-end feature perturbation learning approach for improving the adversarial robustness of deep neural networks. More specifically, we introduce novel perturbation-injection modules that are incorporated at each layer to perturb the feature space and increase uncertainty in the network. This feature perturbation is performed at both the training and the inference stages. Furthermore, inspired by the Expectation-Maximization, an alternating back-propagation training algorithm is introduced to train the network and noise parameters consecutively. Experimental results on CIFAR-10 and CIFAR-100 datasets show that the proposed Learn2Perturb method can result in deep neural networks which are $4-7\%$ more robust on $l_{\infty}$ FGSM and PDG adversarial attacks and significantly outperforms the state-of-the-art against $l_2$ $C\&W$ attack and a wide range of well-known black-box attacks.



### Understanding Contexts Inside Robot and Human Manipulation Tasks through a Vision-Language Model and Ontology System in a Video Stream
- **Arxiv ID**: http://arxiv.org/abs/2003.01163v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2003.01163v1)
- **Published**: 2020-03-02 19:48:59+00:00
- **Updated**: 2020-03-02 19:48:59+00:00
- **Authors**: Chen Jiang, Masood Dehghan, Martin Jagersand
- **Comment**: None
- **Journal**: None
- **Summary**: Manipulation tasks in daily life, such as pouring water, unfold intentionally under specialized manipulation contexts. Being able to process contextual knowledge in these Activities of Daily Living (ADLs) over time can help us understand manipulation intentions, which are essential for an intelligent robot to transition smoothly between various manipulation actions. In this paper, to model the intended concepts of manipulation, we present a vision dataset under a strictly constrained knowledge domain for both robot and human manipulations, where manipulation concepts and relations are stored by an ontology system in a taxonomic manner. Furthermore, we propose a scheme to generate a combination of visual attentions and an evolving knowledge graph filled with commonsense knowledge. Our scheme works with real-world camera streams and fuses an attention-based Vision-Language model with the ontology system. The experimental results demonstrate that the proposed scheme can successfully represent the evolution of an intended object manipulation procedure for both robots and humans. The proposed scheme allows the robot to mimic human-like intentional behaviors by watching real-time videos. We aim to develop this scheme further for real-world robot intelligence in Human-Robot Interaction.



### LiDARNet: A Boundary-Aware Domain Adaptation Model for Point Cloud Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2003.01174v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, I.2.10; I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/2003.01174v3)
- **Published**: 2020-03-02 20:18:31+00:00
- **Updated**: 2021-04-24 21:32:21+00:00
- **Authors**: Peng Jiang, Srikanth Saripalli
- **Comment**: 8 pages, 3 figures
- **Journal**: None
- **Summary**: We present a boundary-aware domain adaptation model for LiDAR scan full-scene semantic segmentation (LiDARNet). Our model can extract both the domain private features and the domain shared features with a two-branch structure. We embedded Gated-SCNN into the segmentor component of LiDARNet to learn boundary information while learning to predict full-scene semantic segmentation labels. Moreover, we further reduce the domain gap by inducing the model to learn a mapping between two domains using the domain shared and private features. Additionally, we introduce a new dataset (SemanticUSL\footnote{The access address of SemanticUSL:\url{https://unmannedlab.github.io/research/SemanticUSL}}) for domain adaptation for LiDAR point cloud semantic segmentation. The dataset has the same data format and ontology as SemanticKITTI. We conducted experiments on real-world datasets SemanticKITTI, SemanticPOSS, and SemanticUSL, which have differences in channel distributions, reflectivity distributions, diversity of scenes, and sensors setup. Using our approach, we can get a single projection-based LiDAR full-scene semantic segmentation model working on both domains. Our model can keep almost the same performance on the source domain after adaptation and get an 8\%-22\% mIoU performance increase in the target domain.



### RandomNet: Towards Fully Automatic Neural Architecture Design for Multimodal Learning
- **Arxiv ID**: http://arxiv.org/abs/2003.01181v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.01181v1)
- **Published**: 2020-03-02 20:41:57+00:00
- **Updated**: 2020-03-02 20:41:57+00:00
- **Authors**: Stefano Alletto, Shenyang Huang, Vincent Francois-Lavet, Yohei Nakata, Guillaume Rabusseau
- **Comment**: 6 pages, 1 figures
- **Journal**: None
- **Summary**: Almost all neural architecture search methods are evaluated in terms of performance (i.e. test accuracy) of the model structures that it finds. Should it be the only metric for a good autoML approach? To examine aspects beyond performance, we propose a set of criteria aimed at evaluating the core of autoML problem: the amount of human intervention required to deploy these methods into real world scenarios. Based on our proposed evaluation checklist, we study the effectiveness of a random search strategy for fully automated multimodal neural architecture search. Compared to traditional methods that rely on manually crafted feature extractors, our method selects each modality from a large search space with minimal human supervision. We show that our proposed random search strategy performs close to the state of the art on the AV-MNIST dataset while meeting the desirable characteristics for a fully automated design process.



### DEEVA: A Deep Learning and IoT Based Computer Vision System to Address Safety and Security of Production Sites in Energy Industry
- **Arxiv ID**: http://arxiv.org/abs/2003.01196v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.01196v1)
- **Published**: 2020-03-02 21:26:00+00:00
- **Updated**: 2020-03-02 21:26:00+00:00
- **Authors**: Nimish M. Awalgaonkar, Haining Zheng, Christopher S. Gurciullo
- **Comment**: AAAI-20 Workshop on Artificial Intelligence of Things (AIoT) workshop
  in conjunction with the 34th AAAI Conference on Artificial Intelligence
- **Journal**: None
- **Summary**: When it comes to addressing the safety/security related needs at different production/construction sites, accurate detection of the presence of workers, vehicles, equipment important and formed an integral part of computer vision-based surveillance systems (CVSS). Traditional CVSS systems focus on the use of different computer vision and pattern recognition algorithms overly reliant on manual extraction of features and small datasets, limiting their usage because of low accuracy, need for expert knowledge and high computational costs. The main objective of this paper is to provide decision makers at sites with a practical yet comprehensive deep learning and IoT based solution to tackle various computer vision related problems such as scene classification, object detection in scenes, semantic segmentation, scene captioning etc. Our overarching goal is to address the central question of What is happening at this site and where is it happening in an automated fashion minimizing the need for human resources dedicated to surveillance. We developed Deep ExxonMobil Eye for Video Analysis (DEEVA) package to handle scene classification, object detection, semantic segmentation and captioning of scenes in a hierarchical approach. The results reveal that transfer learning with the RetinaNet object detector is able to detect the presence of workers, different types of vehicles/construction equipment, safety related objects at a high level of accuracy (above 90%). With the help of deep learning to automatically extract features and IoT technology to automatic capture, transfer and process vast amount of realtime images, this framework is an important step towards the development of intelligent surveillance systems aimed at addressing myriads of open ended problems in the realm of security/safety monitoring, productivity assessments and future decision making.



### Energy-efficient and Robust Cumulative Training with Net2Net Transformation
- **Arxiv ID**: http://arxiv.org/abs/2003.01204v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.01204v1)
- **Published**: 2020-03-02 21:44:47+00:00
- **Updated**: 2020-03-02 21:44:47+00:00
- **Authors**: Aosong Feng, Priyadarshini Panda
- **Comment**: 6 pages, 6 figures, 2 Tables
- **Journal**: None
- **Summary**: Deep learning has achieved state-of-the-art accuracies on several computer vision tasks. However, the computational and energy requirements associated with training such deep neural networks can be quite high. In this paper, we propose a cumulative training strategy with Net2Net transformation that achieves training computational efficiency without incurring large accuracy loss, in comparison to a model trained from scratch. We achieve this by first training a small network (with lesser parameters) on a small subset of the original dataset, and then gradually expanding the network using Net2Net transformation to train incrementally on larger subsets of the dataset. This incremental training strategy with Net2Net utilizes function-preserving transformations that transfers knowledge from each previous small network to the next larger network, thereby, reducing the overall training complexity. Our experiments demonstrate that compared with training from scratch, cumulative training yields ~2x reduction in computational complexity for training TinyImageNet using VGG19 at iso-accuracy. Besides training efficiency, a key advantage of our cumulative training strategy is that we can perform pruning during Net2Net expansion to obtain a final network with optimal configuration (~0.4x lower inference compute complexity) compared to conventional training from scratch. We also demonstrate that the final network obtained from cumulative training yields better generalization performance and noise robustness. Further, we show that mutual inference from all the networks created with cumulative Net2Net expansion enables improved adversarial input detection.



### MRI Super-Resolution with GAN and 3D Multi-Level DenseNet: Smaller, Faster, and Better
- **Arxiv ID**: http://arxiv.org/abs/2003.01217v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.01217v2)
- **Published**: 2020-03-02 22:07:56+00:00
- **Updated**: 2020-03-06 23:46:42+00:00
- **Authors**: Yuhua Chen, Anthony G. Christodoulou, Zhengwei Zhou, Feng Shi, Yibin Xie, Debiao Li
- **Comment**: Preprint submitted to Medical Image Analysis
- **Journal**: None
- **Summary**: High-resolution (HR) magnetic resonance imaging (MRI) provides detailed anatomical information that is critical for diagnosis in the clinical application. However, HR MRI typically comes at the cost of long scan time, small spatial coverage, and low signal-to-noise ratio (SNR). Recent studies showed that with a deep convolutional neural network (CNN), HR generic images could be recovered from low-resolution (LR) inputs via single image super-resolution (SISR) approaches. Additionally, previous works have shown that a deep 3D CNN can generate high-quality SR MRIs by using learned image priors. However, 3D CNN with deep structures, have a large number of parameters and are computationally expensive. In this paper, we propose a novel 3D CNN architecture, namely a multi-level densely connected super-resolution network (mDCSRN), which is light-weight, fast and accurate. We also show that with the generative adversarial network (GAN)-guided training, the mDCSRN-GAN provides appealing sharp SR images with rich texture details that are highly comparable with the referenced HR images. Our results from experiments on a large public dataset with 1,113 subjects showed that this new architecture outperformed other popular deep learning methods in recovering 4x resolution-downgraded images in both quality and speed.



### Vehicle-Human Interactive Behaviors in Emergency: Data Extraction from Traffic Accident Videos
- **Arxiv ID**: http://arxiv.org/abs/2003.02059v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.SY, eess.IV, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2003.02059v2)
- **Published**: 2020-03-02 22:17:46+00:00
- **Updated**: 2020-08-12 04:10:05+00:00
- **Authors**: Wansong Liu, Danyang Luo, Changxu Wu, Minghui Zheng
- **Comment**: ACC 2020 final version
- **Journal**: None
- **Summary**: Currently, studying the vehicle-human interactive behavior in the emergency needs a large amount of datasets in the actual emergent situations that are almost unavailable. Existing public data sources on autonomous vehicles (AVs) mainly focus either on the normal driving scenarios or on emergency situations without human involvement. To fill this gap and facilitate related research, this paper provides a new yet convenient way to extract the interactive behavior data (i.e., the trajectories of vehicles and humans) from actual accident videos that were captured by both the surveillance cameras and driving recorders. The main challenge for data extraction from real-time accident video lies in the fact that the recording cameras are un-calibrated and the angles of surveillance are unknown. The approach proposed in this paper employs image processing to obtain a new perspective which is different from the original video's perspective. Meanwhile, we manually detect and mark object feature points in each image frame. In order to acquire a gradient of reference ratios, a geometric model is implemented in the analysis of reference pixel value, and the feature points are then scaled to the object trajectory based on the gradient of ratios. The generated trajectories not only restore the object movements completely but also reflect changes in vehicle velocity and rotation based on the feature points distributions.



### A Deep learning Approach to Generate Contrast-Enhanced Computerised Tomography Angiography without the Use of Intravenous Contrast Agents
- **Arxiv ID**: http://arxiv.org/abs/2003.01223v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2003.01223v1)
- **Published**: 2020-03-02 22:20:08+00:00
- **Updated**: 2020-03-02 22:20:08+00:00
- **Authors**: Anirudh Chandrashekar, Ashok Handa, Natesh Shivakumar, Pierfrancesco Lapolla, Vicente Grau, Regent Lee
- **Comment**: 7 Pages, 6 Figures
- **Journal**: None
- **Summary**: Contrast-enhanced computed tomography angiograms (CTAs) are widely used in cardiovascular imaging to obtain a non-invasive view of arterial structures. However, contrast agents are associated with complications at the injection site as well as renal toxicity leading to contrast-induced nephropathy (CIN) and renal failure. We hypothesised that the raw data acquired from a non-contrast CT contains sufficient information to differentiate blood and other soft tissue components. We utilised deep learning methods to define the subtleties between soft tissue components in order to simulate contrast enhanced CTAs without contrast agents. Twenty-six patients with paired non-contrast and CTA images were randomly selected from an approved clinical study. Non-contrast axial slices within the AAA from 10 patients (n = 100) were sampled for the underlying Hounsfield unit (HU) distribution at the lumen, intra-luminal thrombus and interface locations. Sampling of HUs in these regions revealed significant differences between all regions (p<0.001 for all comparisons), confirming the intrinsic differences in the radiomic signatures between these regions. To generate a large training dataset, paired axial slices from the training set (n=13) were augmented to produce a total of 23,551 2-D images. We trained a 2-D Cycle Generative Adversarial Network (cycleGAN) for this non-contrast to contrast (NC2C) transformation task. The accuracy of the cycleGAN output was assessed by comparison to the contrast image. This pipeline is able to differentiate between visually incoherent soft tissue regions in non-contrast CT images. The CTAs generated from the non-contrast images bear strong resemblance to the ground truth. Here we describe a novel application of Generative Adversarial Network for CT image processing. This is poised to disrupt clinical pathways requiring contrast enhanced CT imaging.



### MVC-Net: A Convolutional Neural Network Architecture for Manifold-Valued Images With Applications
- **Arxiv ID**: http://arxiv.org/abs/2003.01234v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.01234v2)
- **Published**: 2020-03-02 22:37:56+00:00
- **Updated**: 2020-03-06 17:40:23+00:00
- **Authors**: Jose J. Bouza, Chun-Hao Yang, David Vaillancourt, Baba C. Vemuri
- **Comment**: Equal contribution by J. Bouza and CH. Yang
- **Journal**: None
- **Summary**: Geometric deep learning has attracted significant attention in recent years, in part due to the availability of exotic data types for which traditional neural network architectures are not well suited. Our goal in this paper is to generalize convolutional neural networks (CNN) to the manifold-valued image case which arises commonly in medical imaging and computer vision applications. Explicitly, the input data to the network is an image where each pixel value is a sample from a Riemannian manifold. To achieve this goal, we must generalize the basic building block of traditional CNN architectures, namely, the weighted combinations operation. To this end, we develop a tangent space combination operation which is used to define a convolution operation on manifold-valued images that we call, the Manifold-Valued Convolution (MVC). We prove theoretical properties of the MVC operation, including equivariance to the action of the isometry group admitted by the manifold and characterizing when compositions of MVC layers collapse to a single layer. We present a detailed description of how to use MVC layers to build full, multi-layer neural networks that operate on manifold-valued images, which we call the MVC-net. Further, we empirically demonstrate superior performance of the MVC-nets in medical imaging and computer vision tasks.



### Point-GNN: Graph Neural Network for 3D Object Detection in a Point Cloud
- **Arxiv ID**: http://arxiv.org/abs/2003.01251v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.01251v1)
- **Published**: 2020-03-02 23:44:12+00:00
- **Updated**: 2020-03-02 23:44:12+00:00
- **Authors**: Weijing Shi, Ragunathan, Rajkumar
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a graph neural network to detect objects from a LiDAR point cloud. Towards this end, we encode the point cloud efficiently in a fixed radius near-neighbors graph. We design a graph neural network, named Point-GNN, to predict the category and shape of the object that each vertex in the graph belongs to. In Point-GNN, we propose an auto-registration mechanism to reduce translation variance, and also design a box merging and scoring operation to combine detections from multiple vertices accurately. Our experiments on the KITTI benchmark show the proposed approach achieves leading accuracy using the point cloud alone and can even surpass fusion-based algorithms. Our results demonstrate the potential of using the graph neural network as a new approach for 3D object detection. The code is available https://github.com/WeijingShi/Point-GNN.



