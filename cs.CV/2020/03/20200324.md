# Arxiv Papers in cs.CV on 2020-03-24
### A Simple Fix for Convolutional Neural Network via Coordinate Embedding
- **Arxiv ID**: http://arxiv.org/abs/2003.10589v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.10589v1)
- **Published**: 2020-03-24 00:31:27+00:00
- **Updated**: 2020-03-24 00:31:27+00:00
- **Authors**: Liliang Ren, Zhuonan Hao
- **Comment**: 6 pages, 8 figures, Course Project for ECE271B
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNN) has been widely applied in the realm of computer vision. However, given the fact that CNN models are translation invariant, they are not aware of the coordinate information of each pixel. Thus the generalization ability of CNN will be limited since the coordinate information is crucial for a model to learn affine transformations which directly operate on the coordinate of each pixel. In this project, we proposed a simple approach to incorporate the coordinate information to the CNN model through coordinate embedding. Our approach does not change the downstream model architecture and can be easily applied to the pre-trained models for the task like object detection. Our experiments on the German Traffic Sign Detection Benchmark show that our approach not only significantly improve the model performance but also have better robustness with respect to the affine transformation.



### Spatio-Temporal Handwriting Imitation
- **Arxiv ID**: http://arxiv.org/abs/2003.10593v2
- **DOI**: 10.1007/978-3-030-68238-5_38
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.10593v2)
- **Published**: 2020-03-24 00:46:40+00:00
- **Updated**: 2021-04-16 17:09:48+00:00
- **Authors**: Martin Mayr, Martin Stumpf, Anguelos Nicolaou, Mathias Seuret, Andreas Maier, Vincent Christlein
- **Comment**: Main paper: 16 pages, supplemental material: 7 pages
- **Journal**: None
- **Summary**: Most people think that their handwriting is unique and cannot be imitated by machines, especially not using completely new content. Current cursive handwriting synthesis is visually limited or needs user interaction. We show that subdividing the process into smaller subtasks makes it possible to imitate someone's handwriting with a high chance to be visually indistinguishable for humans. Therefore, a given handwritten sample will be used as the target style. This sample is transferred to an online sequence. Then, a method for online handwriting synthesis is used to produce a new realistic-looking text primed with the online input sequence. This new text is then rendered and style-adapted to the input pen. We show the effectiveness of the pipeline by generating in- and out-of-vocabulary handwritten samples that are validated in a comprehensive user study. Additionally, we show that also a typical writer identification system can partially be fooled by the created fake handwritings.



### Adversarial Perturbations Fool Deepfake Detectors
- **Arxiv ID**: http://arxiv.org/abs/2003.10596v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE, stat.ML, I.4
- **Links**: [PDF](http://arxiv.org/pdf/2003.10596v2)
- **Published**: 2020-03-24 00:54:02+00:00
- **Updated**: 2020-05-15 05:41:32+00:00
- **Authors**: Apurva Gandhi, Shomik Jain
- **Comment**: To appear in the proceedings of the International Joint Conference on
  Neural Networks (IJCNN 2020)
- **Journal**: None
- **Summary**: This work uses adversarial perturbations to enhance deepfake images and fool common deepfake detectors. We created adversarial perturbations using the Fast Gradient Sign Method and the Carlini and Wagner L2 norm attack in both blackbox and whitebox settings. Detectors achieved over 95% accuracy on unperturbed deepfakes, but less than 27% accuracy on perturbed deepfakes. We also explore two improvements to deepfake detectors: (i) Lipschitz regularization, and (ii) Deep Image Prior (DIP). Lipschitz regularization constrains the gradient of the detector with respect to the input in order to increase robustness to input perturbations. The DIP defense removes perturbations using generative convolutional neural networks in an unsupervised manner. Regularization improved the detection of perturbed deepfakes on average, including a 10% accuracy boost in the blackbox case. The DIP defense achieved 95% accuracy on perturbed deepfakes that fooled the original detector, while retaining 98% accuracy in other cases on a 100 image subsample.



### First Investigation Into the Use of Deep Learning for Continuous Assessment of Neonatal Postoperative Pain
- **Arxiv ID**: http://arxiv.org/abs/2003.10601v1
- **DOI**: 10.1109/FG47880.2020.00082
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.10601v1)
- **Published**: 2020-03-24 01:13:07+00:00
- **Updated**: 2020-03-24 01:13:07+00:00
- **Authors**: Md Sirajus Salekin, Ghada Zamzmi, Dmitry Goldgof, Rangachar Kasturi, Thao Ho, Yu Sun
- **Comment**: Accepted in the 15th IEEE International Conference on Automatic Face
  and Gesture Recognition (FG 2020)
- **Journal**: None
- **Summary**: This paper presents the first investigation into the use of fully automated deep learning framework for assessing neonatal postoperative pain. It specifically investigates the use of Bilinear Convolutional Neural Network (B-CNN) to extract facial features during different levels of postoperative pain followed by modeling the temporal pattern using Recurrent Neural Network (RNN). Although acute and postoperative pain have some common characteristics (e.g., visual action units), postoperative pain has a different dynamic, and it evolves in a unique pattern over time. Our experimental results indicate a clear difference between the pattern of acute and postoperative pain. They also suggest the efficiency of using a combination of bilinear CNN with RNN model for the continuous assessment of postoperative pain intensity.



### Video Object Grounding using Semantic Roles in Language Description
- **Arxiv ID**: http://arxiv.org/abs/2003.10606v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2003.10606v1)
- **Published**: 2020-03-24 01:31:14+00:00
- **Updated**: 2020-03-24 01:31:14+00:00
- **Authors**: Arka Sadhu, Kan Chen, Ram Nevatia
- **Comment**: CVPR20 camera-ready including appendix
- **Journal**: None
- **Summary**: We explore the task of Video Object Grounding (VOG), which grounds objects in videos referred to in natural language descriptions. Previous methods apply image grounding based algorithms to address VOG, fail to explore the object relation information and suffer from limited generalization. Here, we investigate the role of object relations in VOG and propose a novel framework VOGNet to encode multi-modal object relations via self-attention with relative position encoding. To evaluate VOGNet, we propose novel contrasting sampling methods to generate more challenging grounding input samples, and construct a new dataset called ActivityNet-SRL (ASRL) based on existing caption and grounding datasets. Experiments on ASRL validate the need of encoding object relations in VOG, and our VOGNet outperforms competitive baselines by a significant margin.



### Synergic Adversarial Label Learning for Grading Retinal Diseases via Knowledge Distillation and Multi-task Learning
- **Arxiv ID**: http://arxiv.org/abs/2003.10607v4
- **DOI**: 10.1109/jbhi.2021.3052916
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.10607v4)
- **Published**: 2020-03-24 01:32:04+00:00
- **Updated**: 2021-01-30 13:55:21+00:00
- **Authors**: Lie Ju, Xin Wang, Xin Zhao, Huimin Lu, Dwarikanath Mahapatra, Paul Bonnington, Zongyuan Ge
- **Comment**: None
- **Journal**: None
- **Summary**: The need for comprehensive and automated screening methods for retinal image classification has long been recognized. Well-qualified doctors annotated images are very expensive and only a limited amount of data is available for various retinal diseases such as age-related macular degeneration (AMD) and diabetic retinopathy (DR). Some studies show that AMD and DR share some common features like hemorrhagic points and exudation but most classification algorithms only train those disease models independently. Inspired by knowledge distillation where additional monitoring signals from various sources is beneficial to train a robust model with much fewer data. We propose a method called synergic adversarial label learning (SALL) which leverages relevant retinal disease labels in both semantic and feature space as additional signals and train the model in a collaborative manner. Our experiments on DR and AMD fundus image classification task demonstrate that the proposed method can significantly improve the accuracy of the model for grading diseases. In addition, we conduct additional experiments to show the effectiveness of SALL from the aspects of reliability and interpretability in the context of medical imaging application.



### UnrealText: Synthesizing Realistic Scene Text Images from the Unreal World
- **Arxiv ID**: http://arxiv.org/abs/2003.10608v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.10608v6)
- **Published**: 2020-03-24 01:37:42+00:00
- **Updated**: 2020-08-18 01:06:39+00:00
- **Authors**: Shangbang Long, Cong Yao
- **Comment**: adding experiments with Mask-RCNN
- **Journal**: None
- **Summary**: Synthetic data has been a critical tool for training scene text detection and recognition models. On the one hand, synthetic word images have proven to be a successful substitute for real images in training scene text recognizers. On the other hand, however, scene text detectors still heavily rely on a large amount of manually annotated real-world images, which are expensive. In this paper, we introduce UnrealText, an efficient image synthesis method that renders realistic images via a 3D graphics engine. 3D synthetic engine provides realistic appearance by rendering scene and text as a whole, and allows for better text region proposals with access to precise scene information, e.g. normal and even object meshes. The comprehensive experiments verify its effectiveness on both scene text detection and recognition. We also generate a multilingual version for future research into multilingual scene text detection and recognition. Additionally, we re-annotate scene text recognition datasets in a case-sensitive way and include punctuation marks for more comprehensive evaluations. The code and the generated datasets are released at: https://github.com/Jyouhou/UnrealText/ .



### TextCaps: a Dataset for Image Captioning with Reading Comprehension
- **Arxiv ID**: http://arxiv.org/abs/2003.12462v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2003.12462v2)
- **Published**: 2020-03-24 02:38:35+00:00
- **Updated**: 2020-08-04 04:08:02+00:00
- **Authors**: Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, Amanpreet Singh
- **Comment**: To appear in ECCV 2020 (oral) Project page:
  https://textvqa.org/textcaps
- **Journal**: None
- **Summary**: Image descriptions can help visually impaired people to quickly understand the image content. While we made significant progress in automatically describing images and optical character recognition, current approaches are unable to include written text in their descriptions, although text is omnipresent in human environments and frequently critical to understand our surroundings. To study how to comprehend text in the context of an image we collect a novel dataset, TextCaps, with 145k captions for 28k images. Our dataset challenges a model to recognize text, relate it to its visual context, and decide what part of the text to copy or paraphrase, requiring spatial, semantic, and visual reasoning between multiple text tokens and visual entities, such as objects. We study baselines and adapt existing approaches to this new task, which we refer to as image captioning with reading comprehension. Our analysis with automatic and human studies shows that our new TextCaps dataset provides many new technical challenges over previous datasets.



### KFNet: Learning Temporal Camera Relocalization using Kalman Filtering
- **Arxiv ID**: http://arxiv.org/abs/2003.10629v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.10629v1)
- **Published**: 2020-03-24 02:52:50+00:00
- **Updated**: 2020-03-24 02:52:50+00:00
- **Authors**: Lei Zhou, Zixin Luo, Tianwei Shen, Jiahui Zhang, Mingmin Zhen, Yao Yao, Tian Fang, Long Quan
- **Comment**: An oral paper of CVPR 2020
- **Journal**: None
- **Summary**: Temporal camera relocalization estimates the pose with respect to each video frame in sequence, as opposed to one-shot relocalization which focuses on a still image. Even though the time dependency has been taken into account, current temporal relocalization methods still generally underperform the state-of-the-art one-shot approaches in terms of accuracy. In this work, we improve the temporal relocalization method by using a network architecture that incorporates Kalman filtering (KFNet) for online camera relocalization. In particular, KFNet extends the scene coordinate regression problem to the time domain in order to recursively establish 2D and 3D correspondences for the pose determination. The network architecture design and the loss formulation are based on Kalman filtering in the context of Bayesian learning. Extensive experiments on multiple relocalization benchmarks demonstrate the high accuracy of KFNet at the top of both one-shot and temporal relocalization approaches. Our codes are released at https://github.com/zlthinker/KFNet.



### Robust and On-the-fly Dataset Denoising for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2003.10647v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.10647v2)
- **Published**: 2020-03-24 03:59:26+00:00
- **Updated**: 2020-04-09 04:59:33+00:00
- **Authors**: Jiaming Song, Lunjia Hu, Michael Auli, Yann Dauphin, Tengyu Ma
- **Comment**: None
- **Journal**: None
- **Summary**: Memorization in over-parameterized neural networks could severely hurt generalization in the presence of mislabeled examples. However, mislabeled examples are hard to avoid in extremely large datasets collected with weak supervision. We address this problem by reasoning counterfactually about the loss distribution of examples with uniform random labels had they were trained with the real examples, and use this information to remove noisy examples from the training set. First, we observe that examples with uniform random labels have higher losses when trained with stochastic gradient descent under large learning rates. Then, we propose to model the loss distribution of the counterfactual examples using only the network parameters, which is able to model such examples with remarkable success. Finally, we propose to remove examples whose loss exceeds a certain quantile of the modeled loss distribution. This leads to On-the-fly Data Denoising (ODD), a simple yet effective algorithm that is robust to mislabeled examples, while introducing almost zero computational overhead compared to standard training. ODD is able to achieve state-of-the-art results on a wide range of datasets including real-world ones such as WebVision and Clothing1M.



### Gen-LaneNet: A Generalized and Scalable Approach for 3D Lane Detection
- **Arxiv ID**: http://arxiv.org/abs/2003.10656v1
- **DOI**: 10.1007/978-3-030-58589-1_40
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.10656v1)
- **Published**: 2020-03-24 04:52:06+00:00
- **Updated**: 2020-03-24 04:52:06+00:00
- **Authors**: Yuliang Guo, Guang Chen, Peitao Zhao, Weide Zhang, Jinghao Miao, Jingao Wang, Tae Eun Choe
- **Comment**: None
- **Journal**: None
- **Summary**: We present a generalized and scalable method, called Gen-LaneNet, to detect 3D lanes from a single image. The method, inspired by the latest state-of-the-art 3D-LaneNet, is a unified framework solving image encoding, spatial transform of features and 3D lane prediction in a single network. However, we propose unique designs for Gen-LaneNet in two folds. First, we introduce a new geometry-guided lane anchor representation in a new coordinate frame and apply a specific geometric transformation to directly calculate real 3D lane points from the network output. We demonstrate that aligning the lane points with the underlying top-view features in the new coordinate frame is critical towards a generalized method in handling unfamiliar scenes. Second, we present a scalable two-stage framework that decouples the learning of image segmentation subnetwork and geometry encoding subnetwork. Compared to 3D-LaneNet, the proposed Gen-LaneNet drastically reduces the amount of 3D lane labels required to achieve a robust solution in real-world application. Moreover, we release a new synthetic dataset and its construction strategy to encourage the development and evaluation of 3D lane detection methods. In experiments, we conduct extensive ablation study to substantiate the proposed Gen-LaneNet significantly outperforms 3D-LaneNet in average precision(AP) and F-score.



### CRNet: Cross-Reference Networks for Few-Shot Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2003.10658v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.10658v1)
- **Published**: 2020-03-24 04:55:43+00:00
- **Updated**: 2020-03-24 04:55:43+00:00
- **Authors**: Weide Liu, Chi Zhang, Guosheng Lin, Fayao Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Over the past few years, state-of-the-art image segmentation algorithms are based on deep convolutional neural networks. To render a deep network with the ability to understand a concept, humans need to collect a large amount of pixel-level annotated data to train the models, which is time-consuming and tedious. Recently, few-shot segmentation is proposed to solve this problem. Few-shot segmentation aims to learn a segmentation model that can be generalized to novel classes with only a few training images. In this paper, we propose a cross-reference network (CRNet) for few-shot segmentation. Unlike previous works which only predict the mask in the query image, our proposed model concurrently make predictions for both the support image and the query image. With a cross-reference mechanism, our network can better find the co-occurrent objects in the two images, thus helping the few-shot segmentation task. We also develop a mask refinement module to recurrently refine the prediction of the foreground regions. For the $k$-shot learning, we propose to finetune parts of networks to take advantage of multiple labeled support images. Experiments on the PASCAL VOC 2012 dataset show that our network achieves state-of-the-art performance.



### Modeling Cross-view Interaction Consistency for Paired Egocentric Interaction Recognition
- **Arxiv ID**: http://arxiv.org/abs/2003.10663v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.10663v1)
- **Published**: 2020-03-24 05:05:34+00:00
- **Updated**: 2020-03-24 05:05:34+00:00
- **Authors**: Zhongguo Li, Fan Lyu, Wei Feng, Song Wang
- **Comment**: ICME2020
- **Journal**: None
- **Summary**: With the development of Augmented Reality (AR), egocentric action recognition (EAR) plays important role in accurately understanding demands from the user. However, EAR is designed to help recognize human-machine interaction in single egocentric view, thus difficult to capture interactions between two face-to-face AR users. Paired egocentric interaction recognition (PEIR) is the task to collaboratively recognize the interactions between two persons with the videos in their corresponding views. Unfortunately, existing PEIR methods always directly use linear decision function to fuse the features extracted from two corresponding egocentric videos, which ignore consistency of interaction in paired egocentric videos. The consistency of interactions in paired videos, and features extracted from them are correlated to each other. On top of that, we propose to build the relevance between two views using biliear pooling, which capture the consistency of two views in feature-level. Specifically, each neuron in the feature maps from one view connects to the neurons from another view, which guarantee the compact consistency between two views. Then all possible paired neurons are used for PEIR for the inside consistent information of them. To be efficient, we use compact bilinear pooling with Count Sketch to avoid directly computing outer product in bilinear. Experimental results on dataset PEV shows the superiority of the proposed methods on the task PEIR.



### On Localizing a Camera from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/2003.10664v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.10664v1)
- **Published**: 2020-03-24 05:09:01+00:00
- **Updated**: 2020-03-24 05:09:01+00:00
- **Authors**: Pradipta Ghosh, Xiaochen Liu, Hang Qiu, Marcos A. M. Vieira, Gaurav S. Sukhatme, Ramesh Govindan
- **Comment**: None
- **Journal**: None
- **Summary**: Public cameras often have limited metadata describing their attributes. A key missing attribute is the precise location of the camera, using which it is possible to precisely pinpoint the location of events seen in the camera. In this paper, we explore the following question: under what conditions is it possible to estimate the location of a camera from a single image taken by the camera? We show that, using a judicious combination of projective geometry, neural networks, and crowd-sourced annotations from human workers, it is possible to position 95% of the images in our test data set to within 12 m. This performance is two orders of magnitude better than PoseNet, a state-of-the-art neural network that, when trained on a large corpus of images in an area, can estimate the pose of a single image. Finally, we show that the camera's inferred position and intrinsic parameters can help design a number of virtual sensors, all of which are reasonably accurate.



### Real-time 3D object proposal generation and classification under limited processing resources
- **Arxiv ID**: http://arxiv.org/abs/2003.10670v1
- **DOI**: 10.1016/j.robot.2020.103557
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2003.10670v1)
- **Published**: 2020-03-24 05:36:53+00:00
- **Updated**: 2020-03-24 05:36:53+00:00
- **Authors**: Xuesong Li, Jose Guivant, Subhan Khan
- **Comment**: None
- **Journal**: Robotics and Autonomous Systems, 130 (2020) 103557
- **Summary**: The task of detecting 3D objects is important to various robotic applications. The existing deep learning-based detection techniques have achieved impressive performance. However, these techniques are limited to run with a graphics processing unit (GPU) in a real-time environment. To achieve real-time 3D object detection with limited computational resources for robots, we propose an efficient detection method consisting of 3D proposal generation and classification. The proposal generation is mainly based on point segmentation, while the proposal classification is performed by a lightweight convolution neural network (CNN) model. To validate our method, KITTI datasets are utilized. The experimental results demonstrate the capability of proposed real-time 3D object detection method from the point cloud with a competitive performance of object recall and classification.



### Deep Line Art Video Colorization with a Few References
- **Arxiv ID**: http://arxiv.org/abs/2003.10685v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.10685v2)
- **Published**: 2020-03-24 06:57:40+00:00
- **Updated**: 2020-03-30 07:34:55+00:00
- **Authors**: Min Shi, Jia-Qi Zhang, Shu-Yu Chen, Lin Gao, Yu-Kun Lai, Fang-Lue Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Coloring line art images based on the colors of reference images is an important stage in animation production, which is time-consuming and tedious. In this paper, we propose a deep architecture to automatically color line art videos with the same color style as the given reference images. Our framework consists of a color transform network and a temporal constraint network. The color transform network takes the target line art images as well as the line art and color images of one or more reference images as input, and generates corresponding target color images. To cope with larger differences between the target line art image and reference color images, our architecture utilizes non-local similarity matching to determine the region correspondences between the target image and the reference images, which are used to transform the local color information from the references to the target. To ensure global color style consistency, we further incorporate Adaptive Instance Normalization (AdaIN) with the transformation parameters obtained from a style embedding vector that describes the global color style of the references, extracted by an embedder. The temporal constraint network takes the reference images and the target image together in chronological order, and learns the spatiotemporal features through 3D convolution to ensure the temporal consistency of the target image and the reference image. Our model can achieve even better coloring results by fine-tuning the parameters with only a small amount of samples when dealing with an animation of a new style. To evaluate our method, we build a line art coloring dataset. Experiments show that our method achieves the best performance on line art video coloring compared to the state-of-the-art methods and other baselines.



### Learning regularization and intensity-gradient-based fidelity for single image super resolution
- **Arxiv ID**: http://arxiv.org/abs/2003.10689v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.10689v1)
- **Published**: 2020-03-24 07:03:18+00:00
- **Updated**: 2020-03-24 07:03:18+00:00
- **Authors**: Hu Liang, Shengrong Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: How to extract more and useful information for single image super resolution is an imperative and difficult problem. Learning-based method is a representative method for such task. However, the results are not so stable as there may exist big difference between the training data and the test data. The regularization-based method can effectively utilize the self-information of observation. However, the degradation model used in regularization-based method just considers the degradation in intensity space. It may not reconstruct images well as the degradation reflections in various feature space are not considered. In this paper, we first study the image degradation progress, and establish degradation model both in intensity and gradient space. Thus, a comprehensive data consistency constraint is established for the reconstruction. Consequently, more useful information can be extracted from the observed data. Second, the regularization term is learned by a designed symmetric residual deep neural-network. It can search similar external information from a predefined dataset avoiding the artificial tendency. Finally, the proposed fidelity term and designed regularization term are embedded into the regularization framework. Further, an optimization method is developed based on the half-quadratic splitting method and the pseudo conjugate method. Experimental results indicated that the subjective and the objective metric corresponding to the proposed method were better than those obtained by the comparison methods.



### Organ Segmentation From Full-size CT Images Using Memory-Efficient FCN
- **Arxiv ID**: http://arxiv.org/abs/2003.10690v1
- **DOI**: 10.1117/12.2551024
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.10690v1)
- **Published**: 2020-03-24 07:12:45+00:00
- **Updated**: 2020-03-24 07:12:45+00:00
- **Authors**: Chenglong Wang, Masahiro Oda, Kensaku Mori
- **Comment**: None
- **Journal**: Proc. SPIE 11314, Medical Imaging 2020: Computer-Aided Diagnosis,
  113140I
- **Summary**: In this work, we present a memory-efficient fully convolutional network (FCN) incorporated with several memory-optimized techniques to reduce the run-time GPU memory demand during training phase. In medical image segmentation tasks, subvolume cropping has become a common preprocessing. Subvolumes (or small patch volumes) were cropped to reduce GPU memory demand. However, small patch volumes capture less spatial context that leads to lower accuracy. As a pilot study, the purpose of this work is to propose a memory-efficient FCN which enables us to train the model on full size CT image directly without subvolume cropping, while maintaining the segmentation accuracy. We optimize our network from both architecture and implementation. With the development of computing hardware, such as graphics processing unit (GPU) and tensor processing unit (TPU), now deep learning applications is able to train networks with large datasets within acceptable time. Among these applications, semantic segmentation using fully convolutional network (FCN) also has gained a significant improvement against traditional image processing approaches in both computer vision and medical image processing fields. However, unlike general color images used in computer vision tasks, medical images have larger scales than color images such as 3D computed tomography (CT) images, micro CT images, and histopathological images. For training these medical images, the large demand of computing resource become a severe problem. In this paper, we present a memory-efficient FCN to tackle the high GPU memory demand challenge in organ segmentation problem from clinical CT images. The experimental results demonstrated that our GPU memory demand is about 40% of baseline architecture, parameter amount is about 30% of the baseline.



### Dynamic Hierarchical Mimicking Towards Consistent Optimization Objectives
- **Arxiv ID**: http://arxiv.org/abs/2003.10739v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.10739v2)
- **Published**: 2020-03-24 09:56:13+00:00
- **Updated**: 2021-08-20 08:36:47+00:00
- **Authors**: Duo Li, Qifeng Chen
- **Comment**: idea overlapped with existing work
- **Journal**: None
- **Summary**: While the depth of modern Convolutional Neural Networks (CNNs) surpasses that of the pioneering networks with a significant margin, the traditional way of appending supervision only over the final classifier and progressively propagating gradient flow upstream remains the training mainstay. Seminal Deeply-Supervised Networks (DSN) were proposed to alleviate the difficulty of optimization arising from gradient flow through a long chain. However, it is still vulnerable to issues including interference to the hierarchical representation generation process and inconsistent optimization objectives, as illustrated theoretically and empirically in this paper. Complementary to previous training strategies, we propose Dynamic Hierarchical Mimicking, a generic feature learning mechanism, to advance CNN training with enhanced generalization ability. Partially inspired by DSN, we fork delicately designed side branches from the intermediate layers of a given neural network. Each branch can emerge from certain locations of the main branch dynamically, which not only retains representation rooted in the backbone network but also generates more diverse representations along its own pathway. We go one step further to promote multi-level interactions among different branches through an optimization formula with probabilistic prediction matching losses, thus guaranteeing a more robust optimization process and better representation ability. Experiments on both category and instance recognition tasks demonstrate the substantial improvements of our proposed method over its corresponding counterparts using diverse state-of-the-art CNN architectures. Code and models are publicly available at https://github.com/d-li14/DHM



### TeCNO: Surgical Phase Recognition with Multi-Stage Temporal Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/2003.10751v1
- **DOI**: 10.1007/978-3-030-59716-0_33
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.10751v1)
- **Published**: 2020-03-24 10:12:30+00:00
- **Updated**: 2020-03-24 10:12:30+00:00
- **Authors**: Tobias Czempiel, Magdalini Paschali, Matthias Keicher, Walter Simson, Hubertus Feussner, Seong Tae Kim, Nassir Navab
- **Comment**: 10 pages, 2 figures
- **Journal**: None
- **Summary**: Automatic surgical phase recognition is a challenging and crucial task with the potential to improve patient safety and become an integral part of intra-operative decision-support systems. In this paper, we propose, for the first time in workflow analysis, a Multi-Stage Temporal Convolutional Network (MS-TCN) that performs hierarchical prediction refinement for surgical phase recognition. Causal, dilated convolutions allow for a large receptive field and online inference with smooth predictions even during ambiguous transitions. Our method is thoroughly evaluated on two datasets of laparoscopic cholecystectomy videos with and without the use of additional surgical tool information. Outperforming various state-of-the-art LSTM approaches, we verify the suitability of the proposed causal MS-TCN for surgical phase recognition.



### Scalable learning for bridging the species gap in image-based plant phenotyping
- **Arxiv ID**: http://arxiv.org/abs/2003.10757v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.10757v2)
- **Published**: 2020-03-24 10:26:40+00:00
- **Updated**: 2020-04-24 02:50:45+00:00
- **Authors**: Daniel Ward, Peyman Moghadam
- **Comment**: Under review. Abstract modified to meed arXiv requirements. Dataset
  available at: https://csiro-robotics.github.io/UPGen_Webpage/
- **Journal**: None
- **Summary**: The traditional paradigm of applying deep learning -- collect, annotate and train on data -- is not applicable to image-based plant phenotyping as almost 400,000 different plant species exists. Data costs include growing physical samples, imaging and labelling them. Model performance is impacted by the species gap between the domain of each plant species, it is not generalisable and may not transfer to unseen plant species. In this paper, we investigate the use of synthetic data for leaf instance segmentation. We study multiple synthetic data training regimes using Mask-RCNN when few or no annotated real data is available. We also present UPGen: a Universal Plant Generator for bridging the species gap. UPGen leverages domain randomisation to produce widely distributed data samples and models stochastic biological variation. Our methods outperform standard practices, such as transfer learning from publicly available plant data, by 26.6% and 51.46% on two unseen plant species respectively. We benchmark UPGen by competing in the CVPPP Leaf Segmentation Challenge and set a new state-of-the-art, a mean of 88% across A1-4 test datasets. This study is applicable to use of synthetic data for automating the measurement of phenotypic traits. Our synthetic dataset and pretrained model are available at https://csiro-robotics.github.io/UPGen_Webpage/.



### FADNet: A Fast and Accurate Network for Disparity Estimation
- **Arxiv ID**: http://arxiv.org/abs/2003.10758v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.10758v1)
- **Published**: 2020-03-24 10:27:11+00:00
- **Updated**: 2020-03-24 10:27:11+00:00
- **Authors**: Qiang Wang, Shaohuai Shi, Shizhen Zheng, Kaiyong Zhao, Xiaowen Chu
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) have achieved great success in the area of computer vision. The disparity estimation problem tends to be addressed by DNNs which achieve much better prediction accuracy in stereo matching than traditional hand-crafted feature based methods. On one hand, however, the designed DNNs require significant memory and computation resources to accurately predict the disparity, especially for those 3D convolution based networks, which makes it difficult for deployment in real-time applications. On the other hand, existing computation-efficient networks lack expression capability in large-scale datasets so that they cannot make an accurate prediction in many scenarios. To this end, we propose an efficient and accurate deep network for disparity estimation named FADNet with three main features: 1) It exploits efficient 2D based correlation layers with stacked blocks to preserve fast computation; 2) It combines the residual structures to make the deeper model easier to learn; 3) It contains multi-scale predictions so as to exploit a multi-scale weight scheduling training technique to improve the accuracy. We conduct experiments to demonstrate the effectiveness of FADNet on two popular datasets, Scene Flow and KITTI 2015. Experimental results show that FADNet achieves state-of-the-art prediction accuracy, and runs at a significant order of magnitude faster speed than existing 3D models. The codes of FADNet are available at https://github.com/HKBU-HPML/FADNet.



### Surface Damage Detection Scheme using Convolutional Neural Network and Artificial Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2003.10760v2
- **DOI**: 10.23919/FUSION45008.2020.9190400
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.10760v2)
- **Published**: 2020-03-24 10:29:02+00:00
- **Updated**: 2020-10-18 14:59:38+00:00
- **Authors**: Alice Yi Yang, Ling Cheng
- **Comment**: None
- **Journal**: None
- **Summary**: Surface damage on concrete is important as the damage can affect the structural integrity of the structure. This paper proposes a two-step surface damage detection scheme using Convolutional Neural Network (CNN) and Artificial Neural Network (ANN). The CNN classifies given input images into two categories: positive and negative. The positive category is where the surface damage is present within the image, otherwise the image is classified as negative. This is an image-based classification. The ANN accepts image inputs that have been classified as positive by the ANN. This reduces the number of images that are further processed by the ANN. The ANN performs feature-based classification, in which the features are extracted from the detected edges within the image. The edges are detected using Canny edge detection. A total of 19 features are extracted from the detected edges. These features are inputs into the ANN. The purpose of the ANN is to highlight only the positive damaged edges within the image. The CNN achieves an accuracy of 80.7% for image classification and the ANN achieves an accuracy of 98.1% for surface detection. The decreased accuracy in the CNN is due to the false positive detection, however false positives are tolerated whereas false negatives are not. The false negative detection for both CNN and ANN in the two-step scheme are 0%.



### Generating Chinese Poetry from Images via Concrete and Abstract Information
- **Arxiv ID**: http://arxiv.org/abs/2003.10773v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.10773v1)
- **Published**: 2020-03-24 11:17:20+00:00
- **Updated**: 2020-03-24 11:17:20+00:00
- **Authors**: Yusen Liu, Dayiheng Liu, Jiancheng Lv, Yongsheng Sang
- **Comment**: Accepted by the 2020 International Joint Conference on Neural
  Networks (IJCNN 2020)
- **Journal**: None
- **Summary**: In recent years, the automatic generation of classical Chinese poetry has made great progress. Besides focusing on improving the quality of the generated poetry, there is a new topic about generating poetry from an image. However, the existing methods for this topic still have the problem of topic drift and semantic inconsistency, and the image-poem pairs dataset is hard to be built when training these models. In this paper, we extract and integrate the Concrete and Abstract information from images to address those issues. We proposed an infilling-based Chinese poetry generation model which can infill the Concrete keywords into each line of poems in an explicit way, and an abstract information embedding to integrate the Abstract information into generated poems. In addition, we use non-parallel data during training and construct separate image datasets and poem datasets to train the different components in our framework. Both automatic and human evaluation results show that our approach can generate poems which have better consistency with images without losing the quality.



### PanNuke Dataset Extension, Insights and Baselines
- **Arxiv ID**: http://arxiv.org/abs/2003.10778v7
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2003.10778v7)
- **Published**: 2020-03-24 11:25:12+00:00
- **Updated**: 2020-04-22 08:52:04+00:00
- **Authors**: Jevgenij Gamper, Navid Alemi Koohbanani, Ksenija Benes, Simon Graham, Mostafa Jahanifar, Syed Ali Khurram, Ayesha Azam, Katherine Hewitt, Nasir Rajpoot
- **Comment**: Work in progress
- **Journal**: None
- **Summary**: The emerging area of computational pathology (CPath) is ripe ground for the application of deep learning (DL) methods to healthcare due to the sheer volume of raw pixel data in whole-slide images (WSIs) of cancerous tissue slides. However, it is imperative for the DL algorithms relying on nuclei-level details to be able to cope with data from `the clinical wild', which tends to be quite challenging.   We study, and extend recently released PanNuke dataset consisting of ~200,000 nuclei categorized into 5 clinically important classes for the challenging tasks of segmenting and classifying nuclei in WSIs. Previous pan-cancer datasets consisted of only up to 9 different tissues and up to 21,000 unlabeled nuclei and just over 24,000 labeled nuclei with segmentation masks. PanNuke consists of 19 different tissue types that have been semi-automatically annotated and quality controlled by clinical pathologists, leading to a dataset with statistics similar to the clinical wild and with minimal selection bias. We study the performance of segmentation and classification models when applied to the proposed dataset and demonstrate the application of models trained on PanNuke to whole-slide images. We provide comprehensive statistics about the dataset and outline recommendations and research directions to address the limitations of existing DL tools when applied to real-world CPath applications.



### Rethinking Class-Balanced Methods for Long-Tailed Visual Recognition from a Domain Adaptation Perspective
- **Arxiv ID**: http://arxiv.org/abs/2003.10780v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.10780v1)
- **Published**: 2020-03-24 11:28:42+00:00
- **Updated**: 2020-03-24 11:28:42+00:00
- **Authors**: Muhammad Abdullah Jamal, Matthew Brown, Ming-Hsuan Yang, Liqiang Wang, Boqing Gong
- **Comment**: Accepted for publication at CVPR2020
- **Journal**: None
- **Summary**: Object frequency in the real world often follows a power law, leading to a mismatch between datasets with long-tailed class distributions seen by a machine learning model and our expectation of the model to perform well on all classes. We analyze this mismatch from a domain adaptation point of view. First of all, we connect existing class-balanced methods for long-tailed classification to target shift, a well-studied scenario in domain adaptation. The connection reveals that these methods implicitly assume that the training data and test data share the same class-conditioned distribution, which does not hold in general and especially for the tail classes. While a head class could contain abundant and diverse training examples that well represent the expected data at inference time, the tail classes are often short of representative training data. To this end, we propose to augment the classic class-balanced learning by explicitly estimating the differences between the class-conditioned distributions with a meta-learning approach. We validate our approach with six benchmark datasets and three loss functions.



### Dataset Cleaning -- A Cross Validation Methodology for Large Facial Datasets using Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2003.10815v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DB
- **Links**: [PDF](http://arxiv.org/pdf/2003.10815v1)
- **Published**: 2020-03-24 13:01:13+00:00
- **Updated**: 2020-03-24 13:01:13+00:00
- **Authors**: Viktor Varkarakis, Peter Corcoran
- **Comment**: 2020 Twelfth International Conference on Quality of Multimedia
  Experience (QoMEX)
- **Journal**: None
- **Summary**: In recent years, large "in the wild" face datasets have been released in an attempt to facilitate progress in tasks such as face detection, face recognition, and other tasks. Most of these datasets are acquired from webpages with automatic procedures. As a consequence, noisy data are often found. Furthermore, in these large face datasets, the annotation of identities is important as they are used for training face recognition algorithms. But due to the automatic way of gathering these datasets and due to their large size, many identities folder contain mislabeled samples which deteriorates the quality of the datasets. In this work, it is presented a semi-automatic method for cleaning the noisy large face datasets with the use of face recognition. This methodology is applied to clean the CelebA dataset and show its effectiveness. Furthermore, the list with the mislabelled samples in the CelebA dataset is made available.



### Registration by tracking for sequential 2D MRI
- **Arxiv ID**: http://arxiv.org/abs/2003.10819v1
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.10819v1)
- **Published**: 2020-03-24 13:12:42+00:00
- **Updated**: 2020-03-24 13:12:42+00:00
- **Authors**: Niklas Gunnarsson, Jens Sjölund, Thomas B. Schön
- **Comment**: Currently under review for a conference
- **Journal**: None
- **Summary**: Our anatomy is in constant motion. With modern MR imaging it is possible to record this motion in real-time during an ongoing radiation therapy session. In this paper we present an image registration method that exploits the sequential nature of 2D MR images to estimate the corresponding displacement field. The method employs several discriminative correlation filters that independently track specific points. Together with a sparse-to-dense interpolation scheme we can then estimate of the displacement field. The discriminative correlation filters are trained online, and our method is modality agnostic. For the interpolation scheme we use a neural network with normalized convolutions that is trained using synthetic diffeomorphic displacement fields. The method is evaluated on a segmented cardiac dataset and when compared to two conventional methods we observe an improved performance. This improvement is especially pronounced when it comes to the detection of larger motions of small objects.



### Re-Training StyleGAN -- A First Step Towards Building Large, Scalable Synthetic Facial Datasets
- **Arxiv ID**: http://arxiv.org/abs/2003.10847v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.10847v1)
- **Published**: 2020-03-24 13:47:07+00:00
- **Updated**: 2020-03-24 13:47:07+00:00
- **Authors**: Viktor Varkarakis, Shabab Bazrafkan, Peter Corcoran
- **Comment**: None
- **Journal**: None
- **Summary**: StyleGAN is a state-of-art generative adversarial network architecture that generates random 2D high-quality synthetic facial data samples. In this paper, we recap the StyleGAN architecture and training methodology and present our experiences of retraining it on a number of alternative public datasets. Practical issues and challenges arising from the retraining process are discussed. Tests and validation results are presented and a comparative analysis of several different re-trained StyleGAN weightings is provided 1. The role of this tool in building large, scalable datasets of synthetic facial data is also discussed.



### Automatic Detection of Coronavirus Disease (COVID-19) Using X-ray Images and Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2003.10849v3
- **DOI**: 10.1007/s10044-021-00984-y
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.10849v3)
- **Published**: 2020-03-24 13:50:23+00:00
- **Updated**: 2020-10-05 08:29:07+00:00
- **Authors**: Ali Narin, Ceren Kaya, Ziynet Pamuk
- **Comment**: The manuscript has 31 pages, 12 figures and 5 tables
- **Journal**: Pattern Analysis and Applications, 24(3):1207-1220, 2021
- **Summary**: The 2019 novel coronavirus disease (COVID-19), with a starting point in China, has spread rapidly among people living in other countries, and is approaching approximately 34,986,502 cases worldwide according to the statistics of European Centre for Disease Prevention and Control. There are a limited number of COVID-19 test kits available in hospitals due to the increasing cases daily. Therefore, it is necessary to implement an automatic detection system as a quick alternative diagnosis option to prevent COVID-19 spreading among people. In this study, five pre-trained convolutional neural network based models (ResNet50, ResNet101, ResNet152, InceptionV3 and Inception-ResNetV2) have been proposed for the detection of coronavirus pneumonia infected patient using chest X-ray radiographs. We have implemented three different binary classifications with four classes (COVID-19, normal (healthy), viral pneumonia and bacterial pneumonia) by using 5-fold cross validation. Considering the performance results obtained, it has seen that the pre-trained ResNet50 model provides the highest classification performance (96.1% accuracy for Dataset-1, 99.5% accuracy for Dataset-2 and 99.7% accuracy for Dataset-3) among other four used models.



### Monocular Human Pose and Shape Reconstruction using Part Differentiable Rendering
- **Arxiv ID**: http://arxiv.org/abs/2003.10873v2
- **DOI**: 10.1111/cgf.14150
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.10873v2)
- **Published**: 2020-03-24 14:25:46+00:00
- **Updated**: 2021-01-29 08:57:26+00:00
- **Authors**: Min Wang, Feng Qiu, Wentao Liu, Chen Qian, Xiaowei Zhou, Lizhuang Ma
- **Comment**: Accepted by Pacific Graphcis 2020
- **Journal**: In Computer Graphics Forum, vol. 39, no. 7, pp. 351-362. 2020
- **Summary**: Superior human pose and shape reconstruction from monocular images depends on removing the ambiguities caused by occlusions and shape variance. Recent works succeed in regression-based methods which estimate parametric models directly through a deep neural network supervised by 3D ground truth. However, 3D ground truth is neither in abundance nor can efficiently be obtained. In this paper, we introduce body part segmentation as critical supervision. Part segmentation not only indicates the shape of each body part but helps to infer the occlusions among parts as well. To improve the reconstruction with part segmentation, we propose a part-level differentiable renderer that enables part-based models to be supervised by part segmentation in neural networks or optimization loops. We also introduce a general parametric model engaged in the rendering pipeline as an intermediate representation between skeletons and detailed shapes, which consists of primitive geometries for better interpretability. The proposed approach combines parameter regression, body model optimization, and detailed model registration altogether. Experimental results demonstrate that the proposed method achieves balanced evaluation on pose and shape, and outperforms the state-of-the-art approaches on Human3.6M, UP-3D and LSP datasets.



### Do We Need Depth in State-Of-The-Art Face Authentication?
- **Arxiv ID**: http://arxiv.org/abs/2003.10895v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.10895v2)
- **Published**: 2020-03-24 14:51:25+00:00
- **Updated**: 2020-11-10 11:52:04+00:00
- **Authors**: Amir Livne, Alex Bronstein, Ron Kimmel, Ziv Aviv, Shahaf Grofit
- **Comment**: None
- **Journal**: None
- **Summary**: Some face recognition methods are designed to utilize geometric information extracted from depth sensors to overcome the weaknesses of single-image based recognition technologies. However, the accurate acquisition of the depth profile is an expensive and challenging process. Here, we introduce a novel method that learns to recognize faces from stereo camera systems without the need to explicitly compute the facial surface or depth map. The raw face stereo images along with the location in the image from which the face is extracted allow the proposed CNN to improve the recognition task while avoiding the need to explicitly handle the geometric structure of the face. This way, we keep the simplicity and cost efficiency of identity authentication from a single image, while enjoying the benefits of geometric data without explicitly reconstructing it. We demonstrate that the suggested method outperforms both existing single-image and explicit depth based methods on large-scale benchmarks, and even capable of recognize spoofing attacks. We also provide an ablation study that shows that the suggested method uses the face locations in the left and right images to encode informative features that improve the overall performance.



### RN-VID: A Feature Fusion Architecture for Video Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2003.10898v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.10898v2)
- **Published**: 2020-03-24 14:54:46+00:00
- **Updated**: 2020-04-02 15:53:28+00:00
- **Authors**: Hughes Perreault, Maguelonne Héritier, Pierre Gravel, Guillaume-Alexandre Bilodeau, Nicolas Saunier
- **Comment**: None
- **Journal**: None
- **Summary**: Consecutive frames in a video are highly redundant. Therefore, to perform the task of video object detection, executing single frame detectors on every frame without reusing any information is quite wasteful. It is with this idea in mind that we propose RN-VID (standing for RetinaNet-VIDeo), a novel approach to video object detection. Our contributions are twofold. First, we propose a new architecture that allows the usage of information from nearby frames to enhance feature maps. Second, we propose a novel module to merge feature maps of same dimensions using re-ordering of channels and 1 x 1 convolutions. We then demonstrate that RN-VID achieves better mean average precision (mAP) than corresponding single frame detectors with little additional cost during inference.



### Learning Compact Reward for Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/2003.10925v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2003.10925v1)
- **Published**: 2020-03-24 15:31:05+00:00
- **Updated**: 2020-03-24 15:31:05+00:00
- **Authors**: Nannan Li, Zhenzhong Chen
- **Comment**: 13 pages, 10 figures
- **Journal**: None
- **Summary**: Adversarial learning has shown its advances in generating natural and diverse descriptions in image captioning. However, the learned reward of existing adversarial methods is vague and ill-defined due to the reward ambiguity problem. In this paper, we propose a refined Adversarial Inverse Reinforcement Learning (rAIRL) method to handle the reward ambiguity problem by disentangling reward for each word in a sentence, as well as achieve stable adversarial training by refining the loss function to shift the generator towards Nash equilibrium. In addition, we introduce a conditional term in the loss function to mitigate mode collapse and to increase the diversity of the generated descriptions. Our experiments on MS COCO and Flickr30K show that our method can learn compact reward for image captioning.



### MaskFlownet: Asymmetric Feature Matching with Learnable Occlusion Mask
- **Arxiv ID**: http://arxiv.org/abs/2003.10955v2
- **DOI**: 10.1109/CVPR42600.2020.00631
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.10955v2)
- **Published**: 2020-03-24 16:29:21+00:00
- **Updated**: 2020-04-08 16:00:20+00:00
- **Authors**: Shengyu Zhao, Yilun Sheng, Yue Dong, Eric I-Chao Chang, Yan Xu
- **Comment**: CVPR 2020 (Oral)
- **Journal**: Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR), 2020, pp. 6278-6287
- **Summary**: Feature warping is a core technique in optical flow estimation; however, the ambiguity caused by occluded areas during warping is a major problem that remains unsolved. In this paper, we propose an asymmetric occlusion-aware feature matching module, which can learn a rough occlusion mask that filters useless (occluded) areas immediately after feature warping without any explicit supervision. The proposed module can be easily integrated into end-to-end network architectures and enjoys performance gains while introducing negligible computational cost. The learned occlusion mask can be further fed into a subsequent network cascade with dual feature pyramids with which we achieve state-of-the-art performance. At the time of submission, our method, called MaskFlownet, surpasses all published optical flow methods on the MPI Sintel, KITTI 2012 and 2015 benchmarks. Code is available at https://github.com/microsoft/MaskFlownet.



### Learning to Exploit Multiple Vision Modalities by Using Grafted Networks
- **Arxiv ID**: http://arxiv.org/abs/2003.10959v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.10959v3)
- **Published**: 2020-03-24 16:37:52+00:00
- **Updated**: 2020-07-22 11:35:18+00:00
- **Authors**: Yuhuang Hu, Tobi Delbruck, Shih-Chii Liu
- **Comment**: Accepted at ECCV 2020, 14 pages
- **Journal**: None
- **Summary**: Novel vision sensors such as thermal, hyperspectral, polarization, and event cameras provide information that is not available from conventional intensity cameras. An obstacle to using these sensors with current powerful deep neural networks is the lack of large labeled training datasets. This paper proposes a Network Grafting Algorithm (NGA), where a new front end network driven by unconventional visual inputs replaces the front end network of a pretrained deep network that processes intensity frames. The self-supervised training uses only synchronously-recorded intensity frames and novel sensor data to maximize feature similarity between the pretrained network and the grafted network. We show that the enhanced grafted network reaches competitive average precision (AP50) scores to the pretrained network on an object detection task using thermal and event camera datasets, with no increase in inference costs. Particularly, the grafted network driven by thermal frames showed a relative improvement of 49.11% over the use of intensity frames. The grafted front end has only 5--8% of the total parameters and can be trained in a few hours on a single GPU equivalent to 5% of the time that would be needed to train the entire object detector from labeled data. NGA allows new vision sensors to capitalize on previously pretrained powerful deep models, saving on training cost and widening a range of applications for novel sensors.



### Deep Local Shapes: Learning Local SDF Priors for Detailed 3D Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2003.10983v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.10983v3)
- **Published**: 2020-03-24 17:21:50+00:00
- **Updated**: 2020-08-21 21:52:09+00:00
- **Authors**: Rohan Chabra, Jan Eric Lenssen, Eddy Ilg, Tanner Schmidt, Julian Straub, Steven Lovegrove, Richard Newcombe
- **Comment**: Accepted at ECCV 2020
- **Journal**: None
- **Summary**: Efficiently reconstructing complex and intricate surfaces at scale is a long-standing goal in machine perception. To address this problem we introduce Deep Local Shapes (DeepLS), a deep shape representation that enables encoding and reconstruction of high-quality 3D shapes without prohibitive memory requirements. DeepLS replaces the dense volumetric signed distance function (SDF) representation used in traditional surface reconstruction systems with a set of locally learned continuous SDFs defined by a neural network, inspired by recent work such as DeepSDF. Unlike DeepSDF, which represents an object-level SDF with a neural network and a single latent code, we store a grid of independent latent codes, each responsible for storing information about surfaces in a small local neighborhood. This decomposition of scenes into local shapes simplifies the prior distribution that the network must learn, and also enables efficient inference. We demonstrate the effectiveness and generalization power of DeepLS by showing object shape encoding and reconstructions of full scenes, where DeepLS delivers high compression, accuracy, and local shape completion.



### Multi-Scale Progressive Fusion Network for Single Image Deraining
- **Arxiv ID**: http://arxiv.org/abs/2003.10985v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.10985v2)
- **Published**: 2020-03-24 17:22:37+00:00
- **Updated**: 2020-03-28 17:05:34+00:00
- **Authors**: Kui Jiang, Zhongyuan Wang, Peng Yi, Chen Chen, Baojin Huang, Yimin Luo, Jiayi Ma, Junjun Jiang
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: Rain streaks in the air appear in various blurring degrees and resolutions due to different distances from their positions to the camera. Similar rain patterns are visible in a rain image as well as its multi-scale (or multi-resolution) versions, which makes it possible to exploit such complementary information for rain streak representation. In this work, we explore the multi-scale collaborative representation for rain streaks from the perspective of input image scales and hierarchical deep features in a unified framework, termed multi-scale progressive fusion network (MSPFN) for single image rain streak removal. For similar rain streaks at different positions, we employ recurrent calculation to capture the global texture, thus allowing to explore the complementary and redundant information at the spatial dimension to characterize target rain streaks. Besides, we construct multi-scale pyramid structure, and further introduce the attention mechanism to guide the fine fusion of this correlated information from different scales. This multi-scale progressive fusion strategy not only promotes the cooperative representation, but also boosts the end-to-end training. Our proposed method is extensively evaluated on several benchmark datasets and achieves state-of-the-art results. Moreover, we conduct experiments on joint deraining, detection, and segmentation tasks, and inspire a new research direction of vision task-driven image deraining. The source code is available at \url{https://github.com/kuihua/MSPFN}.



### Generalizing Spatial Transformers to Projective Geometry with Applications to 2D/3D Registration
- **Arxiv ID**: http://arxiv.org/abs/2003.10987v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.10987v1)
- **Published**: 2020-03-24 17:26:50+00:00
- **Updated**: 2020-03-24 17:26:50+00:00
- **Authors**: Cong Gao, Xingtong Liu, Wenhao Gu, Benjamin Killeen, Mehran Armand, Russell Taylor, Mathias Unberath
- **Comment**: None
- **Journal**: None
- **Summary**: Differentiable rendering is a technique to connect 3D scenes with corresponding 2D images. Since it is differentiable, processes during image formation can be learned. Previous approaches to differentiable rendering focus on mesh-based representations of 3D scenes, which is inappropriate for medical applications where volumetric, voxelized models are used to represent anatomy. We propose a novel Projective Spatial Transformer module that generalizes spatial transformers to projective geometry, thus enabling differentiable volume rendering. We demonstrate the usefulness of this architecture on the example of 2D/3D registration between radiographs and CT scans. Specifically, we show that our transformer enables end-to-end learning of an image processing and projection model that approximates an image similarity function that is convex with respect to the pose parameters, and can thus be optimized effectively using conventional gradient descent. To the best of our knowledge, this is the first time that spatial transformers have been described for projective geometry. The source code will be made public upon publication of this manuscript and we hope that our developments will benefit related 3D research applications.



### Hybrid Classification and Reasoning for Image-based Constraint Solving
- **Arxiv ID**: http://arxiv.org/abs/2003.11001v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.11001v1)
- **Published**: 2020-03-24 17:39:49+00:00
- **Updated**: 2020-03-24 17:39:49+00:00
- **Authors**: Maxime Mulamba, Jayanta Mandi, Rocsildes Canoy, Tias Guns
- **Comment**: None
- **Journal**: None
- **Summary**: There is an increased interest in solving complex constrained problems where part of the input is not given as facts but received as raw sensor data such as images or speech. We will use "visual sudoku" as a prototype problem, where the given cell digits are handwritten and provided as an image thereof. In this case, one first has to train and use a classifier to label the images, so that the labels can be used for solving the problem. In this paper, we explore the hybridization of classifying the images with the reasoning of a constraint solver. We show that pure constraint reasoning on predictions does not give satisfactory results. Instead, we explore the possibilities of a tighter integration, by exposing the probabilistic estimates of the classifier to the constraint solver. This allows joint inference on these probabilistic estimates, where we use the solver to find the maximum likelihood solution. We explore the trade-off between the power of the classifier and the power of the constraint reasoning, as well as further integration through the additional use of structural knowledge. Furthermore, we investigate the effect of calibration of the probabilistic estimates on the reasoning. Our results show that such hybrid approaches vastly outperform a separate approach, which encourages a further integration of prediction (probabilities) and constraint solving.



### Learning to Reconstruct Confocal Microscopy Stacks from Single Light Field Images
- **Arxiv ID**: http://arxiv.org/abs/2003.11004v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/2003.11004v1)
- **Published**: 2020-03-24 17:46:03+00:00
- **Updated**: 2020-03-24 17:46:03+00:00
- **Authors**: Josue Page, Federico Saltarin, Yury Belyaev, Ruth Lyck, Paolo Favaro
- **Comment**: 22 pages, 12 figures
- **Journal**: None
- **Summary**: We present a novel deep learning approach to reconstruct confocal microscopy stacks from single light field images. To perform the reconstruction, we introduce the LFMNet, a novel neural network architecture inspired by the U-Net design. It is able to reconstruct with high-accuracy a 112x112x57.6$\mu m^3$ volume (1287x1287x64 voxels) in 50ms given a single light field image of 1287x1287 pixels, thus dramatically reducing 720-fold the time for confocal scanning of assays at the same volumetric resolution and 64-fold the required storage. To prove the applicability in life sciences, our approach is evaluated both quantitatively and qualitatively on mouse brain slices with fluorescently labelled blood vessels. Because of the drastic reduction in scan time and storage space, our setup and method are directly applicable to real-time in vivo 3D microscopy. We provide analysis of the optical design, of the network architecture and of our training procedure to optimally reconstruct volumes for a given target depth range. To train our network, we built a data set of 362 light field images of mouse brain blood vessels and the corresponding aligned set of 3D confocal scans, which we use as ground truth. The data set will be made available for research purposes.



### Tractogram filtering of anatomically non-plausible fibers with geometric deep learning
- **Arxiv ID**: http://arxiv.org/abs/2003.11013v2
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.11013v2)
- **Published**: 2020-03-24 17:56:44+00:00
- **Updated**: 2020-07-09 13:57:11+00:00
- **Authors**: Pietro Astolfi, Ruben Verhagen, Laurent Petit, Emanuele Olivetti, Jonathan Masci, Davide Boscaini, Paolo Avesani
- **Comment**: Accepted at MICCAI2020
- **Journal**: None
- **Summary**: Tractograms are virtual representations of the white matter fibers of the brain. They are of primary interest for tasks like presurgical planning, and investigation of neuroplasticity or brain disorders. Each tractogram is composed of millions of fibers encoded as 3D polylines. Unfortunately, a large portion of those fibers are not anatomically plausible and can be considered artifacts of the tracking algorithms. Common methods for tractogram filtering are based on signal reconstruction, a principled approach, but unable to consider the knowledge of brain anatomy. In this work, we address the problem of tractogram filtering as a supervised learning problem by exploiting the ground truth annotations obtained with a recent heuristic method, which labels fibers as either anatomically plausible or non-plausible according to well-established anatomical properties. The intuitive idea is to model a fiber as a point cloud and the goal is to investigate whether and how a geometric deep learning model might capture its anatomical properties. Our contribution is an extension of the Dynamic Edge Convolution model that exploits the sequential relations of points in a fiber and discriminates with high accuracy plausible/non-plausible fibers.



### Know Your Surroundings: Exploiting Scene Information for Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2003.11014v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.11014v2)
- **Published**: 2020-03-24 17:59:04+00:00
- **Updated**: 2020-05-01 16:15:51+00:00
- **Authors**: Goutam Bhat, Martin Danelljan, Luc Van Gool, Radu Timofte
- **Comment**: None
- **Journal**: None
- **Summary**: Current state-of-the-art trackers only rely on a target appearance model in order to localize the object in each frame. Such approaches are however prone to fail in case of e.g. fast appearance changes or presence of distractor objects, where a target appearance model alone is insufficient for robust tracking. Having the knowledge about the presence and locations of other objects in the surrounding scene can be highly beneficial in such cases. This scene information can be propagated through the sequence and used to, for instance, explicitly avoid distractor objects and eliminate target candidate regions.   In this work, we propose a novel tracking architecture which can utilize scene information for tracking. Our tracker represents such information as dense localized state vectors, which can encode, for example, if the local region is target, background, or distractor. These state vectors are propagated through the sequence and combined with the appearance model output to localize the target. Our network is learned to effectively utilize the scene information by directly maximizing tracking performance on video segments. The proposed approach sets a new state-of-the-art on 3 tracking benchmarks, achieving an AO score of 63.6% on the recent GOT-10k dataset.



### Deformable Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/2003.11038v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.11038v2)
- **Published**: 2020-03-24 18:00:18+00:00
- **Updated**: 2020-07-20 02:50:28+00:00
- **Authors**: Sunnie S. Y. Kim, Nicholas Kolkin, Jason Salavon, Gregory Shakhnarovich
- **Comment**: ECCV 2020 (21 pages, 11 figures including the supplementary material)
- **Journal**: None
- **Summary**: Both geometry and texture are fundamental aspects of visual style. Existing style transfer methods, however, primarily focus on texture, almost entirely ignoring geometry. We propose deformable style transfer (DST), an optimization-based approach that jointly stylizes the texture and geometry of a content image to better match a style image. Unlike previous geometry-aware stylization methods, our approach is neither restricted to a particular domain (such as human faces), nor does it require training sets of matching style/content pairs. We demonstrate our method on a diverse set of content and style images including portraits, animals, objects, scenes, and paintings. Code has been made publicly available at https://github.com/sunniesuhyoung/DST.



### COVIDX-Net: A Framework of Deep Learning Classifiers to Diagnose COVID-19 in X-Ray Images
- **Arxiv ID**: http://arxiv.org/abs/2003.11055v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.11055v1)
- **Published**: 2020-03-24 18:21:10+00:00
- **Updated**: 2020-03-24 18:21:10+00:00
- **Authors**: Ezz El-Din Hemdan, Marwa A. Shouman, Mohamed Esmail Karar
- **Comment**: 14 pages, 5 figures, 3 tables
- **Journal**: None
- **Summary**: Background and Purpose: Coronaviruses (CoV) are perilous viruses that may cause Severe Acute Respiratory Syndrome (SARS-CoV), Middle East Respiratory Syndrome (MERS-CoV). The novel 2019 Coronavirus disease (COVID-19) was discovered as a novel disease pneumonia in the city of Wuhan, China at the end of 2019. Now, it becomes a Coronavirus outbreak around the world, the number of infected people and deaths are increasing rapidly every day according to the updated reports of the World Health Organization (WHO). Therefore, the aim of this article is to introduce a new deep learning framework; namely COVIDX-Net to assist radiologists to automatically diagnose COVID-19 in X-ray images. Materials and Methods: Due to the lack of public COVID-19 datasets, the study is validated on 50 Chest X-ray images with 25 confirmed positive COVID-19 cases. The COVIDX-Net includes seven different architectures of deep convolutional neural network models, such as modified Visual Geometry Group Network (VGG19) and the second version of Google MobileNet. Each deep neural network model is able to analyze the normalized intensities of the X-ray image to classify the patient status either negative or positive COVID-19 case. Results: Experiments and evaluation of the COVIDX-Net have been successfully done based on 80-20% of X-ray images for the model training and testing phases, respectively. The VGG19 and Dense Convolutional Network (DenseNet) models showed a good and similar performance of automated COVID-19 classification with f1-scores of 0.89 and 0.91 for normal and COVID-19, respectively. Conclusions: This study demonstrated the useful application of deep learning models to classify COVID-19 in X-ray images based on the proposed COVIDX-Net framework. Clinical studies are the next milestone of this research work.



### ML-SIM: A deep neural network for reconstruction of structured illumination microscopy images
- **Arxiv ID**: http://arxiv.org/abs/2003.11064v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/2003.11064v1)
- **Published**: 2020-03-24 18:42:23+00:00
- **Updated**: 2020-03-24 18:42:23+00:00
- **Authors**: Charles N. Christensen, Edward N. Ward, Pietro Lio, Clemens F. Kaminski
- **Comment**: None
- **Journal**: None
- **Summary**: Structured illumination microscopy (SIM) has become an important technique for optical super-resolution imaging because it allows a doubling of image resolution at speeds compatible for live-cell imaging. However, the reconstruction of SIM images is often slow and prone to artefacts. Here we propose a versatile reconstruction method, ML-SIM, which makes use of machine learning. The model is an end-to-end deep residual neural network that is trained on a simulated data set to be free of common SIM artefacts. ML-SIM is thus robust to noise and irregularities in the illumination patterns of the raw SIM input frames. The reconstruction method is widely applicable and does not require the acquisition of experimental training data. Since the training data are generated from simulations of the SIM process on images from generic libraries the method can be efficiently adapted to specific experimental SIM implementations. The reconstruction quality enabled by our method is compared with traditional SIM reconstruction methods, and we demonstrate advantages in terms of noise, reconstruction fidelity and contrast for both simulated and experimental inputs. In addition, reconstruction of one SIM frame typically only takes ~100ms to perform on PCs with modern Nvidia graphics cards, making the technique compatible with real-time imaging. The full implementation and the trained networks are available at http://ML-SIM.com.



### A Survey of Methods for Low-Power Deep Learning and Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/2003.11066v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.11066v1)
- **Published**: 2020-03-24 18:47:24+00:00
- **Updated**: 2020-03-24 18:47:24+00:00
- **Authors**: Abhinav Goel, Caleb Tung, Yung-Hsiang Lu, George K. Thiruvathukal
- **Comment**: Accepted for publication at 2020 IEEE 6th World Forum on Internet of
  Things (WF-IoT), New Orleans, LA, USA 2020
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) are successful in many computer vision tasks. However, the most accurate DNNs require millions of parameters and operations, making them energy, computation and memory intensive. This impedes the deployment of large DNNs in low-power devices with limited compute resources. Recent research improves DNN models by reducing the memory requirement, energy consumption, and number of operations without significantly decreasing the accuracy. This paper surveys the progress of low-power deep learning and computer vision, specifically in regards to inference, and discusses the methods for compacting and accelerating DNN models. The techniques can be divided into four major categories: (1) parameter quantization and pruning, (2) compressed convolutional filters and matrix factorization, (3) network architecture search, and (4) knowledge distillation. We analyze the accuracy, advantages, disadvantages, and potential solutions to the problems with the techniques in each category. We also discuss new evaluation metrics as a guideline for future research.



### Removing Dynamic Objects for Static Scene Reconstruction using Light Fields
- **Arxiv ID**: http://arxiv.org/abs/2003.11076v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.11076v1)
- **Published**: 2020-03-24 19:05:17+00:00
- **Updated**: 2020-03-24 19:05:17+00:00
- **Authors**: Pushyami Kaveti, Sammie Katt, Hanumant Singh
- **Comment**: None
- **Journal**: None
- **Summary**: There is a general expectation that robots should operate in environments that consist of static and dynamic entities including people, furniture and automobiles. These dynamic environments pose challenges to visual simultaneous localization and mapping (SLAM) algorithms by introducing errors into the front-end. Light fields provide one possible method for addressing such problems by capturing a more complete visual information of a scene. In contrast to a single ray from a perspective camera, Light Fields capture a bundle of light rays emerging from a single point in space, allowing us to see through dynamic objects by refocusing past them.   In this paper we present a method to synthesize a refocused image of the static background in the presence of dynamic objects that uses a light-field acquired with a linear camera array. We simultaneously estimate both the depth and the refocused image of the static scene using semantic segmentation for detecting dynamic objects in a single time step. This eliminates the need for initializing a static map . The algorithm is parallelizable and is implemented on GPU allowing us execute it at close to real time speeds. We demonstrate the effectiveness of our method on real-world data acquired using a small robot with a five camera array.



### Bootstrapping Weakly Supervised Segmentation-free Word Spotting through HMM-based Alignment
- **Arxiv ID**: http://arxiv.org/abs/2003.11087v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.11087v1)
- **Published**: 2020-03-24 19:41:18+00:00
- **Updated**: 2020-03-24 19:41:18+00:00
- **Authors**: Tomas Wilkinson, Carl Nettelblad
- **Comment**: None
- **Journal**: None
- **Summary**: Recent work in word spotting in handwritten documents has yielded impressive results. This progress has largely been made by supervised learning systems, which are dependent on manually annotated data, making deployment to new collections a significant effort. In this paper, we propose an approach that utilises transcripts without bounding box annotations to train segmentation-free query-by-string word spotting models, given a partially trained model. This is done through a training-free alignment procedure based on hidden Markov models. This procedure creates a tentative mapping between word region proposals and the transcriptions to automatically create additional weakly annotated training data, without choosing any single alignment possibility as the correct one. When only using between 1% and 7% of the fully annotated training sets for partial convergence, we automatically annotate the remaining training data and successfully train using it. On all our datasets, our final trained model then comes within a few mAP% of the performance from a model trained with the full training set used as ground truth. We believe that this will be a significant advance towards a more general use of word spotting, since digital transcription data will already exist for parts of many collections of interest.



### G2L-Net: Global to Local Network for Real-time 6D Pose Estimation with Embedding Vector Features
- **Arxiv ID**: http://arxiv.org/abs/2003.11089v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.11089v2)
- **Published**: 2020-03-24 19:42:24+00:00
- **Updated**: 2020-03-26 08:36:23+00:00
- **Authors**: Wei Chen, Xi Jia, Hyung Jin Chang, Jinming Duan, Ales Leonardis
- **Comment**: 10 pages, 11 figures, accepted in CVPR 2020
- **Journal**: None
- **Summary**: In this paper, we propose a novel real-time 6D object pose estimation framework, named G2L-Net. Our network operates on point clouds from RGB-D detection in a divide-and-conquer fashion. Specifically, our network consists of three steps. First, we extract the coarse object point cloud from the RGB-D image by 2D detection. Second, we feed the coarse object point cloud to a translation localization network to perform 3D segmentation and object translation prediction. Third, via the predicted segmentation and translation, we transfer the fine object point cloud into a local canonical coordinate, in which we train a rotation localization network to estimate initial object rotation. In the third step, we define point-wise embedding vector features to capture viewpoint-aware information. To calculate more accurate rotation, we adopt a rotation residual estimator to estimate the residual between initial rotation and ground truth, which can boost initial pose estimation performance. Our proposed G2L-Net is real-time despite the fact multiple steps are stacked via the proposed coarse-to-fine framework. Extensive experiments on two benchmark datasets show that G2L-Net achieves state-of-the-art performance in terms of both accuracy and speed.



### How deep is your encoder: an analysis of features descriptors for an autoencoder-based audio-visual quality metric
- **Arxiv ID**: http://arxiv.org/abs/2003.11100v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.11100v1)
- **Published**: 2020-03-24 20:15:12+00:00
- **Updated**: 2020-03-24 20:15:12+00:00
- **Authors**: Helard Martinez, Andrew Hines, Mylene C. Q. Farias
- **Comment**: None
- **Journal**: None
- **Summary**: The development of audio-visual quality assessment models poses a number of challenges in order to obtain accurate predictions. One of these challenges is the modelling of the complex interaction that audio and visual stimuli have and how this interaction is interpreted by human users. The No-Reference Audio-Visual Quality Metric Based on a Deep Autoencoder (NAViDAd) deals with this problem from a machine learning perspective. The metric receives two sets of audio and video features descriptors and produces a low-dimensional set of features used to predict the audio-visual quality. A basic implementation of NAViDAd was able to produce accurate predictions tested with a range of different audio-visual databases. The current work performs an ablation study on the base architecture of the metric. Several modules are removed or re-trained using different configurations to have a better understanding of the metric functionality. The results presented in this study provided important feedback that allows us to understand the real capacity of the metric's architecture and eventually develop a much better audio-visual quality metric.



### PoisHygiene: Detecting and Mitigating Poisoning Attacks in Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2003.11110v2
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.11110v2)
- **Published**: 2020-03-24 20:55:08+00:00
- **Updated**: 2020-06-19 20:03:54+00:00
- **Authors**: Junfeng Guo, Ting Wang, Cong Liu
- **Comment**: None
- **Journal**: None
- **Summary**: The black-box nature of deep neural networks (DNNs) facilitates attackers to manipulate the behavior of DNN through data poisoning. Being able to detect and mitigate poisoning attacks, typically categorized into backdoor and adversarial poisoning (AP), is critical in enabling safe adoption of DNNs in many application domains. Although recent works demonstrate encouraging results on detection of certain backdoor attacks, they exhibit inherent limitations which may significantly constrain the applicability. Indeed, no technique exists for detecting AP attacks, which represents a harder challenge given that such attacks exhibit no common and explicit rules while backdoor attacks do (i.e., embedding backdoor triggers into poisoned data). We believe the key to detect and mitigate AP attacks is the capability of observing and leveraging essential poisoning-induced properties within an infected DNN model. In this paper, we present PoisHygiene, the first effective and robust detection and mitigation framework against AP attacks. PoisHygiene is fundamentally motivated by Dr. Ernest Rutherford's story (i.e., the 1908 Nobel Prize winner), on observing the structure of atom through random electron sampling.



### PADS: Policy-Adapted Sampling for Visual Similarity Learning
- **Arxiv ID**: http://arxiv.org/abs/2003.11113v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.11113v2)
- **Published**: 2020-03-24 21:01:07+00:00
- **Updated**: 2020-03-28 12:56:16+00:00
- **Authors**: Karsten Roth, Timo Milbich, Björn Ommer
- **Comment**: Accepted to CVPR2020
- **Journal**: None
- **Summary**: Learning visual similarity requires to learn relations, typically between triplets of images. Albeit triplet approaches being powerful, their computational complexity mostly limits training to only a subset of all possible training triplets. Thus, sampling strategies that decide when to use which training sample during learning are crucial. Currently, the prominent paradigm are fixed or curriculum sampling strategies that are predefined before training starts. However, the problem truly calls for a sampling process that adjusts based on the actual state of the similarity representation during training. We, therefore, employ reinforcement learning and have a teacher network adjust the sampling distribution based on the current state of the learner network, which represents visual similarity. Experiments on benchmark datasets using standard triplet-based losses show that our adaptive sampling strategy significantly outperforms fixed sampling strategies. Moreover, although our adaptive sampling is only applied on top of basic triplet-learning frameworks, we reach competitive results to state-of-the-art approaches that employ diverse additional learning signals or strong ensemble architectures. Code can be found under https://github.com/Confusezius/CVPR2020_PADS.



### Joint Deep Cross-Domain Transfer Learning for Emotion Recognition
- **Arxiv ID**: http://arxiv.org/abs/2003.11136v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.11136v1)
- **Published**: 2020-03-24 22:30:42+00:00
- **Updated**: 2020-03-24 22:30:42+00:00
- **Authors**: Dung Nguyen, Sridha Sridharan, Duc Thanh Nguyen, Simon Denman, Son N. Tran, Rui Zeng, Clinton Fookes
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning has been applied to achieve significant progress in emotion recognition. Despite such substantial progress, existing approaches are still hindered by insufficient training data, and the resulting models do not generalize well under mismatched conditions. To address this challenge, we propose a learning strategy which jointly transfers the knowledge learned from rich datasets to source-poor datasets. Our method is also able to learn cross-domain features which lead to improved recognition performance. To demonstrate the robustness of our proposed framework, we conducted experiments on three benchmark emotion datasets including eNTERFACE, SAVEE, and EMODB. Experimental results show that the proposed method surpassed state-of-the-art transfer learning schemes by a significant margin.



### BigNAS: Scaling Up Neural Architecture Search with Big Single-Stage Models
- **Arxiv ID**: http://arxiv.org/abs/2003.11142v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.11142v3)
- **Published**: 2020-03-24 23:00:49+00:00
- **Updated**: 2020-07-17 02:00:22+00:00
- **Authors**: Jiahui Yu, Pengchong Jin, Hanxiao Liu, Gabriel Bender, Pieter-Jan Kindermans, Mingxing Tan, Thomas Huang, Xiaodan Song, Ruoming Pang, Quoc Le
- **Comment**: Accepted in ECCV 2020
- **Journal**: None
- **Summary**: Neural architecture search (NAS) has shown promising results discovering models that are both accurate and fast. For NAS, training a one-shot model has become a popular strategy to rank the relative quality of different architectures (child models) using a single set of shared weights. However, while one-shot model weights can effectively rank different network architectures, the absolute accuracies from these shared weights are typically far below those obtained from stand-alone training. To compensate, existing methods assume that the weights must be retrained, finetuned, or otherwise post-processed after the search is completed. These steps significantly increase the compute requirements and complexity of the architecture search and model deployment. In this work, we propose BigNAS, an approach that challenges the conventional wisdom that post-processing of the weights is necessary to get good prediction accuracies. Without extra retraining or post-processing steps, we are able to train a single set of shared weights on ImageNet and use these weights to obtain child models whose sizes range from 200 to 1000 MFLOPs. Our discovered model family, BigNASModels, achieve top-1 accuracies ranging from 76.5% to 80.9%, surpassing state-of-the-art models in this range including EfficientNets and Once-for-All networks without extra retraining or post-processing. We present ablative study and analysis to further understand the proposed BigNASModels.



### Adversarial Light Projection Attacks on Face Recognition Systems: A Feasibility Study
- **Arxiv ID**: http://arxiv.org/abs/2003.11145v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.11145v2)
- **Published**: 2020-03-24 23:06:25+00:00
- **Updated**: 2020-04-17 00:44:07+00:00
- **Authors**: Dinh-Luan Nguyen, Sunpreet S. Arora, Yuhang Wu, Hao Yang
- **Comment**: To appear in the proceedings of the IEEE Computer Vision and Pattern
  Recognition (CVPR) Biometrics Workshop 2020 - 9 pages, 8 figures
- **Journal**: None
- **Summary**: Deep learning-based systems have been shown to be vulnerable to adversarial attacks in both digital and physical domains. While feasible, digital attacks have limited applicability in attacking deployed systems, including face recognition systems, where an adversary typically has access to the input and not the transmission channel. In such setting, physical attacks that directly provide a malicious input through the input channel pose a bigger threat. We investigate the feasibility of conducting real-time physical attacks on face recognition systems using adversarial light projections. A setup comprising a commercially available web camera and a projector is used to conduct the attack. The adversary uses a transformation-invariant adversarial pattern generation method to generate a digital adversarial pattern using one or more images of the target available to the adversary. The digital adversarial pattern is then projected onto the adversary's face in the physical domain to either impersonate a target (impersonation) or evade recognition (obfuscation). We conduct preliminary experiments using two open-source and one commercial face recognition system on a pool of 50 subjects. Our experimental results demonstrate the vulnerability of face recognition systems to light projection attacks in both white-box and black-box attack settings.



### A Systematic Evaluation: Fine-Grained CNN vs. Traditional CNN Classifiers
- **Arxiv ID**: http://arxiv.org/abs/2003.11154v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.11154v3)
- **Published**: 2020-03-24 23:49:14+00:00
- **Updated**: 2021-11-03 01:15:41+00:00
- **Authors**: Saeed Anwar, Nick Barnes, Lars Petersson
- **Comment**: None
- **Journal**: None
- **Summary**: To make the best use of the underlying minute and subtle differences, fine-grained classifiers collect information about inter-class variations. The task is very challenging due to the small differences between the colors, viewpoint, and structure in the same class entities. The classification becomes more difficult due to the similarities between the differences in viewpoint with other classes and differences with its own. In this work, we investigate the performance of the landmark general CNN classifiers, which presented top-notch results on large scale classification datasets, on the fine-grained datasets, and compare it against state-of-the-art fine-grained classifiers. In this paper, we pose two specific questions: (i) Do the general CNN classifiers achieve comparable results to fine-grained classifiers? (ii) Do general CNN classifiers require any specific information to improve upon the fine-grained ones? Throughout this work, we train the general CNN classifiers without introducing any aspect that is specific to fine-grained datasets. We show an extensive evaluation on six datasets to determine whether the fine-grained classifier is able to elevate the baseline in their experiments.



