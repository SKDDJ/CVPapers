# Arxiv Papers in cs.CV on 2020-03-09
### FoCL: Feature-Oriented Continual Learning for Generative Models
- **Arxiv ID**: http://arxiv.org/abs/2003.03877v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.03877v1)
- **Published**: 2020-03-09 00:38:16+00:00
- **Updated**: 2020-03-09 00:38:16+00:00
- **Authors**: Qicheng Lao, Mehrzad Mortazavi, Marzieh Tahaei, Francis Dutil, Thomas Fevens, Mohammad Havaei
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a general framework in continual learning for generative models: Feature-oriented Continual Learning (FoCL). Unlike previous works that aim to solve the catastrophic forgetting problem by introducing regularization in the parameter space or image space, FoCL imposes regularization in the feature space. We show in our experiments that FoCL has faster adaptation to distributional changes in sequentially arriving tasks, and achieves the state-of-the-art performance for generative models in task incremental learning. We discuss choices of combined regularization spaces towards different use case scenarios for boosted performance, e.g., tasks that have high variability in the background. Finally, we introduce a forgetfulness measure that fairly evaluates the degree to which a model suffers from forgetting. Interestingly, the analysis of our proposed forgetfulness score also implies that FoCL tends to have a mitigated forgetting for future tasks.



### An Empirical Evaluation on Robustness and Uncertainty of Regularization Methods
- **Arxiv ID**: http://arxiv.org/abs/2003.03879v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.03879v1)
- **Published**: 2020-03-09 01:15:22+00:00
- **Updated**: 2020-03-09 01:15:22+00:00
- **Authors**: Sanghyuk Chun, Seong Joon Oh, Sangdoo Yun, Dongyoon Han, Junsuk Choe, Youngjoon Yoo
- **Comment**: Accepted at ICML 2019 Workshop on Uncertainty and Robustness in Deep
  Learning. 7 pages, 1 figure
- **Journal**: None
- **Summary**: Despite apparent human-level performances of deep neural networks (DNN), they behave fundamentally differently from humans. They easily change predictions when small corruptions such as blur and noise are applied on the input (lack of robustness), and they often produce confident predictions on out-of-distribution samples (improper uncertainty measure). While a number of researches have aimed to address those issues, proposed solutions are typically expensive and complicated (e.g. Bayesian inference and adversarial training). Meanwhile, many simple and cheap regularization methods have been developed to enhance the generalization of classifiers. Such regularization methods have largely been overlooked as baselines for addressing the robustness and uncertainty issues, as they are not specifically designed for that. In this paper, we provide extensive empirical evaluations on the robustness and uncertainty estimates of image classifiers (CIFAR-100 and ImageNet) trained with state-of-the-art regularization methods. Furthermore, experimental results show that certain regularization methods can serve as strong baseline methods for robustness and uncertainty estimation of DNNs.



### Faster ILOD: Incremental Learning for Object Detectors based on Faster RCNN
- **Arxiv ID**: http://arxiv.org/abs/2003.03901v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.03901v2)
- **Published**: 2020-03-09 03:09:00+00:00
- **Updated**: 2020-10-07 00:42:06+00:00
- **Authors**: Can Peng, Kun Zhao, Brian C. Lovell
- **Comment**: Accepted in Pattern Recognition Letters 2020
- **Journal**: None
- **Summary**: The human vision and perception system is inherently incremental where new knowledge is continually learned over time whilst existing knowledge is retained. On the other hand, deep learning networks are ill-equipped for incremental learning. When a well-trained network is adapted to new categories, its performance on the old categories will dramatically degrade. To address this problem, incremental learning methods have been explored which preserve the old knowledge of deep learning models. However, the state-of-the-art incremental object detector employs an external fixed region proposal method that increases overall computation time and reduces accuracy comparing to Region Proposal Network (RPN) based object detectors such as Faster RCNN. The purpose of this paper is to design an efficient end-to-end incremental object detector using knowledge distillation. We first evaluate and analyze the performance of the RPN-based detector with classic distillation on incremental detection tasks. Then, we introduce multi-network adaptive distillation that properly retains knowledge from the old categories when fine-tuning the model for new task. Experiments on the benchmark datasets, PASCAL VOC and COCO, demonstrate that the proposed incremental detector based on Faster RCNN is more accurate as well as being 13 times faster than the baseline detector.



### FarSee-Net: Real-Time Semantic Segmentation by Efficient Multi-scale Context Aggregation and Feature Space Super-resolution
- **Arxiv ID**: http://arxiv.org/abs/2003.03913v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2003.03913v1)
- **Published**: 2020-03-09 03:53:57+00:00
- **Updated**: 2020-03-09 03:53:57+00:00
- **Authors**: Zhanpeng Zhang, Kaipeng Zhang
- **Comment**: Accepted to ICRA 2020
- **Journal**: None
- **Summary**: Real-time semantic segmentation is desirable in many robotic applications with limited computation resources. One challenge of semantic segmentation is to deal with the object scale variations and leverage the context. How to perform multi-scale context aggregation within limited computation budget is important. In this paper, firstly, we introduce a novel and efficient module called Cascaded Factorized Atrous Spatial Pyramid Pooling (CF-ASPP). It is a lightweight cascaded structure for Convolutional Neural Networks (CNNs) to efficiently leverage context information. On the other hand, for runtime efficiency, state-of-the-art methods will quickly decrease the spatial size of the inputs or feature maps in the early network stages. The final high-resolution result is usually obtained by non-parametric up-sampling operation (e.g. bilinear interpolation). Differently, we rethink this pipeline and treat it as a super-resolution process. We use optimized super-resolution operation in the up-sampling step and improve the accuracy, especially in sub-sampled input image scenario for real-time applications. By fusing the above two improvements, our methods provide better latency-accuracy trade-off than the other state-of-the-art methods. In particular, we achieve 68.4% mIoU at 84 fps on the Cityscapes test set with a single Nivida Titan X (Maxwell) GPU card. The proposed module can be plugged into any feature extraction CNN and benefits from the CNN structure development.



### ROSE: Real One-Stage Effort to Detect the Fingerprint Singular Point Based on Multi-scale Spatial Attention
- **Arxiv ID**: http://arxiv.org/abs/2003.03918v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.03918v1)
- **Published**: 2020-03-09 04:16:31+00:00
- **Updated**: 2020-03-09 04:16:31+00:00
- **Authors**: Liaojun Pang, Jiong Chen, Fei Guo, Zhicheng Cao, Heng Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Detecting the singular point accurately and efficiently is one of the most important tasks for fingerprint recognition. In recent years, deep learning has been gradually used in the fingerprint singular point detection. However, current deep learning-based singular point detection methods are either two-stage or multi-stage, which makes them time-consuming. More importantly, their detection accuracy is yet unsatisfactory, especially in the case of the low-quality fingerprint. In this paper, we make a Real One-Stage Effort to detect fingerprint singular points more accurately and efficiently, and therefore we name the proposed algorithm ROSE for short, in which the multi-scale spatial attention, the Gaussian heatmap and the variant of focal loss are applied together to achieve a higher detection rate. Experimental results on the datasets FVC2002 DB1 and NIST SD4 show that our ROSE outperforms the state-of-art algorithms in terms of detection rate, false alarm rate and detection speed.



### Deconfounded Image Captioning: A Causal Retrospect
- **Arxiv ID**: http://arxiv.org/abs/2003.03923v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.03923v1)
- **Published**: 2020-03-09 04:59:05+00:00
- **Updated**: 2020-03-09 04:59:05+00:00
- **Authors**: Xu Yang, Hanwang Zhang, Jianfei Cai
- **Comment**: None
- **Journal**: None
- **Summary**: The dataset bias in vision-language tasks is becoming one of the main problems that hinder the progress of our community. However, recent studies lack a principled analysis of the bias. In this paper, we present a novel perspective: Deconfounded Image Captioning (DIC), to find out the cause of the bias in image captioning, then retrospect modern neural image captioners, and finally propose a DIC framework: DICv1.0. DIC is based on causal inference, whose two principles: the backdoor and front-door adjustments, help us to review previous works and design the effective models. In particular, we showcase that DICv1.0 can strengthen two prevailing captioning models and achieves a single-model 130.7 CIDEr-D and 128.4 c40 CIDEr-D on Karpathy split and online split of the challenging MS-COCO dataset, respectively. Last but not least, DICv1.0 is merely a natural derivation from our causal retrospect, which opens a promising direction for image captioning.



### Dual-attention Guided Dropblock Module for Weakly Supervised Object Localization
- **Arxiv ID**: http://arxiv.org/abs/2003.04719v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.04719v3)
- **Published**: 2020-03-09 05:07:50+00:00
- **Updated**: 2020-07-09 10:32:06+00:00
- **Authors**: Junhui Yin, Siqing Zhang, Dongliang Chang, Zhanyu Ma, Jun Guo
- **Comment**: Accepted by the 25th International Conference on Pattern Recognition
  (ICPR 2020)
- **Journal**: None
- **Summary**: Attention mechanisms is frequently used to learn the discriminative features for better feature representations. In this paper, we extend the attention mechanism to the task of weakly supervised object localization (WSOL) and propose the dual-attention guided dropblock module (DGDM), which aims at learning the informative and complementary visual patterns for WSOL. This module contains two key components, the channel attention guided dropout (CAGD) and the spatial attention guided dropblock (SAGD). To model channel interdependencies, the CAGD ranks the channel attentions and treats the top-k attentions with the largest magnitudes as the important ones. It also keeps some low-valued elements to increase their value if they become important during training. The SAGD can efficiently remove the most discriminative information by erasing the contiguous regions of feature maps rather than individual pixels. This guides the model to capture the less discriminative parts for classification. Furthermore, it can also distinguish the foreground objects from the background regions to alleviate the attention misdirection. Experimental results demonstrate that the proposed method achieves new state-of-the-art localization performance.



### MCMC Guided CNN Training and Segmentation for Pancreas Extraction
- **Arxiv ID**: http://arxiv.org/abs/2003.03938v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.03938v1)
- **Published**: 2020-03-09 06:27:08+00:00
- **Updated**: 2020-03-09 06:27:08+00:00
- **Authors**: Jinchan He, Xiaxia Yu, Chudong Cai, Yi Gao
- **Comment**: None
- **Journal**: None
- **Summary**: Efficient organ segmentation is the precondition of various quantitative analysis. Segmenting the pancreas from abdominal CT images is a challenging task because of its high anatomical variability in shape, size and location. What's more, the pancreas only occupies a small portion in abdomen, and the organ border is very fuzzy. All these factors make the segmentation methods of other organs less suitable for the pancreas segmentation. In this report, we propose a Markov Chain Monte Carlo (MCMC) sampling guided convolutional neural network (CNN) approach, in order to handle such difficulties in morphological and photometric variabilities. Specifically, the proposed method mainly contains three steps: First, registration is carried out to mitigate the body weight and location variability. Then, an MCMC sampling is employed to guide the sampling of 3D patches, which are fed to the CNN for training. At the same time, the pancreas distribution is also learned for the subsequent segmentation. Third, sampled from the learned distribution, an MCMC process guides the segmentation process. Lastly, the patches based segmentation is fused using a Bayesian voting scheme. This method is evaluated on the NIH pancreatic datasets which contains 82 abdominal contrast-enhanced CT volumes. Finally, we achieved a competitive result of 78.13% Dice Similarity Coefficient value and 82.65% Recall value in testing data.



### Pacemaker: Intermediate Teacher Knowledge Distillation For On-The-Fly Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2003.03944v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.03944v1)
- **Published**: 2020-03-09 06:45:44+00:00
- **Updated**: 2020-03-09 06:45:44+00:00
- **Authors**: Wonchul Son, Youngbin Kim, Wonseok Song, Youngsu Moon, Wonjun Hwang
- **Comment**: None
- **Journal**: None
- **Summary**: There is a need for an on-the-fly computational process with very low performance system such as system-on-chip (SoC) and embedded device etc. This paper presents pacemaker knowledge distillation as intermediate ensemble teacher to use convolutional neural network in these systems. For on-the-fly system, we consider student model using 1xN shape on-the-fly filter and teacher model using normal NxN shape filter. We note three points about training student model, caused by applying on-the-fly filter. First, same depth but unavoidable thin model compression. Second, the large capacity gap and parameter size gap due to only the horizontal field must be selected not the vertical receptive. Third, the performance instability and degradation of direct distilling. To solve these problems, we propose intermediate teacher, named pacemaker, for an on-the-fly student. So, student can be trained from pacemaker and original teacher step by step. Experiments prove our proposed method make significant performance (accuracy) improvements: on CIFAR100, 5.39% increased in WRN-40-4 than conventional knowledge distillation which shows even low performance than baseline. And we solve train instability, occurred when conventional knowledge distillation was applied without proposed method, by reducing deviation range by applying proposed method pacemaker knowledge distillation.



### Cross-Modal Food Retrieval: Learning a Joint Embedding of Food Images and Recipes with Semantic Consistency and Attention Mechanism
- **Arxiv ID**: http://arxiv.org/abs/2003.03955v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2003.03955v3)
- **Published**: 2020-03-09 07:41:17+00:00
- **Updated**: 2021-09-21 02:17:48+00:00
- **Authors**: Hao Wang, Doyen Sahoo, Chenghao Liu, Ke Shu, Palakorn Achananuparp, Ee-peng Lim, Steven C. H. Hoi
- **Comment**: IEEE Transactions on Multimedia
- **Journal**: None
- **Summary**: Food retrieval is an important task to perform analysis of food-related information, where we are interested in retrieving relevant information about the queried food item such as ingredients, cooking instructions, etc. In this paper, we investigate cross-modal retrieval between food images and cooking recipes. The goal is to learn an embedding of images and recipes in a common feature space, such that the corresponding image-recipe embeddings lie close to one another. Two major challenges in addressing this problem are 1) large intra-variance and small inter-variance across cross-modal food data; and 2) difficulties in obtaining discriminative recipe representations. To address these two problems, we propose Semantic-Consistent and Attention-based Networks (SCAN), which regularize the embeddings of the two modalities through aligning output semantic probabilities. Besides, we exploit a self-attention mechanism to improve the embedding of recipes. We evaluate the performance of the proposed method on the large-scale Recipe1M dataset, and show that we can outperform several state-of-the-art cross-modal retrieval strategies for food images and cooking recipes by a significant margin.



### BiDet: An Efficient Binarized Object Detector
- **Arxiv ID**: http://arxiv.org/abs/2003.03961v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.03961v1)
- **Published**: 2020-03-09 08:16:16+00:00
- **Updated**: 2020-03-09 08:16:16+00:00
- **Authors**: Ziwei Wang, Ziyi Wu, Jiwen Lu, Jie Zhou
- **Comment**: Accepted by CVPR2020
- **Journal**: None
- **Summary**: In this paper, we propose a binarized neural network learning method called BiDet for efficient object detection. Conventional network binarization methods directly quantize the weights and activations in one-stage or two-stage detectors with constrained representational capacity, so that the information redundancy in the networks causes numerous false positives and degrades the performance significantly. On the contrary, our BiDet fully utilizes the representational capacity of the binary neural networks for object detection by redundancy removal, through which the detection precision is enhanced with alleviated false positives. Specifically, we generalize the information bottleneck (IB) principle to object detection, where the amount of information in the high-level feature maps is constrained and the mutual information between the feature maps and object detection is maximized. Meanwhile, we learn sparse object priors so that the posteriors are concentrated on informative detection prediction with false positive elimination. Extensive experiments on the PASCAL VOC and COCO datasets show that our method outperforms the state-of-the-art binary neural networks by a sizable margin.



### Cross-View Tracking for Multi-Human 3D Pose Estimation at over 100 FPS
- **Arxiv ID**: http://arxiv.org/abs/2003.03972v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.03972v3)
- **Published**: 2020-03-09 08:54:00+00:00
- **Updated**: 2021-07-29 03:02:33+00:00
- **Authors**: Long Chen, Haizhou Ai, Rui Chen, Zijie Zhuang, Shuang Liu
- **Comment**: 12 pages with supplementary material; accepted to CVPR 2020
- **Journal**: None
- **Summary**: Estimating 3D poses of multiple humans in real-time is a classic but still challenging task in computer vision. Its major difficulty lies in the ambiguity in cross-view association of 2D poses and the huge state space when there are multiple people in multiple views. In this paper, we present a novel solution for multi-human 3D pose estimation from multiple calibrated camera views. It takes 2D poses in different camera coordinates as inputs and aims for the accurate 3D poses in the global coordinate. Unlike previous methods that associate 2D poses among all pairs of views from scratch at every frame, we exploit the temporal consistency in videos to match the 2D inputs with 3D poses directly in 3-space. More specifically, we propose to retain the 3D pose for each person and update them iteratively via the cross-view multi-human tracking. This novel formulation improves both accuracy and efficiency, as we demonstrated on widely-used public datasets. To further verify the scalability of our method, we propose a new large-scale multi-human dataset with 12 to 28 camera views. Without bells and whistles, our solution achieves 154 FPS on 12 cameras and 34 FPS on 28 cameras, indicating its ability to handle large-scale real-world applications. The proposed dataset is released at https://github.com/longcw/crossview_3d_pose_tracking.



### Pseudo-Convolutional Policy Gradient for Sequence-to-Sequence Lip-Reading
- **Arxiv ID**: http://arxiv.org/abs/2003.03983v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.03983v1)
- **Published**: 2020-03-09 09:12:26+00:00
- **Updated**: 2020-03-09 09:12:26+00:00
- **Authors**: Mingshuang Luo, Shuang Yang, Shiguang Shan, Xilin Chen
- **Comment**: 8 pages, Accepted in the 15th IEEE International Conference on
  Automatic Face and Gesture Recognition (FG 2020)
- **Journal**: None
- **Summary**: Lip-reading aims to infer the speech content from the lip movement sequence and can be seen as a typical sequence-to-sequence (seq2seq) problem which translates the input image sequence of lip movements to the text sequence of the speech content. However, the traditional learning process of seq2seq models always suffers from two problems: the exposure bias resulted from the strategy of "teacher-forcing", and the inconsistency between the discriminative optimization target (usually the cross-entropy loss) and the final evaluation metric (usually the character/word error rate). In this paper, we propose a novel pseudo-convolutional policy gradient (PCPG) based method to address these two problems. On the one hand, we introduce the evaluation metric (refers to the character error rate in this paper) as a form of reward to optimize the model together with the original discriminative target. On the other hand, inspired by the local perception property of convolutional operation, we perform a pseudo-convolutional operation on the reward and loss dimension, so as to take more context around each time step into account to generate a robust reward and loss for the whole optimization. Finally, we perform a thorough comparison and evaluation on both the word-level and sentence-level benchmarks. The results show a significant improvement over other related methods, and report either a new state-of-the-art performance or a competitive accuracy on all these challenging benchmarks, which clearly proves the advantages of our approach.



### Context-Aware Domain Adaptation in Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2003.04010v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.04010v1)
- **Published**: 2020-03-09 09:57:24+00:00
- **Updated**: 2020-03-09 09:57:24+00:00
- **Authors**: Jinyu Yang, Weizhi An, Chaochao Yan, Peilin Zhao, Junzhou Huang
- **Comment**: 10 pages, 6 figures, 5 tables
- **Journal**: None
- **Summary**: In this paper, we consider the problem of unsupervised domain adaptation in the semantic segmentation. There are two primary issues in this field, i.e., what and how to transfer domain knowledge across two domains. Existing methods mainly focus on adapting domain-invariant features (what to transfer) through adversarial learning (how to transfer). Context dependency is essential for semantic segmentation, however, its transferability is still not well understood. Furthermore, how to transfer contextual information across two domains remains unexplored. Motivated by this, we propose a cross-attention mechanism based on self-attention to capture context dependencies between two domains and adapt transferable context. To achieve this goal, we design two cross-domain attention modules to adapt context dependencies from both spatial and channel views. Specifically, the spatial attention module captures local feature dependencies between each position in the source and target image. The channel attention module models semantic dependencies between each pair of cross-domain channel maps. To adapt context dependencies, we further selectively aggregate the context information from two domains. The superiority of our method over existing state-of-the-art methods is empirically proved on "GTA5 to Cityscapes" and "SYNTHIA to Cityscapes".



### Dense Dilated Convolutions Merging Network for Land Cover Classification
- **Arxiv ID**: http://arxiv.org/abs/2003.04027v1
- **DOI**: 10.1109/TGRS.2020.2976658
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.04027v1)
- **Published**: 2020-03-09 10:31:38+00:00
- **Updated**: 2020-03-09 10:31:38+00:00
- **Authors**: Qinghui Liu, Michael Kampffmeyer, Robert Jessen, Arnt-Børre Salberg
- **Comment**: Semantic Segmentation, 12 pages, TGRS-2020 early access in IEEE
  Transactions on Geoscience and Remote Sensing. 2020, Code available at
  https://github.com/samleoqh/DDCM-Semantic-Segmentation-PyTorch
- **Journal**: None
- **Summary**: Land cover classification of remote sensing images is a challenging task due to limited amounts of annotated data, highly imbalanced classes, frequent incorrect pixel-level annotations, and an inherent complexity in the semantic segmentation task. In this article, we propose a novel architecture called the dense dilated convolutions' merging network (DDCM-Net) to address this task. The proposed DDCM-Net consists of dense dilated image convolutions merged with varying dilation rates. This effectively utilizes rich combinations of dilated convolutions that enlarge the network's receptive fields with fewer parameters and features compared with the state-of-the-art approaches in the remote sensing domain. Importantly, DDCM-Net obtains fused local- and global-context information, in effect incorporating surrounding discriminative capability for multiscale and complex-shaped objects with similar color and textures in very high-resolution aerial imagery. We demonstrate the effectiveness, robustness, and flexibility of the proposed DDCM-Net on the publicly available ISPRS Potsdam and Vaihingen data sets, as well as the DeepGlobe land cover data set. Our single model, trained on three-band Potsdam and Vaihingen data sets, achieves better accuracy in terms of both mean intersection over union (mIoU) and F1-score compared with other published models trained with more than three-band data. We further validate our model on the DeepGlobe data set, achieving state-of-the-art result 56.2% mIoU with much fewer parameters and at a lower computational cost compared with related recent work. Code available at https://github.com/samleoqh/DDCM-Semantic-Segmentation-PyTorch



### Learning Delicate Local Representations for Multi-Person Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2003.04030v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.04030v3)
- **Published**: 2020-03-09 10:40:49+00:00
- **Updated**: 2020-07-15 13:09:57+00:00
- **Authors**: Yuanhao Cai, Zhicheng Wang, Zhengxiong Luo, Binyi Yin, Angang Du, Haoqian Wang, Xiangyu Zhang, Xinyu Zhou, Erjin Zhou, Jian Sun
- **Comment**: ECCV2020 Spotlight
- **Journal**: None
- **Summary**: In this paper, we propose a novel method called Residual Steps Network (RSN). RSN aggregates features with the same spatial size (Intra-level features) efficiently to obtain delicate local representations, which retain rich low-level spatial information and result in precise keypoint localization. Additionally, we observe the output features contribute differently to final performance. To tackle this problem, we propose an efficient attention mechanism - Pose Refine Machine (PRM) to make a trade-off between local and global representations in output features and further refine the keypoint locations. Our approach won the 1st place of COCO Keypoint Challenge 2019 and achieves state-of-the-art results on both COCO and MPII benchmarks, without using extra training data and pretrained model. Our single model achieves 78.6 on COCO test-dev, 93.0 on MPII test dataset. Ensembled models achieve 79.2 on COCO test-dev, 77.1 on COCO test-challenge dataset. The source code is publicly available for further research at https://github.com/caiyuanhao1998/RSN/



### Transformation-based Adversarial Video Prediction on Large-Scale Data
- **Arxiv ID**: http://arxiv.org/abs/2003.04035v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.04035v3)
- **Published**: 2020-03-09 10:52:25+00:00
- **Updated**: 2021-11-17 17:56:08+00:00
- **Authors**: Pauline Luc, Aidan Clark, Sander Dieleman, Diego de Las Casas, Yotam Doron, Albin Cassirer, Karen Simonyan
- **Comment**: None
- **Journal**: None
- **Summary**: Recent breakthroughs in adversarial generative modeling have led to models capable of producing video samples of high quality, even on large and complex datasets of real-world video. In this work, we focus on the task of video prediction, where given a sequence of frames extracted from a video, the goal is to generate a plausible future sequence. We first improve the state of the art by performing a systematic empirical study of discriminator decompositions and proposing an architecture that yields faster convergence and higher performance than previous approaches. We then analyze recurrent units in the generator, and propose a novel recurrent unit which transforms its past hidden state according to predicted motion-like features, and refines it to handle dis-occlusions, scene changes and other complex behavior. We show that this recurrent unit consistently outperforms previous designs. Our final model leads to a leap in the state-of-the-art performance, obtaining a test set Frechet Video Distance of 25.7, down from 69.2, on the large-scale Kinetics-600 dataset.



### On the Texture Bias for Few-Shot CNN Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2003.04052v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.04052v3)
- **Published**: 2020-03-09 11:55:47+00:00
- **Updated**: 2020-12-23 22:37:09+00:00
- **Authors**: Reza Azad, Abdur R Fayjie, Claude Kauffman, Ismail Ben Ayed, Marco Pedersoli, Jose Dolz
- **Comment**: Accepted at WACV'21
- **Journal**: None
- **Summary**: Despite the initial belief that Convolutional Neural Networks (CNNs) are driven by shapes to perform visual recognition tasks, recent evidence suggests that texture bias in CNNs provides higher performing models when learning on large labeled training datasets. This contrasts with the perceptual bias in the human visual cortex, which has a stronger preference towards shape components. Perceptual differences may explain why CNNs achieve human-level performance when large labeled datasets are available, but their performance significantly degrades in lowlabeled data scenarios, such as few-shot semantic segmentation. To remove the texture bias in the context of few-shot learning, we propose a novel architecture that integrates a set of Difference of Gaussians (DoG) to attenuate high-frequency local components in the feature space. This produces a set of modified feature maps, whose high-frequency components are diminished at different standard deviation values of the Gaussian distribution in the spatial domain. As this results in multiple feature maps for a single image, we employ a bi-directional convolutional long-short-term-memory to efficiently merge the multi scale-space representations. We perform extensive experiments on three well-known few-shot segmentation benchmarks -- Pascal i5, COCO-20i and FSS-1000 -- and demonstrate that our method outperforms state-of-the-art approaches in two datasets under the same conditions. The code is available at: https://github.com/rezazad68/fewshot-segmentation



### When Person Re-identification Meets Changing Clothes
- **Arxiv ID**: http://arxiv.org/abs/2003.04070v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.04070v3)
- **Published**: 2020-03-09 12:32:16+00:00
- **Updated**: 2020-05-25 01:53:18+00:00
- **Authors**: Fangbin Wan, Yang Wu, Xuelin Qian, Yixiong Chen, Yanwei Fu
- **Comment**: Accepted by CVPRW 2020
- **Journal**: None
- **Summary**: Person re-identification (ReID) is now an active research topic for AI-based video surveillance applications such as specific person search, but the practical issue that the target person(s) may change clothes (clothes inconsistency problem) has been overlooked for long. For the first time, this paper systematically studies this problem. We first overcome the difficulty of lack of suitable dataset, by collecting a small yet representative real dataset for testing whilst building a large realistic synthetic dataset for training and deeper studies. Facilitated by our new datasets, we are able to conduct various interesting new experiments for studying the influence of clothes inconsistency. We find that changing clothes makes ReID a much harder problem in the sense of bringing difficulties to learning effective representations and also challenges the generalization ability of previous ReID models to identify persons with unseen (new) clothes. Representative existing ReID models are adopted to show informative results on such a challenging setting, and we also provide some preliminary efforts on improving the robustness of existing models on handling the clothes inconsistency issue in the data. We believe that this study can be inspiring and helpful for encouraging more researches in this direction. The dataset is available on the project website: https://wanfb.github.io/dataset.html.



### Searching Central Difference Convolutional Networks for Face Anti-Spoofing
- **Arxiv ID**: http://arxiv.org/abs/2003.04092v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.04092v1)
- **Published**: 2020-03-09 12:48:37+00:00
- **Updated**: 2020-03-09 12:48:37+00:00
- **Authors**: Zitong Yu, Chenxu Zhao, Zezheng Wang, Yunxiao Qin, Zhuo Su, Xiaobai Li, Feng Zhou, Guoying Zhao
- **Comment**: Accepted by CVPR2020
- **Journal**: None
- **Summary**: Face anti-spoofing (FAS) plays a vital role in face recognition systems. Most state-of-the-art FAS methods 1) rely on stacked convolutions and expert-designed network, which is weak in describing detailed fine-grained information and easily being ineffective when the environment varies (e.g., different illumination), and 2) prefer to use long sequence as input to extract dynamic features, making them difficult to deploy into scenarios which need quick response. Here we propose a novel frame level FAS method based on Central Difference Convolution (CDC), which is able to capture intrinsic detailed patterns via aggregating both intensity and gradient information. A network built with CDC, called the Central Difference Convolutional Network (CDCN), is able to provide more robust modeling capacity than its counterpart built with vanilla convolution. Furthermore, over a specifically designed CDC search space, Neural Architecture Search (NAS) is utilized to discover a more powerful network structure (CDCN++), which can be assembled with Multiscale Attention Fusion Module (MAFM) for further boosting performance. Comprehensive experiments are performed on six benchmark datasets to show that 1) the proposed method not only achieves superior performance on intra-dataset testing (especially 0.2% ACER in Protocol-1 of OULU-NPU dataset), 2) it also generalizes well on cross-dataset testing (particularly 6.5% HTER from CASIA-MFSD to Replay-Attack datasets). The codes are available at \href{https://github.com/ZitongYu/CDCN}{https://github.com/ZitongYu/CDCN}.



### A Strong Baseline for Fashion Retrieval with Person Re-Identification Models
- **Arxiv ID**: http://arxiv.org/abs/2003.04094v1
- **DOI**: 10.1007/978-3-030-63820-7_33
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2003.04094v1)
- **Published**: 2020-03-09 12:50:15+00:00
- **Updated**: 2020-03-09 12:50:15+00:00
- **Authors**: Mikolaj Wieczorek, Andrzej Michalowski, Anna Wroblewska, Jacek Dabrowski
- **Comment**: 33 pages, 14 figures
- **Journal**: short paper in Neural Information Processing, Communications in
  Computer and Information Science, 2020
- **Summary**: Fashion retrieval is the challenging task of finding an exact match for fashion items contained within an image. Difficulties arise from the fine-grained nature of clothing items, very large intra-class and inter-class variance. Additionally, query and source images for the task usually come from different domains - street photos and catalogue photos respectively. Due to these differences, a significant gap in quality, lighting, contrast, background clutter and item presentation exists between domains. As a result, fashion retrieval is an active field of research both in academia and the industry.   Inspired by recent advancements in Person Re-Identification research, we adapt leading ReID models to be used in fashion retrieval tasks. We introduce a simple baseline model for fashion retrieval, significantly outperforming previous state-of-the-art results despite a much simpler architecture. We conduct in-depth experiments on Street2Shop and DeepFashion datasets and validate our results. Finally, we propose a cross-domain (cross-dataset) evaluation method to test the robustness of fashion retrieval models.



### IROF: a low resource evaluation metric for explanation methods
- **Arxiv ID**: http://arxiv.org/abs/2003.08747v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08747v1)
- **Published**: 2020-03-09 13:01:30+00:00
- **Updated**: 2020-03-09 13:01:30+00:00
- **Authors**: Laura Rieger, Lars Kai Hansen
- **Comment**: None
- **Journal**: None
- **Summary**: The adoption of machine learning in health care hinges on the transparency of the used algorithms, necessitating the need for explanation methods. However, despite a growing literature on explaining neural networks, no consensus has been reached on how to evaluate those explanation methods. We propose IROF, a new approach to evaluating explanation methods that circumvents the need for manual evaluation. Compared to other recent work, our approach requires several orders of magnitude less computational resources and no human input, making it accessible to lower resource groups and robust to human bias.



### iFAN: Image-Instance Full Alignment Networks for Adaptive Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2003.04132v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.04132v1)
- **Published**: 2020-03-09 13:27:06+00:00
- **Updated**: 2020-03-09 13:27:06+00:00
- **Authors**: Chenfan Zhuang, Xintong Han, Weilin Huang, Matthew R. Scott
- **Comment**: AAAI 2020
- **Journal**: None
- **Summary**: Training an object detector on a data-rich domain and applying it to a data-poor one with limited performance drop is highly attractive in industry, because it saves huge annotation cost. Recent research on unsupervised domain adaptive object detection has verified that aligning data distributions between source and target images through adversarial learning is very useful. The key is when, where and how to use it to achieve best practice. We propose Image-Instance Full Alignment Networks (iFAN) to tackle this problem by precisely aligning feature distributions on both image and instance levels: 1) Image-level alignment: multi-scale features are roughly aligned by training adversarial domain classifiers in a hierarchically-nested fashion. 2) Full instance-level alignment: deep semantic information and elaborate instance representations are fully exploited to establish a strong relationship among categories and domains. Establishing these correlations is formulated as a metric learning problem by carefully constructing instance pairs. Above-mentioned adaptations can be integrated into an object detector (e.g. Faster RCNN), resulting in an end-to-end trainable framework where multiple alignments can work collaboratively in a coarse-tofine manner. In two domain adaptation tasks: synthetic-to-real (SIM10K->Cityscapes) and normal-to-foggy weather (Cityscapes->Foggy Cityscapes), iFAN outperforms the state-of-the-art methods with a boost of 10%+ AP over the source-only baseline.



### TorchIO: A Python library for efficient loading, preprocessing, augmentation and patch-based sampling of medical images in deep learning
- **Arxiv ID**: http://arxiv.org/abs/2003.04696v5
- **DOI**: 10.1016/j.cmpb.2021.106236
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.04696v5)
- **Published**: 2020-03-09 13:36:16+00:00
- **Updated**: 2021-08-05 10:48:15+00:00
- **Authors**: Fernando Pérez-García, Rachel Sparks, Sébastien Ourselin
- **Comment**: Published in Computer Methods and Programs in Biomedicine
- **Journal**: Computer Methods and Programs in Biomedicine (June 2021), p.
  106236. ISSN: 0169-2607
- **Summary**: Processing of medical images such as MRI or CT presents unique challenges compared to RGB images typically used in computer vision. These include a lack of labels for large datasets, high computational costs, and metadata to describe the physical properties of voxels. Data augmentation is used to artificially increase the size of the training datasets. Training with image patches decreases the need for computational power. Spatial metadata needs to be carefully taken into account in order to ensure a correct alignment of volumes.   We present TorchIO, an open-source Python library to enable efficient loading, preprocessing, augmentation and patch-based sampling of medical images for deep learning. TorchIO follows the style of PyTorch and integrates standard medical image processing libraries to efficiently process images during training of neural networks. TorchIO transforms can be composed, reproduced, traced and extended. We provide multiple generic preprocessing and augmentation operations as well as simulation of MRI-specific artifacts.   Source code, comprehensive tutorials and extensive documentation for TorchIO can be found at https://torchio.rtfd.io/. The package can be installed from the Python Package Index running 'pip install torchio'. It includes a command-line interface which allows users to apply transforms to image files without using Python. Additionally, we provide a graphical interface within a TorchIO extension in 3D Slicer to visualize the effects of transforms.   TorchIO was developed to help researchers standardize medical image processing pipelines and allow them to focus on the deep learning experiments. It encourages open science, as it supports reproducibility and is version controlled so that the software can be cited precisely. Due to its modularity, the library is compatible with other frameworks for deep learning with medical images.



### Learned Spectral Computed Tomography
- **Arxiv ID**: http://arxiv.org/abs/2003.04138v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.NE, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2003.04138v1)
- **Published**: 2020-03-09 13:39:12+00:00
- **Updated**: 2020-03-09 13:39:12+00:00
- **Authors**: Dimitris Kamilis, Mario Blatter, Nick Polydorides
- **Comment**: 8 pages, 5 figures
- **Journal**: None
- **Summary**: Spectral Photon-Counting Computed Tomography (SPCCT) is a promising technology that has shown a number of advantages over conventional X-ray Computed Tomography (CT) in the form of material separation, artefact removal and enhanced image quality. However, due to the increased complexity and non-linearity of the SPCCT governing equations, model-based reconstruction algorithms typically require handcrafted regularisation terms and meticulous tuning of hyperparameters making them impractical to calibrate in variable conditions. Additionally, they typically incur high computational costs and in cases of limited-angle data, their imaging capability deteriorates significantly. Recently, Deep Learning has proven to provide state-of-the-art reconstruction performance in medical imaging applications while circumventing most of these challenges. Inspired by these advances, we propose a Deep Learning imaging method for SPCCT that exploits the expressive power of Neural Networks while also incorporating model knowledge. The method takes the form of a two-step learned primal-dual algorithm that is trained using case-specific data. The proposed approach is characterised by fast reconstruction capability and high imaging performance, even in limited-data cases, while avoiding the hand-tuning that is required by other optimisation approaches. We demonstrate the performance of the method in terms of reconstructed images and quality metrics via numerical examples inspired by the application of cardiovascular imaging.



### Accurate Temporal Action Proposal Generation with Relation-Aware Pyramid Network
- **Arxiv ID**: http://arxiv.org/abs/2003.04145v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.04145v1)
- **Published**: 2020-03-09 13:47:36+00:00
- **Updated**: 2020-03-09 13:47:36+00:00
- **Authors**: Jialin Gao, Zhixiang Shi, Jiani Li, Guanshuo Wang, Yufeng Yuan, Shiming Ge, Xi Zhou
- **Comment**: accepted by AAAI-20
- **Journal**: None
- **Summary**: Accurate temporal action proposals play an important role in detecting actions from untrimmed videos. The existing approaches have difficulties in capturing global contextual information and simultaneously localizing actions with different durations. To this end, we propose a Relation-aware pyramid Network (RapNet) to generate highly accurate temporal action proposals. In RapNet, a novel relation-aware module is introduced to exploit bi-directional long-range relations between local features for context distilling. This embedded module enhances the RapNet in terms of its multi-granularity temporal proposal generation ability, given predefined anchor boxes. We further introduce a two-stage adjustment scheme to refine the proposal boundaries and measure their confidence in containing an action with snippet-level actionness. Extensive experiments on the challenging ActivityNet and THUMOS14 benchmarks demonstrate our RapNet generates superior accurate proposals over the existing state-of-the-art methods.



### Embedding Propagation: Smoother Manifold for Few-Shot Classification
- **Arxiv ID**: http://arxiv.org/abs/2003.04151v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.04151v2)
- **Published**: 2020-03-09 13:51:09+00:00
- **Updated**: 2020-07-13 15:14:03+00:00
- **Authors**: Pau Rodríguez, Issam Laradji, Alexandre Drouin, Alexandre Lacoste
- **Comment**: Published at ECCV2020
- **Journal**: None
- **Summary**: Few-shot classification is challenging because the data distribution of the training set can be widely different to the test set as their classes are disjoint. This distribution shift often results in poor generalization. Manifold smoothing has been shown to address the distribution shift problem by extending the decision boundaries and reducing the noise of the class representations. Moreover, manifold smoothness is a key factor for semi-supervised learning and transductive learning algorithms. In this work, we propose to use embedding propagation as an unsupervised non-parametric regularizer for manifold smoothing in few-shot classification. Embedding propagation leverages interpolations between the extracted features of a neural network based on a similarity graph. We empirically show that embedding propagation yields a smoother embedding manifold. We also show that applying embedding propagation to a transductive classifier achieves new state-of-the-art results in mini-Imagenet, tiered-Imagenet, Imagenet-FS, and CUB. Furthermore, we show that embedding propagation consistently improves the accuracy of the models in multiple semi-supervised learning scenarios by up to 16\% points. The proposed embedding propagation operation can be easily integrated as a non-parametric layer into a neural network. We provide the training code and usage examples at https://github.com/ElementAI/embedding-propagation.



### Hybrid calibration procedure for fringe projection profilometry based on stereo-vision and polynomial fitting
- **Arxiv ID**: http://arxiv.org/abs/2003.04168v1
- **DOI**: 10.1364/AO.383602
- **Categories**: **physics.ins-det**, cs.CV, eess.IV, I.4.5; I.4.7
- **Links**: [PDF](http://arxiv.org/pdf/2003.04168v1)
- **Published**: 2020-03-09 14:25:03+00:00
- **Updated**: 2020-03-09 14:25:03+00:00
- **Authors**: Raul Vargas, Andres G. Marrugo, Song Zhang, Lenny A. Romero
- **Comment**: Accepted for publication in Applied Optics Vol. 59 No. 13, 2020
- **Journal**: None
- **Summary**: The key to accurate 3D shape measurement in Fringe Projection Profilometry (FPP) is the proper calibration of the measurement system. Current calibration techniques rely on phase-coordinate mapping (PCM) or back-projection stereo-vision (SV) methods. PCM methods are cumbersome to implement as they require precise positioning of the calibration target relative to the FPP system but produce highly accurate measurements within the calibration volume. SV methods generally do not achieve the same accuracy level. However, the calibration is more flexible in that the calibration target can be arbitrarily positioned. In this work, we propose a hybrid calibration method that leverages the SV calibration approach using a PCM method to achieve higher accuracy. The method has the flexibility of SV methods, is robust to lens distortions, and has a simple relation between the recovered phase and the metric coordinates. Experimental results show that the proposed Hybrid method outperforms the SV method in terms of accuracy and reconstruction time due to its low computational complexity.



### I-ViSE: Interactive Video Surveillance as an Edge Service using Unsupervised Feature Queries
- **Arxiv ID**: http://arxiv.org/abs/2003.04169v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DC, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.04169v1)
- **Published**: 2020-03-09 14:26:45+00:00
- **Updated**: 2020-03-09 14:26:45+00:00
- **Authors**: Seyed Yahya Nikouei, Yu Chen, Alexander Aved, Erik Blasch
- **Comment**: R1 is under review by the IEEE Internet of Things Journal
- **Journal**: None
- **Summary**: Situation AWareness (SAW) is essential for many mission critical applications. However, SAW is very challenging when trying to immediately identify objects of interest or zoom in on suspicious activities from thousands of video frames. This work aims at developing a queryable system to instantly select interesting content. While face recognition technology is mature, in many scenarios like public safety monitoring, the features of objects of interest may be much more complicated than face features. In addition, human operators may not be always able to provide a descriptive, simple, and accurate query. Actually, it is more often that there are only rough, general descriptions of certain suspicious objects or accidents. This paper proposes an Interactive Video Surveillance as an Edge service (I-ViSE) based on unsupervised feature queries. Adopting unsupervised methods that do not reveal any private information, the I-ViSE scheme utilizes general features of a human body and color of clothes. An I-ViSE prototype is built following the edge-fog computing paradigm and the experimental results verified the I-ViSE scheme meets the design goal of scene recognition in less than two seconds.



### BirdNet+: End-to-End 3D Object Detection in LiDAR Bird's Eye View
- **Arxiv ID**: http://arxiv.org/abs/2003.04188v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.04188v1)
- **Published**: 2020-03-09 15:08:40+00:00
- **Updated**: 2020-03-09 15:08:40+00:00
- **Authors**: Alejandro Barrera, Carlos Guindel, Jorge Beltrán, Fernando García
- **Comment**: Submitted to IEEE International Conference on Intelligent
  Transportation Systems (ITSC2020)
- **Journal**: None
- **Summary**: On-board 3D object detection in autonomous vehicles often relies on geometry information captured by LiDAR devices. Albeit image features are typically preferred for detection, numerous approaches take only spatial data as input. Exploiting this information in inference usually involves the use of compact representations such as the Bird's Eye View (BEV) projection, which entails a loss of information and thus hinders the joint inference of all the parameters of the objects' 3D boxes. In this paper, we present a fully end-to-end 3D object detection framework that can infer oriented 3D boxes solely from BEV images by using a two-stage object detector and ad-hoc regression branches, eliminating the need for a post-processing stage. The method outperforms its predecessor (BirdNet) by a large margin and obtains state-of-the-art results on the KITTI 3D Object Detection Benchmark for all the categories in evaluation.



### Domain Adversarial Training for Infrared-colour Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2003.04191v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.04191v1)
- **Published**: 2020-03-09 15:17:15+00:00
- **Updated**: 2020-03-09 15:17:15+00:00
- **Authors**: Nima Mohammadi Meshky, Sara Iodice, Krystian Mikolajczyk
- **Comment**: None
- **Journal**: ICDP 2019
- **Summary**: Person re-identification (re-ID) is a very active area of research in computer vision, due to the role it plays in video surveillance. Currently, most methods only address the task of matching between colour images. However, in poorly-lit environments CCTV cameras switch to infrared imaging, hence developing a system which can correctly perform matching between infrared and colour images is a necessity. In this paper, we propose a part-feature extraction network to better focus on subtle, unique signatures on the person which are visible across both infrared and colour modalities. To train the model we propose a novel variant of the domain adversarial feature-learning framework. Through extensive experimentation, we show that our approach outperforms state-of-the-art methods.



### Toward Cross-Domain Speech Recognition with End-to-End Models
- **Arxiv ID**: http://arxiv.org/abs/2003.04194v1
- **DOI**: None
- **Categories**: **eess.AS**, cs.CV, cs.LG, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/2003.04194v1)
- **Published**: 2020-03-09 15:19:53+00:00
- **Updated**: 2020-03-09 15:19:53+00:00
- **Authors**: Thai-Son Nguyen, Sebastian Stüker, Alex Waibel
- **Comment**: Presented in Life-Long Learning for Spoken Language Systems Workshop
  - ASRU 2019
- **Journal**: None
- **Summary**: In the area of multi-domain speech recognition, research in the past focused on hybrid acoustic models to build cross-domain and domain-invariant speech recognition systems. In this paper, we empirically examine the difference in behavior between hybrid acoustic models and neural end-to-end systems when mixing acoustic training data from several domains. For these experiments we composed a multi-domain dataset from public sources, with the different domains in the corpus covering a wide variety of topics and acoustic conditions such as telephone conversations, lectures, read speech and broadcast news. We show that for the hybrid models, supplying additional training data from other domains with mismatched acoustic conditions does not increase the performance on specific domains. However, our end-to-end models optimized with sequence-based criterion generalize better than the hybrid models on diverse domains. In term of word-error-rate performance, our experimental acoustic-to-word and attention-based models trained on multi-domain dataset reach the performance of domain-specific long short-term memory (LSTM) hybrid models, thus resulting in multi-domain speech recognition systems that do not suffer in performance over domain specific ones. Moreover, the use of neural end-to-end models eliminates the need of domain-adapted language models during recognition, which is a great advantage when the input domain is unknown.



### Semantic Object Prediction and Spatial Sound Super-Resolution with Binaural Sounds
- **Arxiv ID**: http://arxiv.org/abs/2003.04210v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2003.04210v1)
- **Published**: 2020-03-09 15:49:01+00:00
- **Updated**: 2020-03-09 15:49:01+00:00
- **Authors**: Arun Balajee Vasudevan, Dengxin Dai, Luc Van Gool
- **Comment**: Project page:
  https://www.trace.ethz.ch/publications/2020/sound_perception/index.html
- **Journal**: None
- **Summary**: Humans can robustly recognize and localize objects by integrating visual and auditory cues. While machines are able to do the same now with images, less work has been done with sounds. This work develops an approach for dense semantic labelling of sound-making objects, purely based on binaural sounds. We propose a novel sensor setup and record a new audio-visual dataset of street scenes with eight professional binaural microphones and a 360 degree camera. The co-existence of visual and audio cues is leveraged for supervision transfer. In particular, we employ a cross-modal distillation framework that consists of a vision `teacher' method and a sound `student' method -- the student method is trained to generate the same results as the teacher method. This way, the auditory system can be trained without using human annotations. We also propose two auxiliary tasks namely, a) a novel task on Spatial Sound Super-resolution to increase the spatial resolution of sounds, and b) dense depth prediction of the scene. We then formulate the three tasks into one end-to-end trainable multi-tasking network aiming to boost the overall performance. Experimental results on the dataset show that 1) our method achieves promising results for semantic prediction and the two auxiliary tasks; and 2) the three tasks are mutually beneficial -- training them together achieves the best performance and 3) the number and orientations of microphones are both important. The data and code will be released to facilitate the research in this new direction.



### Hierarchical Kinematic Human Mesh Recovery
- **Arxiv ID**: http://arxiv.org/abs/2003.04232v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2003.04232v2)
- **Published**: 2020-03-09 16:15:11+00:00
- **Updated**: 2020-07-14 17:01:33+00:00
- **Authors**: Georgios Georgakis, Ren Li, Srikrishna Karanam, Terrence Chen, Jana Kosecka, Ziyan Wu
- **Comment**: 17 pages, 8 figures, 5 tables, ECCV 2020
- **Journal**: None
- **Summary**: We consider the problem of estimating a parametric model of 3D human mesh from a single image. While there has been substantial recent progress in this area with direct regression of model parameters, these methods only implicitly exploit the human body kinematic structure, leading to sub-optimal use of the model prior. In this work, we address this gap by proposing a new technique for regression of human parametric model that is explicitly informed by the known hierarchical structure, including joint interdependencies of the model. This results in a strong prior-informed design of the regressor architecture and an associated hierarchical optimization that is flexible to be used in conjunction with the current standard frameworks for 3D human mesh recovery. We demonstrate these aspects by means of extensive experiments on standard benchmark datasets, showing how our proposed new design outperforms several existing and popular methods, establishing new state-of-the-art results. By considering joint interdependencies, our method is equipped to infer joints even under data corruptions, which we demonstrate by conducting experiments under varying degrees of occlusion.



### On the Road with 16 Neurons: Mental Imagery with Bio-inspired Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2003.08745v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.08745v1)
- **Published**: 2020-03-09 16:46:29+00:00
- **Updated**: 2020-03-09 16:46:29+00:00
- **Authors**: Alice Plebe, Mauro Da Lio
- **Comment**: 18 pages, 10 figures
- **Journal**: None
- **Summary**: This paper proposes a strategy for visual prediction in the context of autonomous driving. Humans, when not distracted or drunk, are still the best drivers you can currently find. For this reason we take inspiration from two theoretical ideas about the human mind and its neural organization. The first idea concerns how the brain uses a hierarchical structure of neuron ensembles to extract abstract concepts from visual experience and code them into compact representations. The second idea suggests that these neural perceptual representations are not neutral but functional to the prediction of the future state of affairs in the environment. Similarly, the prediction mechanism is not neutral but oriented to the current planning of a future action. We identify within the deep learning framework two artificial counterparts of the aforementioned neurocognitive theories. We find a correspondence between the first theoretical idea and the architecture of convolutional autoencoders, while we translate the second theory into a training procedure that learns compact representations which are not neutral but oriented to driving tasks, from two distinct perspectives. From a static perspective, we force groups of neural units in the compact representations to distinctly represent specific concepts crucial to the driving task. From a dynamic perspective, we encourage the compact representations to be predictive of how the current road scenario will change in the future. We successfully learn compact representations that use as few as 16 neural units for each of the two basic driving concepts we consider: car and lane. We prove the efficiency of our proposed perceptual representations on the SYNTHIA dataset. Our source code is available at https://github.com/3lis/rnn_vae



### PLOP: Probabilistic poLynomial Objects trajectory Planning for autonomous driving
- **Arxiv ID**: http://arxiv.org/abs/2003.08744v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2003.08744v3)
- **Published**: 2020-03-09 16:55:07+00:00
- **Updated**: 2020-10-22 08:29:19+00:00
- **Authors**: Thibault Buhet, Emilie Wirbel, Andrei Bursuc, Xavier Perrotton
- **Comment**: Accepted at CorRL 2020 (matching camera-ready version)
- **Journal**: None
- **Summary**: To navigate safely in urban environments, an autonomous vehicle (ego vehicle) must understand and anticipate its surroundings, in particular the behavior and intents of other road users (neighbors). Most of the times, multiple decision choices are acceptable for all road users (e.g., turn right or left, or different ways of avoiding an obstacle), leading to a highly uncertain and multi-modal decision space. We focus here on predicting multiple feasible future trajectories for both ego vehicle and neighbors through a probabilistic framework. We rely on a conditional imitation learning algorithm, conditioned by a navigation command for the ego vehicle (e.g., "turn right"). Our model processes ego vehicle front-facing camera images and bird-eye view grid, computed from Lidar point clouds, with detections of past and present objects, in order to generate multiple trajectories for both ego vehicle and its neighbors. Our approach is computationally efficient and relies only on on-board sensors. We evaluate our method offline on the publicly available dataset nuScenes, achieving state-of-the-art performance, investigate the impact of our architecture choices on online simulated experiments and show preliminary insights for real vehicle control



### Motion-Attentive Transition for Zero-Shot Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2003.04253v3
- **DOI**: 10.1109/TIP.2020.3013162
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.04253v3)
- **Published**: 2020-03-09 16:58:42+00:00
- **Updated**: 2020-07-09 17:34:32+00:00
- **Authors**: Tianfei Zhou, Shunzhou Wang, Yi Zhou, Yazhou Yao, Jianwu Li, Ling Shao
- **Comment**: AAAI 2020. Code: https://github.com/tfzhou/MATNet
- **Journal**: None
- **Summary**: In this paper, we present a novel Motion-Attentive Transition Network (MATNet) for zero-shot video object segmentation, which provides a new way of leveraging motion information to reinforce spatio-temporal object representation. An asymmetric attention block, called Motion-Attentive Transition (MAT), is designed within a two-stream encoder, which transforms appearance features into motion-attentive representations at each convolutional stage. In this way, the encoder becomes deeply interleaved, allowing for closely hierarchical interactions between object motion and appearance. This is superior to the typical two-stream architecture, which treats motion and appearance separately in each stream and often suffers from overfitting to appearance information. Additionally, a bridge network is proposed to obtain a compact, discriminative and scale-sensitive representation for multi-level encoder features, which is further fed into a decoder to achieve segmentation results. Extensive experiments on three challenging public benchmarks (i.e. DAVIS-16, FBMS and Youtube-Objects) show that our model achieves compelling performance against the state-of-the-arts.



### SOIC: Semantic Online Initialization and Calibration for LiDAR and Camera
- **Arxiv ID**: http://arxiv.org/abs/2003.04260v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.04260v1)
- **Published**: 2020-03-09 17:02:31+00:00
- **Updated**: 2020-03-09 17:02:31+00:00
- **Authors**: Weimin Wang, Shohei Nobuhara, Ryosuke Nakamura, Ken Sakurada
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a novel semantic-based online extrinsic calibration approach, SOIC (so, I see), for Light Detection and Ranging (LiDAR) and camera sensors. Previous online calibration methods usually need prior knowledge of rough initial values for optimization. The proposed approach removes this limitation by converting the initialization problem to a Perspective-n-Point (PnP) problem with the introduction of semantic centroids (SCs). The closed-form solution of this PnP problem has been well researched and can be found with existing PnP methods. Since the semantic centroid of the point cloud usually does not accurately match with that of the corresponding image, the accuracy of parameters are not improved even after a nonlinear refinement process. Thus, a cost function based on the constraint of the correspondence between semantic elements from both point cloud and image data is formulated. Subsequently, optimal extrinsic parameters are estimated by minimizing the cost function. We evaluate the proposed method either with GT or predicted semantics on KITTI dataset. Experimental results and comparisons with the baseline method verify the feasibility of the initialization strategy and the accuracy of the calibration approach. In addition, we release the source code at https://github.com/--/SOIC.



### Cascaded Human-Object Interaction Recognition
- **Arxiv ID**: http://arxiv.org/abs/2003.04262v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.04262v2)
- **Published**: 2020-03-09 17:05:04+00:00
- **Updated**: 2020-03-11 10:54:41+00:00
- **Authors**: Tianfei Zhou, Wenguan Wang, Siyuan Qi, Haibin Ling, Jianbing Shen
- **Comment**: Accepted to CVPR 2020. Winner of the ICCV-2019 PIC Challenge on both
  HOIW and PIC tracks. Code: https://github.com/tfzhou/C-HOI
- **Journal**: None
- **Summary**: Rapid progress has been witnessed for human-object interaction (HOI) recognition, but most existing models are confined to single-stage reasoning pipelines. Considering the intrinsic complexity of the task, we introduce a cascade architecture for a multi-stage, coarse-to-fine HOI understanding. At each stage, an instance localization network progressively refines HOI proposals and feeds them into an interaction recognition network. Each of the two networks is also connected to its predecessor at the previous stage, enabling cross-stage information propagation. The interaction recognition network has two crucial parts: a relation ranking module for high-quality HOI proposal selection and a triple-stream classifier for relation prediction. With our carefully-designed human-centric relation features, these two modules work collaboratively towards effective interaction understanding. Further beyond relation detection on a bounding-box level, we make our framework flexible to perform fine-grained pixel-wise relation segmentation; this provides a new glimpse into better relation modeling. Our approach reached the $1^{st}$ place in the ICCV2019 Person in Context Challenge, on both relation detection and segmentation tasks. It also shows promising results on V-COCO.



### How to Train Your Super-Net: An Analysis of Training Heuristics in Weight-Sharing NAS
- **Arxiv ID**: http://arxiv.org/abs/2003.04276v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.04276v2)
- **Published**: 2020-03-09 17:34:32+00:00
- **Updated**: 2020-06-17 13:42:15+00:00
- **Authors**: Kaicheng Yu, Rene Ranftl, Mathieu Salzmann
- **Comment**: Updated with latest results on NASBench-101, now we achieve 0.48
  sparse Kendall-Tau on this space
- **Journal**: None
- **Summary**: Weight sharing promises to make neural architecture search (NAS) tractable even on commodity hardware. Existing methods in this space rely on a diverse set of heuristics to design and train the shared-weight backbone network, a.k.a. the super-net. Since heuristics and hyperparameters substantially vary across different methods, a fair comparison between them can only be achieved by systematically analyzing the influence of these factors. In this paper, we therefore provide a systematic evaluation of the heuristics and hyperparameters that are frequently employed by weight-sharing NAS algorithms. Our analysis uncovers that some commonly-used heuristics for super-net training negatively impact the correlation between super-net and stand-alone performance, and evidences the strong influence of certain hyperparameters and architectural choices. Our code and experiments set a strong and reproducible baseline that future works can build on.



### Restore from Restored: Single Image Denoising with Pseudo Clean Image
- **Arxiv ID**: http://arxiv.org/abs/2003.04721v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.04721v3)
- **Published**: 2020-03-09 17:35:31+00:00
- **Updated**: 2020-11-18 06:35:28+00:00
- **Authors**: Seunghwan Lee, Dongkyu Lee, Donghyeon Cho, Jiwon Kim, Tae Hyun Kim
- **Comment**: None
- **Journal**: None
- **Summary**: In this study, we propose a simple and effective fine-tuning algorithm called "restore-from-restored", which can greatly enhance the performance of fully pre-trained image denoising networks. Many supervised denoising approaches can produce satisfactory results using large external training datasets. However, these methods have limitations in using internal information available in a given test image. By contrast, recent self-supervised approaches can remove noise in the input image by utilizing information from the specific test input. However, such methods show relatively lower performance on known noise types such as Gaussian noise compared to supervised methods. Thus, to combine external and internal information, we fine-tune the fully pre-trained denoiser using pseudo training set at test time. By exploiting internal self-similar patches (i.e., patch-recurrence), the baseline network can be adapted to the given specific input image. We demonstrate that our method can be easily employed on top of the state-of-the-art denoising networks and further improve the performance on numerous denoising benchmark datasets including real noisy images.



### Restore from Restored: Video Restoration with Pseudo Clean Video
- **Arxiv ID**: http://arxiv.org/abs/2003.04279v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.04279v3)
- **Published**: 2020-03-09 17:37:28+00:00
- **Updated**: 2021-03-15 04:46:32+00:00
- **Authors**: Seunghwan Lee, Donghyeon Cho, Jiwon Kim, Tae Hyun Kim
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: In this study, we propose a self-supervised video denoising method called "restore-from-restored." This method fine-tunes a pre-trained network by using a pseudo clean video during the test phase. The pseudo clean video is obtained by applying a noisy video to the baseline network. By adopting a fully convolutional neural network (FCN) as the baseline, we can improve video denoising performance without accurate optical flow estimation and registration steps, in contrast to many conventional video restoration methods, due to the translation equivariant property of the FCN. Specifically, the proposed method can take advantage of plentiful similar patches existing across multiple consecutive frames (i.e., patch-recurrence); these patches can boost the performance of the baseline network by a large margin. We analyze the restoration performance of the fine-tuned video denoising networks with the proposed self-supervision-based learning algorithm, and demonstrate that the FCN can utilize recurring patches without requiring accurate registration among adjacent frames. In our experiments, we apply the proposed method to state-of-the-art denoisers and show that our fine-tuned networks achieve a considerable improvement in denoising performance.



### Deep Inverse Feature Learning: A Representation Learning of Error
- **Arxiv ID**: http://arxiv.org/abs/2003.04285v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.04285v1)
- **Published**: 2020-03-09 17:45:44+00:00
- **Updated**: 2020-03-09 17:45:44+00:00
- **Authors**: Behzad Ghazanfari, Fatemeh Afghah
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces a novel perspective about error in machine learning and proposes inverse feature learning (IFL) as a representation learning approach that learns a set of high-level features based on the representation of error for classification or clustering purposes. The proposed perspective about error representation is fundamentally different from current learning methods, where in classification approaches they interpret the error as a function of the differences between the true labels and the predicted ones or in clustering approaches, in which the clustering objective functions such as compactness are used. Inverse feature learning method operates based on a deep clustering approach to obtain a qualitative form of the representation of error as features. The performance of the proposed IFL method is evaluated by applying the learned features along with the original features, or just using the learned features in different classification and clustering techniques for several data sets. The experimental results show that the proposed method leads to promising results in classification and especially in clustering. In classification, the proposed features along with the primary features improve the results of most of the classification methods on several popular data sets. In clustering, the performance of different clustering methods is considerably improved on different data sets. There are interesting results that show some few features of the representation of error capture highly informative aspects of primary features. We hope this paper helps to utilize the error representation learning in different feature learning domains.



### Manifold Regularization for Locally Stable Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2003.04286v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2003.04286v2)
- **Published**: 2020-03-09 17:45:44+00:00
- **Updated**: 2020-09-22 22:53:45+00:00
- **Authors**: Charles Jin, Martin Rinard
- **Comment**: None
- **Journal**: None
- **Summary**: We apply concepts from manifold regularization to develop new regularization techniques for training locally stable deep neural networks. Our regularizers are based on a sparsification of the graph Laplacian which holds with high probability when the data is sparse in high dimensions, as is common in deep learning. Empirically, our networks exhibit stability in a diverse set of perturbation models, including $\ell_2$, $\ell_\infty$, and Wasserstein-based perturbations; in particular, we achieve 40% adversarial accuracy on CIFAR-10 against an adaptive PGD attack using $\ell_\infty$ perturbations of size $\epsilon = 8/255$, and state-of-the-art verified accuracy of 21% in the same perturbation model. Furthermore, our techniques are efficient, incurring overhead on par with two additional parallel forward passes through the network.



### Knowledge distillation via adaptive instance normalization
- **Arxiv ID**: http://arxiv.org/abs/2003.04289v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.04289v1)
- **Published**: 2020-03-09 17:50:12+00:00
- **Updated**: 2020-03-09 17:50:12+00:00
- **Authors**: Jing Yang, Brais Martinez, Adrian Bulat, Georgios Tzimiropoulos
- **Comment**: None
- **Journal**: None
- **Summary**: This paper addresses the problem of model compression via knowledge distillation. To this end, we propose a new knowledge distillation method based on transferring feature statistics, specifically the channel-wise mean and variance, from the teacher to the student. Our method goes beyond the standard way of enforcing the mean and variance of the student to be similar to those of the teacher through an $L_2$ loss, which we found it to be of limited effectiveness. Specifically, we propose a new loss based on adaptive instance normalization to effectively transfer the feature statistics. The main idea is to transfer the learned statistics back to the teacher via adaptive instance normalization (conditioned on the student) and let the teacher network "evaluate" via a loss whether the statistics learned by the student are reliably transferred. We show that our distillation method outperforms other state-of-the-art distillation methods over a large set of experimental settings including different (a) network architectures, (b) teacher-student capacities, (c) datasets, and (d) domains.



### Improved Baselines with Momentum Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2003.04297v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.04297v1)
- **Published**: 2020-03-09 17:56:49+00:00
- **Updated**: 2020-03-09 17:56:49+00:00
- **Authors**: Xinlei Chen, Haoqi Fan, Ross Girshick, Kaiming He
- **Comment**: Tech report, 2 pages + references
- **Journal**: None
- **Summary**: Contrastive unsupervised learning has recently shown encouraging progress, e.g., in Momentum Contrast (MoCo) and SimCLR. In this note, we verify the effectiveness of two of SimCLR's design improvements by implementing them in the MoCo framework. With simple modifications to MoCo---namely, using an MLP projection head and more data augmentation---we establish stronger baselines that outperform SimCLR and do not require large training batches. We hope this will make state-of-the-art unsupervised learning research more accessible. Code will be made public.



### On Compositions of Transformations in Contrastive Self-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2003.04298v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.04298v3)
- **Published**: 2020-03-09 17:56:49+00:00
- **Updated**: 2021-10-27 12:00:29+00:00
- **Authors**: Mandela Patrick, Yuki M. Asano, Polina Kuznetsova, Ruth Fong, João F. Henriques, Geoffrey Zweig, Andrea Vedaldi
- **Comment**: Accepted to ICCV 2021. Code and pretrained models are available at
  https://github.com/facebookresearch/GDT
- **Journal**: None
- **Summary**: In the image domain, excellent representations can be learned by inducing invariance to content-preserving transformations via noise contrastive learning. In this paper, we generalize contrastive learning to a wider set of transformations, and their compositions, for which either invariance or distinctiveness is sought. We show that it is not immediately obvious how existing methods such as SimCLR can be extended to do so. Instead, we introduce a number of formal requirements that all contrastive formulations must satisfy, and propose a practical construction which satisfies these requirements. In order to maximise the reach of this analysis, we express all components of noise contrastive formulations as the choice of certain generalized transformations of the data (GDTs), including data sampling. We then consider videos as an example of data in which a large variety of transformations are applicable, accounting for the extra modalities -- for which we analyze audio and text -- and the dimension of time. We find that being invariant to certain transformations and distinctive to others is critical to learning effective video representations, improving the state-of-the-art for multiple benchmarks by a large margin, and even surpassing supervised pretraining.



### Cross modal video representations for weakly supervised active speaker localization
- **Arxiv ID**: http://arxiv.org/abs/2003.04358v2
- **DOI**: 10.1109/TMM.2022.3229975
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2003.04358v2)
- **Published**: 2020-03-09 18:50:50+00:00
- **Updated**: 2021-11-03 22:30:39+00:00
- **Authors**: Rahul Sharma, Krishna Somandepalli, Shrikanth Narayanan
- **Comment**: None
- **Journal**: None
- **Summary**: An objective understanding of media depictions, such as inclusive portrayals of how much someone is heard and seen on screen such as in film and television, requires the machines to discern automatically who, when, how, and where someone is talking, and not. Speaker activity can be automatically discerned from the rich multimodal information present in the media content. This is however a challenging problem due to the vast variety and contextual variability in the media content, and the lack of labeled data. In this work, we present a cross-modal neural network for learning visual representations, which have implicit information pertaining to the spatial location of a speaker in the visual frames. Avoiding the need for manual annotations for active speakers in visual frames, acquiring of which is very expensive, we present a weakly supervised system for the task of localizing active speakers in movie content. We use the learned cross-modal visual representations, and provide weak supervision from movie subtitles acting as a proxy for voice activity, thus requiring no manual annotations. We evaluate the performance of the proposed system on the AVA active speaker dataset and demonstrate the effectiveness of the cross-modal embeddings for localizing active speakers in comparison to fully supervised systems. We also demonstrate state-of-the-art performance for the task of voice activity detection in an audio-visual framework, especially when speech is accompanied by noise and music.



### Automatic segmentation of spinal multiple sclerosis lesions: How to generalize across MRI contrasts?
- **Arxiv ID**: http://arxiv.org/abs/2003.04377v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.04377v3)
- **Published**: 2020-03-09 19:29:45+00:00
- **Updated**: 2020-06-03 18:33:24+00:00
- **Authors**: Olivier Vincent, Charley Gros, Joseph Paul Cohen, Julien Cohen-Adad
- **Comment**: Presented at OHBM 2020 (v2-3 : corrected typos)
- **Journal**: None
- **Summary**: Despite recent improvements in medical image segmentation, the ability to generalize across imaging contrasts remains an open issue. To tackle this challenge, we implement Feature-wise Linear Modulation (FiLM) to leverage physics knowledge within the segmentation model and learn the characteristics of each contrast. Interestingly, a well-optimised U-Net reached the same performance as our FiLMed-Unet on a multi-contrast dataset (0.72 of Dice score), which suggests that there is a bottleneck in spinal MS lesion segmentation different from the generalization across varying contrasts. This bottleneck likely stems from inter-rater variability, which is estimated at 0.61 of Dice score in our dataset.



### Continuous Domain Adaptation with Variational Domain-Agnostic Feature Replay
- **Arxiv ID**: http://arxiv.org/abs/2003.04382v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.04382v1)
- **Published**: 2020-03-09 19:50:24+00:00
- **Updated**: 2020-03-09 19:50:24+00:00
- **Authors**: Qicheng Lao, Xiang Jiang, Mohammad Havaei, Yoshua Bengio
- **Comment**: None
- **Journal**: None
- **Summary**: Learning in non-stationary environments is one of the biggest challenges in machine learning. Non-stationarity can be caused by either task drift, i.e., the drift in the conditional distribution of labels given the input data, or the domain drift, i.e., the drift in the marginal distribution of the input data. This paper aims to tackle this challenge in the context of continuous domain adaptation, where the model is required to learn new tasks adapted to new domains in a non-stationary environment while maintaining previously learned knowledge. To deal with both drifts, we propose variational domain-agnostic feature replay, an approach that is composed of three components: an inference module that filters the input data into domain-agnostic representations, a generative module that facilitates knowledge transfer, and a solver module that applies the filtered and transferable knowledge to solve the queries. We address the two fundamental scenarios in continuous domain adaptation, demonstrating the effectiveness of our proposed approach for practical usage.



### Spine intervertebral disc labeling using a fully convolutional redundant counting model
- **Arxiv ID**: http://arxiv.org/abs/2003.04387v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.04387v2)
- **Published**: 2020-03-09 20:02:31+00:00
- **Updated**: 2020-03-11 14:13:59+00:00
- **Authors**: Lucas Rouhier, Francisco Perdigon Romero, Joseph Paul Cohen, Julien Cohen-Adad
- **Comment**: MIDL 2020
- **Journal**: None
- **Summary**: Labeling intervertebral discs is relevant as it notably enables clinicians to understand the relationship between a patient's symptoms (pain, paralysis) and the exact level of spinal cord injury. However manually labeling those discs is a tedious and user-biased task which would benefit from automated methods. While some automated methods already exist for MRI and CT-scan, they are either not publicly available, or fail to generalize across various imaging contrasts. In this paper we combine a Fully Convolutional Network (FCN) with inception modules to localize and label intervertebral discs. We demonstrate a proof-of-concept application in a publicly-available multi-center and multi-contrast MRI database (n=235 subjects). The code is publicly available at https://github.com/neuropoly/vertebral-labeling-deep-learning.



### Meta-Baseline: Exploring Simple Meta-Learning for Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2003.04390v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.04390v4)
- **Published**: 2020-03-09 20:06:36+00:00
- **Updated**: 2021-08-19 06:15:12+00:00
- **Authors**: Yinbo Chen, Zhuang Liu, Huijuan Xu, Trevor Darrell, Xiaolong Wang
- **Comment**: ICCV 2021. Code: https://github.com/yinboc/few-shot-meta-baseline
- **Journal**: None
- **Summary**: Meta-learning has been the most common framework for few-shot learning in recent years. It learns the model from collections of few-shot classification tasks, which is believed to have a key advantage of making the training objective consistent with the testing objective. However, some recent works report that by training for whole-classification, i.e. classification on the whole label-set, it can get comparable or even better embedding than many meta-learning algorithms. The edge between these two lines of works has yet been underexplored, and the effectiveness of meta-learning in few-shot learning remains unclear. In this paper, we explore a simple process: meta-learning over a whole-classification pre-trained model on its evaluation metric. We observe this simple method achieves competitive performance to state-of-the-art methods on standard benchmarks. Our further analysis shed some light on understanding the trade-offs between the meta-learning objective and the whole-classification objective in few-shot learning.



### FusionLane: Multi-Sensor Fusion for Lane Marking Semantic Segmentation Using Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2003.04404v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2003.04404v1)
- **Published**: 2020-03-09 20:33:30+00:00
- **Updated**: 2020-03-09 20:33:30+00:00
- **Authors**: Ruochen Yin, Biao Yu, Huapeng Wu, Yutao Song, Runxin Niu
- **Comment**: None
- **Journal**: None
- **Summary**: It is a crucial step to achieve effective semantic segmentation of lane marking during the construction of the lane level high-precision map. In recent years, many image semantic segmentation methods have been proposed. These methods mainly focus on the image from camera, due to the limitation of the sensor itself, the accurate three-dimensional spatial position of the lane marking cannot be obtained, so the demand for the lane level high-precision map construction cannot be met. This paper proposes a lane marking semantic segmentation method based on LIDAR and camera fusion deep neural network. Different from other methods, in order to obtain accurate position information of the segmentation results, the semantic segmentation object of this paper is a bird's eye view converted from a LIDAR points cloud instead of an image captured by a camera. This method first uses the deeplabv3+ [\ref{ref:1}] network to segment the image captured by the camera, and the segmentation result is merged with the point clouds collected by the LIDAR as the input of the proposed network. In this neural network, we also add a long short-term memory (LSTM) structure to assist the network for semantic segmentation of lane markings by using the the time series information. The experiments on more than 14,000 image datasets which we have manually labeled and expanded have shown the proposed method has better performance on the semantic segmentation of the points cloud bird's eye view. Therefore, the automation of high-precision map construction can be significantly improved. Our code is available at https://github.com/rolandying/FusionLane.



### Generative Multi-Stream Architecture For American Sign Language Recognition
- **Arxiv ID**: http://arxiv.org/abs/2003.08743v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08743v1)
- **Published**: 2020-03-09 21:04:51+00:00
- **Updated**: 2020-03-09 21:04:51+00:00
- **Authors**: Dom Huh, Sai Gurrapu, Frederick Olson, Huzefa Rangwala, Parth Pathak, Jana Kosecka
- **Comment**: None
- **Journal**: None
- **Summary**: With advancements in deep model architectures, tasks in computer vision can reach optimal convergence provided proper data preprocessing and model parameter initialization. However, training on datasets with low feature-richness for complex applications limit and detriment optimal convergence below human performance. In past works, researchers have provided external sources of complementary data at the cost of supplementary hardware, which are fed in streams to counteract this limitation and boost performance. We propose a generative multi-stream architecture, eliminating the need for additional hardware with the intent to improve feature richness without risking impracticability. We also introduce the compact spatio-temporal residual block to the standard 3-dimensional convolutional model, C3D. Our rC3D model performs comparatively to the top C3D residual variant architecture, the pseudo-3D model, on the FASL-RGB dataset. Our methods have achieved 95.62% validation accuracy with a variance of 1.42% from training, outperforming past models by 0.45% in validation accuracy and 5.53% in variance.



### Texture Superpixel Clustering from Patch-based Nearest Neighbor Matching
- **Arxiv ID**: http://arxiv.org/abs/2003.04414v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.04414v1)
- **Published**: 2020-03-09 21:11:21+00:00
- **Updated**: 2020-03-09 21:11:21+00:00
- **Authors**: Rémi Giraud, Yannick Berthoumieu
- **Comment**: None
- **Journal**: None
- **Summary**: Superpixels are widely used in computer vision applications. Nevertheless, decomposition methods may still fail to efficiently cluster image pixels according to their local texture. In this paper, we propose a new Nearest Neighbor-based Superpixel Clustering (NNSC) method to generate texture-aware superpixels in a limited computational time compared to previous approaches. We introduce a new clustering framework using patch-based nearest neighbor matching, while most existing methods are based on a pixel-wise K-means clustering. Therefore, we directly group pixels in the patch space enabling to capture texture information. We demonstrate the efficiency of our method with favorable comparison in terms of segmentation performances on both standard color and texture datasets. We also show the computational efficiency of NNSC compared to recent texture-aware superpixel methods.



### Cloth in the Wind: A Case Study of Physical Measurement through Simulation
- **Arxiv ID**: http://arxiv.org/abs/2003.05065v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.05065v1)
- **Published**: 2020-03-09 21:32:23+00:00
- **Updated**: 2020-03-09 21:32:23+00:00
- **Authors**: Tom F. H. Runia, Kirill Gavrilyuk, Cees G. M. Snoek, Arnold W. M. Smeulders
- **Comment**: CVPR 2020. arXiv admin note: substantial text overlap with
  arXiv:1910.07861
- **Journal**: None
- **Summary**: For many of the physical phenomena around us, we have developed sophisticated models explaining their behavior. Nevertheless, measuring physical properties from visual observations is challenging due to the high number of causally underlying physical parameters -- including material properties and external forces. In this paper, we propose to measure latent physical properties for cloth in the wind without ever having seen a real example before. Our solution is an iterative refinement procedure with simulation at its core. The algorithm gradually updates the physical model parameters by running a simulation of the observed phenomenon and comparing the current simulation to a real-world observation. The correspondence is measured using an embedding function that maps physically similar examples to nearby points. We consider a case study of cloth in the wind, with curling flags as our leading example -- a seemingly simple phenomena but physically highly involved. Based on the physics of cloth and its visual manifestation, we propose an instantiation of the embedding function. For this mapping, modeled as a deep network, we introduce a spectral layer that decomposes a video volume into its temporal spectral power and corresponding frequencies. Our experiments demonstrate that the proposed method compares favorably to prior work on the task of measuring cloth material properties and external wind force from a real-world video.



### Multi-Scale Superpatch Matching using Dual Superpixel Descriptors
- **Arxiv ID**: http://arxiv.org/abs/2003.04428v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.04428v1)
- **Published**: 2020-03-09 22:04:04+00:00
- **Updated**: 2020-03-09 22:04:04+00:00
- **Authors**: Rémi Giraud, Merlin Boyer, Michaël Clément
- **Comment**: None
- **Journal**: Pattern Recognition Letters 2020
- **Summary**: Over-segmentation into superpixels is a very effective dimensionality reduction strategy, enabling fast dense image processing. The main issue of this approach is the inherent irregularity of the image decomposition compared to standard hierarchical multi-resolution schemes, especially when searching for similar neighboring patterns. Several works have attempted to overcome this issue by taking into account the region irregularity into their comparison model. Nevertheless, they remain sub-optimal to provide robust and accurate superpixel neighborhood descriptors, since they only compute features within each region, poorly capturing contour information at superpixel borders. In this work, we address these limitations by introducing the dual superpatch, a novel superpixel neighborhood descriptor. This structure contains features computed in reduced superpixel regions, as well as at the interfaces of multiple superpixels to explicitly capture contour structure information. A fast multi-scale non-local matching framework is also introduced for the search of similar descriptors at different resolution levels in an image dataset. The proposed dual superpatch enables to more accurately capture similar structured patterns at different scales, and we demonstrate the robustness and performance of this new strategy on matching and supervised labeling applications.



### SDVTracker: Real-Time Multi-Sensor Association and Tracking for Self-Driving Vehicles
- **Arxiv ID**: http://arxiv.org/abs/2003.04447v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.04447v1)
- **Published**: 2020-03-09 23:07:23+00:00
- **Updated**: 2020-03-09 23:07:23+00:00
- **Authors**: Shivam Gautam, Gregory P. Meyer, Carlos Vallespi-Gonzalez, Brian C. Becker
- **Comment**: 8 pages, 7 figures, Submitted to IROS 2020
- **Journal**: None
- **Summary**: Accurate motion state estimation of Vulnerable Road Users (VRUs), is a critical requirement for autonomous vehicles that navigate in urban environments. Due to their computational efficiency, many traditional autonomy systems perform multi-object tracking using Kalman Filters which frequently rely on hand-engineered association. However, such methods fail to generalize to crowded scenes and multi-sensor modalities, often resulting in poor state estimates which cascade to inaccurate predictions. We present a practical and lightweight tracking system, SDVTracker, that uses a deep learned model for association and state estimation in conjunction with an Interacting Multiple Model (IMM) filter. The proposed tracking method is fast, robust and generalizes across multiple sensor modalities and different VRU classes. In this paper, we detail a model that jointly optimizes both association and state estimation with a novel loss, an algorithm for determining ground-truth supervision, and a training procedure. We show this system significantly outperforms hand-engineered methods on a real-world urban driving dataset while running in less than 2.5 ms on CPU for a scene with 100 actors, making it suitable for self-driving applications where low latency and high accuracy is critical.



### Better Set Representations For Relational Reasoning
- **Arxiv ID**: http://arxiv.org/abs/2003.04448v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.04448v2)
- **Published**: 2020-03-09 23:07:27+00:00
- **Updated**: 2020-06-17 06:40:23+00:00
- **Authors**: Qian Huang, Horace He, Abhay Singh, Yan Zhang, Ser-Nam Lim, Austin Benson
- **Comment**: Preprint, 17 pages
- **Journal**: None
- **Summary**: Incorporating relational reasoning into neural networks has greatly expanded their capabilities and scope. One defining trait of relational reasoning is that it operates on a set of entities, as opposed to standard vector representations. Existing end-to-end approaches typically extract entities from inputs by directly interpreting the latent feature representations as a set. We show that these approaches do not respect set permutational invariance and thus have fundamental representational limitations. To resolve this limitation, we propose a simple and general network module called a Set Refiner Network (SRN). We first use synthetic image experiments to demonstrate how our approach effectively decomposes objects without explicit supervision. Then, we insert our module into existing relational reasoning models and show that respecting set invariance leads to substantial gains in prediction performance and robustness on several relational reasoning tasks.



### Single-view 2D CNNs with Fully Automatic Non-nodule Categorization for False Positive Reduction in Pulmonary Nodule Detection
- **Arxiv ID**: http://arxiv.org/abs/2003.04454v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.04454v1)
- **Published**: 2020-03-09 23:18:52+00:00
- **Updated**: 2020-03-09 23:18:52+00:00
- **Authors**: Hyunjun Eun, Daeyeong Kim, Chanho Jung, Changick Kim
- **Comment**: 12 pages, 16 figures
- **Journal**: Computer Methods and Programs in Biomedicine, vol. 165, Oct. 2018,
  pp. 215-224
- **Summary**: Background and Objective: In pulmonary nodule detection, the first stage, candidate detection, aims to detect suspicious pulmonary nodules. However, detected candidates include many false positives and thus in the following stage, false positive reduction, such false positives are reliably reduced. Note that this task is challenging due to 1) the imbalance between the numbers of nodules and non-nodules and 2) the intra-class diversity of non-nodules. Although techniques using 3D convolutional neural networks (CNNs) have shown promising performance, they suffer from high computational complexity which hinders constructing deep networks. To efficiently address these problems, we propose a novel framework using the ensemble of 2D CNNs using single views, which outperforms existing 3D CNN-based methods.   Methods: Our ensemble of 2D CNNs utilizes single-view 2D patches to improve both computational and memory efficiency compared to previous techniques exploiting 3D CNNs. We first categorize non-nodules on the basis of features encoded by an autoencoder. Then, all 2D CNNs are trained by using the same nodule samples, but with different types of non-nodules. By extending the learning capability, this training scheme resolves difficulties of extracting representative features from non-nodules with large appearance variations. Note that, instead of manual categorization requiring the heavy workload of radiologists, we propose to automatically categorize non-nodules based on the autoencoder and k-means clustering.



### Unsupervised Style and Content Separation by Minimizing Mutual Information for Speech Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2003.06227v1
- **DOI**: None
- **Categories**: **eess.AS**, cs.CV, cs.IT, cs.LG, cs.SD, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/2003.06227v1)
- **Published**: 2020-03-09 23:47:41+00:00
- **Updated**: 2020-03-09 23:47:41+00:00
- **Authors**: Ting-Yao Hu, Ashish Shrivastava, Oncel Tuzel, Chandra Dhir
- **Comment**: Accepted at ICASSP 2020 (for presentation in a lecture session)
- **Journal**: None
- **Summary**: We present a method to generate speech from input text and a style vector that is extracted from a reference speech signal in an unsupervised manner, i.e., no style annotation, such as speaker information, is required. Existing unsupervised methods, during training, generate speech by computing style from the corresponding ground truth sample and use a decoder to combine the style vector with the input text. Training the model in such a way leaks content information into the style vector. The decoder can use the leaked content and ignore some of the input text to minimize the reconstruction loss. At inference time, when the reference speech does not match the content input, the output may not contain all of the content of the input text. We refer to this problem as "content leakage", which we address by explicitly estimating and minimizing the mutual information between the style and the content through an adversarial training formulation. We call our method MIST - Mutual Information based Style Content Separation. The main goal of the method is to preserve the input content in the synthesized speech signal, which we measure by the word error rate (WER) and show substantial improvements over state-of-the-art unsupervised speech synthesis methods.



