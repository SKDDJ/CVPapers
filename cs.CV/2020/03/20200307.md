# Arxiv Papers in cs.CV on 2020-03-07
### SuPer Deep: A Surgical Perception Framework for Robotic Tissue Manipulation using Deep Learning for Feature Extraction
- **Arxiv ID**: http://arxiv.org/abs/2003.03472v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.03472v3)
- **Published**: 2020-03-07 00:08:30+00:00
- **Updated**: 2021-03-25 05:41:43+00:00
- **Authors**: Jingpei Lu, Ambareesh Jayakumari, Florian Richter, Yang Li, Michael C. Yip
- **Comment**: 7 pages, 7 figures, ICRA 2021 camera-ready version
- **Journal**: None
- **Summary**: Robotic automation in surgery requires precise tracking of surgical tools and mapping of deformable tissue. Previous works on surgical perception frameworks require significant effort in developing features for surgical tool and tissue tracking. In this work, we overcome the challenge by exploiting deep learning methods for surgical perception. We integrated deep neural networks, capable of efficient feature extraction, into the tissue reconstruction and instrument pose estimation processes. By leveraging transfer learning, the deep learning based approach requires minimal training data and reduced feature engineering efforts to fully perceive a surgical scene. The framework was tested on three publicly available datasets, which use the da Vinci Surgical System, for comprehensive analysis. Experimental results show that our framework achieves state-of-the-art tracking performance in a surgical environment by utilizing deep learning for feature extraction.



### PoseNet3D: Learning Temporally Consistent 3D Human Pose via Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2003.03473v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2003.03473v2)
- **Published**: 2020-03-07 00:10:59+00:00
- **Updated**: 2020-11-12 05:07:51+00:00
- **Authors**: Shashank Tripathi, Siddhant Ranade, Ambrish Tyagi, Amit Agrawal
- **Comment**: Accepted as Oral in 3DV 2020; supplementary material included; added
  results on 3DPW dataset in revision
- **Journal**: None
- **Summary**: Recovering 3D human pose from 2D joints is a highly unconstrained problem. We propose a novel neural network framework, PoseNet3D, that takes 2D joints as input and outputs 3D skeletons and SMPL body model parameters. By casting our learning approach in a student-teacher framework, we avoid using any 3D data such as paired/unpaired 3D data, motion capture sequences, depth images or multi-view images during training. We first train a teacher network that outputs 3D skeletons, using only 2D poses for training. The teacher network distills its knowledge to a student network that predicts 3D pose in SMPL representation. Finally, both the teacher and the student networks are jointly fine-tuned in an end-to-end manner using temporal, self-consistency and adversarial losses, improving the accuracy of each individual network. Results on Human3.6M dataset for 3D human pose estimation demonstrate that our approach reduces the 3D joint prediction error by 18% compared to previous unsupervised methods. Qualitative results on in-the-wild datasets show that the recovered 3D poses and meshes are natural, realistic, and flow smoothly over consecutive frames.



### Endoscopy disease detection challenge 2020
- **Arxiv ID**: http://arxiv.org/abs/2003.03376v1
- **DOI**: 10.21227/f8xg-wb80
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.03376v1)
- **Published**: 2020-03-07 00:41:28+00:00
- **Updated**: 2020-03-07 00:41:28+00:00
- **Authors**: Sharib Ali, Noha Ghatwary, Barbara Braden, Dominique Lamarque, Adam Bailey, Stefano Realdon, Renato Cannizzaro, Jens Rittscher, Christian Daul, James East
- **Comment**: None
- **Journal**: None
- **Summary**: Whilst many technologies are built around endoscopy, there is a need to have a comprehensive dataset collected from multiple centers to address the generalization issues with most deep learning frameworks. What could be more important than disease detection and localization? Through our extensive network of clinical and computational experts, we have collected, curated and annotated gastrointestinal endoscopy video frames. We have released this dataset and have launched disease detection and segmentation challenge EDD2020 https://edd2020.grand-challenge.org to address the limitations and explore new directions. EDD2020 is a crowd sourcing initiative to test the feasibility of recent deep learning methods and to promote research for building robust technologies. In this paper, we provide an overview of the EDD2020 dataset, challenge tasks, evaluation strategies and a short summary of results on test data. A detailed paper will be drafted after the challenge workshop with more detailed analysis of the results.



### ReActNet: Towards Precise Binary Neural Network with Generalized Activation Functions
- **Arxiv ID**: http://arxiv.org/abs/2003.03488v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.03488v2)
- **Published**: 2020-03-07 02:12:02+00:00
- **Updated**: 2020-07-13 03:05:51+00:00
- **Authors**: Zechun Liu, Zhiqiang Shen, Marios Savvides, Kwang-Ting Cheng
- **Comment**: Accepted to ECCV 2020. Code is available at:
  https://github.com/liuzechun/ReActNet
- **Journal**: None
- **Summary**: In this paper, we propose several ideas for enhancing a binary network to close its accuracy gap from real-valued networks without incurring any additional computational cost. We first construct a baseline network by modifying and binarizing a compact real-valued network with parameter-free shortcuts, bypassing all the intermediate convolutional layers including the downsampling layers. This baseline network strikes a good trade-off between accuracy and efficiency, achieving superior performance than most of existing binary networks at approximately half of the computational cost. Through extensive experiments and analysis, we observed that the performance of binary networks is sensitive to activation distribution variations. Based on this important observation, we propose to generalize the traditional Sign and PReLU functions, denoted as RSign and RPReLU for the respective generalized functions, to enable explicit learning of the distribution reshape and shift at near-zero extra cost. Lastly, we adopt a distributional loss to further enforce the binary network to learn similar output distributions as those of a real-valued network. We show that after incorporating all these ideas, the proposed ReActNet outperforms all the state-of-the-arts by a large margin. Specifically, it outperforms Real-to-Binary Net and MeliusNet29 by 4.0% and 3.6% respectively for the top-1 accuracy and also reduces the gap to its real-valued counterpart to within 3.0% top-1 accuracy on ImageNet dataset. Code and models are available at: https://github.com/liuzechun/ReActNet.



### Super Resolution Using Segmentation-Prior Self-Attention Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/2003.03489v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.03489v1)
- **Published**: 2020-03-07 02:13:14+00:00
- **Updated**: 2020-03-07 02:13:14+00:00
- **Authors**: Yuxin Zhang, Zuquan Zheng, Roland Hu
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Network (CNN) is intensively implemented to solve super resolution (SR) tasks because of its superior performance. However, the problem of super resolution is still challenging due to the lack of prior knowledge and small receptive field of CNN. We propose the Segmentation-Piror Self-Attention Generative Adversarial Network (SPSAGAN) to combine segmentation-priors and feature attentions into a unified framework. This combination is led by a carefully designed weighted addition to balance the influence of feature and segmentation attentions, so that the network can emphasize textures in the same segmentation category and meanwhile focus on the long-distance feature relationship. We also propose a lightweight skip connection architecture called Residual-in-Residual Sparse Block (RRSB) to further improve the super-resolution performance and save computation. Extensive experiments show that SPSAGAN can generate more realistic and visually pleasing textures compared to state-of-the-art SFTGAN and ESRGAN on many SR datasets.



### Semantic Change Pattern Analysis
- **Arxiv ID**: http://arxiv.org/abs/2003.03492v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.03492v1)
- **Published**: 2020-03-07 02:22:19+00:00
- **Updated**: 2020-03-07 02:22:19+00:00
- **Authors**: Wensheng Cheng, Yan Zhang, Xu Lei, Wen Yang, Guisong Xia
- **Comment**: 17 pages, 7 figures
- **Journal**: None
- **Summary**: Change detection is an important problem in vision field, especially for aerial images. However, most works focus on traditional change detection, i.e., where changes happen, without considering the change type information, i.e., what changes happen. Although a few works have tried to apply semantic information to traditional change detection, they either only give the label of emerging objects without taking the change type into consideration, or set some kinds of change subjectively without specifying semantic information. To make use of semantic information and analyze change types comprehensively, we propose a new task called semantic change pattern analysis for aerial images. Given a pair of co-registered aerial images, the task requires a result including both where and what changes happen. We then describe the metric adopted for the task, which is clean and interpretable. We further provide the first well-annotated aerial image dataset for this task. Extensive baseline experiments are conducted as reference for following works. The aim of this work is to explore high-level information based on change detection and facilitate the development of this field with the publicly available dataset.



### MatchingGAN: Matching-based Few-shot Image Generation
- **Arxiv ID**: http://arxiv.org/abs/2003.03497v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.03497v2)
- **Published**: 2020-03-07 03:09:06+00:00
- **Updated**: 2020-03-15 03:17:32+00:00
- **Authors**: Yan Hong, Li Niu, Jianfu Zhang, Liqing Zhang
- **Comment**: This paper is accepted for oral presentation at ICME
  2020(http://www.2020.ieeeicme.org/)
- **Journal**: None
- **Summary**: To generate new images for a given category, most deep generative models require abundant training images from this category, which are often too expensive to acquire. To achieve the goal of generation based on only a few images, we propose matching-based Generative Adversarial Network (GAN) for few-shot generation, which includes a matching generator and a matching discriminator. Matching generator can match random vectors with a few conditional images from the same category and generate new images for this category based on the fused features. The matching discriminator extends conventional GAN discriminator by matching the feature of generated image with the fused feature of conditional images. Extensive experiments on three datasets demonstrate the effectiveness of our proposed method.



### Weight mechanism: adding a constant in concatenation of series connect
- **Arxiv ID**: http://arxiv.org/abs/2003.03500v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.03500v2)
- **Published**: 2020-03-07 03:13:44+00:00
- **Updated**: 2020-11-18 13:47:43+00:00
- **Authors**: Xiaojie Qi
- **Comment**: 7 pages, 9 figures
- **Journal**: None
- **Summary**: It is a consensus that feature maps in the shallow layer are more related to image attributes such as texture and shape, whereas abstract semantic representation exists in the deep layer. Meanwhile, some image information will be lost in the process of the convolution operation. Naturally, the direct method is combining them together to gain lost detailed information through concatenation or adding. In fact, the image representation flowed in feature fusion can not match with the semantic representation completely, and the semantic deviation in different layers also destroy the information purification, that leads to useless information being mixed into the fusion layers. Therefore, it is crucial to narrow the gap among the fused layers and reduce the impact of noises during fusion. In this paper, we propose a method named weight mechanism to reduce the gap between feature maps in concatenation of series connection, and we get a better result of 0.80% mIoU improvement on Massachusetts building dataset by changing the weight of the concatenation of series connection in residual U-Net. Specifically, we design a new architecture named fused U-Net to test weight mechanism, and it also gains 0.12% mIoU improvement.



### Cross-modal Learning for Multi-modal Video Categorization
- **Arxiv ID**: http://arxiv.org/abs/2003.03501v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.03501v3)
- **Published**: 2020-03-07 03:21:15+00:00
- **Updated**: 2020-06-06 00:36:52+00:00
- **Authors**: Palash Goyal, Saurabh Sahu, Shalini Ghosh, Chul Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-modal machine learning (ML) models can process data in multiple modalities (e.g., video, audio, text) and are useful for video content analysis in a variety of problems (e.g., object detection, scene understanding, activity recognition). In this paper, we focus on the problem of video categorization using a multi-modal ML technique. In particular, we have developed a novel multi-modal ML approach that we call "cross-modal learning", where one modality influences another but only when there is correlation between the modalities -- for that, we first train a correlation tower that guides the main multi-modal video categorization tower in the model. We show how this cross-modal principle can be applied to different types of models (e.g., RNN, Transformer, NetVLAD), and demonstrate through experiments how our proposed multi-modal video categorization models with cross-modal learning out-perform strong state-of-the-art baseline models.



### Robust, Occlusion-aware Pose Estimation for Objects Grasped by Adaptive Hands
- **Arxiv ID**: http://arxiv.org/abs/2003.03518v1
- **DOI**: 10.1109/ICRA40945.2020.9197350
- **Categories**: **cs.RO**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.03518v1)
- **Published**: 2020-03-07 05:51:03+00:00
- **Updated**: 2020-03-07 05:51:03+00:00
- **Authors**: Bowen Wen, Chaitanya Mitash, Sruthi Soorian, Andrew Kimmel, Avishai Sintov, Kostas E. Bekris
- **Comment**: None
- **Journal**: IEEE International Conference on Robotics and Automation (ICRA)
  2020
- **Summary**: Many manipulation tasks, such as placement or within-hand manipulation, require the object's pose relative to a robot hand. The task is difficult when the hand significantly occludes the object. It is especially hard for adaptive hands, for which it is not easy to detect the finger's configuration. In addition, RGB-only approaches face issues with texture-less objects or when the hand and the object look similar. This paper presents a depth-based framework, which aims for robust pose estimation and short response times. The approach detects the adaptive hand's state via efficient parallel search given the highest overlap between the hand's model and the point cloud. The hand's point cloud is pruned and robust global registration is performed to generate object pose hypotheses, which are clustered. False hypotheses are pruned via physical reasoning. The remaining poses' quality is evaluated given agreement with observed data. Extensive evaluation on synthetic and real data demonstrates the accuracy and computational efficiency of the framework when applied on challenging, highly-occluded scenarios for different object types. An ablation study identifies how the framework's components help in performance. This work also provides a dataset for in-hand 6D object pose estimation. Code and dataset are available at: https://github.com/wenbowen123/icra20-hand-object-pose



### Distilling portable Generative Adversarial Networks for Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2003.03519v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.03519v1)
- **Published**: 2020-03-07 05:53:01+00:00
- **Updated**: 2020-03-07 05:53:01+00:00
- **Authors**: Hanting Chen, Yunhe Wang, Han Shu, Changyuan Wen, Chunjing Xu, Boxin Shi, Chao Xu, Chang Xu
- **Comment**: None
- **Journal**: AAAI 2020
- **Summary**: Despite Generative Adversarial Networks (GANs) have been widely used in various image-to-image translation tasks, they can be hardly applied on mobile devices due to their heavy computation and storage cost. Traditional network compression methods focus on visually recognition tasks, but never deal with generation tasks. Inspired by knowledge distillation, a student generator of fewer parameters is trained by inheriting the low-level and high-level information from the original heavy teacher generator. To promote the capability of student generator, we include a student discriminator to measure the distances between real images, and images generated by student and teacher generators. An adversarial learning process is therefore established to optimize student generator and student discriminator. Qualitative and quantitative analysis by conducting experiments on benchmark datasets demonstrate that the proposed method can learn portable generative models with strong performance.



### MobilePose: Real-Time Pose Estimation for Unseen Objects with Weak Shape Supervision
- **Arxiv ID**: http://arxiv.org/abs/2003.03522v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.03522v1)
- **Published**: 2020-03-07 06:23:35+00:00
- **Updated**: 2020-03-07 06:23:35+00:00
- **Authors**: Tingbo Hou, Adel Ahmadyan, Liangkai Zhang, Jianing Wei, Matthias Grundmann
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we address the problem of detecting unseen objects from RGB images and estimating their poses in 3D. We propose two mobile friendly networks: MobilePose-Base and MobilePose-Shape. The former is used when there is only pose supervision, and the latter is for the case when shape supervision is available, even a weak one. We revisit shape features used in previous methods, including segmentation and coordinate map. We explain when and why pixel-level shape supervision can improve pose estimation. Consequently, we add shape prediction as an intermediate layer in the MobilePose-Shape, and let the network learn pose from shape. Our models are trained on mixed real and synthetic data, with weak and noisy shape supervision. They are ultra lightweight that can run in real-time on modern mobile devices (e.g. 36 FPS on Galaxy S20). Comparing with previous single-shot solutions, our method has higher accuracy, while using a significantly smaller model (2~3% in model size or number of parameters).



### TTPP: Temporal Transformer with Progressive Prediction for Efficient Action Anticipation
- **Arxiv ID**: http://arxiv.org/abs/2003.03530v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.03530v1)
- **Published**: 2020-03-07 07:59:42+00:00
- **Updated**: 2020-03-07 07:59:42+00:00
- **Authors**: Wen Wang, Xiaojiang Peng, Yanzhou Su, Yu Qiao, Jian Cheng
- **Comment**: None
- **Journal**: None
- **Summary**: Video action anticipation aims to predict future action categories from observed frames. Current state-of-the-art approaches mainly resort to recurrent neural networks to encode history information into hidden states, and predict future actions from the hidden representations. It is well known that the recurrent pipeline is inefficient in capturing long-term information which may limit its performance in predication task. To address this problem, this paper proposes a simple yet efficient Temporal Transformer with Progressive Prediction (TTPP) framework, which repurposes a Transformer-style architecture to aggregate observed features, and then leverages a light-weight network to progressively predict future features and actions. Specifically, predicted features along with predicted probabilities are accumulated into the inputs of subsequent prediction. We evaluate our approach on three action datasets, namely TVSeries, THUMOS-14, and TV-Human-Interaction. Additionally we also conduct a comprehensive study for several popular aggregation and prediction strategies. Extensive results show that TTPP not only outperforms the state-of-the-art methods but also more efficient.



### Early Response Assessment in Lung Cancer Patients using Spatio-temporal CBCT Images
- **Arxiv ID**: http://arxiv.org/abs/2003.05408v1
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV, eess.IV, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/2003.05408v1)
- **Published**: 2020-03-07 08:20:22+00:00
- **Updated**: 2020-03-07 08:20:22+00:00
- **Authors**: Bijju Kranthi Veduruparthi, Jayanta Mukherjee, Partha Pratim Das, Mandira Saha, Sanjoy Chatterjee, Raj Kumar Shrimali, Soumendranath Ray, Sriram Prasath
- **Comment**: None
- **Journal**: None
- **Summary**: We report a model to predict patient's radiological response to curative radiation therapy (RT) for non-small-cell lung cancer (NSCLC).   Cone-Beam Computed Tomography images acquired weekly during the six-week course of RT were contoured with the Gross Tumor Volume (GTV) by senior radiation oncologists for 53 patients (7 images per patient).   Deformable registration of the images yielded six deformation fields for each pair of consecutive images per patient.   Jacobian of a field provides a measure of local expansion/contraction and is used in our model.   Delineations were compared post-registration to compute unchanged ($U$), newly grown ($G$), and reduced ($R$) regions within GTV.   The mean Jacobian of these regions $\mu_U$, $\mu_G$ and $\mu_R$ are statistically compared and a response assessment model is proposed.   A good response is hypothesized if $\mu_R < 1.0$, $\mu_R < \mu_U$, and $\mu_G < \mu_U$.   For early prediction of post-treatment response, first, three weeks' images are used.   Our model predicted clinical response with a precision of $74\%$.   Using reduction in CT numbers (CTN) and percentage GTV reduction as features in logistic regression, yielded an area-under-curve of 0.65 with p=0.005.   Combining logistic regression model with the proposed hypothesis yielded an odds ratio of 20.0 (p=0.0).



### Novel Radiomic Feature for Survival Prediction of Lung Cancer Patients using Low-Dose CBCT Images
- **Arxiv ID**: http://arxiv.org/abs/2003.03537v1
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/2003.03537v1)
- **Published**: 2020-03-07 08:47:26+00:00
- **Updated**: 2020-03-07 08:47:26+00:00
- **Authors**: Bijju Kranthi Veduruparthi, Jayanta Mukherjee, Partha Pratim Das, Moses Arunsingh, Raj Kumar Shrimali, Sriram Prasath, Soumendranath Ray, Sanjay Chatterjee
- **Comment**: Under review in SPIE Journal of Medical Imaging
- **Journal**: None
- **Summary**: Prediction of survivability in a patient for tumor progression is useful to estimate the effectiveness of a treatment protocol. In our work, we present a model to take into account the heterogeneous nature of a tumor to predict survival. The tumor heterogeneity is measured in terms of its mass by combining information regarding the radiodensity obtained in images with the gross tumor volume (GTV). We propose a novel feature called Tumor Mass within a GTV (TMG), that improves the prediction of survivability, compared to existing models which use GTV. Weekly variation in TMG of a patient is computed from the image data and also estimated from a cell survivability model. The parameters obtained from the cell survivability model are indicatives of changes in TMG over the treatment period. We use these parameters along with other patient metadata to perform survival analysis and regression. Cox's Proportional Hazard survival regression was performed using these data. Significant improvement in the average concordance index from 0.47 to 0.64 was observed when TMG is used in the model instead of GTV. The experiments show that there is a difference in the treatment response in responsive and non-responsive patients and that the proposed method can be used to predict patient survivability.



### Crowd Counting via Hierarchical Scale Recalibration Network
- **Arxiv ID**: http://arxiv.org/abs/2003.03545v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.03545v1)
- **Published**: 2020-03-07 10:06:47+00:00
- **Updated**: 2020-03-07 10:06:47+00:00
- **Authors**: Zhikang Zou, Yifan Liu, Shuangjie Xu, Wei Wei, Shiping Wen, Pan Zhou
- **Comment**: Accepted by ECAI2020
- **Journal**: None
- **Summary**: The task of crowd counting is extremely challenging due to complicated difficulties, especially the huge variation in vision scale. Previous works tend to adopt a naive concatenation of multi-scale information to tackle it, while the scale shifts between the feature maps are ignored. In this paper, we propose a novel Hierarchical Scale Recalibration Network (HSRNet), which addresses the above issues by modeling rich contextual dependencies and recalibrating multiple scale-associated information. Specifically, a Scale Focus Module (SFM) first integrates global context into local features by modeling the semantic inter-dependencies along channel and spatial dimensions sequentially. In order to reallocate channel-wise feature responses, a Scale Recalibration Module (SRM) adopts a step-by-step fusion to generate final density maps. Furthermore, we propose a novel Scale Consistency loss to constrain that the scale-associated outputs are coherent with groundtruth of different scales. With the proposed modules, our approach can ignore various noises selectively and focus on appropriate crowd scales automatically. Extensive experiments on crowd counting datasets (ShanghaiTech, MALL, WorldEXPO'10, and UCSD) show that our HSRNet can deliver superior results over all state-of-the-art approaches. More remarkably, we extend experiments on an extra vehicle dataset, whose results indicate that the proposed model is generalized to other applications.



### STD-Net: Structure-preserving and Topology-adaptive Deformation Network for 3D Reconstruction from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/2003.03551v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.03551v1)
- **Published**: 2020-03-07 11:02:47+00:00
- **Updated**: 2020-03-07 11:02:47+00:00
- **Authors**: Aihua Mao, Canglan Dai, Lin Gao, Ying He, Yong-jin Liu
- **Comment**: 14 pages,5 figures
- **Journal**: None
- **Summary**: 3D reconstruction from a single view image is a long-standing prob-lem in computer vision. Various methods based on different shape representations(such as point cloud or volumetric representations) have been proposed. However,the 3D shape reconstruction with fine details and complex structures are still chal-lenging and have not yet be solved. Thanks to the recent advance of the deepshape representations, it becomes promising to learn the structure and detail rep-resentation using deep neural networks. In this paper, we propose a novel methodcalled STD-Net to reconstruct the 3D models utilizing the mesh representationthat is well suitable for characterizing complex structure and geometry details.To reconstruct complex 3D mesh models with fine details, our method consists of(1) an auto-encoder network for recovering the structure of an object with bound-ing box representation from a single image, (2) a topology-adaptive graph CNNfor updating vertex position for meshes of complex topology, and (3) an unifiedmesh deformation block that deforms the structural boxes into structure-awaremeshed models. Experimental results on the images from ShapeNet show that ourproposed STD-Net has better performance than other state-of-the-art methods onreconstructing 3D objects with complex structures and fine geometric details.



### CPM R-CNN: Calibrating Point-guided Misalignment in Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2003.03570v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.03570v2)
- **Published**: 2020-03-07 12:29:43+00:00
- **Updated**: 2020-11-04 09:12:36+00:00
- **Authors**: Bin Zhu, Qing Song, Lu Yang, Zhihui Wang, Chun Liu, Mengjie Hu
- **Comment**: Accepted to WACV 2021
- **Journal**: None
- **Summary**: In object detection, offset-guided and point-guided regression dominate anchor-based and anchor-free method separately. Recently, point-guided approach is introduced to anchor-based method. However, we observe points predicted by this way are misaligned with matched region of proposals and score of localization, causing a notable gap in performance. In this paper, we propose CPM R-CNN which contains three efficient modules to optimize anchor-based point-guided method. According to sufficient evaluations on the COCO dataset, CPM R-CNN is demonstrated efficient to improve the localization accuracy by calibrating mentioned misalignment. Compared with Faster R-CNN and Grid R-CNN based on ResNet-101 with FPN, our approach can substantially improve detection mAP by 3.3% and 1.5% respectively without whistles and bells. Moreover, our best model achieves improvement by a large margin to 49.9% on COCO test-dev. Code and models will be publicly available.



### StyleGAN2 Distillation for Feed-forward Image Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2003.03581v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.03581v2)
- **Published**: 2020-03-07 14:02:06+00:00
- **Updated**: 2020-10-22 14:07:35+00:00
- **Authors**: Yuri Viazovetskyi, Vladimir Ivashkin, Evgeny Kashin
- **Comment**: Camera ready ECCV 2020
- **Journal**: None
- **Summary**: StyleGAN2 is a state-of-the-art network in generating realistic images. Besides, it was explicitly trained to have disentangled directions in latent space, which allows efficient image manipulation by varying latent factors. Editing existing images requires embedding a given image into the latent space of StyleGAN2. Latent code optimization via backpropagation is commonly used for qualitative embedding of real world images, although it is prohibitively slow for many applications. We propose a way to distill a particular image manipulation of StyleGAN2 into image-to-image network trained in paired way. The resulting pipeline is an alternative to existing GANs, trained on unpaired data. We provide results of human faces' transformation: gender swap, aging/rejuvenation, style transfer and image morphing. We show that the quality of generation using our method is comparable to StyleGAN2 backpropagation and current state-of-the-art methods in these particular tasks.



### Generative Low-bitwidth Data Free Quantization
- **Arxiv ID**: http://arxiv.org/abs/2003.03603v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.03603v3)
- **Published**: 2020-03-07 16:38:34+00:00
- **Updated**: 2020-08-10 12:56:06+00:00
- **Authors**: Shoukai Xu, Haokun Li, Bohan Zhuang, Jing Liu, Jiezhang Cao, Chuangrun Liang, Mingkui Tan
- **Comment**: eccv2020 code is available at: https://github.com/xushoukai/GDFQ
- **Journal**: None
- **Summary**: Neural network quantization is an effective way to compress deep models and improve their execution latency and energy efficiency, so that they can be deployed on mobile or embedded devices. Existing quantization methods require original data for calibration or fine-tuning to get better performance. However, in many real-world scenarios, the data may not be available due to confidential or private issues, thereby making existing quantization methods not applicable. Moreover, due to the absence of original data, the recently developed generative adversarial networks (GANs) cannot be applied to generate data. Although the full-precision model may contain rich data information, such information alone is hard to exploit for recovering the original data or generating new meaningful data. In this paper, we investigate a simple-yet-effective method called Generative Low-bitwidth Data Free Quantization (GDFQ) to remove the data dependence burden. Specifically, we propose a knowledge matching generator to produce meaningful fake data by exploiting classification boundary knowledge and distribution information in the pre-trained model. With the help of generated data, we can quantize a model by learning knowledge from the pre-trained model. Extensive experiments on three data sets demonstrate the effectiveness of our method. More critically, our method achieves much higher accuracy on 4-bit quantization than the existing data free quantization method. Code is available at https://github.com/xushoukai/GDFQ.



### DASNet: Dual attentive fully convolutional siamese networks for change detection of high resolution satellite images
- **Arxiv ID**: http://arxiv.org/abs/2003.03608v2
- **DOI**: 10.1109/JSTARS.2020.3037893
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.03608v2)
- **Published**: 2020-03-07 16:57:10+00:00
- **Updated**: 2020-11-11 04:32:22+00:00
- **Authors**: Jie Chen, Ziyang Yuan, Jian Peng, Li Chen, Haozhe Huang, Jiawei Zhu, Yu Liu, Haifeng Li
- **Comment**: 12 pages, 13 figures, 5 tables
- **Journal**: IEEE Journal of Selected Topics in Applied Earth Observations and
  Remote Sensing. 2020
- **Summary**: Change detection is a basic task of remote sensing image processing. The research objective is to identity the change information of interest and filter out the irrelevant change information as interference factors. Recently, the rise of deep learning has provided new tools for change detection, which have yielded impressive results. However, the available methods focus mainly on the difference information between multitemporal remote sensing images and lack robustness to pseudo-change information. To overcome the lack of resistance of current methods to pseudo-changes, in this paper, we propose a new method, namely, dual attentive fully convolutional Siamese networks (DASNet) for change detection in high-resolution images. Through the dual-attention mechanism, long-range dependencies are captured to obtain more discriminant feature representations to enhance the recognition performance of the model. Moreover, the imbalanced sample is a serious problem in change detection, i.e. unchanged samples are much more than changed samples, which is one of the main reasons resulting in pseudo-changes. We put forward the weighted double margin contrastive loss to address this problem by punishing the attention to unchanged feature pairs and increase attention to changed feature pairs. The experimental results of our method on the change detection dataset (CDD) and the building change detection dataset (BCDD) demonstrate that compared with other baseline methods, the proposed method realizes maximum improvements of 2.1\% and 3.6\%, respectively, in the F1 score. Our Pytorch implementation is available at https://github.com/lehaifeng/DASNet.



### AlphaNet: An Attention Guided Deep Network for Automatic Image Matting
- **Arxiv ID**: http://arxiv.org/abs/2003.03613v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, I.2.10; I.4.8; I.5.1
- **Links**: [PDF](http://arxiv.org/pdf/2003.03613v1)
- **Published**: 2020-03-07 17:25:21+00:00
- **Updated**: 2020-03-07 17:25:21+00:00
- **Authors**: Rishab Sharma, Rahul Deora, Anirudha Vishvakarma
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose an end to end solution for image matting i.e high-precision extraction of foreground objects from natural images. Image matting and background detection can be achieved easily through chroma keying in a studio setting when the background is either pure green or blue. Nonetheless, image matting in natural scenes with complex and uneven depth backgrounds remains a tedious task that requires human intervention. To achieve complete automatic foreground extraction in natural scenes, we propose a method that assimilates semantic segmentation and deep image matting processes into a single network to generate detailed semantic mattes for image composition task. The contribution of our proposed method is two-fold, firstly it can be interpreted as a fully automated semantic image matting method and secondly as a refinement of existing semantic segmentation models. We propose a novel model architecture as a combination of segmentation and matting that unifies the function of upsampling and downsampling operators with the notion of attention. As shown in our work, attention guided downsampling and upsampling can extract high-quality boundary details, unlike other normal downsampling and upsampling techniques. For achieving the same, we utilized an attention guided encoder-decoder framework which does unsupervised learning for generating an attention map adaptively from the data to serve and direct the upsampling and downsampling operators. We also construct a fashion e-commerce focused dataset with high-quality alpha mattes to facilitate the training and evaluation for image matting.



### Diffusion State Distances: Multitemporal Analysis, Fast Algorithms, and Applications to Biological Networks
- **Arxiv ID**: http://arxiv.org/abs/2003.03616v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG, math.PR
- **Links**: [PDF](http://arxiv.org/pdf/2003.03616v1)
- **Published**: 2020-03-07 17:43:34+00:00
- **Updated**: 2020-03-07 17:43:34+00:00
- **Authors**: Lenore Cowen, Kapil Devkota, Xiaozhe Hu, James M. Murphy, Kaiyi Wu
- **Comment**: 28 pages
- **Journal**: None
- **Summary**: Data-dependent metrics are powerful tools for learning the underlying structure of high-dimensional data. This article develops and analyzes a data-dependent metric known as diffusion state distance (DSD), which compares points using a data-driven diffusion process. Unlike related diffusion methods, DSDs incorporate information across time scales, which allows for the intrinsic data structure to be inferred in a parameter-free manner. This article develops a theory for DSD based on the multitemporal emergence of mesoscopic equilibria in the underlying diffusion process. New algorithms for denoising and dimension reduction with DSD are also proposed and analyzed. These approaches are based on a weighted spectral decomposition of the underlying diffusion process, and experiments on synthetic datasets and real biological networks illustrate the efficacy of the proposed algorithms in terms of both speed and accuracy. Throughout, comparisons with related methods are made, in order to illustrate the distinct advantages of DSD for datasets exhibiting multiscale structure.



### Explaining Knowledge Distillation by Quantifying the Knowledge
- **Arxiv ID**: http://arxiv.org/abs/2003.03622v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.03622v1)
- **Published**: 2020-03-07 18:09:17+00:00
- **Updated**: 2020-03-07 18:09:17+00:00
- **Authors**: Xu Cheng, Zhefan Rao, Yilan Chen, Quanshi Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a method to interpret the success of knowledge distillation by quantifying and analyzing task-relevant and task-irrelevant visual concepts that are encoded in intermediate layers of a deep neural network (DNN). More specifically, three hypotheses are proposed as follows. 1. Knowledge distillation makes the DNN learn more visual concepts than learning from raw data. 2. Knowledge distillation ensures that the DNN is prone to learning various visual concepts simultaneously. Whereas, in the scenario of learning from raw data, the DNN learns visual concepts sequentially. 3. Knowledge distillation yields more stable optimization directions than learning from raw data. Accordingly, we design three types of mathematical metrics to evaluate feature representations of the DNN. In experiments, we diagnosed various DNNs, and above hypotheses were verified.



### AL2: Progressive Activation Loss for Learning General Representations in Classification Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2003.03633v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.03633v1)
- **Published**: 2020-03-07 18:38:46+00:00
- **Updated**: 2020-03-07 18:38:46+00:00
- **Authors**: Majed El Helou, Frederike Dümbgen, Sabine Süsstrunk
- **Comment**: None
- **Journal**: IEEE International Conference on Acoustics, Speech, and Signal
  Processing (ICASSP 2020)
- **Summary**: The large capacity of neural networks enables them to learn complex functions. To avoid overfitting, networks however require a lot of training data that can be expensive and time-consuming to collect. A common practical approach to attenuate overfitting is the use of network regularization techniques. We propose a novel regularization method that progressively penalizes the magnitude of activations during training. The combined activation signals produced by all neurons in a given layer form the representation of the input image in that feature space. We propose to regularize this representation in the last feature layer before classification layers. Our method's effect on generalization is analyzed with label randomization tests and cumulative ablations. Experimental results show the advantages of our approach in comparison with commonly-used regularizers on standard benchmark datasets.



### Inferring Spatial Uncertainty in Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2003.03644v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2003.03644v2)
- **Published**: 2020-03-07 19:29:43+00:00
- **Updated**: 2020-08-01 07:11:18+00:00
- **Authors**: Zining Wang, Di Feng, Yiyang Zhou, Lars Rosenbaum, Fabian Timm, Klaus Dietmayer, Masayoshi Tomizuka, Wei Zhan
- **Comment**: None
- **Journal**: None
- **Summary**: The availability of real-world datasets is the prerequisite for developing object detection methods for autonomous driving. While ambiguity exists in object labels due to error-prone annotation process or sensor observation noises, current object detection datasets only provide deterministic annotations without considering their uncertainty. This precludes an in-depth evaluation among different object detection methods, especially for those that explicitly model predictive probability. In this work, we propose a generative model to estimate bounding box label uncertainties from LiDAR point clouds, and define a new representation of the probabilistic bounding box through spatial distribution. Comprehensive experiments show that the proposed model represents uncertainties commonly seen in driving scenarios. Based on the spatial distribution, we further propose an extension of IoU, called the Jaccard IoU (JIoU), as a new evaluation metric that incorporates label uncertainty. Experiments on the KITTI and the Waymo Open Datasets show that JIoU is superior to IoU when evaluating probabilistic object detectors.



### SalsaNext: Fast, Uncertainty-aware Semantic Segmentation of LiDAR Point Clouds for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2003.03653v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.03653v3)
- **Published**: 2020-03-07 20:17:06+00:00
- **Updated**: 2020-07-09 10:07:58+00:00
- **Authors**: Tiago Cortinhal, George Tzelepis, Eren Erdal Aksoy
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we introduce SalsaNext for the uncertainty-aware semantic segmentation of a full 3D LiDAR point cloud in real-time. SalsaNext is the next version of SalsaNet [1] which has an encoder-decoder architecture where the encoder unit has a set of ResNet blocks and the decoder part combines upsampled features from the residual blocks. In contrast to SalsaNet, we introduce a new context module, replace the ResNet encoder blocks with a new residual dilated convolution stack with gradually increasing receptive fields and add the pixel-shuffle layer in the decoder. Additionally, we switch from stride convolution to average pooling and also apply central dropout treatment. To directly optimize the Jaccard index, we further combine the weighted cross-entropy loss with Lovasz-Softmax loss [2]. We finally inject a Bayesian treatment to compute the epistemic and aleatoric uncertainties for each point in the cloud. We provide a thorough quantitative evaluation on the Semantic-KITTI dataset [3], which demonstrates that the proposed SalsaNext outperforms other state-of-the-art semantic segmentation networks and ranks first on the Semantic-KITTI leaderboard. We also release our source code https://github.com/TiagoCortinhal/SalsaNext.



### Adaptive Offline Quintuplet Loss for Image-Text Matching
- **Arxiv ID**: http://arxiv.org/abs/2003.03669v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.03669v3)
- **Published**: 2020-03-07 22:09:11+00:00
- **Updated**: 2020-07-22 14:58:18+00:00
- **Authors**: Tianlang Chen, Jiajun Deng, Jiebo Luo
- **Comment**: Accepted by ECCV 2020. Code is available at
  https://github.com/sunnychencool/AOQ
- **Journal**: None
- **Summary**: Existing image-text matching approaches typically leverage triplet loss with online hard negatives to train the model. For each image or text anchor in a training mini-batch, the model is trained to distinguish between a positive and the most confusing negative of the anchor mined from the mini-batch (i.e. online hard negative). This strategy improves the model's capacity to discover fine-grained correspondences and non-correspondences between image and text inputs. However, the above approach has the following drawbacks: (1) the negative selection strategy still provides limited chances for the model to learn from very hard-to-distinguish cases. (2) The trained model has weak generalization capability from the training set to the testing set. (3) The penalty lacks hierarchy and adaptiveness for hard negatives with different "hardness" degrees. In this paper, we propose solutions by sampling negatives offline from the whole training set. It provides "harder" offline negatives than online hard negatives for the model to distinguish. Based on the offline hard negatives, a quintuplet loss is proposed to improve the model's generalization capability to distinguish positives and negatives. In addition, a novel loss function that combines the knowledge of positives, offline hard negatives and online hard negatives is created. It leverages offline hard negatives as the intermediary to adaptively penalize them based on their distance relations to the anchor. We evaluate the proposed training approach on three state-of-the-art image-text models on the MS-COCO and Flickr30K datasets. Significant performance improvements are observed for all the models, proving the effectiveness and generality of our approach. Code is available at https://github.com/sunnychencool/AOQ



