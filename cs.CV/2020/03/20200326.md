# Arxiv Papers in cs.CV on 2020-03-26
### BachGAN: High-Resolution Image Synthesis from Salient Object Layout
- **Arxiv ID**: http://arxiv.org/abs/2003.11690v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.11690v2)
- **Published**: 2020-03-26 00:54:44+00:00
- **Updated**: 2020-03-27 20:53:24+00:00
- **Authors**: Yandong Li, Yu Cheng, Zhe Gan, Licheng Yu, Liqiang Wang, Jingjing Liu
- **Comment**: Accepted to CVPR 2020
- **Journal**: None
- **Summary**: We propose a new task towards more practical application for image generation - high-quality image synthesis from salient object layout. This new setting allows users to provide the layout of salient objects only (i.e., foreground bounding boxes and categories), and lets the model complete the drawing with an invented background and a matching foreground. Two main challenges spring from this new task: (i) how to generate fine-grained details and realistic textures without segmentation map input; and (ii) how to create a background and weave it seamlessly into standalone objects. To tackle this, we propose Background Hallucination Generative Adversarial Network (BachGAN), which first selects a set of segmentation maps from a large candidate pool via a background retrieval module, then encodes these candidate layouts via a background fusion module to hallucinate a suitable background for the given objects. By generating the hallucinated background representation dynamically, our model can synthesize high-resolution images with both photo-realistic foreground and integral background. Experiments on Cityscapes and ADE20K datasets demonstrate the advantage of BachGAN over existing methods, measured on both visual fidelity of generated images and visual alignment between output images and input layouts.



### Classification of Chinese Handwritten Numbers with Labeled Projective Dictionary Pair Learning
- **Arxiv ID**: http://arxiv.org/abs/2003.11700v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.11700v3)
- **Published**: 2020-03-26 01:43:59+00:00
- **Updated**: 2020-12-07 12:21:02+00:00
- **Authors**: Rasool Ameri, Ali Alameer, Saideh Ferdowsi, Kianoush Nazarpour, Vahid Abolghasemi
- **Comment**: None
- **Journal**: None
- **Summary**: Dictionary learning is a cornerstone of image classification. We set out to address a longstanding challenge in using dictionary learning for classification; that is to simultaneously maximise the discriminability and sparse-representability power of the learned dictionaries. Upon this premise, we designed class-specific dictionaries incorporating three factors: discriminability, sparsity and classification error. We integrated these metrics into a unified cost function and adopted a new feature space, i.e., histogram of oriented gradients (HOG), to generate the dictionary atoms. The rationale of using HOG features for designing the dictionaries is their strength in describing fine details of crowded images. The results of applying the proposed method in the classification of Chinese handwritten numbers demonstrated enhanced classification performance $(\sim98\%)$ compared to state-of-the-art deep learning techniques (i.e., SqueezeNet, GoogLeNet and MobileNetV2), but with a fraction of parameters. Furthermore, combination of the HOG features with dictionary learning enhances the accuracy by $11\%$ compared to the case where only pixel domain data are used. These results were supported when the proposed method was applied to both Arabic and English handwritten number databases.



### Mask Encoding for Single Shot Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2003.11712v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.11712v2)
- **Published**: 2020-03-26 02:51:17+00:00
- **Updated**: 2020-05-06 12:44:26+00:00
- **Authors**: Rufeng Zhang, Zhi Tian, Chunhua Shen, Mingyu You, Youliang Yan
- **Comment**: Accepted to Proc. IEEE Conf. Computer Vision and Pattern Recognition
  (CVPR), 2020
- **Journal**: None
- **Summary**: To date, instance segmentation is dominated by twostage methods, as pioneered by Mask R-CNN. In contrast, one-stage alternatives cannot compete with Mask R-CNN in mask AP, mainly due to the difficulty of compactly representing masks, making the design of one-stage methods very challenging. In this work, we propose a simple singleshot instance segmentation framework, termed mask encoding based instance segmentation (MEInst). Instead of predicting the two-dimensional mask directly, MEInst distills it into a compact and fixed-dimensional representation vector, which allows the instance segmentation task to be incorporated into one-stage bounding-box detectors and results in a simple yet efficient instance segmentation framework. The proposed one-stage MEInst achieves 36.4% in mask AP with single-model (ResNeXt-101-FPN backbone) and single-scale testing on the MS-COCO benchmark. We show that the much simpler and flexible one-stage instance segmentation method, can also achieve competitive performance. This framework can be easily adapted for other instance-level recognition tasks. Code is available at: https://git.io/AdelaiDet



### Fastidious Attention Network for Navel Orange Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2003.11734v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.11734v1)
- **Published**: 2020-03-26 03:59:22+00:00
- **Updated**: 2020-03-26 03:59:22+00:00
- **Authors**: Xiaoye Sun, Gongyan Li, Shaoyun Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning achieves excellent performance in many domains, so we not only apply it to the navel orange semantic segmentation task to solve the two problems of distinguishing defect categories and identifying the stem end and blossom end, but also propose a fastidious attention mechanism to further improve model performance. This lightweight attention mechanism includes two learnable parameters, activations and thresholds, to capture long-range dependence. Specifically, the threshold picks out part of the spatial feature map and the activation excite this area. Based on activations and thresholds training from different types of feature maps, we design fastidious self-attention module (FSAM) and fastidious inter-attention module (FIAM). And then construct the Fastidious Attention Network (FANet), which uses U-Net as the backbone and embeds these two modules, to solve the problems with semantic segmentation for stem end, blossom end, flaw and ulcer. Compared with some state-of-the-art deep-learning-based networks under our navel orange dataset, experiments show that our network is the best performance with pixel accuracy 99.105%, mean accuracy 77.468%, mean IU 70.375% and frequency weighted IU 98.335%. And embedded modules show better discrimination of 5 categories including background, especially the IU of flaw is increased by 3.165%.



### Egoshots, an ego-vision life-logging dataset and semantic fidelity metric to evaluate diversity in image captioning models
- **Arxiv ID**: http://arxiv.org/abs/2003.11743v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.11743v2)
- **Published**: 2020-03-26 04:43:30+00:00
- **Updated**: 2020-03-27 09:16:33+00:00
- **Authors**: Pranav Agarwal, Alejandro Betancourt, Vana Panagiotou, Natalia Díaz-Rodríguez
- **Comment**: 15 pages, 25 figures, Accepted at Machine Learning in Real Life
  (ML-IRL) ICLR 2020 Workshop
- **Journal**: None
- **Summary**: Image captioning models have been able to generate grammatically correct and human understandable sentences. However most of the captions convey limited information as the model used is trained on datasets that do not caption all possible objects existing in everyday life. Due to this lack of prior information most of the captions are biased to only a few objects present in the scene, hence limiting their usage in daily life. In this paper, we attempt to show the biased nature of the currently existing image captioning models and present a new image captioning dataset, Egoshots, consisting of 978 real life images with no captions. We further exploit the state of the art pre-trained image captioning and object recognition networks to annotate our images and show the limitations of existing works. Furthermore, in order to evaluate the quality of the generated captions, we propose a new image captioning metric, object based Semantic Fidelity (SF). Existing image captioning metrics can evaluate a caption only in the presence of their corresponding annotations; however, SF allows evaluating captions generated for images without annotations, making it highly useful for real life generated captions.



### Real-time 3D Deep Multi-Camera Tracking
- **Arxiv ID**: http://arxiv.org/abs/2003.11753v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.11753v1)
- **Published**: 2020-03-26 06:08:19+00:00
- **Updated**: 2020-03-26 06:08:19+00:00
- **Authors**: Quanzeng You, Hao Jiang
- **Comment**: 17 pages, 8 figures
- **Journal**: None
- **Summary**: Tracking a crowd in 3D using multiple RGB cameras is a challenging task. Most previous multi-camera tracking algorithms are designed for offline setting and have high computational complexity. Robust real-time multi-camera 3D tracking is still an unsolved problem. In this work, we propose a novel end-to-end tracking pipeline, Deep Multi-Camera Tracking (DMCT), which achieves reliable real-time multi-camera people tracking. Our DMCT consists of 1) a fast and novel perspective-aware Deep GroudPoint Network, 2) a fusion procedure for ground-plane occupancy heatmap estimation, 3) a novel Deep Glimpse Network for person detection and 4) a fast and accurate online tracker. Our design fully unleashes the power of deep neural network to estimate the "ground point" of each person in each color image, which can be optimized to run efficiently and robustly. Our fusion procedure, glimpse network and tracker merge the results from different views, find people candidates using multiple video frames and then track people on the fused heatmap. Our system achieves the state-of-the-art tracking results while maintaining real-time performance. Apart from evaluation on the challenging WILDTRACK dataset, we also collect two more tracking datasets with high-quality labels from two different environments and camera settings. Our experimental results confirm that our proposed real-time pipeline gives superior results to previous approaches.



### The 1st Challenge on Remote Physiological Signal Sensing (RePSS)
- **Arxiv ID**: http://arxiv.org/abs/2003.11756v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.11756v1)
- **Published**: 2020-03-26 06:17:54+00:00
- **Updated**: 2020-03-26 06:17:54+00:00
- **Authors**: Xiaobai Li, Hu Han, Hao Lu, Xuesong Niu, Zitong Yu, Antitza Dantcheva, Guoying Zhao, Shiguang Shan
- **Comment**: None
- **Journal**: None
- **Summary**: Remote measurement of physiological signals from videos is an emerging topic. The topic draws great interests, but the lack of publicly available benchmark databases and a fair validation platform are hindering its further development. For this concern, we organize the first challenge on Remote Physiological Signal Sensing (RePSS), in which two databases of VIPL and OBF are provided as the benchmark for kin researchers to evaluate their approaches. The 1st challenge of RePSS focuses on measuring the average heart rate from facial videos, which is the basic problem of remote physiological measurement. This paper presents an overview of the challenge, including data, protocol, analysis of results and discussion. The top ranked solutions are highlighted to provide insights for researchers, and future directions are outlined for this topic and this challenge.



### DeepCrashTest: Turning Dashcam Videos into Virtual Crash Tests for Automated Driving Systems
- **Arxiv ID**: http://arxiv.org/abs/2003.11766v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.11766v1)
- **Published**: 2020-03-26 07:03:45+00:00
- **Updated**: 2020-03-26 07:03:45+00:00
- **Authors**: Sai Krishna Bashetty, Heni Ben Amor, Georgios Fainekos
- **Comment**: 8 pages, 5 figures, ICRA 2020, Trajectory Extraction, Trajectory
  Simulation
- **Journal**: None
- **Summary**: The goal of this paper is to generate simulations with real-world collision scenarios for training and testing autonomous vehicles. We use numerous dashcam crash videos uploaded on the internet to extract valuable collision data and recreate the crash scenarios in a simulator. We tackle the problem of extracting 3D vehicle trajectories from videos recorded by an unknown and uncalibrated monocular camera source using a modular approach. A working architecture and demonstration videos along with the open-source implementation are provided with the paper.



### Image Generation Via Minimizing Fréchet Distance in Discriminator Feature Space
- **Arxiv ID**: http://arxiv.org/abs/2003.11774v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.11774v2)
- **Published**: 2020-03-26 07:37:18+00:00
- **Updated**: 2020-03-30 20:35:11+00:00
- **Authors**: Khoa D. Doan, Saurav Manchanda, Fengjiao Wang, Sathiya Keerthi, Avradeep Bhowmik, Chandan K. Reddy
- **Comment**: None
- **Journal**: None
- **Summary**: For a given image generation problem, the intrinsic image manifold is often low dimensional. We use the intuition that it is much better to train the GAN generator by minimizing the distributional distance between real and generated images in a small dimensional feature space representing such a manifold than on the original pixel-space. We use the feature space of the GAN discriminator for such a representation. For distributional distance, we employ one of two choices: the Fr\'{e}chet distance or direct optimal transport (OT); these respectively lead us to two new GAN methods: Fr\'{e}chet-GAN and OT-GAN. The idea of employing Fr\'{e}chet distance comes from the success of Fr\'{e}chet Inception Distance as a solid evaluation metric in image generation. Fr\'{e}chet-GAN is attractive in several ways. We propose an efficient, numerically stable approach to calculate the Fr\'{e}chet distance and its gradient. The Fr\'{e}chet distance estimation requires a significantly less computation time than OT; this allows Fr\'{e}chet-GAN to use much larger mini-batch size in training than OT. More importantly, we conduct experiments on a number of benchmark datasets and show that Fr\'{e}chet-GAN (in particular) and OT-GAN have significantly better image generation capabilities than the existing representative primal and dual GAN approaches based on the Wasserstein distance.



### Compact Deep Aggregation for Set Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2003.11794v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.11794v1)
- **Published**: 2020-03-26 08:43:15+00:00
- **Updated**: 2020-03-26 08:43:15+00:00
- **Authors**: Yujie Zhong, Relja Arandjelović, Andrew Zisserman
- **Comment**: 20 pages
- **Journal**: None
- **Summary**: The objective of this work is to learn a compact embedding of a set of descriptors that is suitable for efficient retrieval and ranking, whilst maintaining discriminability of the individual descriptors. We focus on a specific example of this general problem -- that of retrieving images containing multiple faces from a large scale dataset of images. Here the set consists of the face descriptors in each image, and given a query for multiple identities, the goal is then to retrieve, in order, images which contain all the identities, all but one, \etc   To this end, we make the following contributions: first, we propose a CNN architecture -- {\em SetNet} -- to achieve the objective: it learns face descriptors and their aggregation over a set to produce a compact fixed length descriptor designed for set retrieval, and the score of an image is a count of the number of identities that match the query; second, we show that this compact descriptor has minimal loss of discriminability up to two faces per image, and degrades slowly after that -- far exceeding a number of baselines; third, we explore the speed vs.\ retrieval quality trade-off for set retrieval using this compact descriptor; and, finally, we collect and annotate a large dataset of images containing various number of celebrities, which we use for evaluation and is publicly released.



### Neural encoding and interpretation for high-level visual cortices based on fMRI using image caption features
- **Arxiv ID**: http://arxiv.org/abs/2003.11797v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2003.11797v1)
- **Published**: 2020-03-26 08:47:21+00:00
- **Updated**: 2020-03-26 08:47:21+00:00
- **Authors**: Kai Qiao, Chi Zhang, Jian Chen, Linyuan Wang, Li Tong, Bin Yan
- **Comment**: None
- **Journal**: None
- **Summary**: On basis of functional magnetic resonance imaging (fMRI), researchers are devoted to designing visual encoding models to predict the neuron activity of human in response to presented image stimuli and analyze inner mechanism of human visual cortices. Deep network structure composed of hierarchical processing layers forms deep network models by learning features of data on specific task through big dataset. Deep network models have powerful and hierarchical representation of data, and have brought about breakthroughs for visual encoding, while revealing hierarchical structural similarity with the manner of information processing in human visual cortices. However, previous studies almost used image features of those deep network models pre-trained on classification task to construct visual encoding models. Except for deep network structure, the task or corresponding big dataset is also important for deep network models, but neglected by previous studies. Because image classification is a relatively fundamental task, it is difficult to guide deep network models to master high-level semantic representations of data, which causes into that encoding performance for high-level visual cortices is limited. In this study, we introduced one higher-level vision task: image caption (IC) task and proposed the visual encoding model based on IC features (ICFVEM) to encode voxels of high-level visual cortices. Experiment demonstrated that ICFVEM obtained better encoding performance than previous deep network models pre-trained on classification task. In addition, the interpretation of voxels was realized to explore the detailed characteristics of voxels based on the visualization of semantic words, and comparative analysis implied that high-level visual cortices behaved the correlative representation of image content.



### Do Deep Minds Think Alike? Selective Adversarial Attacks for Fine-Grained Manipulation of Multiple Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2003.11816v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, math.OC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.11816v1)
- **Published**: 2020-03-26 10:00:33+00:00
- **Updated**: 2020-03-26 10:00:33+00:00
- **Authors**: Zain Khan, Jirong Yi, Raghu Mudumbai, Xiaodong Wu, Weiyu Xu
- **Comment**: 9 pages, submitted to ICML 2020
- **Journal**: None
- **Summary**: Recent works have demonstrated the existence of {\it adversarial examples} targeting a single machine learning system. In this paper we ask a simple but fundamental question of "selective fooling": given {\it multiple} machine learning systems assigned to solve the same classification problem and taking the same input signal, is it possible to construct a perturbation to the input signal that manipulates the outputs of these {\it multiple} machine learning systems {\it simultaneously} in arbitrary pre-defined ways? For example, is it possible to selectively fool a set of "enemy" machine learning systems but does not fool the other "friend" machine learning systems? The answer to this question depends on the extent to which these different machine learning systems "think alike". We formulate the problem of "selective fooling" as a novel optimization problem, and report on a series of experiments on the MNIST dataset. Our preliminary findings from these experiments show that it is in fact very easy to selectively manipulate multiple MNIST classifiers simultaneously, even when the classifiers are identical in their architectures, training algorithms and training datasets except for random initialization during training. This suggests that two nominally equivalent machine learning systems do not in fact "think alike" at all, and opens the possibility for many novel applications and deeper understandings of the working principles of deep neural networks.



### Hit-Detector: Hierarchical Trinity Architecture Search for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2003.11818v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.11818v1)
- **Published**: 2020-03-26 10:20:52+00:00
- **Updated**: 2020-03-26 10:20:52+00:00
- **Authors**: Jianyuan Guo, Kai Han, Yunhe Wang, Chao Zhang, Zhaohui Yang, Han Wu, Xinghao Chen, Chang Xu
- **Comment**: Accepted in CVPR 2020
- **Journal**: None
- **Summary**: Neural Architecture Search (NAS) has achieved great success in image classification task. Some recent works have managed to explore the automatic design of efficient backbone or feature fusion layer for object detection. However, these methods focus on searching only one certain component of object detector while leaving others manually designed. We identify the inconsistency between searched component and manually designed ones would withhold the detector of stronger performance. To this end, we propose a hierarchical trinity search framework to simultaneously discover efficient architectures for all components (i.e. backbone, neck, and head) of object detector in an end-to-end manner. In addition, we empirically reveal that different parts of the detector prefer different operators. Motivated by this, we employ a novel scheme to automatically screen different sub search spaces for different components so as to perform the end-to-end search for each component on the corresponding sub search space efficiently. Without bells and whistles, our searched architecture, namely Hit-Detector, achieves 41.4\% mAP on COCO minival set with 27M parameters. Our implementation is available at https://github.com/ggjy/HitDet.pytorch.



### Robust Classification of High-Dimensional Spectroscopy Data Using Deep Learning and Data Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2003.11842v1
- **DOI**: 10.1021/acs.jcim.9b01037
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.11842v1)
- **Published**: 2020-03-26 11:33:52+00:00
- **Updated**: 2020-03-26 11:33:52+00:00
- **Authors**: James Houston, Frank G. Glavin, Michael G. Madden
- **Comment**: Journal of Chemical Information and Modeling
- **Journal**: None
- **Summary**: This paper presents a new approach to classification of high dimensional spectroscopy data and demonstrates that it outperforms other current state-of-the art approaches. The specific task we consider is identifying whether samples contain chlorinated solvents or not, based on their Raman spectra. We also examine robustness to classification of outlier samples that are not represented in the training set (negative outliers). A novel application of a locally-connected neural network (NN) for the binary classification of spectroscopy data is proposed and demonstrated to yield improved accuracy over traditionally popular algorithms. Additionally, we present the ability to further increase the accuracy of the locally-connected NN algorithm through the use of synthetic training spectra and we investigate the use of autoencoder based one-class classifiers and outlier detectors. Finally, a two-step classification process is presented as an alternative to the binary and one-class classification paradigms. This process combines the locally-connected NN classifier, the use of synthetic training data, and an autoencoder based outlier detector to produce a model which is shown to both produce high classification accuracy, and be robust to the presence of negative outliers.



### P $\approx$ NP, at least in Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2003.11844v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.11844v2)
- **Published**: 2020-03-26 11:36:09+00:00
- **Updated**: 2020-03-27 08:27:28+00:00
- **Authors**: Shailza Jolly, Sebastian Palacio, Joachim Folz, Federico Raue, Joern Hees, Andreas Dengel
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, progress in the Visual Question Answering (VQA) field has largely been driven by public challenges and large datasets. One of the most widely-used of these is the VQA 2.0 dataset, consisting of polar ("yes/no") and non-polar questions. Looking at the question distribution over all answers, we find that the answers "yes" and "no" account for 38 % of the questions, while the remaining 62% are spread over the more than 3000 remaining answers. While several sources of biases have already been investigated in the field, the effects of such an over-representation of polar vs. non-polar questions remain unclear. In this paper, we measure the potential confounding factors when polar and non-polar samples are used jointly to train a baseline VQA classifier, and compare it to an upper bound where the over-representation of polar questions is excluded from the training. Further, we perform cross-over experiments to analyze how well the feature spaces align. Contrary to expectations, we find no evidence of counterproductive effects in the joint training of unbalanced classes. In fact, by exploring the intermediate feature space of visual-text embeddings, we find that the feature space of polar questions already encodes sufficient structure to answer many non-polar questions. Our results indicate that the polar (P) and the non-polar (NP) feature spaces are strongly aligned, hence the expression P $\approx$ NP



### Weakly-supervised 3D coronary artery reconstruction from two-view angiographic images
- **Arxiv ID**: http://arxiv.org/abs/2003.11846v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.11846v1)
- **Published**: 2020-03-26 11:41:38+00:00
- **Updated**: 2020-03-26 11:41:38+00:00
- **Authors**: Lu Wang, Dong-xue Liang, Xiao-lei Yin, Jing Qiu, Zhi-yun Yang, Jun-hui Xing, Jian-zeng Dong, Zhao-yuan Ma
- **Comment**: None
- **Journal**: None
- **Summary**: The reconstruction of three-dimensional models of coronary arteries is of great significance for the localization, evaluation and diagnosis of stenosis and plaque in the arteries, as well as for the assisted navigation of interventional surgery. In the clinical practice, physicians use a few angles of coronary angiography to capture arterial images, so it is of great practical value to perform 3D reconstruction directly from coronary angiography images. However, this is a very difficult computer vision task due to the complex shape of coronary blood vessels, as well as the lack of data set and key point labeling. With the rise of deep learning, more and more work is being done to reconstruct 3D models of human organs from medical images using deep neural networks. We propose an adversarial and generative way to reconstruct three dimensional coronary artery models, from two different views of angiographic images of coronary arteries. With 3D fully supervised learning and 2D weakly supervised learning schemes, we obtained reconstruction accuracies that outperform state-of-art techniques.



### Coronary Artery Segmentation in Angiographic Videos Using A 3D-2D CE-Net
- **Arxiv ID**: http://arxiv.org/abs/2003.11851v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.11851v3)
- **Published**: 2020-03-26 11:56:17+00:00
- **Updated**: 2020-05-18 01:29:50+00:00
- **Authors**: Lu Wang, Dong-xue Liang, Xiao-lei Yin, Jing Qiu, Zhi-yun Yang, Jun-hui Xing, Jian-zeng Dong, Zhao-yuan Ma
- **Comment**: None
- **Journal**: None
- **Summary**: Coronary angiography is an indispensable assistive technique for cardiac interventional surgery. Segmentation and extraction of blood vessels from coronary angiography videos are very essential prerequisites for physicians to locate, assess and diagnose the plaques and stenosis in blood vessels. This article proposes a new video segmentation framework that can extract the clearest and most comprehensive coronary angiography images from a video sequence, thereby helping physicians to better observe the condition of blood vessels. This framework combines a 3D convolutional layer to extract spatial--temporal information from a video sequence and a 2D CE--Net to accomplish the segmentation task of an image sequence. The input is a few continuous frames of angiographic video, and the output is a mask of segmentation result. From the results of segmentation and extraction, we can get good segmentation results despite the poor quality of coronary angiography video sequences.



### Instance Credibility Inference for Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2003.11853v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.11853v2)
- **Published**: 2020-03-26 12:01:15+00:00
- **Updated**: 2020-04-03 01:40:28+00:00
- **Authors**: Yikai Wang, Chengming Xu, Chen Liu, Li Zhang, Yanwei Fu
- **Comment**: accepted by CVPR 2020
- **Journal**: None
- **Summary**: Few-shot learning (FSL) aims to recognize new objects with extremely limited training data for each category. Previous efforts are made by either leveraging meta-learning paradigm or novel principles in data augmentation to alleviate this extremely data-scarce problem. In contrast, this paper presents a simple statistical approach, dubbed Instance Credibility Inference (ICI) to exploit the distribution support of unlabeled instances for few-shot learning. Specifically, we first train a linear classifier with the labeled few-shot examples and use it to infer the pseudo-labels for the unlabeled data. To measure the credibility of each pseudo-labeled instance, we then propose to solve another linear regression hypothesis by increasing the sparsity of the incidental parameters and rank the pseudo-labeled instances with their sparsity degree. We select the most trustworthy pseudo-labeled instances alongside the labeled examples to re-train the linear classifier. This process is iterated until all the unlabeled samples are included in the expanded training set, i.e. the pseudo-label is converged for unlabeled data pool. Extensive experiments under two few-shot settings show that our simple approach can establish new state-of-the-arts on four widely used few-shot learning benchmark datasets including miniImageNet, tieredImageNet, CIFAR-FS, and CUB. Our code is available at: https://github.com/Yikai-Wang/ICI-FSL



### DCNAS: Densely Connected Neural Architecture Search for Semantic Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2003.11883v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.11883v2)
- **Published**: 2020-03-26 13:21:33+00:00
- **Updated**: 2021-03-27 15:03:47+00:00
- **Authors**: Xiong Zhang, Hongmin Xu, Hong Mo, Jianchao Tan, Cheng Yang, Lei Wang, Wenqi Ren
- **Comment**: accepted by CVPR 2021
- **Journal**: None
- **Summary**: Neural Architecture Search (NAS) has shown great potentials in automatically designing scalable network architectures for dense image predictions. However, existing NAS algorithms usually compromise on restricted search space and search on proxy task to meet the achievable computational demands. To allow as wide as possible network architectures and avoid the gap between target and proxy dataset, we propose a Densely Connected NAS (DCNAS) framework, which directly searches the optimal network structures for the multi-scale representations of visual information, over a large-scale target dataset. Specifically, by connecting cells with each other using learnable weights, we introduce a densely connected search space to cover an abundance of mainstream network designs. Moreover, by combining both path-level and channel-level sampling strategies, we design a fusion module to reduce the memory consumption of ample search space. We demonstrate that the architecture obtained from our DCNAS algorithm achieves state-of-the-art performances on public semantic image segmentation benchmarks, including 84.3% on Cityscapes, and 86.9% on PASCAL VOC 2012. We also retain leading performances when evaluating the architecture on the more challenging ADE20K and Pascal Context dataset.



### Matrix Smoothing: A Regularization for DNN with Transition Matrix under Noisy Labels
- **Arxiv ID**: http://arxiv.org/abs/2003.11904v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.11904v1)
- **Published**: 2020-03-26 13:49:37+00:00
- **Updated**: 2020-03-26 13:49:37+00:00
- **Authors**: Xianbin Lv, Dongxian Wu, Shu-Tao Xia
- **Comment**: ICME 2020
- **Journal**: None
- **Summary**: Training deep neural networks (DNNs) in the presence of noisy labels is an important and challenging task. Probabilistic modeling, which consists of a classifier and a transition matrix, depicts the transformation from true labels to noisy labels and is a promising approach. However, recent probabilistic methods directly apply transition matrix to DNN, neglect DNN's susceptibility to overfitting, and achieve unsatisfactory performance, especially under the uniform noise. In this paper, inspired by label smoothing, we proposed a novel method, in which a smoothed transition matrix is used for updating DNN, to restrict the overfitting of DNN in probabilistic modeling. Our method is termed Matrix Smoothing. We also empirically demonstrate that our method not only improves the robustness of probabilistic modeling significantly, but also even obtains a better estimation of the transition matrix.



### Zero-Assignment Constraint for Graph Matching with Outliers
- **Arxiv ID**: http://arxiv.org/abs/2003.11928v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.11928v1)
- **Published**: 2020-03-26 14:11:10+00:00
- **Updated**: 2020-03-26 14:11:10+00:00
- **Authors**: Fudong Wang, Nan Xue, Jin-Gang Yu, Gui-Song Xia
- **Comment**: None
- **Journal**: None
- **Summary**: Graph matching (GM), as a longstanding problem in computer vision and pattern recognition, still suffers from numerous cluttered outliers in practical applications. To address this issue, we present the zero-assignment constraint (ZAC) for approaching the graph matching problem in the presence of outliers. The underlying idea is to suppress the matchings of outliers by assigning zero-valued vectors to the potential outliers in the obtained optimal correspondence matrix. We provide elaborate theoretical analysis to the problem, i.e., GM with ZAC, and figure out that the GM problem with and without outliers are intrinsically different, which enables us to put forward a sufficient condition to construct valid and reasonable objective function. Consequently, we design an efficient outlier-robust algorithm to significantly reduce the incorrect or redundant matchings caused by numerous outliers. Extensive experiments demonstrate that our method can achieve the state-of-the-art performance in terms of accuracy and efficiency, especially in the presence of numerous outliers.



### Towards Backward-Compatible Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2003.11942v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.11942v3)
- **Published**: 2020-03-26 14:34:09+00:00
- **Updated**: 2021-01-06 06:59:07+00:00
- **Authors**: Yantao Shen, Yuanjun Xiong, Wei Xia, Stefano Soatto
- **Comment**: Accepted to CVPR 2020 as oral, revised version
- **Journal**: None
- **Summary**: We propose a way to learn visual features that are compatible with previously computed ones even when they have different dimensions and are learned via different neural network architectures and loss functions. Compatible means that, if such features are used to compare images, then "new" features can be compared directly to "old" features, so they can be used interchangeably. This enables visual search systems to bypass computing new features for all previously seen images when updating the embedding models, a process known as backfilling. Backward compatibility is critical to quickly deploy new embedding models that leverage ever-growing large-scale training datasets and improvements in deep learning architectures and training methods. We propose a framework to train embedding models, called backward-compatible training (BCT), as a first step towards backward compatible representation learning. In experiments on learning embeddings for face recognition, models trained with BCT successfully achieve backward compatibility without sacrificing accuracy, thus enabling backfill-free model updates of visual embeddings.



### Classification of COVID-19 in chest X-ray images using DeTraC deep convolutional neural network
- **Arxiv ID**: http://arxiv.org/abs/2003.13815v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.13815v3)
- **Published**: 2020-03-26 15:18:45+00:00
- **Updated**: 2020-05-17 12:02:33+00:00
- **Authors**: Asmaa Abbas, Mohammed M. Abdelsamea, Mohamed Medhat Gaber
- **Comment**: None
- **Journal**: None
- **Summary**: Chest X-ray is the first imaging technique that plays an important role in the diagnosis of COVID-19 disease. Due to the high availability of large-scale annotated image datasets, great success has been achieved using convolutional neural networks (CNNs) for image recognition and classification. However, due to the limited availability of annotated medical images, the classification of medical images remains the biggest challenge in medical diagnosis. Thanks to transfer learning, an effective mechanism that can provide a promising solution by transferring knowledge from generic object recognition tasks to domain-specific tasks. In this paper, we validate and adapt our previously developed CNN, called Decompose, Transfer, and Compose (DeTraC), for the classification of COVID-19 chest X-ray images. DeTraC can deal with any irregularities in the image dataset by investigating its class boundaries using a class decomposition mechanism. The experimental results showed the capability of DeTraC in the detection of COVID-19 cases from a comprehensive image dataset collected from several hospitals around the world. High accuracy of 95.12% (with a sensitivity of 97.91%, a specificity of 91.87%, and a precision of 93.36%) was achieved by DeTraC in the detection of COVID-19 X-ray images from normal, and severe acute respiratory syndrome cases.



### Severity Assessment of Coronavirus Disease 2019 (COVID-19) Using Quantitative Features from Chest CT Images
- **Arxiv ID**: http://arxiv.org/abs/2003.11988v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.11988v1)
- **Published**: 2020-03-26 15:49:32+00:00
- **Updated**: 2020-03-26 15:49:32+00:00
- **Authors**: Zhenyu Tang, Wei Zhao, Xingzhi Xie, Zheng Zhong, Feng Shi, Jun Liu, Dinggang Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Background: Chest computed tomography (CT) is recognized as an important tool for COVID-19 severity assessment. As the number of affected patients increase rapidly, manual severity assessment becomes a labor-intensive task, and may lead to delayed treatment. Purpose: Using machine learning method to realize automatic severity assessment (non-severe or severe) of COVID-19 based on chest CT images, and to explore the severity-related features from the resulting assessment model. Materials and Method: Chest CT images of 176 patients (age 45.3$\pm$16.5 years, 96 male and 80 female) with confirmed COVID-19 are used, from which 63 quantitative features, e.g., the infection volume/ratio of the whole lung and the volume of ground-glass opacity (GGO) regions, are calculated. A random forest (RF) model is trained to assess the severity (non-severe or severe) based on quantitative features. Importance of each quantitative feature, which reflects the correlation to the severity of COVID-19, is calculated from the RF model. Results: Using three-fold cross validation, the RF model shows promising results, i.e., 0.933 of true positive rate, 0.745 of true negative rate, 0.875 of accuracy, and 0.91 of area under receiver operating characteristic curve (AUC). The resulting importance of quantitative features shows that the volume and its ratio (with respect to the whole lung volume) of ground glass opacity (GGO) regions are highly related to the severity of COVID-19, and the quantitative features calculated from the right lung are more related to the severity assessment than those of the left lung. Conclusion: The RF based model can achieve automatic severity assessment (non-severe or severe) of COVID-19 infection, and the performance is promising. Several quantitative features, which have the potential to reflect the severity of COVID-19, were revealed.



### Milking CowMask for Semi-Supervised Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2003.12022v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.12022v3)
- **Published**: 2020-03-26 16:42:22+00:00
- **Updated**: 2020-06-05 18:11:20+00:00
- **Authors**: Geoff French, Avital Oliver, Tim Salimans
- **Comment**: 11 pages, 2 figures, submitted to NeurIPS 2020
- **Journal**: None
- **Summary**: Consistency regularization is a technique for semi-supervised learning that underlies a number of strong results for classification with few labeled data. It works by encouraging a learned model to be robust to perturbations on unlabeled data. Here, we present a novel mask-based augmentation method called CowMask. Using it to provide perturbations for semi-supervised consistency regularization, we achieve a state-of-the-art result on ImageNet with 10% labeled data, with a top-5 error of 8.76% and top-1 error of 26.06%. Moreover, we do so with a method that is much simpler than many alternatives. We further investigate the behavior of CowMask for semi-supervised learning by running many smaller scale experiments on the SVHN, CIFAR-10 and CIFAR-100 data sets, where we achieve results competitive with the state of the art, indicating that CowMask is widely applicable. We open source our code at https://github.com/google-research/google-research/tree/master/milking_cowmask



### Convolutional Neural Networks for Image-based Corn Kernel Detection and Counting
- **Arxiv ID**: http://arxiv.org/abs/2003.12025v2
- **DOI**: 10.3390/s20092721
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.12025v2)
- **Published**: 2020-03-26 16:46:23+00:00
- **Updated**: 2020-04-20 02:02:19+00:00
- **Authors**: Saeed Khaki, Hieu Pham, Ye Han, Andy Kuhl, Wade Kent, Lizhi Wang
- **Comment**: 14 pages, 9 figures
- **Journal**: None
- **Summary**: Precise in-season corn grain yield estimates enable farmers to make real-time accurate harvest and grain marketing decisions minimizing possible losses of profitability. A well developed corn ear can have up to 800 kernels, but manually counting the kernels on an ear of corn is labor-intensive, time consuming and prone to human error. From an algorithmic perspective, the detection of the kernels from a single corn ear image is challenging due to the large number of kernels at different angles and very small distance among the kernels. In this paper, we propose a kernel detection and counting method based on a sliding window approach. The proposed method detect and counts all corn kernels in a single corn ear image taken in uncontrolled lighting conditions. The sliding window approach uses a convolutional neural network (CNN) for kernel detection. Then, a non-maximum suppression (NMS) is applied to remove overlapping detections. Finally, windows that are classified as kernel are passed to another CNN regression model for finding the (x,y) coordinates of the center of kernel image patches. Our experiments indicate that the proposed method can successfully detect the corn kernels with a low detection error and is also able to detect kernels on a batch of corn ears positioned at different angles.



### RAFT: Recurrent All-Pairs Field Transforms for Optical Flow
- **Arxiv ID**: http://arxiv.org/abs/2003.12039v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.12039v3)
- **Published**: 2020-03-26 17:12:42+00:00
- **Updated**: 2020-08-25 15:49:48+00:00
- **Authors**: Zachary Teed, Jia Deng
- **Comment**: fixed a formatting issue, Eq 7. no change in content
- **Journal**: None
- **Summary**: We introduce Recurrent All-Pairs Field Transforms (RAFT), a new deep network architecture for optical flow. RAFT extracts per-pixel features, builds multi-scale 4D correlation volumes for all pairs of pixels, and iteratively updates a flow field through a recurrent unit that performs lookups on the correlation volumes. RAFT achieves state-of-the-art performance. On KITTI, RAFT achieves an F1-all error of 5.10%, a 16% error reduction from the best published result (6.10%). On Sintel (final pass), RAFT obtains an end-point-error of 2.855 pixels, a 30% error reduction from the best published result (4.098 pixels). In addition, RAFT has strong cross-dataset generalization as well as high efficiency in inference time, training speed, and parameter count. Code is available at https://github.com/princeton-vl/RAFT.



### Pseudo-Labeling for Small Lesion Detection on Diabetic Retinopathy Images
- **Arxiv ID**: http://arxiv.org/abs/2003.12040v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.12040v1)
- **Published**: 2020-03-26 17:13:48+00:00
- **Updated**: 2020-03-26 17:13:48+00:00
- **Authors**: Qilei Chen, Ping Liu, Jing Ni, Yu Cao, Benyuan Liu, Honggang Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Diabetic retinopathy (DR) is a primary cause of blindness in working-age people worldwide. About 3 to 4 million people with diabetes become blind because of DR every year. Diagnosis of DR through color fundus images is a common approach to mitigate such problem. However, DR diagnosis is a difficult and time consuming task, which requires experienced clinicians to identify the presence and significance of many small features on high resolution images. Convolutional Neural Network (CNN) has proved to be a promising approach for automatic biomedical image analysis recently. In this work, we investigate lesion detection on DR fundus images with CNN-based object detection methods. Lesion detection on fundus images faces two unique challenges. The first one is that our dataset is not fully labeled, i.e., only a subset of all lesion instances are marked. Not only will these unlabeled lesion instances not contribute to the training of the model, but also they will be mistakenly counted as false negatives, leading the model move to the opposite direction. The second challenge is that the lesion instances are usually very small, making them difficult to be found by normal object detectors. To address the first challenge, we introduce an iterative training algorithm for the semi-supervised method of pseudo-labeling, in which a considerable number of unlabeled lesion instances can be discovered to boost the performance of the lesion detector. For the small size targets problem, we extend both the input size and the depth of feature pyramid network (FPN) to produce a large CNN feature map, which can preserve the detail of small lesions and thus enhance the effectiveness of the lesion detector. The experimental results show that our proposed methods significantly outperform the baselines.



### Rethinking Online Action Detection in Untrimmed Videos: A Novel Online Evaluation Protocol
- **Arxiv ID**: http://arxiv.org/abs/2003.12041v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.12041v1)
- **Published**: 2020-03-26 17:13:55+00:00
- **Updated**: 2020-03-26 17:13:55+00:00
- **Authors**: Marcos Baptista Rios, Roberto J. López-Sastre, Fabian Caba Heilbron, Jan van Gemert, F. Javier Acevedo-Rodríguez, S. Maldonado-Bascón
- **Comment**: Published at IEEE Access journal
- **Journal**: None
- **Summary**: The Online Action Detection (OAD) problem needs to be revisited. Unlike traditional offline action detection approaches, where the evaluation metrics are clear and well established, in the OAD setting we find very few works and no consensus on the evaluation protocols to be used. In this work we propose to rethink the OAD scenario, clearly defining the problem itself and the main characteristics that the models which are considered online must comply with. We also introduce a novel metric: the Instantaneous Accuracy ($IA$). This new metric exhibits an \emph{online} nature and solves most of the limitations of the previous metrics. We conduct a thorough experimental evaluation on 3 challenging datasets, where the performance of various baseline methods is compared to that of the state-of-the-art. Our results confirm the problems of the previous evaluation protocols, and suggest that an IA-based protocol is more adequate to the online scenario. The baselines models and a development kit with the novel evaluation protocol are publicly available: https://github.com/gramuah/ia.



### Use the Force, Luke! Learning to Predict Physical Forces by Simulating Effects
- **Arxiv ID**: http://arxiv.org/abs/2003.12045v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2003.12045v1)
- **Published**: 2020-03-26 17:20:23+00:00
- **Updated**: 2020-03-26 17:20:23+00:00
- **Authors**: Kiana Ehsani, Shubham Tulsiani, Saurabh Gupta, Ali Farhadi, Abhinav Gupta
- **Comment**: CVPR 2020 -- (Oral presentation)
- **Journal**: None
- **Summary**: When we humans look at a video of human-object interaction, we can not only infer what is happening but we can even extract actionable information and imitate those interactions. On the other hand, current recognition or geometric approaches lack the physicality of action representation. In this paper, we take a step towards a more physical understanding of actions. We address the problem of inferring contact points and the physical forces from videos of humans interacting with objects. One of the main challenges in tackling this problem is obtaining ground-truth labels for forces. We sidestep this problem by instead using a physics simulator for supervision. Specifically, we use a simulator to predict effects and enforce that estimated forces must lead to the same effect as depicted in the video. Our quantitative and qualitative results show that (a) we can predict meaningful forces from videos whose effects lead to accurate imitation of the motions observed, (b) by jointly optimizing for contact point and force prediction, we can improve the performance on both tasks in comparison to independent training, and (c) we can learn a representation from this model that generalizes to novel objects using few shot examples.



### Learning Inverse Rendering of Faces from Real-world Videos
- **Arxiv ID**: http://arxiv.org/abs/2003.12047v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.12047v1)
- **Published**: 2020-03-26 17:26:40+00:00
- **Updated**: 2020-03-26 17:26:40+00:00
- **Authors**: Yuda Qiu, Zhangyang Xiong, Kai Han, Zhongyuan Wang, Zixiang Xiong, Xiaoguang Han
- **Comment**: First two authors contributed equally.
  Code:https://github.com/RudyQ/InverseFaceRender
- **Journal**: None
- **Summary**: In this paper we examine the problem of inverse rendering of real face images. Existing methods decompose a face image into three components (albedo, normal, and illumination) by supervised training on synthetic face data. However, due to the domain gap between real and synthetic face images, a model trained on synthetic data often does not generalize well to real data. Meanwhile, since no ground truth for any component is available for real images, it is not feasible to conduct supervised learning on real face images. To alleviate this problem, we propose a weakly supervised training approach to train our model on real face videos, based on the assumption of consistency of albedo and normal across different frames, thus bridging the gap between real and synthetic face images. In addition, we introduce a learning framework, called IlluRes-SfSNet, to further extract the residual map to capture the global illumination effects that give the fine details that are largely ignored in existing methods. Our network is trained on both real and synthetic data, benefiting from both. We comprehensively evaluate our methods on various benchmarks, obtaining better inverse rendering results than the state-of-the-art.



### Are Labels Necessary for Neural Architecture Search?
- **Arxiv ID**: http://arxiv.org/abs/2003.12056v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.12056v2)
- **Published**: 2020-03-26 17:55:16+00:00
- **Updated**: 2020-08-03 15:18:16+00:00
- **Authors**: Chenxi Liu, Piotr Dollár, Kaiming He, Ross Girshick, Alan Yuille, Saining Xie
- **Comment**: To appear in ECCV 2020 as spotlight. Code release:
  https://github.com/facebookresearch/unnas
- **Journal**: None
- **Summary**: Existing neural network architectures in computer vision -- whether designed by humans or by machines -- were typically found using both images and their associated labels. In this paper, we ask the question: can we find high-quality neural architectures using only images, but no human-annotated labels? To answer this question, we first define a new setup called Unsupervised Neural Architecture Search (UnNAS). We then conduct two sets of experiments. In sample-based experiments, we train a large number (500) of diverse architectures with either supervised or unsupervised objectives, and find that the architecture rankings produced with and without labels are highly correlated. In search-based experiments, we run a well-established NAS algorithm (DARTS) using various unsupervised objectives, and report that the architectures searched without labels can be competitive to their counterparts searched with labels. Together, these results reveal the potentially surprising finding that labels are not necessary, and the image statistics alone may be sufficient to identify good neural architectures.



### Grounded Situation Recognition
- **Arxiv ID**: http://arxiv.org/abs/2003.12058v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.12058v1)
- **Published**: 2020-03-26 17:57:52+00:00
- **Updated**: 2020-03-26 17:57:52+00:00
- **Authors**: Sarah Pratt, Mark Yatskar, Luca Weihs, Ali Farhadi, Aniruddha Kembhavi
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce Grounded Situation Recognition (GSR), a task that requires producing structured semantic summaries of images describing: the primary activity, entities engaged in the activity with their roles (e.g. agent, tool), and bounding-box groundings of entities. GSR presents important technical challenges: identifying semantic saliency, categorizing and localizing a large and diverse set of entities, overcoming semantic sparsity, and disambiguating roles. Moreover, unlike in captioning, GSR is straightforward to evaluate. To study this new task we create the Situations With Groundings (SWiG) dataset which adds 278,336 bounding-box groundings to the 11,538 entity classes in the imsitu dataset. We propose a Joint Situation Localizer and find that jointly predicting situations and groundings with end-to-end training handily outperforms independent training on the entire grounding metric suite with relative gains between 8% and 32%. Finally, we show initial findings on three exciting future directions enabled by our models: conditional querying, visual chaining, and grounded semantic aware image retrieval. Code and data available at https://prior.allenai.org/projects/gsr.



### Correspondence Networks with Adaptive Neighbourhood Consensus
- **Arxiv ID**: http://arxiv.org/abs/2003.12059v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.12059v1)
- **Published**: 2020-03-26 17:58:09+00:00
- **Updated**: 2020-03-26 17:58:09+00:00
- **Authors**: Shuda Li, Kai Han, Theo W. Costain, Henry Howard-Jenkins, Victor Prisacariu
- **Comment**: CVPR 2020. Project page: https://ancnet.avlcode.org/
- **Journal**: None
- **Summary**: In this paper, we tackle the task of establishing dense visual correspondences between images containing objects of the same category. This is a challenging task due to large intra-class variations and a lack of dense pixel level annotations. We propose a convolutional neural network architecture, called adaptive neighbourhood consensus network (ANC-Net), that can be trained end-to-end with sparse key-point annotations, to handle this challenge. At the core of ANC-Net is our proposed non-isotropic 4D convolution kernel, which forms the building block for the adaptive neighbourhood consensus module for robust matching. We also introduce a simple and efficient multi-scale self-similarity module in ANC-Net to make the learned feature robust to intra-class variations. Furthermore, we propose a novel orthogonal loss that can enforce the one-to-one matching constraint. We thoroughly evaluate the effectiveness of our method on various benchmarks, where it substantially outperforms state-of-the-art methods.



### Negative Margin Matters: Understanding Margin in Few-shot Classification
- **Arxiv ID**: http://arxiv.org/abs/2003.12060v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.12060v1)
- **Published**: 2020-03-26 17:59:05+00:00
- **Updated**: 2020-03-26 17:59:05+00:00
- **Authors**: Bin Liu, Yue Cao, Yutong Lin, Qi Li, Zheng Zhang, Mingsheng Long, Han Hu
- **Comment**: Code is available at https://github.com/bl0/negative-margin.few-shot
- **Journal**: None
- **Summary**: This paper introduces a negative margin loss to metric learning based few-shot learning methods. The negative margin loss significantly outperforms regular softmax loss, and achieves state-of-the-art accuracy on three standard few-shot classification benchmarks with few bells and whistles. These results are contrary to the common practice in the metric learning field, that the margin is zero or positive. To understand why the negative margin loss performs well for the few-shot classification, we analyze the discriminability of learned features w.r.t different margins for training and novel classes, both empirically and theoretically. We find that although negative margin reduces the feature discriminability for training classes, it may also avoid falsely mapping samples of the same novel class to multiple peaks or clusters, and thus benefit the discrimination of novel classes. Code is available at https://github.com/bl0/negative-margin.few-shot.



### Memory Enhanced Global-Local Aggregation for Video Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2003.12063v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.12063v1)
- **Published**: 2020-03-26 17:59:38+00:00
- **Updated**: 2020-03-26 17:59:38+00:00
- **Authors**: Yihong Chen, Yue Cao, Han Hu, Liwei Wang
- **Comment**: Accepted by CVPR2020
- **Journal**: None
- **Summary**: How do humans recognize an object in a piece of video? Due to the deteriorated quality of single frame, it may be hard for people to identify an occluded object in this frame by just utilizing information within one image. We argue that there are two important cues for humans to recognize objects in videos: the global semantic information and the local localization information. Recently, plenty of methods adopt the self-attention mechanisms to enhance the features in key frame with either global semantic information or local localization information. In this paper we introduce memory enhanced global-local aggregation (MEGA) network, which is among the first trials that takes full consideration of both global and local information. Furthermore, empowered by a novel and carefully-designed Long Range Memory (LRM) module, our proposed MEGA could enable the key frame to get access to much more content than any previous methods. Enhanced by these two sources of information, our method achieves state-of-the-art performance on ImageNet VID dataset. Code is available at \url{https://github.com/Scalsol/mega.pytorch}.



### Real-time information retrieval from Identity cards
- **Arxiv ID**: http://arxiv.org/abs/2003.12103v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.12103v1)
- **Published**: 2020-03-26 18:37:29+00:00
- **Updated**: 2020-03-26 18:37:29+00:00
- **Authors**: Niloofar Tavakolian, Azadeh Nazemi, Donal Fitzpatrick
- **Comment**: 6pages,10 figures,conference
- **Journal**: None
- **Summary**: Information is frequently retrieved from valid personal ID cards by the authorised organisation to address different purposes. The successful information retrieval (IR) depends on the accuracy and timing process. A process which necessitates a long time to respond is frustrating for both sides in the exchange of data. This paper aims to propose a series of state-of-the-art methods for the journey of an Identification card (ID) from the scanning or capture phase to the point before Optical character recognition (OCR). The key factors for this proposal are the accuracy and speed of the process during the journey. The experimental results of this research prove that utilising the methods based on deep learning, such as Efficient and Accurate Scene Text (EAST) detector and Deep Neural Network (DNN) for face detection, instead of traditional methods increase the efficiency considerably.



### Pedestrian Detection with Wearable Cameras for the Blind: A Two-way Perspective
- **Arxiv ID**: http://arxiv.org/abs/2003.12122v2
- **DOI**: 10.1145/3313831.3376398
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.12122v2)
- **Published**: 2020-03-26 19:34:54+00:00
- **Updated**: 2020-05-22 19:17:38+00:00
- **Authors**: Kyungjun Lee, Daisuke Sato, Saki Asakawa, Hernisa Kacorri, Chieko Asakawa
- **Comment**: The 2020 ACM CHI Conference on Human Factors in Computing Systems
  (CHI 2020)
- **Journal**: None
- **Summary**: Blind people have limited access to information about their surroundings, which is important for ensuring one's safety, managing social interactions, and identifying approaching pedestrians. With advances in computer vision, wearable cameras can provide equitable access to such information. However, the always-on nature of these assistive technologies poses privacy concerns for parties that may get recorded. We explore this tension from both perspectives, those of sighted passersby and blind users, taking into account camera visibility, in-person versus remote experience, and extracted visual information. We conduct two studies: an online survey with MTurkers (N=206) and an in-person experience study between pairs of blind (N=10) and sighted (N=40) participants, where blind participants wear a working prototype for pedestrian detection and pass by sighted participants. Our results suggest that both of the perspectives of users and bystanders and the several factors mentioned above need to be carefully considered to mitigate potential social tensions.



### SaccadeNet: A Fast and Accurate Object Detector
- **Arxiv ID**: http://arxiv.org/abs/2003.12125v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.12125v1)
- **Published**: 2020-03-26 19:47:17+00:00
- **Updated**: 2020-03-26 19:47:17+00:00
- **Authors**: Shiyi Lan, Zhou Ren, Yi Wu, Larry S. Davis, Gang Hua
- **Comment**: None
- **Journal**: None
- **Summary**: Object detection is an essential step towards holistic scene understanding. Most existing object detection algorithms attend to certain object areas once and then predict the object locations. However, neuroscientists have revealed that humans do not look at the scene in fixed steadiness. Instead, human eyes move around, locating informative parts to understand the object location. This active perceiving movement process is called \textit{saccade}.   %In this paper, Inspired by such mechanism, we propose a fast and accurate object detector called \textit{SaccadeNet}. It contains four main modules, the \cenam, the \coram, the \atm, and the \aggatt, which allows it to attend to different informative object keypoints, and predict object locations from coarse to fine. The \coram~is used only during training to extract more informative corner features which brings free-lunch performance boost. On the MS COCO dataset, we achieve the performance of 40.4\% mAP at 28 FPS and 30.5\% mAP at 118 FPS. Among all the real-time object detectors, %that can run faster than 25 FPS, our SaccadeNet achieves the best detection performance, which demonstrates the effectiveness of the proposed detection mechanism.



### Going in circles is the way forward: the role of recurrence in visual inference
- **Arxiv ID**: http://arxiv.org/abs/2003.12128v3
- **DOI**: 10.1016/j.conb.2020.11.009
- **Categories**: **q-bio.NC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.12128v3)
- **Published**: 2020-03-26 19:53:05+00:00
- **Updated**: 2020-11-16 13:33:25+00:00
- **Authors**: Ruben S. van Bergen, Nikolaus Kriegeskorte
- **Comment**: None
- **Journal**: None
- **Summary**: Biological visual systems exhibit abundant recurrent connectivity. State-of-the-art neural network models for visual recognition, by contrast, rely heavily or exclusively on feedforward computation. Any finite-time recurrent neural network (RNN) can be unrolled along time to yield an equivalent feedforward neural network (FNN). This important insight suggests that computational neuroscientists may not need to engage recurrent computation, and that computer-vision engineers may be limiting themselves to a special case of FNN if they build recurrent models. Here we argue, to the contrary, that FNNs are a special case of RNNs and that computational neuroscientists and engineers should engage recurrence to understand how brains and machines can (1) achieve greater and more flexible computational depth, (2) compress complex computations into limited hardware, (3) integrate priors and priorities into visual inference through expectation and attention, (4) exploit sequential dependencies in their data for better inference and prediction, and (5) leverage the power of iterative computation.



### Cycle Text-To-Image GAN with BERT
- **Arxiv ID**: http://arxiv.org/abs/2003.12137v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.12137v1)
- **Published**: 2020-03-26 20:17:55+00:00
- **Updated**: 2020-03-26 20:17:55+00:00
- **Authors**: Trevor Tsue, Samir Sen, Jason Li
- **Comment**: None
- **Journal**: None
- **Summary**: We explore novel approaches to the task of image generation from their respective captions, building on state-of-the-art GAN architectures. Particularly, we baseline our models with the Attention-based GANs that learn attention mappings from words to image features. To better capture the features of the descriptions, we then built a novel cyclic design that learns an inverse function to maps the image back to original caption. Additionally, we incorporated recently developed BERT pretrained word embeddings as our initial text featurizer and observe a noticeable improvement in qualitative and quantitative performance compared to the Attention GAN baseline.



### Strategies for Robust Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2004.03452v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.03452v2)
- **Published**: 2020-03-26 21:22:39+00:00
- **Updated**: 2020-08-13 16:50:35+00:00
- **Authors**: Jason Stock, Andy Dolan, Tom Cavey
- **Comment**: 15 pages, and 39 figure (with Appendix)
- **Journal**: None
- **Summary**: In this work we evaluate the impact of digitally altered images on the performance of artificial neural networks. We explore factors that negatively affect the ability of an image classification model to produce consistent and accurate results. A model's ability to classify is negatively influenced by alterations to images as a result of digital abnormalities or changes in the physical environment. The focus of this paper is to discover and replicate scenarios that modify the appearance of an image and evaluate them on state-of-the-art machine learning models. Our contributions present various training techniques that enhance a model's ability to generalize and improve robustness against these alterations.



### Using constraint structure and an improved object detection network to detect the 12^{th} Vertebra from CT images with a limited field of view for image-guided radiotherapy
- **Arxiv ID**: http://arxiv.org/abs/2003.12163v2
- **DOI**: None
- **Categories**: **cs.CV**, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2003.12163v2)
- **Published**: 2020-03-26 21:41:00+00:00
- **Updated**: 2023-03-12 03:00:58+00:00
- **Authors**: Yunhe Xie, Kongbin Kang, Gregory Sharp, David P. Gierga, Theodore S. Hong, Thomas Bortfeld
- **Comment**: 10 pages, 2 figures
- **Journal**: None
- **Summary**: Image guidance has been widely used in radiation therapy. Correctly identifying the bounding box of the anatomical landmarks from limited field of views is the key to success. In image-guided radiation therapy (IGRT), the detection of those landmarks like the 12th vertebra (T12) still requires tedious manual inspections and annotations; and superior-inferior misalignment to the wrong vertebral body is still relatively common. It is necessary to develop an automated approach to detect those landmarks from images. The challenges of training a model to identify T12 vertebrae automatically mainly are high shape similarity between T12 and neighboring vertebrae, limited annotated data, and class imbalance. This study proposed a novel 3D detection network, requiring only a small amount of training data. Our approach has the following innovations, including 1) the introduction of an auxiliary network to build constraint feature map for improving the model's generalization, especially when the constraint structure is easier to be detected than the main one; 2) an improved detection head and target functions for accurate bounding box detection; and 3) an improved loss functions to address the high class imbalance. Our proposed network was trained, validated and tested on anotated CT images from 55 patients and demonstrated accurate distinguish T12 vertebra from its neighboring vertebrae of high shape similarity. Our proposed algorithm yielded the bounding box center and size errors of 3.98\pm2.04mm and 16.83\pm8.34mm, respectively. Our approach significantly outperformed state-of-the-arts Retina-Net3D in average precision (AP) at IoU thresholds of 0.35 and 0.5, with AP increasing from 0 to 95.4 and 0 to 64.7, respectively. In summary, our approach has a great potential to be integrated into the clinical workflow to improve the safety of IGRT.



### Simultaneous Learning from Human Pose and Object Cues for Real-Time Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/2004.03453v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2004.03453v1)
- **Published**: 2020-03-26 22:04:37+00:00
- **Updated**: 2020-03-26 22:04:37+00:00
- **Authors**: Brian Reily, Qingzhao Zhu, Christopher Reardon, Hao Zhang
- **Comment**: Accepted to International Conference on Robotics and Automation
  (ICRA) 2020, IEEE copyright
- **Journal**: None
- **Summary**: Real-time human activity recognition plays an essential role in real-world human-centered robotics applications, such as assisted living and human-robot collaboration. Although previous methods based on skeletal data to encode human poses showed promising results on real-time activity recognition, they lacked the capability to consider the context provided by objects within the scene and in use by the humans, which can provide a further discriminant between human activity categories. In this paper, we propose a novel approach to real-time human activity recognition, through simultaneously learning from observations of both human poses and objects involved in the human activity. We formulate human activity recognition as a joint optimization problem under a unified mathematical framework, which uses a regression-like loss function to integrate human pose and object cues and defines structured sparsity-inducing norms to identify discriminative body joints and object attributes. To evaluate our method, we perform extensive experiments on two benchmark datasets and a physical robot in a home assistance setting. Experimental results have shown that our method outperforms previous methods and obtains real-time performance for human activity recognition with a processing speed of 10^4 Hz.



### ParSeNet: A Parametric Surface Fitting Network for 3D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2003.12181v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.12181v5)
- **Published**: 2020-03-26 22:54:18+00:00
- **Updated**: 2020-09-22 16:05:16+00:00
- **Authors**: Gopal Sharma, Difan Liu, Subhransu Maji, Evangelos Kalogerakis, Siddhartha Chaudhuri, Radomír Měch
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel, end-to-end trainable, deep network called ParSeNet that decomposes a 3D point cloud into parametric surface patches, including B-spline patches as well as basic geometric primitives. ParSeNet is trained on a large-scale dataset of man-made 3D shapes and captures high-level semantic priors for shape decomposition. It handles a much richer class of primitives than prior work, and allows us to represent surfaces with higher fidelity. It also produces repeatable and robust parametrizations of a surface compared to purely geometric approaches. We present extensive experiments to validate our approach against analytical and learning-based alternatives. Our source code is publicly available at: https://hippogriff.github.io/parsenet.



### Action Localization through Continual Predictive Learning
- **Arxiv ID**: http://arxiv.org/abs/2003.12185v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.12185v1)
- **Published**: 2020-03-26 23:32:43+00:00
- **Updated**: 2020-03-26 23:32:43+00:00
- **Authors**: Sathyanarayanan N. Aakur, Sudeep Sarkar
- **Comment**: 18 pages, 4 figures and 3 tables
- **Journal**: None
- **Summary**: The problem of action recognition involves locating the action in the video, both over time and spatially in the image. The dominant current approaches use supervised learning to solve this problem, and require large amounts of annotated training data, in the form of frame-level bounding box annotations around the region of interest. In this paper, we present a new approach based on continual learning that uses feature-level predictions for self-supervision. It does not require any training annotations in terms of frame-level bounding boxes. The approach is inspired by cognitive models of visual event perception that propose a prediction-based approach to event understanding. We use a stack of LSTMs coupled with CNN encoder, along with novel attention mechanisms, to model the events in the video and use this model to predict high-level features for the future frames. The prediction errors are used to continuously learn the parameters of the models. This self-supervised framework is not complicated as other approaches but is very effective in learning robust visual representations for both labeling and localization. It should be noted that the approach outputs in a streaming fashion, requiring only a single pass through the video, making it amenable for real-time processing. We demonstrate this on three datasets - UCF Sports, JHMDB, and THUMOS'13 and show that the proposed approach outperforms weakly-supervised and unsupervised baselines and obtains competitive performance compared to fully supervised baselines. Finally, we show that the proposed framework can generalize to egocentric videos and obtain state-of-the-art results in unsupervised gaze prediction.



