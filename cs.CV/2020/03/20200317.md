# Arxiv Papers in cs.CV on 2020-03-17
### Deep Object Detection based Mitosis Analysis in Breast Cancer Histopathological Images
- **Arxiv ID**: http://arxiv.org/abs/2003.08803v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.08803v1)
- **Published**: 2020-03-17 00:51:16+00:00
- **Updated**: 2020-03-17 00:51:16+00:00
- **Authors**: Anabia Sohail, Muhammad Ahsan Mukhtar, Asifullah Khan, Muhammad Mohsin Zafar, Aneela Zameer, Saranjam Khan
- **Comment**: Tables: 4, Figures 11, Pages: 21
- **Journal**: None
- **Summary**: Empirical evaluation of breast tissue biopsies for mitotic nuclei detection is considered an important prognostic biomarker in tumor grading and cancer progression. However, automated mitotic nuclei detection poses several challenges because of the unavailability of pixel-level annotations, different morphological configurations of mitotic nuclei, their sparse representation, and close resemblance with non-mitotic nuclei. These challenges undermine the precision of the automated detection model and thus make detection difficult in a single phase. This work proposes an end-to-end detection system for mitotic nuclei identification in breast cancer histopathological images. Deep object detection-based Mask R-CNN is adapted for mitotic nuclei detection that initially selects the candidate mitotic region with maximum recall. However, in the second phase, these candidate regions are refined by multi-object loss function to improve the precision. The performance of the proposed detection model shows improved discrimination ability (F-score of 0.86) for mitotic nuclei with significant precision (0.86) as compared to the two-stage detection models (F-score of 0.701) on TUPAC16 dataset. Promising results suggest that the deep object detection-based model has the potential to learn the characteristic features of mitotic nuclei from weakly annotated data and suggests that it can be adapted for the identification of other nuclear bodies in histopathological images.



### Real Time Multi-Class Object Detection and Recognition Using Vision Augmentation Algorithm
- **Arxiv ID**: http://arxiv.org/abs/2003.07442v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2003.07442v4)
- **Published**: 2020-03-17 01:08:24+00:00
- **Updated**: 2020-11-11 18:22:22+00:00
- **Authors**: Al-Akhir Nayan, Joyeta Saha, Ahamad Nokib Mozumder, Khan Raqib Mahmud, Abul Kalam Al Azad
- **Comment**: None
- **Journal**: International Journal of Advanced Science and Technology, Vol. 29,
  No. 5, (2020), pp. 14070 - 14083
- **Summary**: The aim of this research is to detect small objects with low resolution and noise. The existing real time object detection algorithm is based on the deep neural network of convolution need to perform multilevel convolution and pooling operations on the entire image to extract a deep semantic characteristic of the image. The detection models perform better for large objects. The features of existing models do not fully represent the essential features of small objects after repeated convolution operations. We have introduced a novel real time detection algorithm which employs upsampling and skip connection to extract multiscale features at different convolution levels in a learning task resulting a remarkable performance in detecting small objects. The detection precision of the model is shown to be higher and faster than that of the state-of-the-art models.



### Deep Relational Reasoning Graph Network for Arbitrary Shape Text Detection
- **Arxiv ID**: http://arxiv.org/abs/2003.07493v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.07493v2)
- **Published**: 2020-03-17 01:50:07+00:00
- **Updated**: 2020-08-30 07:36:56+00:00
- **Authors**: Shi-Xue Zhang, Xiaobin Zhu, Jie-Bo Hou, Chang Liu, Chun Yang, Hongfa Wang, Xu-Cheng Yin
- **Comment**: 10 pages, Accepted by CVPR2020 Oral
- **Journal**: None
- **Summary**: Arbitrary shape text detection is a challenging task due to the high variety and complexity of scenes texts. In this paper, we propose a novel unified relational reasoning graph network for arbitrary shape text detection. In our method, an innovative local graph bridges a text proposal model via Convolutional Neural Network (CNN) and a deep relational reasoning network via Graph Convolutional Network (GCN), making our network end-to-end trainable. To be concrete, every text instance will be divided into a series of small rectangular components, and the geometry attributes (e.g., height, width, and orientation) of the small components will be estimated by our text proposal model. Given the geometry attributes, the local graph construction model can roughly establish linkages between different text components. For further reasoning and deducing the likelihood of linkages between the component and its neighbors, we adopt a graph-based network to perform deep relational reasoning on local graphs. Experiments on public available datasets demonstrate the state-of-the-art performance of our method.



### DEPARA: Deep Attribution Graph for Deep Knowledge Transferability
- **Arxiv ID**: http://arxiv.org/abs/2003.07496v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.07496v1)
- **Published**: 2020-03-17 02:07:50+00:00
- **Updated**: 2020-03-17 02:07:50+00:00
- **Authors**: Jie Song, Yixin Chen, Jingwen Ye, Xinchao Wang, Chengchao Shen, Feng Mao, Mingli Song
- **Comment**: Accepted by CVPR2020 (oral)
- **Journal**: None
- **Summary**: Exploring the intrinsic interconnections between the knowledge encoded in PRe-trained Deep Neural Networks (PR-DNNs) of heterogeneous tasks sheds light on their mutual transferability, and consequently enables knowledge transfer from one task to another so as to reduce the training effort of the latter. In this paper, we propose the DEeP Attribution gRAph (DEPARA) to investigate the transferability of knowledge learned from PR-DNNs. In DEPARA, nodes correspond to the inputs and are represented by their vectorized attribution maps with regards to the outputs of the PR-DNN. Edges denote the relatedness between inputs and are measured by the similarity of their features extracted from the PR-DNN. The knowledge transferability of two PR-DNNs is measured by the similarity of their corresponding DEPARAs. We apply DEPARA to two important yet under-studied problems in transfer learning: pre-trained model selection and layer selection. Extensive experiments are conducted to demonstrate the effectiveness and superiority of the proposed method in solving both these problems. Code, data and models reproducing the results in this paper are available at \url{https://github.com/zju-vipa/DEPARA}.



### Dynamic Multiscale Graph Neural Networks for 3D Skeleton-Based Human Motion Prediction
- **Arxiv ID**: http://arxiv.org/abs/2003.08802v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.08802v1)
- **Published**: 2020-03-17 02:49:51+00:00
- **Updated**: 2020-03-17 02:49:51+00:00
- **Authors**: Maosen Li, Siheng Chen, Yangheng Zhao, Ya Zhang, Yanfeng Wang, Qi Tian
- **Comment**: None
- **Journal**: None
- **Summary**: We propose novel dynamic multiscale graph neural networks (DMGNN) to predict 3D skeleton-based human motions. The core idea of DMGNN is to use a multiscale graph to comprehensively model the internal relations of a human body for motion feature learning. This multiscale graph is adaptive during training and dynamic across network layers. Based on this graph, we propose a multiscale graph computational unit (MGCU) to extract features at individual scales and fuse features across scales. The entire model is action-category-agnostic and follows an encoder-decoder framework. The encoder consists of a sequence of MGCUs to learn motion features. The decoder uses a proposed graph-based gate recurrent unit to generate future poses. Extensive experiments show that the proposed DMGNN outperforms state-of-the-art methods in both short and long-term predictions on the datasets of Human 3.6M and CMU Mocap. We further investigate the learned multiscale graphs for the interpretability. The codes could be downloaded from https://github.com/limaosen0/DMGNN.



### Predictively Encoded Graph Convolutional Network for Noise-Robust Skeleton-based Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2003.07514v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.07514v1)
- **Published**: 2020-03-17 03:37:36+00:00
- **Updated**: 2020-03-17 03:37:36+00:00
- **Authors**: Jongmin Yu, Yongsang Yoon, Moongu Jeon
- **Comment**: Submitted to ECCV 2020
- **Journal**: None
- **Summary**: In skeleton-based action recognition, graph convolutional networks (GCNs), which model human body skeletons using graphical components such as nodes and connections, have achieved remarkable performance recently. However, current state-of-the-art methods for skeleton-based action recognition usually work on the assumption that the completely observed skeletons will be provided. This may be problematic to apply this assumption in real scenarios since there is always a possibility that captured skeletons are incomplete or noisy. In this work, we propose a skeleton-based action recognition method which is robust to noise information of given skeleton features. The key insight of our approach is to train a model by maximizing the mutual information between normal and noisy skeletons using a predictive coding manner. We have conducted comprehensive experiments about skeleton-based action recognition with defected skeletons using NTU-RGB+D and Kinetics-Skeleton datasets. The experimental results demonstrate that our approach achieves outstanding performance when skeleton samples are noised compared with existing state-of-the-art methods.



### Augmented Parallel-Pyramid Net for Attention Guided Pose-Estimation
- **Arxiv ID**: http://arxiv.org/abs/2003.07516v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.07516v1)
- **Published**: 2020-03-17 03:52:17+00:00
- **Updated**: 2020-03-17 03:52:17+00:00
- **Authors**: Luanxuan Hou, Jie Cao, Yuan Zhao, Haifeng Shen, Yiping Meng, Ran He, Jieping Ye
- **Comment**: None
- **Journal**: None
- **Summary**: The target of human pose estimation is to determine body part or joint locations of each person from an image. This is a challenging problems with wide applications. To address this issue, this paper proposes an augmented parallel-pyramid net with attention partial module and differentiable auto-data augmentation. Technically, a parallel pyramid structure is proposed to compensate the loss of information. We take the design of parallel structure for reverse compensation. Meanwhile, the overall computational complexity does not increase. We further define an Attention Partial Module (APM) operator to extract weighted features from different scale feature maps generated by the parallel pyramid structure. Compared with refining through upsampling operator, APM can better capture the relationship between channels. At last, we proposed a differentiable auto data augmentation method to further improve estimation accuracy. We define a new pose search space where the sequences of data augmentations are formulated as a trainable and operational CNN component. Experiments corroborate the effectiveness of our proposed method. Notably, our method achieves the top-1 accuracy on the challenging COCO keypoint benchmark and the state-of-the-art results on the MPII datasets.



### Cooperative Object Detection and Parameter Estimation Using Visible Light Communications
- **Arxiv ID**: http://arxiv.org/abs/2003.07525v1
- **DOI**: 10.1109/OJCOMS.2020.3020574
- **Categories**: **cs.IT**, cs.CV, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/2003.07525v1)
- **Published**: 2020-03-17 04:40:33+00:00
- **Updated**: 2020-03-17 04:40:33+00:00
- **Authors**: Hamid Hosseinianfar, Maite Brandt-Pearce
- **Comment**: 6 pages, 5 figures
- **Journal**: None
- **Summary**: Visible light communication (VLC) systems are promising candidates for future indoor access and peer-to-peer networks. The performance of these systems, however, is vulnerable to the line of sight (LOS) link blockage due to objects inside the room. In this paper, we develop a probabilistic object detection method that takes advantage of the blockage status of the LOS links between the user devices and transceivers on the ceiling to locate those objects. The target objects are modeled as cylinders with random radii. The location and size of an object can be estimated by using a quadratic programming approach. Simulation results show that the root-mean-squared error can be less than $1$ cm and $8$ cm for estimating the center and the radius of the object, respectively.



### Synthesis of Brain Tumor MR Images for Learning Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2003.07526v1
- **DOI**: 10.1002/mp.14701
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.07526v1)
- **Published**: 2020-03-17 04:43:20+00:00
- **Updated**: 2020-03-17 04:43:20+00:00
- **Authors**: Sunho Kim, Byungjai Kim, HyunWook Park
- **Comment**: 14 pages, 8 figures
- **Journal**: None
- **Summary**: Medical image analysis using deep neural networks has been actively studied. Deep neural networks are trained by learning data. For accurate training of deep neural networks, the learning data should be sufficient, of good quality, and should have a generalized property. However, in medical images, it is difficult to acquire sufficient patient data because of the difficulty of patient recruitment, the burden of annotation of lesions by experts, and the invasion of patients' privacy. In comparison, the medical images of healthy volunteers can be easily acquired. Using healthy brain images, the proposed method synthesizes multi-contrast magnetic resonance images of brain tumors. Because tumors have complex features, the proposed method simplifies them into concentric circles that are easily controllable. Then it converts the concentric circles into various realistic shapes of tumors through deep neural networks. Because numerous healthy brain images are easily available, our method can synthesize a huge number of the brain tumor images with various concentric circles. We performed qualitative and quantitative analysis to assess the usefulness of augmented data from the proposed method. Intuitive and interesting experimental results are available online at https://github.com/KSH0660/BrainTumor



### Dynamic Point Cloud Denoising via Manifold-to-Manifold Distance
- **Arxiv ID**: http://arxiv.org/abs/2003.08355v3
- **DOI**: 10.1109/TIP.2021.3092826
- **Categories**: **eess.IV**, cs.CV, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2003.08355v3)
- **Published**: 2020-03-17 04:54:20+00:00
- **Updated**: 2020-10-28 12:53:04+00:00
- **Authors**: Wei Hu, Qianjiang Hu, Zehua Wang, Xiang Gao
- **Comment**: None
- **Journal**: None
- **Summary**: 3D dynamic point clouds provide a natural discrete representation of real-world objects or scenes in motion, with a wide range of applications in immersive telepresence, autonomous driving, surveillance, \etc. Nevertheless, dynamic point clouds are often perturbed by noise due to hardware, software or other causes. While a plethora of methods have been proposed for static point cloud denoising, few efforts are made for the denoising of dynamic point clouds, which is quite challenging due to the irregular sampling patterns both spatially and temporally. In this paper, we represent dynamic point clouds naturally on spatial-temporal graphs, and exploit the temporal consistency with respect to the underlying surface (manifold). In particular, we define a manifold-to-manifold distance and its discrete counterpart on graphs to measure the variation-based intrinsic distance between surface patches in the temporal domain, provided that graph operators are discrete counterparts of functionals on Riemannian manifolds. Then, we construct the spatial-temporal graph connectivity between corresponding surface patches based on the temporal distance and between points in adjacent patches in the spatial domain. Leveraging the initial graph representation, we formulate dynamic point cloud denoising as the joint optimization of the desired point cloud and underlying graph representation, regularized by both spatial smoothness and temporal consistency. We reformulate the optimization and present an efficient algorithm. Experimental results show that the proposed method significantly outperforms independent denoising of each frame from state-of-the-art static point cloud denoising approaches, on both Gaussian noise and simulated LiDAR noise.



### Cytology Image Analysis Techniques Towards Automation: Systematically Revisited
- **Arxiv ID**: http://arxiv.org/abs/2003.07529v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.07529v1)
- **Published**: 2020-03-17 04:56:19+00:00
- **Updated**: 2020-03-17 04:56:19+00:00
- **Authors**: Shyamali Mitra, Nibaran Das, Soumyajyoti Dey, Sukanta Chakrabarty, Mita Nasipuri, Mrinal Kanti Naskar
- **Comment**: None
- **Journal**: None
- **Summary**: Cytology is the branch of pathology which deals with the microscopic examination of cells for diagnosis of carcinoma or inflammatory conditions. Automation in cytology started in the early 1950s with the aim to reduce manual efforts in diagnosis of cancer. The inflush of intelligent technological units with high computational power and improved specimen collection techniques helped to achieve its technological heights. In the present survey, we focus on such image processing techniques which put steps forward towards the automation of cytology. We take a short tour to 17 types of cytology and explore various segmentation and/or classification techniques which evolved during last three decades boosting the concept of automation in cytology. It is observed, that most of the works are aligned towards three types of cytology: Cervical, Breast and Lung, which are discussed elaborately in this paper. The user-end systems developed during that period are summarized to comprehend the overall growth in the respective domains. To be precise, we discuss the diversity of the state-of-the-art methodologies, their challenges to provide prolific and competent future research directions inbringing the cytology-based commercial systems into the mainstream.



### Revisiting the Sibling Head in Object Detector
- **Arxiv ID**: http://arxiv.org/abs/2003.07540v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.07540v1)
- **Published**: 2020-03-17 05:21:54+00:00
- **Updated**: 2020-03-17 05:21:54+00:00
- **Authors**: Guanglu Song, Yu Liu, Xiaogang Wang
- **Comment**: Accept to CVPR 2020 & Method of Champion of OpenImage Challenge 2019,
  detection track
- **Journal**: None
- **Summary**: The ``shared head for classification and localization'' (sibling head), firstly denominated in Fast RCNN~\cite{girshick2015fast}, has been leading the fashion of the object detection community in the past five years. This paper provides the observation that the spatial misalignment between the two object functions in the sibling head can considerably hurt the training process, but this misalignment can be resolved by a very simple operator called task-aware spatial disentanglement (TSD). Considering the classification and regression, TSD decouples them from the spatial dimension by generating two disentangled proposals for them, which are estimated by the shared proposal. This is inspired by the natural insight that for one instance, the features in some salient area may have rich information for classification while these around the boundary may be good at bounding box regression. Surprisingly, this simple design can boost all backbones and models on both MS COCO and Google OpenImage consistently by ~3% mAP. Further, we propose a progressive constraint to enlarge the performance margin between the disentangled and the shared proposals, and gain ~1% more mAP. We show the \algname{} breaks through the upper bound of nowadays single-model detector by a large margin (mAP 49.4 with ResNet-101, 51.2 with SENet154), and is the core model of our 1st place solution on the Google OpenImage Challenge 2019.



### KPNet: Towards Minimal Face Detector
- **Arxiv ID**: http://arxiv.org/abs/2003.07543v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.07543v1)
- **Published**: 2020-03-17 05:37:45+00:00
- **Updated**: 2020-03-17 05:37:45+00:00
- **Authors**: Guanglu Song, Yu Liu, Yuhang Zang, Xiaogang Wang, Biao Leng, Qingsheng Yuan
- **Comment**: AAAI 2020
- **Journal**: None
- **Summary**: The small receptive field and capacity of minimal neural networks limit their performance when using them to be the backbone of detectors. In this work, we find that the appearance feature of a generic face is discriminative enough for a tiny and shallow neural network to verify from the background. And the essential barriers behind us are 1) the vague definition of the face bounding box and 2) tricky design of anchor-boxes or receptive field. Unlike most top-down methods for joint face detection and alignment, the proposed KPNet detects small facial keypoints instead of the whole face by in a bottom-up manner. It first predicts the facial landmarks from a low-resolution image via the well-designed fine-grained scale approximation and scale adaptive soft-argmax operator. Finally, the precise face bounding boxes, no matter how we define it, can be inferred from the keypoints. Without any complex head architecture or meticulous network designing, the KPNet achieves state-of-the-art accuracy on generic face detection and alignment benchmarks with only $\sim1M$ parameters, which runs at 1000fps on GPU and is easy to perform real-time on most modern front-end chips.



### 1st Place Solutions for OpenImage2019 -- Object Detection and Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2003.07557v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.07557v1)
- **Published**: 2020-03-17 06:45:07+00:00
- **Updated**: 2020-03-17 06:45:07+00:00
- **Authors**: Yu Liu, Guanglu Song, Yuhang Zang, Yan Gao, Enze Xie, Junjie Yan, Chen Change Loy, Xiaogang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: This article introduces the solutions of the two champion teams, `MMfruit' for the detection track and `MMfruitSeg' for the segmentation track, in OpenImage Challenge 2019. It is commonly known that for an object detector, the shared feature at the end of the backbone is not appropriate for both classification and regression, which greatly limits the performance of both single stage detector and Faster RCNN \cite{ren2015faster} based detector. In this competition, we observe that even with a shared feature, different locations in one object has completely inconsistent performances for the two tasks. \textit{E.g. the features of salient locations are usually good for classification, while those around the object edge are good for regression.} Inspired by this, we propose the Decoupling Head (DH) to disentangle the object classification and regression via the self-learned optimal feature extraction, which leads to a great improvement. Furthermore, we adjust the soft-NMS algorithm to adj-NMS to obtain stable performance improvement. Finally, a well-designed ensemble strategy via voting the bounding box location and confidence is proposed. We will also introduce several training/inferencing strategies and a bag of tricks that give minor improvement. Given those masses of details, we train and aggregate 28 global models with various backbones, heads and 3+2 expert models, and achieves the 1st place on the OpenImage 2019 Object Detection Challenge on the both public and private leadboards. Given such good instance bounding box, we further design a simple instance-level semantic segmentation pipeline and achieve the 1st place on the segmentation challenge.



### GFTE: Graph-based Financial Table Extraction
- **Arxiv ID**: http://arxiv.org/abs/2003.07560v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.07560v1)
- **Published**: 2020-03-17 07:10:05+00:00
- **Updated**: 2020-03-17 07:10:05+00:00
- **Authors**: Yiren Li, Zheng Huang, Junchi Yan, Yi Zhou, Fan Ye, Xianhui Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Tabular data is a crucial form of information expression, which can organize data in a standard structure for easy information retrieval and comparison. However, in financial industry and many other fields tables are often disclosed in unstructured digital files, e.g. Portable Document Format (PDF) and images, which are difficult to be extracted directly. In this paper, to facilitate deep learning based table extraction from unstructured digital files, we publish a standard Chinese dataset named FinTab, which contains more than 1,600 financial tables of diverse kinds and their corresponding structure representation in JSON. In addition, we propose a novel graph-based convolutional neural network model named GFTE as a baseline for future comparison. GFTE integrates image feature, position feature and textual feature together for precise edge prediction and reaches overall good results.



### Neural Mesh Refiner for 6-DoF Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2003.07561v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.07561v3)
- **Published**: 2020-03-17 07:12:22+00:00
- **Updated**: 2020-03-26 10:14:40+00:00
- **Authors**: Di Wu, Yihao Chen, Xianbiao Qi, Yongjian Yu, Weixuan Chen, Rong Xiao
- **Comment**: None
- **Journal**: None
- **Summary**: How can we effectively utilise the 2D monocular image information for recovering the 6D pose (6-DoF) of the visual objects? Deep learning has shown to be effective for robust and real-time monocular pose estimation. Oftentimes, the network learns to regress the 6-DoF pose using a naive loss function. However, due to a lack of geometrical scene understanding from the directly regressed pose estimation, there are misalignments between the rendered mesh from the 3D object and the 2D instance segmentation result, e.g., bounding boxes and masks prediction. This paper bridges the gap between 2D mask generation and 3D location prediction via a differentiable neural mesh renderer. We utilise the overlay between the accurate mask prediction and less accurate mesh prediction to iteratively optimise the direct regressed 6D pose information with a focus on translation estimation. By leveraging geometry, we demonstrate that our technique significantly improves direct regression performance on the difficult task of translation estimation and achieve the state of the art results on Peking University/Baidu - Autonomous Driving dataset and the ApolloScape 3D Car Instance dataset. The code can be found at \url{https://bit.ly/2IRihfU}.



### Feedback Graph Convolutional Network for Skeleton-based Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2003.07564v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.07564v1)
- **Published**: 2020-03-17 07:20:47+00:00
- **Updated**: 2020-03-17 07:20:47+00:00
- **Authors**: Hao Yang, Dan Yan, Li Zhang, Dong Li, YunDa Sun, ShaoDi You, Stephen J. Maybank
- **Comment**: 18 pages, 5 figures
- **Journal**: None
- **Summary**: Skeleton-based action recognition has attracted considerable attention in computer vision since skeleton data is more robust to the dynamic circumstance and complicated background than other modalities. Recently, many researchers have used the Graph Convolutional Network (GCN) to model spatial-temporal features of skeleton sequences by an end-to-end optimization. However, conventional GCNs are feedforward networks which are impossible for low-level layers to access semantic information in the high-level layers. In this paper, we propose a novel network, named Feedback Graph Convolutional Network (FGCN). This is the first work that introduces the feedback mechanism into GCNs and action recognition. Compared with conventional GCNs, FGCN has the following advantages: (1) a multi-stage temporal sampling strategy is designed to extract spatial-temporal features for action recognition in a coarse-to-fine progressive process; (2) A dense connections based Feedback Graph Convolutional Block (FGCB) is proposed to introduce feedback connections into the GCNs. It transmits the high-level semantic features to the low-level layers and flows temporal information stage by stage to progressively model global spatial-temporal features for action recognition; (3) The FGCN model provides early predictions. In the early stages, the model receives partial information about actions. Naturally, its predictions are relatively coarse. The coarse predictions are treated as the prior to guide the feature learning of later stages for a accurate prediction. Extensive experiments on the datasets, NTU-RGB+D, NTU-RGB+D120 and Northwestern-UCLA, demonstrate that the proposed FGCN is effective for action recognition. It achieves the state-of-the-art performance on the three datasets.



### Two Tier Prediction of Stroke Using Artificial Neural Networks and Support Vector Machines
- **Arxiv ID**: http://arxiv.org/abs/2003.08354v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2003.08354v2)
- **Published**: 2020-03-17 07:20:59+00:00
- **Updated**: 2020-03-19 00:29:19+00:00
- **Authors**: Jerrin Thomas Panachakel, Jeena R. S
- **Comment**: arXiv admin note: text overlap with arXiv:1706.08227
- **Journal**: None
- **Summary**: Cerebrovascular accident (CVA) or stroke is the rapid loss of brain function due to disturbance in the blood supply to the brain. Statistically, stroke is the second leading cause of death. This has motivated us to suggest a two-tier system for predicting stroke; the first tier makes use of Artificial Neural Network (ANN) to predict the chances of a person suffering from stroke. The ANN is trained the using the values of various risk factors of stroke of several patients who had stroke. Once a person is classified as having a high risk of stroke, s/he undergoes another the tier-2 classification test where his/her neuro MRI (Magnetic resonance imaging) is analysed to predict the chances of stroke. The tier-2 uses Non-negative Matrix Factorization and Haralick Textural features for feature extraction and SVM classifier for classification. We have obtained an accuracy of 96.67% in tier-1 and an accuracy of 70% in tier-2.



### Optimal Image Smoothing and Its Applications in Anomaly Detection in Remote Sensing
- **Arxiv ID**: http://arxiv.org/abs/2003.08210v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.08210v1)
- **Published**: 2020-03-17 07:26:14+00:00
- **Updated**: 2020-03-17 07:26:14+00:00
- **Authors**: M. Kiani
- **Comment**: Second International Conference On Biology and Earth Sciences
- **Journal**: None
- **Summary**: This paper is focused on deriving an optimal image smoother. The optimization is done through the minimization of the norm of the Laplace operator in the image coordinate system. Discretizing the Laplace operator and using the method of Euler-Lagrange result in a weighted average scheme for the optimal smoother. Satellite imagery can be smoothed by this optimal smoother. It is also very fast and can be used for detecting the anomalies in the image. A real anomaly detecting problem is considered for the Qom region in Iran. Satellite image in different bands are smoothed. Comparing the smoothed and original images in different bands, the maps of anomalies are presented. Comparison between the derived method and the existing methods reveals that it is more efficient in detecting anomalies in the region.



### Identification and Classification of Phenomena in Multispectral Satellite Imagery Using a New Image Smoother Method and its Applications in Environmental Remote Sensing
- **Arxiv ID**: http://arxiv.org/abs/2003.08209v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.08209v1)
- **Published**: 2020-03-17 07:34:43+00:00
- **Updated**: 2020-03-17 07:34:43+00:00
- **Authors**: M. Kiani
- **Comment**: Second International Congress on Engineering, Technology and
  Innovation
- **Journal**: None
- **Summary**: In this paper a new method of image smoothing for satellite imagery and its applications in environmental remote sensing are presented. This method is based on the global gradient minimization over the whole image. With respect to the image discrete identity, the continuous minimization problem is discretized. Using the finite difference numerical method of differentiation, a simple yet efficient 5*5-pixel template is derived. Convolution of the derived template with the image in different bands results in the discrimination of various image elements. This method is extremely fast, besides being highly precise. A case study is presented for the northern Iran, covering parts of the Caspian Sea. Comparison of the method with the usual Laplacian template reveals that it is more capable of distinguishing phenomena in the image.



### Heat and Blur: An Effective and Fast Defense Against Adversarial Examples
- **Arxiv ID**: http://arxiv.org/abs/2003.07573v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2003.07573v1)
- **Published**: 2020-03-17 08:11:18+00:00
- **Updated**: 2020-03-17 08:11:18+00:00
- **Authors**: Haya Brama, Tal Grinshpoun
- **Comment**: Submitted to IJCAI 2020 conference
- **Journal**: None
- **Summary**: The growing incorporation of artificial neural networks (NNs) into many fields, and especially into life-critical systems, is restrained by their vulnerability to adversarial examples (AEs). Some existing defense methods can increase NNs' robustness, but they often require special architecture or training procedures and are irrelevant to already trained models. In this paper, we propose a simple defense that combines feature visualization with input modification, and can, therefore, be applicable to various pre-trained networks. By reviewing several interpretability methods, we gain new insights regarding the influence of AEs on NNs' computation. Based on that, we hypothesize that information about the "true" object is preserved within the NN's activity, even when the input is adversarial, and present a feature visualization version that can extract that information in the form of relevance heatmaps. We then use these heatmaps as a basis for our defense, in which the adversarial effects are corrupted by massive blurring. We also provide a new evaluation metric that can capture the effects of both attacks and defenses more thoroughly and descriptively, and demonstrate the effectiveness of the defense and the utility of the suggested evaluation measurement with VGG19 results on the ImageNet dataset.



### Weakly-Supervised 3D Human Pose Learning via Multi-view Images in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2003.07581v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.07581v1)
- **Published**: 2020-03-17 08:47:16+00:00
- **Updated**: 2020-03-17 08:47:16+00:00
- **Authors**: Umar Iqbal, Pavlo Molchanov, Jan Kautz
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: One major challenge for monocular 3D human pose estimation in-the-wild is the acquisition of training data that contains unconstrained images annotated with accurate 3D poses. In this paper, we address this challenge by proposing a weakly-supervised approach that does not require 3D annotations and learns to estimate 3D poses from unlabeled multi-view data, which can be acquired easily in in-the-wild environments. We propose a novel end-to-end learning framework that enables weakly-supervised training using multi-view consistency. Since multi-view consistency is prone to degenerated solutions, we adopt a 2.5D pose representation and propose a novel objective function that can only be minimized when the predictions of the trained model are consistent and plausible across all camera views. We evaluate our proposed approach on two large scale datasets (Human3.6M and MPII-INF-3DHP) where it achieves state-of-the-art performance among semi-/weakly-supervised methods.



### SiamSNN: Siamese Spiking Neural Networks for Energy-Efficient Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2003.07584v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2003.07584v3)
- **Published**: 2020-03-17 08:49:51+00:00
- **Updated**: 2021-06-19 05:25:33+00:00
- **Authors**: Yihao Luo, Min Xu, Caihong Yuan, Xiang Cao, Liangqi Zhang, Yan Xu, Tianjiang Wang, Qi Feng
- **Comment**: Accepted by ICANN2021, 12 pages, 5figures
- **Journal**: None
- **Summary**: Recently spiking neural networks (SNNs), the third-generation of neural networks has shown remarkable capabilities of energy-efficient computing, which is a promising alternative for deep neural networks (DNNs) with high energy consumption. SNNs have reached competitive results compared to DNNs in relatively simple tasks and small datasets such as image classification and MNIST/CIFAR, while few studies on more challenging vision tasks on complex datasets. In this paper, we focus on extending deep SNNs to object tracking, a more advanced vision task with embedded applications and energy-saving requirements, and present a spike-based Siamese network called SiamSNN. Specifically, we propose an optimized hybrid similarity estimation method to exploit temporal information in the SNNs, and introduce a novel two-status coding scheme to optimize the temporal distribution of output spike trains for further improvements. SiamSNN is the first deep SNN tracker that achieves short latency and low precision loss on the visual object tracking benchmarks OTB2013/2015, VOT2016/2018, and GOT-10k. Moreover, SiamSNN achieves notably low energy consumption and real-time on Neuromorphic chip TrueNorth.



### Construe: a software solution for the explanation-based interpretation of time series
- **Arxiv ID**: http://arxiv.org/abs/2003.07596v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LO, I.2; I.5
- **Links**: [PDF](http://arxiv.org/pdf/2003.07596v1)
- **Published**: 2020-03-17 09:26:55+00:00
- **Updated**: 2020-03-17 09:26:55+00:00
- **Authors**: Tomas Teijeiro, Paulo Felix
- **Comment**: Original Software Publication. 10 pages, 4 figures
- **Journal**: None
- **Summary**: This paper presents a software implementation of a general framework for time series interpretation based on abductive reasoning. The software provides a data model and a set of algorithms to make inference to the best explanation of a time series, resulting in a description in multiple abstraction levels of the processes underlying the time series. As a proof of concept, a comprehensive knowledge base for the electrocardiogram (ECG) domain is provided, so it can be used directly as a tool for ECG analysis. This tool has been successfully validated in several noteworthy problems, such as heartbeat classification or atrial fibrillation detection.



### Rectified Meta-Learning from Noisy Labels for Robust Image-based Plant Disease Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/2003.07603v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.07603v2)
- **Published**: 2020-03-17 09:51:30+00:00
- **Updated**: 2020-03-18 03:01:25+00:00
- **Authors**: Ruifeng Shi, Deming Zhai, Xianming Liu, Junjun Jiang, Wen Gao
- **Comment**: None
- **Journal**: None
- **Summary**: Plant diseases serve as one of main threats to food security and crop production. It is thus valuable to exploit recent advances of artificial intelligence to assist plant disease diagnosis. One popular approach is to transform this problem as a leaf image classification task, which can be then addressed by the powerful convolutional neural networks (CNNs). However, the performance of CNN-based classification approach depends on a large amount of high-quality manually labeled training data, which are inevitably introduced noise on labels in practice, leading to model overfitting and performance degradation. To overcome this problem, we propose a novel framework that incorporates rectified meta-learning module into common CNN paradigm to train a noise-robust deep network without using extra supervision information. The proposed method enjoys the following merits: i) A rectified meta-learning is designed to pay more attention to unbiased samples, leading to accelerated convergence and improved classification accuracy. ii) Our method is free on assumption of label noise distribution, which works well on various kinds of noise. iii) Our method serves as a plug-and-play module, which can be embedded into any deep models optimized by gradient descent based method. Extensive experiments are conducted to demonstrate the superior performance of our algorithm over the state-of-the-arts.



### Building Computationally Efficient and Well-Generalizing Person Re-Identification Models with Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/2003.07618v2
- **DOI**: 10.1109/ICPR48806.2021.9412598
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.07618v2)
- **Published**: 2020-03-17 10:24:58+00:00
- **Updated**: 2020-07-07 12:23:15+00:00
- **Authors**: Vladislav Sovrasov, Dmitry Sidnev
- **Comment**: Submitted to International Conference on Pattern Recognition (ICPR
  2020)
- **Journal**: None
- **Summary**: This work considers the problem of domain shift in person re-identification.Being trained on one dataset, a re-identification model usually performs much worse on unseen data. Partially this gap is caused by the relatively small scale of person re-identification datasets (compared to face recognition ones, for instance), but it is also related to training objectives. We propose to use the metric learning objective, namely AM-Softmax loss, and some additional training practices to build well-generalizing, yet, computationally efficient models. We use recently proposed Omni-Scale Network (OSNet) architecture combined with several training tricks and architecture adjustments to obtain state-of-the art results in cross-domain generalization problem on a large-scale MSMT17 dataset in three setups: MSMT17-all->DukeMTMC, MSMT17-train->Market1501 and MSMT17-all->Market1501.



### Unsupervised Learning of Category-Specific Symmetric 3D Keypoints from Point Sets
- **Arxiv ID**: http://arxiv.org/abs/2003.07619v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.07619v3)
- **Published**: 2020-03-17 10:28:02+00:00
- **Updated**: 2021-01-06 09:56:02+00:00
- **Authors**: Clara Fernandez-Labrador, Ajad Chhatkuli, Danda Pani Paudel, Jose J. Guerrero, Cédric Demonceaux, Luc Van Gool
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic discovery of category-specific 3D keypoints from a collection of objects of some category is a challenging problem. One reason is that not all objects in a category necessarily have the same semantic parts. The level of difficulty adds up further when objects are represented by 3D point clouds, with variations in shape and unknown coordinate frames. We define keypoints to be category-specific, if they meaningfully represent objects' shape and their correspondences can be simply established order-wise across all objects. This paper aims at learning category-specific 3D keypoints, in an unsupervised manner, using a collection of misaligned 3D point clouds of objects from an unknown category. In order to do so, we model shapes defined by the keypoints, within a category, using the symmetric linear basis shapes without assuming the plane of symmetry to be known. The usage of symmetry prior leads us to learn stable keypoints suitable for higher misalignments. To the best of our knowledge, this is the first work on learning such keypoints directly from 3D point clouds. Using categories from four benchmark datasets, we demonstrate the quality of our learned keypoints by quantitative and qualitative evaluations. Our experiments also show that the keypoints discovered by our method are geometrically and semantically consistent.



### Anomaly Detection in Video Data Based on Probabilistic Latent Space Models
- **Arxiv ID**: http://arxiv.org/abs/2003.07623v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.07623v1)
- **Published**: 2020-03-17 10:32:22+00:00
- **Updated**: 2020-03-17 10:32:22+00:00
- **Authors**: Giulia Slavic, Damian Campo, Mohamad Baydoun, Pablo Marin, David Martin, Lucio Marcenaro, Carlo Regazzoni
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a method for detecting anomalies in video data. A Variational Autoencoder (VAE) is used for reducing the dimensionality of video frames, generating latent space information that is comparable to low-dimensional sensory data (e.g., positioning, steering angle), making feasible the development of a consistent multi-modal architecture for autonomous vehicles. An Adapted Markov Jump Particle Filter defined by discrete and continuous inference levels is employed to predict the following frames and detecting anomalies in new video sequences. Our method is evaluated on different video scenarios where a semi-autonomous vehicle performs a set of tasks in a closed environment.



### Explaining Deep Neural Networks and Beyond: A Review of Methods and Applications
- **Arxiv ID**: http://arxiv.org/abs/2003.07631v2
- **DOI**: 10.1109/JPROC.2021.3060483
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.07631v2)
- **Published**: 2020-03-17 10:45:51+00:00
- **Updated**: 2021-02-25 12:39:41+00:00
- **Authors**: Wojciech Samek, Grégoire Montavon, Sebastian Lapuschkin, Christopher J. Anders, Klaus-Robert Müller
- **Comment**: 30 pages, 20 figures
- **Journal**: None
- **Summary**: With the broader and highly successful usage of machine learning in industry and the sciences, there has been a growing demand for Explainable AI. Interpretability and explanation methods for gaining a better understanding about the problem solving abilities and strategies of nonlinear Machine Learning, in particular, deep neural networks, are therefore receiving increased attention. In this work we aim to (1) provide a timely overview of this active emerging field, with a focus on 'post-hoc' explanations, and explain its theoretical foundations, (2) put interpretability algorithms to a test both from a theory and comparative evaluation perspective using extensive simulations, (3) outline best practice aspects i.e. how to best include interpretation methods into the standard usage of machine learning and (4) demonstrate successful usage of explainable AI in a representative selection of application scenarios. Finally, we discuss challenges and possible future directions of this exciting foundational field of machine learning.



### Motion-Excited Sampler: Video Adversarial Attack with Sparked Prior
- **Arxiv ID**: http://arxiv.org/abs/2003.07637v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.07637v2)
- **Published**: 2020-03-17 10:54:12+00:00
- **Updated**: 2020-10-06 01:37:47+00:00
- **Authors**: Hu Zhang, Linchao Zhu, Yi Zhu, Yi Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks are known to be susceptible to adversarial noise, which are tiny and imperceptible perturbations. Most of previous work on adversarial attack mainly focus on image models, while the vulnerability of video models is less explored. In this paper, we aim to attack video models by utilizing intrinsic movement pattern and regional relative motion among video frames. We propose an effective motion-excited sampler to obtain motion-aware noise prior, which we term as sparked prior. Our sparked prior underlines frame correlations and utilizes video dynamics via relative motion. By using the sparked prior in gradient estimation, we can successfully attack a variety of video classification models with fewer number of queries. Extensive experimental results on four benchmark datasets validate the efficacy of our proposed method.



### EventSR: From Asynchronous Events to Image Reconstruction, Restoration, and Super-Resolution via End-to-End Adversarial Learning
- **Arxiv ID**: http://arxiv.org/abs/2003.07640v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.07640v1)
- **Published**: 2020-03-17 10:58:10+00:00
- **Updated**: 2020-03-17 10:58:10+00:00
- **Authors**: Lin Wang, Tae-Kyun Kim, Kuk-Jin Yoon
- **Comment**: Accepted by CVPR 2020
- **Journal**: None
- **Summary**: Event cameras sense intensity changes and have many advantages over conventional cameras. To take advantage of event cameras, some methods have been proposed to reconstruct intensity images from event streams. However, the outputs are still in low resolution (LR), noisy, and unrealistic. The low-quality outputs stem broader applications of event cameras, where high spatial resolution (HR) is needed as well as high temporal resolution, dynamic range, and no motion blur. We consider the problem of reconstructing and super-resolving intensity images from LR events, when no ground truth (GT) HR images and down-sampling kernels are available. To tackle the challenges, we propose a novel end-to-end pipeline that reconstructs LR images from event streams, enhances the image qualities and upsamples the enhanced images, called EventSR. For the absence of real GT images, our method is primarily unsupervised, deploying adversarial learning. To train EventSR, we create an open dataset including both real-world and simulated scenes. The use of both datasets boosts up the network performance, and the network architectures and various loss functions in each phase help improve the image qualities. The whole pipeline is trained in three phases. While each phase is mainly for one of the three tasks, the networks in earlier phases are fine-tuned by respective loss functions in an end-to-end manner. Experimental results show that EventSR reconstructs high-quality SR images from events for both simulated and real-world data.



### M$^5$L: Multi-Modal Multi-Margin Metric Learning for RGBT Tracking
- **Arxiv ID**: http://arxiv.org/abs/2003.07650v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.07650v1)
- **Published**: 2020-03-17 11:37:56+00:00
- **Updated**: 2020-03-17 11:37:56+00:00
- **Authors**: Zhengzheng Tu, Chun Lin, Chenglong Li, Jin Tang, Bin Luo
- **Comment**: 11 pages, 7 figures
- **Journal**: None
- **Summary**: Classifying the confusing samples in the course of RGBT tracking is a quite challenging problem, which hasn't got satisfied solution. Existing methods only focus on enlarging the boundary between positive and negative samples, however, the structured information of samples might be harmed, e.g., confusing positive samples are closer to the anchor than normal positive samples.To handle this problem, we propose a novel Multi-Modal Multi-Margin Metric Learning framework, named M$^5$L for RGBT tracking in this paper. In particular, we design a multi-margin structured loss to distinguish the confusing samples which play a most critical role in tracking performance boosting. To alleviate this problem, we additionally enlarge the boundaries between confusing positive samples and normal ones, between confusing negative samples and normal ones with predefined margins, by exploiting the structured information of all samples in each modality.Moreover, a cross-modality constraint is employed to reduce the difference between modalities and push positive samples closer to the anchor than negative ones from two modalities.In addition, to achieve quality-aware RGB and thermal feature fusion, we introduce the modality attentions and learn them using a feature fusion module in our network. Extensive experiments on large-scale datasets testify that our framework clearly improves the tracking performance and outperforms the state-of-the-art RGBT trackers.



### Weakly-Supervised Salient Object Detection via Scribble Annotations
- **Arxiv ID**: http://arxiv.org/abs/2003.07685v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.07685v1)
- **Published**: 2020-03-17 12:59:50+00:00
- **Updated**: 2020-03-17 12:59:50+00:00
- **Authors**: Jing Zhang, Xin Yu, Aixuan Li, Peipei Song, Bowen Liu, Yuchao Dai
- **Comment**: Accepted by IEEE/CVF CVPR 2020
- **Journal**: None
- **Summary**: Compared with laborious pixel-wise dense labeling, it is much easier to label data by scribbles, which only costs 1$\sim$2 seconds to label one image. However, using scribble labels to learn salient object detection has not been explored. In this paper, we propose a weakly-supervised salient object detection model to learn saliency from such annotations. In doing so, we first relabel an existing large-scale salient object detection dataset with scribbles, namely S-DUTS dataset. Since object structure and detail information is not identified by scribbles, directly training with scribble labels will lead to saliency maps of poor boundary localization. To mitigate this problem, we propose an auxiliary edge detection task to localize object edges explicitly, and a gated structure-aware loss to place constraints on the scope of structure to be recovered. Moreover, we design a scribble boosting scheme to iteratively consolidate our scribble annotations, which are then employed as supervision to learn high-quality saliency maps. As existing saliency evaluation metrics neglect to measure structure alignment of the predictions, the saliency map ranking metric may not comply with human perception. We present a new metric, termed saliency structure measure, to measure the structure alignment of the predicted saliency maps, which is more consistent with human perception. Extensive experiments on six benchmark datasets demonstrate that our method not only outperforms existing weakly-supervised/unsupervised methods, but also is on par with several fully-supervised state-of-the-art models. Our code and data is publicly available at https://github.com/JingZhang617/Scribble_Saliency.



### Parameter-Free Style Projection for Arbitrary Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/2003.07694v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2003.07694v2)
- **Published**: 2020-03-17 13:07:41+00:00
- **Updated**: 2022-02-08 16:54:20+00:00
- **Authors**: Siyu Huang, Haoyi Xiong, Tianyang Wang, Bihan Wen, Qingzhong Wang, Zeyu Chen, Jun Huan, Dejing Dou
- **Comment**: ICASSP 2022. Project page
  https://www.paddlepaddle.org.cn/hubdetail?name=stylepro_artistic and Code
  https://github.com/PaddlePaddle/PaddleHub/tree/dbca09ae78b5387ebe3b49f37ce88de45d41d26a/hub_module/modules/image/style_transfer/stylepro_artistic
- **Journal**: None
- **Summary**: Arbitrary image style transfer is a challenging task which aims to stylize a content image conditioned on arbitrary style images. In this task the feature-level content-style transformation plays a vital role for proper fusion of features. Existing feature transformation algorithms often suffer from loss of content or style details, non-natural stroke patterns, and unstable training. To mitigate these issues, this paper proposes a new feature-level style transformation technique, named Style Projection, for parameter-free, fast, and effective content-style transformation. This paper further presents a real-time feed-forward model to leverage Style Projection for arbitrary image style transfer, which includes a regularization term for matching the semantics between input contents and stylized outputs. Extensive qualitative analysis, quantitative evaluation, and user study have demonstrated the effectiveness and efficiency of the proposed methods.



### $F$, $B$, Alpha Matting
- **Arxiv ID**: http://arxiv.org/abs/2003.07711v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.07711v1)
- **Published**: 2020-03-17 13:27:51+00:00
- **Updated**: 2020-03-17 13:27:51+00:00
- **Authors**: Marco Forte, François Pitié
- **Comment**: Submitted to ECCV2020
- **Journal**: None
- **Summary**: Cutting out an object and estimating its opacity mask, known as image matting, is a key task in many image editing applications. Deep learning approaches have made significant progress by adapting the encoder-decoder architecture of segmentation networks. However, most of the existing networks only predict the alpha matte and post-processing methods must then be used to recover the original foreground and background colours in the transparent regions. Recently, two methods have shown improved results by also estimating the foreground colours, but at a significant computational and memory cost.   In this paper, we propose a low-cost modification to alpha matting networks to also predict the foreground and background colours. We study variations of the training regime and explore a wide range of existing and novel loss functions for the joint prediction.   Our method achieves the state of the art performance on the Adobe Composition-1k dataset for alpha matte and composite colour quality. It is also the current best performing method on the alphamatting.com online evaluation.



### Multimodal Shape Completion via Conditional Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2003.07717v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.07717v3)
- **Published**: 2020-03-17 13:37:52+00:00
- **Updated**: 2020-07-08 13:38:29+00:00
- **Authors**: Rundi Wu, Xuelin Chen, Yixin Zhuang, Baoquan Chen
- **Comment**: Accepted to ECCV 2020 (spotlight). Project page at
  https://chriswu1997.github.io/files/multimodal-pc/index.html
- **Journal**: None
- **Summary**: Several deep learning methods have been proposed for completing partial data from shape acquisition setups, i.e., filling the regions that were missing in the shape. These methods, however, only complete the partial shape with a single output, ignoring the ambiguity when reasoning the missing geometry. Hence, we pose a multi-modal shape completion problem, in which we seek to complete the partial shape with multiple outputs by learning a one-to-many mapping. We develop the first multimodal shape completion method that completes the partial shape via conditional generative modeling, without requiring paired training data. Our approach distills the ambiguity by conditioning the completion on a learned multimodal distribution of possible results. We extensively evaluate the approach on several datasets that contain varying forms of shape incompleteness, and compare among several baseline methods and variants of our methods qualitatively and quantitatively, demonstrating the merit of our method in completing partial shapes with both diversity and quality.



### Incremental Object Detection via Meta-Learning
- **Arxiv ID**: http://arxiv.org/abs/2003.08798v3
- **DOI**: 10.1109/TPAMI.2021.3124133
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.08798v3)
- **Published**: 2020-03-17 13:40:00+00:00
- **Updated**: 2021-12-15 16:56:11+00:00
- **Authors**: K J Joseph, Jathushan Rajasegaran, Salman Khan, Fahad Shahbaz Khan, Vineeth N Balasubramanian
- **Comment**: Published in IEEE Transactions on Pattern Analysis & Machine
  Intelligence, Nov 2021. Code is available in https://github.com/JosephKJ/iOD
- **Journal**: TPAMI, Nov 2021
- **Summary**: In a real-world setting, object instances from new classes can be continuously encountered by object detectors. When existing object detectors are applied to such scenarios, their performance on old classes deteriorates significantly. A few efforts have been reported to address this limitation, all of which apply variants of knowledge distillation to avoid catastrophic forgetting. We note that although distillation helps to retain previous learning, it obstructs fast adaptability to new tasks, which is a critical requirement for incremental learning. In this pursuit, we propose a meta-learning approach that learns to reshape model gradients, such that information across incremental tasks is optimally shared. This ensures a seamless information transfer via a meta-learned gradient preconditioning that minimizes forgetting and maximizes knowledge transfer. In comparison to existing meta-learning methods, our approach is task-agnostic, allows incremental addition of new-classes and scales to high-capacity models for object detection. We evaluate our approach on a variety of incremental learning settings defined on PASCAL-VOC and MS COCO datasets, where our approach performs favourably well against state-of-the-art methods.



### Teacher-Student chain for efficient semi-supervised histology image classification
- **Arxiv ID**: http://arxiv.org/abs/2003.08797v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.08797v2)
- **Published**: 2020-03-17 14:01:43+00:00
- **Updated**: 2020-03-20 09:53:31+00:00
- **Authors**: Shayne Shaw, Maciej Pajak, Aneta Lisowska, Sotirios A Tsaftaris, Alison Q O'Neil
- **Comment**: AI for Affordable Healthcare (AI4AH) workshop at ICLR 2020
- **Journal**: None
- **Summary**: Deep learning shows great potential for the domain of digital pathology. An automated digital pathology system could serve as a second reader, perform initial triage in large screening studies, or assist in reporting. However, it is expensive to exhaustively annotate large histology image databases, since medical specialists are a scarce resource. In this paper, we apply the semi-supervised teacher-student knowledge distillation technique proposed by Yalniz et al. (2019) to the task of quantifying prognostic features in colorectal cancer. We obtain accuracy improvements through extending this approach to a chain of students, where each student's predictions are used to train the next student i.e. the student becomes the teacher. Using the chain approach, and only 0.5% labelled data (the remaining 99.5% in the unlabelled pool), we match the accuracy of training on 100% labelled data. At lower percentages of labelled data, similar gains in accuracy are seen, allowing some recovery of accuracy even from a poor initial choice of labelled training set. In conclusion, this approach shows promise for reducing the annotation burden, thus increasing the affordability of automated digital pathology systems.



### Learning Meta Face Recognition in Unseen Domains
- **Arxiv ID**: http://arxiv.org/abs/2003.07733v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.07733v2)
- **Published**: 2020-03-17 14:10:30+00:00
- **Updated**: 2020-03-25 04:50:08+00:00
- **Authors**: Jianzhu Guo, Xiangyu Zhu, Chenxu Zhao, Dong Cao, Zhen Lei, Stan Z. Li
- **Comment**: Accepted to CVPR2020 (Oral)
- **Journal**: None
- **Summary**: Face recognition systems are usually faced with unseen domains in real-world applications and show unsatisfactory performance due to their poor generalization. For example, a well-trained model on webface data cannot deal with the ID vs. Spot task in surveillance scenario. In this paper, we aim to learn a generalized model that can directly handle new unseen domains without any model updating. To this end, we propose a novel face recognition method via meta-learning named Meta Face Recognition (MFR). MFR synthesizes the source/target domain shift with a meta-optimization objective, which requires the model to learn effective representations not only on synthesized source domains but also on synthesized target domains. Specifically, we build domain-shift batches through a domain-level sampling strategy and get back-propagated gradients/meta-gradients on synthesized source/target domains by optimizing multi-domain distributions. The gradients and meta-gradients are further combined to update the model to improve generalization. Besides, we propose two benchmarks for generalized face recognition evaluation. Experiments on our benchmarks validate the generalization of our method compared to several baselines and other state-of-the-arts. The proposed benchmarks will be available at https://github.com/cleardusk/MFR.



### A Novel Online Action Detection Framework from Untrimmed Video Streams
- **Arxiv ID**: http://arxiv.org/abs/2003.07734v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.07734v1)
- **Published**: 2020-03-17 14:11:24+00:00
- **Updated**: 2020-03-17 14:11:24+00:00
- **Authors**: Da-Hye Yoon, Nam-Gyu Cho, Seong-Whan Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Online temporal action localization from an untrimmed video stream is a challenging problem in computer vision. It is challenging because of i) in an untrimmed video stream, more than one action instance may appear, including background scenes, and ii) in online settings, only past and current information is available. Therefore, temporal priors, such as the average action duration of training data, which have been exploited by previous action detection methods, are not suitable for this task because of the high intra-class variation in human actions. We propose a novel online action detection framework that considers actions as a set of temporally ordered subclasses and leverages a future frame generation network to cope with the limited information issue associated with the problem outlined above. Additionally, we augment our data by varying the lengths of videos to allow the proposed method to learn about the high intra-class variation in human actions. We evaluate our method using two benchmark datasets, THUMOS'14 and ActivityNet, for an online temporal action localization scenario and demonstrate that the performance is comparable to state-of-the-art methods that have been proposed for offline settings.



### Human Activity Recognition from Wearable Sensor Data Using Self-Attention
- **Arxiv ID**: http://arxiv.org/abs/2003.09018v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.09018v1)
- **Published**: 2020-03-17 14:16:57+00:00
- **Updated**: 2020-03-17 14:16:57+00:00
- **Authors**: Saif Mahmud, M Tanjid Hasan Tonmoy, Kishor Kumar Bhaumik, A K M Mahbubur Rahman, M Ashraful Amin, Mohammad Shoyaib, Muhammad Asif Hossain Khan, Amin Ahsan Ali
- **Comment**: Accepted for publication at the 24th European Conference on
  Artificial Intelligence (ECAI-2020); 8 pages, 4 figures
- **Journal**: None
- **Summary**: Human Activity Recognition from body-worn sensor data poses an inherent challenge in capturing spatial and temporal dependencies of time-series signals. In this regard, the existing recurrent or convolutional or their hybrid models for activity recognition struggle to capture spatio-temporal context from the feature space of sensor reading sequence. To address this complex problem, we propose a self-attention based neural network model that foregoes recurrent architectures and utilizes different types of attention mechanisms to generate higher dimensional feature representation used for classification. We performed extensive experiments on four popular publicly available HAR datasets: PAMAP2, Opportunity, Skoda and USC-HAD. Our model achieve significant performance improvement over recent state-of-the-art models in both benchmark test subjects and Leave-one-subject-out evaluation. We also observe that the sensor attention maps produced by our model is able capture the importance of the modality and placement of the sensors in predicting the different activity classes.



### Geometric Approaches to Increase the Expressivity of Deep Neural Networks for MR Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2003.07740v1
- **DOI**: 10.1109/JSTSP.2020.2982777
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.07740v1)
- **Published**: 2020-03-17 14:18:37+00:00
- **Updated**: 2020-03-17 14:18:37+00:00
- **Authors**: Eunju Cha, Gyutaek Oh, Jong Chul Ye
- **Comment**: Accepted for IEEE JSTSP Special Issue on Domain Enriched Learning for
  Medical Imaging
- **Journal**: None
- **Summary**: Recently, deep learning approaches have been extensively investigated to reconstruct images from accelerated magnetic resonance image (MRI) acquisition. Although these approaches provide significant performance gain compared to compressed sensing MRI (CS-MRI), it is not clear how to choose a suitable network architecture to balance the trade-off between network complexity and performance. Recently, it was shown that an encoder-decoder convolutional neural network (CNN) can be interpreted as a piecewise linear basis-like representation, whose specific representation is determined by the ReLU activation patterns for a given input image. Thus, the expressivity or the representation power is determined by the number of piecewise linear regions. As an extension of this geometric understanding, this paper proposes a systematic geometric approach using bootstrapping and subnetwork aggregation using an attention module to increase the expressivity of the underlying neural network. Our method can be implemented in both k-space domain and image domain that can be trained in an end-to-end manner. Experimental results show that the proposed schemes significantly improve reconstruction performance with negligible complexity increases.



### Multi-modal Dense Video Captioning
- **Arxiv ID**: http://arxiv.org/abs/2003.07758v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG, cs.SD, eess.AS, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.07758v2)
- **Published**: 2020-03-17 15:15:17+00:00
- **Updated**: 2020-05-05 18:12:10+00:00
- **Authors**: Vladimir Iashin, Esa Rahtu
- **Comment**: To appear in the proceedings of CVPR Workshops 2020; Code:
  https://github.com/v-iashin/MDVC Project Page:
  https://v-iashin.github.io/mdvc
- **Journal**: None
- **Summary**: Dense video captioning is a task of localizing interesting events from an untrimmed video and producing textual description (captions) for each localized event. Most of the previous works in dense video captioning are solely based on visual information and completely ignore the audio track. However, audio, and speech, in particular, are vital cues for a human observer in understanding an environment. In this paper, we present a new dense video captioning approach that is able to utilize any number of modalities for event description. Specifically, we show how audio and speech modalities may improve a dense video captioning model. We apply automatic speech recognition (ASR) system to obtain a temporally aligned textual description of the speech (similar to subtitles) and treat it as a separate input alongside video frames and the corresponding audio track. We formulate the captioning task as a machine translation problem and utilize recently proposed Transformer architecture to convert multi-modal input data into textual descriptions. We demonstrate the performance of our model on ActivityNet Captions dataset. The ablation studies indicate a considerable contribution from audio and speech components suggesting that these modalities contain substantial complementary information to video frames. Furthermore, we provide an in-depth analysis of the ActivityNet Caption results by leveraging the category tags obtained from original YouTube videos. Code is publicly available: github.com/v-iashin/MDVC



### CycleISP: Real Image Restoration via Improved Data Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2003.07761v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.07761v1)
- **Published**: 2020-03-17 15:20:25+00:00
- **Updated**: 2020-03-17 15:20:25+00:00
- **Authors**: Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Ming-Hsuan Yang, Ling Shao
- **Comment**: CVPR 2020 (Oral)
- **Journal**: None
- **Summary**: The availability of large-scale datasets has helped unleash the true potential of deep convolutional neural networks (CNNs). However, for the single-image denoising problem, capturing a real dataset is an unacceptably expensive and cumbersome procedure. Consequently, image denoising algorithms are mostly developed and evaluated on synthetic data that is usually generated with a widespread assumption of additive white Gaussian noise (AWGN). While the CNNs achieve impressive results on these synthetic datasets, they do not perform well when applied on real camera images, as reported in recent benchmark datasets. This is mainly because the AWGN is not adequate for modeling the real camera noise which is signal-dependent and heavily transformed by the camera imaging pipeline. In this paper, we present a framework that models camera imaging pipeline in forward and reverse directions. It allows us to produce any number of realistic image pairs for denoising both in RAW and sRGB spaces. By training a new image denoising network on realistic synthetic data, we achieve the state-of-the-art performance on real camera benchmark datasets. The parameters in our model are ~5 times lesser than the previous best method for RAW denoising. Furthermore, we demonstrate that the proposed framework generalizes beyond image denoising problem e.g., for color matching in stereoscopic cinema. The source code and pre-trained models are available at https://github.com/swz30/CycleISP.



### Deep Active Learning for Remote Sensing Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2003.08793v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.08793v1)
- **Published**: 2020-03-17 15:57:36+00:00
- **Updated**: 2020-03-17 15:57:36+00:00
- **Authors**: Zhenshen Qu, Jingda Du, Yong Cao, Qiuyu Guan, Pengbo Zhao
- **Comment**: 6 pages, 3 figures
- **Journal**: None
- **Summary**: Recently, CNN object detectors have achieved high accuracy on remote sensing images but require huge labor and time costs on annotation. In this paper, we propose a new uncertainty-based active learning which can select images with more information for annotation and detector can still reach high performance with a fraction of the training images. Our method not only analyzes objects' classification uncertainty to find least confident objects but also considers their regression uncertainty to declare outliers. Besides, we bring out two extra weights to overcome two difficulties in remote sensing datasets, class-imbalance and difference in images' objects amount. We experiment our active learning algorithm on DOTA dataset with CenterNet as object detector. We achieve same-level performance as full supervision with only half images. We even override full supervision with 55% images and augmented weights on least confident images.



### A novel Deep Structure U-Net for Sea-Land Segmentation in Remote Sensing Images
- **Arxiv ID**: http://arxiv.org/abs/2003.07784v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.07784v1)
- **Published**: 2020-03-17 16:00:59+00:00
- **Updated**: 2020-03-17 16:00:59+00:00
- **Authors**: Pourya Shamsolmoali, Masoumeh Zareapoor, Ruili Wang, Huiyu Zhou, Jie Yang
- **Comment**: 14 pages, 14 figures
- **Journal**: None
- **Summary**: Sea-land segmentation is an important process for many key applications in remote sensing. Proper operative sea-land segmentation for remote sensing images remains a challenging issue due to complex and diverse transition between sea and lands. Although several Convolutional Neural Networks (CNNs) have been developed for sea-land segmentation, the performance of these CNNs is far from the expected target. This paper presents a novel deep neural network structure for pixel-wise sea-land segmentation, a Residual Dense U-Net (RDU-Net), in complex and high-density remote sensing images. RDU-Net is a combination of both down-sampling and up-sampling paths to achieve satisfactory results. In each down- and up-sampling path, in addition to the convolution layers, several densely connected residual network blocks are proposed to systematically aggregate multi-scale contextual information. Each dense network block contains multilevel convolution layers, short-range connections and an identity mapping connection which facilitates features re-use in the network and makes full use of the hierarchical features from the original images. These proposed blocks have a certain number of connections that are designed with shorter distance backpropagation between the layers and can significantly improve segmentation results whilst minimizing computational costs. We have performed extensive experiments on two real datasets Google Earth and ISPRS and compare the proposed RDUNet against several variations of Dense Networks. The experimental results show that RDUNet outperforms the other state-of-the-art approaches on the sea-land segmentation tasks.



### DistNet: Deep Tracking by displacement regression: application to bacteria growing in the Mother Machine
- **Arxiv ID**: http://arxiv.org/abs/2003.07790v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, q-bio.QM, I.4.8; I.4.6; I.2.10; J.3
- **Links**: [PDF](http://arxiv.org/pdf/2003.07790v2)
- **Published**: 2020-03-17 16:13:44+00:00
- **Updated**: 2020-09-19 10:12:44+00:00
- **Authors**: Jean Ollion, Charles Ollion
- **Comment**: 10 pages, 3 figures, 3 tables. Accepted in MICCAI 2020 conference
- **Journal**: None
- **Summary**: The mother machine is a popular microfluidic device that allows long-term time-lapse imaging of thousands of cells in parallel by microscopy. It has become a valuable tool for single-cell level quantitative analysis and characterization of many cellular processes such as gene expression and regulation, mutagenesis or response to antibiotics. The automated and quantitative analysis of the massive amount of data generated by such experiments is now the limiting step. In particular the segmentation and tracking of bacteria cells imaged in phase-contrast microscopy---with error rates compatible with high-throughput data---is a challenging problem.   In this work, we describe a novel formulation of the multi-object tracking problem, in which tracking is performed by a regression of the bacteria's displacement, allowing simultaneous tracking of multiple bacteria, despite their growth and division over time. Our method performs jointly segmentation and tracking, leveraging sequential information to increase segmentation accuracy.   We introduce a Deep Neural Network architecture taking advantage of a self-attention mechanism which yields extremely low tracking error rate and segmentation error rate. We demonstrate superior performance and speed compared to state-of-the-art methods. Our method is named DiSTNet which stands for DISTance+DISplacement Segmentation and Tracking Network.   While this method is particularly well suited for mother machine microscopy data, its general joint tracking and segmentation formulation could be applied to many other problems with different geometries.



### Hyperplane Arrangements of Trained ConvNets Are Biased
- **Arxiv ID**: http://arxiv.org/abs/2003.07797v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.07797v2)
- **Published**: 2020-03-17 16:22:17+00:00
- **Updated**: 2023-04-14 12:52:24+00:00
- **Authors**: Matteo Gamba, Stefan Carlsson, Hossein Azizpour, Mårten Björkman
- **Comment**: None
- **Journal**: None
- **Summary**: We investigate the geometric properties of the functions learned by trained ConvNets in the preactivation space of their convolutional layers, by performing an empirical study of hyperplane arrangements induced by a convolutional layer. We introduce statistics over the weights of a trained network to study local arrangements and relate them to the training dynamics. We observe that trained ConvNets show a significant statistical bias towards regular hyperplane configurations. Furthermore, we find that layers showing biased configurations are critical to validation performance for the architectures considered, trained on CIFAR10, CIFAR100 and ImageNet.



### Virtual staining for mitosis detection in Breast Histopathology
- **Arxiv ID**: http://arxiv.org/abs/2003.07801v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.07801v1)
- **Published**: 2020-03-17 16:33:34+00:00
- **Updated**: 2020-03-17 16:33:34+00:00
- **Authors**: Caner Mercan, Germonda Reijnen-Mooij, David Tellez Martin, Johannes Lotz, Nick Weiss, Marcel van Gerven, Francesco Ciompi
- **Comment**: 5 pages, 4 figures. Accepted for publication at the IEEE
  International Symposium on Biomedical Imaging (ISBI), 2020
- **Journal**: None
- **Summary**: We propose a virtual staining methodology based on Generative Adversarial Networks to map histopathology images of breast cancer tissue from H&E stain to PHH3 and vice versa. We use the resulting synthetic images to build Convolutional Neural Networks (CNN) for automatic detection of mitotic figures, a strong prognostic biomarker used in routine breast cancer diagnosis and grading. We propose several scenarios, in which CNN trained with synthetically generated histopathology images perform on par with or even better than the same baseline model trained with real images. We discuss the potential of this application to scale the number of training samples without the need for manual annotations.



### SAR Tomography at the Limit: Building Height Reconstruction Using Only 3-5 TanDEM-X Bistatic Interferograms
- **Arxiv ID**: http://arxiv.org/abs/2003.07803v1
- **DOI**: 10.1109/TGRS.2020.2986052
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.07803v1)
- **Published**: 2020-03-17 16:37:33+00:00
- **Updated**: 2020-03-17 16:37:33+00:00
- **Authors**: Yilei Shi, Richard Bamler, Yuanyuan Wang, Xiao Xiang Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-baseline interferometric synthetic aperture radar (InSAR) techniques are effective approaches for retrieving the 3-D information of urban areas. In order to obtain a plausible reconstruction, it is necessary to use more than twenty interferograms. Hence, these methods are commonly not appropriate for large-scale 3-D urban mapping using TanDEM-X data where only a few acquisitions are available in average for each city. This work proposes a new SAR tomographic processing framework to work with those extremely small stacks, which integrates the non-local filtering into SAR tomography inversion. The applicability of the algorithm is demonstrated using a TanDEM-X multi-baseline stack with 5 bistatic interferograms over the whole city of Munich, Germany. Systematic comparison of our result with TanDEM-X raw digital elevation models (DEM) and airborne LiDAR data shows that the relative height accuracy of two third buildings is within two meters, which outperforms the TanDEM-X raw DEM. The promising performance of the proposed algorithm paved the first step towards high quality large-scale 3-D urban mapping.



### Burst Denoising of Dark Images
- **Arxiv ID**: http://arxiv.org/abs/2003.07823v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.07823v2)
- **Published**: 2020-03-17 17:17:36+00:00
- **Updated**: 2020-06-18 05:28:21+00:00
- **Authors**: Ahmet Serdar Karadeniz, Erkut Erdem, Aykut Erdem
- **Comment**: This paper has been withdrawn by the authors to be replaced by a new
  version available at arXiv:2006.09845
- **Journal**: None
- **Summary**: Capturing images under extremely low-light conditions poses significant challenges for the standard camera pipeline. Images become too dark and too noisy, which makes traditional image enhancement techniques almost impossible to apply. Very recently, researchers have shown promising results using learning based approaches. Motivated by these ideas, in this paper, we propose a deep learning framework for obtaining clean and colorful RGB images from extremely dark raw images. The backbone of our framework is a novel coarse-to-fine network architecture that generates high-quality outputs in a progressive manner. The coarse network predicts a low-resolution, denoised raw image, which is then fed to the fine network to recover fine-scale details and realistic textures. To further reduce noise and improve color accuracy, we extend this network to a permutation invariant structure so that it takes a burst of low-light images as input and merges information from multiple images at the feature-level. Our experiments demonstrate that the proposed approach leads to perceptually more pleasing results than state-of-the-art methods by producing much sharper and higher quality images.



### Latent Embedding Feedback and Discriminative Features for Zero-Shot Classification
- **Arxiv ID**: http://arxiv.org/abs/2003.07833v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.07833v2)
- **Published**: 2020-03-17 17:34:16+00:00
- **Updated**: 2020-07-18 12:27:38+00:00
- **Authors**: Sanath Narayan, Akshita Gupta, Fahad Shahbaz Khan, Cees G. M. Snoek, Ling Shao
- **Comment**: Accepted for publication at ECCV 2020
- **Journal**: None
- **Summary**: Zero-shot learning strives to classify unseen categories for which no data is available during training. In the generalized variant, the test samples can further belong to seen or unseen categories. The state-of-the-art relies on Generative Adversarial Networks that synthesize unseen class features by leveraging class-specific semantic embeddings. During training, they generate semantically consistent features, but discard this constraint during feature synthesis and classification. We propose to enforce semantic consistency at all stages of (generalized) zero-shot learning: training, feature synthesis and classification. We first introduce a feedback loop, from a semantic embedding decoder, that iteratively refines the generated features during both the training and feature synthesis stages. The synthesized features together with their corresponding latent embeddings from the decoder are then transformed into discriminative features and utilized during classification to reduce ambiguities among categories. Experiments on (generalized) zero-shot object and action classification reveal the benefit of semantic consistency and iterative feedback, outperforming existing methods on six zero-shot learning benchmarks. Source code at https://github.com/akshitac8/tfvaegan.



### PTP: Parallelized Tracking and Prediction with Graph Neural Networks and Diversity Sampling
- **Arxiv ID**: http://arxiv.org/abs/2003.07847v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2003.07847v2)
- **Published**: 2020-03-17 17:53:33+00:00
- **Updated**: 2021-04-03 13:56:15+00:00
- **Authors**: Xinshuo Weng, Ye Yuan, Kris Kitani
- **Comment**: Published in Robotics and Automation Letters (RA-L) 2021, with the
  ICRA 2021 option. The first two authors contributed equally. Project website:
  https://www.xinshuoweng.com/projects/PTP/
- **Journal**: None
- **Summary**: Multi-object tracking (MOT) and trajectory prediction are two critical components in modern 3D perception systems that require accurate modeling of multi-agent interaction. We hypothesize that it is beneficial to unify both tasks under one framework in order to learn a shared feature representation of agent interaction. Furthermore, instead of performing tracking and prediction sequentially which can propagate errors from tracking to prediction, we propose a parallelized framework to mitigate the issue. Also, our parallel track-forecast framework incorporates two additional novel computational units. First, we use a feature interaction technique by introducing Graph Neural Networks (GNNs) to capture the way in which agents interact with one another. The GNN is able to improve discriminative feature learning for MOT association and provide socially-aware contexts for trajectory prediction. Second, we use a diversity sampling function to improve the quality and diversity of our forecasted trajectories. The learned sampling function is trained to efficiently extract a variety of outcomes from a generative trajectory distribution and helps avoid the problem of generating duplicate trajectory samples. We evaluate on KITTI and nuScenes datasets showing that our method with socially-aware feature learning and diversity sampling achieves new state-of-the-art performance on 3D MOT and trajectory prediction. Project website is: https://www.xinshuoweng.com/projects/PTP



### Learning to Structure an Image with Few Colors
- **Arxiv ID**: http://arxiv.org/abs/2003.07848v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.07848v2)
- **Published**: 2020-03-17 17:56:15+00:00
- **Updated**: 2021-05-11 07:41:13+00:00
- **Authors**: Yunzhong Hou, Liang Zheng, Stephen Gould
- **Comment**: None
- **Journal**: CVPR 2020
- **Summary**: Color and structure are the two pillars that construct an image. Usually, the structure is well expressed through a rich spectrum of colors, allowing objects in an image to be recognized by neural networks. However, under extreme limitations of color space, the structure tends to vanish, and thus a neural network might fail to understand the image. Interested in exploring this interplay between color and structure, we study the scientific problem of identifying and preserving the most informative image structures while constraining the color space to just a few bits, such that the resulting image can be recognized with possibly high accuracy. To this end, we propose a color quantization network, ColorCNN, which learns to structure the images from the classification loss in an end-to-end manner. Given a color space size, ColorCNN quantizes colors in the original image by generating a color index map and an RGB color palette. Then, this color-quantized image is fed to a pre-trained task network to evaluate its performance. In our experiment, with only a 1-bit color space (i.e., two colors), the proposed network achieves 82.1% top-1 accuracy on the CIFAR10 dataset, outperforming traditional color quantization methods by a large margin. For applications, when encoded with PNG, the proposed color quantization shows superiority over other image compression methods in the extremely low bit-rate regime. The code is available at: https://github.com/hou-yz/color_distillation.



### Blur, Noise, and Compression Robust Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2003.07849v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.07849v2)
- **Published**: 2020-03-17 17:56:22+00:00
- **Updated**: 2021-06-23 14:49:46+00:00
- **Authors**: Takuhiro Kaneko, Tatsuya Harada
- **Comment**: Accepted to CVPR 2021. Project page:
  https://takuhirok.github.io/BNCR-GAN/
- **Journal**: None
- **Summary**: Generative adversarial networks (GANs) have gained considerable attention owing to their ability to reproduce images. However, they can recreate training images faithfully despite image degradation in the form of blur, noise, and compression, generating similarly degraded images. To solve this problem, the recently proposed noise robust GAN (NR-GAN) provides a partial solution by demonstrating the ability to learn a clean image generator directly from noisy images using a two-generator model comprising image and noise generators. However, its application is limited to noise, which is relatively easy to decompose owing to its additive and reversible characteristics, and its application to irreversible image degradation, in the form of blur, compression, and combination of all, remains a challenge. To address these problems, we propose blur, noise, and compression robust GAN (BNCR-GAN) that can learn a clean image generator directly from degraded images without knowledge of degradation parameters (e.g., blur kernel types, noise amounts, or quality factor values). Inspired by NR-GAN, BNCR-GAN uses a multiple-generator model composed of image, blur-kernel, noise, and quality-factor generators. However, in contrast to NR-GAN, to address irreversible characteristics, we introduce masking architectures adjusting degradation strength values in a data-driven manner using bypasses before and after degradation. Furthermore, to suppress uncertainty caused by the combination of blur, noise, and compression, we introduce adaptive consistency losses imposing consistency between irreversible degradation processes according to the degradation strengths. We demonstrate the effectiveness of BNCR-GAN through large-scale comparative studies on CIFAR-10 and a generality analysis on FFHQ. In addition, we demonstrate the applicability of BNCR-GAN in image restoration.



### Axial-DeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2003.07853v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.07853v2)
- **Published**: 2020-03-17 17:59:56+00:00
- **Updated**: 2020-08-06 18:09:32+00:00
- **Authors**: Huiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan Yuille, Liang-Chieh Chen
- **Comment**: ECCV 2020 camera-ready
- **Journal**: None
- **Summary**: Convolution exploits locality for efficiency at a cost of missing long range context. Self-attention has been adopted to augment CNNs with non-local interactions. Recent works prove it possible to stack self-attention layers to obtain a fully attentional network by restricting the attention to a local region. In this paper, we attempt to remove this constraint by factorizing 2D self-attention into two 1D self-attentions. This reduces computation complexity and allows performing attention within a larger or even global region. In companion, we also propose a position-sensitive self-attention design. Combining both yields our position-sensitive axial-attention layer, a novel building block that one could stack to form axial-attention models for image classification and dense prediction. We demonstrate the effectiveness of our model on four large-scale datasets. In particular, our model outperforms all existing stand-alone self-attention models on ImageNet. Our Axial-DeepLab improves 2.8% PQ over bottom-up state-of-the-art on COCO test-dev. This previous state-of-the-art is attained by our small variant that is 3.8x parameter-efficient and 27x computation-efficient. Axial-DeepLab also achieves state-of-the-art results on Mapillary Vistas and Cityscapes.



### Learning to Accelerate Decomposition for Multi-Directional 3D Printing
- **Arxiv ID**: http://arxiv.org/abs/2004.03450v3
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2004.03450v3)
- **Published**: 2020-03-17 18:37:44+00:00
- **Updated**: 2020-07-18 04:50:55+00:00
- **Authors**: Chenming Wu, Yong-Jin Liu, Charlie C. L. Wang
- **Comment**: 8 pages, accepted by IEEE Robotics and Automation Letters 2020
- **Journal**: None
- **Summary**: Multi-directional 3D printing has the capability of decreasing or eliminating the need for support structures. Recent work proposed a beam-guided search algorithm to find an optimized sequence of plane-clipping, which gives volume decomposition of a given 3D model. Different printing directions are employed in different regions to fabricate a model with tremendously less support (or even no support in many cases).To obtain optimized decomposition, a large beam width needs to be used in the search algorithm, leading to a very time-consuming computation. In this paper, we propose a learning framework that can accelerate the beam-guided search by using a smaller number of the original beam width to obtain results with similar quality. Specifically, we use the results of beam-guided search with large beam width to train a scoring function for candidate clipping planes based on six newly proposed feature metrics. With the help of these feature metrics, both the current and the sequence-dependent information are captured by the neural network to score candidates of clipping. As a result, we can achieve around 3x computational speed. We test and demonstrate our accelerated decomposition on a large dataset of models for 3D printing.



### Deep connections between learning from limited labels & physical parameter estimation -- inspiration for regularization
- **Arxiv ID**: http://arxiv.org/abs/2003.07908v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, physics.geo-ph, 68T45, I.2.10; I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/2003.07908v1)
- **Published**: 2020-03-17 19:33:50+00:00
- **Updated**: 2020-03-17 19:33:50+00:00
- **Authors**: Bas Peters
- **Comment**: None
- **Journal**: None
- **Summary**: Recently established equivalences between differential equations and the structure of neural networks enabled some interpretation of training of a neural network as partial-differential-equation (PDE) constrained optimization. We add to the previously established connections, explicit regularization that is particularly beneficial in the case of single large-scale examples with partial annotation. We show that explicit regularization of model parameters in PDE constrained optimization translates to regularization of the network output. Examination of the structure of the corresponding Lagrangian and backpropagation algorithm do not reveal additional computational challenges. A hyperspectral imaging example shows that minimum prior information together with cross-validation for optimal regularization parameters boosts the segmentation accuracy.



### Breast Cancer Detection Using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2003.07911v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.07911v3)
- **Published**: 2020-03-17 19:41:00+00:00
- **Updated**: 2020-08-19 06:11:54+00:00
- **Authors**: Simon Hadush, Yaecob Girmay, Abiot Sinamo, Gebrekirstos Hagos
- **Comment**: None
- **Journal**: None
- **Summary**: Breast cancer is prevalent in Ethiopia that accounts 34% among women cancer patients. The diagnosis technique in Ethiopia is manual which was proven to be tedious, subjective, and challenging. Deep learning techniques are revolutionizing the field of medical image analysis and hence in this study, we proposed Convolutional Neural Networks (CNNs) for breast mass detection so as to minimize the overheads of manual analysis. CNN architecture is designed for the feature extraction stage and adapted both the Region Proposal Network (RPN) and Region of Interest (ROI) portion of the faster R-CNN for the automated breast mass abnormality detection. Our model detects mass region and classifies them into benign or malignant abnormality in mammogram(MG) images at once. For the proposed model, MG images were collected from different hospitals, locally.The images were passed through different preprocessing stages such as gaussian filter, median filter, bilateral filters and extracted the region of the breast from the background of the MG image. The performance of the model on test dataset is found to be: detection accuracy 91.86%, sensitivity of 94.67% and AUC-ROC of 92.2%.



### 3D medical image segmentation with labeled and unlabeled data using autoencoders at the example of liver segmentation in CT images
- **Arxiv ID**: http://arxiv.org/abs/2003.07923v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.07923v1)
- **Published**: 2020-03-17 20:20:43+00:00
- **Updated**: 2020-03-17 20:20:43+00:00
- **Authors**: Cheryl Sital, Tom Brosch, Dominique Tio, Alexander Raaijmakers, Jürgen Weese
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic segmentation of anatomical structures with convolutional neural networks (CNNs) constitutes a large portion of research in medical image analysis. The majority of CNN-based methods rely on an abundance of labeled data for proper training. Labeled medical data is often scarce, but unlabeled data is more widely available. This necessitates approaches that go beyond traditional supervised learning and leverage unlabeled data for segmentation tasks. This work investigates the potential of autoencoder-extracted features to improve segmentation with a CNN. Two strategies were considered. First, transfer learning where pretrained autoencoder features were used as initialization for the convolutional layers in the segmentation network. Second, multi-task learning where the tasks of segmentation and feature extraction, by means of input reconstruction, were learned and optimized simultaneously. A convolutional autoencoder was used to extract features from unlabeled data and a multi-scale, fully convolutional CNN was used to perform the target task of 3D liver segmentation in CT images. For both strategies, experiments were conducted with varying amounts of labeled and unlabeled training data. The proposed learning strategies improved results in $75\%$ of the experiments compared to training from scratch and increased the dice score by up to $0.040$ and $0.024$ for a ratio of unlabeled to labeled training data of about $32 : 1$ and $12.5 : 1$, respectively. The results indicate that both training strategies are more effective with a large ratio of unlabeled to labeled training data.



### Getting to 99% Accuracy in Interactive Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2003.07932v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.07932v1)
- **Published**: 2020-03-17 20:50:22+00:00
- **Updated**: 2020-03-17 20:50:22+00:00
- **Authors**: Marco Forte, Brian Price, Scott Cohen, Ning Xu, François Pitié
- **Comment**: Submitted for review to Signal Processing: Image Communication
- **Journal**: None
- **Summary**: Interactive object cutout tools are the cornerstone of the image editing workflow. Recent deep-learning based interactive segmentation algorithms have made significant progress in handling complex images and rough binary selections can typically be obtained with just a few clicks. Yet, deep learning techniques tend to plateau once this rough selection has been reached. In this work, we interpret this plateau as the inability of current algorithms to sufficiently leverage each user interaction and also as the limitations of current training/testing datasets.   We propose a novel interactive architecture and a novel training scheme that are both tailored to better exploit the user workflow. We also show that significant improvements can be further gained by introducing a synthetic training dataset that is specifically designed for complex object boundaries. Comprehensive experiments support our approach, and our network achieves state of the art performance.



### Segmentation of brain tumor on magnetic resonance imaging using a convolutional architecture
- **Arxiv ID**: http://arxiv.org/abs/2003.07934v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.07934v1)
- **Published**: 2020-03-17 20:55:48+00:00
- **Updated**: 2020-03-17 20:55:48+00:00
- **Authors**: Miriam Zulema Jacobo, Jose Mejia
- **Comment**: 6 pages, 5 figures
- **Journal**: None
- **Summary**: The brain is a complex organ controlling cognitive process and physical functions. Tumors in the brain are accelerated cell growths affecting the normal function and processes in the brain. MRI scans provides detailed images of the body being one of the most common tests to diagnose brain tumors. The process of segmentation of brain tumors from magnetic resonance imaging can provide a valuable guide for diagnosis, treatment planning and prediction of results. Here we consider the problem brain tumor segmentation using a Deep learning architecture for use in tumor segmentation. Although the proposed architecture is simple and computationally easy to train, it is capable of reaching $IoU$ levels of 0.95.



### Boosting Unconstrained Face Recognition with Auxiliary Unlabeled Data
- **Arxiv ID**: http://arxiv.org/abs/2003.07936v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.07936v2)
- **Published**: 2020-03-17 20:58:56+00:00
- **Updated**: 2021-04-18 09:11:41+00:00
- **Authors**: Yichun Shi, Anil K. Jain
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, significant progress has been made in face recognition, which can be partially attributed to the availability of large-scale labeled face datasets. However, since the faces in these datasets usually contain limited degree and types of variation, the resulting trained models generalize poorly to more realistic unconstrained face datasets. While collecting labeled faces with larger variations could be helpful, it is practically infeasible due to privacy and labor cost. In comparison, it is easier to acquire a large number of unlabeled faces from different domains, which could be used to regularize the learning of face representations. We present an approach to use such unlabeled faces to learn generalizable face representations, where we assume neither the access to identity labels nor domain labels for unlabeled images. Experimental results on unconstrained datasets show that a small amount of unlabeled data with sufficient diversity can (i) lead to an appreciable gain in recognition performance and (ii) outperform the supervised baseline when combined with less than half of the labeled data. Compared with the state-of-the-art face recognition methods, our method further improves their performance on challenging benchmarks, such as IJB-B, IJB-C and IJB-S.



### BrazilDAM: A Benchmark dataset for Tailings Dam Detection
- **Arxiv ID**: http://arxiv.org/abs/2003.07948v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.07948v2)
- **Published**: 2020-03-17 21:20:13+00:00
- **Updated**: 2020-05-13 14:47:06+00:00
- **Authors**: Edemir Ferreira, Matheus Brito, Remis Balaniuk, Mário S. Alvim, Jefersson A. dos Santos
- **Comment**: None
- **Journal**: None
- **Summary**: In this work we present BrazilDAM, a novel public dataset based on Sentinel-2 and Landsat-8 satellite images covering all tailings dams cataloged by the Brazilian National Mining Agency (ANM). The dataset was built using georeferenced images from 769 dams, recorded between 2016 and 2019. The time series were processed in order to produce cloud free images. The dams contain mining waste from different ore categories and have highly varying shapes, areas and volumes, making BrazilDAM particularly interesting and challenging to be used in machine learning benchmarks. The original catalog contains, besides the dam coordinates, information about: the main ore, constructive method, risk category, and associated potential damage. To evaluate BrazilDAM's predictive potential we performed classification essays using state-of-the-art deep Convolutional Neural Network (CNNs). In the experiments, we achieved an average classification accuracy of 94.11% in tailing dam binary classification task. In addition, others four setups of experiments were made using the complementary information from the original catalog, exhaustively exploiting the capacity of the proposed dataset.



### An End-to-end Framework For Low-Resolution Remote Sensing Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2003.07955v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.07955v1)
- **Published**: 2020-03-17 21:41:22+00:00
- **Updated**: 2020-03-17 21:41:22+00:00
- **Authors**: Matheus Barros Pereira, Jefersson Alex dos Santos
- **Comment**: None
- **Journal**: None
- **Summary**: High-resolution images for remote sensing applications are often not affordable or accessible, especially when in need of a wide temporal span of recordings. Given the easy access to low-resolution (LR) images from satellites, many remote sensing works rely on this type of data. The problem is that LR images are not appropriate for semantic segmentation, due to the need for high-quality data for accurate pixel prediction for this task. In this paper, we propose an end-to-end framework that unites a super-resolution and a semantic segmentation module in order to produce accurate thematic maps from LR inputs. It allows the semantic segmentation network to conduct the reconstruction process, modifying the input image with helpful textures. We evaluate the framework with three remote sensing datasets. The results show that the framework is capable of achieving a semantic segmentation performance close to native high-resolution data, while also surpassing the performance of a network trained with LR inputs.



### Ford Multi-AV Seasonal Dataset
- **Arxiv ID**: http://arxiv.org/abs/2003.07969v1
- **DOI**: 10.1177/0278364920961451
- **Categories**: **cs.RO**, cs.CV, cs.MA
- **Links**: [PDF](http://arxiv.org/pdf/2003.07969v1)
- **Published**: 2020-03-17 22:33:38+00:00
- **Updated**: 2020-03-17 22:33:38+00:00
- **Authors**: Siddharth Agarwal, Ankit Vora, Gaurav Pandey, Wayne Williams, Helen Kourous, James McBride
- **Comment**: 7 pages, 7 figures, Submitted to International Journal of Robotics
  Research (IJRR), Visit website at https://avdata.ford.com
- **Journal**: IJRR, Volume: 39 issue: 12 (2020), page(s): 1367-1376
- **Summary**: This paper presents a challenging multi-agent seasonal dataset collected by a fleet of Ford autonomous vehicles at different days and times during 2017-18. The vehicles traversed an average route of 66 km in Michigan that included a mix of driving scenarios such as the Detroit Airport, freeways, city-centers, university campus and suburban neighbourhoods, etc. Each vehicle used in this data collection is a Ford Fusion outfitted with an Applanix POS-LV GNSS system, four HDL-32E Velodyne 3D-lidar scanners, 6 Point Grey 1.3 MP Cameras arranged on the rooftop for 360-degree coverage and 1 Pointgrey 5 MP camera mounted behind the windshield for the forward field of view. We present the seasonal variation in weather, lighting, construction and traffic conditions experienced in dynamic urban environments. This dataset can help design robust algorithms for autonomous vehicles and multi-agent systems. Each log in the dataset is time-stamped and contains raw data from all the sensors, calibration values, pose trajectory, ground truth pose, and 3D maps. All data is available in Rosbag format that can be visualized, modified and applied using the open-source Robot Operating System (ROS). We also provide the output of state-of-the-art reflectivity-based localization for bench-marking purposes. The dataset can be freely downloaded at our website.



### Child Face Age-Progression via Deep Feature Aging
- **Arxiv ID**: http://arxiv.org/abs/2003.08788v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08788v1)
- **Published**: 2020-03-17 23:03:46+00:00
- **Updated**: 2020-03-17 23:03:46+00:00
- **Authors**: Debayan Deb, Divyansh Aggarwal, Anil K. Jain
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1911.07538
- **Journal**: None
- **Summary**: Given a gallery of face images of missing children, state-of-the-art face recognition systems fall short in identifying a child (probe) recovered at a later age. We propose a feature aging module that can age-progress deep face features output by a face matcher. In addition, the feature aging module guides age-progression in the image space such that synthesized aged faces can be utilized to enhance longitudinal face recognition performance of any face matcher without requiring any explicit training. For time lapses larger than 10 years (the missing child is found after 10 or more years), the proposed age-progression module improves the closed-set identification accuracy of FaceNet from 16.53% to 21.44% and CosFace from 60.72% to 66.12% on a child celebrity dataset, namely ITWCC. The proposed method also outperforms state-of-the-art approaches with a rank-1 identification rate of 95.91%, compared to 94.91%, on a public aging dataset, FG-NET, and 99.58%, compared to 99.50%, on CACD-VS. These results suggest that aging face features enhances the ability to identify young children who are possible victims of child trafficking or abduction.



