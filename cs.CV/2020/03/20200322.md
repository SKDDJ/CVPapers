# Arxiv Papers in cs.CV on 2020-03-22
### HDF: Hybrid Deep Features for Scene Image Representation
- **Arxiv ID**: http://arxiv.org/abs/2003.09773v1
- **DOI**: 10.1109/IJCNN48605.2020.9207106
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.09773v1)
- **Published**: 2020-03-22 01:05:08+00:00
- **Updated**: 2020-03-22 01:05:08+00:00
- **Authors**: Chiranjibi Sitaula, Yong Xiang, Anish Basnet, Sunil Aryal, Xuequan Lu
- **Comment**: 8 pages, Accepted in IEEE WCCI 2020 Conference
- **Journal**: Proceedings of IJCNN2020
- **Summary**: Nowadays it is prevalent to take features extracted from pre-trained deep learning models as image representations which have achieved promising classification performance. Existing methods usually consider either object-based features or scene-based features only. However, both types of features are important for complex images like scene images, as they can complement each other. In this paper, we propose a novel type of features -- hybrid deep features, for scene images. Specifically, we exploit both object-based and scene-based features at two levels: part image level (i.e., parts of an image) and whole image level (i.e., a whole image), which produces a total number of four types of deep features. Regarding the part image level, we also propose two new slicing techniques to extract part based features. Finally, we aggregate these four types of deep features via the concatenation operator. We demonstrate the effectiveness of our hybrid deep features on three commonly used scene datasets (MIT-67, Scene-15, and Event-8), in terms of the scene image classification task. Extensive comparisons show that our introduced features can produce state-of-the-art classification accuracies which are more consistent and stable than the results of existing features across all datasets.



### Computer Vision and Abnormal Patient Gait Assessment a Comparison of Machine Learning Models
- **Arxiv ID**: http://arxiv.org/abs/2004.02810v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.02810v1)
- **Published**: 2020-03-22 02:00:15+00:00
- **Updated**: 2020-03-22 02:00:15+00:00
- **Authors**: Jasmin Hundall, Benson A. Babu
- **Comment**: 2 tables
- **Journal**: None
- **Summary**: Abnormal gait, its associated falls and complications have high patient morbidity, mortality. Computer vision detects, predicts patient gait abnormalities, assesses fall risk and serves as clinical decision support tool for physicians. This paper performs a systematic review of how computer vision, machine learning models perform an abnormal patient's gait assessment. Computer vision is beneficial in gait analysis, it helps capture the patient posture. Several literature suggests the use of different machine learning algorithms such as SVM, ANN, K-Star, Random Forest, KNN, among others to perform the classification on the features extracted to study patient gait abnormalities.



### AQPDCITY Dataset: Picture-Based PM Monitoring in the Urban Area of Big Cities
- **Arxiv ID**: http://arxiv.org/abs/2003.09784v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, J.6.0
- **Links**: [PDF](http://arxiv.org/pdf/2003.09784v2)
- **Published**: 2020-03-22 02:37:11+00:00
- **Updated**: 2020-04-06 02:32:21+00:00
- **Authors**: Yonghui Zhang, Ke Gu
- **Comment**: None
- **Journal**: None
- **Summary**: Since Particulate Matters (PMs) are closely related to people's living and health, it has become one of the most important indicator of air quality monitoring around the world. But the existing sensor-based methods for PM monitoring have remarkable disadvantages, such as low-density monitoring stations and high-requirement monitoring conditions. It is highly desired to devise a method that can obtain the PM concentration at any location for the following air quality control in time. The prior works indicate that the PM concentration can be monitored by using ubiquitous photos. To further investigate such issue, we gathered 1,500 photos in big cities to establish a new AQPDCITY dataset. Experiments conducted to check nine state-of-the-art methods on this dataset show that the performance of those above methods perform poorly in the AQPDCITY dataset.



### Mission-Aware Spatio-Temporal Deep Learning Model for UAS Instantaneous Density Prediction
- **Arxiv ID**: http://arxiv.org/abs/2003.09785v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2003.09785v1)
- **Published**: 2020-03-22 02:40:28+00:00
- **Updated**: 2020-03-22 02:40:28+00:00
- **Authors**: Ziyi Zhao, Zhao Jin, Wentian Bai, Wentan Bai, Carlos Caicedo, M. Cenk Gursoy, Qinru Qiu
- **Comment**: None
- **Journal**: None
- **Summary**: The number of daily sUAS operations in uncontrolled low altitude airspace is expected to reach into the millions in a few years. Therefore, UAS density prediction has become an emerging and challenging problem. In this paper, a deep learning-based UAS instantaneous density prediction model is presented. The model takes two types of data as input: 1) the historical density generated from the historical data, and 2) the future sUAS mission information. The architecture of our model contains four components: Historical Density Formulation module, UAS Mission Translation module, Mission Feature Extraction module, and Density Map Projection module. The training and testing data are generated by a python based simulator which is inspired by the multi-agent air traffic resource usage simulator (MATRUS) framework. The quality of prediction is measured by the correlation score and the Area Under the Receiver Operating Characteristics (AUROC) between the predicted value and simulated value. The experimental results demonstrate outstanding performance of the deep learning-based UAS density predictor. Compared to the baseline models, for simplified traffic scenario where no-fly zones and safe distance among sUASs are not considered, our model improves the prediction accuracy by more than 15.2% and its correlation score reaches 0.947. In a more realistic scenario, where the no-fly zone avoidance and the safe distance among sUASs are maintained using A* routing algorithm, our model can still achieve 0.823 correlation score. Meanwhile, the AUROC can reach 0.951 for the hot spot prediction.



### Exploring Bottom-up and Top-down Cues with Attentive Learning for Webly Supervised Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2003.09790v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.09790v1)
- **Published**: 2020-03-22 03:11:24+00:00
- **Updated**: 2020-03-22 03:11:24+00:00
- **Authors**: Zhonghua Wu, Qingyi Tao, Guosheng Lin, Jianfei Cai
- **Comment**: None
- **Journal**: None
- **Summary**: Fully supervised object detection has achieved great success in recent years. However, abundant bounding boxes annotations are needed for training a detector for novel classes. To reduce the human labeling effort, we propose a novel webly supervised object detection (WebSOD) method for novel classes which only requires the web images without further annotations. Our proposed method combines bottom-up and top-down cues for novel class detection. Within our approach, we introduce a bottom-up mechanism based on the well-trained fully supervised object detector (i.e. Faster RCNN) as an object region estimator for web images by recognizing the common objectiveness shared by base and novel classes. With the estimated regions on the web images, we then utilize the top-down attention cues as the guidance for region classification. Furthermore, we propose a residual feature refinement (RFR) block to tackle the domain mismatch between web domain and the target domain. We demonstrate our proposed method on PASCAL VOC dataset with three different novel/base splits. Without any target-domain novel-class images and annotations, our proposed webly supervised object detection model is able to achieve promising performance for novel classes. Moreover, we also conduct transfer learning experiments on large scale ILSVRC 2013 detection dataset and achieve state-of-the-art performance.



### GISNet: Graph-Based Information Sharing Network For Vehicle Trajectory Prediction
- **Arxiv ID**: http://arxiv.org/abs/2003.11973v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.11973v1)
- **Published**: 2020-03-22 03:24:31+00:00
- **Updated**: 2020-03-22 03:24:31+00:00
- **Authors**: Ziyi Zhao, Haowen Fang, Zhao Jin, Qinru Qiu
- **Comment**: None
- **Journal**: None
- **Summary**: The trajectory prediction is a critical and challenging problem in the design of an autonomous driving system. Many AI-oriented companies, such as Google Waymo, Uber and DiDi, are investigating more accurate vehicle trajectory prediction algorithms. However, the prediction performance is governed by lots of entangled factors, such as the stochastic behaviors of surrounding vehicles, historical information of self-trajectory, and relative positions of neighbors, etc. In this paper, we propose a novel graph-based information sharing network (GISNet) that allows the information sharing between the target vehicle and its surrounding vehicles. Meanwhile, the model encodes the historical trajectory information of all the vehicles in the scene. Experiments are carried out on the public NGSIM US-101 and I-80 Dataset and the prediction performance is measured by the Root Mean Square Error (RMSE). The quantitative and qualitative experimental results show that our model significantly improves the trajectory prediction accuracy, by up to 50.00%, compared to existing models.



### Modal Regression based Structured Low-rank Matrix Recovery for Multi-view Learning
- **Arxiv ID**: http://arxiv.org/abs/2003.09799v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.09799v1)
- **Published**: 2020-03-22 03:57:38+00:00
- **Updated**: 2020-03-22 03:57:38+00:00
- **Authors**: Jiamiao Xu, Fangzhao Wang, Qinmu Peng, Xinge You, Shuo Wang, Xiao-Yuan Jing, C. L. Philip Chen
- **Comment**: This article has been accepted by IEEE Transactions on Neural
  Networks and Learning Systems
- **Journal**: None
- **Summary**: Low-rank Multi-view Subspace Learning (LMvSL) has shown great potential in cross-view classification in recent years. Despite their empirical success, existing LMvSL based methods are incapable of well handling view discrepancy and discriminancy simultaneously, which thus leads to the performance degradation when there is a large discrepancy among multi-view data. To circumvent this drawback, motivated by the block-diagonal representation learning, we propose Structured Low-rank Matrix Recovery (SLMR), a unique method of effectively removing view discrepancy and improving discriminancy through the recovery of structured low-rank matrix. Furthermore, recent low-rank modeling provides a satisfactory solution to address data contaminated by predefined assumptions of noise distribution, such as Gaussian or Laplacian distribution. However, these models are not practical since complicated noise in practice may violate those assumptions and the distribution is generally unknown in advance. To alleviate such limitation, modal regression is elegantly incorporated into the framework of SLMR (term it MR-SLMR). Different from previous LMvSL based methods, our MR-SLMR can handle any zero-mode noise variable that contains a wide range of noise, such as Gaussian noise, random noise and outliers. The alternating direction method of multipliers (ADMM) framework and half-quadratic theory are used to efficiently optimize MR-SLMR. Experimental results on four public databases demonstrate the superiority of MR-SLMR and its robustness to complicated noise.



### Toward Accurate and Realistic Virtual Try-on Through Shape Matching and Multiple Warps
- **Arxiv ID**: http://arxiv.org/abs/2003.10817v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.10817v2)
- **Published**: 2020-03-22 03:59:06+00:00
- **Updated**: 2020-03-27 01:15:54+00:00
- **Authors**: Kedan Li, Min Jin Chong, Jingen Liu, David Forsyth
- **Comment**: None
- **Journal**: None
- **Summary**: A virtual try-on method takes a product image and an image of a model and produces an image of the model wearing the product. Most methods essentially compute warps from the product image to the model image and combine using image generation methods. However, obtaining a realistic image is challenging because the kinematics of garments is complex and because outline, texture, and shading cues in the image reveal errors to human viewers. The garment must have appropriate drapes; texture must be warped to be consistent with the shape of a draped garment; small details (buttons, collars, lapels, pockets, etc.) must be placed appropriately on the garment, and so on. Evaluation is particularly difficult and is usually qualitative.   This paper uses quantitative evaluation on a challenging, novel dataset to demonstrate that (a) for any warping method, one can choose target models automatically to improve results, and (b) learning multiple coordinated specialized warpers offers further improvements on results. Target models are chosen by a learned embedding procedure that predicts a representation of the products the model is wearing. This prediction is used to match products to models. Specialized warpers are trained by a method that encourages a second warper to perform well in locations where the first works poorly. The warps are then combined using a U-Net. Qualitative evaluation confirms that these improvements are wholesale over outline, texture shading, and garment details.



### Review of data analysis in vision inspection of power lines with an in-depth discussion of deep learning technology
- **Arxiv ID**: http://arxiv.org/abs/2003.09802v1
- **DOI**: 10.1016/j.arcontrol.2020.09.002
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.09802v1)
- **Published**: 2020-03-22 04:09:59+00:00
- **Updated**: 2020-03-22 04:09:59+00:00
- **Authors**: Xinyu Liu, Xiren Miao, Hao Jiang, Jing Chen
- **Comment**: None
- **Journal**: None
- **Summary**: The widespread popularity of unmanned aerial vehicles enables an immense amount of power lines inspection data to be collected. How to employ massive inspection data especially the visible images to maintain the reliability, safety, and sustainability of power transmission is a pressing issue. To date, substantial works have been conducted on the analysis of power lines inspection data. With the aim of providing a comprehensive overview for researchers who are interested in developing a deep-learning-based analysis system for power lines inspection data, this paper conducts a thorough review of the current literature and identifies the challenges for future research. Following the typical procedure of inspection data analysis, we categorize current works in this area into component detection and fault diagnosis. For each aspect, the techniques and methodologies adopted in the literature are summarized. Some valuable information is also included such as data description and method performance. Further, an in-depth discussion of existing deep-learning-related analysis methods in power lines inspection is proposed. Finally, we conclude the paper with several research trends for the future of this area, such as data quality problems, small object detection, embedded application, and evaluation baseline.



### Multiview Neural Surface Reconstruction by Disentangling Geometry and Appearance
- **Arxiv ID**: http://arxiv.org/abs/2003.09852v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.09852v3)
- **Published**: 2020-03-22 10:20:13+00:00
- **Updated**: 2020-10-25 10:30:06+00:00
- **Authors**: Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Ronen Basri, Yaron Lipman
- **Comment**: None
- **Journal**: None
- **Summary**: In this work we address the challenging problem of multiview 3D surface reconstruction. We introduce a neural network architecture that simultaneously learns the unknown geometry, camera parameters, and a neural renderer that approximates the light reflected from the surface towards the camera. The geometry is represented as a zero level-set of a neural network, while the neural renderer, derived from the rendering equation, is capable of (implicitly) modeling a wide set of lighting conditions and materials. We trained our network on real world 2D images of objects with different material properties, lighting conditions, and noisy camera initializations from the DTU MVS dataset. We found our model to produce state of the art 3D surface reconstructions with high fidelity, resolution and detail.



### Visual Question Answering for Cultural Heritage
- **Arxiv ID**: http://arxiv.org/abs/2003.09853v1
- **DOI**: 10.1088/1757-899X/949/1/012074
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2003.09853v1)
- **Published**: 2020-03-22 10:26:08+00:00
- **Updated**: 2020-03-22 10:26:08+00:00
- **Authors**: Pietro Bongini, Federico Becattini, Andrew D. Bagdanov, Alberto Del Bimbo
- **Comment**: accepted at FlorenceHeritech 2020
- **Journal**: None
- **Summary**: Technology and the fruition of cultural heritage are becoming increasingly more entwined, especially with the advent of smart audio guides, virtual and augmented reality, and interactive installations. Machine learning and computer vision are important components of this ongoing integration, enabling new interaction modalities between user and museum. Nonetheless, the most frequent way of interacting with paintings and statues still remains taking pictures. Yet images alone can only convey the aesthetics of the artwork, lacking is information which is often required to fully understand and appreciate it. Usually this additional knowledge comes both from the artwork itself (and therefore the image depicting it) and from an external source of knowledge, such as an information sheet. While the former can be inferred by computer vision algorithms, the latter needs more structured data to pair visual content with relevant information. Regardless of its source, this information still must be be effectively transmitted to the user. A popular emerging trend in computer vision is Visual Question Answering (VQA), in which users can interact with a neural network by posing questions in natural language and receiving answers about the visual content. We believe that this will be the evolution of smart audio guides for museum visits and simple image browsing on personal smartphones. This will turn the classic audio guide into a smart personal instructor with which the visitor can interact by asking for explanations focused on specific interests. The advantages are twofold: on the one hand the cognitive burden of the visitor will decrease, limiting the flow of information to what the user actually wants to hear; and on the other hand it proposes the most natural way of interacting with a guide, favoring engagement.



### Pre-processing Image using Brightening, CLAHE and RETINEX
- **Arxiv ID**: http://arxiv.org/abs/2003.10822v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.10822v1)
- **Published**: 2020-03-22 10:35:24+00:00
- **Updated**: 2020-03-22 10:35:24+00:00
- **Authors**: Thi Phuoc Hanh Nguyen, Zinan Cai, Khanh Nguyen, Sokuntheariddh Keth, Ningyuan Shen, Mira Park
- **Comment**: None
- **Journal**: None
- **Summary**: This paper focuses on finding the most optimal pre-processing methods considering three common algorithms for image enhancement: Brightening, CLAHE and Retinex. For the purpose of image training in general, these methods will be combined to find out the most optimal method for image enhancement. We have carried out the research on the different permutation of three methods: Brightening, CLAHE and Retinex. The evaluation is based on Canny Edge detection applied to all processed images. Then the sharpness of objects will be justified by true positive pixels number in comparison between images. After using different number combinations pre-processing functions on images, CLAHE proves to be the most effective in edges improvement, Brightening does not show much effect on the edges enhancement, and the Retinex even reduces the sharpness of images and shows little contribution on images enhancement.



### TanhExp: A Smooth Activation Function with High Convergence Speed for Lightweight Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2003.09855v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2003.09855v2)
- **Published**: 2020-03-22 10:40:31+00:00
- **Updated**: 2020-09-09 13:43:34+00:00
- **Authors**: Xinyu Liu, Xiaoguang Di
- **Comment**: This paper is a preprint of a paper accepted by IET Computer Vision
  and is subject to Institution of Engineering and Technology Copyright. When
  the final version is published, the copy of record will be available at the
  IET Digital Library
- **Journal**: None
- **Summary**: Lightweight or mobile neural networks used for real-time computer vision tasks contain fewer parameters than normal networks, which lead to a constrained performance. In this work, we proposed a novel activation function named Tanh Exponential Activation Function (TanhExp) which can improve the performance for these networks on image classification task significantly. The definition of TanhExp is f(x) = xtanh(e^x). We demonstrate the simplicity, efficiency, and robustness of TanhExp on various datasets and network models and TanhExp outperforms its counterparts in both convergence speed and accuracy. Its behaviour also remains stable even with noise added and dataset altered. We show that without increasing the size of the network, the capacity of lightweight neural networks can be enhanced by TanhExp with only a few training epochs and no extra parameters added.



### Large-Scale Screening of COVID-19 from Community Acquired Pneumonia using Infection Size-Aware Classification
- **Arxiv ID**: http://arxiv.org/abs/2003.09860v1
- **DOI**: 10.1088/1361-6560/abe838
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.09860v1)
- **Published**: 2020-03-22 11:12:06+00:00
- **Updated**: 2020-03-22 11:12:06+00:00
- **Authors**: Feng Shi, Liming Xia, Fei Shan, Dijia Wu, Ying Wei, Huan Yuan, Huiting Jiang, Yaozong Gao, He Sui, Dinggang Shen
- **Comment**: None
- **Journal**: Physics in Medicine & Biology (2021)
- **Summary**: The worldwide spread of coronavirus disease (COVID-19) has become a threatening risk for global public health. It is of great importance to rapidly and accurately screen patients with COVID-19 from community acquired pneumonia (CAP). In this study, a total of 1658 patients with COVID-19 and 1027 patients of CAP underwent thin-section CT. All images were preprocessed to obtain the segmentations of both infections and lung fields, which were used to extract location-specific features. An infection Size Aware Random Forest method (iSARF) was proposed, in which subjects were automated categorized into groups with different ranges of infected lesion sizes, followed by random forests in each group for classification. Experimental results show that the proposed method yielded sensitivity of 0.907, specificity of 0.833, and accuracy of 0.879 under five-fold cross-validation. Large performance margins against comparison methods were achieved especially for the cases with infection size in the medium range, from 0.01% to 10%. The further inclusion of Radiomics features show slightly improvement. It is anticipated that our proposed framework could assist clinical decision making.



### Progressive Domain-Independent Feature Decomposition Network for Zero-Shot Sketch-Based Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2003.09869v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.09869v2)
- **Published**: 2020-03-22 12:07:23+00:00
- **Updated**: 2022-05-06 12:07:01+00:00
- **Authors**: Xinxun Xu, Muli Yang, Yanhua Yang, Hao Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Zero-shot sketch-based image retrieval (ZS-SBIR) is a specific cross-modal retrieval task for searching natural images given free-hand sketches under the zero-shot scenario. Most existing methods solve this problem by simultaneously projecting visual features and semantic supervision into a low-dimensional common space for efficient retrieval. However, such low-dimensional projection destroys the completeness of semantic knowledge in original semantic space, so that it is unable to transfer useful knowledge well when learning semantic from different modalities. Moreover, the domain information and semantic information are entangled in visual features, which is not conducive for cross-modal matching since it will hinder the reduction of domain gap between sketch and image. In this paper, we propose a Progressive Domain-independent Feature Decomposition (PDFD) network for ZS-SBIR. Specifically, with the supervision of original semantic knowledge, PDFD decomposes visual features into domain features and semantic ones, and then the semantic features are projected into common space as retrieval features for ZS-SBIR. The progressive projection strategy maintains strong semantic supervision. Besides, to guarantee the retrieval features to capture clean and complete semantic information, the cross-reconstruction loss is introduced to encourage that any combinations of retrieval features and domain features can reconstruct the visual features. Extensive experiments demonstrate the superiority of our PDFD over state-of-the-art competitors.



### COVID-Net: A Tailored Deep Convolutional Neural Network Design for Detection of COVID-19 Cases from Chest X-Ray Images
- **Arxiv ID**: http://arxiv.org/abs/2003.09871v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.09871v4)
- **Published**: 2020-03-22 12:26:36+00:00
- **Updated**: 2020-05-11 17:48:55+00:00
- **Authors**: Linda Wang, Alexander Wong
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: The COVID-19 pandemic continues to have a devastating effect on the health and well-being of the global population. A critical step in the fight against COVID-19 is effective screening of infected patients, with one of the key screening approaches being radiology examination using chest radiography. Motivated by this and inspired by the open source efforts of the research community, in this study we introduce COVID-Net, a deep convolutional neural network design tailored for the detection of COVID-19 cases from chest X-ray (CXR) images that is open source and available to the general public. To the best of the authors' knowledge, COVID-Net is one of the first open source network designs for COVID-19 detection from CXR images at the time of initial release. We also introduce COVIDx, an open access benchmark dataset that we generated comprising of 13,975 CXR images across 13,870 patient patient cases, with the largest number of publicly available COVID-19 positive cases to the best of the authors' knowledge. Furthermore, we investigate how COVID-Net makes predictions using an explainability method in an attempt to not only gain deeper insights into critical factors associated with COVID cases, which can aid clinicians in improved screening, but also audit COVID-Net in a responsible and transparent manner to validate that it is making decisions based on relevant information from the CXR images. By no means a production-ready solution, the hope is that the open access COVID-Net, along with the description on constructing the open source COVIDx dataset, will be leveraged and build upon by both researchers and citizen data scientists alike to accelerate the development of highly accurate yet practical deep learning solutions for detecting COVID-19 cases and accelerate treatment of those who need it the most.



### Ensembles of Deep Neural Networks for Action Recognition in Still Images
- **Arxiv ID**: http://arxiv.org/abs/2003.09893v1
- **DOI**: 10.1109/ICCKE48569.2019.8965014
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.09893v1)
- **Published**: 2020-03-22 13:44:09+00:00
- **Updated**: 2020-03-22 13:44:09+00:00
- **Authors**: Sina Mohammadi, Sina Ghofrani Majelan, Shahriar B. Shokouhi
- **Comment**: 5 pages, 2 figures, 3 tables, Accepted by ICCKE 2019
- **Journal**: 2019 9th International Conference on Computer and Knowledge
  Engineering (ICCKE), Mashhad, Iran, 2019, pp. 315-318
- **Summary**: Despite the fact that notable improvements have been made recently in the field of feature extraction and classification, human action recognition is still challenging, especially in images, in which, unlike videos, there is no motion. Thus, the methods proposed for recognizing human actions in videos cannot be applied to still images. A big challenge in action recognition in still images is the lack of large enough datasets, which is problematic for training deep Convolutional Neural Networks (CNNs) due to the overfitting issue. In this paper, by taking advantage of pre-trained CNNs, we employ the transfer learning technique to tackle the lack of massive labeled action recognition datasets. Furthermore, since the last layer of the CNN has class-specific information, we apply an attention mechanism on the output feature maps of the CNN to extract more discriminative and powerful features for classification of human actions. Moreover, we use eight different pre-trained CNNs in our framework and investigate their performance on Stanford 40 dataset. Finally, we propose using the Ensemble Learning technique to enhance the overall accuracy of action classification by combining the predictions of multiple models. The best setting of our method is able to achieve 93.17$\%$ accuracy on the Stanford 40 dataset.



### Curved Buildings Reconstruction from Airborne LiDAR Data by Matching and Deforming Geometric Primitives
- **Arxiv ID**: http://arxiv.org/abs/2003.09934v1
- **DOI**: 10.1109/TGRS.2020.2995732
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.09934v1)
- **Published**: 2020-03-22 16:05:10+00:00
- **Updated**: 2020-03-22 16:05:10+00:00
- **Authors**: Jingwei Song, Shaobo Xia, Jun Wang, Dong Chen
- **Comment**: 12 pages. 14 figures
- **Journal**: None
- **Summary**: Airborne LiDAR (Light Detection and Ranging) data is widely applied in building reconstruction, with studies reporting success in typical buildings. However, the reconstruction of curved buildings remains an open research problem. To this end, we propose a new framework for curved building reconstruction via assembling and deforming geometric primitives. The input LiDAR point cloud are first converted into contours where individual buildings are identified. After recognizing geometric units (primitives) from building contours, we get initial models by matching basic geometric primitives to these primitives. To polish assembly models, we employ a warping field for model refinements. Specifically, an embedded deformation (ED) graph is constructed via downsampling the initial model. Then, the point-to-model displacements are minimized by adjusting node parameters in the ED graph based on our objective function. The presented framework is validated on several highly curved buildings collected by various LiDAR in different cities. The experimental results, as well as accuracy comparison, demonstrate the advantage and effectiveness of our method. {The new insight attributes to an efficient reconstruction manner.} Moreover, we prove that the primitive-based framework significantly reduces the data storage to 10-20 percent of classical mesh models.



### Dynamic Reconstruction of Deformable Soft-tissue with Stereo Scope in Minimal Invasive Surgery
- **Arxiv ID**: http://arxiv.org/abs/2003.10867v1
- **DOI**: 10.1109/LRA.2017.2735487
- **Categories**: **cs.CV**, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.10867v1)
- **Published**: 2020-03-22 16:50:38+00:00
- **Updated**: 2020-03-22 16:50:38+00:00
- **Authors**: Jingwei Song, Jun Wang, Liang Zhao, Shoudong Huang, Gamini Dissanayake
- **Comment**: Published in IROS2017 ()2017 IEEE/RSJ International Conference on
  Intelligent Robots and Systems. arXiv admin note: text overlap with
  arXiv:1803.02009
- **Journal**: None
- **Summary**: In minimal invasive surgery, it is important to rebuild and visualize the latest deformed shape of soft-tissue surfaces to mitigate tissue damages. This paper proposes an innovative Simultaneous Localization and Mapping (SLAM) algorithm for deformable dense reconstruction of surfaces using a sequence of images from a stereoscope. We introduce a warping field based on the Embedded Deformation (ED) nodes with 3D shapes recovered from consecutive pairs of stereo images. The warping field is estimated by deforming the last updated model to the current live model. Our SLAM system can: (1) Incrementally build a live model by progressively fusing new observations with vivid accurate texture. (2) Estimate the deformed shape of unobserved region with the principle As-Rigid-As-Possible. (3) Show the consecutive shape of models. (4) Estimate the current relative pose between the soft-tissue and the scope. In-vivo experiments with publicly available datasets demonstrate that the 3D models can be incrementally built for different soft-tissues with different deformations from sequences of stereo images obtained by laparoscopes. Results show the potential clinical application of our SLAM system for providing surgeon useful shape and texture information in minimal invasive surgery.



### The Instantaneous Accuracy: a Novel Metric for the Problem of Online Human Behaviour Recognition in Untrimmed Videos
- **Arxiv ID**: http://arxiv.org/abs/2003.09970v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.09970v2)
- **Published**: 2020-03-22 19:04:05+00:00
- **Updated**: 2020-03-25 10:06:37+00:00
- **Authors**: Marcos Baptista Rios, Roberto J. López-Sastre, Fabian Caba Heilbron, Jan van Gemert, Francisco Javier Acevedo-Rodríguez, Saturnino Maldonado-Bascón
- **Comment**: Published at ICCV 2019 workshop: Human Behaviour Understanding
- **Journal**: None
- **Summary**: The problem of Online Human Behaviour Recognition in untrimmed videos, aka Online Action Detection (OAD), needs to be revisited. Unlike traditional offline action detection approaches, where the evaluation metrics are clear and well established, in the OAD setting we find few works and no consensus on the evaluation protocols to be used. In this paper we introduce a novel online metric, the Instantaneous Accuracy ($IA$), that exhibits an \emph{online} nature, solving most of the limitations of the previous (offline) metrics. We conduct a thorough experimental evaluation on TVSeries dataset, comparing the performance of various baseline methods to the state of the art. Our results confirm the problems of previous evaluation protocols, and suggest that an IA-based protocol is more adequate to the online scenario for human behaviour understanding. Code of the metric available https://github.com/gramuah/ia



### A Better Variant of Self-Critical Sequence Training
- **Arxiv ID**: http://arxiv.org/abs/2003.09971v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2003.09971v2)
- **Published**: 2020-03-22 19:04:25+00:00
- **Updated**: 2020-05-10 21:59:36+00:00
- **Authors**: Ruotian Luo
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we present a simple yet better variant of Self-Critical Sequence Training. We make a simple change in the choice of baseline function in REINFORCE algorithm. The new baseline can bring better performance with no extra cost, compared to the greedy decoding baseline.



### Estimating Uncertainty and Interpretability in Deep Learning for Coronavirus (COVID-19) Detection
- **Arxiv ID**: http://arxiv.org/abs/2003.10769v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.10769v2)
- **Published**: 2020-03-22 21:58:13+00:00
- **Updated**: 2020-03-27 16:48:13+00:00
- **Authors**: Biraja Ghoshal, Allan Tucker
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Learning has achieved state of the art performance in medical imaging. However, these methods for disease detection focus exclusively on improving the accuracy of classification or predictions without quantifying uncertainty in a decision. Knowing how much confidence there is in a computer-based medical diagnosis is essential for gaining clinicians trust in the technology and therefore improve treatment. Today, the 2019 Coronavirus (SARS-CoV-2) infections are a major healthcare challenge around the world. Detecting COVID-19 in X-ray images is crucial for diagnosis, assessment and treatment. However, diagnostic uncertainty in the report is a challenging and yet inevitable task for radiologist. In this paper, we investigate how drop-weights based Bayesian Convolutional Neural Networks (BCNN) can estimate uncertainty in Deep Learning solution to improve the diagnostic performance of the human-machine team using publicly available COVID-19 chest X-ray dataset and show that the uncertainty in prediction is highly correlates with accuracy of prediction. We believe that the availability of uncertainty-aware deep learning solution will enable a wider adoption of Artificial Intelligence (AI) in a clinical setting.



### DAISI: Database for AI Surgical Instruction
- **Arxiv ID**: http://arxiv.org/abs/2004.02809v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.02809v1)
- **Published**: 2020-03-22 22:07:43+00:00
- **Updated**: 2020-03-22 22:07:43+00:00
- **Authors**: Edgar Rojas-Muñoz, Kyle Couperus, Juan Wachs
- **Comment**: 10 pages, 4 figures, to access database, see
  https://engineering.purdue.edu/starproj/_daisi
- **Journal**: None
- **Summary**: Telementoring surgeons as they perform surgery can be essential in the treatment of patients when in situ expertise is not available. Nonetheless, expert mentors are often unavailable to provide trainees with real-time medical guidance. When mentors are unavailable, a fallback autonomous mechanism should provide medical practitioners with the required guidance. However, AI/autonomous mentoring in medicine has been limited by the availability of generalizable prediction models, and surgical procedures datasets to train those models with. This work presents the initial steps towards the development of an intelligent artificial system for autonomous medical mentoring. Specifically, we present the first Database for AI Surgical Instruction (DAISI). DAISI leverages on images and instructions to provide step-by-step demonstrations of how to perform procedures from various medical disciplines. The dataset was acquired from real surgical procedures and data from academic textbooks. We used DAISI to train an encoder-decoder neural network capable of predicting medical instructions given a current view of the surgery. Afterwards, the instructions predicted by the network were evaluated using cumulative BLEU scores and input from expert physicians. According to the BLEU scores, the predicted and ground truth instructions were as high as 67% similar. Additionally, expert physicians subjectively assessed the algorithm using Likert scale, and considered that the predicted descriptions were related to the images. This work provides a baseline for AI algorithms to assist in autonomous medical mentoring.



### One-Shot Informed Robotic Visual Search in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2003.10010v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.10010v2)
- **Published**: 2020-03-22 22:14:42+00:00
- **Updated**: 2020-09-03 07:14:23+00:00
- **Authors**: Karim Koreitem, Florian Shkurti, Travis Manderson, Wei-Di Chang, Juan Camilo Gamboa Higuera, Gregory Dudek
- **Comment**: Accepted at IROS 2020. Code
  https://github.com/rvl-lab-utoronto/visual_search_in_the_wild and videos
  https://www.youtube.com/watch?v=0i_el5XGCus
- **Journal**: None
- **Summary**: We consider the task of underwater robot navigation for the purpose of collecting scientifically relevant video data for environmental monitoring. The majority of field robots that currently perform monitoring tasks in unstructured natural environments navigate via path-tracking a pre-specified sequence of waypoints. Although this navigation method is often necessary, it is limiting because the robot does not have a model of what the scientist deems to be relevant visual observations. Thus, the robot can neither visually search for particular types of objects, nor focus its attention on parts of the scene that might be more relevant than the pre-specified waypoints and viewpoints. In this paper we propose a method that enables informed visual navigation via a learned visual similarity operator that guides the robot's visual search towards parts of the scene that look like an exemplar image, which is given by the user as a high-level specification for data collection. We propose and evaluate a weakly supervised video representation learning method that outperforms ImageNet embeddings for similarity tasks in the underwater domain. We also demonstrate the deployment of this similarity operator during informed visual navigation in collaborative environmental monitoring scenarios, in large-scale field trials, where the robot and a human scientist collaboratively search for relevant visual content.



### Self-Supervised 2D Image to 3D Shape Translation with Disentangled Representations
- **Arxiv ID**: http://arxiv.org/abs/2003.10016v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.10016v2)
- **Published**: 2020-03-22 22:44:02+00:00
- **Updated**: 2021-01-29 22:55:47+00:00
- **Authors**: Berk Kaya, Radu Timofte
- **Comment**: Published in 2020 International Conference on 3D Vision (3DV)
- **Journal**: None
- **Summary**: We present a framework to translate between 2D image views and 3D object shapes. Recent progress in deep learning enabled us to learn structure-aware representations from a scene. However, the existing literature assumes that pairs of images and 3D shapes are available for training in full supervision. In this paper, we propose SIST, a Self-supervised Image to Shape Translation framework that fulfills three tasks: (i) reconstructing the 3D shape from a single image; (ii) learning disentangled representations for shape, appearance and viewpoint; and (iii) generating a realistic RGB image from these independent factors. In contrast to the existing approaches, our method does not require image-shape pairs for training. Instead, it uses unpaired image and shape datasets from the same object class and jointly trains image generator and shape reconstruction networks. Our translation method achieves promising results, comparable in quantitative and qualitative terms to the state-of-the-art achieved by fully-supervised methods.



### Dynamic ReLU
- **Arxiv ID**: http://arxiv.org/abs/2003.10027v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.10027v2)
- **Published**: 2020-03-22 23:45:35+00:00
- **Updated**: 2020-08-05 17:46:33+00:00
- **Authors**: Yinpeng Chen, Xiyang Dai, Mengchen Liu, Dongdong Chen, Lu Yuan, Zicheng Liu
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: Rectified linear units (ReLU) are commonly used in deep neural networks. So far ReLU and its generalizations (non-parametric or parametric) are static, performing identically for all input samples. In this paper, we propose dynamic ReLU (DY-ReLU), a dynamic rectifier of which parameters are generated by a hyper function over all in-put elements. The key insight is that DY-ReLU encodes the global context into the hyper function, and adapts the piecewise linear activation function accordingly. Compared to its static counterpart, DY-ReLU has negligible extra computational cost, but significantly more representation capability, especially for light-weight neural networks. By simply using DY-ReLU for MobileNetV2, the top-1 accuracy on ImageNet classification is boosted from 72.0% to 76.2% with only 5% additional FLOPs.



