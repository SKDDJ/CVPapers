# Arxiv Papers in cs.CV on 2020-03-10
### Tracking Road Users using Constraint Programming
- **Arxiv ID**: http://arxiv.org/abs/2003.04468v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.04468v1)
- **Published**: 2020-03-10 00:04:32+00:00
- **Updated**: 2020-03-10 00:04:32+00:00
- **Authors**: Alexandre Pineault, Guillaume-Alexandre Bilodeau, Gilles Pesant
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we aim at improving the tracking of road users in urban scenes. We present a constraint programming (CP) approach for the data association phase found in the tracking-by-detection paradigm of the multiple object tracking (MOT) problem. Such an approach can solve the data association problem more efficiently than graph-based methods and can handle better the combinatorial explosion occurring when multiple frames are analyzed. Because our focus is on the data association problem, our MOT method only uses simple image features, which are the center position and color of detections for each frame. Constraints are defined on these two features and on the general MOT problem. For example, we enforce color appearance preservation over trajectories and constrain the extent of motion between frames. Filtering layers are used in order to eliminate detection candidates before using CP and to remove dummy trajectories produced by the CP solver. Our proposed method was tested on a motorized vehicles tracking dataset and produces results that outperform the top methods of the UA-DETRAC benchmark.



### Deep learning approach for breast cancer diagnosis
- **Arxiv ID**: http://arxiv.org/abs/2003.04480v1
- **DOI**: 10.1145/3328833.3328867
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.04480v1)
- **Published**: 2020-03-10 00:47:37+00:00
- **Updated**: 2020-03-10 00:47:37+00:00
- **Authors**: Essam A. Rashed, M. Samir Abou El Seoud
- **Comment**: None
- **Journal**: ICSIE '19: Proceedings of the 2019 8th International Conference on
  Software and Information Engineering
- **Summary**: Breast cancer is one of the leading fatal disease worldwide with high risk control if early discovered. Conventional method for breast screening is x-ray mammography, which is known to be challenging for early detection of cancer lesions. The dense breast structure produced due to the compression process during imaging lead to difficulties to recognize small size abnormalities. Also, inter- and intra-variations of breast tissues lead to significant difficulties to achieve high diagnosis accuracy using hand-crafted features. Deep learning is an emerging machine learning technology that requires a relatively high computation power. Yet, it proved to be very effective in several difficult tasks that requires decision making at the level of human intelligence. In this paper, we develop a new network architecture inspired by the U-net structure that can be used for effective and early detection of breast cancer. Results indicate a high rate of sensitivity and specificity that indicate potential usefulness of the proposed approach in clinical use.



### Compositional Convolutional Neural Networks: A Deep Architecture with Innate Robustness to Partial Occlusion
- **Arxiv ID**: http://arxiv.org/abs/2003.04490v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.04490v3)
- **Published**: 2020-03-10 01:45:38+00:00
- **Updated**: 2020-04-17 07:23:05+00:00
- **Authors**: Adam Kortylewski, Ju He, Qing Liu, Alan Yuille
- **Comment**: CVPR 2020; Code is available
  https://github.com/AdamKortylewski/CompositionalNets; Supplementary material:
  https://adamkortylewski.com/data/compnet_supp.pdf
- **Journal**: None
- **Summary**: Recent findings show that deep convolutional neural networks (DCNNs) do not generalize well under partial occlusion. Inspired by the success of compositional models at classifying partially occluded objects, we propose to integrate compositional models and DCNNs into a unified deep model with innate robustness to partial occlusion. We term this architecture Compositional Convolutional Neural Network. In particular, we propose to replace the fully connected classification head of a DCNN with a differentiable compositional model. The generative nature of the compositional model enables it to localize occluders and subsequently focus on the non-occluded parts of the object. We conduct classification experiments on artificially occluded images as well as real images of partially occluded objects from the MS-COCO dataset. The results show that DCNNs do not classify occluded objects robustly, even when trained with data that is strongly augmented with partial occlusions. Our proposed model outperforms standard DCNNs by a large margin at classifying partially occluded objects, even when it has not been exposed to occluded objects during training. Additional experiments demonstrate that CompositionalNets can also localize the occluders accurately, despite being trained with class labels only. The code used in this work is publicly available.



### FOAL: Fast Online Adaptive Learning for Cardiac Motion Estimation
- **Arxiv ID**: http://arxiv.org/abs/2003.04492v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.04492v2)
- **Published**: 2020-03-10 01:51:27+00:00
- **Updated**: 2020-08-14 18:02:53+00:00
- **Authors**: Hanchao Yu, Shanhui Sun, Haichao Yu, Xiao Chen, Honghui Shi, Thomas Huang, Terrence Chen
- **Comment**: Accepted by CVPR 2020
- **Journal**: None
- **Summary**: Motion estimation of cardiac MRI videos is crucial for the evaluation of human heart anatomy and function. Recent researches show promising results with deep learning-based methods. In clinical deployment, however, they suffer dramatic performance drops due to mismatched distributions between training and testing datasets, commonly encountered in the clinical environment. On the other hand, it is arguably impossible to collect all representative datasets and to train a universal tracker before deployment. In this context, we proposed a novel fast online adaptive learning (FOAL) framework: an online gradient descent based optimizer that is optimized by a meta-learner. The meta-learner enables the online optimizer to perform a fast and robust adaptation. We evaluated our method through extensive experiments on two public clinical datasets. The results showed the superior performance of FOAL in accuracy compared to the offline-trained tracking method. On average, the FOAL took only $0.4$ second per video for online optimization.



### Diversity inducing Information Bottleneck in Model Ensembles
- **Arxiv ID**: http://arxiv.org/abs/2003.04514v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.04514v3)
- **Published**: 2020-03-10 03:10:41+00:00
- **Updated**: 2020-12-08 20:14:08+00:00
- **Authors**: Samarth Sinha, Homanga Bharadhwaj, Anirudh Goyal, Hugo Larochelle, Animesh Garg, Florian Shkurti
- **Comment**: AAAI 2021. Samarth Sinha* and Homanga Bharadhwaj* contributed equally
  to this work
- **Journal**: None
- **Summary**: Although deep learning models have achieved state-of-the-art performance on a number of vision tasks, generalization over high dimensional multi-modal data, and reliable predictive uncertainty estimation are still active areas of research. Bayesian approaches including Bayesian Neural Nets (BNNs) do not scale well to modern computer vision tasks, as they are difficult to train, and have poor generalization under dataset-shift. This motivates the need for effective ensembles which can generalize and give reliable uncertainty estimates. In this paper, we target the problem of generating effective ensembles of neural networks by encouraging diversity in prediction. We explicitly optimize a diversity inducing adversarial loss for learning the stochastic latent variables and thereby obtain diversity in the output predictions necessary for modeling multi-modal data. We evaluate our method on benchmark datasets: MNIST, CIFAR100, TinyImageNet and MIT Places 2, and compared to the most competitive baselines show significant improvements in classification accuracy, under a shift in the data distribution and in out-of-distribution detection. Code will be released in this url https://github.com/rvl-lab-utoronto/dibs



### HEMlets PoSh: Learning Part-Centric Heatmap Triplets for 3D Human Pose and Shape Estimation
- **Arxiv ID**: http://arxiv.org/abs/2003.04894v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.04894v3)
- **Published**: 2020-03-10 04:03:45+00:00
- **Updated**: 2021-01-12 07:01:23+00:00
- **Authors**: Kun Zhou, Xiaoguang Han, Nianjuan Jiang, Kui Jia, Jiangbo Lu
- **Comment**: 14 pages, 14figures, To appear in TPAMI2021. arXiv admin note:
  substantial text overlap with arXiv:1910.12032
- **Journal**: None
- **Summary**: Estimating 3D human pose from a single image is a challenging task. This work attempts to address the uncertainty of lifting the detected 2D joints to the 3D space by introducing an intermediate state-Part-Centric Heatmap Triplets (HEMlets), which shortens the gap between the 2D observation and the 3D interpretation. The HEMlets utilize three joint-heatmaps to represent the relative depth information of the end-joints for each skeletal body part. In our approach, a Convolutional Network (ConvNet) is first trained to predict HEMlets from the input image, followed by a volumetric joint-heatmap regression. We leverage on the integral operation to extract the joint locations from the volumetric heatmaps, guaranteeing end-to-end learning. Despite the simplicity of the network design, the quantitative comparisons show a significant performance improvement over the best-of-grade methods (e.g. $20\%$ on Human3.6M). The proposed method naturally supports training with "in-the-wild" images, where only weakly-annotated relative depth information of skeletal joints is available. This further improves the generalization ability of our model, as validated by qualitative comparisons on outdoor images. Leveraging the strength of the HEMlets pose estimation, we further design and append a shallow yet effective network module to regress the SMPL parameters of the body pose and shape. We term the entire HEMlets-based human pose and shape recovery pipeline HEMlets PoSh. Extensive quantitative and qualitative experiments on the existing human body recovery benchmarks justify the state-of-the-art results obtained with our HEMlets PoSh approach.



### PBRnet: Pyramidal Bounding Box Refinement to Improve Object Localization Accuracy
- **Arxiv ID**: http://arxiv.org/abs/2003.04541v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.04541v1)
- **Published**: 2020-03-10 05:27:09+00:00
- **Updated**: 2020-03-10 05:27:09+00:00
- **Authors**: Li Xiao, Yufan Luo, Chunlong Luo, Lianhe Zhao, Quanshui Fu, Guoqing Yang, Anpeng Huang, Yi Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Many recently developed object detectors focused on coarse-to-fine framework which contains several stages that classify and regress proposals from coarse-grain to fine-grain, and obtains more accurate detection gradually. Multi-resolution models such as Feature Pyramid Network(FPN) integrate information of different levels of resolution and effectively improve the performance. Previous researches also have revealed that localization can be further improved by: 1) using fine-grained information which is more translational variant; 2) refining local areas which is more focused on local boundary information. Based on these principles, we designed a novel boundary refinement architecture to improve localization accuracy by combining coarse-to-fine framework with feature pyramid structure, named as Pyramidal Bounding Box Refinement network(PBRnet), which parameterizes gradually focused boundary areas of objects and leverages lower-level feature maps to extract finer local information when refining the predicted bounding boxes. Extensive experiments are performed on the MS-COCO dataset. The PBRnet brings a significant performance gains by roughly 3 point of $mAP$ when added to FPN or Libra R-CNN. Moreover, by treating Cascade R-CNN as a coarse-to-fine detector and replacing its localization branch by the regressor of PBRnet, it leads an extra performance improvement by 1.5 $mAP$, yielding a total performance boosting by as high as 5 point of $mAP$.



### 3D Quasi-Recurrent Neural Network for Hyperspectral Image Denoising
- **Arxiv ID**: http://arxiv.org/abs/2003.04547v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.04547v1)
- **Published**: 2020-03-10 06:14:53+00:00
- **Updated**: 2020-03-10 06:14:53+00:00
- **Authors**: Kaixuan Wei, Ying Fu, Hua Huang
- **Comment**: Accepted by IEEE Transactions on Neural Network and Learning System
  (TNNLS), 2020
- **Journal**: None
- **Summary**: In this paper, we propose an alternating directional 3D quasi-recurrent neural network for hyperspectral image (HSI) denoising, which can effectively embed the domain knowledge -- structural spatio-spectral correlation and global correlation along spectrum. Specifically, 3D convolution is utilized to extract structural spatio-spectral correlation in an HSI, while a quasi-recurrent pooling function is employed to capture the global correlation along spectrum. Moreover, alternating directional structure is introduced to eliminate the causal dependency with no additional computation cost. The proposed model is capable of modeling spatio-spectral dependency while preserving the flexibility towards HSIs with arbitrary number of bands. Extensive experiments on HSI denoising demonstrate significant improvement over state-of-the-arts under various noise settings, in terms of both restoration accuracy and computation time. Our code is available at https://github.com/Vandermode/QRNN3D.



### Channel Pruning via Optimal Thresholding
- **Arxiv ID**: http://arxiv.org/abs/2003.04566v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.04566v5)
- **Published**: 2020-03-10 08:24:24+00:00
- **Updated**: 2020-09-10 05:42:37+00:00
- **Authors**: Yun Ye, Ganmei You, Jong-Kae Fwu, Xia Zhu, Qing Yang, Yuan Zhu
- **Comment**: ICONIP 2020
- **Journal**: None
- **Summary**: Structured pruning, especially channel pruning is widely used for the reduced computational cost and the compatibility with off-the-shelf hardware devices. Among existing works, weights are typically removed using a predefined global threshold, or a threshold computed from a predefined metric. The predefined global threshold based designs ignore the variation among different layers and weights distribution, therefore, they may often result in sub-optimal performance caused by over-pruning or under-pruning. In this paper, we present a simple yet effective method, termed Optimal Thresholding (OT), to prune channels with layer dependent thresholds that optimally separate important from negligible channels. By using OT, most negligible or unimportant channels are pruned to achieve high sparsity while minimizing performance degradation. Since most important weights are preserved, the pruned model can be further fine-tuned and quickly converge with very few iterations. Our method demonstrates superior performance, especially when compared to the state-of-the-art designs at high levels of sparsity. On CIFAR-100, a pruned and fine-tuned DenseNet-121 by using OT achieves 75.99% accuracy with only 1.46e8 FLOPs and 0.71M parameters.



### DymSLAM:4D Dynamic Scene Reconstruction Based on Geometrical Motion Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2003.04569v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2003.04569v1)
- **Published**: 2020-03-10 08:25:21+00:00
- **Updated**: 2020-03-10 08:25:21+00:00
- **Authors**: Chenjie Wang, Bin Luo, Yun Zhang, Qing Zhao, Lu Yin, Wei Wang, Xin Su, Yajun Wang, Chengyuan Li
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: Most SLAM algorithms are based on the assumption that the scene is static. However, in practice, most scenes are dynamic which usually contains moving objects, these methods are not suitable. In this paper, we introduce DymSLAM, a dynamic stereo visual SLAM system being capable of reconstructing a 4D (3D + time) dynamic scene with rigid moving objects. The only input of DymSLAM is stereo video, and its output includes a dense map of the static environment, 3D model of the moving objects and the trajectories of the camera and the moving objects. We at first detect and match the interesting points between successive frames by using traditional SLAM methods. Then the interesting points belonging to different motion models (including ego-motion and motion models of rigid moving objects) are segmented by a multi-model fitting approach. Based on the interesting points belonging to the ego-motion, we are able to estimate the trajectory of the camera and reconstruct the static background. The interesting points belonging to the motion models of rigid moving objects are then used to estimate their relative motion models to the camera and reconstruct the 3D models of the objects. We then transform the relative motion to the trajectories of the moving objects in the global reference frame. Finally, we then fuse the 3D models of the moving objects into the 3D map of the environment by considering their motion trajectories to obtain a 4D (3D+time) sequence. DymSLAM obtains information about the dynamic objects instead of ignoring them and is suitable for unknown rigid objects. Hence, the proposed system allows the robot to be employed for high-level tasks, such as obstacle avoidance for dynamic objects. We conducted experiments in a real-world environment where both the camera and the objects were moving in a wide range.



### TailorNet: Predicting Clothing in 3D as a Function of Human Pose, Shape and Garment Style
- **Arxiv ID**: http://arxiv.org/abs/2003.04583v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2003.04583v2)
- **Published**: 2020-03-10 08:49:51+00:00
- **Updated**: 2020-03-15 16:35:56+00:00
- **Authors**: Chaitanya Patel, Zhouyingcheng Liao, Gerard Pons-Moll
- **Comment**: Accepted to CVPR 2020. Chaitanya Patel and Zhouyingcheng Liao
  contributed equally
- **Journal**: None
- **Summary**: In this paper, we present TailorNet, a neural model which predicts clothing deformation in 3D as a function of three factors: pose, shape and style (garment geometry), while retaining wrinkle detail. This goes beyond prior models, which are either specific to one style and shape, or generalize to different shapes producing smooth results, despite being style specific. Our hypothesis is that (even non-linear) combinations of examples smooth out high frequency components such as fine-wrinkles, which makes learning the three factors jointly hard. At the heart of our technique is a decomposition of deformation into a high frequency and a low frequency component. While the low-frequency component is predicted from pose, shape and style parameters with an MLP, the high-frequency component is predicted with a mixture of shape-style specific pose models. The weights of the mixture are computed with a narrow bandwidth kernel to guarantee that only predictions with similar high-frequency patterns are combined. The style variation is obtained by computing, in a canonical pose, a subspace of deformation, which satisfies physical constraints such as inter-penetration, and draping on the body. TailorNet delivers 3D garments which retain the wrinkles from the physics based simulations (PBS) it is learned from, while running more than 1000 times faster. In contrast to PBS, TailorNet is easy to use and fully differentiable, which is crucial for computer vision algorithms. Several experiments demonstrate TailorNet produces more realistic results than prior work, and even generates temporally coherent deformations on sequences of the AMASS dataset, despite being trained on static poses from a different dataset. To stimulate further research in this direction, we will make a dataset consisting of 55800 frames, as well as our model publicly available at https://virtualhumans.mpi-inf.mpg.de/tailornet.



### Ambiguity in Sequential Data: Predicting Uncertain Futures with Recurrent Models
- **Arxiv ID**: http://arxiv.org/abs/2003.10381v1
- **DOI**: 10.1109/LRA.2020.2974716
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.10381v1)
- **Published**: 2020-03-10 09:15:42+00:00
- **Updated**: 2020-03-10 09:15:42+00:00
- **Authors**: Alessandro Berlati, Oliver Scheel, Luigi Di Stefano, Federico Tombari
- **Comment**: None
- **Journal**: Robotics and Automation Letters 2020 (RA-L)
- **Summary**: Ambiguity is inherently present in many machine learning tasks, but especially for sequential models seldom accounted for, as most only output a single prediction. In this work we propose an extension of the Multiple Hypothesis Prediction (MHP) model to handle ambiguous predictions with sequential data, which is of special importance, as often multiple futures are equally likely. Our approach can be applied to the most common recurrent architectures and can be used with any loss function. Additionally, we introduce a novel metric for ambiguous problems, which is better suited to account for uncertainties and coincides with our intuitive understanding of correctness in the presence of multiple labels. We test our method on several experiments and across diverse tasks dealing with time series data, such as trajectory forecasting and maneuver prediction, achieving promising results.



### Label-Driven Reconstruction for Domain Adaptation in Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2003.04614v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.04614v3)
- **Published**: 2020-03-10 10:06:35+00:00
- **Updated**: 2020-08-23 16:23:23+00:00
- **Authors**: Jinyu Yang, Weizhi An, Sheng Wang, Xinliang Zhu, Chaochao Yan, Junzhou Huang
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: Unsupervised domain adaptation enables to alleviate the need for pixel-wise annotation in the semantic segmentation. One of the most common strategies is to translate images from the source domain to the target domain and then align their marginal distributions in the feature space using adversarial learning. However, source-to-target translation enlarges the bias in translated images and introduces extra computations, owing to the dominant data size of the source domain. Furthermore, consistency of the joint distribution in source and target domains cannot be guaranteed through global feature alignment. Here, we present an innovative framework, designed to mitigate the image translation bias and align cross-domain features with the same category. This is achieved by 1) performing the target-to-source translation and 2) reconstructing both source and target images from their predicted labels. Extensive experiments on adapting from synthetic to real urban scene understanding demonstrate that our framework competes favorably against existing state-of-the-art methods.



### Convolutional Occupancy Networks
- **Arxiv ID**: http://arxiv.org/abs/2003.04618v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.04618v2)
- **Published**: 2020-03-10 10:17:07+00:00
- **Updated**: 2020-08-01 20:38:29+00:00
- **Authors**: Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc Pollefeys, Andreas Geiger
- **Comment**: ECCV 2020 (Spotlight). Project page with supplementary material and
  code: https://pengsongyou.github.io/conv_onet
- **Journal**: None
- **Summary**: Recently, implicit neural representations have gained popularity for learning-based 3D reconstruction. While demonstrating promising results, most implicit approaches are limited to comparably simple geometry of single objects and do not scale to more complicated or large-scale scenes. The key limiting factor of implicit methods is their simple fully-connected network architecture which does not allow for integrating local information in the observations or incorporating inductive biases such as translational equivariance. In this paper, we propose Convolutional Occupancy Networks, a more flexible implicit representation for detailed reconstruction of objects and 3D scenes. By combining convolutional encoders with implicit occupancy decoders, our model incorporates inductive biases, enabling structured reasoning in 3D space. We investigate the effectiveness of the proposed representation by reconstructing complex geometry from noisy point clouds and low-resolution voxel representations. We empirically find that our method enables the fine-grained implicit 3D reconstruction of single objects, scales to large indoor scenes, and generalizes well from synthetic to real data.



### Hierarchical Neural Architecture Search for Single Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2003.04619v3
- **DOI**: 10.1109/LSP.2020.3003517
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.04619v3)
- **Published**: 2020-03-10 10:19:44+00:00
- **Updated**: 2020-06-16 12:43:24+00:00
- **Authors**: Yong Guo, Yongsheng Luo, Zhenhao He, Jin Huang, Jian Chen
- **Comment**: This paper is accepted by IEEE Signal Processing Letters
- **Journal**: None
- **Summary**: Deep neural networks have exhibited promising performance in image super-resolution (SR). Most SR models follow a hierarchical architecture that contains both the cell-level design of computational blocks and the network-level design of the positions of upsampling blocks. However, designing SR models heavily relies on human expertise and is very labor-intensive. More critically, these SR models often contain a huge number of parameters and may not meet the requirements of computation resources in real-world applications. To address the above issues, we propose a Hierarchical Neural Architecture Search (HNAS) method to automatically design promising architectures with different requirements of computation cost. To this end, we design a hierarchical SR search space and propose a hierarchical controller for architecture search. Such a hierarchical controller is able to simultaneously find promising cell-level blocks and network-level positions of upsampling layers. Moreover, to design compact architectures with promising performance, we build a joint reward by considering both the performance and computation cost to guide the search process. Extensive experiments on five benchmark datasets demonstrate the superiority of our method over existing methods.



### A Compact Spectral Descriptor for Shape Deformations
- **Arxiv ID**: http://arxiv.org/abs/2003.08758v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.08758v1)
- **Published**: 2020-03-10 10:34:30+00:00
- **Updated**: 2020-03-10 10:34:30+00:00
- **Authors**: Skylar Sible, Rodrigo Iza-Teran, Jochen Garcke, Nikola Aulig, Patricia Wollstadt
- **Comment**: To be published in Proc. of the European Conference on Artificial
  Intelligence 2020, 12 pages, 5 figures
- **Journal**: None
- **Summary**: Modern product design in the engineering domain is increasingly driven by computational analysis including finite-element based simulation, computational optimization, and modern data analysis techniques such as machine learning. To apply these methods, suitable data representations for components under development as well as for related design criteria have to be found. While a component's geometry is typically represented by a polygon surface mesh, it is often not clear how to parametrize critical design properties in order to enable efficient computational analysis. In the present work, we propose a novel methodology to obtain a parameterization of a component's plastic deformation behavior under stress, which is an important design criterion in many application domains, for example, when optimizing the crash behavior in the automotive context. Existing parameterizations limit computational analysis to relatively simple deformations and typically require extensive input by an expert, making the design process time intensive and costly. Hence, we propose a way to derive a compact descriptor of deformation behavior that is based on spectral mesh processing and enables a low-dimensional representation of also complex deformations.We demonstrate the descriptor's ability to represent relevant deformation behavior by applying it in a nearest-neighbor search to identify similar simulation results in a filtering task. The proposed descriptor provides a novel approach to the parametrization of geometric deformation behavior and enables the use of state-of-the-art data analysis techniques such as machine learning to engineering tasks concerned with plastic deformation behavior.



### PnP-Net: A hybrid Perspective-n-Point Network
- **Arxiv ID**: http://arxiv.org/abs/2003.04626v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.04626v1)
- **Published**: 2020-03-10 10:43:14+00:00
- **Updated**: 2020-03-10 10:43:14+00:00
- **Authors**: Roy Sheffer, Ami Wiesel
- **Comment**: None
- **Journal**: None
- **Summary**: We consider the robust Perspective-n-Point (PnP) problem using a hybrid approach that combines deep learning with model based algorithms. PnP is the problem of estimating the pose of a calibrated camera given a set of 3D points in the world and their corresponding 2D projections in the image. In its more challenging robust version, some of the correspondences may be mismatched and must be efficiently discarded. Classical solutions address PnP via iterative robust non-linear least squares method that exploit the problem's geometry but are either inaccurate or computationally intensive. In contrast, we propose to combine a deep learning initial phase followed by a model-based fine tuning phase. This hybrid approach, denoted by PnP-Net, succeeds in estimating the unknown pose parameters under correspondence errors and noise, with low and fixed computational complexity requirements. We demonstrate its advantages on both synthetic data and real world data.



### MQA: Answering the Question via Robotic Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2003.04641v4
- **DOI**: 10.15607/rss.2021.xvii.044
- **Categories**: **cs.AI**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2003.04641v4)
- **Published**: 2020-03-10 11:30:09+00:00
- **Updated**: 2023-02-21 05:20:30+00:00
- **Authors**: Yuhong Deng, Di Guo, Xiaofeng Guo, Naifu Zhang, Huaping Liu, Fuchun Sun
- **Comment**: has been accepted by Robotics: Science and Systems 2021
- **Journal**: Robotics: Science and System (RSS2021)
- **Summary**: In this paper, we propose a novel task, Manipulation Question Answering (MQA), where the robot performs manipulation actions to change the environment in order to answer a given question. To solve this problem, a framework consisting of a QA module and a manipulation module is proposed. For the QA module, we adopt the method for the Visual Question Answering (VQA) task. For the manipulation module, a Deep Q Network (DQN) model is designed to generate manipulation actions for the robot to interact with the environment. We consider the situation where the robot continuously manipulating objects inside a bin until the answer to the question is found. Besides, a novel dataset that contains a variety of object models, scenarios and corresponding question-answer pairs is established in a simulation environment. Extensive experiments have been conducted to validate the effectiveness of the proposed framework.



### HeatNet: Bridging the Day-Night Domain Gap in Semantic Segmentation with Thermal Images
- **Arxiv ID**: http://arxiv.org/abs/2003.04645v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2003.04645v1)
- **Published**: 2020-03-10 11:36:42+00:00
- **Updated**: 2020-03-10 11:36:42+00:00
- **Authors**: Johan Vertens, Jannik Zürn, Wolfram Burgard
- **Comment**: None
- **Journal**: None
- **Summary**: The majority of learning-based semantic segmentation methods are optimized for daytime scenarios and favorable lighting conditions. Real-world driving scenarios, however, entail adverse environmental conditions such as nighttime illumination or glare which remain a challenge for existing approaches. In this work, we propose a multimodal semantic segmentation model that can be applied during daytime and nighttime. To this end, besides RGB images, we leverage thermal images, making our network significantly more robust. We avoid the expensive annotation of nighttime images by leveraging an existing daytime RGB-dataset and propose a teacher-student training approach that transfers the dataset's knowledge to the nighttime domain. We further employ a domain adaptation method to align the learned feature spaces across the domains and propose a novel two-stage training scheme. Furthermore, due to a lack of thermal data for autonomous driving, we present a new dataset comprising over 20,000 time-synchronized and aligned RGB-thermal image pairs. In this context, we also present a novel target-less calibration method that allows for automatic robust extrinsic and intrinsic thermal camera calibration. Among others, we employ our new dataset to show state-of-the-art results for nighttime semantic segmentation.



### Enabling Viewpoint Learning through Dynamic Label Generation
- **Arxiv ID**: http://arxiv.org/abs/2003.04651v2
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.04651v2)
- **Published**: 2020-03-10 11:49:27+00:00
- **Updated**: 2021-02-09 14:35:11+00:00
- **Authors**: Michael Schelling, Pedro Hermosilla, Pere-Pau Vazquez, Timo Ropinski
- **Comment**: None
- **Journal**: None
- **Summary**: Optimal viewpoint prediction is an essential task in many computer graphics applications. Unfortunately, common viewpoint qualities suffer from two major drawbacks: dependency on clean surface meshes, which are not always available, and the lack of closed-form expressions, which requires a costly search involving rendering. To overcome these limitations we propose to separate viewpoint selection from rendering through an end-to-end learning approach, whereby we reduce the influence of the mesh quality by predicting viewpoints from unstructured point clouds instead of polygonal meshes. While this makes our approach insensitive to the mesh discretization during evaluation, it only becomes possible when resolving label ambiguities that arise in this context. Therefore, we additionally propose to incorporate the label generation into the training procedure, making the label decision adaptive to the current network predictions. We show how our proposed approach allows for learning viewpoint predictions for models from different object categories and for different viewpoint qualities. Additionally, we show that prediction times are reduced from several minutes to a fraction of a second, as compared to state-of-the-art (SOTA) viewpoint quality evaluation. We will further release the code and training data, which will to our knowledge be the biggest viewpoint quality dataset available.



### Lung Infection Quantification of COVID-19 in CT Images with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2003.04655v3
- **DOI**: 10.1002/mp.14609
- **Categories**: **cs.CV**, eess.IV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2003.04655v3)
- **Published**: 2020-03-10 11:58:40+00:00
- **Updated**: 2020-03-30 08:30:39+00:00
- **Authors**: Fei Shan, Yaozong Gao, Jun Wang, Weiya Shi, Nannan Shi, Miaofei Han, Zhong Xue, Dinggang Shen, Yuxin Shi
- **Comment**: 23 pages, 6 figures
- **Journal**: None
- **Summary**: CT imaging is crucial for diagnosis, assessment and staging COVID-19 infection. Follow-up scans every 3-5 days are often recommended for disease progression. It has been reported that bilateral and peripheral ground glass opacification (GGO) with or without consolidation are predominant CT findings in COVID-19 patients. However, due to lack of computerized quantification tools, only qualitative impression and rough description of infected areas are currently used in radiological reports. In this paper, a deep learning (DL)-based segmentation system is developed to automatically quantify infection regions of interest (ROIs) and their volumetric ratios w.r.t. the lung. The performance of the system was evaluated by comparing the automatically segmented infection regions with the manually-delineated ones on 300 chest CT scans of 300 COVID-19 patients. For fast manual delineation of training samples and possible manual intervention of automatic results, a human-in-the-loop (HITL) strategy has been adopted to assist radiologists for infection region segmentation, which dramatically reduced the total segmentation time to 4 minutes after 3 iterations of model updating. The average Dice simiarility coefficient showed 91.6% agreement between automatic and manual infaction segmentations, and the mean estimation error of percentage of infection (POI) was 0.3% for the whole lung. Finally, possible applications, including but not limited to analysis of follow-up CT scans and infection distributions in the lobes and segments correlated with clinical findings, were discussed.



### Multi-level Context Gating of Embedded Collective Knowledge for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2003.05056v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.05056v1)
- **Published**: 2020-03-10 12:29:59+00:00
- **Updated**: 2020-03-10 12:29:59+00:00
- **Authors**: Maryam Asadi-Aghbolaghi, Reza Azad, Mahmood Fathy, Sergio Escalera
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1909.00166
- **Journal**: None
- **Summary**: Medical image segmentation has been very challenging due to the large variation of anatomy across different cases. Recent advances in deep learning frameworks have exhibited faster and more accurate performance in image segmentation. Among the existing networks, U-Net has been successfully applied on medical image segmentation. In this paper, we propose an extension of U-Net for medical image segmentation, in which we take full advantages of U-Net, Squeeze and Excitation (SE) block, bi-directional ConvLSTM (BConvLSTM), and the mechanism of dense convolutions. (I) We improve the segmentation performance by utilizing SE modules within the U-Net, with a minor effect on model complexity. These blocks adaptively recalibrate the channel-wise feature responses by utilizing a self-gating mechanism of the global information embedding of the feature maps. (II) To strengthen feature propagation and encourage feature reuse, we use densely connected convolutions in the last convolutional layer of the encoding path. (III) Instead of a simple concatenation in the skip connection of U-Net, we employ BConvLSTM in all levels of the network to combine the feature maps extracted from the corresponding encoding path and the previous decoding up-convolutional layer in a non-linear way. The proposed model is evaluated on six datasets DRIVE, ISIC 2017 and 2018, lung segmentation, $PH^2$, and cell nuclei segmentation, achieving state-of-the-art performance.



### Incremental Few-Shot Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2003.04668v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.04668v2)
- **Published**: 2020-03-10 12:56:59+00:00
- **Updated**: 2020-03-12 20:58:07+00:00
- **Authors**: Juan-Manuel Perez-Rua, Xiatian Zhu, Timothy Hospedales, Tao Xiang
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: Most existing object detection methods rely on the availability of abundant labelled training samples per class and offline model training in a batch mode. These requirements substantially limit their scalability to open-ended accommodation of novel classes with limited labelled training data. We present a study aiming to go beyond these limitations by considering the Incremental Few-Shot Detection (iFSD) problem setting, where new classes must be registered incrementally (without revisiting base classes) and with few examples. To this end we propose OpeN-ended Centre nEt (ONCE), a detector designed for incrementally learning to detect novel class objects with few examples. This is achieved by an elegant adaptation of the CentreNet detector to the few-shot learning scenario, and meta-learning a class-specific code generator model for registering novel classes. ONCE fully respects the incremental learning paradigm, with novel class registration requiring only a single forward pass of few-shot training samples, and no access to base classes -- thus making it suitable for deployment on embedded devices. Extensive experiments conducted on both the standard object detection and fashion landmark detection tasks show the feasibility of iFSD for the first time, opening an interesting and very important line of research.



### Realizing Pixel-Level Semantic Learning in Complex Driving Scenes based on Only One Annotated Pixel per Class
- **Arxiv ID**: http://arxiv.org/abs/2003.04671v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.04671v1)
- **Published**: 2020-03-10 12:57:55+00:00
- **Updated**: 2020-03-10 12:57:55+00:00
- **Authors**: Xi Li, Huimin Ma, Sheng Yi, Yanxian Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic segmentation tasks based on weakly supervised condition have been put forward to achieve a lightweight labeling process. For simple images that only include a few categories, researches based on image-level annotations have achieved acceptable performance. However, when facing complex scenes, since image contains a large amount of classes, it becomes difficult to learn visual appearance based on image tags. In this case, image-level annotations are not effective in providing information. Therefore, we set up a new task in which only one annotated pixel is provided for each category. Based on the more lightweight and informative condition, a three step process is built for pseudo labels generation, which progressively implement optimal feature representation for each category, image inference and context-location based refinement. In particular, since high-level semantics and low-level imaging feature have different discriminative ability for each class under driving scenes, we divide each category into "object" or "scene" and then provide different operations for the two types separately. Further, an alternate iterative structure is established to gradually improve segmentation performance, which combines CNN-based inter-image common semantic learning and imaging prior based intra-image modification process. Experiments on Cityscapes dataset demonstrate that the proposed method provides a feasible way to solve weakly supervised semantic segmentation task under complex driving scenes.



### Deep Hough Transform for Semantic Line Detection
- **Arxiv ID**: http://arxiv.org/abs/2003.04676v4
- **DOI**: 10.1109/TPAMI.2021.3077129
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.04676v4)
- **Published**: 2020-03-10 13:08:42+00:00
- **Updated**: 2021-05-01 17:46:25+00:00
- **Authors**: Kai Zhao, Qi Han, Chang-Bin Zhang, Jun Xu, Ming-Ming Cheng
- **Comment**: https://github.com/Hanqer/deep-hough-transform
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence
  2021
- **Summary**: We focus on a fundamental task of detecting meaningful line structures, a.k.a. semantic line, in natural scenes. Many previous methods regard this problem as a special case of object detection and adjust existing object detectors for semantic line detection. However, these methods neglect the inherent characteristics of lines, leading to sub-optimal performance. Lines enjoy much simpler geometric property than complex objects and thus can be compactly parameterized by a few arguments. To better exploit the property of lines, in this paper, we incorporate the classical Hough transform technique into deeply learned representations and propose a one-shot end-to-end learning framework for line detection. By parameterizing lines with slopes and biases, we perform Hough transform to translate deep representations into the parametric domain, in which we perform line detection. Specifically, we aggregate features along candidate lines on the feature map plane and then assign the aggregated features to corresponding locations in the parametric domain. Consequently, the problem of detecting semantic lines in the spatial domain is transformed into spotting individual points in the parametric domain, making the post-processing steps, i.e. non-maximal suppression, more efficient. Furthermore, our method makes it easy to extract contextual line features eg features along lines close to a specific line, that are critical for accurate line detection. In addition to the proposed method, we design an evaluation metric to assess the quality of line detection and construct a large scale dataset for the line detection task. Experimental results on our proposed dataset and another public dataset demonstrate the advantages of our method over previous state-of-the-art alternatives.



### Learning to Respond with Stickers: A Framework of Unifying Multi-Modality in Multi-Turn Dialog
- **Arxiv ID**: http://arxiv.org/abs/2003.04679v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2003.04679v1)
- **Published**: 2020-03-10 13:10:26+00:00
- **Updated**: 2020-03-10 13:10:26+00:00
- **Authors**: Shen Gao, Xiuying Chen, Chang Liu, Li Liu, Dongyan Zhao, Rui Yan
- **Comment**: Accepted by The Web Conference 2020 (WWW 2020). Equal contribution
  from first two authors. Dataset and code are released at
  https://github.com/gsh199449/stickerchat
- **Journal**: None
- **Summary**: Stickers with vivid and engaging expressions are becoming increasingly popular in online messaging apps, and some works are dedicated to automatically select sticker response by matching text labels of stickers with previous utterances. However, due to their large quantities, it is impractical to require text labels for the all stickers. Hence, in this paper, we propose to recommend an appropriate sticker to user based on multi-turn dialog context history without any external labels. Two main challenges are confronted in this task. One is to learn semantic meaning of stickers without corresponding text labels. Another challenge is to jointly model the candidate sticker with the multi-turn dialog context. To tackle these challenges, we propose a sticker response selector (SRS) model. Specifically, SRS first employs a convolutional based sticker image encoder and a self-attention based multi-turn dialog encoder to obtain the representation of stickers and utterances. Next, deep interaction network is proposed to conduct deep matching between the sticker with each utterance in the dialog history. SRS then learns the short-term and long-term dependency between all interaction results by a fusion network to output the the final matching score. To evaluate our proposed method, we collect a large-scale real-world dialog dataset with stickers from one of the most popular online chatting platform. Extensive experiments conducted on this dataset show that our model achieves the state-of-the-art performance for all commonly-used metrics. Experiments also verify the effectiveness of each component of SRS. To facilitate further research in sticker selection field, we release this dataset of 340K multi-turn dialog and sticker pairs.



### A Convolutional Neural Network-based Patent Image Retrieval Method for Design Ideation
- **Arxiv ID**: http://arxiv.org/abs/2003.08741v3
- **DOI**: 10.1115/DETC2020-22048
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2003.08741v3)
- **Published**: 2020-03-10 13:32:08+00:00
- **Updated**: 2020-05-19 22:58:48+00:00
- **Authors**: Shuo Jiang, Jianxi Luo, Guillermo Ruiz Pava, Jie Hu, Christopher L. Magee
- **Comment**: 11 pages, 11 figures
- **Journal**: ASME 2020 International Design Engineering Technical Conferences
  and Computers and Information in Engineering Conference
- **Summary**: The patent database is often used in searches of inspirational stimuli for innovative design opportunities because of its large size, extensive variety and rich design information in patent documents. However, most patent mining research only focuses on textual information and ignores visual information. Herein, we propose a convolutional neural network (CNN)-based patent image retrieval method. The core of this approach is a novel neural network architecture named Dual-VGG that is aimed to accomplish two tasks: visual material type prediction and international patent classification (IPC) class label prediction. In turn, the trained neural network provides the deep features in the image embedding vectors that can be utilized for patent image retrieval and visual mapping. The accuracy of both training tasks and patent image embedding space are evaluated to show the performance of our model. This approach is also illustrated in a case study of robot arm design retrieval. Compared to traditional keyword-based searching and Google image searching, the proposed method discovers more useful visual information for engineering design.



### Deep Blind Video Super-resolution
- **Arxiv ID**: http://arxiv.org/abs/2003.04716v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.04716v1)
- **Published**: 2020-03-10 13:43:24+00:00
- **Updated**: 2020-03-10 13:43:24+00:00
- **Authors**: Jinshan Pan, Songsheng Cheng, Jiawei Zhang, Jinhui Tang
- **Comment**: None
- **Journal**: None
- **Summary**: Existing video super-resolution (SR) algorithms usually assume that the blur kernels in the degradation process are known and do not model the blur kernels in the restoration. However, this assumption does not hold for video SR and usually leads to over-smoothed super-resolved images. In this paper, we propose a deep convolutional neural network (CNN) model to solve video SR by a blur kernel modeling approach. The proposed deep CNN model consists of motion blur estimation, motion estimation, and latent image restoration modules. The motion blur estimation module is used to provide reliable blur kernels. With the estimated blur kernel, we develop an image deconvolution method based on the image formation model of video SR to generate intermediate latent images so that some sharp image contents can be restored well. However, the generated intermediate latent images may contain artifacts. To generate high-quality images, we use the motion estimation module to explore the information from adjacent frames, where the motion estimation can constrain the deep CNN model for better image restoration. We show that the proposed algorithm is able to generate clearer images with finer structural details. Extensive experimental results show that the proposed algorithm performs favorably against state-of-the-art methods.



### Rainy screens: Collecting rainy datasets, indoors
- **Arxiv ID**: http://arxiv.org/abs/2003.04742v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.04742v1)
- **Published**: 2020-03-10 13:57:37+00:00
- **Updated**: 2020-03-10 13:57:37+00:00
- **Authors**: Horia Porav, Valentina-Nicoleta Musat, Tom Bruls, Paul Newman
- **Comment**: None
- **Journal**: None
- **Summary**: Acquisition of data with adverse conditions in robotics is a cumbersome task due to the difficulty in guaranteeing proper ground truth and synchronising with desired weather conditions. In this paper, we present a simple method - recording a high resolution screen - for generating diverse rainy images from existing clear ground-truth images that is domain- and source-agnostic, simple and scales up. This setup allows us to leverage the diversity of existing datasets with auxiliary task ground-truth data, such as semantic segmentation, object positions etc. We generate rainy images with real adherent droplets and rain streaks based on Cityscapes and BDD, and train a de-raining model. We present quantitative results for image reconstruction and semantic segmentation, and qualitative results for an out-of-sample domain, showing that models trained with our data generalize well.



### Spitzoid Lesions Diagnosis based on GA feature selection and Random Forest
- **Arxiv ID**: http://arxiv.org/abs/2003.04745v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2003.04745v2)
- **Published**: 2020-03-10 14:03:28+00:00
- **Updated**: 2020-06-03 13:23:16+00:00
- **Authors**: Abir Belaala, Labib Sadek, Noureddine Zerhouni, Christine Devalland
- **Comment**: None
- **Journal**: None
- **Summary**: Spitzoid lesions broadly categorized into Spitz Nevus (SN), Atypical Spitz Tumors (AST), and Spitz Melanomas (SM). The accurate diagnosis of these lesions is one of the most challenges for dermapathologists; this is due to the high similarities between them. Data mining techniques are successfully applied to situations like these where complexity exists. This study aims to develop an artificial intelligence model to support the diagnosis of Spitzoid lesions. A private spitzoid lesions dataset have been used to evaluate the system proposed in this study. The proposed system has three stages. In the first stage, SMOTE method applied to solve the imbalance data problem, in the second stage, in order to eliminate irrelevant features; genetic algorithm is used to select significant features. This later reduces the computational complexity and speed up the data mining process. In the third stage, Random forest classifier is employed to make a decision for two different categories of lesions (Spitz nevus or Atypical Spitz Tumors). The performance of our proposed scheme is evaluated using accuracy, sensitivity, specificity, G-mean, F- measure, ROC and AUC. Results obtained with our SMOTE-GA-RF model with GA-based 16 features show a great performance with accuracy 0.97, F-measure 0.98, AUC 0.98, and G-mean 0.97.Results obtained in this study have potential to open new opportunities in diagnosis of spitzoid lesions.



### AP-MTL: Attention Pruned Multi-task Learning Model for Real-time Instrument Detection and Segmentation in Robot-assisted Surgery
- **Arxiv ID**: http://arxiv.org/abs/2003.04769v2
- **DOI**: 10.1109/ICRA40945.2020.9196905
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.04769v2)
- **Published**: 2020-03-10 14:24:51+00:00
- **Updated**: 2020-05-31 12:30:42+00:00
- **Authors**: Mobarakol Islam, Vibashan VS, Hongliang Ren
- **Comment**: Accepted in the conference of ICRA 2020
- **Journal**: None
- **Summary**: Surgical scene understanding and multi-tasking learning are crucial for image-guided robotic surgery. Training a real-time robotic system for the detection and segmentation of high-resolution images provides a challenging problem with the limited computational resource. The perception drawn can be applied in effective real-time feedback, surgical skill assessment, and human-robot collaborative surgeries to enhance surgical outcomes. For this purpose, we develop a novel end-to-end trainable real-time Multi-Task Learning (MTL) model with weight-shared encoder and task-aware detection and segmentation decoders. Optimization of multiple tasks at the same convergence point is vital and presents a complex problem. Thus, we propose an asynchronous task-aware optimization (ATO) technique to calculate task-oriented gradients and train the decoders independently. Moreover, MTL models are always computationally expensive, which hinder real-time applications. To address this challenge, we introduce a global attention dynamic pruning (GADP) by removing less significant and sparse parameters. We further design a skip squeeze and excitation (SE) module, which suppresses weak features, excites significant features and performs dynamic spatial and channel-wise feature re-calibration. Validating on the robotic instrument segmentation dataset of MICCAI endoscopic vision challenge, our model significantly outperforms state-of-the-art segmentation and detection models, including best-performed models in the challenge.



### Multi-Task Recurrent Neural Network for Surgical Gesture Recognition and Progress Prediction
- **Arxiv ID**: http://arxiv.org/abs/2003.04772v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, I.5.0, I.2.9
- **Links**: [PDF](http://arxiv.org/pdf/2003.04772v1)
- **Published**: 2020-03-10 14:28:02+00:00
- **Updated**: 2020-03-10 14:28:02+00:00
- **Authors**: Beatrice van Amsterdam, Matthew J. Clarkson, Danail Stoyanov
- **Comment**: Accepted to ICRA 2020
- **Journal**: None
- **Summary**: Surgical gesture recognition is important for surgical data science and computer-aided intervention. Even with robotic kinematic information, automatically segmenting surgical steps presents numerous challenges because surgical demonstrations are characterized by high variability in style, duration and order of actions. In order to extract discriminative features from the kinematic signals and boost recognition accuracy, we propose a multi-task recurrent neural network for simultaneous recognition of surgical gestures and estimation of a novel formulation of surgical task progress. To show the effectiveness of the presented approach, we evaluate its application on the JIGSAWS dataset, that is currently the only publicly available dataset for surgical gesture recognition featuring robot kinematic data. We demonstrate that recognition performance improves in multi-task frameworks with progress estimation without any additional manual labelling and training.



### Off-Road Drivable Area Extraction Using 3D LiDAR Data
- **Arxiv ID**: http://arxiv.org/abs/2003.04780v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.04780v1)
- **Published**: 2020-03-10 14:44:45+00:00
- **Updated**: 2020-03-10 14:44:45+00:00
- **Authors**: Biao Gao, Anran Xu, Yancheng Pan, Xijun Zhao, Wen Yao, Huijing Zhao
- **Comment**: Accepted by IEEE Intelligent Vehicles Symposium (IV2019)
- **Journal**: None
- **Summary**: We propose a method for off-road drivable area extraction using 3D LiDAR data with the goal of autonomous driving application. A specific deep learning framework is designed to deal with the ambiguous area, which is one of the main challenges in the off-road environment. To reduce the considerable demand for human-annotated data for network training, we utilize the information from vast quantities of vehicle paths and auto-generated obstacle labels. Using these autogenerated annotations, the proposed network can be trained using weakly supervised or semi-supervised methods, which can achieve better performance with fewer human annotations. The experiments on our dataset illustrate the reasonability of our framework and the validity of our weakly and semi-supervised methods.



### Reconstruction of 3D flight trajectories from ad-hoc camera networks
- **Arxiv ID**: http://arxiv.org/abs/2003.04784v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.04784v2)
- **Published**: 2020-03-10 14:57:32+00:00
- **Updated**: 2020-07-29 09:40:39+00:00
- **Authors**: Jingtong Li, Jesse Murray, Dorina Ismaili, Konrad Schindler, Cenek Albl
- **Comment**: IROS 2020 camera ready version
- **Journal**: None
- **Summary**: We present a method to reconstruct the 3D trajectory of an airborne robotic system only from videos recorded with cameras that are unsynchronized, may feature rolling shutter distortion, and whose viewpoints are unknown. Our approach enables robust and accurate outside-in tracking of dynamically flying targets, with cheap and easy-to-deploy equipment. We show that, in spite of the weakly constrained setting, recent developments in computer vision make it possible to reconstruct trajectories in 3D from unsynchronized, uncalibrated networks of consumer cameras, and validate the proposed method in a realistic field experiment. We make our code available along with the data, including cm-accurate ground-truth from differential GNSS navigation.



### SAD: Saliency-based Defenses Against Adversarial Examples
- **Arxiv ID**: http://arxiv.org/abs/2003.04820v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.04820v1)
- **Published**: 2020-03-10 15:55:23+00:00
- **Updated**: 2020-03-10 15:55:23+00:00
- **Authors**: Richard Tran, David Patrick, Michael Geyer, Amanda Fernandez
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: With the rise in popularity of machine and deep learning models, there is an increased focus on their vulnerability to malicious inputs. These adversarial examples drift model predictions away from the original intent of the network and are a growing concern in practical security. In order to combat these attacks, neural networks can leverage traditional image processing approaches or state-of-the-art defensive models to reduce perturbations in the data. Defensive approaches that take a global approach to noise reduction are effective against adversarial attacks, however their lossy approach often distorts important data within the image. In this work, we propose a visual saliency based approach to cleaning data affected by an adversarial attack. Our model leverages the salient regions of an adversarial image in order to provide a targeted countermeasure while comparatively reducing loss within the cleaned images. We measure the accuracy of our model by evaluating the effectiveness of state-of-the-art saliency methods prior to attack, under attack, and after application of cleaning methods. We demonstrate the effectiveness of our proposed approach in comparison with related defenses and against established adversarial attack methods, across two saliency datasets. Our targeted approach shows significant improvements in a range of standard statistical and distance saliency metrics, in comparison with both traditional and state-of-the-art approaches.



### Hierarchical Human Parsing with Typed Part-Relation Reasoning
- **Arxiv ID**: http://arxiv.org/abs/2003.04845v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.04845v2)
- **Published**: 2020-03-10 16:45:41+00:00
- **Updated**: 2020-03-11 10:14:43+00:00
- **Authors**: Wenguan Wang, Hailong Zhu, Jifeng Dai, Yanwei Pang, Jianbing Shen, Ling Shao
- **Comment**: Accepted to CVPR 2020.
  Code:https://github.com/hlzhu09/Hierarchical-Human-Parsing
- **Journal**: None
- **Summary**: Human parsing is for pixel-wise human semantic understanding. As human bodies are underlying hierarchically structured, how to model human structures is the central theme in this task. Focusing on this, we seek to simultaneously exploit the representational capacity of deep graph networks and the hierarchical human structures. In particular, we provide following two contributions. First, three kinds of part relations, i.e., decomposition, composition, and dependency, are, for the first time, completely and precisely described by three distinct relation networks. This is in stark contrast to previous parsers, which only focus on a portion of the relations and adopt a type-agnostic relation modeling strategy. More expressive relation information can be captured by explicitly imposing the parameters in the relation networks to satisfy the specific characteristics of different relations. Second, previous parsers largely ignore the need for an approximation algorithm over the loopy human hierarchy, while we instead address an iterative reasoning process, by assimilating generic message-passing networks with their edge-typed, convolutional counterparts. With these efforts, our parser lays the foundation for more sophisticated and flexible human relation patterns of reasoning. Comprehensive experiments on five datasets demonstrate that our parser sets a new state-of-the-art on each.



### PANDA: A Gigapixel-level Human-centric Video Dataset
- **Arxiv ID**: http://arxiv.org/abs/2003.04852v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.04852v1)
- **Published**: 2020-03-10 16:58:32+00:00
- **Updated**: 2020-03-10 16:58:32+00:00
- **Authors**: Xueyang Wang, Xiya Zhang, Yinheng Zhu, Yuchen Guo, Xiaoyun Yuan, Liuyu Xiang, Zerun Wang, Guiguang Ding, David J Brady, Qionghai Dai, Lu Fang
- **Comment**: Accepted by IEEE International Conference on Computer Vision and
  Pattern Recognition (CVPR) 2020
- **Journal**: None
- **Summary**: We present PANDA, the first gigaPixel-level humAN-centric viDeo dAtaset, for large-scale, long-term, and multi-object visual analysis. The videos in PANDA were captured by a gigapixel camera and cover real-world scenes with both wide field-of-view (~1 square kilometer area) and high-resolution details (~gigapixel-level/frame). The scenes may contain 4k head counts with over 100x scale variation. PANDA provides enriched and hierarchical ground-truth annotations, including 15,974.6k bounding boxes, 111.8k fine-grained attribute labels, 12.7k trajectories, 2.2k groups and 2.9k interactions. We benchmark the human detection and tracking tasks. Due to the vast variance of pedestrian pose, scale, occlusion and trajectory, existing approaches are challenged by both accuracy and efficiency. Given the uniqueness of PANDA with both wide FoV and high resolution, a new task of interaction-aware group detection is introduced. We design a 'global-to-local zoom-in' framework, where global trajectories and local interactions are simultaneously encoded, yielding promising results. We believe PANDA will contribute to the community of artificial intelligence and praxeology by understanding human behaviors and interactions in large-scale real-world scenes. PANDA Website: http://www.panda-dataset.com.



### Image Restoration for Under-Display Camera
- **Arxiv ID**: http://arxiv.org/abs/2003.04857v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.04857v2)
- **Published**: 2020-03-10 17:09:00+00:00
- **Updated**: 2021-03-14 07:54:43+00:00
- **Authors**: Yuqian Zhou, David Ren, Neil Emerton, Sehoon Lim, Timothy Large
- **Comment**: Accepted by CVPR 2021
- **Journal**: None
- **Summary**: The new trend of full-screen devices encourages us to position a camera behind a screen. Removing the bezel and centralizing the camera under the screen brings larger display-to-body ratio and enhances eye contact in video chat, but also causes image degradation. In this paper, we focus on a newly-defined Under-Display Camera (UDC), as a novel real-world single image restoration problem. First, we take a 4k Transparent OLED (T-OLED) and a phone Pentile OLED (P-OLED) and analyze their optical systems to understand the degradation. Second, we design a Monitor-Camera Imaging System (MCIS) for easier real pair data acquisition, and a model-based data synthesizing pipeline to generate Point Spread Function (PSF) and UDC data only from display pattern and camera measurements. Finally, we resolve the complicated degradation using deconvolution-based pipeline and learning-based methods. Our model demonstrates a real-time high-quality restoration. The presented methods and results reveal the promising research values and directions of UDC.



### Unpaired Image-to-Image Translation using Adversarial Consistency Loss
- **Arxiv ID**: http://arxiv.org/abs/2003.04858v7
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.04858v7)
- **Published**: 2020-03-10 17:10:38+00:00
- **Updated**: 2021-01-18 12:13:57+00:00
- **Authors**: Yihao Zhao, Ruihai Wu, Hao Dong
- **Comment**: None
- **Journal**: None
- **Summary**: Unpaired image-to-image translation is a class of vision problems whose goal is to find the mapping between different image domains using unpaired training data. Cycle-consistency loss is a widely used constraint for such problems. However, due to the strict pixel-level constraint, it cannot perform geometric changes, remove large objects, or ignore irrelevant texture. In this paper, we propose a novel adversarial-consistency loss for image-to-image translation. This loss does not require the translated image to be translated back to be a specific source image but can encourage the translated images to retain important features of the source images and overcome the drawbacks of cycle-consistency loss noted above. Our method achieves state-of-the-art results on three challenging tasks: glasses removal, male-to-female translation, and selfie-to-anime translation.



### Out-of-Distribution Detection in Multi-Label Datasets using Latent Space of $β$-VAE
- **Arxiv ID**: http://arxiv.org/abs/2003.08740v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.08740v1)
- **Published**: 2020-03-10 17:12:02+00:00
- **Updated**: 2020-03-10 17:12:02+00:00
- **Authors**: Vijaya Kumar Sundar, Shreyas Ramakrishna, Zahra Rahiminasab, Arvind Easwaran, Abhishek Dubey
- **Comment**: Workshop on Assured Autonomy (WAAS) -2020
- **Journal**: None
- **Summary**: Learning Enabled Components (LECs) are widely being used in a variety of perception based autonomy tasks like image segmentation, object detection, end-to-end driving, etc. These components are trained with large image datasets with multimodal factors like weather conditions, time-of-day, traffic-density, etc. The LECs learn from these factors during training, and while testing if there is variation in any of these factors, the components get confused resulting in low confidence predictions. The images with factors not seen during training is commonly referred to as Out-of-Distribution (OOD). For safe autonomy it is important to identify the OOD images, so that a suitable mitigation strategy can be performed. Classical one-class classifiers like SVM and SVDD are used to perform OOD detection. However, the multiple labels attached to the images in these datasets, restricts the direct application of these techniques. We address this problem using the latent space of the $\beta$-Variational Autoencoder ($\beta$-VAE). We use the fact that compact latent space generated by an appropriately selected $\beta$-VAE will encode the information about these factors in a few latent variables, and that can be used for computationally inexpensive detection. We evaluate our approach on the nuScenes dataset, and our results shows the latent space of $\beta$-VAE is sensitive to encode changes in the values of the generative factor.



### Video Caption Dataset for Describing Human Actions in Japanese
- **Arxiv ID**: http://arxiv.org/abs/2003.04865v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.04865v1)
- **Published**: 2020-03-10 17:15:48+00:00
- **Updated**: 2020-03-10 17:15:48+00:00
- **Authors**: Yutaro Shigeto, Yuya Yoshikawa, Jiaqing Lin, Akikazu Takeuchi
- **Comment**: Accepted for LREC 2020. Dataset available at
  https://actions.stair.center/captions.html
- **Journal**: None
- **Summary**: In recent years, automatic video caption generation has attracted considerable attention. This paper focuses on the generation of Japanese captions for describing human actions. While most currently available video caption datasets have been constructed for English, there is no equivalent Japanese dataset. To address this, we constructed a large-scale Japanese video caption dataset consisting of 79,822 videos and 399,233 captions. Each caption in our dataset describes a video in the form of "who does what and where." To describe human actions, it is important to identify the details of a person, place, and action. Indeed, when we describe human actions, we usually mention the scene, person, and action. In our experiments, we evaluated two caption generation methods to obtain benchmark results. Further, we investigated whether those generation methods could specify "who does what and where."



### Maximum Likelihood Speed Estimation of Moving Objects in Video Signals
- **Arxiv ID**: http://arxiv.org/abs/2003.04883v2
- **DOI**: 10.1016/j.sigpro.2022.108528
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.04883v2)
- **Published**: 2020-03-10 17:56:50+00:00
- **Updated**: 2021-12-02 17:33:07+00:00
- **Authors**: Veronica Mattioli, Davide Alinovi, Riccardo Raheli
- **Comment**: None
- **Journal**: None
- **Summary**: Video processing solutions for motion analysis are key tasks in many computer vision applications, ranging from human activity recognition to object detection. In particular, speed estimation algorithms may be relevant in contexts such as street monitoring and environment surveillance. In most realistic scenarios, the projection of a framed object of interest onto the image plane is likely to be affected by dynamic changes mainly related to perspectival transformations or periodic behaviours. Therefore, advanced speed estimation techniques need to rely on robust algorithms for object detection that are able to deal with potential geometrical modifications. The proposed method is composed of a sequence of pre-processing operations, that aim to reduce or neglect perspetival effects affecting the objects of interest, followed by the estimation phase based on the Maximum Likelihood (ML) principle, where the speed of the foreground objects is estimated. The ML estimation method represents, indeed, a consolidated statistical tool that may be exploited to obtain reliable results. The performance of the proposed algorithm is evaluated on a set of real video recordings and compared with a block-matching motion estimation algorithm. The obtained results indicate that the proposed method shows good and robust performance.



### Cross-modal Multi-task Learning for Graphic Recognition of Caricature Face
- **Arxiv ID**: http://arxiv.org/abs/2003.05787v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.05787v1)
- **Published**: 2020-03-10 18:08:19+00:00
- **Updated**: 2020-03-10 18:08:19+00:00
- **Authors**: Zuheng Ming, Jean-Christophe Burie, Muhammad Muzzamil Luqman
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1911.03341
- **Journal**: None
- **Summary**: Face recognition of realistic visual images has been well studied and made a significant progress in the recent decade. Unlike the realistic visual images, the face recognition of the caricatures is far from the performance of the visual images. This is largely due to the extreme non-rigid distortions of the caricatures introduced by exaggerating the facial features to strengthen the characters. The heterogeneous modalities of the caricatures and the visual images result the caricature-visual face recognition is a cross-modal problem. In this paper, we propose a method to conduct caricature-visual face recognition via multi-task learning. Rather than the conventional multi-task learning with fixed weights of tasks, this work proposes an approach to learn the weights of tasks according to the importance of tasks. The proposed multi-task learning with dynamic tasks weights enables to appropriately train the hard task and easy task instead of being stuck in the over-training easy task as conventional methods. The experimental results demonstrate the effectiveness of the proposed dynamic multi-task learning for cross-modal caricature-visual face recognition. The performances on the datasets CaVI and WebCaricature show the superiority over the state-of-art methods.



### Tidying Deep Saliency Prediction Architectures
- **Arxiv ID**: http://arxiv.org/abs/2003.04942v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.04942v1)
- **Published**: 2020-03-10 19:34:49+00:00
- **Updated**: 2020-03-10 19:34:49+00:00
- **Authors**: Navyasri Reddy, Samyak Jain, Pradeep Yarlagadda, Vineet Gandhi
- **Comment**: None
- **Journal**: None
- **Summary**: Learning computational models for visual attention (saliency estimation) is an effort to inch machines/robots closer to human visual cognitive abilities. Data-driven efforts have dominated the landscape since the introduction of deep neural network architectures. In deep learning research, the choices in architecture design are often empirical and frequently lead to more complex models than necessary. The complexity, in turn, hinders the application requirements. In this paper, we identify four key components of saliency models, i.e., input features, multi-level integration, readout architecture, and loss functions. We review the existing state of the art models on these four components and propose novel and simpler alternatives. As a result, we propose two novel end-to-end architectures called SimpleNet and MDNSal, which are neater, minimal, more interpretable and achieve state of the art performance on public saliency benchmarks. SimpleNet is an optimized encoder-decoder architecture and brings notable performance gains on the SALICON dataset (the largest saliency benchmark). MDNSal is a parametric model that directly predicts parameters of a GMM distribution and is aimed to bring more interpretability to the prediction maps. The proposed saliency models can be inferred at 25fps, making them suitable for real-time applications. Code and pre-trained models are available at https://github.com/samyak0210/saliency.



### LC-GAN: Image-to-image Translation Based on Generative Adversarial Network for Endoscopic Images
- **Arxiv ID**: http://arxiv.org/abs/2003.04949v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.04949v2)
- **Published**: 2020-03-10 19:59:25+00:00
- **Updated**: 2020-08-13 21:24:33+00:00
- **Authors**: Shan Lin, Fangbo Qin, Yangming Li, Randall A. Bly, Kris S. Moe, Blake Hannaford
- **Comment**: Accepted by 2020 IEEE/RSJ International Conference on Intelligent
  Robots and Systems (IROS)
- **Journal**: None
- **Summary**: Intelligent vision is appealing in computer-assisted and robotic surgeries. Vision-based analysis with deep learning usually requires large labeled datasets, but manual data labeling is expensive and time-consuming in medical problems. We investigate a novel cross-domain strategy to reduce the need for manual data labeling by proposing an image-to-image translation model live-cadaver GAN (LC-GAN) based on generative adversarial networks (GANs). We consider a situation when a labeled cadaveric surgery dataset is available while the task is instrument segmentation on an unlabeled live surgery dataset. We train LC-GAN to learn the mappings between the cadaveric and live images. For live image segmentation, we first translate the live images to fake-cadaveric images with LC-GAN and then perform segmentation on the fake-cadaveric images with models trained on the real cadaveric dataset. The proposed method fully makes use of the labeled cadaveric dataset for live image segmentation without the need to label the live dataset. LC-GAN has two generators with different architectures that leverage the deep feature representation learned from the cadaveric image based segmentation task. Moreover, we propose the structural similarity loss and segmentation consistency loss to improve the semantic consistency during translation. Our model achieves better image-to-image translation and leads to improved segmentation performance in the proposed cross-domain segmentation task.



### A Matlab Toolbox for Feature Importance Ranking
- **Arxiv ID**: http://arxiv.org/abs/2003.08737v1
- **DOI**: 10.1109/ICMIPE47306.2019.9098233
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08737v1)
- **Published**: 2020-03-10 20:35:10+00:00
- **Updated**: 2020-03-10 20:35:10+00:00
- **Authors**: Shaode Yu, Zhicheng Zhang, Xiaokun Liang, Junjie Wu, Erlei Zhang, Wenjian Qin, Yaoqin Xie
- **Comment**: None
- **Journal**: None
- **Summary**: More attention is being paid for feature importance ranking (FIR), in particular when thousands of features can be extracted for intelligent diagnosis and personalized medicine. A large number of FIR approaches have been proposed, while few are integrated for comparison and real-life applications. In this study, a matlab toolbox is presented and a total of 30 algorithms are collected. Moreover, the toolbox is evaluated on a database of 163 ultrasound images. To each breast mass lesion, 15 features are extracted. To figure out the optimal subset of features for classification, all combinations of features are tested and linear support vector machine is used for the malignancy prediction of lesions annotated in ultrasound images. At last, the effectiveness of FIR is analyzed according to performance comparison. The toolbox is online (https://github.com/NicoYuCN/matFIR). In our future work, more FIR methods, feature selection methods and machine learning classifiers will be integrated.



### Computed Tomography Reconstruction Using Deep Image Prior and Learned Reconstruction Methods
- **Arxiv ID**: http://arxiv.org/abs/2003.04989v2
- **DOI**: 10.1088/1361-6420/aba415
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.04989v2)
- **Published**: 2020-03-10 21:03:34+00:00
- **Updated**: 2020-03-12 12:09:52+00:00
- **Authors**: Daniel Otero Baguer, Johannes Leuschner, Maximilian Schmidt
- **Comment**: None
- **Journal**: Inverse Problems, Volume 36, Number 9 (2020)
- **Summary**: In this work, we investigate the application of deep learning methods for computed tomography in the context of having a low-data regime. As motivation, we review some of the existing approaches and obtain quantitative results after training them with different amounts of data. We find that the learned primal-dual has an outstanding performance in terms of reconstruction quality and data efficiency. However, in general, end-to-end learned methods have two issues: a) lack of classical guarantees in inverse problems and b) lack of generalization when not trained with enough data. To overcome these issues, we bring in the deep image prior approach in combination with classical regularization. The proposed methods improve the state-of-the-art results in the low data-regime.



### Using an ensemble color space model to tackle adversarial examples
- **Arxiv ID**: http://arxiv.org/abs/2003.05005v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2003.05005v1)
- **Published**: 2020-03-10 21:20:53+00:00
- **Updated**: 2020-03-10 21:20:53+00:00
- **Authors**: Shreyank N Gowda, Chun Yuan
- **Comment**: None
- **Journal**: None
- **Summary**: Minute pixel changes in an image drastically change the prediction that the deep learning model makes. One of the most significant problems that could arise due to this, for instance, is autonomous driving. Many methods have been proposed to combat this with varying amounts of success. We propose a 3 step method for defending such attacks. First, we denoise the image using statistical methods. Second, we show that adopting multiple color spaces in the same model can help us to fight these adversarial attacks further as each color space detects certain features explicit to itself. Finally, the feature maps generated are enlarged and sent back as an input to obtain even smaller features. We show that the proposed model does not need to be trained to defend an particular type of attack and is inherently more robust to black-box, white-box, and grey-box adversarial attack techniques. In particular, the model is 56.12 percent more robust than compared models in case of white box attacks when the models are not subject to adversarial example training.



### PL${}_{1}$P -- Point-line Minimal Problems under Partial Visibility in Three Views
- **Arxiv ID**: http://arxiv.org/abs/2003.05015v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NA, math.AG, math.NA, 14M20, 14Q15, 14N99, 15A69, 65H20, 68T45, 13P10, 13P25
- **Links**: [PDF](http://arxiv.org/pdf/2003.05015v1)
- **Published**: 2020-03-10 21:50:52+00:00
- **Updated**: 2020-03-10 21:50:52+00:00
- **Authors**: Timothy Duff, Kathlén Kohn, Anton Leykin, Tomas Pajdla
- **Comment**: None
- **Journal**: None
- **Summary**: We present a complete classification of minimal problems for generic arrangements of points and lines in space observed partially by three calibrated perspective cameras when each line is incident to at most one point. This is a large class of interesting minimal problems that allows missing observations in images due to occlusions and missed detections. There is an infinite number of such minimal problems; however, we show that they can be reduced to 140616 equivalence classes by removing superfluous features and relabeling the cameras. We also introduce camera-minimal problems, which are practical for designing minimal solvers, and show how to pick a simplest camera-minimal problem for each minimal problem. This simplification results in 74575 equivalence classes. Only 76 of these were known; the rest are new. In order to identify problems that have potential for practical solving of image matching and 3D reconstruction, we present several smaller natural subfamilies of camera-minimal problems as well as compute solution counts for all camera-minimal problems which have less than 300 solutions for generic data.



### Learning Video Object Segmentation from Unlabeled Videos
- **Arxiv ID**: http://arxiv.org/abs/2003.05020v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.05020v1)
- **Published**: 2020-03-10 22:12:15+00:00
- **Updated**: 2020-03-10 22:12:15+00:00
- **Authors**: Xiankai Lu, Wenguan Wang, Jianbing Shen, Yu-Wing Tai, David Crandall, Steven C. H. Hoi
- **Comment**: Accepted to CVPR 2020. Code: https://github.com/carrierlxk/MuG
- **Journal**: None
- **Summary**: We propose a new method for video object segmentation (VOS) that addresses object pattern learning from unlabeled videos, unlike most existing methods which rely heavily on extensive annotated data. We introduce a unified unsupervised/weakly supervised learning framework, called MuG, that comprehensively captures intrinsic properties of VOS at multiple granularities. Our approach can help advance understanding of visual patterns in VOS and significantly reduce annotation burden. With a carefully-designed architecture and strong representation learning ability, our learned model can be applied to diverse VOS settings, including object-level zero-shot VOS, instance-level zero-shot VOS, and one-shot VOS. Experiments demonstrate promising performance in these settings, as well as the potential of MuG in leveraging unlabeled data to further improve the segmentation accuracy.



### SuperMix: Supervising the Mixing Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2003.05034v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.05034v2)
- **Published**: 2020-03-10 23:24:38+00:00
- **Updated**: 2021-12-10 18:49:00+00:00
- **Authors**: Ali Dabouei, Sobhan Soleymani, Fariborz Taherkhani, Nasser M. Nasrabadi
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a supervised mixing augmentation method termed SuperMix, which exploits the salient regions within input images to construct mixed training samples. SuperMix is designed to obtain mixed images rich in visual features and complying with realistic image priors. To enhance the efficiency of the algorithm, we develop a variant of the Newton iterative method, $65\times$ faster than gradient descent on this problem. We validate the effectiveness of SuperMix through extensive evaluations and ablation studies on two tasks of object classification and knowledge distillation. On the classification task, SuperMix provides comparable performance to the advanced augmentation methods, such as AutoAugment and RandAugment. In particular, combining SuperMix with RandAugment achieves 78.2\% top-1 accuracy on ImageNet with ResNet50. On the distillation task, solely classifying images mixed using the teacher's knowledge achieves comparable performance to the state-of-the-art distillation methods. Furthermore, on average, incorporating mixed images into the distillation objective improves the performance by 3.4\% and 3.1\% on CIFAR-100 and ImageNet, respectively. {\it The code is available at https://github.com/alldbi/SuperMix}.



### Rapid AI Development Cycle for the Coronavirus (COVID-19) Pandemic: Initial Results for Automated Detection & Patient Monitoring using Deep Learning CT Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/2003.05037v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.05037v3)
- **Published**: 2020-03-10 23:37:21+00:00
- **Updated**: 2020-03-24 08:20:12+00:00
- **Authors**: Ophir Gozes, Maayan Frid-Adar, Hayit Greenspan, Patrick D. Browning, Huangqi Zhang, Wenbin Ji, Adam Bernheim, Eliot Siegel
- **Comment**: 19 pages, 6 figures
- **Journal**: None
- **Summary**: Purpose: Develop AI-based automated CT image analysis tools for detection, quantification, and tracking of Coronavirus; demonstrate they can differentiate coronavirus patients from non-patients. Materials and Methods: Multiple international datasets, including from Chinese disease-infected areas were included. We present a system that utilizes robust 2D and 3D deep learning models, modifying and adapting existing AI models and combining them with clinical understanding. We conducted multiple retrospective experiments to analyze the performance of the system in the detection of suspected COVID-19 thoracic CT features and to evaluate evolution of the disease in each patient over time using a 3D volume review, generating a Corona score. The study includes a testing set of 157 international patients (China and U.S). Results: Classification results for Coronavirus vs Non-coronavirus cases per thoracic CT studies were 0.996 AUC (95%CI: 0.989-1.00) ; on datasets of Chinese control and infected patients. Possible working point: 98.2% sensitivity, 92.2% specificity. For time analysis of Coronavirus patients, the system output enables quantitative measurements for smaller opacities (volume, diameter) and visualization of the larger opacities in a slice-based heat map or a 3D volume display. Our suggested Corona score measures the progression of disease over time. Conclusion: This initial study, which is currently being expanded to a larger population, demonstrated that rapidly developed AI-based image analysis can achieve high accuracy in detection of Coronavirus as well as quantification and tracking of disease burden.



