# Arxiv Papers in cs.CV on 2020-03-29
### Predict the model of a camera
- **Arxiv ID**: http://arxiv.org/abs/2004.03336v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2004.03336v1)
- **Published**: 2020-03-29 02:08:11+00:00
- **Updated**: 2020-03-29 02:08:11+00:00
- **Authors**: Ciro Javier Diaz Penedo
- **Comment**: None
- **Journal**: None
- **Summary**: In this work we address the problem of predicting the model of a camera based on the content of their photographs. We use two set of features, one set consist in properties extracted from a Discrete Wavelet Domain (DWD) obtained by applying a 4 level Fast Wavelet Decomposition of the images, and a second set are Local Binary Patterns (LBP) features from the after filter noise of images. The algorithms used for classification were Logistic regression, K-NN and Artificial Neural Networks



### Superpixel Segmentation with Fully Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/2003.12929v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.12929v1)
- **Published**: 2020-03-29 02:42:07+00:00
- **Updated**: 2020-03-29 02:42:07+00:00
- **Authors**: Fengting Yang, Qian Sun, Hailin Jin, Zihan Zhou
- **Comment**: 16 pages, 15 figures, to be published in CVPR'20
- **Journal**: None
- **Summary**: In computer vision, superpixels have been widely used as an effective way to reduce the number of image primitives for subsequent processing. But only a few attempts have been made to incorporate them into deep neural networks. One main reason is that the standard convolution operation is defined on regular grids and becomes inefficient when applied to superpixels. Inspired by an initialization strategy commonly adopted by traditional superpixel algorithms, we present a novel method that employs a simple fully convolutional network to predict superpixels on a regular image grid. Experimental results on benchmark datasets show that our method achieves state-of-the-art superpixel segmentation performance while running at about 50fps. Based on the predicted superpixels, we further develop a downsampling/upsampling scheme for deep networks with the goal of generating high-resolution outputs for dense prediction tasks. Specifically, we modify a popular network architecture for stereo matching to simultaneously predict superpixels and disparities. We show that improved disparity estimation accuracy can be obtained on public datasets.



### Co-occurrence Background Model with Superpixels for Robust Background Initialization
- **Arxiv ID**: http://arxiv.org/abs/2003.12931v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.12931v1)
- **Published**: 2020-03-29 02:48:41+00:00
- **Updated**: 2020-03-29 02:48:41+00:00
- **Authors**: Wenjun Zhou, Yuheng Deng, Bo Peng, Dong Liang, Shun'ichi Kaneko
- **Comment**: None
- **Journal**: None
- **Summary**: Background initialization is an important step in many high-level applications of video processing,ranging from video surveillance to video inpainting.However,this process is often affected by practical challenges such as illumination changes,background motion,camera jitter and intermittent movement,etc.In this paper,we develop a co-occurrence background model with superpixel segmentation for robust background initialization. We first introduce a novel co-occurrence background modeling method called as Co-occurrence Pixel-Block Pairs(CPB)to generate a reliable initial background model,and the superpixel segmentation is utilized to further acquire the spatial texture Information of foreground and background.Then,the initial background can be determined by combining the foreground extraction results with the superpixel segmentation information.Experimental results obtained from the dataset of the challenging benchmark(SBMnet)validate it's performance under various challenges.



### Adaptive Object Detection with Dual Multi-Label Prediction
- **Arxiv ID**: http://arxiv.org/abs/2003.12943v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.12943v2)
- **Published**: 2020-03-29 04:23:22+00:00
- **Updated**: 2020-08-11 00:25:27+00:00
- **Authors**: Zhen Zhao, Yuhong Guo, Haifeng Shen, Jieping Ye
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: In this paper, we propose a novel end-to-end unsupervised deep domain adaptation model for adaptive object detection by exploiting multi-label object recognition as a dual auxiliary task. The model exploits multi-label prediction to reveal the object category information in each image and then uses the prediction results to perform conditional adversarial global feature alignment, such that the multi-modal structure of image features can be tackled to bridge the domain divergence at the global feature level while preserving the discriminability of the features. Moreover, we introduce a prediction consistency regularization mechanism to assist object detection, which uses the multi-label prediction results as an auxiliary regularization information to ensure consistent object category discoveries between the object recognition task and the object detection task. Experiments are conducted on a few benchmark datasets and the results show the proposed model outperforms the state-of-the-art comparison methods.



### Mutual Learning Network for Multi-Source Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2003.12944v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.12944v1)
- **Published**: 2020-03-29 04:31:43+00:00
- **Updated**: 2020-03-29 04:31:43+00:00
- **Authors**: Zhenpeng Li, Zhen Zhao, Yuhong Guo, Haifeng Shen, Jieping Ye
- **Comment**: None
- **Journal**: None
- **Summary**: Early Unsupervised Domain Adaptation (UDA) methods have mostly assumed the setting of a single source domain, where all the labeled source data come from the same distribution. However, in practice the labeled data can come from multiple source domains with different distributions. In such scenarios, the single source domain adaptation methods can fail due to the existence of domain shifts across different source domains and multi-source domain adaptation methods need to be designed. In this paper, we propose a novel multi-source domain adaptation method, Mutual Learning Network for Multiple Source Domain Adaptation (ML-MSDA). Under the framework of mutual learning, the proposed method pairs the target domain with each single source domain to train a conditional adversarial domain adaptation network as a branch network, while taking the pair of the combined multi-source domain and target domain to train a conditional adversarial adaptive network as the guidance network. The multiple branch networks are aligned with the guidance network to achieve mutual learning by enforcing JS-divergence regularization over their prediction probability distributions on the corresponding target data. We conduct extensive experiments on multiple multi-source domain adaptation benchmark datasets. The results show the proposed ML-MSDA method outperforms the comparison methods and achieves the state-of-the-art performance.



### AutoTrack: Towards High-Performance Visual Tracking for UAV with Automatic Spatio-Temporal Regularization
- **Arxiv ID**: http://arxiv.org/abs/2003.12949v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.12949v1)
- **Published**: 2020-03-29 05:02:25+00:00
- **Updated**: 2020-03-29 05:02:25+00:00
- **Authors**: Yiming Li, Changhong Fu, Fangqiang Ding, Ziyuan Huang, Geng Lu
- **Comment**: 2020 IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)
- **Journal**: None
- **Summary**: Most existing trackers based on discriminative correlation filters (DCF) try to introduce predefined regularization term to improve the learning of target objects, e.g., by suppressing background learning or by restricting change rate of correlation filters. However, predefined parameters introduce much effort in tuning them and they still fail to adapt to new situations that the designer did not think of. In this work, a novel approach is proposed to online automatically and adaptively learn spatio-temporal regularization term. Spatially local response map variation is introduced as spatial regularization to make DCF focus on the learning of trust-worthy parts of the object, and global response map variation determines the updating rate of the filter. Extensive experiments on four UAV benchmarks have proven the superiority of our method compared to the state-of-the-art CPU- and GPU-based trackers, with a speed of ~60 frames per second running on a single CPU.   Our tracker is additionally proposed to be applied in UAV localization. Considerable tests in the indoor practical scenarios have proven the effectiveness and versatility of our localization method. The code is available at https://github.com/vision4robotics/AutoTrack.



### Realistic Face Reenactment via Self-Supervised Disentangling of Identity and Pose
- **Arxiv ID**: http://arxiv.org/abs/2003.12957v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.12957v1)
- **Published**: 2020-03-29 06:45:17+00:00
- **Updated**: 2020-03-29 06:45:17+00:00
- **Authors**: Xianfang Zeng, Yusu Pan, Mengmeng Wang, Jiangning Zhang, Yong Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Recent works have shown how realistic talking face images can be obtained under the supervision of geometry guidance, e.g., facial landmark or boundary. To alleviate the demand for manual annotations, in this paper, we propose a novel self-supervised hybrid model (DAE-GAN) that learns how to reenact face naturally given large amounts of unlabeled videos. Our approach combines two deforming autoencoders with the latest advances in the conditional generation. On the one hand, we adopt the deforming autoencoder to disentangle identity and pose representations. A strong prior in talking face videos is that each frame can be encoded as two parts: one for video-specific identity and the other for various poses. Inspired by that, we utilize a multi-frame deforming autoencoder to learn a pose-invariant embedded face for each video. Meanwhile, a multi-scale deforming autoencoder is proposed to extract pose-related information for each frame. On the other hand, the conditional generator allows for enhancing fine details and overall reality. It leverages the disentangled features to generate photo-realistic and pose-alike face images. We evaluate our model on VoxCeleb1 and RaFD dataset. Experiment results demonstrate the superior quality of reenacted images and the flexibility of transferring facial movements between identities.



### GPS-Net: Graph Property Sensing Network for Scene Graph Generation
- **Arxiv ID**: http://arxiv.org/abs/2003.12962v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.12962v1)
- **Published**: 2020-03-29 07:22:31+00:00
- **Updated**: 2020-03-29 07:22:31+00:00
- **Authors**: Xin Lin, Changxing Ding, Jinquan Zeng, Dacheng Tao
- **Comment**: Accepted by CVPR 2020 as Oral. Code is available
- **Journal**: None
- **Summary**: Scene graph generation (SGG) aims to detect objects in an image along with their pairwise relationships. There are three key properties of scene graph that have been underexplored in recent works: namely, the edge direction information, the difference in priority between nodes, and the long-tailed distribution of relationships. Accordingly, in this paper, we propose a Graph Property Sensing Network (GPS-Net) that fully explores these three properties for SGG. First, we propose a novel message passing module that augments the node feature with node-specific contextual information and encodes the edge direction information via a tri-linear model. Second, we introduce a node priority sensitive loss to reflect the difference in priority between nodes during training. This is achieved by designing a mapping function that adjusts the focusing parameter in the focal loss. Third, since the frequency of relationships is affected by the long-tailed distribution problem, we mitigate this issue by first softening the distribution and then enabling it to be adjusted for each subject-object pair according to their visual appearance. Systematic experiments demonstrate the effectiveness of the proposed techniques. Moreover, GPS-Net achieves state-of-the-art performance on three popular databases: VG, OI, and VRD by significant gains under various settings and metrics. The code and models are available at \url{https://github.com/taksau/GPS-Net}.



### Global-Local Bidirectional Reasoning for Unsupervised Representation Learning of 3D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2003.12971v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.12971v1)
- **Published**: 2020-03-29 08:26:08+00:00
- **Updated**: 2020-03-29 08:26:08+00:00
- **Authors**: Yongming Rao, Jiwen Lu, Jie Zhou
- **Comment**: Accepted to CVPR2020
- **Journal**: None
- **Summary**: Local and global patterns of an object are closely related. Although each part of an object is incomplete, the underlying attributes about the object are shared among all parts, which makes reasoning the whole object from a single part possible. We hypothesize that a powerful representation of a 3D object should model the attributes that are shared between parts and the whole object, and distinguishable from other objects. Based on this hypothesis, we propose to learn point cloud representation by bidirectional reasoning between the local structures at different abstraction hierarchies and the global shape without human supervision. Experimental results on various benchmark datasets demonstrate the unsupervisedly learned representation is even better than supervised representation in discriminative power, generalization ability, and robustness. We show that unsupervisedly trained point cloud models can outperform their supervised counterparts on downstream classification tasks. Most notably, by simply increasing the channel width of an SSG PointNet++, our unsupervised model surpasses the state-of-the-art supervised methods on both synthetic and real-world 3D object classification datasets. We expect our observations to offer a new perspective on learning better representation from data structures instead of human annotations for point cloud understanding.



### Spatial Attention Pyramid Network for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2003.12979v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.12979v3)
- **Published**: 2020-03-29 09:03:23+00:00
- **Updated**: 2020-07-22 14:50:31+00:00
- **Authors**: Congcong Li, Dawei Du, Libo Zhang, Longyin Wen, Tiejian Luo, Yanjun Wu, Pengfei Zhu
- **Comment**: Accepted to ECCV 2020
- **Journal**: None
- **Summary**: Unsupervised domain adaptation is critical in various computer vision tasks, such as object detection, instance segmentation, and semantic segmentation, which aims to alleviate performance degradation caused by domain-shift. Most of previous methods rely on a single-mode distribution of source and target domains to align them with adversarial learning, leading to inferior results in various scenarios. To that end, in this paper, we design a new spatial attention pyramid network for unsupervised domain adaptation. Specifically, we first build the spatial pyramid representation to capture context information of objects at different scales. Guided by the task-specific information, we combine the dense global structure representation and local texture patterns at each spatial location effectively using the spatial attention mechanism. In this way, the network is enforced to focus on the discriminative regions with context information for domain adaption. We conduct extensive experiments on various challenging datasets for unsupervised domain adaptation on object detection, instance segmentation, and semantic segmentation, which demonstrates that our method performs favorably against the state-of-the-art methods by a large margin. Our source code is available at https://isrc.iscas.ac.cn/gitlab/research/domain-adaption.



### ClusterVO: Clustering Moving Instances and Estimating Visual Odometry for Self and Surroundings
- **Arxiv ID**: http://arxiv.org/abs/2003.12980v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2003.12980v1)
- **Published**: 2020-03-29 09:06:28+00:00
- **Updated**: 2020-03-29 09:06:28+00:00
- **Authors**: Jiahui Huang, Sheng Yang, Tai-Jiang Mu, Shi-Min Hu
- **Comment**: 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition
  (CVPR)
- **Journal**: None
- **Summary**: We present ClusterVO, a stereo Visual Odometry which simultaneously clusters and estimates the motion of both ego and surrounding rigid clusters/objects. Unlike previous solutions relying on batch input or imposing priors on scene structure or dynamic object models, ClusterVO is online, general and thus can be used in various scenarios including indoor scene understanding and autonomous driving. At the core of our system lies a multi-level probabilistic association mechanism and a heterogeneous Conditional Random Field (CRF) clustering approach combining semantic, spatial and motion information to jointly infer cluster segmentations online for every frame. The poses of camera and dynamic objects are instantly solved through a sliding-window optimization. Our system is evaluated on Oxford Multimotion and KITTI dataset both quantitatively and qualitatively, reaching comparable results to state-of-the-art solutions on both odometry and dynamic trajectory recovery.



### Data-Driven Neuromorphic DRAM-based CNN and RNN Accelerators
- **Arxiv ID**: http://arxiv.org/abs/2003.13006v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE, I.5.5
- **Links**: [PDF](http://arxiv.org/pdf/2003.13006v1)
- **Published**: 2020-03-29 11:45:53+00:00
- **Updated**: 2020-03-29 11:45:53+00:00
- **Authors**: Tobi Delbruck, Shih-Chii Liu
- **Comment**: To appear in 2019 IEEE Sig. Proc. Soc. Asilomar Conference on
  Signals, Systems, and Computers Session MP6b: Neuromorphic Computing
  (Invited)
- **Journal**: None
- **Summary**: The energy consumed by running large deep neural networks (DNNs) on hardware accelerators is dominated by the need for lots of fast memory to store both states and weights. This large required memory is currently only economically viable through DRAM. Although DRAM is high-throughput and low-cost memory (costing 20X less than SRAM), its long random access latency is bad for the unpredictable access patterns in spiking neural networks (SNNs). In addition, accessing data from DRAM costs orders of magnitude more energy than doing arithmetic with that data. SNNs are energy-efficient if local memory is available and few spikes are generated. This paper reports on our developments over the last 5 years of convolutional and recurrent deep neural network hardware accelerators that exploit either spatial or temporal sparsity similar to SNNs but achieve SOA throughput, power efficiency and latency even with the use of DRAM for the required storage of the weights and states of large DNNs.



### Fast-MVSNet: Sparse-to-Dense Multi-View Stereo With Learned Propagation and Gauss-Newton Refinement
- **Arxiv ID**: http://arxiv.org/abs/2003.13017v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.13017v1)
- **Published**: 2020-03-29 13:31:00+00:00
- **Updated**: 2020-03-29 13:31:00+00:00
- **Authors**: Zehao Yu, Shenghua Gao
- **Comment**: Accepted by CVPR2020
- **Journal**: None
- **Summary**: Almost all previous deep learning-based multi-view stereo (MVS) approaches focus on improving reconstruction quality. Besides quality, efficiency is also a desirable feature for MVS in real scenarios. Towards this end, this paper presents a Fast-MVSNet, a novel sparse-to-dense coarse-to-fine framework, for fast and accurate depth estimation in MVS. Specifically, in our Fast-MVSNet, we first construct a sparse cost volume for learning a sparse and high-resolution depth map. Then we leverage a small-scale convolutional neural network to encode the depth dependencies for pixels within a local region to densify the sparse high-resolution depth map. At last, a simple but efficient Gauss-Newton layer is proposed to further optimize the depth map. On one hand, the high-resolution depth map, the data-adaptive propagation method and the Gauss-Newton layer jointly guarantee the effectiveness of our method. On the other hand, all modules in our Fast-MVSNet are lightweight and thus guarantee the efficiency of our approach. Besides, our approach is also memory-friendly because of the sparse depth representation. Extensive experimental results show that our method is 5$\times$ and 14$\times$ faster than Point-MVSNet and R-MVSNet, respectively, while achieving comparable or even better results on the challenging Tanks and Temples dataset as well as the DTU dataset. Code is available at https://github.com/svip-lab/FastMVSNet.



### Multi-Path Region Mining For Weakly Supervised 3D Semantic Segmentation on Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2003.13035v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.13035v1)
- **Published**: 2020-03-29 14:13:29+00:00
- **Updated**: 2020-03-29 14:13:29+00:00
- **Authors**: Jiacheng Wei, Guosheng Lin, Kim-Hui Yap, Tzu-Yi Hung, Lihua Xie
- **Comment**: Accepted by CVPR2020
- **Journal**: None
- **Summary**: Point clouds provide intrinsic geometric information and surface context for scene understanding. Existing methods for point cloud segmentation require a large amount of fully labeled data. Using advanced depth sensors, collection of large scale 3D dataset is no longer a cumbersome process. However, manually producing point-level label on the large scale dataset is time and labor-intensive. In this paper, we propose a weakly supervised approach to predict point-level results using weak labels on 3D point clouds. We introduce our multi-path region mining module to generate pseudo point-level label from a classification network trained with weak labels. It mines the localization cues for each class from various aspects of the network feature using different attention modules. Then, we use the point-level pseudo labels to train a point cloud segmentation network in a fully supervised manner. To the best of our knowledge, this is the first method that uses cloud-level weak labels on raw 3D space to train a point cloud semantic segmentation network. In our setting, the 3D weak labels only indicate the classes that appeared in our input sample. We discuss both scene- and subcloud-level weakly labels on raw 3D point cloud data and perform in-depth experiments on them. On ScanNet dataset, our result trained with subcloud-level labels is compatible with some fully supervised methods.



### Self-Supervised Learning for Domain Adaptation on Point-Clouds
- **Arxiv ID**: http://arxiv.org/abs/2003.12641v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.12641v5)
- **Published**: 2020-03-29 14:19:15+00:00
- **Updated**: 2022-05-13 11:45:58+00:00
- **Authors**: Idan Achituve, Haggai Maron, Gal Chechik
- **Comment**: None
- **Journal**: None
- **Summary**: Self-supervised learning (SSL) is a technique for learning useful representations from unlabeled data. It has been applied effectively to domain adaptation (DA) on images and videos. It is still unknown if and how it can be leveraged for domain adaptation in 3D perception problems. Here we describe the first study of SSL for DA on point clouds. We introduce a new family of pretext tasks, Deformation Reconstruction, inspired by the deformations encountered in sim-to-real transformations. In addition, we propose a novel training procedure for labeled point cloud data motivated by the MixUp method called Point cloud Mixup (PCM). Evaluations on domain adaptations datasets for classification and segmentation, demonstrate a large improvement over existing and baseline methods.



### Omni-sourced Webly-supervised Learning for Video Recognition
- **Arxiv ID**: http://arxiv.org/abs/2003.13042v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.13042v2)
- **Published**: 2020-03-29 14:47:31+00:00
- **Updated**: 2020-08-25 06:36:16+00:00
- **Authors**: Haodong Duan, Yue Zhao, Yuanjun Xiong, Wentao Liu, Dahua Lin
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce OmniSource, a novel framework for leveraging web data to train video recognition models. OmniSource overcomes the barriers between data formats, such as images, short videos, and long untrimmed videos for webly-supervised learning. First, data samples with multiple formats, curated by task-specific data collection and automatically filtered by a teacher model, are transformed into a unified form. Then a joint-training strategy is proposed to deal with the domain gaps between multiple data sources and formats in webly-supervised learning. Several good practices, including data balancing, resampling, and cross-dataset mixup are adopted in joint training. Experiments show that by utilizing data from multiple sources and formats, OmniSource is more data-efficient in training. With only 3.5M images and 800K minutes videos crawled from the internet without human labeling (less than 2% of prior works), our models learned with OmniSource improve Top-1 accuracy of 2D- and 3D-ConvNet baseline models by 3.0% and 3.9%, respectively, on the Kinetics-400 benchmark. With OmniSource, we establish new records with different pretraining strategies for video recognition. Our best models achieve 80.4%, 80.5%, and 83.6 Top-1 accuracies on the Kinetics-400 benchmark respectively for training-from-scratch, ImageNet pre-training and IG-65M pre-training.



### Noise Modeling, Synthesis and Classification for Generic Object Anti-Spoofing
- **Arxiv ID**: http://arxiv.org/abs/2003.13043v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.13043v2)
- **Published**: 2020-03-29 14:52:36+00:00
- **Updated**: 2020-03-31 16:15:59+00:00
- **Authors**: Joel Stehouwer, Amin Jourabloo, Yaojie Liu, Xiaoming Liu
- **Comment**: In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
  2020
- **Journal**: None
- **Summary**: Using printed photograph and replaying videos of biometric modalities, such as iris, fingerprint and face, are common attacks to fool the recognition systems for granting access as the genuine user. With the growing online person-to-person shopping (e.g., Ebay and Craigslist), such attacks also threaten those services, where the online photo illustration might not be captured from real items but from paper or digital screen. Thus, the study of anti-spoofing should be extended from modality-specific solutions to generic-object-based ones. In this work, we define and tackle the problem of Generic Object Anti-Spoofing (GOAS) for the first time. One significant cue to detect these attacks is the noise patterns introduced by the capture sensors and spoof mediums. Different sensor/medium combinations can result in diverse noise patterns. We propose a GAN-based architecture to synthesize and identify the noise patterns from seen and unseen medium/sensor combinations. We show that the procedure of synthesis and identification are mutually beneficial. We further demonstrate the learned GOAS models can directly contribute to modality-specific anti-spoofing without domain transfer. The code and GOSet dataset are available at cvlab.cse.msu.edu/project-goas.html.



### Learning by Analogy: Reliable Supervision from Transformations for Unsupervised Optical Flow Estimation
- **Arxiv ID**: http://arxiv.org/abs/2003.13045v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.13045v2)
- **Published**: 2020-03-29 14:55:24+00:00
- **Updated**: 2020-11-29 12:26:25+00:00
- **Authors**: Liang Liu, Jiangning Zhang, Ruifei He, Yong Liu, Yabiao Wang, Ying Tai, Donghao Luo, Chengjie Wang, Jilin Li, Feiyue Huang
- **Comment**: Accepted to CVPR 2020, https://github.com/lliuz/ARFlow
- **Journal**: None
- **Summary**: Unsupervised learning of optical flow, which leverages the supervision from view synthesis, has emerged as a promising alternative to supervised methods. However, the objective of unsupervised learning is likely to be unreliable in challenging scenes. In this work, we present a framework to use more reliable supervision from transformations. It simply twists the general unsupervised learning pipeline by running another forward pass with transformed data from augmentation, along with using transformed predictions of original data as the self-supervision signal. Besides, we further introduce a lightweight network with multiple frames by a highly-shared flow decoder. Our method consistently gets a leap of performance on several benchmarks with the best accuracy among deep unsupervised methods. Also, our method achieves competitive results to recent fully supervised methods while with much fewer parameters.



### Attentive CutMix: An Enhanced Data Augmentation Approach for Deep Learning Based Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2003.13048v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.13048v2)
- **Published**: 2020-03-29 15:01:05+00:00
- **Updated**: 2020-04-05 13:35:20+00:00
- **Authors**: Devesh Walawalkar, Zhiqiang Shen, Zechun Liu, Marios Savvides
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks (CNN) are capable of learning robust representation with different regularization methods and activations as convolutional layers are spatially correlated. Based on this property, a large variety of regional dropout strategies have been proposed, such as Cutout, DropBlock, CutMix, etc. These methods aim to promote the network to generalize better by partially occluding the discriminative parts of objects. However, all of them perform this operation randomly, without capturing the most important region(s) within an object. In this paper, we propose Attentive CutMix, a naturally enhanced augmentation strategy based on CutMix. In each training iteration, we choose the most descriptive regions based on the intermediate attention maps from a feature extractor, which enables searching for the most discriminative parts in an image. Our proposed method is simple yet effective, easy to implement and can boost the baseline significantly. Extensive experiments on CIFAR-10/100, ImageNet datasets with various CNN architectures (in a unified setting) demonstrate the effectiveness of our proposed method, which consistently outperforms the baseline CutMix and other methods by a significant margin.



### Deep Face Super-Resolution with Iterative Collaboration between Attentive Recovery and Landmark Estimation
- **Arxiv ID**: http://arxiv.org/abs/2003.13063v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.13063v1)
- **Published**: 2020-03-29 16:04:48+00:00
- **Updated**: 2020-03-29 16:04:48+00:00
- **Authors**: Cheng Ma, Zhenyu Jiang, Yongming Rao, Jiwen Lu, Jie Zhou
- **Comment**: Accepted to CVPR 2020
- **Journal**: None
- **Summary**: Recent works based on deep learning and facial priors have succeeded in super-resolving severely degraded facial images. However, the prior knowledge is not fully exploited in existing methods, since facial priors such as landmark and component maps are always estimated by low-resolution or coarsely super-resolved images, which may be inaccurate and thus affect the recovery performance. In this paper, we propose a deep face super-resolution (FSR) method with iterative collaboration between two recurrent networks which focus on facial image recovery and landmark estimation respectively. In each recurrent step, the recovery branch utilizes the prior knowledge of landmarks to yield higher-quality images which facilitate more accurate landmark estimation in turn. Therefore, the iterative information interaction between two processes boosts the performance of each other progressively. Moreover, a new attentive fusion module is designed to strengthen the guidance of landmark maps, where facial components are generated individually and aggregated attentively for better restoration. Quantitative and qualitative experimental results show the proposed method significantly outperforms state-of-the-art FSR methods in recovering high-quality face images.



### Structure-Preserving Super Resolution with Gradient Guidance
- **Arxiv ID**: http://arxiv.org/abs/2003.13081v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.13081v1)
- **Published**: 2020-03-29 17:26:58+00:00
- **Updated**: 2020-03-29 17:26:58+00:00
- **Authors**: Cheng Ma, Yongming Rao, Yean Cheng, Ce Chen, Jiwen Lu, Jie Zhou
- **Comment**: Accepted to CVPR 2020
- **Journal**: None
- **Summary**: Structures matter in single image super resolution (SISR). Recent studies benefiting from generative adversarial network (GAN) have promoted the development of SISR by recovering photo-realistic images. However, there are always undesired structural distortions in the recovered images. In this paper, we propose a structure-preserving super resolution method to alleviate the above issue while maintaining the merits of GAN-based methods to generate perceptual-pleasant details. Specifically, we exploit gradient maps of images to guide the recovery in two aspects. On the one hand, we restore high-resolution gradient maps by a gradient branch to provide additional structure priors for the SR process. On the other hand, we propose a gradient loss which imposes a second-order restriction on the super-resolved images. Along with the previous image-space loss functions, the gradient-space objectives help generative networks concentrate more on geometric structures. Moreover, our method is model-agnostic, which can be potentially used for off-the-shelf SR networks. Experimental results show that we achieve the best PI and LPIPS performance and meanwhile comparable PSNR and SSIM compared with state-of-the-art perceptual-driven SR methods. Visual results demonstrate our superiority in restoring structures while generating natural SR images.



### Generative Partial Multi-View Clustering
- **Arxiv ID**: http://arxiv.org/abs/2003.13088v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.13088v1)
- **Published**: 2020-03-29 17:48:27+00:00
- **Updated**: 2020-03-29 17:48:27+00:00
- **Authors**: Qianqian Wang, Zhengming Ding, Zhiqiang Tao, Quanxue Gao, Yun Fu
- **Comment**: This paper is an extension to our previous work: "Wang Q, Ding Z, Tao
  Z, et al. Partial multi-view clustering via consistent GAN[C]//2018 IEEE
  International Conference on Data Mining (ICDM). IEEE, 2018: 1290-1295."
- **Journal**: None
- **Summary**: Nowadays, with the rapid development of data collection sources and feature extraction methods, multi-view data are getting easy to obtain and have received increasing research attention in recent years, among which, multi-view clustering (MVC) forms a mainstream research direction and is widely used in data analysis. However, existing MVC methods mainly assume that each sample appears in all the views, without considering the incomplete view case due to data corruption, sensor failure, equipment malfunction, etc. In this study, we design and build a generative partial multi-view clustering model, named as GP-MVC, to address the incomplete multi-view problem by explicitly generating the data of missing views. The main idea of GP-MVC lies at two-fold. First, multi-view encoder networks are trained to learn common low-dimensional representations, followed by a clustering layer to capture the consistent cluster structure across multiple views. Second, view-specific generative adversarial networks are developed to generate the missing data of one view conditioning on the shared representation given by other views. These two steps could be promoted mutually, where learning common representations facilitates data imputation and the generated data could further explores the view consistency. Moreover, an weighted adaptive fusion scheme is implemented to exploit the complementary information among different views. Experimental results on four benchmark datasets are provided to show the effectiveness of the proposed GP-MVC over the state-of-the-art methods.



### Disturbance-immune Weight Sharing for Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2003.13089v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.13089v1)
- **Published**: 2020-03-29 17:54:49+00:00
- **Updated**: 2020-03-29 17:54:49+00:00
- **Authors**: Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yong Guo, Peilin Zhao, Junzhou Huang, Mingkui Tan
- **Comment**: None
- **Journal**: None
- **Summary**: Neural architecture search (NAS) has gained increasing attention in the community of architecture design. One of the key factors behind the success lies in the training efficiency created by the weight sharing (WS) technique. However, WS-based NAS methods often suffer from a performance disturbance (PD) issue. That is, the training of subsequent architectures inevitably disturbs the performance of previously trained architectures due to the partially shared weights. This leads to inaccurate performance estimation for the previous architectures, which makes it hard to learn a good search strategy. To alleviate the performance disturbance issue, we propose a new disturbance-immune update strategy for model updating. Specifically, to preserve the knowledge learned by previous architectures, we constrain the training of subsequent architectures in an orthogonal space via orthogonal gradient descent. Equipped with this strategy, we propose a novel disturbance-immune training scheme for NAS. We theoretically analyze the effectiveness of our strategy in alleviating the PD risk. Extensive experiments on CIFAR-10 and ImageNet verify the superiority of our method.



### High-Order Residual Network for Light Field Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2003.13094v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.13094v1)
- **Published**: 2020-03-29 18:06:05+00:00
- **Updated**: 2020-03-29 18:06:05+00:00
- **Authors**: Nan Meng, Xiaofei Wu, Jianzhuang Liu, Edmund Y. Lam
- **Comment**: 9 pages, 14 figures, accepted by the thirty-fourth AAAI Conference on
  Artificial Intelligence
- **Journal**: None
- **Summary**: Plenoptic cameras usually sacrifice the spatial resolution of their SAIs to acquire geometry information from different viewpoints. Several methods have been proposed to mitigate such spatio-angular trade-off, but seldom make use of the structural properties of the light field (LF) data efficiently. In this paper, we propose a novel high-order residual network to learn the geometric features hierarchically from the LF for reconstruction. An important component in the proposed network is the high-order residual block (HRB), which learns the local geometric features by considering the information from all input views. After fully obtaining the local features learned from each HRB, our model extracts the representative geometric features for spatio-angular upsampling through the global residual learning. Additionally, a refinement network is followed to further enhance the spatial details by minimizing a perceptual loss. Compared with previous work, our model is tailored to the rich structure inherent in the LF, and therefore can reduce the artifacts near non-Lambertian and occlusion regions. Experimental results show that our approach enables high-quality reconstruction even in challenging regions and outperforms state-of-the-art single image or LF reconstruction methods with both quantitative measurements and visual evaluation.



### Unsupervised Deep Learning for MR Angiography with Flexible Temporal Resolution
- **Arxiv ID**: http://arxiv.org/abs/2003.13096v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.13096v1)
- **Published**: 2020-03-29 18:08:59+00:00
- **Updated**: 2020-03-29 18:08:59+00:00
- **Authors**: Eunju Cha, Hyungjin Chung, Eung Yeop Kim, Jong Chul Ye
- **Comment**: None
- **Journal**: None
- **Summary**: Time-resolved MR angiography (tMRA) has been widely used for dynamic contrast enhanced MRI (DCE-MRI) due to its highly accelerated acquisition. In tMRA, the periphery of the k-space data are sparsely sampled so that neighbouring frames can be merged to construct one temporal frame. However, this view-sharing scheme fundamentally limits the temporal resolution, and it is not possible to change the view-sharing number to achieve different spatio-temporal resolution trade-off. Although many deep learning approaches have been recently proposed for MR reconstruction from sparse samples, the existing approaches usually require matched fully sampled k-space reference data for supervised training, which is not suitable for tMRA. This is because high spatio-temporal resolution ground-truth images are not available for tMRA. To address this problem, here we propose a novel unsupervised deep learning using optimal transport driven cycle-consistent generative adversarial network (cycleGAN). In contrast to the conventional cycleGAN with two pairs of generator and discriminator, the new architecture requires just a single pair of generator and discriminator, which makes the training much simpler and improves the performance. Reconstruction results using in vivo tMRA data set confirm that the proposed method can immediately generate high quality reconstruction results at various choices of view-sharing numbers, allowing us to exploit better trade-off between spatial and temporal resolution in time-resolved MR angiography.



### Proximity-Based Active Learning on Streaming Data: A Personalized Eating Moment Recognition
- **Arxiv ID**: http://arxiv.org/abs/2003.13098v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.13098v1)
- **Published**: 2020-03-29 18:17:29+00:00
- **Updated**: 2020-03-29 18:17:29+00:00
- **Authors**: Marjan Nourollahi, Seyed Ali Rokni, Hassan Ghasemzadeh
- **Comment**: None
- **Journal**: None
- **Summary**: Detecting when eating occurs is an essential step toward automatic dietary monitoring, medication adherence assessment, and diet-related health interventions. Wearable technologies play a central role in designing unubtrusive diet monitoring solutions by leveraging machine learning algorithms that work on time-series sensor data to detect eating moments. While much research has been done on developing activity recognition and eating moment detection algorithms, the performance of the detection algorithms drops substantially when the model trained with one user is utilized by a new user. To facilitate development of personalized models, we propose PALS, Proximity-based Active Learning on Streaming data, a novel proximity-based model for recognizing eating gestures with the goal of significantly decreasing the need for labeled data with new users. Particularly, we propose an optimization problem to perform active learning under limited query budget by leveraging unlabeled data. Our extensive analysis on data collected in both controlled and uncontrolled settings indicates that the F-score of PLAS ranges from 22% to 39% for a budget that varies from 10 to 60 query. Furthermore, compared to the state-of-the-art approaches, off-line PALS, on average, achieves to 40% higher recall and 12\% higher f-score in detecting eating gestures.



### Defect segmentation: Mapping tunnel lining internal defects with ground penetrating radar data using a convolutional neural network
- **Arxiv ID**: http://arxiv.org/abs/2003.13120v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, physics.geo-ph
- **Links**: [PDF](http://arxiv.org/pdf/2003.13120v1)
- **Published**: 2020-03-29 19:30:59+00:00
- **Updated**: 2020-03-29 19:30:59+00:00
- **Authors**: Senlin Yang, Zhengfang Wang, Jing Wang, Anthony G. Cohn, Jiaqi Zhang, Peng Jiang, Peng Jiang, Qingmei Sui
- **Comment**: 24 pages,11 figures
- **Journal**: None
- **Summary**: This research proposes a Ground Penetrating Radar (GPR) data processing method for non-destructive detection of tunnel lining internal defects, called defect segmentation. To perform this critical step of automatic tunnel lining detection, the method uses a CNN called Segnet combined with the Lov\'asz softmax loss function to map the internal defect structure with GPR synthetic data, which improves the accuracy, automation and efficiency of defects detection. The novel method we present overcomes several difficulties of traditional GPR data interpretation as demonstrated by an evaluation on both synthetic and real datas -- to verify the method on real data, a test model containing a known defect was designed and built and GPR data was obtained and analyzed.



### Detection of 3D Bounding Boxes of Vehicles Using Perspective Transformation for Accurate Speed Measurement
- **Arxiv ID**: http://arxiv.org/abs/2003.13137v2
- **DOI**: 10.1007/s00138-020-01117-x
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.13137v2)
- **Published**: 2020-03-29 21:01:25+00:00
- **Updated**: 2020-08-04 23:19:58+00:00
- **Authors**: Viktor Kocur, Milan Ftáčnik
- **Comment**: Submitted to Machine Vision and Applications
- **Journal**: None
- **Summary**: Detection and tracking of vehicles captured by traffic surveillance cameras is a key component of intelligent transportation systems. We present an improved version of our algorithm for detection of 3D bounding boxes of vehicles, their tracking and subsequent speed estimation. Our algorithm utilizes the known geometry of vanishing points in the surveilled scene to construct a perspective transformation. The transformation enables an intuitive simplification of the problem of detecting 3D bounding boxes to detection of 2D bounding boxes with one additional parameter using a standard 2D object detector. Main contribution of this paper is an improved construction of the perspective transformation which is more robust and fully automatic and an extended experimental evaluation of speed estimation. We test our algorithm on the speed estimation task of the BrnoCompSpeed dataset. We evaluate our approach with different configurations to gauge the relationship between accuracy and computational costs and benefits of 3D bounding box detection over 2D detection. All of the tested configurations run in real-time and are fully automatic. Compared to other published state-of-the-art fully automatic results our algorithm reduces the mean absolute speed measurement error by 32% (1.10 km/h to 0.75 km/h) and the absolute median error by 40% (0.97 km/h to 0.58 km/h).



### Learning a Weakly-Supervised Video Actor-Action Segmentation Model with a Wise Selection
- **Arxiv ID**: http://arxiv.org/abs/2003.13141v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.13141v1)
- **Published**: 2020-03-29 21:15:18+00:00
- **Updated**: 2020-03-29 21:15:18+00:00
- **Authors**: Jie Chen, Zhiheng Li, Jiebo Luo, Chenliang Xu
- **Comment**: 11 pages, 8 figures, cvpr-2020 supplementary video:
  https://youtu.be/CX1hEOV9tlo
- **Journal**: None
- **Summary**: We address weakly-supervised video actor-action segmentation (VAAS), which extends general video object segmentation (VOS) to additionally consider action labels of the actors. The most successful methods on VOS synthesize a pool of pseudo-annotations (PAs) and then refine them iteratively. However, they face challenges as to how to select from a massive amount of PAs high-quality ones, how to set an appropriate stop condition for weakly-supervised training, and how to initialize PAs pertaining to VAAS. To overcome these challenges, we propose a general Weakly-Supervised framework with a Wise Selection of training samples and model evaluation criterion (WS^2). Instead of blindly trusting quality-inconsistent PAs, WS^2 employs a learning-based selection to select effective PAs and a novel region integrity criterion as a stopping condition for weakly-supervised training. In addition, a 3D-Conv GCAM is devised to adapt to the VAAS task. Extensive experiments show that WS^2 achieves state-of-the-art performance on both weakly-supervised VOS and VAAS tasks and is on par with the best fully-supervised method on VAAS.



### Can AI help in screening Viral and COVID-19 pneumonia?
- **Arxiv ID**: http://arxiv.org/abs/2003.13145v3
- **DOI**: 10.1109/ACCESS.2020.3010287
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.13145v3)
- **Published**: 2020-03-29 21:37:21+00:00
- **Updated**: 2020-06-15 08:43:36+00:00
- **Authors**: Muhammad E. H. Chowdhury, Tawsifur Rahman, Amith Khandakar, Rashid Mazhar, Muhammad Abdul Kadir, Zaid Bin Mahbub, Khandaker Reajul Islam, Muhammad Salman Khan, Atif Iqbal, Nasser Al-Emadi, Mamun Bin Ibne Reaz, T. I. Islam
- **Comment**: 12 pages, 9 Figures
- **Journal**: IEEE Access 2020
- **Summary**: Coronavirus disease (COVID-19) is a pandemic disease, which has already caused thousands of causalities and infected several millions of people worldwide. Any technological tool enabling rapid screening of the COVID-19 infection with high accuracy can be crucially helpful to healthcare professionals. The main clinical tool currently in use for the diagnosis of COVID-19 is the Reverse transcription polymerase chain reaction (RT-PCR), which is expensive, less-sensitive and requires specialized medical personnel. X-ray imaging is an easily accessible tool that can be an excellent alternative in the COVID-19 diagnosis. This research was taken to investigate the utility of artificial intelligence (AI) in the rapid and accurate detection of COVID-19 from chest X-ray images. The aim of this paper is to propose a robust technique for automatic detection of COVID-19 pneumonia from digital chest X-ray images applying pre-trained deep-learning algorithms while maximizing the detection accuracy. A public database was created by the authors combining several public databases and also by collecting images from recently published articles. The database contains a mixture of 423 COVID-19, 1485 viral pneumonia, and 1579 normal chest X-ray images. Transfer learning technique was used with the help of image augmentation to train and validate several pre-trained deep Convolutional Neural Networks (CNNs). The networks were trained to classify two different schemes: i) normal and COVID-19 pneumonia; ii) normal, viral and COVID-19 pneumonia with and without image augmentation. The classification accuracy, precision, sensitivity, and specificity for both the schemes were 99.7%, 99.7%, 99.7% and 99.55% and 97.9%, 97.95%, 97.9%, and 98.8%, respectively.



### Learning Interactions and Relationships between Movie Characters
- **Arxiv ID**: http://arxiv.org/abs/2003.13158v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.13158v1)
- **Published**: 2020-03-29 23:11:24+00:00
- **Updated**: 2020-03-29 23:11:24+00:00
- **Authors**: Anna Kukleva, Makarand Tapaswi, Ivan Laptev
- **Comment**: CVPR 2020 (Oral)
- **Journal**: None
- **Summary**: Interactions between people are often governed by their relationships. On the flip side, social relationships are built upon several interactions. Two strangers are more likely to greet and introduce themselves while becoming friends over time. We are fascinated by this interplay between interactions and relationships, and believe that it is an important aspect of understanding social situations. In this work, we propose neural models to learn and jointly predict interactions, relationships, and the pair of characters that are involved. We note that interactions are informed by a mixture of visual and dialog cues, and present a multimodal architecture to extract meaningful information from them. Localizing the pair of interacting characters in video is a time-consuming process, instead, we train our model to learn from clip-level weak labels. We evaluate our models on the MovieGraphs dataset and show the impact of modalities, use of longer temporal context for predicting relationships, and achieve encouraging performance using weak labels as compared with ground-truth labels. Code is online.



