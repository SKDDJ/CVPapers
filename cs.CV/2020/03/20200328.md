# Arxiv Papers in cs.CV on 2020-03-28
### Semantic Implicit Neural Scene Representations With Semi-Supervised Training
- **Arxiv ID**: http://arxiv.org/abs/2003.12673v2
- **DOI**: None
- **Categories**: **cs.CV**, I.2.10; I.4.5; I.4.6; I.4.8; I.4.10
- **Links**: [PDF](http://arxiv.org/pdf/2003.12673v2)
- **Published**: 2020-03-28 00:43:17+00:00
- **Updated**: 2021-01-17 01:53:26+00:00
- **Authors**: Amit Kohli, Vincent Sitzmann, Gordon Wetzstein
- **Comment**: 3DV 2020 Camera Ready
  https://www.computationalimaging.org/publications/
- **Journal**: None
- **Summary**: The recent success of implicit neural scene representations has presented a viable new method for how we capture and store 3D scenes. Unlike conventional 3D representations, such as point clouds, which explicitly store scene properties in discrete, localized units, these implicit representations encode a scene in the weights of a neural network which can be queried at any coordinate to produce these same scene properties. Thus far, implicit representations have primarily been optimized to estimate only the appearance and/or 3D geometry information in a scene. We take the next step and demonstrate that an existing implicit representation (SRNs) is actually multi-modal; it can be further leveraged to perform per-point semantic segmentation while retaining its ability to represent appearance and geometry. To achieve this multi-modal behavior, we utilize a semi-supervised learning strategy atop the existing pre-trained scene representation. Our method is simple, general, and only requires a few tens of labeled 2D segmentation masks in order to achieve dense 3D semantic segmentation. We explore two novel applications for this semantically aware implicit neural scene representation: 3D novel view and semantic label synthesis given only a single input RGB image or 2D label mask, as well as 3D interpolation of appearance and semantics.



### Using the Split Bregman Algorithm to Solve the Self-repelling Snake Model
- **Arxiv ID**: http://arxiv.org/abs/2003.12693v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.12693v2)
- **Published**: 2020-03-28 03:41:47+00:00
- **Updated**: 2021-02-19 02:59:58+00:00
- **Authors**: Huizhu Pan, Jintao Song, Wanquan Liu, Ling Li, Guanglu Zhou, Lu Tan, Shichu Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Preserving contour topology during image segmentation is useful in many practical scenarios. By keeping the contours isomorphic, it is possible to prevent over-segmentation and under-segmentation, as well as to adhere to given topologies. The Self-repelling Snake model (SR) is a variational model that preserves contour topology by combining a non-local repulsion term with the geodesic active contour model (GAC). The SR is traditionally solved using the additive operator splitting (AOS) scheme. In our paper, we propose an alternative solution to the SR using the Split Bregman method. Our algorithm breaks the problem down into simpler sub-problems to use lower-order evolution equations and a simple projection scheme rather than re-initialization. The sub-problems can be solved via fast Fourier transform (FFT) or an approximate soft thresholding formula which maintains stability, shortening the convergence time, and reduces the memory requirement. The Split Bregman and AOS algorithms are compared theoretically and experimentally.



### Semantically Multi-modal Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2003.12697v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.12697v3)
- **Published**: 2020-03-28 04:03:46+00:00
- **Updated**: 2020-04-02 09:07:29+00:00
- **Authors**: Zhen Zhu, Zhiliang Xu, Ansheng You, Xiang Bai
- **Comment**: To appear in CVPR 2020
- **Journal**: None
- **Summary**: In this paper, we focus on semantically multi-modal image synthesis (SMIS) task, namely, generating multi-modal images at the semantic level. Previous work seeks to use multiple class-specific generators, constraining its usage in datasets with a small number of classes. We instead propose a novel Group Decreasing Network (GroupDNet) that leverages group convolutions in the generator and progressively decreases the group numbers of the convolutions in the decoder. Consequently, GroupDNet is armed with much more controllability on translating semantic labels to natural images and has plausible high-quality yields for datasets with many classes. Experiments on several challenging datasets demonstrate the superiority of GroupDNet on performing the SMIS task. We also show that GroupDNet is capable of performing a wide range of interesting synthesis applications. Codes and models are available at: https://github.com/Seanseattle/SMIS.



### Learning Dense Visual Correspondences in Simulation to Smooth and Fold Real Fabrics
- **Arxiv ID**: http://arxiv.org/abs/2003.12698v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.12698v2)
- **Published**: 2020-03-28 04:06:20+00:00
- **Updated**: 2020-11-12 01:01:59+00:00
- **Authors**: Aditya Ganapathi, Priya Sundaresan, Brijen Thananjeyan, Ashwin Balakrishna, Daniel Seita, Jennifer Grannen, Minho Hwang, Ryan Hoque, Joseph E. Gonzalez, Nawid Jamali, Katsu Yamane, Soshi Iba, Ken Goldberg
- **Comment**: None
- **Journal**: None
- **Summary**: Robotic fabric manipulation is challenging due to the infinite dimensional configuration space, self-occlusion, and complex dynamics of fabrics. There has been significant prior work on learning policies for specific deformable manipulation tasks, but comparatively less focus on algorithms which can efficiently learn many different tasks. In this paper, we learn visual correspondences for deformable fabrics across different configurations in simulation and show that this representation can be used to design policies for a variety of tasks. Given a single demonstration of a new task from an initial fabric configuration, the learned correspondences can be used to compute geometrically equivalent actions in a new fabric configuration. This makes it possible to robustly imitate a broad set of multi-step fabric smoothing and folding tasks on multiple physical robotic systems. The resulting policies achieve 80.3% average task success rate across 10 fabric manipulation tasks on two different robotic systems, the da Vinci surgical robot and the ABB YuMi. Results also suggest robustness to fabrics of various colors, sizes, and shapes. See https://tinyurl.com/fabric-descriptors for supplementary material and videos.



### DaST: Data-free Substitute Training for Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/2003.12703v2
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.12703v2)
- **Published**: 2020-03-28 04:28:13+00:00
- **Updated**: 2020-03-31 15:25:06+00:00
- **Authors**: Mingyi Zhou, Jing Wu, Yipeng Liu, Shuaicheng Liu, Ce Zhu
- **Comment**: Accepted by CVPR2020
- **Journal**: None
- **Summary**: Machine learning models are vulnerable to adversarial examples. For the black-box setting, current substitute attacks need pre-trained models to generate adversarial examples. However, pre-trained models are hard to obtain in real-world tasks. In this paper, we propose a data-free substitute training method (DaST) to obtain substitute models for adversarial black-box attacks without the requirement of any real data. To achieve this, DaST utilizes specially designed generative adversarial networks (GANs) to train the substitute models. In particular, we design a multi-branch architecture and label-control loss for the generative model to deal with the uneven distribution of synthetic samples. The substitute model is then trained by the synthetic samples generated by the generative model, which are labeled by the attacked model subsequently. The experiments demonstrate the substitute models produced by DaST can achieve competitive performance compared with the baseline models which are trained by the same train set with attacked models. Additionally, to evaluate the practicability of the proposed method on the real-world task, we attack an online machine learning model on the Microsoft Azure platform. The remote model misclassifies 98.35% of the adversarial examples crafted by our method. To the best of our knowledge, we are the first to train a substitute model for adversarial attacks without any real data.



### Multiform Fonts-to-Fonts Translation via Style and Content Disentangled Representations of Chinese Character
- **Arxiv ID**: http://arxiv.org/abs/2004.03338v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.03338v1)
- **Published**: 2020-03-28 04:30:00+00:00
- **Updated**: 2020-03-28 04:30:00+00:00
- **Authors**: Fenxi Xiao, Jie Zhang, Bo Huang, Xia Wu
- **Comment**: None
- **Journal**: None
- **Summary**: This paper mainly discusses the generation of personalized fonts as the problem of image style transfer. The main purpose of this paper is to design a network framework that can extract and recombine the content and style of the characters. These attempts can be used to synthesize the entire set of fonts with only a small amount of characters. The paper combines various depth networks such as Convolutional Neural Network, Multi-layer Perceptron and Residual Network to find the optimal model to extract the features of the fonts character. The result shows that those characters we have generated is very close to real characters, using Structural Similarity index and Peak Signal-to-Noise Ratio evaluation criterions.



### Predicting the Popularity of Micro-videos with Multimodal Variational Encoder-Decoder Framework
- **Arxiv ID**: http://arxiv.org/abs/2003.12724v1
- **DOI**: 10.1109/TMM.2021.3120537
- **Categories**: **cs.LG**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.12724v1)
- **Published**: 2020-03-28 06:08:16+00:00
- **Updated**: 2020-03-28 06:08:16+00:00
- **Authors**: Yaochen Zhu, Jiayi Xie, Zhenzhong Chen
- **Comment**: None
- **Journal**: None
- **Summary**: As an emerging type of user-generated content, micro-video drastically enriches people's entertainment experiences and social interactions. However, the popularity pattern of an individual micro-video still remains elusive among the researchers. One of the major challenges is that the potential popularity of a micro-video tends to fluctuate under the impact of various external factors, which makes it full of uncertainties. In addition, since micro-videos are mainly uploaded by individuals that lack professional techniques, multiple types of noise could exist that obscure useful information. In this paper, we propose a multimodal variational encoder-decoder (MMVED) framework for micro-video popularity prediction tasks. MMVED learns a stochastic Gaussian embedding of a micro-video that is informative to its popularity level while preserves the inherent uncertainties simultaneously. Moreover, through the optimization of a deep variational information bottleneck lower-bound (IBLBO), the learned hidden representation is shown to be maximally expressive about the popularity target while maximally compressive to the noise in micro-video features. Furthermore, the Bayesian product-of-experts principle is applied to the multimodal encoder, where the decision for information keeping or discarding is made comprehensively with all available modalities. Extensive experiments conducted on a public dataset and a dataset we collect from Xigua demonstrate the effectiveness of the proposed MMVED framework.



### NMS by Representative Region: Towards Crowded Pedestrian Detection by Proposal Pairing
- **Arxiv ID**: http://arxiv.org/abs/2003.12729v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.12729v2)
- **Published**: 2020-03-28 06:33:54+00:00
- **Updated**: 2020-04-21 09:05:54+00:00
- **Authors**: Xin Huang, Zheng Ge, Zequn Jie, Osamu Yoshie
- **Comment**: Accepted by CVPR2020. The first two authors contributed equally, and
  are listed in alphabetical order
- **Journal**: None
- **Summary**: Although significant progress has been made in pedestrian detection recently, pedestrian detection in crowded scenes is still challenging. The heavy occlusion between pedestrians imposes great challenges to the standard Non-Maximum Suppression (NMS). A relative low threshold of intersection over union (IoU) leads to missing highly overlapped pedestrians, while a higher one brings in plenty of false positives. To avoid such a dilemma, this paper proposes a novel Representative Region NMS approach leveraging the less occluded visible parts, effectively removing the redundant boxes without bringing in many false positives. To acquire the visible parts, a novel Paired-Box Model (PBM) is proposed to simultaneously predict the full and visible boxes of a pedestrian. The full and visible boxes constitute a pair serving as the sample unit of the model, thus guaranteeing a strong correspondence between the two boxes throughout the detection pipeline. Moreover, convenient feature integration of the two boxes is allowed for the better performance on both full and visible pedestrian detection tasks. Experiments on the challenging CrowdHuman and CityPersons benchmarks sufficiently validate the effectiveness of the proposed approach on pedestrian detection in the crowded situation.



### Exploit Clues from Views: Self-Supervised and Regularized Learning for Multiview Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/2003.12735v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.12735v1)
- **Published**: 2020-03-28 07:06:06+00:00
- **Updated**: 2020-03-28 07:06:06+00:00
- **Authors**: Chih-Hui Ho, Bo Liu, Tz-Ying Wu, Nuno Vasconcelos
- **Comment**: Accepted to CVPR2020
- **Journal**: None
- **Summary**: Multiview recognition has been well studied in the literature and achieves decent performance in object recognition and retrieval task. However, most previous works rely on supervised learning and some impractical underlying assumptions, such as the availability of all views in training and inference time. In this work, the problem of multiview self-supervised learning (MV-SSL) is investigated, where only image to object association is given. Given this setup, a novel surrogate task for self-supervised learning is proposed by pursuing "object invariant" representation. This is solved by randomly selecting an image feature of an object as object prototype, accompanied with multiview consistency regularization, which results in view invariant stochastic prototype embedding (VISPE). Experiments shows that the recognition and retrieval results using VISPE outperform that of other self-supervised learning methods on seen and unseen data. VISPE can also be applied to semi-supervised scenario and demonstrates robust performance with limited data available. Code is available at https://github.com/chihhuiho/VISPE



### Actor-Transformers for Group Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/2003.12737v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.12737v1)
- **Published**: 2020-03-28 07:21:58+00:00
- **Updated**: 2020-03-28 07:21:58+00:00
- **Authors**: Kirill Gavrilyuk, Ryan Sanford, Mehrsan Javan, Cees G. M. Snoek
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: This paper strives to recognize individual actions and group activities from videos. While existing solutions for this challenging problem explicitly model spatial and temporal relationships based on location of individual actors, we propose an actor-transformer model able to learn and selectively extract information relevant for group activity recognition. We feed the transformer with rich actor-specific static and dynamic representations expressed by features from a 2D pose network and 3D CNN, respectively. We empirically study different ways to combine these representations and show their complementary benefits. Experiments show what is important to transform and how it should be transformed. What is more, actor-transformers achieve state-of-the-art results on two publicly available benchmarks for group activity recognition, outperforming the previous best published results by a considerable margin.



### Modulating Bottom-Up and Top-Down Visual Processing via Language-Conditional Filters
- **Arxiv ID**: http://arxiv.org/abs/2003.12739v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.12739v3)
- **Published**: 2020-03-28 07:54:03+00:00
- **Updated**: 2022-06-23 14:02:40+00:00
- **Authors**: İlker Kesen, Ozan Arkan Can, Erkut Erdem, Aykut Erdem, Deniz Yuret
- **Comment**: 13 pages, 6 figures, 6 tables. Appeared in MULA Workshop at CVPR 2022
- **Journal**: Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR) Workshops, 2022, pp. 4610-4620
- **Summary**: How to best integrate linguistic and perceptual processing in multi-modal tasks that involve language and vision is an important open problem. In this work, we argue that the common practice of using language in a top-down manner, to direct visual attention over high-level visual features, may not be optimal. We hypothesize that the use of language to also condition the bottom-up processing from pixels to high-level features can provide benefits to the overall performance. To support our claim, we propose a U-Net-based model and perform experiments on two language-vision dense-prediction tasks: referring expression segmentation and language-guided image colorization. We compare results where either one or both of the top-down and bottom-up visual branches are conditioned on language. Our experiments reveal that using language to control the filters for bottom-up visual processing in addition to top-down attention leads to better results on both tasks and achieves competitive performance. Our linguistic analysis suggests that bottom-up conditioning improves segmentation of objects especially when input text refers to low-level visual concepts. Code is available at https://github.com/ilkerkesen/bvpr.



### A Physics-based Noise Formation Model for Extreme Low-light Raw Denoising
- **Arxiv ID**: http://arxiv.org/abs/2003.12751v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.12751v2)
- **Published**: 2020-03-28 09:16:48+00:00
- **Updated**: 2020-04-09 04:58:43+00:00
- **Authors**: Kaixuan Wei, Ying Fu, Jiaolong Yang, Hua Huang
- **Comment**: Accepted to CVPR 2020 (oral); code is available at
  https://github.com/Vandermode/NoiseModel
- **Journal**: None
- **Summary**: Lacking rich and realistic data, learned single image denoising algorithms generalize poorly to real raw images that do not resemble the data used for training. Although the problem can be alleviated by the heteroscedastic Gaussian model for noise synthesis, the noise sources caused by digital camera electronics are still largely overlooked, despite their significant effect on raw measurement, especially under extremely low-light condition. To address this issue, we present a highly accurate noise formation model based on the characteristics of CMOS photosensors, thereby enabling us to synthesize realistic samples that better match the physics of image formation process. Given the proposed noise model, we additionally propose a method to calibrate the noise parameters for available modern digital cameras, which is simple and reproducible for any new device. We systematically study the generalizability of a neural network trained with existing schemes, by introducing a new low-light denoising dataset that covers many modern digital cameras from diverse brands. Extensive empirical results collectively show that by utilizing our proposed noise formation model, a network can reach the capability as if it had been trained with rich real data, which demonstrates the effectiveness of our noise formation model.



### Deep Fashion3D: A Dataset and Benchmark for 3D Garment Reconstruction from Single Images
- **Arxiv ID**: http://arxiv.org/abs/2003.12753v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.12753v2)
- **Published**: 2020-03-28 09:20:04+00:00
- **Updated**: 2020-07-04 12:43:49+00:00
- **Authors**: Heming Zhu, Yu Cao, Hang Jin, Weikai Chen, Dong Du, Zhangye Wang, Shuguang Cui, Xiaoguang Han
- **Comment**: 23 pages, 9 figures. For project page, see
  https://kv2000.github.io/2020/03/25/deepFashion3DRevisited/
- **Journal**: None
- **Summary**: High-fidelity clothing reconstruction is the key to achieving photorealism in a wide range of applications including human digitization, virtual try-on, etc. Recent advances in learning-based approaches have accomplished unprecedented accuracy in recovering unclothed human shape and pose from single images, thanks to the availability of powerful statistical models, e.g. SMPL, learned from a large number of body scans. In contrast, modeling and recovering clothed human and 3D garments remains notoriously difficult, mostly due to the lack of large-scale clothing models available for the research community. We propose to fill this gap by introducing Deep Fashion3D, the largest collection to date of 3D garment models, with the goal of establishing a novel benchmark and dataset for the evaluation of image-based garment reconstruction systems. Deep Fashion3D contains 2078 models reconstructed from real garments, which covers 10 different categories and 563 garment instances. It provides rich annotations including 3D feature lines, 3D body pose and the corresponded multi-view real images. In addition, each garment is randomly posed to enhance the variety of real clothing deformations. To demonstrate the advantage of Deep Fashion3D, we propose a novel baseline approach for single-view garment reconstruction, which leverages the merits of both mesh and implicit representations. A novel adaptable template is proposed to enable the learning of all types of clothing in a single network. Extensive experiments have been conducted on the proposed dataset to verify its significance and usefulness. We will make Deep Fashion3D publicly available upon publication.



### Adversarial Imitation Attack
- **Arxiv ID**: http://arxiv.org/abs/2003.12760v2
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.12760v2)
- **Published**: 2020-03-28 10:02:49+00:00
- **Updated**: 2020-03-31 05:10:40+00:00
- **Authors**: Mingyi Zhou, Jing Wu, Yipeng Liu, Xiaolin Huang, Shuaicheng Liu, Xiang Zhang, Ce Zhu
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: Deep learning models are known to be vulnerable to adversarial examples. A practical adversarial attack should require as little as possible knowledge of attacked models. Current substitute attacks need pre-trained models to generate adversarial examples and their attack success rates heavily rely on the transferability of adversarial examples. Current score-based and decision-based attacks require lots of queries for the attacked models. In this study, we propose a novel adversarial imitation attack. First, it produces a replica of the attacked model by a two-player game like the generative adversarial networks (GANs). The objective of the generative model is to generate examples that lead the imitation model returning different outputs with the attacked model. The objective of the imitation model is to output the same labels with the attacked model under the same inputs. Then, the adversarial examples generated by the imitation model are utilized to fool the attacked model. Compared with the current substitute attacks, imitation attacks can use less training data to produce a replica of the attacked model and improve the transferability of adversarial examples. Experiments demonstrate that our imitation attack requires less training data than the black-box substitute attacks, but achieves an attack success rate close to the white-box attack on unseen data with no query.



### Trajectory Poisson multi-Bernoulli filters
- **Arxiv ID**: http://arxiv.org/abs/2003.12767v3
- **DOI**: 10.1109/TSP.2020.3017046
- **Categories**: **cs.CV**, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/2003.12767v3)
- **Published**: 2020-03-28 11:04:45+00:00
- **Updated**: 2020-09-17 12:59:16+00:00
- **Authors**: Ángel F. García-Fernández, Lennart Svensson, Jason L. Williams, Yuxuan Xia, Karl Granström
- **Comment**: Matlab code is provided at https://github.com/Agarciafernandez/MTT
- **Journal**: in IEEE Transactions on Signal Processing, vol. 68, pp. 4933-4945,
  2020
- **Summary**: This paper presents two trajectory Poisson multi-Bernoulli (TPMB) filters for multi-target tracking: one to estimate the set of alive trajectories at each time step and another to estimate the set of all trajectories, which includes alive and dead trajectories, at each time step. The filters are based on propagating a Poisson multi-Bernoulli (PMB) density on the corresponding set of trajectories through the filtering recursion. After the update step, the posterior is a PMB mixture (PMBM) so, in order to obtain a PMB density, a Kullback-Leibler divergence minimisation on an augmented space is performed. The developed filters are computationally lighter alternatives to the trajectory PMBM filters, which provide the closed-form recursion for sets of trajectories with Poisson birth model, and are shown to outperform previous multi-target tracking algorithms.



### Learning Invariant Representation for Unsupervised Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/2003.12769v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.12769v1)
- **Published**: 2020-03-28 11:20:21+00:00
- **Updated**: 2020-03-28 11:20:21+00:00
- **Authors**: Wenchao Du, Hu Chen, Hongyu Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, cross domain transfer has been applied for unsupervised image restoration tasks. However, directly applying existing frameworks would lead to domain-shift problems in translated images due to lack of effective supervision. Instead, we propose an unsupervised learning method that explicitly learns invariant presentation from noisy data and reconstructs clear observations. To do so, we introduce discrete disentangling representation and adversarial domain adaption into general domain transfer framework, aided by extra self-supervised modules including background and semantic consistency constraints, learning robust representation under dual domain constraints, such as feature and image domains. Experiments on synthetic and real noise removal tasks show the proposed method achieves comparable performance with other state-of-the-art supervised and unsupervised methods, while having faster and stable convergence than other domain adaption methods.



### Real-MFF: A Large Realistic Multi-focus Image Dataset with Ground Truth
- **Arxiv ID**: http://arxiv.org/abs/2003.12779v3
- **DOI**: 10.1016/j.patrec.2020.08.002
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.12779v3)
- **Published**: 2020-03-28 12:33:46+00:00
- **Updated**: 2020-08-28 11:25:18+00:00
- **Authors**: Juncheng Zhang, Qingmin Liao, Shaojun Liu, Haoyu Ma, Wenming Yang, Jing-Hao Xue
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-focus image fusion, a technique to generate an all-in-focus image from two or more partially-focused source images, can benefit many computer vision tasks. However, currently there is no large and realistic dataset to perform convincing evaluation and comparison of algorithms in multi-focus image fusion. Moreover, it is difficult to train a deep neural network for multi-focus image fusion without a suitable dataset. In this letter, we introduce a large and realistic multi-focus dataset called Real-MFF, which contains 710 pairs of source images with corresponding ground truth images. The dataset is generated by light field images, and both the source images and the ground truth images are realistic. To serve as both a well-established benchmark for existing multi-focus image fusion algorithms and an appropriate training dataset for future development of deep-learning-based methods, the dataset contains a variety of scenes, including buildings, plants, humans, shopping malls, squares and so on. We also evaluate 10 typical multi-focus algorithms on this dataset for the purpose of illustration.



### CNN-based Density Estimation and Crowd Counting: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2003.12783v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.12783v1)
- **Published**: 2020-03-28 13:17:30+00:00
- **Updated**: 2020-03-28 13:17:30+00:00
- **Authors**: Guangshuai Gao, Junyu Gao, Qingjie Liu, Qi Wang, Yunhong Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Accurately estimating the number of objects in a single image is a challenging yet meaningful task and has been applied in many applications such as urban planning and public safety. In the various object counting tasks, crowd counting is particularly prominent due to its specific significance to social security and development. Fortunately, the development of the techniques for crowd counting can be generalized to other related fields such as vehicle counting and environment survey, if without taking their characteristics into account. Therefore, many researchers are devoting to crowd counting, and many excellent works of literature and works have spurted out. In these works, they are must be helpful for the development of crowd counting. However, the question we should consider is why they are effective for this task. Limited by the cost of time and energy, we cannot analyze all the algorithms. In this paper, we have surveyed over 220 works to comprehensively and systematically study the crowd counting models, mainly CNN-based density map estimation methods. Finally, according to the evaluation metrics, we select the top three performers on their crowd counting datasets and analyze their merits and drawbacks. Through our analysis, we expect to make reasonable inference and prediction for the future development of crowd counting, and meanwhile, it can also provide feasible solutions for the problem of object counting in other fields. We provide the density maps and prediction results of some mainstream algorithm in the validation set of NWPU dataset for comparison and testing. Meanwhile, density map generation and evaluation tools are also provided. All the codes and evaluation results are made publicly available at https://github.com/gaoguangshuai/survey-for-crowd-counting.



### Polarized Reflection Removal with Perfect Alignment in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2003.12789v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.12789v1)
- **Published**: 2020-03-28 13:29:31+00:00
- **Updated**: 2020-03-28 13:29:31+00:00
- **Authors**: Chenyang Lei, Xuhua Huang, Mengdi Zhang, Qiong Yan, Wenxiu Sun, Qifeng Chen
- **Comment**: CVPR2020
- **Journal**: None
- **Summary**: We present a novel formulation to removing reflection from polarized images in the wild. We first identify the misalignment issues of existing reflection removal datasets where the collected reflection-free images are not perfectly aligned with input mixed images due to glass refraction. Then we build a new dataset with more than 100 types of glass in which obtained transmission images are perfectly aligned with input mixed images. Second, capitalizing on the special relationship between reflection and polarized light, we propose a polarized reflection removal model with a two-stage architecture. In addition, we design a novel perceptual NCC loss that can improve the performance of reflection removal and general image decomposition tasks. We conduct extensive experiments, and results suggest that our model outperforms state-of-the-art methods on reflection removal.



### CAKES: Channel-wise Automatic KErnel Shrinking for Efficient 3D Networks
- **Arxiv ID**: http://arxiv.org/abs/2003.12798v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.12798v3)
- **Published**: 2020-03-28 14:21:12+00:00
- **Updated**: 2020-12-16 19:03:30+00:00
- **Authors**: Qihang Yu, Yingwei Li, Jieru Mei, Yuyin Zhou, Alan L. Yuille
- **Comment**: AAAI 2021
- **Journal**: None
- **Summary**: 3D Convolution Neural Networks (CNNs) have been widely applied to 3D scene understanding, such as video analysis and volumetric image recognition. However, 3D networks can easily lead to over-parameterization which incurs expensive computation cost. In this paper, we propose Channel-wise Automatic KErnel Shrinking (CAKES), to enable efficient 3D learning by shrinking standard 3D convolutions into a set of economic operations e.g., 1D, 2D convolutions. Unlike previous methods, CAKES performs channel-wise kernel shrinkage, which enjoys the following benefits: 1) enabling operations deployed in every layer to be heterogeneous, so that they can extract diverse and complementary information to benefit the learning process; and 2) allowing for an efficient and flexible replacement design, which can be generalized to both spatial-temporal and volumetric data. Further, we propose a new search space based on CAKES, so that the replacement configuration can be determined automatically for simplifying 3D networks. CAKES shows superior performance to other methods with similar model size, and it also achieves comparable performance to state-of-the-art with much fewer parameters and computational costs on tasks including 3D medical imaging segmentation and video action recognition. Codes and models are available at https://github.com/yucornetto/CAKES



### Gradient-based Data Augmentation for Semi-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2003.12824v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.12824v2)
- **Published**: 2020-03-28 15:57:20+00:00
- **Updated**: 2020-04-02 01:10:27+00:00
- **Authors**: Hiroshi Kaizuka
- **Comment**: The lower bound of the inequality (line 2 on page 6 ) changed to fit
  fact 1 (2). Typos in (9) corrected
- **Journal**: None
- **Summary**: In semi-supervised learning (SSL), a technique called consistency regularization (CR) achieves high performance. It has been proved that the diversity of data used in CR is extremely important to obtain a model with high discrimination performance by CR. We propose a new data augmentation (Gradient-based Data Augmentation (GDA)) that is deterministically calculated from the image pixel value gradient of the posterior probability distribution that is the model output. We aim to secure effective data diversity for CR by utilizing three types of GDA. On the other hand, it has been demonstrated that the mixup method for labeled data and unlabeled data is also effective in SSL. We propose an SSL method named MixGDA by combining various mixup methods and GDA. The discrimination performance achieved by MixGDA is evaluated against the 13-layer CNN that is used as standard in SSL research. As a result, for CIFAR-10 (4000 labels), MixGDA achieves the same level of performance as the best performance ever achieved. For SVHN (250 labels, 500 labels and 1000 labels) and CIFAR-100 (10000 labels), MixGDA achieves state-of-the-art performance.



### An End-to-End Approach for Recognition of Modern and Historical Handwritten Numeral Strings
- **Arxiv ID**: http://arxiv.org/abs/2004.03337v1
- **DOI**: 10.1109/IJCNN48605.2020.9207468
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.03337v1)
- **Published**: 2020-03-28 16:51:00+00:00
- **Updated**: 2020-03-28 16:51:00+00:00
- **Authors**: Andre G. Hochuli, Alceu S. Britto Jr., Jean P. Barddal, Luiz E. S. Oliveira, Robert Sabourin
- **Comment**: None
- **Journal**: None
- **Summary**: An end-to-end solution for handwritten numeral string recognition is proposed, in which the numeral string is considered as composed of objects automatically detected and recognized by a YoLo-based model. The main contribution of this paper is to avoid heuristic-based methods for string preprocessing and segmentation, the need for task-oriented classifiers, and also the use of specific constraints related to the string length. A robust experimental protocol based on several numeral string datasets, including one composed of historical documents, has shown that the proposed method is a feasible end-to-end solution for numeral string recognition. Besides, it reduces the complexity of the string recognition task considerably since it drops out classical steps, in special preprocessing, segmentation, and a set of classifiers devoted to strings with a specific length.



### A Benchmark for Point Clouds Registration Algorithms
- **Arxiv ID**: http://arxiv.org/abs/2003.12841v3
- **DOI**: 10.1016/j.robot.2021.103734
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.12841v3)
- **Published**: 2020-03-28 17:02:26+00:00
- **Updated**: 2022-04-26 12:23:52+00:00
- **Authors**: Simone Fontana, Daniele Cattaneo, Augusto Luis Ballardini, Matteo Vaghi, Domenico Giorgio Sorrenti
- **Comment**: None
- **Journal**: Robotics and Autonomous Systems, 2021, 140: 103734
- **Summary**: Point clouds registration is a fundamental step of many point clouds processing pipelines; however, most algorithms are tested on data that are collected ad-hoc and not shared with the research community. These data often cover only a very limited set of use cases; therefore, the results cannot be generalised. Public datasets proposed until now, taken individually, cover only a few kinds of environment and mostly a single sensor. For these reasons, we developed a benchmark, for localization and mapping applications, using multiple publicly available datasets. In this way, we are able to cover many kinds of environment and many kinds of sensor that can produce point clouds. Furthermore, the ground truth has been thoroughly inspected and evaluated to ensure its quality. For some of the datasets, the accuracy of the ground truth measuring system was not reported by the original authors, therefore we estimated it with our own novel method, based on an iterative registration algorithm. Along with the data, we provide a broad set of registration problems, chosen to cover different types of initial misalignment, various degrees of overlap, and different kinds of registration problems. Lastly, we propose a metric to measure the performances of registration algorithms: it combines the commonly used rotation and translation errors together, to allow an objective comparison of the alignments. This work aims at encouraging authors to use a public and shared benchmark, instead of data collected ad-hoc, to ensure objectivity and repeatability, two fundamental characteristics in any scientific field.



### Cross-domain Detection via Graph-induced Prototype Alignment
- **Arxiv ID**: http://arxiv.org/abs/2003.12849v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.12849v1)
- **Published**: 2020-03-28 17:46:55+00:00
- **Updated**: 2020-03-28 17:46:55+00:00
- **Authors**: Minghao Xu, Hang Wang, Bingbing Ni, Qi Tian, Wenjun Zhang
- **Comment**: Accepted as ORAL presentation at IEEE Conference on Computer Vision
  and Pattern Recognition (CVPR), 2020
- **Journal**: None
- **Summary**: Applying the knowledge of an object detector trained on a specific domain directly onto a new domain is risky, as the gap between two domains can severely degrade model's performance. Furthermore, since different instances commonly embody distinct modal information in object detection scenario, the feature alignment of source and target domain is hard to be realized. To mitigate these problems, we propose a Graph-induced Prototype Alignment (GPA) framework to seek for category-level domain alignment via elaborate prototype representations. In the nutshell, more precise instance-level features are obtained through graph-based information propagation among region proposals, and, on such basis, the prototype representation of each class is derived for category-level domain alignment. In addition, in order to alleviate the negative effect of class-imbalance on domain adaptation, we design a Class-reweighted Contrastive Loss to harmonize the adaptation training process. Combining with Faster R-CNN, the proposed framework conducts feature alignment in a two-stage manner. Comprehensive results on various cross-domain detection tasks demonstrate that our approach outperforms existing methods with a remarkable margin. Our code is available at https://github.com/ChrisAllenMing/GPA-detection.



### NPENAS: Neural Predictor Guided Evolution for Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2003.12857v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.12857v3)
- **Published**: 2020-03-28 17:56:31+00:00
- **Updated**: 2020-09-10 06:22:06+00:00
- **Authors**: Chen Wei, Chuang Niu, Yiping Tang, Yue Wang, Haihong Hu, Jimin Liang
- **Comment**: None
- **Journal**: None
- **Summary**: Neural architecture search (NAS) is a promising method for automatically design neural architectures. NAS adopts a search strategy to explore the predefined search space to find outstanding performance architecture with the minimum searching costs. Bayesian optimization and evolutionary algorithms are two commonly used search strategies, but they suffer from computationally expensive, challenge to implement or inefficient exploration ability. In this paper, we propose a neural predictor guided evolutionary algorithm to enhance the exploration ability of EA for NAS (NPENAS) and design two kinds of neural predictors. The first predictor is defined from Bayesian optimization and we propose a graph-based uncertainty estimation network as a surrogate model that is easy to implement and computationally efficient. The second predictor is a graph-based neural network that directly outputs the performance prediction of the input neural architecture. The NPENAS using the two neural predictors are denoted as NPENAS-BO and NPENAS-NP respectively. In addition, we introduce a new random architecture sampling method to overcome the drawbacks of the existing sampling method. Extensive experiments demonstrate the superiority of NPENAS. Quantitative results on three NAS search spaces indicate that both NPENAS-BO and NPENAS-NP outperform most existing NAS algorithms, with NPENAS-BO achieving state-of-the-art performance on NASBench-201 and NPENAS-NP on NASBench-101 and DARTS, respectively.



### Adversarial Robustness: From Self-Supervised Pre-Training to Fine-Tuning
- **Arxiv ID**: http://arxiv.org/abs/2003.12862v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.12862v1)
- **Published**: 2020-03-28 18:28:33+00:00
- **Updated**: 2020-03-28 18:28:33+00:00
- **Authors**: Tianlong Chen, Sijia Liu, Shiyu Chang, Yu Cheng, Lisa Amini, Zhangyang Wang
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: Pretrained models from self-supervision are prevalently used in fine-tuning downstream tasks faster or for better accuracy. However, gaining robustness from pretraining is left unexplored. We introduce adversarial training into self-supervision, to provide general-purpose robust pre-trained models for the first time. We find these robust pre-trained models can benefit the subsequent fine-tuning in two ways: i) boosting final model robustness; ii) saving the computation cost, if proceeding towards adversarial fine-tuning. We conduct extensive experiments to demonstrate that the proposed framework achieves large performance margins (eg, 3.83% on robust accuracy and 1.3% on standard accuracy, on the CIFAR-10 dataset), compared with the conventional end-to-end adversarial training baseline. Moreover, we find that different self-supervised pre-trained models have a diverse adversarial vulnerability. It inspires us to ensemble several pretraining tasks, which boosts robustness more. Our ensemble strategy contributes to a further improvement of 3.59% on robust accuracy, while maintaining a slightly higher standard accuracy on CIFAR-10. Our codes are available at https://github.com/TAMU-VITA/Adv-SS-Pretraining.



### One-Shot Domain Adaptation For Face Generation
- **Arxiv ID**: http://arxiv.org/abs/2003.12869v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.12869v1)
- **Published**: 2020-03-28 18:50:13+00:00
- **Updated**: 2020-03-28 18:50:13+00:00
- **Authors**: Chao Yang, Ser-Nam Lim
- **Comment**: Accepted to CVPR 2020
- **Journal**: None
- **Summary**: In this paper, we propose a framework capable of generating face images that fall into the same distribution as that of a given one-shot example. We leverage a pre-trained StyleGAN model that already learned the generic face distribution. Given the one-shot target, we develop an iterative optimization scheme that rapidly adapts the weights of the model to shift the output's high-level distribution to the target's. To generate images of the same distribution, we introduce a style-mixing technique that transfers the low-level statistics from the target to faces randomly generated with the model. With that, we are able to generate an unlimited number of faces that inherit from the distribution of both generic human faces and the one-shot example. The newly generated faces can serve as augmented training data for other downstream tasks. Such setting is appealing as it requires labeling very few, or even one example, in the target domain, which is often the case of real-world face manipulations that result from a variety of unknown and unique distributions, each with extremely low prevalence. We show the effectiveness of our one-shot approach for detecting face manipulations and compare it with other few-shot domain adaptation methods qualitatively and quantitatively.



### Refined Plane Segmentation for Cuboid-Shaped Objects by Leveraging Edge Detection
- **Arxiv ID**: http://arxiv.org/abs/2003.12870v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.12870v1)
- **Published**: 2020-03-28 18:51:43+00:00
- **Updated**: 2020-03-28 18:51:43+00:00
- **Authors**: Alexander Naumann, Laura Dörr, Niels Ole Salscheider, Kai Furmans
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in the area of plane segmentation from single RGB images show strong accuracy improvements and now allow a reliable segmentation of indoor scenes into planes. Nonetheless, fine-grained details of these segmentation masks are still lacking accuracy, thus restricting the usability of such techniques on a larger scale in numerous applications, such as inpainting for Augmented Reality use cases. We propose a post-processing algorithm to align the segmented plane masks with edges detected in the image. This allows us to increase the accuracy of state-of-the-art approaches, while limiting ourselves to cuboid-shaped objects. Our approach is motivated by logistics, where this assumption is valid and refined planes can be used to perform robust object detection without the need for supervised learning. Results for two baselines and our approach are reported on our own dataset, which we made publicly available. The results show a consistent improvement over the state-of-the-art. The influence of the prior segmentation and the edge detection is investigated and finally, areas for future research are proposed.



