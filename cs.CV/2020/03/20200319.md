# Arxiv Papers in cs.CV on 2020-03-19
### Evaluating Salient Object Detection in Natural Images with Multiple Objects having Multi-level Saliency
- **Arxiv ID**: http://arxiv.org/abs/2003.08514v1
- **DOI**: 10.1049/iet-ipr.2019.0787
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.08514v1)
- **Published**: 2020-03-19 00:06:40+00:00
- **Updated**: 2020-03-19 00:06:40+00:00
- **Authors**: Gökhan Yildirim, Debashis Sen, Mohan Kankanhalli, Sabine Süsstrunk
- **Comment**: Accepted Article
- **Journal**: IET Image Processing, 2019
- **Summary**: Salient object detection is evaluated using binary ground truth with the labels being salient object class and background. In this paper, we corroborate based on three subjective experiments on a novel image dataset that objects in natural images are inherently perceived to have varying levels of importance. Our dataset, named SalMoN (saliency in multi-object natural images), has 588 images containing multiple objects. The subjective experiments performed record spontaneous attention and perception through eye fixation duration, point clicking and rectangle drawing. As object saliency in a multi-object image is inherently multi-level, we propose that salient object detection must be evaluated for the capability to detect all multi-level salient objects apart from the salient object class detection capability. For this purpose, we generate multi-level maps as ground truth corresponding to all the dataset images using the results of the subjective experiments, with the labels being multi-level salient objects and background. We then propose the use of mean absolute error, Kendall's rank correlation and average area under precision-recall curve to evaluate existing salient object detection methods on our multi-level saliency ground truth dataset. Approaches that represent saliency detection on images as local-global hierarchical processing of a graph perform well in our dataset.



### SAPIEN: A SimulAted Part-based Interactive ENvironment
- **Arxiv ID**: http://arxiv.org/abs/2003.08515v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2003.08515v1)
- **Published**: 2020-03-19 00:11:34+00:00
- **Updated**: 2020-03-19 00:11:34+00:00
- **Authors**: Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao Zhu, Fangchen Liu, Minghua Liu, Hanxiao Jiang, Yifu Yuan, He Wang, Li Yi, Angel X. Chang, Leonidas J. Guibas, Hao Su
- **Comment**: None
- **Journal**: None
- **Summary**: Building home assistant robots has long been a pursuit for vision and robotics researchers. To achieve this task, a simulated environment with physically realistic simulation, sufficient articulated objects, and transferability to the real robot is indispensable. Existing environments achieve these requirements for robotics simulation with different levels of simplification and focus. We take one step further in constructing an environment that supports household tasks for training robot learning algorithm. Our work, SAPIEN, is a realistic and physics-rich simulated environment that hosts a large-scale set for articulated objects. Our SAPIEN enables various robotic vision and interaction tasks that require detailed part-level understanding.We evaluate state-of-the-art vision algorithms for part detection and motion attribute recognition as well as demonstrate robotic interaction tasks using heuristic approaches and reinforcement learning algorithms. We hope that our SAPIEN can open a lot of research directions yet to be explored, including learning cognition through interaction, part motion discovery, and construction of robotics-ready simulated game environment.



### Efficiently Calibrating Cable-Driven Surgical Robots with RGBD Fiducial Sensing and Recurrent Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2003.08520v4
- **DOI**: 10.1109/LRA.2020.3010746
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.08520v4)
- **Published**: 2020-03-19 00:24:56+00:00
- **Updated**: 2020-07-31 23:45:22+00:00
- **Authors**: Minho Hwang, Brijen Thananjeyan, Samuel Paradis, Daniel Seita, Jeffrey Ichnowski, Danyal Fer, Thomas Low, Ken Goldberg
- **Comment**: 8 pages, 11 figures, 3 tables
- **Journal**: IEEE Robotics and Automation Letters, 5 (2020) 5937-5944
- **Summary**: Automation of surgical subtasks using cable-driven robotic surgical assistants (RSAs) such as Intuitive Surgical's da Vinci Research Kit (dVRK) is challenging due to imprecision in control from cable-related effects such as cable stretching and hysteresis. We propose a novel approach to efficiently calibrate such robots by placing a 3D printed fiducial coordinate frames on the arm and end-effector that is tracked using RGBD sensing. To measure the coupling and history-dependent effects between joints, we analyze data from sampled trajectories and consider 13 approaches to modeling. These models include linear regression and LSTM recurrent neural networks, each with varying temporal window length to provide compensatory feedback. With the proposed method, data collection of 1800 samples takes 31 minutes and model training takes under 1 minute. Results on a test set of reference trajectories suggest that the trained model can reduce the mean tracking error of the physical robot from 2.96 mm to 0.65 mm. Results on the execution of open-loop trajectories of the FLS peg transfer surgeon training task suggest that the best model increases success rate from 39.4 % to 96.7 %, producing performance comparable to that of an expert surgical resident. Supplementary materials, including code and 3D-printable models, are available at https://sites.google.com/berkeley.edu/surgical-calibration



### Extremal Region Analysis based Deep Learning Framework for Detecting Defects
- **Arxiv ID**: http://arxiv.org/abs/2003.08525v2
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.08525v2)
- **Published**: 2020-03-19 00:35:10+00:00
- **Updated**: 2020-05-23 01:32:13+00:00
- **Authors**: Zelin Deng, Xiaolong Yan, Shengjun Zhang, Colleen P. Bailey
- **Comment**: Unsatisfied with results
- **Journal**: None
- **Summary**: A maximally stable extreme region (MSER) analysis based convolutional neural network (CNN) for unified defect detection framework is proposed in this paper. Our proposed framework utilizes the generality and stability of MSER to generate the desired defect candidates. Then a specific trained binary CNN classifier is adopted over the defect candidates to produce the final defect set. Defect datasets over different categories \blue{are used} in the experiments. More generally, the parameter settings in MSER can be adjusted to satisfy different requirements in various industries (high precision, high recall, etc). Extensive experimental results have shown the efficacy of the proposed framework.



### Pose Augmentation: Class-agnostic Object Pose Transformation for Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/2003.08526v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08526v4)
- **Published**: 2020-03-19 00:39:37+00:00
- **Updated**: 2021-01-14 02:03:09+00:00
- **Authors**: Yunhao Ge, Jiaping Zhao, Laurent Itti
- **Comment**: ECCV 2020, with supplementary materials
- **Journal**: None
- **Summary**: Object pose increases intraclass object variance which makes object recognition from 2D images harder. To render a classifier robust to pose variations, most deep neural networks try to eliminate the influence of pose by using large datasets with many poses for each class. Here, we propose a different approach: a class-agnostic object pose transformation network (OPT-Net) can transform an image along 3D yaw and pitch axes to synthesize additional poses continuously. Synthesized images lead to better training of an object classifier. We design a novel eliminate-add structure to explicitly disentangle pose from object identity: first eliminate pose information of the input image and then add target pose information (regularized as continuous variables) to synthesize any target pose. We trained OPT-Net on images of toy vehicles shot on a turntable from the iLab-20M dataset. After training on unbalanced discrete poses (5 classes with 6 poses per object instance, plus 5 classes with only 2 poses), we show that OPT-Net can synthesize balanced continuous new poses along yaw and pitch axes with high quality. Training a ResNet-18 classifier with original plus synthesized poses improves mAP accuracy by 9% overtraining on original poses only. Further, the pre-trained OPT-Net can generalize to new object classes, which we demonstrate on both iLab-20M and RGB-D. We also show that the learned features can generalize to ImageNet.



### Towards Detection of Sheep Onboard a UAV
- **Arxiv ID**: http://arxiv.org/abs/2004.02758v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.02758v1)
- **Published**: 2020-03-19 00:40:48+00:00
- **Updated**: 2020-03-19 00:40:48+00:00
- **Authors**: Farah Sarwar, Anthony Griffin, Saeed Ur Rehman, Timotius Pasang
- **Comment**: This was accepted for publication and presentation at the Embedded AI
  for Real-time Machine Vision 2019 in conjunction with the British Machine
  Vision Conference (BMVC) 2019. It was presented on 12 September 2019 in
  Cardiff, Wales. 10 pages, 3 figures, and 1 table, note that this is a
  web-friendly format as used at BMVC, so the pages are about A5 size (but not
  exactly!)
- **Journal**: None
- **Summary**: In this work we consider the task of detecting sheep onboard an unmanned aerial vehicle (UAV) flying at an altitude of 80 m. At this height, the sheep are relatively small, only about 15 pixels across. Although deep learning strategies have gained enormous popularity in the last decade and are now extensively used for object detection in many fields, state-of-the-art detectors perform poorly in the case of smaller objects. We develop a novel dataset of UAV imagery of sheep and consider a variety of object detectors to determine which is the most suitable for our task in terms of both accuracy and speed. Our findings indicate that a UNet detector using the weighted Hausdorff distance as a loss function during training is an excellent option for detection of sheep onboard a UAV.



### Stereo Endoscopic Image Super-Resolution Using Disparity-Constrained Parallel Attention
- **Arxiv ID**: http://arxiv.org/abs/2003.08539v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.08539v1)
- **Published**: 2020-03-19 02:12:08+00:00
- **Updated**: 2020-03-19 02:12:08+00:00
- **Authors**: Tianyi Zhang, Yun Gu, Xiaolin Huang, Enmei Tu, Jie Yang
- **Comment**: 6 pages, 4 figures, accepted as a workshop paper at AI4AH, ICLR 2020
- **Journal**: None
- **Summary**: With the popularity of stereo cameras in computer assisted surgery techniques, a second viewpoint would provide additional information in surgery. However, how to effectively access and use stereo information for the super-resolution (SR) purpose is often a challenge. In this paper, we propose a disparity-constrained stereo super-resolution network (DCSSRnet) to simultaneously compute a super-resolved image in a stereo image pair. In particular, we incorporate a disparity-based constraint mechanism into the generation of SR images in a deep neural network framework with an additional atrous parallax-attention modules. Experiment results on laparoscopic images demonstrate that the proposed framework outperforms current SR methods on both quantitative and qualitative evaluations. Our DCSSRnet provides a promising solution on enhancing spatial resolution of stereo image pairs, which will be extremely beneficial for the endoscopic surgery.



### Detecting Lane and Road Markings at A Distance with Perspective Transformer Layers
- **Arxiv ID**: http://arxiv.org/abs/2003.08550v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08550v2)
- **Published**: 2020-03-19 03:22:52+00:00
- **Updated**: 2020-10-25 06:38:46+00:00
- **Authors**: Zhuoping Yu, Xiaozhou Ren, Yuyao Huang, Wei Tian, Junqiao Zhao
- **Comment**: 6 pages, 8 figures
- **Journal**: None
- **Summary**: Accurate detection of lane and road markings is a task of great importance for intelligent vehicles. In existing approaches, the detection accuracy often degrades with the increasing distance. This is due to the fact that distant lane and road markings occupy a small number of pixels in the image, and scales of lane and road markings are inconsistent at various distances and perspectives. The Inverse Perspective Mapping (IPM) can be used to eliminate the perspective distortion, but the inherent interpolation can lead to artifacts especially around distant lane and road markings and thus has a negative impact on the accuracy of lane marking detection and segmentation. To solve this problem, we adopt the Encoder-Decoder architecture in Fully Convolutional Networks and leverage the idea of Spatial Transformer Networks to introduce a novel semantic segmentation neural network. This approach decomposes the IPM process into multiple consecutive differentiable homographic transform layers, which are called "Perspective Transformer Layers". Furthermore, the interpolated feature map is refined by subsequent convolutional layers thus reducing the artifacts and improving the accuracy. The effectiveness of the proposed method in lane marking detection is validated on two public datasets: TuSimple and ApolloScape



### Quality Control of Neuron Reconstruction Based on Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2003.08556v1
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2003.08556v1)
- **Published**: 2020-03-19 03:44:29+00:00
- **Updated**: 2020-03-19 03:44:29+00:00
- **Authors**: Donghuan Lu, Sujun Zhao, Peng Xie, Kai Ma, Lijuan Liu, Yefeng Zheng
- **Comment**: 9 pages, 2 figures
- **Journal**: None
- **Summary**: Neuron reconstruction is essential to generate exquisite neuron connectivity map for understanding brain function. Despite the significant amount of effect that has been made on automatic reconstruction methods, manual tracing by well-trained human annotators is still necessary. To ensure the quality of reconstructed neurons and provide guidance for annotators to improve their efficiency, we propose a deep learning based quality control method for neuron reconstruction in this paper. By formulating the quality control problem into a binary classification task regarding each single point, the proposed approach overcomes the technical difficulties resulting from the large image size and complex neuron morphology. Not only it provides the evaluation of reconstruction quality, but also can locate exactly where the wrong tracing begins. This work presents one of the first comprehensive studies for whole-brain scale quality control of neuron reconstructions. Experiments on five-fold cross validation with a large dataset demonstrate that the proposed approach can detect 74.7% errors with only 1.4% false alerts.



### Lifelong Learning with Searchable Extension Units
- **Arxiv ID**: http://arxiv.org/abs/2003.08559v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.08559v1)
- **Published**: 2020-03-19 03:45:51+00:00
- **Updated**: 2020-03-19 03:45:51+00:00
- **Authors**: Wenjin Wang, Yunqing Hu, Yin Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Lifelong learning remains an open problem. One of its main difficulties is catastrophic forgetting. Many dynamic expansion approaches have been proposed to address this problem, but they all use homogeneous models of predefined structure for all tasks. The common original model and expansion structures ignore the requirement of different model structures on different tasks, which leads to a less compact model for multiple tasks and causes the model size to increase rapidly as the number of tasks increases. Moreover, they can not perform best on all tasks. To solve those problems, in this paper, we propose a new lifelong learning framework named Searchable Extension Units (SEU) by introducing Neural Architecture Search into lifelong learning, which breaks down the need for a predefined original model and searches for specific extension units for different tasks, without compromising the performance of the model on different tasks. Our approach can obtain a much more compact model without catastrophic forgetting. The experimental results on the PMNIST, the split CIFAR10 dataset, the split CIFAR100 dataset, and the Mixture dataset empirically prove that our method can achieve higher accuracy with much smaller model, whose size is about 25-33 percentage of that of the state-of-the-art methods.



### CPR-GCN: Conditional Partial-Residual Graph Convolutional Network in Automated Anatomical Labeling of Coronary Arteries
- **Arxiv ID**: http://arxiv.org/abs/2003.08560v4
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.08560v4)
- **Published**: 2020-03-19 04:02:12+00:00
- **Updated**: 2020-04-18 03:07:03+00:00
- **Authors**: Han Yang, Xingjian Zhen, Ying Chi, Lei Zhang, Xian-Sheng Hua
- **Comment**: This work is done by Xingjian Zhen during internship in Alibaba Damo
  Academy
- **Journal**: CVPR 2020 oral
- **Summary**: Automated anatomical labeling plays a vital role in coronary artery disease diagnosing procedure. The main challenge in this problem is the large individual variability inherited in human anatomy. Existing methods usually rely on the position information and the prior knowledge of the topology of the coronary artery tree, which may lead to unsatisfactory performance when the main branches are confusing. Motivated by the wide application of the graph neural network in structured data, in this paper, we propose a conditional partial-residual graph convolutional network (CPR-GCN), which takes both position and CT image into consideration, since CT image contains abundant information such as branch size and spanning direction. Two majority parts, a Partial-Residual GCN and a conditions extractor, are included in CPR-GCN. The conditions extractor is a hybrid model containing the 3D CNN and the LSTM, which can extract 3D spatial image features along the branches. On the technical side, the Partial-Residual GCN takes the position features of the branches, with the 3D spatial image features as conditions, to predict the label for each branches. While on the mathematical side, our approach twists the partial differential equation (PDE) into the graph modeling. A dataset with 511 subjects is collected from the clinic and annotated by two experts with a two-phase annotation process. According to the five-fold cross-validation, our CPR-GCN yields 95.8% meanRecall, 95.4% meanPrecision and 0.955 meanF1, which outperforms state-of-the-art approaches.



### Ensemble learning in CNN augmented with fully connected subnetworks
- **Arxiv ID**: http://arxiv.org/abs/2003.08562v3
- **DOI**: 10.1587/transinf.2022EDL8098
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.08562v3)
- **Published**: 2020-03-19 04:02:49+00:00
- **Updated**: 2020-03-24 07:03:45+00:00
- **Authors**: Daiki Hirata, Norikazu Takahashi
- **Comment**: 6 pages, 2 figures, 5 tables
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) have shown remarkable performance in general object recognition tasks. In this paper, we propose a new model called EnsNet which is composed of one base CNN and multiple Fully Connected SubNetworks (FCSNs). In this model, the set of feature-maps generated by the last convolutional layer in the base CNN is divided along channels into disjoint subsets, and these subsets are assigned to the FCSNs. Each of the FCSNs is trained independent of others so that it can predict the class label from the subset of the feature-maps assigned to it. The output of the overall model is determined by majority vote of the base CNN and the FCSNs. Experimental results using the MNIST, Fashion-MNIST and CIFAR-10 datasets show that the proposed approach further improves the performance of CNNs. In particular, an EnsNet achieves a state-of-the-art error rate of 0.16% on MNIST.



### The Chiral Domain of a Camera Arrangement
- **Arxiv ID**: http://arxiv.org/abs/2003.09265v4
- **DOI**: None
- **Categories**: **math.AG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.09265v4)
- **Published**: 2020-03-19 04:48:40+00:00
- **Updated**: 2022-04-26 21:02:09+00:00
- **Authors**: Sameer Agarwal, Andrew Pryhuber, Rainer Sinn, Rekha R. Thomas
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce the chiral domain of an arrangement of cameras $\mathcal{A} = \{A_1,\dots, A_m\}$ which is the subset of $\mathbb{P}^3$ visible in $\mathcal{A}$. It generalizes the classical definition of chirality to include all of $\mathbb{P}^3$ and offers a unifying framework for studying multiview chirality. We give an algebraic description of the chiral domain which allows us to define and describe a chiral version of Triggs' joint image. We then use the chiral domain to re-derive and extend prior results on chirality due to Hartley.



### High Accuracy Face Geometry Capture using a Smartphone Video
- **Arxiv ID**: http://arxiv.org/abs/2003.08583v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08583v1)
- **Published**: 2020-03-19 05:46:58+00:00
- **Updated**: 2020-03-19 05:46:58+00:00
- **Authors**: Shubham Agrawal, Anuj Pahuja, Simon Lucey
- **Comment**: Presented at The IEEE Winter Conference on Applications of Computer
  Vision (WACV), 2020, pp. 81-90
- **Journal**: The IEEE Winter Conference on Applications of Computer Vision
  (WACV), 2020, pp. 81-90
- **Summary**: What's the most accurate 3D model of your face you can obtain while sitting at your desk? We attempt to answer this question in our work. High fidelity face reconstructions have so far been limited to either studio settings or through expensive 3D scanners. On the other hand, unconstrained reconstruction methods are typically limited by low-capacity models. Our method reconstructs accurate face geometry of a subject using a video shot from a smartphone in an unconstrained environment. Our approach takes advantage of recent advances in visual SLAM, keypoint detection, and object detection to improve accuracy and robustness. By not being constrained to a model subspace, our reconstructed meshes capture important details while being robust to noise and being topologically consistent. Our evaluations show that our method outperforms current single and multi-view baselines by a significant margin, both in terms of geometric accuracy and in capturing person-specific details important for making realistic looking models.



### Curriculum DeepSDF
- **Arxiv ID**: http://arxiv.org/abs/2003.08593v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08593v3)
- **Published**: 2020-03-19 06:40:59+00:00
- **Updated**: 2020-07-16 21:18:48+00:00
- **Authors**: Yueqi Duan, Haidong Zhu, He Wang, Li Yi, Ram Nevatia, Leonidas J. Guibas
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: When learning to sketch, beginners start with simple and flexible shapes, and then gradually strive for more complex and accurate ones in the subsequent training sessions. In this paper, we design a "shape curriculum" for learning continuous Signed Distance Function (SDF) on shapes, namely Curriculum DeepSDF. Inspired by how humans learn, Curriculum DeepSDF organizes the learning task in ascending order of difficulty according to the following two criteria: surface accuracy and sample difficulty. The former considers stringency in supervising with ground truth, while the latter regards the weights of hard training samples near complex geometry and fine structure. More specifically, Curriculum DeepSDF learns to reconstruct coarse shapes at first, and then gradually increases the accuracy and focuses more on complex local details. Experimental results show that a carefully-designed curriculum leads to significantly better shape reconstructions with the same training data, training epochs and network architecture as DeepSDF. We believe that the application of shape curricula can benefit the training process of a wide variety of 3D shape representation learning methods.



### Deep convolutional embedding for digitized painting clustering
- **Arxiv ID**: http://arxiv.org/abs/2003.08597v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08597v2)
- **Published**: 2020-03-19 06:49:38+00:00
- **Updated**: 2020-10-22 15:21:49+00:00
- **Authors**: Giovanna Castellano, Gennaro Vessio
- **Comment**: Accepted at ICPR2020. Added references. Corrected typos. Added new
  results and observations according to reviewers
- **Journal**: None
- **Summary**: Clustering artworks is difficult for several reasons. On the one hand, recognizing meaningful patterns in accordance with domain knowledge and visual perception is extremely difficult. On the other hand, applying traditional clustering and feature reduction techniques to the highly dimensional pixel space can be ineffective. To address these issues, we propose to use a deep convolutional embedding model for digitized painting clustering, in which the task of mapping the raw input data to an abstract, latent space is jointly optimized with the task of finding a set of cluster centroids in this latent feature space. Quantitative and qualitative experimental results show the effectiveness of the proposed method. The model is also capable of outperforming other state-of-the-art deep clustering approaches to the same problem. The proposed method can be useful for several art-related tasks, in particular visual link retrieval and historical knowledge discovery in painting datasets.



### HyNNA: Improved Performance for Neuromorphic Vision Sensor based Surveillance using Hybrid Neural Network Architecture
- **Arxiv ID**: http://arxiv.org/abs/2003.08603v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.08603v1)
- **Published**: 2020-03-19 07:18:33+00:00
- **Updated**: 2020-03-19 07:18:33+00:00
- **Authors**: Deepak Singla, Soham Chatterjee, Lavanya Ramapantulu, Andres Ussa, Bharath Ramesh, Arindam Basu
- **Comment**: 4 pages, 2 figures
- **Journal**: None
- **Summary**: Applications in the Internet of Video Things (IoVT) domain have very tight constraints with respect to power and area. While neuromorphic vision sensors (NVS) may offer advantages over traditional imagers in this domain, the existing NVS systems either do not meet the power constraints or have not demonstrated end-to-end system performance. To address this, we improve on a recently proposed hybrid event-frame approach by using morphological image processing algorithms for region proposal and address the low-power requirement for object detection and classification by exploring various convolutional neural network (CNN) architectures. Specifically, we compare the results obtained from our object detection framework against the state-of-the-art low-power NVS surveillance system and show an improved accuracy of 82.16% from 63.1%. Moreover, we show that using multiple bits does not improve accuracy, and thus, system designers can save power and area by using only single bit event polarity information. In addition, we explore the CNN architecture space for object classification and show useful insights to trade-off accuracy for lower power using lesser memory and arithmetic operations.



### End-to-End Deep Diagnosis of X-ray Images
- **Arxiv ID**: http://arxiv.org/abs/2003.08605v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.08605v1)
- **Published**: 2020-03-19 07:20:48+00:00
- **Updated**: 2020-03-19 07:20:48+00:00
- **Authors**: Kudaibergen Urinbayev, Yerassyl Orazbek, Yernur Nurambek, Almas Mirzakhmetov, Huseyin Atakan Varol
- **Comment**: 4 pages, 5 figures
- **Journal**: None
- **Summary**: In this work, we present an end-to-end deep learning framework for X-ray image diagnosis. As the first step, our system determines whether a submitted image is an X-ray or not. After it classifies the type of the X-ray, it runs the dedicated abnormality classification network. In this work, we only focus on the chest X-rays for abnormality classification. However, the system can be extended to other X-ray types easily. Our deep learning classifiers are based on DenseNet-121 architecture. The test set accuracy obtained for 'X-ray or Not', 'X-ray Type Classification', and 'Chest Abnormality Classification' tasks are 0.987, 0.976, and 0.947, respectively, resulting into an end-to-end accuracy of 0.91. For achieving better results than the state-of-the-art in the 'Chest Abnormality Classification', we utilize the new RAdam optimizer. We also use Gradient-weighted Class Activation Mapping for visual explanation of the results. Our results show the feasibility of a generalized online projectional radiography diagnosis system.



### Unsupervised Domain Adaptation via Structurally Regularized Deep Clustering
- **Arxiv ID**: http://arxiv.org/abs/2003.08607v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08607v1)
- **Published**: 2020-03-19 07:26:41+00:00
- **Updated**: 2020-03-19 07:26:41+00:00
- **Authors**: Hui Tang, Ke Chen, Kui Jia
- **Comment**: 14 pages, 5 figures, 8 tables, accepted by CVPR2020 - Oral
- **Journal**: None
- **Summary**: Unsupervised domain adaptation (UDA) is to make predictions for unlabeled data on a target domain, given labeled data on a source domain whose distribution shifts from the target one. Mainstream UDA methods learn aligned features between the two domains, such that a classifier trained on the source features can be readily applied to the target ones. However, such a transferring strategy has a potential risk of damaging the intrinsic discrimination of target data. To alleviate this risk, we are motivated by the assumption of structural domain similarity, and propose to directly uncover the intrinsic target discrimination via discriminative clustering of target data. We constrain the clustering solutions using structural source regularization that hinges on our assumed structural domain similarity. Technically, we use a flexible framework of deep network based discriminative clustering that minimizes the KL divergence between predictive label distribution of the network and an introduced auxiliary one; replacing the auxiliary distribution with that formed by ground-truth labels of source data implements the structural source regularization via a simple strategy of joint network training. We term our proposed method as Structurally Regularized Deep Clustering (SRDC), where we also enhance target discrimination with clustering of intermediate network features, and enhance structural regularization with soft selection of less divergent source examples. Careful ablation studies show the efficacy of our proposed SRDC. Notably, with no explicit domain alignment, SRDC outperforms all existing methods on three UDA benchmarks.



### DPANet: Depth Potentiality-Aware Gated Attention Network for RGB-D Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2003.08608v4
- **DOI**: 10.1109/TIP.2020.3028289
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08608v4)
- **Published**: 2020-03-19 07:27:54+00:00
- **Updated**: 2020-09-29 02:34:17+00:00
- **Authors**: Zuyao Chen, Runmin Cong, Qianqian Xu, Qingming Huang
- **Comment**: Accepted by IEEE Transactions on Image Processing 2020
- **Journal**: None
- **Summary**: There are two main issues in RGB-D salient object detection: (1) how to effectively integrate the complementarity from the cross-modal RGB-D data; (2) how to prevent the contamination effect from the unreliable depth map. In fact, these two problems are linked and intertwined, but the previous methods tend to focus only on the first problem and ignore the consideration of depth map quality, which may yield the model fall into the sub-optimal state. In this paper, we address these two issues in a holistic model synergistically, and propose a novel network named DPANet to explicitly model the potentiality of the depth map and effectively integrate the cross-modal complementarity. By introducing the depth potentiality perception, the network can perceive the potentiality of depth information in a learning-based manner, and guide the fusion process of two modal data to prevent the contamination occurred. The gated multi-modality attention module in the fusion process exploits the attention mechanism with a gate controller to capture long-range dependencies from a cross-modal perspective. Experimental results compared with 15 state-of-the-art methods on 8 datasets demonstrate the validity of the proposed approach both quantitatively and qualitatively.



### AQPDBJUT Dataset: Picture-Based PM Monitoring in the Campus of BJUT
- **Arxiv ID**: http://arxiv.org/abs/2003.08609v2
- **DOI**: None
- **Categories**: **cs.CV**, J.6.0
- **Links**: [PDF](http://arxiv.org/pdf/2003.08609v2)
- **Published**: 2020-03-19 07:28:13+00:00
- **Updated**: 2020-03-21 15:28:56+00:00
- **Authors**: Yonghui Zhang, Ke Gu
- **Comment**: None
- **Journal**: None
- **Summary**: Ensuring the students in good physical levels is imperative for their future health. In recent years, the continually growing concentration of Particulate Matter (PM) has done increasingly serious harm to student health. Hence, it is highly required to prevent and control PM concentrations in the campus. As the source of PM prevention and control, developing a good model for PM monitoring is extremely urgent and has posed a big challenge. It has been found in prior works that photobased methods are available for PM monitoring. To verify the effectiveness of existing PM monitoring methods in the campus, we establish a new dataset which includes 1,500 photos collected in the Beijing University of Technology. Experiments show that stated-of-the-art methods are far from ideal for PM monitoring in the campus.



### Blockchain meets Biometrics: Concepts, Application to Template Protection, and Trends
- **Arxiv ID**: http://arxiv.org/abs/2003.09262v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2003.09262v1)
- **Published**: 2020-03-19 08:11:13+00:00
- **Updated**: 2020-03-19 08:11:13+00:00
- **Authors**: Oscar Delgado-Mohatar, Julian Fierrez, Ruben Tolosana, Ruben Vera-Rodriguez
- **Comment**: arXiv admin note: text overlap with arXiv:1904.13128
- **Journal**: None
- **Summary**: Blockchain technologies provide excellent architectures and practical tools for securing and managing the sensitive and private data stored in biometric templates, but at a cost. We discuss opportunities and challenges in the integration of blockchain and biometrics, with emphasis in biometric template storage and protection, a key problem in biometrics still largely unsolved. Key tradeoffs involved in that integration, namely, latency, processing time, economic cost, and biometric performance are experimentally studied through the implementation of a smart contract on the Ethereum blockchain platform, which is publicly available in github for research purposes.



### PT2PC: Learning to Generate 3D Point Cloud Shapes from Part Tree Conditions
- **Arxiv ID**: http://arxiv.org/abs/2003.08624v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2003.08624v2)
- **Published**: 2020-03-19 08:27:25+00:00
- **Updated**: 2020-07-16 01:28:18+00:00
- **Authors**: Kaichun Mo, He Wang, Xinchen Yan, Leonidas J. Guibas
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: 3D generative shape modeling is a fundamental research area in computer vision and interactive computer graphics, with many real-world applications. This paper investigates the novel problem of generating 3D shape point cloud geometry from a symbolic part tree representation. In order to learn such a conditional shape generation procedure in an end-to-end fashion, we propose a conditional GAN "part tree"-to-"point cloud" model (PT2PC) that disentangles the structural and geometric factors. The proposed model incorporates the part tree condition into the architecture design by passing messages top-down and bottom-up along the part tree hierarchy. Experimental results and user study demonstrate the strengths of our method in generating perceptually plausible and diverse 3D point clouds, given the part tree condition. We also propose a novel structural measure for evaluating if the generated shape point clouds satisfy the part tree conditions.



### Domain-Adaptive Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2003.08626v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08626v1)
- **Published**: 2020-03-19 08:31:14+00:00
- **Updated**: 2020-03-19 08:31:14+00:00
- **Authors**: An Zhao, Mingyu Ding, Zhiwu Lu, Tao Xiang, Yulei Niu, Jiechao Guan, Ji-Rong Wen, Ping Luo
- **Comment**: None
- **Journal**: None
- **Summary**: Existing few-shot learning (FSL) methods make the implicit assumption that the few target class samples are from the same domain as the source class samples. However, in practice this assumption is often invalid -- the target classes could come from a different domain. This poses an additional challenge of domain adaptation (DA) with few training samples. In this paper, the problem of domain-adaptive few-shot learning (DA-FSL) is tackled, which requires solving FSL and DA in a unified framework. To this end, we propose a novel domain-adversarial prototypical network (DAPN) model. It is designed to address a specific challenge in DA-FSL: the DA objective means that the source and target data distributions need to be aligned, typically through a shared domain-adaptive feature embedding space; but the FSL objective dictates that the target domain per class distribution must be different from that of any source domain class, meaning aligning the distributions across domains may harm the FSL performance. How to achieve global domain distribution alignment whilst maintaining source/target per-class discriminativeness thus becomes the key. Our solution is to explicitly enhance the source/target per-class separation before domain-adaptive feature embedding learning in the DAPN, in order to alleviate the negative effect of domain alignment on FSL. Extensive experiments show that our DAPN outperforms the state-of-the-art FSL and DA models, as well as their na\"ive combinations. The code is available at https://github.com/dingmyu/DAPN.



### Foldover Features for Dynamic Object Behavior Description in Microscopic Videos
- **Arxiv ID**: http://arxiv.org/abs/2003.08628v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08628v2)
- **Published**: 2020-03-19 08:39:39+00:00
- **Updated**: 2020-03-21 02:49:32+00:00
- **Authors**: Xialin Li, Chen Li, Wenwei Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Behavior description is conducive to the analysis of tiny objects, similar objects, objects with weak visual information and objects with similar visual information, playing a fundamental role in the identification and classification of dynamic objects in microscopic videos. To this end, we propose foldover features to describe the behavior of dynamic objects. First, we generate foldover for each object in microscopic videos in X, Y and Z directions, respectively. Then, we extract foldover features from the X, Y and Z directions with statistical methods, respectively. Finally, we use four different classifiers to test the effectiveness of the proposed foldover features. In the experiment, we use a sperm microscopic video dataset to evaluate the proposed foldover features, including three types of 1374 sperms, and obtain the highest classification accuracy of 96.5%.



### Accuracy of MRI Classification Algorithms in a Tertiary Memory Center Clinical Routine Cohort
- **Arxiv ID**: http://arxiv.org/abs/2003.09260v1
- **DOI**: 10.3233/JAD-190594
- **Categories**: **q-bio.QM**, cs.CV, eess.IV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2003.09260v1)
- **Published**: 2020-03-19 08:44:46+00:00
- **Updated**: 2020-03-19 08:44:46+00:00
- **Authors**: Alexandre Morin, Jorge Samper-González, Anne Bertrand, Sebastian Stroer, Didier Dormont, Aline Mendes, Pierrick Coupé, Jamila Ahdidan, Marcel Lévy, Dalila Samri, Harald Hampel, Bruno Dubois, Marc Teichmann, Stéphane Epelbaum, Olivier Colliot
- **Comment**: None
- **Journal**: Journal of Alzheimer's Disease, IOS Press, 2020, pp.1-10
- **Summary**: BACKGROUND:Automated volumetry software (AVS) has recently become widely available to neuroradiologists. MRI volumetry with AVS may support the diagnosis of dementias by identifying regional atrophy. Moreover, automatic classifiers using machine learning techniques have recently emerged as promising approaches to assist diagnosis. However, the performance of both AVS and automatic classifiers has been evaluated mostly in the artificial setting of research datasets.OBJECTIVE:Our aim was to evaluate the performance of two AVS and an automatic classifier in the clinical routine condition of a memory clinic.METHODS:We studied 239 patients with cognitive troubles from a single memory center cohort. Using clinical routine T1-weighted MRI, we evaluated the classification performance of: 1) univariate volumetry using two AVS (volBrain and Neuroreader$^{TM}$); 2) Support Vector Machine (SVM) automatic classifier, using either the AVS volumes (SVM-AVS), or whole gray matter (SVM-WGM); 3) reading by two neuroradiologists. The performance measure was the balanced diagnostic accuracy. The reference standard was consensus diagnosis by three neurologists using clinical, biological (cerebrospinal fluid) and imaging data and following international criteria.RESULTS:Univariate AVS volumetry provided only moderate accuracies (46% to 71% with hippocampal volume). The accuracy improved when using SVM-AVS classifier (52% to 85%), becoming close to that of SVM-WGM (52 to 90%). Visual classification by neuroradiologists ranged between SVM-AVS and SVM-WGM.CONCLUSION:In the routine practice of a memory clinic, the use of volumetric measures provided by AVS yields only moderate accuracy. Automatic classifiers can improve accuracy and could be a useful tool to assist diagnosis.



### Unsupervised deep learning for text line segmentation
- **Arxiv ID**: http://arxiv.org/abs/2003.08632v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.08632v2)
- **Published**: 2020-03-19 08:57:53+00:00
- **Updated**: 2020-10-24 21:11:57+00:00
- **Authors**: Berat Kurar Barakat, Ahmad Droby, Rym Alasam, Boraq Madi, Irina Rabaev, Raed Shammes, Jihad El-Sana
- **Comment**: An unsupervised CNN for text line segmentation
- **Journal**: None
- **Summary**: We present an unsupervised deep learning method for text line segmentation that is inspired by the relative variance between text lines and spaces among text lines. Handwritten text line segmentation is important for the efficiency of further processing. A common method is to train a deep learning network for embedding the document image into an image of blob lines that are tracing the text lines. Previous methods learned such embedding in a supervised manner, requiring the annotation of many document images. This paper presents an unsupervised embedding of document image patches without a need for annotations. The number of foreground pixels over the text lines is relatively different from the number of foreground pixels over the spaces among text lines. Generating similar and different pairs relying on this principle definitely leads to outliers. However, as the results show, the outliers do not harm the convergence and the network learns to discriminate the text lines from the spaces between text lines. Remarkably, with a challenging Arabic handwritten text line segmentation dataset, VML-AHTE, we achieved superior performance over the supervised methods. Additionally, the proposed method was evaluated on the ICDAR 2017 and ICFHR 2010 handwritten text line segmentation datasets.



### Backdooring and Poisoning Neural Networks with Image-Scaling Attacks
- **Arxiv ID**: http://arxiv.org/abs/2003.08633v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.08633v1)
- **Published**: 2020-03-19 08:59:50+00:00
- **Updated**: 2020-03-19 08:59:50+00:00
- **Authors**: Erwin Quiring, Konrad Rieck
- **Comment**: IEEE Deep Learning and Security Workshop (DLS) 2020
- **Journal**: None
- **Summary**: Backdoors and poisoning attacks are a major threat to the security of machine-learning and vision systems. Often, however, these attacks leave visible artifacts in the images that can be visually detected and weaken the efficacy of the attacks. In this paper, we propose a novel strategy for hiding backdoor and poisoning attacks. Our approach builds on a recent class of attacks against image scaling. These attacks enable manipulating images such that they change their content when scaled to a specific resolution. By combining poisoning and image-scaling attacks, we can conceal the trigger of backdoors as well as hide the overlays of clean-label poisoning. Furthermore, we consider the detection of image-scaling attacks and derive an adaptive attack. In an empirical evaluation, we demonstrate the effectiveness of our strategy. First, we show that backdoors and poisoning work equally well when combined with image-scaling attacks. Second, we demonstrate that current detection defenses against image-scaling attacks are insufficient to uncover our manipulations. Overall, our work provides a novel means for hiding traces of manipulations, being applicable to different poisoning approaches.



### Photo-Realistic Video Prediction on Natural Videos of Largely Changing Frames
- **Arxiv ID**: http://arxiv.org/abs/2003.08635v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.08635v1)
- **Published**: 2020-03-19 09:06:06+00:00
- **Updated**: 2020-03-19 09:06:06+00:00
- **Authors**: Osamu Shouno
- **Comment**: 16 pages, 7 figures
- **Journal**: None
- **Summary**: Recent advances in deep learning have significantly improved performance of video prediction. However, state-of-the-art methods still suffer from blurriness and distortions in their future predictions, especially when there are large motions between frames. To address these issues, we propose a deep residual network with the hierarchical architecture where each layer makes a prediction of future state at different spatial resolution, and these predictions of different layers are merged via top-down connections to generate future frames. We trained our model with adversarial and perceptual loss functions, and evaluated it on a natural video dataset captured by car-mounted cameras. Our model quantitatively outperforms state-of-the-art baselines in future frame prediction on video sequences of both largely and slightly changing frames. Furthermore, our model generates future frames with finer details and textures that are perceptually more realistic than the baselines, especially under fast camera motions.



### Detecting Deepfakes with Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/2003.08645v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08645v1)
- **Published**: 2020-03-19 09:44:23+00:00
- **Updated**: 2020-03-19 09:44:23+00:00
- **Authors**: Akash Kumar, Arnav Bhavsar
- **Comment**: None
- **Journal**: None
- **Summary**: With the arrival of several face-swapping applications such as FaceApp, SnapChat, MixBooth, FaceBlender and many more, the authenticity of digital media content is hanging on a very loose thread. On social media platforms, videos are widely circulated often at a high compression factor. In this work, we analyze several deep learning approaches in the context of deepfakes classification in high compression scenario and demonstrate that a proposed approach based on metric learning can be very effective in performing such a classification. Using less number of frames per video to assess its realism, the metric learning approach using a triplet network architecture proves to be fruitful. It learns to enhance the feature space distance between the cluster of real and fake videos embedding vectors. We validated our approaches on two datasets to analyze the behavior in different environments. We achieved a state-of-the-art AUC score of 99.2% on the Celeb-DF dataset and accuracy of 90.71% on a highly compressed Neural Texture dataset. Our approach is especially helpful on social media platforms where data compression is inevitable.



### LANCE: Efficient Low-Precision Quantized Winograd Convolution for Neural Networks Based on Graphics Processing Units
- **Arxiv ID**: http://arxiv.org/abs/2003.08646v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2003.08646v3)
- **Published**: 2020-03-19 09:46:50+00:00
- **Updated**: 2020-07-28 13:15:20+00:00
- **Authors**: Guangli Li, Lei Liu, Xueying Wang, Xiu Ma, Xiaobing Feng
- **Comment**: Accepted by ICASSP 2020
- **Journal**: None
- **Summary**: Accelerating deep convolutional neural networks has become an active topic and sparked an interest in academia and industry. In this paper, we propose an efficient low-precision quantized Winograd convolution algorithm, called LANCE, which combines the advantages of fast convolution and quantization techniques. By embedding linear quantization operations into the Winograd-domain, the fast convolution can be performed efficiently under low-precision computation on graphics processing units. We test neural network models with LANCE on representative image classification datasets, including SVHN, CIFAR, and ImageNet. The experimental results show that our 8-bit quantized Winograd convolution improves the performance by up to 2.40x over the full-precision convolution with trivial accuracy loss.



### RADIOGAN: Deep Convolutional Conditional Generative adversarial Network To Generate PET Images
- **Arxiv ID**: http://arxiv.org/abs/2003.08663v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.08663v1)
- **Published**: 2020-03-19 10:14:40+00:00
- **Updated**: 2020-03-19 10:14:40+00:00
- **Authors**: Amine Amyar, Su Ruan, Pierre Vera, Pierre Decazes, Romain Modzelewski
- **Comment**: 4 pages, 5 figures
- **Journal**: None
- **Summary**: One of the most challenges in medical imaging is the lack of data. It is proven that classical data augmentation methods are useful but still limited due to the huge variation in images. Using generative adversarial networks (GAN) is a promising way to address this problem, however, it is challenging to train one model to generate different classes of lesions. In this paper, we propose a deep convolutional conditional generative adversarial network to generate MIP positron emission tomography image (PET) which is a 2D image that represents a 3D volume for fast interpretation, according to different lesions or non lesion (normal). The advantage of our proposed method consists of one model that is capable of generating different classes of lesions trained on a small sample size for each class of lesion, and showing a very promising results. In addition, we show that a walk through a latent space can be used as a tool to evaluate the images generated.



### Efficient and Robust Shape Correspondence via Sparsity-Enforced Quadratic Assignment
- **Arxiv ID**: http://arxiv.org/abs/2003.08680v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08680v2)
- **Published**: 2020-03-19 10:56:16+00:00
- **Updated**: 2020-03-20 21:23:38+00:00
- **Authors**: Rui Xiang, Rongjie Lai, Hongkai Zhao
- **Comment**: 8 pages, 6 figures. Compared to the version to be published in the
  2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),
  Figure 1 has been changed to a more illustrative example and run time table 1
  has been updated by our recently optimized code
- **Journal**: None
- **Summary**: In this work, we introduce a novel local pairwise descriptor and then develop a simple, effective iterative method to solve the resulting quadratic assignment through sparsity control for shape correspondence between two approximate isometric surfaces. Our pairwise descriptor is based on the stiffness and mass matrix of finite element approximation of the Laplace-Beltrami differential operator, which is local in space, sparse to represent, and extremely easy to compute while containing global information. It allows us to deal with open surfaces, partial matching, and topological perturbations robustly. To solve the resulting quadratic assignment problem efficiently, the two key ideas of our iterative algorithm are: 1) select pairs with good (approximate) correspondence as anchor points, 2) solve a regularized quadratic assignment problem only in the neighborhood of selected anchor points through sparsity control. These two ingredients can improve and increase the number of anchor points quickly while reducing the computation cost in each quadratic assignment iteration significantly. With enough high-quality anchor points, one may use various pointwise global features with reference to these anchor points to further improve the dense shape correspondence. We use various experiments to show the efficiency, quality, and versatility of our method on large data sets, patches, and point clouds (without global meshes).



### Leveraging Frequency Analysis for Deep Fake Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/2003.08685v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.08685v3)
- **Published**: 2020-03-19 11:06:54+00:00
- **Updated**: 2020-06-26 11:21:45+00:00
- **Authors**: Joel Frank, Thorsten Eisenhofer, Lea Schönherr, Asja Fischer, Dorothea Kolossa, Thorsten Holz
- **Comment**: Accepted to ICML 2020. New experiments, updated several sections,
  code: https://github.com/RUB-SysSec/GANDCTAnalysis
- **Journal**: None
- **Summary**: Deep neural networks can generate images that are astonishingly realistic, so much so that it is often hard for humans to distinguish them from actual photos. These achievements have been largely made possible by Generative Adversarial Networks (GANs). While deep fake images have been thoroughly investigated in the image domain - a classical approach from the area of image forensics - an analysis in the frequency domain has been missing so far. In this paper, we address this shortcoming and our results reveal that in frequency space, GAN-generated images exhibit severe artifacts that can be easily identified. We perform a comprehensive analysis, showing that these artifacts are consistent across different neural network architectures, data sets, and resolutions. In a further investigation, we demonstrate that these artifacts are caused by upsampling operations found in all current GAN architectures, indicating a structural and fundamental problem in the way images are generated via GANs. Based on this analysis, we demonstrate how the frequency representation can be used to identify deep fake images in an automated way, surpassing state-of-the-art methods.



### Giving Commands to a Self-driving Car: A Multimodal Reasoner for Visual Grounding
- **Arxiv ID**: http://arxiv.org/abs/2003.08717v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2003.08717v3)
- **Published**: 2020-03-19 12:40:41+00:00
- **Updated**: 2021-05-26 12:08:13+00:00
- **Authors**: Thierry Deruyttere, Guillem Collell, Marie-Francine Moens
- **Comment**: Updated acknowledgements
- **Journal**: None
- **Summary**: We propose a new spatial memory module and a spatial reasoner for the Visual Grounding (VG) task. The goal of this task is to find a certain object in an image based on a given textual query. Our work focuses on integrating the regions of a Region Proposal Network (RPN) into a new multi-step reasoning model which we have named a Multimodal Spatial Region Reasoner (MSRR). The introduced model uses the object regions from an RPN as initialization of a 2D spatial memory and then implements a multi-step reasoning process scoring each region according to the query, hence why we call it a multimodal reasoner. We evaluate this new model on challenging datasets and our experiments show that our model that jointly reasons over the object regions of the image and words of the query largely improves accuracy compared to current state-of-the-art models.



### Exemplar Normalization for Learning Deep Representation
- **Arxiv ID**: http://arxiv.org/abs/2003.08761v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08761v2)
- **Published**: 2020-03-19 13:23:40+00:00
- **Updated**: 2020-03-20 14:58:40+00:00
- **Authors**: Ruimao Zhang, Zhanglin Peng, Lingyun Wu, Zhen Li, Ping Luo
- **Comment**: Accepted by CVPR2020, normalization methods, image classification
- **Journal**: None
- **Summary**: Normalization techniques are important in different advanced neural networks and different tasks. This work investigates a novel dynamic learning-to-normalize (L2N) problem by proposing Exemplar Normalization (EN), which is able to learn different normalization methods for different convolutional layers and image samples of a deep network. EN significantly improves flexibility of the recently proposed switchable normalization (SN), which solves a static L2N problem by linearly combining several normalizers in each normalization layer (the combination is the same for all samples). Instead of directly employing a multi-layer perceptron (MLP) to learn data-dependent parameters as conditional batch normalization (cBN) did, the internal architecture of EN is carefully designed to stabilize its optimization, leading to many appealing benefits. (1) EN enables different convolutional layers, image samples, categories, benchmarks, and tasks to use different normalization methods, shedding light on analyzing them in a holistic view. (2) EN is effective for various network architectures and tasks. (3) It could replace any normalization layers in a deep network and still produce stable model training. Extensive experiments demonstrate the effectiveness of EN in a wide spectrum of tasks including image recognition, noisy label learning, and semantic segmentation. For example, by replacing BN in the ordinary ResNet50, improvement produced by EN is 300% more than that of SN on both ImageNet and the noisy WebVision dataset.



### Self-Guided Adaptation: Progressive Representation Alignment for Domain Adaptive Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2003.08777v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08777v2)
- **Published**: 2020-03-19 13:30:45+00:00
- **Updated**: 2020-03-22 09:18:10+00:00
- **Authors**: Zongxian Li, Qixiang Ye, Chong Zhang, Jingjing Liu, Shijian Lu, Yonghong Tian
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised domain adaptation (UDA) has achieved unprecedented success in improving the cross-domain robustness of object detection models. However, existing UDA methods largely ignore the instantaneous data distribution during model learning, which could deteriorate the feature representation given large domain shift. In this work, we propose a Self-Guided Adaptation (SGA) model, target at aligning feature representation and transferring object detection models across domains while considering the instantaneous alignment difficulty. The core of SGA is to calculate "hardness" factors for sample pairs indicating domain distance in a kernel space. With the hardness factor, the proposed SGA adaptively indicates the importance of samples and assigns them different constrains. Indicated by hardness factors, Self-Guided Progressive Sampling (SPS) is implemented in an "easy-to-hard" way during model adaptation. Using multi-stage convolutional features, SGA is further aggregated to fully align hierarchical representations of detection models. Extensive experiments on commonly used benchmarks show that SGA improves the state-of-the-art methods with significant margins, while demonstrating the effectiveness on large domain shift.



### High-Resolution Daytime Translation Without Domain Labels
- **Arxiv ID**: http://arxiv.org/abs/2003.08791v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.08791v2)
- **Published**: 2020-03-19 13:59:31+00:00
- **Updated**: 2020-03-23 11:59:50+00:00
- **Authors**: Ivan Anokhin, Pavel Solovev, Denis Korzhenkov, Alexey Kharlamov, Taras Khakhulin, Alexey Silvestrov, Sergey Nikolenko, Victor Lempitsky, Gleb Sterkin
- **Comment**: accepted to CVPR 2020
- **Journal**: None
- **Summary**: Modeling daytime changes in high resolution photographs, e.g., re-rendering the same scene under different illuminations typical for day, night, or dawn, is a challenging image manipulation task. We present the high-resolution daytime translation (HiDT) model for this task. HiDT combines a generative image-to-image model and a new upsampling scheme that allows to apply image translation at high resolution. The model demonstrates competitive results in terms of both commonly used GAN metrics and human evaluation. Importantly, this good performance comes as a result of training on a dataset of still landscape images with no daytime labels available. Our results are available at https://saic-mdal.github.io/HiDT/.



### Generalizable Pedestrian Detection: The Elephant In The Room
- **Arxiv ID**: http://arxiv.org/abs/2003.08799v7
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08799v7)
- **Published**: 2020-03-19 14:14:52+00:00
- **Updated**: 2020-12-09 08:56:09+00:00
- **Authors**: Irtiza Hasan, Shengcai Liao, Jinpeng Li, Saad Ullah Akram, Ling Shao
- **Comment**: 10 pages, 1 figure
- **Journal**: None
- **Summary**: Pedestrian detection is used in many vision based applications ranging from video surveillance to autonomous driving. Despite achieving high performance, it is still largely unknown how well existing detectors generalize to unseen data. This is important because a practical detector should be ready to use in various scenarios in applications. To this end, we conduct a comprehensive study in this paper, using a general principle of direct cross-dataset evaluation. Through this study, we find that existing state-of-the-art pedestrian detectors, though perform quite well when trained and tested on the same dataset, generalize poorly in cross dataset evaluation. We demonstrate that there are two reasons for this trend. Firstly, their designs (e.g. anchor settings) may be biased towards popular benchmarks in the traditional single-dataset training and test pipeline, but as a result largely limit their generalization capability. Secondly, the training source is generally not dense in pedestrians and diverse in scenarios. Under direct cross-dataset evaluation, surprisingly, we find that a general purpose object detector, without pedestrian-tailored adaptation in design, generalizes much better compared to existing state-of-the-art pedestrian detectors. Furthermore, we illustrate that diverse and dense datasets, collected by crawling the web, serve to be an efficient source of pre-training for pedestrian detection. Accordingly, we propose a progressive training pipeline and find that it works well for autonomous-driving oriented pedestrian detection. Consequently, the study conducted in this paper suggests that more emphasis should be put on cross-dataset evaluation for the future design of generalizable pedestrian detectors. Code and models can be accessed at https://github.com/hasanirtiza/Pedestron.



### Multi-task Collaborative Network for Joint Referring Expression Comprehension and Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2003.08813v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08813v1)
- **Published**: 2020-03-19 14:25:18+00:00
- **Updated**: 2020-03-19 14:25:18+00:00
- **Authors**: Gen Luo, Yiyi Zhou, Xiaoshuai Sun, Liujuan Cao, Chenglin Wu, Cheng Deng, Rongrong Ji
- **Comment**: accpected by CVPR2020 (oral)
- **Journal**: None
- **Summary**: Referring expression comprehension (REC) and segmentation (RES) are two highly-related tasks, which both aim at identifying the referent according to a natural language expression. In this paper, we propose a novel Multi-task Collaborative Network (MCN) to achieve a joint learning of REC and RES for the first time. In MCN, RES can help REC to achieve better language-vision alignment, while REC can help RES to better locate the referent. In addition, we address a key challenge in this multi-task setup, i.e., the prediction conflict, with two innovative designs namely, Consistency Energy Maximization (CEM) and Adaptive Soft Non-Located Suppression (ASNLS). Specifically, CEM enables REC and RES to focus on similar visual regions by maximizing the consistency energy between two tasks. ASNLS supresses the response of unrelated regions in RES based on the prediction of REC. To validate our model, we conduct extensive experiments on three benchmark datasets of REC and RES, i.e., RefCOCO, RefCOCO+ and RefCOCOg. The experimental results report the significant performance gains of MCN over all existing methods, i.e., up to +7.13% for REC and +11.50% for RES over SOTA, which well confirm the validity of our model for joint REC and RES learning.



### Goal-Conditioned End-to-End Visuomotor Control for Versatile Skill Primitives
- **Arxiv ID**: http://arxiv.org/abs/2003.08854v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG, I.2.9; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2003.08854v3)
- **Published**: 2020-03-19 15:04:37+00:00
- **Updated**: 2021-09-24 14:01:22+00:00
- **Authors**: Oliver Groth, Chia-Man Hung, Andrea Vedaldi, Ingmar Posner
- **Comment**: revised manuscript with additional baselines and generalisation
  experiments; 11 pages, 8 figures, 7 tables
- **Journal**: None
- **Summary**: Visuomotor control (VMC) is an effective means of achieving basic manipulation tasks such as pushing or pick-and-place from raw images. Conditioning VMC on desired goal states is a promising way of achieving versatile skill primitives. However, common conditioning schemes either rely on task-specific fine tuning - e.g. using one-shot imitation learning (IL) - or on sampling approaches using a forward model of scene dynamics i.e. model-predictive control (MPC), leaving deployability and planning horizon severely limited. In this paper we propose a conditioning scheme which avoids these pitfalls by learning the controller and its conditioning in an end-to-end manner. Our model predicts complex action sequences based directly on a dynamic image representation of the robot motion and the distance to a given target observation. In contrast to related works, this enables our approach to efficiently perform complex manipulation tasks from raw image observations without predefined control primitives or test time demonstrations. We report significant improvements in task success over representative MPC and IL baselines. We also demonstrate our model's generalisation capabilities in challenging, unseen tasks featuring visual noise, cluttered scenes and unseen object geometries.



### DRST: Deep Residual Shearlet Transform for Densely Sampled Light Field Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2003.08865v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.08865v1)
- **Published**: 2020-03-19 15:28:08+00:00
- **Updated**: 2020-03-19 15:28:08+00:00
- **Authors**: Yuan Gao, Robert Bregovic, Reinhard Koch, Atanas Gotchev
- **Comment**: None
- **Journal**: None
- **Summary**: The Image-Based Rendering (IBR) approach using Shearlet Transform (ST) is one of the most effective methods for Densely-Sampled Light Field (DSLF) reconstruction. The ST-based DSLF reconstruction typically relies on an iterative thresholding algorithm for Epipolar-Plane Image (EPI) sparse regularization in shearlet domain, involving dozens of transformations between image domain and shearlet domain, which are in general time-consuming. To overcome this limitation, a novel learning-based ST approach, referred to as Deep Residual Shearlet Transform (DRST), is proposed in this paper. Specifically, for an input sparsely-sampled EPI, DRST employs a deep fully Convolutional Neural Network (CNN) to predict the residuals of the shearlet coefficients in shearlet domain in order to reconstruct a densely-sampled EPI in image domain. The DRST network is trained on synthetic Sparsely-Sampled Light Field (SSLF) data only by leveraging elaborately-designed masks. Experimental results on three challenging real-world light field evaluation datasets with varying moderate disparity ranges (8 - 16 pixels) demonstrate the superiority of the proposed learning-based DRST approach over the non-learning-based ST method for DSLF reconstruction. Moreover, DRST provides a 2.4x speedup over ST, at least.



### Spatially Adaptive Inference with Stochastic Feature Sampling and Interpolation
- **Arxiv ID**: http://arxiv.org/abs/2003.08866v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08866v4)
- **Published**: 2020-03-19 15:36:31+00:00
- **Updated**: 2020-09-04 07:40:10+00:00
- **Authors**: Zhenda Xie, Zheng Zhang, Xizhou Zhu, Gao Huang, Stephen Lin
- **Comment**: ECCV2020(Oral), Code:
  https://github.com/zdaxie/SpatiallyAdaptiveInference-Detection
- **Journal**: None
- **Summary**: In the feature maps of CNNs, there commonly exists considerable spatial redundancy that leads to much repetitive processing. Towards reducing this superfluous computation, we propose to compute features only at sparsely sampled locations, which are probabilistically chosen according to activation responses, and then densely reconstruct the feature map with an efficient interpolation procedure. With this sampling-interpolation scheme, our network avoids expending computation on spatial locations that can be effectively interpolated, while being robust to activation prediction errors through broadly distributed sampling. A technical challenge of this sampling-based approach is that the binary decision variables for representing discrete sampling locations are non-differentiable, making them incompatible with backpropagation. To circumvent this issue, we make use of a reparameterization trick based on the Gumbel-Softmax distribution, with which backpropagation can iterate these variables towards binary values. The presented network is experimentally shown to save substantial computation while maintaining accuracy over a variety of computer vision tasks.



### Brain tumor segmentation with missing modalities via latent multi-source correlation representation
- **Arxiv ID**: http://arxiv.org/abs/2003.08870v5
- **DOI**: 10.1007/978-3-030-59719-1_52
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.08870v5)
- **Published**: 2020-03-19 15:47:36+00:00
- **Updated**: 2021-04-20 12:27:22+00:00
- **Authors**: Tongxue Zhou, Stéphane Canu, Pierre Vera, Su Ruan
- **Comment**: 10 pages, 6 figures, accepted by MICCAI 2020. arXiv admin note: text
  overlap with arXiv:2102.03111
- **Journal**: MICCAI 2020 pp. 533-541
- **Summary**: Multimodal MR images can provide complementary information for accurate brain tumor segmentation. However, it's common to have missing imaging modalities in clinical practice. Since there exists a strong correlation between multi modalities, a novel correlation representation block is proposed to specially discover the latent multi-source correlation. Thanks to the obtained correlation representation, the segmentation becomes more robust in the case of missing modalities. The model parameter estimation module first maps the individual representation produced by each encoder to obtain independent parameters, then, under these parameters, the correlation expression module transforms all the individual representations to form a latent multi-source correlation representation. Finally, the correlation representations across modalities are fused via the attention mechanism into a shared representation to emphasize the most important features for segmentation. We evaluate our model on BraTS 2018 datasets, it outperforms the current state-of-the-art method and produces robust results when one or more modalities are missing.



### Across Scales & Across Dimensions: Temporal Super-Resolution using Deep Internal Learning
- **Arxiv ID**: http://arxiv.org/abs/2003.08872v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08872v3)
- **Published**: 2020-03-19 15:53:01+00:00
- **Updated**: 2020-10-15 10:09:29+00:00
- **Authors**: Liad Pollak Zuckerman, Eyal Naor, George Pisha, Shai Bagon, Michal Irani
- **Comment**: Accepted to ECCV 2020
- **Journal**: None
- **Summary**: When a very fast dynamic event is recorded with a low-framerate camera, the resulting video suffers from severe motion blur (due to exposure time) and motion aliasing (due to low sampling rate in time). True Temporal Super-Resolution (TSR) is more than just Temporal-Interpolation (increasing framerate). It can also recover new high temporal frequencies beyond the temporal Nyquist limit of the input video, thus resolving both motion-blur and motion-aliasing effects that temporal frame interpolation (as sophisticated as it maybe) cannot undo. In this paper we propose a "Deep Internal Learning" approach for true TSR. We train a video-specific CNN on examples extracted directly from the low-framerate input video. Our method exploits the strong recurrence of small space-time patches inside a single video sequence, both within and across different spatio-temporal scales of the video. We further observe (for the first time) that small space-time patches recur also across-dimensions of the video sequence - i.e., by swapping the spatial and temporal dimensions. In particular, the higher spatial resolution of video frames provides strong examples as to how to increase the temporal resolution of that video. Such internal video-specific examples give rise to strong self-supervision, requiring no data but the input video itself. This results in Zero-Shot Temporal-SR of complex videos, which removes both motion blur and motion aliasing, outperforming previous supervised methods trained on external video datasets.



### Unique Geometry and Texture from Corresponding Image Patches
- **Arxiv ID**: http://arxiv.org/abs/2003.08885v3
- **DOI**: 10.1109/TPAMI.2021.3081360
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08885v3)
- **Published**: 2020-03-19 16:14:13+00:00
- **Updated**: 2021-11-07 01:47:04+00:00
- **Authors**: Dor Verbin, Steven J. Gortler, Todd Zickler
- **Comment**: None
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence,
  Volume: 43, Issue: 12, Dec. 1 2021
- **Summary**: We present a sufficient condition for recovering unique texture and viewpoints from unknown orthographic projections of a flat texture process. We show that four observations are sufficient in general, and we characterize the ambiguous cases. The results are applicable to shape from texture and texture-based structure from motion.



### Local Rotation Invariance in 3D CNNs
- **Arxiv ID**: http://arxiv.org/abs/2003.08890v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.08890v1)
- **Published**: 2020-03-19 16:24:49+00:00
- **Updated**: 2020-03-19 16:24:49+00:00
- **Authors**: Vincent Andrearczyk, Julien Fageot, Valentin Oreiller, Xavier Montet, Adrien Depeursinge
- **Comment**: None
- **Journal**: None
- **Summary**: Locally Rotation Invariant (LRI) image analysis was shown to be fundamental in many applications and in particular in medical imaging where local structures of tissues occur at arbitrary rotations. LRI constituted the cornerstone of several breakthroughs in texture analysis, including Local Binary Patterns (LBP), Maximum Response 8 (MR8) and steerable filterbanks. Whereas globally rotation invariant Convolutional Neural Networks (CNN) were recently proposed, LRI was very little investigated in the context of deep learning. LRI designs allow learning filters accounting for all orientations, which enables a drastic reduction of trainable parameters and training data when compared to standard 3D CNNs. In this paper, we propose and compare several methods to obtain LRI CNNs with directional sensitivity. Two methods use orientation channels (responses to rotated kernels), either by explicitly rotating the kernels or using steerable filters. These orientation channels constitute a locally rotation equivariant representation of the data. Local pooling across orientations yields LRI image analysis. Steerable filters are used to achieve a fine and efficient sampling of 3D rotations as well as a reduction of trainable parameters and operations, thanks to a parametric representations involving solid Spherical Harmonics (SH), which are products of SH with associated learned radial profiles.Finally, we investigate a third strategy to obtain LRI based on rotational invariants calculated from responses to a learned set of solid SHs. The proposed methods are evaluated and compared to standard CNNs on 3D datasets including synthetic textured volumes composed of rotated patterns, and pulmonary nodule classification in CT. The results show the importance of LRI image analysis while resulting in a drastic reduction of trainable parameters, outperforming standard 3D CNNs trained with data augmentation.



### Normalized and Geometry-Aware Self-Attention Network for Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/2003.08897v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2003.08897v1)
- **Published**: 2020-03-19 16:54:16+00:00
- **Updated**: 2020-03-19 16:54:16+00:00
- **Authors**: Longteng Guo, Jing Liu, Xinxin Zhu, Peng Yao, Shichen Lu, Hanqing Lu
- **Comment**: Accepted by CVPR 2020
- **Journal**: None
- **Summary**: Self-attention (SA) network has shown profound value in image captioning. In this paper, we improve SA from two aspects to promote the performance of image captioning. First, we propose Normalized Self-Attention (NSA), a reparameterization of SA that brings the benefits of normalization inside SA. While normalization is previously only applied outside SA, we introduce a novel normalization method and demonstrate that it is both possible and beneficial to perform it on the hidden activations inside SA. Second, to compensate for the major limit of Transformer that it fails to model the geometry structure of the input objects, we propose a class of Geometry-aware Self-Attention (GSA) that extends SA to explicitly and efficiently consider the relative geometry relations between the objects in the image. To construct our image captioning model, we combine the two modules and apply it to the vanilla self-attention network. We extensively evaluate our proposals on MS-COCO image captioning dataset and superior results are achieved when comparing to state-of-the-art approaches. Further experiments on three challenging tasks, i.e. video captioning, machine translation, and visual question answering, show the generality of our methods.



### Overinterpretation reveals image classification model pathologies
- **Arxiv ID**: http://arxiv.org/abs/2003.08907v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.08907v3)
- **Published**: 2020-03-19 17:12:23+00:00
- **Updated**: 2021-12-07 16:38:50+00:00
- **Authors**: Brandon Carter, Siddhartha Jain, Jonas Mueller, David Gifford
- **Comment**: NeurIPS 2021
- **Journal**: None
- **Summary**: Image classifiers are typically scored on their test set accuracy, but high accuracy can mask a subtle type of model failure. We find that high scoring convolutional neural networks (CNNs) on popular benchmarks exhibit troubling pathologies that allow them to display high accuracy even in the absence of semantically salient features. When a model provides a high-confidence decision without salient supporting input features, we say the classifier has overinterpreted its input, finding too much class-evidence in patterns that appear nonsensical to humans. Here, we demonstrate that neural networks trained on CIFAR-10 and ImageNet suffer from overinterpretation, and we find models on CIFAR-10 make confident predictions even when 95% of input images are masked and humans cannot discern salient features in the remaining pixel-subsets. We introduce Batched Gradient SIS, a new method for discovering sufficient input subsets for complex datasets, and use this method to show the sufficiency of border pixels in ImageNet for training and testing. Although these patterns portend potential model fragility in real-world deployment, they are in fact valid statistical patterns of the benchmark that alone suffice to attain high test accuracy. Unlike adversarial examples, overinterpretation relies upon unmodified image pixels. We find ensembling and input dropout can each help mitigate overinterpretation.



### GIQA: Generated Image Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/2003.08932v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.08932v3)
- **Published**: 2020-03-19 17:56:08+00:00
- **Updated**: 2020-07-14 09:57:48+00:00
- **Authors**: Shuyang Gu, Jianmin Bao, Dong Chen, Fang Wen
- **Comment**: ECCV2020
- **Journal**: None
- **Summary**: Generative adversarial networks (GANs) have achieved impressive results today, but not all generated images are perfect. A number of quantitative criteria have recently emerged for generative model, but none of them are designed for a single generated image. In this paper, we propose a new research topic, Generated Image Quality Assessment (GIQA), which quantitatively evaluates the quality of each generated image. We introduce three GIQA algorithms from two perspectives: learning-based and data-based. We evaluate a number of images generated by various recent GAN models on different datasets and demonstrate that they are consistent with human assessments. Furthermore, GIQA is available to many applications, like separately evaluating the realism and diversity of generative models, and enabling online hard negative mining (OHEM) in the training of GANs to improve the results.



### DELTAS: Depth Estimation by Learning Triangulation And densification of Sparse points
- **Arxiv ID**: http://arxiv.org/abs/2003.08933v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.08933v2)
- **Published**: 2020-03-19 17:56:41+00:00
- **Updated**: 2020-08-25 17:59:12+00:00
- **Authors**: Ayan Sinha, Zak Murez, James Bartolozzi, Vijay Badrinarayanan, Andrew Rabinovich
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: Multi-view stereo (MVS) is the golden mean between the accuracy of active depth sensing and the practicality of monocular depth estimation. Cost volume based approaches employing 3D convolutional neural networks (CNNs) have considerably improved the accuracy of MVS systems. However, this accuracy comes at a high computational cost which impedes practical adoption. Distinct from cost volume approaches, we propose an efficient depth estimation approach by first (a) detecting and evaluating descriptors for interest points, then (b) learning to match and triangulate a small set of interest points, and finally (c) densifying this sparse set of 3D points using CNNs. An end-to-end network efficiently performs all three steps within a deep learning framework and trained with intermediate 2D image and 3D geometric supervision, along with depth supervision. Crucially, our first step complements pose estimation using interest point detection and descriptor learning. We demonstrate state-of-the-art results on depth estimation with lower compute for different scene lengths. Furthermore, our method generalizes to newer environments and the descriptors output by our network compare favorably to strong baselines. Code is available at https://github.com/magicleap/DELTAS



### NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2003.08934v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2003.08934v2)
- **Published**: 2020-03-19 17:57:23+00:00
- **Updated**: 2020-08-03 22:17:31+00:00
- **Authors**: Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng
- **Comment**: ECCV 2020 (oral). Project page with videos and code:
  http://tancik.com/nerf
- **Journal**: None
- **Summary**: We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\theta, \phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.



### Group Sparsity: The Hinge Between Filter Pruning and Decomposition for Network Compression
- **Arxiv ID**: http://arxiv.org/abs/2003.08935v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08935v1)
- **Published**: 2020-03-19 17:57:26+00:00
- **Updated**: 2020-03-19 17:57:26+00:00
- **Authors**: Yawei Li, Shuhang Gu, Christoph Mayer, Luc Van Gool, Radu Timofte
- **Comment**: Accepted by CVPR2020. Code is available at
  https://github.com/ofsoundof/group_sparsity
- **Journal**: None
- **Summary**: In this paper, we analyze two popular network compression techniques, i.e. filter pruning and low-rank decomposition, in a unified sense. By simply changing the way the sparsity regularization is enforced, filter pruning and low-rank decomposition can be derived accordingly. This provides another flexible choice for network compression because the techniques complement each other. For example, in popular network architectures with shortcut connections (e.g. ResNet), filter pruning cannot deal with the last convolutional layer in a ResBlock while the low-rank decomposition methods can. In addition, we propose to compress the whole network jointly instead of in a layer-wise manner. Our approach proves its potential as it compares favorably to the state-of-the-art on several benchmarks.



### GAN Compression: Efficient Architectures for Interactive Conditional GANs
- **Arxiv ID**: http://arxiv.org/abs/2003.08936v4
- **DOI**: 10.1109/TPAMI.2021.3126742
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.08936v4)
- **Published**: 2020-03-19 17:59:05+00:00
- **Updated**: 2021-11-11 03:45:16+00:00
- **Authors**: Muyang Li, Ji Lin, Yaoyao Ding, Zhijian Liu, Jun-Yan Zhu, Song Han
- **Comment**: None
- **Journal**: None
- **Summary**: Conditional Generative Adversarial Networks (cGANs) have enabled controllable image synthesis for many vision and graphics applications. However, recent cGANs are 1-2 orders of magnitude more compute-intensive than modern recognition CNNs. For example, GauGAN consumes 281G MACs per image, compared to 0.44G MACs for MobileNet-v3, making it difficult for interactive deployment. In this work, we propose a general-purpose compression framework for reducing the inference time and model size of the generator in cGANs. Directly applying existing compression methods yields poor performance due to the difficulty of GAN training and the differences in generator architectures. We address these challenges in two ways. First, to stabilize GAN training, we transfer knowledge of multiple intermediate representations of the original model to its compressed model and unify unpaired and paired learning. Second, instead of reusing existing CNN designs, our method finds efficient architectures via neural architecture search. To accelerate the search process, we decouple the model training and search via weight sharing. Experiments demonstrate the effectiveness of our method across different supervision settings, network architectures, and learning methods. Without losing image quality, we reduce the computation of CycleGAN by 21x, Pix2pix by 12x, MUNIT by 29x, and GauGAN by 9x, paving the way for interactive image synthesis.



### Temporal Extension Module for Skeleton-Based Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2003.08951v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08951v2)
- **Published**: 2020-03-19 18:00:04+00:00
- **Updated**: 2020-10-19 02:39:04+00:00
- **Authors**: Yuya Obinata, Takuma Yamamoto
- **Comment**: Accepted on ICPR2020, 7 pages, 4 figures
- **Journal**: None
- **Summary**: We present a module that extends the temporal graph of a graph convolutional network (GCN) for action recognition with a sequence of skeletons. Existing methods attempt to represent a more appropriate spatial graph on an intra-frame, but disregard optimization of the temporal graph on the interframe. Concretely, these methods connect between vertices corresponding only to the same joint on the inter-frame. In this work, we focus on adding connections to neighboring multiple vertices on the inter-frame and extracting additional features based on the extended temporal graph. Our module is a simple yet effective method to extract correlated features of multiple joints in human movement. Moreover, our module aids in further performance improvements, along with other GCN methods that optimize only the spatial graph. We conduct extensive experiments on two large datasets, NTU RGB+D and Kinetics-Skeleton, and demonstrate that our module is effective for several existing models and our final model achieves state-of-the-art performance.



### Vox2Vox: 3D-GAN for Brain Tumour Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2003.13653v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.13653v3)
- **Published**: 2020-03-19 18:57:08+00:00
- **Updated**: 2020-11-26 11:38:25+00:00
- **Authors**: Marco Domenico Cirillo, David Abramian, Anders Eklund
- **Comment**: None
- **Journal**: None
- **Summary**: Gliomas are the most common primary brain malignancies, with different degrees of aggressiveness, variable prognosis and various heterogeneous histological sub-regions, i.e., peritumoral edema, necrotic core, enhancing and non-enhancing tumour core. Although brain tumours can easily be detected using multi-modal MRI, accurate tumor segmentation is a challenging task. Hence, using the data provided by the BraTS Challenge 2020, we propose a 3D volume-to-volume Generative Adversarial Network for segmentation of brain tumours. The model, called Vox2Vox, generates realistic segmentation outputs from multi-channel 3D MR images, segmenting the whole, core and enhancing tumor with mean values of 87.20%, 81.14%, and 78.67% as dice scores and 6.44mm, 24.36mm, and 18.95mm for Hausdorff distance 95 percentile for the BraTS testing set after ensembling 10 Vox2Vox models obtained with a 10-fold cross-validation.



### Local Implicit Grid Representations for 3D Scenes
- **Arxiv ID**: http://arxiv.org/abs/2003.08981v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.08981v1)
- **Published**: 2020-03-19 18:58:13+00:00
- **Updated**: 2020-03-19 18:58:13+00:00
- **Authors**: Chiyu Max Jiang, Avneesh Sud, Ameesh Makadia, Jingwei Huang, Matthias Nießner, Thomas Funkhouser
- **Comment**: CVPR 2020. Supplementary Video: https://youtu.be/XCyl1-vxfII
- **Journal**: None
- **Summary**: Shape priors learned from data are commonly used to reconstruct 3D objects from partial or noisy data. Yet no such shape priors are available for indoor scenes, since typical 3D autoencoders cannot handle their scale, complexity, or diversity. In this paper, we introduce Local Implicit Grid Representations, a new 3D shape representation designed for scalability and generality. The motivating idea is that most 3D surfaces share geometric details at some scale -- i.e., at a scale smaller than an entire object and larger than a small patch. We train an autoencoder to learn an embedding of local crops of 3D shapes at that size. Then, we use the decoder as a component in a shape optimization that solves for a set of latent codes on a regular grid of overlapping crops such that an interpolation of the decoded local shapes matches a partial or noisy observation. We demonstrate the value of this proposed approach for 3D surface reconstruction from sparse point observations, showing significantly better results than alternative approaches.



### A unifying mutual information view of metric learning: cross-entropy vs. pairwise losses
- **Arxiv ID**: http://arxiv.org/abs/2003.08983v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.08983v3)
- **Published**: 2020-03-19 18:59:54+00:00
- **Updated**: 2021-11-26 09:56:44+00:00
- **Authors**: Malik Boudiaf, Jérôme Rony, Imtiaz Masud Ziko, Eric Granger, Marco Pedersoli, Pablo Piantanida, Ismail Ben Ayed
- **Comment**: ECCV 2020 (Spotlight) - Code available at:
  https://github.com/jeromerony/dml_cross_entropy
- **Journal**: None
- **Summary**: Recently, substantial research efforts in Deep Metric Learning (DML) focused on designing complex pairwise-distance losses, which require convoluted schemes to ease optimization, such as sample mining or pair weighting. The standard cross-entropy loss for classification has been largely overlooked in DML. On the surface, the cross-entropy may seem unrelated and irrelevant to metric learning as it does not explicitly involve pairwise distances. However, we provide a theoretical analysis that links the cross-entropy to several well-known and recent pairwise losses. Our connections are drawn from two different perspectives: one based on an explicit optimization insight; the other on discriminative and generative views of the mutual information between the labels and the learned features. First, we explicitly demonstrate that the cross-entropy is an upper bound on a new pairwise loss, which has a structure similar to various pairwise losses: it minimizes intra-class distances while maximizing inter-class distances. As a result, minimizing the cross-entropy can be seen as an approximate bound-optimization (or Majorize-Minimize) algorithm for minimizing this pairwise loss. Second, we show that, more generally, minimizing the cross-entropy is actually equivalent to maximizing the mutual information, to which we connect several well-known pairwise losses. Furthermore, we show that various standard pairwise losses can be explicitly related to one another via bound relationships. Our findings indicate that the cross-entropy represents a proxy for maximizing the mutual information -- as pairwise losses do -- without the need for convoluted sample-mining heuristics. Our experiments over four standard DML benchmarks strongly support our findings. We obtain state-of-the-art results, outperforming recent and complex DML methods.



### MOT20: A benchmark for multi object tracking in crowded scenes
- **Arxiv ID**: http://arxiv.org/abs/2003.09003v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.09003v1)
- **Published**: 2020-03-19 20:08:24+00:00
- **Updated**: 2020-03-19 20:08:24+00:00
- **Authors**: Patrick Dendorfer, Hamid Rezatofighi, Anton Milan, Javen Shi, Daniel Cremers, Ian Reid, Stefan Roth, Konrad Schindler, Laura Leal-Taixé
- **Comment**: The sequences of the new MOT20 benchmark were previously presented in
  the CVPR 2019 tracking challenge ( arXiv:1906.04567 ). The differences
  between the two challenges are: - New and corrected annotations - New
  sequences, as we had to crop and transform some old sequences to achieve
  higher quality in the annotations. - New baselines evaluations and different
  sets of public detections
- **Journal**: None
- **Summary**: Standardized benchmarks are crucial for the majority of computer vision applications. Although leaderboards and ranking tables should not be over-claimed, benchmarks often provide the most objective measure of performance and are therefore important guides for research. The benchmark for Multiple Object Tracking, MOTChallenge, was launched with the goal to establish a standardized evaluation of multiple object tracking methods. The challenge focuses on multiple people tracking, since pedestrians are well studied in the tracking community, and precise tracking and detection has high practical relevance. Since the first release, MOT15, MOT16, and MOT17 have tremendously contributed to the community by introducing a clean dataset and precise framework to benchmark multi-object trackers. In this paper, we present our MOT20benchmark, consisting of 8 new sequences depicting very crowded challenging scenes. The benchmark was presented first at the 4thBMTT MOT Challenge Workshop at the Computer Vision and Pattern Recognition Conference (CVPR) 2019, and gives to chance to evaluate state-of-the-art methods for multiple object tracking when handling extremely crowded scenarios.



### Semi-Supervised Semantic Segmentation with Cross-Consistency Training
- **Arxiv ID**: http://arxiv.org/abs/2003.09005v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.09005v3)
- **Published**: 2020-03-19 20:10:37+00:00
- **Updated**: 2020-06-09 14:11:06+00:00
- **Authors**: Yassine Ouali, Céline Hudelot, Myriam Tami
- **Comment**: Published at CVPR 2020
- **Journal**: None
- **Summary**: In this paper, we present a novel cross-consistency based semi-supervised approach for semantic segmentation. Consistency training has proven to be a powerful semi-supervised learning framework for leveraging unlabeled data under the cluster assumption, in which the decision boundary should lie in low-density regions. In this work, we first observe that for semantic segmentation, the low-density regions are more apparent within the hidden representations than within the inputs. We thus propose cross-consistency training, where an invariance of the predictions is enforced over different perturbations applied to the outputs of the encoder. Concretely, a shared encoder and a main decoder are trained in a supervised manner using the available labeled examples. To leverage the unlabeled examples, we enforce a consistency between the main decoder predictions and those of the auxiliary decoders, taking as inputs different perturbed versions of the encoder's output, and consequently, improving the encoder's representations. The proposed method is simple and can easily be extended to use additional training signal, such as image-level labels or pixel-level labels across different domains. We perform an ablation study to tease apart the effectiveness of each component, and conduct extensive experiments to demonstrate that our method achieves state-of-the-art results in several datasets.



### Multilayer Dense Connections for Hierarchical Concept Classification
- **Arxiv ID**: http://arxiv.org/abs/2003.09015v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.09015v2)
- **Published**: 2020-03-19 20:56:09+00:00
- **Updated**: 2021-02-22 19:20:39+00:00
- **Authors**: Toufiq Parag, Hongcheng Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Classification is a pivotal function for many computer vision tasks such as object classification, detection, scene segmentation. Multinomial logistic regression with a single final layer of dense connections has become the ubiquitous technique for CNN-based classification. While these classifiers project a mapping between the input and a set of output category classes, they do not typically yield a comprehensive description of the category. In particular, when a CNN based image classifier correctly identifies the image of a Chimpanzee, its output does not clarify that Chimpanzee is a member of Primate, Mammal, Chordate families and a living thing. We propose a multilayer dense connectivity for concurrent prediction of category and its conceptual superclasses in hierarchical order by the same CNN. We experimentally demonstrate that our proposed network can simultaneously predict both the coarse superclasses and finer categories better than several existing algorithms in multiple datasets.



### Efficient Deep Representation Learning by Adaptive Latent Space Sampling
- **Arxiv ID**: http://arxiv.org/abs/2004.02757v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.02757v2)
- **Published**: 2020-03-19 22:17:02+00:00
- **Updated**: 2020-04-12 18:25:55+00:00
- **Authors**: Yuanhan Mo, Shuo Wang, Chengliang Dai, Rui Zhou, Zhongzhao Teng, Wenjia Bai, Yike Guo
- **Comment**: None
- **Journal**: None
- **Summary**: Supervised deep learning requires a large amount of training samples with annotations (e.g. label class for classification task, pixel- or voxel-wised label map for segmentation tasks), which are expensive and time-consuming to obtain. During the training of a deep neural network, the annotated samples are fed into the network in a mini-batch way, where they are often regarded of equal importance. However, some of the samples may become less informative during training, as the magnitude of the gradient start to vanish for these samples. In the meantime, other samples of higher utility or hardness may be more demanded for the training process to proceed and require more exploitation. To address the challenges of expensive annotations and loss of sample informativeness, here we propose a novel training framework which adaptively selects informative samples that are fed to the training process. The adaptive selection or sampling is performed based on a hardness-aware strategy in the latent space constructed by a generative model. To evaluate the proposed training framework, we perform experiments on three different datasets, including MNIST and CIFAR-10 for image classification task and a medical image dataset IVUS for biophysical simulation task. On all three datasets, the proposed framework outperforms a random sampling method, which demonstrates the effectiveness of proposed framework.



### Microvasculature Segmentation and Inter-capillary Area Quantification of the Deep Vascular Complex using Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/2003.09033v1
- **DOI**: 10.1167/tvst.9.2.38
- **Categories**: **eess.IV**, cs.CV, cs.LG, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2003.09033v1)
- **Published**: 2020-03-19 22:27:02+00:00
- **Updated**: 2020-03-19 22:27:02+00:00
- **Authors**: Julian Lo, Morgan Heisler, Vinicius Vanzan, Sonja Karst, Ivana Zadro Matovinovic, Sven Loncaric, Eduardo V. Navajas, Mirza Faisal Beg, Marinko V. Sarunic
- **Comment**: 27 pages, 8 figures
- **Journal**: None
- **Summary**: Purpose: Optical Coherence Tomography Angiography (OCT-A) permits visualization of the changes to the retinal circulation due to diabetic retinopathy (DR), a microvascular complication of diabetes. We demonstrate accurate segmentation of the vascular morphology for the superficial capillary plexus and deep vascular complex (SCP and DVC) using a convolutional neural network (CNN) for quantitative analysis.   Methods: Retinal OCT-A with a 6x6mm field of view (FOV) were acquired using a Zeiss PlexElite. Multiple-volume acquisition and averaging enhanced the vessel network contrast used for training the CNN. We used transfer learning from a CNN trained on 76 images from smaller FOVs of the SCP acquired using different OCT systems. Quantitative analysis of perfusion was performed on the automated vessel segmentations in representative patients with DR.   Results: The automated segmentations of the OCT-A images maintained the hierarchical branching and lobular morphologies of the SCP and DVC, respectively. The network segmented the SCP with an accuracy of 0.8599, and a Dice index of 0.8618. For the DVC, the accuracy was 0.7986, and the Dice index was 0.8139. The inter-rater comparisons for the SCP had an accuracy and Dice index of 0.8300 and 0.6700, respectively, and 0.6874 and 0.7416 for the DVC.   Conclusions: Transfer learning reduces the amount of manually-annotated images required, while producing high quality automatic segmentations of the SCP and DVC. Using high quality training data preserves the characteristic appearance of the capillary networks in each layer.   Translational Relevance: Accurate retinal microvasculature segmentation with the CNN results in improved perfusion analysis in diabetic retinopathy.



### VisuoSpatial Foresight for Multi-Step, Multi-Task Fabric Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2003.09044v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.09044v3)
- **Published**: 2020-03-19 23:12:10+00:00
- **Updated**: 2021-02-18 07:10:49+00:00
- **Authors**: Ryan Hoque, Daniel Seita, Ashwin Balakrishna, Aditya Ganapathi, Ajay Kumar Tanwani, Nawid Jamali, Katsu Yamane, Soshi Iba, Ken Goldberg
- **Comment**: Robotics: Science and Systems (RSS) 2020
- **Journal**: None
- **Summary**: Robotic fabric manipulation has applications in home robotics, textiles, senior care and surgery. Existing fabric manipulation techniques, however, are designed for specific tasks, making it difficult to generalize across different but related tasks. We extend the Visual Foresight framework to learn fabric dynamics that can be efficiently reused to accomplish different fabric manipulation tasks with a single goal-conditioned policy. We introduce VisuoSpatial Foresight (VSF), which builds on prior work by learning visual dynamics on domain randomized RGB images and depth maps simultaneously and completely in simulation. We experimentally evaluate VSF on multi-step fabric smoothing and folding tasks against 5 baseline methods in simulation and on the da Vinci Research Kit (dVRK) surgical robot without any demonstrations at train or test time. Furthermore, we find that leveraging depth significantly improves performance. RGBD data yields an 80% improvement in fabric folding success rate over pure RGB data. Code, data, videos, and supplementary material are available at https://sites.google.com/view/fabric-vsf/.



### Affinity Graph Supervision for Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/2003.09049v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.09049v1)
- **Published**: 2020-03-19 23:52:51+00:00
- **Updated**: 2020-03-19 23:52:51+00:00
- **Authors**: Chu Wang, Babak Samari, Vladimir G. Kim, Siddhartha Chaudhuri, Kaleem Siddiqi
- **Comment**: None
- **Journal**: None
- **Summary**: Affinity graphs are widely used in deep architectures, including graph convolutional neural networks and attention networks. Thus far, the literature has focused on abstracting features from such graphs, while the learning of the affinities themselves has been overlooked. Here we propose a principled method to directly supervise the learning of weights in affinity graphs, to exploit meaningful connections between entities in the data source. Applied to a visual attention network, our affinity supervision improves relationship recovery between objects, even without the use of manually annotated relationship labels. We further show that affinity learning between objects boosts scene categorization performance and that the supervision of affinity can also be applied to graphs built from mini-batches, for neural network training. In an image classification task we demonstrate consistent improvement over the baseline, with diverse network architectures and datasets.



