# Arxiv Papers in cs.CV on 2020-03-11
### Visual Grounding in Video for Unsupervised Word Translation
- **Arxiv ID**: http://arxiv.org/abs/2003.05078v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.05078v2)
- **Published**: 2020-03-11 02:03:37+00:00
- **Updated**: 2020-03-26 15:20:44+00:00
- **Authors**: Gunnar A. Sigurdsson, Jean-Baptiste Alayrac, Aida Nematzadeh, Lucas Smaira, Mateusz Malinowski, Jo√£o Carreira, Phil Blunsom, Andrew Zisserman
- **Comment**: CVPR 2020
- **Journal**: CVPR 2020
- **Summary**: There are thousands of actively spoken languages on Earth, but a single visual world. Grounding in this visual world has the potential to bridge the gap between all these languages. Our goal is to use visual grounding to improve unsupervised word mapping between languages. The key idea is to establish a common visual representation between two languages by learning embeddings from unpaired instructional videos narrated in the native language. Given this shared embedding we demonstrate that (i) we can map words between the languages, particularly the 'visual' words; (ii) that the shared embedding provides a good initialization for existing unsupervised text-based word translation techniques, forming the basis for our proposed hybrid visual-text mapping algorithm, MUVE; and (iii) our approach achieves superior performance by addressing the shortcomings of text-based methods -- it is more robust, handles datasets with less commonality, and is applicable to low-resource languages. We apply these methods to translate words from English to French, Korean, and Japanese -- all without any parallel corpora and simply by watching many videos of people speaking while doing things.



### SOS: Selective Objective Switch for Rapid Immunofluorescence Whole Slide Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2003.05080v1
- **DOI**: 10.1109/CVPR42600.2020.00392
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.05080v1)
- **Published**: 2020-03-11 02:08:46+00:00
- **Updated**: 2020-03-11 02:08:46+00:00
- **Authors**: Sam Maksoud, Kun Zhao, Peter Hobson, Anthony Jennings, Brian Lovell
- **Comment**: Accepted for publication at CVPR2020
- **Journal**: None
- **Summary**: The difficulty of processing gigapixel whole slide images (WSIs) in clinical microscopy has been a long-standing barrier to implementing computer aided diagnostic systems. Since modern computing resources are unable to perform computations at this extremely large scale, current state of the art methods utilize patch-based processing to preserve the resolution of WSIs. However, these methods are often resource intensive and make significant compromises on processing time. In this paper, we demonstrate that conventional patch-based processing is redundant for certain WSI classification tasks where high resolution is only required in a minority of cases. This reflects what is observed in clinical practice; where a pathologist may screen slides using a low power objective and only switch to a high power in cases where they are uncertain about their findings. To eliminate these redundancies, we propose a method for the selective use of high resolution processing based on the confidence of predictions on downscaled WSIs --- we call this the Selective Objective Switch (SOS). Our method is validated on a novel dataset of 684 Liver-Kidney-Stomach immunofluorescence WSIs routinely used in the investigation of autoimmune liver disease. By limiting high resolution processing to cases which cannot be classified confidently at low resolution, we maintain the accuracy of patch-level analysis whilst reducing the inference time by a factor of 7.74.



### Learning-Based Human Segmentation and Velocity Estimation Using Automatic Labeled LiDAR Sequence for Training
- **Arxiv ID**: http://arxiv.org/abs/2003.05093v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.05093v1)
- **Published**: 2020-03-11 03:14:52+00:00
- **Updated**: 2020-03-11 03:14:52+00:00
- **Authors**: Wonjik Kim, Masayuki Tanaka, Masatoshi Okutomi, Yoko Sasaki
- **Comment**: Please check the following URL for more information.
  http://www.ok.sc.e.titech.ac.jp/res/LHD/
- **Journal**: None
- **Summary**: In this paper, we propose an automatic labeled sequential data generation pipeline for human segmentation and velocity estimation with point clouds. Considering the impact of deep neural networks, state-of-the-art network architectures have been proposed for human recognition using point clouds captured by Light Detection and Ranging (LiDAR). However, one disadvantage is that legacy datasets may only cover the image domain without providing important label information and this limitation has disturbed the progress of research to date. Therefore, we develop an automatic labeled sequential data generation pipeline, in which we can control any parameter or data generation environment with pixel-wise and per-frame ground truth segmentation and pixel-wise velocity information for human recognition. Our approach uses a precise human model and reproduces a precise motion to generate realistic artificial data. We present more than 7K video sequences which consist of 32 frames generated by the proposed pipeline. With the proposed sequence generator, we confirm that human segmentation performance is improved when using the video domain compared to when using the image domain. We also evaluate our data by comparing with data generated under different conditions. In addition, we estimate pedestrian velocity with LiDAR by only utilizing data generated by the proposed pipeline.



### FlowFusion: Dynamic Dense RGB-D SLAM Based on Optical Flow
- **Arxiv ID**: http://arxiv.org/abs/2003.05102v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.05102v1)
- **Published**: 2020-03-11 04:00:49+00:00
- **Updated**: 2020-03-11 04:00:49+00:00
- **Authors**: Tianwei Zhang, Huayan Zhang, Yang Li, Yoshihiko Nakamura, Lei Zhang
- **Comment**: To be published in ICRA2020
- **Journal**: None
- **Summary**: Dynamic environments are challenging for visual SLAM since the moving objects occlude the static environment features and lead to wrong camera motion estimation. In this paper, we present a novel dense RGB-D SLAM solution that simultaneously accomplishes the dynamic/static segmentation and camera ego-motion estimation as well as the static background reconstructions. Our novelty is using optical flow residuals to highlight the dynamic semantics in the RGB-D point clouds and provide more accurate and efficient dynamic/static segmentation for camera tracking and background reconstruction. The dense reconstruction results on public datasets and real dynamic scenes indicate that the proposed approach achieved accurate and efficient performances in both dynamic and static environments compared to state-of-the-art approaches.



### PONAS: Progressive One-shot Neural Architecture Search for Very Efficient Deployment
- **Arxiv ID**: http://arxiv.org/abs/2003.05112v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.05112v2)
- **Published**: 2020-03-11 05:00:31+00:00
- **Updated**: 2020-04-09 05:27:40+00:00
- **Authors**: Sian-Yao Huang, Wei-Ta Chu
- **Comment**: None
- **Journal**: None
- **Summary**: We achieve very efficient deep learning model deployment that designs neural network architectures to fit different hardware constraints. Given a constraint, most neural architecture search (NAS) methods either sample a set of sub-networks according to a pre-trained accuracy predictor, or adopt the evolutionary algorithm to evolve specialized networks from the supernet. Both approaches are time consuming. Here our key idea for very efficient deployment is, when searching the architecture space, constructing a table that stores the validation accuracy of all candidate blocks at all layers. For a stricter hardware constraint, the architecture of a specialized network can be very efficiently determined based on this table by picking the best candidate blocks that yield the least accuracy loss. To accomplish this idea, we propose Progressive One-shot Neural Architecture Search (PONAS) that combines advantages of progressive NAS and one-shot methods. In PONAS, we propose a two-stage training scheme, including the meta training stage and the fine-tuning stage, to make the search process efficient and stable. During search, we evaluate candidate blocks in different layers and construct the accuracy table that is to be used in deployment. Comprehensive experiments verify that PONAS is extremely flexible, and is able to find architecture of a specialized network in around 10 seconds. In ImageNet classification, 75.2% top-1 accuracy can be obtained, which is comparable with the state of the arts.



### Uncertainty depth estimation with gated images for 3D reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2003.05122v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.05122v1)
- **Published**: 2020-03-11 06:00:21+00:00
- **Updated**: 2020-03-11 06:00:21+00:00
- **Authors**: Stefanie Walz, Tobias Gruber, Werner Ritter, Klaus Dietmayer
- **Comment**: None
- **Journal**: None
- **Summary**: Gated imaging is an emerging sensor technology for self-driving cars that provides high-contrast images even under adverse weather influence. It has been shown that this technology can even generate high-fidelity dense depth maps with accuracy comparable to scanning LiDAR systems. In this work, we extend the recent Gated2Depth framework with aleatoric uncertainty providing an additional confidence measure for the depth estimates. This confidence can help to filter out uncertain estimations in regions without any illumination. Moreover, we show that training on dense depth maps generated by LiDAR depth completion algorithms can further improve the performance.



### Cars Can't Fly up in the Sky: Improving Urban-Scene Segmentation via Height-driven Attention Networks
- **Arxiv ID**: http://arxiv.org/abs/2003.05128v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.05128v3)
- **Published**: 2020-03-11 06:22:12+00:00
- **Updated**: 2020-04-07 02:34:31+00:00
- **Authors**: Sungha Choi, Joanne T. Kim, Jaegul Choo
- **Comment**: Accepted to CVPR 2020
- **Journal**: None
- **Summary**: This paper exploits the intrinsic features of urban-scene images and proposes a general add-on module, called height-driven attention networks (HANet), for improving semantic segmentation for urban-scene images. It emphasizes informative features or classes selectively according to the vertical position of a pixel. The pixel-wise class distributions are significantly different from each other among horizontally segmented sections in the urban-scene images. Likewise, urban-scene images have their own distinct characteristics, but most semantic segmentation networks do not reflect such unique attributes in the architecture. The proposed network architecture incorporates the capability exploiting the attributes to handle the urban scene dataset effectively. We validate the consistent performance (mIoU) increase of various semantic segmentation models on two datasets when HANet is adopted. This extensive quantitative analysis demonstrates that adding our module to existing models is easy and cost-effective. Our method achieves a new state-of-the-art performance on the Cityscapes benchmark with a large margin among ResNet-101 based segmentation models. Also, we show that the proposed model is coherent with the facts observed in the urban scene by visualizing and interpreting the attention map. Our code and trained models are publicly available at https://github.com/shachoi/HANet



### CASIA-SURF CeFA: A Benchmark for Multi-modal Cross-ethnicity Face Anti-spoofing
- **Arxiv ID**: http://arxiv.org/abs/2003.05136v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.05136v1)
- **Published**: 2020-03-11 06:58:54+00:00
- **Updated**: 2020-03-11 06:58:54+00:00
- **Authors**: Ajian Li, Zichang Tan, Xuan Li, Jun Wan, Sergio Escalera, Guodong Guo, Stan Z. Li
- **Comment**: 17 pages, 4 figures. arXiv admin note: substantial text overlap with
  arXiv:1912.02340
- **Journal**: None
- **Summary**: Ethnic bias has proven to negatively affect the performance of face recognition systems, and it remains an open research problem in face anti-spoofing. In order to study the ethnic bias for face anti-spoofing, we introduce the largest up to date CASIA-SURF Cross-ethnicity Face Anti-spoofing (CeFA) dataset (briefly named CeFA), covering $3$ ethnicities, $3$ modalities, $1,607$ subjects, and 2D plus 3D attack types. Four protocols are introduced to measure the affect under varied evaluation conditions, such as cross-ethnicity, unknown spoofs or both of them. To the best of our knowledge, CeFA is the first dataset including explicit ethnic labels in current published/released datasets for face anti-spoofing. Then, we propose a novel multi-modal fusion method as a strong baseline to alleviate these bias, namely, the static-dynamic fusion mechanism applied in each modality (i.e., RGB, Depth and infrared image). Later, a partially shared fusion strategy is proposed to learn complementary information from multiple modalities. Extensive experiments demonstrate that the proposed method achieves state-of-the-art results on the CASIA-SURF, OULU-NPU, SiW and the CeFA dataset.



### Regularized Adaptation for Stable and Efficient Continuous-Level Learning on Image Processing Networks
- **Arxiv ID**: http://arxiv.org/abs/2003.05145v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.05145v2)
- **Published**: 2020-03-11 07:46:57+00:00
- **Updated**: 2020-03-12 03:52:48+00:00
- **Authors**: Hyeongmin Lee, Taeoh Kim, Hanbin Son, Sangwook Baek, Minsu Cheon, Sangyoun Lee
- **Comment**: None
- **Journal**: None
- **Summary**: In Convolutional Neural Network (CNN) based image processing, most of the studies propose networks that are optimized for a single-level (or a single-objective); thus, they underperform on other levels and must be retrained for delivery of optimal performance. Using multiple models to cover multiple levels involves very high computational costs. To solve these problems, recent approaches train the networks on two different levels and propose their own interpolation methods to enable the arbitrary intermediate levels. However, many of them fail to adapt hard tasks or interpolate smoothly, or the others still require large memory and computational cost. In this paper, we propose a novel continuous-level learning framework using a Filter Transition Network (FTN) which is a non-linear module that easily adapt to new levels, and is regularized to prevent undesirable side-effects. Additionally, for stable learning of FTN, we newly propose a method to initialize non-linear CNNs with identity mappings. Furthermore, FTN is extremely lightweight module since it is a data-independent module, which means it is not affected by the spatial resolution of the inputs. Extensive results for various image processing tasks indicate that the performance of FTN is stable in terms of adaptation and interpolation, and comparable to that of the other heavy frameworks.



### Video2Commonsense: Generating Commonsense Descriptions to Enrich Video Captioning
- **Arxiv ID**: http://arxiv.org/abs/2003.05162v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2003.05162v4)
- **Published**: 2020-03-11 08:42:57+00:00
- **Updated**: 2023-01-07 20:25:05+00:00
- **Authors**: Zhiyuan Fang, Tejas Gokhale, Pratyay Banerjee, Chitta Baral, Yezhou Yang
- **Comment**: EMNLP 2020. V2C Website: https://asu-apg.github.io/Video2Commonsense/
- **Journal**: None
- **Summary**: Captioning is a crucial and challenging task for video understanding. In videos that involve active agents such as humans, the agent's actions can bring about myriad changes in the scene. Observable changes such as movements, manipulations, and transformations of the objects in the scene, are reflected in conventional video captioning. Unlike images, actions in videos are also inherently linked to social aspects such as intentions (why the action is taking place), effects (what changes due to the action), and attributes that describe the agent. Thus for video understanding, such as when captioning videos or when answering questions about videos, one must have an understanding of these commonsense aspects. We present the first work on generating commonsense captions directly from videos, to describe latent aspects such as intentions, effects, and attributes. We present a new dataset "Video-to-Commonsense (V2C)" that contains $\sim9k$ videos of human agents performing various actions, annotated with 3 types of commonsense descriptions. Additionally we explore the use of open-ended video-based commonsense question answering (V2C-QA) as a way to enrich our captions. Both the generation task and the QA task can be used to enrich video captions.



### Real-Time High-Performance Semantic Image Segmentation of Urban Street Scenes
- **Arxiv ID**: http://arxiv.org/abs/2003.08736v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08736v2)
- **Published**: 2020-03-11 08:45:53+00:00
- **Updated**: 2020-04-03 12:27:53+00:00
- **Authors**: Genshun Dong, Yan Yan, Chunhua Shen, Hanzi Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Convolutional Neural Networks (DCNNs) have recently shown outstanding performance in semantic image segmentation. However, state-of-the-art DCNN-based semantic segmentation methods usually suffer from high computational complexity due to the use of complex network architectures. This greatly limits their applications in the real-world scenarios that require real-time processing. In this paper, we propose a real-time high-performance DCNN-based method for robust semantic segmentation of urban street scenes, which achieves a good trade-off between accuracy and speed. Specifically, a Lightweight Baseline Network with Atrous convolution and Attention (LBN-AA) is firstly used as our baseline network to efficiently obtain dense feature maps. Then, the Distinctive Atrous Spatial Pyramid Pooling (DASPP), which exploits the different sizes of pooling operations to encode the rich and distinctive semantic information, is developed to detect objects at multiple scales. Meanwhile, a Spatial detail-Preserving Network (SPN) with shallow convolutional layers is designed to generate high-resolution feature maps preserving the detailed spatial information. Finally, a simple but practical Feature Fusion Network (FFN) is used to effectively combine both shallow and deep features from the semantic branch (DASPP) and the spatial branch (SPN), respectively. Extensive experimental results show that the proposed method respectively achieves the accuracy of 73.6% and 68.0% mean Intersection over Union (mIoU) with the inference speed of 51.0 fps and 39.3 fps on the challenging Cityscapes and CamVid test datasets (by only using a single NVIDIA TITAN X card). This demonstrates that the proposed method offers excellent performance at the real-time speed for semantic segmentation of urban street scenes.



### Equalization Loss for Long-Tailed Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/2003.05176v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.05176v2)
- **Published**: 2020-03-11 09:14:53+00:00
- **Updated**: 2020-04-14 15:31:25+00:00
- **Authors**: Jingru Tan, Changbao Wang, Buyu Li, Quanquan Li, Wanli Ouyang, Changqing Yin, Junjie Yan
- **Comment**: CVPR 2020. Winner of LVIS Challenge 2019. Code has been available at
  https: //github.com/tztztztztz/eql.detectron2
- **Journal**: None
- **Summary**: Object recognition techniques using convolutional neural networks (CNN) have achieved great success. However, state-of-the-art object detection methods still perform poorly on large vocabulary and long-tailed datasets, e.g. LVIS. In this work, we analyze this problem from a novel perspective: each positive sample of one category can be seen as a negative sample for other categories, making the tail categories receive more discouraging gradients. Based on it, we propose a simple but effective loss, named equalization loss, to tackle the problem of long-tailed rare categories by simply ignoring those gradients for rare categories. The equalization loss protects the learning of rare categories from being at a disadvantage during the network parameter updating. Thus the model is capable of learning better discriminative features for objects of rare classes. Without any bells and whistles, our method achieves AP gains of 4.1% and 4.8% for the rare and common categories on the challenging LVIS benchmark, compared to the Mask R-CNN baseline. With the utilization of the effective equalization loss, we finally won the 1st place in the LVIS Challenge 2019. Code has been made available at: https: //github.com/tztztztztz/eql.detectron2



### Improving Convolutional Neural Networks Via Conservative Field Regularisation and Integration
- **Arxiv ID**: http://arxiv.org/abs/2003.05182v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2003.05182v1)
- **Published**: 2020-03-11 09:29:48+00:00
- **Updated**: 2020-03-11 09:29:48+00:00
- **Authors**: Dominique Beaini, Sofiane Achiche, Maxime Raison
- **Comment**: 11 pages, 3 figures
- **Journal**: None
- **Summary**: Current research in convolutional neural networks (CNN) focuses mainly on changing the architecture of the networks, optimizing the hyper-parameters and improving the gradient descent. However, most work use only 3 standard families of operations inside the CNN, the convolution, the activation function, and the pooling. In this work, we propose a new family of operations based on the Green's function of the Laplacian, which allows the network to solve the Laplacian, to integrate any vector field and to regularize the field by forcing it to be conservative. Hence, the Green's function (GF) is the first operation that regularizes the 2D or 3D feature space by forcing it to be conservative and physically interpretable, instead of regularizing the norm of the weights. Our results show that such regularization allows the network to learn faster, to have smoother training curves and to better generalize, without any additional parameter. The current manuscript presents early results, more work is required to benchmark the proposed method.



### A Fourier Domain Feature Approach for Human Activity Recognition & Fall Detection
- **Arxiv ID**: http://arxiv.org/abs/2003.05209v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.05209v1)
- **Published**: 2020-03-11 10:49:11+00:00
- **Updated**: 2020-03-11 10:49:11+00:00
- **Authors**: Asma Khatun, Sk. Golam Sarowar Hossain
- **Comment**: None
- **Journal**: None
- **Summary**: Elder people consequence a variety of problems while living Activities of Daily Living (ADL) for the reason of age, sense, loneliness and cognitive changes. These cause the risk to ADL which leads to several falls. Getting real life fall data is a difficult process and are not available whereas simulated falls become ubiquitous to evaluate the proposed methodologies. From the literature review, it is investigated that most of the researchers used raw and energy features (time domain features) of the signal data as those are most discriminating. However, in real life situations fall signal may be noisy than the current simulated data. Hence the result using raw feature may dramatically changes when using in a real life scenario. This research is using frequency domain Fourier coefficient features to differentiate various human activities of daily life. The feature vector constructed using those Fast Fourier Transform are robust to noise and rotation invariant. Two different supervised classifiers kNN and SVM are used for evaluating the method. Two standard publicly available datasets are used for benchmark analysis. In this research, more discriminating results are obtained applying kNN classifier than the SVM classifier. Various standard measure including Standard Accuracy (SA), Macro Average Accuracy (MAA), Sensitivity (SE) and Specificity (SP) has been accounted. In all cases, the proposed method outperforms energy features whereas competitive results are shown with raw features. It is also noticed that the proposed method performs better than the recently risen deep learning approach in which data augmentation method were not used.



### A Mobile Robot Hand-Arm Teleoperation System by Vision and IMU
- **Arxiv ID**: http://arxiv.org/abs/2003.05212v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2003.05212v1)
- **Published**: 2020-03-11 10:57:24+00:00
- **Updated**: 2020-03-11 10:57:24+00:00
- **Authors**: Shuang Li, Jiaxi Jiang, Philipp Ruppel, Hongzhuo Liang, Xiaojian Ma, Norman Hendrich, Fuchun Sun, Jianwei Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a multimodal mobile teleoperation system that consists of a novel vision-based hand pose regression network (Transteleop) and an IMU-based arm tracking method. Transteleop observes the human hand through a low-cost depth camera and generates not only joint angles but also depth images of paired robot hand poses through an image-to-image translation process. A keypoint-based reconstruction loss explores the resemblance in appearance and anatomy between human and robotic hands and enriches the local features of reconstructed images. A wearable camera holder enables simultaneous hand-arm control and facilitates the mobility of the whole teleoperation system. Network evaluation results on a test dataset and a variety of complex manipulation tasks that go beyond simple pick-and-place operations show the efficiency and stability of our multimodal teleoperation system.



### Keyfilter-Aware Real-Time UAV Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2003.05218v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.05218v1)
- **Published**: 2020-03-11 11:11:16+00:00
- **Updated**: 2020-03-11 11:11:16+00:00
- **Authors**: Yiming Li, Changhong Fu, Ziyuan Huang, Yinqiang Zhang, Jia Pan
- **Comment**: 2020 International Conference on Robotics and Automation (ICRA)
- **Journal**: None
- **Summary**: Correlation filter-based tracking has been widely applied in unmanned aerial vehicle (UAV) with high efficiency. However, it has two imperfections, i.e., boundary effect and filter corruption. Several methods enlarging the search area can mitigate boundary effect, yet introducing undesired background distraction. Existing frame-by-frame context learning strategies for repressing background distraction nevertheless lower the tracking speed. Inspired by keyframe-based simultaneous localization and mapping, keyfilter is proposed in visual tracking for the first time, in order to handle the above issues efficiently and effectively. Keyfilters generated by periodically selected keyframes learn the context intermittently and are used to restrain the learning of filters, so that 1) context awareness can be transmitted to all the filters via keyfilter restriction, and 2) filter corruption can be repressed. Compared to the state-of-the-art results, our tracker performs better on two challenging benchmarks, with enough speed for UAV real-time applications.



### A Computer-Aided Diagnosis System Using Artificial Intelligence for Hip Fractures -Multi-Institutional Joint Development Research-
- **Arxiv ID**: http://arxiv.org/abs/2003.12443v5
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV, eess.IV, q-bio.TO, 68-T01
- **Links**: [PDF](http://arxiv.org/pdf/2003.12443v5)
- **Published**: 2020-03-11 11:16:39+00:00
- **Updated**: 2020-05-20 04:29:20+00:00
- **Authors**: Yoichi Sato, Yasuhiko Takegami, Takamune Asamoto, Yutaro Ono, Tsugeno Hidetoshi, Ryosuke Goto, Akira Kitamura, Seiwa Honda
- **Comment**: 9 pages, 4 tables, 7 figures. / author's homepage :
  https://www.fracture-ai.org
- **Journal**: None
- **Summary**: [Objective] To develop a Computer-aided diagnosis (CAD) system for plane frontal hip X-rays with a deep learning model trained on a large dataset collected at multiple centers. [Materials and Methods]. We included 5295 cases with neck fracture or trochanteric fracture who were diagnosed and treated by orthopedic surgeons using plane X-rays or computed tomography (CT) or magnetic resonance imaging (MRI) who visited each institution between April 2009 and March 2019 were enrolled. Cases in which both hips were not included in the photographing range, femoral shaft fractures, and periprosthetic fractures were excluded, and 5242 plane frontal pelvic X-rays obtained from 4,851 cases were used for machine learning. These images were divided into 5242 images including the fracture side and 5242 images without the fracture side, and a total of 10484 images were used for machine learning. A deep convolutional neural network approach was used for machine learning. Pytorch 1.3 and Fast.ai 1.0 were used as frameworks, and EfficientNet-B4, which is pre-trained ImageNet model, was used. In the final evaluation, accuracy, sensitivity, specificity, F-value and area under the curve (AUC) were evaluated. Gradient-weighted class activation mapping (Grad-CAM) was used to conceptualize the diagnostic basis of the CAD system. [Results] The diagnostic accuracy of the learning model was accuracy of 96. 1 %, sensitivity of 95.2 %, specificity of 96.9 %, F-value of 0.961, and AUC of 0.99. The cases who were correct for the diagnosis showed generally correct diagnostic basis using Grad-CAM. [Conclusions] The CAD system using deep learning model which we developed was able to diagnose hip fracture in the plane X-ray with the high accuracy, and it was possible to present the decision reason.



### Channel Interaction Networks for Fine-Grained Image Categorization
- **Arxiv ID**: http://arxiv.org/abs/2003.05235v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.05235v1)
- **Published**: 2020-03-11 11:51:51+00:00
- **Updated**: 2020-03-11 11:51:51+00:00
- **Authors**: Yu Gao, Xintong Han, Xun Wang, Weilin Huang, Matthew R. Scott
- **Comment**: AAAI 2020
- **Journal**: None
- **Summary**: Fine-grained image categorization is challenging due to the subtle inter-class differences.We posit that exploiting the rich relationships between channels can help capture such differences since different channels correspond to different semantics. In this paper, we propose a channel interaction network (CIN), which models the channel-wise interplay both within an image and across images. For a single image, a self-channel interaction (SCI) module is proposed to explore channel-wise correlation within the image. This allows the model to learn the complementary features from the correlated channels, yielding stronger fine-grained features. Furthermore, given an image pair, we introduce a contrastive channel interaction (CCI) module to model the cross-sample channel interaction with a metric learning framework, allowing the CIN to distinguish the subtle visual differences between images. Our model can be trained efficiently in an end-to-end fashion without the need of multi-stage training and testing. Finally, comprehensive experiments are conducted on three publicly available benchmarks, where the proposed method consistently outperforms the state-of-theart approaches, such as DFL-CNN (Wang, Morariu, and Davis 2018) and NTS (Yang et al. 2018).



### GID-Net: Detecting Human-Object Interaction with Global and Instance Dependency
- **Arxiv ID**: http://arxiv.org/abs/2003.05242v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.05242v1)
- **Published**: 2020-03-11 11:58:43+00:00
- **Updated**: 2020-03-11 11:58:43+00:00
- **Authors**: Dongming Yang, YueXian Zou, Jian Zhang, Ge Li
- **Comment**: 30 pages, 8 figures
- **Journal**: None
- **Summary**: Since detecting and recognizing individual human or object are not adequate to understand the visual world, learning how humans interact with surrounding objects becomes a core technology. However, convolution operations are weak in depicting visual interactions between the instances since they only build blocks that process one local neighborhood at a time. To address this problem, we learn from human perception in observing HOIs to introduce a two-stage trainable reasoning mechanism, referred to as GID block. GID block breaks through the local neighborhoods and captures long-range dependency of pixels both in global-level and instance-level from the scene to help detecting interactions between instances. Furthermore, we conduct a multi-stream network called GID-Net, which is a human-object interaction detection framework consisting of a human branch, an object branch and an interaction branch. Semantic information in global-level and local-level are efficiently reasoned and aggregated in each of the branches. We have compared our proposed GID-Net with existing state-of-the-art methods on two public benchmarks, including V-COCO and HICO-DET. The results have showed that GID-Net outperforms the existing best-performing methods on both the above two benchmarks, validating its efficacy in detecting human-object interactions.



### Semi-Local 3D Lane Detection and Uncertainty Estimation
- **Arxiv ID**: http://arxiv.org/abs/2003.05257v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.05257v1)
- **Published**: 2020-03-11 12:35:24+00:00
- **Updated**: 2020-03-11 12:35:24+00:00
- **Authors**: Netalee Efrat, Max Bluvstein, Noa Garnett, Dan Levi, Shaul Oron, Bat El Shlomo
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel camera-based DNN method for 3D lane detection with uncertainty estimation. Our method is based on a semi-local, BEV, tile representation that breaks down lanes into simple lane segments. It combines learning a parametric model for the segments along with a deep feature embedding that is then used to cluster segment together into full lanes. This combination allows our method to generalize to complex lane topologies, curvatures and surface geometries. Additionally, our method is the first to output a learning based uncertainty estimation for the lane detection task. The efficacy of our method is demonstrated in extensive experiments achieving state-of-the-art results for camera-based 3D lane detection, while also showing our ability to generalize to complex topologies, curvatures and road geometries as well as to different cameras. We also demonstrate how our uncertainty estimation aligns with the empirical error statistics indicating that it is well calibrated and truly reflects the detection noise.



### DeepFake Detection: Current Challenges and Next Steps
- **Arxiv ID**: http://arxiv.org/abs/2003.09234v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.09234v1)
- **Published**: 2020-03-11 13:20:42+00:00
- **Updated**: 2020-03-11 13:20:42+00:00
- **Authors**: Siwei Lyu
- **Comment**: arXiv admin note: text overlap with arXiv:1909.12962
- **Journal**: None
- **Summary**: High quality fake videos and audios generated by AI-algorithms (the deep fakes) have started to challenge the status of videos and audios as definitive evidence of events. In this paper, we highlight a few of these challenges and discuss the research opportunities in this direction.



### Training-Set Distillation for Real-Time UAV Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2003.05326v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.05326v1)
- **Published**: 2020-03-11 14:28:09+00:00
- **Updated**: 2020-03-11 14:28:09+00:00
- **Authors**: Fan Li, Changhong Fu, Fuling Lin, Yiming Li, Peng Lu
- **Comment**: 2020 IEEE International Conference on Robotics and Automation
- **Journal**: None
- **Summary**: Correlation filter (CF) has recently exhibited promising performance in visual object tracking for unmanned aerial vehicle (UAV). Such online learning method heavily depends on the quality of the training-set, yet complicated aerial scenarios like occlusion or out of view can reduce its reliability. In this work, a novel time slot-based distillation approach is proposed to efficiently and effectively optimize the training-set's quality on the fly. A cooperative energy minimization function is established to score the historical samples adaptively. To accelerate the scoring process, frames with high confident tracking results are employed as the keyframes to divide the tracking process into multiple time slots. After the establishment of a new slot, the weighted fusion of the previous samples generates one key-sample, in order to reduce the number of samples to be scored. Besides, when the current time slot exceeds the maximum frame number, which can be scored, the sample with the lowest score will be discarded. Consequently, the training-set can be efficiently and reliably distilled. Comprehensive tests on two well-known UAV benchmarks prove the effectiveness of our method with real-time speed on a single CPU.



### ENSEI: Efficient Secure Inference via Frequency-Domain Homomorphic Convolution for Privacy-Preserving Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/2003.05328v2
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.05328v2)
- **Published**: 2020-03-11 14:35:48+00:00
- **Updated**: 2021-10-31 04:52:48+00:00
- **Authors**: Song Bian, Tianchen Wang, Masayuki Hiromoto, Yiyu Shi, Takashi Sato
- **Comment**: 10 pages, 3 figures, in Proceedings of Conference on Computer Vision
  and Pattern Recognition (CVPR 2020)
- **Journal**: None
- **Summary**: In this work, we propose ENSEI, a secure inference (SI) framework based on the frequency-domain secure convolution (FDSC) protocol for the efficient execution of privacy-preserving visual recognition. Our observation is that, under the combination of homomorphic encryption and secret sharing, homomorphic convolution can be obliviously carried out in the frequency domain, significantly simplifying the related computations. We provide protocol designs and parameter derivations for number-theoretic transform (NTT) based FDSC. In the experiment, we thoroughly study the accuracy-efficiency trade-offs between time- and frequency-domain homomorphic convolution. With ENSEI, compared to the best known works, we achieve 5--11x online time reduction, up to 33x setup time reduction, and up to 10x reduction in the overall inference time. A further 33% of bandwidth reductions can be obtained on binary neural networks with only 1% of accuracy degradation on the CIFAR-10 dataset.



### A Graph Attention Spatio-temporal Convolutional Network for 3D Human Pose Estimation in Video
- **Arxiv ID**: http://arxiv.org/abs/2003.14179v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.14179v4)
- **Published**: 2020-03-11 14:54:40+00:00
- **Updated**: 2020-10-20 01:19:08+00:00
- **Authors**: Junfa Liu, Juan Rojas, Zhijun Liang, Yihui Li, Yisheng Guan
- **Comment**: 8 pages, 8 figures, 5 tables
- **Journal**: None
- **Summary**: Spatio-temporal information is key to resolve occlusion and depth ambiguity in 3D pose estimation. Previous methods have focused on either temporal contexts or local-to-global architectures that embed fixed-length spatio-temporal information. To date, there have not been effective proposals to simultaneously and flexibly capture varying spatio-temporal sequences and effectively achieves real-time 3D pose estimation. In this work, we improve the learning of kinematic constraints in the human skeleton: posture, local kinematic connections, and symmetry by modeling local and global spatial information via attention mechanisms. To adapt to single- and multi-frame estimation, the dilated temporal model is employed to process varying skeleton sequences. Also, importantly, we carefully design the interleaving of spatial semantics with temporal dependencies to achieve a synergistic effect. To this end, we propose a simple yet effective graph attention spatio-temporal convolutional network (GAST-Net) that comprises of interleaved temporal convolutional and graph attention blocks. Experiments on two challenging benchmark datasets (Human3.6M and HumanEva-I) and YouTube videos demonstrate that our approach effectively mitigates depth ambiguity and self-occlusion, generalizes to half upper body estimation, and achieves competitive performance on 2D-to-3D video pose estimation. Code, video, and supplementary information is available at: \href{http://www.juanrojas.net/gast/}{http://www.juanrojas.net/gast/}



### xCos: An Explainable Cosine Metric for Face Verification Task
- **Arxiv ID**: http://arxiv.org/abs/2003.05383v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.05383v3)
- **Published**: 2020-03-11 16:03:44+00:00
- **Updated**: 2021-07-15 13:38:20+00:00
- **Authors**: Yu-Sheng Lin, Zhe-Yu Liu, Yu-An Chen, Yu-Siang Wang, Ya-Liang Chang, Winston H. Hsu
- **Comment**: ACM Transactions on Multimedia Computing Communications and
  Applications (TOMM). 2021
- **Journal**: None
- **Summary**: We study the XAI (explainable AI) on the face recognition task, particularly the face verification here. Face verification is a crucial task in recent days and it has been deployed to plenty of applications, such as access control, surveillance, and automatic personal log-on for mobile devices. With the increasing amount of data, deep convolutional neural networks can achieve very high accuracy for the face verification task. Beyond exceptional performances, deep face verification models need more interpretability so that we can trust the results they generate. In this paper, we propose a novel similarity metric, called explainable cosine ($xCos$), that comes with a learnable module that can be plugged into most of the verification models to provide meaningful explanations. With the help of $xCos$, we can see which parts of the two input faces are similar, where the model pays its attention to, and how the local similarities are weighted to form the output $xCos$ score. We demonstrate the effectiveness of our proposed method on LFW and various competitive benchmarks, resulting in not only providing novel and desiring model interpretability for face verification but also ensuring the accuracy as plugging into existing face recognition models.



### Learning Diverse Fashion Collocation by Neural Graph Filtering
- **Arxiv ID**: http://arxiv.org/abs/2003.04888v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2003.04888v1)
- **Published**: 2020-03-11 16:17:08+00:00
- **Updated**: 2020-03-11 16:17:08+00:00
- **Authors**: Xin Liu, Yongbin Sun, Ziwei Liu, Dahua Lin
- **Comment**: None
- **Journal**: None
- **Summary**: Fashion recommendation systems are highly desired by customers to find visually-collocated fashion items, such as clothes, shoes, bags, etc. While existing methods demonstrate promising results, they remain lacking in flexibility and diversity, e.g. assuming a fixed number of items or favoring safe but boring recommendations. In this paper, we propose a novel fashion collocation framework, Neural Graph Filtering, that models a flexible set of fashion items via a graph neural network. Specifically, we consider the visual embeddings of each garment as a node in the graph, and describe the inter-garment relationship as the edge between nodes. By applying symmetric operations on the edge vectors, this framework allows varying numbers of inputs/outputs and is invariant to their ordering. We further include a style classifier augmented with focal loss to enable the collocation of significantly diverse styles, which are inherently imbalanced in the training set. To facilitate a comprehensive study on diverse fashion collocation, we reorganize Amazon Fashion dataset with carefully designed evaluation protocols. We evaluate the proposed approach on three popular benchmarks, the Polyvore dataset, the Polyvore-D dataset, and our reorganized Amazon Fashion dataset. Extensive experimental results show that our approach significantly outperforms the state-of-the-art methods with over 10% improvements on the standard AUC metric on the established tasks. More importantly, 82.5% of the users prefer our diverse-style recommendations over other alternatives in a real-world perception study.



### How Powerful Are Randomly Initialized Pointcloud Set Functions?
- **Arxiv ID**: http://arxiv.org/abs/2003.05410v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.05410v1)
- **Published**: 2020-03-11 16:54:43+00:00
- **Updated**: 2020-03-11 16:54:43+00:00
- **Authors**: Aditya Sanghi, Pradeep Kumar Jayaraman
- **Comment**: 6 pages, 2 figures, NeurIPS 2019 Sets & Partitions Workshop
- **Journal**: None
- **Summary**: We study random embeddings produced by untrained neural set functions, and show that they are powerful representations which well capture the input features for downstream tasks such as classification, and are often linearly separable. We obtain surprising results that show that random set functions can often obtain close to or even better accuracy than fully trained models. We investigate factors that affect the representative power of such embeddings quantitatively and qualitatively.



### Bi-Directional Attention for Joint Instance and Semantic Segmentation in Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2003.05420v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.05420v1)
- **Published**: 2020-03-11 17:16:07+00:00
- **Updated**: 2020-03-11 17:16:07+00:00
- **Authors**: Guangnan Wu, Zhiyi Pan, Peng Jiang, Changhe Tu
- **Comment**: None
- **Journal**: None
- **Summary**: Instance segmentation in point clouds is one of the most fine-grained ways to understand the 3D scene. Due to its close relationship to semantic segmentation, many works approach these two tasks simultaneously and leverage the benefits of multi-task learning. However, most of them only considered simple strategies such as element-wise feature fusion, which may not lead to mutual promotion. In this work, we build a Bi-Directional Attention module on backbone neural networks for 3D point cloud perception, which uses similarity matrix measured from features for one task to help aggregate non-local information for the other task, avoiding the potential feature exclusion and task conflict. From comprehensive experiments and ablation studies on the S3DIS dataset and the PartNet dataset, the superiority of our method is verified. Moreover, the mechanism of how bi-directional attention module helps joint instance and semantic segmentation is also analyzed.



### Gauge Equivariant Mesh CNNs: Anisotropic convolutions on geometric graphs
- **Arxiv ID**: http://arxiv.org/abs/2003.05425v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.05425v3)
- **Published**: 2020-03-11 17:21:15+00:00
- **Updated**: 2021-11-19 12:00:16+00:00
- **Authors**: Pim de Haan, Maurice Weiler, Taco Cohen, Max Welling
- **Comment**: Published at ICLR 2021
- **Journal**: None
- **Summary**: A common approach to define convolutions on meshes is to interpret them as a graph and apply graph convolutional networks (GCNs). Such GCNs utilize isotropic kernels and are therefore insensitive to the relative orientation of vertices and thus to the geometry of the mesh as a whole. We propose Gauge Equivariant Mesh CNNs which generalize GCNs to apply anisotropic gauge equivariant kernels. Since the resulting features carry orientation information, we introduce a geometric message passing scheme defined by parallel transporting features over mesh edges. Our experiments validate the significantly improved expressivity of the proposed model over conventional GCNs and other methods.



### Learning Predictive Representations for Deformable Objects Using Contrastive Estimation
- **Arxiv ID**: http://arxiv.org/abs/2003.05436v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.05436v1)
- **Published**: 2020-03-11 17:55:15+00:00
- **Updated**: 2020-03-11 17:55:15+00:00
- **Authors**: Wilson Yan, Ashwin Vangipuram, Pieter Abbeel, Lerrel Pinto
- **Comment**: Project website:
  https://sites.google.com/view/contrastive-predictive-model
- **Journal**: None
- **Summary**: Using visual model-based learning for deformable object manipulation is challenging due to difficulties in learning plannable visual representations along with complex dynamic models. In this work, we propose a new learning framework that jointly optimizes both the visual representation model and the dynamics model using contrastive estimation. Using simulation data collected by randomly perturbing deformable objects on a table, we learn latent dynamics models for these objects in an offline fashion. Then, using the learned models, we use simple model-based planning to solve challenging deformable object manipulation tasks such as spreading ropes and cloths. Experimentally, we show substantial improvements in performance over standard model-based learning techniques across our rope and cloth manipulation suite. Finally, we transfer our visual manipulation policies trained on data purely collected in simulation to a real PR2 robot through domain randomization.



### Un-Mix: Rethinking Image Mixtures for Unsupervised Visual Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2003.05438v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.05438v5)
- **Published**: 2020-03-11 17:59:04+00:00
- **Updated**: 2022-02-17 14:56:37+00:00
- **Authors**: Zhiqiang Shen, Zechun Liu, Zhuang Liu, Marios Savvides, Trevor Darrell, Eric Xing
- **Comment**: AAAI 2022 camera ready version with Appendix (add a formula example
  with InfoNCE). Code is available at: https://github.com/szq0214/Un-Mix
- **Journal**: None
- **Summary**: The recently advanced unsupervised learning approaches use the siamese-like framework to compare two "views" from the same image for learning representations. Making the two views distinctive is a core to guarantee that unsupervised methods can learn meaningful information. However, such frameworks are sometimes fragile on overfitting if the augmentations used for generating two views are not strong enough, causing the over-confident issue on the training data. This drawback hinders the model from learning subtle variance and fine-grained information. To address this, in this work we aim to involve the distance concept on label space in the unsupervised learning and let the model be aware of the soft degree of similarity between positive or negative pairs through mixing the input data space, to further work collaboratively for the input and loss spaces. Despite its conceptual simplicity, we show empirically that with the solution -- Unsupervised image mixtures (Un-Mix), we can learn subtler, more robust and generalized representations from the transformed input and corresponding new label space. Extensive experiments are conducted on CIFAR-10, CIFAR-100, STL-10, Tiny ImageNet and standard ImageNet with popular unsupervised methods SimCLR, BYOL, MoCo V1&V2, SwAV, etc. Our proposed image mixture and label assignment strategy can obtain consistent improvement by 1~3% following exactly the same hyperparameters and training procedures of the base methods. Code is publicly available at https://github.com/szq0214/Un-Mix.



### Addressing the Memory Bottleneck in AI Model Training
- **Arxiv ID**: http://arxiv.org/abs/2003.08732v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.08732v1)
- **Published**: 2020-03-11 18:16:51+00:00
- **Updated**: 2020-03-11 18:16:51+00:00
- **Authors**: David Ojika, Bhavesh Patel, G. Anthony Reina, Trent Boyer, Chad Martin, Prashant Shah
- **Comment**: Presented at Workshop on MLOps Systems at MLSys 2020 Conference,
  Austin TX
- **Journal**: None
- **Summary**: Using medical imaging as case-study, we demonstrate how Intel-optimized TensorFlow on an x86-based server equipped with 2nd Generation Intel Xeon Scalable Processors with large system memory allows for the training of memory-intensive AI/deep-learning models in a scale-up server configuration. We believe our work represents the first training of a deep neural network having large memory footprint (~ 1 TB) on a single-node server. We recommend this configuration to scientists and researchers who wish to develop large, state-of-the-art AI models but are currently limited by memory.



### Deep Vectorization of Technical Drawings
- **Arxiv ID**: http://arxiv.org/abs/2003.05471v3
- **DOI**: 10.1007/978-3-030-58601-0_35
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2003.05471v3)
- **Published**: 2020-03-11 18:19:00+00:00
- **Updated**: 2020-07-30 14:32:11+00:00
- **Authors**: Vage Egiazarian, Oleg Voynov, Alexey Artemov, Denis Volkhonskiy, Aleksandr Safin, Maria Taktasheva, Denis Zorin, Evgeny Burnaev
- **Comment**: None
- **Journal**: None
- **Summary**: We present a new method for vectorization of technical line drawings, such as floor plans, architectural drawings, and 2D CAD images. Our method includes (1) a deep learning-based cleaning stage to eliminate the background and imperfections in the image and fill in missing parts, (2) a transformer-based network to estimate vector primitives, and (3) optimization procedure to obtain the final primitive configurations. We train the networks on synthetic data, renderings of vector line drawings, and manually vectorized scans of line drawings. Our method quantitatively and qualitatively outperforms a number of existing techniques on a collection of representative technical drawings.



### Unified Image and Video Saliency Modeling
- **Arxiv ID**: http://arxiv.org/abs/2003.05477v3
- **DOI**: 10.1007/978-3-030-58558-7_25
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.05477v3)
- **Published**: 2020-03-11 18:28:29+00:00
- **Updated**: 2020-11-07 13:43:34+00:00
- **Authors**: Richard Droste, Jianbo Jiao, J. Alison Noble
- **Comment**: Presented at the European Conference on Computer Vision (ECCV) 2020.
  R. Droste and J. Jiao contributed equally to this work. v3: Updated Fig. 5a)
  and added new MTI300 benchmark results to supp. material
- **Journal**: In: ECCV 2020, Springer LNCS 12350, pp. 419-435
- **Summary**: Visual saliency modeling for images and videos is treated as two independent tasks in recent computer vision literature. While image saliency modeling is a well-studied problem and progress on benchmarks like SALICON and MIT300 is slowing, video saliency models have shown rapid gains on the recent DHF1K benchmark. Here, we take a step back and ask: Can image and video saliency modeling be approached via a unified model, with mutual benefit? We identify different sources of domain shift between image and video saliency data and between different video saliency datasets as a key challenge for effective joint modelling. To address this we propose four novel domain adaptation techniques - Domain-Adaptive Priors, Domain-Adaptive Fusion, Domain-Adaptive Smoothing and Bypass-RNN - in addition to an improved formulation of learned Gaussian priors. We integrate these techniques into a simple and lightweight encoder-RNN-decoder-style network, UNISAL, and train it jointly with image and video saliency data. We evaluate our method on the video saliency datasets DHF1K, Hollywood-2 and UCF-Sports, and the image saliency datasets SALICON and MIT300. With one set of parameters, UNISAL achieves state-of-the-art performance on all video saliency datasets and is on par with the state-of-the-art for image saliency datasets, despite faster runtime and a 5 to 20-fold smaller model size compared to all competing deep methods. We provide retrospective analyses and ablation studies which confirm the importance of the domain shift modeling. The code is available at https://github.com/rdroste/unisal



### Confidence Guided Stereo 3D Object Detection with Split Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2003.05505v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.05505v1)
- **Published**: 2020-03-11 20:00:11+00:00
- **Updated**: 2020-03-11 20:00:11+00:00
- **Authors**: Chengyao Li, Jason Ku, Steven L. Waslander
- **Comment**: 8 pages, 6 figures
- **Journal**: None
- **Summary**: Accurate and reliable 3D object detection is vital to safe autonomous driving. Despite recent developments, the performance gap between stereo-based methods and LiDAR-based methods is still considerable. Accurate depth estimation is crucial to the performance of stereo-based 3D object detection methods, particularly for those pixels associated with objects in the foreground. Moreover, stereo-based methods suffer from high variance in the depth estimation accuracy, which is often not considered in the object detection pipeline. To tackle these two issues, we propose CG-Stereo, a confidence-guided stereo 3D object detection pipeline that uses separate decoders for foreground and background pixels during depth estimation, and leverages the confidence estimation from the depth estimation network as a soft attention mechanism in the 3D object detector. Our approach outperforms all state-of-the-art stereo-based 3D detectors on the KITTI benchmark.



### DeepURL: Deep Pose Estimation Framework for Underwater Relative Localization
- **Arxiv ID**: http://arxiv.org/abs/2003.05523v4
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.05523v4)
- **Published**: 2020-03-11 21:11:05+00:00
- **Updated**: 2021-01-21 18:10:20+00:00
- **Authors**: Bharat Joshi, Md Modasshir, Travis Manderson, Hunter Damron, Marios Xanthidis, Alberto Quattrini Li, Ioannis Rekleitis, Gregory Dudek
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a real-time deep learning approach for determining the 6D relative pose of Autonomous Underwater Vehicles (AUV) from a single image. A team of autonomous robots localizing themselves in a communication-constrained underwater environment is essential for many applications such as underwater exploration, mapping, multi-robot convoying, and other multi-robot tasks. Due to the profound difficulty of collecting ground truth images with accurate 6D poses underwater, this work utilizes rendered images from the Unreal Game Engine simulation for training. An image-to-image translation network is employed to bridge the gap between the rendered and the real images producing synthetic images for training. The proposed method predicts the 6D pose of an AUV from a single image as 2D image keypoints representing 8 corners of the 3D model of the AUV, and then the 6D pose in the camera coordinates is determined using RANSAC-based PnP. Experimental results in real-world underwater environments (swimming pool and ocean) with different cameras demonstrate the robustness and accuracy of the proposed technique in terms of translation error and orientation error over the state-of-the-art methods. The code is publicly available.



### Softmax Splatting for Video Frame Interpolation
- **Arxiv ID**: http://arxiv.org/abs/2003.05534v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.05534v1)
- **Published**: 2020-03-11 21:38:56+00:00
- **Updated**: 2020-03-11 21:38:56+00:00
- **Authors**: Simon Niklaus, Feng Liu
- **Comment**: CVPR 2020, http://sniklaus.com/softsplat
- **Journal**: None
- **Summary**: Differentiable image sampling in the form of backward warping has seen broad adoption in tasks like depth estimation and optical flow prediction. In contrast, how to perform forward warping has seen less attention, partly due to additional challenges such as resolving the conflict of mapping multiple pixels to the same target location in a differentiable way. We propose softmax splatting to address this paradigm shift and show its effectiveness on the application of frame interpolation. Specifically, given two input frames, we forward-warp the frames and their feature pyramid representations based on an optical flow estimate using softmax splatting. In doing so, the softmax splatting seamlessly handles cases where multiple source pixels map to the same target location. We then use a synthesis network to predict the interpolation result from the warped representations. Our softmax splatting allows us to not only interpolate frames at an arbitrary time but also to fine tune the feature pyramid and the optical flow. We show that our synthesis approach, empowered by softmax splatting, achieves new state-of-the-art results for video frame interpolation.



### VSGNet: Spatial Attention Network for Detecting Human Object Interactions Using Graph Convolutions
- **Arxiv ID**: http://arxiv.org/abs/2003.05541v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.05541v1)
- **Published**: 2020-03-11 22:23:51+00:00
- **Updated**: 2020-03-11 22:23:51+00:00
- **Authors**: Oytun Ulutan, A S M Iftekhar, B. S. Manjunath
- **Comment**: Accepted in IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR 2020)
- **Journal**: None
- **Summary**: Comprehensive visual understanding requires detection frameworks that can effectively learn and utilize object interactions while analyzing objects individually. This is the main objective in Human-Object Interaction (HOI) detection task. In particular, relative spatial reasoning and structural connections between objects are essential cues for analyzing interactions, which is addressed by the proposed Visual-Spatial-Graph Network (VSGNet) architecture. VSGNet extracts visual features from the human-object pairs, refines the features with spatial configurations of the pair, and utilizes the structural connections between the pair via graph convolutions. The performance of VSGNet is thoroughly evaluated using the Verbs in COCO (V-COCO) and HICO-DET datasets. Experimental results indicate that VSGNet outperforms state-of-the-art solutions by 8% or 4 mAP in V-COCO and 16% or 3 mAP in HICO-DET.



### Frequency-Tuned Universal Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/2003.05549v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.05549v2)
- **Published**: 2020-03-11 22:52:19+00:00
- **Updated**: 2020-06-09 18:37:09+00:00
- **Authors**: Yingpeng Deng, Lina J. Karam
- **Comment**: None
- **Journal**: None
- **Summary**: Researchers have shown that the predictions of a convolutional neural network (CNN) for an image set can be severely distorted by one single image-agnostic perturbation, or universal perturbation, usually with an empirically fixed threshold in the spatial domain to restrict its perceivability. However, by considering the human perception, we propose to adopt JND thresholds to guide the perceivability of universal adversarial perturbations. Based on this, we propose a frequency-tuned universal attack method to compute universal perturbations and show that our method can realize a good balance between perceivability and effectiveness in terms of fooling rate by adapting the perturbations to the local frequency content. Compared with existing universal adversarial attack techniques, our frequency-tuned attack method can achieve cutting-edge quantitative results. We demonstrate that our approach can significantly improve the performance of the baseline on both white-box and black-box attacks.



### Memory-efficient Learning for Large-scale Computational Imaging
- **Arxiv ID**: http://arxiv.org/abs/2003.05551v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, eess.SP, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.05551v1)
- **Published**: 2020-03-11 23:08:04+00:00
- **Updated**: 2020-03-11 23:08:04+00:00
- **Authors**: Michael Kellman, Kevin Zhang, Jon Tamir, Emrah Bostan, Michael Lustig, Laura Waller
- **Comment**: 9 pages, 8 figures. See also relate NeurIPS 2019 presentation
  arXiv:1912.05098
- **Journal**: None
- **Summary**: Critical aspects of computational imaging systems, such as experimental design and image priors, can be optimized through deep networks formed by the unrolled iterations of classical model-based reconstructions (termed physics-based networks). However, for real-world large-scale inverse problems, computing gradients via backpropagation is infeasible due to memory limitations of graphics processing units. In this work, we propose a memory-efficient learning procedure that exploits the reversibility of the network's layers to enable data-driven design for large-scale computational imaging systems. We demonstrate our method on a small-scale compressed sensing example, as well as two large-scale real-world systems: multi-channel magnetic resonance imaging and super-resolution optical microscopy.



