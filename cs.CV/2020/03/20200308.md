# Arxiv Papers in cs.CV on 2020-03-08
### Unifying Specialist Image Embedding into Universal Image Embedding
- **Arxiv ID**: http://arxiv.org/abs/2003.03701v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.03701v1)
- **Published**: 2020-03-08 02:51:11+00:00
- **Updated**: 2020-03-08 02:51:11+00:00
- **Authors**: Yang Feng, Futang Peng, Xu Zhang, Wei Zhu, Shanfeng Zhang, Howard Zhou, Zhen Li, Tom Duerig, Shih-Fu Chang, Jiebo Luo
- **Comment**: None
- **Journal**: None
- **Summary**: Deep image embedding provides a way to measure the semantic similarity of two images. It plays a central role in many applications such as image search, face verification, and zero-shot learning. It is desirable to have a universal deep embedding model applicable to various domains of images. However, existing methods mainly rely on training specialist embedding models each of which is applicable to images from a single domain. In this paper, we study an important but unexplored task: how to train a single universal image embedding model to match the performance of several specialists on each specialist's domain. Simply fusing the training data from multiple domains cannot solve this problem because some domains become overfitted sooner when trained together using existing methods. Therefore, we propose to distill the knowledge in multiple specialists into a universal embedding to solve this problem. In contrast to existing embedding distillation methods that distill the absolute distances between images, we transform the absolute distances between images into a probabilistic distribution and minimize the KL-divergence between the distributions of the specialists and the universal embedding. Using several public datasets, we validate that our proposed method accomplishes the goal of universal image embedding.



### Transferring Cross-domain Knowledge for Video Sign Language Recognition
- **Arxiv ID**: http://arxiv.org/abs/2003.03703v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2003.03703v2)
- **Published**: 2020-03-08 03:05:21+00:00
- **Updated**: 2020-03-17 14:53:06+00:00
- **Authors**: Dongxu Li, Xin Yu, Chenchen Xu, Lars Petersson, Hongdong Li
- **Comment**: CVPR2020 (oral) preprint
- **Journal**: None
- **Summary**: Word-level sign language recognition (WSLR) is a fundamental task in sign language interpretation. It requires models to recognize isolated sign words from videos. However, annotating WSLR data needs expert knowledge, thus limiting WSLR dataset acquisition. On the contrary, there are abundant subtitled sign news videos on the internet. Since these videos have no word-level annotation and exhibit a large domain gap from isolated signs, they cannot be directly used for training WSLR models. We observe that despite the existence of a large domain gap, isolated and news signs share the same visual concepts, such as hand gestures and body movements. Motivated by this observation, we propose a novel method that learns domain-invariant visual concepts and fertilizes WSLR models by transferring knowledge of subtitled news sign to them. To this end, we extract news signs using a base WSLR model, and then design a classifier jointly trained on news and isolated signs to coarsely align these two domain features. In order to learn domain-invariant features within each class and suppress domain-specific features, our method further resorts to an external memory to store the class centroids of the aligned news signs. We then design a temporal attention based on the learnt descriptor to improve recognition performance. Experimental results on standard WSLR datasets show that our method outperforms previous state-of-the-art methods significantly. We also demonstrate the effectiveness of our method on automatically localizing signs from sign news, achieving 28.1 for AP@0.5.



### Adaptive Semantic-Visual Tree for Hierarchical Embeddings
- **Arxiv ID**: http://arxiv.org/abs/2003.03707v1
- **DOI**: 10.1145/3343031.3350995
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.03707v1)
- **Published**: 2020-03-08 03:36:42+00:00
- **Updated**: 2020-03-08 03:36:42+00:00
- **Authors**: Shuo Yang, Wei Yu, Ying Zheng, Hongxun Yao, Tao Mei
- **Comment**: None
- **Journal**: The 27th ACM Multimedia (2019) 2097-2105
- **Summary**: Merchandise categories inherently form a semantic hierarchy with different levels of concept abstraction, especially for fine-grained categories. This hierarchy encodes rich correlations among various categories across different levels, which can effectively regularize the semantic space and thus make predictions less ambiguous. However, previous studies of fine-grained image retrieval primarily focus on semantic similarities or visual similarities. In a real application, merely using visual similarity may not satisfy the need of consumers to search merchandise with real-life images, e.g., given a red coat as a query image, we might get a red suit in recall results only based on visual similarity since they are visually similar. But the users actually want a coat rather than suit even the coat is with different color or texture attributes. We introduce this new problem based on photoshopping in real practice. That's why semantic information are integrated to regularize the margins to make "semantic" prior to "visual". To solve this new problem, we propose a hierarchical adaptive semantic-visual tree (ASVT) to depict the architecture of merchandise categories, which evaluates semantic similarities between different semantic levels and visual similarities within the same semantic class simultaneously. The semantic information satisfies the demand of consumers for similar merchandise with the query while the visual information optimizes the correlations within the semantic class. At each level, we set different margins based on the semantic hierarchy and incorporate them as prior information to learn a fine-grained feature embedding. To evaluate our framework, we propose a new dataset named JDProduct, with hierarchical labels collected from actual image queries and official merchandise images on an online shopping application. Extensive experimental results on the public CARS196 and CUB-



### Trajectory Grouping with Curvature Regularization for Tubular Structure Tracking
- **Arxiv ID**: http://arxiv.org/abs/2003.03710v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.03710v4)
- **Published**: 2020-03-08 03:40:26+00:00
- **Updated**: 2021-12-09 02:07:51+00:00
- **Authors**: Li Liu, Da Chen, Minglei Shu, Baosheng Li, Huazhong Shu, Michel Paques, Laurent D. Cohen
- **Comment**: None
- **Journal**: None
- **Summary**: Tubular structure tracking is a crucial task in the fields of computer vision and medical image analysis. The minimal paths-based approaches have exhibited their strong ability in tracing tubular structures, by which a tubular structure can be naturally modeled as a minimal geodesic path computed with a suitable geodesic metric. However, existing minimal paths-based tracing approaches still suffer from difficulties such as the shortcuts and short branches combination problems, especially when dealing with the images involving complicated tubular tree structures or background. In this paper, we introduce a new minimal paths-based model for minimally interactive tubular structure centerline extraction in conjunction with a perceptual grouping scheme. Basically, we take into account the prescribed tubular trajectories and curvature-penalized geodesic paths to seek suitable shortest paths. The proposed approach can benefit from the local smoothness prior on tubular structures and the global optimality of the used graph-based path searching scheme. Experimental results on both synthetic and real images prove that the proposed model indeed obtains outperformance comparing with the state-of-the-art minimal paths-based tubular structure tracing algorithms.



### Single-View 3D Object Reconstruction from Shape Priors in Memory
- **Arxiv ID**: http://arxiv.org/abs/2003.03711v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.03711v3)
- **Published**: 2020-03-08 03:51:07+00:00
- **Updated**: 2021-03-04 10:34:09+00:00
- **Authors**: Shuo Yang, Min Xu, Haozhe Xie, Stuart Perry, Jiahao Xia
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: Existing methods for single-view 3D object reconstruction directly learn to transform image features into 3D representations. However, these methods are vulnerable to images containing noisy backgrounds and heavy occlusions because the extracted image features do not contain enough information to reconstruct high-quality 3D shapes. Humans routinely use incomplete or noisy visual cues from an image to retrieve similar 3D shapes from their memory and reconstruct the 3D shape of an object. Inspired by this, we propose a novel method, named Mem3D, that explicitly constructs shape priors to supplement the missing information in the image. Specifically, the shape priors are in the forms of "image-voxel" pairs in the memory network, which is stored by a well-designed writing strategy during training. We also propose a voxel triplet loss function that helps to retrieve the precise 3D shapes that are highly related to the input image from shape priors. The LSTM-based shape encoder is introduced to extract information from the retrieved 3D shapes, which are useful in recovering the 3D shape of an object that is heavily occluded or in complex environments. Experimental results demonstrate that Mem3D significantly improves reconstruction quality and performs favorably against state-of-the-art methods on the ShapeNet and Pix3D datasets.



### OVC-Net: Object-Oriented Video Captioning with Temporal Graph and Detail Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2003.03715v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.03715v5)
- **Published**: 2020-03-08 04:34:58+00:00
- **Updated**: 2020-07-14 16:51:47+00:00
- **Authors**: Fangyi Zhu, Jenq-Neng Hwang, Zhanyu Ma, Guang Chen, Jun Guo
- **Comment**: None
- **Journal**: None
- **Summary**: Traditional video captioning requests a holistic description of the video, yet the detailed descriptions of the specific objects may not be available. Without associating the moving trajectories, these image-based data-driven methods cannot understand the activities from the spatio-temporal transitions in the inter-object visual features. Besides, adopting ambiguous clip-sentence pairs in training, it goes against learning the multi-modal functional mappings owing to the one-to-many nature. In this paper, we propose a novel task to understand the videos in object-level, named object-oriented video captioning. We introduce the video-based object-oriented video captioning network (OVC)-Net via temporal graph and detail enhancement to effectively analyze the activities along time and stably capture the vision-language connections under small-sample condition. The temporal graph provides useful supplement over previous image-based approaches, allowing to reason the activities from the temporal evolution of visual features and the dynamic movement of spatial locations. The detail enhancement helps to capture the discriminative features among different objects, with which the subsequent captioning module can yield more informative and precise descriptions. Thereafter, we construct a new dataset, providing consistent object-sentence pairs, to facilitate effective cross-modal learning. To demonstrate the effectiveness, we conduct experiments on the new dataset and compare it with the state-of-the-art video captioning methods. From the experimental results, the OVC-Net exhibits the ability of precisely describing the concurrent objects, and achieves the state-of-the-art performance.



### Online Self-Supervised Learning for Object Picking: Detecting Optimum Grasping Position using a Metric Learning Approach
- **Arxiv ID**: http://arxiv.org/abs/2003.03717v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.03717v1)
- **Published**: 2020-03-08 04:36:24+00:00
- **Updated**: 2020-03-08 04:36:24+00:00
- **Authors**: Kanata Suzuki, Yasuto Yokota, Yuzi Kanazawa, Tomoyoshi Takebayashi
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: Self-supervised learning methods are attractive candidates for automatic object picking. However, the trial samples lack the complete ground truth because the observable parts of the agent are limited. That is, the information contained in the trial samples is often insufficient to learn the specific grasping position of each object. Consequently, the training falls into a local solution, and the grasp positions learned by the robot are independent of the state of the object. In this study, the optimal grasping position of an individual object is determined from the grasping score, defined as the distance in the feature space obtained using metric learning. The closeness of the solution to the pre-designed optimal grasping position was evaluated in trials. The proposed method incorporates two types of feedback control: one feedback enlarges the grasping score when the grasping position approaches the optimum; the other reduces the negative feedback of the potential grasping positions among the grasping candidates. The proposed online self-supervised learning method employs two deep neural networks. : SSD that detects the grasping position of an object, and Siamese networks (SNs) that evaluate the trial sample using the similarity of two input data in the feature space. Our method embeds the relation of each grasping position as feature vectors by training the trial samples and a few pre-samples indicating the optimum grasping position. By incorporating the grasping score based on the feature space of SNs into the SSD training process, the method preferentially trains the optimum grasping position. In the experiment, the proposed method achieved a higher success rate than the baseline method using simple teaching signals. And the grasping scores in the feature space of the SNs accurately represented the grasping positions of the objects.



### Adversarial Camouflage: Hiding Physical-World Attacks with Natural Styles
- **Arxiv ID**: http://arxiv.org/abs/2003.08757v2
- **DOI**: None
- **Categories**: **cs.CV**, I.4.8; I.5
- **Links**: [PDF](http://arxiv.org/pdf/2003.08757v2)
- **Published**: 2020-03-08 07:22:41+00:00
- **Updated**: 2020-06-22 05:15:12+00:00
- **Authors**: Ranjie Duan, Xingjun Ma, Yisen Wang, James Bailey, A. K. Qin, Yun Yang
- **Comment**: Accepted to CVPR2020
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) are known to be vulnerable to adversarial examples. Existing works have mostly focused on either digital adversarial examples created via small and imperceptible perturbations, or physical-world adversarial examples created with large and less realistic distortions that are easily identified by human observers. In this paper, we propose a novel approach, called Adversarial Camouflage (\emph{AdvCam}), to craft and camouflage physical-world adversarial examples into natural styles that appear legitimate to human observers. Specifically, \emph{AdvCam} transfers large adversarial perturbations into customized styles, which are then "hidden" on-target object or off-target background. Experimental evaluation shows that, in both digital and physical-world scenarios, adversarial examples crafted by \emph{AdvCam} are well camouflaged and highly stealthy, while remaining effective in fooling state-of-the-art DNN image classifiers. Hence, \emph{AdvCam} is a flexible approach that can help craft stealthy attacks to evaluate the robustness of DNNs. \emph{AdvCam} can also be used to protect private information from being detected by deep learning systems.



### A Multi-scale CNN-CRF Framework for Environmental Microorganism Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2003.03744v2
- **DOI**: 10.1155/2020/4621403
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.03744v2)
- **Published**: 2020-03-08 08:30:30+00:00
- **Updated**: 2020-07-08 12:32:51+00:00
- **Authors**: Jinghua Zhang, Chen Li, Frank Kulwa, Xin Zhao, Changhao Sun, Zihan Li, Tao Jiang, Hong Li, Shouliang Qi
- **Comment**: None
- **Journal**: BioMed Research International, vol. 2020, Article ID 4621403, 27
  pages, 2020
- **Summary**: To assist researchers to identify Environmental Microorganisms (EMs) effectively, a Multiscale CNN-CRF (MSCC) framework for the EM image segmentation is proposed in this paper. There are two parts in this framework: The first is a novel pixel-level segmentation approach, using a newly introduced Convolutional Neural Network (CNN), namely, "mU-Net-B3", with a dense Conditional Random Field (CRF) postprocessing. The second is a VGG-16 based patch-level segmentation method with a novel "buffer" strategy, which further improves the segmentation quality of the details of the EMs. In the experiment, compared with the state-of-the-art methods on 420 EM images, the proposed MSCC method reduces the memory requirement from 355 MB to 103 MB, improves the overall evaluation indexes (Dice, Jaccard, Recall, Accuracy) from 85.24%, 77.42%, 82.27%, and 96.76% to 87.13%, 79.74%, 87.12%, and 96.91%, respectively, and reduces the volume overlap error from 22.58% to 20.26%. Therefore, the MSCC method shows great potential in the EM segmentation field.



### Better Captioning with Sequence-Level Exploration
- **Arxiv ID**: http://arxiv.org/abs/2003.03749v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.03749v1)
- **Published**: 2020-03-08 09:08:03+00:00
- **Updated**: 2020-03-08 09:08:03+00:00
- **Authors**: Jia Chen, Qin Jin
- **Comment**: accepted by CVPR 2020
- **Journal**: None
- **Summary**: Sequence-level learning objective has been widely used in captioning tasks to achieve the state-of-the-art performance for many models. In this objective, the model is trained by the reward on the quality of its generated captions (sequence-level). In this work, we show the limitation of the current sequence-level learning objective for captioning tasks from both theory and empirical result. In theory, we show that the current objective is equivalent to only optimizing the precision side of the caption set generated by the model and therefore overlooks the recall side. Empirical result shows that the model trained by this objective tends to get lower score on the recall side. We propose to add a sequence-level exploration term to the current objective to boost recall. It guides the model to explore more plausible captions in the training. In this way, the proposed objective takes both the precision and recall sides of generated captions into account. Experiments show the effectiveness of the proposed method on both video and image captioning datasets.



### Perceptual Image Super-Resolution with Progressive Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/2003.03756v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.03756v4)
- **Published**: 2020-03-08 10:19:34+00:00
- **Updated**: 2020-03-19 03:13:50+00:00
- **Authors**: Lone Wong, Deli Zhao, Shaohua Wan, Bo Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Single Image Super-Resolution (SISR) aims to improve resolution of small-size low-quality image from a single one. With popularity of consumer electronics in our daily life, this topic has become more and more attractive. In this paper, we argue that the curse of dimensionality is the underlying reason of limiting the performance of state-of-the-art algorithms. To address this issue, we propose Progressive Adversarial Network (PAN) that is capable of coping with this difficulty for domain-specific image super-resolution. The key principle of PAN is that we do not apply any distance-based reconstruction errors as the loss to be optimized, thus free from the restriction of the curse of dimensionality. To maintain faithful reconstruction precision, we resort to U-Net and progressive growing of neural architecture. The low-level features in encoder can be transferred into decoder to enhance textural details with U-Net. Progressive growing enhances image resolution gradually, thereby preserving precision of recovered image. Moreover, to obtain high-fidelity outputs, we leverage the framework of the powerful StyleGAN to perform adversarial learning. Without the curse of dimensionality, our model can super-resolve large-size images with remarkable photo-realistic details and few distortions. Extensive experiments demonstrate the superiority of our algorithm over state-of-the-arts both quantitatively and qualitatively.



### 3D Object Detection from a Single Fisheye Image Without a Single Fisheye Training Image
- **Arxiv ID**: http://arxiv.org/abs/2003.03759v3
- **DOI**: None
- **Categories**: **cs.CV**, I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/2003.03759v3)
- **Published**: 2020-03-08 11:03:05+00:00
- **Updated**: 2021-05-31 05:56:56+00:00
- **Authors**: Elad Plaut, Erez Ben Yaacov, Bat El Shlomo
- **Comment**: 9 pages, 7 figures
- **Journal**: None
- **Summary**: Existing monocular 3D object detection methods have been demonstrated on rectilinear perspective images and fail in images with alternative projections such as those acquired by fisheye cameras. Previous works on object detection in fisheye images have focused on 2D object detection, partly due to the lack of 3D datasets of such images. In this work, we show how to use existing monocular 3D object detection models, trained only on rectilinear images, to detect 3D objects in images from fisheye cameras, without using any fisheye training data. We outperform the only existing method for monocular 3D object detection in panoramas on a benchmark of synthetic data, despite the fact that the existing method trains on the target non-rectilinear projection whereas we train only on rectilinear images. We also experiment with an internal dataset of real fisheye images.



### A Benchmark for Temporal Color Constancy
- **Arxiv ID**: http://arxiv.org/abs/2003.03763v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.03763v1)
- **Published**: 2020-03-08 11:17:18+00:00
- **Updated**: 2020-03-08 11:17:18+00:00
- **Authors**: Yanlin Qian, Jani Käpylä, Joni-Kristian Kämäräinen, Samu Koskinen, Jiri Matas
- **Comment**: 16 pages, 6 figures
- **Journal**: None
- **Summary**: Temporal Color Constancy (CC) is a recently proposed approach that challenges the conventional single-frame color constancy. The conventional approach is to use a single frame - shot frame - to estimate the scene illumination color. In temporal CC, multiple frames from the view finder sequence are used to estimate the color. However, there are no realistic large scale temporal color constancy datasets for method evaluation. In this work, a new temporal CC benchmark is introduced. The benchmark comprises of (1) 600 real-world sequences recorded with a high-resolution mobile phone camera, (2) a fixed train-test split which ensures consistent evaluation, and (3) a baseline method which achieves high accuracy in the new benchmark and the dataset used in previous works. Results for more than 20 well-known color constancy methods including the recent state-of-the-arts are reported in our experiments.



### DFVS: Deep Flow Guided Scene Agnostic Image Based Visual Servoing
- **Arxiv ID**: http://arxiv.org/abs/2003.03766v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.03766v1)
- **Published**: 2020-03-08 11:42:36+00:00
- **Updated**: 2020-03-08 11:42:36+00:00
- **Authors**: Y V S Harish, Harit Pandya, Ayush Gaud, Shreya Terupally, Sai Shankar, K. Madhava Krishna
- **Comment**: Accepted in International Conference on Robotics and Automation
  (ICRA) 2020, IEEE
- **Journal**: None
- **Summary**: Existing deep learning based visual servoing approaches regress the relative camera pose between a pair of images. Therefore, they require a huge amount of training data and sometimes fine-tuning for adaptation to a novel scene. Furthermore, current approaches do not consider underlying geometry of the scene and rely on direct estimation of camera pose. Thus, inaccuracies in prediction of the camera pose, especially for distant goals, lead to a degradation in the servoing performance. In this paper, we propose a two-fold solution: (i) We consider optical flow as our visual features, which are predicted using a deep neural network. (ii) These flow features are then systematically integrated with depth estimates provided by another neural network using interaction matrix. We further present an extensive benchmark in a photo-realistic 3D simulation across diverse scenes to study the convergence and generalisation of visual servoing approaches. We show convergence for over 3m and 40 degrees while maintaining precise positioning of under 2cm and 1 degree on our challenging benchmark where the existing approaches that are unable to converge for majority of scenarios for over 1.5m and 20 degrees. Furthermore, we also evaluate our approach for a real scenario on an aerial robot. Our approach generalizes to novel scenarios producing precise and robust servoing performance for 6 degrees of freedom positioning tasks with even large camera transformations without any retraining or fine-tuning.



### Pixel-in-Pixel Net: Towards Efficient Facial Landmark Detection in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2003.03771v3
- **DOI**: 10.1007/s11263-021-01521-4
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.03771v3)
- **Published**: 2020-03-08 12:23:42+00:00
- **Updated**: 2021-09-11 04:52:46+00:00
- **Authors**: Haibo Jin, Shengcai Liao, Ling Shao
- **Comment**: Accepted to IJCV
- **Journal**: None
- **Summary**: Recently, heatmap regression models have become popular due to their superior performance in locating facial landmarks. However, three major problems still exist among these models: (1) they are computationally expensive; (2) they usually lack explicit constraints on global shapes; (3) domain gaps are commonly present. To address these problems, we propose Pixel-in-Pixel Net (PIPNet) for facial landmark detection. The proposed model is equipped with a novel detection head based on heatmap regression, which conducts score and offset predictions simultaneously on low-resolution feature maps. By doing so, repeated upsampling layers are no longer necessary, enabling the inference time to be largely reduced without sacrificing model accuracy. Besides, a simple but effective neighbor regression module is proposed to enforce local constraints by fusing predictions from neighboring landmarks, which enhances the robustness of the new detection head. To further improve the cross-domain generalization capability of PIPNet, we propose self-training with curriculum. This training strategy is able to mine more reliable pseudo-labels from unlabeled data across domains by starting with an easier task, then gradually increasing the difficulty to provide more precise labels. Extensive experiments demonstrate the superiority of PIPNet, which obtains state-of-the-art results on three out of six popular benchmarks under the supervised setting. The results on two cross-domain test sets are also consistently improved compared to the baselines. Notably, our lightweight version of PIPNet runs at 35.7 FPS and 200 FPS on CPU and GPU, respectively, while still maintaining a competitive accuracy to state-of-the-art methods. The code of PIPNet is available at https://github.com/jhb86253817/PIPNet.



### IMRAM: Iterative Matching with Recurrent Attention Memory for Cross-Modal Image-Text Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2003.03772v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.03772v1)
- **Published**: 2020-03-08 12:24:41+00:00
- **Updated**: 2020-03-08 12:24:41+00:00
- **Authors**: Hui Chen, Guiguang Ding, Xudong Liu, Zijia Lin, Ji Liu, Jungong Han
- **Comment**: 9 pages; Accepted by CVPR2020
- **Journal**: None
- **Summary**: Enabling bi-directional retrieval of images and texts is important for understanding the correspondence between vision and language. Existing methods leverage the attention mechanism to explore such correspondence in a fine-grained manner. However, most of them consider all semantics equally and thus align them uniformly, regardless of their diverse complexities. In fact, semantics are diverse (i.e. involving different kinds of semantic concepts), and humans usually follow a latent structure to combine them into understandable languages. It may be difficult to optimally capture such sophisticated correspondences in existing methods. In this paper, to address such a deficiency, we propose an Iterative Matching with Recurrent Attention Memory (IMRAM) method, in which correspondences between images and texts are captured with multiple steps of alignments. Specifically, we introduce an iterative matching scheme to explore such fine-grained correspondence progressively. A memory distillation unit is used to refine alignment knowledge from early steps to later ones. Experiment results on three benchmark datasets, i.e. Flickr8K, Flickr30K, and MS COCO, show that our IMRAM achieves state-of-the-art performance, well demonstrating its effectiveness. Experiments on a practical business advertisement dataset, named \Ads{}, further validates the applicability of our method in practical scenarios.



### Rectifying Pseudo Label Learning via Uncertainty Estimation for Domain Adaptive Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2003.03773v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.03773v3)
- **Published**: 2020-03-08 12:37:19+00:00
- **Updated**: 2020-10-15 06:07:14+00:00
- **Authors**: Zhedong Zheng, Yi Yang
- **Comment**: 13 pages, 6 figures, 10 tables, accepted by IJCV
- **Journal**: None
- **Summary**: This paper focuses on the unsupervised domain adaptation of transferring the knowledge from the source domain to the target domain in the context of semantic segmentation. Existing approaches usually regard the pseudo label as the ground truth to fully exploit the unlabeled target-domain data. Yet the pseudo labels of the target-domain data are usually predicted by the model trained on the source domain. Thus, the generated labels inevitably contain the incorrect prediction due to the discrepancy between the training domain and the test domain, which could be transferred to the final adapted model and largely compromises the training process. To overcome the problem, this paper proposes to explicitly estimate the prediction uncertainty during training to rectify the pseudo label learning for unsupervised semantic segmentation adaptation. Given the input image, the model outputs the semantic segmentation prediction as well as the uncertainty of the prediction. Specifically, we model the uncertainty via the prediction variance and involve the uncertainty into the optimization objective. To verify the effectiveness of the proposed method, we evaluate the proposed method on two prevalent synthetic-to-real semantic segmentation benchmarks, i.e., GTA5 -> Cityscapes and SYNTHIA -> Cityscapes, as well as one cross-city benchmark, i.e., Cityscapes -> Oxford RobotCar. We demonstrate through extensive experiments that the proposed approach (1) dynamically sets different confidence thresholds according to the prediction variance, (2) rectifies the learning from noisy pseudo labels, and (3) achieves significant improvements over the conventional pseudo label learning and yields competitive performance on all three benchmarks.



### DADA: Differentiable Automatic Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2003.03780v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.03780v3)
- **Published**: 2020-03-08 13:23:14+00:00
- **Updated**: 2020-07-30 14:32:24+00:00
- **Authors**: Yonggang Li, Guosheng Hu, Yongtao Wang, Timothy Hospedales, Neil M. Robertson, Yongxin Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Data augmentation (DA) techniques aim to increase data variability, and thus train deep networks with better generalisation. The pioneering AutoAugment automated the search for optimal DA policies with reinforcement learning. However, AutoAugment is extremely computationally expensive, limiting its wide applicability. Followup works such as Population Based Augmentation (PBA) and Fast AutoAugment improved efficiency, but their optimization speed remains a bottleneck. In this paper, we propose Differentiable Automatic Data Augmentation (DADA) which dramatically reduces the cost. DADA relaxes the discrete DA policy selection to a differentiable optimization problem via Gumbel-Softmax. In addition, we introduce an unbiased gradient estimator, RELAX, leading to an efficient and effective one-pass optimization strategy to learn an efficient and accurate DA policy. We conduct extensive experiments on CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets. Furthermore, we demonstrate the value of Auto DA in pre-training for downstream detection problems. Results show our DADA is at least one order of magnitude faster than the state-of-the-art while achieving very comparable accuracy. The code is available at https://github.com/VDIGPKU/DADA.



### Mind the Gap: Enlarging the Domain Gap in Open Set Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2003.03787v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.03787v2)
- **Published**: 2020-03-08 14:20:24+00:00
- **Updated**: 2020-03-10 09:15:32+00:00
- **Authors**: Dongliang Chang, Aneeshan Sain, Zhanyu Ma, Yi-Zhe Song, Jun Guo
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised domain adaptation aims to leverage labeled data from a source domain to learn a classifier for an unlabeled target domain. Among its many variants, open set domain adaptation (OSDA) is perhaps the most challenging, as it further assumes the presence of unknown classes in the target domain. In this paper, we study OSDA with a particular focus on enriching its ability to traverse across larger domain gaps. Firstly, we show that existing state-of-the-art methods suffer a considerable performance drop in the presence of larger domain gaps, especially on a new dataset (PACS) that we re-purposed for OSDA. We then propose a novel framework to specifically address the larger domain gaps. The key insight lies with how we exploit the mutually beneficial information between two networks; (a) to separate samples of known and unknown classes, (b) to maximize the domain confusion between source and target domain without the influence of unknown samples. It follows that (a) and (b) will mutually supervise each other and alternate until convergence. Extensive experiments are conducted on Office-31, Office-Home, and PACS datasets, demonstrating the superiority of our method in comparison to other state-of-the-arts. Code available at https://github.com/dongliangchang/Mutual-to-Separate/



### 1D Probabilistic Undersampling Pattern Optimization for MR Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2003.03797v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.03797v3)
- **Published**: 2020-03-08 15:15:37+00:00
- **Updated**: 2022-01-09 03:37:53+00:00
- **Authors**: Shengke Xue, Ruiliang Bai, Xinyu Jin
- **Comment**: Manuscript temporarily, will be submitted IEEE Trans. Med. Imag.
  eventually
- **Journal**: None
- **Summary**: Magnetic resonance imaging (MRI) is mainly limited by long scanning time and vulnerable to human tissue motion artifacts, in 3D clinical scenarios. Thus, k-space undersampling is used to accelerate the acquisition of MRI while leading to visually poor MR images. Recently, some studies 1) use effective undersampling patterns, or 2) design deep neural networks to improve the quality of resulting images. However, they are considered as two separate optimization strategies. In this paper, we propose a cross-domain network for MR image reconstruction, in a retrospective data-driven manner, under limited sampling rates. Our method can simultaneously obtain the optimal undersampling pattern (in k-space) and the reconstruction model, which are customized to the type of training data, by using an end-to-end learning strategy. We propose a 1D probabilistic undersampling layer, to obtain the optimal undersampling pattern and its probability distribution in a differentiable way. We propose a 1D inverse Fourier transform layer, which connects the Fourier domain and the image domain during the forward pass and the backpropagation. In addition, by training 3D fully-sampled k-space data and MR images with the traditional Euclidean loss, we discover the universal relationship between the probability distribution of the optimal undersampling pattern and its corresponding sampling rate. Experiments show that the quantitative and qualitative results of recovered MR images by our 1D probabilistic undersampling pattern obviously outperform those of several existing sampling strategies.



### PULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models
- **Arxiv ID**: http://arxiv.org/abs/2003.03808v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.03808v3)
- **Published**: 2020-03-08 16:44:31+00:00
- **Updated**: 2020-07-20 21:38:32+00:00
- **Authors**: Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, Cynthia Rudin
- **Comment**: Sachit Menon and Alexandru Damian contributed equally. Computer
  Vision and Pattern Recognition (CVPR) 2020
- **Journal**: None
- **Summary**: The primary aim of single-image super-resolution is to construct high-resolution (HR) images from corresponding low-resolution (LR) inputs. In previous approaches, which have generally been supervised, the training objective typically measures a pixel-wise average distance between the super-resolved (SR) and HR images. Optimizing such metrics often leads to blurring, especially in high variance (detailed) regions. We propose an alternative formulation of the super-resolution problem based on creating realistic SR images that downscale correctly. We present an algorithm addressing this problem, PULSE (Photo Upsampling via Latent Space Exploration), which generates high-resolution, realistic images at resolutions previously unseen in the literature. It accomplishes this in an entirely self-supervised fashion and is not confined to a specific degradation operator used during training, unlike previous methods (which require supervised training on databases of LR-HR image pairs). Instead of starting with the LR image and slowly adding detail, PULSE traverses the high-resolution natural image manifold, searching for images that downscale to the original LR image. This is formalized through the "downscaling loss," which guides exploration through the latent space of a generative model. By leveraging properties of high-dimensional Gaussians, we restrict the search space to guarantee realistic outputs. PULSE thereby generates super-resolved images that both are realistic and downscale correctly. We show proof of concept of our approach in the domain of face super-resolution (i.e., face hallucination). We also present a discussion of the limitations and biases of the method as currently implemented with an accompanying model card with relevant metrics. Our method outperforms state-of-the-art methods in perceptual quality at higher resolutions and scale factors than previously possible.



### Reduction of Surgical Risk Through the Evaluation of Medical Imaging Diagnostics
- **Arxiv ID**: http://arxiv.org/abs/2003.08748v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/2003.08748v1)
- **Published**: 2020-03-08 17:06:57+00:00
- **Updated**: 2020-03-08 17:06:57+00:00
- **Authors**: Marco A. V. M. Grinet, Nuno M. Garcia, Ana I. R. Gouveia, Jose A. F. Moutinho, Abel J. P. Gomes
- **Comment**: 25 pages, 7 figures, Scientific grant report
- **Journal**: None
- **Summary**: Computer aided diagnosis (CAD) of Breast Cancer (BRCA) images has been an active area of research in recent years. The main goals of this research is to develop reliable automatic methods for detecting and diagnosing different types of BRCA from diagnostic images. In this paper, we present a review of the state of the art CAD methods applied to magnetic resonance (MRI) and mammography images of BRCA patients. The review aims to provide an extensive introduction to different features extracted from BRCA images through texture and statistical analysis and to categorize deep learning frameworks and data structures capable of using metadata to aggregate relevant information to assist oncologists and radiologists. We divide the existing literature according to the imaging modality and into radiomics, machine learning, or combination of both. We also emphasize the difference between each modality and methods strengths and weaknesses and analyze their performance in detecting BRCA through a quantitative comparison. We compare the results of various approaches for implementing CAD systems for the detection of BRCA. Each approachs standard workflow components are reviewed and summary tables provided. We present an extensive literature review of radiomics feature extraction techniques and machine learning methods applied in BRCA diagnosis and detection, focusing on data preparation, data structures, pre processing and post processing strategies available in the literature. There is a growing interest on radiomic feature extraction and machine learning methods for BRCA detection through histopathological images, MRI and mammography images. However, there isnt a CAD method able to combine distinct data types to provide the best diagnostic results. Employing data fusion techniques to medical images and patient data could lead to improved detection and classification results.



### Implementation of Deep Neural Networks to Classify EEG Signals using Gramian Angular Summation Field for Epilepsy Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/2003.04534v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2003.04534v1)
- **Published**: 2020-03-08 17:30:14+00:00
- **Updated**: 2020-03-08 17:30:14+00:00
- **Authors**: K. Palani Thanaraj, B. Parvathavarthini, U. John Tanik, V. Rajinikanth, Seifedine Kadry, K. Kamalanand
- **Comment**: 9 pages, 8 Figures, 2 Tables
- **Journal**: None
- **Summary**: This paper evaluates the approach of imaging timeseries data such as EEG in the diagnosis of epilepsy through Deep Neural Network (DNN). EEG signal is transformed into an RGB image using Gramian Angular Summation Field (GASF). Many such EEG epochs are transformed into GASF images for the normal and focal EEG signals. Then, some of the widely used Deep Neural Networks for image classification problems are used here to detect the focal GASF images. Three pre-trained DNN such as the AlexNet, VGG16, and VGG19 are validated for epilepsy detection based on the transfer learning approach. Furthermore, the textural features are extracted from GASF images, and prominent features are selected for a multilayer Artificial Neural Network (ANN) classifier. Lastly, a Custom Convolutional Neural Network (CNN) with three CNN layers, Batch Normalization, Max-pooling layer, and Dense layers, is proposed for epilepsy diagnosis from GASF images. The results of this paper show that the Custom CNN model was able to discriminate against the focal and normal GASF images with an average peak Precision of 0.885, Recall of 0.92, and F1-score of 0.90. Moreover, the Area Under the Curve (AUC) value of the Receiver Operating Characteristic (ROC) curve is 0.92 for the Custom CNN model. This paper suggests that Deep Learning methods widely used in image classification problems can be an alternative approach for epilepsy detection from EEG signals through GASF images.



### No Surprises: Training Robust Lung Nodule Detection for Low-Dose CT Scans by Augmenting with Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/2003.03824v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.03824v2)
- **Published**: 2020-03-08 18:32:46+00:00
- **Updated**: 2020-10-28 23:55:00+00:00
- **Authors**: Siqi Liu, Arnaud Arindra Adiyoso Setio, Florin C. Ghesu, Eli Gibson, Sasa Grbic, Bogdan Georgescu, Dorin Comaniciu
- **Comment**: Published on IEEE Trans. on Medical Imaging
- **Journal**: None
- **Summary**: Detecting malignant pulmonary nodules at an early stage can allow medical interventions which may increase the survival rate of lung cancer patients. Using computer vision techniques to detect nodules can improve the sensitivity and the speed of interpreting chest CT for lung cancer screening. Many studies have used CNNs to detect nodule candidates. Though such approaches have been shown to outperform the conventional image processing based methods regarding the detection accuracy, CNNs are also known to be limited to generalize on under-represented samples in the training set and prone to imperceptible noise perturbations. Such limitations can not be easily addressed by scaling up the dataset or the models. In this work, we propose to add adversarial synthetic nodules and adversarial attack samples to the training data to improve the generalization and the robustness of the lung nodule detection systems. To generate hard examples of nodules from a differentiable nodule synthesizer, we use projected gradient descent (PGD) to search the latent code within a bounded neighbourhood that would generate nodules to decrease the detector response. To make the network more robust to unanticipated noise perturbations, we use PGD to search for noise patterns that can trigger the network to give over-confident mistakes. By evaluating on two different benchmark datasets containing consensus annotations from three radiologists, we show that the proposed techniques can improve the detection performance on real CT data. To understand the limitations of both the conventional networks and the proposed augmented networks, we also perform stress-tests on the false positive reduction networks by feeding different types of artificially produced patches. We show that the augmented networks are more robust to both under-represented nodules as well as resistant to noise perturbations.



### $Π-$nets: Deep Polynomial Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2003.03828v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.03828v2)
- **Published**: 2020-03-08 18:48:43+00:00
- **Updated**: 2020-03-26 17:25:40+00:00
- **Authors**: Grigorios G. Chrysos, Stylianos Moschoglou, Giorgos Bouritsas, Yannis Panagakis, Jiankang Deng, Stefanos Zafeiriou
- **Comment**: Accepted in CVPR 2020
- **Journal**: None
- **Summary**: Deep Convolutional Neural Networks (DCNNs) is currently the method of choice both for generative, as well as for discriminative learning in computer vision and machine learning. The success of DCNNs can be attributed to the careful selection of their building blocks (e.g., residual blocks, rectifiers, sophisticated normalization schemes, to mention but a few). In this paper, we propose $\Pi$-Nets, a new class of DCNNs. $\Pi$-Nets are polynomial neural networks, i.e., the output is a high-order polynomial of the input. $\Pi$-Nets can be implemented using special kind of skip connections and their parameters can be represented via high-order tensors. We empirically demonstrate that $\Pi$-Nets have better representation power than standard DCNNs and they even produce good results without the use of non-linear activation functions in a large battery of tasks and signals, i.e., images, graphs, and audio. When used in conjunction with activation functions, $\Pi$-Nets produce state-of-the-art results in challenging tasks, such as image generation. Lastly, our framework elucidates why recent generative models, such as StyleGAN, improve upon their predecessors, e.g., ProGAN.



### Fine-Grained Visual Classification via Progressive Multi-Granularity Training of Jigsaw Patches
- **Arxiv ID**: http://arxiv.org/abs/2003.03836v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.03836v3)
- **Published**: 2020-03-08 19:27:30+00:00
- **Updated**: 2020-07-19 06:56:10+00:00
- **Authors**: Ruoyi Du, Dongliang Chang, Ayan Kumar Bhunia, Jiyang Xie, Zhanyu Ma, Yi-Zhe Song, Jun Guo
- **Comment**: None
- **Journal**: None
- **Summary**: Fine-grained visual classification (FGVC) is much more challenging than traditional classification tasks due to the inherently subtle intra-class object variations. Recent works mainly tackle this problem by focusing on how to locate the most discriminative parts, more complementary parts, and parts of various granularities. However, less effort has been placed to which granularities are the most discriminative and how to fuse information cross multi-granularity. In this work, we propose a novel framework for fine-grained visual classification to tackle these problems. In particular, we propose: (i) a progressive training strategy that effectively fuses features from different granularities, and (ii) a random jigsaw patch generator that encourages the network to learn features at specific granularities. We obtain state-of-the-art performances on several standard FGVC benchmark datasets, where the proposed method consistently outperforms existing methods or delivers competitive results. The code will be available at https://github.com/PRIS-CV/PMG-Progressive-Multi-Granularity-Training.



### Active Fine-Tuning from gMAD Examples Improves Blind Image Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/2003.03849v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.03849v2)
- **Published**: 2020-03-08 21:19:01+00:00
- **Updated**: 2021-04-08 10:45:16+00:00
- **Authors**: Zhihua Wang, Kede Ma
- **Comment**: 14 pages
- **Journal**: None
- **Summary**: The research in image quality assessment (IQA) has a long history, and significant progress has been made by leveraging recent advances in deep neural networks (DNNs). Despite high correlation numbers on existing IQA datasets, DNN-based models may be easily falsified in the group maximum differentiation (gMAD) competition with strong counterexamples being identified. Here we show that gMAD examples can be used to improve blind IQA (BIQA) methods. Specifically, we first pre-train a DNN-based BIQA model using multiple noisy annotators, and fine-tune it on multiple subject-rated databases of synthetically distorted images, resulting in a top-performing baseline model. We then seek pairs of images by comparing the baseline model with a set of full-reference IQA methods in gMAD. The resulting gMAD examples are most likely to reveal the relative weaknesses of the baseline, and suggest potential ways for refinement. We query ground truth quality annotations for the selected images in a well controlled laboratory environment, and further fine-tune the baseline on the combination of human-rated images from gMAD and existing databases. This process may be iterated, enabling active and progressive fine-tuning from gMAD examples for BIQA. We demonstrate the feasibility of our active learning scheme on a large-scale unlabeled image set, and show that the fine-tuned method achieves improved generalizability in gMAD, without destroying performance on previously trained databases.



### A Tracking System For Baseball Game Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2003.03856v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.03856v1)
- **Published**: 2020-03-08 22:04:54+00:00
- **Updated**: 2020-03-08 22:04:54+00:00
- **Authors**: Nina Wiedemann, Carlos Dietrich, Claudio T. Silva
- **Comment**: None
- **Journal**: None
- **Summary**: The baseball game is often seen as many contests that are performed between individuals. The duel between the pitcher and the batter, for example, is considered the engine that drives the sport. The pitchers use a variety of strategies to gain competitive advantage against the batter, who does his best to figure out the ball trajectory and react in time for a hit. In this work, we propose a system that captures the movements of the pitcher, the batter, and the ball in a high level of detail, and discuss several ways how this information may be processed to compute interesting statistics. We demonstrate on a large database of videos that our methods achieve comparable results as previous systems, while operating solely on video material. In addition, state-of-the-art AI techniques are incorporated to augment the amount of information that is made available for players, coaches, teams, and fans.



### Salient Facial Features from Humans and Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2003.08765v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08765v1)
- **Published**: 2020-03-08 22:41:04+00:00
- **Updated**: 2020-03-08 22:41:04+00:00
- **Authors**: Shanmeng Sun, Wei Zhen Teoh, Michael Guerzhoy
- **Comment**: 10 pages, 6 figures
- **Journal**: None
- **Summary**: In this work, we explore the features that are used by humans and by convolutional neural networks (ConvNets) to classify faces. We use Guided Backpropagation (GB) to visualize the facial features that influence the output of a ConvNet the most when identifying specific individuals; we explore how to best use GB for that purpose. We use a human intelligence task to find out which facial features humans find to be the most important for identifying specific individuals. We explore the differences between the saliency information gathered from humans and from ConvNets.   Humans develop biases in employing available information on facial features to discriminate across faces. Studies show these biases are influenced both by neurological development and by each individual's social experience. In recent years the computer vision community has achieved human-level performance in many face processing tasks with deep neural network-based models. These face processing systems are also subject to systematic biases due to model architectural choices and training data distribution.



