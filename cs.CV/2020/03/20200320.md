# Arxiv Papers in cs.CV on 2020-03-20
### Cross-Shape Attention for Part Segmentation of 3D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2003.09053v6
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.09053v6)
- **Published**: 2020-03-20 00:23:10+00:00
- **Updated**: 2023-07-05 16:26:28+00:00
- **Authors**: Marios Loizou, Siddhant Garg, Dmitry Petrov, Melinos Averkiou, Evangelos Kalogerakis
- **Comment**: None
- **Journal**: None
- **Summary**: We present a deep learning method that propagates point-wise feature representations across shapes within a collection for the purpose of 3D shape segmentation. We propose a cross-shape attention mechanism to enable interactions between a shape's point-wise features and those of other shapes. The mechanism assesses both the degree of interaction between points and also mediates feature propagation across shapes, improving the accuracy and consistency of the resulting point-wise feature representations for shape segmentation. Our method also proposes a shape retrieval measure to select suitable shapes for cross-shape attention operations for each test shape. Our experiments demonstrate that our approach yields state-of-the-art results in the popular PartNet dataset.



### Weakly Supervised Context Encoder using DICOM metadata in Ultrasound Imaging
- **Arxiv ID**: http://arxiv.org/abs/2003.09070v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.09070v1)
- **Published**: 2020-03-20 02:17:03+00:00
- **Updated**: 2020-03-20 02:17:03+00:00
- **Authors**: Szu-Yeu Hu, Shuhang Wang, Wei-Hung Weng, JingChao Wang, XiaoHong Wang, Arinc Ozturk, Qian Li, Viksit Kumar, Anthony E. Samir
- **Comment**: Accept as a workshop paper at AI4AH, ICLR 2020
- **Journal**: None
- **Summary**: Modern deep learning algorithms geared towards clinical adaption rely on a significant amount of high fidelity labeled data. Low-resource settings pose challenges like acquiring high fidelity data and becomes the bottleneck for developing artificial intelligence applications. Ultrasound images, stored in Digital Imaging and Communication in Medicine (DICOM) format, have additional metadata data corresponding to ultrasound image parameters and medical exams. In this work, we leverage DICOM metadata from ultrasound images to help learn representations of the ultrasound image. We demonstrate that the proposed method outperforms the non-metadata based approaches across different downstream tasks.



### Kidney segmentation using 3D U-Net localized with Expectation Maximization
- **Arxiv ID**: http://arxiv.org/abs/2003.09075v1
- **DOI**: 10.1109/SSIAI49293.2020.9094601
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.09075v1)
- **Published**: 2020-03-20 02:38:32+00:00
- **Updated**: 2020-03-20 02:38:32+00:00
- **Authors**: Omid Bazgir, Kai Barck, Richard A. D. Carano, Robby M. Weimer, Luke Xie
- **Comment**: None
- **Journal**: None
- **Summary**: Kidney volume is greatly affected in several renal diseases. Precise and automatic segmentation of the kidney can help determine kidney size and evaluate renal function. Fully convolutional neural networks have been used to segment organs from large biomedical 3D images. While these networks demonstrate state-of-the-art segmentation performances, they do not immediately translate to small foreground objects, small sample sizes, and anisotropic resolution in MRI datasets. In this paper we propose a new framework to address some of the challenges for segmenting 3D MRI. These methods were implemented on preclinical MRI for segmenting kidneys in an animal model of lupus nephritis. Our implementation strategy is twofold: 1) to utilize additional MRI diffusion images to detect the general kidney area, and 2) to reduce the 3D U-Net kernels to handle small sample sizes. Using this approach, a Dice similarity coefficient of 0.88 was achieved with a limited dataset of n=196. This segmentation strategy with careful optimization can be applied to various renal injuries or other organ systems.



### Reducing the Sim-to-Real Gap for Event Cameras
- **Arxiv ID**: http://arxiv.org/abs/2003.09078v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.09078v5)
- **Published**: 2020-03-20 02:44:29+00:00
- **Updated**: 2020-08-22 05:49:32+00:00
- **Authors**: Timo Stoffregen, Cedric Scheerlinck, Davide Scaramuzza, Tom Drummond, Nick Barnes, Lindsay Kleeman, Robert Mahony
- **Comment**: Figure 5 fixed (had a glitch)
- **Journal**: European Conference on Computer Vision, 2020
- **Summary**: Event cameras are paradigm-shifting novel sensors that report asynchronous, per-pixel brightness changes called 'events' with unparalleled low latency. This makes them ideal for high speed, high dynamic range scenes where conventional cameras would fail. Recent work has demonstrated impressive results using Convolutional Neural Networks (CNNs) for video reconstruction and optic flow with events. We present strategies for improving training data for event based CNNs that result in 20-40% boost in performance of existing state-of-the-art (SOTA) video reconstruction networks retrained with our method, and up to 15% for optic flow networks. A challenge in evaluating event based video reconstruction is lack of quality ground truth images in existing datasets. To address this, we present a new High Quality Frames (HQF) dataset, containing events and ground truth frames from a DAVIS240C that are well-exposed and minimally motion-blurred. We evaluate our method on HQF + several existing major event camera datasets.



### A Graduated Filter Method for Large Scale Robust Estimation
- **Arxiv ID**: http://arxiv.org/abs/2003.09080v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.09080v1)
- **Published**: 2020-03-20 02:51:31+00:00
- **Updated**: 2020-03-20 02:51:31+00:00
- **Authors**: Huu Le, Christopher Zach
- **Comment**: None
- **Journal**: None
- **Summary**: Due to the highly non-convex nature of large-scale robust parameter estimation, avoiding poor local minima is challenging in real-world applications where input data is contaminated by a large or unknown fraction of outliers. In this paper, we introduce a novel solver for robust estimation that possesses a strong ability to escape poor local minima. Our algorithm is built upon the class of traditional graduated optimization techniques, which are considered state-of-the-art local methods to solve problems having many poor minima. The novelty of our work lies in the introduction of an adaptive kernel (or residual) scaling scheme, which allows us to achieve faster convergence rates. Like other existing methods that aim to return good local minima for robust estimation tasks, our method relaxes the original robust problem but adapts a filter framework from non-linear constrained optimization to automatically choose the level of relaxation. Experimental results on real large-scale datasets such as bundle adjustment instances demonstrate that our proposed method achieves competitive results.



### Small-Object Detection in Remote Sensing Images with End-to-End Edge-Enhanced GAN and Object Detector Network
- **Arxiv ID**: http://arxiv.org/abs/2003.09085v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.09085v5)
- **Published**: 2020-03-20 03:07:30+00:00
- **Updated**: 2020-04-28 20:11:11+00:00
- **Authors**: Jakaria Rabbi, Nilanjan Ray, Matthias Schubert, Subir Chowdhury, Dennis Chao
- **Comment**: This paper contains 27 pages and accepted for publication in MDPI
  remote sensing journal. GitHub Repository:
  https://github.com/Jakaria08/EESRGAN (Implementation)
- **Journal**: None
- **Summary**: The detection performance of small objects in remote sensing images is not satisfactory compared to large objects, especially in low-resolution and noisy images. A generative adversarial network (GAN)-based model called enhanced super-resolution GAN (ESRGAN) shows remarkable image enhancement performance, but reconstructed images miss high-frequency edge information. Therefore, object detection performance degrades for small objects on recovered noisy and low-resolution remote sensing images. Inspired by the success of edge enhanced GAN (EEGAN) and ESRGAN, we apply a new edge-enhanced super-resolution GAN (EESRGAN) to improve the image quality of remote sensing images and use different detector networks in an end-to-end manner where detector loss is backpropagated into the EESRGAN to improve the detection performance. We propose an architecture with three components: ESRGAN, Edge Enhancement Network (EEN), and Detection network. We use residual-in-residual dense blocks (RRDB) for both the ESRGAN and EEN, and for the detector network, we use the faster region-based convolutional network (FRCNN) (two-stage detector) and single-shot multi-box detector (SSD) (one stage detector). Extensive experiments on a public (car overhead with context) and a self-assembled (oil and gas storage tank) satellite dataset show superior performance of our method compared to the standalone state-of-the-art object detectors.



### Fully Automated Hand Hygiene Monitoring\\in Operating Room using 3D Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2003.09087v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.09087v1)
- **Published**: 2020-03-20 03:18:15+00:00
- **Updated**: 2020-03-20 03:18:15+00:00
- **Authors**: Minjee Kim, Joonmyeong Choi, Namkug Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Hand hygiene is one of the most significant factors in preventing hospital acquired infections (HAI) which often be transmitted by medical staffs in contact with patients in the operating room (OR). Hand hygiene monitoring could be important to investigate and reduce the outbreak of infections within the OR. However, an effective monitoring tool for hand hygiene compliance is difficult to develop due to the visual complexity of the OR scene. Recent progress in video understanding with convolutional neural net (CNN) has increased the application of recognition and detection of human actions. Leveraging this progress, we proposed a fully automated hand hygiene monitoring tool of the alcohol-based hand rubbing action of anesthesiologists on OR video using spatio-temporal features with 3D CNN. First, the region of interest (ROI) of anesthesiologists' upper body were detected and cropped. A temporal smoothing filter was applied to the ROIs. Then, the ROIs were given to a 3D CNN and classified into two classes: rubbing hands or other actions. We observed that a transfer learning from Kinetics-400 is beneficial and the optical flow stream was not helpful in our dataset. The final accuracy, precision, recall and F1 score in testing is 0.76, 0.85, 0.65 and 0.74, respectively.



### Data-Free Knowledge Amalgamation via Group-Stack Dual-GAN
- **Arxiv ID**: http://arxiv.org/abs/2003.09088v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.09088v1)
- **Published**: 2020-03-20 03:20:52+00:00
- **Updated**: 2020-03-20 03:20:52+00:00
- **Authors**: Jingwen Ye, Yixin Ji, Xinchao Wang, Xin Gao, Mingli Song
- **Comment**: Accepted by CVPR 2020
- **Journal**: None
- **Summary**: Recent advances in deep learning have provided procedures for learning one network to amalgamate multiple streams of knowledge from the pre-trained Convolutional Neural Network (CNN) models, thus reduce the annotation cost. However, almost all existing methods demand massive training data, which may be unavailable due to privacy or transmission issues. In this paper, we propose a data-free knowledge amalgamate strategy to craft a well-behaved multi-task student network from multiple single/multi-task teachers. The main idea is to construct the group-stack generative adversarial networks (GANs) which have two dual generators. First one generator is trained to collect the knowledge by reconstructing the images approximating the original dataset utilized for pre-training the teachers. Then a dual generator is trained by taking the output from the former generator as input. Finally we treat the dual part generator as the target network and regroup it. As demonstrated on several benchmarks of multi-label classification, the proposed method without any training data achieves the surprisingly competitive results, even compared with some full-supervised methods.



### Hierarchical Severity Staging of Anterior Cruciate Ligament Injuries using Deep Learning with MRI Images
- **Arxiv ID**: http://arxiv.org/abs/2003.09089v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.09089v2)
- **Published**: 2020-03-20 03:21:40+00:00
- **Updated**: 2020-04-13 19:40:52+00:00
- **Authors**: Nikan K. Namiri, Io Flament, Bruno Astuto, Rutwik Shah, Radhika Tibrewala, Francesco Caliva, Thomas M. Link, Valentina Pedoia, Sharmila Majumdar
- **Comment**: None
- **Journal**: None
- **Summary**: Purpose: To evaluate the diagnostic utility of two convolutional neural networks (CNNs) for severity staging of anterior cruciate ligament (ACL) injuries.   Materials and Methods: This retrospective analysis was conducted on 1243 knee MR images (1008 intact, 18 partially torn, 77 fully torn, and 140 reconstructed ACLs) from 224 patients (age 47 +/- 14 years, 54% women) acquired between 2011 and 2014. The radiologists used a modified scoring metric. To classify ACL injuries with deep learning, two types of CNNs were used, one with three-dimensional (3D) and the other with two-dimensional (2D) convolutional kernels. Performance metrics included sensitivity, specificity, weighted Cohen's kappa, and overall accuracy, followed by McNemar's test to compare the CNNs performance.   Results: The overall accuracy and weighted Cohen's kappa reported for ACL injury classification were higher using the 2D CNN (accuracy: 92% (233/254) and kappa: 0.83) than the 3D CNN (accuracy: 89% (225/254) and kappa: 0.83) (P = .27). The 2D CNN and 3D CNN performed similarly in classifying intact ACLs (2D CNN: 93% (188/203) sensitivity and 90% (46/51) specificity; 3D CNN: 89% (180/203) sensitivity and 88% (45/51) specificity). Classification of full tears by both networks were also comparable (2D CNN: 82% (14/17) sensitivity and 94% (222/237) specificity; 3D CNN: 76% (13/17) sensitivity and 100% (236/237) specificity). The 2D CNN classified all reconstructed ACLs correctly.   Conclusion: 2D and 3D CNNs applied to ACL lesion classification had high sensitivity and specificity, suggesting that these networks could be used to help grade ACL injuries by non-experts.



### Masked Face Recognition Dataset and Application
- **Arxiv ID**: http://arxiv.org/abs/2003.09093v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.09093v2)
- **Published**: 2020-03-20 04:15:19+00:00
- **Updated**: 2020-03-23 07:58:57+00:00
- **Authors**: Zhongyuan Wang, Guangcheng Wang, Baojin Huang, Zhangyang Xiong, Qi Hong, Hao Wu, Peng Yi, Kui Jiang, Nanxi Wang, Yingjiao Pei, Heling Chen, Yu Miao, Zhibing Huang, Jinbi Liang
- **Comment**: None
- **Journal**: None
- **Summary**: In order to effectively prevent the spread of COVID-19 virus, almost everyone wears a mask during coronavirus epidemic. This almost makes conventional facial recognition technology ineffective in many cases, such as community access control, face access control, facial attendance, facial security checks at train stations, etc. Therefore, it is very urgent to improve the recognition performance of the existing face recognition technology on the masked faces. Most current advanced face recognition approaches are designed based on deep learning, which depend on a large number of face samples. However, at present, there are no publicly available masked face recognition datasets. To this end, this work proposes three types of masked face datasets, including Masked Face Detection Dataset (MFDD), Real-world Masked Face Recognition Dataset (RMFRD) and Simulated Masked Face Recognition Dataset (SMFRD). Among them, to the best of our knowledge, RMFRD is currently theworld's largest real-world masked face dataset. These datasets are freely available to industry and academia, based on which various applications on masked faces can be developed. The multi-granularity masked face recognition model we developed achieves 95% accuracy, exceeding the results reported by the industry. Our datasets are available at: https://github.com/X-zhangyang/Real-World-Masked-Face-Dataset.



### FocalMix: Semi-Supervised Learning for 3D Medical Image Detection
- **Arxiv ID**: http://arxiv.org/abs/2003.09108v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.09108v1)
- **Published**: 2020-03-20 05:12:31+00:00
- **Updated**: 2020-03-20 05:12:31+00:00
- **Authors**: Dong Wang, Yuan Zhang, Kexin Zhang, Liwei Wang
- **Comment**: Accepted by CVPR 2020
- **Journal**: None
- **Summary**: Applying artificial intelligence techniques in medical imaging is one of the most promising areas in medicine. However, most of the recent success in this area highly relies on large amounts of carefully annotated data, whereas annotating medical images is a costly process. In this paper, we propose a novel method, called FocalMix, which, to the best of our knowledge, is the first to leverage recent advances in semi-supervised learning (SSL) for 3D medical image detection. We conducted extensive experiments on two widely used datasets for lung nodule detection, LUNA16 and NLST. Results show that our proposed SSL methods can achieve a substantial improvement of up to 17.3% over state-of-the-art supervised learning approaches with 400 unlabeled CT scans.



### Online Continual Learning on Sequences
- **Arxiv ID**: http://arxiv.org/abs/2003.09114v1
- **DOI**: 10.1007/978-3-030-43883-8_8
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2003.09114v1)
- **Published**: 2020-03-20 05:49:31+00:00
- **Updated**: 2020-03-20 05:49:31+00:00
- **Authors**: German I. Parisi, Vincenzo Lomonaco
- **Comment**: L. Oneto et al. (eds.), Recent Trends in Learning From Data, Studies
  in Computational Intelligence 896
- **Journal**: None
- **Summary**: Online continual learning (OCL) refers to the ability of a system to learn over time from a continuous stream of data without having to revisit previously encountered training samples. Learning continually in a single data pass is crucial for agents and robots operating in changing environments and required to acquire, fine-tune, and transfer increasingly complex representations from non-i.i.d. input distributions. Machine learning models that address OCL must alleviate \textit{catastrophic forgetting} in which hidden representations are disrupted or completely overwritten when learning from streams of novel input. In this chapter, we summarize and discuss recent deep learning models that address OCL on sequential input through the use (and combination) of synaptic regularization, structural plasticity, and experience replay. Different implementations of replay have been proposed that alleviate catastrophic forgetting in connectionists architectures via the re-occurrence of (latent representations of) input sequences and that functionally resemble mechanisms of hippocampal replay in the mammalian brain. Empirical evidence shows that architectures endowed with experience replay typically outperform architectures without in (online) incremental learning tasks.



### Dual-discriminator GAN: A GAN way of profile face recognition
- **Arxiv ID**: http://arxiv.org/abs/2003.09116v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.09116v1)
- **Published**: 2020-03-20 06:01:58+00:00
- **Updated**: 2020-03-20 06:01:58+00:00
- **Authors**: Xinyu Zhang, Yang Zhao, Hao Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: A wealth of angle problems occur when facial recognition is performed: At present, the feature extraction network presents eigenvectors with large differences between the frontal face and profile face recognition of the same person in many cases. For this reason, the state-of-the-art facial recognition network will use multiple samples for the same target to ensure that eigenvector differences caused by angles are ignored during training. However, there is another solution available, which is to generate frontal face images with profile face images before recognition. In this paper, we proposed a method of generating frontal faces with image-to-image profile faces based on Generative Adversarial Network (GAN).



### CentripetalNet: Pursuing High-quality Keypoint Pairs for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2003.09119v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.09119v1)
- **Published**: 2020-03-20 06:23:32+00:00
- **Updated**: 2020-03-20 06:23:32+00:00
- **Authors**: Zhiwei Dong, Guoxuan Li, Yue Liao, Fei Wang, Pengju Ren, Chen Qian
- **Comment**: Accepted by CVPR2020
- **Journal**: None
- **Summary**: Keypoint-based detectors have achieved pretty-well performance. However, incorrect keypoint matching is still widespread and greatly affects the performance of the detector. In this paper, we propose CentripetalNet which uses centripetal shift to pair corner keypoints from the same instance. CentripetalNet predicts the position and the centripetal shift of the corner points and matches corners whose shifted results are aligned. Combining position information, our approach matches corner points more accurately than the conventional embedding approaches do. Corner pooling extracts information inside the bounding boxes onto the border. To make this information more aware at the corners, we design a cross-star deformable convolution network to conduct feature adaption. Furthermore, we explore instance segmentation on anchor-free detectors by equipping our CentripetalNet with a mask prediction module. On MS-COCO test-dev, our CentripetalNet not only outperforms all existing anchor-free detectors with an AP of 48.0% but also achieves comparable performance to the state-of-the-art instance segmentation approaches with a 40.2% MaskAP. Code will be available at https://github.com/KiveeDong/CentripetalNet.



### Learning the Loss Functions in a Discriminative Space for Video Restoration
- **Arxiv ID**: http://arxiv.org/abs/2003.09124v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.09124v1)
- **Published**: 2020-03-20 06:58:27+00:00
- **Updated**: 2020-03-20 06:58:27+00:00
- **Authors**: Younghyun Jo, Jaeyeon Kang, Seoung Wug Oh, Seonghyeon Nam, Peter Vajda, Seon Joo Kim
- **Comment**: 24 pages
- **Journal**: None
- **Summary**: With more advanced deep network architectures and learning schemes such as GANs, the performance of video restoration algorithms has greatly improved recently. Meanwhile, the loss functions for optimizing deep neural networks remain relatively unchanged. To this end, we propose a new framework for building effective loss functions by learning a discriminative space specific to a video restoration task. Our framework is similar to GANs in that we iteratively train two networks - a generator and a loss network. The generator learns to restore videos in a supervised fashion, by following ground truth features through the feature matching in the discriminative space learned by the loss network. In addition, we also introduce a new relation loss in order to maintain the temporal consistency in output videos. Experiments on video superresolution and deblurring show that our method generates visually more pleasing videos with better quantitative perceptual metric values than the other state-of-the-art methods.



### Efficient computation of backprojection arrays for 3D light field deconvolution
- **Arxiv ID**: http://arxiv.org/abs/2003.09133v2
- **DOI**: 10.1364/OE.431174
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.09133v2)
- **Published**: 2020-03-20 07:55:45+00:00
- **Updated**: 2021-05-10 08:15:32+00:00
- **Authors**: Martin Eberhart
- **Comment**: 15 pages, 11 figures, 1 table. This is a thoroughly reworked version
  of the manuscript, with a clearer structure. It avoids any ambigiuties
  envoked by using the term 'transpose' in the context of multi-dimensional
  arrays
- **Journal**: Optics Express 29(15) 24129-24143 (2021)
- **Summary**: Light field deconvolution allows three-dimensional investigations from a single snapshot recording of a plenoptic camera. It is based on a linear image formation model, and iterative volume reconstruction requires to define the backprojection of individual image pixels into object space. This is effectively a reversal of the point spread function (PSF), and backprojection arrays H' can be derived from the shift-variant PSFs H of the optical system, which is a very time consuming step for high resolution cameras. This paper illustrates the common structure of backprojection arrays and the significance of their efficient computation. A new algorithm is presented to determine H' from H, which is based on the distinct relation of the elements' positions within the two multi-dimensional arrays. It permits a pure array re-arrangement, and while results are identical to those from published codes, computation times are drastically reduced. This is shown by benchmarking the new method using various sample PSF arrays against existing algorithms. The paper is complemented by practical hints for the experimental acquisition of light field PSFs in a photographic setup.



### Multi-Person Pose Estimation with Enhanced Feature Aggregation and Selection
- **Arxiv ID**: http://arxiv.org/abs/2003.10238v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.10238v1)
- **Published**: 2020-03-20 08:33:25+00:00
- **Updated**: 2020-03-20 08:33:25+00:00
- **Authors**: Xixia Xu, Qi Zou, Xue Lin
- **Comment**: arXiv admin note: text overlap with arXiv:1905.03466 by other authors
- **Journal**: None
- **Summary**: We propose a novel Enhanced Feature Aggregation and Selection network (EFASNet) for multi-person 2D human pose estimation. Due to enhanced feature representation, our method can well handle crowded, cluttered and occluded scenes. More specifically, a Feature Aggregation and Selection Module (FASM), which constructs hierarchical multi-scale feature aggregation and makes the aggregated features discriminative, is proposed to get more accurate fine-grained representation, leading to more precise joint locations. Then, we perform a simple Feature Fusion (FF) strategy which effectively fuses high-resolution spatial features and low-resolution semantic features to obtain more reliable context information for well-estimated joints. Finally, we build a Dense Upsampling Convolution (DUC) module to generate more precise prediction, which can recover missing joint details that are usually unavailable in common upsampling process. As a result, the predicted keypoint heatmaps are more accurate. Comprehensive experiments demonstrate that the proposed approach outperforms the state-of-the-art methods and achieves the superior performance over three benchmark datasets: the recent big dataset CrowdPose, the COCO keypoint detection dataset and the MPII Human Pose dataset. Our code will be released upon acceptance.



### Event-based Asynchronous Sparse Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/2003.09148v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2003.09148v2)
- **Published**: 2020-03-20 08:39:49+00:00
- **Updated**: 2020-07-17 15:52:12+00:00
- **Authors**: Nico Messikommer, Daniel Gehrig, Antonio Loquercio, Davide Scaramuzza
- **Comment**: None
- **Journal**: European Conference on Computer Vision (ECCV), 2020
- **Summary**: Event cameras are bio-inspired sensors that respond to per-pixel brightness changes in the form of asynchronous and sparse "events". Recently, pattern recognition algorithms, such as learning-based methods, have made significant progress with event cameras by converting events into synchronous dense, image-like representations and applying traditional machine learning methods developed for standard cameras. However, these approaches discard the spatial and temporal sparsity inherent in event data at the cost of higher computational complexity and latency. In this work, we present a general framework for converting models trained on synchronous image-like event representations into asynchronous models with identical output, thus directly leveraging the intrinsic asynchronous and sparse nature of the event data. We show both theoretically and experimentally that this drastically reduces the computational complexity and latency of high-capacity, synchronous neural networks without sacrificing accuracy. In addition, our framework has several desirable characteristics: (i) it exploits spatio-temporal sparsity of events explicitly, (ii) it is agnostic to the event representation, network architecture, and task, and (iii) it does not require any train-time change, since it is compatible with the standard neural networks' training process. We thoroughly validate the proposed framework on two computer vision tasks: object detection and object recognition. In these tasks, we reduce the computational complexity up to 20 times with respect to high-latency neural networks. At the same time, we outperform state-of-the-art asynchronous approaches up to 24% in prediction accuracy.



### Unsupervised Latent Space Translation Network
- **Arxiv ID**: http://arxiv.org/abs/2003.09149v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.09149v1)
- **Published**: 2020-03-20 08:41:37+00:00
- **Updated**: 2020-03-20 08:41:37+00:00
- **Authors**: Magda Friedjungová, Daniel Vašata, Tomáš Chobola, Marcel Jiřina
- **Comment**: To be published in conference proceedings of ESANN 2020
- **Journal**: None
- **Summary**: One task that is often discussed in a computer vision is the mapping of an image from one domain to a corresponding image in another domain known as image-to-image translation. Currently there are several approaches solving this task. In this paper, we present an enhancement of the UNIT framework that aids in removing its main drawbacks. More specifically, we introduce an additional adversarial discriminator on the latent representation used instead of VAE, which enforces the latent space distributions of both domains to be similar. On MNIST and USPS domain adaptation tasks, this approach greatly outperforms competing approaches.



### Multi-branch and Multi-scale Attention Learning for Fine-Grained Visual Categorization
- **Arxiv ID**: http://arxiv.org/abs/2003.09150v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.09150v3)
- **Published**: 2020-03-20 08:43:28+00:00
- **Updated**: 2020-07-21 14:15:33+00:00
- **Authors**: Fan Zhang, Meng Li, Guisheng Zhai, Yizhao Liu
- **Comment**: None
- **Journal**: None
- **Summary**: ImageNet Large Scale Visual Recognition Challenge (ILSVRC) is one of the most authoritative academic competitions in the field of Computer Vision (CV) in recent years. But applying ILSVRC's annual champion directly to fine-grained visual categorization (FGVC) tasks does not achieve good performance. To FGVC tasks, the small inter-class variations and the large intra-class variations make it a challenging problem. Our attention object location module (AOLM) can predict the position of the object and attention part proposal module (APPM) can propose informative part regions without the need of bounding-box or part annotations. The obtained object images not only contain almost the entire structure of the object, but also contains more details, part images have many different scales and more fine-grained features, and the raw images contain the complete object. The three kinds of training images are supervised by our multi-branch network. Therefore, our multi-branch and multi-scale learning network(MMAL-Net) has good classification ability and robustness for images of different scales. Our approach can be trained end-to-end, while provides short inference time. Through the comprehensive experiments demonstrate that our approach can achieves state-of-the-art results on CUB-200-2011, FGVC-Aircraft and Stanford Cars datasets. Our code will be available at https://github.com/ZF1044404254/MMAL-Net



### Few-Shot Learning with Geometric Constraints
- **Arxiv ID**: http://arxiv.org/abs/2003.09151v1
- **DOI**: 10.1109/TNNLS.2019.2957187
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.09151v1)
- **Published**: 2020-03-20 08:50:32+00:00
- **Updated**: 2020-03-20 08:50:32+00:00
- **Authors**: Hong-Gyu Jung, Seong-Whan Lee
- **Comment**: Accepted for publication in IEEE Transactions on Neural Networks and
  Learning Systems (T-NNLS)
- **Journal**: None
- **Summary**: In this article, we consider the problem of few-shot learning for classification. We assume a network trained for base categories with a large number of training examples, and we aim to add novel categories to it that have only a few, e.g., one or five, training examples. This is a challenging scenario because: 1) high performance is required in both the base and novel categories; and 2) training the network for the new categories with a few training examples can contaminate the feature space trained well for the base categories. To address these challenges, we propose two geometric constraints to fine-tune the network with a few training examples. The first constraint enables features of the novel categories to cluster near the category weights, and the second maintains the weights of the novel categories far from the weights of the base categories. By applying the proposed constraints, we extract discriminative features for the novel categories while preserving the feature space learned for the base categories. Using public data sets for few-shot learning that are subsets of ImageNet, we demonstrate that the proposed method outperforms prevalent methods by a large margin.



### Exploring Categorical Regularization for Domain Adaptive Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2003.09152v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.09152v1)
- **Published**: 2020-03-20 08:53:10+00:00
- **Updated**: 2020-03-20 08:53:10+00:00
- **Authors**: Chang-Dong Xu, Xing-Ran Zhao, Xin Jin, Xiu-Shen Wei
- **Comment**: To appear in CVPR 2020. X.-S. Wei is the corresponding author
- **Journal**: None
- **Summary**: In this paper, we tackle the domain adaptive object detection problem, where the main challenge lies in significant domain gaps between source and target domains. Previous work seeks to plainly align image-level and instance-level shifts to eventually minimize the domain discrepancy. However, they still overlook to match crucial image regions and important instances across domains, which will strongly affect domain shift mitigation. In this work, we propose a simple but effective categorical regularization framework for alleviating this issue. It can be applied as a plug-and-play component on a series of Domain Adaptive Faster R-CNN methods which are prominent for dealing with domain adaptive detection. Specifically, by integrating an image-level multi-label classifier upon the detection backbone, we can obtain the sparse but crucial image regions corresponding to categorical information, thanks to the weakly localization ability of the classification manner. Meanwhile, at the instance level, we leverage the categorical consistency between image-level predictions (by the classifier) and instance-level predictions (by the detection head) as a regularization factor to automatically hunt for the hard aligned instances of target domains. Extensive experiments of various domain shift scenarios show that our method obtains a significant performance gain over original Domain Adaptive Faster R-CNN detectors. Furthermore, qualitative visualization and analyses can demonstrate the ability of our method for attending on the key regions/instances targeting on domain adaptation. Our code is open-source and available at \url{https://github.com/Megvii-Nanjing/CR-DA-DET}.



### Detection in Crowded Scenes: One Proposal, Multiple Predictions
- **Arxiv ID**: http://arxiv.org/abs/2003.09163v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.09163v2)
- **Published**: 2020-03-20 09:48:53+00:00
- **Updated**: 2020-06-24 14:59:34+00:00
- **Authors**: Xuangeng Chu, Anlin Zheng, Xiangyu Zhang, Jian Sun
- **Comment**: CVPR 2020 Oral
- **Journal**: None
- **Summary**: We propose a simple yet effective proposal-based object detector, aiming at detecting highly-overlapped instances in crowded scenes. The key of our approach is to let each proposal predict a set of correlated instances rather than a single one in previous proposal-based frameworks. Equipped with new techniques such as EMD Loss and Set NMS, our detector can effectively handle the difficulty of detecting highly overlapped objects. On a FPN-Res50 baseline, our detector can obtain 4.9\% AP gains on challenging CrowdHuman dataset and 1.0\% $\text{MR}^{-2}$ improvements on CityPersons dataset, without bells and whistles. Moreover, on less crowed datasets like COCO, our approach can still achieve moderate improvement, suggesting the proposed method is robust to crowdedness. Code and pre-trained models will be released at https://github.com/megvii-model/CrowdDetection.



### Fine-grained Species Recognition with Privileged Pooling: Better Sample Efficiency Through Supervised Attention
- **Arxiv ID**: http://arxiv.org/abs/2003.09168v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.09168v4)
- **Published**: 2020-03-20 10:03:01+00:00
- **Updated**: 2023-08-04 14:53:42+00:00
- **Authors**: Andres C. Rodriguez, Stefano D'Aronco, Konrad Schindler, Jan Dirk Wegner
- **Comment**: Updated version with iNaturalist2018 dataset. privileged pooling,
  supervised attention, training set bias, fine-grained species recognition,
  camera trap images
- **Journal**: None
- **Summary**: We propose a scheme for supervised image classification that uses privileged information, in the form of keypoint annotations for the training data, to learn strong models from small and/or biased training sets. Our main motivation is the recognition of animal species for ecological applications such as biodiversity modelling, which is challenging because of long-tailed species distributions due to rare species, and strong dataset biases such as repetitive scene background in camera traps. To counteract these challenges, we propose a visual attention mechanism that is supervised via keypoint annotations that highlight important object parts. This privileged information, implemented as a novel privileged pooling operation, is only required during training and helps the model to focus on regions that are discriminative. In experiments with three different animal species datasets, we show that deep networks with privileged pooling can use small training sets more efficiently and generalize better.



### DMV: Visual Object Tracking via Part-level Dense Memory and Voting-based Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2003.09171v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.09171v1)
- **Published**: 2020-03-20 10:05:30+00:00
- **Updated**: 2020-03-20 10:05:30+00:00
- **Authors**: Gunhee Nam, Seoung Wug Oh, Joon-Young Lee, Seon Joo Kim
- **Comment**: 19 pages, 9 figures
- **Journal**: None
- **Summary**: We propose a novel memory-based tracker via part-level dense memory and voting-based retrieval, called DMV. Since deep learning techniques have been introduced to the tracking field, Siamese trackers have attracted many researchers due to the balance between speed and accuracy. However, most of them are based on a single template matching, which limits the performance as it restricts the accessible in-formation to the initial target features. In this paper, we relieve this limitation by maintaining an external memory that saves the tracking record. Part-level retrieval from the memory also liberates the information from the template and allows our tracker to better handle the challenges such as appearance changes and occlusions. By updating the memory during tracking, the representative power for the target object can be enhanced without online learning. We also propose a novel voting mechanism for the memory reading to filter out unreliable information in the memory. We comprehensively evaluate our tracker on OTB-100,TrackingNet, GOT-10k, LaSOT, and UAV123, which show that our method yields comparable results to the state-of-the-art methods.



### 3dDepthNet: Point Cloud Guided Depth Completion Network for Sparse Depth and Single Color Image
- **Arxiv ID**: http://arxiv.org/abs/2003.09175v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.09175v1)
- **Published**: 2020-03-20 10:19:32+00:00
- **Updated**: 2020-03-20 10:19:32+00:00
- **Authors**: Rui Xiang, Feng Zheng, Huapeng Su, Zhe Zhang
- **Comment**: 8 pages, 10 figures and 4 tables
- **Journal**: None
- **Summary**: In this paper, we propose an end-to-end deep learning network named 3dDepthNet, which produces an accurate dense depth image from a single pair of sparse LiDAR depth and color image for robotics and autonomous driving tasks. Based on the dimensional nature of depth images, our network offers a novel 3D-to-2D coarse-to-fine dual densification design that is both accurate and lightweight. Depth densification is first performed in 3D space via point cloud completion, followed by a specially designed encoder-decoder structure that utilizes the projected dense depth from 3D completion and the original RGB-D images to perform 2D image completion. Experiments on the KITTI dataset show our network achieves state-of-art accuracy while being more efficient. Ablation and generalization tests prove that each module in our network has positive influences on the final results, and furthermore, our network is resilient to even sparser depth.



### Superaccurate Camera Calibration via Inverse Rendering
- **Arxiv ID**: http://arxiv.org/abs/2003.09177v1
- **DOI**: 10.1117/12.2531769
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.09177v1)
- **Published**: 2020-03-20 10:26:16+00:00
- **Updated**: 2020-03-20 10:26:16+00:00
- **Authors**: Morten Hannemose, Jakob Wilm, Jeppe Revall Frisvad
- **Comment**: 10 pages, 6 figures
- **Journal**: None
- **Summary**: The most prevalent routine for camera calibration is based on the detection of well-defined feature points on a purpose-made calibration artifact. These could be checkerboard saddle points, circles, rings or triangles, often printed on a planar structure. The feature points are first detected and then used in a nonlinear optimization to estimate the internal camera parameters.We propose a new method for camera calibration using the principle of inverse rendering. Instead of relying solely on detected feature points, we use an estimate of the internal parameters and the pose of the calibration object to implicitly render a non-photorealistic equivalent of the optical features. This enables us to compute pixel-wise differences in the image domain without interpolation artifacts. We can then improve our estimate of the internal parameters by minimizing pixel-wise least-squares differences. In this way, our model optimizes a meaningful metric in the image space assuming normally distributed noise characteristic for camera sensors.We demonstrate using synthetic and real camera images that our method improves the accuracy of estimated camera parameters as compared with current state-of-the-art calibration routines. Our method also estimates these parameters more robustly in the presence of noise and in situations where the number of calibration images is limited.



### Across-scale Process Similarity based Interpolation for Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2003.09182v1
- **DOI**: 10.1016/j.asoc.2019.105508
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.09182v1)
- **Published**: 2020-03-20 10:39:46+00:00
- **Updated**: 2020-03-20 10:39:46+00:00
- **Authors**: Sobhan Kanti Dhara, Debashis Sen
- **Comment**: None
- **Journal**: Applied Soft Computing, Volume 81, August 2019, 105508
- **Summary**: A pivotal step in image super-resolution techniques is interpolation, which aims at generating high resolution images without introducing artifacts such as blurring and ringing. In this paper, we propose a technique that performs interpolation through an infusion of high frequency signal components computed by exploiting `process similarity'. By `process similarity', we refer to the resemblance between a decomposition of the image at a resolution to the decomposition of the image at another resolution. In our approach, the decompositions generating image details and approximations are obtained through the discrete wavelet (DWT) and stationary wavelet (SWT) transforms. The complementary nature of DWT and SWT is leveraged to get the structural relation between the input image and its low resolution approximation. The structural relation is represented by optimal model parameters obtained through particle swarm optimization (PSO). Owing to process similarity, these parameters are used to generate the high resolution output image from the input image. The proposed approach is compared with six existing techniques qualitatively and in terms of PSNR, SSIM, and FSIM measures, along with computation time (CPU time). It is found that our approach is the fastest in terms of CPU time and produces comparable results.



### Diagnosis of Diabetic Retinopathy in Ethiopia: Before the Deep Learning based Automation
- **Arxiv ID**: http://arxiv.org/abs/2003.09208v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.09208v2)
- **Published**: 2020-03-20 11:41:28+00:00
- **Updated**: 2020-04-29 21:13:17+00:00
- **Authors**: Misgina Tsighe Hagos
- **Comment**: None
- **Journal**: None
- **Summary**: Introducing automated Diabetic Retinopathy (DR) diagnosis into Ethiopia is still a challenging task, despite recent reports that present trained Deep Learning (DL) based DR classifiers surpassing manual graders. This is mainly because of the expensive cost of conventional retinal imaging devices used in DL based classifiers. Current approaches that provide mobile based binary classification of DR, and the way towards a cheaper and offline multi-class classification of DR will be discussed in this paper.



### DIDFuse: Deep Image Decomposition for Infrared and Visible Image Fusion
- **Arxiv ID**: http://arxiv.org/abs/2003.09210v3
- **DOI**: 10.24963/ijcai.2020/135
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.09210v3)
- **Published**: 2020-03-20 11:45:20+00:00
- **Updated**: 2021-04-08 07:41:43+00:00
- **Authors**: Zixiang Zhao, Shuang Xu, Chunxia Zhang, Junmin Liu, Pengfei Li, Jiangshe Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Infrared and visible image fusion, a hot topic in the field of image processing, aims at obtaining fused images keeping the advantages of source images. This paper proposes a novel auto-encoder (AE) based fusion network. The core idea is that the encoder decomposes an image into background and detail feature maps with low- and high-frequency information, respectively, and that the decoder recovers the original image. To this end, the loss function makes the background/detail feature maps of source images similar/dissimilar. In the test phase, background and detail feature maps are respectively merged via a fusion module, and the fused image is recovered by the decoder. Qualitative and quantitative results illustrate that our method can generate fusion images containing highlighted targets and abundant detail texture information with strong robustness and meanwhile surpass state-of-the-art (SOTA) approaches.



### Coronavirus (COVID-19) Classification using CT Images by Machine Learning Methods
- **Arxiv ID**: http://arxiv.org/abs/2003.09424v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML, F.2.2; I.2.7
- **Links**: [PDF](http://arxiv.org/pdf/2003.09424v1)
- **Published**: 2020-03-20 12:48:39+00:00
- **Updated**: 2020-03-20 12:48:39+00:00
- **Authors**: Mucahid Barstugan, Umut Ozkaya, Saban Ozturk
- **Comment**: The paper has 10 pages
- **Journal**: None
- **Summary**: This study presents early phase detection of Coronavirus (COVID-19), which is named by World Health Organization (WHO), by machine learning methods. The detection process was implemented on abdominal Computed Tomography (CT) images. The expert radiologists detected from CT images that COVID-19 shows different behaviours from other viral pneumonia. Therefore, the clinical experts specify that COV\.ID-19 virus needs to be diagnosed in early phase. For detection of the COVID-19, four different datasets were formed by taking patches sized as 16x16, 32x32, 48x48, 64x64 from 150 CT images. The feature extraction process was applied to patches to increase the classification performance. Grey Level Co-occurrence Matrix (GLCM), Local Directional Pattern (LDP), Grey Level Run Length Matrix (GLRLM), Grey-Level Size Zone Matrix (GLSZM), and Discrete Wavelet Transform (DWT) algorithms were used as feature extraction methods. Support Vector Machines (SVM) classified the extracted features. 2-fold, 5-fold and 10-fold cross-validations were implemented during the classification process. Sensitivity, specificity, accuracy, precision, and F-score metrics were used to evaluate the classification performance. The best classification accuracy was obtained as 99.68% with 10-fold cross-validation and GLSZM feature extraction method.



### Weakly Supervised 3D Hand Pose Estimation via Biomechanical Constraints
- **Arxiv ID**: http://arxiv.org/abs/2003.09282v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.09282v2)
- **Published**: 2020-03-20 14:03:13+00:00
- **Updated**: 2020-08-04 15:55:41+00:00
- **Authors**: Adrian Spurr, Umar Iqbal, Pavlo Molchanov, Otmar Hilliges, Jan Kautz
- **Comment**: None
- **Journal**: None
- **Summary**: Estimating 3D hand pose from 2D images is a difficult, inverse problem due to the inherent scale and depth ambiguities. Current state-of-the-art methods train fully supervised deep neural networks with 3D ground-truth data. However, acquiring 3D annotations is expensive, typically requiring calibrated multi-view setups or labor intensive manual annotations. While annotations of 2D keypoints are much easier to obtain, how to efficiently leverage such weakly-supervised data to improve the task of 3D hand pose prediction remains an important open question. The key difficulty stems from the fact that direct application of additional 2D supervision mostly benefits the 2D proxy objective but does little to alleviate the depth and scale ambiguities. Embracing this challenge we propose a set of novel losses. We show by extensive experiments that our proposed constraints significantly reduce the depth ambiguity and allow the network to more effectively leverage additional 2D annotated images. For example, on the challenging freiHAND dataset using additional 2D annotation without our proposed biomechanical constraints reduces the depth error by only $15\%$, whereas the error is reduced significantly by $50\%$ when the proposed biomechanical constraints are used.



### U-Det: A Modified U-Net architecture with bidirectional feature network for lung nodule segmentation
- **Arxiv ID**: http://arxiv.org/abs/2003.09293v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.09293v1)
- **Published**: 2020-03-20 14:25:22+00:00
- **Updated**: 2020-03-20 14:25:22+00:00
- **Authors**: Nikhil Varma Keetha, Samson Anosh Babu P, Chandra Sekhara Rao Annavarapu
- **Comment**: 14 pages, 7 figures, 5 tables
- **Journal**: None
- **Summary**: Early diagnosis and analysis of lung cancer involve a precise and efficient lung nodule segmentation in computed tomography (CT) images. However, the anonymous shapes, visual features, and surroundings of the nodule in the CT image pose a challenging problem to the robust segmentation of the lung nodules. This article proposes U-Det, a resource-efficient model architecture, which is an end to end deep learning approach to solve the task at hand. It incorporates a Bi-FPN (bidirectional feature network) between the encoder and decoder. Furthermore, it uses Mish activation function and class weights of masks to enhance segmentation efficiency. The proposed model is extensively trained and evaluated on the publicly available LUNA-16 dataset consisting of 1186 lung nodules. The U-Det architecture outperforms the existing U-Net model with the Dice similarity coefficient (DSC) of 82.82% and achieves results comparable to human experts.



### Detection of Information Hiding at Anti-Copying 2D Barcodes
- **Arxiv ID**: http://arxiv.org/abs/2003.09316v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.09316v1)
- **Published**: 2020-03-20 15:06:50+00:00
- **Updated**: 2020-03-20 15:06:50+00:00
- **Authors**: Ning Xie, Ji Hu, Junjie Chen, Qiqi Zhang, Changsheng Chen
- **Comment**: None
- **Journal**: None
- **Summary**: This paper concerns the problem of detecting the use of information hiding at anti-copying 2D barcodes. Prior hidden information detection schemes are either heuristicbased or Machine Learning (ML) based. The key limitation of prior heuristics-based schemes is that they do not answer the fundamental question of why the information hidden at a 2D barcode can be detected. The key limitation of prior MLbased information schemes is that they lack robustness because a printed 2D barcode is very much environmentally dependent, and thus an information hiding detection scheme trained in one environment often does not work well in another environment. In this paper, we propose two hidden information detection schemes at the existing anti-copying 2D barcodes. The first scheme is to directly use the pixel distance to detect the use of an information hiding scheme in a 2D barcode, referred as to the Pixel Distance Based Detection (PDBD) scheme. The second scheme is first to calculate the variance of the raw signal and the covariance between the recovered signal and the raw signal, and then based on the variance results, detects the use of information hiding scheme in a 2D barcode, referred as to the Pixel Variance Based Detection (PVBD) scheme. Moreover, we design advanced IC attacks to evaluate the security of two existing anti-copying 2D barcodes. We implemented our schemes and conducted extensive performance comparison between our schemes and prior schemes under different capturing devices, such as a scanner and a camera phone. Our experimental results show that the PVBD scheme can correctly detect the existence of the hidden information at both the 2LQR code and the LCAC 2D barcode. Moreover, the probability of successfully attacking of our IC attacks achieves 0.6538 for the 2LQR code and 1 for the LCAC 2D barcode.



### Selecting Relevant Features from a Multi-domain Representation for Few-shot Classification
- **Arxiv ID**: http://arxiv.org/abs/2003.09338v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.09338v2)
- **Published**: 2020-03-20 15:44:17+00:00
- **Updated**: 2020-07-20 12:51:34+00:00
- **Authors**: Nikita Dvornik, Cordelia Schmid, Julien Mairal
- **Comment**: ECCV'20
- **Journal**: None
- **Summary**: Popular approaches for few-shot classification consist of first learning a generic data representation based on a large annotated dataset, before adapting the representation to new classes given only a few labeled samples. In this work, we propose a new strategy based on feature selection, which is both simpler and more effective than previous feature adaptation approaches. First, we obtain a multi-domain representation by training a set of semantically different feature extractors. Then, given a few-shot learning task, we use our multi-domain feature bank to automatically select the most relevant representations. We show that a simple non-parametric classifier built on top of such features produces high accuracy and generalizes to domains never seen during training, which leads to state-of-the-art results on MetaDataset and improved accuracy on mini-ImageNet.



### Visual Navigation Among Humans with Optimal Control as a Supervisor
- **Arxiv ID**: http://arxiv.org/abs/2003.09354v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2003.09354v2)
- **Published**: 2020-03-20 16:13:47+00:00
- **Updated**: 2021-02-12 21:09:24+00:00
- **Authors**: Varun Tolani, Somil Bansal, Aleksandra Faust, Claire Tomlin
- **Comment**: Project Website: https://smlbansal.github.io/LB-WayPtNav-DH/
- **Journal**: None
- **Summary**: Real world visual navigation requires robots to operate in unfamiliar, human-occupied dynamic environments. Navigation around humans is especially difficult because it requires anticipating their future motion, which can be quite challenging. We propose an approach that combines learning-based perception with model-based optimal control to navigate among humans based only on monocular, first-person RGB images. Our approach is enabled by our novel data-generation tool, HumANav that allows for photorealistic renderings of indoor environment scenes with humans in them, which are then used to train the perception module entirely in simulation. Through simulations and experiments on a mobile robot, we demonstrate that the learned navigation policies can anticipate and react to humans without explicitly predicting future human motion, generalize to previously unseen environments and human behaviors, and transfer directly from simulation to reality. Videos describing our approach and experiments, as well as a demo of HumANav are available on the project website.



### Out-of-Distribution Detection for Skin Lesion Images with Deep Isolation Forest
- **Arxiv ID**: http://arxiv.org/abs/2003.09365v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.09365v1)
- **Published**: 2020-03-20 16:39:50+00:00
- **Updated**: 2020-03-20 16:39:50+00:00
- **Authors**: Xuan Li, Yuchen Lu, Christian Desrosiers, Xue Liu
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we study the problem of out-of-distribution detection in skin disease images. Publicly available medical datasets normally have a limited number of lesion classes (e.g. HAM10000 has 8 lesion classes). However, there exists a few thousands of clinically identified diseases. Hence, it is important if lesions not in the training data can be differentiated. Toward this goal, we propose DeepIF, a non-parametric Isolation Forest based approach combined with deep convolutional networks. We conduct comprehensive experiments to compare our DeepIF with three baseline models. Results demonstrate state-of-the-art performance of our proposed approach on the task of detecting abnormal skin lesions.



### SER-FIQ: Unsupervised Estimation of Face Image Quality Based on Stochastic Embedding Robustness
- **Arxiv ID**: http://arxiv.org/abs/2003.09373v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.09373v1)
- **Published**: 2020-03-20 16:50:30+00:00
- **Updated**: 2020-03-20 16:50:30+00:00
- **Authors**: Philipp Terhörst, Jan Niklas Kolf, Naser Damer, Florian Kirchbuchner, Arjan Kuijper
- **Comment**: Accepted at CVPR2020
- **Journal**: None
- **Summary**: Face image quality is an important factor to enable high performance face recognition systems. Face quality assessment aims at estimating the suitability of a face image for recognition. Previous work proposed supervised solutions that require artificially or human labelled quality values. However, both labelling mechanisms are error-prone as they do not rely on a clear definition of quality and may not know the best characteristics for the utilized face recognition system. Avoiding the use of inaccurate quality labels, we proposed a novel concept to measure face quality based on an arbitrary face recognition model. By determining the embedding variations generated from random subnetworks of a face model, the robustness of a sample representation and thus, its quality is estimated. The experiments are conducted in a cross-database evaluation setting on three publicly available databases. We compare our proposed solution on two face embeddings against six state-of-the-art approaches from academia and industry. The results show that our unsupervised solution outperforms all other approaches in the majority of the investigated scenarios. In contrast to previous works, the proposed solution shows a stable performance over all scenarios. Utilizing the deployed face recognition model for our face quality assessment methodology avoids the training phase completely and further outperforms all baseline approaches by a large margin. Our solution can be easily integrated into current face recognition systems and can be modified to other tasks beyond face recognition.



### Domain Adaptation by Class Centroid Matching and Local Manifold Self-Learning
- **Arxiv ID**: http://arxiv.org/abs/2003.09391v4
- **DOI**: 10.1109/TIP.2020.3031220
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.09391v4)
- **Published**: 2020-03-20 16:59:27+00:00
- **Updated**: 2020-10-20 06:17:08+00:00
- **Authors**: Lei Tian, Yongqiang Tang, Liangchen Hu, Zhida Ren, Wensheng Zhang
- **Comment**: Accepted by IEEE Transactions on Image Processing, Code Available:
  https://github.com/LeiTian-qj/CMMS/tree/master
- **Journal**: None
- **Summary**: Domain adaptation has been a fundamental technology for transferring knowledge from a source domain to a target domain. The key issue of domain adaptation is how to reduce the distribution discrepancy between two domains in a proper way such that they can be treated indifferently for learning. In this paper, we propose a novel domain adaptation approach, which can thoroughly explore the data distribution structure of target domain.Specifically, we regard the samples within the same cluster in target domain as a whole rather than individuals and assigns pseudo-labels to the target cluster by class centroid matching. Besides, to exploit the manifold structure information of target data more thoroughly, we further introduce a local manifold self-learning strategy into our proposal to adaptively capture the inherent local connectivity of target samples. An efficient iterative optimization algorithm is designed to solve the objective function of our proposal with theoretical convergence guarantee. In addition to unsupervised domain adaptation, we further extend our method to the semi-supervised scenario including both homogeneous and heterogeneous settings in a direct but elegant way. Extensive experiments on seven benchmark datasets validate the significant superiority of our proposal in both unsupervised and semi-supervised manners.



### Comprehensive Instructional Video Analysis: The COIN Dataset and Performance Evaluation
- **Arxiv ID**: http://arxiv.org/abs/2003.09392v1
- **DOI**: 10.1109/TPAMI.2020.2980824
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.09392v1)
- **Published**: 2020-03-20 16:59:44+00:00
- **Updated**: 2020-03-20 16:59:44+00:00
- **Authors**: Yansong Tang, Jiwen Lu, Jie Zhou
- **Comment**: Accepted by T-PAMI, journal version of COIN dataset arXiv:1903.02874
- **Journal**: None
- **Summary**: Thanks to the substantial and explosively inscreased instructional videos on the Internet, novices are able to acquire knowledge for completing various tasks. Over the past decade, growing efforts have been devoted to investigating the problem on instructional video analysis. However, the most existing datasets in this area have limitations in diversity and scale, which makes them far from many real-world applications where more diverse activities occur. To address this, we present a large-scale dataset named as "COIN" for COmprehensive INstructional video analysis. Organized with a hierarchical structure, the COIN dataset contains 11,827 videos of 180 tasks in 12 domains (e.g., vehicles, gadgets, etc.) related to our daily life. With a new developed toolbox, all the videos are annotated efficiently with a series of step labels and the corresponding temporal boundaries. In order to provide a benchmark for instructional video analysis, we evaluate plenty of approaches on the COIN dataset under five different settings. Furthermore, we exploit two important characteristics (i.e., task-consistency and ordering-dependency) for localizing important steps in instructional videos. Accordingly, we propose two simple yet effective methods, which can be easily plugged into conventional proposal-based action detection models. We believe the introduction of the COIN dataset will promote the future in-depth research on instructional video analysis for the community. Our dataset, annotation toolbox and source code are available at http://coin-dataset.github.io.



### RGB-Topography and X-rays Image Registration for Idiopathic Scoliosis Children Patient Follow-up
- **Arxiv ID**: http://arxiv.org/abs/2003.09404v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.09404v1)
- **Published**: 2020-03-20 17:33:23+00:00
- **Updated**: 2020-03-20 17:33:23+00:00
- **Authors**: Insaf Setitra, Noureddine Aouaa, Abdelkrim Meziane, Afef Benrabia, Houria Kaced, Hanene Belabassi, Sara Ait Ziane, Nadia Henda Zenati, Oualid Djekkoune
- **Comment**: None
- **Journal**: None
- **Summary**: Children diagnosed with a scoliosis pathology are exposed during their follow up to ionic radiations in each X-rays diagnosis. This exposure can have negative effects on the patient's health and cause diseases in the adult age. In order to reduce X-rays scanning, recent systems provide diagnosis of scoliosis patients using solely RGB images. The output of such systems is a set of augmented images and scoliosis related angles. These angles, however, confuse the physicians due to their large number. Moreover, the lack of X-rays scans makes it impossible for the physician to compare RGB and X-rays images, and decide whether to reduce X-rays exposure or not. In this work, we exploit both RGB images of scoliosis captured during clinical diagnosis, and X-rays hard copies provided by patients in order to register both images and give a rich comparison of diagnoses. The work consists in, first, establishing the monomodal (RGB topography of the back) and multimodal (RGB and Xrays) image database, then registering images based on patient landmarks, and finally blending registered images for a visual analysis and follow up by the physician. The proposed registration is based on a rigid transformation that preserves the topology of the patient's back. Parameters of the rigid transformation are estimated using a proposed angle minimization of Cervical vertebra 7, and Posterior Superior Iliac Spine landmarks of a source and target diagnoses. Experiments conducted on the constructed database show a better monomodal and multimodal registration using our proposed method compared to registration using an Equation System Solving based registration.



### Explainable Object-induced Action Decision for Autonomous Vehicles
- **Arxiv ID**: http://arxiv.org/abs/2003.09405v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.09405v1)
- **Published**: 2020-03-20 17:33:44+00:00
- **Updated**: 2020-03-20 17:33:44+00:00
- **Authors**: Yiran Xu, Xiaoyin Yang, Lihang Gong, Hsuan-Chu Lin, Tz-Ying Wu, Yunsheng Li, Nuno Vasconcelos
- **Comment**: None
- **Journal**: None
- **Summary**: A new paradigm is proposed for autonomous driving. The new paradigm lies between the end-to-end and pipelined approaches, and is inspired by how humans solve the problem. While it relies on scene understanding, the latter only considers objects that could originate hazard. These are denoted as action-inducing, since changes in their state should trigger vehicle actions. They also define a set of explanations for these actions, which should be produced jointly with the latter. An extension of the BDD100K dataset, annotated for a set of 4 actions and 21 explanations, is proposed. A new multi-task formulation of the problem, which optimizes the accuracy of both action commands and explanations, is then introduced. A CNN architecture is finally proposed to solve this problem, by combining reasoning about action inducing objects and global scene context. Experimental results show that the requirement of explanations improves the recognition of action-inducing objects, which in turn leads to better action predictions.



### Investigating Image Applications Based on Spatial-Frequency Transform and Deep Learning Techniques
- **Arxiv ID**: http://arxiv.org/abs/2004.02756v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.02756v1)
- **Published**: 2020-03-20 17:34:47+00:00
- **Updated**: 2020-03-20 17:34:47+00:00
- **Authors**: Qinkai Zheng, Han Qiu, Gerard Memmi, Isabelle Bloch
- **Comment**: None
- **Journal**: None
- **Summary**: This is the report for the PRIM project in Telecom Paris. This report is about applications based on spatial-frequency transform and deep learning techniques. In this report, there are two main works. The first work is about the enhanced JPEG compression method based on deep learning. we propose a novel method to highly enhance the JPEG compression by transmitting fewer image data at the sender's end. At the receiver's end, we propose a DC recovery algorithm together with the deep residual learning framework to recover images with high quality. The second work is about adversarial examples defenses based on signal processing. We propose the wavelet extension method to extend image data features, which makes it more difficult to generate adversarial examples. We further adopt wavelet denoising to reduce the influence of the adversarial perturbations. With intensive experiments, we demonstrate that both works are effective in their application scenarios.



### ROAM: Random Layer Mixup for Semi-Supervised Learning in Medical Imaging
- **Arxiv ID**: http://arxiv.org/abs/2003.09439v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.09439v4)
- **Published**: 2020-03-20 18:07:12+00:00
- **Updated**: 2020-11-16 22:50:12+00:00
- **Authors**: Tariq Bdair, Benedikt Wiestler, Nassir Navab, Shadi Albarqouni
- **Comment**: None
- **Journal**: None
- **Summary**: Medical image segmentation is one of the major challenges addressed by machine learning methods. Yet, deep learning methods profoundly depend on a large amount of annotated data, which is time-consuming and costly. Though, semi-supervised learning methods approach this problem by leveraging an abundant amount of unlabeled data along with a small amount of labeled data in the training process. Recently, MixUp regularizer has been successfully introduced to semi-supervised learning methods showing superior performance. MixUp augments the model with new data points through linear interpolation of the data at the input space. We argue that this option is limited. Instead, we propose ROAM, a RandOm lAyer Mixup, which encourages the network to be less confident for interpolated data points at randomly selected space. ROAM generates more data points that have never seen before, and hence it avoids over-fitting and enhances the generalization ability. We conduct extensive experiments to validate our method on three publicly available datasets on whole-brain image segmentation. ROAM achieves state-of-the-art (SOTA) results in fully supervised (89.5%) and semi-supervised (87.0%) settings with a relative improvement of up to 2.40% and 16.50%, respectively for the whole-brain segmentation.



### Adversarial Robustness on In- and Out-Distribution Improves Explainability
- **Arxiv ID**: http://arxiv.org/abs/2003.09461v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.09461v2)
- **Published**: 2020-03-20 18:57:52+00:00
- **Updated**: 2020-07-29 17:36:00+00:00
- **Authors**: Maximilian Augustin, Alexander Meinke, Matthias Hein
- **Comment**: None
- **Journal**: None
- **Summary**: Neural networks have led to major improvements in image classification but suffer from being non-robust to adversarial changes, unreliable uncertainty estimates on out-distribution samples and their inscrutable black-box decisions. In this work we propose RATIO, a training procedure for Robustness via Adversarial Training on In- and Out-distribution, which leads to robust models with reliable and robust confidence estimates on the out-distribution. RATIO has similar generative properties to adversarial training so that visual counterfactuals produce class specific features. While adversarial training comes at the price of lower clean accuracy, RATIO achieves state-of-the-art $l_2$-adversarial robustness on CIFAR10 and maintains better clean accuracy.



### Do Public Datasets Assure Unbiased Comparisons for Registration Evaluation?
- **Arxiv ID**: http://arxiv.org/abs/2003.09483v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.09483v1)
- **Published**: 2020-03-20 20:04:47+00:00
- **Updated**: 2020-03-20 20:04:47+00:00
- **Authors**: Jie Luo, Guangshen Ma, Sarah Frisken, Parikshit Juvekar, Nazim Haouchine, Zhe Xu, Yiming Xiao, Alexandra Golby, Patrick Codd, Masashi Sugiyama, William Wells III
- **Comment**: Draft 1
- **Journal**: None
- **Summary**: With the increasing availability of new image registration approaches, an unbiased evaluation is becoming more needed so that clinicians can choose the most suitable approaches for their applications. Current evaluations typically use landmarks in manually annotated datasets. As a result, the quality of annotations is crucial for unbiased comparisons. Even though most data providers claim to have quality control over their datasets, an objective third-party screening can be reassuring for intended users. In this study, we use the variogram to screen the manually annotated landmarks in two datasets used to benchmark registration in image-guided neurosurgeries. The variogram provides an intuitive 2D representation of the spatial characteristics of annotated landmarks. Using variograms, we identified potentially problematic cases and had them examined by experienced radiologists. We found that (1) a small number of annotations may have fiducial localization errors; (2) the landmark distribution for some cases is not ideal to offer fair comparisons. If unresolved, both findings could incur bias in registration evaluation.



### A Robotic 3D Perception System for Operating Room Environment Awareness
- **Arxiv ID**: http://arxiv.org/abs/2003.09487v2
- **DOI**: None
- **Categories**: **cs.CV**, 65D19
- **Links**: [PDF](http://arxiv.org/pdf/2003.09487v2)
- **Published**: 2020-03-20 20:27:06+00:00
- **Updated**: 2020-03-30 17:20:29+00:00
- **Authors**: Zhaoshuo Li, Amirreza Shaban, Jean-Gabriel Simard, Dinesh Rabindran, Simon DiMaio, Omid Mohareri
- **Comment**: Accepted in IPCAI 2020
- **Journal**: None
- **Summary**: Purpose: We describe a 3D multi-view perception system for the da Vinci surgical system to enable Operating room (OR) scene understanding and context awareness.   Methods: Our proposed system is comprised of four Time-of-Flight (ToF) cameras rigidly attached to strategic locations on the daVinci Xi patient side cart (PSC). The cameras are registered to the robot's kinematic chain by performing a one-time calibration routine and therefore, information from all cameras can be fused and represented in one common coordinate frame. Based on this architecture, a multi-view 3D scene semantic segmentation algorithm is created to enable recognition of common and salient objects/equipment and surgical activities in a da Vinci OR. Our proposed 3D semantic segmentation method has been trained and validated on a novel densely annotated dataset that has been captured from clinical scenarios.   Results: The results show that our proposed architecture has acceptable registration error ($3.3\%\pm1.4\%$ of object-camera distance) and can robustly improve scene segmentation performance (mean Intersection Over Union - mIOU) for less frequently appearing classes ($\ge 0.013$) compared to a single-view method.   Conclusion: We present the first dynamic multi-view perception system with a novel segmentation architecture, which can be used as a building block technology for applications such as surgical workflow analysis, automation of surgical sub-tasks and advanced guidance systems.



### Bone Structures Extraction and Enhancement in Chest Radiographs via CNN Trained on Synthetic Data
- **Arxiv ID**: http://arxiv.org/abs/2003.10839v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.10839v1)
- **Published**: 2020-03-20 20:27:50+00:00
- **Updated**: 2020-03-20 20:27:50+00:00
- **Authors**: Ophir Gozes, Hayit Greenspan
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1810.05989
- **Journal**: None
- **Summary**: In this paper, we present a deep learning-based image processing technique for extraction of bone structures in chest radiographs using a U-Net FCNN. The U-Net was trained to accomplish the task in a fully supervised setting. To create the training image pairs, we employed simulated X-Ray or Digitally Reconstructed Radiographs (DRR), derived from 664 CT scans belonging to the LIDC-IDRI dataset. Using HU based segmentation of bone structures in the CT domain, a synthetic 2D "Bone x-ray" DRR is produced and used for training the network. For the reconstruction loss, we utilize two loss functions- L1 Loss and perceptual loss. Once the bone structures are extracted, the original image can be enhanced by fusing the original input x-ray and the synthesized "Bone X-ray". We show that our enhancement technique is applicable to real x-ray data, and display our results on the NIH Chest X-Ray-14 dataset.



### Detection and skeletonization of single neurons and tracer injections using topological methods
- **Arxiv ID**: http://arxiv.org/abs/2004.02755v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.02755v1)
- **Published**: 2020-03-20 20:58:38+00:00
- **Updated**: 2020-03-20 20:58:38+00:00
- **Authors**: Dingkang Wang, Lucas Magee, Bing-Xing Huo, Samik Banerjee, Xu Li, Jaikishan Jayakumar, Meng Kuan Lin, Keerthi Ram, Suyi Wang, Yusu Wang, Partha P. Mitra
- **Comment**: 20 pages (14 pages main-text and 6 pages supplementary information).
  5 main-text figures. 5 supplementary figures. 2 supplementary tables
- **Journal**: None
- **Summary**: Neuroscientific data analysis has traditionally relied on linear algebra and stochastic process theory. However, the tree-like shapes of neurons cannot be described easily as points in a vector space (the subtraction of two neuronal shapes is not a meaningful operation), and methods from computational topology are better suited to their analysis. Here we introduce methods from Discrete Morse (DM) Theory to extract the tree-skeletons of individual neurons from volumetric brain image data, and to summarize collections of neurons labelled by tracer injections. Since individual neurons are topologically trees, it is sensible to summarize the collection of neurons using a consensus tree-shape that provides a richer information summary than the traditional regional 'connectivity matrix' approach. The conceptually elegant DM approach lacks hand-tuned parameters and captures global properties of the data as opposed to previous approaches which are inherently local. For individual skeletonization of sparsely labelled neurons we obtain substantial performance gains over state-of-the-art non-topological methods (over 10% improvements in precision and faster proofreading). The consensus-tree summary of tracer injections incorporates the regional connectivity matrix information, but in addition captures the collective collateral branching patterns of the set of neurons connected to the injection site, and provides a bridge between single-neuron morphology and tracer-injection data.



### Fast Symmetric Diffeomorphic Image Registration with Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2003.09514v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.09514v3)
- **Published**: 2020-03-20 22:07:24+00:00
- **Updated**: 2021-02-28 08:43:47+00:00
- **Authors**: Tony C. W. Mok, Albert C. S. Chung
- **Comment**: Accepted to CVPR 2020
- **Journal**: None
- **Summary**: Diffeomorphic deformable image registration is crucial in many medical image studies, as it offers unique, special properties including topology preservation and invertibility of the transformation. Recent deep learning-based deformable image registration methods achieve fast image registration by leveraging a convolutional neural network (CNN) to learn the spatial transformation from the synthetic ground truth or the similarity metric. However, these approaches often ignore the topology preservation of the transformation and the smoothness of the transformation which is enforced by a global smoothing energy function alone. Moreover, deep learning-based approaches often estimate the displacement field directly, which cannot guarantee the existence of the inverse transformation. In this paper, we present a novel, efficient unsupervised symmetric image registration method which maximizes the similarity between images within the space of diffeomorphic maps and estimates both forward and inverse transformations simultaneously. We evaluate our method on 3D image registration with a large scale brain image dataset. Our method achieves state-of-the-art registration accuracy and running time while maintaining desirable diffeomorphic properties.



