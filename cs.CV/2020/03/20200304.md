# Arxiv Papers in cs.CV on 2020-03-04
### Region adaptive graph fourier transform for 3d point clouds
- **Arxiv ID**: http://arxiv.org/abs/2003.01866v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2003.01866v2)
- **Published**: 2020-03-04 02:47:44+00:00
- **Updated**: 2020-05-27 21:45:58+00:00
- **Authors**: Eduardo Pavez, Benjamin Girault, Antonio Ortega, Philip A. Chou
- **Comment**: 5 pages, 3 figures, accepted ICIP 2020
- **Journal**: None
- **Summary**: We introduce the Region Adaptive Graph Fourier Transform (RA-GFT) for compression of 3D point cloud attributes. The RA-GFT is a multiresolution transform, formed by combining spatially localized block transforms. We assume the points are organized by a family of nested partitions represented by a rooted tree. At each resolution level, attributes are processed in clusters using block transforms. Each block transform produces a single approximation (DC) coefficient, and various detail (AC) coefficients. The DC coefficients are promoted up the tree to the next (lower resolution) level, where the process can be repeated until reaching the root. Since clusters may have a different numbers of points, each block transform must incorporate the relative importance of each coefficient. For this, we introduce the $\mathbf{Q}$-normalized graph Laplacian, and propose using its eigenvectors as the block transform. The RA-GFT achieves better complexity-performance trade-offs than previous approaches. In particular, it outperforms the Region Adaptive Haar Transform (RAHT) by up to 2.5 dB, with a small complexity overhead.



### Semantic sensor fusion: from camera to sparse lidar information
- **Arxiv ID**: http://arxiv.org/abs/2003.01871v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, 00-02, I.4
- **Links**: [PDF](http://arxiv.org/pdf/2003.01871v1)
- **Published**: 2020-03-04 03:09:33+00:00
- **Updated**: 2020-03-04 03:09:33+00:00
- **Authors**: Julie Stephany Berrio, Mao Shan, Stewart Worrall, James Ward, Eduardo Nebot
- **Comment**: 8 pages, this paper was submitted to ITSC 2020
- **Journal**: None
- **Summary**: To navigate through urban roads, an automated vehicle must be able to perceive and recognize objects in a three-dimensional environment. A high-level contextual understanding of the surroundings is necessary to plan and execute accurate driving maneuvers. This paper presents an approach to fuse different sensory information, Light Detection and Ranging (lidar) scans and camera images. The output of a convolutional neural network (CNN) is used as classifier to obtain the labels of the environment. The transference of semantic information between the labelled image and the lidar point cloud is performed in four steps: initially, we use heuristic methods to associate probabilities to all the semantic classes contained in the labelled images. Then, the lidar points are corrected to compensate for the vehicle's motion given the difference between the timestamps of each lidar scan and camera image. In a third step, we calculate the pixel coordinate for the corresponding camera image. In the last step we perform the transfer of semantic information from the heuristic probability images to the lidar frame, while removing the lidar information that is not visible to the camera. We tested our approach in the Usyd Dataset \cite{usyd_dataset}, obtaining qualitative and quantitative results that demonstrate the validity of our probabilistic sensory fusion approach.



### Propagating Asymptotic-Estimated Gradients for Low Bitwidth Quantized Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2003.04296v1
- **DOI**: 10.1109/JSTSP.2020.2966327
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.04296v1)
- **Published**: 2020-03-04 03:17:47+00:00
- **Updated**: 2020-03-04 03:17:47+00:00
- **Authors**: Jun Chen, Yong Liu, Hao Zhang, Shengnan Hou, Jian Yang
- **Comment**: This paper has been accepted for publication in the IEEE Journal of
  Selected Topics in Signal Processing
- **Journal**: IEEE Journal of Selected Topics in Signal Processing 2020
- **Summary**: The quantized neural networks (QNNs) can be useful for neural network acceleration and compression, but during the training process they pose a challenge: how to propagate the gradient of loss function through the graph flow with a derivative of 0 almost everywhere. In response to this non-differentiable situation, we propose a novel Asymptotic-Quantized Estimator (AQE) to estimate the gradient. In particular, during back-propagation, the graph that relates inputs to output remains smoothness and differentiability. At the end of training, the weights and activations have been quantized to low-precision because of the asymptotic behaviour of AQE. Meanwhile, we propose a M-bit Inputs and N-bit Weights Network (MINW-Net) trained by AQE, a quantized neural network with 1-3 bits weights and activations. In the inference phase, we can use XNOR or SHIFT operations instead of convolution operations to accelerate the MINW-Net. Our experiments on CIFAR datasets demonstrate that our AQE is well defined, and the QNNs with AQE perform better than that with Straight-Through Estimator (STE). For example, in the case of the same ConvNet that has 1-bit weights and activations, our MINW-Net with AQE can achieve a prediction accuracy 1.5\% higher than the Binarized Neural Network (BNN) with STE. The MINW-Net, which is trained from scratch by AQE, can achieve comparable classification accuracy as 32-bit counterparts on CIFAR test sets. Extensive experimental results on ImageNet dataset show great superiority of the proposed AQE and our MINW-Net achieves comparable results with other state-of-the-art QNNs.



### Weighted Encoding Based Image Interpolation With Nonlocal Linear Regression Model
- **Arxiv ID**: http://arxiv.org/abs/2003.04811v1
- **DOI**: 10.1364/AO.397652
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.04811v1)
- **Published**: 2020-03-04 03:20:21+00:00
- **Updated**: 2020-03-04 03:20:21+00:00
- **Authors**: Junchao Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Image interpolation is a special case of image super-resolution, where the low-resolution image is directly down-sampled from its high-resolution counterpart without blurring and noise. Therefore, assumptions adopted in super-resolution models are not valid for image interpolation. To address this problem, we propose a novel image interpolation model based on sparse representation. Two widely used priors including sparsity and nonlocal self-similarity are used as the regularization terms to enhance the stability of interpolation model. Meanwhile, we incorporate the nonlocal linear regression into this model since nonlocal similar patches could provide a better approximation to a given patch. Moreover, we propose a new approach to learn adaptive sub-dictionary online instead of clustering. For each patch, similar patches are grouped to learn adaptive sub-dictionary, generating a more sparse and accurate representation. Finally, the weighted encoding is introduced to suppress tailing of fitting residuals in data fidelity. Abundant experimental results demonstrate that our proposed method outperforms several state-of-the-art methods in terms of quantitative measures and visual quality.



### Type I Attack for Generative Models
- **Arxiv ID**: http://arxiv.org/abs/2003.01872v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.01872v1)
- **Published**: 2020-03-04 03:20:59+00:00
- **Updated**: 2020-03-04 03:20:59+00:00
- **Authors**: Chengjin Sun, Sizhe Chen, Jia Cai, Xiaolin Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Generative models are popular tools with a wide range of applications. Nevertheless, it is as vulnerable to adversarial samples as classifiers. The existing attack methods mainly focus on generating adversarial examples by adding imperceptible perturbations to input, which leads to wrong result. However, we focus on another aspect of attack, i.e., cheating models by significant changes. The former induces Type II error and the latter causes Type I error. In this paper, we propose Type I attack to generative models such as VAE and GAN. One example given in VAE is that we can change an original image significantly to a meaningless one but their reconstruction results are similar. To implement the Type I attack, we destroy the original one by increasing the distance in input space while keeping the output similar because different inputs may correspond to similar features for the property of deep neural network. Experimental results show that our attack method is effective to generate Type I adversarial examples for generative models on large-scale image datasets.



### A Deep Learning Method for Complex Human Activity Recognition Using Virtual Wearable Sensors
- **Arxiv ID**: http://arxiv.org/abs/2003.01874v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.01874v2)
- **Published**: 2020-03-04 03:31:23+00:00
- **Updated**: 2020-03-06 01:18:54+00:00
- **Authors**: Fanyi Xiao, Ling Pei, Lei Chu, Danping Zou, Wenxian Yu, Yifan Zhu, Tao Li
- **Comment**: None
- **Journal**: None
- **Summary**: Sensor-based human activity recognition (HAR) is now a research hotspot in multiple application areas. With the rise of smart wearable devices equipped with inertial measurement units (IMUs), researchers begin to utilize IMU data for HAR. By employing machine learning algorithms, early IMU-based research for HAR can achieve accurate classification results on traditional classical HAR datasets, containing only simple and repetitive daily activities. However, these datasets rarely display a rich diversity of information in real-scene. In this paper, we propose a novel method based on deep learning for complex HAR in the real-scene. Specially, in the off-line training stage, the AMASS dataset, containing abundant human poses and virtual IMU data, is innovatively adopted for enhancing the variety and diversity. Moreover, a deep convolutional neural network with an unsupervised penalty is proposed to automatically extract the features of AMASS and improve the robustness. In the on-line testing stage, by leveraging advantages of the transfer learning, we obtain the final result by fine-tuning the partial neural network (optimizing the parameters in the fully-connected layers) using the real IMU data. The experimental results show that the proposed method can surprisingly converge in a few iterations and achieve an accuracy of 91.15% on a real IMU dataset, demonstrating the efficiency and effectiveness of the proposed method.



### Localising Faster: Efficient and precise lidar-based robot localisation in large-scale environments
- **Arxiv ID**: http://arxiv.org/abs/2003.01875v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.01875v2)
- **Published**: 2020-03-04 03:39:37+00:00
- **Updated**: 2020-07-15 18:13:23+00:00
- **Authors**: Li Sun, Daniel Adolfsson, Martin Magnusson, Henrik Andreasson, Ingmar Posner, Tom Duckett
- **Comment**: 7 pages, 5 pages. Accepted by IEEE International Conference on
  Robotics and Automation (ICRA) 2020
- **Journal**: None
- **Summary**: This paper proposes a novel approach for global localisation of mobile robots in large-scale environments. Our method leverages learning-based localisation and filtering-based localisation, to localise the robot efficiently and precisely through seeding Monte Carlo Localisation (MCL) with a deep-learned distribution. In particular, a fast localisation system rapidly estimates the 6-DOF pose through a deep-probabilistic model (Gaussian Process Regression with a deep kernel), then a precise recursive estimator refines the estimated robot pose according to the geometric alignment. More importantly, the Gaussian method (i.e. deep probabilistic localisation) and non-Gaussian method (i.e. MCL) can be integrated naturally via importance sampling. Consequently, the two systems can be integrated seamlessly and mutually benefit from each other. To verify the proposed framework, we provide a case study in large-scale localisation with a 3D lidar sensor. Our experiments on the Michigan NCLT long-term dataset show that the proposed method is able to localise the robot in 1.94 s on average (median of 0.8 s) with precision 0.75~m in a large-scale environment of approximately 0.5 km2.



### MoVi: A Large Multipurpose Motion and Video Dataset
- **Arxiv ID**: http://arxiv.org/abs/2003.01888v1
- **DOI**: 10.1371/journal.pone.0253157
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.01888v1)
- **Published**: 2020-03-04 04:43:03+00:00
- **Updated**: 2020-03-04 04:43:03+00:00
- **Authors**: Saeed Ghorbani, Kimia Mahdaviani, Anne Thaler, Konrad Kording, Douglas James Cook, Gunnar Blohm, Nikolaus F. Troje
- **Comment**: None
- **Journal**: None
- **Summary**: Human movements are both an area of intense study and the basis of many applications such as character animation. For many applications, it is crucial to identify movements from videos or analyze datasets of movements. Here we introduce a new human Motion and Video dataset MoVi, which we make available publicly. It contains 60 female and 30 male actors performing a collection of 20 predefined everyday actions and sports movements, and one self-chosen movement. In five capture rounds, the same actors and movements were recorded using different hardware systems, including an optical motion capture system, video cameras, and inertial measurement units (IMU). For some of the capture rounds, the actors were recorded when wearing natural clothing, for the other rounds they wore minimal clothing. In total, our dataset contains 9 hours of motion capture data, 17 hours of video data from 4 different points of view (including one hand-held camera), and 6.6 hours of IMU data. In this paper, we describe how the dataset was collected and post-processed; We present state-of-the-art estimates of skeletal motions and full-body shape deformations associated with skeletal motion. We discuss examples for potential studies this dataset could enable.



### GarmentGAN: Photo-realistic Adversarial Fashion Transfer
- **Arxiv ID**: http://arxiv.org/abs/2003.01894v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.01894v1)
- **Published**: 2020-03-04 05:01:15+00:00
- **Updated**: 2020-03-04 05:01:15+00:00
- **Authors**: Amir Hossein Raffiee, Michael Sollami
- **Comment**: 9 pages and 7 figures
- **Journal**: None
- **Summary**: The garment transfer problem comprises two tasks: learning to separate a person's body (pose, shape, color) from their clothing (garment type, shape, style) and then generating new images of the wearer dressed in arbitrary garments. We present GarmentGAN, a new algorithm that performs image-based garment transfer through generative adversarial methods. The GarmentGAN framework allows users to virtually try-on items before purchase and generalizes to various apparel types. GarmentGAN requires as input only two images, namely, a picture of the target fashion item and an image containing the customer. The output is a synthetic image wherein the customer is wearing the target apparel. In order to make the generated image look photo-realistic, we employ the use of novel generative adversarial techniques. GarmentGAN improves on existing methods in the realism of generated imagery and solves various problems related to self-occlusions. Our proposed model incorporates additional information during training, utilizing both segmentation maps and body key-point information. We show qualitative and quantitative comparisons to several other networks to demonstrate the effectiveness of this technique.



### Double Backpropagation for Training Autoencoders against Adversarial Attack
- **Arxiv ID**: http://arxiv.org/abs/2003.01895v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.01895v1)
- **Published**: 2020-03-04 05:12:27+00:00
- **Updated**: 2020-03-04 05:12:27+00:00
- **Authors**: Chengjin Sun, Sizhe Chen, Xiaolin Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning, as widely known, is vulnerable to adversarial samples. This paper focuses on the adversarial attack on autoencoders. Safety of the autoencoders (AEs) is important because they are widely used as a compression scheme for data storage and transmission, however, the current autoencoders are easily attacked, i.e., one can slightly modify an input but has totally different codes. The vulnerability is rooted the sensitivity of the autoencoders and to enhance the robustness, we propose to adopt double backpropagation (DBP) to secure autoencoder such as VAE and DRAW. We restrict the gradient from the reconstruction image to the original one so that the autoencoder is not sensitive to trivial perturbation produced by the adversarial attack. After smoothing the gradient by DBP, we further smooth the label by Gaussian Mixture Model (GMM), aiming for accurate and robust classification. We demonstrate in MNIST, CelebA, SVHN that our method leads to a robust autoencoder resistant to attack and a robust classifier able for image transition and immune to adversarial attack if combined with GMM.



### Denoised Smoothing: A Provable Defense for Pretrained Classifiers
- **Arxiv ID**: http://arxiv.org/abs/2003.01908v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.01908v2)
- **Published**: 2020-03-04 06:15:55+00:00
- **Updated**: 2020-09-21 02:20:16+00:00
- **Authors**: Hadi Salman, Mingjie Sun, Greg Yang, Ashish Kapoor, J. Zico Kolter
- **Comment**: 10 pages main text; 29 pages total
- **Journal**: None
- **Summary**: We present a method for provably defending any pretrained image classifier against $\ell_p$ adversarial attacks. This method, for instance, allows public vision API providers and users to seamlessly convert pretrained non-robust classification services into provably robust ones. By prepending a custom-trained denoiser to any off-the-shelf image classifier and using randomized smoothing, we effectively create a new classifier that is guaranteed to be $\ell_p$-robust to adversarial examples, without modifying the pretrained classifier. Our approach applies to both the white-box and the black-box settings of the pretrained classifier. We refer to this defense as denoised smoothing, and we demonstrate its effectiveness through extensive experimentation on ImageNet and CIFAR-10. Finally, we use our approach to provably defend the Azure, Google, AWS, and ClarifAI image classification APIs. Our code replicating all the experiments in the paper can be found at: https://github.com/microsoft/denoised-smoothing.



### Reveal of Domain Effect: How Visual Restoration Contributes to Object Detection in Aquatic Scenes
- **Arxiv ID**: http://arxiv.org/abs/2003.01913v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.01913v1)
- **Published**: 2020-03-04 06:44:19+00:00
- **Updated**: 2020-03-04 06:44:19+00:00
- **Authors**: Xingyu Chen, Yue Lu, Zhengxing Wu, Junzhi Yu, Li Wen
- **Comment**: None
- **Journal**: None
- **Summary**: Underwater robotic perception usually requires visual restoration and object detection, both of which have been studied for many years. Meanwhile, data domain has a huge impact on modern data-driven leaning process. However, exactly indicating domain effect, the relation between restoration and detection remains unclear. In this paper, we generally investigate the relation of quality-diverse data domain to detection performance. In the meantime, we unveil how visual restoration contributes to object detection in real-world underwater scenes. According to our analysis, five key discoveries are reported: 1) Domain quality has an ignorable effect on within-domain convolutional representation and detection accuracy; 2) low-quality domain leads to higher generalization ability in cross-domain detection; 3) low-quality domain can hardly be well learned in a domain-mixed learning process; 4) degrading recall efficiency, restoration cannot improve within-domain detection accuracy; 5) visual restoration is beneficial to detection in the wild by reducing the domain shift between training data and real-world scenes. Finally, as an illustrative example, we successfully perform underwater object detection with an aquatic robot.



### ETRI-Activity3D: A Large-Scale RGB-D Dataset for Robots to Recognize Daily Activities of the Elderly
- **Arxiv ID**: http://arxiv.org/abs/2003.01920v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.01920v2)
- **Published**: 2020-03-04 07:30:16+00:00
- **Updated**: 2020-03-11 05:01:07+00:00
- **Authors**: Jinhyeok Jang, Dohyung Kim, Cheonshu Park, Minsu Jang, Jaeyeon Lee, Jaehong Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning, based on which many modern algorithms operate, is well known to be data-hungry. In particular, the datasets appropriate for the intended application are difficult to obtain. To cope with this situation, we introduce a new dataset called ETRI-Activity3D, focusing on the daily activities of the elderly in robot-view. The major characteristics of the new dataset are as follows: 1) practical action categories that are selected from the close observation of the daily lives of the elderly; 2) realistic data collection, which reflects the robot's working environment and service situations; and 3) a large-scale dataset that overcomes the limitations of the current 3D activity analysis benchmark datasets. The proposed dataset contains 112,620 samples including RGB videos, depth maps, and skeleton sequences. During the data acquisition, 100 subjects were asked to perform 55 daily activities. Additionally, we propose a novel network called four-stream adaptive CNN (FSA-CNN). The proposed FSA-CNN has three main properties: robustness to spatio-temporal variations, input-adaptive activation function, and extension of the conventional two-stream approach. In the experiment section, we confirmed the superiority of the proposed FSA-CNN using NTU RGB+D and ETRI-Activity3D. Further, the domain difference between both groups of age was verified experimentally. Finally, the extension of FSA-CNN to deal with the multimodal data was investigated.



### Automatic Signboard Detection and Localization in Densely Populated Developing Cities
- **Arxiv ID**: http://arxiv.org/abs/2003.01936v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.01936v4)
- **Published**: 2020-03-04 08:04:03+00:00
- **Updated**: 2022-08-22 15:03:46+00:00
- **Authors**: Md. Sadrul Islam Toaha, Sakib Bin Asad, Chowdhury Rafeed Rahman, S. M. Shahriar Haque, Mahfuz Ara Proma, Md. Ahsan Habib Shuvo, Tashin Ahmed, Md. Amimul Basher
- **Comment**: None
- **Journal**: None
- **Summary**: Most city establishments of developing cities are digitally unlabeled because of the lack of automatic annotation systems. Hence location and trajectory services such as Google Maps, Uber etc remain underutilized in such cities. Accurate signboard detection in natural scene images is the foremost task for error-free information retrieval from such city streets. Yet, developing accurate signboard localization system is still an unresolved challenge because of its diverse appearances that include textual images and perplexing backgrounds. We present a novel object detection approach that can detect signboards automatically and is suitable for such cities. We use Faster R-CNN based localization by incorporating two specialized pretraining methods and a run time efficient hyperparameter value selection algorithm. We have taken an incremental approach in reaching our final proposed method through detailed evaluation and comparison with baselines using our constructed SVSO (Street View Signboard Objects) signboard dataset containing signboard natural scene images of six developing countries. We demonstrate state-of-the-art performance of our proposed method on both SVSO dataset and Open Image Dataset. Our proposed method can detect signboards accurately (even if the images contain multiple signboards with diverse shapes and colours in a noisy background) achieving 0.90 mAP (mean average precision) score on SVSO independent test set. Our implementation is available at: https://github.com/sadrultoaha/Signboard-Detection



### Gaussianization Flows
- **Arxiv ID**: http://arxiv.org/abs/2003.01941v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.01941v1)
- **Published**: 2020-03-04 08:15:06+00:00
- **Updated**: 2020-03-04 08:15:06+00:00
- **Authors**: Chenlin Meng, Yang Song, Jiaming Song, Stefano Ermon
- **Comment**: AISTATS 2020
- **Journal**: None
- **Summary**: Iterative Gaussianization is a fixed-point iteration procedure that can transform any continuous random vector into a Gaussian one. Based on iterative Gaussianization, we propose a new type of normalizing flow model that enables both efficient computation of likelihoods and efficient inversion for sample generation. We demonstrate that these models, named Gaussianization flows, are universal approximators for continuous probability distributions under some regularity conditions. Because of this guaranteed expressivity, they can capture multimodal target distributions without compromising the efficiency of sample generation. Experimentally, we show that Gaussianization flows achieve better or comparable performance on several tabular datasets compared to other efficiently invertible flow models such as Real NVP, Glow and FFJORD. In particular, Gaussianization flows are easier to initialize, demonstrate better robustness with respect to different transformations of the training data, and generalize better on small training sets.



### A Survey on Deep Hashing Methods
- **Arxiv ID**: http://arxiv.org/abs/2003.03369v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.03369v5)
- **Published**: 2020-03-04 08:25:15+00:00
- **Updated**: 2022-04-23 11:43:07+00:00
- **Authors**: Xiao Luo, Haixin Wang, Daqing Wu, Chong Chen, Minghua Deng, Jianqiang Huang, Xian-Sheng Hua
- **Comment**: Accepted by ACM Transactions on Knowledge Discovery from Data (TKDD)
- **Journal**: None
- **Summary**: Nearest neighbor search aims to obtain the samples in the database with the smallest distances from them to the queries, which is a basic task in a range of fields, including computer vision and data mining. Hashing is one of the most widely used methods for its computational and storage efficiency. With the development of deep learning, deep hashing methods show more advantages than traditional methods. In this survey, we detailedly investigate current deep hashing algorithms including deep supervised hashing and deep unsupervised hashing. Specifically, we categorize deep supervised hashing methods into pairwise methods, ranking-based methods, pointwise methods as well as quantization according to how measuring the similarities of the learned hash codes. Moreover, deep unsupervised hashing is categorized into similarity reconstruction-based methods, pseudo-label-based methods and prediction-free self-supervised learning-based methods based on their semantic learning manners. We also introduce three related important topics including semi-supervised deep hashing, domain adaption deep hashing and multi-modal deep hashing. Meanwhile, we present some commonly used public datasets and the scheme to measure the performance of deep hashing algorithms. Finally, we discuss some potential research directions in conclusion.



### Semixup: In- and Out-of-Manifold Regularization for Deep Semi-Supervised Knee Osteoarthritis Severity Grading from Plain Radiographs
- **Arxiv ID**: http://arxiv.org/abs/2003.01944v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.01944v3)
- **Published**: 2020-03-04 08:33:36+00:00
- **Updated**: 2020-08-12 09:44:29+00:00
- **Authors**: Huy Hoang Nguyen, Simo Saarakkala, Matthew Blaschko, Aleksei Tiulpin
- **Comment**: 11 main, 03 supplementary pages. The manuscript was accepted to IEEE
  Transactions on Medical Imaging in August 2020
- **Journal**: None
- **Summary**: Knee osteoarthritis (OA) is one of the highest disability factors in the world. This musculoskeletal disorder is assessed from clinical symptoms, and typically confirmed via radiographic assessment. This visual assessment done by a radiologist requires experience, and suffers from moderate to high inter-observer variability. The recent literature has shown that deep learning methods can reliably perform the OA severity assessment according to the gold standard Kellgren-Lawrence (KL) grading system. However, these methods require large amounts of labeled data, which are costly to obtain. In this study, we propose the Semixup algorithm, a semi-supervised learning (SSL) approach to leverage unlabeled data. Semixup relies on consistency regularization using in- and out-of-manifold samples, together with interpolated consistency. On an independent test set, our method significantly outperformed other state-of-the-art SSL methods in most cases. Finally, when compared to a well-tuned fully supervised baseline that yielded a balanced accuracy (BA) of $70.9\pm0.8%$ on the test set, Semixup had comparable performance -- BA of $71\pm0.8%$ $(p=0.368)$ while requiring $6$ times less labeled data. These results show that our proposed SSL method allows building fully automatic OA severity assessment tools with datasets that are available outside research settings.



### ADRN: Attention-based Deep Residual Network for Hyperspectral Image Denoising
- **Arxiv ID**: http://arxiv.org/abs/2003.01947v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.01947v1)
- **Published**: 2020-03-04 08:36:27+00:00
- **Updated**: 2020-03-04 08:36:27+00:00
- **Authors**: Yongsen Zhao, Deming Zhai, Junjun Jiang, Xianming Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Hyperspectral image (HSI) denoising is of crucial importance for many subsequent applications, such as HSI classification and interpretation. In this paper, we propose an attention-based deep residual network to directly learn a mapping from noisy HSI to the clean one. To jointly utilize the spatial-spectral information, the current band and its $K$ adjacent bands are simultaneously exploited as the input. Then, we adopt convolution layer with different filter sizes to fuse the multi-scale feature, and use shortcut connection to incorporate the multi-level information for better noise removal. In addition, the channel attention mechanism is employed to make the network concentrate on the most relevant auxiliary information and features that are beneficial to the denoising process best. To ease the training procedure, we reconstruct the output through a residual mode rather than a straightforward prediction. Experimental results demonstrate that our proposed ADRN scheme outperforms the state-of-the-art methods both in quantitative and visual evaluations.



### Occlusion Aware Unsupervised Learning of Optical Flow From Video
- **Arxiv ID**: http://arxiv.org/abs/2003.01960v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/2003.01960v1)
- **Published**: 2020-03-04 09:08:03+00:00
- **Updated**: 2020-03-04 09:08:03+00:00
- **Authors**: Jianfeng Li, Junqiao Zhao, Tiantian Feng, Chen Ye, Lu Xiong
- **Comment**: 6 pages, 5 figures
- **Journal**: None
- **Summary**: In this paper, we proposed an unsupervised learning method for estimating the optical flow between video frames, especially to solve the occlusion problem. Occlusion is caused by the movement of an object or the movement of the camera, defined as when certain pixels are visible in one video frame but not in adjacent frames. Due to the lack of pixel correspondence between frames in the occluded area, incorrect photometric loss calculation can mislead the optical flow training process. In the video sequence, we found that the occlusion in the forward ($t\rightarrow t+1$) and backward ($t\rightarrow t-1$) frame pairs are usually complementary. That is, pixels that are occluded in subsequent frames are often not occluded in the previous frame and vice versa. Therefore, by using this complementarity, a new weighted loss is proposed to solve the occlusion problem. In addition, we calculate gradients in multiple directions to provide richer supervision information. Our method achieves competitive optical flow accuracy compared to the baseline and some supervised methods on KITTI 2012 and 2015 benchmarks. This source code has been released at https://github.com/jianfenglihg/UnOpticalFlow.git.



### Learning for Video Compression with Hierarchical Quality and Recurrent Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2003.01966v7
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.01966v7)
- **Published**: 2020-03-04 09:31:37+00:00
- **Updated**: 2020-08-03 18:35:37+00:00
- **Authors**: Ren Yang, Fabian Mentzer, Luc Van Gool, Radu Timofte
- **Comment**: Published in CVPR 2020; corrected a minor typo in the footnote of
  Table 1; corrected Figure 11
- **Journal**: None
- **Summary**: In this paper, we propose a Hierarchical Learned Video Compression (HLVC) method with three hierarchical quality layers and a recurrent enhancement network. The frames in the first layer are compressed by an image compression method with the highest quality. Using these frames as references, we propose the Bi-Directional Deep Compression (BDDC) network to compress the second layer with relatively high quality. Then, the third layer frames are compressed with the lowest quality, by the proposed Single Motion Deep Compression (SMDC) network, which adopts a single motion map to estimate the motions of multiple frames, thus saving bits for motion information. In our deep decoder, we develop the Weighted Recurrent Quality Enhancement (WRQE) network, which takes both compressed frames and the bit stream as inputs. In the recurrent cell of WRQE, the memory and update signal are weighted by quality features to reasonably leverage multi-frame information for enhancement. In our HLVC approach, the hierarchical quality benefits the coding efficiency, since the high quality information facilitates the compression and enhancement of low quality frames at encoder and decoder sides, respectively. Finally, the experiments validate that our HLVC approach advances the state-of-the-art of deep video compression methods, and outperforms the "Low-Delay P (LDP) very fast" mode of x265 in terms of both PSNR and MS-SSIM. The project page is at https://github.com/RenYang-home/HLVC.



### Annotation-free Learning of Deep Representations for Word Spotting using Synthetic Data and Self Labeling
- **Arxiv ID**: http://arxiv.org/abs/2003.01989v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.01989v4)
- **Published**: 2020-03-04 10:46:25+00:00
- **Updated**: 2020-05-25 08:58:17+00:00
- **Authors**: Fabian Wolf, Gernot A. Fink
- **Comment**: Accepted to Workshop on Document Analysis Systems (DAS) 2020
- **Journal**: None
- **Summary**: Word spotting is a popular tool for supporting the first exploration of historic, handwritten document collections. Today, the best performing methods rely on machine learning techniques, which require a high amount of annotated training material. As training data is usually not available in the application scenario, annotation-free methods aim at solving the retrieval task without representative training samples. In this work, we present an annotation-free method that still employs machine learning techniques and therefore outperforms other learning-free approaches. The weakly supervised training scheme relies on a lexicon, that does not need to precisely fit the dataset. In combination with a confidence based selection of pseudo-labeled training samples, we achieve state-of-the-art query-by-example performances. Furthermore, our method allows to perform query-by-string, which is usually not the case for other annotation-free methods.



### Metrics and methods for robustness evaluation of neural networks with generative models
- **Arxiv ID**: http://arxiv.org/abs/2003.01993v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.01993v2)
- **Published**: 2020-03-04 10:58:59+00:00
- **Updated**: 2020-03-15 15:55:23+00:00
- **Authors**: Igor Buzhinsky, Arseny Nerinovsky, Stavros Tripakis
- **Comment**: 24 pages, 9 figures; data in Table 3 and Fig. 3 corrected (results
  unchanged), several typos fixed, references updated
- **Journal**: None
- **Summary**: Recent studies have shown that modern deep neural network classifiers are easy to fool, assuming that an adversary is able to slightly modify their inputs. Many papers have proposed adversarial attacks, defenses and methods to measure robustness to such adversarial perturbations. However, most commonly considered adversarial examples are based on $\ell_p$-bounded perturbations in the input space of the neural network, which are unlikely to arise naturally. Recently, especially in computer vision, researchers discovered "natural" or "semantic" perturbations, such as rotations, changes of brightness, or more high-level changes, but these perturbations have not yet been systematically utilized to measure the performance of classifiers. In this paper, we propose several metrics to measure robustness of classifiers to natural adversarial examples, and methods to evaluate them. These metrics, called latent space performance metrics, are based on the ability of generative models to capture probability distributions, and are defined in their latent spaces. On three image classification case studies, we evaluate the proposed metrics for several classifiers, including ones trained in conventional and robust ways. We find that the latent counterparts of adversarial robustness are associated with the accuracy of the classifier rather than its conventional adversarial robustness, but the latter is still reflected on the properties of found latent perturbations. In addition, our novel method of finding latent adversarial perturbations demonstrates that these perturbations are often perceptually small.



### The iCub multisensor datasets for robot and computer vision applications
- **Arxiv ID**: http://arxiv.org/abs/2003.01994v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.01994v1)
- **Published**: 2020-03-04 10:59:29+00:00
- **Updated**: 2020-03-04 10:59:29+00:00
- **Authors**: Murat Kirtay, Ugo Albanese, Lorenzo Vannucci, Guido Schillaci, Cecilia Laschi, Egidio Falotico
- **Comment**: 6 pages, 6 figures
- **Journal**: None
- **Summary**: This document presents novel datasets, constructed by employing the iCub robot equipped with an additional depth sensor and color camera. We used the robot to acquire color and depth information for 210 objects in different acquisition scenarios. At this end, the results were large scale datasets for robot and computer vision applications: object representation, object recognition and classification, and action recognition.



### A Learning Strategy for Contrast-agnostic MRI Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2003.01995v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.01995v3)
- **Published**: 2020-03-04 11:00:57+00:00
- **Updated**: 2021-04-08 11:47:24+00:00
- **Authors**: Benjamin Billot, Douglas Greve, Koen Van Leemput, Bruce Fischl, Juan Eugenio Iglesias, Adrian V. Dalca
- **Comment**: 19 pages, 5 figures
- **Journal**: Proceedings of the Third Conference on Medical Imaging with Deep
  Learning (2020), vol.121, pp. 75-93
- **Summary**: We present a deep learning strategy that enables, for the first time, contrast-agnostic semantic segmentation of completely unpreprocessed brain MRI scans, without requiring additional training or fine-tuning for new modalities. Classical Bayesian methods address this segmentation problem with unsupervised intensity models, but require significant computational resources. In contrast, learning-based methods can be fast at test time, but are sensitive to the data available at training. Our proposed learning method, SynthSeg, leverages a set of training segmentations (no intensity images required) to generate synthetic sample images of widely varying contrasts on the fly during training. These samples are produced using the generative model of the classical Bayesian segmentation framework, with randomly sampled parameters for appearance, deformation, noise, and bias field. Because each mini-batch has a different synthetic contrast, the final network is not biased towards any MRI contrast. We comprehensively evaluate our approach on four datasets comprising over 1,000 subjects and four types of MR contrast. The results show that our approach successfully segments every contrast in the data, performing slightly better than classical Bayesian segmentation, and three orders of magnitude faster. Moreover, even within the same type of MRI contrast, our strategy generalizes significantly better across datasets, compared to training using real images. Finally, we find that synthesizing a broad range of contrasts, even if unrealistic, increases the generalization of the neural network. Our code and model are open source at https://github.com/BBillot/SynthSeg.



### Asymmetric Gained Deep Image Compression With Continuous Rate Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2003.02012v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.02012v3)
- **Published**: 2020-03-04 11:42:05+00:00
- **Updated**: 2022-08-02 11:40:48+00:00
- **Authors**: Ze Cui, Jing Wang, Shangyin Gao, Bo Bai, Tiansheng Guo, Yihui Feng
- **Comment**: Accepted by CVPR 2021
- **Journal**: None
- **Summary**: With the development of deep learning techniques, the combination of deep learning with image compression has drawn lots of attention. Recently, learned image compression methods had exceeded their classical counterparts in terms of rate-distortion performance. However, continuous rate adaptation remains an open question. Some learned image compression methods use multiple networks for multiple rates, while others use one single model at the expense of computational complexity increase and performance degradation. In this paper, we propose a continuously rate adjustable learned image compression framework, Asymmetric Gained Variational Autoencoder (AG-VAE). AG-VAE utilizes a pair of gain units to achieve discrete rate adaptation in one single model with a negligible additional computation. Then, by using exponential interpolation, continuous rate adaptation is achieved without compromising performance. Besides, we propose the asymmetric Gaussian entropy model for more accurate entropy estimation. Exhaustive experiments show that our method achieves comparable quantitative performance with SOTA learned image compression methods and better qualitative performance than classical image codecs. In the ablation study, we confirm the usefulness and superiority of gain units and the asymmetric Gaussian entropy model.



### Redesigning SLAM for Arbitrary Multi-Camera Systems
- **Arxiv ID**: http://arxiv.org/abs/2003.02014v1
- **DOI**: 10.1109/ICRA40945.2020.9197553
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.02014v1)
- **Published**: 2020-03-04 11:44:42+00:00
- **Updated**: 2020-03-04 11:44:42+00:00
- **Authors**: Juichung Kuo, Manasi Muglikar, Zichao Zhang, Davide Scaramuzza
- **Comment**: None
- **Journal**: IEEE Conference on Robotics and Automation (ICRA), Paris, 2020
- **Summary**: Adding more cameras to SLAM systems improves robustness and accuracy but complicates the design of the visual front-end significantly. Thus, most systems in the literature are tailored for specific camera configurations. In this work, we aim at an adaptive SLAM system that works for arbitrary multi-camera setups. To this end, we revisit several common building blocks in visual SLAM. In particular, we propose an adaptive initialization scheme, a sensor-agnostic, information-theoretic keyframe selection algorithm, and a scalable voxel-based map. These techniques make little assumption about the actual camera setups and prefer theoretically grounded methods over heuristics. We adapt a state-of-the-art visual-inertial odometry with these modifications, and experimental results show that the modified pipeline can adapt to a wide range of camera setups (e.g., 2 to 6 cameras in one experiment) without the need of sensor-specific modifications or tuning.



### Joint Device-Edge Inference over Wireless Links with Pruning
- **Arxiv ID**: http://arxiv.org/abs/2003.02027v2
- **DOI**: 10.1109/SPAWC48557.2020.9154306
- **Categories**: **cs.IT**, cs.CV, cs.LG, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/2003.02027v2)
- **Published**: 2020-03-04 12:06:11+00:00
- **Updated**: 2020-10-20 10:32:01+00:00
- **Authors**: Mikolaj Jankowski, Deniz Gunduz, Krystian Mikolajczyk
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a joint feature compression and transmission scheme for efficient inference at the wireless network edge. Our goal is to enable efficient and reliable inference at the edge server assuming limited computational resources at the edge device. Previous work focused mainly on feature compression, ignoring the computational cost of channel coding. We incorporate the recently proposed deep joint source-channel coding (DeepJSCC) scheme, and combine it with novel filter pruning strategies aimed at reducing the redundant complexity from neural networks. We evaluate our approach on a classification task, and show improved results in both end-to-end reliability and workload reduction at the edge device. This is the first work that combines DeepJSCC with network pruning, and applies it to image classification over the wireless edge.



### Learning to Transfer Texture from Clothing Images to 3D Humans
- **Arxiv ID**: http://arxiv.org/abs/2003.02050v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.02050v2)
- **Published**: 2020-03-04 12:53:58+00:00
- **Updated**: 2020-03-30 23:35:26+00:00
- **Authors**: Aymen Mir, Thiemo Alldieck, Gerard Pons-Moll
- **Comment**: IEEE Conference on Computer Vision and Pattern Recognition
- **Journal**: None
- **Summary**: In this paper, we present a simple yet effective method to automatically transfer textures of clothing images (front and back) to 3D garments worn on top SMPL, in real time. We first automatically compute training pairs of images with aligned 3D garments using a custom non-rigid 3D to 2D registration method, which is accurate but slow. Using these pairs, we learn a mapping from pixels to the 3D garment surface. Our idea is to learn dense correspondences from garment image silhouettes to a 2D-UV map of a 3D garment surface using shape information alone, completely ignoring texture, which allows us to generalize to the wide range of web images. Several experiments demonstrate that our model is more accurate than widely used baselines such as thin-plate-spline warping and image-to-image translation networks while being orders of magnitude faster. Our model opens the door for applications such as virtual try-on, and allows for generation of 3D humans with varied textures which is necessary for learning.



### Mixup Regularization for Region Proposal based Object Detectors
- **Arxiv ID**: http://arxiv.org/abs/2003.02065v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.02065v1)
- **Published**: 2020-03-04 13:16:45+00:00
- **Updated**: 2020-03-04 13:16:45+00:00
- **Authors**: Shahine Bouabid, Vincent Delaitre
- **Comment**: None
- **Journal**: None
- **Summary**: Mixup - a neural network regularization technique based on linear interpolation of labeled sample pairs - has stood out by its capacity to improve model's robustness and generalizability through a surprisingly simple formalism. However, its extension to the field of object detection remains unclear as the interpolation of bounding boxes cannot be naively defined. In this paper, we propose to leverage the inherent region mapping structure of anchors to introduce a mixup-driven training regularization for region proposal based object detectors. The proposed method is benchmarked on standard datasets with challenging detection settings. Our experiments show an enhanced robustness to image alterations along with an ability to decontextualize detections, resulting in an improved generalization power.



### Unity Style Transfer for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2003.02068v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.02068v1)
- **Published**: 2020-03-04 13:22:57+00:00
- **Updated**: 2020-03-04 13:22:57+00:00
- **Authors**: Chong Liu, Xiaojun Chang, Yi-Dong Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Style variation has been a major challenge for person re-identification, which aims to match the same pedestrians across different cameras. Existing works attempted to address this problem with camera-invariant descriptor subspace learning. However, there will be more image artifacts when the difference between the images taken by different cameras is larger. To solve this problem, we propose a UnityStyle adaption method, which can smooth the style disparities within the same camera and across different cameras. Specifically, we firstly create UnityGAN to learn the style changes between cameras, producing shape-stable style-unity images for each camera, which is called UnityStyle images. Meanwhile, we use UnityStyle images to eliminate style differences between different images, which makes a better match between query and gallery. Then, we apply the proposed method to Re-ID models, expecting to obtain more style-robust depth features for querying. We conduct extensive experiments on widely used benchmark datasets to evaluate the performance of the proposed framework, the results of which confirm the superiority of the proposed model.



### Deep Neural Network Perception Models and Robust Autonomous Driving Systems
- **Arxiv ID**: http://arxiv.org/abs/2003.08756v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.08756v1)
- **Published**: 2020-03-04 15:02:05+00:00
- **Updated**: 2020-03-04 15:02:05+00:00
- **Authors**: Mohammad Javad Shafiee, Ahmadreza Jeddi, Amir Nazemi, Paul Fieguth, Alexander Wong
- **Comment**: None
- **Journal**: None
- **Summary**: This paper analyzes the robustness of deep learning models in autonomous driving applications and discusses the practical solutions to address that.



### VESR-Net: The Winning Solution to Youku Video Enhancement and Super-Resolution Challenge
- **Arxiv ID**: http://arxiv.org/abs/2003.02115v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.02115v1)
- **Published**: 2020-03-04 15:09:17+00:00
- **Updated**: 2020-03-04 15:09:17+00:00
- **Authors**: Jiale Chen, Xu Tan, Chaowei Shan, Sen Liu, Zhibo Chen
- **Comment**: The champion of Youku-VESR challenge
- **Journal**: None
- **Summary**: This paper introduces VESR-Net, a method for video enhancement and super-resolution (VESR). We design a separate non-local module to explore the relations among video frames and fuse video frames efficiently, and a channel attention residual block to capture the relations among feature maps for video frame reconstruction in VESR-Net. We conduct experiments to analyze the effectiveness of these designs in VESR-Net, which demonstrates the advantages of VESR-Net over previous state-of-the-art VESR methods. It is worth to mention that among more than thousands of participants for Youku video enhancement and super-resolution (Youku-VESR) challenge, our proposed VESR-Net beat other competitive methods and ranked the first place.



### HintPose
- **Arxiv ID**: http://arxiv.org/abs/2003.02170v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.02170v1)
- **Published**: 2020-03-04 16:29:31+00:00
- **Updated**: 2020-03-04 16:29:31+00:00
- **Authors**: Sanghoon Hong, Hunchul Park, Jonghyuk Park, Sukhyun Cho, Heewoong Park
- **Comment**: Presented at "Joint COCO and Mapillary Workshop at ICCV 2019:
  Keypoint Detection Challenge Track"
- **Journal**: None
- **Summary**: Most of the top-down pose estimation models assume that there exists only one person in a bounding box. However, the assumption is not always correct. In this technical report, we introduce two ideas, instance cue and recurrent refinement, to an existing pose estimator so that the model is able to handle detection boxes with multiple persons properly. When we evaluated our model on the COCO17 keypoints dataset, it showed non-negligible improvement compared to its baseline model. Our model achieved 76.2 mAP as a single model and 77.3 mAP as an ensemble on the test-dev set without additional training data. After additional post-processing with a separate refinement network, our final predictions achieved 77.8 mAP on the COCO test-dev set.



### Colored Noise Injection for Training Adversarially Robust Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2003.02188v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.02188v2)
- **Published**: 2020-03-04 17:01:54+00:00
- **Updated**: 2020-03-20 12:21:56+00:00
- **Authors**: Evgenii Zheltonozhskii, Chaim Baskin, Yaniv Nemcovsky, Brian Chmiel, Avi Mendelson, Alex M. Bronstein
- **Comment**: None
- **Journal**: None
- **Summary**: Even though deep learning has shown unmatched performance on various tasks, neural networks have been shown to be vulnerable to small adversarial perturbations of the input that lead to significant performance degradation. In this work we extend the idea of adding white Gaussian noise to the network weights and activations during adversarial training (PNI) to the injection of colored noise for defense against common white-box and black-box attacks. We show that our approach outperforms PNI and various previous approaches in terms of adversarial accuracy on CIFAR-10 and CIFAR-100 datasets. In addition, we provide an extensive ablation study of the proposed method justifying the chosen configurations.



### Robust Perceptual Night Vision in Thermal Colorization
- **Arxiv ID**: http://arxiv.org/abs/2003.02204v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.02204v1)
- **Published**: 2020-03-04 17:17:08+00:00
- **Updated**: 2020-03-04 17:17:08+00:00
- **Authors**: Feras Almasri, Olivier Debeir
- **Comment**: 9 pages, 7 figures, VISAPP2020 conference
- **Journal**: None
- **Summary**: Transforming a thermal infrared image into a robust perceptual colour Visible image is an ill-posed problem due to the differences in their spectral domains and in the objects' representations. Objects appear in one spectrum but not necessarily in the other, and the thermal signature of a single object may have different colours in its Visible representation. This makes a direct mapping from thermal to Visible images impossible and necessitates a solution that preserves texture captured in the thermal spectrum while predicting the possible colour for certain objects. In this work, a deep learning method to map the thermal signature from the thermal image's spectrum to a Visible representation in their low-frequency space is proposed. A pan-sharpening method is then used to merge the predicted low-frequency representation with the high-frequency representation extracted from the thermal image. The proposed model generates colour values consistent with the Visible ground truth when the object does not vary much in its appearance and generates averaged grey values in other cases. The proposed method shows robust perceptual night vision images in preserving the object's appearance and image context compared with the existing state-of-the-art.



### Adaptive binarization based on fuzzy integrals
- **Arxiv ID**: http://arxiv.org/abs/2003.08755v1
- **DOI**: 10.1016/j.inffus.2020.10.020
- **Categories**: **cs.CV**, cs.LG, stat.ML, 68W25, I.4; I.5
- **Links**: [PDF](http://arxiv.org/pdf/2003.08755v1)
- **Published**: 2020-03-04 18:30:57+00:00
- **Updated**: 2020-03-04 18:30:57+00:00
- **Authors**: Francesco Bardozzo, Borja De La Osa, Lubomira Horanska, Javier Fumanal-Idocin, Mattia delli Priscoli, Luigi Troiano, Roberto Tagliaferri, Javier Fernandez, Humberto Bustince
- **Comment**: 11 pages, 3 figures, 3 algorithms, Journal paper under a revision of
  IEEE Transactions on Image Processing
- **Journal**: None
- **Summary**: Adaptive binarization methodologies threshold the intensity of the pixels with respect to adjacent pixels exploiting the integral images. In turn, the integral images are generally computed optimally using the summed-area-table algorithm (SAT). This document presents a new adaptive binarization technique based on fuzzy integral images through an efficient design of a modified SAT for fuzzy integrals. We define this new methodology as FLAT (Fuzzy Local Adaptive Thresholding). The experimental results show that the proposed methodology have produced an image quality thresholding often better than traditional algorithms and saliency neural networks. We propose a new generalization of the Sugeno and CF 1,2 integrals to improve existing results with an efficient integral image computation. Therefore, these new generalized fuzzy integrals can be used as a tool for grayscale processing in real-time and deep-learning applications. Index Terms: Image Thresholding, Image Processing, Fuzzy Integrals, Aggregation Functions



### Voxel Map for Visual SLAM
- **Arxiv ID**: http://arxiv.org/abs/2003.02247v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.02247v1)
- **Published**: 2020-03-04 18:39:14+00:00
- **Updated**: 2020-03-04 18:39:14+00:00
- **Authors**: Manasi Muglikar, Zichao Zhang, Davide Scaramuzza
- **Comment**: None
- **Journal**: IEEE Conference on Robotics and Automation(ICRA), Paris, 2020
- **Summary**: In modern visual SLAM systems, it is a standard practice to retrieve potential candidate map points from overlapping keyframes for further feature matching or direct tracking. In this work, we argue that keyframes are not the optimal choice for this task, due to several inherent limitations, such as weak geometric reasoning and poor scalability. We propose a voxel-map representation to efficiently retrieve map points for visual SLAM. In particular, we organize the map points in a regular voxel grid. Visible points from a camera pose are queried by sampling the camera frustum in a raycasting manner, which can be done in constant time using an efficient voxel hashing method. Compared with keyframes, the retrieved points using our method are geometrically guaranteed to fall in the camera field-of-view, and occluded points can be identified and removed to a certain extend. This method also naturally scales up to large scenes and complicated multicamera configurations. Experimental results show that our voxel map representation is as efficient as a keyframe map with 5 keyframes and provides significantly higher localization accuracy (average 46% improvement in RMSE) on the EuRoC dataset. The proposed voxel-map representation is a general approach to a fundamental functionality in visual SLAM and widely applicable.



### Multi-object Tracking via End-to-end Tracklet Searching and Ranking
- **Arxiv ID**: http://arxiv.org/abs/2003.02795v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.02795v1)
- **Published**: 2020-03-04 18:46:01+00:00
- **Updated**: 2020-03-04 18:46:01+00:00
- **Authors**: Tao Hu, Lichao Huang, Han Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Recent works in multiple object tracking use sequence model to calculate the similarity score between the detections and the previous tracklets. However, the forced exposure to ground-truth in the training stage leads to the training-inference discrepancy problem, i.e., exposure bias, where association error could accumulate in the inference and make the trajectories drift. In this paper, we propose a novel method for optimizing tracklet consistency, which directly takes the prediction errors into account by introducing an online, end-to-end tracklet search training process. Notably, our methods directly optimize the whole tracklet score instead of pairwise affinity. With sequence model as appearance encoders of tracklet, our tracker achieves remarkable performance gain from conventional tracklet association baseline. Our methods have also achieved state-of-the-art in MOT15~17 challenge benchmarks using public detection and online settings.



### Spatiotemporal-Aware Augmented Reality: Redefining HCI in Image-Guided Therapy
- **Arxiv ID**: http://arxiv.org/abs/2003.02260v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.02260v1)
- **Published**: 2020-03-04 18:59:55+00:00
- **Updated**: 2020-03-04 18:59:55+00:00
- **Authors**: Javad Fotouhi, Arian Mehrfard, Tianyu Song, Alex Johnson, Greg Osgood, Mathias Unberath, Mehran Armand, Nassir Navab
- **Comment**: None
- **Journal**: None
- **Summary**: Suboptimal interaction with patient data and challenges in mastering 3D anatomy based on ill-posed 2D interventional images are essential concerns in image-guided therapies. Augmented reality (AR) has been introduced in the operating rooms in the last decade; however, in image-guided interventions, it has often only been considered as a visualization device improving traditional workflows. As a consequence, the technology is gaining minimum maturity that it requires to redefine new procedures, user interfaces, and interactions. The main contribution of this paper is to reveal how exemplary workflows are redefined by taking full advantage of head-mounted displays when entirely co-registered with the imaging system at all times. The proposed AR landscape is enabled by co-localizing the users and the imaging devices via the operating room environment and exploiting all involved frustums to move spatial information between different bodies. The awareness of the system from the geometric and physical characteristics of X-ray imaging allows the redefinition of different human-machine interfaces. We demonstrate that this AR paradigm is generic, and can benefit a wide variety of procedures. Our system achieved an error of $4.76\pm2.91$ mm for placing K-wire in a fracture management procedure, and yielded errors of $1.57\pm1.16^\circ$ and $1.46\pm1.00^\circ$ in the abduction and anteversion angles, respectively, for total hip arthroplasty. We hope that our holistic approach towards improving the interface of surgery not only augments the surgeon's capabilities but also augments the surgical team's experience in carrying out an effective intervention with reduced complications and provide novel approaches of documenting procedures for training purposes.



### Exploring Partial Intrinsic and Extrinsic Symmetry in 3D Medical Imaging
- **Arxiv ID**: http://arxiv.org/abs/2003.02294v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.02294v2)
- **Published**: 2020-03-04 19:08:55+00:00
- **Updated**: 2020-09-27 00:23:51+00:00
- **Authors**: Javad Fotouhi, Giacomo Taylor, Mathias Unberath, Alex Johnson, Sing Chun Lee, Greg Osgood, Mehran Armand, Nassir Navab
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel methodology to detect imperfect bilateral symmetry in CT of human anatomy. In this paper, the structurally symmetric nature of the pelvic bone is explored and is used to provide interventional image augmentation for treatment of unilateral fractures in patients with traumatic injuries. The mathematical basis of our solution is on the incorporation of attributes and characteristics that satisfy the properties of intrinsic and extrinsic symmetry and are robust to outliers. In the first step, feature points that satisfy intrinsic symmetry are automatically detected in the M\"obius space defined on the CT data. These features are then pruned via a two-stage RANSAC to attain correspondences that satisfy also the extrinsic symmetry. Then, a disparity function based on Tukey's biweight robust estimator is introduced and minimized to identify a symmetry plane parametrization that yields maximum contralateral similarity. Finally, a novel regularization term is introduced to enhance similarity between bone density histograms across the partial symmetry plane, relying on the important biological observation that, even if injured, the dislocated bone segments remain within the body. Our extensive evaluations on various cases of common fracture types demonstrate the validity of the novel concepts and the robustness and accuracy of the proposed method.



### The Impact of Hole Geometry on Relative Robustness of In-Painting Networks: An Empirical Study
- **Arxiv ID**: http://arxiv.org/abs/2003.02314v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.02314v1)
- **Published**: 2020-03-04 20:14:14+00:00
- **Updated**: 2020-03-04 20:14:14+00:00
- **Authors**: Masood S. Mortazavi, Ning Yan
- **Comment**: None
- **Journal**: None
- **Summary**: In-painting networks use existing pixels to generate appropriate pixels to fill "holes" placed on parts of an image. A 2-D in-painting network's input usually consists of (1) a three-channel 2-D image, and (2) an additional channel for the "holes" to be in-painted in that image. In this paper, we study the robustness of a given in-painting neural network against variations in hole geometry distributions. We observe that the robustness of an in-painting network is dependent on the probability distribution function (PDF) of the hole geometry presented to it during its training even if the underlying image dataset used (in training and testing) does not alter. We develop an experimental methodology for testing and evaluating relative robustness of in-painting networks against four different kinds of hole geometry PDFs. We examine a number of hypothesis regarding (1) the natural bias of in-painting networks to the hole distribution used for their training, (2) the underlying dataset's ability to differentiate relative robustness as hole distributions vary in a train-test (cross-comparison) grid, and (3) the impact of the directional distribution of edges in the holes and in the image dataset. We present results for L1, PSNR and SSIM quality metrics and develop a specific measure of relative in-painting robustness to be used in cross-comparison grids based on these quality metrics. (One can incorporate other quality metrics in this relative measure.) The empirical work reported here is an initial step in a broader and deeper investigation of "filling the blank" neural networks' sensitivity, robustness and regularization with respect to hole "geometry" PDFs, and it suggests further research in this domain.



### Learning View and Target Invariant Visual Servoing for Navigation
- **Arxiv ID**: http://arxiv.org/abs/2003.02327v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2003.02327v1)
- **Published**: 2020-03-04 20:36:43+00:00
- **Updated**: 2020-03-04 20:36:43+00:00
- **Authors**: Yimeng Li, Jana Kosecka
- **Comment**: Accepted to ICRA 2020
- **Journal**: None
- **Summary**: The advances in deep reinforcement learning recently revived interest in data-driven learning based approaches to navigation. In this paper we propose to learn viewpoint invariant and target invariant visual servoing for local mobile robot navigation; given an initial view and the goal view or an image of a target, we train deep convolutional network controller to reach the desired goal. We present a new architecture for this task which rests on the ability of establishing correspondences between the initial and goal view and novel reward structure motivated by the traditional feedback control error. The advantage of the proposed model is that it does not require calibration and depth information and achieves robust visual servoing in a variety of environments and targets without any parameter fine tuning. We present comprehensive evaluation of the approach and comparison with other deep learning architectures as well as classical visual servoing methods in visually realistic simulation environment. The presented model overcomes the brittleness of classical visual servoing based methods and achieves significantly higher generalization capability compared to the previous learning approaches.



### SAM: The Sensitivity of Attribution Methods to Hyperparameters
- **Arxiv ID**: http://arxiv.org/abs/2003.08754v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.08754v2)
- **Published**: 2020-03-04 22:09:22+00:00
- **Updated**: 2020-04-13 03:32:08+00:00
- **Authors**: Naman Bansal, Chirag Agarwal, Anh Nguyen
- **Comment**: Oral paper at CVPR 2020
- **Journal**: None
- **Summary**: Attribution methods can provide powerful insights into the reasons for a classifier's decision. We argue that a key desideratum of an explanation method is its robustness to input hyperparameters which are often randomly set or empirically tuned. High sensitivity to arbitrary hyperparameter choices does not only impede reproducibility but also questions the correctness of an explanation and impairs the trust of end-users. In this paper, we provide a thorough empirical study on the sensitivity of existing attribution methods. We found an alarming trend that many methods are highly sensitive to changes in their common hyperparameters e.g. even changing a random seed can yield a different explanation! Interestingly, such sensitivity is not reflected in the average explanation accuracy scores over the dataset as commonly reported in the literature. In addition, explanations generated for robust classifiers (i.e. which are trained to be invariant to pixel-wise perturbations) are surprisingly more robust than those generated for regular classifiers.



### Creating High Resolution Images with a Latent Adversarial Generator
- **Arxiv ID**: http://arxiv.org/abs/2003.02365v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.02365v1)
- **Published**: 2020-03-04 23:23:08+00:00
- **Updated**: 2020-03-04 23:23:08+00:00
- **Authors**: David Berthelot, Peyman Milanfar, Ian Goodfellow
- **Comment**: None
- **Journal**: None
- **Summary**: Generating realistic images is difficult, and many formulations for this task have been proposed recently. If we restrict the task to that of generating a particular class of images, however, the task becomes more tractable. That is to say, instead of generating an arbitrary image as a sample from the manifold of natural images, we propose to sample images from a particular "subspace" of natural images, directed by a low-resolution image from the same subspace. The problem we address, while close to the formulation of the single-image super-resolution problem, is in fact rather different. Single image super-resolution is the task of predicting the image closest to the ground truth from a relatively low resolution image. We propose to produce samples of high resolution images given extremely small inputs with a new method called Latent Adversarial Generator (LAG). In our generative sampling framework, we only use the input (possibly of very low-resolution) to direct what class of samples the network should produce. As such, the output of our algorithm is not a unique image that relates to the input, but rather a possible se} of related images sampled from the manifold of natural images. Our method learns exclusively in the latent space of the adversary using perceptual loss -- it does not have a pixel loss.



### Towards Fair Cross-Domain Adaptation via Generative Learning
- **Arxiv ID**: http://arxiv.org/abs/2003.02366v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.02366v2)
- **Published**: 2020-03-04 23:25:09+00:00
- **Updated**: 2020-11-03 18:44:21+00:00
- **Authors**: Tongxin Wang, Zhengming Ding, Wei Shao, Haixu Tang, Kun Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Domain Adaptation (DA) targets at adapting a model trained over the well-labeled source domain to the unlabeled target domain lying in different distributions. Existing DA normally assumes the well-labeled source domain is class-wise balanced, which means the size per source class is relatively similar. However, in real-world applications, labeled samples for some categories in the source domain could be extremely few due to the difficulty of data collection and annotation, which leads to decreasing performance over target domain on those few-shot categories. To perform fair cross-domain adaptation and boost the performance on these minority categories, we develop a novel Generative Few-shot Cross-domain Adaptation (GFCA) algorithm for fair cross-domain classification. Specifically, generative feature augmentation is explored to synthesize effective training data for few-shot source classes, while effective cross-domain alignment aims to adapt knowledge from source to facilitate the target learning. Experimental results on two large cross-domain visual datasets demonstrate the effectiveness of our proposed method on improving both few-shot and overall classification accuracy comparing with the state-of-the-art DA approaches.



### FineHand: Learning Hand Shapes for American Sign Language Recognition
- **Arxiv ID**: http://arxiv.org/abs/2003.08753v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.08753v1)
- **Published**: 2020-03-04 23:32:08+00:00
- **Updated**: 2020-03-04 23:32:08+00:00
- **Authors**: Al Amin Hosain, Panneer Selvam Santhalingam, Parth Pathak, Huzefa Rangwala, Jana Kosecka
- **Comment**: None
- **Journal**: None
- **Summary**: American Sign Language recognition is a difficult gesture recognition problem, characterized by fast, highly articulate gestures. These are comprised of arm movements with different hand shapes, facial expression and head movements. Among these components, hand shape is the vital, often the most discriminative part of a gesture. In this work, we present an approach for effective learning of hand shape embeddings, which are discriminative for ASL gestures. For hand shape recognition our method uses a mix of manually labelled hand shapes and high confidence predictions to train deep convolutional neural network (CNN). The sequential gesture component is captured by recursive neural network (RNN) trained on the embeddings learned in the first stage. We will demonstrate that higher quality hand shape models can significantly improve the accuracy of final video gesture classification in challenging conditions with variety of speakers, different illumination and significant motion blurr. We compare our model to alternative approaches exploiting different modalities and representations of the data and show improved video gesture recognition accuracy on GMU-ASL51 benchmark dataset



### A Benchmark for LiDAR-based Panoptic Segmentation based on KITTI
- **Arxiv ID**: http://arxiv.org/abs/2003.02371v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2003.02371v1)
- **Published**: 2020-03-04 23:44:40+00:00
- **Updated**: 2020-03-04 23:44:40+00:00
- **Authors**: Jens Behley, Andres Milioto, Cyrill Stachniss
- **Comment**: None
- **Journal**: None
- **Summary**: Panoptic segmentation is the recently introduced task that tackles semantic segmentation and instance segmentation jointly. In this paper, we present an extension of SemanticKITTI, which is a large-scale dataset providing dense point-wise semantic labels for all sequences of the KITTI Odometry Benchmark, for training and evaluation of laser-based panoptic segmentation. We provide the data and discuss the processing steps needed to enrich a given semantic annotation with temporally consistent instance information, i.e., instance information that supplements the semantic labels and identifies the same instance over sequences of LiDAR point clouds. Additionally, we present two strong baselines that combine state-of-the-art LiDAR-based semantic segmentation approaches with a state-of-the-art detector enriching the segmentation with instance information and that allow other researchers to compare their approaches against. We hope that our extension of SemanticKITTI with strong baselines enables the creation of novel algorithms for LiDAR-based panoptic segmentation as much as it has for the original semantic segmentation and semantic scene completion tasks. Data, code, and an online evaluation using a hidden test set will be published on http://semantic-kitti.org.



