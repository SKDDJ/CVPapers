# Arxiv Papers in cs.CV on 2020-03-15
### NoiseRank: Unsupervised Label Noise Reduction with Dependence Models
- **Arxiv ID**: http://arxiv.org/abs/2003.06729v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.06729v1)
- **Published**: 2020-03-15 01:10:25+00:00
- **Updated**: 2020-03-15 01:10:25+00:00
- **Authors**: Karishma Sharma, Pinar Donmez, Enming Luo, Yan Liu, I. Zeki Yalniz
- **Comment**: None
- **Journal**: None
- **Summary**: Label noise is increasingly prevalent in datasets acquired from noisy channels. Existing approaches that detect and remove label noise generally rely on some form of supervision, which is not scalable and error-prone. In this paper, we propose NoiseRank, for unsupervised label noise reduction using Markov Random Fields (MRF). We construct a dependence model to estimate the posterior probability of an instance being incorrectly labeled given the dataset, and rank instances based on their estimated probabilities. Our method 1) Does not require supervision from ground-truth labels, or priors on label or noise distribution. 2) It is interpretable by design, enabling transparency in label noise removal. 3) It is agnostic to classifier architecture/optimization framework and content modality. These advantages enable wide applicability in real noise settings, unlike prior works constrained by one or more conditions. NoiseRank improves state-of-the-art classification on Food101-N (~20% noise), and is effective on high noise Clothing-1M (~40% noise).



### A model of figure ground organization incorporating local and global cues
- **Arxiv ID**: http://arxiv.org/abs/2003.06731v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.06731v1)
- **Published**: 2020-03-15 01:18:40+00:00
- **Updated**: 2020-03-15 01:18:40+00:00
- **Authors**: Sudarshan Ramenahalli
- **Comment**: 46 pages, 5 figures
- **Journal**: None
- **Summary**: Figure Ground Organization (FGO) -- inferring spatial depth ordering of objects in a visual scene -- involves determining which side of an occlusion boundary is figure (closer to the observer) and which is ground (further away from the observer). A combination of global cues, like convexity, and local cues, like T-junctions are involved in this process. We present a biologically motivated, feed forward computational model of FGO incorporating convexity, surroundedness, parallelism as global cues and Spectral Anisotropy (SA), T-junctions as local cues. While SA is computed in a biologically plausible manner, the inclusion of T-Junctions is biologically motivated. The model consists of three independent feature channels, Color, Intensity and Orientation, but SA and T-Junctions are introduced only in the Orientation channel as these properties are specific to that feature of objects. We study the effect of adding each local cue independently and both of them simultaneously to the model with no local cues. We evaluate model performance based on figure-ground classification accuracy (FGCA) at every border location using the BSDS 300 figure-ground dataset. Each local cue, when added alone, gives statistically significant improvement in the FGCA of the model suggesting its usefulness as an independent FGO cue. The model with both local cues achieves higher FGCA than the models with individual cues, indicating SA and T-Junctions are not mutually contradictory. Compared to the model with no local cues, the feed-forward model with both local cues achieves $\geq 8.78$% improvement in terms of FGCA.



### Active Perception and Representation for Robotic Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2003.06734v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2003.06734v1)
- **Published**: 2020-03-15 01:43:51+00:00
- **Updated**: 2020-03-15 01:43:51+00:00
- **Authors**: Youssef Zaky, Gaurav Paruthi, Bryan Tripp, James Bergstra
- **Comment**: None
- **Journal**: None
- **Summary**: The vast majority of visual animals actively control their eyes, heads, and/or bodies to direct their gaze toward different parts of their environment. In contrast, recent applications of reinforcement learning in robotic manipulation employ cameras as passive sensors. These are carefully placed to view a scene from a fixed pose. Active perception allows animals to gather the most relevant information about the world and focus their computational resources where needed. It also enables them to view objects from different distances and viewpoints, providing a rich visual experience from which to learn abstract representations of the environment. Inspired by the primate visual-motor system, we present a framework that leverages the benefits of active perception to accomplish manipulation tasks. Our agent uses viewpoint changes to localize objects, to learn state representations in a self-supervised manner, and to perform goal-directed actions. We apply our model to a simulated grasping task with a 6-DoF action space. Compared to its passive, fixed-camera counterpart, the active model achieves 8% better performance in targeted grasping. Compared to vanilla deep Q-learning algorithms, our model is at least four times more sample-efficient, highlighting the benefits of both active perception and representation learning.



### Vision-Dialog Navigation by Exploring Cross-modal Memory
- **Arxiv ID**: http://arxiv.org/abs/2003.06745v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2003.06745v1)
- **Published**: 2020-03-15 03:08:06+00:00
- **Updated**: 2020-03-15 03:08:06+00:00
- **Authors**: Yi Zhu, Fengda Zhu, Zhaohuan Zhan, Bingqian Lin, Jianbin Jiao, Xiaojun Chang, Xiaodan Liang
- **Comment**: CVPR2020
- **Journal**: None
- **Summary**: Vision-dialog navigation posed as a new holy-grail task in vision-language disciplinary targets at learning an agent endowed with the capability of constant conversation for help with natural language and navigating according to human responses. Besides the common challenges faced in visual language navigation, vision-dialog navigation also requires to handle well with the language intentions of a series of questions about the temporal context from dialogue history and co-reasoning both dialogs and visual scenes. In this paper, we propose the Cross-modal Memory Network (CMN) for remembering and understanding the rich information relevant to historical navigation actions. Our CMN consists of two memory modules, the language memory module (L-mem) and the visual memory module (V-mem). Specifically, L-mem learns latent relationships between the current language interaction and a dialog history by employing a multi-head attention mechanism. V-mem learns to associate the current visual views and the cross-modal memory about the previous navigation actions. The cross-modal memory is generated via a vision-to-language attention and a language-to-vision attention. Benefiting from the collaborative learning of the L-mem and the V-mem, our CMN is able to explore the memory about the decision making of historical navigation actions which is for the current step. Experiments on the CVDN dataset show that our CMN outperforms the previous state-of-the-art model by a significant margin on both seen and unseen environments.



### Beyond without Forgetting: Multi-Task Learning for Classification with Disjoint Datasets
- **Arxiv ID**: http://arxiv.org/abs/2003.06746v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.06746v1)
- **Published**: 2020-03-15 03:19:18+00:00
- **Updated**: 2020-03-15 03:19:18+00:00
- **Authors**: Yan Hong, Li Niu, Jianfu Zhang, Liqing Zhang
- **Comment**: This paper is accepted by ICME 2020(http://www.2020.ieeeicme.org/)
- **Journal**: None
- **Summary**: Multi-task Learning (MTL) for classification with disjoint datasets aims to explore MTL when one task only has one labeled dataset. In existing methods, for each task, the unlabeled datasets are not fully exploited to facilitate this task. Inspired by semi-supervised learning, we use unlabeled datasets with pseudo labels to facilitate each task. However, there are two major issues: 1) the pseudo labels are very noisy; 2) the unlabeled datasets and the labeled dataset for each task has considerable data distribution mismatch. To address these issues, we propose our MTL with Selective Augmentation (MTL-SA) method to select the training samples in unlabeled datasets with confident pseudo labels and close data distribution to the labeled dataset. Then, we use the selected training samples to add information and use the remaining training samples to preserve information. Extensive experiments on face-centric and human-centric applications demonstrate the effectiveness of our MTL-SA method.



### A Novel Learnable Gradient Descent Type Algorithm for Non-convex Non-smooth Inverse Problems
- **Arxiv ID**: http://arxiv.org/abs/2003.06748v2
- **DOI**: None
- **Categories**: **cs.CV**, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2003.06748v2)
- **Published**: 2020-03-15 03:44:43+00:00
- **Updated**: 2020-03-24 23:39:46+00:00
- **Authors**: Qingchao Zhang, Xiaojing Ye, Hongcheng Liu, Yunmei Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Optimization algorithms for solving nonconvex inverse problem have attracted significant interests recently. However, existing methods require the nonconvex regularization to be smooth or simple to ensure convergence. In this paper, we propose a novel gradient descent type algorithm, by leveraging the idea of residual learning and Nesterov's smoothing technique, to solve inverse problems consisting of general nonconvex and nonsmooth regularization with provable convergence. Moreover, we develop a neural network architecture intimating this algorithm to learn the nonlinear sparsity transformation adaptively from training data, which also inherits the convergence to accommodate the general nonconvex structure of this learned transformation. Numerical results demonstrate that the proposed network outperforms the state-of-the-art methods on a variety of different image reconstruction problems in terms of efficiency and accuracy.



### Learning hierarchical relationships for object-goal navigation
- **Arxiv ID**: http://arxiv.org/abs/2003.06749v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.06749v2)
- **Published**: 2020-03-15 04:01:09+00:00
- **Updated**: 2020-11-18 22:22:11+00:00
- **Authors**: Yiding Qiu, Anwesan Pal, Henrik I. Christensen
- **Comment**: Paper accepted at CoRL-2020
- **Journal**: None
- **Summary**: Direct search for objects as part of navigation poses a challenge for small items. Utilizing context in the form of object-object relationships enable hierarchical search for targets efficiently. Most of the current approaches tend to directly incorporate sensory input into a reward-based learning approach, without learning about object relationships in the natural environment, and thus generalize poorly across domains. We present Memory-utilized Joint hierarchical Object Learning for Navigation in Indoor Rooms (MJOLNIR), a target-driven navigation algorithm, which considers the inherent relationship between target objects, and the more salient contextual objects occurring in its surrounding. Extensive experiments conducted across multiple environment settings show an $82.9\%$ and $93.5\%$ gain over existing state-of-the-art navigation methods in terms of the success rate (SR), and success weighted by path length (SPL), respectively. We also show that our model learns to converge much faster than other algorithms, without suffering from the well-known overfitting problem. Additional details regarding the supplementary material and code are available at https://sites.google.com/eng.ucsd.edu/mjolnir.



### Learning 2D-3D Correspondences To Solve The Blind Perspective-n-Point Problem
- **Arxiv ID**: http://arxiv.org/abs/2003.06752v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.06752v1)
- **Published**: 2020-03-15 04:17:30+00:00
- **Updated**: 2020-03-15 04:17:30+00:00
- **Authors**: Liu Liu, Dylan Campbell, Hongdong Li, Dingfu Zhou, Xibin Song, Ruigang Yang
- **Comment**: A blind-PnP solver
- **Journal**: None
- **Summary**: Conventional absolute camera pose via a Perspective-n-Point (PnP) solver often assumes that the correspondences between 2D image pixels and 3D points are given. When the correspondences between 2D and 3D points are not known a priori, the task becomes the much more challenging blind PnP problem. This paper proposes a deep CNN model which simultaneously solves for both the 6-DoF absolute camera pose and 2D--3D correspondences. Our model comprises three neural modules connected in sequence. First, a two-stream PointNet-inspired network is applied directly to both the 2D image keypoints and the 3D scene points in order to extract discriminative point-wise features harnessing both local and contextual information. Second, a global feature matching module is employed to estimate a matchability matrix among all 2D--3D pairs. Third, the obtained matchability matrix is fed into a classification module to disambiguate inlier matches. The entire network is trained end-to-end, followed by a robust model fitting (P3P-RANSAC) at test time only to recover the 6-DoF camera pose. Extensive tests on both real and simulated data have shown that our method substantially outperforms existing approaches, and is capable of processing thousands of points a second with the state-of-the-art accuracy.



### MotionNet: Joint Perception and Motion Prediction for Autonomous Driving Based on Bird's Eye View Maps
- **Arxiv ID**: http://arxiv.org/abs/2003.06754v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.06754v1)
- **Published**: 2020-03-15 04:37:12+00:00
- **Updated**: 2020-03-15 04:37:12+00:00
- **Authors**: Pengxiang Wu, Siheng Chen, Dimitris Metaxas
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: The ability to reliably perceive the environmental states, particularly the existence of objects and their motion behavior, is crucial for autonomous driving. In this work, we propose an efficient deep model, called MotionNet, to jointly perform perception and motion prediction from 3D point clouds. MotionNet takes a sequence of LiDAR sweeps as input and outputs a bird's eye view (BEV) map, which encodes the object category and motion information in each grid cell. The backbone of MotionNet is a novel spatio-temporal pyramid network, which extracts deep spatial and temporal features in a hierarchical fashion. To enforce the smoothness of predictions over both space and time, the training of MotionNet is further regularized with novel spatial and temporal consistency losses. Extensive experiments show that the proposed method overall outperforms the state-of-the-arts, including the latest scene-flow- and 3D-object-detection-based methods. This indicates the potential value of the proposed method serving as a backup to the bounding-box-based system, and providing complementary information to the motion planner in autonomous driving. Code is available at https://github.com/pxiangwu/MotionNet.



### Channel Pruning Guided by Classification Loss and Feature Importance
- **Arxiv ID**: http://arxiv.org/abs/2003.06757v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.06757v1)
- **Published**: 2020-03-15 05:08:47+00:00
- **Updated**: 2020-03-15 05:08:47+00:00
- **Authors**: Jinyang Guo, Wanli Ouyang, Dong Xu
- **Comment**: AAAI2020
- **Journal**: None
- **Summary**: In this work, we propose a new layer-by-layer channel pruning method called Channel Pruning guided by classification Loss and feature Importance (CPLI). In contrast to the existing layer-by-layer channel pruning approaches that only consider how to reconstruct the features from the next layer, our approach additionally take the classification loss into account in the channel pruning process. We also observe that some reconstructed features will be removed at the next pruning stage. So it is unnecessary to reconstruct these features. To this end, we propose a new strategy to suppress the influence of unimportant features (i.e., the features will be removed at the next pruning stage). Our comprehensive experiments on three benchmark datasets, i.e., CIFAR-10, ImageNet, and UCF-101, demonstrate the effectiveness of our CPLI method.



### Siamese Box Adaptive Network for Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/2003.06761v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.06761v2)
- **Published**: 2020-03-15 05:58:12+00:00
- **Updated**: 2020-04-22 09:59:30+00:00
- **Authors**: Zedu Chen, Bineng Zhong, Guorong Li, Shengping Zhang, Rongrong Ji
- **Comment**: Accepted to CVPR 2020
- **Journal**: None
- **Summary**: Most of the existing trackers usually rely on either a multi-scale searching scheme or pre-defined anchor boxes to accurately estimate the scale and aspect ratio of a target. Unfortunately, they typically call for tedious and heuristic configurations. To address this issue, we propose a simple yet effective visual tracking framework (named Siamese Box Adaptive Network, SiamBAN) by exploiting the expressive power of the fully convolutional network (FCN). SiamBAN views the visual tracking problem as a parallel classification and regression problem, and thus directly classifies objects and regresses their bounding boxes in a unified FCN. The no-prior box design avoids hyper-parameters associated with the candidate boxes, making SiamBAN more flexible and general. Extensive experiments on visual tracking benchmarks including VOT2018, VOT2019, OTB100, NFS, UAV123, and LaSOT demonstrate that SiamBAN achieves state-of-the-art performance and runs at 40 FPS, confirming its effectiveness and efficiency. The code will be available at https://github.com/hqucv/siamban.



### DeepEMD: Differentiable Earth Mover's Distance for Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2003.06777v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.06777v5)
- **Published**: 2020-03-15 08:13:16+00:00
- **Updated**: 2023-03-30 10:48:54+00:00
- **Authors**: Chi Zhang, Yujun Cai, Guosheng Lin, Chunhua Shen
- **Comment**: DeepEMD V2, accepted by TPAMI
- **Journal**: None
- **Summary**: In this work, we develop methods for few-shot image classification from a new perspective of optimal matching between image regions. We employ the Earth Mover's Distance (EMD) as a metric to compute a structural distance between dense image representations to determine image relevance. The EMD generates the optimal matching flows between structural elements that have the minimum matching cost, which is used to calculate the image distance for classification. To generate the important weights of elements in the EMD formulation, we design a cross-reference mechanism, which can effectively alleviate the adverse impact caused by the cluttered background and large intra-class appearance variations. To implement k-shot classification, we propose to learn a structured fully connected layer that can directly classify dense image representations with the EMD. Based on the implicit function theorem, the EMD can be inserted as a layer into the network for end-to-end training. Our extensive experiments validate the effectiveness of our algorithm which outperforms state-of-the-art methods by a significant margin on five widely used few-shot classification benchmarks, namely, miniImageNet, tieredImageNet, Fewshot-CIFAR100 (FC100), Caltech-UCSD Birds-200-2011 (CUB), and CIFAR-FewShot (CIFAR-FS). We also demonstrate the effectiveness of our method on the image retrieval task in our experiments.



### A proto-object based audiovisual saliency map
- **Arxiv ID**: http://arxiv.org/abs/2003.06779v1
- **DOI**: None
- **Categories**: **eess.AS**, cs.CV, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/2003.06779v1)
- **Published**: 2020-03-15 08:34:35+00:00
- **Updated**: 2020-03-15 08:34:35+00:00
- **Authors**: Sudarshan Ramenahalli
- **Comment**: 50 pages, 12 figures
- **Journal**: None
- **Summary**: Natural environment and our interaction with it is essentially multisensory, where we may deploy visual, tactile and/or auditory senses to perceive, learn and interact with our environment. Our objective in this study is to develop a scene analysis algorithm using multisensory information, specifically vision and audio. We develop a proto-object based audiovisual saliency map (AVSM) for the analysis of dynamic natural scenes. A specialized audiovisual camera with $360 \degree$ Field of View, capable of locating sound direction, is used to collect spatiotemporally aligned audiovisual data. We demonstrate that the performance of proto-object based audiovisual saliency map in detecting and localizing salient objects/events is in agreement with human judgment. In addition, the proto-object based AVSM that we compute as a linear combination of visual and auditory feature conspicuity maps captures a higher number of valid salient events compared to unisensory saliency maps. Such an algorithm can be useful in surveillance, robotic navigation, video compression and related applications.



### Self-trained Deep Ordinal Regression for End-to-End Video Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2003.06780v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.06780v1)
- **Published**: 2020-03-15 08:44:55+00:00
- **Updated**: 2020-03-15 08:44:55+00:00
- **Authors**: Guansong Pang, Cheng Yan, Chunhua Shen, Anton van den Hengel, Xiao Bai
- **Comment**: Accepted to Proc. IEEE Conf. Computer Vision and Pattern Recognition
  2020
- **Journal**: None
- **Summary**: Video anomaly detection is of critical practical importance to a variety of real applications because it allows human attention to be focused on events that are likely to be of interest, in spite of an otherwise overwhelming volume of video. We show that applying self-trained deep ordinal regression to video anomaly detection overcomes two key limitations of existing methods, namely, 1) being highly dependent on manually labeled normal training data; and 2) sub-optimal feature learning. By formulating a surrogate two-class ordinal regression task we devise an end-to-end trainable video anomaly detection approach that enables joint representation learning and anomaly scoring without manually labeled normal/abnormal data. Experiments on eight real-world video scenes show that our proposed method outperforms state-of-the-art methods that require no labeled training data by a substantial margin, and enables easy and accurate localization of the identified anomalies. Furthermore, we demonstrate that our method offers effective human-in-the-loop anomaly detection which can be critical in applications where anomalies are rare and the false-negative cost is high.



### Hierarchical Models: Intrinsic Separability in High Dimensions
- **Arxiv ID**: http://arxiv.org/abs/2003.07770v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.07770v1)
- **Published**: 2020-03-15 09:27:24+00:00
- **Updated**: 2020-03-15 09:27:24+00:00
- **Authors**: Wen-Yan Lin
- **Comment**: None
- **Journal**: None
- **Summary**: It has long been noticed that high dimension data exhibits strange patterns. This has been variously interpreted as either a "blessing" or a "curse", causing uncomfortable inconsistencies in the literature. We propose that these patterns arise from an intrinsically hierarchical generative process. Modeling the process creates a web of constraints that reconcile many different theories and results. The model also implies high dimensional data posses an innate separability that can be exploited for machine learning. We demonstrate how this permits the open-set learning problem to be defined mathematically, leading to qualitative and quantitative improvements in performance.



### GMM-UNIT: Unsupervised Multi-Domain and Multi-Modal Image-to-Image Translation via Attribute Gaussian Mixture Modeling
- **Arxiv ID**: http://arxiv.org/abs/2003.06788v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.06788v2)
- **Published**: 2020-03-15 10:18:56+00:00
- **Updated**: 2020-03-21 22:15:26+00:00
- **Authors**: Yahui Liu, Marco De Nadai, Jian Yao, Nicu Sebe, Bruno Lepri, Xavier Alameda-Pineda
- **Comment**: 27 pages, 17 figures
- **Journal**: None
- **Summary**: Unsupervised image-to-image translation (UNIT) aims at learning a mapping between several visual domains by using unpaired training images. Recent studies have shown remarkable success for multiple domains but they suffer from two main limitations: they are either built from several two-domain mappings that are required to be learned independently, or they generate low-diversity results, a problem known as mode collapse. To overcome these limitations, we propose a method named GMM-UNIT, which is based on a content-attribute disentangled representation where the attribute space is fitted with a GMM. Each GMM component represents a domain, and this simple assumption has two prominent advantages. First, it can be easily extended to most multi-domain and multi-modal image-to-image translation tasks. Second, the continuous domain encoding allows for interpolation between domains and for extrapolation to unseen domains and translations. Additionally, we show how GMM-UNIT can be constrained down to different methods in the literature, meaning that GMM-UNIT is a unifying framework for unsupervised image-to-image translation.



### Learning Enriched Features for Real Image Restoration and Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2003.06792v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.06792v2)
- **Published**: 2020-03-15 11:04:30+00:00
- **Updated**: 2020-07-08 12:58:28+00:00
- **Authors**: Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Ming-Hsuan Yang, Ling Shao
- **Comment**: Accepted for publication at ECCV 2020
- **Journal**: None
- **Summary**: With the goal of recovering high-quality image content from its degraded version, image restoration enjoys numerous applications, such as in surveillance, computational photography, medical imaging, and remote sensing. Recently, convolutional neural networks (CNNs) have achieved dramatic improvements over conventional approaches for image restoration task. Existing CNN-based methods typically operate either on full-resolution or on progressively low-resolution representations. In the former case, spatially precise but contextually less robust results are achieved, while in the latter case, semantically reliable but spatially less accurate outputs are generated. In this paper, we present a novel architecture with the collective goals of maintaining spatially-precise high-resolution representations through the entire network and receiving strong contextual information from the low-resolution representations. The core of our approach is a multi-scale residual block containing several key elements: (a) parallel multi-resolution convolution streams for extracting multi-scale features, (b) information exchange across the multi-resolution streams, (c) spatial and channel attention mechanisms for capturing contextual information, and (d) attention based multi-scale feature aggregation. In a nutshell, our approach learns an enriched set of features that combines contextual information from multiple scales, while simultaneously preserving the high-resolution spatial details. Extensive experiments on five real image benchmark datasets demonstrate that our method, named as MIRNet, achieves state-of-the-art results for a variety of image processing tasks, including image denoising, super-resolution, and image enhancement. The source code and pre-trained models are available at https://github.com/swz30/MIRNet.



### Performance Evaluation of Advanced Deep Learning Architectures for Offline Handwritten Character Recognition
- **Arxiv ID**: http://arxiv.org/abs/2003.06794v1
- **DOI**: 10.1109/FIT.2017.00071
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.06794v1)
- **Published**: 2020-03-15 11:17:16+00:00
- **Updated**: 2020-03-15 11:17:16+00:00
- **Authors**: Moazam Soomro, Muhammad Ali Farooq, Rana Hammad Raza
- **Comment**: FIT 2017 Paper Published in IEEE FIT 2017
- **Journal**: None
- **Summary**: This paper presents a hand-written character recognition comparison and performance evaluation for robust and precise classification of different hand-written characters. The system utilizes advanced multilayer deep neural network by collecting features from raw pixel values. The hidden layers stack deep hierarchies of non-linear features since learning complex features from conventional neural networks is very challenging. Two state of the art deep learning architectures were used which includes Caffe AlexNet and GoogleNet models in NVIDIA DIGITS.The frameworks were trained and tested on two different datasets for incorporating diversity and complexity. One of them is the publicly available dataset i.e. Chars74K comprising of 7705 characters and has upper and lowercase English alphabets, along with numerical digits. While the other dataset created locally consists of 4320 characters. The local dataset consists of 62 classes and was created by 40 subjects. It also consists upper and lowercase English alphabets, along with numerical digits. The overall dataset is divided in the ratio of 80% for training and 20% for testing phase. The time required for training phase is approximately 90 minutes. For validation part, the results obtained were compared with the groundtruth. The accuracy level achieved with AlexNet was 77.77% and 88.89% with Google Net. The higher accuracy level of GoogleNet is due to its unique combination of inception modules, each including pooling, convolutions at various scales and concatenation procedures.



### StarNet: towards Weakly Supervised Few-Shot Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2003.06798v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.06798v3)
- **Published**: 2020-03-15 11:35:28+00:00
- **Updated**: 2020-09-17 11:37:25+00:00
- **Authors**: Leonid Karlinsky, Joseph Shtok, Amit Alfassy, Moshe Lichtenstein, Sivan Harary, Eli Schwartz, Sivan Doveh, Prasanna Sattigeri, Rogerio Feris, Alexander Bronstein, Raja Giryes
- **Comment**: None
- **Journal**: None
- **Summary**: Few-shot detection and classification have advanced significantly in recent years. Yet, detection approaches require strong annotation (bounding boxes) both for pre-training and for adaptation to novel classes, and classification approaches rarely provide localization of objects in the scene. In this paper, we introduce StarNet - a few-shot model featuring an end-to-end differentiable non-parametric star-model detection and classification head. Through this head, the backbone is meta-trained using only image-level labels to produce good features for jointly localizing and classifying previously unseen categories of few-shot test tasks using a star-model that geometrically matches between the query and support images (to find corresponding object instances). Being a few-shot detector, StarNet does not require any bounding box annotations, neither during pre-training nor for novel classes adaptation. It can thus be applied to the previously unexplored and challenging task of Weakly Supervised Few-Shot Object Detection (WS-FSOD), where it attains significant improvements over the baselines. In addition, StarNet shows significant gains on few-shot classification benchmarks that are less cropped around the objects (where object localization is key).



### OS2D: One-Stage One-Shot Object Detection by Matching Anchor Features
- **Arxiv ID**: http://arxiv.org/abs/2003.06800v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.06800v2)
- **Published**: 2020-03-15 11:39:47+00:00
- **Updated**: 2020-08-19 13:59:00+00:00
- **Authors**: Anton Osokin, Denis Sumin, Vasily Lomakin
- **Comment**: Published at ECCV 2020
- **Journal**: None
- **Summary**: In this paper, we consider the task of one-shot object detection, which consists in detecting objects defined by a single demonstration. Differently from the standard object detection, the classes of objects used for training and testing do not overlap. We build the one-stage system that performs localization and recognition jointly. We use dense correlation matching of learned local features to find correspondences, a feed-forward geometric transformation model to align features and bilinear resampling of the correlation tensor to compute the detection score of the aligned features. All the components are differentiable, which allows end-to-end training. Experimental evaluation on several challenging domains (retail products, 3D objects, buildings and logos) shows that our method can detect unseen classes (e.g., toothpaste when trained on groceries) and outperforms several baselines by a significant margin. Our code is available online: https://github.com/aosokin/os2d .



### Experimenting with Convolutional Neural Network Architectures for the automatic characterization of Solitary Pulmonary Nodules' malignancy rating
- **Arxiv ID**: http://arxiv.org/abs/2003.06801v1
- **DOI**: 10.5281/zenodo.3711239
- **Categories**: **eess.IV**, cs.CV, physics.data-an, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2003.06801v1)
- **Published**: 2020-03-15 11:46:00+00:00
- **Updated**: 2020-03-15 11:46:00+00:00
- **Authors**: Ioannis D. Apostolopoulos
- **Comment**: 22 pages. arXiv admin note: text overlap with arXiv:1409.4842,
  arXiv:1512.07108 by other authors
- **Journal**: None
- **Summary**: Lung Cancer is the most common cause of cancer-related death worldwide. Early and automatic diagnosis of Solitary Pulmonary Nodules (SPN) in Computer Tomography (CT) chest scans can provide early treatment as well as doctor liberation from time-consuming procedures. Deep Learning has been proven as a popular and influential method in many medical imaging diagnosis areas. In this study, we consider the problem of diagnostic classification between benign and malignant lung nodules in CT images derived from a PET/CT scanner. More specifically, we intend to develop experimental Convolutional Neural Network (CNN) architectures and conduct experiments, by tuning their parameters, to investigate their behavior, and to define the optimal setup for the accurate classification. For the experiments, we utilize PET/CT images obtained from the Laboratory of Nuclear Medicine of the University of Patras, and the publically available database called Lung Image Database Consortium Image Collection (LIDC-IDRI). Furthermore, we apply simple data augmentation to generate new instances and to inspect the performance of the developed networks. Classification accuracy of 91% and 93% on the PET/CT dataset and on a selection of nodule images form the LIDC-IDRI dataset, is achieved accordingly. The results demonstrate that CNNs are a trustworth method for nodule classification. Also, the experiment confirms that data augmentation enhances the robustness of the CNNs.



### Iterative training of neural networks for intra prediction
- **Arxiv ID**: http://arxiv.org/abs/2003.06812v2
- **DOI**: 10.1109/TIP.2020.3038348
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.06812v2)
- **Published**: 2020-03-15 12:29:51+00:00
- **Updated**: 2020-11-25 07:46:42+00:00
- **Authors**: Thierry Dumas, Franck Galpin, Philippe Bordes
- **Comment**: 15 pages, 16 figures
- **Journal**: None
- **Summary**: This paper presents an iterative training of neural networks for intra prediction in a block-based image and video codec. First, the neural networks are trained on blocks arising from the codec partitioning of images, each paired with its context. Then, iteratively, blocks are collected from the partitioning of images via the codec including the neural networks trained at the previous iteration, each paired with its context, and the neural networks are retrained on the new pairs. Thanks to this training, the neural networks can learn intra prediction functions that both stand out from those already in the initial codec and boost the codec in terms of rate-distortion. Moreover, the iterative process allows the design of training data cleansings essential for the neural network training. When the iteratively trained neural networks are put into H.265 (HM-16.15), -4.2% of mean dB-rate reduction is obtained. By moving them into H.266 (VTM-5.0), the mean dB-rate reduction reaches -1.9%.



### Towards Face Encryption by Generating Adversarial Identity Masks
- **Arxiv ID**: http://arxiv.org/abs/2003.06814v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.06814v2)
- **Published**: 2020-03-15 12:45:10+00:00
- **Updated**: 2021-08-16 05:52:55+00:00
- **Authors**: Xiao Yang, Yinpeng Dong, Tianyu Pang, Hang Su, Jun Zhu, Yuefeng Chen, Hui Xue
- **Comment**: Accepted by ICCV2021
- **Journal**: None
- **Summary**: As billions of personal data being shared through social media and network, the data privacy and security have drawn an increasing attention. Several attempts have been made to alleviate the leakage of identity information from face photos, with the aid of, e.g., image obfuscation techniques. However, most of the present results are either perceptually unsatisfactory or ineffective against face recognition systems. Our goal in this paper is to develop a technique that can encrypt the personal photos such that they can protect users from unauthorized face recognition systems but remain visually identical to the original version for human beings. To achieve this, we propose a targeted identity-protection iterative method (TIP-IM) to generate adversarial identity masks which can be overlaid on facial images, such that the original identities can be concealed without sacrificing the visual quality. Extensive experiments demonstrate that TIP-IM provides 95\%+ protection success rate against various state-of-the-art face recognition models under practical test scenarios. Besides, we also show the practical and effective applicability of our method on a commercial API service.



### VCNet: A Robust Approach to Blind Image Inpainting
- **Arxiv ID**: http://arxiv.org/abs/2003.06816v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.06816v1)
- **Published**: 2020-03-15 12:47:57+00:00
- **Updated**: 2020-03-15 12:47:57+00:00
- **Authors**: Yi Wang, Ying-Cong Chen, Xin Tao, Jiaya Jia
- **Comment**: None
- **Journal**: None
- **Summary**: Blind inpainting is a task to automatically complete visual contents without specifying masks for missing areas in an image. Previous works assume missing region patterns are known, limiting its application scope. In this paper, we relax the assumption by defining a new blind inpainting setting, making training a blind inpainting neural system robust against various unknown missing region patterns. Specifically, we propose a two-stage visual consistency network (VCN), meant to estimate where to fill (via masks) and generate what to fill. In this procedure, the unavoidable potential mask prediction errors lead to severe artifacts in the subsequent repairing. To address it, our VCN predicts semantically inconsistent regions first, making mask prediction more tractable. Then it repairs these estimated missing regions using a new spatial normalization, enabling VCN to be robust to the mask prediction errors. In this way, semantically convincing and visually compelling content is thus generated. Extensive experiments are conducted, showing our method is effective and robust in blind image inpainting. And our VCN allows for a wide spectrum of applications.



### Intra Order-preserving Functions for Calibration of Multi-Class Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2003.06820v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.06820v2)
- **Published**: 2020-03-15 12:57:21+00:00
- **Updated**: 2020-10-23 06:59:28+00:00
- **Authors**: Amir Rahimi, Amirreza Shaban, Ching-An Cheng, Richard Hartley, Byron Boots
- **Comment**: NeurIPS 2020
- **Journal**: None
- **Summary**: Predicting calibrated confidence scores for multi-class deep networks is important for avoiding rare but costly mistakes. A common approach is to learn a post-hoc calibration function that transforms the output of the original network into calibrated confidence scores while maintaining the network's accuracy. However, previous post-hoc calibration techniques work only with simple calibration functions, potentially lacking sufficient representation to calibrate the complex function landscape of deep networks. In this work, we aim to learn general post-hoc calibration functions that can preserve the top-k predictions of any deep network. We call this family of functions intra order-preserving functions. We propose a new neural network architecture that represents a class of intra order-preserving functions by combining common neural network components. Additionally, we introduce order-invariant and diagonal sub-families, which can act as regularization for better generalization when the training data size is small. We show the effectiveness of the proposed method across a wide range of datasets and classifiers. Our method outperforms state-of-the-art post-hoc calibration methods, namely temperature scaling and Dirichlet calibration, in several evaluation metrics for the task.



### FGSD: A Dataset for Fine-Grained Ship Detection in High Resolution Satellite Images
- **Arxiv ID**: http://arxiv.org/abs/2003.06832v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.06832v1)
- **Published**: 2020-03-15 13:54:20+00:00
- **Updated**: 2020-03-15 13:54:20+00:00
- **Authors**: Kaiyan Chen, Ming Wu, Jiaming Liu, Chuang Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Ship detection using high-resolution remote sensing images is an important task, which contribute to sea surface regulation. The complex background and special visual angle make ship detection relies in high quality datasets to a certain extent. However, there is few works on giving both precise classification and accurate location of ships in existing ship detection datasets. To further promote the research of ship detection, we introduced a new fine-grained ship detection datasets, which is named as FGSD. The dataset collects high-resolution remote sensing images that containing ship samples from multiple large ports around the world. Ship samples were fine categorized and annotated with both horizontal and rotating bounding boxes. To further detailed the information of the dataset, we put forward a new representation method of ships' orientation. For future research, the dock as a new class was annotated in the dataset. Besides, rich information of images were provided in FGSD, including the source port, resolution and corresponding GoogleEarth' s resolution level of each image. As far as we know, FGSD is the most comprehensive ship detection dataset currently and it'll be available soon. Some baselines for FGSD are also provided in this paper.



### Energy-based Periodicity Mining with Deep Features for Action Repetition Counting in Unconstrained Videos
- **Arxiv ID**: http://arxiv.org/abs/2003.06838v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.06838v1)
- **Published**: 2020-03-15 14:21:18+00:00
- **Updated**: 2020-03-15 14:21:18+00:00
- **Authors**: Jianqin Yin, Yanchun Wu, Huaping Liu, Yonghao Dang, Zhiyi Liu, Jun Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Action repetition counting is to estimate the occurrence times of the repetitive motion in one action, which is a relatively new, important but challenging measurement problem. To solve this problem, we propose a new method superior to the traditional ways in two aspects, without preprocessing and applicable for arbitrary periodicity actions. Without preprocessing, the proposed model makes our method convenient for real applications; processing the arbitrary periodicity action makes our model more suitable for the actual circumstance. In terms of methodology, firstly, we analyze the movement patterns of the repetitive actions based on the spatial and temporal features of actions extracted by deep ConvNets; Secondly, the Principal Component Analysis algorithm is used to generate the intuitive periodic information from the chaotic high-dimensional deep features; Thirdly, the periodicity is mined based on the high-energy rule using Fourier transform; Finally, the inverse Fourier transform with a multi-stage threshold filter is proposed to improve the quality of the mined periodicity, and peak detection is introduced to finish the repetition counting. Our work features two-fold: 1) An important insight that deep features extracted for action recognition can well model the self-similarity periodicity of the repetitive action is presented. 2) A high-energy based periodicity mining rule using deep features is presented, which can process arbitrary actions without preprocessing. Experimental results show that our method achieves comparable results on the public datasets YT Segments and QUVA.



### 3D-CariGAN: An End-to-End Solution to 3D Caricature Generation from Face Photos
- **Arxiv ID**: http://arxiv.org/abs/2003.06841v2
- **DOI**: 10.1109/TVCG.2021.3126659
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.06841v2)
- **Published**: 2020-03-15 14:42:15+00:00
- **Updated**: 2021-11-13 01:27:47+00:00
- **Authors**: Zipeng Ye, Mengfei Xia, Yanan Sun, Ran Yi, Minjing Yu, Juyong Zhang, Yu-Kun Lai, Yong-jin Liu
- **Comment**: Accepted by IEEE Transactions on Visualization and Computer Graphics
- **Journal**: None
- **Summary**: Caricature is a type of artistic style of human faces that attracts considerable attention in the entertainment industry. So far a few 3D caricature generation methods exist and all of them require some caricature information (e.g., a caricature sketch or 2D caricature) as input. This kind of input, however, is difficult to provide by non-professional users. In this paper, we propose an end-to-end deep neural network model that generates high-quality 3D caricatures directly from a normal 2D face photo. The most challenging issue for our system is that the source domain of face photos (characterized by normal 2D faces) is significantly different from the target domain of 3D caricatures (characterized by 3D exaggerated face shapes and textures). To address this challenge, we: (1) build a large dataset of 5,343 3D caricature meshes and use it to establish a PCA model in the 3D caricature shape space; (2) reconstruct a normal full 3D head from the input face photo and use its PCA representation in the 3D caricature shape space to establish correspondences between the input photo and 3D caricature shape; and (3) propose a novel character loss and a novel caricature loss based on previous psychological studies on caricatures. Experiments including a novel two-level user study show that our system can generate high-quality 3D caricatures directly from normal face photos.



### SF-Net: Single-Frame Supervision for Temporal Action Localization
- **Arxiv ID**: http://arxiv.org/abs/2003.06845v6
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.06845v6)
- **Published**: 2020-03-15 15:06:01+00:00
- **Updated**: 2020-08-15 04:20:57+00:00
- **Authors**: Fan Ma, Linchao Zhu, Yi Yang, Shengxin Zha, Gourab Kundu, Matt Feiszli, Zheng Shou
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: In this paper, we study an intermediate form of supervision, i.e., single-frame supervision, for temporal action localization (TAL). To obtain the single-frame supervision, the annotators are asked to identify only a single frame within the temporal window of an action. This can significantly reduce the labor cost of obtaining full supervision which requires annotating the action boundary. Compared to the weak supervision that only annotates the video-level label, the single-frame supervision introduces extra temporal action signals while maintaining low annotation overhead. To make full use of such single-frame supervision, we propose a unified system called SF-Net. First, we propose to predict an actionness score for each video frame. Along with a typical category score, the actionness score can provide comprehensive information about the occurrence of a potential action and aid the temporal boundary refinement during inference. Second, we mine pseudo action and background frames based on the single-frame annotations. We identify pseudo action frames by adaptively expanding each annotated single frame to its nearby, contextual frames and we mine pseudo background frames from all the unannotated frames across multiple videos. Together with the ground-truth labeled frames, these pseudo-labeled frames are further used for training the classifier. In extensive experiments on THUMOS14, GTEA, and BEOID, SF-Net significantly improves upon state-of-the-art weakly-supervised methods in terms of both segment localization and single-frame localization. Notably, SF-Net achieves comparable results to its fully-supervised counterpart which requires much more resource intensive annotations. The code is available at https://github.com/Flowerfan/SF-Net.



### Deep Affinity Net: Instance Segmentation via Affinity
- **Arxiv ID**: http://arxiv.org/abs/2003.06849v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.06849v1)
- **Published**: 2020-03-15 15:22:56+00:00
- **Updated**: 2020-03-15 15:22:56+00:00
- **Authors**: Xingqian Xu, Mang Tik Chiu, Thomas S. Huang, Honghui Shi
- **Comment**: None
- **Journal**: None
- **Summary**: Most of the modern instance segmentation approaches fall into two categories: region-based approaches in which object bounding boxes are detected first and later used in cropping and segmenting instances; and keypoint-based approaches in which individual instances are represented by a set of keypoints followed by a dense pixel clustering around those keypoints. Despite the maturity of these two paradigms, we would like to report an alternative affinity-based paradigm where instances are segmented based on densely predicted affinities and graph partitioning algorithms. Such affinity-based approaches indicate that high-level graph features other than regions or keypoints can be directly applied in the instance segmentation task. In this work, we propose Deep Affinity Net, an effective affinity-based approach accompanied with a new graph partitioning algorithm Cascade-GAEC. Without bells and whistles, our end-to-end model results in 32.4% AP on Cityscapes val and 27.5% AP on test. It achieves the best single-shot result as well as the fastest running time among all affinity-based models. It also outperforms the region-based method Mask R-CNN.



### Multistage Curvilinear Coordinate Transform Based Document Image Dewarping using a Novel Quality Estimator
- **Arxiv ID**: http://arxiv.org/abs/2003.06872v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.06872v1)
- **Published**: 2020-03-15 17:17:53+00:00
- **Updated**: 2020-03-15 17:17:53+00:00
- **Authors**: Tanmoy Dasgupta, Nibaran Das, Mita Nasipuri
- **Comment**: None
- **Journal**: None
- **Summary**: The present work demonstrates a fast and improved technique for dewarping nonlinearly warped document images. The images are first dewarped at the page-level by estimating optimum inverse projections using curvilinear homography. The quality of the process is then estimated by evaluating a set of metrics related to the characteristics of the text lines and rectilinear objects for measuring parallelism, orthogonality, etc. These are designed specifically to estimate the quality of the dewarping process without the need of any ground truth. If the quality is estimated to be unsatisfactory, the page-level dewarping process is repeated with finer approximations. This is followed by a line-level dewarping process that makes granular corrections to the warps in individual text-lines. The methodology has been tested on the CBDAR 2007 / IUPR 2011 document image dewarping dataset and is seen to yield the best OCR accuracy in the shortest amount of time, till date. The usefulness of the methodology has also been evaluated on the DocUNet 2018 dataset with some minor tweaks, and is seen to produce comparable results.



### Guidance and Evaluation: Semantic-Aware Image Inpainting for Mixed Scenes
- **Arxiv ID**: http://arxiv.org/abs/2003.06877v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.06877v3)
- **Published**: 2020-03-15 17:49:20+00:00
- **Updated**: 2020-07-10 10:58:49+00:00
- **Authors**: Liang Liao, Jing Xiao, Zheng Wang, Chia-Wen Lin, Shin'ichi Satoh
- **Comment**: None
- **Journal**: None
- **Summary**: Completing a corrupted image with correct structures and reasonable textures for a mixed scene remains an elusive challenge. Since the missing hole in a mixed scene of a corrupted image often contains various semantic information, conventional two-stage approaches utilizing structural information often lead to the problem of unreliable structural prediction and ambiguous image texture generation. In this paper, we propose a Semantic Guidance and Evaluation Network (SGE-Net) to iteratively update the structural priors and the inpainted image in an interplay framework of semantics extraction and image inpainting. It utilizes semantic segmentation map as guidance in each scale of inpainting, under which location-dependent inferences are re-evaluated, and, accordingly, poorly-inferred regions are refined in subsequent scales. Extensive experiments on real-world images of mixed scenes demonstrated the superiority of our proposed method over state-of-the-art approaches, in terms of clear boundaries and photo-realistic textures.



### Diversity can be Transferred: Output Diversification for White- and Black-box Attacks
- **Arxiv ID**: http://arxiv.org/abs/2003.06878v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.06878v3)
- **Published**: 2020-03-15 17:49:25+00:00
- **Updated**: 2020-10-30 00:12:48+00:00
- **Authors**: Yusuke Tashiro, Yang Song, Stefano Ermon
- **Comment**: NeurIPS 2020
- **Journal**: None
- **Summary**: Adversarial attacks often involve random perturbations of the inputs drawn from uniform or Gaussian distributions, e.g., to initialize optimization-based white-box attacks or generate update directions in black-box attacks. These simple perturbations, however, could be sub-optimal as they are agnostic to the model being attacked. To improve the efficiency of these attacks, we propose Output Diversified Sampling (ODS), a novel sampling strategy that attempts to maximize diversity in the target model's outputs among the generated samples. While ODS is a gradient-based strategy, the diversity offered by ODS is transferable and can be helpful for both white-box and black-box attacks via surrogate models. Empirically, we demonstrate that ODS significantly improves the performance of existing white-box and black-box attacks. In particular, ODS reduces the number of queries needed for state-of-the-art black-box attacks on ImageNet by a factor of two.



### Night-time Scene Parsing with a Large Real Dataset
- **Arxiv ID**: http://arxiv.org/abs/2003.06883v3
- **DOI**: 10.1109/TIP.2021.3122004
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.06883v3)
- **Published**: 2020-03-15 18:11:34+00:00
- **Updated**: 2022-04-01 05:40:06+00:00
- **Authors**: Xin Tan, Ke Xu, Ying Cao, Yiheng Zhang, Lizhuang Ma, Rynson W. H. Lau
- **Comment**: 13 pages, 11 figures. This paper is accepted by IEEE Transactions on
  Image Processing. The dataset can be accessed via
  https://dmcv.sjtu.edu.cn/people/phd/tanxin/NightCity/index.html
- **Journal**: None
- **Summary**: Although huge progress has been made on scene analysis in recent years, most existing works assume the input images to be in day-time with good lighting conditions. In this work, we aim to address the night-time scene parsing (NTSP) problem, which has two main challenges: 1) labeled night-time data are scarce, and 2) over- and under-exposures may co-occur in the input night-time images and are not explicitly modeled in existing pipelines. To tackle the scarcity of night-time data, we collect a novel labeled dataset, named {\it NightCity}, of 4,297 real night-time images with ground truth pixel-level semantic annotations. To our knowledge, NightCity is the largest dataset for NTSP. In addition, we also propose an exposure-aware framework to address the NTSP problem through augmenting the segmentation process with explicitly learned exposure features. Extensive experiments show that training on NightCity can significantly improve NTSP performances and that our exposure-aware model outperforms the state-of-the-art methods, yielding top performances on our dataset as well as existing datasets.



### Evaluation of Rounding Functions in Nearest-Neighbor Interpolation
- **Arxiv ID**: http://arxiv.org/abs/2003.06885v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GT
- **Links**: [PDF](http://arxiv.org/pdf/2003.06885v2)
- **Published**: 2020-03-15 18:17:36+00:00
- **Updated**: 2021-01-22 19:10:17+00:00
- **Authors**: Olivier Rukundo
- **Comment**: 9 pages, 8 figures, 9 tables
- **Journal**: None
- **Summary**: A novel evaluation study of the most appropriate round function for nearest-neighbor (NN) image interpolation is presented. Evaluated rounding functions are selected among the five rounding rules defined by the Institute of Electrical and Electronics Engineers (IEEE) 754-2008 standard. Both full- and no-reference image quality assessment (IQA) metrics are used to study and evaluate the influence of rounding functions on NN interpolation image quality. The concept of achieved occurrences over targeted occurrences is used to determine the percentage of achieved occurrences based on the number of test images used. Inferential statistical analysis is applied to deduce from a small number of images and draw a conclusion of the behavior of each rounding function on a bigger number of images. Under the normal distribution and at the level of confidence equals to 95%, the maximum and minimum achievable occurrences by each evaluated rounding function are both provided based on the inferential analysis-based experiments.



### Self-Constructing Graph Convolutional Networks for Semantic Labeling
- **Arxiv ID**: http://arxiv.org/abs/2003.06932v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.06932v2)
- **Published**: 2020-03-15 21:55:24+00:00
- **Updated**: 2020-04-23 13:44:08+00:00
- **Authors**: Qinghui Liu, Michael Kampffmeyer, Robert Jenssen, Arnt-Børre Salberg
- **Comment**: IGARSS-2020, code at: github.com/samleoqh/MSCG-Net
- **Journal**: None
- **Summary**: Graph Neural Networks (GNNs) have received increasing attention in many fields. However, due to the lack of prior graphs, their use for semantic labeling has been limited. Here, we propose a novel architecture called the Self-Constructing Graph (SCG), which makes use of learnable latent variables to generate embeddings and to self-construct the underlying graphs directly from the input features without relying on manually built prior knowledge graphs. SCG can automatically obtain optimized non-local context graphs from complex-shaped objects in aerial imagery. We optimize SCG via an adaptive diagonal enhancement method and a variational lower bound that consists of a customized graph reconstruction term and a Kullback-Leibler divergence regularization term. We demonstrate the effectiveness and flexibility of the proposed SCG on the publicly available ISPRS Vaihingen dataset and our model SCG-Net achieves competitive results in terms of F1-score with much fewer parameters and at a lower computational cost compared to related pure-CNN based work. Our code will be made public soon.



### Hyperspectral-Multispectral Image Fusion with Weighted LASSO
- **Arxiv ID**: http://arxiv.org/abs/2003.06944v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.06944v1)
- **Published**: 2020-03-15 23:07:56+00:00
- **Updated**: 2020-03-15 23:07:56+00:00
- **Authors**: Nguyen Tran, Rupali Mankar, David Mayerich, Zhu Han
- **Comment**: None
- **Journal**: None
- **Summary**: Spectral imaging enables spatially-resolved identification of materials in remote sensing, biomedicine, and astronomy. However, acquisition times require balancing spectral and spatial resolution with signal-to-noise. Hyperspectral imaging provides superior material specificity, while multispectral images are faster to collect at greater fidelity. We propose an approach for fusing hyperspectral and multispectral images to provide high-quality hyperspectral output. The proposed optimization leverages the least absolute shrinkage and selection operator (LASSO) to perform variable selection and regularization. Computational time is reduced by applying the alternating direction method of multipliers (ADMM), as well as initializing the fusion image by estimating it using maximum a posteriori (MAP) based on Hardie's method. We demonstrate that the proposed sparse fusion and reconstruction provides quantitatively superior results when compared to existing methods on publicly available images. Finally, we show how the proposed method can be practically applied in biomedical infrared spectroscopic microscopy.



### Scene Completeness-Aware Lidar Depth Completion for Driving Scenario
- **Arxiv ID**: http://arxiv.org/abs/2003.06945v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2003.06945v3)
- **Published**: 2020-03-15 23:23:26+00:00
- **Updated**: 2021-02-20 23:50:16+00:00
- **Authors**: Cho-Ying Wu, Ulrich Neumann
- **Comment**: Present at ICASSP 2021
- **Journal**: None
- **Summary**: This paper introduces Scene Completeness-Aware Depth Completion (SCADC) to complete raw lidar scans into dense depth maps with fine and complete scene structures. Recent sparse depth completion for lidars only focuses on the lower scenes and produces irregular estimations on the upper because existing datasets, such as KITTI, do not provide groundtruth for upper areas. These areas are considered less important since they are usually sky or trees of less scene understanding interest. However, we argue that in several driving scenarios such as large trucks or cars with loads, objects could extend to the upper parts of scenes. Thus depth maps with structured upper scene estimation are important for RGBD algorithms. SCADC adopts stereo images that produce disparities with better scene completeness but are generally less precise than lidars, to help sparse lidar depth completion. To our knowledge, we are the first to focus on scene completeness of sparse depth completion. We validate our SCADC on both depth estimate precision and scene-completeness on KITTI. Moreover, we experiment on less-explored outdoor RGBD semantic segmentation with scene completeness-aware D-input to validate our method.



