# Arxiv Papers in cs.CV on 2020-03-23
### Additive Angular Margin for Few Shot Learning to Classify Clinical Endoscopy Images
- **Arxiv ID**: http://arxiv.org/abs/2003.10033v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.10033v2)
- **Published**: 2020-03-23 00:20:52+00:00
- **Updated**: 2020-03-26 20:28:04+00:00
- **Authors**: Sharib Ali, Binod Bhattarai, Tae-Kyun Kim, Jens Rittscher
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Endoscopy is a widely used imaging modality to diagnose and treat diseases in hollow organs as for example the gastrointestinal tract, the kidney and the liver. However, due to varied modalities and use of different imaging protocols at various clinical centers impose significant challenges when generalising deep learning models. Moreover, the assembly of large datasets from different clinical centers can introduce a huge label bias that renders any learnt model unusable. Also, when using new modality or presence of images with rare patterns, a bulk amount of similar image data and their corresponding labels are required for training these models. In this work, we propose to use a few-shot learning approach that requires less training data and can be used to predict label classes of test samples from an unseen dataset. We propose a novel additive angular margin metric in the framework of prototypical network in few-shot learning setting. We compare our approach to the several established methods on a large cohort of multi-center, multi-organ, and multi-modal endoscopy data. The proposed algorithm outperforms existing state-of-the-art methods.



### Understanding the robustness of deep neural network classifiers for breast cancer screening
- **Arxiv ID**: http://arxiv.org/abs/2003.10041v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.10041v1)
- **Published**: 2020-03-23 01:26:36+00:00
- **Updated**: 2020-03-23 01:26:36+00:00
- **Authors**: Witold Oleszkiewicz, Taro Makino, Stanisław Jastrzębski, Tomasz Trzciński, Linda Moy, Kyunghyun Cho, Laura Heacock, Krzysztof J. Geras
- **Comment**: Accepted as a workshop paper at AI4AH, ICLR 2020
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) show promise in breast cancer screening, but their robustness to input perturbations must be better understood before they can be clinically implemented. There exists extensive literature on this subject in the context of natural images that can potentially be built upon. However, it cannot be assumed that conclusions about robustness will transfer from natural images to mammogram images, due to significant differences between the two image modalities. In order to determine whether conclusions will transfer, we measure the sensitivity of a radiologist-level screening mammogram image classifier to four commonly studied input perturbations that natural image classifiers are sensitive to. We find that mammogram image classifiers are also sensitive to these perturbations, which suggests that we can build on the existing literature. We also perform a detailed analysis on the effects of low-pass filtering, and find that it degrades the visibility of clinically meaningful features called microcalcifications. Since low-pass filtering removes semantically meaningful information that is predictive of breast cancer, we argue that it is undesirable for mammogram image classifiers to be invariant to it. This is in contrast to natural images, where we do not want DNNs to be sensitive to low-pass filtering due to its tendency to remove information that is human-incomprehensible.



### Bridge the Domain Gap Between Ultra-wide-field and Traditional Fundus Images via Adversarial Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2003.10042v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.10042v2)
- **Published**: 2020-03-23 01:31:39+00:00
- **Updated**: 2020-03-24 01:42:22+00:00
- **Authors**: Lie Ju, Xin Wang, Quan Zhou, Hu Zhu, Mehrtash Harandi, Paul Bonnington, Tom Drummond, Zongyuan Ge
- **Comment**: None
- **Journal**: None
- **Summary**: For decades, advances in retinal imaging technology have enabled effective diagnosis and management of retinal disease using fundus cameras. Recently, ultra-wide-field (UWF) fundus imaging by Optos camera is gradually put into use because of its broader insights on fundus for some lesions that are not typically seen in traditional fundus images. Research on traditional fundus images is an active topic but studies on UWF fundus images are few. One of the most important reasons is that UWF fundus images are hard to obtain. In this paper, for the first time, we explore domain adaptation from the traditional fundus to UWF fundus images. We propose a flexible framework to bridge the domain gap between two domains and co-train a UWF fundus diagnosis model by pseudo-labelling and adversarial learning. We design a regularisation technique to regulate the domain adaptation. Also, we apply MixUp to overcome the over-fitting issue from incorrect generated pseudo-labels. Our experimental results on either single or both domains demonstrate that the proposed method can well adapt and transfer the knowledge from traditional fundus images to UWF fundus images and improve the performance of retinal disease recognition.



### Architectural Resilience to Foreground-and-Background Adversarial Noise
- **Arxiv ID**: http://arxiv.org/abs/2003.10045v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.10045v2)
- **Published**: 2020-03-23 01:38:20+00:00
- **Updated**: 2020-06-07 05:28:09+00:00
- **Authors**: Carl Cheng, Evan Hu
- **Comment**: 9 pages, 8 figures; updated email addresses
- **Journal**: None
- **Summary**: Adversarial attacks in the form of imperceptible perturbations of normal images have been extensively studied, and for every new defense methodology created, multiple adversarial attacks are found to counteract it. In particular, a popular style of attack, exemplified in recent years by DeepFool and Carlini-Wagner, relies solely on white-box scenarios in which full access to the predictive model and its weights are required. In this work, we instead propose distinct model-agnostic benchmark perturbations of images in order to investigate the resilience and robustness of different network architectures. Results empirically determine that increasing depth within most types of Convolutional Neural Networks typically improves model resilience towards general attacks, with improvement steadily decreasing as the model becomes deeper. Additionally, we find that a notable difference in adversarial robustness exists between residual architectures with skip connections and non-residual architectures of similar complexity. Our findings provide direction for future understanding of residual connections and depth on network robustness.



### Pix2Shape: Towards Unsupervised Learning of 3D Scenes from Images using a View-based Representation
- **Arxiv ID**: http://arxiv.org/abs/2003.14166v2
- **DOI**: 10.1007/s11263-020-01322-1
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.14166v2)
- **Published**: 2020-03-23 03:01:34+00:00
- **Updated**: 2020-04-17 13:22:58+00:00
- **Authors**: Sai Rajeswar, Fahim Mannan, Florian Golemo, Jérôme Parent-Lévesque, David Vazquez, Derek Nowrouzezahrai, Aaron Courville
- **Comment**: This is a pre-print of an article published in International Journal
  of Computer Vision. The final authenticated version is available online at:
  https://doi.org/10.1007/s11263-020-01322-1
- **Journal**: International Journal of Computer Vision, (2020), 1-16
- **Summary**: We infer and generate three-dimensional (3D) scene information from a single input image and without supervision. This problem is under-explored, with most prior work relying on supervision from, e.g., 3D ground-truth, multiple images of a scene, image silhouettes or key-points. We propose Pix2Shape, an approach to solve this problem with four components: (i) an encoder that infers the latent 3D representation from an image, (ii) a decoder that generates an explicit 2.5D surfel-based reconstruction of a scene from the latent code (iii) a differentiable renderer that synthesizes a 2D image from the surfel representation, and (iv) a critic network trained to discriminate between images generated by the decoder-renderer and those from a training distribution. Pix2Shape can generate complex 3D scenes that scale with the view-dependent on-screen resolution, unlike representations that capture world-space resolution, i.e., voxels or meshes. We show that Pix2Shape learns a consistent scene representation in its encoded latent space and that the decoder can then be applied to this latent representation in order to synthesize the scene from a novel viewpoint. We evaluate Pix2Shape with experiments on the ShapeNet dataset as well as on a novel benchmark we developed, called 3D-IQTT, to evaluate models based on their ability to enable 3d spatial reasoning. Qualitative and quantitative evaluation demonstrate Pix2Shape's ability to solve scene reconstruction, generation, and understanding tasks.



### Fast(er) Reconstruction of Shredded Text Documents via Self-Supervised Deep Asymmetric Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/2003.10063v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.10063v4)
- **Published**: 2020-03-23 03:22:06+00:00
- **Updated**: 2020-04-29 00:21:31+00:00
- **Authors**: Thiago M. Paixão, Rodrigo F. Berriel, Maria C. S. Boeres, Alessando L. Koerich, Claudine Badue, Alberto F. De Souza, Thiago Oliveira-Santos
- **Comment**: Accepted to CVPR 2020. Main Paper (9 pages, 10 figures) and
  Supplementary Material (5 pages, 9 figures)
- **Journal**: None
- **Summary**: The reconstruction of shredded documents consists in arranging the pieces of paper (shreds) in order to reassemble the original aspect of such documents. This task is particularly relevant for supporting forensic investigation as documents may contain criminal evidence. As an alternative to the laborious and time-consuming manual process, several researchers have been investigating ways to perform automatic digital reconstruction. A central problem in automatic reconstruction of shredded documents is the pairwise compatibility evaluation of the shreds, notably for binary text documents. In this context, deep learning has enabled great progress for accurate reconstructions in the domain of mechanically-shredded documents. A sensitive issue, however, is that current deep model solutions require an inference whenever a pair of shreds has to be evaluated. This work proposes a scalable deep learning approach for measuring pairwise compatibility in which the number of inferences scales linearly (rather than quadratically) with the number of shreds. Instead of predicting compatibility directly, deep models are leveraged to asymmetrically project the raw shred content onto a common metric space in which distance is proportional to the compatibility. Experimental results show that our method has accuracy comparable to the state-of-the-art with a speed-up of about 22 times for a test instance with 505 shreds (20 mixed shredded-pages from different documents).



### Linguistically Driven Graph Capsule Network for Visual Question Reasoning
- **Arxiv ID**: http://arxiv.org/abs/2003.10065v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.10065v1)
- **Published**: 2020-03-23 03:34:25+00:00
- **Updated**: 2020-03-23 03:34:25+00:00
- **Authors**: Qingxing Cao, Xiaodan Liang, Keze Wang, Liang Lin
- **Comment**: Submitted to TPAMI 2020. We have achieved an end-to-end interpretable
  structural reasoning for general images without the requirement of layout
  annotations
- **Journal**: None
- **Summary**: Recently, studies of visual question answering have explored various architectures of end-to-end networks and achieved promising results on both natural and synthetic datasets, which require explicitly compositional reasoning. However, it has been argued that these black-box approaches lack interpretability of results, and thus cannot perform well on generalization tasks due to overfitting the dataset bias. In this work, we aim to combine the benefits of both sides and overcome their limitations to achieve an end-to-end interpretable structural reasoning for general images without the requirement of layout annotations. Inspired by the property of a capsule network that can carve a tree structure inside a regular convolutional neural network (CNN), we propose a hierarchical compositional reasoning model called the "Linguistically driven Graph Capsule Network", where the compositional process is guided by the linguistic parse tree. Specifically, we bind each capsule in the lowest layer to bridge the linguistic embedding of a single word in the original question with visual evidence and then route them to the same capsule if they are siblings in the parse tree. This compositional process is achieved by performing inference on a linguistically driven conditional random field (CRF) and is performed across multiple graph capsule layers, which results in a compositional reasoning process inside a CNN. Experiments on the CLEVR dataset, CLEVR compositional generation test, and FigureQA dataset demonstrate the effectiveness and composition generalization ability of our end-to-end model.



### ASLFeat: Learning Local Features of Accurate Shape and Localization
- **Arxiv ID**: http://arxiv.org/abs/2003.10071v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.10071v2)
- **Published**: 2020-03-23 04:03:03+00:00
- **Updated**: 2020-04-19 12:47:53+00:00
- **Authors**: Zixin Luo, Lei Zhou, Xuyang Bai, Hongkai Chen, Jiahui Zhang, Yao Yao, Shiwei Li, Tian Fang, Long Quan
- **Comment**: Accepted to CVPR 2020, supplementary materials included, code
  available
- **Journal**: None
- **Summary**: This work focuses on mitigating two limitations in the joint learning of local feature detectors and descriptors. First, the ability to estimate the local shape (scale, orientation, etc.) of feature points is often neglected during dense feature extraction, while the shape-awareness is crucial to acquire stronger geometric invariance. Second, the localization accuracy of detected keypoints is not sufficient to reliably recover camera geometry, which has become the bottleneck in tasks such as 3D reconstruction. In this paper, we present ASLFeat, with three light-weight yet effective modifications to mitigate above issues. First, we resort to deformable convolutional networks to densely estimate and apply local transformation. Second, we take advantage of the inherent feature hierarchy to restore spatial resolution and low-level details for accurate keypoint localization. Finally, we use a peakiness measurement to relate feature responses and derive more indicative detection scores. The effect of each modification is thoroughly studied, and the evaluation is extensively conducted across a variety of practical scenarios. State-of-the-art results are reported that demonstrate the superiority of our methods.



### Diagnosis of Breast Cancer Based on Modern Mammography using Hybrid Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/2003.13503v3
- **DOI**: 10.1007/s11045-020-00756-7
- **Categories**: **eess.IV**, cs.CV, cs.LG, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2003.13503v3)
- **Published**: 2020-03-23 05:16:34+00:00
- **Updated**: 2020-05-27 05:53:17+00:00
- **Authors**: Aditya Khamparia, Subrato Bharati, Prajoy Podder, Deepak Gupta, Ashish Khanna, Thai Kim Phung, Dang N. H. Thanh
- **Comment**: 24 pages, 11 figures
- **Journal**: Multidimensional Systems and Signal Processing, 2021
- **Summary**: Breast cancer is a common cancer for women. Early detection of breast cancer can considerably increase the survival rate of women. This paper mainly focuses on transfer learning process to detect breast cancer. Modified VGG (MVGG), residual network, mobile network is proposed and implemented in this paper. DDSM dataset is used in this paper. Experimental results show that our proposed hybrid transfers learning model (Fusion of MVGG16 and ImageNet) provides an accuracy of 88.3% where the number of epoch is 15. On the other hand, only modified VGG 16 architecture (MVGG 16) provides an accuracy 80.8% and MobileNet provides an accuracy of 77.2%. So, it is clearly stated that the proposed hybrid pre-trained network outperforms well compared to single architecture. This architecture can be considered as an effective tool for the radiologists in order to reduce the false negative and false positive rate. Therefore, the efficiency of mammography analysis will be improved.



### Illumination-based Transformations Improve Skin Lesion Segmentation in Dermoscopic Images
- **Arxiv ID**: http://arxiv.org/abs/2003.10111v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.10111v1)
- **Published**: 2020-03-23 07:43:35+00:00
- **Updated**: 2020-03-23 07:43:35+00:00
- **Authors**: Kumar Abhishek, Ghassan Hamarneh, Mark S. Drew
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: The semantic segmentation of skin lesions is an important and common initial task in the computer aided diagnosis of dermoscopic images. Although deep learning-based approaches have considerably improved the segmentation accuracy, there is still room for improvement by addressing the major challenges, such as variations in lesion shape, size, color and varying levels of contrast. In this work, we propose the first deep semantic segmentation framework for dermoscopic images which incorporates, along with the original RGB images, information extracted using the physics of skin illumination and imaging. In particular, we incorporate information from specific color bands, illumination invariant grayscale images, and shading-attenuated images. We evaluate our method on three datasets: the ISBI ISIC 2017 Skin Lesion Segmentation Challenge dataset, the DermoFit Image Library, and the PH2 dataset and observe improvements of 12.02%, 4.30%, and 8.86% respectively in the mean Jaccard index over a baseline model trained only with RGB images.



### Efficient Crowd Counting via Structured Knowledge Transfer
- **Arxiv ID**: http://arxiv.org/abs/2003.10120v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.10120v3)
- **Published**: 2020-03-23 08:05:41+00:00
- **Updated**: 2020-08-11 15:31:57+00:00
- **Authors**: Lingbo Liu, Jiaqi Chen, Hefeng Wu, Tianshui Chen, Guanbin Li, Liang Lin
- **Comment**: This paper has been accepted by ACM MM 2020. Our code and models are
  available at {\url{https://github.com/HCPLab-SYSU/SKT}}
- **Journal**: None
- **Summary**: Crowd counting is an application-oriented task and its inference efficiency is crucial for real-world applications. However, most previous works relied on heavy backbone networks and required prohibitive run-time consumption, which would seriously restrict their deployment scopes and cause poor scalability. To liberate these crowd counting models, we propose a novel Structured Knowledge Transfer (SKT) framework, which fully exploits the structured knowledge of a well-trained teacher network to generate a lightweight but still highly effective student network. Specifically, it is integrated with two complementary transfer modules, including an Intra-Layer Pattern Transfer which sequentially distills the knowledge embedded in layer-wise features of the teacher network to guide feature learning of the student network and an Inter-Layer Relation Transfer which densely distills the cross-layer correlation knowledge of the teacher to regularize the student's feature evolutio Consequently, our student network can derive the layer-wise and cross-layer knowledge from the teacher network to learn compact yet effective features. Extensive evaluations on three benchmarks well demonstrate the effectiveness of our SKT for extensive crowd counting models. In particular, only using around $6\%$ of the parameters and computation cost of original models, our distilled VGG-based models obtain at least 6.5$\times$ speed-up on an Nvidia 1080 GPU and even achieve state-of-the-art performance. Our code and models are available at {\url{https://github.com/HCPLab-SYSU/SKT}}.



### Multi-Plateau Ensemble for Endoscopic Artefact Segmentation and Detection
- **Arxiv ID**: http://arxiv.org/abs/2003.10129v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.10129v1)
- **Published**: 2020-03-23 08:28:36+00:00
- **Updated**: 2020-03-23 08:28:36+00:00
- **Authors**: Suyog Jadhav, Udbhav Bamba, Arnav Chavan, Rishabh Tiwari, Aryan Raj
- **Comment**: EndoCV2020 workshop ISBI 2020 camera ready
- **Journal**: http://ceur-ws.org/Vol-2595/endoCV2020_paper_id_20.pdf
- **Summary**: Endoscopic artefact detection challenge consists of 1) Artefact detection, 2) Semantic segmentation, and 3) Out-of-sample generalisation. For Semantic segmentation task, we propose a multi-plateau ensemble of FPN (Feature Pyramid Network) with EfficientNet as feature extractor/encoder. For Object detection task, we used a three model ensemble of RetinaNet with Resnet50 Backbone and FasterRCNN (FPN + DC5) with Resnext101 Backbone}. A PyTorch implementation to our approach to the problem is available at https://github.com/ubamba98/EAD2020.



### Incomplete Graph Representation and Learning via Partial Graph Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2003.10130v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.10130v2)
- **Published**: 2020-03-23 08:29:59+00:00
- **Updated**: 2021-06-04 09:23:46+00:00
- **Authors**: Bo Jiang, Ziyan Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Graph Neural Networks (GNNs) are gaining increasing attention on graph data learning tasks in recent years. However, in many applications, graph may be coming in an incomplete form where attributes of graph nodes are partially unknown/missing. Existing GNNs are generally designed on complete graphs which can not deal with attribute-incomplete graph data directly. To address this problem, we develop a novel partial aggregation based GNNs, named Partial Graph Neural Networks (PaGNNs), for attribute-incomplete graph representation and learning. Our work is motivated by the observation that the neighborhood aggregation function in standard GNNs can be equivalently viewed as the neighborhood reconstruction formulation. Based on it, we define two novel partial aggregation (reconstruction) functions on incomplete graph and derive PaGNNs for incomplete graph data learning. Extensive experiments on several datasets demonstrate the effectiveness and efficiency of the proposed PaGNNs.



### Depth Edge Guided CNNs for Sparse Depth Upsampling
- **Arxiv ID**: http://arxiv.org/abs/2003.10138v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.10138v1)
- **Published**: 2020-03-23 08:56:32+00:00
- **Updated**: 2020-03-23 08:56:32+00:00
- **Authors**: Yi Guo, Ji Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Guided sparse depth upsampling aims to upsample an irregularly sampled sparse depth map when an aligned high-resolution color image is given as guidance. Many neural networks have been designed for this task. However, they often ignore the structural difference between the depth and the color image, resulting in obvious artifacts such as texture copy and depth blur at the upsampling depth. Inspired by the normalized convolution operation, we propose a guided convolutional layer to recover dense depth from sparse and irregular depth image with an depth edge image as guidance. Our novel guided network can prevent the depth value from crossing the depth edge to facilitate upsampling. We further design a convolution network based on proposed convolutional layer to combine the advantages of different algorithms and achieve better performance. We conduct comprehensive experiments to verify our method on real-world indoor and synthetic outdoor datasets. Our method produces strong results. It outperforms state-of-the-art methods on the Virtual KITTI dataset and the Middlebury dataset. It also presents strong generalization capability under different 3D point densities, various lighting and weather conditions.



### EPSNet: Efficient Panoptic Segmentation Network with Cross-layer Attention Fusion
- **Arxiv ID**: http://arxiv.org/abs/2003.10142v3
- **DOI**: 10.1007/978-3-030-69525-5_41
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.10142v3)
- **Published**: 2020-03-23 09:11:44+00:00
- **Updated**: 2020-12-24 05:15:41+00:00
- **Authors**: Chia-Yuan Chang, Shuo-En Chang, Pei-Yung Hsiao, Li-Chen Fu
- **Comment**: ACCV 2020. Code is available at: https://github.com/neo85824/epsnet
- **Journal**: None
- **Summary**: Panoptic segmentation is a scene parsing task which unifies semantic segmentation and instance segmentation into one single task. However, the current state-of-the-art studies did not take too much concern on inference time. In this work, we propose an Efficient Panoptic Segmentation Network (EPSNet) to tackle the panoptic segmentation tasks with fast inference speed. Basically, EPSNet generates masks based on simple linear combination of prototype masks and mask coefficients. The light-weight network branches for instance segmentation and semantic segmentation only need to predict mask coefficients and produce masks with the shared prototypes predicted by prototype network branch. Furthermore, to enhance the quality of shared prototypes, we adopt a module called "cross-layer attention fusion module", which aggregates the multi-scale features with attention mechanism helping them capture the long-range dependencies between each other. To validate the proposed work, we have conducted various experiments on the challenging COCO panoptic dataset, which achieve highly promising performance with significantly faster inference speed (53ms on GPU).



### DeepFit: 3D Surface Fitting via Neural Network Weighted Least Squares
- **Arxiv ID**: http://arxiv.org/abs/2003.10826v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.10826v1)
- **Published**: 2020-03-23 09:18:54+00:00
- **Updated**: 2020-03-23 09:18:54+00:00
- **Authors**: Yizhak Ben-Shabat, Stephen Gould
- **Comment**: arXiv admin note: text overlap with arXiv:1812.00709
- **Journal**: None
- **Summary**: We propose a surface fitting method for unstructured 3D point clouds. This method, called DeepFit, incorporates a neural network to learn point-wise weights for weighted least squares polynomial surface fitting. The learned weights act as a soft selection for the neighborhood of surface points thus avoiding the scale selection required of previous methods. To train the network we propose a novel surface consistency loss that improves point weight estimation. The method enables extracting normal vectors and other geometrical properties, such as principal curvatures, the latter were not presented as ground truth during training. We achieve state-of-the-art results on a benchmark normal and curvature estimation dataset, demonstrate robustness to noise, outliers and density variations, and show its application on noise removal.



### CF2-Net: Coarse-to-Fine Fusion Convolutional Network for Breast Ultrasound Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2003.10144v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, 68T07(Primary) 68T45(Secondary), I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/2003.10144v1)
- **Published**: 2020-03-23 09:27:26+00:00
- **Updated**: 2020-03-23 09:27:26+00:00
- **Authors**: Zhenyuan Ning, Ke Wang, Shengzhou Zhong, Qianjin Feng, Yu Zhang
- **Comment**: 8 pages, 6 figures
- **Journal**: None
- **Summary**: Breast ultrasound (BUS) image segmentation plays a crucial role in a computer-aided diagnosis system, which is regarded as a useful tool to help increase the accuracy of breast cancer diagnosis. Recently, many deep learning methods have been developed for segmentation of BUS image and show some advantages compared with conventional region-, model-, and traditional learning-based methods. However, previous deep learning methods typically use skip-connection to concatenate the encoder and decoder, which might not make full fusion of coarse-to-fine features from encoder and decoder. Since the structure and edge of lesion in BUS image are common blurred, these would make it difficult to learn the discriminant information of structure and edge, and reduce the performance. To this end, we propose and evaluate a coarse-to-fine fusion convolutional network (CF2-Net) based on a novel feature integration strategy (forming an 'E'-like type) for BUS image segmentation. To enhance contour and provide structural information, we concatenate a super-pixel image and the original image as the input of CF2-Net. Meanwhile, to highlight the differences in the lesion regions with variable sizes and relieve the imbalance issue, we further design a weighted-balanced loss function to train the CF2-Net effectively. The proposed CF2-Net was evaluated on an open dataset by using four-fold cross validation. The results of the experiment demonstrate that the CF2-Net obtains state-of-the-art performance when compared with other deep learning-based methods



### GeoGraph: Learning graph-based multi-view object detection with geometric cues end-to-end
- **Arxiv ID**: http://arxiv.org/abs/2003.10151v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.10151v2)
- **Published**: 2020-03-23 09:40:35+00:00
- **Updated**: 2020-03-24 14:38:07+00:00
- **Authors**: Ahmed Samy Nassar, Stefano D'Aronco, Sébastien Lefèvre, Jan D. Wegner
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we propose an end-to-end learnable approach that detects static urban objects from multiple views, re-identifies instances, and finally assigns a geographic position per object. Our method relies on a Graph Neural Network (GNN) to, detect all objects and output their geographic positions given images and approximate camera poses as input. Our GNN simultaneously models relative pose and image evidence, and is further able to deal with an arbitrary number of input views. Our method is robust to occlusion, with similar appearance of neighboring objects, and severe changes in viewpoints by jointly reasoning about visual image appearance and relative pose. Experimental evaluation on two challenging, large-scale datasets and comparison with state-of-the-art methods show significant and systematic improvements both in accuracy and efficiency, with 2-6% gain in detection and re-ID average precision as well as 8x reduction of training time.



### SOLOv2: Dynamic and Fast Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2003.10152v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.10152v3)
- **Published**: 2020-03-23 09:44:21+00:00
- **Updated**: 2020-10-23 23:49:17+00:00
- **Authors**: Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li, Chunhua Shen
- **Comment**: Accepted to Proc. Advances in Neural Information Processing Systems
  (NeurIPS'20). Code is available at: https://git.io/AdelaiDet
- **Journal**: None
- **Summary**: In this work, we aim at building a simple, direct, and fast instance segmentation framework with strong performance. We follow the principle of the SOLO method of Wang et al. "SOLO: segmenting objects by locations". Importantly, we take one step further by dynamically learning the mask head of the object segmenter such that the mask head is conditioned on the location. Specifically, the mask branch is decoupled into a mask kernel branch and mask feature branch, which are responsible for learning the convolution kernel and the convolved features respectively. Moreover, we propose Matrix NMS (non maximum suppression) to significantly reduce the inference time overhead due to NMS of masks. Our Matrix NMS performs NMS with parallel matrix operations in one shot, and yields better results. We demonstrate a simple direct instance segmentation system, outperforming a few state-of-the-art methods in both speed and accuracy. A light-weight version of SOLOv2 executes at 31.3 FPS and yields 37.1% AP. Moreover, our state-of-the-art results in object detection (from our mask byproduct) and panoptic segmentation show the potential to serve as a new strong baseline for many instance-level recognition tasks besides instance segmentation. Code is available at: https://git.io/AdelaiDet



### Balanced Alignment for Face Recognition: A Joint Learning Approach
- **Arxiv ID**: http://arxiv.org/abs/2003.10168v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.10168v1)
- **Published**: 2020-03-23 10:35:22+00:00
- **Updated**: 2020-03-23 10:35:22+00:00
- **Authors**: Huawei Wei, Peng Lu, Yichen Wei
- **Comment**: 17 pages, 9 figures
- **Journal**: None
- **Summary**: Face alignment is crucial for face recognition and has been widely adopted. However, current practice is too simple and under-explored. There lacks an understanding of how important face alignment is and how it should be performed, for recognition. This work studies these problems and makes two contributions. First, it provides an in-depth and quantitative study of how alignment strength affects recognition accuracy. Our results show that excessive alignment is harmful and an optimal balanced point of alignment is in need. To strike the balance, our second contribution is a novel joint learning approach where alignment learning is controllable with respect to its strength and driven by recognition. Our proposed method is validated by comprehensive experiments on several benchmarks, especially the challenging ones with large pose.



### Deep Soft Procrustes for Markerless Volumetric Sensor Alignment
- **Arxiv ID**: http://arxiv.org/abs/2003.10176v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.10176v1)
- **Published**: 2020-03-23 10:51:32+00:00
- **Updated**: 2020-03-23 10:51:32+00:00
- **Authors**: Vladimiros Sterzentsenko, Alexandros Doumanoglou, Spyridon Thermos, Nikolaos Zioulis, Dimitrios Zarpalas, Petros Daras
- **Comment**: 10 pages, 7 figures, to appear in IEEE VR 2020. Code and models at
  https://vcl3d.github.io/StructureNet/
- **Journal**: None
- **Summary**: With the advent of consumer grade depth sensors, low-cost volumetric capture systems are easier to deploy. Their wider adoption though depends on their usability and by extension on the practicality of spatially aligning multiple sensors. Most existing alignment approaches employ visual patterns, e.g. checkerboards, or markers and require high user involvement and technical knowledge. More user-friendly and easier-to-use approaches rely on markerless methods that exploit geometric patterns of a physical structure. However, current SoA approaches are bounded by restrictions in the placement and the number of sensors. In this work, we improve markerless data-driven correspondence estimation to achieve more robust and flexible multi-sensor spatial alignment. In particular, we incorporate geometric constraints in an end-to-end manner into a typical segmentation based model and bridge the intermediate dense classification task with the targeted pose estimation one. This is accomplished by a soft, differentiable procrustes analysis that regularizes the segmentation and achieves higher extrinsic calibration performance in expanded sensor placement configurations, while being unrestricted by the number of sensors of the volumetric capture system. Our model is experimentally shown to achieve similar results with marker-based methods and outperform the markerless ones, while also being robust to the pose variations of the calibration structure. Code and pretrained models are available at https://vcl3d.github.io/StructureNet/.



### Learning Better Lossless Compression Using Lossy Compression
- **Arxiv ID**: http://arxiv.org/abs/2003.10184v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.10184v1)
- **Published**: 2020-03-23 11:21:52+00:00
- **Updated**: 2020-03-23 11:21:52+00:00
- **Authors**: Fabian Mentzer, Luc Van Gool, Michael Tschannen
- **Comment**: CVPR'20 camera-ready version
- **Journal**: None
- **Summary**: We leverage the powerful lossy image compression algorithm BPG to build a lossless image compression system. Specifically, the original image is first decomposed into the lossy reconstruction obtained after compressing it with BPG and the corresponding residual. We then model the distribution of the residual with a convolutional neural network-based probabilistic model that is conditioned on the BPG reconstruction, and combine it with entropy coding to losslessly encode the residual. Finally, the image is stored using the concatenation of the bitstreams produced by BPG and the learned residual coder. The resulting compression system achieves state-of-the-art performance in learned lossless full-resolution image compression, outperforming previous learned approaches as well as PNG, WebP, and JPEG2000.



### Spatial Pyramid Based Graph Reasoning for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2003.10211v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.10211v1)
- **Published**: 2020-03-23 12:28:07+00:00
- **Updated**: 2020-03-23 12:28:07+00:00
- **Authors**: Xia Li, Yibo Yang, Qijie Zhao, Tiancheng Shen, Zhouchen Lin, Hong Liu
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: The convolution operation suffers from a limited receptive filed, while global modeling is fundamental to dense prediction tasks, such as semantic segmentation. In this paper, we apply graph convolution into the semantic segmentation task and propose an improved Laplacian. The graph reasoning is directly performed in the original feature space organized as a spatial pyramid. Different from existing methods, our Laplacian is data-dependent and we introduce an attention diagonal matrix to learn a better distance metric. It gets rid of projecting and re-projecting processes, which makes our proposed method a light-weight module that can be easily plugged into current computer vision architectures. More importantly, performing graph reasoning directly in the feature space retains spatial relationships and makes spatial pyramid possible to explore multiple long-range contextual patterns from different scales. Experiments on Cityscapes, COCO Stuff, PASCAL Context and PASCAL VOC demonstrate the effectiveness of our proposed methods on semantic segmentation. We achieve comparable performance with advantages in computational and memory overhead.



### Sample-Specific Output Constraints for Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2003.10258v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.10258v1)
- **Published**: 2020-03-23 13:13:11+00:00
- **Updated**: 2020-03-23 13:13:11+00:00
- **Authors**: Mathis Brosowsky, Olaf Dünkel, Daniel Slieter, Marius Zöllner
- **Comment**: None
- **Journal**: None
- **Summary**: Neural networks reach state-of-the-art performance in a variety of learning tasks. However, a lack of understanding the decision making process yields to an appearance as black box. We address this and propose ConstraintNet, a neural network with the capability to constrain the output space in each forward pass via an additional input. The prediction of ConstraintNet is proven within the specified domain. This enables ConstraintNet to exclude unintended or even hazardous outputs explicitly whereas the final prediction is still learned from data. We focus on constraints in form of convex polytopes and show the generalization to further classes of constraints. ConstraintNet can be constructed easily by modifying existing neural network architectures. We highlight that ConstraintNet is end-to-end trainable with no overhead in the forward and backward pass. For illustration purposes, we model ConstraintNet by modifying a CNN and construct constraints for facial landmark prediction tasks. Furthermore, we demonstrate the application to a follow object controller for vehicles as a safety-critical application. We submitted an approach and system for the generation of safety-critical outputs of an entity based on ConstraintNet at the German Patent and Trademark Office with the official registration mark DE10 2019 119 739.



### Cross-domain Object Detection through Coarse-to-Fine Feature Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2003.10275v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.10275v1)
- **Published**: 2020-03-23 13:40:06+00:00
- **Updated**: 2020-03-23 13:40:06+00:00
- **Authors**: Yangtao Zheng, Di Huang, Songtao Liu, Yunhong Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Recent years have witnessed great progress in deep learning based object detection. However, due to the domain shift problem, applying off-the-shelf detectors to an unseen domain leads to significant performance drop. To address such an issue, this paper proposes a novel coarse-to-fine feature adaptation approach to cross-domain object detection. At the coarse-grained stage, different from the rough image-level or instance-level feature alignment used in the literature, foreground regions are extracted by adopting the attention mechanism, and aligned according to their marginal distributions via multi-layer adversarial learning in the common feature space. At the fine-grained stage, we conduct conditional distribution alignment of foregrounds by minimizing the distance of global prototypes with the same category but from different domains. Thanks to this coarse-to-fine feature adaptation, domain knowledge in foreground regions can be effectively transferred. Extensive experiments are carried out in various cross-domain detection scenarios. The results are state-of-the-art, which demonstrate the broad applicability and effectiveness of the proposed approach.



### Accurate Optimization of Weighted Nuclear Norm for Non-Rigid Structure from Motion
- **Arxiv ID**: http://arxiv.org/abs/2003.10281v2
- **DOI**: None
- **Categories**: **cs.CV**, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2003.10281v2)
- **Published**: 2020-03-23 13:52:16+00:00
- **Updated**: 2020-07-08 20:22:14+00:00
- **Authors**: José Pedro Iglesias, Carl Olsson, Marcus Valtonen Örnhag
- **Comment**: None
- **Journal**: None
- **Summary**: Fitting a matrix of a given rank to data in a least squares sense can be done very effectively using 2nd order methods such as Levenberg-Marquardt by explicitly optimizing over a bilinear parameterization of the matrix. In contrast, when applying more general singular value penalties, such as weighted nuclear norm priors, direct optimization over the elements of the matrix is typically used. Due to non-differentiability of the resulting objective function, first order sub-gradient or splitting methods are predominantly used. While these offer rapid iterations it is well known that they become inefficent near the minimum due to zig-zagging and in practice one is therefore often forced to settle for an approximate solution.   In this paper we show that more accurate results can in many cases be achieved with 2nd order methods. Our main result shows how to construct bilinear formulations, for a general class of regularizers including weighted nuclear norm penalties, that are provably equivalent to the original problems. With these formulations the regularizing function becomes twice differentiable and 2nd order methods can be applied. We show experimentally, on a number of structure from motion problems, that our approach outperforms state-of-the-art methods.



### High-Dimensional Data Set Simplification by Laplace-Beltrami Operator
- **Arxiv ID**: http://arxiv.org/abs/2004.02808v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.02808v1)
- **Published**: 2020-03-23 13:52:58+00:00
- **Updated**: 2020-03-23 13:52:58+00:00
- **Authors**: Chenkai Xu, Hongwei Lin
- **Comment**: None
- **Journal**: None
- **Summary**: With the development of the Internet and other digital technologies, the speed of data generation has become considerably faster than the speed of data processing. Because big data typically contain massive redundant information, it is possible to significantly simplify a big data set while maintaining the key information it contains. In this paper, we develop a big data simplification method based on the eigenvalues and eigenfunctions of the Laplace-Beltrami operator (LBO). Specifically, given a data set that can be considered as an unorganized data point set in high-dimensional space, a discrete LBO defined on the big data set is constructed and its eigenvalues and eigenvectors are calculated. Then, the local extremum and the saddle points of the eigenfunctions are proposed to be the feature points of a data set in high-dimensional space, constituting a simplified data set. Moreover, we develop feature point detection methods for the functions defined on an unorganized data point set in high-dimensional space, and devise metrics for measuring the fidelity of the simplified data set to the original set. Finally, examples and applications are demonstrated to validate the efficiency and effectiveness of the proposed methods, demonstrating that data set simplification is a method for processing a maximum-sized data set using a limited data processing capability.



### Robust Medical Instrument Segmentation Challenge 2019
- **Arxiv ID**: http://arxiv.org/abs/2003.10299v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.10299v2)
- **Published**: 2020-03-23 14:35:08+00:00
- **Updated**: 2020-05-19 12:27:18+00:00
- **Authors**: Tobias Ross, Annika Reinke, Peter M. Full, Martin Wagner, Hannes Kenngott, Martin Apitz, Hellena Hempe, Diana Mindroc Filimon, Patrick Scholz, Thuy Nuong Tran, Pierangela Bruno, Pablo Arbeláez, Gui-Bin Bian, Sebastian Bodenstedt, Jon Lindström Bolmgren, Laura Bravo-Sánchez, Hua-Bin Chen, Cristina González, Dong Guo, Pål Halvorsen, Pheng-Ann Heng, Enes Hosgor, Zeng-Guang Hou, Fabian Isensee, Debesh Jha, Tingting Jiang, Yueming Jin, Kadir Kirtac, Sabrina Kletz, Stefan Leger, Zhixuan Li, Klaus H. Maier-Hein, Zhen-Liang Ni, Michael A. Riegler, Klaus Schoeffmann, Ruohua Shi, Stefanie Speidel, Michael Stenzel, Isabell Twick, Gutai Wang, Jiacheng Wang, Liansheng Wang, Lu Wang, Yujie Zhang, Yan-Jie Zhou, Lei Zhu, Manuel Wiesenfarth, Annette Kopp-Schneider, Beat P. Müller-Stich, Lena Maier-Hein
- **Comment**: A pre-print
- **Journal**: None
- **Summary**: Intraoperative tracking of laparoscopic instruments is often a prerequisite for computer and robotic-assisted interventions. While numerous methods for detecting, segmenting and tracking of medical instruments based on endoscopic video images have been proposed in the literature, key limitations remain to be addressed: Firstly, robustness, that is, the reliable performance of state-of-the-art methods when run on challenging images (e.g. in the presence of blood, smoke or motion artifacts). Secondly, generalization; algorithms trained for a specific intervention in a specific hospital should generalize to other interventions or institutions.   In an effort to promote solutions for these limitations, we organized the Robust Medical Instrument Segmentation (ROBUST-MIS) challenge as an international benchmarking competition with a specific focus on the robustness and generalization capabilities of algorithms. For the first time in the field of endoscopic image processing, our challenge included a task on binary segmentation and also addressed multi-instance detection and segmentation. The challenge was based on a surgical data set comprising 10,040 annotated images acquired from a total of 30 surgical procedures from three different types of surgery. The validation of the competing methods for the three tasks (binary segmentation, multi-instance detection and multi-instance segmentation) was performed in three different stages with an increasing domain gap between the training and the test data. The results confirm the initial hypothesis, namely that algorithm performance degrades with an increasing domain gap. While the average detection and segmentation quality of the best-performing algorithms is high, future research should concentrate on detection and segmentation of small, crossing, moving and transparent instrument(s) (parts).



### Attention U-Net Based Adversarial Architectures for Chest X-ray Lung Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2003.10304v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.10304v1)
- **Published**: 2020-03-23 14:45:48+00:00
- **Updated**: 2020-03-23 14:45:48+00:00
- **Authors**: Gusztáv Gaál, Balázs Maga, András Lukács
- **Comment**: 7 pages, 4 figures
- **Journal**: None
- **Summary**: Chest X-ray is the most common test among medical imaging modalities. It is applied for detection and differentiation of, among others, lung cancer, tuberculosis, and pneumonia, the last with importance due to the COVID-19 disease. Integrating computer-aided detection methods into the radiologist diagnostic pipeline, greatly reduces the doctors' workload, increasing reliability and quantitative analysis. Here we present a novel deep learning approach for lung segmentation, a basic, but arduous task in the diagnostic pipeline. Our method uses state-of-the-art fully convolutional neural networks in conjunction with an adversarial critic model. It generalized well to CXR images of unseen datasets with different patient profiles, achieving a final DSC of 97.5% on the JSRT dataset.



### A Developmental Neuro-Robotics Approach for Boosting the Recognition of Handwritten Digits
- **Arxiv ID**: http://arxiv.org/abs/2003.10308v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.10308v1)
- **Published**: 2020-03-23 14:55:00+00:00
- **Updated**: 2020-03-23 14:55:00+00:00
- **Authors**: Alessandro Di Nuovo
- **Comment**: Accepted for presentation at IJCNN 2020
- **Journal**: None
- **Summary**: Developmental psychology and neuroimaging research identified a close link between numbers and fingers, which can boost the initial number knowledge in children. Recent evidence shows that a simulation of the children's embodied strategies can improve the machine intelligence too. This article explores the application of embodied strategies to convolutional neural network models in the context of developmental neuro-robotics, where the training information is likely to be gradually acquired while operating rather than being abundant and fully available as the classical machine learning scenarios. The experimental analyses show that the proprioceptive information from the robot fingers can improve network accuracy in the recognition of handwritten Arabic digits when training examples and epochs are few. This result is comparable to brain imaging and longitudinal studies with young children. In conclusion, these findings also support the relevance of the embodiment in the case of artificial agents' training and show a possible way for the humanization of the learning process, where the robotic body can express the internal processes of artificial intelligence making it more understandable for humans.



### Adversarial Attacks on Monocular Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2003.10315v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.10315v1)
- **Published**: 2020-03-23 15:04:30+00:00
- **Updated**: 2020-03-23 15:04:30+00:00
- **Authors**: Ziqi Zhang, Xinge Zhu, Yingwei Li, Xiangqun Chen, Yao Guo
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances of deep learning have brought exceptional performance on many computer vision tasks such as semantic segmentation and depth estimation. However, the vulnerability of deep neural networks towards adversarial examples have caused grave concerns for real-world deployment. In this paper, we present to the best of our knowledge the first systematic study of adversarial attacks on monocular depth estimation, an important task of 3D scene understanding in scenarios such as autonomous driving and robot navigation. In order to understand the impact of adversarial attacks on depth estimation, we first define a taxonomy of different attack scenarios for depth estimation, including non-targeted attacks, targeted attacks and universal attacks. We then adapt several state-of-the-art attack methods for classification on the field of depth estimation. Besides, multi-task attacks are introduced to further improve the attack performance for universal attacks. Experimental results show that it is possible to generate significant errors on depth estimation. In particular, we demonstrate that our methods can conduct targeted attacks on given objects (such as a car), resulting in depth estimation 3-4x away from the ground truth (e.g., from 20m to 80m).



### Neural Contours: Learning to Draw Lines from 3D Shapes
- **Arxiv ID**: http://arxiv.org/abs/2003.10333v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2003.10333v3)
- **Published**: 2020-03-23 15:37:49+00:00
- **Updated**: 2020-04-05 03:22:55+00:00
- **Authors**: Difan Liu, Mohamed Nabail, Aaron Hertzmann, Evangelos Kalogerakis
- **Comment**: Accepted to CVPR 2020
- **Journal**: None
- **Summary**: This paper introduces a method for learning to generate line drawings from 3D models. Our architecture incorporates a differentiable module operating on geometric features of the 3D model, and an image-based module operating on view-based shape representations. At test time, geometric and view-based reasoning are combined with the help of a neural module to create a line drawing. The model is trained on a large number of crowdsourced comparisons of line drawings. Experiments demonstrate that our method achieves significant improvements in line drawing over the state-of-the-art when evaluated on standard benchmarks, resulting in drawings that are comparable to those produced by experienced human artists.



### Weakly Supervised 3D Human Pose and Shape Reconstruction with Normalizing Flows
- **Arxiv ID**: http://arxiv.org/abs/2003.10350v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.10350v2)
- **Published**: 2020-03-23 16:11:51+00:00
- **Updated**: 2020-08-22 14:46:19+00:00
- **Authors**: Andrei Zanfir, Eduard Gabriel Bazavan, Hongyi Xu, Bill Freeman, Rahul Sukthankar, Cristian Sminchisescu
- **Comment**: None
- **Journal**: ECCV 2020
- **Summary**: Monocular 3D human pose and shape estimation is challenging due to the many degrees of freedom of the human body and thedifficulty to acquire training data for large-scale supervised learning in complex visual scenes. In this paper we present practical semi-supervised and self-supervised models that support training and good generalization in real-world images and video. Our formulation is based on kinematic latent normalizing flow representations and dynamics, as well as differentiable, semantic body part alignment loss functions that support self-supervised learning. In extensive experiments using 3D motion capture datasets like CMU, Human3.6M, 3DPW, or AMASS, as well as image repositories like COCO, we show that the proposed methods outperform the state of the art, supporting the practical construction of an accurate family of models based on large-scale training with diverse and incompletely labeled image and video data.



### Inherent Adversarial Robustness of Deep Spiking Neural Networks: Effects of Discrete Input Encoding and Non-Linear Activations
- **Arxiv ID**: http://arxiv.org/abs/2003.10399v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2003.10399v2)
- **Published**: 2020-03-23 17:20:24+00:00
- **Updated**: 2020-07-23 21:05:40+00:00
- **Authors**: Saima Sharmin, Nitin Rathi, Priyadarshini Panda, Kaushik Roy
- **Comment**: Accepted in 16th European Conference on Computer Vision (ECCV 2020)
- **Journal**: None
- **Summary**: In the recent quest for trustworthy neural networks, we present Spiking Neural Network (SNN) as a potential candidate for inherent robustness against adversarial attacks. In this work, we demonstrate that adversarial accuracy of SNNs under gradient-based attacks is higher than their non-spiking counterparts for CIFAR datasets on deep VGG and ResNet architectures, particularly in blackbox attack scenario. We attribute this robustness to two fundamental characteristics of SNNs and analyze their effects. First, we exhibit that input discretization introduced by the Poisson encoder improves adversarial robustness with reduced number of timesteps. Second, we quantify the amount of adversarial accuracy with increased leak rate in Leaky-Integrate-Fire (LIF) neurons. Our results suggest that SNNs trained with LIF neurons and smaller number of timesteps are more robust than the ones with IF (Integrate-Fire) neurons and larger number of timesteps. Also we overcome the bottleneck of creating gradient-based adversarial inputs in temporal domain by proposing a technique for crafting attacks from SNN



### Learning Dynamic Routing for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2003.10401v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.10401v1)
- **Published**: 2020-03-23 17:22:14+00:00
- **Updated**: 2020-03-23 17:22:14+00:00
- **Authors**: Yanwei Li, Lin Song, Yukang Chen, Zeming Li, Xiangyu Zhang, Xingang Wang, Jian Sun
- **Comment**: Accepted by CVPR 2020 as Oral
- **Journal**: None
- **Summary**: Recently, numerous handcrafted and searched networks have been applied for semantic segmentation. However, previous works intend to handle inputs with various scales in pre-defined static architectures, such as FCN, U-Net, and DeepLab series. This paper studies a conceptually new method to alleviate the scale variance in semantic representation, named dynamic routing. The proposed framework generates data-dependent routes, adapting to the scale distribution of each image. To this end, a differentiable gating function, called soft conditional gate, is proposed to select scale transform paths on the fly. In addition, the computational cost can be further reduced in an end-to-end manner by giving budget constraints to the gating function. We further relax the network level routing space to support multi-path propagations and skip-connections in each forward, bringing substantial network capacity. To demonstrate the superiority of the dynamic property, we compare with several static architectures, which can be modeled as special cases in the routing space. Extensive experiments are conducted on Cityscapes and PASCAL VOC 2012 to illustrate the effectiveness of the dynamic framework. Code is available at https://github.com/yanwei-li/DynamicRouting.



### Learning a Probabilistic Strategy for Computational Imaging Sensor Selection
- **Arxiv ID**: http://arxiv.org/abs/2003.10424v1
- **DOI**: None
- **Categories**: **eess.IV**, astro-ph.IM, cs.CV, cs.LG, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2003.10424v1)
- **Published**: 2020-03-23 17:52:17+00:00
- **Updated**: 2020-03-23 17:52:17+00:00
- **Authors**: He Sun, Adrian V. Dalca, Katherine L. Bouman
- **Comment**: This paper has been accepted to the IEEE International Conference on
  Computational Photography (ICCP) 2020. Keywords: Computational Imaging,
  Optimized Sensing, Ising Model, Deep Learning, VLBI, Interferometry
- **Journal**: None
- **Summary**: Optimized sensing is important for computational imaging in low-resource environments, when images must be recovered from severely limited measurements. In this paper, we propose a physics-constrained, fully differentiable, autoencoder that learns a probabilistic sensor-sampling strategy for optimized sensor design. The proposed method learns a system's preferred sampling distribution that characterizes the correlations between different sensor selections as a binary, fully-connected Ising model. The learned probabilistic model is achieved by using a Gibbs sampling inspired network architecture, and is trained end-to-end with a reconstruction network for efficient co-design. The proposed framework is applicable to sensor selection problems in a variety of computational imaging applications. In this paper, we demonstrate the approach in the context of a very-long-baseline-interferometry (VLBI) array design task, where sensor correlations and atmospheric noise present unique challenges. We demonstrate results broadly consistent with expectation, and draw attention to particular structures preferred in the telescope array geometry that can be leveraged to plan future observations and design array expansions.



### Deep Unfolding Network for Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2003.10428v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.10428v1)
- **Published**: 2020-03-23 17:55:42+00:00
- **Updated**: 2020-03-23 17:55:42+00:00
- **Authors**: Kai Zhang, Luc Van Gool, Radu Timofte
- **Comment**: Accepted by CVPR 2020. Project page: https://github.com/cszn/USRNet
- **Journal**: None
- **Summary**: Learning-based single image super-resolution (SISR) methods are continuously showing superior effectiveness and efficiency over traditional model-based methods, largely due to the end-to-end training. However, different from model-based methods that can handle the SISR problem with different scale factors, blur kernels and noise levels under a unified MAP (maximum a posteriori) framework, learning-based methods generally lack such flexibility. To address this issue, this paper proposes an end-to-end trainable unfolding network which leverages both learning-based methods and model-based methods. Specifically, by unfolding the MAP inference via a half-quadratic splitting algorithm, a fixed number of iterations consisting of alternately solving a data subproblem and a prior subproblem can be obtained. The two subproblems then can be solved with neural modules, resulting in an end-to-end trainable, iterative network. As a result, the proposed network inherits the flexibility of model-based methods to super-resolve blurry, noisy images for different scale factors via a single model, while maintaining the advantages of learning-based methods. Extensive experiments demonstrate the superiority of the proposed deep unfolding network in terms of flexibility, effectiveness and also generalizability.



### Atlas: End-to-End 3D Scene Reconstruction from Posed Images
- **Arxiv ID**: http://arxiv.org/abs/2003.10432v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.10432v3)
- **Published**: 2020-03-23 17:59:15+00:00
- **Updated**: 2020-10-14 13:46:36+00:00
- **Authors**: Zak Murez, Tarrence van As, James Bartolozzi, Ayan Sinha, Vijay Badrinarayanan, Andrew Rabinovich
- **Comment**: None
- **Journal**: None
- **Summary**: We present an end-to-end 3D reconstruction method for a scene by directly regressing a truncated signed distance function (TSDF) from a set of posed RGB images. Traditional approaches to 3D reconstruction rely on an intermediate representation of depth maps prior to estimating a full 3D model of a scene. We hypothesize that a direct regression to 3D is more effective. A 2D CNN extracts features from each image independently which are then back-projected and accumulated into a voxel volume using the camera intrinsics and extrinsics. After accumulation, a 3D CNN refines the accumulated features and predicts the TSDF values. Additionally, semantic segmentation of the 3D model is obtained without significant computation. This approach is evaluated on the Scannet dataset where we significantly outperform state-of-the-art baselines (deep multiview stereo followed by traditional TSDF fusion) both quantitatively and qualitatively. We compare our 3D semantic segmentation to prior methods that use a depth sensor since no previous work attempts the problem with only RGB input.



### Learning Object Permanence from Video
- **Arxiv ID**: http://arxiv.org/abs/2003.10469v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.10469v4)
- **Published**: 2020-03-23 18:03:01+00:00
- **Updated**: 2020-07-16 09:16:04+00:00
- **Authors**: Aviv Shamsian, Ofri Kleinfeld, Amir Globerson, Gal Chechik
- **Comment**: 16th European Conference on Computer Vision (ECCV 2020)
- **Journal**: None
- **Summary**: Object Permanence allows people to reason about the location of non-visible objects, by understanding that they continue to exist even when not perceived directly. Object Permanence is critical for building a model of the world, since objects in natural visual scenes dynamically occlude and contain each-other. Intensive studies in developmental psychology suggest that object permanence is a challenging task that is learned through extensive experience. Here we introduce the setup of learning Object Permanence from data. We explain why this learning problem should be dissected into four components, where objects are (1) visible, (2) occluded, (3) contained by another object and (4) carried by a containing object. The fourth subtask, where a target object is carried by a containing object, is particularly challenging because it requires a system to reason about a moving location of an invisible object. We then present a unified deep architecture that learns to predict object location under these four scenarios. We evaluate the architecture and system on a new dataset based on CATER, and find that it outperforms previous localization methods and various baselines.



### Label Noise Types and Their Effects on Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2003.10471v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.10471v1)
- **Published**: 2020-03-23 18:03:39+00:00
- **Updated**: 2020-03-23 18:03:39+00:00
- **Authors**: Görkem Algan, İlkay Ulusoy
- **Comment**: None
- **Journal**: None
- **Summary**: The recent success of deep learning is mostly due to the availability of big datasets with clean annotations. However, gathering a cleanly annotated dataset is not always feasible due to practical challenges. As a result, label noise is a common problem in datasets, and numerous methods to train deep neural networks in the presence of noisy labels are proposed in the literature. These methods commonly use benchmark datasets with synthetic label noise on the training set. However, there are multiple types of label noise, and each of them has its own characteristic impact on learning. Since each work generates a different kind of label noise, it is problematic to test and compare those algorithms in the literature fairly. In this work, we provide a detailed analysis of the effects of different kinds of label noise on learning. Moreover, we propose a generic framework to generate feature-dependent label noise, which we show to be the most challenging case for learning. Our proposed method aims to emphasize similarities among data instances by sparsely distributing them in the feature domain. By this approach, samples that are more likely to be mislabeled are detected from their softmax probabilities, and their labels are flipped to the corresponding class. The proposed method can be applied to any clean dataset to synthesize feature-dependent noisy labels. For the ease of other researchers to test their algorithms with noisy labels, we share corrupted labels for the most commonly used benchmark datasets. Our code and generated noisy synthetic labels are available online.



### Distilling Knowledge from Graph Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/2003.10477v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.10477v4)
- **Published**: 2020-03-23 18:23:11+00:00
- **Updated**: 2021-01-10 03:55:05+00:00
- **Authors**: Yiding Yang, Jiayan Qiu, Mingli Song, Dacheng Tao, Xinchao Wang
- **Comment**: Accepted by CVPR 2020
- **Journal**: None
- **Summary**: Existing knowledge distillation methods focus on convolutional neural networks (CNNs), where the input samples like images lie in a grid domain, and have largely overlooked graph convolutional networks (GCN) that handle non-grid data. In this paper, we propose to our best knowledge the first dedicated approach to distilling knowledge from a pre-trained GCN model. To enable the knowledge transfer from the teacher GCN to the student, we propose a local structure preserving module that explicitly accounts for the topological semantics of the teacher. In this module, the local structure information from both the teacher and the student are extracted as distributions, and hence minimizing the distance between these distributions enables topology-aware knowledge transfer from the teacher, yielding a compact yet high-performance student model. Moreover, the proposed approach is readily extendable to dynamic graph models, where the input graphs for the teacher and the student may differ. We evaluate the proposed method on two different datasets using GCN models of different architectures, and demonstrate that our method achieves the state-of-the-art knowledge distillation performance for GCN models. Code is publicly available at https://github.com/ihollywhy/DistillGCN.PyTorch.



### Peeking into occluded joints: A novel framework for crowd pose estimation
- **Arxiv ID**: http://arxiv.org/abs/2003.10506v3
- **DOI**: None
- **Categories**: **cs.CV**, 68T45, I.4.8; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2003.10506v3)
- **Published**: 2020-03-23 19:32:40+00:00
- **Updated**: 2020-03-31 02:01:35+00:00
- **Authors**: Lingteng Qiu, Xuanye Zhang, Yanran Li, Guanbin Li, Xiaojun Wu, Zixiang Xiong, Xiaoguang Han, Shuguang Cui
- **Comment**: The code of OPEC-Net is available at:
  https://lingtengqiu.github.io/2020/03/22/OPEC-Net/
- **Journal**: None
- **Summary**: Although occlusion widely exists in nature and remains a fundamental challenge for pose estimation, existing heatmap-based approaches suffer serious degradation on occlusions. Their intrinsic problem is that they directly localize the joints based on visual information; however, the invisible joints are lack of that. In contrast to localization, our framework estimates the invisible joints from an inference perspective by proposing an Image-Guided Progressive GCN module which provides a comprehensive understanding of both image context and pose structure. Moreover, existing benchmarks contain limited occlusions for evaluation. Therefore, we thoroughly pursue this problem and propose a novel OPEC-Net framework together with a new Occluded Pose (OCPose) dataset with 9k annotated images. Extensive quantitative and qualitative evaluations on benchmarks demonstrate that OPEC-Net achieves significant improvements over recent leading works. Notably, our OCPose is the most complex occlusion dataset with respect to average IoU between adjacent instances. Source code and OCPose will be publicly available.



### Automated Detection of Cribriform Growth Patterns in Prostate Histology Images
- **Arxiv ID**: http://arxiv.org/abs/2003.10543v2
- **DOI**: 10.1038/s41598-020-71942-7
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.10543v2)
- **Published**: 2020-03-23 20:56:24+00:00
- **Updated**: 2020-09-11 13:15:52+00:00
- **Authors**: Pierre Ambrosini, Eva Hollemans, Charlotte F. Kweldam, Geert J. L. H. van Leenders, Sjoerd Stallinga, Frans Vos
- **Comment**: 15 pages, 6 figures
- **Journal**: Sci Rep 10, 14904 (2020)
- **Summary**: Cribriform growth patterns in prostate carcinoma are associated with poor prognosis. We aimed to introduce a deep learning method to detect such patterns automatically. To do so, convolutional neural network was trained to detect cribriform growth patterns on 128 prostate needle biopsies. Ensemble learning taking into account other tumor growth patterns during training was used to cope with heterogeneous and limited tumor tissue occurrences. ROC and FROC analyses were applied to assess network performance regarding detection of biopsies harboring cribriform growth pattern. The ROC analysis yielded a mean area under the curve up to 0.81. FROC analysis demonstrated a sensitivity of 0.9 for regions larger than 0.0150 mm2 with on average 7.5 false positives. To benchmark method performance for intra-observer annotation variability, false positive and negative detections were re-evaluated by the pathologists. Pathologists considered 9% of the false positive regions as cribriform, and 11% as possibly cribriform; 44% of the false negative regions were not annotated as cribriform. As a final experiment, the network was also applied on a dataset of 60 biopsy regions annotated by 23 pathologists. With the cut-off reaching highest sensitivity, all images annotated as cribriform by at least 7/23 of the pathologists, were all detected as cribriform by the network and 9/60 of the images were detected as cribriform whereas no pathologist labelled them as such. In conclusion, the proposed deep learning method has high sensitivity for detecting cribriform growth patterns at the expense of a limited number of false positives. It can detect cribriform regions that are labelled as such by at least a minority of pathologists. Therefore, it could assist clinical decision making by suggesting suspicious regions.



### ScrabbleGAN: Semi-Supervised Varying Length Handwritten Text Generation
- **Arxiv ID**: http://arxiv.org/abs/2003.10557v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.10557v1)
- **Published**: 2020-03-23 21:41:19+00:00
- **Updated**: 2020-03-23 21:41:19+00:00
- **Authors**: Sharon Fogel, Hadar Averbuch-Elor, Sarel Cohen, Shai Mazor, Roee Litman
- **Comment**: in CVPR 2020
- **Journal**: None
- **Summary**: Optical character recognition (OCR) systems performance have improved significantly in the deep learning era. This is especially true for handwritten text recognition (HTR), where each author has a unique style, unlike printed text, where the variation is smaller by design. That said, deep learning based HTR is limited, as in every other task, by the number of training examples. Gathering data is a challenging and costly task, and even more so, the labeling task that follows, of which we focus here. One possible approach to reduce the burden of data annotation is semi-supervised learning. Semi supervised methods use, in addition to labeled data, some unlabeled samples to improve performance, compared to fully supervised ones. Consequently, such methods may adapt to unseen images during test time.   We present ScrabbleGAN, a semi-supervised approach to synthesize handwritten text images that are versatile both in style and lexicon. ScrabbleGAN relies on a novel generative model which can generate images of words with an arbitrary length. We show how to operate our approach in a semi-supervised manner, enjoying the aforementioned benefits such as performance boost over state of the art supervised HTR. Furthermore, our generator can manipulate the resulting text style. This allows us to change, for instance, whether the text is cursive, or how thin is the pen stroke.



### Broad Area Search and Detection of Surface-to-Air Missile Sites Using Spatial Fusion of Component Object Detections from Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2003.10566v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.10566v3)
- **Published**: 2020-03-23 22:10:19+00:00
- **Updated**: 2020-07-20 17:44:38+00:00
- **Authors**: Alan B. Cannaday II, Curt H. Davis, Grant J. Scott, Blake Ruprecht, Derek T. Anderson
- **Comment**: 9 pages, 9 figures, 9 tables, pre-published expansion of IGARSS2019
  conference paper "Improved Search and Detection of Surface-to-Air Missile
  Sites Using Spatial Fusion of Component Object Detections from Deep Neural
  Networks"
- **Journal**: None
- **Summary**: Here we demonstrate how Deep Neural Network (DNN) detections of multiple constitutive or component objects that are part of a larger, more complex, and encompassing feature can be spatially fused to improve the search, detection, and retrieval (ranking) of the larger complex feature. First, scores computed from a spatial clustering algorithm are normalized to a reference space so that they are independent of image resolution and DNN input chip size. Then, multi-scale DNN detections from various component objects are fused to improve the detection and retrieval of DNN detections of a larger complex feature. We demonstrate the utility of this approach for broad area search and detection of Surface-to-Air Missile (SAM) sites that have a very low occurrence rate (only 16 sites) over a ~90,000 km^2 study area in SE China. The results demonstrate that spatial fusion of multi-scale component-object DNN detections can reduce the detection error rate of SAM Sites by $>$85% while still maintaining a 100% recall. The novel spatial fusion approach demonstrated here can be easily extended to a wide variety of other challenging object search and detection problems in large-scale remote sensing image datasets.



