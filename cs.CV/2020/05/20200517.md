# Arxiv Papers in cs.CV on 2020-05-17
### VPR-Bench: An Open-Source Visual Place Recognition Evaluation Framework with Quantifiable Viewpoint and Appearance Change
- **Arxiv ID**: http://arxiv.org/abs/2005.08135v2
- **DOI**: 10.1007/s11263-021-01469-5
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.08135v2)
- **Published**: 2020-05-17 00:27:53+00:00
- **Updated**: 2021-10-01 18:09:13+00:00
- **Authors**: Mubariz Zaffar, Sourav Garg, Michael Milford, Julian Kooij, David Flynn, Klaus McDonald-Maier, Shoaib Ehsan
- **Comment**: Accepted version of our IJCV paper
- **Journal**: International Journal of Computer Vision. 2021 May 7:1-39
- **Summary**: Visual Place Recognition (VPR) is the process of recognising a previously visited place using visual information, often under varying appearance conditions and viewpoint changes and with computational constraints. VPR is related to the concepts of localisation, loop closure, image retrieval and is a critical component of many autonomous navigation systems ranging from autonomous vehicles to drones and computer vision systems. While the concept of place recognition has been around for many years, VPR research has grown rapidly as a field over the past decade due to improving camera hardware and its potential for deep learning-based techniques, and has become a widely studied topic in both the computer vision and robotics communities. This growth however has led to fragmentation and a lack of standardisation in the field, especially concerning performance evaluation. Moreover, the notion of viewpoint and illumination invariance of VPR techniques has largely been assessed qualitatively and hence ambiguously in the past. In this paper, we address these gaps through a new comprehensive open-source framework for assessing the performance of VPR techniques, dubbed "VPR-Bench". VPR-Bench (Open-sourced at: https://github.com/MubarizZaffar/VPR-Bench) introduces two much-needed capabilities for VPR researchers: firstly, it contains a benchmark of 12 fully-integrated datasets and 10 VPR techniques, and secondly, it integrates a comprehensive variation-quantified dataset for quantifying viewpoint and illumination invariance. We apply and analyse popular evaluation metrics for VPR from both the computer vision and robotics communities, and discuss how these different metrics complement and/or replace each other, depending upon the underlying applications and system requirements.



### Train in Germany, Test in The USA: Making 3D Object Detectors Generalize
- **Arxiv ID**: http://arxiv.org/abs/2005.08139v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.08139v1)
- **Published**: 2020-05-17 00:56:18+00:00
- **Updated**: 2020-05-17 00:56:18+00:00
- **Authors**: Yan Wang, Xiangyu Chen, Yurong You, Li Erran, Bharath Hariharan, Mark Campbell, Kilian Q. Weinberger, Wei-Lun Chao
- **Comment**: Accepted to 2020 Conference on Computer Vision and Pattern
  Recognition (CVPR 2020)
- **Journal**: None
- **Summary**: In the domain of autonomous driving, deep learning has substantially improved the 3D object detection accuracy for LiDAR and stereo camera data alike. While deep networks are great at generalization, they are also notorious to over-fit to all kinds of spurious artifacts, such as brightness, car sizes and models, that may appear consistently throughout the data. In fact, most datasets for autonomous driving are collected within a narrow subset of cities within one country, typically under similar weather conditions. In this paper we consider the task of adapting 3D object detectors from one dataset to another. We observe that naively, this appears to be a very challenging task, resulting in drastic drops in accuracy levels. We provide extensive experiments to investigate the true adaptation challenges and arrive at a surprising conclusion: the primary adaptation hurdle to overcome are differences in car sizes across geographic areas. A simple correction based on the average car size yields a strong correction of the adaptation gap. Our proposed method is simple and easily incorporated into most 3D object detection frameworks. It provides a first baseline for 3D object detection adaptation across countries, and gives hope that the underlying problem may be more within grasp than one may have hoped to believe. Our code is available at https://github.com/cxy1997/3D_adapt_auto_driving.



### High-dimensional Convolutional Networks for Geometric Pattern Recognition
- **Arxiv ID**: http://arxiv.org/abs/2005.08144v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.08144v1)
- **Published**: 2020-05-17 01:46:12+00:00
- **Updated**: 2020-05-17 01:46:12+00:00
- **Authors**: Christopher Choy, Junha Lee, Rene Ranftl, Jaesik Park, Vladlen Koltun
- **Comment**: Accepted for CVPR 2020 oral presentation
- **Journal**: None
- **Summary**: Many problems in science and engineering can be formulated in terms of geometric patterns in high-dimensional spaces. We present high-dimensional convolutional networks (ConvNets) for pattern recognition problems that arise in the context of geometric registration. We first study the effectiveness of convolutional networks in detecting linear subspaces in high-dimensional spaces with up to 32 dimensions: much higher dimensionality than prior applications of ConvNets. We then apply high-dimensional ConvNets to 3D registration under rigid motions and image correspondence estimation. Experiments indicate that our high-dimensional ConvNets outperform prior approaches that relied on deep networks based on global pooling operators.



### PatchGuard: A Provably Robust Defense against Adversarial Patches via Small Receptive Fields and Masking
- **Arxiv ID**: http://arxiv.org/abs/2005.10884v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.10884v5)
- **Published**: 2020-05-17 03:38:34+00:00
- **Updated**: 2021-03-31 14:20:39+00:00
- **Authors**: Chong Xiang, Arjun Nitin Bhagoji, Vikash Sehwag, Prateek Mittal
- **Comment**: USENIX Security Symposium 2021; extended technical report
- **Journal**: None
- **Summary**: Localized adversarial patches aim to induce misclassification in machine learning models by arbitrarily modifying pixels within a restricted region of an image. Such attacks can be realized in the physical world by attaching the adversarial patch to the object to be misclassified, and defending against such attacks is an unsolved/open problem. In this paper, we propose a general defense framework called PatchGuard that can achieve high provable robustness while maintaining high clean accuracy against localized adversarial patches. The cornerstone of PatchGuard involves the use of CNNs with small receptive fields to impose a bound on the number of features corrupted by an adversarial patch. Given a bounded number of corrupted features, the problem of designing an adversarial patch defense reduces to that of designing a secure feature aggregation mechanism. Towards this end, we present our robust masking defense that robustly detects and masks corrupted features to recover the correct prediction. Notably, we can prove the robustness of our defense against any adversary within our threat model. Our extensive evaluation on ImageNet, ImageNette (a 10-class subset of ImageNet), and CIFAR-10 datasets demonstrates that our defense achieves state-of-the-art performance in terms of both provable robust accuracy and clean accuracy.



### Three-Filters-to-Normal: An Accurate and Ultrafast Surface Normal Estimator
- **Arxiv ID**: http://arxiv.org/abs/2005.08165v3
- **DOI**: 10.1109/LRA.2021.3067308
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2005.08165v3)
- **Published**: 2020-05-17 04:46:24+00:00
- **Updated**: 2021-03-08 23:06:18+00:00
- **Authors**: Rui Fan, Hengli Wang, Bohuan Xue, Huaiyang Huang, Yuan Wang, Ming Liu, Ioannis Pitas
- **Comment**: webpage: sites.google.com/view/3f2n, accepted to IEEE RA-L and
  ICRA'21
- **Journal**: None
- **Summary**: This paper proposes three-filters-to-normal (3F2N), an accurate and ultrafast surface normal estimator (SNE), which is designed for structured range sensor data, e.g., depth/disparity images. 3F2N SNE computes surface normals by simply performing three filtering operations (two image gradient filters in horizontal and vertical directions, respectively, and a mean/median filter) on an inverse depth image or a disparity image. Despite the simplicity of 3F2N SNE, no similar method already exists in the literature. To evaluate the performance of our proposed SNE, we created three large-scale synthetic datasets (easy, medium and hard) using 24 3D mesh models, each of which is used to generate 1800--2500 pairs of depth images (resolution: 480X640 pixels) and the corresponding ground-truth surface normal maps from different views. 3F2N SNE demonstrates the state-of-the-art performance, outperforming all other existing geometry-based SNEs, where the average angular errors with respect to the easy, medium and hard datasets are 1.66 degrees, 5.69 degrees and 15.31 degrees, respectively. Furthermore, our C++ and CUDA implementations achieve a processing speed of over 260 Hz and 21 kHz, respectively. Our datasets and source code are publicly available at sites.google.com/view/3f2n.



### FA-GANs: Facial Attractiveness Enhancement with Generative Adversarial Networks on Frontal Faces
- **Arxiv ID**: http://arxiv.org/abs/2005.08168v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.08168v2)
- **Published**: 2020-05-17 05:02:39+00:00
- **Updated**: 2020-06-06 00:58:07+00:00
- **Authors**: Jingwu He, Chuan Wang, Yang Zhang, Jie Guo, Yanwen Guo
- **Comment**: 10 pages, 8 figures
- **Journal**: None
- **Summary**: Facial attractiveness enhancement has been an interesting application in Computer Vision and Graphics over these years. It aims to generate a more attractive face via manipulations on image and geometry structure while preserving face identity. In this paper, we propose the first Generative Adversarial Networks (GANs) for enhancing facial attractiveness in both geometry and appearance aspects, which we call "FA-GANs". FA-GANs contain two branches and enhance facial attractiveness in two perspectives: facial geometry and facial appearance. Each branch consists of individual GANs with the appearance branch adjusting the facial image and the geometry branch adjusting the facial landmarks in appearance and geometry aspects, respectively. Unlike the traditional facial manipulations learning from paired faces, which are infeasible to collect before and after enhancement of the same individual, we achieve this by learning the features of attractiveness faces through unsupervised adversarial learning. The proposed FA-GANs are able to extract attractiveness features and impose them on the enhancement results. To better enhance faces, both the geometry and appearance networks are considered to refine the facial attractiveness by adjusting the geometry layout of faces and the appearance of faces independently. To the best of our knowledge, we are the first to enhance the facial attractiveness with GANs in both geometry and appearance aspects. The experimental results suggest that our FA-GANs can generate compelling perceptual results in both geometry structure and facial appearance and outperform current state-of-the-art methods.



### Neural Networks for Fashion Image Classification and Visual Search
- **Arxiv ID**: http://arxiv.org/abs/2005.08170v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.08170v1)
- **Published**: 2020-05-17 05:25:41+00:00
- **Updated**: 2020-05-17 05:25:41+00:00
- **Authors**: Fengzi Li, Shashi Kant, Shunichi Araki, Sumer Bangera, Swapna Samir Shukla
- **Comment**: None
- **Journal**: None
- **Summary**: We discuss two potentially challenging problems faced by the ecommerce industry. One relates to the problem faced by sellers while uploading pictures of products on the platform for sale and the consequent manual tagging involved. It gives rise to misclassifications leading to its absence from search results. The other problem concerns with the potential bottleneck in placing orders when a customer may not know the right keywords but has a visual impression of an image. An image based search algorithm can unleash the true potential of ecommerce by enabling customers to click a picture of an object and search for similar products without the need for typing. In this paper, we explore machine learning algorithms which can help us solve both these problems.



### Co-occurrence Based Texture Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2005.08186v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.08186v2)
- **Published**: 2020-05-17 08:01:44+00:00
- **Updated**: 2020-07-22 19:34:09+00:00
- **Authors**: Anna Darzi, Itai Lang, Ashutosh Taklikar, Hadar Averbuch-Elor, Shai Avidan
- **Comment**: None
- **Journal**: None
- **Summary**: As image generation techniques mature, there is a growing interest in explainable representations that are easy to understand and intuitive to manipulate. In this work, we turn to co-occurrence statistics, which have long been used for texture analysis, to learn a controllable texture synthesis model. We propose a fully convolutional generative adversarial network, conditioned locally on co-occurrence statistics, to generate arbitrarily large images while having local, interpretable control over the texture appearance. To encourage fidelity to the input condition, we introduce a novel differentiable co-occurrence loss that is integrated seamlessly into our framework in an end-to-end fashion. We demonstrate that our solution offers a stable, intuitive and interpretable latent representation for texture synthesis, which can be used to generate a smooth texture morph between different textures. We further show an interactive texture tool that allows a user to adjust local characteristics of the synthesized texture image using the co-occurrence values directly.



### Hyperspectral Image Classification Based on Sparse Modeling of Spectral Blocks
- **Arxiv ID**: http://arxiv.org/abs/2005.08191v1
- **DOI**: 10.1016/j.neucom.2020.04.138
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.08191v1)
- **Published**: 2020-05-17 08:18:13+00:00
- **Updated**: 2020-05-17 08:18:13+00:00
- **Authors**: Saeideh Ghanbari Azar, Saeed Meshgini, Tohid Yousefi Rezaii, Soosan Beheshti
- **Comment**: None
- **Journal**: None
- **Summary**: Hyperspectral images provide abundant spatial and spectral information that is very valuable for material detection in diverse areas of practical science. The high-dimensions of data lead to many processing challenges that can be addressed via existent spatial and spectral redundancies. In this paper, a sparse modeling framework is proposed for hyperspectral image classification. Spectral blocks are introduced to be used along with spatial groups to jointly exploit spectral and spatial redundancies. To reduce the computational complexity of sparse modeling, spectral blocks are used to break the high-dimensional optimization problems into small-size sub-problems that are faster to solve. Furthermore, the proposed sparse structure enables to extract the most discriminative spectral blocks and further reduce the computational burden. Experiments on three benchmark datasets, i.e., Pavia University Image and Indian Pines Image verify that the proposed method leads to a robust sparse modeling of hyperspectral images and improves the classification accuracy compared to several state-of-the-art methods. Moreover, the experiments demonstrate that the proposed method requires less processing time.



### Learning Individual Speaking Styles for Accurate Lip to Speech Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2005.08209v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2005.08209v1)
- **Published**: 2020-05-17 10:29:19+00:00
- **Updated**: 2020-05-17 10:29:19+00:00
- **Authors**: K R Prajwal, Rudrabha Mukhopadhyay, Vinay Namboodiri, C V Jawahar
- **Comment**: 10 pages (including references), 5 figures, Accepted in CVPR, 2020
- **Journal**: None
- **Summary**: Humans involuntarily tend to infer parts of the conversation from lip movements when the speech is absent or corrupted by external noise. In this work, we explore the task of lip to speech synthesis, i.e., learning to generate natural speech given only the lip movements of a speaker. Acknowledging the importance of contextual and speaker-specific cues for accurate lip-reading, we take a different path from existing works. We focus on learning accurate lip sequences to speech mappings for individual speakers in unconstrained, large vocabulary settings. To this end, we collect and release a large-scale benchmark dataset, the first of its kind, specifically to train and evaluate the single-speaker lip to speech task in natural settings. We propose a novel approach with key design choices to achieve accurate, natural lip to speech synthesis in such unconstrained scenarios for the first time. Extensive evaluation using quantitative, qualitative metrics and human evaluation shows that our method is four times more intelligible than previous works in this space. Please check out our demo video for a quick overview of the paper, method, and qualitative results. https://www.youtube.com/watch?v=HziA-jmlk_4&feature=youtu.be



### Graph Density-Aware Losses for Novel Compositions in Scene Graph Generation
- **Arxiv ID**: http://arxiv.org/abs/2005.08230v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.08230v2)
- **Published**: 2020-05-17 11:45:29+00:00
- **Updated**: 2020-08-18 00:47:17+00:00
- **Authors**: Boris Knyazev, Harm de Vries, Cătălina Cangea, Graham W. Taylor, Aaron Courville, Eugene Belilovsky
- **Comment**: accepted at BMVC 2020, the code is available at
  https://github.com/bknyaz/sgg
- **Journal**: None
- **Summary**: Scene graph generation (SGG) aims to predict graph-structured descriptions of input images, in the form of objects and relationships between them. This task is becoming increasingly useful for progress at the interface of vision and language. Here, it is important - yet challenging - to perform well on novel (zero-shot) or rare (few-shot) compositions of objects and relationships. In this paper, we identify two key issues that limit such generalization. Firstly, we show that the standard loss used in this task is unintentionally a function of scene graph density. This leads to the neglect of individual edges in large sparse graphs during training, even though these contain diverse few-shot examples that are important for generalization. Secondly, the frequency of relationships can create a strong bias in this task, such that a blind model predicting the most frequent relationship achieves good performance. Consequently, some state-of-the-art models exploit this bias to improve results. We show that such models can suffer the most in their ability to generalize to rare compositions, evaluating two different models on the Visual Genome dataset and its more recent, improved version, GQA. To address these issues, we introduce a density-normalized edge loss, which provides more than a two-fold improvement in certain generalization metrics. Compared to other works in this direction, our enhancements require only a few lines of code and no added computational cost. We also highlight the difficulty of accurately evaluating models using existing metrics, especially on zero/few shots, and introduce a novel weighted metric.



### FuCiTNet: Improving the generalization of deep learning networks by the fusion of learned class-inherent transformations
- **Arxiv ID**: http://arxiv.org/abs/2005.08235v1
- **DOI**: 10.1016/j.inffus.2020.06.015
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.08235v1)
- **Published**: 2020-05-17 12:04:20+00:00
- **Updated**: 2020-05-17 12:04:20+00:00
- **Authors**: Manuel Rey-Area, Emilio Guirado, Siham Tabik, Javier Ruiz-Hidalgo
- **Comment**: None
- **Journal**: None
- **Summary**: It is widely known that very small datasets produce overfitting in Deep Neural Networks (DNNs), i.e., the network becomes highly biased to the data it has been trained on. This issue is often alleviated using transfer learning, regularization techniques and/or data augmentation. This work presents a new approach, independent but complementary to the previous mentioned techniques, for improving the generalization of DNNs on very small datasets in which the involved classes share many visual features. The proposed methodology, called FuCiTNet (Fusion Class inherent Transformations Network), inspired by GANs, creates as many generators as classes in the problem. Each generator, $k$, learns the transformations that bring the input image into the k-class domain. We introduce a classification loss in the generators to drive the leaning of specific k-class transformations. Our experiments demonstrate that the proposed transformations improve the generalization of the classification model in three diverse datasets.



### A Better Use of Audio-Visual Cues: Dense Video Captioning with Bi-modal Transformer
- **Arxiv ID**: http://arxiv.org/abs/2005.08271v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2005.08271v2)
- **Published**: 2020-05-17 15:00:05+00:00
- **Updated**: 2020-08-11 09:17:48+00:00
- **Authors**: Vladimir Iashin, Esa Rahtu
- **Comment**: Accepted by BMVC 2020. More experiments. Code:
  https://github.com/v-iashin/bmt Project page: https://v-iashin.github.io/bmt
- **Journal**: None
- **Summary**: Dense video captioning aims to localize and describe important events in untrimmed videos. Existing methods mainly tackle this task by exploiting only visual features, while completely neglecting the audio track. Only a few prior works have utilized both modalities, yet they show poor results or demonstrate the importance on a dataset with a specific domain. In this paper, we introduce Bi-modal Transformer which generalizes the Transformer architecture for a bi-modal input. We show the effectiveness of the proposed model with audio and visual modalities on the dense video captioning task, yet the module is capable of digesting any two modalities in a sequence-to-sequence task. We also show that the pre-trained bi-modal encoder as a part of the bi-modal transformer can be used as a feature extractor for a simple proposal generation module. The performance is demonstrated on a challenging ActivityNet Captions dataset where our model achieves outstanding performance. The code is available: v-iashin.github.io/bmt



### Review on Computer Vision in Gastric Cancer: Potential Efficient Tools for Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/2005.09459v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.09459v2)
- **Published**: 2020-05-17 16:14:15+00:00
- **Updated**: 2020-05-31 20:02:05+00:00
- **Authors**: Yihua Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Rapid diagnosis of gastric cancer is a great challenge for clinical doctors. Dramatic progress of computer vision on gastric cancer has been made recently and this review focuses on advances during the past five years. Different methods for data generation and augmentation are presented, and various approaches to extract discriminative features compared and evaluated. Classification and segmentation techniques are carefully discussed for assisting more precise diagnosis and timely treatment. For classification, various methods have been developed to better proceed specific images, such as images with rotation and estimated real-timely (endoscopy), high resolution images (histopathology), low diagnostic accuracy images (X-ray), poor contrast images of the soft-tissue with cavity (CT) or those images with insufficient annotation. For detection and segmentation, traditional methods and machine learning methods are compared. Application of those methods will greatly reduce the labor and time consumption for the diagnosis of gastric cancers.



### AC-VRNN: Attentive Conditional-VRNN for Multi-Future Trajectory Prediction
- **Arxiv ID**: http://arxiv.org/abs/2005.08307v2
- **DOI**: 10.1016/j.cviu.2021.103245
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.08307v2)
- **Published**: 2020-05-17 17:21:23+00:00
- **Updated**: 2021-07-08 08:23:15+00:00
- **Authors**: Alessia Bertugli, Simone Calderara, Pasquale Coscia, Lamberto Ballan, Rita Cucchiara
- **Comment**: Accepted at Computer Vision and Image Understanding (CVIU)
- **Journal**: None
- **Summary**: Anticipating human motion in crowded scenarios is essential for developing intelligent transportation systems, social-aware robots and advanced video surveillance applications. A key component of this task is represented by the inherently multi-modal nature of human paths which makes socially acceptable multiple futures when human interactions are involved. To this end, we propose a generative architecture for multi-future trajectory predictions based on Conditional Variational Recurrent Neural Networks (C-VRNNs). Conditioning mainly relies on prior belief maps, representing most likely moving directions and forcing the model to consider past observed dynamics in generating future positions. Human interactions are modeled with a graph-based attention mechanism enabling an online attentive hidden state refinement of the recurrent estimation. To corroborate our model, we perform extensive experiments on publicly-available datasets (e.g., ETH/UCY, Stanford Drone Dataset, STATS SportVU NBA, Intersection Drone Dataset and TrajNet++) and demonstrate its effectiveness in crowded scenes compared to several state-of-the-art methods.



### A Survey on Unknown Presentation Attack Detection for Fingerprint
- **Arxiv ID**: http://arxiv.org/abs/2005.08337v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.08337v1)
- **Published**: 2020-05-17 18:46:23+00:00
- **Updated**: 2020-05-17 18:46:23+00:00
- **Authors**: Jag Mohan Singh, Ahmed Madhun, Guoqiang Li, Raghavendra Ramachandra
- **Comment**: Submitted to 3rd International Conference on Intelligent Technologies
  and Applications INTAP 2020
- **Journal**: None
- **Summary**: Fingerprint recognition systems are widely deployed in various real-life applications as they have achieved high accuracy. The widely used applications include border control, automated teller machine (ATM), and attendance monitoring systems. However, these critical systems are prone to spoofing attacks (a.k.a presentation attacks (PA)). PA for fingerprint can be performed by presenting gummy fingers made from different materials such as silicone, gelatine, play-doh, ecoflex, 2D printed paper, 3D printed material, or latex. Biometrics Researchers have developed Presentation Attack Detection (PAD) methods as a countermeasure to PA. PAD is usually done by training a machine learning classifier for known attacks for a given dataset, and they achieve high accuracy in this task. However, generalizing to unknown attacks is an essential problem from applicability to real-world systems, mainly because attacks cannot be exhaustively listed in advance. In this survey paper, we present a comprehensive survey on existing PAD algorithms for fingerprint recognition systems, specifically from the standpoint of detecting unknown PAD. We categorize PAD algorithms, point out their advantages/disadvantages, and future directions for this area.



### Subject Identification Across Large Expression Variations Using 3D Facial Landmarks
- **Arxiv ID**: http://arxiv.org/abs/2005.08339v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.08339v1)
- **Published**: 2020-05-17 18:56:04+00:00
- **Updated**: 2020-05-17 18:56:04+00:00
- **Authors**: Sk Rahatul Jannat, Diego Fabiano, Shaun Canavan, Tempestt Neal
- **Comment**: None
- **Journal**: None
- **Summary**: Landmark localization is an important first step towards geometric based vision research including subject identification. Considering this, we propose to use 3D facial landmarks for the task of subject identification, over a range of expressed emotion. Landmarks are detected, using a Temporal Deformable Shape Model and used to train a Support Vector Machine (SVM), Random Forest (RF), and Long Short-term Memory (LSTM) neural network for subject identification. As we are interested in subject identification with large variations in expression, we conducted experiments on 3 emotion-based databases, namely the BU-4DFE, BP4D, and BP4D+ 3D/4D face databases. We show that our proposed method outperforms current state of the art methods for subject identification on BU-4DFE and BP4D. To the best of our knowledge, this is the first work to investigate subject identification on the BP4D+, resulting in a baseline for the community.



### Impact of multiple modalities on emotion recognition: investigation into 3d facial landmarks, action units, and physiological data
- **Arxiv ID**: http://arxiv.org/abs/2005.08341v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.08341v1)
- **Published**: 2020-05-17 18:59:57+00:00
- **Updated**: 2020-05-17 18:59:57+00:00
- **Authors**: Diego Fabiano, Manikandan Jaishanker, Shaun Canavan
- **Comment**: None
- **Journal**: None
- **Summary**: To fully understand the complexities of human emotion, the integration of multiple physical features from different modalities can be advantageous. Considering this, we present an analysis of 3D facial data, action units, and physiological data as it relates to their impact on emotion recognition. We analyze each modality independently, as well as the fusion of each for recognizing human emotion. This analysis includes which features are most important for specific emotions (e.g. happy). Our analysis indicates that both 3D facial landmarks and physiological data are encouraging for expression/emotion recognition. On the other hand, while action units can positively impact emotion recognition when fused with other modalities, the results suggest it is difficult to detect emotion using them in a unimodal fashion.



### Facial Action Unit Detection using 3D Facial Landmarks
- **Arxiv ID**: http://arxiv.org/abs/2005.08343v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.08343v1)
- **Published**: 2020-05-17 19:02:10+00:00
- **Updated**: 2020-05-17 19:02:10+00:00
- **Authors**: Saurabh Hinduja, Shaun Canavan
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose to detect facial action units (AU) using 3D facial landmarks. Specifically, we train a 2D convolutional neural network (CNN) on 3D facial landmarks, tracked using a shape index-based statistical shape model, for binary and multi-class AU detection. We show that the proposed approach is able to accurately model AU occurrences, as the movement of the facial landmarks corresponds directly to the movement of the AUs. By training a CNN on 3D landmarks, we can achieve accurate AU detection on two state-of-the-art emotion datasets, namely BP4D and BP4D+. Using the proposed method, we detect multiple AUs on over 330,000 frames, reporting improved results over state-of-the-art methods.



### Detecting Forged Facial Videos using convolutional neural network
- **Arxiv ID**: http://arxiv.org/abs/2005.08344v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.08344v1)
- **Published**: 2020-05-17 19:04:59+00:00
- **Updated**: 2020-05-17 19:04:59+00:00
- **Authors**: Neilesh Sambhu, Shaun Canavan
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose to detect forged videos, of faces, in online videos. To facilitate this detection, we propose to use smaller (fewer parameters to learn) convolutional neural networks (CNN), for a data-driven approach to forged video detection. To validate our approach, we investigate the FaceForensics public dataset detailing both frame-based and video-based results. The proposed method is shown to outperform current state of the art. We also perform an ablation study, analyzing the impact of batch size, number of filters, and number of network layers on the accuracy of detecting forged videos.



### T-VSE: Transformer-Based Visual Semantic Embedding
- **Arxiv ID**: http://arxiv.org/abs/2005.08399v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.08399v1)
- **Published**: 2020-05-17 23:40:33+00:00
- **Updated**: 2020-05-17 23:40:33+00:00
- **Authors**: Muhammet Bastan, Arnau Ramisa, Mehmet Tek
- **Comment**: To appear: CVPR 2020 Workshop on Computer Vision for Fashion, Art and
  Design (CVFAD 2020)
- **Journal**: None
- **Summary**: Transformer models have recently achieved impressive performance on NLP tasks, owing to new algorithms for self-supervised pre-training on very large text corpora. In contrast, recent literature suggests that simple average word models outperform more complicated language models, e.g., RNNs and Transformers, on cross-modal image/text search tasks on standard benchmarks, like MS COCO. In this paper, we show that dataset scale and training strategy are critical and demonstrate that transformer-based cross-modal embeddings outperform word average and RNN-based embeddings by a large margin, when trained on a large dataset of e-commerce product image-title pairs.



