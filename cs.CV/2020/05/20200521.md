# Arxiv Papers in cs.CV on 2020-05-21
### Interpretable and Accurate Fine-grained Recognition via Region Grouping
- **Arxiv ID**: http://arxiv.org/abs/2005.10411v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.10411v1)
- **Published**: 2020-05-21 01:18:26+00:00
- **Updated**: 2020-05-21 01:18:26+00:00
- **Authors**: Zixuan Huang, Yin Li
- **Comment**: Accepted to CVPR 2020 (Oral)
- **Journal**: None
- **Summary**: We present an interpretable deep model for fine-grained visual recognition. At the core of our method lies the integration of region-based part discovery and attribution within a deep neural network. Our model is trained using image-level object labels, and provides an interpretation of its results via the segmentation of object parts and the identification of their contributions towards classification. To facilitate the learning of object parts without direct supervision, we explore a simple prior of the occurrence of object parts. We demonstrate that this prior, when combined with our region-based part discovery and attribution, leads to an interpretable model that remains highly accurate. Our model is evaluated on major fine-grained recognition datasets, including CUB-200, CelebA and iNaturalist. Our results compare favorably to state-of-the-art methods on classification tasks, and our method outperforms previous approaches on the localization of object parts.



### Towards Streaming Perception
- **Arxiv ID**: http://arxiv.org/abs/2005.10420v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.10420v2)
- **Published**: 2020-05-21 01:51:35+00:00
- **Updated**: 2020-08-25 01:16:43+00:00
- **Authors**: Mengtian Li, Yu-Xiong Wang, Deva Ramanan
- **Comment**: ECCV 2020 (Oral). Code and data can be found on the project page at
  https://www.cs.cmu.edu/~mengtial/proj/streaming/
- **Journal**: None
- **Summary**: Embodied perception refers to the ability of an autonomous agent to perceive its environment so that it can (re)act. The responsiveness of the agent is largely governed by latency of its processing pipeline. While past work has studied the algorithmic trade-off between latency and accuracy, there has not been a clear metric to compare different methods along the Pareto optimal latency-accuracy curve. We point out a discrepancy between standard offline evaluation and real-time applications: by the time an algorithm finishes processing a particular frame, the surrounding world has changed. To these ends, we present an approach that coherently integrates latency and accuracy into a single metric for real-time online perception, which we refer to as "streaming accuracy". The key insight behind this metric is to jointly evaluate the output of the entire perception stack at every time instant, forcing the stack to consider the amount of streaming data that should be ignored while computation is occurring. More broadly, building upon this metric, we introduce a meta-benchmark that systematically converts any single-frame task into a streaming perception task. We focus on the illustrative tasks of object detection and instance segmentation in urban video streams, and contribute a novel dataset with high-quality and temporally-dense annotations. Our proposed solutions and their empirical analysis demonstrate a number of surprising conclusions: (1) there exists an optimal "sweet spot" that maximizes streaming accuracy along the Pareto optimal latency-accuracy curve, (2) asynchronous tracking and future forecasting naturally emerge as internal representations that enable streaming perception, and (3) dynamic scheduling can be used to overcome temporal aliasing, yielding the paradoxical result that latency is sometimes minimized by sitting idle and "doing nothing".



### Gender Slopes: Counterfactual Fairness for Computer Vision Models by Attribute Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2005.10430v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.10430v1)
- **Published**: 2020-05-21 02:33:28+00:00
- **Updated**: 2020-05-21 02:33:28+00:00
- **Authors**: Jungseock Joo, Kimmo Kärkkäinen
- **Comment**: None
- **Journal**: None
- **Summary**: Automated computer vision systems have been applied in many domains including security, law enforcement, and personal devices, but recent reports suggest that these systems may produce biased results, discriminating against people in certain demographic groups. Diagnosing and understanding the underlying true causes of model biases, however, are challenging tasks because modern computer vision systems rely on complex black-box models whose behaviors are hard to decode. We propose to use an encoder-decoder network developed for image attribute manipulation to synthesize facial images varying in the dimensions of gender and race while keeping other signals intact. We use these synthesized images to measure counterfactual fairness of commercial computer vision classifiers by examining the degree to which these classifiers are affected by gender and racial cues controlled in the images, e.g., feminine faces may elicit higher scores for the concept of nurse and lower scores for STEM-related concepts. We also report the skewed gender representations in an online search service on profession-related keywords, which may explain the origin of the biases encoded in the models.



### Deep Learning-Based Automated Image Segmentation for Concrete Petrographic Analysis
- **Arxiv ID**: http://arxiv.org/abs/2005.10434v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.10434v3)
- **Published**: 2020-05-21 02:46:29+00:00
- **Updated**: 2020-05-28 20:16:26+00:00
- **Authors**: Yu Song, Zilong Huang, Chuanyue Shen, Humphrey Shi, David A Lange
- **Comment**: Accepted as a journal publication by Cement & Concrete Research
- **Journal**: None
- **Summary**: The standard petrography test method for measuring air voids in concrete (ASTM C457) requires a meticulous and long examination of sample phase composition under a stereomicroscope. The high expertise and specialized equipment discourage this test for routine concrete quality control. Though the task can be alleviated with the aid of color-based image segmentation, additional surface color treatment is required. Recently, deep learning algorithms using convolutional neural networks (CNN) have achieved unprecedented segmentation performance on image testing benchmarks. In this study, we investigated the feasibility of using CNN to conduct concrete segmentation without the use of color treatment. The CNN demonstrated a strong potential to process a wide range of concretes, including those not involved in model training. The experimental results showed that CNN outperforms the color-based segmentation by a considerable margin, and has comparable accuracy to human experts. Furthermore, the segmentation time is reduced to mere seconds.



### HF-UNet: Learning Hierarchically Inter-Task Relevance in Multi-Task U-Net for Accurate Prostate Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2005.10439v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.10439v2)
- **Published**: 2020-05-21 02:53:52+00:00
- **Updated**: 2020-05-23 13:26:25+00:00
- **Authors**: Kelei He, Chunfeng Lian, Bing Zhang, Xin Zhang, Xiaohuan Cao, Dong Nie, Yang Gao, Junfeng Zhang, Dinggang Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate segmentation of the prostate is a key step in external beam radiation therapy treatments. In this paper, we tackle the challenging task of prostate segmentation in CT images by a two-stage network with 1) the first stage to fast localize, and 2) the second stage to accurately segment the prostate. To precisely segment the prostate in the second stage, we formulate prostate segmentation into a multi-task learning framework, which includes a main task to segment the prostate, and an auxiliary task to delineate the prostate boundary. Here, the second task is applied to provide additional guidance of unclear prostate boundary in CT images. Besides, the conventional multi-task deep networks typically share most of the parameters (i.e., feature representations) across all tasks, which may limit their data fitting ability, as the specificities of different tasks are inevitably ignored. By contrast, we solve them by a hierarchically-fused U-Net structure, namely HF-UNet. The HF-UNet has two complementary branches for two tasks, with the novel proposed attention-based task consistency learning block to communicate at each level between the two decoding branches. Therefore, HF-UNet endows the ability to learn hierarchically the shared representations for different tasks, and preserve the specificities of learned representations for different tasks simultaneously. We did extensive evaluations of the proposed method on a large planning CT image dataset, including images acquired from 339 patients. The experimental results show HF-UNet outperforms the conventional multi-task network architectures and the state-of-the-art methods.



### CPOT: Channel Pruning via Optimal Transport
- **Arxiv ID**: http://arxiv.org/abs/2005.10451v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.10451v1)
- **Published**: 2020-05-21 03:43:09+00:00
- **Updated**: 2020-05-21 03:43:09+00:00
- **Authors**: Yucong Shen, Li Shen, Hao-Zhi Huang, Xuan Wang, Wei Liu
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: Recent advances in deep neural networks (DNNs) lead to tremendously growing network parameters, making the deployments of DNNs on platforms with limited resources extremely difficult. Therefore, various pruning methods have been developed to compress the deep network architectures and accelerate the inference process. Most of the existing channel pruning methods discard the less important filters according to well-designed filter ranking criteria. However, due to the limited interpretability of deep learning models, designing an appropriate ranking criterion to distinguish redundant filters is difficult. To address such a challenging issue, we propose a new technique of Channel Pruning via Optimal Transport, dubbed CPOT. Specifically, we locate the Wasserstein barycenter for channels of each layer in the deep models, which is the mean of a set of probability distributions under the optimal transport metric. Then, we prune the redundant information located by Wasserstein barycenters. At last, we empirically demonstrate that, for classification tasks, CPOT outperforms the state-of-the-art methods on pruning ResNet-20, ResNet-32, ResNet-56, and ResNet-110. Furthermore, we show that the proposed CPOT technique is good at compressing the StarGAN models by pruning in the more difficult case of image-to-image translation tasks.



### Single Image Super-Resolution via Residual Neuron Attention Networks
- **Arxiv ID**: http://arxiv.org/abs/2005.10455v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.10455v1)
- **Published**: 2020-05-21 04:01:19+00:00
- **Updated**: 2020-05-21 04:01:19+00:00
- **Authors**: Wenjie Ai, Xiaoguang Tu, Shilei Cheng, Mei Xie
- **Comment**: 6 pages, 4 figures, Accepted by IEEE ICIP 2020
- **Journal**: None
- **Summary**: Deep Convolutional Neural Networks (DCNNs) have achieved impressive performance in Single Image Super-Resolution (SISR). To further improve the performance, existing CNN-based methods generally focus on designing deeper architecture of the network. However, we argue blindly increasing network's depth is not the most sensible way. In this paper, we propose a novel end-to-end Residual Neuron Attention Networks (RNAN) for more efficient and effective SISR. Structurally, our RNAN is a sequential integration of the well-designed Global Context-enhanced Residual Groups (GCRGs), which extracts super-resolved features from coarse to fine. Our GCRG is designed with two novelties. Firstly, the Residual Neuron Attention (RNA) mechanism is proposed in each block of GCRG to reveal the relevance of neurons for better feature representation. Furthermore, the Global Context (GC) block is embedded into RNAN at the end of each GCRG for effectively modeling the global contextual information. Experiments results demonstrate that our RNAN achieves the comparable results with state-of-the-art methods in terms of both quantitative metrics and visual quality, however, with simplified network architecture.



### AOWS: Adaptive and optimal network width search with latency constraints
- **Arxiv ID**: http://arxiv.org/abs/2005.10481v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.10481v1)
- **Published**: 2020-05-21 06:46:16+00:00
- **Updated**: 2020-05-21 06:46:16+00:00
- **Authors**: Maxim Berman, Leonid Pishchulin, Ning Xu, Matthew B. Blaschko, Gerard Medioni
- **Comment**: Accepted to CVPR 2020 (oral)
- **Journal**: None
- **Summary**: Neural architecture search (NAS) approaches aim at automatically finding novel CNN architectures that fit computational constraints while maintaining a good performance on the target platform. We introduce a novel efficient one-shot NAS approach to optimally search for channel numbers, given latency constraints on a specific hardware. We first show that we can use a black-box approach to estimate a realistic latency model for a specific inference platform, without the need for low-level access to the inference computation. Then, we design a pairwise MRF to score any channel configuration and use dynamic programming to efficiently decode the best performing configuration, yielding an optimal solution for the network width search. Finally, we propose an adaptive channel configuration sampling scheme to gradually specialize the training phase to the target computational constraints. Experiments on ImageNet classification show that our approach can find networks fitting the resource constraints on different target platforms while improving accuracy over the state-of-the-art efficient networks.



### GroupFace: Learning Latent Groups and Constructing Group-based Representations for Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2005.10497v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.10497v2)
- **Published**: 2020-05-21 07:30:34+00:00
- **Updated**: 2020-05-25 04:51:33+00:00
- **Authors**: Yonghyun Kim, Wonpyo Park, Myung-Cheol Roh, Jongju Shin
- **Comment**: Accepted to CVPR 2020
- **Journal**: None
- **Summary**: In the field of face recognition, a model learns to distinguish millions of face images with fewer dimensional embedding features, and such vast information may not be properly encoded in the conventional model with a single branch. We propose a novel face-recognition-specialized architecture called GroupFace that utilizes multiple group-aware representations, simultaneously, to improve the quality of the embedding feature. The proposed method provides self-distributed labels that balance the number of samples belonging to each group without additional human annotations, and learns the group-aware representations that can narrow down the search space of the target identity. We prove the effectiveness of the proposed method by showing extensive ablation studies and visualizations. All the components of the proposed method can be trained in an end-to-end manner with a marginal increase of computational complexity. Finally, the proposed method achieves the state-of-the-art results with significant improvements in 1:1 face verification and 1:N face identification tasks on the following public datasets: LFW, YTF, CALFW, CPLFW, CFP, AgeDB-30, MegaFace, IJB-B and IJB-C.



### Panoptic Instance Segmentation on Pigs
- **Arxiv ID**: http://arxiv.org/abs/2005.10499v1
- **DOI**: 10.3390/s20133710
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.10499v1)
- **Published**: 2020-05-21 07:36:03+00:00
- **Updated**: 2020-05-21 07:36:03+00:00
- **Authors**: Johannes Brünger, Maria Gentz, Imke Traulsen, Reinhard Koch
- **Comment**: 18 pages, 10 figures. Submitted to MDPI Sensors
- **Journal**: None
- **Summary**: The behavioural research of pigs can be greatly simplified if automatic recognition systems are used. Especially systems based on computer vision have the advantage that they allow an evaluation without affecting the normal behaviour of the animals. In recent years, methods based on deep learning have been introduced and have shown pleasingly good results. Especially object and keypoint detectors have been used to detect the individual animals. Despite good results, bounding boxes and sparse keypoints do not trace the contours of the animals, resulting in a lot of information being lost. Therefore this work follows the relatively new definition of a panoptic segmentation and aims at the pixel accurate segmentation of the individual pigs. For this a framework of a neural network for semantic segmentation, different network heads and postprocessing methods is presented. With the resulting instance segmentation masks further information like the size or weight of the animals could be estimated. The method is tested on a specially created data set with 1000 hand-labeled images and achieves detection rates of around 95% (F1 Score) despite disturbances such as occlusions and dirty lenses.



### Few-shot Compositional Font Generation with Dual Memory
- **Arxiv ID**: http://arxiv.org/abs/2005.10510v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.10510v2)
- **Published**: 2020-05-21 08:13:40+00:00
- **Updated**: 2020-07-16 11:47:52+00:00
- **Authors**: Junbum Cha, Sanghyuk Chun, Gayoung Lee, Bado Lee, Seonghyeon Kim, Hwalsuk Lee
- **Comment**: ECCV 2020 camera-ready
- **Journal**: None
- **Summary**: Generating a new font library is a very labor-intensive and time-consuming job for glyph-rich scripts. Despite the remarkable success of existing font generation methods, they have significant drawbacks; they require a large number of reference images to generate a new font set, or they fail to capture detailed styles with only a few samples. In this paper, we focus on compositional scripts, a widely used letter system in the world, where each glyph can be decomposed by several components. By utilizing the compositionality of compositional scripts, we propose a novel font generation framework, named Dual Memory-augmented Font Generation Network (DM-Font), which enables us to generate a high-quality font library with only a few samples. We employ memory components and global-context awareness in the generator to take advantage of the compositionality. In the experiments on Korean-handwriting fonts and Thai-printing fonts, we observe that our method generates a significantly better quality of samples with faithful stylization compared to the state-of-the-art generation methods quantitatively and qualitatively. Source code is available at https://github.com/clovaai/dmfont.



### Powering One-shot Topological NAS with Stabilized Share-parameter Proxy
- **Arxiv ID**: http://arxiv.org/abs/2005.10511v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.10511v1)
- **Published**: 2020-05-21 08:18:55+00:00
- **Updated**: 2020-05-21 08:18:55+00:00
- **Authors**: Ronghao Guo, Chen Lin, Chuming Li, Keyu Tian, Ming Sun, Lu Sheng, Junjie Yan
- **Comment**: None
- **Journal**: None
- **Summary**: One-shot NAS method has attracted much interest from the research community due to its remarkable training efficiency and capacity to discover high performance models. However, the search spaces of previous one-shot based works usually relied on hand-craft design and were short for flexibility on the network topology. In this work, we try to enhance the one-shot NAS by exploring high-performing network architectures in our large-scale Topology Augmented Search Space (i.e., over 3.4*10^10 different topological structures). Specifically, the difficulties for architecture searching in such a complex space has been eliminated by the proposed stabilized share-parameter proxy, which employs Stochastic Gradient Langevin Dynamics to enable fast shared parameter sampling, so as to achieve stabilized measurement of architecture performance even in search space with complex topological structures. The proposed method, namely Stablized Topological Neural Architecture Search (ST-NAS), achieves state-of-the-art performance under Multiply-Adds (MAdds) constraint on ImageNet. Our lite model ST-NAS-A achieves 76.4% top-1 accuracy with only 326M MAdds. Our moderate model ST-NAS-B achieves 77.9% top-1 accuracy just required 503M MAdds. Both of our models offer superior performances in comparison to other concurrent works on one-shot NAS.



### Unsupervised segmentation via semantic-apparent feature fusion
- **Arxiv ID**: http://arxiv.org/abs/2005.10513v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.10513v1)
- **Published**: 2020-05-21 08:28:49+00:00
- **Updated**: 2020-05-21 08:28:49+00:00
- **Authors**: Xi Li, Huimin Ma, Hongbing Ma, Yidong Wang
- **Comment**: in Chinese. Accepted by NCIG 2020
- **Journal**: None
- **Summary**: Foreground segmentation is an essential task in the field of image understanding. Under unsupervised conditions, different images and instances always have variable expressions, which make it difficult to achieve stable segmentation performance based on fixed rules or single type of feature. In order to solve this problem, the research proposes an unsupervised foreground segmentation method based on semantic-apparent feature fusion (SAFF). Here, we found that key regions of foreground object can be accurately responded via semantic features, while apparent features (represented by saliency and edge) provide richer detailed expression. To combine the advantages of the two type of features, an encoding method for unary region features and binary context features is established, which realizes a comprehensive description of the two types of expressions. Then, a method for adaptive parameter learning is put forward to calculate the most suitable feature weights and generate foreground confidence score map. Furthermore, segmentation network is used to learn foreground common features from different instances. By fusing semantic and apparent features, as well as cascading the modules of intra-image adaptive feature weight learning and inter-image common feature learning, the research achieves performance that significantly exceeds baselines on the PASCAL VOC 2012 dataset.



### HyperSTAR: Task-Aware Hyperparameters for Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/2005.10524v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.10524v1)
- **Published**: 2020-05-21 08:56:50+00:00
- **Updated**: 2020-05-21 08:56:50+00:00
- **Authors**: Gaurav Mittal, Chang Liu, Nikolaos Karianakis, Victor Fragoso, Mei Chen, Yun Fu
- **Comment**: Published at CVPR 2020 (Oral)
- **Journal**: None
- **Summary**: While deep neural networks excel in solving visual recognition tasks, they require significant effort to find hyperparameters that make them work optimally. Hyperparameter Optimization (HPO) approaches have automated the process of finding good hyperparameters but they do not adapt to a given task (task-agnostic), making them computationally inefficient. To reduce HPO time, we present HyperSTAR (System for Task Aware Hyperparameter Recommendation), a task-aware method to warm-start HPO for deep neural networks. HyperSTAR ranks and recommends hyperparameters by predicting their performance conditioned on a joint dataset-hyperparameter space. It learns a dataset (task) representation along with the performance predictor directly from raw images in an end-to-end fashion. The recommendations, when integrated with an existing HPO method, make it task-aware and significantly reduce the time to achieve optimal performance. We conduct extensive experiments on 10 publicly available large-scale image classification datasets over two different network architectures, validating that HyperSTAR evaluates 50% less configurations to achieve the best performance compared to existing methods. We further demonstrate that HyperSTAR makes Hyperband (HB) task-aware, achieving the optimal accuracy in just 25% of the budget required by both vanilla HB and Bayesian Optimized HB~(BOHB).



### Cross-Domain Few-Shot Learning with Meta Fine-Tuning
- **Arxiv ID**: http://arxiv.org/abs/2005.10544v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.10544v4)
- **Published**: 2020-05-21 09:55:26+00:00
- **Updated**: 2020-08-25 15:45:55+00:00
- **Authors**: John Cai, Sheng Mei Shen
- **Comment**: CVPR 2020 Workshop on Visual Learning with Limited Labels (VL3)
- **Journal**: None
- **Summary**: In this paper, we tackle the new Cross-Domain Few-Shot Learning benchmark proposed by the CVPR 2020 Challenge. To this end, we build upon state-of-the-art methods in domain adaptation and few-shot learning to create a system that can be trained to perform both tasks. Inspired by the need to create models designed to be fine-tuned, we explore the integration of transfer-learning (fine-tuning) with meta-learning algorithms, to train a network that has specific layers that are designed to be adapted at a later fine-tuning stage. To do so, we modify the episodic training process to include a first-order MAML-based meta-learning algorithm, and use a Graph Neural Network model as the subsequent meta-learning module. We find that our proposed method helps to boost accuracy significantly, especially when combined with data augmentation. In our final results, we combine the novel method with the baseline method in a simple ensemble, and achieve an average accuracy of 73.78% on the benchmark. This is a 6.51% improvement over existing benchmarks that were trained solely on miniImagenet.



### Perceptual Quality Assessment of Omnidirectional Images as Moving Camera Videos
- **Arxiv ID**: http://arxiv.org/abs/2005.10547v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.10547v2)
- **Published**: 2020-05-21 10:03:40+00:00
- **Updated**: 2021-01-05 03:23:16+00:00
- **Authors**: Xiangjie Sui, Kede Ma, Yiru Yao, Yuming Fang
- **Comment**: 11 pages, 11 figure, 9 tables. This paper has been accepted by IEEE
  Transactions on Visualization and Computer Graphics
- **Journal**: None
- **Summary**: Omnidirectional images (also referred to as static 360{\deg} panoramas) impose viewing conditions much different from those of regular 2D images. How do humans perceive image distortions in immersive virtual reality (VR) environments is an important problem which receives less attention. We argue that, apart from the distorted panorama itself, two types of VR viewing conditions are crucial in determining the viewing behaviors of users and the perceived quality of the panorama: the starting point and the exploration time. We first carry out a psychophysical experiment to investigate the interplay among the VR viewing conditions, the user viewing behaviors, and the perceived quality of 360{\deg} images. Then, we provide a thorough analysis of the collected human data, leading to several interesting findings. Moreover, we propose a computational framework for objective quality assessment of 360{\deg} images, embodying viewing conditions and behaviors in a delightful way. Specifically, we first transform an omnidirectional image to several video representations using different user viewing behaviors under different viewing conditions. We then leverage advanced 2D full-reference video quality models to compute the perceived quality. We construct a set of specific quality measures within the proposed framework, and demonstrate their promises on three VR quality databases.



### Region Proposals for Saliency Map Refinement for Weakly-supervised Disease Localisation and Classification
- **Arxiv ID**: http://arxiv.org/abs/2005.10550v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.10550v2)
- **Published**: 2020-05-21 10:07:43+00:00
- **Updated**: 2020-05-22 01:15:47+00:00
- **Authors**: Renato Hermoza, Gabriel Maicas, Jacinto C. Nascimento, Gustavo Carneiro
- **Comment**: Early accept at MICCAI 2020
- **Journal**: None
- **Summary**: The deployment of automated systems to diagnose diseases from medical images is challenged by the requirement to localise the diagnosed diseases to justify or explain the classification decision. This requirement is hard to fulfil because most of the training sets available to develop these systems only contain global annotations, making the localisation of diseases a weakly supervised approach. The main methods designed for weakly supervised disease classification and localisation rely on saliency or attention maps that are not specifically trained for localisation, or on region proposals that can not be refined to produce accurate detections. In this paper, we introduce a new model that combines region proposal and saliency detection to overcome both limitations for weakly supervised disease classification and localisation. Using the ChestX-ray14 data set, we show that our proposed model establishes the new state-of-the-art for weakly-supervised disease diagnosis and localisation.



### MBA-RainGAN: Multi-branch Attention Generative Adversarial Network for Mixture of Rain Removal from Single Images
- **Arxiv ID**: http://arxiv.org/abs/2005.10582v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.10582v2)
- **Published**: 2020-05-21 11:44:21+00:00
- **Updated**: 2020-05-23 00:21:00+00:00
- **Authors**: Yiyang Shen, Yidan Feng, Sen Deng, Dong Liang, Jing Qin, Haoran Xie, Mingqiang Wei
- **Comment**: None
- **Journal**: None
- **Summary**: Rain severely hampers the visibility of scene objects when images are captured through glass in heavily rainy days. We observe three intriguing phenomenons that, 1) rain is a mixture of raindrops, rain streaks and rainy haze; 2) the depth from the camera determines the degrees of object visibility, where objects nearby and faraway are visually blocked by rain streaks and rainy haze, respectively; and 3) raindrops on the glass randomly affect the object visibility of the whole image space. We for the first time consider that, the overall visibility of objects is determined by the mixture of rain (MOR). However, existing solutions and established datasets lack full consideration of the MOR. In this work, we first formulate a new rain imaging model; by then, we enrich the popular RainCityscapes by considering raindrops, named RainCityscapes++. Furthermore, we propose a multi-branch attention generative adversarial network (termed an MBA-RainGAN) to fully remove the MOR. The experiment shows clear visual and numerical improvements of our approach over the state-of-the-arts on RainCityscapes++. The code and dataset will be available.



### Bridging the gap between Natural and Medical Images through Deep Colorization
- **Arxiv ID**: http://arxiv.org/abs/2005.10589v2
- **DOI**: None
- **Categories**: **cs.CV**, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2005.10589v2)
- **Published**: 2020-05-21 12:03:14+00:00
- **Updated**: 2020-10-19 21:47:58+00:00
- **Authors**: Lia Morra, Luca Piano, Fabrizio Lamberti, Tatiana Tommasi
- **Comment**: accepted for publication at ICPR2020
- **Journal**: None
- **Summary**: Deep learning has thrived by training on large-scale datasets. However, in many applications, as for medical image diagnosis, getting massive amount of data is still prohibitive due to privacy, lack of acquisition homogeneity and annotation cost. In this scenario, transfer learning from natural image collections is a standard practice that attempts to tackle shape, texture and color discrepancies all at once through pretrained model fine-tuning. In this work, we propose to disentangle those challenges and design a dedicated network module that focuses on color adaptation. We combine learning from scratch of the color module with transfer learning of different classification backbones, obtaining an end-to-end, easy-to-train architecture for diagnostic image recognition on X-ray images. Extensive experiments showed how our approach is particularly efficient in case of data scarcity and provides a new path for further transferring the learned color information across multiple medical datasets.



### A Neural Network Looks at Leonardo's(?) Salvator Mundi
- **Arxiv ID**: http://arxiv.org/abs/2005.10600v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2005.10600v1)
- **Published**: 2020-05-21 12:27:40+00:00
- **Updated**: 2020-05-21 12:27:40+00:00
- **Authors**: Steven J. Frank, Andrea M. Frank
- **Comment**: This is the author's final version. The article has been accepted for
  publication in Leonardo (MIT Press)
- **Journal**: None
- **Summary**: We use convolutional neural networks (CNNs) to analyze authorship questions surrounding the works of Leonardo da Vinci -- in particular, Salvator Mundi, the world's most expensive painting and among the most controversial. Trained on the works of an artist under study and visually comparable works of other artists, our system can identify likely forgeries and shed light on attribution controversies. Leonardo's few extant paintings test the limits of our system and require corroborative techniques of testing and analysis.



### Efficient and Phase-aware Video Super-resolution for Cardiac MRI
- **Arxiv ID**: http://arxiv.org/abs/2005.10626v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.10626v4)
- **Published**: 2020-05-21 13:29:03+00:00
- **Updated**: 2020-07-08 14:35:54+00:00
- **Authors**: Jhih-Yuan Lin, Yu-Cheng Chang, Winston H. Hsu
- **Comment**: MICCAI 2020
- **Journal**: None
- **Summary**: Cardiac Magnetic Resonance Imaging (CMR) is widely used since it can illustrate the structure and function of heart in a non-invasive and painless way. However, it is time-consuming and high-cost to acquire the high-quality scans due to the hardware limitation. To this end, we propose a novel end-to-end trainable network to solve CMR video super-resolution problem without the hardware upgrade and the scanning protocol modifications. We incorporate the cardiac knowledge into our model to assist in utilizing the temporal information. Specifically, we formulate the cardiac knowledge as the periodic function, which is tailored to meet the cyclic characteristic of CMR. In addition, the proposed residual of residual learning scheme facilitates the network to learn the LR-HR mapping in a progressive refinement fashion. This mechanism enables the network to have the adaptive capability by adjusting refinement iterations depending on the difficulty of the task. Extensive experimental results on large-scale datasets demonstrate the superiority of the proposed method compared with numerous state-of-the-art methods.



### SymJAX: symbolic CPU/GPU/TPU programming
- **Arxiv ID**: http://arxiv.org/abs/2005.10635v1
- **DOI**: None
- **Categories**: **cs.MS**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.10635v1)
- **Published**: 2020-05-21 13:37:25+00:00
- **Updated**: 2020-05-21 13:37:25+00:00
- **Authors**: Randall Balestriero
- **Comment**: None
- **Journal**: None
- **Summary**: SymJAX is a symbolic programming version of JAX simplifying graph input/output/updates and providing additional functionalities for general machine learning and deep learning applications. From an user perspective SymJAX provides a la Theano experience with fast graph optimization/compilation and broad hardware support, along with Lasagne-like deep learning functionalities.



### Wish You Were Here: Context-Aware Human Generation
- **Arxiv ID**: http://arxiv.org/abs/2005.10663v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.10663v1)
- **Published**: 2020-05-21 14:09:14+00:00
- **Updated**: 2020-05-21 14:09:14+00:00
- **Authors**: Oran Gafni, Lior Wolf
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel method for inserting objects, specifically humans, into existing images, such that they blend in a photorealistic manner, while respecting the semantic context of the scene. Our method involves three subnetworks: the first generates the semantic map of the new person, given the pose of the other persons in the scene and an optional bounding box specification. The second network renders the pixels of the novel person and its blending mask, based on specifications in the form of multiple appearance components. A third network refines the generated face in order to match those of the target person. Our experiments present convincing high-resolution outputs in this novel and challenging application domain. In addition, the three networks are evaluated individually, demonstrating for example, state of the art results in pose transfer benchmarks.



### A Nearest Neighbor Network to Extract Digital Terrain Models from 3D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2005.10745v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.10745v2)
- **Published**: 2020-05-21 15:54:55+00:00
- **Updated**: 2020-06-20 19:51:13+00:00
- **Authors**: Mohammed Yousefhussien, David J. Kelbe, Carl Salvaggio
- **Comment**: Preprint submitted to Science of Remote Sensing
- **Journal**: None
- **Summary**: When 3D-point clouds from overhead sensors are used as input to remote sensing data exploitation pipelines, a large amount of effort is devoted to data preparation. Among the multiple stages of the preprocessing chain, estimating the Digital Terrain Model (DTM) model is considered to be of a high importance; however, this remains a challenge, especially for raw point clouds derived from optical imagery. Current algorithms estimate the ground points using either a set of geometrical rules that require tuning multiple parameters and human interaction, or cast the problem as a binary classification machine learning task where ground and non-ground classes are found. In contrast, here we present an algorithm that directly operates on 3D-point clouds and estimate the underlying DTM for the scene using an end-to-end approach without the need to classify points into ground and non-ground cover types. Our model learns neighborhood information and seamlessly integrates this with point-wise and block-wise global features. We validate our model using the ISPRS 3D Semantic Labeling Contest LiDAR data, as well as three scenes generated using dense stereo matching, representative of high-rise buildings, lower urban structures, and a dense old-city residential area. We compare our findings with two widely used software packages for DTM extraction, namely ENVI and LAStools. Our preliminary results show that the proposed method is able to achieve an overall Mean Absolute Error of 11.5% compared to 29% and 16% for ENVI and LAStools.



### Revisiting Role of Autoencoders in Adversarial Settings
- **Arxiv ID**: http://arxiv.org/abs/2005.10750v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.10750v1)
- **Published**: 2020-05-21 16:01:23+00:00
- **Updated**: 2020-05-21 16:01:23+00:00
- **Authors**: Byeong Cheon Kim, Jung Uk Kim, Hakmin Lee, Yong Man Ro
- **Comment**: Accepted at ICIP 2020
- **Journal**: None
- **Summary**: To combat against adversarial attacks, autoencoder structure is widely used to perform denoising which is regarded as gradient masking. In this paper, we revisit the role of autoencoders in adversarial settings. Through the comprehensive experimental results and analysis, this paper presents the inherent property of adversarial robustness in the autoencoders. We also found that autoencoders may use robust features that cause inherent adversarial robustness. We believe that our discovery of the adversarial robustness of the autoencoders can provide clues to the future research and applications for adversarial defense.



### Efficient Ensemble Model Generation for Uncertainty Estimation with Bayesian Approximation in Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2005.10754v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.10754v2)
- **Published**: 2020-05-21 16:08:38+00:00
- **Updated**: 2020-05-22 09:21:27+00:00
- **Authors**: Hong Joo Lee, Seong Tae Kim, Hakmin Lee, Nassir Navab, Yong Man Ro
- **Comment**: None
- **Journal**: None
- **Summary**: Recent studies have shown that ensemble approaches could not only improve accuracy and but also estimate model uncertainty in deep learning. However, it requires a large number of parameters according to the increase of ensemble models for better prediction and uncertainty estimation. To address this issue, a generic and efficient segmentation framework to construct ensemble segmentation models is devised in this paper. In the proposed method, ensemble models can be efficiently generated by using the stochastic layer selection method. The ensemble models are trained to estimate uncertainty through Bayesian approximation. Moreover, to overcome its limitation from uncertain instances, we devise a new pixel-wise uncertainty loss, which improves the predictive performance. To evaluate our method, comprehensive and comparative experiments have been conducted on two datasets. Experimental results show that the proposed method could provide useful uncertainty information by Bayesian approximation with the efficient ensemble model generation and improve the predictive performance.



### Robust Ensemble Model Training via Random Layer Sampling Against Adversarial Attack
- **Arxiv ID**: http://arxiv.org/abs/2005.10757v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.10757v2)
- **Published**: 2020-05-21 16:14:18+00:00
- **Updated**: 2021-01-27 13:20:57+00:00
- **Authors**: Hakmin Lee, Hong Joo Lee, Seong Tae Kim, Yong Man Ro
- **Comment**: Accepted at BMVC 2020
- **Journal**: None
- **Summary**: Deep neural networks have achieved substantial achievements in several computer vision areas, but have vulnerabilities that are often fooled by adversarial examples that are not recognized by humans. This is an important issue for security or medical applications. In this paper, we propose an ensemble model training framework with random layer sampling to improve the robustness of deep neural networks. In the proposed training framework, we generate various sampled model through the random layer sampling and update the weight of the sampled model. After the ensemble models are trained, it can hide the gradient efficiently and avoid the gradient-based attack by the random layer sampling method. To evaluate our proposed method, comprehensive and comparative experiments have been conducted on three datasets. Experimental results show that the proposed method improves the adversarial robustness.



### Dense Semantic 3D Map Based Long-Term Visual Localization with Hybrid Features
- **Arxiv ID**: http://arxiv.org/abs/2005.10766v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.10766v1)
- **Published**: 2020-05-21 16:36:37+00:00
- **Updated**: 2020-05-21 16:36:37+00:00
- **Authors**: Tianxin Shi, Hainan Cui, Zhuo Song, Shuhan Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Visual localization plays an important role in many applications. However, due to the large appearance variations such as season and illumination changes, as well as weather and day-night variations, it's still a big challenge for robust long-term visual localization algorithms. In this paper, we present a novel visual localization method using hybrid handcrafted and learned features with dense semantic 3D map. Hybrid features help us to make full use of their strengths in different imaging conditions, and the dense semantic map provide us reliable and complete geometric and semantic information for constructing sufficient 2D-3D matching pairs with semantic consistency scores. In our pipeline, we retrieve and score each candidate database image through the semantic consistency between the dense model and the query image. Then the semantic consistency score is used as a soft constraint in the weighted RANSAC-based PnP pose solver. Experimental results on long-term visual localization benchmarks demonstrate the effectiveness of our method compared with state-of-the-arts.



### Manifold Alignment for Semantically Aligned Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/2005.10777v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2005.10777v2)
- **Published**: 2020-05-21 16:52:37+00:00
- **Updated**: 2021-09-02 05:41:18+00:00
- **Authors**: Jing Huo, Shiyin Jin, Wenbin Li, Jing Wu, Yu-Kun Lai, Yinghuan Shi, Yang Gao
- **Comment**: 9 pages
- **Journal**: ICCV 2021
- **Summary**: Most existing style transfer methods follow the assumption that styles can be represented with global statistics (e.g., Gram matrices or covariance matrices), and thus address the problem by forcing the output and style images to have similar global statistics. An alternative is the assumption of local style patterns, where algorithms are designed to swap similar local features of content and style images. However, the limitation of these existing methods is that they neglect the semantic structure of the content image which may lead to corrupted content structure in the output. In this paper, we make a new assumption that image features from the same semantic region form a manifold and an image with multiple semantic regions follows a multi-manifold distribution. Based on this assumption, the style transfer problem is formulated as aligning two multi-manifold distributions and a Manifold Alignment based Style Transfer (MAST) framework is proposed. The proposed framework allows semantically similar regions between the output and the style image share similar style patterns. Moreover, the proposed manifold alignment method is flexible to allow user editing or using semantic segmentation maps as guidance for style transfer. To allow the method to be applicable to photorealistic style transfer, we propose a new adaptive weight skip connection network structure to preserve the content details. Extensive experiments verify the effectiveness of the proposed framework for both artistic and photorealistic style transfer. Code is available at https://github.com/NJUHuoJing/MAST.



### Hierarchical Multi-Scale Attention for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2005.10821v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.10821v1)
- **Published**: 2020-05-21 17:55:59+00:00
- **Updated**: 2020-05-21 17:55:59+00:00
- **Authors**: Andrew Tao, Karan Sapra, Bryan Catanzaro
- **Comment**: 11 pages, 5 figures
- **Journal**: None
- **Summary**: Multi-scale inference is commonly used to improve the results of semantic segmentation. Multiple images scales are passed through a network and then the results are combined with averaging or max pooling. In this work, we present an attention-based approach to combining multi-scale predictions. We show that predictions at certain scales are better at resolving particular failures modes, and that the network learns to favor those scales for such cases in order to generate better predictions. Our attention mechanism is hierarchical, which enables it to be roughly 4x more memory efficient to train than other recent approaches. In addition to enabling faster training, this allows us to train with larger crop sizes which leads to greater model accuracy. We demonstrate the result of our method on two datasets: Cityscapes and Mapillary Vistas. For Cityscapes, which has a large number of weakly labelled images, we also leverage auto-labelling to improve generalization. Using our approach we achieve a new state-of-the-art results in both Mapillary (61.1 IOU val) and Cityscapes (85.1 IOU test).



### Instance-aware Image Colorization
- **Arxiv ID**: http://arxiv.org/abs/2005.10825v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.10825v1)
- **Published**: 2020-05-21 17:59:23+00:00
- **Updated**: 2020-05-21 17:59:23+00:00
- **Authors**: Jheng-Wei Su, Hung-Kuo Chu, Jia-Bin Huang
- **Comment**: CVPR 2020. Project: https://ericsujw.github.io/InstColorization/
  Code: https://github.com/ericsujw/InstColorization
- **Journal**: None
- **Summary**: Image colorization is inherently an ill-posed problem with multi-modal uncertainty. Previous methods leverage the deep neural network to map input grayscale images to plausible color outputs directly. Although these learning-based methods have shown impressive performance, they usually fail on the input images that contain multiple objects. The leading cause is that existing models perform learning and colorization on the entire image. In the absence of a clear figure-ground separation, these models cannot effectively locate and learn meaningful object-level semantics. In this paper, we propose a method for achieving instance-aware colorization. Our network architecture leverages an off-the-shelf object detector to obtain cropped object images and uses an instance colorization network to extract object-level features. We use a similar network to extract the full-image features and apply a fusion module to full object-level and image-level features to predict the final colors. Both colorization networks and fusion modules are learned from a large-scale dataset. Experimental results show that our work outperforms existing methods on different quality metrics and achieves state-of-the-art performance on image colorization.



### Conditionally Deep Hybrid Neural Networks Across Edge and Cloud
- **Arxiv ID**: http://arxiv.org/abs/2005.10851v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.10851v1)
- **Published**: 2020-05-21 18:18:43+00:00
- **Updated**: 2020-05-21 18:18:43+00:00
- **Authors**: Yinghan Long, Indranil Chakraborty, Kaushik Roy
- **Comment**: 6 pages, 5 figures, 4 tables
- **Journal**: None
- **Summary**: The pervasiveness of "Internet-of-Things" in our daily life has led to a recent surge in fog computing, encompassing a collaboration of cloud computing and edge intelligence. To that effect, deep learning has been a major driving force towards enabling such intelligent systems. However, growing model sizes in deep learning pose a significant challenge towards deployment in resource-constrained edge devices. Moreover, in a distributed intelligence environment, efficient workload distribution is necessary between edge and cloud systems. To address these challenges, we propose a conditionally deep hybrid neural network for enabling AI-based fog computing. The proposed network can be deployed in a distributed manner, consisting of quantized layers and early exits at the edge and full-precision layers on the cloud. During inference, if an early exit has high confidence in the classification results, it would allow samples to exit at the edge, and the deeper layers on the cloud are activated conditionally, which can lead to improved energy efficiency and inference latency. We perform an extensive design space exploration with the goal of minimizing energy consumption at the edge while achieving state-of-the-art classification accuracies on image classification tasks. We show that with binarized layers at the edge, the proposed conditional hybrid network can process 65% of inferences at the edge, leading to 5.5x computational energy reduction with minimal accuracy degradation on CIFAR-10 dataset. For the more complex dataset CIFAR-100, we observe that the proposed network with 4-bit quantization at the edge achieves 52% early classification at the edge with 4.8x energy reduction. The analysis gives us insights on designing efficient hybrid networks which achieve significantly higher energy efficiency than full-precision networks for edge-cloud based distributed intelligence systems.



### RV-FuseNet: Range View Based Fusion of Time-Series LiDAR Data for Joint 3D Object Detection and Motion Forecasting
- **Arxiv ID**: http://arxiv.org/abs/2005.10863v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2005.10863v3)
- **Published**: 2020-05-21 19:22:27+00:00
- **Updated**: 2021-03-23 02:43:33+00:00
- **Authors**: Ankit Laddha, Shivam Gautam, Gregory P. Meyer, Carlos Vallespi-Gonzalez, Carl K. Wellington
- **Comment**: Submitted to IROS 2021
- **Journal**: None
- **Summary**: Robust real-time detection and motion forecasting of traffic participants is necessary for autonomous vehicles to safely navigate urban environments. In this paper, we present RV-FuseNet, a novel end-to-end approach for joint detection and trajectory estimation directly from time-series LiDAR data. Instead of the widely used bird's eye view (BEV) representation, we utilize the native range view (RV) representation of LiDAR data. The RV preserves the full resolution of the sensor by avoiding the voxelization used in the BEV. Furthermore, RV can be processed efficiently due to its compactness. Previous approaches project time-series data to a common viewpoint for temporal fusion, and often this viewpoint is different from where it was captured. This is sufficient for BEV methods, but for RV methods, this can lead to loss of information and data distortion which has an adverse impact on performance. To address this challenge we propose a simple yet effective novel architecture, \textit{Incremental Fusion}, that minimizes the information loss by sequentially projecting each RV sweep into the viewpoint of the next sweep in time. We show that our approach significantly improves motion forecasting performance over the existing state-of-the-art. Furthermore, we demonstrate that our sequential fusion approach is superior to alternative RV based fusion methods on multiple datasets.



### Unsupervised Domain Adaptation in Semantic Segmentation: a Review
- **Arxiv ID**: http://arxiv.org/abs/2005.10876v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.10876v1)
- **Published**: 2020-05-21 20:10:38+00:00
- **Updated**: 2020-05-21 20:10:38+00:00
- **Authors**: Marco Toldo, Andrea Maracani, Umberto Michieli, Pietro Zanuttigh
- **Comment**: 34 pages, 7 figures, 2 tables
- **Journal**: None
- **Summary**: The aim of this paper is to give an overview of the recent advancements in the Unsupervised Domain Adaptation (UDA) of deep networks for semantic segmentation. This task is attracting a wide interest, since semantic segmentation models require a huge amount of labeled data and the lack of data fitting specific requirements is the main limitation in the deployment of these techniques. This problem has been recently explored and has rapidly grown with a large number of ad-hoc approaches. This motivates us to build a comprehensive overview of the proposed methodologies and to provide a clear categorization. In this paper, we start by introducing the problem, its formulation and the various scenarios that can be considered. Then, we introduce the different levels at which adaptation strategies may be applied: namely, at the input (image) level, at the internal features representation and at the output level. Furthermore, we present a detailed overview of the literature in the field, dividing previous methods based on the following (non mutually exclusive) categories: adversarial learning, generative-based, analysis of the classifier discrepancies, self-teaching, entropy minimization, curriculum learning and multi-task learning. Novel research directions are also briefly introduced to give a hint of interesting open problems in the field. Finally, a comparison of the performance of the various methods in the widely used autonomous driving scenario is presented.



### SpotFast Networks with Memory Augmented Lateral Transformers for Lipreading
- **Arxiv ID**: http://arxiv.org/abs/2005.10903v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.10903v1)
- **Published**: 2020-05-21 21:04:12+00:00
- **Updated**: 2020-05-21 21:04:12+00:00
- **Authors**: Peratham Wiriyathammabhum
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a novel deep learning architecture for word-level lipreading. Previous works suggest a potential for incorporating a pretrained deep 3D Convolutional Neural Networks as a front-end feature extractor. We introduce a SpotFast networks, a variant of the state-of-the-art SlowFast networks for action recognition, which utilizes a temporal window as a spot pathway and all frames as a fast pathway. We further incorporate memory augmented lateral transformers to learn sequential features for classification. We evaluate the proposed model on the LRW dataset. The experiments show that our proposed model outperforms various state-of-the-art models and incorporating the memory augmented lateral transformers makes a 3.7% improvement to the SpotFast networks.



### Joint Detection and Tracking in Videos with Identification Features
- **Arxiv ID**: http://arxiv.org/abs/2005.10905v2
- **DOI**: 10.1016/j.imavis.2020.103932
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.10905v2)
- **Published**: 2020-05-21 21:06:40+00:00
- **Updated**: 2020-05-25 11:42:49+00:00
- **Authors**: Bharti Munjal, Abdul Rafey Aftab, Sikandar Amin, Meltem D. Brandlmaier, Federico Tombari, Fabio Galasso
- **Comment**: Accepted at Image and Vision Computing Journal
- **Journal**: None
- **Summary**: Recent works have shown that combining object detection and tracking tasks, in the case of video data, results in higher performance for both tasks, but they require a high frame-rate as a strict requirement for performance. This is assumption is often violated in real-world applications, when models run on embedded devices, often at only a few frames per second.   Videos at low frame-rate suffer from large object displacements. Here re-identification features may support to match large-displaced object detections, but current joint detection and re-identification formulations degrade the detector performance, as these two are contrasting tasks. In the real-world application having separate detector and re-id models is often not feasible, as both the memory and runtime effectively double.   Towards robust long-term tracking applicable to reduced-computational-power devices, we propose the first joint optimization of detection, tracking and re-identification features for videos. Notably, our joint optimization maintains the detector performance, a typical multi-task challenge. At inference time, we leverage detections for tracking (tracking-by-detection) when the objects are visible, detectable and slowly moving in the image. We leverage instead re-identification features to match objects which disappeared (e.g. due to occlusion) for several frames or were not tracked due to fast motion (or low-frame-rate videos). Our proposed method reaches the state-of-the-art on MOT, it ranks 1st in the UA-DETRAC'18 tracking challenge among online trackers, and 3rd overall.



### Deep learning mediated single time-point image-based prediction of embryo developmental outcome at the cleavage stage
- **Arxiv ID**: http://arxiv.org/abs/2006.08346v1
- **DOI**: None
- **Categories**: **q-bio.TO**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.08346v1)
- **Published**: 2020-05-21 21:21:15+00:00
- **Updated**: 2020-05-21 21:21:15+00:00
- **Authors**: Manoj Kumar Kanakasabapathy, Prudhvi Thirumalaraju, Charles L Bormann, Raghav Gupta, Rohan Pooniwala, Hemanth Kandula, Irene Souter, Irene Dimitriadis, Hadi Shafiee
- **Comment**: None
- **Journal**: None
- **Summary**: In conventional clinical in-vitro fertilization practices embryos are transferred either at the cleavage or blastocyst stages of development. Cleavage stage transfers, particularly, are beneficial for patients with relatively poor prognosis and at fertility centers in resource-limited settings where there is a higher chance of developmental failure in embryos in-vitro. However, one of the major limitations of embryo selections at the cleavage stage is the availability of very low number of manually discernable features to predict developmental outcomes. Although, time-lapse imaging systems have been proposed as possible solutions, they are cost-prohibitive and require bulky and expensive hardware, and labor-intensive. Advances in convolutional neural networks (CNNs) have been utilized to provide accurate classifications across many medical and non-medical object categories. Here, we report an automated system for classification and selection of human embryos at the cleavage stage using a trained CNN combined with a genetic algorithm. The system selected the cleavage stage embryo at 70 hours post insemination (hpi) that ultimately developed into top-quality blastocyst at 70 hpi with 64% accuracy, outperforming the abilities of embryologists in identifying embryos with the highest developmental potential. Such systems can have a significant impact on IVF procedures by empowering embryologists for accurate and consistent embryo assessment in both resource-poor and resource-rich settings.



### Evaluation of deep convolutional neural networks in classifying human embryo images based on their morphological quality
- **Arxiv ID**: http://arxiv.org/abs/2005.10912v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.10912v1)
- **Published**: 2020-05-21 21:21:22+00:00
- **Updated**: 2020-05-21 21:21:22+00:00
- **Authors**: Prudhvi Thirumalaraju, Manoj Kumar Kanakasabapathy, Charles L Bormann, Raghav Gupta, Rohan Pooniwala, Hemanth Kandula, Irene Souter, Irene Dimitriadis, Hadi Shafiee
- **Comment**: None
- **Journal**: None
- **Summary**: A critical factor that influences the success of an in-vitro fertilization (IVF) procedure is the quality of the transferred embryo. Embryo morphology assessments, conventionally performed through manual microscopic analysis suffer from disparities in practice, selection criteria, and subjectivity due to the experience of the embryologist. Convolutional neural networks (CNNs) are powerful, promising algorithms with significant potential for accurate classifications across many object categories. Network architectures and hyper-parameters affect the efficiency of CNNs for any given task. Here, we evaluate multi-layered CNNs developed from scratch and popular deep-learning architectures such as Inception v3, ResNET, Inception-ResNET-v2, and Xception in differentiating between embryos based on their morphological quality at 113 hours post insemination (hpi). Xception performed the best in differentiating between the embryos based on their morphological quality.



### Team Neuro at SemEval-2020 Task 8: Multi-Modal Fine Grain Emotion Classification of Memes using Multitask Learning
- **Arxiv ID**: http://arxiv.org/abs/2005.10915v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.10915v1)
- **Published**: 2020-05-21 21:29:44+00:00
- **Updated**: 2020-05-21 21:29:44+00:00
- **Authors**: Sourya Dipta Das, Soumil Mandal
- **Comment**: Proceedings of the International Workshop on Semantic Evaluation
  (SemEval)
- **Journal**: None
- **Summary**: In this article, we describe the system that we used for the memotion analysis challenge, which is Task 8 of SemEval-2020. This challenge had three subtasks where affect based sentiment classification of the memes was required along with intensities. The system we proposed combines the three tasks into a single one by representing it as multi-label hierarchical classification problem.Here,Multi-Task learning or Joint learning Procedure is used to train our model.We have used dual channels to extract text and image based features from separate Deep Neural Network Backbone and aggregate them to create task specific features. These task specific aggregated feature vectors ware then passed on to smaller networks with dense layers, each one assigned for predicting one type of fine grain sentiment label. Our Proposed method show the superiority of this system in few tasks to other best models from the challenge.



### Large scale evaluation of importance maps in automatic speech recognition
- **Arxiv ID**: http://arxiv.org/abs/2005.10929v1
- **DOI**: 10.21437/Interspeech.2020-2883
- **Categories**: **cs.SD**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.10929v1)
- **Published**: 2020-05-21 22:39:51+00:00
- **Updated**: 2020-05-21 22:39:51+00:00
- **Authors**: Viet Anh Trinh, Michael I Mandel
- **Comment**: submitted to INTERSPEECH 2020
- **Journal**: Proceedings of Interspeech 2020
- **Summary**: In this paper, we propose a metric that we call the structured saliency benchmark (SSBM) to evaluate importance maps computed for automatic speech recognizers on individual utterances. These maps indicate time-frequency points of the utterance that are most important for correct recognition of a target word. Our evaluation technique is not only suitable for standard classification tasks, but is also appropriate for structured prediction tasks like sequence-to-sequence models. Additionally, we use this approach to perform a large scale comparison of the importance maps created by our previously introduced technique using "bubble noise" to identify important points through correlation with a baseline approach based on smoothed speech energy and forced alignment. Our results show that the bubble analysis approach is better at identifying important speech regions than this baseline on 100 sentences from the AMI corpus.



### When Dictionary Learning Meets Deep Learning: Deep Dictionary Learning and Coding Network for Image Recognition with Limited Data
- **Arxiv ID**: http://arxiv.org/abs/2005.10940v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.10940v1)
- **Published**: 2020-05-21 23:12:10+00:00
- **Updated**: 2020-05-21 23:12:10+00:00
- **Authors**: Hao Tang, Hong Liu, Wei Xiao, Nicu Sebe
- **Comment**: Accepted to TNNLS, an extended version of a paper published in
  WACV2019. arXiv admin note: substantial text overlap with arXiv:1809.04185
- **Journal**: None
- **Summary**: We present a new Deep Dictionary Learning and Coding Network (DDLCN) for image recognition tasks with limited data. The proposed DDLCN has most of the standard deep learning layers (e.g., input/output, pooling, fully connected, etc.), but the fundamental convolutional layers are replaced by our proposed compound dictionary learning and coding layers. The dictionary learning learns an over-complete dictionary for input training data. At the deep coding layer, a locality constraint is added to guarantee that the activated dictionary bases are close to each other. Then the activated dictionary atoms are assembled and passed to the compound dictionary learning and coding layers. In this way, the activated atoms in the first layer can be represented by the deeper atoms in the second dictionary. Intuitively, the second dictionary is designed to learn the fine-grained components shared among the input dictionary atoms, thus a more informative and discriminative low-level representation of the dictionary atoms can be obtained. We empirically compare DDLCN with several leading dictionary learning methods and deep learning models. Experimental results on five popular datasets show that DDLCN achieves competitive results compared with state-of-the-art methods when the training data is limited. Code is available at https://github.com/Ha0Tang/DDLCN.



