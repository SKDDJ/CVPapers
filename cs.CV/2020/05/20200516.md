# Arxiv Papers in cs.CV on 2020-05-16
### Saving the Sonorine: Photovisual Audio Recovery Using Image Processing and Computer Vision Techniques
- **Arxiv ID**: http://arxiv.org/abs/2005.08944v3
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, cs.GR, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2005.08944v3)
- **Published**: 2020-05-16 00:45:26+00:00
- **Updated**: 2020-05-22 20:08:01+00:00
- **Authors**: Kevin Feng
- **Comment**: This version has been removed by arXiv administrators because the
  submitter did not have the right to agree to the license applied at the time
  of submission
- **Journal**: None
- **Summary**: This paper presents a novel technique to recover audio from sonorines, an early 20th century form of analogue sound storage. Our method uses high resolution photographs of sonorines under different lighting conditions to observe the change in reflection behavior of the physical surface features and create a three-dimensional height map of the surface. Sound can then be extracted using height information within the surface's grooves, mimicking a physical stylus on a phonograph. Unlike traditional playback methods, our method has the advantage of being contactless: the medium will not incur damage and wear from being played repeatedly. We compare the results of our technique to a previously successful contactless method using flatbed scans of the sonorines, and conclude with future research that can be applied to this photovisual approach to audio recovery.



### Joint Progressive Knowledge Distillation and Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2005.07839v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.07839v1)
- **Published**: 2020-05-16 01:07:03+00:00
- **Updated**: 2020-05-16 01:07:03+00:00
- **Authors**: Le Thanh Nguyen-Meidine, Eric Granger, Madhu Kiran, Jose Dolz, Louis-Antoine Blais-Morin
- **Comment**: Accepted to WCCI/IJCNN 2020
- **Journal**: None
- **Summary**: Currently, the divergence in distributions of design and operational data, and large computational complexity are limiting factors in the adoption of CNNs in real-world applications. For instance, person re-identification systems typically rely on a distributed set of cameras, where each camera has different capture conditions. This can translate to a considerable shift between source (e.g. lab setting) and target (e.g. operational camera) domains. Given the cost of annotating image data captured for fine-tuning in each target domain, unsupervised domain adaptation (UDA) has become a popular approach to adapt CNNs. Moreover, state-of-the-art deep learning models that provide a high level of accuracy often rely on architectures that are too complex for real-time applications. Although several compression and UDA approaches have recently been proposed to overcome these limitations, they do not allow optimizing a CNN to simultaneously address both. In this paper, we propose an unexplored direction -- the joint optimization of CNNs to provide a compressed model that is adapted to perform well for a given target domain. In particular, the proposed approach performs unsupervised knowledge distillation (KD) from a complex teacher model to a compact student model, by leveraging both source and target data. It also improves upon existing UDA techniques by progressively teaching the student about domain-invariant features, instead of directly adapting a compact model on target domain data. Our method is compared against state-of-the-art compression and UDA techniques, using two popular classification datasets for UDA -- Office31 and ImageClef-DA. In both datasets, results indicate that our method can achieve the highest level of accuracy while requiring a comparable or lower time complexity.



### Partial Domain Adaptation Using Graph Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/2005.07858v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.07858v1)
- **Published**: 2020-05-16 03:37:38+00:00
- **Updated**: 2020-05-16 03:37:38+00:00
- **Authors**: Seunghan Yang, Youngeun Kim, Dongki Jung, Changick Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Partial domain adaptation (PDA), in which we assume the target label space is included in the source label space, is a general version of standard domain adaptation. Since the target label space is unknown, the main challenge of PDA is to reduce the learning impact of irrelevant source samples, named outliers, which do not belong to the target label space. Although existing partial domain adaptation methods effectively down-weigh outliers' importance, they do not consider data structure of each domain and do not directly align the feature distributions of the same class in the source and target domains, which may lead to misalignment of category-level distributions. To overcome these problems, we propose a graph partial domain adaptation (GPDA) network, which exploits Graph Convolutional Networks for jointly considering data structure and the feature distribution of each class. Specifically, we propose a label relational graph to align the distributions of the same category in two domains and introduce moving average centroid separation for learning networks from the label relational graph. We demonstrate that considering data structure and the distribution of each category is effective for PDA and our GPDA network achieves state-of-the-art performance on the Digit and Office-31 datasets.



### COCAS: A Large-Scale Clothes Changing Person Dataset for Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2005.07862v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.07862v1)
- **Published**: 2020-05-16 03:50:08+00:00
- **Updated**: 2020-05-16 03:50:08+00:00
- **Authors**: Shijie Yu, Shihua Li, Dapeng Chen, Rui Zhao, Junjie Yan, Yu Qiao
- **Comment**: Accepted by CVPR2020
- **Journal**: None
- **Summary**: Recent years have witnessed great progress in person re-identification (re-id). Several academic benchmarks such as Market1501, CUHK03 and DukeMTMC play important roles to promote the re-id research. To our best knowledge, all the existing benchmarks assume the same person will have the same clothes. While in real-world scenarios, it is very often for a person to change clothes. To address the clothes changing person re-id problem, we construct a novel large-scale re-id benchmark named ClOthes ChAnging Person Set (COCAS), which provides multiple images of the same identity with different clothes. COCAS totally contains 62,382 body images from 5,266 persons. Based on COCAS, we introduce a new person re-id setting for clothes changing problem, where the query includes both a clothes template and a person image taking another clothes. Moreover, we propose a two-branch network named Biometric-Clothes Network (BC-Net) which can effectively integrate biometric and clothes feature for re-id under our setting. Experiments show that it is feasible for clothes changing re-id with clothes templates.



### Attribute2Font: Creating Fonts You Want From Attributes
- **Arxiv ID**: http://arxiv.org/abs/2005.07865v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2005.07865v1)
- **Published**: 2020-05-16 04:06:53+00:00
- **Updated**: 2020-05-16 04:06:53+00:00
- **Authors**: Yizhi Wang, Yue Gao, Zhouhui Lian
- **Comment**: SIGGRAPH 2020 techniqual paper; Wang and Gao contribute equally;
  Code: https://hologerry.github.io/Attr2Font/
- **Journal**: None
- **Summary**: Font design is now still considered as an exclusive privilege of professional designers, whose creativity is not possessed by existing software systems. Nevertheless, we also notice that most commercial font products are in fact manually designed by following specific requirements on some attributes of glyphs, such as italic, serif, cursive, width, angularity, etc. Inspired by this fact, we propose a novel model, Attribute2Font, to automatically create fonts by synthesizing visually-pleasing glyph images according to user-specified attributes and their corresponding values. To the best of our knowledge, our model is the first one in the literature which is capable of generating glyph images in new font styles, instead of retrieving existing fonts, according to given values of specified font attributes. Specifically, Attribute2Font is trained to perform font style transfer between any two fonts conditioned on their attribute values. After training, our model can generate glyph images in accordance with an arbitrary set of font attribute values. Furthermore, a novel unit named Attribute Attention Module is designed to make those generated glyph images better embody the prominent font attributes. Considering that the annotations of font attribute values are extremely expensive to obtain, a semi-supervised learning scheme is also introduced to exploit a large number of unlabeled fonts. Experimental results demonstrate that our model achieves impressive performance on many tasks, such as creating glyph images in new font styles, editing existing fonts, interpolation among different fonts, etc.



### Multi-scale Grouped Dense Network for VVC Intra Coding
- **Arxiv ID**: http://arxiv.org/abs/2005.07896v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.07896v1)
- **Published**: 2020-05-16 08:08:44+00:00
- **Updated**: 2020-05-16 08:08:44+00:00
- **Authors**: Xin Li, Simeng Sun, Zhizheng Zhang, Zhibo Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Versatile Video Coding (H.266/VVC) standard achieves better image quality when keeping the same bits than any other conventional image codec, such as BPG, JPEG, and etc. However, it is still attractive and challenging to improve the image quality with high compression ratio on the basis of traditional coding techniques. In this paper, we design the multi-scale grouped dense network (MSGDN) to further reduce the compression artifacts by combining the multi-scale and grouped dense block, which are integrated as the post-process network of VVC intra coding. Besides, to improve the subjective quality of compressed image, we also present a generative adversarial network (MSGDN-GAN) by utilizing our MSGDN as generator. Across the extensive experiments on validation set, our MSGDN trained by MSE losses yields the PSNR of 32.622 on average with teams IMC at the bit-rate of 0.15 in Lowrate track. Moreover, our MSGDN-GAN could achieve the better subjective performance.



### The Power of Triply Complementary Priors for Image Compressive Sensing
- **Arxiv ID**: http://arxiv.org/abs/2005.07902v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.07902v1)
- **Published**: 2020-05-16 08:17:44+00:00
- **Updated**: 2020-05-16 08:17:44+00:00
- **Authors**: Zhiyuan Zha, Xin Yuan, Joey Tianyi Zhou, Jiantao Zhou, Bihan Wen, Ce Zhu
- **Comment**: None
- **Journal**: 2020 International Conference on Image Processing
- **Summary**: Recent works that utilized deep models have achieved superior results in various image restoration applications. Such approach is typically supervised which requires a corpus of training images with distribution similar to the images to be recovered. On the other hand, the shallow methods which are usually unsupervised remain promising performance in many inverse problems, \eg, image compressive sensing (CS), as they can effectively leverage non-local self-similarity priors of natural images. However, most of such methods are patch-based leading to the restored images with various ringing artifacts due to naive patch aggregation. Using either approach alone usually limits performance and generalizability in image restoration tasks. In this paper, we propose a joint low-rank and deep (LRD) image model, which contains a pair of triply complementary priors, namely \textit{external} and \textit{internal}, \textit{deep} and \textit{shallow}, and \textit{local} and \textit{non-local} priors. We then propose a novel hybrid plug-and-play (H-PnP) framework based on the LRD model for image CS. To make the optimization tractable, a simple yet effective algorithm is proposed to solve the proposed H-PnP based image CS problem. Extensive experimental results demonstrate that the proposed H-PnP algorithm significantly outperforms the state-of-the-art techniques for image CS recovery such as SCSNet and WNNM.



### Deep feature fusion for self-supervised monocular depth prediction
- **Arxiv ID**: http://arxiv.org/abs/2005.07922v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.07922v1)
- **Published**: 2020-05-16 09:42:36+00:00
- **Updated**: 2020-05-16 09:42:36+00:00
- **Authors**: Vinay Kaushik, Brejesh Lall
- **Comment**: 4 pages, 2 Tables, 2 Figures
- **Journal**: None
- **Summary**: Recent advances in end-to-end unsupervised learning has significantly improved the performance of monocular depth prediction and alleviated the requirement of ground truth depth. Although a plethora of work has been done in enforcing various structural constraints by incorporating multiple losses utilising smoothness, left-right consistency, regularisation and matching surface normals, a few of them take into consideration multi-scale structures present in real world images. Most works utilise a VGG16 or ResNet50 model pre-trained on ImageNet weights for predicting depth. We propose a deep feature fusion method utilising features at multiple scales for learning self-supervised depth from scratch. Our fusion network selects features from both upper and lower levels at every level in the encoder network, thereby creating multiple feature pyramid sub-networks that are fed to the decoder after applying the CoordConv solution. We also propose a refinement module learning higher scale residual depth from a combination of higher level deep features and lower level residual depth using a pixel shuffling framework that super-resolves lower level residual depth. We select the KITTI dataset for evaluation and show that our proposed architecture can produce better or comparable results in depth prediction.



### HVS-Based Perceptual Color Compression of Image Data
- **Arxiv ID**: http://arxiv.org/abs/2005.07930v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.07930v4)
- **Published**: 2020-05-16 10:05:20+00:00
- **Updated**: 2021-02-09 17:02:45+00:00
- **Authors**: Lee Prangnell, Victor Sanchez
- **Comment**: Preprint: 2021 IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP 2021)
- **Journal**: None
- **Summary**: In perceptual image coding applications, the main objective is to decrease, as much as possible, Bits Per Pixel (BPP) while avoiding noticeable distortions in the reconstructed image. In this paper, we propose a novel perceptual image coding technique, named Perceptual Color Compression (PCC). PCC is based on a novel model related to Human Visual System (HVS) spectral sensitivity and CIELAB Just Noticeable Color Difference (JNCD). We utilize this modeling to capitalize on the inability of the HVS to perceptually differentiate photons in very similar wavelength bands (e.g., distinguishing very similar shades of a particular color or different colors that look similar). The proposed PCC technique can be used with RGB (4:4:4) image data of various bit depths and spatial resolutions. In the evaluations, we compare the proposed PCC technique with a set of reference methods including Versatile Video Coding (VVC) and High Efficiency Video Coding (HEVC) in addition to two other recently proposed algorithms. Our PCC method attains considerable BPP reductions compared with all four reference techniques including, on average, 52.6% BPP reductions compared with VVC (VVC in All Intra still image coding mode). Regarding image perceptual reconstruction quality, PCC achieves a score of SSIM = 0.99 in all tests in addition to a score of MS-SSIM = 0.99 in all but one test. Moreover, MOS = 5 is attained in 75% of subjective evaluation assessments conducted.



### Multi-level Feature Fusion-based CNN for Local Climate Zone Classification from Sentinel-2 Images: Benchmark Results on the So2Sat LCZ42 Dataset
- **Arxiv ID**: http://arxiv.org/abs/2005.07983v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.07983v1)
- **Published**: 2020-05-16 13:16:47+00:00
- **Updated**: 2020-05-16 13:16:47+00:00
- **Authors**: Chunping Qiu, Xiaochong Tong, Michael Schmitt, Benjamin Bechtel, Xiao Xiang Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: As a unique classification scheme for urban forms and functions, the local climate zone (LCZ) system provides essential general information for any studies related to urban environments, especially on a large scale. Remote sensing data-based classification approaches are the key to large-scale mapping and monitoring of LCZs. The potential of deep learning-based approaches is not yet fully explored, even though advanced convolutional neural networks (CNNs) continue to push the frontiers for various computer vision tasks. One reason is that published studies are based on different datasets, usually at a regional scale, which makes it impossible to fairly and consistently compare the potential of different CNNs for real-world scenarios. This study is based on the big So2Sat LCZ42 benchmark dataset dedicated to LCZ classification. Using this dataset, we studied a range of CNNs of varying sizes. In addition, we proposed a CNN to classify LCZs from Sentinel-2 images, Sen2LCZ-Net. Using this base network, we propose fusing multi-level features using the extended Sen2LCZ-Net-MF. With this proposed simple network architecture and the highly competitive benchmark dataset, we obtain results that are better than those obtained by the state-of-the-art CNNs, while requiring less computation with fewer layers and parameters. Large-scale LCZ classification examples of completely unseen areas are presented, demonstrating the potential of our proposed Sen2LCZ-Net-MF as well as the So2Sat LCZ42 dataset. We also intensively investigated the influence of network depth and width and the effectiveness of the design choices made for Sen2LCZ-Net-MF. Our work will provide important baselines for future CNN-based algorithm developments for both LCZ classification and other urban land cover land use classification.



### Non-Linearities Improve OrigiNet based on Active Imaging for Micro Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/2005.07991v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.07991v1)
- **Published**: 2020-05-16 13:44:49+00:00
- **Updated**: 2020-05-16 13:44:49+00:00
- **Authors**: Monu Verma, Santosh Kumar Vipparthi, Girdhari Singh
- **Comment**: None
- **Journal**: None
- **Summary**: Micro expression recognition (MER)is a very challenging task as the expression lives very short in nature and demands feature modeling with the involvement of both spatial and temporal dynamics. Existing MER systems exploit CNN networks to spot the significant features of minor muscle movements and subtle changes. However, existing networks fail to establish a relationship between spatial features of facial appearance and temporal variations of facial dynamics. Thus, these networks were not able to effectively capture minute variations and subtle changes in expressive regions. To address these issues, we introduce an active imaging concept to segregate active changes in expressive regions of a video into a single frame while preserving facial appearance information. Moreover, we propose a shallow CNN network: hybrid local receptive field based augmented learning network (OrigiNet) that efficiently learns significant features of the micro-expressions in a video. In this paper, we propose a new refined rectified linear unit (RReLU), which overcome the problem of vanishing gradient and dying ReLU. RReLU extends the range of derivatives as compared to existing activation functions. The RReLU not only injects a nonlinearity but also captures the true edges by imposing additive and multiplicative property. Furthermore, we present an augmented feature learning block to improve the learning capabilities of the network by embedding two parallel fully connected layers. The performance of proposed OrigiNet is evaluated by conducting leave one subject out experiments on four comprehensive ME datasets. The experimental results demonstrate that OrigiNet outperformed state-of-the-art techniques with less computational complexity.



### Revisiting Agglomerative Clustering
- **Arxiv ID**: http://arxiv.org/abs/2005.07995v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.07995v2)
- **Published**: 2020-05-16 14:07:25+00:00
- **Updated**: 2020-06-26 23:00:41+00:00
- **Authors**: Eric K. Tokuda, Cesar H. Comin, Luciano da F. Costa
- **Comment**: None
- **Journal**: None
- **Summary**: An important issue in clustering concerns the avoidance of false positives while searching for clusters. This work addressed this problem considering agglomerative methods, namely single, average, median, complete, centroid and Ward's approaches applied to unimodal and bimodal datasets obeying uniform, gaussian, exponential and power-law distributions. A model of clusters was also adopted, involving a higher density nucleus surrounded by a transition, followed by outliers. This paved the way to defining an objective means for identifying the clusters from dendrograms. The adopted model also allowed the relevance of the clusters to be quantified in terms of the height of their subtrees. The obtained results include the verification that many methods detect two clusters in unimodal data. The single-linkage method was found to be more resilient to false positives. Also, several methods detected clusters not corresponding directly to the nucleus. The possibility of identifying the type of distribution was also investigated.



### Deep Lighting Environment Map Estimation from Spherical Panoramas
- **Arxiv ID**: http://arxiv.org/abs/2005.08000v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2005.08000v1)
- **Published**: 2020-05-16 14:23:05+00:00
- **Updated**: 2020-05-16 14:23:05+00:00
- **Authors**: Vasileios Gkitsas, Nikolaos Zioulis, Federico Alvarez, Dimitrios Zarpalas, Petros Daras
- **Comment**: Code and models available at
  https://vcl3d.github.io/DeepPanoramaLighting
- **Journal**: None
- **Summary**: Estimating a scene's lighting is a very important task when compositing synthetic content within real environments, with applications in mixed reality and post-production. In this work we present a data-driven model that estimates an HDR lighting environment map from a single LDR monocular spherical panorama. In addition to being a challenging and ill-posed problem, the lighting estimation task also suffers from a lack of facile illumination ground truth data, a fact that hinders the applicability of data-driven methods. We approach this problem differently, exploiting the availability of surface geometry to employ image-based relighting as a data generator and supervision mechanism. This relies on a global Lambertian assumption that helps us overcome issues related to pre-baked lighting. We relight our training data and complement the model's supervision with a photometric loss, enabled by a differentiable image-based relighting technique. Finally, since we predict spherical spectral coefficients, we show that by imposing a distribution prior on the predicted coefficients, we can greatly boost performance. Code and models available at https://vcl3d.github.io/DeepPanoramaLighting.



### Extreme Low-Light Imaging with Multi-granulation Cooperative Networks
- **Arxiv ID**: http://arxiv.org/abs/2005.08001v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.08001v1)
- **Published**: 2020-05-16 14:26:06+00:00
- **Updated**: 2020-05-16 14:26:06+00:00
- **Authors**: Keqi Wang, Peng Gao, Steven Hoi, Qian Guo, Yuhua Qian
- **Comment**: None
- **Journal**: None
- **Summary**: Low-light imaging is challenging since images may appear to be dark and noised due to low signal-to-noise ratio, complex image content, and the variety in shooting scenes in extreme low-light condition. Many methods have been proposed to enhance the imaging quality under extreme low-light conditions, but it remains difficult to obtain satisfactory results, especially when they attempt to retain high dynamic range (HDR). In this paper, we propose a novel method of multi-granulation cooperative networks (MCN) with bidirectional information flow to enhance extreme low-light images, and design an illumination map estimation function (IMEF) to preserve high dynamic range (HDR). To facilitate this research, we also contribute to create a new benchmark dataset of real-world Dark High Dynamic Range (DHDR) images to evaluate the performance of high dynamic preservation in low light environment. Experimental results show that the proposed method outperforms the state-of-the-art approaches in terms of both visual effects and quantitative analysis.



### Towards in-store multi-person tracking using head detection and track heatmaps
- **Arxiv ID**: http://arxiv.org/abs/2005.08009v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.08009v2)
- **Published**: 2020-05-16 15:07:19+00:00
- **Updated**: 2020-07-02 03:22:46+00:00
- **Authors**: Aibek Musaev, Jiangping Wang, Liang Zhu, Cheng Li, Yi Chen, Jialin Liu, Wanqi Zhang, Juan Mei, De Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Computer vision algorithms are being implemented across a breadth of industries to enable technological innovations. In this paper, we study the problem of computer vision based customer tracking in retail industry. To this end, we introduce a dataset collected from a camera in an office environment where participants mimic various behaviors of customers in a supermarket. In addition, we describe an illustrative example of the use of this dataset for tracking participants based on a head tracking model in an effort to minimize errors due to occlusion. Furthermore, we propose a model for recognizing customers and staff based on their movement patterns. The model is evaluated using a real-world dataset collected in a supermarket over a 24-hour period that achieves 98% accuracy during training and 93% accuracy during evaluation.



### Various Total Variation for Snapshot Video Compressive Imaging
- **Arxiv ID**: http://arxiv.org/abs/2005.08028v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.08028v1)
- **Published**: 2020-05-16 16:20:56+00:00
- **Updated**: 2020-05-16 16:20:56+00:00
- **Authors**: Xin Yuan
- **Comment**: 5 pages, 4 figures
- **Journal**: None
- **Summary**: Sampling high-dimensional images is challenging due to limited availability of sensors; scanning is usually necessary in these cases. To mitigate this challenge, snapshot compressive imaging (SCI) was proposed to capture the high-dimensional (usually 3D) images using a 2D sensor (detector). Via novel optical design, the {\em measurement} captured by the sensor is an encoded image of multiple frames of the 3D desired signal. Following this, reconstruction algorithms are employed to retrieve the high-dimensional data. Though various algorithms have been proposed, the total variation (TV) based method is still the most efficient one due to a good trade-off between computational time and performance. This paper aims to answer the question of which TV penalty (anisotropic TV, isotropic TV and vectorized TV) works best for video SCI reconstruction? Various TV denoising and projection algorithms are developed and tested for video SCI reconstruction on both simulation and real datasets.



### Visual Relationship Detection using Scene Graphs: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2005.08045v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.08045v1)
- **Published**: 2020-05-16 17:06:06+00:00
- **Updated**: 2020-05-16 17:06:06+00:00
- **Authors**: Aniket Agarwal, Ayush Mangal, Vipul
- **Comment**: None
- **Journal**: None
- **Summary**: Understanding a scene by decoding the visual relationships depicted in an image has been a long studied problem. While the recent advances in deep learning and the usage of deep neural networks have achieved near human accuracy on many tasks, there still exists a pretty big gap between human and machine level performance when it comes to various visual relationship detection tasks. Developing on earlier tasks like object recognition, segmentation and captioning which focused on a relatively coarser image understanding, newer tasks have been introduced recently to deal with a finer level of image understanding. A Scene Graph is one such technique to better represent a scene and the various relationships present in it. With its wide number of applications in various tasks like Visual Question Answering, Semantic Image Retrieval, Image Generation, among many others, it has proved to be a useful tool for deeper and better visual relationship understanding. In this paper, we present a detailed survey on the various techniques for scene graph generation, their efficacy to represent visual relationships and how it has been used to solve various downstream tasks. We also attempt to analyze the various future directions in which the field might advance in the future. Being one of the first papers to give a detailed survey on this topic, we also hope to give a succinct introduction to scene graphs, and guide practitioners while developing approaches for their applications.



### A Deep Learning based Wearable Healthcare IoT Device for AI-enabled Hearing Assistance Automation
- **Arxiv ID**: http://arxiv.org/abs/2005.08076v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2005.08076v1)
- **Published**: 2020-05-16 19:42:16+00:00
- **Updated**: 2020-05-16 19:42:16+00:00
- **Authors**: Fraser Young, L Zhang, Richard Jiang, Han Liu, Conor Wall
- **Comment**: None
- **Journal**: The 2020 International Conference on Machine Learning and
  Cybernetics
- **Summary**: With the recent booming of artificial intelligence (AI), particularly deep learning techniques, digital healthcare is one of the prevalent areas that could gain benefits from AI-enabled functionality. This research presents a novel AI-enabled Internet of Things (IoT) device operating from the ESP-8266 platform capable of assisting those who suffer from impairment of hearing or deafness to communicate with others in conversations. In the proposed solution, a server application is created that leverages Google's online speech recognition service to convert the received conversations into texts, then deployed to a micro-display attached to the glasses to display the conversation contents to deaf people, to enable and assist conversation as normal with the general population. Furthermore, in order to raise alert of traffic or dangerous scenarios, an 'urban-emergency' classifier is developed using a deep learning model, Inception-v4, with transfer learning to detect/recognize alerting/alarming sounds, such as a horn sound or a fire alarm, with texts generated to alert the prospective user. The training of Inception-v4 was carried out on a consumer desktop PC and then implemented into the AI based IoT application. The empirical results indicate that the developed prototype system achieves an accuracy rate of 92% for sound recognition and classification with real-time performance.



### Predicting Video features from EEG and Vice versa
- **Arxiv ID**: http://arxiv.org/abs/2005.11235v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2005.11235v1)
- **Published**: 2020-05-16 20:04:38+00:00
- **Updated**: 2020-05-16 20:04:38+00:00
- **Authors**: Gautam Krishna, Co Tran, Mason Carnahan, Ahmed Tewfik
- **Comment**: under review
- **Journal**: None
- **Summary**: In this paper we explore predicting facial or lip video features from electroencephalography (EEG) features and predicting EEG features from recorded facial or lip video frames using deep learning models. The subjects were asked to read out loud English sentences shown to them on a computer screen and their simultaneous EEG signals and facial video frames were recorded. Our model was able to generate very broad characteristics of the facial or lip video frame from input EEG features. Our results demonstrate the first step towards synthesizing high quality facial or lip video from recorded EEG features. We demonstrate results for a data set consisting of seven subjects.



### Universal Adversarial Perturbations: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2005.08087v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.08087v1)
- **Published**: 2020-05-16 20:18:26+00:00
- **Updated**: 2020-05-16 20:18:26+00:00
- **Authors**: Ashutosh Chaubey, Nikhil Agrawal, Kavya Barnwal, Keerat K. Guliani, Pramod Mehta
- **Comment**: 20 pages, 17 figures
- **Journal**: None
- **Summary**: Over the past decade, Deep Learning has emerged as a useful and efficient tool to solve a wide variety of complex learning problems ranging from image classification to human pose estimation, which is challenging to solve using statistical machine learning algorithms. However, despite their superior performance, deep neural networks are susceptible to adversarial perturbations, which can cause the network's prediction to change without making perceptible changes to the input image, thus creating severe security issues at the time of deployment of such systems. Recent works have shown the existence of Universal Adversarial Perturbations, which, when added to any image in a dataset, misclassifies it when passed through a target model. Such perturbations are more practical to deploy since there is minimal computation done during the actual attack. Several techniques have also been proposed to defend the neural networks against these perturbations. In this paper, we attempt to provide a detailed discussion on the various data-driven and data-independent methods for generating universal perturbations, along with measures to defend against such perturbations. We also cover the applications of such universal perturbations in various deep learning tasks.



### Improving Robustness using Joint Attention Network For Detecting Retinal Degeneration From Optical Coherence Tomography Images
- **Arxiv ID**: http://arxiv.org/abs/2005.08094v2
- **DOI**: 10.1109/ICIP40778.2020.9190742
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.08094v2)
- **Published**: 2020-05-16 20:32:49+00:00
- **Updated**: 2020-05-19 01:16:42+00:00
- **Authors**: Sharif Amit Kamran, Alireza Tavakkoli, Stewart Lee Zuckerbrod
- **Comment**: \c{opyright} 2020 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works
- **Journal**: None
- **Summary**: Noisy data and the similarity in the ocular appearances caused by different ophthalmic pathologies pose significant challenges for an automated expert system to accurately detect retinal diseases. In addition, the lack of knowledge transferability and the need for unreasonably large datasets limit clinical application of current machine learning systems. To increase robustness, a better understanding of how the retinal subspace deformations lead to various levels of disease severity needs to be utilized for prioritizing disease-specific model details. In this paper we propose the use of disease-specific feature representation as a novel architecture comprised of two joint networks -- one for supervised encoding of disease model and the other for producing attention maps in an unsupervised manner to retain disease specific spatial information. Our experimental results on publicly available datasets show the proposed joint-network significantly improves the accuracy and robustness of state-of-the-art retinal disease classification networks on unseen datasets.



### Single-Stage Semantic Segmentation from Image Labels
- **Arxiv ID**: http://arxiv.org/abs/2005.08104v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.08104v1)
- **Published**: 2020-05-16 21:10:10+00:00
- **Updated**: 2020-05-16 21:10:10+00:00
- **Authors**: Nikita Araslanov, Stefan Roth
- **Comment**: To appear at CVPR 2020; minor corrections in Eq. (9). Code:
  https://github.com/visinf/1-stage-wseg
- **Journal**: None
- **Summary**: Recent years have seen a rapid growth in new approaches improving the accuracy of semantic segmentation in a weakly supervised setting, i.e. with only image-level labels available for training. However, this has come at the cost of increased model complexity and sophisticated multi-stage training procedures. This is in contrast to earlier work that used only a single stage $-$ training one segmentation network on image labels $-$ which was abandoned due to inferior segmentation accuracy. In this work, we first define three desirable properties of a weakly supervised method: local consistency, semantic fidelity, and completeness. Using these properties as guidelines, we then develop a segmentation-based network model and a self-supervised training scheme to train for semantic masks from image-level annotations in a single stage. We show that despite its simplicity, our method achieves results that are competitive with significantly more complex pipelines, substantially outperforming earlier single-stage methods.



### Analytic Signal Phase in $N-D$ by Linear Symmetry Tensor--fingerprint modeling
- **Arxiv ID**: http://arxiv.org/abs/2005.08108v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.08108v1)
- **Published**: 2020-05-16 21:17:26+00:00
- **Updated**: 2020-05-16 21:17:26+00:00
- **Authors**: Josef Bigun, Fernando Alonso-Fernandez
- **Comment**: None
- **Journal**: None
- **Summary**: We reveal that the Analytic Signal phase, and its gradient have a hitherto unstudied discontinuity in $2-D $ and higher dimensions. The shortcoming can result in severe artifacts whereas the problem does not exist in $1-D $ signals. Direct use of Gabor phase, or its gradient, in computer vision and biometric recognition e.g., as done in influential studies \cite{fleet90,wiskott1997face}, may produce undesired results that will go unnoticed unless special images similar to ours reveal them. Instead of the Analytic Signal phase, we suggest the use of Linear Symmetry phase, relying on more than one set of Gabor filters, but with a negligible computational add-on, as a remedy. Gradient magnitudes of this phase are continuous in contrast to that of the analytic signal whereas continuity of the gradient direction of the phase is guaranteed if Linear Symmetry Tensor replaces gradient vector. The suggested phase has also a built-in automatic scale estimator, useful for robust detection of patterns by multi-scale processing. We show crucial concepts on synthesized fingerprint images, where ground truth regarding instantaneous frequency, (scale \& direction), and phase are known with favorable results. A comparison to a baseline alternative is also reported. To that end, a novel multi-scale minutia model where location, direction, and scale of minutia parameters are steerable, without the creation of uncontrollable minutia is also presented. This is a useful tool, to reduce development times of minutia detection methods with explainable behavior. A revealed consequence is that minutia directions are not determined by the linear phase alone, but also by each other and the influence must be corrected to obtain steerability and accurate ground truths. Essential conclusions are readily transferable to $N-D $, and unrelated applications, e.g. optical flow or disparity estimation in stereo.



### Mutual Information Maximization for Robust Plannable Representations
- **Arxiv ID**: http://arxiv.org/abs/2005.08114v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.08114v1)
- **Published**: 2020-05-16 21:58:47+00:00
- **Updated**: 2020-05-16 21:58:47+00:00
- **Authors**: Yiming Ding, Ignasi Clavera, Pieter Abbeel
- **Comment**: Accepted at NeurIPS 2019 Workshop on Robot Learning: Control and
  Interaction in the Real World
- **Journal**: None
- **Summary**: Extending the capabilities of robotics to real-world complex, unstructured environments requires the need of developing better perception systems while maintaining low sample complexity. When dealing with high-dimensional state spaces, current methods are either model-free or model-based based on reconstruction objectives. The sample inefficiency of the former constitutes a major barrier for applying them to the real-world. The later, while they present low sample complexity, they learn latent spaces that need to reconstruct every single detail of the scene. In real environments, the task typically just represents a small fraction of the scene. Reconstruction objectives suffer in such scenarios as they capture all the unnecessary components. In this work, we present MIRO, an information theoretic representational learning algorithm for model-based reinforcement learning. We design a latent space that maximizes the mutual information with the future information while being able to capture all the information needed for planning. We show that our approach is more robust than reconstruction objectives in the presence of distractors and cluttered scenes



### From Boundaries to Bumps: when closed (extremal) contours are critical
- **Arxiv ID**: http://arxiv.org/abs/2005.08116v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.08116v1)
- **Published**: 2020-05-16 22:27:12+00:00
- **Updated**: 2020-05-16 22:27:12+00:00
- **Authors**: Benjamin Kunsberg, Steven W. Zucker
- **Comment**: None
- **Journal**: None
- **Summary**: Invariants underlying shape inference are elusive: a variety of shapes can give rise to the same image, and a variety of images can be rendered from the same shape. The occluding contour is a rare exception: it has both image salience, in terms of isophotes, and surface meaning, in terms of surface normal. We relax the notion of occluding contour to define closed extremal curves, a new shape invariant that exists at the topological level. They surround bumps, a common but ill-specified interior shape component, and formalize the qualitative nature of bump perception. Extremal curves are biologically computable, unify shape inferences from shading, texture, and specular materials, and predict new phenomena in bump perception.



