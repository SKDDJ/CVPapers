# Arxiv Papers in cs.CV on 2020-05-29
### UGC-VQA: Benchmarking Blind Video Quality Assessment for User Generated Content
- **Arxiv ID**: http://arxiv.org/abs/2005.14354v2
- **DOI**: 10.1109/TIP.2021.3072221
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.14354v2)
- **Published**: 2020-05-29 00:39:20+00:00
- **Updated**: 2021-04-17 04:40:56+00:00
- **Authors**: Zhengzhong Tu, Yilin Wang, Neil Birkbeck, Balu Adsumilli, Alan C. Bovik
- **Comment**: IEEE Transactions on Image Processing 2021
- **Journal**: None
- **Summary**: Recent years have witnessed an explosion of user-generated content (UGC) videos shared and streamed over the Internet, thanks to the evolution of affordable and reliable consumer capture devices, and the tremendous popularity of social media platforms. Accordingly, there is a great need for accurate video quality assessment (VQA) models for UGC/consumer videos to monitor, control, and optimize this vast content. Blind quality prediction of in-the-wild videos is quite challenging, since the quality degradations of UGC content are unpredictable, complicated, and often commingled. Here we contribute to advancing the UGC-VQA problem by conducting a comprehensive evaluation of leading no-reference/blind VQA (BVQA) features and models on a fixed evaluation architecture, yielding new empirical insights on both subjective video quality studies and VQA model design. By employing a feature selection strategy on top of leading VQA model features, we are able to extract 60 of the 763 statistical features used by the leading models to create a new fusion-based BVQA model, which we dub the \textbf{VID}eo quality \textbf{EVAL}uator (VIDEVAL), that effectively balances the trade-off between VQA performance and efficiency. Our experimental results show that VIDEVAL achieves state-of-the-art performance at considerably lower computational cost than other leading models. Our study protocol also defines a reliable benchmark for the UGC-VQA problem, which we believe will facilitate further research on deep learning-based VQA modeling, as well as perceptually-optimized efficient UGC video processing, transcoding, and streaming. To promote reproducible research and public evaluation, an implementation of VIDEVAL has been made available online: \url{https://github.com/tu184044109/VIDEVAL_release}.



### Enhancing Foreground Boundaries for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2005.14355v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.14355v1)
- **Published**: 2020-05-29 00:50:08+00:00
- **Updated**: 2020-05-29 00:50:08+00:00
- **Authors**: Dong Yang, Holger Roth, Xiaosong Wang, Ziyue Xu, Andriy Myronenko, Daguang Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Object segmentation plays an important role in the modern medical image analysis, which benefits clinical study, disease diagnosis, and surgery planning. Given the various modalities of medical images, the automated or semi-automated segmentation approaches have been used to identify and parse organs, bones, tumors, and other regions-of-interest (ROI). However, these contemporary segmentation approaches tend to fail to predict the boundary areas of ROI, because of the fuzzy appearance contrast caused during the imaging procedure. To further improve the segmentation quality of boundary areas, we propose a boundary enhancement loss to enforce additional constraints on optimizing machine learning models. The proposed loss function is light-weighted and easy to implement without any pre- or post-processing. Our experimental results validate that our loss function are better than, or at least comparable to, other state-of-the-art loss functions in terms of segmentation accuracy.



### Extracting low-dimensional psychological representations from convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/2005.14363v1
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.14363v1)
- **Published**: 2020-05-29 01:29:39+00:00
- **Updated**: 2020-05-29 01:29:39+00:00
- **Authors**: Aditi Jha, Joshua Peterson, Thomas L. Griffiths
- **Comment**: Accepted to CogSci 2020
- **Journal**: None
- **Summary**: Deep neural networks are increasingly being used in cognitive modeling as a means of deriving representations for complex stimuli such as images. While the predictive power of these networks is high, it is often not clear whether they also offer useful explanations of the task at hand. Convolutional neural network representations have been shown to be predictive of human similarity judgments for images after appropriate adaptation. However, these high-dimensional representations are difficult to interpret. Here we present a method for reducing these representations to a low-dimensional space which is still predictive of similarity judgments. We show that these low-dimensional representations also provide insightful explanations of factors underlying human similarity judgments.



### A Light-Weighted Convolutional Neural Network for Bitemporal SAR Image Change Detection
- **Arxiv ID**: http://arxiv.org/abs/2005.14376v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.14376v2)
- **Published**: 2020-05-29 04:01:32+00:00
- **Updated**: 2020-06-21 02:40:04+00:00
- **Authors**: Rongfang Wang, Fan Ding, Licheng Jiao, Jia-Wei Chen, Bo Liu, Wenping Ma, Mi Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, many Convolution Neural Networks (CNN) have been successfully employed in bitemporal SAR image change detection. However, most of the existing networks are too heavy and occupy a large volume of memory for storage and calculation. Motivated by this, in this paper, we propose a lightweight neural network to reduce the computational and spatial complexity and facilitate the change detection on an edge device. In the proposed network, we replace normal convolutional layers with bottleneck layers that keep the same number of channels between input and output. Next, we employ dilated convolutional kernels with a few non-zero entries that reduce the running time in convolutional operators. Comparing with the conventional convolutional neural network, our light-weighted neural network will be more efficient with fewer parameters. We verify our light-weighted neural network on four sets of bitemporal SAR images. The experimental results show that the proposed network can obtain better performance than the conventional CNN and has better model generalization, especially on the challenging datasets with complex scenes.



### Controlling Length in Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/2005.14386v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2005.14386v1)
- **Published**: 2020-05-29 05:03:15+00:00
- **Updated**: 2020-05-29 05:03:15+00:00
- **Authors**: Ruotian Luo, Greg Shakhnarovich
- **Comment**: None
- **Journal**: None
- **Summary**: We develop and evaluate captioning models that allow control of caption length. Our models can leverage this control to generate captions of different style and descriptiveness.



### Privacy-Protection Drone Patrol System based on Face Anonymization
- **Arxiv ID**: http://arxiv.org/abs/2005.14390v1
- **DOI**: 10.1109/ACCESS.2021.3113186
- **Categories**: **cs.CV**, cs.CR, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.14390v1)
- **Published**: 2020-05-29 05:14:18+00:00
- **Updated**: 2020-05-29 05:14:18+00:00
- **Authors**: Harim Lee, Myeung Un Kim, Yeongjun Kim, Hyeonsu Lyu, Hyun Jong Yang
- **Comment**: None
- **Journal**: None
- **Summary**: The robot market has been growing significantly and is expected to become 1.5 times larger in 2024 than what it was in 2019. Robots have attracted attention of security companies thanks to their mobility. These days, for security robots, unmanned aerial vehicles (UAVs) have quickly emerged by highlighting their advantage: they can even go to any hazardous place that humans cannot access. For UAVs, Drone has been a representative model and has several merits to consist of various sensors such as high-resolution cameras. Therefore, Drone is the most suitable as a mobile surveillance robot. These attractive advantages such as high-resolution cameras and mobility can be a double-edged sword, i.e., privacy infringement. Surveillance drones take videos with high-resolution to fulfill their role, however, those contain a lot of privacy sensitive information. The indiscriminate shooting is a critical issue for those who are very reluctant to be exposed. To tackle the privacy infringement, this work proposes face-anonymizing drone patrol system. In this system, one person's face in a video is transformed into a different face with facial components maintained. To construct our privacy-preserving system, we have adopted the latest generative adversarial networks frameworks and have some modifications on losses of those frameworks. Our face-anonymzing approach is evaluated with various public face-image and video dataset. Moreover, our system is evaluated with a customized drone consisting of a high-resolution camera, a companion computer, and a drone control computer. Finally, we confirm that our system can protect privacy sensitive information with our face-anonymzing algorithm while preserving the performance of robot perception, i.e., simultaneous localization and mapping.



### Hyperspectral Image Super-resolution via Deep Spatio-spectral Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2005.14400v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.14400v1)
- **Published**: 2020-05-29 05:56:50+00:00
- **Updated**: 2020-05-29 05:56:50+00:00
- **Authors**: Jin-Fan Hu, Ting-Zhu Huang, Liang-Jian Deng, Tai-Xiang Jiang, Gemine Vivone, Jocelyn Chanussot
- **Comment**: None
- **Journal**: None
- **Summary**: Hyperspectral images are of crucial importance in order to better understand features of different materials. To reach this goal, they leverage on a high number of spectral bands. However, this interesting characteristic is often paid by a reduced spatial resolution compared with traditional multispectral image systems. In order to alleviate this issue, in this work, we propose a simple and efficient architecture for deep convolutional neural networks to fuse a low-resolution hyperspectral image (LR-HSI) and a high-resolution multispectral image (HR-MSI), yielding a high-resolution hyperspectral image (HR-HSI). The network is designed to preserve both spatial and spectral information thanks to an architecture from two folds: one is to utilize the HR-HSI at a different scale to get an output with a satisfied spectral preservation; another one is to apply concepts of multi-resolution analysis to extract high-frequency information, aiming to output high quality spatial details. Finally, a plain mean squared error loss function is used to measure the performance during the training. Extensive experiments demonstrate that the proposed network architecture achieves best performance (both qualitatively and quantitatively) compared with recent state-of-the-art hyperspectral image super-resolution approaches. Moreover, other significant advantages can be pointed out by the use of the proposed approach, such as, a better network generalization ability, a limited computational burden, and a robustness with respect to the number of training samples.



### Deep graph learning for semi-supervised classification
- **Arxiv ID**: http://arxiv.org/abs/2005.14403v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.14403v1)
- **Published**: 2020-05-29 05:59:45+00:00
- **Updated**: 2020-05-29 05:59:45+00:00
- **Authors**: Guangfeng Lin, Xiaobing Kang, Kaiyang Liao, Fan Zhao, Yajun Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Graph learning (GL) can dynamically capture the distribution structure (graph structure) of data based on graph convolutional networks (GCN), and the learning quality of the graph structure directly influences GCN for semi-supervised classification. Existing methods mostly combine the computational layer and the related losses into GCN for exploring the global graph(measuring graph structure from all data samples) or local graph (measuring graph structure from local data samples). Global graph emphasises on the whole structure description of the inter-class data, while local graph trend to the neighborhood structure representation of intra-class data. However, it is difficult to simultaneously balance these graphs of the learning process for semi-supervised classification because of the interdependence of these graphs. To simulate the interdependence, deep graph learning(DGL) is proposed to find the better graph representation for semi-supervised classification. DGL can not only learn the global structure by the previous layer metric computation updating, but also mine the local structure by next layer local weight reassignment. Furthermore, DGL can fuse the different structures by dynamically encoding the interdependence of these structures, and deeply mine the relationship of the different structures by the hierarchical progressive learning for improving the performance of semi-supervised classification. Experiments demonstrate the DGL outperforms state-of-the-art methods on three benchmark datasets (Citeseer,Cora, and Pubmed) for citation networks and two benchmark datasets (MNIST and Cifar10) for images.



### Not made for each other- Audio-Visual Dissonance-based Deepfake Detection and Localization
- **Arxiv ID**: http://arxiv.org/abs/2005.14405v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2005.14405v3)
- **Published**: 2020-05-29 06:09:33+00:00
- **Updated**: 2021-03-20 15:09:49+00:00
- **Authors**: Komal Chugh, Parul Gupta, Abhinav Dhall, Ramanathan Subramanian
- **Comment**: None
- **Journal**: None
- **Summary**: We propose detection of deepfake videos based on the dissimilarity between the audio and visual modalities, termed as the Modality Dissonance Score (MDS). We hypothesize that manipulation of either modality will lead to dis-harmony between the two modalities, eg, loss of lip-sync, unnatural facial and lip movements, etc. MDS is computed as an aggregate of dissimilarity scores between audio and visual segments in a video. Discriminative features are learnt for the audio and visual channels in a chunk-wise manner, employing the cross-entropy loss for individual modalities, and a contrastive loss that models inter-modality similarity. Extensive experiments on the DFDC and DeepFake-TIMIT Datasets show that our approach outperforms the state-of-the-art by up to 7%. We also demonstrate temporal forgery localization, and show how our technique identifies the manipulated video segments.



### High-order structure preserving graph neural network for few-shot learning
- **Arxiv ID**: http://arxiv.org/abs/2005.14415v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.14415v1)
- **Published**: 2020-05-29 06:38:51+00:00
- **Updated**: 2020-05-29 06:38:51+00:00
- **Authors**: Guangfeng Lin, Ying Yang, Yindi Fan, Xiaobing Kang, Kaiyang Liao, Fan Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Few-shot learning can find the latent structure information between the prior knowledge and the queried data by the similarity metric of meta-learning to construct the discriminative model for recognizing the new categories with the rare labeled samples. Most existing methods try to model the similarity relationship of the samples in the intra tasks, and generalize the model to identify the new categories. However, the relationship of samples between the separated tasks is difficultly considered because of the different metric criterion in the respective tasks. In contrast, the proposed high-order structure preserving graph neural network(HOSP-GNN) can further explore the rich structure of the samples to predict the label of the queried data on graph that enables the structure evolution to explicitly discriminate the categories by iteratively updating the high-order structure relationship (the relative metric in multi-samples,instead of pairwise sample metric) with the manifold structure constraints. HOSP-GNN can not only mine the high-order structure for complementing the relevance between samples that may be divided into the different task in meta-learning, and but also generate the rule of the structure updating by manifold constraint. Furthermore, HOSP-GNN doesn't need retrain the learning model for recognizing the new classes, and HOSP-GNN has the well-generalizable high-order structure for model adaptability. Experiments show that HOSP-GNN outperforms the state-of-the-art methods on supervised and semi-supervised few-shot learning in three benchmark datasets that are miniImageNet, tieredImageNet and FC100.



### CoDiNet: Path Distribution Modeling with Consistency and Diversity for Dynamic Routing
- **Arxiv ID**: http://arxiv.org/abs/2005.14439v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.14439v3)
- **Published**: 2020-05-29 08:09:21+00:00
- **Updated**: 2021-05-26 08:21:06+00:00
- **Authors**: Huanyu Wang, Zequn Qin, Songyuan Li, Xi Li
- **Comment**: None
- **Journal**: None
- **Summary**: Dynamic routing networks, aimed at finding the best routing paths in the networks, have achieved significant improvements to neural networks in terms of accuracy and efficiency. In this paper, we see dynamic routing networks in a fresh light, formulating a routing method as a mapping from a sample space to a routing space. From the perspective of space mapping, prevalent methods of dynamic routing didn't consider how inference paths would be distributed in the routing space. Thus, we propose a novel method, termed CoDiNet, to model the relationship between a sample space and a routing space by regularizing the distribution of routing paths with the properties of consistency and diversity. Specifically, samples with similar semantics should be mapped into the same area in routing space, while those with dissimilar semantics should be mapped into different areas. Moreover, we design a customizable dynamic routing module, which can strike a balance between accuracy and efficiency. When deployed upon ResNet models, our method achieves higher performance and effectively reduces average computational cost on four widely used datasets.



### HourNAS: Extremely Fast Neural Architecture Search Through an Hourglass Lens
- **Arxiv ID**: http://arxiv.org/abs/2005.14446v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.14446v3)
- **Published**: 2020-05-29 08:35:32+00:00
- **Updated**: 2020-12-08 00:43:12+00:00
- **Authors**: Zhaohui Yang, Yunhe Wang, Xinghao Chen, Jianyuan Guo, Wei Zhang, Chao Xu, Chunjing Xu, Dacheng Tao, Chang Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Neural Architecture Search (NAS) refers to automatically design the architecture. We propose an hourglass-inspired approach (HourNAS) for this problem that is motivated by the fact that the effects of the architecture often proceed from the vital few blocks. Acting like the narrow neck of an hourglass, vital blocks in the guaranteed path from the input to the output of a deep neural network restrict the information flow and influence the network accuracy. The other blocks occupy the major volume of the network and determine the overall network complexity, corresponding to the bulbs of an hourglass. To achieve an extremely fast NAS while preserving the high accuracy, we propose to identify the vital blocks and make them the priority in the architecture search. The search space of those non-vital blocks is further shrunk to only cover the candidates that are affordable under the computational resource constraints. Experimental results on the ImageNet show that only using 3 hours (0.1 days) with one GPU, our HourNAS can search an architecture that achieves a 77.0% Top-1 accuracy, which outperforms the state-of-the-art methods.



### WaveSNet: Wavelet Integrated Deep Networks for Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2005.14461v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.14461v1)
- **Published**: 2020-05-29 09:17:37+00:00
- **Updated**: 2020-05-29 09:17:37+00:00
- **Authors**: Qiufu Li, Linlin Shen
- **Comment**: 7 pages, 7 figures
- **Journal**: None
- **Summary**: In deep networks, the lost data details significantly degrade the performances of image segmentation. In this paper, we propose to apply Discrete Wavelet Transform (DWT) to extract the data details during feature map down-sampling, and adopt Inverse DWT (IDWT) with the extracted details during the up-sampling to recover the details. We firstly transform DWT/IDWT as general network layers, which are applicable to 1D/2D/3D data and various wavelets like Haar, Cohen, and Daubechies, etc. Then, we design wavelet integrated deep networks for image segmentation (WaveSNets) based on various architectures, including U-Net, SegNet, and DeepLabv3+. Due to the effectiveness of the DWT/IDWT in processing data details, experimental results on CamVid, Pascal VOC, and Cityscapes show that our WaveSNets achieve better segmentation performances than their vanilla versions.



### Weakly Supervised Lesion Localization With Probabilistic-CAM Pooling
- **Arxiv ID**: http://arxiv.org/abs/2005.14480v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.14480v1)
- **Published**: 2020-05-29 09:57:17+00:00
- **Updated**: 2020-05-29 09:57:17+00:00
- **Authors**: Wenwu Ye, Jin Yao, Hui Xue, Yi Li
- **Comment**: None
- **Journal**: None
- **Summary**: Localizing thoracic diseases on chest X-ray plays a critical role in clinical practices such as diagnosis and treatment planning. However, current deep learning based approaches often require strong supervision, e.g. annotated bounding boxes, for training such systems, which is infeasible to harvest in large-scale. We present Probabilistic Class Activation Map (PCAM) pooling, a novel global pooling operation for lesion localization with only image-level supervision. PCAM pooling explicitly leverages the excellent localization ability of CAM during training in a probabilistic fashion. Experiments on the ChestX-ray14 dataset show a ResNet-34 model trained with PCAM pooling outperforms state-of-the-art baselines on both the classification task and the localization task. Visual examination on the probability maps generated by PCAM pooling shows clear and sharp boundaries around lesion regions compared to the localization heatmaps generated by CAM. PCAM pooling is open sourced at https://github.com/jfhealthcare/Chexpert.



### Unconstrained Matching of 2D and 3D Descriptors for 6-DOF Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2005.14502v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2005.14502v1)
- **Published**: 2020-05-29 11:17:32+00:00
- **Updated**: 2020-05-29 11:17:32+00:00
- **Authors**: Uzair Nadeem, Mohammed Bennamoun, Roberto Togneri, Ferdous Sohel
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a novel concept to directly match feature descriptors extracted from 2D images with feature descriptors extracted from 3D point clouds. We use this concept to directly localize images in a 3D point cloud. We generate a dataset of matching 2D and 3D points and their corresponding feature descriptors, which is used to learn a Descriptor-Matcher classifier. To localize the pose of an image at test time, we extract keypoints and feature descriptors from the query image. The trained Descriptor-Matcher is then used to match the features from the image and the point cloud. The locations of the matched features are used in a robust pose estimation algorithm to predict the location and orientation of the query image. We carried out an extensive evaluation of the proposed method for indoor and outdoor scenarios and with different types of point clouds to verify the feasibility of our approach. Experimental results demonstrate that direct matching of feature descriptors from images and point clouds is not only a viable idea but can also be reliably used to estimate the 6-DOF poses of query cameras in any type of 3D point cloud in an unconstrained manner with high precision.



### NuClick: A Deep Learning Framework for Interactive Segmentation of Microscopy Images
- **Arxiv ID**: http://arxiv.org/abs/2005.14511v2
- **DOI**: None
- **Categories**: **cs.CV**, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/2005.14511v2)
- **Published**: 2020-05-29 11:51:27+00:00
- **Updated**: 2020-07-07 15:27:41+00:00
- **Authors**: Navid Alemi Koohbanani, Mostafa Jahanifar, Neda Zamani Tajadin, Nasir Rajpoot
- **Comment**: None
- **Journal**: None
- **Summary**: Object segmentation is an important step in the workflow of computational pathology. Deep learning based models generally require large amount of labeled data for precise and reliable prediction. However, collecting labeled data is expensive because it often requires expert knowledge, particularly in medical imaging domain where labels are the result of a time-consuming analysis made by one or more human experts. As nuclei, cells and glands are fundamental objects for downstream analysis in computational pathology/cytology, in this paper we propose a simple CNN-based approach to speed up collecting annotations for these objects which requires minimum interaction from the annotator. We show that for nuclei and cells in histology and cytology images, one click inside each object is enough for NuClick to yield a precise annotation. For multicellular structures such as glands, we propose a novel approach to provide the NuClick with a squiggle as a guiding signal, enabling it to segment the glandular boundaries. These supervisory signals are fed to the network as auxiliary inputs along with RGB channels. With detailed experiments, we show that NuClick is adaptable to the object scale, robust against variations in the user input, adaptable to new domains, and delivers reliable annotations. An instance segmentation model trained on masks generated by NuClick achieved the first rank in LYON19 challenge. As exemplar outputs of our framework, we are releasing two datasets: 1) a dataset of lymphocyte annotations within IHC images, and 2) a dataset of segmented WBCs in blood smear images.



### Probabilistic Object Classification using CNN ML-MAP layers
- **Arxiv ID**: http://arxiv.org/abs/2005.14565v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.14565v2)
- **Published**: 2020-05-29 13:34:15+00:00
- **Updated**: 2020-08-24 11:37:53+00:00
- **Authors**: G. Melotti, C. Premebida, J. J. Bird, D. R. Faria, N. Gonçalves
- **Comment**: Accepted at the Workshop on Perception for Autonomous Driving (PAD),
  European Conference on Computer Vision (ECCV) 2020
- **Journal**: None
- **Summary**: Deep networks are currently the state-of-the-art for sensory perception in autonomous driving and robotics. However, deep models often generate overconfident predictions precluding proper probabilistic interpretation which we argue is due to the nature of the SoftMax layer. To reduce the overconfidence without compromising the classification performance, we introduce a CNN probabilistic approach based on distributions calculated in the network's Logit layer. The approach enables Bayesian inference by means of ML and MAP layers. Experiments with calibrated and the proposed prediction layers are carried out on object classification using data from the KITTI database. Results are reported for camera ($RGB$) and LiDAR (range-view) modalities, where the new approach shows promising performance compared to SoftMax.



### Fixed-size Objects Encoding for Visual Relationship Detection
- **Arxiv ID**: http://arxiv.org/abs/2005.14600v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.14600v1)
- **Published**: 2020-05-29 14:36:25+00:00
- **Updated**: 2020-05-29 14:36:25+00:00
- **Authors**: Hengyue Pan, Xin Niu, Rongchun Li, Siqi Shen, Yong Dou
- **Comment**: 14 pages
- **Journal**: None
- **Summary**: In this paper, we propose a fixed-size object encoding method (FOE-VRD) to improve performance of visual relationship detection tasks. Comparing with previous methods, FOE-VRD has an important feature, i.e., it uses one fixed-size vector to encoding all objects in each input image to assist the process of relationship detection. Firstly, we use a regular convolution neural network as a feature extractor to generate high-level features of input images. Then, for each relationship triplet in input images, i.e., $<$subject-predicate-object$>$, we apply ROI-pooling to get feature vectors of two regions on the feature maps that corresponding to bounding boxes of the subject and object. Besides the subject and object, our analysis implies that the results of predicate classification may also related to the rest objects in input images (we call them background objects). Due to the variable number of background objects in different images and computational costs, we cannot generate feature vectors for them one-by-one by using ROI pooling technique. Instead, we propose a novel method to encode all background objects in each image by using one fixed-size vector (i.e., FBE vector). By concatenating the 3 vectors we generate above, we successfully encode the objects using one fixed-size vector. The generated feature vector is then feed into a fully connected neural network to get predicate classification results. Experimental results on VRD database (entire set and zero-shot tests) show that the proposed method works well on both predicate classification and relationship detection.



### Federated Face Presentation Attack Detection
- **Arxiv ID**: http://arxiv.org/abs/2005.14638v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.14638v2)
- **Published**: 2020-05-29 15:56:01+00:00
- **Updated**: 2020-09-29 03:01:14+00:00
- **Authors**: Rui Shao, Pramuditha Perera, Pong C. Yuen, Vishal M. Patel
- **Comment**: None
- **Journal**: None
- **Summary**: Face presentation attack detection (fPAD) plays a critical role in the modern face recognition pipeline. A face presentation attack detection model with good generalization can be obtained when it is trained with face images from different input distributions and different types of spoof attacks. In reality, training data (both real face images and spoof images) are not directly shared between data owners due to legal and privacy issues. In this paper, with the motivation of circumventing this challenge, we propose Federated Face Presentation Attack Detection (FedPAD) framework. FedPAD simultaneously takes advantage of rich fPAD information available at different data owners while preserving data privacy. In the proposed framework, each data owner (referred to as \textit{data centers}) locally trains its own fPAD model. A server learns a global fPAD model by iteratively aggregating model updates from all data centers without accessing private data in each of them. Once the learned global model converges, it is used for fPAD inference. We introduce the experimental setting to evaluate the proposed FedPAD framework and carry out extensive experiments to provide various insights about federated learning for fPAD.



### Exploring Spatial Significance via Hybrid Pyramidal Graph Network for Vehicle Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2005.14684v2
- **DOI**: 10.1109/TITS.2021.3086142
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2005.14684v2)
- **Published**: 2020-05-29 17:21:12+00:00
- **Updated**: 2020-06-05 02:23:37+00:00
- **Authors**: Fei Shen, Jianqing Zhu, Xiaobin Zhu, Yi Xie, Jingchang Huang
- **Comment**: None
- **Journal**: IEEE Transactions on Intelligent Transportation Systems, 2021
- **Summary**: Existing vehicle re-identification methods commonly use spatial pooling operations to aggregate feature maps extracted via off-the-shelf backbone networks. They ignore exploring the spatial significance of feature maps, eventually degrading the vehicle re-identification performance. In this paper, firstly, an innovative spatial graph network (SGN) is proposed to elaborately explore the spatial significance of feature maps. The SGN stacks multiple spatial graphs (SGs). Each SG assigns feature map's elements as nodes and utilizes spatial neighborhood relationships to determine edges among nodes. During the SGN's propagation, each node and its spatial neighbors on an SG are aggregated to the next SG. On the next SG, each aggregated node is re-weighted with a learnable parameter to find the significance at the corresponding location. Secondly, a novel pyramidal graph network (PGN) is designed to comprehensively explore the spatial significance of feature maps at multiple scales. The PGN organizes multiple SGNs in a pyramidal manner and makes each SGN handles feature maps of a specific scale. Finally, a hybrid pyramidal graph network (HPGN) is developed by embedding the PGN behind a ResNet-50 based backbone network. Extensive experiments on three large scale vehicle databases (i.e., VeRi776, VehicleID, and VeRi-Wild) demonstrate that the proposed HPGN is superior to state-of-the-art vehicle re-identification approaches.



### PnPNet: End-to-End Perception and Prediction with Tracking in the Loop
- **Arxiv ID**: http://arxiv.org/abs/2005.14711v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2005.14711v2)
- **Published**: 2020-05-29 17:57:25+00:00
- **Updated**: 2020-06-27 21:32:07+00:00
- **Authors**: Ming Liang, Bin Yang, Wenyuan Zeng, Yun Chen, Rui Hu, Sergio Casas, Raquel Urtasun
- **Comment**: CVPR2020
- **Journal**: None
- **Summary**: We tackle the problem of joint perception and motion forecasting in the context of self-driving vehicles. Towards this goal we propose PnPNet, an end-to-end model that takes as input sequential sensor data, and outputs at each time step object tracks and their future trajectories. The key component is a novel tracking module that generates object tracks online from detections and exploits trajectory level features for motion forecasting. Specifically, the object tracks get updated at each time step by solving both the data association problem and the trajectory estimation problem. Importantly, the whole model is end-to-end trainable and benefits from joint optimization of all tasks. We validate PnPNet on two large-scale driving datasets, and show significant improvements over the state-of-the-art with better occlusion recovery and more accurate future prediction.



### Glaucoma Detection From Raw Circumapillary OCT Images Using Fully Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2006.00027v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.00027v1)
- **Published**: 2020-05-29 18:31:07+00:00
- **Updated**: 2020-05-29 18:31:07+00:00
- **Authors**: Gabriel García, Rocío del Amor, Adrián Colomer, Valery Naranjo
- **Comment**: None
- **Journal**: None
- **Summary**: Nowadays, glaucoma is the leading cause of blindness worldwide. We propose in this paper two different deep-learning-based approaches to address glaucoma detection just from raw circumpapillary OCT images. The first one is based on the development of convolutional neural networks (CNNs) trained from scratch. The second one lies in fine-tuning some of the most common state-of-the-art CNNs architectures. The experiments were performed on a private database composed of 93 glaucomatous and 156 normal B-scans around the optic nerve head of the retina, which were diagnosed by expert ophthalmologists. The validation results evidence that fine-tuned CNNs outperform the networks trained from scratch when small databases are addressed. Additionally, the VGG family of networks reports the most promising results, with an area under the ROC curve of 0.96 and an accuracy of 0.92, during the prediction of the independent test set.



### Learning stochastic object models from medical imaging measurements using Progressively-Growing AmbientGANs
- **Arxiv ID**: http://arxiv.org/abs/2006.00033v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.00033v1)
- **Published**: 2020-05-29 18:45:37+00:00
- **Updated**: 2020-05-29 18:45:37+00:00
- **Authors**: Weimin Zhou, Sayantan Bhadra, Frank J. Brooks, Hua Li, Mark A. Anastasio
- **Comment**: Submitted to IEEE Transactions on Medical Imaging
- **Journal**: None
- **Summary**: It has been advocated that medical imaging systems and reconstruction algorithms should be assessed and optimized by use of objective measures of image quality that quantify the performance of an observer at specific diagnostic tasks. One important source of variability that can significantly limit observer performance is variation in the objects to-be-imaged. This source of variability can be described by stochastic object models (SOMs). A SOM is a generative model that can be employed to establish an ensemble of to-be-imaged objects with prescribed statistical properties. In order to accurately model variations in anatomical structures and object textures, it is desirable to establish SOMs from experimental imaging measurements acquired by use of a well-characterized imaging system. Deep generative neural networks, such as generative adversarial networks (GANs) hold great potential for this task. However, conventional GANs are typically trained by use of reconstructed images that are influenced by the effects of measurement noise and the reconstruction process. To circumvent this, an AmbientGAN has been proposed that augments a GAN with a measurement operator. However, the original AmbientGAN could not immediately benefit from modern training procedures, such as progressive growing, which limited its ability to be applied to realistically sized medical image data. To circumvent this, in this work, a new Progressive Growing AmbientGAN (ProAmGAN) strategy is developed for establishing SOMs from medical imaging measurements. Stylized numerical studies corresponding to common medical imaging modalities are conducted to demonstrate and validate the proposed method for establishing SOMs.



### Applying the Decisiveness and Robustness Metrics to Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2006.00058v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.00058v1)
- **Published**: 2020-05-29 20:05:06+00:00
- **Updated**: 2020-05-29 20:05:06+00:00
- **Authors**: Christopher A. George, Eduardo A. Barrera, Kenric P. Nelson
- **Comment**: None
- **Journal**: None
- **Summary**: We review three recently-proposed classifier quality metrics and consider their suitability for large-scale classification challenges such as applying convolutional neural networks to the 1000-class ImageNet dataset. These metrics, referred to as the "geometric accuracy," "decisiveness," and "robustness," are based on the generalized mean ($\rho$ equals 0, 1, and -2/3, respectively) of the classifier's self-reported and measured probabilities of correct classification. We also propose some minor clarifications to standardize the metric definitions. With these updates, we show some examples of calculating the metrics using deep convolutional neural networks (AlexNet and DenseNet) acting on large datasets (the German Traffic Sign Recognition Benchmark and ImageNet).



### Towards a Human-Centred Cognitive Model of Visuospatial Complexity in Everyday Driving
- **Arxiv ID**: http://arxiv.org/abs/2006.00059v2
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2006.00059v2)
- **Published**: 2020-05-29 20:12:39+00:00
- **Updated**: 2020-06-02 07:01:09+00:00
- **Authors**: Vasiliki Kondyli, Mehul Bhatt, Jakob Suchan
- **Comment**: 9th European Starting AI Researchers Symposium (STAIRS), at ECAI
  2020, the 24th European Conference on Artificial Intelligence (ECAI).,
  Santiago de Compostela, Spain
- **Journal**: None
- **Summary**: We develop a human-centred, cognitive model of visuospatial complexity in everyday, naturalistic driving conditions. With a focus on visual perception, the model incorporates quantitative, structural, and dynamic attributes identifiable in the chosen context; the human-centred basis of the model lies in its behavioural evaluation with human subjects with respect to psychophysical measures pertaining to embodied visuoauditory attention. We report preliminary steps to apply the developed cognitive model of visuospatial complexity for human-factors guided dataset creation and benchmarking, and for its use as a semantic template for the (explainable) computational analysis of visuospatial complexity.



### Assessing the validity of saliency maps for abnormality localization in medical imaging
- **Arxiv ID**: http://arxiv.org/abs/2006.00063v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.00063v1)
- **Published**: 2020-05-29 20:17:26+00:00
- **Updated**: 2020-05-29 20:17:26+00:00
- **Authors**: Nishanth Thumbavanam Arun, Nathan Gaw, Praveer Singh, Ken Chang, Katharina Viktoria Hoebel, Jay Patel, Mishka Gidwani, Jayashree Kalpathy-Cramer
- **Comment**: None
- **Journal**: None
- **Summary**: Saliency maps have become a widely used method to assess which areas of the input image are most pertinent to the prediction of a trained neural network. However, in the context of medical imaging, there is no study to our knowledge that has examined the efficacy of these techniques and quantified them using overlap with ground truth bounding boxes. In this work, we explored the credibility of the various existing saliency map methods on the RSNA Pneumonia dataset. We found that GradCAM was the most sensitive to model parameter and label randomization, and was highly agnostic to model architecture.



### Automated Measurements of Key Morphological Features of Human Embryos for IVF
- **Arxiv ID**: http://arxiv.org/abs/2006.00067v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, q-bio.OT
- **Links**: [PDF](http://arxiv.org/pdf/2006.00067v2)
- **Published**: 2020-05-29 20:27:17+00:00
- **Updated**: 2020-07-20 21:34:27+00:00
- **Authors**: Brian D. Leahy, Won-Dong Jang, Helen Y. Yang, Robbert Struyven, Donglai Wei, Zhe Sun, Kylie R. Lee, Charlotte Royston, Liz Cam, Yael Kalma, Foad Azem, Dalit Ben-Yosef, Hanspeter Pfister, Daniel Needleman
- **Comment**: to be presented at MICCAI 2020
- **Journal**: None
- **Summary**: A major challenge in clinical In-Vitro Fertilization (IVF) is selecting the highest quality embryo to transfer to the patient in the hopes of achieving a pregnancy. Time-lapse microscopy provides clinicians with a wealth of information for selecting embryos. However, the resulting movies of embryos are currently analyzed manually, which is time consuming and subjective. Here, we automate feature extraction of time-lapse microscopy of human embryos with a machine-learning pipeline of five convolutional neural networks (CNNs). Our pipeline consists of (1) semantic segmentation of the regions of the embryo, (2) regression predictions of fragment severity, (3) classification of the developmental stage, and object instance segmentation of (4) cells and (5) pronuclei. Our approach greatly speeds up the measurement of quantitative, biologically relevant features that may aid in embryo selection.



### Automatic Diagnosis of Pulmonary Embolism Using an Attention-guided Framework: A Large-scale Study
- **Arxiv ID**: http://arxiv.org/abs/2006.00074v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.00074v1)
- **Published**: 2020-05-29 20:46:24+00:00
- **Updated**: 2020-05-29 20:46:24+00:00
- **Authors**: Luyao Shi, Deepta Rajan, Shafiq Abedin, Manikanta Srikar Yellapragada, David Beymer, Ehsan Dehghan
- **Comment**: MIDL 2020 Full Paper
- **Journal**: None
- **Summary**: Pulmonary Embolism (PE) is a life-threatening disorder associated with high mortality and morbidity. Prompt diagnosis and immediate initiation of therapeutic action is important. We explored a deep learning model to detect PE on volumetric contrast-enhanced chest CT scans using a 2-stage training strategy. First, a residual convolutional neural network (ResNet) was trained using annotated 2D images. In addition to the classification loss, an attention loss was added during training to help the network focus attention on PE. Next, a recurrent network was used to scan sequentially through the features provided by the pre-trained ResNet to detect PE. This combination allows the network to be trained using both a limited and sparse set of pixel-level annotated images and a large number of easily obtainable patient-level image-label pairs. We used 1,670 sparsely annotated studies and more than 10,000 labeled studies in our training. On a test set with 2,160 patient studies, the proposed method achieved an area under the ROC curve (AUC) of 0.812. The proposed framework is also able to provide localized attention maps that indicate possible PE lesions, which could potentially help radiologists accelerate the diagnostic process.



### Synthetic Learning: Learn From Distributed Asynchronized Discriminator GAN Without Sharing Medical Image Data
- **Arxiv ID**: http://arxiv.org/abs/2006.00080v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.00080v2)
- **Published**: 2020-05-29 21:05:49+00:00
- **Updated**: 2020-06-14 04:18:29+00:00
- **Authors**: Qi Chang, Hui Qu, Yikai Zhang, Mert Sabuncu, Chao Chen, Tong Zhang, Dimitris Metaxas
- **Comment**: None
- **Journal**: The IEEE/CVF Conference on Computer Vision and Pattern Recognition
  (CVPR), 2020, pp. 13856-13866
- **Summary**: In this paper, we propose a data privacy-preserving and communication efficient distributed GAN learning framework named Distributed Asynchronized Discriminator GAN (AsynDGAN). Our proposed framework aims to train a central generator learns from distributed discriminator, and use the generated synthetic image solely to train the segmentation model.We validate the proposed framework on the application of health entities learning problem which is known to be privacy sensitive. Our experiments show that our approach: 1) could learn the real image's distribution from multiple datasets without sharing the patient's raw data. 2) is more efficient and requires lower bandwidth than other distributed deep learning methods. 3) achieves higher performance compared to the model trained by one real dataset, and almost the same performance compared to the model trained by all real datasets. 4) has provable guarantees that the generator could learn the distributed distribution in an all important fashion thus is unbiased.



### Automatic segmentation of the pulmonary lobes with a 3D u-net and optimized loss function
- **Arxiv ID**: http://arxiv.org/abs/2006.00083v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.00083v1)
- **Published**: 2020-05-29 21:18:34+00:00
- **Updated**: 2020-05-29 21:18:34+00:00
- **Authors**: Bianca Lassen-Schmidt, Alessa Hering, Stefan Krass, Hans Meine
- **Comment**: MIDL2020 short paper
- **Journal**: None
- **Summary**: Fully-automatic lung lobe segmentation is challenging due to anatomical variations, pathologies, and incomplete fissures. We trained a 3D u-net for pulmonary lobe segmentation on 49 mainly publically available datasets and introduced a weighted Dice loss function to emphasize the lobar boundaries. To validate the performance of the proposed method we compared the results to two other methods. The new loss function improved the mean distance to 1.46 mm (compared to 2.08 mm for simple loss function without weighting).



### Synthesizing lesions using contextual GANs improves breast cancer classification on mammograms
- **Arxiv ID**: http://arxiv.org/abs/2006.00086v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.00086v1)
- **Published**: 2020-05-29 21:23:00+00:00
- **Updated**: 2020-05-29 21:23:00+00:00
- **Authors**: Eric Wu, Kevin Wu, William Lotter
- **Comment**: None
- **Journal**: None
- **Summary**: Data scarcity and class imbalance are two fundamental challenges in many machine learning applications to healthcare. Breast cancer classification in mammography exemplifies these challenges, with a malignancy rate of around 0.5% in a screening population, which is compounded by the relatively small size of lesions (~1% of the image) in malignant cases. Simultaneously, the prevalence of screening mammography creates a potential abundance of non-cancer exams to use for training. Altogether, these characteristics lead to overfitting on cancer cases, while under-utilizing non-cancer data. Here, we present a novel generative adversarial network (GAN) model for data augmentation that can realistically synthesize and remove lesions on mammograms. With self-attention and semi-supervised learning components, the U-net-based architecture can generate high resolution (256x256px) outputs, as necessary for mammography. When augmenting the original training set with the GAN-generated samples, we find a significant improvement in malignancy classification performance on a test set of real mammogram patches. Overall, the empirical results of our algorithm and the relevance to other medical imaging paradigms point to potentially fruitful further applications.



### Anatomical Predictions using Subject-Specific Medical Data
- **Arxiv ID**: http://arxiv.org/abs/2006.00090v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.00090v1)
- **Published**: 2020-05-29 21:30:46+00:00
- **Updated**: 2020-05-29 21:30:46+00:00
- **Authors**: Marianne Rakic, John Guttag, Adrian V. Dalca
- **Comment**: Accepted as a short paper to MIDL2020. Keywords: Medical Imaging,
  Multi-Modal, Prediction
- **Journal**: None
- **Summary**: Changes over time in brain anatomy can provide important insight for treatment design or scientific analyses. We present a method that predicts how a brain MRI for an individual will change over time. We model changes using a diffeomorphic deformation field that we predict using function using convolutional neural networks. Given a predicted deformation field, a baseline scan can be warped to give a prediction of the brain scan at a future time. We demonstrate the method using the ADNI cohort, and analyze how performance is affected by model variants and the subject-specific information provided. We show that the model provides good predictions and that external clinical data can improve predictions.



### IMUTube: Automatic Extraction of Virtual on-body Accelerometry from Video for Human Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/2006.05675v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.05675v2)
- **Published**: 2020-05-29 21:50:38+00:00
- **Updated**: 2020-08-04 15:21:46+00:00
- **Authors**: Hyeokhyen Kwon, Catherine Tong, Harish Haresamudram, Yan Gao, Gregory D. Abowd, Nicholas D. Lane, Thomas Ploetz
- **Comment**: None
- **Journal**: None
- **Summary**: The lack of large-scale, labeled data sets impedes progress in developing robust and generalized predictive models for on-body sensor-based human activity recognition (HAR). Labeled data in human activity recognition is scarce and hard to come by, as sensor data collection is expensive, and the annotation is time-consuming and error-prone. To address this problem, we introduce IMUTube, an automated processing pipeline that integrates existing computer vision and signal processing techniques to convert videos of human activity into virtual streams of IMU data. These virtual IMU streams represent accelerometry at a wide variety of locations on the human body. We show how the virtually-generated IMU data improves the performance of a variety of models on known HAR datasets. Our initial results are very promising, but the greater promise of this work lies in a collective approach by the computer vision, signal processing, and activity recognition communities to extend this work in ways that we outline. This should lead to on-body, sensor-based HAR becoming yet another success story in large-dataset breakthroughs in recognition.



### Automated Neuron Shape Analysis from Electron Microscopy
- **Arxiv ID**: http://arxiv.org/abs/2006.00100v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.9; I.5.3; J.3
- **Links**: [PDF](http://arxiv.org/pdf/2006.00100v1)
- **Published**: 2020-05-29 22:19:00+00:00
- **Updated**: 2020-05-29 22:19:00+00:00
- **Authors**: Sharmishtaa Seshamani, Leila Elabbady, Casey Schneider-Mizell, Gayathri Mahalingam, Sven Dorkenwald, Agnes Bodor, Thomas Macrina, Daniel Bumbarger, JoAnn Buchanan, Marc Takeno, Wenjing Yin, Derrick Brittain, Russel Torres, Daniel Kapner, Kisuk lee, Ran Lu, Jinpeng Wu, Nuno daCosta, Clay Reid, Forrest Collman
- **Comment**: 9 pages, 4 figures
- **Journal**: None
- **Summary**: Morphology based analysis of cell types has been an area of great interest to the neuroscience community for several decades. Recently, high resolution electron microscopy (EM) datasets of the mouse brain have opened up opportunities for data analysis at a level of detail that was previously impossible. These datasets are very large in nature and thus, manual analysis is not a practical solution. Of particular interest are details to the level of post synaptic structures. This paper proposes a fully automated framework for analysis of post-synaptic structure based neuron analysis from EM data. The processing framework involves shape extraction, representation with an autoencoder, and whole cell modeling and analysis based on shape distributions. We apply our novel framework on a dataset of 1031 neurons obtained from imaging a 1mm x 1mm x 40 micrometer volume of the mouse visual cortex and show the strength of our method in clustering and classification of neuronal shapes.



### Approximating the Ideal Observer for joint signal detection and localization tasks by use of supervised learning methods
- **Arxiv ID**: http://arxiv.org/abs/2006.00112v2
- **DOI**: 10.1109/TMI.2020.3009022
- **Categories**: **eess.SP**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.00112v2)
- **Published**: 2020-05-29 22:53:45+00:00
- **Updated**: 2020-07-15 02:01:08+00:00
- **Authors**: Weimin Zhou, Hua Li, Mark A. Anastasio
- **Comment**: IEEE Transactions on Medical Imaging (Early Access), 2020
- **Journal**: None
- **Summary**: Medical imaging systems are commonly assessed and optimized by use of objective measures of image quality (IQ). The Ideal Observer (IO) performance has been advocated to provide a figure-of-merit for use in assessing and optimizing imaging systems because the IO sets an upper performance limit among all observers. When joint signal detection and localization tasks are considered, the IO that employs a modified generalized likelihood ratio test maximizes observer performance as characterized by the localization receiver operating characteristic (LROC) curve. Computations of likelihood ratios are analytically intractable in the majority of cases. Therefore, sampling-based methods that employ Markov-Chain Monte Carlo (MCMC) techniques have been developed to approximate the likelihood ratios. However, the applications of MCMC methods have been limited to relatively simple object models. Supervised learning-based methods that employ convolutional neural networks have been recently developed to approximate the IO for binary signal detection tasks. In this paper, the ability of supervised learning-based methods to approximate the IO for joint signal detection and localization tasks is explored. Both background-known-exactly and background-known-statistically signal detection and localization tasks are considered. The considered object models include a lumpy object model and a clustered lumpy model, and the considered measurement noise models include Laplacian noise, Gaussian noise, and mixed Poisson-Gaussian noise. The LROC curves produced by the supervised learning-based method are compared to those produced by the MCMC approach or analytical computation when feasible. The potential utility of the proposed method for computing objective measures of IQ for optimizing imaging system performance is explored.



### Overview of Scanner Invariant Representations
- **Arxiv ID**: http://arxiv.org/abs/2006.00115v1
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.00115v1)
- **Published**: 2020-05-29 22:56:47+00:00
- **Updated**: 2020-05-29 22:56:47+00:00
- **Authors**: Daniel Moyer, Greg Ver Steeg, Paul M. Thompson
- **Comment**: Accepted as a short paper in MIDL 2020. In accordance with the MIDL
  2020 Call for Papers, this short paper is an overview of an already published
  work arXiv:1904.05375, and was submitted to MIDL in order to allow
  presentation and discussion at the meeting
- **Journal**: None
- **Summary**: Pooled imaging data from multiple sources is subject to bias from each source. Studies that do not correct for these scanner/site biases at best lose statistical power, and at worst leave spurious correlations in their data. Estimation of the bias effects is non-trivial due to the paucity of data with correspondence across sites, so called "traveling phantom" data, which is expensive to collect. Nevertheless, numerous solutions leveraging direct correspondence have been proposed. In contrast to this, Moyer et al. (2019) proposes an unsupervised solution using invariant representations, one which does not require correspondence and thus does not require paired images. By leveraging the data processing inequality, an invariant representation can then be used to create an image reconstruction that is uninformative of its original source, yet still faithful to the underlying structure. In the present abstract we provide an overview of this method.



