# Arxiv Papers in cs.CV on 2020-05-27
### Benchmarking Differentially Private Residual Networks for Medical Imagery
- **Arxiv ID**: http://arxiv.org/abs/2005.13099v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.13099v5)
- **Published**: 2020-05-27 00:29:56+00:00
- **Updated**: 2020-09-05 02:25:06+00:00
- **Authors**: Sahib Singh, Harshvardhan Sikka, Sasikanth Kotti, Andrew Trask
- **Comment**: 5 Pages, 4 Figures
- **Journal**: None
- **Summary**: In this paper we measure the effectiveness of $\epsilon$-Differential Privacy (DP) when applied to medical imaging. We compare two robust differential privacy mechanisms: Local-DP and DP-SGD and benchmark their performance when analyzing medical imagery records. We analyze the trade-off between the model's accuracy and the level of privacy it guarantees, and also take a closer look to evaluate how useful these theoretical privacy guarantees actually prove to be in the real world medical setting.



### Road Segmentation on low resolution Lidar point clouds for autonomous vehicles
- **Arxiv ID**: http://arxiv.org/abs/2005.13102v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.13102v1)
- **Published**: 2020-05-27 00:38:39+00:00
- **Updated**: 2020-05-27 00:38:39+00:00
- **Authors**: Leonardo Gigli, B Ravi Kiran, Thomas Paul, Andres Serna, Nagarjuna Vemuri, Beatriz Marcotegui, Santiago Velasco-Forero
- **Comment**: ISPRS 2020
- **Journal**: None
- **Summary**: Point cloud datasets for perception tasks in the context of autonomous driving often rely on high resolution 64-layer Light Detection and Ranging (LIDAR) scanners. They are expensive to deploy on real-world autonomous driving sensor architectures which usually employ 16/32 layer LIDARs. We evaluate the effect of subsampling image based representations of dense point clouds on the accuracy of the road segmentation task. In our experiments the low resolution 16/32 layer LIDAR point clouds are simulated by subsampling the original 64 layer data, for subsequent transformation in to a feature map in the Bird-Eye-View (BEV) and SphericalView (SV) representations of the point cloud. We introduce the usage of the local normal vector with the LIDAR's spherical coordinates as an input channel to existing LoDNN architectures. We demonstrate that this local normal feature in conjunction with classical features not only improves performance for binary road segmentation on full resolution point clouds, but it also reduces the negative impact on the accuracy when subsampling dense point clouds as compared to the usage of classical features alone. We assess our method with several experiments on two datasets: KITTI Road-segmentation benchmark and the recently released Semantic KITTI dataset.



### Image Restoration from Parametric Transformations using Generative Models
- **Arxiv ID**: http://arxiv.org/abs/2005.14036v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.14036v2)
- **Published**: 2020-05-27 01:14:40+00:00
- **Updated**: 2020-06-16 12:09:41+00:00
- **Authors**: Kalliopi Basioti, George V. Moustakides
- **Comment**: None
- **Journal**: None
- **Summary**: When images are statistically described by a generative model we can use this information to develop optimum techniques for various image restoration problems as inpainting, super-resolution, image coloring, generative model inversion, etc. With the help of the generative model it is possible to formulate, in a natural way, these restoration problems as Statistical estimation problems. Our approach, by combining maximum a-posteriori probability with maximum likelihood estimation, is capable of restoring images that are distorted by transformations even when the latter contain unknown parameters. The resulting optimization is completely defined with no parameters requiring tuning. This must be compared with the current state of the art which requires exact knowledge of the transformations and contains regularizer terms with weights that must be properly defined. Finally, we must mention that we extend our method to accommodate mixtures of multiple images where each image is described by its own generative model and we are able of successfully separating each participating image from a single mixture.



### Evolutionary NAS with Gene Expression Programming of Cellular Encoding
- **Arxiv ID**: http://arxiv.org/abs/2005.13110v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2005.13110v2)
- **Published**: 2020-05-27 01:19:32+00:00
- **Updated**: 2020-12-03 15:41:20+00:00
- **Authors**: Clifford Broni-Bediako, Yuki Murata, Luiz Henrique Mormille, Masayasu Atsumi
- **Comment**: Accepted at IEEE SSCI 2020 (7 pages, 3 figures)
- **Journal**: None
- **Summary**: The renaissance of neural architecture search (NAS) has seen classical methods such as genetic algorithms (GA) and genetic programming (GP) being exploited for convolutional neural network (CNN) architectures. While recent work have achieved promising performance on visual perception tasks, the direct encoding scheme of both GA and GP has functional complexity deficiency and does not scale well on large architectures like CNN. To address this, we present a new generative encoding scheme -- $symbolic\ linear\ generative\ encoding$ (SLGE) -- simple, yet powerful scheme which embeds local graph transformations in chromosomes of linear fixed-length string to develop CNN architectures of variant shapes and sizes via evolutionary process of gene expression programming. In experiments, the effectiveness of SLGE is shown in discovering architectures that improve the performance of the state-of-the-art handcrafted CNN architectures on CIFAR-10 and CIFAR-100 image classification tasks; and achieves a competitive classification error rate with the existing NAS methods using less GPU resources.



### Object-QA: Towards High Reliable Object Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/2005.13116v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.13116v1)
- **Published**: 2020-05-27 01:46:58+00:00
- **Updated**: 2020-05-27 01:46:58+00:00
- **Authors**: Jing Lu, Baorui Zou, Zhanzhan Cheng, Shiliang Pu, Shuigeng Zhou, Yi Niu, Fei Wu
- **Comment**: None
- **Journal**: None
- **Summary**: In object recognition applications, object images usually appear with different quality levels. Practically, it is very important to indicate object image qualities for better application performance, e.g. filtering out low-quality object image frames to maintain robust video object recognition results and speed up inference. However, no previous works are explicitly proposed for addressing the problem. In this paper, we define the problem of object quality assessment for the first time and propose an effective approach named Object-QA to assess high-reliable quality scores for object images. Concretely, Object-QA first employs a well-designed relative quality assessing module that learns the intra-class-level quality scores by referring to the difference between object images and their estimated templates. Then an absolute quality assessing module is designed to generate the final quality scores by aligning the quality score distributions in inter-class. Besides, Object-QA can be implemented with only object-level annotations, and is also easily deployed to a variety of object recognition tasks. To our best knowledge this is the first work to put forward the definition of this problem and conduct quantitative evaluations. Validations on 5 different datasets show that Object-QA can not only assess high-reliable quality scores according with human cognition, but also improve application performance.



### SPIN: Structure-Preserving Inner Offset Network for Scene Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/2005.13117v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.13117v4)
- **Published**: 2020-05-27 01:47:07+00:00
- **Updated**: 2021-10-25 09:33:59+00:00
- **Authors**: Chengwei Zhang, Yunlu Xu, Zhanzhan Cheng, Shiliang Pu, Yi Niu, Fei Wu, Futai Zou
- **Comment**: Accepted to AAAI21. Code is available at
  https://davar-lab.github.io/publication.html or
  https://github.com/hikopensource/DAVAR-Lab-OCR
- **Journal**: None
- **Summary**: Arbitrary text appearance poses a great challenge in scene text recognition tasks. Existing works mostly handle with the problem in consideration of the shape distortion, including perspective distortions, line curvature or other style variations. Therefore, methods based on spatial transformers are extensively studied. However, chromatic difficulties in complex scenes have not been paid much attention on. In this work, we introduce a new learnable geometric-unrelated module, the Structure-Preserving Inner Offset Network (SPIN), which allows the color manipulation of source data within the network. This differentiable module can be inserted before any recognition architecture to ease the downstream tasks, giving neural networks the ability to actively transform input intensity rather than the existing spatial rectification. It can also serve as a complementary module to known spatial transformations and work in both independent and collaborative ways with them. Extensive experiments show that the use of SPIN results in a significant improvement on multiple text recognition benchmarks compared to the state-of-the-arts.



### TRIE: End-to-End Text Reading and Information Extraction for Document Understanding
- **Arxiv ID**: http://arxiv.org/abs/2005.13118v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.13118v3)
- **Published**: 2020-05-27 01:47:26+00:00
- **Updated**: 2021-10-25 09:33:53+00:00
- **Authors**: Peng Zhang, Yunlu Xu, Zhanzhan Cheng, Shiliang Pu, Jing Lu, Liang Qiao, Yi Niu, Fei Wu
- **Comment**: Accepted to ACM MM2020. Code is available at
  https://davar-lab.github.io/publication.html or
  https://github.com/hikopensource/DAVAR-Lab-OCR
- **Journal**: None
- **Summary**: Since real-world ubiquitous documents (e.g., invoices, tickets, resumes and leaflets) contain rich information, automatic document image understanding has become a hot topic. Most existing works decouple the problem into two separate tasks, (1) text reading for detecting and recognizing texts in images and (2) information extraction for analyzing and extracting key elements from previously extracted plain text. However, they mainly focus on improving information extraction task, while neglecting the fact that text reading and information extraction are mutually correlated. In this paper, we propose a unified end-to-end text reading and information extraction network, where the two tasks can reinforce each other. Specifically, the multimodal visual and textual features of text reading are fused for information extraction and in turn, the semantics in information extraction contribute to the optimization of text reading. On three real-world datasets with diverse document images (from fixed layout to variable layout, from structured text to semi-structured text), our proposed method significantly outperforms the state-of-the-art methods in both efficiency and accuracy.



### Towards Mesh Saliency Detection in 6 Degrees of Freedom
- **Arxiv ID**: http://arxiv.org/abs/2005.13127v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.13127v2)
- **Published**: 2020-05-27 02:04:33+00:00
- **Updated**: 2020-06-23 01:07:33+00:00
- **Authors**: Xiaoying Ding, Zhenzhong Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Traditional 3D mesh saliency detection algorithms and corresponding databases were proposed under several constraints such as providing limited viewing directions and not taking the subject's movement into consideration. In this work, a novel 6DoF mesh saliency database is developed which provides both the subject's 6DoF data and eye-movement data. Different from traditional databases, subjects in the experiment are allowed to move freely to observe 3D meshes in a virtual reality environment. Based on the database, we first analyze the inter-observer variation and the influence of viewing direction towards subject's visual attention, then we provide further investigations about the subject's visual attention bias during observation. Furthermore, we propose a 6DoF mesh saliency detection algorithm based on the uniqueness measure and the bias preference. To evaluate the proposed approach, we also design an evaluation metric accordingly which takes the 6DoF information into consideration, and extend some state-of-the-art 3D saliency detection methods to make comparisons. The experimental results demonstrate the superior performance of our approach for 6DoF mesh saliency detection, in addition to providing benchmarks for the presented 6DoF mesh saliency database. The database and the corresponding algorithms will be made publicly available for research purposes.



### Efficient Pig Counting in Crowds with Keypoints Tracking and Spatial-aware Temporal Response Filtering
- **Arxiv ID**: http://arxiv.org/abs/2005.13131v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.13131v1)
- **Published**: 2020-05-27 02:17:54+00:00
- **Updated**: 2020-05-27 02:17:54+00:00
- **Authors**: Guang Chen, Shiwen Shen, Longyin Wen, Si Luo, Liefeng Bo
- **Comment**: IEEE International Conference on Robotics and Automation (ICRA) 2020
- **Journal**: None
- **Summary**: Pig counting is a crucial task for large-scale pig farming, which is usually completed by human visually. But this process is very time-consuming and error-prone. Few studies in literature developed automated pig counting method. Existing methods only focused on pig counting using single image, and its accuracy is challenged by several factors, including pig movements, occlusion and overlapping. Especially, the field of view of a single image is very limited, and could not meet the requirements of pig counting for large pig grouping houses. To that end, we presented a real-time automated pig counting system in crowds using only one monocular fisheye camera with an inspection robot. Our system showed that it produces accurate results surpassing human. Our pipeline began with a novel bottom-up pig detection algorithm to avoid false negatives due to overlapping, occlusion and deformation of pigs. A deep convolution neural network (CNN) is designed to detect keypoints of pig body part and associate the keypoints to identify individual pigs. After that, an efficient on-line tracking method is used to associate pigs across video frames. Finally, a novel spatial-aware temporal response filtering (STRF) method is proposed to predict the counts of pigs, which is effective to suppress false positives caused by pig or camera movements or tracking failures. The whole pipeline has been deployed in an edge computing device, and demonstrated the effectiveness.



### Unifying Few- and Zero-Shot Egocentric Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2006.11393v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.11393v1)
- **Published**: 2020-05-27 02:23:38+00:00
- **Updated**: 2020-05-27 02:23:38+00:00
- **Authors**: Tyler R. Scott, Michael Shvartsman, Karl Ridgeway
- **Comment**: Accepted for presentation at the EPIC@CVPR2020 workshop
- **Journal**: None
- **Summary**: Although there has been significant research in egocentric action recognition, most methods and tasks, including EPIC-KITCHENS, suppose a fixed set of action classes. Fixed-set classification is useful for benchmarking methods, but is often unrealistic in practical settings due to the compositionality of actions, resulting in a functionally infinite-cardinality label set. In this work, we explore generalization with an open set of classes by unifying two popular approaches: few- and zero-shot generalization (the latter which we reframe as cross-modal few-shot generalization). We propose a new set of splits derived from the EPIC-KITCHENS dataset that allow evaluation of open-set classification, and use these splits to show that adding a metric-learning loss to the conventional direct-alignment baseline can improve zero-shot classification by as much as 10%, while not sacrificing few-shot performance.



### Robust Trajectory Forecasting for Multiple Intelligent Agents in Dynamic Scene
- **Arxiv ID**: http://arxiv.org/abs/2005.13133v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2005.13133v1)
- **Published**: 2020-05-27 02:32:55+00:00
- **Updated**: 2020-05-27 02:32:55+00:00
- **Authors**: Yanliang Zhu, Dongchun Ren, Mingyu Fan, Deheng Qian, Xin Li, Huaxia Xia
- **Comment**: None
- **Journal**: None
- **Summary**: Trajectory forecasting, or trajectory prediction, of multiple interacting agents in dynamic scenes, is an important problem for many applications, such as robotic systems and autonomous driving. The problem is a great challenge because of the complex interactions among the agents and their interactions with the surrounding scenes. In this paper, we present a novel method for the robust trajectory forecasting of multiple intelligent agents in dynamic scenes. The proposed method consists of three major interrelated components: an interaction net for global spatiotemporal interactive feature extraction, an environment net for decoding dynamic scenes (i.e., the surrounding road topology of an agent), and a prediction net that combines the spatiotemporal feature, the scene feature, the past trajectories of agents and some random noise for the robust trajectory prediction of agents. Experiments on pedestrian-walking and vehicle-pedestrian heterogeneous datasets demonstrate that the proposed method outperforms the state-of-the-art prediction methods in terms of prediction accuracy.



### Permutation Matters: Anisotropic Convolutional Layer for Learning on Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2005.13135v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.13135v2)
- **Published**: 2020-05-27 02:42:29+00:00
- **Updated**: 2020-06-05 16:32:43+00:00
- **Authors**: Zhongpai Gao, Guangtao Zhai, Junchi Yan, Xiaokang Yang
- **Comment**: None
- **Journal**: None
- **Summary**: It has witnessed a growing demand for efficient representation learning on point clouds in many 3D computer vision applications. Behind the success story of convolutional neural networks (CNNs) is that the data (e.g., images) are Euclidean structured. However, point clouds are irregular and unordered. Various point neural networks have been developed with isotropic filters or using weighting matrices to overcome the structure inconsistency on point clouds. However, isotropic filters or weighting matrices limit the representation power. In this paper, we propose a permutable anisotropic convolutional operation (PAI-Conv) that calculates soft-permutation matrices for each point using dot-product attention according to a set of evenly distributed kernel points on a sphere's surface and performs shared anisotropic filters. In fact, dot product with kernel points is by analogy with the dot-product with keys in Transformer as widely used in natural language processing (NLP). From this perspective, PAI-Conv can be regarded as the transformer for point clouds, which is physically meaningful and is robust to cooperate with the efficient random point sampling method. Comprehensive experiments on point clouds demonstrate that PAI-Conv produces competitive results in classification and semantic segmentation tasks compared to state-of-the-art methods.



### SSM-Net for Plants Disease Identification in Low Data Regime
- **Arxiv ID**: http://arxiv.org/abs/2005.13140v4
- **DOI**: 10.1109/AI4G50087.2020.9311073
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.13140v4)
- **Published**: 2020-05-27 03:43:38+00:00
- **Updated**: 2020-09-07 20:16:31+00:00
- **Authors**: Shruti Jadon
- **Comment**: 5 pages, 7 Figures
- **Journal**: Poster @CVPR workshop, Proceedings at IEEE / ITU International
  Conference on Artificial Intelligence for Good 2020
- **Summary**: Plant disease detection is an essential factor in increasing agricultural production. Due to the difficulty of disease detection, farmers spray various pesticides on their crops to protect them, causing great harm to crop growth and food standards. Deep learning can offer critical aid in detecting such diseases. However, it is highly inconvenient to collect a large volume of data on all forms of the diseases afflicting a specific plant species. In this paper, we propose a new metrics-based few-shot learning SSM net architecture, which consists of stacked siamese and matching network components to address the problem of disease detection in low data regimes. We demonstrated our experiments on two datasets: mini-leaves diseases and sugarcane diseases dataset. We have showcased that the SSM-Net approach can achieve better decision boundaries with an accuracy of 92.7% on the mini-leaves dataset and 94.3% on the sugarcane dataset. The accuracy increased by ~10% and ~5% respectively, compared to the widely used VGG16 transfer learning approach. Furthermore, we attained F1 score of 0.90 using SSM Net on the sugarcane dataset and 0.91 on the mini-leaves dataset. Our code implementation is available on Github: https://github.com/shruti-jadon/PlantsDiseaseDetection.



### On Mutual Information in Contrastive Learning for Visual Representations
- **Arxiv ID**: http://arxiv.org/abs/2005.13149v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.13149v2)
- **Published**: 2020-05-27 04:21:53+00:00
- **Updated**: 2020-06-05 16:39:20+00:00
- **Authors**: Mike Wu, Chengxu Zhuang, Milan Mosse, Daniel Yamins, Noah Goodman
- **Comment**: 8 pages content; 15 pages supplement with proofs
- **Journal**: None
- **Summary**: In recent years, several unsupervised, "contrastive" learning algorithms in vision have been shown to learn representations that perform remarkably well on transfer tasks. We show that this family of algorithms maximizes a lower bound on the mutual information between two or more "views" of an image where typical views come from a composition of image augmentations. Our bound generalizes the InfoNCE objective to support negative sampling from a restricted region of "difficult" contrasts. We find that the choice of negative samples and views are critical to the success of these algorithms. Reformulating previous learning objectives in terms of mutual information also simplifies and stabilizes them. In practice, our new objectives yield representations that outperform those learned with previous approaches for transfer to classification, bounding box detection, instance segmentation, and keypoint detection. % experiments show that choosing more difficult negative samples results in a stronger representation, outperforming those learned with IR, LA, and CMC in classification, bounding box detection, instance segmentation, and keypoint detection. The mutual information framework provides a unifying comparison of approaches to contrastive learning and uncovers the choices that impact representation learning.



### False Positive Removal for 3D Vehicle Detection with Penetrated Point Classifier
- **Arxiv ID**: http://arxiv.org/abs/2005.13153v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.13153v2)
- **Published**: 2020-05-27 04:36:53+00:00
- **Updated**: 2020-05-28 02:11:16+00:00
- **Authors**: Sungmin Woo, Sangwon Hwang, Woojin Kim, Junhyeop Lee, Dogyoon Lee, Sangyoun Lee
- **Comment**: Accepted by ICIP 2020
- **Journal**: None
- **Summary**: Recently, researchers have been leveraging LiDAR point cloud for higher accuracy in 3D vehicle detection. Most state-of-the-art methods are deep learning based, but are easily affected by the number of points generated on the object. This vulnerability leads to numerous false positive boxes at high recall positions, where objects are occasionally predicted with few points. To address the issue, we introduce Penetrated Point Classifier (PPC) based on the underlying property of LiDAR that points cannot be generated behind vehicles. It determines whether a point exists behind the vehicle of the predicted box, and if does, the box is distinguished as false positive. Our straightforward yet unprecedented approach is evaluated on KITTI dataset and achieved performance improvement of PointRCNN, one of the state-of-the-art methods. The experiment results show that precision at the highest recall position is dramatically increased by 15.46 percentage points and 14.63 percentage points on the moderate and hard difficulty of car class, respectively.



### Generative Adversarial Networks (GANs): An Overview of Theoretical Model, Evaluation Metrics, and Recent Developments
- **Arxiv ID**: http://arxiv.org/abs/2005.13178v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.13178v1)
- **Published**: 2020-05-27 05:56:53+00:00
- **Updated**: 2020-05-27 05:56:53+00:00
- **Authors**: Pegah Salehi, Abdolah Chalechale, Maryam Taghizadeh
- **Comment**: Submitted to a journal in the computer vision field
- **Journal**: None
- **Summary**: One of the most significant challenges in statistical signal processing and machine learning is how to obtain a generative model that can produce samples of large-scale data distribution, such as images and speeches. Generative Adversarial Network (GAN) is an effective method to address this problem. The GANs provide an appropriate way to learn deep representations without widespread use of labeled training data. This approach has attracted the attention of many researchers in computer vision since it can generate a large amount of data without precise modeling of the probability density function (PDF). In GANs, the generative model is estimated via a competitive process where the generator and discriminator networks are trained simultaneously. The generator learns to generate plausible data, and the discriminator learns to distinguish fake data created by the generator from real data samples. Given the rapid growth of GANs over the last few years and their application in various fields, it is necessary to investigate these networks accurately. In this paper, after introducing the main concepts and the theory of GAN, two new deep generative models are compared, the evaluation metrics utilized in the literature and challenges of GANs are also explained. Moreover, the most remarkable GAN architectures are categorized and discussed. Finally, the essential applications in computer vision are examined.



### Learning to segment from misaligned and partial labels
- **Arxiv ID**: http://arxiv.org/abs/2005.13180v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY, I.4.6; J.2
- **Links**: [PDF](http://arxiv.org/pdf/2005.13180v1)
- **Published**: 2020-05-27 06:02:58+00:00
- **Updated**: 2020-05-27 06:02:58+00:00
- **Authors**: Simone Fobi, Terence Conlon, Jayant Taneja, Vijay Modi
- **Comment**: This is the extended version of a paper to be published in ACM
  COMPASS 2020
- **Journal**: None
- **Summary**: To extract information at scale, researchers increasingly apply semantic segmentation techniques to remotely-sensed imagery. While fully-supervised learning enables accurate pixel-wise segmentation, compiling the exhaustive datasets required is often prohibitively expensive. As a result, many non-urban settings lack the ground-truth needed for accurate segmentation. Existing open source infrastructure data for these regions can be inexact and non-exhaustive. Open source infrastructure annotations like OpenStreetMaps (OSM) are representative of this issue: while OSM labels provide global insights to road and building footprints, noisy and partial annotations limit the performance of segmentation algorithms that learn from them. In this paper, we present a novel and generalizable two-stage framework that enables improved pixel-wise image segmentation given misaligned and missing annotations. First, we introduce the Alignment Correction Network to rectify incorrectly registered open source labels. Next, we demonstrate a segmentation model -- the Pointer Segmentation Network -- that uses corrected labels to predict infrastructure footprints despite missing annotations. We test sequential performance on the AIRS dataset, achieving a mean intersection-over-union score of 0.79; more importantly, model performance remains stable as we decrease the fraction of annotations present. We demonstrate the transferability of our method to lower quality data, by applying the Alignment Correction Network to OSM labels to correct building footprints; we also demonstrate the accuracy of the Pointer Segmentation Network in predicting cropland boundaries in California from medium resolution data. Overall, our methodology is robust for multiple applications with varied amounts of training data present, thus offering a method to extract reliable information from noisy, partial data.



### TIME: Text and Image Mutual-Translation Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2005.13192v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.13192v2)
- **Published**: 2020-05-27 06:40:12+00:00
- **Updated**: 2020-12-22 20:46:36+00:00
- **Authors**: Bingchen Liu, Kunpeng Song, Yizhe Zhu, Gerard de Melo, Ahmed Elgammal
- **Comment**: AAAI-2021
- **Journal**: None
- **Summary**: Focusing on text-to-image (T2I) generation, we propose Text and Image Mutual-Translation Adversarial Networks (TIME), a lightweight but effective model that jointly learns a T2I generator G and an image captioning discriminator D under the Generative Adversarial Network framework. While previous methods tackle the T2I problem as a uni-directional task and use pre-trained language models to enforce the image--text consistency, TIME requires neither extra modules nor pre-training. We show that the performance of G can be boosted substantially by training it jointly with D as a language model. Specifically, we adopt Transformers to model the cross-modal connections between the image features and word embeddings, and design an annealing conditional hinge loss that dynamically balances the adversarial learning. In our experiments, TIME achieves state-of-the-art (SOTA) performance on the CUB and MS-COCO dataset (Inception Score of 4.91 and Fr\'echet Inception Distance of 14.3 on CUB), and shows promising performance on MS-COCO on image captioning and downstream vision-language tasks.



### Extrapolative-Interpolative Cycle-Consistency Learning for Video Frame Extrapolation
- **Arxiv ID**: http://arxiv.org/abs/2005.13194v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.13194v1)
- **Published**: 2020-05-27 06:42:21+00:00
- **Updated**: 2020-05-27 06:42:21+00:00
- **Authors**: Sangjin Lee, Hyeongmin Lee, Taeoh Kim, Sangyoun Lee
- **Comment**: This paper has been accepted to 2020 IEEE International Conference on
  Image Processing (ICIP 2020)
- **Journal**: None
- **Summary**: Video frame extrapolation is a task to predict future frames when the past frames are given. Unlike previous studies that usually have been focused on the design of modules or construction of networks, we propose a novel Extrapolative-Interpolative Cycle (EIC) loss using pre-trained frame interpolation module to improve extrapolation performance. Cycle-consistency loss has been used for stable prediction between two function spaces in many visual tasks. We formulate this cycle-consistency using two mapping functions; frame extrapolation and interpolation. Since it is easier to predict intermediate frames than to predict future frames in terms of the object occlusion and motion uncertainty, interpolation module can give guidance signal effectively for training the extrapolation function. EIC loss can be applied to any existing extrapolation algorithms and guarantee consistent prediction in the short future as well as long future frames. Experimental results show that simply adding EIC loss to the existing baseline increases extrapolation performance on both UCF101 and KITTI datasets.



### Co-Heterogeneous and Adaptive Segmentation from Multi-Source and Multi-Phase CT Imaging Data: A Study on Pathological Liver and Lesion Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2005.13201v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.13201v4)
- **Published**: 2020-05-27 06:58:39+00:00
- **Updated**: 2021-07-19 18:54:43+00:00
- **Authors**: Ashwin Raju, Chi-Tung Cheng, Yunakai Huo, Jinzheng Cai, Junzhou Huang, Jing Xiao, Le Lu, ChienHuang Liao, Adam P Harrison
- **Comment**: 23 pages, 8 figures
- **Journal**: None
- **Summary**: In medical imaging, organ/pathology segmentation models trained on current publicly available and fully-annotated datasets usually do not well-represent the heterogeneous modalities, phases, pathologies, and clinical scenarios encountered in real environments. On the other hand, there are tremendous amounts of unlabelled patient imaging scans stored by many modern clinical centers. In this work, we present a novel segmentation strategy, co-heterogenous and adaptive segmentation (CHASe), which only requires a small labeled cohort of single phase imaging data to adapt to any unlabeled cohort of heterogenous multi-phase data with possibly new clinical scenarios and pathologies. To do this, we propose a versatile framework that fuses appearance based semi-supervision, mask based adversarial domain adaptation, and pseudo-labeling. We also introduce co-heterogeneous training, which is a novel integration of co-training and hetero modality learning. We have evaluated CHASe using a clinically comprehensive and challenging dataset of multi-phase computed tomography (CT) imaging studies (1147 patients and 4577 3D volumes). Compared to previous state-of-the-art baselines, CHASe can further improve pathological liver mask Dice-Sorensen coefficients by ranges of $4.2\% \sim 9.4\%$, depending on the phase combinations: e.g., from $84.6\%$ to $94.0\%$ on non-contrast CTs.



### Concurrent Segmentation and Object Detection CNNs for Aircraft Detection and Identification in Satellite Images
- **Arxiv ID**: http://arxiv.org/abs/2005.13215v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.13215v1)
- **Published**: 2020-05-27 07:35:55+00:00
- **Updated**: 2020-05-27 07:35:55+00:00
- **Authors**: Damien Grosgeorge, Maxime Arbelot, Alex Goupilleau, Tugdual Ceillier, Renaud Allioux
- **Comment**: None
- **Journal**: IEEE International Geoscience and Remote Sensing Symposium
  (IGARSS), 2020, Waikoloa, Hawaii, United States
- **Summary**: Detecting and identifying objects in satellite images is a very challenging task: objects of interest are often very small and features can be difficult to recognize even using very high resolution imagery. For most applications, this translates into a trade-off between recall and precision. We present here a dedicated method to detect and identify aircraft, combining two very different convolutional neural networks (CNNs): a segmentation model, based on a modified U-net architecture, and a detection model, based on the RetinaNet architecture. The results we present show that this combination outperforms significantly each unitary model, reducing drastically the false negative rate.



### Learning Tversky Similarity
- **Arxiv ID**: http://arxiv.org/abs/2006.11372v1
- **DOI**: 10.1007/978-3-030-50143-3_21
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.11372v1)
- **Published**: 2020-05-27 07:58:35+00:00
- **Updated**: 2020-05-27 07:58:35+00:00
- **Authors**: Javad Rahnama, Eyke Hüllermeier
- **Comment**: None
- **Journal**: Proc. IPMU, International Conference on Information Processing and
  Management of Uncertainty in Knowledge-Based Systems, Springer, CCIS 1238,
  2020
- **Summary**: In this paper, we advocate Tversky's ratio model as an appropriate basis for computational approaches to semantic similarity, that is, the comparison of objects such as images in a semantically meaningful way. We consider the problem of learning Tversky similarity measures from suitable training data indicating whether two objects tend to be similar or dissimilar. Experimentally, we evaluate our approach to similarity learning on two image datasets, showing that is performs very well compared to existing methods.



### Arbitrary Style Transfer via Multi-Adaptation Network
- **Arxiv ID**: http://arxiv.org/abs/2005.13219v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2005.13219v2)
- **Published**: 2020-05-27 08:00:22+00:00
- **Updated**: 2020-08-16 05:28:46+00:00
- **Authors**: Yingying Deng, Fan Tang, Weiming Dong, Wen Sun, Feiyue Huang, Changsheng Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Arbitrary style transfer is a significant topic with research value and application prospect. A desired style transfer, given a content image and referenced style painting, would render the content image with the color tone and vivid stroke patterns of the style painting while synchronously maintaining the detailed content structure information. Style transfer approaches would initially learn content and style representations of the content and style references and then generate the stylized images guided by these representations. In this paper, we propose the multi-adaptation network which involves two self-adaptation (SA) modules and one co-adaptation (CA) module: the SA modules adaptively disentangle the content and style representations, i.e., content SA module uses position-wise self-attention to enhance content representation and style SA module uses channel-wise self-attention to enhance style representation; the CA module rearranges the distribution of style representation based on content representation distribution by calculating the local similarity between the disentangled content and style features in a non-local fashion. Moreover, a new disentanglement loss function enables our network to extract main style patterns and exact content structures to adapt to various input images, respectively. Various qualitative and quantitative experiments demonstrate that the proposed multi-adaptation network leads to better results than the state-of-the-art style transfer methods.



### Zoom in to the details of human-centric videos
- **Arxiv ID**: http://arxiv.org/abs/2005.13222v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.13222v1)
- **Published**: 2020-05-27 08:04:47+00:00
- **Updated**: 2020-05-27 08:04:47+00:00
- **Authors**: Guanghan Li, Yaping Zhao, Mengqi Ji, Xiaoyun Yuan, Lu Fang
- **Comment**: 5 pages, 6 figures, accepted for presentation at IEEE ICIP 2020
- **Journal**: None
- **Summary**: Presenting high-resolution (HR) human appearance is always critical for the human-centric videos. However, current imagery equipment can hardly capture HR details all the time. Existing super-resolution algorithms barely mitigate the problem by only considering universal and low-level priors of im-age patches. In contrast, our algorithm is under bias towards the human body super-resolution by taking advantage of high-level prior defined by HR human appearance. Firstly, a motion analysis module extracts inherent motion pattern from the HR reference video to refine the pose estimation of the low-resolution (LR) sequence. Furthermore, a human body reconstruction module maps the HR texture in the reference frames onto a 3D mesh model. Consequently, the input LR videos get super-resolved HR human sequences are generated conditioned on the original LR videos as well as few HR reference frames. Experiments on an existing dataset and real-world data captured by hybrid cameras show that our approach generates superior visual quality of human body compared with the traditional method.



### Poly-YOLO: higher speed, more precise detection and instance segmentation for YOLOv3
- **Arxiv ID**: http://arxiv.org/abs/2005.13243v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, 68T45, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2005.13243v2)
- **Published**: 2020-05-27 08:53:35+00:00
- **Updated**: 2020-05-29 11:58:33+00:00
- **Authors**: Petr Hurtik, Vojtech Molek, Jan Hula, Marek Vajgl, Pavel Vlasanek, Tomas Nejezchleba
- **Comment**: 18 pages, 15 figures, submitted to IEEE Transactions on Pattern
  Analysis and Machine Intelligence (under review), Source code is available at
  https://gitlab.com/irafm-ai/poly-yolo
- **Journal**: None
- **Summary**: We present a new version of YOLO with better performance and extended with instance segmentation called Poly-YOLO. Poly-YOLO builds on the original ideas of YOLOv3 and removes two of its weaknesses: a large amount of rewritten labels and inefficient distribution of anchors. Poly-YOLO reduces the issues by aggregating features from a light SE-Darknet-53 backbone with a hypercolumn technique, using stairstep upsampling, and produces a single scale output with high resolution. In comparison with YOLOv3, Poly-YOLO has only 60% of its trainable parameters but improves mAP by a relative 40%. We also present Poly-YOLO lite with fewer parameters and a lower output resolution. It has the same precision as YOLOv3, but it is three times smaller and twice as fast, thus suitable for embedded devices. Finally, Poly-YOLO performs instance segmentation using bounding polygons. The network is trained to detect size-independent polygons defined on a polar grid. Vertices of each polygon are being predicted with their confidence, and therefore Poly-YOLO produces polygons with a varying number of vertices.



### An Entropy Based Outlier Score and its Application to Novelty Detection for Road Infrastructure Images
- **Arxiv ID**: http://arxiv.org/abs/2005.13288v2
- **DOI**: 10.1109/IV47402.2020.9304733
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.13288v2)
- **Published**: 2020-05-27 11:34:42+00:00
- **Updated**: 2021-05-05 08:40:47+00:00
- **Authors**: Jonas Wurst, Alberto Flores Fernández, Michael Botsch, Wolfgang Utschick
- **Comment**: Copyright 2020 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works
- **Journal**: 2020 IEEE Intelligent Vehicles Symposium (IV)
- **Summary**: A novel unsupervised outlier score, which can be embedded into graph based dimensionality reduction techniques, is presented in this work. The score uses the directed nearest neighbor graphs of those techniques. Hence, the same measure of similarity that is used to project the data into lower dimensions, is also utilized to determine the outlier score. The outlier score is realized through a weighted normalized entropy of the similarities. This score is applied to road infrastructure images. The aim is to identify newly observed infrastructures given a pre-collected base dataset. Detecting unknown scenarios is a key for accelerated validation of autonomous vehicles. The results show the high potential of the proposed technique. To validate the generalization capabilities of the outlier score, it is additionally applied to various real world datasets. The overall average performance in identifying outliers using the proposed methods is higher compared to state-of-the-art methods. In order to generate the infrastructure images, an openDRIVE parsing and plotting tool for Matlab is developed as part of this work. This tool and the implementation of the entropy based outlier score in combination with Uniform Manifold Approximation and Projection are made publicly available.



### Deep Sensory Substitution: Noninvasively Enabling Biological Neural Networks to Receive Input from Artificial Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2005.13291v3
- **DOI**: None
- **Categories**: **eess.AS**, cs.CV, cs.LG, cs.SD, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.13291v3)
- **Published**: 2020-05-27 11:41:48+00:00
- **Updated**: 2021-08-25 23:20:52+00:00
- **Authors**: Andrew Port, Chelhwon Kim, Mitesh Patel
- **Comment**: 9 pages, 3 figures
- **Journal**: None
- **Summary**: As is expressed in the adage "a picture is worth a thousand words", when using spoken language to communicate visual information, brevity can be a challenge. This work describes a novel technique for leveraging machine-learned feature embeddings to sonify visual (and other types of) information into a perceptual audio domain, allowing users to perceive this information using only their aural faculty. The system uses a pretrained image embedding network to extract visual features and embed them in a compact subset of Euclidean space -- this converts the images into feature vectors whose $L^2$ distances can be used as a meaningful measure of similarity. A generative adversarial network (GAN) is then used to find a distance preserving map from this metric space of feature vectors into the metric space defined by a target audio dataset equipped with either the Euclidean metric or a mel-frequency cepstrum-based psychoacoustic distance metric. We demonstrate this technique by sonifying images of faces into human speech-like audio. For both target audio metrics, the GAN successfully found a metric preserving mapping, and in human subject tests, users were able to accurately classify audio sonifications of faces.



### Accelerating Neural Network Inference by Overflow Aware Quantization
- **Arxiv ID**: http://arxiv.org/abs/2005.13297v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.13297v1)
- **Published**: 2020-05-27 11:56:22+00:00
- **Updated**: 2020-05-27 11:56:22+00:00
- **Authors**: Hongwei Xie, Shuo Zhang, Huanghao Ding, Yafei Song, Baitao Shao, Conggang Hu, Ling Cai, Mingyang Li
- **Comment**: None
- **Journal**: None
- **Summary**: The inherent heavy computation of deep neural networks prevents their widespread applications. A widely used method for accelerating model inference is quantization, by replacing the input operands of a network using fixed-point values. Then the majority of computation costs focus on the integer matrix multiplication accumulation. In fact, high-bit accumulator leads to partially wasted computation and low-bit one typically suffers from numerical overflow. To address this problem, we propose an overflow aware quantization method by designing trainable adaptive fixed-point representation, to optimize the number of bits for each input tensor while prohibiting numeric overflow during the computation. With the proposed method, we are able to fully utilize the computing power to minimize the quantization loss and obtain optimized inference performance. To verify the effectiveness of our method, we conduct image classification, object detection, and semantic segmentation tasks on ImageNet, Pascal VOC, and COCO datasets, respectively. Experimental results demonstrate that the proposed method can achieve comparable performance with state-of-the-art quantization methods while accelerating the inference process by about 2 times.



### An Iteratively Optimized Patch Label Inference Network for Automatic Pavement Distress Detection
- **Arxiv ID**: http://arxiv.org/abs/2005.13298v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.13298v3)
- **Published**: 2020-05-27 11:56:38+00:00
- **Updated**: 2022-09-08 07:09:12+00:00
- **Authors**: Wenhao Tang, Sheng Huang, Qiming Zhao, Ren Li, Luwen Huangfu
- **Comment**: Published on IEEE Trans on ITS
- **Journal**: None
- **Summary**: We present a novel deep learning framework named the Iteratively Optimized Patch Label Inference Network (IOPLIN) for automatically detecting various pavement distresses that are not solely limited to specific ones, such as cracks and potholes. IOPLIN can be iteratively trained with only the image label via the Expectation-Maximization Inspired Patch Label Distillation (EMIPLD) strategy, and accomplish this task well by inferring the labels of patches from the pavement images. IOPLIN enjoys many desirable properties over the state-of-the-art single branch CNN models such as GoogLeNet and EfficientNet. It is able to handle images in different resolutions, and sufficiently utilize image information particularly for the high-resolution ones, since IOPLIN extracts the visual features from unrevised image patches instead of the resized entire image. Moreover, it can roughly localize the pavement distress without using any prior localization information in the training phase. In order to better evaluate the effectiveness of our method in practice, we construct a large-scale Bituminous Pavement Disease Detection dataset named CQU-BPDD consisting of 60,059 high-resolution pavement images, which are acquired from different areas at different times. Extensive results on this dataset demonstrate the superiority of IOPLIN over the state-of-the-art image classification approaches in automatic pavement distress detection. The source codes of IOPLIN are released on \url{https://github.com/DearCaat/ioplin}, and the CQU-BPDD dataset is able to be accessed on \url{https://dearcaat.github.io/CQU-BPDD/}.



### AutoSweep: Recovering 3D Editable Objectsfrom a Single Photograph
- **Arxiv ID**: http://arxiv.org/abs/2005.13312v2
- **DOI**: 10.1109/TVCG.2018.2871190
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.13312v2)
- **Published**: 2020-05-27 12:16:24+00:00
- **Updated**: 2020-05-28 01:08:27+00:00
- **Authors**: Xin Chen, Yuwei Li, Xi Luo, Tianjia Shao, Jingyi Yu, Kun Zhou, Youyi Zheng
- **Comment**: 10 pages, 12 figures
- **Journal**: IEEE Transactions on Visualization and Computer Graphics, vol. 26,
  no. 3, pp. 1466-1475, 1 March 2020
- **Summary**: This paper presents a fully automatic framework for extracting editable 3D objects directly from a single photograph. Unlike previous methods which recover either depth maps, point clouds, or mesh surfaces, we aim to recover 3D objects with semantic parts and can be directly edited. We base our work on the assumption that most human-made objects are constituted by parts and these parts can be well represented by generalized primitives. Our work makes an attempt towards recovering two types of primitive-shaped objects, namely, generalized cuboids and generalized cylinders. To this end, we build a novel instance-aware segmentation network for accurate part separation. Our GeoNet outputs a set of smooth part-level masks labeled as profiles and bodies. Then in a key stage, we simultaneously identify profile-body relations and recover 3D parts by sweeping the recognized profile along their body contour and jointly optimize the geometry to align with the recovered masks. Qualitative and quantitative experiments show that our algorithm can recover high quality 3D models and outperforms existing methods in both instance segmentation and 3D reconstruction. The dataset and code of AutoSweep are available at https://chenxin.tech/AutoSweep.html.



### Joint Learning of Vessel Segmentation and Artery/Vein Classification with Post-processing
- **Arxiv ID**: http://arxiv.org/abs/2005.13337v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.13337v1)
- **Published**: 2020-05-27 13:06:16+00:00
- **Updated**: 2020-05-27 13:06:16+00:00
- **Authors**: Liangzhi Li, Manisha Verma, Yuta Nakashima, Ryo Kawasaki, Hajime Nagahara
- **Comment**: Accepted in Medical Imaging with Deep Learning (MIDL) 2020
- **Journal**: None
- **Summary**: Retinal imaging serves as a valuable tool for diagnosis of various diseases. However, reading retinal images is a difficult and time-consuming task even for experienced specialists. The fundamental step towards automated retinal image analysis is vessel segmentation and artery/vein classification, which provide various information on potential disorders. To improve the performance of the existing automated methods for retinal image analysis, we propose a two-step vessel classification. We adopt a UNet-based model, SeqNet, to accurately segment vessels from the background and make prediction on the vessel type. Our model does segmentation and classification sequentially, which alleviates the problem of label distribution bias and facilitates training. To further refine classification results, we post-process them considering the structural information among vessels to propagate highly confident prediction to surrounding vessels. Our experiments show that our method improves AUC to 0.98 for segmentation and the accuracy to 0.92 in classification over DRIVE dataset.



### Tackling the Problem of Large Deformations in Deep Learning Based Medical Image Registration Using Displacement Embeddings
- **Arxiv ID**: http://arxiv.org/abs/2005.13338v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.13338v1)
- **Published**: 2020-05-27 13:06:24+00:00
- **Updated**: 2020-05-27 13:06:24+00:00
- **Authors**: Lasse Hansen, Mattias P. Heinrich
- **Comment**: None
- **Journal**: None
- **Summary**: Though, deep learning based medical image registration is currently starting to show promising advances, often, it still fells behind conventional frameworks in terms of registration accuracy. This is especially true for applications where large deformations exist, such as registration of interpatient abdominal MRI or inhale-to-exhale CT lung registration. Most current works use U-Net-like architectures to predict dense displacement fields from the input images in different supervised and unsupervised settings. We believe that the U-Net architecture itself to some level limits the ability to predict large deformations (even when using multilevel strategies) and therefore propose a novel approach, where the input images are mapped into a displacement space and final registrations are reconstructed from this embedding. Experiments on inhale-to-exhale CT lung registration demonstrate the ability of our architecture to predict large deformations in a single forward path through our network (leading to errors below 2 mm).



### Data-Driven Continuum Dynamics via Transport-Teleport Duality
- **Arxiv ID**: http://arxiv.org/abs/2005.13358v2
- **DOI**: None
- **Categories**: **physics.comp-ph**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.13358v2)
- **Published**: 2020-05-27 13:39:09+00:00
- **Updated**: 2020-06-30 20:58:22+00:00
- **Authors**: Jong-Hoon Ahn
- **Comment**: 11 pages, 10 figures (Added references, Added figures, Reorganized
  sections)
- **Journal**: None
- **Summary**: In recent years, machine learning methods have been widely used to study physical systems that are challenging to solve with governing equations. Physicists and engineers are framing the data-driven paradigm as an alternative approach to physical sciences. In this paradigm change, the deep learning approach is playing a pivotal role. However, most learning architectures do not inherently incorporate conservation laws in the form of continuity equations, and they require dense data to learn the dynamics of conserved quantities. In this study, we introduce a clever mathematical transform to represent the classical dynamics as a point-wise process of disappearance and reappearance of a quantity, which dramatically reduces model complexity and training data for machine learning of transport phenomena. We demonstrate that just a few observational data and a simple learning model can be enough to learn the dynamics of real-world objects. The approach does not require the explicit use of governing equations and only depends on observation data. Because the continuity equation is a general equation that any conserved quantity should obey, the applicability may range from physical to social and medical sciences or any field where data are conserved quantities.



### NDD20: A large-scale few-shot dolphin dataset for coarse and fine-grained categorisation
- **Arxiv ID**: http://arxiv.org/abs/2005.13359v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.13359v1)
- **Published**: 2020-05-27 13:41:39+00:00
- **Updated**: 2020-05-27 13:41:39+00:00
- **Authors**: Cameron Trotter, Georgia Atkinson, Matt Sharpe, Kirsten Richardson, A. Stephen McGough, Nick Wright, Ben Burville, Per Berggren
- **Comment**: 5 pages, 6 figures, download link, submitted to FGVC7 Workshop @
  CVPR20
- **Journal**: None
- **Summary**: We introduce the Northumberland Dolphin Dataset 2020 (NDD20), a challenging image dataset annotated for both coarse and fine-grained instance segmentation and categorisation. This dataset, the first release of the NDD, was created in response to the rapid expansion of computer vision into conservation research and the production of field-deployable systems suited to extreme environmental conditions -- an area with few open source datasets. NDD20 contains a large collection of above and below water images of two different dolphin species for traditional coarse and fine-grained segmentation. All data contained in NDD20 was obtained via manual collection in the North Sea around the Northumberland coastline, UK. We present experimentation using standard deep learning network architecture trained using NDD20 and report baselines results.



### A Multi-modal Approach to Fine-grained Opinion Mining on Video Reviews
- **Arxiv ID**: http://arxiv.org/abs/2005.13362v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.13362v2)
- **Published**: 2020-05-27 13:46:11+00:00
- **Updated**: 2020-05-28 03:13:49+00:00
- **Authors**: Edison Marrese-Taylor, Cristian Rodriguez-Opazo, Jorge A. Balazs, Stephen Gould, Yutaka Matsuo
- **Comment**: Second Grand Challenge and Workshop on Multimodal Language ACL 2020
- **Journal**: None
- **Summary**: Despite the recent advances in opinion mining for written reviews, few works have tackled the problem on other sources of reviews. In light of this issue, we propose a multi-modal approach for mining fine-grained opinions from video reviews that is able to determine the aspects of the item under review that are being discussed and the sentiment orientation towards them. Our approach works at the sentence level without the need for time annotations and uses features derived from the audio, video and language transcriptions of its contents. We evaluate our approach on two datasets and show that leveraging the video and audio modalities consistently provides increased performance over text-only baselines, providing evidence these extra modalities are key in better understanding video reviews.



### GSTO: Gated Scale-Transfer Operation for Multi-Scale Feature Learning in Pixel Labeling
- **Arxiv ID**: http://arxiv.org/abs/2005.13363v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.13363v2)
- **Published**: 2020-05-27 13:46:58+00:00
- **Updated**: 2020-06-28 13:51:04+00:00
- **Authors**: Zhuoying Wang, Yongtao Wang, Zhi Tang, Yangyan Li, Ying Chen, Haibin Ling, Weisi Lin
- **Comment**: None
- **Journal**: None
- **Summary**: Existing CNN-based methods for pixel labeling heavily depend on multi-scale features to meet the requirements of both semantic comprehension and detail preservation. State-of-the-art pixel labeling neural networks widely exploit conventional scale-transfer operations, i.e., up-sampling and down-sampling to learn multi-scale features. In this work, we find that these operations lead to scale-confused features and suboptimal performance because they are spatial-invariant and directly transit all feature information cross scales without spatial selection. To address this issue, we propose the Gated Scale-Transfer Operation (GSTO) to properly transit spatial-filtered features to another scale. Specifically, GSTO can work either with or without extra supervision. Unsupervised GSTO is learned from the feature itself while the supervised one is guided by the supervised probability matrix. Both forms of GSTO are lightweight and plug-and-play, which can be flexibly integrated into networks or modules for learning better multi-scale features. In particular, by plugging GSTO into HRNet, we get a more powerful backbone (namely GSTO-HRNet) for pixel labeling, and it achieves new state-of-the-art results on the COCO benchmark for human pose estimation and other benchmarks for semantic segmentation including Cityscapes, LIP and Pascal Context, with negligible extra computational cost. Moreover, experiment results demonstrate that GSTO can also significantly boost the performance of multi-scale feature aggregation modules like PPM and ASPP. Code will be made available at https://github.com/VDIGPKU/GSTO.



### Weakly Supervised Vessel Segmentation in X-ray Angiograms by Self-Paced Learning from Noisy Labels with Suggestive Annotation
- **Arxiv ID**: http://arxiv.org/abs/2005.13366v1
- **DOI**: 10.1016/j.neucom.2020.06.122
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.13366v1)
- **Published**: 2020-05-27 13:55:33+00:00
- **Updated**: 2020-05-27 13:55:33+00:00
- **Authors**: Jingyang Zhang, Guotai Wang, Hongzhi Xie, Shuyang Zhang, Ning Huang, Shaoting Zhang, Lixu Gu
- **Comment**: None
- **Journal**: None
- **Summary**: The segmentation of coronary arteries in X-ray angiograms by convolutional neural networks (CNNs) is promising yet limited by the requirement of precisely annotating all pixels in a large number of training images, which is extremely labor-intensive especially for complex coronary trees. To alleviate the burden on the annotator, we propose a novel weakly supervised training framework that learns from noisy pseudo labels generated from automatic vessel enhancement, rather than accurate labels obtained by fully manual annotation. A typical self-paced learning scheme is used to make the training process robust against label noise while challenged by the systematic biases in pseudo labels, thus leading to the decreased performance of CNNs at test time. To solve this problem, we propose an annotation-refining self-paced learning framework (AR-SPL) to correct the potential errors using suggestive annotation. An elaborate model-vesselness uncertainty estimation is also proposed to enable the minimal annotation cost for suggestive annotation, based on not only the CNNs in training but also the geometric features of coronary arteries derived directly from raw data. Experiments show that our proposed framework achieves 1) comparable accuracy to fully supervised learning, which also significantly outperforms other weakly supervised learning frameworks; 2) largely reduced annotation cost, i.e., 75.18% of annotation time is saved, and only 3.46% of image regions are required to be annotated; and 3) an efficient intervention process, leading to superior performance with even fewer manual interactions.



### AVGZSLNet: Audio-Visual Generalized Zero-Shot Learning by Reconstructing Label Features from Multi-Modal Embeddings
- **Arxiv ID**: http://arxiv.org/abs/2005.13402v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2005.13402v3)
- **Published**: 2020-05-27 14:58:34+00:00
- **Updated**: 2020-11-23 06:13:16+00:00
- **Authors**: Pratik Mazumder, Pravendra Singh, Kranti Kumar Parida, Vinay P. Namboodiri
- **Comment**: Accepted in WACV 2021
- **Journal**: None
- **Summary**: In this paper, we propose a novel approach for generalized zero-shot learning in a multi-modal setting, where we have novel classes of audio/video during testing that are not seen during training. We use the semantic relatedness of text embeddings as a means for zero-shot learning by aligning audio and video embeddings with the corresponding class label text feature space. Our approach uses a cross-modal decoder and a composite triplet loss. The cross-modal decoder enforces a constraint that the class label text features can be reconstructed from the audio and video embeddings of data points. This helps the audio and video embeddings to move closer to the class label text embedding. The composite triplet loss makes use of the audio, video, and text embeddings. It helps bring the embeddings from the same class closer and push away the embeddings from different classes in a multi-modal setting. This helps the network to perform better on the multi-modal zero-shot learning task. Importantly, our multi-modal zero-shot learning approach works even if a modality is missing at test time. We test our approach on the generalized zero-shot classification and retrieval tasks and show that our approach outperforms other models in the presence of a single modality as well as in the presence of multiple modalities. We validate our approach by comparing it with previous approaches and using various ablations.



### Center3D: Center-based Monocular 3D Object Detection with Joint Depth Understanding
- **Arxiv ID**: http://arxiv.org/abs/2005.13423v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.13423v1)
- **Published**: 2020-05-27 15:29:09+00:00
- **Updated**: 2020-05-27 15:29:09+00:00
- **Authors**: Yunlei Tang, Sebastian Dorn, Chiragkumar Savani
- **Comment**: None
- **Journal**: None
- **Summary**: Localizing objects in 3D space and understanding their associated 3D properties is challenging given only monocular RGB images. The situation is compounded by the loss of depth information during perspective projection. We present Center3D, a one-stage anchor-free approach, to efficiently estimate 3D location and depth using only monocular RGB images. By exploiting the difference between 2D and 3D centers, we are able to estimate depth consistently. Center3D uses a combination of classification and regression to understand the hidden depth information more robustly than each method alone. Our method employs two joint approaches: (1) LID: a classification-dominated approach with sequential Linear Increasing Discretization. (2) DepJoint: a regression-dominated approach with multiple Eigen's transformations for depth estimation. Evaluating on KITTI dataset for moderate objects, Center3D improved the AP in BEV from $29.7\%$ to $42.8\%$, and the AP in 3D from $18.6\%$ to $39.1\%$. Compared with state-of-the-art detectors, Center3D has achieved the best speed-accuracy trade-off in realtime monocular object detection.



### Segmentation Loss Odyssey
- **Arxiv ID**: http://arxiv.org/abs/2005.13449v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.13449v1)
- **Published**: 2020-05-27 16:00:55+00:00
- **Updated**: 2020-05-27 16:00:55+00:00
- **Authors**: Jun Ma
- **Comment**: Educational Materials
  (https://miccai-sb.github.io/materials/Ma2019.pdf)
- **Journal**: None
- **Summary**: Loss functions are one of the crucial ingredients in deep learning-based medical image segmentation methods. Many loss functions have been proposed in existing literature, but are studied separately or only investigated with few other losses. In this paper, we present a systematic taxonomy to sort existing loss functions into four meaningful categories. This helps to reveal links and fundamental similarities between them. Moreover, we explore the relationship between the traditional region-based and the more recent boundary-based loss functions. The PyTorch implementations of these loss functions are publicly available at \url{https://github.com/JunMa11/SegLoss}.



### Improve bone age assessment by learning from anatomical local regions
- **Arxiv ID**: http://arxiv.org/abs/2005.13452v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.13452v1)
- **Published**: 2020-05-27 16:08:30+00:00
- **Updated**: 2020-05-27 16:08:30+00:00
- **Authors**: Dong Wang, Kexin Zhang, Jia Ding, Liwei Wang
- **Comment**: Early accepted to MICCAI2020
- **Journal**: None
- **Summary**: Skeletal bone age assessment (BAA), as an essential imaging examination, aims at evaluating the biological and structural maturation of human bones. In the clinical practice, Tanner and Whitehouse (TW2) method is a widely-used method for radiologists to perform BAA. The TW2 method splits the hands into Region Of Interests (ROI) and analyzes each of the anatomical ROI separately to estimate the bone age. Because of considering the analysis of local information, the TW2 method shows accurate results in practice. Following the spirit of TW2, we propose a novel model called Anatomical Local-Aware Network (ALA-Net) for automatic bone age assessment. In ALA-Net, anatomical local extraction module is introduced to learn the hand structure and extract local information. Moreover, we design an anatomical patch training strategy to provide extra regularization during the training process. Our model can detect the anatomical ROIs and estimate bone age jointly in an end-to-end manner. The experimental results show that our ALA-Net achieves a new state-of-the-art single model performance of 3.91 mean absolute error (MAE) on the public available RSNA dataset. Since the design of our model is well consistent with the well recognized TW2 method, it is interpretable and reliable for clinical usage.



### Gram filtering and sinogram interpolation for pixel-basis in parallel-beam X-ray CT reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2005.13471v1
- **DOI**: 10.1109/ISBI45749.2020.9098330
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.13471v1)
- **Published**: 2020-05-27 16:26:19+00:00
- **Updated**: 2020-05-27 16:26:19+00:00
- **Authors**: Ziyu Shu, Alireza Entezari
- **Comment**: None
- **Journal**: ISBI 2020
- **Summary**: The key aspect of parallel-beam X-ray CT is forward and back projection, but its computational burden continues to be an obstacle for applications. We propose a method to improve the performance of related algorithms by calculating the Gram filter exactly and interpolating the sinogram signal optimally. In addition, the detector blur effect can be included in our model efficiently. The improvements in speed and quality for back projection and iterative reconstruction are shown in our experiments on both analytical phantoms and real CT images.



### Kernel methods library for pattern analysis and machine learning in python
- **Arxiv ID**: http://arxiv.org/abs/2005.13483v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.CO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.13483v1)
- **Published**: 2020-05-27 16:44:42+00:00
- **Updated**: 2020-05-27 16:44:42+00:00
- **Authors**: Pradeep Reddy Raamana
- **Comment**: 6 pages, 3 code examples, 1 table
- **Journal**: None
- **Summary**: Kernel methods have proven to be powerful techniques for pattern analysis and machine learning (ML) in a variety of domains. However, many of their original or advanced implementations remain in Matlab. With the incredible rise and adoption of Python in the ML and data science world, there is a clear need for a well-defined library that enables not only the use of popular kernels, but also allows easy definition of customized kernels to fine-tune them for diverse applications. The kernelmethods library fills that important void in the python ML ecosystem in a domain-agnostic fashion, allowing the sample data type to be anything from numerical, categorical, graphs or a combination of them. In addition, this library provides a number of well-defined classes to make various kernel-based operations efficient (for large scale datasets), modular (for ease of domain adaptation), and inter-operable (across different ecosystems). The library is available at https://github.com/raamana/kernelmethods.



### How to do Physics-based Learning
- **Arxiv ID**: http://arxiv.org/abs/2005.13531v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2005.13531v2)
- **Published**: 2020-05-27 17:54:45+00:00
- **Updated**: 2020-05-28 17:08:12+00:00
- **Authors**: Michael Kellman, Michael Lustig, Laura Waller
- **Comment**: 3 pages, 2 figures, linked repository
  https://github.com/kellman/physics_based_learning
- **Journal**: None
- **Summary**: The goal of this tutorial is to explain step-by-step how to implement physics-based learning for the rapid prototyping of a computational imaging system. We provide a basic overview of physics-based learning, the construction of a physics-based network, and its reduction to practice. Specifically, we advocate exploiting the auto-differentiation functionality twice, once to build a physics-based network and again to perform physics-based learning. Thus, the user need only implement the forward model process for their system, speeding up prototyping time. We provide an open-source Pytorch implementation of a physics-based network and training procedure for a generic sparse recovery problem



### 4D Visualization of Dynamic Events from Unconstrained Multi-View Videos
- **Arxiv ID**: http://arxiv.org/abs/2005.13532v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2005.13532v1)
- **Published**: 2020-05-27 17:57:19+00:00
- **Updated**: 2020-05-27 17:57:19+00:00
- **Authors**: Aayush Bansal, Minh Vo, Yaser Sheikh, Deva Ramanan, Srinivasa Narasimhan
- **Comment**: Project Page - http://www.cs.cmu.edu/~aayushb/Open4D/
- **Journal**: None
- **Summary**: We present a data-driven approach for 4D space-time visualization of dynamic events from videos captured by hand-held multiple cameras. Key to our approach is the use of self-supervised neural networks specific to the scene to compose static and dynamic aspects of an event. Though captured from discrete viewpoints, this model enables us to move around the space-time of the event continuously. This model allows us to create virtual cameras that facilitate: (1) freezing the time and exploring views; (2) freezing a view and moving through time; and (3) simultaneously changing both time and view. We can also edit the videos and reveal occluded objects for a given view if it is visible in any of the other views. We validate our approach on challenging in-the-wild events captured using up to 15 mobile cameras.



### Network-to-Network Translation with Conditional Invertible Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2005.13580v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.13580v2)
- **Published**: 2020-05-27 18:14:22+00:00
- **Updated**: 2020-11-09 20:34:36+00:00
- **Authors**: Robin Rombach, Patrick Esser, Björn Ommer
- **Comment**: NeurIPS 2020 (oral). Code at https://github.com/CompVis/net2net
- **Journal**: None
- **Summary**: Given the ever-increasing computational costs of modern machine learning models, we need to find new ways to reuse such expert models and thus tap into the resources that have been invested in their creation. Recent work suggests that the power of these massive models is captured by the representations they learn. Therefore, we seek a model that can relate between different existing representations and propose to solve this task with a conditionally invertible network. This network demonstrates its capability by (i) providing generic transfer between diverse domains, (ii) enabling controlled content synthesis by allowing modification in other domains, and (iii) facilitating diagnosis of existing representations by translating them into interpretable domains such as images. Our domain transfer network can translate between fixed representations without having to learn or finetune them. This allows users to utilize various existing domain-specific expert models from the literature that had been trained with extensive computational resources. Experiments on diverse conditional image synthesis tasks, competitive image modification results and experiments on image-to-image and text-to-image generation demonstrate the generic applicability of our approach. For example, we translate between BERT and BigGAN, state-of-the-art text and image models to provide text-to-image generation, which neither of both experts can perform on their own.



### D2D: Keypoint Extraction with Describe to Detect Approach
- **Arxiv ID**: http://arxiv.org/abs/2005.13605v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.13605v1)
- **Published**: 2020-05-27 19:27:46+00:00
- **Updated**: 2020-05-27 19:27:46+00:00
- **Authors**: Yurun Tian, Vassileios Balntas, Tony Ng, Axel Barroso-Laguna, Yiannis Demiris, Krystian Mikolajczyk
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a novel approach that exploits the information within the descriptor space to propose keypoint locations. Detect then describe, or detect and describe jointly are two typical strategies for extracting local descriptors. In contrast, we propose an approach that inverts this process by first describing and then detecting the keypoint locations. % Describe-to-Detect (D2D) leverages successful descriptor models without the need for any additional training. Our method selects keypoints as salient locations with high information content which is defined by the descriptors rather than some independent operators. We perform experiments on multiple benchmarks including image matching, camera localisation, and 3D reconstruction. The results indicate that our method improves the matching performance of various descriptors and that it generalises across methods and tasks.



### Looking back to lower-level information in few-shot learning
- **Arxiv ID**: http://arxiv.org/abs/2005.13638v2
- **DOI**: 10.3390/info11070345
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.13638v2)
- **Published**: 2020-05-27 20:32:13+00:00
- **Updated**: 2020-07-16 02:32:21+00:00
- **Authors**: Zhongjie Yu, Sebastian Raschka
- **Comment**: 13 pages, 2 figures; fixed typographic errors and added journal ref
- **Journal**: Information 2020, 11, 345
- **Summary**: Humans are capable of learning new concepts from small numbers of examples. In contrast, supervised deep learning models usually lack the ability to extract reliable predictive rules from limited data scenarios when attempting to classify new examples. This challenging scenario is commonly known as few-shot learning. Few-shot learning has garnered increased attention in recent years due to its significance for many real-world problems. Recently, new methods relying on meta-learning paradigms combined with graph-based structures, which model the relationship between examples, have shown promising results on a variety of few-shot classification tasks. However, existing work on few-shot learning is only focused on the feature embeddings produced by the last layer of the neural network. In this work, we propose the utilization of lower-level, supporting information, namely the feature embeddings of the hidden neural network layers, to improve classifier accuracy. Based on a graph-based meta-learning framework, we develop a method called Looking-Back, where such lower-level information is used to construct additional graphs for label propagation in limited data settings. Our experiments on two popular few-shot learning datasets, miniImageNet and tieredImageNet, show that our method can utilize the lower-level information in the network to improve state-of-the-art classification performance.



### Segmentation of the Myocardium on Late-Gadolinium Enhanced MRI based on 2.5 D Residual Squeeze and Excitation Deep Learning Model
- **Arxiv ID**: http://arxiv.org/abs/2005.13643v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.13643v1)
- **Published**: 2020-05-27 20:44:38+00:00
- **Updated**: 2020-05-27 20:44:38+00:00
- **Authors**: Abdul Qayyum, Alain Lalande, Thomas Decourselle, Thibaut Pommier, Alexandre Cochet, Fabrice Meriaudeau
- **Comment**: None
- **Journal**: None
- **Summary**: Cardiac left ventricular (LV) segmentation from short-axis MRI acquired 10 minutes after the injection of a contrast agent (LGE-MRI) is a necessary step in the processing allowing the identification and diagnosis of cardiac diseases such as myocardial infarction. However, this segmentation is challenging due to high variability across subjects and the potential lack of contrast between structures. Then, the main objective of this work is to develop an accurate automatic segmentation method based on deep learning models for the myocardial borders on LGE-MRI. To this end, 2.5 D residual neural network integrated with a squeeze and excitation blocks in encoder side with specialized convolutional has been proposed. Late fusion has been used to merge the output of the best trained proposed models from a different set of hyperparameters. A total number of 320 exams (with a mean number of 6 slices per exam) were used for training and 28 exams used for testing. The performance analysis of the proposed ensemble model in the basal and middle slices was similar as compared to intra-observer study and slightly lower at apical slices. The overall Dice score was 82.01% by our proposed method as compared to Dice score of 83.22% obtained from the intra observer study. The proposed model could be used for the automatic segmentation of myocardial border that is a very important step for accurate quantification of no-reflow, myocardial infarction, myocarditis, and hypertrophic cardiomyopathy, among others.



### Multiple resolution residual network for automatic thoracic organs-at-risk segmentation from CT
- **Arxiv ID**: http://arxiv.org/abs/2005.13690v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.13690v2)
- **Published**: 2020-05-27 22:39:09+00:00
- **Updated**: 2020-05-31 22:50:43+00:00
- **Authors**: Hyemin Um, Jue Jiang, Maria Thor, Andreas Rimner, Leo Luo, Joseph O. Deasy, Harini Veeraraghavan
- **Comment**: MIDL 2020 short paper
- **Journal**: None
- **Summary**: We implemented and evaluated a multiple resolution residual network (MRRN) for multiple normal organs-at-risk (OAR) segmentation from computed tomography (CT) images for thoracic radiotherapy treatment (RT) planning. Our approach simultaneously combines feature streams computed at multiple image resolutions and feature levels through residual connections. The feature streams at each level are updated as the images are passed through various feature levels. We trained our approach using 206 thoracic CT scans of lung cancer patients with 35 scans held out for validation to segment the left and right lungs, heart, esophagus, and spinal cord. This approach was tested on 60 CT scans from the open-source AAPM Thoracic Auto-Segmentation Challenge dataset. Performance was measured using the Dice Similarity Coefficient (DSC). Our approach outperformed the best-performing method in the grand challenge for hard-to-segment structures like the esophagus and achieved comparable results for all other structures. Median DSC using our method was 0.97 (interquartile range [IQR]: 0.97-0.98) for the left and right lungs, 0.93 (IQR: 0.93-0.95) for the heart, 0.78 (IQR: 0.76-0.80) for the esophagus, and 0.88 (IQR: 0.86-0.89) for the spinal cord.



### An ENAS Based Approach for Constructing Deep Learning Models for Breast Cancer Recognition from Ultrasound Images
- **Arxiv ID**: http://arxiv.org/abs/2005.13695v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.13695v1)
- **Published**: 2020-05-27 22:49:45+00:00
- **Updated**: 2020-05-27 22:49:45+00:00
- **Authors**: Mohammed Ahmed, Hongbo Du, Alaa AlZoubi
- **Comment**: 6 pages, 3 figures, Conference: Medical Imaging with Deep Learning
  2020
- **Journal**: None
- **Summary**: Deep Convolutional Neural Networks (CNN) provides an "end-to-end" solution for image pattern recognition with impressive performance in many areas of application including medical imaging. Most CNN models of high performance use hand-crafted network architectures that require expertise in CNNs to utilise their potentials. In this paper, we applied the Efficient Neural Architecture Search (ENAS) method to find optimal CNN architectures for classifying breast lesions from ultrasound (US) images. Our empirical study with a dataset of 524 US images shows that the optimal models generated by using ENAS achieve an average accuracy of 89.3%, surpassing other hand-crafted alternatives. Furthermore, the models are simpler in complexity and more efficient. Our study demonstrates that the ENAS approach to CNN model design is a promising direction for classifying ultrasound images of breast lesions.



### On the Difficulty of Membership Inference Attacks
- **Arxiv ID**: http://arxiv.org/abs/2005.13702v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.13702v3)
- **Published**: 2020-05-27 23:09:17+00:00
- **Updated**: 2021-03-22 20:17:22+00:00
- **Authors**: Shahbaz Rezaei, Xin Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Recent studies propose membership inference (MI) attacks on deep models, where the goal is to infer if a sample has been used in the training process. Despite their apparent success, these studies only report accuracy, precision, and recall of the positive class (member class). Hence, the performance of these attacks have not been clearly reported on negative class (non-member class). In this paper, we show that the way the MI attack performance has been reported is often misleading because they suffer from high false positive rate or false alarm rate (FAR) that has not been reported. FAR shows how often the attack model mislabel non-training samples (non-member) as training (member) ones. The high FAR makes MI attacks fundamentally impractical, which is particularly more significant for tasks such as membership inference where the majority of samples in reality belong to the negative (non-training) class. Moreover, we show that the current MI attack models can only identify the membership of misclassified samples with mediocre accuracy at best, which only constitute a very small portion of training samples.   We analyze several new features that have not been comprehensively explored for membership inference before, including distance to the decision boundary and gradient norms, and conclude that deep models' responses are mostly similar among train and non-train samples. We conduct several experiments on image classification tasks, including MNIST, CIFAR-10, CIFAR-100, and ImageNet, using various model architecture, including LeNet, AlexNet, ResNet, etc. We show that the current state-of-the-art MI attacks cannot achieve high accuracy and low FAR at the same time, even when the attacker is given several advantages.   The source code is available at https://github.com/shrezaei/MI-Attack.



### Graph-based Proprioceptive Localization Using a Discrete Heading-Length Feature Sequence Matching Approach
- **Arxiv ID**: http://arxiv.org/abs/2005.13704v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.13704v1)
- **Published**: 2020-05-27 23:10:15+00:00
- **Updated**: 2020-05-27 23:10:15+00:00
- **Authors**: Hsin-Min Cheng, Dezhen Song
- **Comment**: 13 pages, 32 figures
- **Journal**: None
- **Summary**: Proprioceptive localization refers to a new class of robot egocentric localization methods that do not rely on the perception and recognition of external landmarks. These methods are naturally immune to bad weather, poor lighting conditions, or other extreme environmental conditions that may hinder exteroceptive sensors such as a camera or a laser ranger finder. These methods depend on proprioceptive sensors such as inertial measurement units (IMUs) and/or wheel encoders. Assisted by magnetoreception, the sensors can provide a rudimentary estimation of vehicle trajectory which is used to query a prior known map to obtain location. Named as graph-based proprioceptive localization (GBPL), we provide a low cost fallback solution for localization under challenging environmental conditions. As a robot/vehicle travels, we extract a sequence of heading-length values for straight segments from the trajectory and match the sequence with a pre-processed heading-length graph (HLG) abstracted from the prior known map to localize the robot under a graph-matching approach. Using the information from HLG, our location alignment and verification module compensates for trajectory drift, wheel slip, or tire inflation level. We have implemented our algorithm and tested it in both simulated and physical experiments. The algorithm runs successfully in finding robot location continuously and achieves localization accurate at the level that the prior map allows (less than 10m).



### Detecting Scatteredly-Distributed, Small, andCritically Important Objects in 3D OncologyImaging via Decision Stratification
- **Arxiv ID**: http://arxiv.org/abs/2005.13705v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.13705v1)
- **Published**: 2020-05-27 23:12:11+00:00
- **Updated**: 2020-05-27 23:12:11+00:00
- **Authors**: Zhuotun Zhu, Ke Yan, Dakai Jin, Jinzheng Cai, Tsung-Ying Ho, Adam P Harrison, Dazhou Guo, Chun-Hung Chao, Xianghua Ye, Jing Xiao, Alan Yuille, Le Lu
- **Comment**: 14 pages, 4 Figures
- **Journal**: None
- **Summary**: Finding and identifying scatteredly-distributed, small, and critically important objects in 3D oncology images is very challenging. We focus on the detection and segmentation of oncology-significant (or suspicious cancer metastasized) lymph nodes (OSLNs), which has not been studied before as a computational task. Determining and delineating the spread of OSLNs is essential in defining the corresponding resection/irradiating regions for the downstream workflows of surgical resection and radiotherapy of various cancers. For patients who are treated with radiotherapy, this task is performed by experienced radiation oncologists that involves high-level reasoning on whether LNs are metastasized, which is subject to high inter-observer variations. In this work, we propose a divide-and-conquer decision stratification approach that divides OSLNs into tumor-proximal and tumor-distal categories. This is motivated by the observation that each category has its own different underlying distributions in appearance, size and other characteristics. Two separate detection-by-segmentation networks are trained per category and fused. To further reduce false positives (FP), we present a novel global-local network (GLNet) that combines high-level lesion characteristics with features learned from localized 3D image patches. Our method is evaluated on a dataset of 141 esophageal cancer patients with PET and CT modalities (the largest to-date). Our results significantly improve the recall from $45\%$ to $67\%$ at $3$ FPs per patient as compared to previous state-of-the-art methods. The highest achieved OSLN recall of $0.828$ is clinically relevant and valuable.



### AFAT: Adaptive Failure-Aware Tracker for Robust Visual Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2005.13708v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.13708v1)
- **Published**: 2020-05-27 23:21:12+00:00
- **Updated**: 2020-05-27 23:21:12+00:00
- **Authors**: Tianyang Xu, Zhen-Hua Feng, Xiao-Jun Wu, Josef Kittler
- **Comment**: None
- **Journal**: None
- **Summary**: Siamese approaches have achieved promising performance in visual object tracking recently. The key to the success of Siamese trackers is to learn appearance-invariant feature embedding functions via pair-wise offline training on large-scale video datasets. However, the Siamese paradigm uses one-shot learning to model the online tracking task, which impedes online adaptation in the tracking process. Additionally, the uncertainty of an online tracking response is not measured, leading to the problem of ignoring potential failures. In this paper, we advocate online adaptation in the tracking stage. To this end, we propose a failure-aware system, realised by a Quality Prediction Network (QPN), based on convolutional and LSTM modules in the decision stage, enabling online reporting of potential tracking failures. Specifically, sequential response maps from previous successive frames as well as current frame are collected to predict the tracking confidence, realising spatio-temporal fusion in the decision level. In addition, we further provide an Adaptive Failure-Aware Tracker (AFAT) by combing the state-of-the-art Siamese trackers with our system. The experimental results obtained on standard benchmarking datasets demonstrate the effectiveness of the proposed failure-aware system and the merits of our AFAT tracker, with outstanding and balanced performance in both accuracy and speed.



### Few-Shot Open-Set Recognition using Meta-Learning
- **Arxiv ID**: http://arxiv.org/abs/2005.13713v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.13713v2)
- **Published**: 2020-05-27 23:49:26+00:00
- **Updated**: 2020-06-07 19:15:41+00:00
- **Authors**: Bo Liu, Hao Kang, Haoxiang Li, Gang Hua, Nuno Vasconcelos
- **Comment**: None
- **Journal**: None
- **Summary**: The problem of open-set recognition is considered. While previous approaches only consider this problem in the context of large-scale classifier training, we seek a unified solution for this and the low-shot classification setting. It is argued that the classic softmax classifier is a poor solution for open-set recognition, since it tends to overfit on the training classes. Randomization is then proposed as a solution to this problem. This suggests the use of meta-learning techniques, commonly used for few-shot classification, for the solution of open-set recognition. A new oPen sEt mEta LEaRning (PEELER) algorithm is then introduced. This combines the random selection of a set of novel classes per episode, a loss that maximizes the posterior entropy for examples of those classes, and a new metric learning formulation based on the Mahalanobis distance. Experimental results show that PEELER achieves state of the art open set recognition performance for both few-shot and large-scale recognition. On CIFAR and miniImageNet, it achieves substantial gains in seen/unseen class detection AUROC for a given seen-class classification accuracy.



