# Arxiv Papers in cs.CV on 2020-05-25
### An efficient iterative method for reconstructing surface from point clouds
- **Arxiv ID**: http://arxiv.org/abs/2005.11864v1
- **DOI**: None
- **Categories**: **math.NA**, cs.CV, cs.NA
- **Links**: [PDF](http://arxiv.org/pdf/2005.11864v1)
- **Published**: 2020-05-25 00:01:53+00:00
- **Updated**: 2020-05-25 00:01:53+00:00
- **Authors**: Dong Wang
- **Comment**: 23 pages, 15 figures
- **Journal**: None
- **Summary**: Surface reconstruction from point clouds is a fundamental step in many applications in computer vision. In this paper, we develop an efficient iterative method on a variational model for the surface reconstruction from point clouds. The surface is implicitly represented by indicator functions and the energy functional is then approximated based on such representations using heat kernel convolutions. We then develop a novel iterative method to minimize the approximate energy and prove the energy decaying property during each iteration. We then use asymptotic expansion to give a connection between the proposed algorithm and active contour models. Extensive numerical experiments are performed in both 2- and 3- dimensional Euclidean spaces to show that the proposed method is simple, efficient, and accurate.



### Bayesian Conditional GAN for MRI Brain Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2005.11875v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, I.4.5; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2005.11875v2)
- **Published**: 2020-05-25 00:58:23+00:00
- **Updated**: 2021-04-22 15:04:17+00:00
- **Authors**: Gengyan Zhao, Mary E. Meyerand, Rasmus M. Birn
- **Comment**: 26 pages, 7 figures
- **Journal**: None
- **Summary**: As a powerful technique in medical imaging, image synthesis is widely used in applications such as denoising, super resolution and modality transformation etc. Recently, the revival of deep neural networks made immense progress in the field of medical imaging. Although many deep leaning based models have been proposed to improve the image synthesis accuracy, the evaluation of the model uncertainty, which is highly important for medical applications, has been a missing part. In this work, we propose to use Bayesian conditional generative adversarial network (GAN) with concrete dropout to improve image synthesis accuracy. Meanwhile, an uncertainty calibration approach is involved in the whole pipeline to make the uncertainty generated by Bayesian network interpretable. The method is validated with the T1w to T2w MR image translation with a brain tumor dataset of 102 subjects. Compared with the conventional Bayesian neural network with Monte Carlo dropout, results of the proposed method reach a significant lower RMSE with a p-value of 0.0186. Improvement of the calibration of the generated uncertainty by the uncertainty recalibration method is also illustrated.



### Adaptive Adversarial Logits Pairing
- **Arxiv ID**: http://arxiv.org/abs/2005.11904v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.11904v2)
- **Published**: 2020-05-25 03:12:20+00:00
- **Updated**: 2021-04-16 01:57:11+00:00
- **Authors**: Shangxi Wu, Jitao Sang, Kaiyuan Xu, Guanhua Zheng, Changsheng Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Adversarial examples provide an opportunity as well as impose a challenge for understanding image classification systems. Based on the analysis of the adversarial training solution Adversarial Logits Pairing (ALP), we observed in this work that: (1) The inference of adversarially robust model tends to rely on fewer high-contribution features compared with vulnerable ones. (2) The training target of ALP doesn't fit well to a noticeable part of samples, where the logits pairing loss is overemphasized and obstructs minimizing the classification loss. Motivated by these observations, we design an Adaptive Adversarial Logits Pairing (AALP) solution by modifying the training process and training target of ALP. Specifically, AALP consists of an adaptive feature optimization module with Guided Dropout to systematically pursue fewer high-contribution features, and an adaptive sample weighting module by setting sample-specific training weights to balance between logits pairing loss and classification loss. The proposed AALP solution demonstrates superior defense performance on multiple datasets with extensive experiments.



### Rethinking of Pedestrian Attribute Recognition: Realistic Datasets with Efficient Method
- **Arxiv ID**: http://arxiv.org/abs/2005.11909v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.11909v2)
- **Published**: 2020-05-25 03:30:15+00:00
- **Updated**: 2020-05-26 01:13:23+00:00
- **Authors**: Jian Jia, Houjing Huang, Wenjie Yang, Xiaotang Chen, Kaiqi Huang
- **Comment**: 12 pages, 4 figures. Code is availabel at
  https://github.com/valencebond/Strong_Baseline_of_Pedestrian_Attribute_Recognition
- **Journal**: None
- **Summary**: Despite various methods are proposed to make progress in pedestrian attribute recognition, a crucial problem on existing datasets is often neglected, namely, a large number of identical pedestrian identities in train and test set, which is not consistent with practical application. Thus, images of the same pedestrian identity in train set and test set are extremely similar, leading to overestimated performance of state-of-the-art methods on existing datasets. To address this problem, we propose two realistic datasets PETA\textsubscript{$zs$} and RAPv2\textsubscript{$zs$} following zero-shot setting of pedestrian identities based on PETA and RAPv2 datasets. Furthermore, compared to our strong baseline method, we have observed that recent state-of-the-art methods can not make performance improvement on PETA, RAPv2, PETA\textsubscript{$zs$} and RAPv2\textsubscript{$zs$}. Thus, through solving the inherent attribute imbalance in pedestrian attribute recognition, an efficient method is proposed to further improve the performance. Experiments on existing and proposed datasets verify the superiority of our method by achieving state-of-the-art performance.



### What am I Searching for: Zero-shot Target Identity Inference in Visual Search
- **Arxiv ID**: http://arxiv.org/abs/2005.12741v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.12741v2)
- **Published**: 2020-05-25 04:53:32+00:00
- **Updated**: 2020-05-28 19:49:41+00:00
- **Authors**: Mengmi Zhang, Gabriel Kreiman
- **Comment**: this was a mistaken new submission and a pointer to arXiv:1807.11926
- **Journal**: None
- **Summary**: Can we infer intentions from a person's actions? As an example problem, here we consider how to decipher what a person is searching for by decoding their eye movement behavior. We conducted two psychophysics experiments where we monitored eye movements while subjects searched for a target object. We defined the fixations falling on \textit{non-target} objects as "error fixations". Using those error fixations, we developed a model (InferNet) to infer what the target was. InferNet uses a pre-trained convolutional neural network to extract features from the error fixations and computes a similarity map between the error fixations and all locations across the search image. The model consolidates the similarity maps across layers and integrates these maps across all error fixations. InferNet successfully identifies the subject's goal and outperforms competitive null models, even without any object-specific training on the inference task.



### Visual Localization Using Semantic Segmentation and Depth Prediction
- **Arxiv ID**: http://arxiv.org/abs/2005.11922v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.11922v1)
- **Published**: 2020-05-25 04:55:27+00:00
- **Updated**: 2020-05-25 04:55:27+00:00
- **Authors**: Huanhuan Fan, Yuhao Zhou, Ang Li, Shuang Gao, Jijunnan Li, Yandong Guo
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a monocular visual localization pipeline leveraging semantic and depth cues. We apply semantic consistency evaluation to rank the image retrieval results and a practical clustering technique to reject estimation outliers. In addition, we demonstrate a substantial performance boost achieved with a combination of multiple feature extractors. Furthermore, by using depth prediction with a deep neural network, we show that a significant amount of falsely matched keypoints are identified and eliminated. The proposed pipeline outperforms most of the existing approaches at the Long-Term Visual Localization benchmark 2020.



### mr2NST: Multi-Resolution and Multi-Reference Neural Style Transfer for Mammography
- **Arxiv ID**: http://arxiv.org/abs/2005.11926v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.11926v1)
- **Published**: 2020-05-25 05:24:29+00:00
- **Updated**: 2020-05-25 05:24:29+00:00
- **Authors**: Sheng Wang, Jiayu Huo, Xi Ouyang, Jifei Che, Xuhua Ren, Zhong Xue, Qian Wang, Jie-Zhi Cheng
- **Comment**: None
- **Journal**: None
- **Summary**: Computer-aided diagnosis with deep learning techniques has been shown to be helpful for the diagnosis of the mammography in many clinical studies. However, the image styles of different vendors are very distinctive, and there may exist domain gap among different vendors that could potentially compromise the universal applicability of one deep learning model. In this study, we explicitly address style variety issue with the proposed multi-resolution and multi-reference neural style transfer (mr2NST) network. The mr2NST can normalize the styles from different vendors to the same style baseline with very high resolution. We illustrate that the image quality of the transferred images is comparable to the quality of original images of the target domain (vendor) in terms of NIMA scores. Meanwhile, the mr2NST results are also shown to be helpful for the lesion detection in mammograms.



### A Bayesian-inspired, deep learning-based, semi-supervised domain adaptation technique for land cover mapping
- **Arxiv ID**: http://arxiv.org/abs/2005.11930v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.11930v2)
- **Published**: 2020-05-25 05:36:50+00:00
- **Updated**: 2021-03-10 05:57:44+00:00
- **Authors**: Benjamin Lucas, Charlotte Pelletier, Daniel Schmidt, Geoffrey I. Webb, François Petitjean
- **Comment**: None
- **Journal**: None
- **Summary**: Land cover maps are a vital input variable to many types of environmental research and management. While they can be produced automatically by machine learning techniques, these techniques require substantial training data to achieve high levels of accuracy, which are not always available. One technique researchers use when labelled training data are scarce is domain adaptation (DA) -- where data from an alternate region, known as the source domain, are used to train a classifier and this model is adapted to map the study region, or target domain. The scenario we address in this paper is known as semi-supervised DA, where some labelled samples are available in the target domain. In this paper we present Sourcerer, a Bayesian-inspired, deep learning-based, semi-supervised DA technique for producing land cover maps from SITS data. The technique takes a convolutional neural network trained on a source domain and then trains further on the available target domain with a novel regularizer applied to the model weights. The regularizer adjusts the degree to which the model is modified to fit the target data, limiting the degree of change when the target data are few in number and increasing it as target data quantity increases. Our experiments on Sentinel-2 time series images compare Sourcerer with two state-of-the-art semi-supervised domain adaptation techniques and four baseline models. We show that on two different source-target domain pairings Sourcerer outperforms all other methods for any quantity of labelled target data available. In fact, the results on the more difficult target domain show that the starting accuracy of Sourcerer (when no labelled target data are available), 74.2%, is greater than the next-best state-of-the-art method trained on 20,000 labelled target instances.



### Interlayer and Intralayer Scale Aggregation for Scale-invariant Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/2005.11943v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.11943v1)
- **Published**: 2020-05-25 06:59:31+00:00
- **Updated**: 2020-05-25 06:59:31+00:00
- **Authors**: Mingjie Wang, Hao Cai, Jun Zhou, Minglun Gong
- **Comment**: None
- **Journal**: None
- **Summary**: Crowd counting is an important vision task, which faces challenges on continuous scale variation within a given scene and huge density shift both within and across images. These challenges are typically addressed using multi-column structures in existing methods. However, such an approach does not provide consistent improvement and transferability due to limited ability in capturing multi-scale features, sensitiveness to large density shift, and difficulty in training multi-branch models. To overcome these limitations, a Single-column Scale-invariant Network (ScSiNet) is presented in this paper, which extracts sophisticated scale-invariant features via the combination of interlayer multi-scale integration and a novel intralayer scale-invariant transformation (SiT). Furthermore, in order to enlarge the diversity of densities, a randomly integrated loss is presented for training our single-branch method. Extensive experiments on public datasets demonstrate that the proposed method consistently outperforms state-of-the-art approaches in counting accuracy and achieves remarkable transferability and scale-invariant property.



### Multi-Margin based Decorrelation Learning for Heterogeneous Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2005.11945v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.11945v1)
- **Published**: 2020-05-25 07:01:12+00:00
- **Updated**: 2020-05-25 07:01:12+00:00
- **Authors**: Bing Cao, Nannan Wang, Xinbo Gao, Jie Li, Zhifeng Li
- **Comment**: IJCAI 2019
- **Journal**: None
- **Summary**: Heterogeneous face recognition (HFR) refers to matching face images acquired from different domains with wide applications in security scenarios. This paper presents a deep neural network approach namely Multi-Margin based Decorrelation Learning (MMDL) to extract decorrelation representations in a hyperspherical space for cross-domain face images. The proposed framework can be divided into two components: heterogeneous representation network and decorrelation representation learning. First, we employ a large scale of accessible visual face images to train heterogeneous representation network. The decorrelation layer projects the output of the first component into decorrelation latent subspace and obtains decorrelation representation. In addition, we design a multi-margin loss (MML), which consists of quadruplet margin loss (QML) and heterogeneous angular margin loss (HAML), to constrain the proposed framework. Experimental results on two challenging heterogeneous face databases show that our approach achieves superior performance on both verification and recognition tasks, comparing with state-of-the-art methods.



### Keypoints Localization for Joint Vertebra Detection and Fracture Severity Quantification
- **Arxiv ID**: http://arxiv.org/abs/2005.11960v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.11960v2)
- **Published**: 2020-05-25 08:05:27+00:00
- **Updated**: 2020-07-20 12:46:48+00:00
- **Authors**: Maxim Pisov, Vladimir Kondratenko, Alexey Zakharov, Alexey Petraikin, Victor Gombolevskiy, Sergey Morozov, Mikhail Belyaev
- **Comment**: Accepted to MICCAI-2020
- **Journal**: None
- **Summary**: Vertebral body compression fractures are reliable early signs of osteoporosis. Though these fractures are visible on Computed Tomography (CT) images, they are frequently missed by radiologists in clinical settings. Prior research on automatic methods of vertebral fracture classification proves its reliable quality; however, existing methods provide hard-to-interpret outputs and sometimes fail to process cases with severe abnormalities such as highly pathological vertebrae or scoliosis. We propose a new two-step algorithm to localize the vertebral column in 3D CT images and then to simultaneously detect individual vertebrae and quantify fractures in 2D. We train neural networks for both steps using a simple 6-keypoints based annotation scheme, which corresponds precisely to current medical recommendation. Our algorithm has no exclusion criteria, processes 3D CT in 2 seconds on a single GPU, and provides an intuitive and verifiable output. The method approaches expert-level performance and demonstrates state-of-the-art results in vertebrae 3D localization (the average error is 1 mm), vertebrae 2D detection (precision is 0.99, recall is 1), and fracture identification (ROC AUC at the patient level is 0.93).



### Hyperspectral Image Classification with Attention Aided CNNs
- **Arxiv ID**: http://arxiv.org/abs/2005.11977v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.11977v2)
- **Published**: 2020-05-25 08:40:56+00:00
- **Updated**: 2020-06-12 14:25:00+00:00
- **Authors**: Renlong Hang, Zhu Li, Qingshan Liu, Pedram Ghamisi, Shuvra S. Bhattacharyya
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have been widely used for hyperspectral image classification. As a common process, small cubes are firstly cropped from the hyperspectral image and then fed into CNNs to extract spectral and spatial features. It is well known that different spectral bands and spatial positions in the cubes have different discriminative abilities. If fully explored, this prior information will help improve the learning capacity of CNNs. Along this direction, we propose an attention aided CNN model for spectral-spatial classification of hyperspectral images. Specifically, a spectral attention sub-network and a spatial attention sub-network are proposed for spectral and spatial classification, respectively. Both of them are based on the traditional CNN model, and incorporate attention modules to aid networks focus on more discriminative channels or positions. In the final classification phase, the spectral classification result and the spatial classification result are combined together via an adaptively weighted summation method. To evaluate the effectiveness of the proposed model, we conduct experiments on three standard hyperspectral datasets. The experimental results show that the proposed model can achieve superior performance compared to several state-of-the-art CNN-related models.



### Eye Gaze Controlled Robotic Arm for Persons with SSMI
- **Arxiv ID**: http://arxiv.org/abs/2005.11994v1
- **DOI**: 10.3233/TAD-200264
- **Categories**: **cs.HC**, cs.CV, eess.IV, I.4; I.2; H.5.2; K.4
- **Links**: [PDF](http://arxiv.org/pdf/2005.11994v1)
- **Published**: 2020-05-25 09:23:20+00:00
- **Updated**: 2020-05-25 09:23:20+00:00
- **Authors**: Vinay Krishna Sharma, L. R. D. Murthy, KamalPreet Singh Saluja, Vimal Mollyn, Gourav Sharma, Pradipta Biswas
- **Comment**: Citation: VK Sharma, KPS Saluja, LRD Murthy, G Sharma and P Biswas,
  Webcam Controlled Robotic Arm for Persons with SSMI, Technology and
  Disability 32 (3), IOS Press 2020 [Official journal of EU AAATE association]
- **Journal**: None
- **Summary**: Background: People with severe speech and motor impairment (SSMI) often uses a technique called eye pointing to communicate with outside world. One of their parents, caretakers or teachers hold a printed board in front of them and by analyzing their eye gaze manually, their intentions are interpreted. This technique is often error prone and time consuming and depends on a single caretaker.   Objective: We aimed to automate the eye tracking process electronically by using commercially available tablet, computer or laptop and without requiring any dedicated hardware for eye gaze tracking. The eye gaze tracker is used to develop a video see through based AR (augmented reality) display that controls a robotic device with eye gaze and deployed for a fabric printing task.   Methodology: We undertook a user centred design process and separately evaluated the web cam based gaze tracker and the video see through based human robot interaction involving users with SSMI. We also reported a user study on manipulating a robotic arm with webcam based eye gaze tracker.   Results: Using our bespoke eye gaze controlled interface, able bodied users can select one of nine regions of screen at a median of less than 2 secs and users with SSMI can do so at a median of 4 secs. Using the eye gaze controlled human-robot AR display, users with SSMI could undertake representative pick and drop task at an average duration less than 15 secs and reach a randomly designated target within 60 secs using a COTS eye tracker and at an average time of 2 mins using the webcam based eye gaze tracker.



### A Preliminary Study for Identification of Additive Manufactured Objects with Transmitted Images
- **Arxiv ID**: http://arxiv.org/abs/2005.12027v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.12027v1)
- **Published**: 2020-05-25 11:04:04+00:00
- **Updated**: 2020-05-25 11:04:04+00:00
- **Authors**: Kenta Yamamoto, Ryota Kawamura, Kazuki Takazawa, Hiroyuki Osone, Yoichi Ochiai
- **Comment**: None
- **Journal**: None
- **Summary**: Additive manufacturing has the potential to become a standard method for manufacturing products, and product information is indispensable for the item distribution system. While most products are given barcodes to the exterior surfaces, research on embedding barcodes inside products is underway. This is because additive manufacturing makes it possible to carry out manufacturing and information adding at the same time, and embedding information inside does not impair the exterior appearance of the product. However, products that have not been embedded information can not be identified, and embedded information can not be rewritten later. In this study, we have developed a product identification system that does not require embedding barcodes inside. This system uses a transmission image of the product which contains information of each product such as different inner support structures and manufacturing errors. We have shown through experiments that if datasets of transmission images are available, objects can be identified with an accuracy of over 90%. This result suggests that our approach can be useful for identifying objects without embedded information.



### Interpreting Chest X-rays via CNNs that Exploit Hierarchical Disease Dependencies and Uncertainty Labels
- **Arxiv ID**: http://arxiv.org/abs/2005.12734v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.12734v1)
- **Published**: 2020-05-25 11:07:53+00:00
- **Updated**: 2020-05-25 11:07:53+00:00
- **Authors**: Hieu H. Pham, Tung T. Le, Dat T. Ngo, Dat Q. Tran, Ha Q. Nguyen
- **Comment**: MIDL 2020 Accepted Short Paper. arXiv admin note: substantial text
  overlap with arXiv:1911.06475
- **Journal**: None
- **Summary**: The chest X-rays (CXRs) is one of the views most commonly ordered by radiologists (NHS),which is critical for diagnosis of many different thoracic diseases. Accurately detecting thepresence of multiple diseases from CXRs is still a challenging task. We present a multi-labelclassification framework based on deep convolutional neural networks (CNNs) for diagnos-ing the presence of 14 common thoracic diseases and observations. Specifically, we trained astrong set of CNNs that exploit dependencies among abnormality labels and used the labelsmoothing regularization (LSR) for a better handling of uncertain samples. Our deep net-works were trained on over 200,000 CXRs of the recently released CheXpert dataset (Irvinandal., 2019) and the final model, which was an ensemble of the best performing networks,achieved a mean area under the curve (AUC) of 0.940 in predicting 5 selected pathologiesfrom the validation set. To the best of our knowledge, this is the highest AUC score yetreported to date. More importantly, the proposed method was also evaluated on an inde-pendent test set of the CheXpert competition, containing 500 CXR studies annotated by apanel of 5 experienced radiologists. The reported performance was on average better than2.6 out of 3 other individual radiologists with a mean AUC of 0.930, which had led to thecurrent state-of-the-art performance on the CheXpert test set.



### A Joint Pixel and Feature Alignment Framework for Cross-dataset Palmprint Recognition
- **Arxiv ID**: http://arxiv.org/abs/2005.12044v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.12044v1)
- **Published**: 2020-05-25 11:40:51+00:00
- **Updated**: 2020-05-25 11:40:51+00:00
- **Authors**: Huikai Shao, Dexing Zhong
- **Comment**: 12 pages, 7 figures
- **Journal**: None
- **Summary**: Deep learning-based palmprint recognition algorithms have shown great potential. Most of them are mainly focused on identifying samples from the same dataset. However, they may be not suitable for a more convenient case that the images for training and test are from different datasets, such as collected by embedded terminals and smartphones. Therefore, we propose a novel Joint Pixel and Feature Alignment (JPFA) framework for such cross-dataset palmprint recognition scenarios. Two stage-alignment is applied to obtain adaptive features in source and target datasets. 1) Deep style transfer model is adopted to convert source images into fake images to reduce the dataset gaps and perform data augmentation on pixel level. 2) A new deep domain adaptation model is proposed to extract adaptive features by aligning the dataset-specific distributions of target-source and target-fake pairs on feature level. Adequate experiments are conducted on several benchmarks including constrained and unconstrained palmprint databases. The results demonstrate that our JPFA outperforms other models to achieve the state-of-the-arts. Compared with baseline, the accuracy of cross-dataset identification is improved by up to 28.10% and the Equal Error Rate (EER) of cross-dataset verification is reduced by up to 4.69%. To make our results reproducible, the codes are publicly available at http://gr.xjtu.edu.cn/web/bell/resource.



### An interpretable automated detection system for FISH-based HER2 oncogene amplification testing in histo-pathological routine images of breast and gastric cancer diagnostics
- **Arxiv ID**: http://arxiv.org/abs/2005.12066v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.12066v1)
- **Published**: 2020-05-25 12:14:38+00:00
- **Updated**: 2020-05-25 12:14:38+00:00
- **Authors**: Sarah Schmell, Falk Zakrzewski, Walter de Back, Martin Weigert, Uwe Schmidt, Torsten Wenke, Silke Zeugner, Robert Mantey, Christian Sperling, Ingo Roeder, Pia Hoenscheid, Daniela Aust, Gustavo Baretton
- **Comment**: None
- **Journal**: None
- **Summary**: Histo-pathological diagnostics are an inherent part of the everyday work but are particularly laborious and associated with time-consuming manual analysis of image data. In order to cope with the increasing diagnostic case numbers due to the current growth and demographic change of the global population and the progress in personalized medicine, pathologists ask for assistance. Profiting from digital pathology and the use of artificial intelligence, individual solutions can be offered (e.g. detect labeled cancer tissue sections). The testing of the human epidermal growth factor receptor 2 (HER2) oncogene amplification status via fluorescence in situ hybridization (FISH) is recommended for breast and gastric cancer diagnostics and is regularly performed at clinics. Here, we develop an interpretable, deep learning (DL)-based pipeline which automates the evaluation of FISH images with respect to HER2 gene amplification testing. It mimics the pathological assessment and relies on the detection and localization of interphase nuclei based on instance segmentation networks. Furthermore, it localizes and classifies fluorescence signals within each nucleus with the help of image classification and object detection convolutional neural networks (CNNs). Finally, the pipeline classifies the whole image regarding its HER2 amplification status. The visualization of pixels on which the networks' decision occurs, complements an essential part to enable interpretability by pathologists.



### Visual Attention: Deep Rare Features
- **Arxiv ID**: http://arxiv.org/abs/2005.12073v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.12073v1)
- **Published**: 2020-05-25 12:28:08+00:00
- **Updated**: 2020-05-25 12:28:08+00:00
- **Authors**: Matei Mancas, Phutphalla Kong, Bernard Gosselin
- **Comment**: 6 pages, double-colmun, accepted to IVPR2020
- **Journal**: None
- **Summary**: Human visual system is modeled in engineering field providing feature-engineered methods which detect contrasted/surprising/unusual data into images. This data is "interesting" for humans and leads to numerous applications. Deep learning (DNNs) drastically improved the algorithms efficiency on the main benchmark datasets. However, DNN-based models are counter-intuitive: surprising or unusual data is by definition difficult to learn because of its low occurrence probability. In reality, DNNs models mainly learn top-down features such as faces, text, people, or animals which usually attract human attention, but they have low efficiency in extracting surprising or unusual data in the images. In this paper, we propose a model called DeepRare2019 (DR) which uses the power of DNNs feature extraction and the genericity of feature-engineered algorithms. DR 1) does not need any training, 2) it takes less than a second per image on CPU only and 3) our tests on three very different eye-tracking datasets show that DR is generic and is always in the top-3 models on all datasets and metrics while no other model exhibits such a regularity and genericity. DeepRare2019 code can be found at https://github.com/numediart/VisualAttention-RareFamily



### Egocentric Human Segmentation for Mixed Reality
- **Arxiv ID**: http://arxiv.org/abs/2005.12074v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.12074v2)
- **Published**: 2020-05-25 12:34:47+00:00
- **Updated**: 2020-06-08 14:58:07+00:00
- **Authors**: Andrija Gajic, Ester Gonzalez-Sosa, Diego Gonzalez-Morin, Marcos Escudero-Viñolo, Alvaro Villegas
- **Comment**: Accepted for presentation at EPIC@CVPR2020 workshop
- **Journal**: None
- **Summary**: The objective of this work is to segment human body parts from egocentric video using semantic segmentation networks. Our contribution is two-fold: i) we create a semi-synthetic dataset composed of more than 15, 000 realistic images and associated pixel-wise labels of egocentric human body parts, such as arms or legs including different demographic factors; ii) building upon the ThunderNet architecture, we implement a deep learning semantic segmentation algorithm that is able to perform beyond real-time requirements (16 ms for 720 x 720 images). It is believed that this method will enhance sense of presence of Virtual Environments and will constitute a more realistic solution to the standard virtual avatars.



### The efficiency of deep learning algorithms for detecting anatomical reference points on radiological images of the head profile
- **Arxiv ID**: http://arxiv.org/abs/2005.12110v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.12110v2)
- **Published**: 2020-05-25 13:51:03+00:00
- **Updated**: 2020-06-18 09:12:49+00:00
- **Authors**: Konstantin Dobratulin, Andrey Gaidel, Irina Aupova, Anna Ivleva, Aleksandr Kapishnikov, Pavel Zelter
- **Comment**: None
- **Journal**: None
- **Summary**: In this article we investigate the efficiency of deep learning algorithms in solving the task of detecting anatomical reference points on radiological images of the head in lateral projection using a fully convolutional neural network and a fully convolutional neural network with an extended architecture for biomedical image segmentation - U-Net. A comparison is made for the results of detection anatomical reference points for each of the selected neural network architectures and their comparison with the results obtained when orthodontists detected anatomical reference points. Based on the obtained results, it was concluded that a U-Net neural network allows performing the detection of anatomical reference points more accurately than a fully convolutional neural network. The results of the detection of anatomical reference points by the U-Net neural network are closer to the average results of the detection of reference points by a group of orthodontists.



### Learning to Simulate Dynamic Environments with GameGAN
- **Arxiv ID**: http://arxiv.org/abs/2005.12126v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.12126v1)
- **Published**: 2020-05-25 14:10:17+00:00
- **Updated**: 2020-05-25 14:10:17+00:00
- **Authors**: Seung Wook Kim, Yuhao Zhou, Jonah Philion, Antonio Torralba, Sanja Fidler
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: Simulation is a crucial component of any robotic system. In order to simulate correctly, we need to write complex rules of the environment: how dynamic agents behave, and how the actions of each of the agents affect the behavior of others. In this paper, we aim to learn a simulator by simply watching an agent interact with an environment. We focus on graphics games as a proxy of the real environment. We introduce GameGAN, a generative model that learns to visually imitate a desired game by ingesting screenplay and keyboard actions during training. Given a key pressed by the agent, GameGAN "renders" the next screen using a carefully designed generative adversarial network. Our approach offers key advantages over existing work: we design a memory module that builds an internal map of the environment, allowing for the agent to return to previously visited locations with high visual consistency. In addition, GameGAN is able to disentangle static and dynamic components within an image making the behavior of the model more interpretable, and relevant for downstream tasks that require explicit reasoning over dynamic elements. This enables many interesting applications such as swapping different components of the game to build new games that do not exist.



### NENET: An Edge Learnable Network for Link Prediction in Scene Text
- **Arxiv ID**: http://arxiv.org/abs/2005.12147v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.12147v1)
- **Published**: 2020-05-25 14:47:16+00:00
- **Updated**: 2020-05-25 14:47:16+00:00
- **Authors**: Mayank Kumar Singh, Sayan Banerjee, Shubhasis Chaudhuri
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Text detection in scenes based on deep neural networks have shown promising results. Instead of using word bounding box regression, recent state-of-the-art methods have started focusing on character bounding box and pixel-level prediction. This necessitates the need to link adjacent characters, which we propose in this paper using a novel Graph Neural Network (GNN) architecture that allows us to learn both node and edge features as opposed to only the node features under the typical GNN. The main advantage of using GNN for link prediction lies in its ability to connect characters which are spatially separated and have an arbitrary orientation. We show our concept on the well known SynthText dataset, achieving top results as compared to state-of-the-art methods.



### Kernel Self-Attention in Deep Multiple Instance Learning
- **Arxiv ID**: http://arxiv.org/abs/2005.12991v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.12991v2)
- **Published**: 2020-05-25 14:59:13+00:00
- **Updated**: 2021-03-05 12:36:50+00:00
- **Authors**: Dawid Rymarczyk, Adriana Borowa, Jacek Tabor, Bartosz Zieliński
- **Comment**: https://openaccess.thecvf.com/content/WACV2021/papers/Rymarczyk_Kernel_Self-Attention_for_Weakly-Supervised_Image_Classification_Using_Deep_Multiple_Instance_WACV_2021_paper.pdf
- **Journal**: None
- **Summary**: Not all supervised learning problems are described by a pair of a fixed-size input tensor and a label. In some cases, especially in medical image analysis, a label corresponds to a bag of instances (e.g. image patches), and to classify such bag, aggregation of information from all of the instances is needed. There have been several attempts to create a model working with a bag of instances, however, they are assuming that there are no dependencies within the bag and the label is connected to at least one instance. In this work, we introduce Self-Attention Attention-based MIL Pooling (SA-AbMILP) aggregation operation to account for the dependencies between instances. We conduct several experiments on MNIST, histological, microbiological, and retinal databases to show that SA-AbMILP performs better than other models. Additionally, we investigate kernel variations of Self-Attention and their influence on the results.



### DeepSSM: Deep State-Space Model for 3D Human Motion Prediction
- **Arxiv ID**: http://arxiv.org/abs/2005.12155v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.12155v4)
- **Published**: 2020-05-25 15:06:12+00:00
- **Updated**: 2021-12-29 03:44:15+00:00
- **Authors**: Xiaoli Liu, Jianqin Yin, Huaping Liu, Jun Liu
- **Comment**: submitted to IEEE Transactions on Systems Man Cybernetics-Systems
- **Journal**: None
- **Summary**: Predicting future human motion plays a significant role in human-machine interactions for various real-life applications. A unified formulation and multi-order modeling are two critical perspectives for analyzing and representing human motion. In contrast to prior works, we improve the multi-order modeling ability of human motion systems for more accurate predictions by building a deep state-space model (DeepSSM). The DeepSSM utilizes the advantages of both the state-space theory and the deep network. Specifically, we formulate the human motion system as the state-space model of a dynamic system and model the motion system by the state-space theory, offering a unified formulation for diverse human motion systems. Moreover, a novel deep network is designed to parameterize this system, which jointly models the state-state transition and state-observation transition processes. In this way, the state of a system is updated by the multi-order information of a time-varying human motion sequence. Multiple future poses are recursively predicted via the state-observation transition. To further improve the model ability of the system, a novel loss, WT-MPJPE (Weighted Temporal Mean Per Joint Position Error), is introduced to optimize the model. The proposed loss encourages the system to achieve more accurate predictions by increasing weights to the early time steps. The experiments on two benchmark datasets (i.e., Human3.6M and 3DPW) confirm that our method achieves state-of-the-art performance with improved accuracy of at least 2.2mm per joint. The code will be available at: \url{https://github.com/lily2lab/DeepSSM.git}.



### Automating the Surveillance of Mosquito Vectors from Trapped Specimens Using Computer Vision Techniques
- **Arxiv ID**: http://arxiv.org/abs/2005.12188v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.12188v2)
- **Published**: 2020-05-25 15:58:27+00:00
- **Updated**: 2020-07-21 19:58:45+00:00
- **Authors**: Mona Minakshi, Pratool Bharti, Willie B. McClinton III, Jamshidbek Mirzakhalov, Ryan M. Carney, Sriram Chellappan
- **Comment**: None
- **Journal**: None
- **Summary**: Among all animals, mosquitoes are responsible for the most deaths worldwide. Interestingly, not all types of mosquitoes spread diseases, but rather, a select few alone are competent enough to do so. In the case of any disease outbreak, an important first step is surveillance of vectors (i.e., those mosquitoes capable of spreading diseases). To do this today, public health workers lay several mosquito traps in the area of interest. Hundreds of mosquitoes will get trapped. Naturally, among these hundreds, taxonomists have to identify only the vectors to gauge their density. This process today is manual, requires complex expertise/ training, and is based on visual inspection of each trapped specimen under a microscope. It is long, stressful and self-limiting. This paper presents an innovative solution to this problem. Our technique assumes the presence of an embedded camera (similar to those in smart-phones) that can take pictures of trapped mosquitoes. Our techniques proposed here will then process these images to automatically classify the genus and species type. Our CNN model based on Inception-ResNet V2 and Transfer Learning yielded an overall accuracy of 80% in classifying mosquitoes when trained on 25,867 images of 250 trapped mosquito vector specimens captured via many smart-phone cameras. In particular, the accuracy of our model in classifying Aedes aegypti and Anopheles stephensi mosquitoes (both of which are deadly vectors) is amongst the highest. We present important lessons learned and practical impact of our techniques towards the end of the paper.



### JSSR: A Joint Synthesis, Segmentation, and Registration System for 3D Multi-Modal Image Alignment of Large-scale Pathological CT Scans
- **Arxiv ID**: http://arxiv.org/abs/2005.12209v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.12209v3)
- **Published**: 2020-05-25 16:30:02+00:00
- **Updated**: 2020-07-17 18:00:31+00:00
- **Authors**: Fengze Liu, Jinzheng Cai, Yuankai Huo, Chi-Tung Cheng, Ashwin Raju, Dakai Jin, Jing Xiao, Alan Yuille, Le Lu, ChienHung Liao, Adam P Harrison
- **Comment**: accepted to ECCV 2020
- **Journal**: None
- **Summary**: Multi-modal image registration is a challenging problem that is also an important clinical task for many real applications and scenarios. As a first step in analysis, deformable registration among different image modalities is often required in order to provide complementary visual information. During registration, semantic information is key to match homologous points and pixels. Nevertheless, many conventional registration methods are incapable in capturing high-level semantic anatomical dense correspondences. In this work, we propose a novel multi-task learning system, JSSR, based on an end-to-end 3D convolutional neural network that is composed of a generator, a registration and a segmentation component. The system is optimized to satisfy the implicit constraints between different tasks in an unsupervised manner. It first synthesizes the source domain images into the target domain, then an intra-modal registration is applied on the synthesized images and target images. The segmentation module are then applied on the synthesized and target images, providing additional cues based on semantic correspondences. The supervision from another fully-annotated dataset is used to regularize the segmentation. We extensively evaluate JSSR on a large-scale medical image dataset containing 1,485 patient CT imaging studies of four different contrast phases (i.e., 5,940 3D CT scans with pathological livers) on the registration, segmentation and synthesis tasks. The performance is improved after joint training on the registration and segmentation tasks by 0.9% and 1.9% respectively compared to a highly competitive and accurate deep learning baseline. The registration also consistently outperforms conventional state-of-the-art multi-modal registration methods.



### Attention-based Neural Bag-of-Features Learning for Sequence Data
- **Arxiv ID**: http://arxiv.org/abs/2005.12250v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.12250v1)
- **Published**: 2020-05-25 17:51:54+00:00
- **Updated**: 2020-05-25 17:51:54+00:00
- **Authors**: Dat Thanh Tran, Nikolaos Passalis, Anastasios Tefas, Moncef Gabbouj, Alexandros Iosifidis
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose 2D-Attention (2DA), a generic attention formulation for sequence data, which acts as a complementary computation block that can detect and focus on relevant sources of information for the given learning objective. The proposed attention module is incorporated into the recently proposed Neural Bag of Feature (NBoF) model to enhance its learning capacity. Since 2DA acts as a plug-in layer, injecting it into different computation stages of the NBoF model results in different 2DA-NBoF architectures, each of which possesses a unique interpretation. We conducted extensive experiments in financial forecasting, audio analysis as well as medical diagnosis problems to benchmark the proposed formulations in comparison with existing methods, including the widely used Gated Recurrent Units. Our empirical analysis shows that the proposed attention formulations can not only improve performances of NBoF models but also make them resilient to noisy data.



### Neural Topological SLAM for Visual Navigation
- **Arxiv ID**: http://arxiv.org/abs/2005.12256v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2005.12256v2)
- **Published**: 2020-05-25 17:56:29+00:00
- **Updated**: 2020-05-28 22:56:12+00:00
- **Authors**: Devendra Singh Chaplot, Ruslan Salakhutdinov, Abhinav Gupta, Saurabh Gupta
- **Comment**: Published in CVPR 2020. See the project webpage at
  https://devendrachaplot.github.io/projects/Neural-Topological-SLAM
- **Journal**: None
- **Summary**: This paper studies the problem of image-goal navigation which involves navigating to the location indicated by a goal image in a novel previously unseen environment. To tackle this problem, we design topological representations for space that effectively leverage semantics and afford approximate geometric reasoning. At the heart of our representations are nodes with associated semantic features, that are interconnected using coarse geometric information. We describe supervised learning-based algorithms that can build, maintain and use such representations under noisy actuation. Experimental study in visually and physically realistic simulation suggests that our method builds effective representations that capture structural regularities and efficiently solve long-horizon navigation problems. We observe a relative improvement of more than 50% over existing methods that study this task.



### Identity-Preserving Realistic Talking Face Generation
- **Arxiv ID**: http://arxiv.org/abs/2005.12318v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.12318v1)
- **Published**: 2020-05-25 18:08:28+00:00
- **Updated**: 2020-05-25 18:08:28+00:00
- **Authors**: Sanjana Sinha, Sandika Biswas, Brojeshwar Bhowmick
- **Comment**: Accepted in IJCNN 2020
- **Journal**: None
- **Summary**: Speech-driven facial animation is useful for a variety of applications such as telepresence, chatbots, etc. The necessary attributes of having a realistic face animation are 1) audio-visual synchronization (2) identity preservation of the target individual (3) plausible mouth movements (4) presence of natural eye blinks. The existing methods mostly address the audio-visual lip synchronization, and few recent works have addressed the synthesis of natural eye blinks for overall video realism. In this paper, we propose a method for identity-preserving realistic facial animation from speech. We first generate person-independent facial landmarks from audio using DeepSpeech features for invariance to different voices, accents, etc. To add realism, we impose eye blinks on facial landmarks using unsupervised learning and retargets the person-independent landmarks to person-specific landmarks to preserve the identity-related facial structure which helps in the generation of plausible mouth shapes of the target identity. Finally, we use LSGAN to generate the facial texture from person-specific facial landmarks, using an attention mechanism that helps to preserve identity-related texture. An extensive comparison of our proposed method with the current state-of-the-art methods demonstrates a significant improvement in terms of lip synchronization accuracy, image reconstruction quality, sharpness, and identity-preservation. A user study also reveals improved realism of our animation results over the state-of-the-art methods. To the best of our knowledge, this is the first work in speech-driven 2D facial animation that simultaneously addresses all the above-mentioned attributes of a realistic speech-driven face animation.



### SCAN: Learning to Classify Images without Labels
- **Arxiv ID**: http://arxiv.org/abs/2005.12320v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.12320v2)
- **Published**: 2020-05-25 18:12:33+00:00
- **Updated**: 2020-07-03 15:25:54+00:00
- **Authors**: Wouter Van Gansbeke, Simon Vandenhende, Stamatios Georgoulis, Marc Proesmans, Luc Van Gool
- **Comment**: Accepted at ECCV 2020. Includes supplementary. Code and pretrained
  models at https://github.com/wvangansbeke/Unsupervised-Classification
- **Journal**: None
- **Summary**: Can we automatically group images into semantically meaningful clusters when ground-truth annotations are absent? The task of unsupervised image classification remains an important, and open challenge in computer vision. Several recent approaches have tried to tackle this problem in an end-to-end fashion. In this paper, we deviate from recent works, and advocate a two-step approach where feature learning and clustering are decoupled. First, a self-supervised task from representation learning is employed to obtain semantically meaningful features. Second, we use the obtained features as a prior in a learnable clustering approach. In doing so, we remove the ability for cluster learning to depend on low-level features, which is present in current end-to-end learning approaches. Experimental evaluation shows that we outperform state-of-the-art methods by large margins, in particular +26.6% on CIFAR10, +25.0% on CIFAR100-20 and +21.3% on STL10 in terms of classification accuracy. Furthermore, our method is the first to perform well on a large-scale dataset for image classification. In particular, we obtain promising results on ImageNet, and outperform several semi-supervised learning methods in the low-data regime without the use of any ground-truth annotations. The code is made publicly available at https://github.com/wvangansbeke/Unsupervised-Classification.



### Network Bending: Expressive Manipulation of Deep Generative Models
- **Arxiv ID**: http://arxiv.org/abs/2005.12420v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.12420v2)
- **Published**: 2020-05-25 21:48:45+00:00
- **Updated**: 2021-03-12 15:06:56+00:00
- **Authors**: Terence Broad, Frederic Fol Leymarie, Mick Grierson
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a new framework for manipulating and interacting with deep generative models that we call network bending. We present a comprehensive set of deterministic transformations that can be inserted as distinct layers into the computational graph of a trained generative neural network and applied during inference. In addition, we present a novel algorithm for analysing the deep generative model and clustering features based on their spatial activation maps. This allows features to be grouped together based on spatial similarity in an unsupervised fashion. This results in the meaningful manipulation of sets of features that correspond to the generation of a broad array of semantically significant features of the generated images. We outline this framework, demonstrating our results on state-of-the-art deep generative models trained on several image datasets. We show how it allows for the direct manipulation of semantically meaningful aspects of the generative process as well as allowing for a broad range of expressive outcomes.



### Personalized Fashion Recommendation from Personal Social Media Data: An Item-to-Set Metric Learning Approach
- **Arxiv ID**: http://arxiv.org/abs/2005.12439v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2005.12439v1)
- **Published**: 2020-05-25 23:24:24+00:00
- **Updated**: 2020-05-25 23:24:24+00:00
- **Authors**: Haitian Zheng, Kefei Wu, Jong-Hwi Park, Wei Zhu, Jiebo Luo
- **Comment**: 9 pages, 7 figures
- **Journal**: None
- **Summary**: With the growth of online shopping for fashion products, accurate fashion recommendation has become a critical problem. Meanwhile, social networks provide an open and new data source for personalized fashion analysis. In this work, we study the problem of personalized fashion recommendation from social media data, i.e. recommending new outfits to social media users that fit their fashion preferences. To this end, we present an item-to-set metric learning framework that learns to compute the similarity between a set of historical fashion items of a user to a new fashion item. To extract features from multi-modal street-view fashion items, we propose an embedding module that performs multi-modality feature extraction and cross-modality gated fusion. To validate the effectiveness of our approach, we collect a real-world social media dataset. Extensive experiments on the collected dataset show the superior performance of our proposed approach.



### SegAttnGAN: Text to Image Generation with Segmentation Attention
- **Arxiv ID**: http://arxiv.org/abs/2005.12444v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.12444v1)
- **Published**: 2020-05-25 23:56:41+00:00
- **Updated**: 2020-05-25 23:56:41+00:00
- **Authors**: Yuchuan Gou, Qiancheng Wu, Minghao Li, Bo Gong, Mei Han
- **Comment**: Accepted to the AI for Content Creation Workshop at CVPR 2020
- **Journal**: None
- **Summary**: In this paper, we propose a novel generative network (SegAttnGAN) that utilizes additional segmentation information for the text-to-image synthesis task. As the segmentation data introduced to the model provides useful guidance on the generator training, the proposed model can generate images with better realism quality and higher quantitative measures compared with the previous state-of-art methods. We achieved Inception Score of 4.84 on the CUB dataset and 3.52 on the Oxford-102 dataset. Besides, we tested the self-attention SegAttnGAN which uses generated segmentation data instead of masks from datasets for attention and achieved similar high-quality results, suggesting that our model can be adapted for the text-to-image synthesis task.



