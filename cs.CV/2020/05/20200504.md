# Arxiv Papers in cs.CV on 2020-05-04
### AIM 2019 Challenge on Video Temporal Super-Resolution: Methods and Results
- **Arxiv ID**: http://arxiv.org/abs/2005.01233v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.01233v1)
- **Published**: 2020-05-04 01:51:23+00:00
- **Updated**: 2020-05-04 01:51:23+00:00
- **Authors**: Seungjun Nah, Sanghyun Son, Radu Timofte, Kyoung Mu Lee
- **Comment**: Published in ICCV 2019 Workshop (Advances in Image Manipulation)
- **Journal**: 2019 IEEE/CVF International Conference on Computer Vision Workshop
  (ICCVW), Seoul, Korea (South), 2019, pp. 3388-3398
- **Summary**: Videos contain various types and strengths of motions that may look unnaturally discontinuous in time when the recorded frame rate is low. This paper reviews the first AIM challenge on video temporal super-resolution (frame interpolation) with a focus on the proposed solutions and results. From low-frame-rate (15 fps) video sequences, the challenge participants are asked to submit higher-framerate (60 fps) video sequences by estimating temporally intermediate frames. We employ the REDS VTSR dataset derived from diverse videos captured in a hand-held camera for training and evaluation purposes. The competition had 62 registered participants, and a total of 8 teams competed in the final testing phase. The challenge winning methods achieve the state-of-the-art in video temporal superresolution.



### One-Shot Image Classification by Learning to Restore Prototypes
- **Arxiv ID**: http://arxiv.org/abs/2005.01234v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.01234v1)
- **Published**: 2020-05-04 02:11:30+00:00
- **Updated**: 2020-05-04 02:11:30+00:00
- **Authors**: Wanqi Xue, Wei Wang
- **Comment**: Published as a conference paper in AAAI 2020
- **Journal**: None
- **Summary**: One-shot image classification aims to train image classifiers over the dataset with only one image per category. It is challenging for modern deep neural networks that typically require hundreds or thousands of images per class. In this paper, we adopt metric learning for this problem, which has been applied for few- and many-shot image classification by comparing the distance between the test image and the center of each class in the feature space. However, for one-shot learning, the existing metric learning approaches would suffer poor performance because the single training image may not be representative of the class. For example, if the image is far away from the class center in the feature space, the metric-learning based algorithms are unlikely to make correct predictions for the test images because the decision boundary is shifted by this noisy image. To address this issue, we propose a simple yet effective regression model, denoted by RestoreNet, which learns a class agnostic transformation on the image feature to move the image closer to the class center in the feature space. Experiments demonstrate that RestoreNet obtains superior performance over the state-of-the-art methods on a broad range of datasets. Moreover, RestoreNet can be easily combined with other methods to achieve further improvement.



### Visual Question Answering with Prior Class Semantics
- **Arxiv ID**: http://arxiv.org/abs/2005.01239v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.01239v1)
- **Published**: 2020-05-04 02:46:31+00:00
- **Updated**: 2020-05-04 02:46:31+00:00
- **Authors**: Violetta Shevchenko, Damien Teney, Anthony Dick, Anton van den Hengel
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel mechanism to embed prior knowledge in a model for visual question answering. The open-set nature of the task is at odds with the ubiquitous approach of training of a fixed classifier. We show how to exploit additional information pertaining to the semantics of candidate answers. We extend the answer prediction process with a regression objective in a semantic space, in which we project candidate answers using prior knowledge derived from word embeddings. We perform an extensive study of learned representations with the GQA dataset, revealing that important semantic information is captured in the relations between embeddings in the answer space. Our method brings improvements in consistency and accuracy over a range of question types. Experiments with novel answers, unseen during training, indicate the method's potential for open-set prediction.



### NTIRE 2020 Challenge on Image and Video Deblurring
- **Arxiv ID**: http://arxiv.org/abs/2005.01244v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.01244v2)
- **Published**: 2020-05-04 03:17:30+00:00
- **Updated**: 2020-05-10 03:39:13+00:00
- **Authors**: Seungjun Nah, Sanghyun Son, Radu Timofte, Kyoung Mu Lee
- **Comment**: To be published in CVPR 2020 Workshop (New Trends in Image
  Restoration and Enhancement)
- **Journal**: None
- **Summary**: Motion blur is one of the most common degradation artifacts in dynamic scene photography. This paper reviews the NTIRE 2020 Challenge on Image and Video Deblurring. In this challenge, we present the evaluation results from 3 competition tracks as well as the proposed solutions. Track 1 aims to develop single-image deblurring methods focusing on restoration quality. On Track 2, the image deblurring methods are executed on a mobile platform to find the balance of the running speed and the restoration accuracy. Track 3 targets developing video deblurring methods that exploit the temporal relation between input frames. In each competition, there were 163, 135, and 102 registered participants and in the final testing phase, 9, 4, and 7 teams competed. The winning methods demonstrate the state-ofthe-art performance on image and video deblurring tasks.



### Learning of Art Style Using AI and Its Evaluation Based on Psychological Experiments
- **Arxiv ID**: http://arxiv.org/abs/2005.02220v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.02220v1)
- **Published**: 2020-05-04 07:19:37+00:00
- **Updated**: 2020-05-04 07:19:37+00:00
- **Authors**: Mai Cong Hung, Ryohei Nakatsu, Naoko Tosa, Takashi Kusumi, Koji Koyamada
- **Comment**: None
- **Journal**: None
- **Summary**: GANs (Generative adversarial networks) is a new AI technology that can perform deep learning with less training data and has the capability of achieving transformation between two image sets. Using GAN we have carried out a comparison between several art sets with different art style. We have prepared several image sets; a flower photo set (A), an art image set (B1) of Impressionism drawings, an art image set of abstract paintings (B2), an art image set of Chinese figurative paintings, (B3), and an art image set of abstract images (B4) created by Naoko Tosa, one of the authors. Transformation between set A to each of B was carried out using GAN and four image sets (B1, B2, B3, B4) was obtained. Using these four image sets we have carried out psychological experiment by asking subjects consisting of 23 students to fill in questionnaires. By analyzing the obtained questionnaires, we have found the followings. Abstract drawings and figurative drawings are clearly judged to be different. Figurative drawings in West and East were judged to be similar. Abstract images by Naoko Tosa were judged as similar to Western abstract images. These results show that AI could be used as an analysis tool to reveal differences between art genres.



### CDC: Classification Driven Compression for Bandwidth Efficient Edge-Cloud Collaborative Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2005.02177v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.DC, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.02177v1)
- **Published**: 2020-05-04 07:40:32+00:00
- **Updated**: 2020-05-04 07:40:32+00:00
- **Authors**: Yuanrui Dong, Peng Zhao, Hanqiao Yu, Cong Zhao, Shusen Yang
- **Comment**: Accepted by IJCAI 2020
- **Journal**: None
- **Summary**: The emerging edge-cloud collaborative Deep Learning (DL) paradigm aims at improving the performance of practical DL implementations in terms of cloud bandwidth consumption, response latency, and data privacy preservation. Focusing on bandwidth efficient edge-cloud collaborative training of DNN-based classifiers, we present CDC, a Classification Driven Compression framework that reduces bandwidth consumption while preserving classification accuracy of edge-cloud collaborative DL. Specifically, to reduce bandwidth consumption, for resource-limited edge servers, we develop a lightweight autoencoder with a classification guidance for compression with classification driven feature preservation, which allows edges to only upload the latent code of raw data for accurate global training on the Cloud. Additionally, we design an adjustable quantization scheme adaptively pursuing the tradeoff between bandwidth consumption and classification accuracy under different network conditions, where only fine-tuning is required for rapid compression ratio adjustment. Results of extensive experiments demonstrate that, compared with DNN training with raw data, CDC consumes 14.9 times less bandwidth with an accuracy loss no more than 1.06%, and compared with DNN training with data compressed by AE without guidance, CDC introduces at least 100% lower accuracy loss.



### A Model-driven Deep Neural Network for Single Image Rain Removal
- **Arxiv ID**: http://arxiv.org/abs/2005.01333v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.01333v1)
- **Published**: 2020-05-04 09:13:25+00:00
- **Updated**: 2020-05-04 09:13:25+00:00
- **Authors**: Hong Wang, Qi Xie, Qian Zhao, Deyu Meng
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning (DL) methods have achieved state-of-the-art performance in the task of single image rain removal. Most of current DL architectures, however, are still lack of sufficient interpretability and not fully integrated with physical structures inside general rain streaks. To this issue, in this paper, we propose a model-driven deep neural network for the task, with fully interpretable network structures. Specifically, based on the convolutional dictionary learning mechanism for representing rain, we propose a novel single image deraining model and utilize the proximal gradient descent technique to design an iterative algorithm only containing simple operators for solving the model. Such a simple implementation scheme facilitates us to unfold it into a new deep network architecture, called rain convolutional dictionary network (RCDNet), with almost every network module one-to-one corresponding to each operation involved in the algorithm. By end-to-end training the proposed RCDNet, all the rain kernels and proximal operators can be automatically extracted, faithfully characterizing the features of both rain and clean background layers, and thus naturally lead to its better deraining performance, especially in real scenarios. Comprehensive experiments substantiate the superiority of the proposed network, especially its well generality to diverse testing scenarios and good interpretability for all its modules, as compared with state-of-the-arts both visually and quantitatively. The source codes are available at \url{https://github.com/hongwang01/RCDNet}.



### Comparison of Image Quality Models for Optimization of Image Processing Systems
- **Arxiv ID**: http://arxiv.org/abs/2005.01338v3
- **DOI**: 10.1007/s11263-020-01419-7
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.01338v3)
- **Published**: 2020-05-04 09:26:40+00:00
- **Updated**: 2020-12-08 12:59:48+00:00
- **Authors**: Keyan Ding, Kede Ma, Shiqi Wang, Eero P. Simoncelli
- **Comment**: None
- **Journal**: International Journal of Computer Vision, 2021
- **Summary**: The performance of objective image quality assessment (IQA) models has been evaluated primarily by comparing model predictions to human quality judgments. Perceptual datasets gathered for this purpose have provided useful benchmarks for improving IQA methods, but their heavy use creates a risk of overfitting. Here, we perform a large-scale comparison of IQA models in terms of their use as objectives for the optimization of image processing algorithms. Specifically, we use eleven full-reference IQA models to train deep neural networks for four low-level vision tasks: denoising, deblurring, super-resolution, and compression. Subjective testing on the optimized images allows us to rank the competing models in terms of their perceptual performance, elucidate their relative advantages and disadvantages in these tasks, and propose a set of desirable properties for incorporation into future IQA models.



### Tamed Warping Network for High-Resolution Semantic Video Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2005.01344v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.01344v4)
- **Published**: 2020-05-04 09:36:03+00:00
- **Updated**: 2023-07-11 08:54:31+00:00
- **Authors**: Songyuan Li, Junyi Feng, Xi Li
- **Comment**: None
- **Journal**: None
- **Summary**: Recent approaches for fast semantic video segmentation have reduced redundancy by warping feature maps across adjacent frames, greatly speeding up the inference phase. However, the accuracy drops seriously owing to the errors incurred by warping. In this paper, we propose a novel framework and design a simple and effective correction stage after warping. Specifically, we build a non-key-frame CNN, fusing warped context features with current spatial details. Based on the feature fusion, our Context Feature Rectification~(CFR) module learns the model's difference from a per-frame model to correct the warped features. Furthermore, our Residual-Guided Attention~(RGA) module utilizes the residual maps in the compressed domain to help CRF focus on error-prone regions. Results on Cityscapes show that the accuracy significantly increases from $67.3\%$ to $71.6\%$, and the speed edges down from $65.5$ FPS to $61.8$ FPS at a resolution of $1024\times 2048$. For non-rigid categories, e.g., ``human'' and ``object'', the improvements are even higher than 18 percentage points.



### Anchors Based Method for Fingertips Position Estimation from a Monocular RGB Image using Deep Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2005.01351v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.01351v2)
- **Published**: 2020-05-04 09:45:56+00:00
- **Updated**: 2020-05-14 06:57:58+00:00
- **Authors**: Purnendu Mishra, Kishor Sarawadekar
- **Comment**: 10 pages, 10 figures
- **Journal**: None
- **Summary**: In Virtual, augmented, and mixed reality, the use of hand gestures is increasingly becoming popular to reduce the difference between the virtual and real world. The precise location of the fingertip is essential/crucial for a seamless experience. Much of the research work is based on using depth information for the estimation of the fingertips position. However, most of the work using RGB images for fingertips detection is limited to a single finger. The detection of multiple fingertips from a single RGB image is very challenging due to various factors. In this paper, we propose a deep neural network (DNN) based methodology to estimate the fingertips position. We christened this methodology as an Anchor based Fingertips Position Estimation (ABFPE), and it is a two-step process. The fingertips location is estimated using regression by computing the difference in the location of a fingertip from the nearest anchor point. The proposed framework performs the best with limited dependence on hand detection results. In our experiments on the SCUT-Ego-Gesture dataset, we achieved the fingertips detection error of 2.3552 pixels on a video frame with a resolution of $640 \times 480$ and about $92.98\%$ of test images have average pixel errors of five pixels.



### Monitoring COVID-19 social distancing with person detection and tracking via fine-tuned YOLO v3 and Deepsort techniques
- **Arxiv ID**: http://arxiv.org/abs/2005.01385v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.01385v4)
- **Published**: 2020-05-04 10:58:20+00:00
- **Updated**: 2021-04-27 05:08:05+00:00
- **Authors**: Narinder Singh Punn, Sanjay Kumar Sonbhadra, Sonali Agarwal, Gaurav Rai
- **Comment**: None
- **Journal**: None
- **Summary**: The rampant coronavirus disease 2019 (COVID-19) has brought global crisis with its deadly spread to more than 180 countries, and about 3,519,901 confirmed cases along with 247,630 deaths globally as on May 4, 2020. The absence of any active therapeutic agents and the lack of immunity against COVID-19 increases the vulnerability of the population. Since there are no vaccines available, social distancing is the only feasible approach to fight against this pandemic. Motivated by this notion, this article proposes a deep learning based framework for automating the task of monitoring social distancing using surveillance video. The proposed framework utilizes the YOLO v3 object detection model to segregate humans from the background and Deepsort approach to track the identified people with the help of bounding boxes and assigned IDs. The results of the YOLO v3 model are further compared with other popular state-of-the-art models, e.g. faster region-based CNN (convolution neural network) and single shot detector (SSD) in terms of mean average precision (mAP), frames per second (FPS) and loss values defined by object classification and localization. Later, the pairwise vectorized L2 norm is computed based on the three-dimensional feature space obtained by using the centroid coordinates and dimensions of the bounding box. The violation index term is proposed to quantize the non adoption of social distancing protocol. From the experimental analysis, it is observed that the YOLO v3 with Deepsort tracking scheme displayed best results with balanced mAP and FPS score to monitor the social distancing in real-time.



### Does Visual Self-Supervision Improve Learning of Speech Representations for Emotion Recognition?
- **Arxiv ID**: http://arxiv.org/abs/2005.01400v3
- **DOI**: 10.1109/TAFFC.2021.3062406
- **Categories**: **eess.AS**, cs.CL, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.01400v3)
- **Published**: 2020-05-04 11:33:40+00:00
- **Updated**: 2021-03-18 11:35:38+00:00
- **Authors**: Abhinav Shukla, Stavros Petridis, Maja Pantic
- **Comment**: Accepted for publication in IEEE Transactions on Affective Computing;
  v3: Publication-ready version including additional experiments and discussion
- **Journal**: None
- **Summary**: Self-supervised learning has attracted plenty of recent research interest. However, most works for self-supervision in speech are typically unimodal and there has been limited work that studies the interaction between audio and visual modalities for cross-modal self-supervision. This work (1) investigates visual self-supervision via face reconstruction to guide the learning of audio representations; (2) proposes an audio-only self-supervision approach for speech representation learning; (3) shows that a multi-task combination of the proposed visual and audio self-supervision is beneficial for learning richer features that are more robust in noisy conditions; (4) shows that self-supervised pretraining can outperform fully supervised training and is especially useful to prevent overfitting on smaller sized datasets. We evaluate our learned audio representations for discrete emotion recognition, continuous affect recognition and automatic speech recognition. We outperform existing self-supervised methods for all tested downstream tasks. Our results demonstrate the potential of visual self-supervision for audio feature learning and suggest that joint visual and audio self-supervision leads to more informative audio representations for speech and emotion recognition.



### Correlating Edge, Pose with Parsing
- **Arxiv ID**: http://arxiv.org/abs/2005.01431v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.01431v1)
- **Published**: 2020-05-04 12:39:13+00:00
- **Updated**: 2020-05-04 12:39:13+00:00
- **Authors**: Ziwei Zhang, Chi Su, Liang Zheng, Xiaodong Xie
- **Comment**: CVPR2020
- **Journal**: None
- **Summary**: According to existing studies, human body edge and pose are two beneficial factors to human parsing. The effectiveness of each of the high-level features (edge and pose) is confirmed through the concatenation of their features with the parsing features. Driven by the insights, this paper studies how human semantic boundaries and keypoint locations can jointly improve human parsing. Compared with the existing practice of feature concatenation, we find that uncovering the correlation among the three factors is a superior way of leveraging the pivotal contextual cues provided by edges and poses. To capture such correlations, we propose a Correlation Parsing Machine (CorrPM) employing a heterogeneous non-local block to discover the spatial affinity among feature maps from the edge, pose and parsing. The proposed CorrPM allows us to report new state-of-the-art accuracy on three human parsing datasets. Importantly, comparative studies confirm the advantages of feature correlation over the concatenation.



### Automated eye disease classification method from anterior eye image using anatomical structure focused image classification technique
- **Arxiv ID**: http://arxiv.org/abs/2005.01433v1
- **DOI**: 10.1117/12.2549951
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.01433v1)
- **Published**: 2020-05-04 12:42:54+00:00
- **Updated**: 2020-05-04 12:42:54+00:00
- **Authors**: Masahiro Oda, Takefumi Yamaguchi, Hideki Fukuoka, Yuta Ueno, Kensaku Mori
- **Comment**: Accepted paper as a poster presentation at SPIE Medical Imaging 2020,
  Houston, TX, USA
- **Journal**: Proceedings of SPIE Medical Imaging 2020: Computer-Aided
  Diagnosis, Vol.11314, 1131446
- **Summary**: This paper presents an automated classification method of infective and non-infective diseases from anterior eye images. Treatments for cases of infective and non-infective diseases are different. Distinguishing them from anterior eye images is important to decide a treatment plan. Ophthalmologists distinguish them empirically. Quantitative classification of them based on computer assistance is necessary. We propose an automated classification method of anterior eye images into cases of infective or non-infective disease. Anterior eye images have large variations of the eye position and brightness of illumination. This makes the classification difficult. If we focus on the cornea, positions of opacified areas in the corneas are different between cases of the infective and non-infective diseases. Therefore, we solve the anterior eye image classification task by using an object detection approach targeting the cornea. This approach can be said as "anatomical structure focused image classification". We use the YOLOv3 object detection method to detect corneas of infective disease and corneas of non-infective disease. The detection result is used to define a classification result of a image. In our experiments using anterior eye images, 88.3% of images were correctly classified by the proposed method.



### Stochastic Sparse Subspace Clustering
- **Arxiv ID**: http://arxiv.org/abs/2005.01449v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.01449v1)
- **Published**: 2020-05-04 13:09:17+00:00
- **Updated**: 2020-05-04 13:09:17+00:00
- **Authors**: Ying Chen, Chun-Guang Li, Chong You
- **Comment**: 16 pages, 9 figures and 8 tables. This work is accepted by IEEE
  Conference on Computer Vision and Pattern Recognition (CVPR) 2020
- **Journal**: None
- **Summary**: State-of-the-art subspace clustering methods are based on self-expressive model, which represents each data point as a linear combination of other data points. By enforcing such representation to be sparse, sparse subspace clustering is guaranteed to produce a subspace-preserving data affinity where two points are connected only if they are from the same subspace. On the other hand, however, data points from the same subspace may not be well-connected, leading to the issue of over-segmentation. We introduce dropout to address the issue of over-segmentation, which is based on randomly dropping out data points in self-expressive model. In particular, we show that dropout is equivalent to adding a squared $\ell_2$ norm regularization on the representation coefficients, therefore induces denser solutions. Then, we reformulate the optimization problem as a consensus problem over a set of small-scale subproblems. This leads to a scalable and flexible sparse subspace clustering approach, termed Stochastic Sparse Subspace Clustering, which can effectively handle large scale datasets. Extensive experiments on synthetic data and real world datasets validate the efficiency and effectiveness of our proposal.



### CARRADA Dataset: Camera and Automotive Radar with Range-Angle-Doppler Annotations
- **Arxiv ID**: http://arxiv.org/abs/2005.01456v6
- **DOI**: None
- **Categories**: **cs.CV**, I.2.10; I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/2005.01456v6)
- **Published**: 2020-05-04 13:14:29+00:00
- **Updated**: 2021-05-26 13:52:09+00:00
- **Authors**: A. Ouaknine, A. Newson, J. Rebut, F. Tupin, P. Pérez
- **Comment**: 9 pages, 5 figues. Accepted at ICPR 2020. Erratum: results in Table
  III have been updated since the ICPR proceedings, models are selected using
  the PP metric instead of the previously used PR metric
- **Journal**: None
- **Summary**: High quality perception is essential for autonomous driving (AD) systems. To reach the accuracy and robustness that are required by such systems, several types of sensors must be combined. Currently, mostly cameras and laser scanners (lidar) are deployed to build a representation of the world around the vehicle. While radar sensors have been used for a long time in the automotive industry, they are still under-used for AD despite their appealing characteristics (notably, their ability to measure the relative speed of obstacles and to operate even in adverse weather conditions). To a large extent, this situation is due to the relative lack of automotive datasets with real radar signals that are both raw and annotated. In this work, we introduce CARRADA, a dataset of synchronized camera and radar recordings with range-angle-Doppler annotations. We also present a semi-automatic annotation approach, which was used to annotate the dataset, and a radar semantic segmentation baseline, which we evaluate on several metrics. Both our code and dataset are available online.



### Spiking Neural Networks Hardware Implementations and Challenges: a Survey
- **Arxiv ID**: http://arxiv.org/abs/2005.01467v1
- **DOI**: 10.1145/3304103
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.01467v1)
- **Published**: 2020-05-04 13:24:00+00:00
- **Updated**: 2020-05-04 13:24:00+00:00
- **Authors**: Maxence Bouvier, Alexandre Valentian, Thomas Mesquida, François Rummens, Marina Reyboz, Elisa Vianello, Edith Beigné
- **Comment**: Pre-print version of the file authorized for publication
- **Journal**: None
- **Summary**: Neuromorphic computing is henceforth a major research field for both academic and industrial actors. As opposed to Von Neumann machines, brain-inspired processors aim at bringing closer the memory and the computational elements to efficiently evaluate machine-learning algorithms. Recently, Spiking Neural Networks, a generation of cognitive algorithms employing computational primitives mimicking neuron and synapse operational principles, have become an important part of deep learning. They are expected to improve the computational performance and efficiency of neural networks, but are best suited for hardware able to support their temporal dynamics. In this survey, we present the state of the art of hardware implementations of spiking neural networks and the current trends in algorithm elaboration from model selection to training mechanisms. The scope of existing solutions is extensive; we thus present the general framework and study on a case-by-case basis the relevant particularities. We describe the strategies employed to leverage the characteristics of these event-driven algorithms at the hardware level and discuss their related advantages and challenges.



### On the Benefits of Models with Perceptually-Aligned Gradients
- **Arxiv ID**: http://arxiv.org/abs/2005.01499v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.01499v1)
- **Published**: 2020-05-04 14:05:38+00:00
- **Updated**: 2020-05-04 14:05:38+00:00
- **Authors**: Gunjan Aggarwal, Abhishek Sinha, Nupur Kumari, Mayank Singh
- **Comment**: Accepted at ICLR 2020 Workshop: Towards Trustworthy ML
- **Journal**: None
- **Summary**: Adversarial robust models have been shown to learn more robust and interpretable features than standard trained models. As shown in [\cite{tsipras2018robustness}], such robust models inherit useful interpretable properties where the gradient aligns perceptually well with images, and adding a large targeted adversarial perturbation leads to an image resembling the target class. We perform experiments to show that interpretable and perceptually aligned gradients are present even in models that do not show high robustness to adversarial attacks. Specifically, we perform adversarial training with attack for different max-perturbation bound. Adversarial training with low max-perturbation bound results in models that have interpretable features with only slight drop in performance over clean samples. In this paper, we leverage models with interpretable perceptually-aligned features and show that adversarial training with low max-perturbation bound can improve the performance of models for zero-shot and weakly supervised localization tasks.



### Rolling-Unrolling LSTMs for Action Anticipation from First-Person Video
- **Arxiv ID**: http://arxiv.org/abs/2005.02190v2
- **DOI**: 10.1109/TPAMI.2020.2992889
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.02190v2)
- **Published**: 2020-05-04 14:13:41+00:00
- **Updated**: 2020-05-08 13:56:58+00:00
- **Authors**: Antonino Furnari, Giovanni Maria Farinella
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1905.09035
- **Journal**: Published in IEEE Transaction on Pattern Analysis and Machine
  Interaction, 2020
- **Summary**: In this paper, we tackle the problem of egocentric action anticipation, i.e., predicting what actions the camera wearer will perform in the near future and which objects they will interact with. Specifically, we contribute Rolling-Unrolling LSTM, a learning architecture to anticipate actions from egocentric videos. The method is based on three components: 1) an architecture comprised of two LSTMs to model the sub-tasks of summarizing the past and inferring the future, 2) a Sequence Completion Pre-Training technique which encourages the LSTMs to focus on the different sub-tasks, and 3) a Modality ATTention (MATT) mechanism to efficiently fuse multi-modal predictions performed by processing RGB frames, optical flow fields and object-based features. The proposed approach is validated on EPIC-Kitchens, EGTEA Gaze+ and ActivityNet. The experiments show that the proposed architecture is state-of-the-art in the domain of egocentric videos, achieving top performances in the 2019 EPIC-Kitchens egocentric action anticipation challenge. The approach also achieves competitive performance on ActivityNet with respect to methods not based on unsupervised pre-training and generalizes to the tasks of early action recognition and action recognition. To encourage research on this challenging topic, we made our code, trained models, and pre-extracted features available at our web page: http://iplab.dmi.unict.it/rulstm.



### The Newspaper Navigator Dataset: Extracting And Analyzing Visual Content from 16 Million Historic Newspaper Pages in Chronicling America
- **Arxiv ID**: http://arxiv.org/abs/2005.01583v1
- **DOI**: None
- **Categories**: **cs.IR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.01583v1)
- **Published**: 2020-05-04 15:51:13+00:00
- **Updated**: 2020-05-04 15:51:13+00:00
- **Authors**: Benjamin Charles Germain Lee, Jaime Mears, Eileen Jakeway, Meghan Ferriter, Chris Adams, Nathan Yarasavage, Deborah Thomas, Kate Zwaard, Daniel S. Weld
- **Comment**: 14 pages, 5 figures
- **Journal**: None
- **Summary**: Chronicling America is a product of the National Digital Newspaper Program, a partnership between the Library of Congress and the National Endowment for the Humanities to digitize historic newspapers. Over 16 million pages of historic American newspapers have been digitized for Chronicling America to date, complete with high-resolution images and machine-readable METS/ALTO OCR. Of considerable interest to Chronicling America users is a semantified corpus, complete with extracted visual content and headlines. To accomplish this, we introduce a visual content recognition model trained on bounding box annotations of photographs, illustrations, maps, comics, and editorial cartoons collected as part of the Library of Congress's Beyond Words crowdsourcing initiative and augmented with additional annotations including those of headlines and advertisements. We describe our pipeline that utilizes this deep learning model to extract 7 classes of visual content: headlines, photographs, illustrations, maps, comics, editorial cartoons, and advertisements, complete with textual content such as captions derived from the METS/ALTO OCR, as well as image embeddings for fast image similarity querying. We report the results of running the pipeline on 16.3 million pages from the Chronicling America corpus and describe the resulting Newspaper Navigator dataset, the largest dataset of extracted visual content from historic newspapers ever produced. The Newspaper Navigator dataset, finetuned visual content recognition model, and all source code are placed in the public domain for unrestricted re-use.



### MorphoCluster: Efficient Annotation of Plankton images by Clustering
- **Arxiv ID**: http://arxiv.org/abs/2005.01595v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.01595v1)
- **Published**: 2020-05-04 16:08:03+00:00
- **Updated**: 2020-05-04 16:08:03+00:00
- **Authors**: Simon-Martin Schröder, Rainer Kiko, Reinhard Koch
- **Comment**: 27 pages, 11 figures. Submitted to MDPI Sensors
- **Journal**: None
- **Summary**: In this work, we present MorphoCluster, a software tool for data-driven, fast and accurate annotation of large image data sets. While already having surpassed the annotation rate of human experts, volume and complexity of marine data will continue to increase in the coming years. Still, this data requires interpretation. MorphoCluster augments the human ability to discover patterns and perform object classification in large amounts of data by embedding unsupervised clustering in an interactive process. By aggregating similar images into clusters, our novel approach to image annotation increases consistency, multiplies the throughput of an annotator and allows experts to adapt the granularity of their sorting scheme to the structure in the data. By sorting a set of 1.2M objects into 280 data-driven classes in 71 hours (16k objects per hour), with 90% of these classes having a precision of 0.889 or higher. This shows that MorphoCluster is at the same time fast, accurate and consistent, provides a fine-grained and data-driven classification and enables novelty detection. MorphoCluster is available as open-source software at https://github.com/morphocluster.



### VisualEchoes: Spatial Image Representation Learning through Echolocation
- **Arxiv ID**: http://arxiv.org/abs/2005.01616v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2005.01616v2)
- **Published**: 2020-05-04 16:16:58+00:00
- **Updated**: 2020-07-17 17:13:38+00:00
- **Authors**: Ruohan Gao, Changan Chen, Ziad Al-Halah, Carl Schissler, Kristen Grauman
- **Comment**: Appears in ECCV 2020
- **Journal**: None
- **Summary**: Several animal species (e.g., bats, dolphins, and whales) and even visually impaired humans have the remarkable ability to perform echolocation: a biological sonar used to perceive spatial layout and locate objects in the world. We explore the spatial cues contained in echoes and how they can benefit vision tasks that require spatial reasoning. First we capture echo responses in photo-realistic 3D indoor scene environments. Then we propose a novel interaction-based representation learning framework that learns useful visual features via echolocation. We show that the learned image features are useful for multiple downstream vision tasks requiring spatial reasoning---monocular depth estimation, surface normal estimation, and visual navigation---with results comparable or even better than heavily supervised pre-training. Our work opens a new path for representation learning for embodied agents, where supervision comes from interacting with the physical world.



### Ego-motion and Surrounding Vehicle State Estimation Using a Monocular Camera
- **Arxiv ID**: http://arxiv.org/abs/2005.01632v3
- **DOI**: 10.1109/IVS.2019.8814037
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.01632v3)
- **Published**: 2020-05-04 16:41:38+00:00
- **Updated**: 2020-05-06 00:52:46+00:00
- **Authors**: Jun Hayakawa, Behzad Dariush
- **Comment**: None
- **Journal**: 2019 IEEE Intelligent Vehicles Symposium (IV)
- **Summary**: Understanding ego-motion and surrounding vehicle state is essential to enable automated driving and advanced driving assistance technologies. Typical approaches to solve this problem use fusion of multiple sensors such as LiDAR, camera, and radar to recognize surrounding vehicle state, including position, velocity, and orientation. Such sensing modalities are overly complex and costly for production of personal use vehicles. In this paper, we propose a novel machine learning method to estimate ego-motion and surrounding vehicle state using a single monocular camera. Our approach is based on a combination of three deep neural networks to estimate the 3D vehicle bounding box, depth, and optical flow from a sequence of images. The main contribution of this paper is a new framework and algorithm that integrates these three networks in order to estimate the ego-motion and surrounding vehicle state. To realize more accurate 3D position estimation, we address ground plane correction in real-time. The efficacy of the proposed method is demonstrated through experimental evaluations that compare our results to ground truth data available from other sensors including Can-Bus and LiDAR.



### Words aren't enough, their order matters: On the Robustness of Grounding Visual Referring Expressions
- **Arxiv ID**: http://arxiv.org/abs/2005.01655v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.01655v1)
- **Published**: 2020-05-04 17:09:15+00:00
- **Updated**: 2020-05-04 17:09:15+00:00
- **Authors**: Arjun R Akula, Spandana Gella, Yaser Al-Onaizan, Song-Chun Zhu, Siva Reddy
- **Comment**: ACL 2020
- **Journal**: None
- **Summary**: Visual referring expression recognition is a challenging task that requires natural language understanding in the context of an image. We critically examine RefCOCOg, a standard benchmark for this task, using a human study and show that 83.7% of test instances do not require reasoning on linguistic structure, i.e., words are enough to identify the target object, the word order doesn't matter. To measure the true progress of existing models, we split the test set into two sets, one which requires reasoning on linguistic structure and the other which doesn't. Additionally, we create an out-of-distribution dataset Ref-Adv by asking crowdworkers to perturb in-domain examples such that the target object changes. Using these datasets, we empirically show that existing methods fail to exploit linguistic structure and are 12% to 23% lower in performance than the established progress for this task. We also propose two methods, one based on contrastive learning and the other based on multi-task learning, to increase the robustness of ViLBERT, the current state-of-the-art model for this task. Our datasets are publicly available at https://github.com/aws/aws-refcocog-adv



### Group Equivariant Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2005.01683v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.01683v2)
- **Published**: 2020-05-04 17:38:49+00:00
- **Updated**: 2021-03-30 18:00:21+00:00
- **Authors**: Neel Dey, Antong Chen, Soheil Ghafurian
- **Comment**: Accepted by the International Conference on Learning Representations
  (ICLR) 2021
- **Journal**: None
- **Summary**: Recent improvements in generative adversarial visual synthesis incorporate real and fake image transformation in a self-supervised setting, leading to increased stability and perceptual fidelity. However, these approaches typically involve image augmentations via additional regularizers in the GAN objective and thus spend valuable network capacity towards approximating transformation equivariance instead of their desired task. In this work, we explicitly incorporate inductive symmetry priors into the network architectures via group-equivariant convolutional networks. Group-convolutions have higher expressive power with fewer samples and lead to better gradient feedback between generator and discriminator. We show that group-equivariance integrates seamlessly with recent techniques for GAN training across regularizers, architectures, and loss functions. We demonstrate the utility of our methods for conditional synthesis by improving generation in the limited data regime across symmetric imaging datasets and even find benefits for natural images with preferred orientation.



### How to Train Your Energy-Based Model for Regression
- **Arxiv ID**: http://arxiv.org/abs/2005.01698v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.01698v2)
- **Published**: 2020-05-04 17:55:01+00:00
- **Updated**: 2020-08-14 10:08:52+00:00
- **Authors**: Fredrik K. Gustafsson, Martin Danelljan, Radu Timofte, Thomas B. Schön
- **Comment**: BMVC 2020. Code is available at
  https://github.com/fregu856/ebms_regression
- **Journal**: None
- **Summary**: Energy-based models (EBMs) have become increasingly popular within computer vision in recent years. While they are commonly employed for generative image modeling, recent work has applied EBMs also for regression tasks, achieving state-of-the-art performance on object detection and visual tracking. Training EBMs is however known to be challenging. While a variety of different techniques have been explored for generative modeling, the application of EBMs to regression is not a well-studied problem. How EBMs should be trained for best possible regression performance is thus currently unclear. We therefore accept the task of providing the first detailed study of this problem. To that end, we propose a simple yet highly effective extension of noise contrastive estimation, and carefully compare its performance to six popular methods from literature on the tasks of 1D regression and object detection. The results of this comparison suggest that our training method should be considered the go-to approach. We also apply our method to the visual tracking task, achieving state-of-the-art performance on five datasets. Notably, our tracker achieves 63.7% AUC on LaSOT and 78.7% Success on TrackingNet. Code is available at https://github.com/fregu856/ebms_regression.



### Transforming and Projecting Images into Class-conditional Generative Networks
- **Arxiv ID**: http://arxiv.org/abs/2005.01703v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.01703v2)
- **Published**: 2020-05-04 17:57:47+00:00
- **Updated**: 2020-08-27 18:10:52+00:00
- **Authors**: Minyoung Huh, Richard Zhang, Jun-Yan Zhu, Sylvain Paris, Aaron Hertzmann
- **Comment**: Accepted to ECCV2020 (oral)
- **Journal**: None
- **Summary**: We present a method for projecting an input image into the space of a class-conditional generative neural network. We propose a method that optimizes for transformation to counteract the model biases in generative neural networks. Specifically, we demonstrate that one can solve for image translation, scale, and global color transformation, during the projection optimization to address the object-center bias and color bias of a Generative Adversarial Network. This projection process poses a difficult optimization problem, and purely gradient-based optimizations fail to find good solutions. We describe a hybrid optimization strategy that finds good projections by estimating transformations and class parameters. We show the effectiveness of our method on real images and further demonstrate how the corresponding projections lead to better editability of these images.



### HOG, LBP and SVM based Traffic Density Estimation at Intersection
- **Arxiv ID**: http://arxiv.org/abs/2005.01770v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.01770v1)
- **Published**: 2020-05-04 18:08:35+00:00
- **Updated**: 2020-05-04 18:08:35+00:00
- **Authors**: Devashish Prasad, Kshitij Kapadni, Ayan Gadpal, Manish Visave, Kavita Sultanpure
- **Comment**: paper accepted at IEEE PuneCon 2019
- **Journal**: None
- **Summary**: Increased amount of vehicular traffic on roads is a significant issue. High amount of vehicular traffic creates traffic congestion, unwanted delays, pollution, money loss, health issues, accidents, emergency vehicle passage and traffic violations that ends up in the decline in productivity. In peak hours, the issues become even worse. Traditional traffic management and control systems fail to tackle this problem. Currently, the traffic lights at intersections aren't adaptive and have fixed time delays. There's a necessity of an optimized and sensible control system which would enhance the efficiency of traffic flow. Smart traffic systems perform estimation of traffic density and create the traffic lights modification consistent with the quantity of traffic. We tend to propose an efficient way to estimate the traffic density on intersection using image processing and machine learning techniques in real time. The proposed methodology takes pictures of traffic at junction to estimate the traffic density. We use Histogram of Oriented Gradients (HOG), Local Binary Patterns (LBP) and Support Vector Machine (SVM) based approach for traffic density estimation. The strategy is computationally inexpensive and can run efficiently on raspberry pi board. Code is released at https://github.com/DevashishPrasad/Smart-Traffic-Junction.



### Learning-based Tracking of Fast Moving Objects
- **Arxiv ID**: http://arxiv.org/abs/2005.01802v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.01802v1)
- **Published**: 2020-05-04 19:20:09+00:00
- **Updated**: 2020-05-04 19:20:09+00:00
- **Authors**: Ales Zita, Filip Sroubek
- **Comment**: None
- **Journal**: None
- **Summary**: Tracking fast moving objects, which appear as blurred streaks in video sequences, is a difficult task for standard trackers as the object position does not overlap in consecutive video frames and texture information of the objects is blurred. Up-to-date approaches tuned for this task are based on background subtraction with static background and slow deblurring algorithms. In this paper, we present a tracking-by-segmentation approach implemented using state-of-the-art deep learning methods that performs near-realtime tracking on real-world video sequences. We implemented a physically plausible FMO sequence generator to be a robust foundation for our training pipeline and demonstrate the ease of fast generator and network adaptation for different FMO scenarios in terms of foreground variations.



### Semi-supervised lung nodule retrieval
- **Arxiv ID**: http://arxiv.org/abs/2005.01805v1
- **DOI**: None
- **Categories**: **cs.IR**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.01805v1)
- **Published**: 2020-05-04 19:26:14+00:00
- **Updated**: 2020-05-04 19:26:14+00:00
- **Authors**: Mark Loyman, Hayit Greenspan
- **Comment**: None
- **Journal**: None
- **Summary**: Content based image retrieval (CBIR) provides the clinician with visual information that can support, and hopefully improve, his or her decision making process. Given an input query image, a CBIR system provides as its output a set of images, ranked by similarity to the query image. Retrieved images may come with relevant information, such as biopsy-based malignancy labeling, or categorization. Ground truth on similarity between dataset elements (e.g. between nodules) is not readily available, thus greatly challenging machine learning methods. Such annotations are particularly difficult to obtain, due to the subjective nature of the task, with high inter-observer variability requiring multiple expert annotators. Consequently, past approaches have focused on manual feature extraction, while current approaches use auxiliary tasks, such as a binary classification task (e.g. malignancy), for which ground-true is more readily accessible. However, in a previous study, we have shown that binary auxiliary tasks are inferior to the usage of a rough similarity estimate that are derived from data annotations. The current study suggests a semi-supervised approach that involves two steps: 1) Automatic annotation of a given partially labeled dataset; 2) Learning a semantic similarity metric space based on the predicated annotations. The proposed system is demonstrated in lung nodule retrieval using the LIDC dataset, and shows that it is feasible to learn embedding from predicted ratings. The semi-supervised approach has demonstrated a significantly higher discriminative ability than the fully-unsupervised reference.



### Enabling Deep Spiking Neural Networks with Hybrid Conversion and Spike Timing Dependent Backpropagation
- **Arxiv ID**: http://arxiv.org/abs/2005.01807v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.01807v1)
- **Published**: 2020-05-04 19:30:43+00:00
- **Updated**: 2020-05-04 19:30:43+00:00
- **Authors**: Nitin Rathi, Gopalakrishnan Srinivasan, Priyadarshini Panda, Kaushik Roy
- **Comment**: International Conference on Learning Representations (ICLR), 2020
  https://openreview.net/forum?id=B1xSperKvH&noteId=B1xSperKvH
- **Journal**: None
- **Summary**: Spiking Neural Networks (SNNs) operate with asynchronous discrete events (or spikes) which can potentially lead to higher energy-efficiency in neuromorphic hardware implementations. Many works have shown that an SNN for inference can be formed by copying the weights from a trained Artificial Neural Network (ANN) and setting the firing threshold for each layer as the maximum input received in that layer. These type of converted SNNs require a large number of time steps to achieve competitive accuracy which diminishes the energy savings. The number of time steps can be reduced by training SNNs with spike-based backpropagation from scratch, but that is computationally expensive and slow. To address these challenges, we present a computationally-efficient training technique for deep SNNs. We propose a hybrid training methodology: 1) take a converted SNN and use its weights and thresholds as an initialization step for spike-based backpropagation, and 2) perform incremental spike-timing dependent backpropagation (STDB) on this carefully initialized network to obtain an SNN that converges within few epochs and requires fewer time steps for input processing. STDB is performed with a novel surrogate gradient function defined using neuron's spike time. The proposed training methodology converges in less than 20 epochs of spike-based backpropagation for most standard image classification datasets, thereby greatly reducing the training complexity compared to training SNNs from scratch. We perform experiments on CIFAR-10, CIFAR-100, and ImageNet datasets for both VGG and ResNet architectures. We achieve top-1 accuracy of 65.19% for ImageNet dataset on SNN with 250 time steps, which is 10X faster compared to converted SNNs with similar accuracy.



### Selecting Data Augmentation for Simulating Interventions
- **Arxiv ID**: http://arxiv.org/abs/2005.01856v4
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.01856v4)
- **Published**: 2020-05-04 21:33:29+00:00
- **Updated**: 2020-10-26 10:52:21+00:00
- **Authors**: Maximilian Ilse, Jakub M. Tomczak, Patrick Forré
- **Comment**: None
- **Journal**: None
- **Summary**: Machine learning models trained with purely observational data and the principle of empirical risk minimization \citep{vapnik_principles_1992} can fail to generalize to unseen domains. In this paper, we focus on the case where the problem arises through spurious correlation between the observed domains and the actual task labels. We find that many domain generalization methods do not explicitly take this spurious correlation into account. Instead, especially in more application-oriented research areas like medical imaging or robotics, data augmentation techniques that are based on heuristics are used to learn domain invariant features. To bridge the gap between theory and practice, we develop a causal perspective on the problem of domain generalization. We argue that causal concepts can be used to explain the success of data augmentation by describing how they can weaken the spurious correlation between the observed domains and the task labels. We demonstrate that data augmentation can serve as a tool for simulating interventional data. We use these theoretical insights to derive a simple algorithm that is able to select data augmentation techniques that will lead to better domain generalization.



### Streaming Object Detection for 3-D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2005.01864v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.01864v1)
- **Published**: 2020-05-04 21:55:15+00:00
- **Updated**: 2020-05-04 21:55:15+00:00
- **Authors**: Wei Han, Zhengdong Zhang, Benjamin Caine, Brandon Yang, Christoph Sprunk, Ouais Alsharif, Jiquan Ngiam, Vijay Vasudevan, Jonathon Shlens, Zhifeng Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Autonomous vehicles operate in a dynamic environment, where the speed with which a vehicle can perceive and react impacts the safety and efficacy of the system. LiDAR provides a prominent sensory modality that informs many existing perceptual systems including object detection, segmentation, motion estimation, and action recognition. The latency for perceptual systems based on point cloud data can be dominated by the amount of time for a complete rotational scan (e.g. 100 ms). This built-in data capture latency is artificial, and based on treating the point cloud as a camera image in order to leverage camera-inspired architectures. However, unlike camera sensors, most LiDAR point cloud data is natively a streaming data source in which laser reflections are sequentially recorded based on the precession of the laser beam. In this work, we explore how to build an object detector that removes this artificial latency constraint, and instead operates on native streaming data in order to significantly reduce latency. This approach has the added benefit of reducing the peak computational burden on inference hardware by spreading the computation over the acquisition time for a scan. We demonstrate a family of streaming detection systems based on sequential modeling through a series of modifications to the traditional detection meta-architecture. We highlight how this model may achieve competitive if not superior predictive performance with state-of-the-art, traditional non-streaming detection systems while achieving significant latency gains (e.g. 1/15'th - 1/3'rd of peak latency). Our results show that operating on LiDAR data in its native streaming formulation offers several advantages for self driving object detection -- advantages that we hope will be useful for any LiDAR perception system where minimizing latency is critical for safe and efficient operation.



### Illumination-Invariant Image from 4-Channel Images: The Effect of Near-Infrared Data in Shadow Removal
- **Arxiv ID**: http://arxiv.org/abs/2005.01878v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2005.01878v1)
- **Published**: 2020-05-04 22:51:36+00:00
- **Updated**: 2020-05-04 22:51:36+00:00
- **Authors**: Sorour Mohajerani, Mark S. Drew, Parvaneh Saeedi
- **Comment**: Accepted for oral presentation in London Imaging Meeting 2020
- **Journal**: None
- **Summary**: Removing the effect of illumination variation in images has been proved to be beneficial in many computer vision applications such as object recognition and semantic segmentation. Although generating illumination-invariant images has been studied in the literature before, it has not been investigated on real 4-channel (4D) data. In this study, we examine the quality of illumination-invariant images generated from red, green, blue, and near-infrared (RGBN) data. Our experiments show that the near-infrared channel substantively contributes toward removing illumination. As shown in our numerical and visual results, the illumination-invariant image obtained by RGBN data is superior compared to that obtained by RGB alone.



