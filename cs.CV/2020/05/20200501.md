# Arxiv Papers in cs.CV on 2020-05-01
### HERO: Hierarchical Encoder for Video+Language Omni-representation Pre-training
- **Arxiv ID**: http://arxiv.org/abs/2005.00200v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.00200v2)
- **Published**: 2020-05-01 03:49:26+00:00
- **Updated**: 2020-09-29 20:37:17+00:00
- **Authors**: Linjie Li, Yen-Chun Chen, Yu Cheng, Zhe Gan, Licheng Yu, Jingjing Liu
- **Comment**: Accepted by EMNLP 2020
- **Journal**: None
- **Summary**: We present HERO, a novel framework for large-scale video+language omni-representation learning. HERO encodes multimodal inputs in a hierarchical structure, where local context of a video frame is captured by a Cross-modal Transformer via multimodal fusion, and global video context is captured by a Temporal Transformer. In addition to standard Masked Language Modeling (MLM) and Masked Frame Modeling (MFM) objectives, we design two new pre-training tasks: (i) Video-Subtitle Matching (VSM), where the model predicts both global and local temporal alignment; and (ii) Frame Order Modeling (FOM), where the model predicts the right order of shuffled video frames. HERO is jointly trained on HowTo100M and large-scale TV datasets to gain deep understanding of complex social dynamics with multi-character interactions. Comprehensive experiments demonstrate that HERO achieves new state of the art on multiple benchmarks over Text-based Video/Video-moment Retrieval, Video Question Answering (QA), Video-and-language Inference and Video Captioning tasks across different domains. We also introduce two new challenging benchmarks How2QA and How2R for Video QA and Retrieval, collected from diverse video content over multimodalities.



### The AVA-Kinetics Localized Human Actions Video Dataset
- **Arxiv ID**: http://arxiv.org/abs/2005.00214v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.00214v2)
- **Published**: 2020-05-01 04:17:14+00:00
- **Updated**: 2020-05-20 17:40:28+00:00
- **Authors**: Ang Li, Meghana Thotakuri, David A. Ross, Jo√£o Carreira, Alexander Vostrikov, Andrew Zisserman
- **Comment**: 8 pages, 8 figures
- **Journal**: None
- **Summary**: This paper describes the AVA-Kinetics localized human actions video dataset. The dataset is collected by annotating videos from the Kinetics-700 dataset using the AVA annotation protocol, and extending the original AVA dataset with these new AVA annotated Kinetics clips. The dataset contains over 230k clips annotated with the 80 AVA action classes for each of the humans in key-frames. We describe the annotation process and provide statistics about the new dataset. We also include a baseline evaluation using the Video Action Transformer Network on the AVA-Kinetics dataset, demonstrating improved performance for action classification on the AVA test set. The dataset can be downloaded from https://research.google.com/ava/



### Deeply Cascaded U-Net for Multi-Task Image Processing
- **Arxiv ID**: http://arxiv.org/abs/2005.00225v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.00225v1)
- **Published**: 2020-05-01 05:06:35+00:00
- **Updated**: 2020-05-01 05:06:35+00:00
- **Authors**: Ilja Gubins, Remco C. Veltkamp
- **Comment**: 6 pages, 6 figures
- **Journal**: None
- **Summary**: In current practice, many image processing tasks are done sequentially (e.g. denoising, dehazing, followed by semantic segmentation). In this paper, we propose a novel multi-task neural network architecture designed for combining sequential image processing tasks. We extend U-Net by additional decoding pathways for each individual task, and explore deep cascading of outputs and connectivity from one pathway to another. We demonstrate effectiveness of the proposed approach on denoising and semantic segmentation, as well as on progressive coarse-to-fine semantic segmentation, and achieve better performance than multiple individual or jointly-trained networks, with lower number of trainable parameters.



### Deepfake Forensics Using Recurrent Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2005.00229v1
- **DOI**: 10.1729/Journal.22894
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.00229v1)
- **Published**: 2020-05-01 05:27:16+00:00
- **Updated**: 2020-05-01 05:27:16+00:00
- **Authors**: Rahul U, Ragul M, Raja Vignesh K, Tejeswinee K
- **Comment**: This submission has been removed by arXiv administrators due to
  copyright infringement
- **Journal**: None
- **Summary**: As of late an AI based free programming device has made it simple to make authentic face swaps in recordings that leaves barely any hints of control, in what are known as "deepfake" recordings. Situations where these genuine istic counterfeit recordings are utilized to make political pain, extort somebody or phony fear based oppression occasions are effectively imagined. This paper proposes a transient mindful pipeline to automat-ically recognize deepfake recordings. Our framework utilizes a convolutional neural system (CNN) to remove outline level highlights. These highlights are then used to prepare a repetitive neural net-work (RNN) that figures out how to characterize if a video has been sub-ject to control or not. We assess our technique against a huge arrangement of deepfake recordings gathered from different video sites. We show how our framework can accomplish aggressive outcomes in this assignment while utilizing a basic design.



### Cross-modal Language Generation using Pivot Stabilization for Web-scale Language Coverage
- **Arxiv ID**: http://arxiv.org/abs/2005.00246v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.00246v1)
- **Published**: 2020-05-01 06:58:18+00:00
- **Updated**: 2020-05-01 06:58:18+00:00
- **Authors**: Ashish V. Thapliyal, Radu Soricut
- **Comment**: ACL 2020
- **Journal**: None
- **Summary**: Cross-modal language generation tasks such as image captioning are directly hurt in their ability to support non-English languages by the trend of data-hungry models combined with the lack of non-English annotations. We investigate potential solutions for combining existing language-generation annotations in English with translation capabilities in order to create solutions at web-scale in both domain and language coverage. We describe an approach called Pivot-Language Generation Stabilization (PLuGS), which leverages directly at training time both existing English annotations (gold data) as well as their machine-translated versions (silver data); at run-time, it generates first an English caption and then a corresponding target-language caption. We show that PLuGS models outperform other candidate solutions in evaluations performed over 5 different target languages, under a large-domain testset using images from the Open Images dataset. Furthermore, we find an interesting effect where the English captions generated by the PLuGS models are better than the captions generated by the original, monolingual English model.



### Recognizing American Sign Language Nonmanual Signal Grammar Errors in Continuous Videos
- **Arxiv ID**: http://arxiv.org/abs/2005.00253v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.00253v1)
- **Published**: 2020-05-01 07:25:07+00:00
- **Updated**: 2020-05-01 07:25:07+00:00
- **Authors**: Elahe Vahdani, Longlong Jing, Yingli Tian, Matt Huenerfauth
- **Comment**: None
- **Journal**: None
- **Summary**: As part of the development of an educational tool that can help students achieve fluency in American Sign Language (ASL) through independent and interactive practice with immediate feedback, this paper introduces a near real-time system to recognize grammatical errors in continuous signing videos without necessarily identifying the entire sequence of signs. Our system automatically recognizes if performance of ASL sentences contains grammatical errors made by ASL students. We first recognize the ASL grammatical elements including both manual gestures and nonmanual signals independently from multiple modalities (i.e. hand gestures, facial expressions, and head movements) by 3D-ResNet networks. Then the temporal boundaries of grammatical elements from different modalities are examined to detect ASL grammatical mistakes by using a sliding window-based approach. We have collected a dataset of continuous sign language, ASL-HW-RGBD, covering different aspects of ASL grammars for training and testing. Our system is able to recognize grammatical elements on ASL-HW-RGBD from manual gestures, facial expressions, and head movements and successfully detect 8 ASL grammatical mistakes.



### Thermal vulnerability detection in integrated electronic and photonic circuits using IR thermography
- **Arxiv ID**: http://arxiv.org/abs/2006.12201v1
- **DOI**: 10.1364/AO.389960
- **Categories**: **physics.app-ph**, cs.CV, I.4
- **Links**: [PDF](http://arxiv.org/pdf/2006.12201v1)
- **Published**: 2020-05-01 09:25:55+00:00
- **Updated**: 2020-05-01 09:25:55+00:00
- **Authors**: Bilal Hussain, Bushra Jalil, Maria Antonietta Pascali, Muhammad Imran, Giovanni Serafino, Davide Moroni, Paolo Ghelfi
- **Comment**: 2020 Optical Society of America. One print or electronic copy may be
  made for personal use only. Systematic reproduction and distribution,
  duplication of any material in this paper for a fee or for commercial
  purposes, or modifications of the content of this paper are prohibited
- **Journal**: None
- **Summary**: Failure prediction of any electrical/optical component is crucial for estimating its operating life. Using high temperature operating life (HTOL) tests, it is possible to model the failure mechanisms for integrated circuits. Conventional HTOL standards are not suitable for operating life prediction of photonic components owing to their functional dependence on thermo-optic effect. This work presents an IR-assisted thermal vulnerability detection technique suitable for photonic as well as electronic components. By accurately mapping the thermal profile of an integrated circuit under a stress condition, it is possible to precisely locate the heat center for predicting the long-term operational failures within the device under test. For the first time, the reliability testing is extended to a fully functional microwave photonic system using conventional IR thermography. By applying image fusion using affine transformation on multimodal acquisition, it was demonstrated that by comparing the IR profile and GDSII layout, it is possible to accurately locate the heat centers along with spatial information on the type of component. Multiple IR profiles of optical as well as electrical components/circuits were acquired and mapped onto the layout files. In order to ascertain the degree of effectiveness of the proposed technique, IR profiles of CMOS RF and digital circuits were also analyzed. The presented technique offers a reliable automated identification of heat spots within a circuit/system.



### Multi-Camera Trajectory Forecasting: Pedestrian Trajectory Prediction in a Network of Cameras
- **Arxiv ID**: http://arxiv.org/abs/2005.00282v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.00282v1)
- **Published**: 2020-05-01 09:28:32+00:00
- **Updated**: 2020-05-01 09:28:32+00:00
- **Authors**: Olly Styles, Tanaya Guha, Victor Sanchez, Alex Kot
- **Comment**: CVPR 2020 Precognition workshop
- **Journal**: None
- **Summary**: We introduce the task of multi-camera trajectory forecasting (MCTF), where the future trajectory of an object is predicted in a network of cameras. Prior works consider forecasting trajectories in a single camera view. Our work is the first to consider the challenging scenario of forecasting across multiple non-overlapping camera views. This has wide applicability in tasks such as re-identification and multi-target multi-camera tracking. To facilitate research in this new area, we release the Warwick-NTU Multi-camera Forecasting Database (WNMF), a unique dataset of multi-camera pedestrian trajectories from a network of 15 synchronized cameras. To accurately label this large dataset (600 hours of video footage), we also develop a semi-automated annotation method. An effective MCTF model should proactively anticipate where and when a person will re-appear in the camera network. In this paper, we consider the task of predicting the next camera a pedestrian will re-appear after leaving the view of another camera, and present several baseline approaches for this. The labeled database is available online: https://github.com/olly-styles/Multi-Camera-Trajectory-Forecasting.



### Distilling Spikes: Knowledge Distillation in Spiking Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2005.00288v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.00288v1)
- **Published**: 2020-05-01 09:36:32+00:00
- **Updated**: 2020-05-01 09:36:32+00:00
- **Authors**: Ravi Kumar Kushawaha, Saurabh Kumar, Biplab Banerjee, Rajbabu Velmurugan
- **Comment**: Preprint: Manuscript under review
- **Journal**: None
- **Summary**: Spiking Neural Networks (SNN) are energy-efficient computing architectures that exchange spikes for processing information, unlike classical Artificial Neural Networks (ANN). Due to this, SNNs are better suited for real-life deployments. However, similar to ANNs, SNNs also benefit from deeper architectures to obtain improved performance. Furthermore, like the deep ANNs, the memory, compute and power requirements of SNNs also increase with model size, and model compression becomes a necessity. Knowledge distillation is a model compression technique that enables transferring the learning of a large machine learning model to a smaller model with minimal loss in performance. In this paper, we propose techniques for knowledge distillation in spiking neural networks for the task of image classification. We present ways to distill spikes from a larger SNN, also called the teacher network, to a smaller one, also called the student network, while minimally impacting the classification accuracy. We demonstrate the effectiveness of the proposed method with detailed experiments on three standard datasets while proposing novel distillation methodologies and loss functions. We also present a multi-stage knowledge distillation technique for SNNs using an intermediate network to obtain higher performance from the student network. Our approach is expected to open up new avenues for deploying high performing large SNN models on resource-constrained hardware platforms.



### A cascade network for Detecting COVID-19 using chest x-rays
- **Arxiv ID**: http://arxiv.org/abs/2005.01468v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.01468v1)
- **Published**: 2020-05-01 09:56:56+00:00
- **Updated**: 2020-05-01 09:56:56+00:00
- **Authors**: Dailin Lv, Wuteng Qi, Yunxiang Li, Lingling Sun, Yaqi Wang
- **Comment**: None
- **Journal**: None
- **Summary**: The worldwide spread of pneumonia caused by a novel coronavirus poses an unprecedented challenge to the world's medical resources and prevention and control measures. Covid-19 attacks not only the lungs, making it difficult to breathe and life-threatening, but also the heart, kidneys, brain and other vital organs of the body, with possible sequela. At present, the detection of COVID-19 needs to be realized by the reverse transcription-polymerase Chain Reaction (RT-PCR). However, many countries are in the outbreak period of the epidemic, and the medical resources are very limited. They cannot provide sufficient numbers of gene sequence detection, and many patients may not be isolated and treated in time. Given this situation, we researched the analytical and diagnostic capabilities of deep learning on chest radiographs and proposed Cascade-SEMEnet which is cascaded with SEME-ResNet50 and SEME-DenseNet169. The two cascade networks of Cascade - SEMEnet both adopt large input sizes and SE-Structure and use MoEx and histogram equalization to enhance the data. We first used SEME-ResNet50 to screen chest X-ray and diagnosed three classes: normal, bacterial, and viral pneumonia. Then we used SEME-DenseNet169 for fine-grained classification of viral pneumonia and determined if it is caused by COVID-19. To exclude the influence of non-pathological features on the network, we preprocessed the data with U-Net during the training of SEME-DenseNet169. The results showed that our network achieved an accuracy of 85.6\% in determining the type of pneumonia infection and 97.1\% in the fine-grained classification of COVID-19. We used Grad-CAM to visualize the judgment based on the model and help doctors understand the chest radiograph while verifying the effectivene.



### Defocus Deblurring Using Dual-Pixel Data
- **Arxiv ID**: http://arxiv.org/abs/2005.00305v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.00305v3)
- **Published**: 2020-05-01 10:38:00+00:00
- **Updated**: 2020-07-16 23:49:50+00:00
- **Authors**: Abdullah Abuolaim, Michael S. Brown
- **Comment**: Camera-ready version
- **Journal**: None
- **Summary**: Defocus blur arises in images that are captured with a shallow depth of field due to the use of a wide aperture. Correcting defocus blur is challenging because the blur is spatially varying and difficult to estimate. We propose an effective defocus deblurring method that exploits data available on dual-pixel (DP) sensors found on most modern cameras. DP sensors are used to assist a camera's auto-focus by capturing two sub-aperture views of the scene in a single image shot. The two sub-aperture images are used to calculate the appropriate lens position to focus on a particular scene region and are discarded afterwards. We introduce a deep neural network (DNN) architecture that uses these discarded sub-aperture images to reduce defocus blur. A key contribution of our effort is a carefully captured dataset of 500 scenes (2000 images) where each scene has: (i) an image with defocus blur captured at a large aperture; (ii) the two associated DP sub-aperture views; and (iii) the corresponding all-in-focus image captured with a small aperture. Our proposed DNN produces results that are significantly better than conventional single image methods in terms of both quantitative and perceptual metrics -- all from data that is already available on the camera but ignored. The dataset, code, and trained models are available at https://github.com/Abdullah-Abuolaim/defocus-deblurring-dual-pixel.



### PCA-SRGAN: Incremental Orthogonal Projection Discrimination for Face Super-resolution
- **Arxiv ID**: http://arxiv.org/abs/2005.00306v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.00306v2)
- **Published**: 2020-05-01 10:40:57+00:00
- **Updated**: 2020-08-28 10:26:21+00:00
- **Authors**: Hao Dou, Chen Chen, Xiyuan Hu, Zuxing Xuan, Zhisen Hu, Silong Peng
- **Comment**: None
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GAN) have been employed for face super resolution but they bring distorted facial details easily and still have weakness on recovering realistic texture. To further improve the performance of GAN based models on super-resolving face images, we propose PCA-SRGAN which pays attention to the cumulative discrimination in the orthogonal projection space spanned by PCA projection matrix of face data. By feeding the principal component projections ranging from structure to details into the discriminator, the discrimination difficulty will be greatly alleviated and the generator can be enhanced to reconstruct clearer contour and finer texture, helpful to achieve the high perception and low distortion eventually. This incremental orthogonal projection discrimination has ensured a precise optimization procedure from coarse to fine and avoids the dependence on the perceptual regularization. We conduct experiments on CelebA and FFHQ face datasets. The qualitative visual effect and quantitative evaluation have demonstrated the overwhelming performance of our model over related works.



### ACCL: Adversarial constrained-CNN loss for weakly supervised medical image segmentation
- **Arxiv ID**: http://arxiv.org/abs/2005.00328v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.00328v1)
- **Published**: 2020-05-01 12:16:22+00:00
- **Updated**: 2020-05-01 12:16:22+00:00
- **Authors**: Pengyi Zhang, Yunxin Zhong, Xiaoqiong Li
- **Comment**: None
- **Journal**: None
- **Summary**: We propose adversarial constrained-CNN loss, a new paradigm of constrained-CNN loss methods, for weakly supervised medical image segmentation. In the new paradigm, prior knowledge is encoded and depicted by reference masks, and is further employed to impose constraints on segmentation outputs through adversarial learning with reference masks. Unlike pseudo label methods for weakly supervised segmentation, such reference masks are used to train a discriminator rather than a segmentation network, and thus are not required to be paired with specific images. Our new paradigm not only greatly facilitates imposing prior knowledge on network's outputs, but also provides stronger and higher-order constraints, i.e., distribution approximation, through adversarial learning. Extensive experiments involving different medical modalities, different anatomical structures, different topologies of the object of interest, different levels of prior knowledge and weakly supervised annotations with different annotation ratios is conducted to evaluate our ACCL method. Consistently superior segmentation results over the size constrained-CNN loss method have been achieved, some of which are close to the results of full supervision, thus fully verifying the effectiveness and generalization of our method. Specifically, we report an average Dice score of 75.4% with an average annotation ratio of 0.65%, surpassing the prior art, i.e., the size constrained-CNN loss method, by a large margin of 11.4%. Our codes are made publicly available at https://github.com/PengyiZhang/ACCL.



### Visuo-Linguistic Question Answering (VLQA) Challenge
- **Arxiv ID**: http://arxiv.org/abs/2005.00330v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2005.00330v3)
- **Published**: 2020-05-01 12:18:55+00:00
- **Updated**: 2020-11-18 07:45:20+00:00
- **Authors**: Shailaja Keyur Sampat, Yezhou Yang, Chitta Baral
- **Comment**: Findings of EMNLP 2020 (22 pages, 13 figures)
- **Journal**: None
- **Summary**: Understanding images and text together is an important aspect of cognition and building advanced Artificial Intelligence (AI) systems. As a community, we have achieved good benchmarks over language and vision domains separately, however joint reasoning is still a challenge for state-of-the-art computer vision and natural language processing (NLP) systems. We propose a novel task to derive joint inference about a given image-text modality and compile the Visuo-Linguistic Question Answering (VLQA) challenge corpus in a question answering setting. Each dataset item consists of an image and a reading passage, where questions are designed to combine both visual and textual information i.e., ignoring either modality would make the question unanswerable. We first explore the best existing vision-language architectures to solve VLQA subsets and show that they are unable to reason well. We then develop a modular method with slightly better baseline performance, but it is still far behind human performance. We believe that VLQA will be a good benchmark for reasoning over a visuo-linguistic context. The dataset, code and leaderboard is available at https://shailaja183.github.io/vlqa/.



### Adversarial Synthesis of Human Pose from Text
- **Arxiv ID**: http://arxiv.org/abs/2005.00340v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.00340v2)
- **Published**: 2020-05-01 12:32:04+00:00
- **Updated**: 2020-10-16 09:38:08+00:00
- **Authors**: Yifei Zhang, Rania Briq, Julian Tanke, Juergen Gall
- **Comment**: DAGM GCPR 2020
- **Journal**: None
- **Summary**: This work focuses on synthesizing human poses from human-level text descriptions. We propose a model that is based on a conditional generative adversarial network. It is designed to generate 2D human poses conditioned on human-written text descriptions. The model is trained and evaluated using the COCO dataset, which consists of images capturing complex everyday scenes with various human poses. We show through qualitative and quantitative results that the model is capable of synthesizing plausible poses matching the given text, indicating that it is possible to generate poses that are consistent with the given semantic features, especially for actions with distinctive poses.



### Understanding the Perceived Quality of Video Predictions
- **Arxiv ID**: http://arxiv.org/abs/2005.00356v5
- **DOI**: 10.1016/j.image.2021.116626
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.00356v5)
- **Published**: 2020-05-01 13:05:22+00:00
- **Updated**: 2021-12-25 05:33:19+00:00
- **Authors**: Nagabhushan Somraj, Manoj Surya Kashi, S. P. Arun, Rajiv Soundararajan
- **Comment**: Project website:
  https://nagabhushansn95.github.io/publications/2020/pvqa.html. Accepted in
  SPIC
- **Journal**: Signal Processing: Image Communication. 102 (2022) 116626
- **Summary**: The study of video prediction models is believed to be a fundamental approach to representation learning for videos. While a plethora of generative models for predicting the future frame pixel values given the past few frames exist, the quantitative evaluation of the predicted frames has been found to be extremely challenging. In this context, we study the problem of quality assessment of predicted videos. We create the Indian Institute of Science Predicted Videos Quality Assessment (IISc PVQA) Database consisting of 300 videos, obtained by applying different prediction models on different datasets, and accompanying human opinion scores. We collected subjective ratings of quality from 50 human participants for these videos. Our subjective study reveals that human observers were highly consistent in their judgments of quality of predicted videos. We benchmark several popularly used measures for evaluating video prediction and show that they do not adequately correlate with these subjective scores. We introduce two new features to effectively capture the quality of predicted videos, motion-compensated cosine similarities of deep features of predicted frames with past frames, and deep features extracted from rescaled frame differences. We show that our feature design leads to state of the art quality prediction in accordance with human judgments on our IISc PVQA Database. The database and code are publicly available on our project website: https://nagabhushansn95.github.io/publications/2020/pvqa



### Towards Visually Explaining Video Understanding Networks with Perturbation
- **Arxiv ID**: http://arxiv.org/abs/2005.00375v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.00375v2)
- **Published**: 2020-05-01 13:41:38+00:00
- **Updated**: 2020-11-09 15:30:07+00:00
- **Authors**: Zhenqiang Li, Weimin Wang, Zuoyue Li, Yifei Huang, Yoichi Sato
- **Comment**: Accepted by WACV2021
- **Journal**: None
- **Summary**: ''Making black box models explainable'' is a vital problem that accompanies the development of deep learning networks. For networks taking visual information as input, one basic but challenging explanation method is to identify and visualize the input pixels/regions that dominate the network's prediction. However, most existing works focus on explaining networks taking a single image as input and do not consider the temporal relationship that exists in videos. Providing an easy-to-use visual explanation method that is applicable to diversified structures of video understanding networks still remains an open challenge. In this paper, we investigate a generic perturbation-based method for visually explaining video understanding networks. Besides, we propose a novel loss function to enhance the method by constraining the smoothness of its results in both spatial and temporal dimensions. The method enables the comparison of explanation results between different network structures to become possible and can also avoid generating the pathological adversarial explanations for video inputs. Experimental comparison results verified the effectiveness of our method.



### An Adaptive Enhancement Based Hybrid CNN Model for Digital Dental X-ray Positions Classification
- **Arxiv ID**: http://arxiv.org/abs/2005.01509v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.01509v1)
- **Published**: 2020-05-01 13:55:44+00:00
- **Updated**: 2020-05-01 13:55:44+00:00
- **Authors**: Yaqi Wang, Lingling Sun, Yifang Zhang, Dailin Lv, Zhixing Li, Wuteng Qi
- **Comment**: arXiv admin note: text overlap with arXiv:1802.03086 by other authors
- **Journal**: None
- **Summary**: Analysis of dental radiographs is an important part of the diagnostic process in daily clinical practice. Interpretation by an expert includes teeth detection and numbering. In this project, a novel solution based on adaptive histogram equalization and convolution neural network (CNN) is proposed, which automatically performs the task for dental x-rays. In order to improve the detection accuracy, we propose three pre-processing techniques to supplement the baseline CNN based on some prior domain knowledge. Firstly, image sharpening and median filtering are used to remove impulse noise, and the edge is enhanced to some extent. Next, adaptive histogram equalization is used to overcome the problem of excessive amplification noise of HE. Finally, a multi-CNN hybrid model is proposed to classify six different locations of dental slices. The results showed that the accuracy and specificity of the test set exceeded 90\%, and the AUC reached 0.97. In addition, four dentists were invited to manually annotate the test data set (independently) and then compare it with the labels obtained by our proposed algorithm. The results show that our method can effectively identify the X-ray location of teeth.



### MOPS-Net: A Matrix Optimization-driven Network forTask-Oriented 3D Point Cloud Downsampling
- **Arxiv ID**: http://arxiv.org/abs/2005.00383v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.00383v4)
- **Published**: 2020-05-01 14:01:53+00:00
- **Updated**: 2021-04-12 17:36:29+00:00
- **Authors**: Yue Qian, Junhui Hou, Qijian Zhang, Yiming Zeng, Sam Kwong, Ying He
- **Comment**: 15 pages, 16 figures, 10 tables
- **Journal**: None
- **Summary**: This paper explores the problem of task-oriented downsampling over 3D point clouds, which aims to downsample a point cloud while maintaining the performance of subsequent applications applied to the downsampled sparse points as much as possible. Designing from the perspective of matrix optimization, we propose MOPS-Net, a novel interpretable deep learning-based method, which is fundamentally different from the existing deep learning-based methods due to its interpretable feature. The optimization problem is challenging due to its discrete and combinatorial nature. We tackle the challenges by relaxing the binary constraint of the variables, and formulate a constrained and differentiable matrix optimization problem. We then design a deep neural network to mimic the matrix optimization by exploring both the local and global structures of the input data. MOPS-Net can be end-to-end trained with a task network and is permutation-invariant, making it robust to the input. We also extend MOPS-Net such that a single network after one-time training is capable of handling arbitrary downsampling ratios. Extensive experimental results show that MOPS-Net can achieve favorable performance against state-of-the-art deep learning-based methods over various tasks, including classification, reconstruction, and registration. Besides, we validate the robustness of MOPS-Net on noisy data.



### Aggregation and Finetuning for Clothes Landmark Detection
- **Arxiv ID**: http://arxiv.org/abs/2005.00419v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.00419v1)
- **Published**: 2020-05-01 14:47:08+00:00
- **Updated**: 2020-05-01 14:47:08+00:00
- **Authors**: Tzu-Heng Lin
- **Comment**: Technical report, 4 pages
- **Journal**: None
- **Summary**: Landmark detection for clothes is a fundamental problem for many applications. In this paper, a new training scheme for clothes landmark detection: $\textit{Aggregation and Finetuning}$, is proposed. We investigate the homogeneity among landmarks of different categories of clothes, and utilize it to design the procedure of training. Extensive experiments show that our method outperforms current state-of-the-art methods by a large margin. Our method also won the 1st place in the DeepFashion2 Challenge 2020 - Clothes Landmark Estimation Track with an AP of 0.590 on the test set, and 0.615 on the validation set. Code will be publicly available at https://github.com/lzhbrian/deepfashion2-kps-agg-finetune .



### Investigating Class-level Difficulty Factors in Multi-label Classification Problems
- **Arxiv ID**: http://arxiv.org/abs/2005.00430v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.00430v1)
- **Published**: 2020-05-01 15:06:53+00:00
- **Updated**: 2020-05-01 15:06:53+00:00
- **Authors**: Mark Marsden, Kevin McGuinness, Joseph Antony, Haolin Wei, Milan Redzic, Jian Tang, Zhilan Hu, Alan Smeaton, Noel E O'Connor
- **Comment**: Published in ICME 2020
- **Journal**: None
- **Summary**: This work investigates the use of class-level difficulty factors in multi-label classification problems for the first time. Four class-level difficulty factors are proposed: frequency, visual variation, semantic abstraction, and class co-occurrence. Once computed for a given multi-label classification dataset, these difficulty factors are shown to have several potential applications including the prediction of class-level performance across datasets and the improvement of predictive performance through difficulty weighted optimisation. Significant improvements to mAP and AUC performance are observed for two challenging multi-label datasets (WWW Crowd and Visual Genome) with the inclusion of difficulty weighted optimisation. The proposed technique does not require any additional computational complexity during training or inference and can be extended over time with inclusion of other class-level difficulty factors.



### Computing the Testing Error without a Testing Set
- **Arxiv ID**: http://arxiv.org/abs/2005.00450v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.00450v1)
- **Published**: 2020-05-01 15:35:50+00:00
- **Updated**: 2020-05-01 15:35:50+00:00
- **Authors**: Ciprian Corneanu, Meysam Madadi, Sergio Escalera, Aleix Martinez
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Neural Networks (DNNs) have revolutionized computer vision. We now have DNNs that achieve top (performance) results in many problems, including object recognition, facial expression analysis, and semantic segmentation, to name but a few. The design of the DNNs that achieve top results is, however, non-trivial and mostly done by trail-and-error. That is, typically, researchers will derive many DNN architectures (i.e., topologies) and then test them on multiple datasets. However, there are no guarantees that the selected DNN will perform well in the real world. One can use a testing set to estimate the performance gap between the training and testing sets, but avoiding overfitting-to-the-testing-data is almost impossible. Using a sequestered testing dataset may address this problem, but this requires a constant update of the dataset, a very expensive venture. Here, we derive an algorithm to estimate the performance gap between training and testing that does not require any testing dataset. Specifically, we derive a number of persistent topology measures that identify when a DNN is learning to generalize to unseen samples. This allows us to compute the DNN's testing error on unseen samples, even when we do not have access to them. We provide extensive experimental validation on multiple networks and datasets to demonstrate the feasibility of the proposed approach.



### HLVU : A New Challenge to Test Deep Understanding of Movies the Way Humans do
- **Arxiv ID**: http://arxiv.org/abs/2005.00463v1
- **DOI**: 10.1145/3372278.3390742
- **Categories**: **cs.AI**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.00463v1)
- **Published**: 2020-05-01 15:58:13+00:00
- **Updated**: 2020-05-01 15:58:13+00:00
- **Authors**: Keith Curtis, George Awad, Shahzad Rajput, Ian Soboroff
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we propose a new evaluation challenge and direction in the area of High-level Video Understanding. The challenge we are proposing is designed to test automatic video analysis and understanding, and how accurately systems can comprehend a movie in terms of actors, entities, events and their relationship to each other. A pilot High-Level Video Understanding (HLVU) dataset of open source movies were collected for human assessors to build a knowledge graph representing each of them. A set of queries will be derived from the knowledge graph to test systems on retrieving relationships among actors, as well as reasoning and retrieving non-visual concepts. The objective is to benchmark if a computer system can "understand" non-explicit but obvious relationships the same way humans do when they watch the same movies. This is long-standing problem that is being addressed in the text domain and this project moves similar research to the video domain. Work of this nature is foundational to future video analytics and video understanding technologies. This work can be of interest to streaming services and broadcasters hoping to provide more intuitive ways for their customers to interact with and consume video content.



### An Efficient Integration of Disentangled Attended Expression and Identity FeaturesFor Facial Expression Transfer andSynthesis
- **Arxiv ID**: http://arxiv.org/abs/2005.00499v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.00499v1)
- **Published**: 2020-05-01 17:14:53+00:00
- **Updated**: 2020-05-01 17:14:53+00:00
- **Authors**: Kamran Ali, Charles E. Hughes
- **Comment**: 10 Pages, excluding references
- **Journal**: None
- **Summary**: In this paper, we present an Attention-based Identity Preserving Generative Adversarial Network (AIP-GAN) to overcome the identity leakage problem from a source image to a generated face image, an issue that is encountered in a cross-subject facial expression transfer and synthesis process. Our key insight is that the identity preserving network should be able to disentangle and compose shape, appearance, and expression information for efficient facial expression transfer and synthesis. Specifically, the expression encoder of our AIP-GAN disentangles the expression information from the input source image by predicting its facial landmarks using our supervised spatial and channel-wise attention module. Similarly, the disentangled expression-agnostic identity features are extracted from the input target image by inferring its combined intrinsic-shape and appearance image employing our self-supervised spatial and channel-wise attention mod-ule. To leverage the expression and identity information encoded by the intermediate layers of both of our encoders, we combine these features with the features learned by the intermediate layers of our decoder using a cross-encoder bilinear pooling operation. Experimental results show the promising performance of our AIP-GAN based technique.



### RigNet: Neural Rigging for Articulated Characters
- **Arxiv ID**: http://arxiv.org/abs/2005.00559v2
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.00559v2)
- **Published**: 2020-05-01 18:12:44+00:00
- **Updated**: 2020-07-05 19:38:56+00:00
- **Authors**: Zhan Xu, Yang Zhou, Evangelos Kalogerakis, Chris Landreth, Karan Singh
- **Comment**: SIGGRAPH 2020. Project page https://zhan-xu.github.io/rig-net/
- **Journal**: None
- **Summary**: We present RigNet, an end-to-end automated method for producing animation rigs from input character models. Given an input 3D model representing an articulated character, RigNet predicts a skeleton that matches the animator expectations in joint placement and topology. It also estimates surface skin weights based on the predicted skeleton. Our method is based on a deep architecture that directly operates on the mesh representation without making assumptions on shape class and structure. The architecture is trained on a large and diverse collection of rigged models, including their mesh, skeletons and corresponding skin weights. Our evaluation is three-fold: we show better results than prior art when quantitatively compared to animator rigs; qualitatively we show that our rigs can be expressively posed and animated at multiple levels of detail; and finally, we evaluate the impact of various algorithm choices on our output rigs.



### When Ensembling Smaller Models is More Efficient than Single Large Models
- **Arxiv ID**: http://arxiv.org/abs/2005.00570v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.00570v1)
- **Published**: 2020-05-01 18:56:18+00:00
- **Updated**: 2020-05-01 18:56:18+00:00
- **Authors**: Dan Kondratyuk, Mingxing Tan, Matthew Brown, Boqing Gong
- **Comment**: None
- **Journal**: None
- **Summary**: Ensembling is a simple and popular technique for boosting evaluation performance by training multiple models (e.g., with different initializations) and aggregating their predictions. This approach is commonly reserved for the largest models, as it is commonly held that increasing the model size provides a more substantial reduction in error than ensembling smaller models. However, we show results from experiments on CIFAR-10 and ImageNet that ensembles can outperform single models with both higher accuracy and requiring fewer total FLOPs to compute, even when those individual models' weights and hyperparameters are highly optimized. Furthermore, this gap in improvement widens as models become large. This presents an interesting observation that output diversity in ensembling can often be more efficient than training larger models, especially when the models approach the size of what their dataset can foster. Instead of using the common practice of tuning a single large model, one can use ensembles as a more flexible trade-off between a model's inference speed and accuracy. This also potentially eases hardware design, e.g., an easier way to parallelize the model across multiple workers for real-time or distributed inference.



### Global Table Extractor (GTE): A Framework for Joint Table Identification and Cell Structure Recognition Using Visual Context
- **Arxiv ID**: http://arxiv.org/abs/2005.00589v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.00589v2)
- **Published**: 2020-05-01 20:14:49+00:00
- **Updated**: 2020-12-02 04:45:25+00:00
- **Authors**: Xinyi Zheng, Doug Burdick, Lucian Popa, Xu Zhong, Nancy Xin Ru Wang
- **Comment**: None
- **Journal**: Winter Conference for Applications in Computer Vision (WACV) 2021
- **Summary**: Documents are often used for knowledge sharing and preservation in business and science, within which are tables that capture most of the critical data. Unfortunately, most documents are stored and distributed as PDF or scanned images, which fail to preserve logical table structure. Recent vision-based deep learning approaches have been proposed to address this gap, but most still cannot achieve state-of-the-art results. We present Global Table Extractor (GTE), a vision-guided systematic framework for joint table detection and cell structured recognition, which could be built on top of any object detection model. With GTE-Table, we invent a new penalty based on the natural cell containment constraint of tables to train our table network aided by cell location predictions. GTE-Cell is a new hierarchical cell detection network that leverages table styles. Further, we design a method to automatically label table and cell structure in existing documents to cheaply create a large corpus of training and test data. We use this to enhance PubTabNet with cell labels and create FinTabNet, real-world and complex scientific and financial datasets with detailed table structure annotations to help train and test structure recognition. Our framework surpasses previous state-of-the-art results on the ICDAR 2013 and ICDAR 2019 table competition in both table detection and cell structure recognition with a significant 5.8% improvement in the full table extraction system. Further experiments demonstrate a greater than 45% improvement in cell structure recognition when compared to a vanilla RetinaNet object detection model in our new out-of-domain FinTabNet.



### Learning from Noisy Labels with Noise Modeling Network
- **Arxiv ID**: http://arxiv.org/abs/2005.00596v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.00596v1)
- **Published**: 2020-05-01 20:32:22+00:00
- **Updated**: 2020-05-01 20:32:22+00:00
- **Authors**: Zhuolin Jiang, Jan Silovsky, Man-Hung Siu, William Hartmann, Herbert Gish, Sancar Adali
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-label image classification has generated significant interest in recent years and the performance of such systems often suffers from the not so infrequent occurrence of incorrect or missing labels in the training data. In this paper, we extend the state-of the-art of training classifiers to jointly deal with both forms of errorful data. We accomplish this by modeling noisy and missing labels in multi-label images with a new Noise Modeling Network (NMN) that follows our convolutional neural network (CNN), integrates with it, forming an end-to-end deep learning system, which can jointly learn the noise distribution and CNN parameters. The NMN learns the distribution of noise patterns directly from the noisy data without the need for any clean training data. The NMN can model label noise that depends only on the true label or is also dependent on the image features. We show that the integrated NMN/CNN learning system consistently improves the classification performance, for different levels of label noise, on the MSR-COCO dataset and MSR-VTT dataset. We also show that noise performance improvements are obtained when multiple instance learning methods are used.



### Probing Contextual Language Models for Common Ground with Visual Representations
- **Arxiv ID**: http://arxiv.org/abs/2005.00619v5
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.00619v5)
- **Published**: 2020-05-01 21:28:28+00:00
- **Updated**: 2021-04-13 16:02:39+00:00
- **Authors**: Gabriel Ilharco, Rowan Zellers, Ali Farhadi, Hannaneh Hajishirzi
- **Comment**: Proceedings of the 2021 North American Chapter of the Association for
  Computational Linguistics (NAACL 2021)
- **Journal**: None
- **Summary**: The success of large-scale contextual language models has attracted great interest in probing what is encoded in their representations. In this work, we consider a new question: to what extent contextual representations of concrete nouns are aligned with corresponding visual representations? We design a probing model that evaluates how effective are text-only representations in distinguishing between matching and non-matching visual representations. Our findings show that language representations alone provide a strong signal for retrieving image patches from the correct object categories. Moreover, they are effective in retrieving specific instances of image patches; textual context plays an important role in this process. Visually grounded language models slightly outperform text-only language models in instance retrieval, but greatly under-perform humans. We hope our analyses inspire future research in understanding and improving the visual capabilities of language models.



### Jacks of All Trades, Masters Of None: Addressing Distributional Shift and Obtrusiveness via Transparent Patch Attacks
- **Arxiv ID**: http://arxiv.org/abs/2005.00656v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.00656v1)
- **Published**: 2020-05-01 23:50:37+00:00
- **Updated**: 2020-05-01 23:50:37+00:00
- **Authors**: Neil Fendley, Max Lennon, I-Jeng Wang, Philippe Burlina, Nathan Drenkow
- **Comment**: None
- **Journal**: None
- **Summary**: We focus on the development of effective adversarial patch attacks and -- for the first time -- jointly address the antagonistic objectives of attack success and obtrusiveness via the design of novel semi-transparent patches. This work is motivated by our pursuit of a systematic performance analysis of patch attack robustness with regard to geometric transformations. Specifically, we first elucidate a) key factors underpinning patch attack success and b) the impact of distributional shift between training and testing/deployment when cast under the Expectation over Transformation (EoT) formalism. By focusing our analysis on three principal classes of transformations (rotation, scale, and location), our findings provide quantifiable insights into the design of effective patch attacks and demonstrate that scale, among all factors, significantly impacts patch attack success. Working from these findings, we then focus on addressing how to overcome the principal limitations of scale for the deployment of attacks in real physical settings: namely the obtrusiveness of large patches. Our strategy is to turn to the novel design of irregularly-shaped, semi-transparent partial patches which we construct via a new optimization process that jointly addresses the antagonistic goals of mitigating obtrusiveness and maximizing effectiveness. Our study -- we hope -- will help encourage more focus in the community on the issues of obtrusiveness, scale, and success in patch attacks.



