# Arxiv Papers in cs.CV on 2020-05-09
### An Investigation of Why Overparameterization Exacerbates Spurious Correlations
- **Arxiv ID**: http://arxiv.org/abs/2005.04345v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.04345v3)
- **Published**: 2020-05-09 01:59:13+00:00
- **Updated**: 2020-08-26 19:32:58+00:00
- **Authors**: Shiori Sagawa, Aditi Raghunathan, Pang Wei Koh, Percy Liang
- **Comment**: None
- **Journal**: None
- **Summary**: We study why overparameterization -- increasing model size well beyond the point of zero training error -- can hurt test error on minority groups despite improving average test error when there are spurious correlations in the data. Through simulations and experiments on two image datasets, we identify two key properties of the training data that drive this behavior: the proportions of majority versus minority groups, and the signal-to-noise ratio of the spurious correlations. We then analyze a linear setting and theoretically show how the inductive bias of models towards "memorizing" fewer examples can cause overparameterization to hurt. Our analysis leads to a counterintuitive approach of subsampling the majority group, which empirically achieves low minority error in the overparameterized regime, even though the standard approach of upweighting the minority fails. Overall, our results suggest a tension between using overparameterized models versus using all the training data for achieving low worst-group error.



### ICE-GAN: Identity-aware and Capsule-Enhanced GAN with Graph-based Reasoning for Micro-Expression Recognition and Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2005.04370v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.04370v2)
- **Published**: 2020-05-09 05:37:44+00:00
- **Updated**: 2021-04-26 05:39:26+00:00
- **Authors**: Jianhui Yu, Chaoyi Zhang, Yang Song, Weidong Cai
- **Comment**: 9 pages, 5 figures
- **Journal**: None
- **Summary**: Micro-expressions are reflections of people's true feelings and motives, which attract an increasing number of researchers into the study of automatic facial micro-expression recognition. The short detection window, the subtle facial muscle movements, and the limited training samples make micro-expression recognition challenging. To this end, we propose a novel Identity-aware and Capsule-Enhanced Generative Adversarial Network with graph-based reasoning (ICE-GAN), introducing micro-expression synthesis as an auxiliary task to assist recognition. The generator produces synthetic faces with controllable micro-expressions and identity-aware features, whose long-ranged dependencies are captured through the graph reasoning module (GRM), and the discriminator detects the image authenticity and expression classes. Our ICE-GAN was evaluated on Micro-Expression Grand Challenge 2019 (MEGC2019) with a significant improvement (12.9%) over the winner and surpassed other state-of-the-art methods.



### AutoCLINT: The Winning Method in AutoCV Challenge 2019
- **Arxiv ID**: http://arxiv.org/abs/2005.04373v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.04373v1)
- **Published**: 2020-05-09 05:50:38+00:00
- **Updated**: 2020-05-09 05:50:38+00:00
- **Authors**: Woonhyuk Baek, Ildoo Kim, Sungwoong Kim, Sungbin Lim
- **Comment**: 9 pages, 8 figures, 4 tables
- **Journal**: None
- **Summary**: NeurIPS 2019 AutoDL challenge is a series of six automated machine learning competitions. Particularly, AutoCV challenges mainly focused on classification tasks on visual domain. In this paper, we introduce the winning method in the competition, AutoCLINT. The proposed method implements an autonomous training strategy, including efficient code optimization, and applies an automated data augmentation to achieve the fast adaptation of pretrained networks. We implement a light version of Fast AutoAugment to search for data augmentation policies efficiently for the arbitrarily given image domains. We also empirically analyze the components of the proposed method and provide ablation studies focusing on AutoCV datasets.



### Character Matters: Video Story Understanding with Character-Aware Relations
- **Arxiv ID**: http://arxiv.org/abs/2005.08646v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.08646v1)
- **Published**: 2020-05-09 06:51:13+00:00
- **Updated**: 2020-05-09 06:51:13+00:00
- **Authors**: Shijie Geng, Ji Zhang, Zuohui Fu, Peng Gao, Hang Zhang, Gerard de Melo
- **Comment**: None
- **Journal**: None
- **Summary**: Different from short videos and GIFs, video stories contain clear plots and lists of principal characters. Without identifying the connection between appearing people and character names, a model is not able to obtain a genuine understanding of the plots. Video Story Question Answering (VSQA) offers an effective way to benchmark higher-level comprehension abilities of a model. However, current VSQA methods merely extract generic visual features from a scene. With such an approach, they remain prone to learning just superficial correlations. In order to attain a genuine understanding of who did what to whom, we propose a novel model that continuously refines character-aware relations. This model specifically considers the characters in a video story, as well as the relations connecting different characters and objects. Based on these signals, our framework enables weakly-supervised face naming through multi-instance co-occurrence matching and supports high-level reasoning utilizing Transformer structures. We train and test our model on the six diverse TV shows in the TVQA dataset, which is by far the largest and only publicly available dataset for VSQA. We validate our proposed approach over TVQA dataset through extensive ablation study.



### Enhancing LGMD's Looming Selectivity for UAV with Spatial-temporal Distributed Presynaptic Connections
- **Arxiv ID**: http://arxiv.org/abs/2005.04397v3
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2005.04397v3)
- **Published**: 2020-05-09 09:15:02+00:00
- **Updated**: 2021-04-17 04:38:13+00:00
- **Authors**: Jiannan Zhao, Hongxin Wang, Shigang Yue
- **Comment**: 15 pages, 17 figures, 4 tables
- **Journal**: None
- **Summary**: Collision detection is one of the most challenging tasks for Unmanned Aerial Vehicles (UAVs). This is especially true for small or micro UAVs, due to their limited computational power. In nature, flying insects with compact and simple visual systems demonstrate their remarkable ability to navigate and avoid collision in complex environments. A good example of this is provided by locusts. They can avoid collisions in a dense swarm through the activity of a motion based visual neuron called the Lobula Giant Movement Detector (LGMD). The defining feature of the LGMD neuron is its preference for looming. As a flying insect's visual neuron, LGMD is considered to be an ideal basis for building UAV's collision detecting system. However, existing LGMD models cannot distinguish looming clearly from other visual cues such as complex background movements caused by UAV agile flights. To address this issue, we proposed a new model implementing distributed spatial-temporal synaptic interactions, which is inspired by recent findings in locusts' synaptic morphology. We first introduced the locally distributed excitation to enhance the excitation caused by visual motion with preferred velocities. Then radially extending temporal latency for inhibition is incorporated to compete with the distributed excitation and selectively suppress the non-preferred visual motions. Systematic experiments have been conducted to verify the performance of the proposed model for UAV agile flights. The results have demonstrated that this new model enhances the looming selectivity in complex flying scenes considerably, and has potential to be implemented on embedded collision detection systems for small or micro UAVs.



### Comment on "No-Reference Video Quality Assessment Based on the Temporal Pooling of Deep Features"
- **Arxiv ID**: http://arxiv.org/abs/2005.04400v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.04400v1)
- **Published**: 2020-05-09 09:28:01+00:00
- **Updated**: 2020-05-09 09:28:01+00:00
- **Authors**: Franz GÃ¶tz-Hahn, Vlad Hosu, Dietmar Saupe
- **Comment**: None
- **Journal**: None
- **Summary**: In Neural Processing Letters 50,3 (2019) a machine learning approach to blind video quality assessment was proposed. It is based on temporal pooling of features of video frames, taken from the last pooling layer of deep convolutional neural networks. The method was validated on two established benchmark datasets and gave results far better than the previous state-of-the-art. In this letter we report the results from our careful reimplementations. The performance results, claimed in the paper, cannot be reached, and are even below the state-of-the-art by a large margin. We show that the originally reported wrong performance results are a consequence of two cases of data leakage. Information from outside the training dataset was used in the fine-tuning stage and in the model evaluation.



### A Weighted Difference of Anisotropic and Isotropic Total Variation for Relaxed Mumford-Shah Color and Multiphase Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2005.04401v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.04401v6)
- **Published**: 2020-05-09 09:35:44+00:00
- **Updated**: 2021-07-18 02:07:57+00:00
- **Authors**: Kevin Bui, Fredrick Park, Yifei Lou, Jack Xin
- **Comment**: latest version has typos fixed; Clean, official version will be on
  SIAM Journal on Imaging Sciences
- **Journal**: None
- **Summary**: In a class of piecewise-constant image segmentation models, we propose to incorporate a weighted difference of anisotropic and isotropic total variation (AITV) to regularize the partition boundaries in an image. In particular, we replace the total variation regularization in the Chan-Vese segmentation model and a fuzzy region competition model by the proposed AITV. To deal with the nonconvex nature of AITV, we apply the difference-of-convex algorithm (DCA), in which the subproblems can be minimized by the primal-dual hybrid gradient method with linesearch. The convergence of the DCA scheme is analyzed. In addition, a generalization to color image segmentation is discussed. In the numerical experiments, we compare the proposed models with the classic convex approaches and the two-stage segmentation methods (smoothing and then thresholding) on various images, showing that our models are effective in image segmentation and robust with respect to impulsive noises.



### RUHSNet: 3D Object Detection Using Lidar Data in Real Time
- **Arxiv ID**: http://arxiv.org/abs/2006.01250v6
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.01250v6)
- **Published**: 2020-05-09 09:41:46+00:00
- **Updated**: 2021-06-21 18:21:58+00:00
- **Authors**: Abhinav Sagar
- **Comment**: The results in this paper is not correct as assumptions used while
  designing the network was found to be wrong
- **Journal**: None
- **Summary**: In this work, we address the problem of 3D object detection from point cloud data in real time. For autonomous vehicles to work, it is very important for the perception component to detect the real world objects with both high accuracy and fast inference. We propose a novel neural network architecture along with the training and optimization details for detecting 3D objects in point cloud data. We compare the results with different backbone architectures including the standard ones like VGG, ResNet, Inception with our backbone. Also we present the optimization and ablation studies including designing an efficient anchor. We use the Kitti 3D Birds Eye View dataset for benchmarking and validating our results. Our work surpasses the state of the art in this domain both in terms of average precision and speed running at > 30 FPS. This makes it a feasible option to be deployed in real time applications including self driving cars.



### Photo style transfer with consistency losses
- **Arxiv ID**: http://arxiv.org/abs/2005.04408v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.04408v1)
- **Published**: 2020-05-09 09:58:06+00:00
- **Updated**: 2020-05-09 09:58:06+00:00
- **Authors**: Xu Yao, Gilles Puy, Patrick PÃ©rez
- **Comment**: None
- **Journal**: In 2019 IEEE International Conference on Image Processing (ICIP)
  (pp. 2314-2318). IEEE
- **Summary**: We address the problem of style transfer between two photos and propose a new way to preserve photorealism. Using the single pair of photos available as input, we train a pair of deep convolution networks (convnets), each of which transfers the style of one photo to the other. To enforce photorealism, we introduce a content preserving mechanism by combining a cycle-consistency loss with a self-consistency loss. Experimental results show that this method does not suffer from typical artifacts observed in methods working in the same settings. We then further analyze some properties of these trained convnets. First, we notice that they can be used to stylize other unseen images with same known style. Second, we show that retraining only a small subset of the network parameters can be sufficient to adapt these convnets to new styles.



### High Resolution Face Age Editing
- **Arxiv ID**: http://arxiv.org/abs/2005.04410v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.04410v1)
- **Published**: 2020-05-09 09:59:51+00:00
- **Updated**: 2020-05-09 09:59:51+00:00
- **Authors**: Xu Yao, Gilles Puy, Alasdair Newson, Yann Gousseau, Pierre Hellier
- **Comment**: None
- **Journal**: None
- **Summary**: Face age editing has become a crucial task in film post-production, and is also becoming popular for general purpose photography. Recently, adversarial training has produced some of the most visually impressive results for image manipulation, including the face aging/de-aging task. In spite of considerable progress, current methods often present visual artifacts and can only deal with low-resolution images. In order to achieve aging/de-aging with the high quality and robustness necessary for wider use, these problems need to be addressed. This is the goal of the present work. We present an encoder-decoder architecture for face age editing. The core idea of our network is to create both a latent space containing the face identity, and a feature modulation layer corresponding to the age of the individual. We then combine these two elements to produce an output image of the person with a desired target age. Our architecture is greatly simplified with respect to other approaches, and allows for continuous age editing on high resolution images in a single unified model.



### Memory-Augmented Relation Network for Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2005.04414v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.04414v2)
- **Published**: 2020-05-09 10:09:13+00:00
- **Updated**: 2020-05-13 03:25:36+00:00
- **Authors**: Jun He, Richang Hong, Xueliang Liu, Mingliang Xu, Zhengjun Zha, Meng Wang
- **Comment**: To be submitted to ACM Multimedia 2020
- **Journal**: None
- **Summary**: Metric-based few-shot learning methods concentrate on learning transferable feature embedding that generalizes well from seen categories to unseen categories under the supervision of limited number of labelled instances. However, most of them treat each individual instance in the working context separately without considering its relationships with the others. In this work, we investigate a new metric-learning method, Memory-Augmented Relation Network (MRN), to explicitly exploit these relationships. In particular, for an instance, we choose the samples that are visually similar from the working context, and perform weighted information propagation to attentively aggregate helpful information from the chosen ones to enhance its representation. In MRN, we also formulate the distance metric as a learnable relation module which learns to compare for similarity measurement, and augment the working context with memory slots, both contributing to its generality. We empirically demonstrate that MRN yields significant improvement over its ancestor and achieves competitive or even better performance when compared with other few-shot learning approaches on the two major benchmark datasets, i.e. miniImagenet and tieredImagenet.



### Multi-Task Learning in Histo-pathology for Widely Generalizable Model
- **Arxiv ID**: http://arxiv.org/abs/2005.08645v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.08645v1)
- **Published**: 2020-05-09 12:13:43+00:00
- **Updated**: 2020-05-09 12:13:43+00:00
- **Authors**: Jevgenij Gamper, Navid Alemi Kooohbanani, Nasir Rajpoot
- **Comment**: None
- **Journal**: AI4CC ICLR 2020 workshop
- **Summary**: In this work we show preliminary results of deep multi-task learning in the area of computational pathology. We combine 11 tasks ranging from patch-wise oral cancer classification, one of the most prevalent cancers in the developing world, to multi-tissue nuclei instance segmentation and classification.



### Building a Manga Dataset "Manga109" with Annotations for Multimedia Applications
- **Arxiv ID**: http://arxiv.org/abs/2005.04425v2
- **DOI**: 10.1109/MMUL.2020.2987895
- **Categories**: **cs.MM**, cs.CV, I.4
- **Links**: [PDF](http://arxiv.org/pdf/2005.04425v2)
- **Published**: 2020-05-09 12:26:58+00:00
- **Updated**: 2020-05-12 14:07:55+00:00
- **Authors**: Kiyoharu Aizawa, Azuma Fujimoto, Atsushi Otsubo, Toru Ogawa, Yusuke Matsui, Koki Tsubota, Hikaru Ikuta
- **Comment**: 10 pages, 8 figures
- **Journal**: IEEE MultiMedia 2020
- **Summary**: Manga, or comics, which are a type of multimodal artwork, have been left behind in the recent trend of deep learning applications because of the lack of a proper dataset. Hence, we built Manga109, a dataset consisting of a variety of 109 Japanese comic books (94 authors and 21,142 pages) and made it publicly available by obtaining author permissions for academic use. We carefully annotated the frames, speech texts, character faces, and character bodies; the total number of annotations exceeds 500k. This dataset provides numerous manga images and annotations, which will be beneficial for use in machine learning algorithms and their evaluation. In addition to academic use, we obtained further permission for a subset of the dataset for industrial use. In this article, we describe the details of the dataset and present a few examples of multimedia processing applications (detection, retrieval, and generation) that apply existing deep learning methods and are made possible by the dataset.



### Understanding Dynamic Scenes using Graph Convolution Networks
- **Arxiv ID**: http://arxiv.org/abs/2005.04437v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.04437v5)
- **Published**: 2020-05-09 13:05:06+00:00
- **Updated**: 2020-08-14 15:56:51+00:00
- **Authors**: Sravan Mylavarapu, Mahtab Sandhu, Priyesh Vijayan, K Madhava Krishna, Balaraman Ravindran, Anoop Namboodiri
- **Comment**: To appear at IROS 2020
- **Journal**: None
- **Summary**: We present a novel Multi-Relational Graph Convolutional Network (MRGCN) based framework to model on-road vehicle behaviors from a sequence of temporally ordered frames as grabbed by a moving monocular camera. The input to MRGCN is a multi-relational graph where the graph's nodes represent the active and passive agents/objects in the scene, and the bidirectional edges that connect every pair of nodes are encodings of their Spatio-temporal relations. We show that this proposed explicit encoding and usage of an intermediate spatio-temporal interaction graph to be well suited for our tasks over learning end-end directly on a set of temporally ordered spatial relations. We also propose an attention mechanism for MRGCNs that conditioned on the scene dynamically scores the importance of information from different interaction types. The proposed framework achieves significant performance gain over prior methods on vehicle-behavior classification tasks on four datasets. We also show a seamless transfer of learning to multiple datasets without resorting to fine-tuning. Such behavior prediction methods find immediate relevance in a variety of navigation tasks such as behavior planning, state estimation, and applications relating to the detection of traffic violations over videos.



### Vehicle Re-Identification Based on Complementary Features
- **Arxiv ID**: http://arxiv.org/abs/2005.04463v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.04463v1)
- **Published**: 2020-05-09 15:24:51+00:00
- **Updated**: 2020-05-09 15:24:51+00:00
- **Authors**: Cunyuan Gao, Yi Hu, Yi Zhang, Rui Yao, Yong Zhou, Jiaqi Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we present our solution to the vehicle re-identification (vehicle Re-ID) track in AI City Challenge 2020 (AIC2020). The purpose of vehicle Re-ID is to retrieve the same vehicle appeared across multiple cameras, and it could make a great contribution to the Intelligent Traffic System(ITS) and smart city. Due to the vehicle's orientation, lighting and inter-class similarity, it is difficult to achieve robust and discriminative representation feature. For the vehicle Re-ID track in AIC2020, our method is to fuse features extracted from different networks in order to take advantages of these networks and achieve complementary features. For each single model, several methods such as multi-loss, filter grafting, semi-supervised are used to increase the representation ability as better as possible. Top performance in City-Scale Multi-Camera Vehicle Re-Identification demonstrated the advantage of our methods, and we got 5-th place in the vehicle Re-ID track of AIC2020. The codes are available at https://github.com/gggcy/AIC2020_ReID.



### Visually Impaired Aid using Convolutional Neural Networks, Transfer Learning, and Particle Competition and Cooperation
- **Arxiv ID**: http://arxiv.org/abs/2005.04473v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2005.04473v1)
- **Published**: 2020-05-09 16:11:48+00:00
- **Updated**: 2020-05-09 16:11:48+00:00
- **Authors**: Fabricio Breve, Carlos Norberto Fischer
- **Comment**: BREVE, Fabricio Aparecido; FISCHER, Carlos Norberto. Visually
  Impaired Aid using Convolutional Neural Networks, Transfer Learning, and
  Particle Competition and Cooperation In: 2020 International Joint Conference
  on Neural Networks (IJCNN 2020), 2020, Glasgow, UK. Proceedings of 2020
  International Joint Conference on Neural Networks (IJCNN 2020), 2020.
  (accepted for publication)
- **Journal**: None
- **Summary**: Navigation and mobility are some of the major problems faced by visually impaired people in their daily lives. Advances in computer vision led to the proposal of some navigation systems. However, most of them require expensive and/or heavy hardware. In this paper we propose the use of convolutional neural networks (CNN), transfer learning, and semi-supervised learning (SSL) to build a framework aimed at the visually impaired aid. It has low computational costs and, therefore, may be implemented on current smartphones, without relying on any additional equipment. The smartphone camera can be used to automatically take pictures of the path ahead. Then, they will be immediately classified, providing almost instantaneous feedback to the user. We also propose a dataset to train the classifiers, including indoor and outdoor situations with different types of light, floor, and obstacles. Many different CNN architectures are evaluated as feature extractors and classifiers, by fine-tuning weights pre-trained on a much larger dataset. The graph-based SSL method, known as particle competition and cooperation, is also used for classification, allowing feedback from the user to be incorporated without retraining the underlying network. 92\% and 80\% classification accuracy is achieved in the proposed dataset in the best supervised and SSL scenarios, respectively.



### Human in Events: A Large-Scale Benchmark for Human-centric Video Analysis in Complex Events
- **Arxiv ID**: http://arxiv.org/abs/2005.04490v6
- **DOI**: 10.1007/s11263-023-01842-6
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.04490v6)
- **Published**: 2020-05-09 18:24:52+00:00
- **Updated**: 2023-07-13 13:23:05+00:00
- **Authors**: Weiyao Lin, Huabin Liu, Shizhan Liu, Yuxi Li, Rui Qian, Tao Wang, Ning Xu, Hongkai Xiong, Guo-Jun Qi, Nicu Sebe
- **Comment**: Dataset for Large-scale Human-centric Video Analysis in Complex
  Events (http://humaninevents.org), the paper has been published in Int J
  Comput Vis (2023)
- **Journal**: None
- **Summary**: Along with the development of modern smart cities, human-centric video analysis has been encountering the challenge of analyzing diverse and complex events in real scenes. A complex event relates to dense crowds, anomalous individuals, or collective behaviors. However, limited by the scale and coverage of existing video datasets, few human analysis approaches have reported their performances on such complex events. To this end, we present a new large-scale dataset with comprehensive annotations, named Human-in-Events or HiEve (Human-centric video analysis in complex Events), for the understanding of human motions, poses, and actions in a variety of realistic events, especially in crowd & complex events. It contains a record number of poses (>1M), the largest number of action instances (>56k) under complex events, as well as one of the largest numbers of trajectories lasting for longer time (with an average trajectory length of >480 frames). Based on its diverse annotation, we present two simple baselines for action recognition and pose estimation, respectively. They leverage cross-label information during training to enhance the feature learning in corresponding visual tasks. Experiments show that they could boost the performance of existing action recognition and pose estimation pipelines. More importantly, they prove the widely ranged annotations in HiEve can improve various video tasks. Furthermore, we conduct extensive experiments to benchmark recent video analysis approaches together with our baseline methods, demonstrating HiEve is a challenging dataset for human-centric video analysis. We expect that the dataset will advance the development of cutting-edge techniques in human-centric analysis and the understanding of complex events. The dataset is available at http://humaninevents.org



### Generative Model-driven Structure Aligning Discriminative Embeddings for Transductive Zero-shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2005.04492v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.04492v1)
- **Published**: 2020-05-09 18:48:20+00:00
- **Updated**: 2020-05-09 18:48:20+00:00
- **Authors**: Omkar Gune, Mainak Pal, Preeti Mukherjee, Biplab Banerjee, Subhasis Chaudhuri
- **Comment**: None
- **Journal**: None
- **Summary**: Zero-shot Learning (ZSL) is a transfer learning technique which aims at transferring knowledge from seen classes to unseen classes. This knowledge transfer is possible because of underlying semantic space which is common to seen and unseen classes. Most existing approaches learn a projection function using labelled seen class data which maps visual data to semantic data. In this work, we propose a shallow but effective neural network-based model for learning such a projection function which aligns the visual and semantic data in the latent space while simultaneously making the latent space embeddings discriminative. As the above projection function is learned using the seen class data, the so-called projection domain shift exists. We propose a transductive approach to reduce the effect of domain shift, where we utilize unlabeled visual data from unseen classes to generate corresponding semantic features for unseen class visual samples. While these semantic features are initially generated using a conditional variational auto-encoder, they are used along with the seen class data to improve the projection function. We experiment on both inductive and transductive setting of ZSL and generalized ZSL and show superior performance on standard benchmark datasets AWA1, AWA2, CUB, SUN, FLO, and APY. We also show the efficacy of our model in the case of extremely less labelled data regime on different datasets in the context of ZSL.



