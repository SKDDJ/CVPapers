# Arxiv Papers in cs.CV on 2020-05-03
### Deep Generative Adversarial Residual Convolutional Networks for Real-World Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2005.00953v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.00953v1)
- **Published**: 2020-05-03 00:12:38+00:00
- **Updated**: 2020-05-03 00:12:38+00:00
- **Authors**: Rao Muhammad Umer, Gian Luca Foresti, Christian Micheloni
- **Comment**: None
- **Journal**: None
- **Summary**: Most current deep learning based single image super-resolution (SISR) methods focus on designing deeper / wider models to learn the non-linear mapping between low-resolution (LR) inputs and the high-resolution (HR) outputs from a large number of paired (LR/HR) training data. They usually take as assumption that the LR image is a bicubic down-sampled version of the HR image. However, such degradation process is not available in real-world settings i.e. inherent sensor noise, stochastic noise, compression artifacts, possible mismatch between image degradation process and camera device. It reduces significantly the performance of current SISR methods due to real-world image corruptions. To address these problems, we propose a deep Super-Resolution Residual Convolutional Generative Adversarial Network (SRResCGAN) to follow the real-world degradation settings by adversarial training the model with pixel-wise supervision in the HR domain from its generated LR counterpart. The proposed network exploits the residual learning by minimizing the energy-based objective function with powerful image regularization and convex optimization techniques. We demonstrate our proposed approach in quantitative and qualitative experiments that generalize robustly to real input and it is easy to deploy for other down-scaling operators and mobile/embedded devices.



### On the Convergence Rate of Projected Gradient Descent for a Back-Projection based Objective
- **Arxiv ID**: http://arxiv.org/abs/2005.00959v3
- **DOI**: None
- **Categories**: **math.OC**, cs.CV, cs.LG, cs.NA, math.NA, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.00959v3)
- **Published**: 2020-05-03 00:58:23+00:00
- **Updated**: 2021-08-08 12:40:58+00:00
- **Authors**: Tom Tirer, Raja Giryes
- **Comment**: Accepted to SIAM Journal on Imaging Sciences (SIIMS)
- **Journal**: None
- **Summary**: Ill-posed linear inverse problems appear in many scientific setups, and are typically addressed by solving optimization problems, which are composed of data fidelity and prior terms. Recently, several works have considered a back-projection (BP) based fidelity term as an alternative to the common least squares (LS), and demonstrated excellent results for popular inverse problems. These works have also empirically shown that using the BP term, rather than the LS term, requires fewer iterations of optimization algorithms. In this paper, we examine the convergence rate of the projected gradient descent (PGD) algorithm for the BP objective. Our analysis allows to identify an inherent source for its faster convergence compared to using the LS objective, while making only mild assumptions. We also analyze the more general proximal gradient method under a relaxed contraction condition on the proximal mapping of the prior. This analysis further highlights the advantage of BP when the linear measurement operator is badly conditioned. Numerical experiments with both $\ell_1$-norm and GAN-based priors corroborate our theoretical results.



### Boundary-aware Context Neural Network for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2005.00966v1
- **DOI**: 10.1016/j.media.2022.102395
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.00966v1)
- **Published**: 2020-05-03 02:35:49+00:00
- **Updated**: 2020-05-03 02:35:49+00:00
- **Authors**: Ruxin Wang, Shuyuan Chen, Chaojie Ji, Jianping Fan, Ye Li
- **Comment**: None
- **Journal**: Medical Image Analysis, 2022
- **Summary**: Medical image segmentation can provide a reliable basis for further clinical analysis and disease diagnosis. The performance of medical image segmentation has been significantly advanced with the convolutional neural networks (CNNs). However, most existing CNNs-based methods often produce unsatisfactory segmentation mask without accurate object boundaries. This is caused by the limited context information and inadequate discriminative feature maps after consecutive pooling and convolution operations. In that the medical image is characterized by the high intra-class variation, inter-class indistinction and noise, extracting powerful context and aggregating discriminative features for fine-grained segmentation are still challenging today. In this paper, we formulate a boundary-aware context neural network (BA-Net) for 2D medical image segmentation to capture richer context and preserve fine spatial information. BA-Net adopts encoder-decoder architecture. In each stage of encoder network, pyramid edge extraction module is proposed for obtaining edge information with multiple granularities firstly. Then we design a mini multi-task learning module for jointly learning to segment object masks and detect lesion boundaries. In particular, a new interactive attention is proposed to bridge two tasks for achieving information complementarity between different tasks, which effectively leverages the boundary information for offering a strong cue to better segmentation prediction. At last, a cross feature fusion module aims to selectively aggregate multi-level features from the whole encoder network. By cascaded three modules, richer context and fine-grain features of each stage are encoded. Extensive experiments on five datasets show that the proposed BA-Net outperforms state-of-the-art approaches.



### Self-Training with Improved Regularization for Sample-Efficient Chest X-Ray Classification
- **Arxiv ID**: http://arxiv.org/abs/2005.02231v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.02231v2)
- **Published**: 2020-05-03 02:36:00+00:00
- **Updated**: 2021-02-10 18:46:26+00:00
- **Authors**: Deepta Rajan, Jayaraman J. Thiagarajan, Alexandros Karargyris, Satyananda Kashyap
- **Comment**: None
- **Journal**: None
- **Summary**: Automated diagnostic assistants in healthcare necessitate accurate AI models that can be trained with limited labeled data, can cope with severe class imbalances and can support simultaneous prediction of multiple disease conditions. To this end, we present a deep learning framework that utilizes a number of key components to enable robust modeling in such challenging scenarios. Using an important use-case in chest X-ray classification, we provide several key insights on the effective use of data augmentation, self-training via distillation and confidence tempering for small data learning in medical imaging. Our results show that using 85% lesser labeled data, we can build predictive models that match the performance of classifiers trained in a large-scale data setting.



### Lossy Event Compression based on Image-derived Quad Trees and Poisson Disk Sampling
- **Arxiv ID**: http://arxiv.org/abs/2005.00974v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2005.00974v2)
- **Published**: 2020-05-03 03:18:43+00:00
- **Updated**: 2020-12-01 07:41:48+00:00
- **Authors**: Srutarshi Banerjee, Zihao W. Wang, Henry H. Chopp, Oliver Cossairt, Aggelos Katsaggelos
- **Comment**: 8 main pages
- **Journal**: None
- **Summary**: With several advantages over conventional RGB cameras, event cameras have provided new opportunities for tackling visual tasks under challenging scenarios with fast motion, high dynamic range, and/or power constraint. Yet unlike image/video compression, the performance of event compression algorithm is far from satisfying and practical. The main challenge for compressing events is the unique event data form, i.e., a stream of asynchronously fired event tuples each encoding the 2D spatial location, timestamp, and polarity (denoting an increase or decrease in brightness). Since events only encode temporal variations, they lack spatial structure which is crucial for compression. To address this problem, we propose a novel event compression algorithm based on a quad tree (QT) segmentation map derived from the adjacent intensity images. The QT informs 2D spatial priority within the 3D space-time volume. In the event encoding step, events are first aggregated over time to form polarity-based event histograms. The histograms are then variably sampled via Poisson Disk Sampling prioritized by the QT based segmentation map. Next, differential encoding and run length encoding are employed for encoding the spatial and polarity information of the sampled events, respectively, followed by Huffman encoding to produce the final encoded events. Our Poisson Disk Sampling based Lossy Event Compression (PDS-LEC) algorithm performs rate-distortion based optimal allocation. On average, our algorithm achieves greater than 6x compression compared to the state of the art.



### A Concise yet Effective model for Non-Aligned Incomplete Multi-view and Missing Multi-label Learning
- **Arxiv ID**: http://arxiv.org/abs/2005.00976v2
- **DOI**: 10.1109/TPAMI.2021.3086895
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.00976v2)
- **Published**: 2020-05-03 03:38:24+00:00
- **Updated**: 2021-06-08 12:01:24+00:00
- **Authors**: Xiang Li, Songcan Chen
- **Comment**: 15 pages, 7 figures
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence
  2021
- **Summary**: In reality, learning from multi-view multi-label data inevitably confronts three challenges: missing labels, incomplete views, and non-aligned views. Existing methods mainly concern the first two and commonly need multiple assumptions to attack them, making even state-of-the-arts involve at least two explicit hyper-parameters such that model selection is quite difficult. More roughly, they will fail in handling the third challenge, let alone addressing the three jointly. In this paper, we aim at meeting these under the least assumption by building a concise yet effective model with just one hyper-parameter. To ease insufficiency of available labels, we exploit not only the consensus of multiple views but also the global and local structures hidden among multiple labels. Specifically, we introduce an indicator matrix to tackle the first two challenges in a regression form while aligning the same individual labels and all labels of different views in a common label space to battle the third challenge. In aligning, we characterize the global and local structures of multiple labels to be high-rank and low-rank, respectively. Subsequently, an efficient algorithm with linear time complexity in the number of samples is established. Finally, even without view-alignment, our method substantially outperforms state-of-the-arts with view-alignment on five real datasets.



### Joint-SRVDNet: Joint Super Resolution and Vehicle Detection Network
- **Arxiv ID**: http://arxiv.org/abs/2005.00983v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.00983v1)
- **Published**: 2020-05-03 04:28:44+00:00
- **Updated**: 2020-05-03 04:28:44+00:00
- **Authors**: Moktari Mostofa, Syeda Nyma Ferdous, Benjamin S. Riggan, Nasser M. Nasrabadi
- **Comment**: None
- **Journal**: None
- **Summary**: In many domestic and military applications, aerial vehicle detection and super-resolutionalgorithms are frequently developed and applied independently. However, aerial vehicle detection on super-resolved images remains a challenging task due to the lack of discriminative information in the super-resolved images. To address this problem, we propose a Joint Super-Resolution and Vehicle DetectionNetwork (Joint-SRVDNet) that tries to generate discriminative, high-resolution images of vehicles fromlow-resolution aerial images. First, aerial images are up-scaled by a factor of 4x using a Multi-scaleGenerative Adversarial Network (MsGAN), which has multiple intermediate outputs with increasingresolutions. Second, a detector is trained on super-resolved images that are upscaled by factor 4x usingMsGAN architecture and finally, the detection loss is minimized jointly with the super-resolution loss toencourage the target detector to be sensitive to the subsequent super-resolution training. The network jointlylearns hierarchical and discriminative features of targets and produces optimal super-resolution results. Weperform both quantitative and qualitative evaluation of our proposed network on VEDAI, xView and DOTAdatasets. The experimental results show that our proposed framework achieves better visual quality than thestate-of-the-art methods for aerial super-resolution with 4x up-scaling factor and improves the accuracy ofaerial vehicle detection.



### Using Artificial Intelligence to Analyze Fashion Trends
- **Arxiv ID**: http://arxiv.org/abs/2005.00986v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2005.00986v1)
- **Published**: 2020-05-03 04:46:12+00:00
- **Updated**: 2020-05-03 04:46:12+00:00
- **Authors**: Mengyun Shi, Van Dyk Lewis
- **Comment**: None
- **Journal**: None
- **Summary**: Analyzing fashion trends is essential in the fashion industry. Current fashion forecasting firms, such as WGSN, utilize the visual information from around the world to analyze and predict fashion trends. However, analyzing fashion trends is time-consuming and extremely labor intensive, requiring individual employees' manual editing and classification. To improve the efficiency of data analysis of such image-based information and lower the cost of analyzing fashion images, this study proposes a data-driven quantitative abstracting approach using an artificial intelligence (A.I.) algorithm. Specifically, an A.I. model was trained on fashion images from a large-scale dataset under different scenarios, for example in online stores and street snapshots. This model was used to detect garments and classify clothing attributes such as textures, garment style, and details for runway photos and videos. It was found that the A.I. model can generate rich attribute descriptions of detected regions and accurately bind the garments in the images. Adoption of A.I. algorithm demonstrated promising results and the potential to classify garment types and details automatically, which can make the process of trend forecasting more cost-effective and faster.



### Explaining How Deep Neural Networks Forget by Deep Visualization
- **Arxiv ID**: http://arxiv.org/abs/2005.01004v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.01004v3)
- **Published**: 2020-05-03 06:44:38+00:00
- **Updated**: 2022-07-22 22:18:18+00:00
- **Authors**: Giang Nguyen, Shuan Chen, Tae Joon Jun, Daeyoung Kim
- **Comment**: 12 pages, 4 figures, 1 table. arXiv admin note: substantial text
  overlap with arXiv:2001.01578. Keywords: XAI, Catastrophic Forgetting,
  Attribution map, Continual Learning and Regularization
- **Journal**: ICPR 2020
- **Summary**: Explaining the behaviors of deep neural networks, usually considered as black boxes, is critical especially when they are now being adopted over diverse aspects of human life. Taking the advantages of interpretable machine learning (interpretable ML), this paper proposes a novel tool called Catastrophic Forgetting Dissector (or CFD) to explain catastrophic forgetting in continual learning settings. We also introduce a new method called Critical Freezing based on the observations of our tool. Experiments on ResNet articulate how catastrophic forgetting happens, particularly showing which components of this famous network are forgetting. Our new continual learning algorithm defeats various recent techniques by a significant margin, proving the capability of the investigation. Critical freezing not only attacks catastrophic forgetting but also exposes explainability.



### Feature-metric Registration: A Fast Semi-supervised Approach for Robust Point Cloud Registration without Correspondences
- **Arxiv ID**: http://arxiv.org/abs/2005.01014v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.01014v1)
- **Published**: 2020-05-03 07:26:59+00:00
- **Updated**: 2020-05-03 07:26:59+00:00
- **Authors**: Xiaoshui Huang, Guofeng Mei, Jian Zhang
- **Comment**: CVPR2020 final
- **Journal**: None
- **Summary**: We present a fast feature-metric point cloud registration framework, which enforces the optimisation of registration by minimising a feature-metric projection error without correspondences. The advantage of the feature-metric projection error is robust to noise, outliers and density difference in contrast to the geometric projection error. Besides, minimising the feature-metric projection error does not need to search the correspondences so that the optimisation speed is fast. The principle behind the proposed method is that the feature difference is smallest if point clouds are aligned very well. We train the proposed method in a semi-supervised or unsupervised approach, which requires limited or no registration label data. Experiments demonstrate our method obtains higher accuracy and robustness than the state-of-the-art methods. Besides, experimental results show that the proposed method can handle significant noise and density difference, and solve both same-source and cross-source point cloud registration.



### Lupulus: A Flexible Hardware Accelerator for Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2005.01016v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.AR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.01016v1)
- **Published**: 2020-05-03 07:35:36+00:00
- **Updated**: 2020-05-03 07:35:36+00:00
- **Authors**: Andreas Toftegaard Kristensen, Robert Giterman, Alexios Balatsoukas-Stimming, Andreas Burg
- **Comment**: To be presented at the 2020 International Conference on Acoustics,
  Speech, and Signal Processing
- **Journal**: None
- **Summary**: Neural networks have become indispensable for a wide range of applications, but they suffer from high computational- and memory-requirements, requiring optimizations from the algorithmic description of the network to the hardware implementation. Moreover, the high rate of innovation in machine learning makes it important that hardware implementations provide a high level of programmability to support current and future requirements of neural networks. In this work, we present a flexible hardware accelerator for neural networks, called Lupulus, supporting various methods for scheduling and mapping of operations onto the accelerator. Lupulus was implemented in a 28nm FD-SOI technology and demonstrates a peak performance of 380 GOPS/GHz with latencies of 21.4ms and 183.6ms for the convolutional layers of AlexNet and VGG-16, respectively.



### Fusion of visible and infrared images via complex function
- **Arxiv ID**: http://arxiv.org/abs/2005.01047v1
- **DOI**: 10.33577/2312-4458.22.2020
- **Categories**: **eess.IV**, cs.CV, I.4.5; I.4.8; C.7; J.7
- **Links**: [PDF](http://arxiv.org/pdf/2005.01047v1)
- **Published**: 2020-05-03 10:55:31+00:00
- **Updated**: 2020-05-03 10:55:31+00:00
- **Authors**: Ya. Ye. Khaustov, D. Ye, Ye. Ryzhov, E. Lychkovskyy, Yu. A. Nastishin
- **Comment**: 12 pages with 7 figures, submitted to Military Technical Collection
  22 (2020) see http://vtz.asv.gov.ua
- **Journal**: None
- **Summary**: We propose an algorithm for the fusion of partial images collected from the visual and infrared cameras such that the visual and infrared images are the real and imaginary parts of a complex function. The proposed image fusion algorithm of the complex function is a generalization for the algorithm of conventional image addition in the same way as the addition of complex numbers is the generalization for the addition of real numbers. The proposed algorithm of the complex function is simple in use and non-demanding in computer power. The complex form of the fused image opens a possibility to form the fused image either as the amplitude image or as a phase image, which in turn can be in several forms. We show theoretically that the local contrast of the fused phase images is higher than those of the partial images as well as in comparison with the images obtained by the algorithm of the simple or weighted addition. Experimental image quality assessment of the fused phase images performed using the histograms, entropy shows the higher quality of the phase images in comparison with those of the input partial images as well as those obtained with different fusion methods reported in the literature. Keywords: digital image processing, image fusion, infrared imaging, image quality assessment



### NTIRE 2020 Challenge on Perceptual Extreme Super-Resolution: Methods and Results
- **Arxiv ID**: http://arxiv.org/abs/2005.01056v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.01056v1)
- **Published**: 2020-05-03 11:30:51+00:00
- **Updated**: 2020-05-03 11:30:51+00:00
- **Authors**: Kai Zhang, Shuhang Gu, Radu Timofte, Taizhang Shang, Qiuju Dai, Shengchen Zhu, Tong Yang, Yandong Guo, Younghyun Jo, Sejong Yang, Seon Joo Kim, Lin Zha, Jiande Jiang, Xinbo Gao, Wen Lu, Jing Liu, Kwangjin Yoon, Taegyun Jeon, Kazutoshi Akita, Takeru Ooba, Norimichi Ukita, Zhipeng Luo, Yuehan Yao, Zhenyu Xu, Dongliang He, Wenhao Wu, Yukang Ding, Chao Li, Fu Li, Shilei Wen, Jianwei Li, Fuzhi Yang, Huan Yang, Jianlong Fu, Byung-Hoon Kim, JaeHyun Baek, Jong Chul Ye, Yuchen Fan, Thomas S. Huang, Junyeop Lee, Bokyeung Lee, Jungki Min, Gwantae Kim, Kanghyu Lee, Jaihyun Park, Mykola Mykhailych, Haoyu Zhong, Yukai Shi, Xiaojun Yang, Zhijing Yang, Liang Lin, Tongtong Zhao, Jinjia Peng, Huibing Wang, Zhi Jin, Jiahao Wu, Yifu Chen, Chenming Shang, Huanrong Zhang, Jeongki Min, Hrishikesh P S, Densen Puthussery, Jiji C V
- **Comment**: CVPRW 2020
- **Journal**: None
- **Summary**: This paper reviews the NTIRE 2020 challenge on perceptual extreme super-resolution with focus on proposed solutions and results. The challenge task was to super-resolve an input image with a magnification factor 16 based on a set of prior examples of low and corresponding high resolution images. The goal is to obtain a network design capable to produce high resolution results with the best perceptual quality and similar to the ground truth. The track had 280 registered participants, and 19 teams submitted the final results. They gauge the state-of-the-art in single image super-resolution.



### A Little Bit More: Bitplane-Wise Bit-Depth Recovery
- **Arxiv ID**: http://arxiv.org/abs/2005.01091v2
- **DOI**: 10.1109/TPAMI.2021.3125692
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.01091v2)
- **Published**: 2020-05-03 14:06:33+00:00
- **Updated**: 2021-12-22 06:32:20+00:00
- **Authors**: Abhijith Punnappurath, Michael S. Brown
- **Comment**: None
- **Journal**: A. Punnappurath and M. S. Brown, "A Little Bit More: Bitplane-Wise
  Bit-Depth Recovery," in IEEE Transactions on Pattern Analysis and Machine
  Intelligence, doi: 10.1109/TPAMI.2021.3125692
- **Summary**: Imaging sensors digitize incoming scene light at a dynamic range of 10--12 bits (i.e., 1024--4096 tonal values). The sensor image is then processed onboard the camera and finally quantized to only 8 bits (i.e., 256 tonal values) to conform to prevailing encoding standards. There are a number of important applications, such as high-bit-depth displays and photo editing, where it is beneficial to recover the lost bit depth. Deep neural networks are effective at this bit-depth reconstruction task. Given the quantized low-bit-depth image as input, existing deep learning methods employ a single-shot approach that attempts to either (1) directly estimate the high-bit-depth image, or (2) directly estimate the residual between the high- and low-bit-depth images. In contrast, we propose a training and inference strategy that recovers the residual image bitplane-by-bitplane. Our bitplane-wise learning framework has the advantage of allowing for multiple levels of supervision during training and is able to obtain state-of-the-art results using a simple network architecture. We test our proposed method extensively on several image datasets and demonstrate an improvement from 0.5dB to 2.3dB PSNR over prior methods depending on the quantization level.



### Remote Sensing Image Scene Classification Meets Deep Learning: Challenges, Methods, Benchmarks, and Opportunities
- **Arxiv ID**: http://arxiv.org/abs/2005.01094v2
- **DOI**: 10.1109/JSTARS.2020.3005403
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.01094v2)
- **Published**: 2020-05-03 14:18:00+00:00
- **Updated**: 2020-06-25 09:58:28+00:00
- **Authors**: Gong Cheng, Xingxing Xie, Junwei Han, Lei Guo, Gui-Song Xia
- **Comment**: This manuscript is the accepted version for IEEE JSTARS
- **Journal**: IEEE Journal of Selected Topics in Applied Earth Observations and
  Remote Sensing, 13: 3735-3756, 2020
- **Summary**: Remote sensing image scene classification, which aims at labeling remote sensing images with a set of semantic categories based on their contents, has broad applications in a range of fields. Propelled by the powerful feature learning capabilities of deep neural networks, remote sensing image scene classification driven by deep learning has drawn remarkable attention and achieved significant breakthroughs. However, to the best of our knowledge, a comprehensive review of recent achievements regarding deep learning for scene classification of remote sensing images is still lacking. Considering the rapid evolution of this field, this paper provides a systematic survey of deep learning methods for remote sensing image scene classification by covering more than 160 papers. To be specific, we discuss the main challenges of remote sensing image scene classification and survey (1) Autoencoder-based remote sensing image scene classification methods, (2) Convolutional Neural Network-based remote sensing image scene classification methods, and (3) Generative Adversarial Network-based remote sensing image scene classification methods. In addition, we introduce the benchmarks used for remote sensing image scene classification and summarize the performance of more than two dozen of representative algorithms on three commonly-used benchmark data sets. Finally, we discuss the promising opportunities for further research.



### Deep Encoder-Decoder Neural Network for Fingerprint Image Denoising and Inpainting
- **Arxiv ID**: http://arxiv.org/abs/2005.01115v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.01115v1)
- **Published**: 2020-05-03 15:24:22+00:00
- **Updated**: 2020-05-03 15:24:22+00:00
- **Authors**: Weiya Fan
- **Comment**: None
- **Journal**: None
- **Summary**: Fingerprint image denoising is a very important step in fingerprint identification. to improve the denoising effect of fingerprint image,we have designs a fingerprint denoising algorithm based on deep encoder-decoder network,which encoder subnet to learn the fingerprint features of noisy images.the decoder subnet reconstructs the original fingerprint image based on the features to achieve denoising, while using the dilated convolution in the network to increase the receptor field without increasing the complexity and improve the network inference speed. In addition, feature fusion at different levels of the network is achieved through the introduction of residual learning, which further restores the detailed features of the fingerprint and improves the denoising effect. Finally, the experimental results show that the algorithm enables better recovery of edge, line and curve features in fingerprint images, with better visual effects and higher peak signal-to-noise ratio (PSNR) compared to other methods.



### Multi-focus Image Fusion: A Benchmark
- **Arxiv ID**: http://arxiv.org/abs/2005.01116v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.01116v1)
- **Published**: 2020-05-03 15:25:43+00:00
- **Updated**: 2020-05-03 15:25:43+00:00
- **Authors**: Xingchen Zhang
- **Comment**: 12 pages, 4 figures, 6 tables
- **Journal**: None
- **Summary**: Multi-focus image fusion (MFIF) has attracted considerable interests due to its numerous applications. While much progress has been made in recent years with efforts on developing various MFIF algorithms, some issues significantly hinder the fair and comprehensive performance comparison of MFIF methods, such as the lack of large-scale test set and the random choices of objective evaluation metrics in the literature. To solve these issues, this paper presents a multi-focus image fusion benchmark (MFIFB) which consists a test set of 105 image pairs, a code library of 30 MFIF algorithms, and 20 evaluation metrics. MFIFB is the first benchmark in the field of MFIF and provides the community a platform to compare MFIF algorithms fairly and comprehensively. Extensive experiments have been conducted using the proposed MFIFB to understand the performance of these algorithms. By analyzing the experimental results, effective MFIF algorithms are identified. More importantly, some observations on the status of the MFIF field are given, which can help to understand this field better.



### Mutual Information Gradient Estimation for Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2005.01123v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.01123v1)
- **Published**: 2020-05-03 16:05:58+00:00
- **Updated**: 2020-05-03 16:05:58+00:00
- **Authors**: Liangjian Wen, Yiji Zhou, Lirong He, Mingyuan Zhou, Zenglin Xu
- **Comment**: ICLR 2020
- **Journal**: None
- **Summary**: Mutual Information (MI) plays an important role in representation learning. However, MI is unfortunately intractable in continuous and high-dimensional settings. Recent advances establish tractable and scalable MI estimators to discover useful representation. However, most of the existing methods are not capable of providing an accurate estimation of MI with low-variance when the MI is large. We argue that directly estimating the gradients of MI is more appealing for representation learning than estimating MI in itself. To this end, we propose the Mutual Information Gradient Estimator (MIGE) for representation learning based on the score estimation of implicit distributions. MIGE exhibits a tight and smooth gradient estimation of MI in the high-dimensional and large-MI settings. We expand the applications of MIGE in both unsupervised learning of deep representations based on InfoMax and the Information Bottleneck method. Experimental results have indicated significant performance improvement in learning useful representation.



### Minor Privacy Protection Through Real-time Video Processing at the Edge
- **Arxiv ID**: http://arxiv.org/abs/2005.01178v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.01178v1)
- **Published**: 2020-05-03 20:19:15+00:00
- **Updated**: 2020-05-03 20:19:15+00:00
- **Authors**: Meng Yuan, Seyed Yahya Nikouei, Alem Fitwi, Yu Chen, Yunxi Dong
- **Comment**: Accepted by the 2nd International Workshop on Smart City
  Communication and Networking at the ICCCN 2020
- **Journal**: None
- **Summary**: The collection of a lot of personal information about individuals, including the minor members of a family, by closed-circuit television (CCTV) cameras creates a lot of privacy concerns. Particularly, revealing children's identifications or activities may compromise their well-being. In this paper, we investigate lightweight solutions that are affordable to edge surveillance systems, which is made feasible and accurate to identify minors such that appropriate privacy-preserving measures can be applied accordingly. State of the art deep learning architectures are modified and re-purposed in a cascaded fashion to maximize the accuracy of our model. A pipeline extracts faces from the input frames and classifies each one to be of an adult or a child. Over 20,000 labeled sample points are used for classification. We explore the timing and resources needed for such a model to be used in the Edge-Fog architecture at the edge of the network, where we can achieve near real-time performance on the CPU. Quantitative experimental results show the superiority of our proposed model with an accuracy of 92.1% in classification compared to some other face recognition based child detection approaches.



