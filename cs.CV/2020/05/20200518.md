# Arxiv Papers in cs.CV on 2020-05-18
### Single-sample writers -- "Document Filter" and their impacts on writer identification
- **Arxiv ID**: http://arxiv.org/abs/2005.08424v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.08424v1)
- **Published**: 2020-05-18 02:02:31+00:00
- **Updated**: 2020-05-18 02:02:31+00:00
- **Authors**: Fabio Pinhelli, Alceu S. Britto Jr, Luiz S. Oliveira, Yandre M. G. Costa, Diego Bertolini
- **Comment**: None
- **Journal**: None
- **Summary**: The writing can be used as an important biometric modality which allows to unequivocally identify an individual. It happens because the writing of two different persons present differences that can be explored both in terms of graphometric properties or even by addressing the manuscript as a digital image, taking into account the use of image processing techniques that can properly capture different visual attributes of the image (e.g. texture). In this work, perform a detailed study in which we dissect whether or not the use of a database with only a single sample taken from some writers may skew the results obtained in the experimental protocol. In this sense, we propose here what we call "document filter". The "document filter" protocol is supposed to be used as a preprocessing technique, such a way that all the data taken from fragments of the same document must be placed either into the training or into the test set. The rationale behind it, is that the classifier must capture the features from the writer itself, and not features regarding other particularities which could affect the writing in a specific document (i.e. emotional state of the writer, pen used, paper type, and etc.). By analyzing the literature, one can find several works dealing the writer identification problem. However, the performance of the writer identification systems must be evaluated also taking into account the occurrence of writer volunteers who contributed with a single sample during the creation of the manuscript databases. To address the open issue investigated here, a comprehensive set of experiments was performed on the IAM, BFL and CVL databases. They have shown that, in the most extreme case, the recognition rate obtained using the "document filter" protocol drops from 81.80% to 50.37%.



### Deep Learning and Bayesian Deep Learning Based Gender Prediction in Multi-Scale Brain Functional Connectivity
- **Arxiv ID**: http://arxiv.org/abs/2005.08431v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML, I.5.4; I.4.7
- **Links**: [PDF](http://arxiv.org/pdf/2005.08431v1)
- **Published**: 2020-05-18 02:43:26+00:00
- **Updated**: 2020-05-18 02:43:26+00:00
- **Authors**: Gengyan Zhao, Gyujoon Hwang, Cole J. Cook, Fang Liu, Mary E. Meyerand, Rasmus M. Birn
- **Comment**: 40 pages, 10 figures
- **Journal**: None
- **Summary**: Brain gender differences have been known for a long time and are the possible reason for many psychological, psychiatric and behavioral differences between males and females. Predicting genders from brain functional connectivity (FC) can build the relationship between brain activities and gender, and extracting important gender related FC features from the prediction model offers a way to investigate the brain gender difference. Current predictive models applied to gender prediction demonstrate good accuracies, but usually extract individual functional connections instead of connectivity patterns in the whole connectivity matrix as features. In addition, current models often omit the effect of the input brain FC scale on prediction and cannot give any model uncertainty information. Hence, in this study we propose to predict gender from multiple scales of brain FC with deep learning, which can extract full FC patterns as features. We further develop the understanding of the feature extraction mechanism in deep neural network (DNN) and propose a DNN feature ranking method to extract the highly important features based on their contributions to the prediction. Moreover, we apply Bayesian deep learning to the brain FC gender prediction, which as a probabilistic model can not only make accurate predictions but also generate model uncertainty for each prediction. Experiments were done on the high-quality Human Connectome Project S1200 release dataset comprising the resting state functional MRI data of 1003 healthy adults. First, DNN reaches 83.0%, 87.6%, 92.0%, 93.5% and 94.1% accuracies respectively with the FC input derived from 25, 50, 100, 200, 300 independent component analysis (ICA) components. DNN outperforms the conventional machine learning methods on the 25-ICA-component scale FC, but the linear machine learning method catches up as the number of ICA components increases...



### Deep Convolutional Sparse Coding Networks for Image Fusion
- **Arxiv ID**: http://arxiv.org/abs/2005.08448v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2005.08448v1)
- **Published**: 2020-05-18 04:12:01+00:00
- **Updated**: 2020-05-18 04:12:01+00:00
- **Authors**: Shuang Xu, Zixiang Zhao, Yicheng Wang, Chunxia Zhang, Junmin Liu, Jiangshe Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Image fusion is a significant problem in many fields including digital photography, computational imaging and remote sensing, to name but a few. Recently, deep learning has emerged as an important tool for image fusion. This paper presents three deep convolutional sparse coding (CSC) networks for three kinds of image fusion tasks (i.e., infrared and visible image fusion, multi-exposure image fusion, and multi-modal image fusion). The CSC model and the iterative shrinkage and thresholding algorithm are generalized into dictionary convolution units. As a result, all hyper-parameters are learned from data. Our extensive experiments and comprehensive comparisons reveal the superiority of the proposed networks with regard to quantitative evaluation and visual inspection.



### Cross-Task Transfer for Geotagged Audiovisual Aerial Scene Recognition
- **Arxiv ID**: http://arxiv.org/abs/2005.08449v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2005.08449v2)
- **Published**: 2020-05-18 04:14:16+00:00
- **Updated**: 2020-07-16 03:33:17+00:00
- **Authors**: Di Hu, Xuhong Li, Lichao Mou, Pu Jin, Dong Chen, Liping Jing, Xiaoxiang Zhu, Dejing Dou
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: Aerial scene recognition is a fundamental task in remote sensing and has recently received increased interest. While the visual information from overhead images with powerful models and efficient algorithms yields considerable performance on scene recognition, it still suffers from the variation of ground objects, lighting conditions etc. Inspired by the multi-channel perception theory in cognition science, in this paper, for improving the performance on the aerial scene recognition, we explore a novel audiovisual aerial scene recognition task using both images and sounds as input. Based on an observation that some specific sound events are more likely to be heard at a given geographic location, we propose to exploit the knowledge from the sound events to improve the performance on the aerial scene recognition. For this purpose, we have constructed a new dataset named AuDio Visual Aerial sceNe reCognition datasEt (ADVANCE). With the help of this dataset, we evaluate three proposed approaches for transferring the sound event knowledge to the aerial scene recognition task in a multimodal learning framework, and show the benefit of exploiting the audio information for the aerial scene recognition. The source code is publicly available for reproducibility purposes.



### Large-Scale Object Detection in the Wild from Imbalanced Multi-Labels
- **Arxiv ID**: http://arxiv.org/abs/2005.08455v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.08455v1)
- **Published**: 2020-05-18 04:36:36+00:00
- **Updated**: 2020-05-18 04:36:36+00:00
- **Authors**: Junran Peng, Xingyuan Bu, Ming Sun, Zhaoxiang Zhang, Tieniu Tan, Junjie Yan
- **Comment**: CVPR2020 oral. The first two authors contribute equally
- **Journal**: None
- **Summary**: Training with more data has always been the most stable and effective way of improving performance in deep learning era. As the largest object detection dataset so far, Open Images brings great opportunities and challenges for object detection in general and sophisticated scenarios. However, owing to its semi-automatic collecting and labeling pipeline to deal with the huge data scale, Open Images dataset suffers from label-related problems that objects may explicitly or implicitly have multiple labels and the label distribution is extremely imbalanced. In this work, we quantitatively analyze these label problems and provide a simple but effective solution. We design a concurrent softmax to handle the multi-label problems in object detection and propose a soft-sampling methods with hybrid training scheduler to deal with the label imbalance. Overall, our method yields a dramatic improvement of 3.34 points, leading to the best single model with 60.90 mAP on the public object detection test set of Open Images. And our ensembling result achieves 67.17 mAP, which is 4.29 points higher than the best result of Open Images public test 2018.



### Bayesian convolutional neural network based MRI brain extraction on nonhuman primates
- **Arxiv ID**: http://arxiv.org/abs/2005.08460v1
- **DOI**: 10.1016/j.neuroimage.2018.03.065
- **Categories**: **eess.IV**, cs.CV, I.4.6; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2005.08460v1)
- **Published**: 2020-05-18 05:08:30+00:00
- **Updated**: 2020-05-18 05:08:30+00:00
- **Authors**: Gengyan Zhao, Fang Liu, Jonathan A. Oler, Mary E. Meyerand, Ned H. Kalin, Rasmus M. Birn
- **Comment**: 37 pages, 14 figures
- **Journal**: Neuroimage 175 (2018): 32-44
- **Summary**: Brain extraction or skull stripping of magnetic resonance images (MRI) is an essential step in neuroimaging studies, the accuracy of which can severely affect subsequent image processing procedures. Current automatic brain extraction methods demonstrate good results on human brains, but are often far from satisfactory on nonhuman primates, which are a necessary part of neuroscience research. To overcome the challenges of brain extraction in nonhuman primates, we propose a fully-automated brain extraction pipeline combining deep Bayesian convolutional neural network (CNN) and fully connected three-dimensional (3D) conditional random field (CRF). The deep Bayesian CNN, Bayesian SegNet, is used as the core segmentation engine. As a probabilistic network, it is not only able to perform accurate high-resolution pixel-wise brain segmentation, but also capable of measuring the model uncertainty by Monte Carlo sampling with dropout in the testing stage. Then, fully connected 3D CRF is used to refine the probability result from Bayesian SegNet in the whole 3D context of the brain volume. The proposed method was evaluated with a manually brain-extracted dataset comprising T1w images of 100 nonhuman primates. Our method outperforms six popular publicly available brain extraction packages and three well-established deep learning based methods with a mean Dice coefficient of 0.985 and a mean average symmetric surface distance of 0.220 mm. A better performance against all the compared methods was verified by statistical tests (all p-values<10-4, two-sided, Bonferroni corrected). The maximum uncertainty of the model on nonhuman primate brain extraction has a mean value of 0.116 across all the 100 subjects...



### Feature Transformation Ensemble Model with Batch Spectral Regularization for Cross-Domain Few-Shot Classification
- **Arxiv ID**: http://arxiv.org/abs/2005.08463v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.08463v3)
- **Published**: 2020-05-18 05:31:04+00:00
- **Updated**: 2020-05-21 02:44:03+00:00
- **Authors**: Bingyu Liu, Zhen Zhao, Zhenpeng Li, Jianan Jiang, Yuhong Guo, Jieping Ye
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a feature transformation ensemble model with batch spectral regularization for the Cross-domain few-shot learning (CD-FSL) challenge. Specifically, we proposes to construct an ensemble prediction model by performing diverse feature transformations after a feature extraction network. On each branch prediction network of the model we use a batch spectral regularization term to suppress the singular values of the feature matrix during pre-training to improve the generalization ability of the model. The proposed model can then be fine tuned in the target domain to address few-shot classification. We also further apply label propagation, entropy minimization and data augmentation to mitigate the shortage of labeled data in target domains. Experiments are conducted on a number of CD-FSL benchmark tasks with four target domains and the results demonstrate the superiority of our proposed model.



### Context-aware and Scale-insensitive Temporal Repetition Counting
- **Arxiv ID**: http://arxiv.org/abs/2005.08465v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.08465v1)
- **Published**: 2020-05-18 05:49:48+00:00
- **Updated**: 2020-05-18 05:49:48+00:00
- **Authors**: Huaidong Zhang, Xuemiao Xu, Guoqiang Han, Shengfeng He
- **Comment**: Accepted by CVPR2020
- **Journal**: None
- **Summary**: Temporal repetition counting aims to estimate the number of cycles of a given repetitive action. Existing deep learning methods assume repetitive actions are performed in a fixed time-scale, which is invalid for the complex repetitive actions in real life. In this paper, we tailor a context-aware and scale-insensitive framework, to tackle the challenges in repetition counting caused by the unknown and diverse cycle-lengths. Our approach combines two key insights: (1) Cycle lengths from different actions are unpredictable that require large-scale searching, but, once a coarse cycle length is determined, the variety between repetitions can be overcome by regression. (2) Determining the cycle length cannot only rely on a short fragment of video but a contextual understanding. The first point is implemented by a coarse-to-fine cycle refinement method. It avoids the heavy computation of exhaustively searching all the cycle lengths in the video, and, instead, it propagates the coarse prediction for further refinement in a hierarchical manner. We secondly propose a bidirectional cycle length estimation method for a context-aware prediction. It is a regression network that takes two consecutive coarse cycles as input, and predicts the locations of the previous and next repetitive cycles. To benefit the training and evaluation of temporal repetition counting area, we construct a new and largest benchmark, which contains 526 videos with diverse repetitive actions. Extensive experiments show that the proposed network trained on a single dataset outperforms state-of-the-art methods on several benchmarks, indicating that the proposed framework is general enough to capture repetition patterns across domains.



### VecQ: Minimal Loss DNN Model Compression With Vectorized Weight Quantization
- **Arxiv ID**: http://arxiv.org/abs/2005.08501v2
- **DOI**: 10.1109/TC.2020.2995593
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.08501v2)
- **Published**: 2020-05-18 07:38:44+00:00
- **Updated**: 2020-06-10 07:09:15+00:00
- **Authors**: Cheng Gong, Yao Chen, Ye Lu, Tao Li, Cong Hao, Deming Chen
- **Comment**: 14 pages, 9 figures, Journal
- **Journal**: None
- **Summary**: Quantization has been proven to be an effective method for reducing the computing and/or storage cost of DNNs. However, the trade-off between the quantization bitwidth and final accuracy is complex and non-convex, which makes it difficult to be optimized directly. Minimizing direct quantization loss (DQL) of the coefficient data is an effective local optimization method, but previous works often neglect the accurate control of the DQL, resulting in a higher loss of the final DNN model accuracy. In this paper, we propose a novel metric called Vector Loss. Based on this new metric, we develop a new quantization solution called VecQ, which can guarantee minimal direct quantization loss and better model accuracy. In addition, in order to speed up the proposed quantization process during model training, we accelerate the quantization process with a parameterized probability estimation method and template-based derivation calculation. We evaluate our proposed algorithm on MNIST, CIFAR, ImageNet, IMDB movie review and THUCNews text data sets with numerical DNN models. The results demonstrate that our proposed quantization solution is more accurate and effective than the state-of-the-art approaches yet with more flexible bitwidth support. Moreover, the evaluation of our quantized models on Saliency Object Detection (SOD) tasks maintains comparable feature extraction quality with up to 16$\times$ weight size reduction.



### Spatio-Temporal Graph Transformer Networks for Pedestrian Trajectory Prediction
- **Arxiv ID**: http://arxiv.org/abs/2005.08514v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2005.08514v2)
- **Published**: 2020-05-18 08:08:09+00:00
- **Updated**: 2020-07-24 03:32:07+00:00
- **Authors**: Cunjun Yu, Xiao Ma, Jiawei Ren, Haiyu Zhao, Shuai Yi
- **Comment**: ECCV camera-ready
- **Journal**: None
- **Summary**: Understanding crowd motion dynamics is critical to real-world applications, e.g., surveillance systems and autonomous driving. This is challenging because it requires effectively modeling the socially aware crowd spatial interaction and complex temporal dependencies. We believe attention is the most important factor for trajectory prediction. In this paper, we present STAR, a Spatio-Temporal grAph tRansformer framework, which tackles trajectory prediction by only attention mechanisms. STAR models intra-graph crowd interaction by TGConv, a novel Transformer-based graph convolution mechanism. The inter-graph temporal dependencies are modeled by separate temporal Transformers. STAR captures complex spatio-temporal interactions by interleaving between spatial and temporal Transformers. To calibrate the temporal prediction for the long-lasting effect of disappeared pedestrians, we introduce a read-writable external memory module, consistently being updated by the temporal Transformer. We show that with only attention mechanism, STAR achieves state-of-the-art performance on 5 commonly used real-world pedestrian prediction datasets.



### Learning to rank music tracks using triplet loss
- **Arxiv ID**: http://arxiv.org/abs/2005.12977v1
- **DOI**: 10.1109/ICASSP40776.2020.9053135
- **Categories**: **cs.IR**, cs.CV, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2005.12977v1)
- **Published**: 2020-05-18 08:20:54+00:00
- **Updated**: 2020-05-18 08:20:54+00:00
- **Authors**: Laure Prétet, Gaël Richard, Geoffroy Peeters
- **Comment**: None
- **Journal**: None
- **Summary**: Most music streaming services rely on automatic recommendation algorithms to exploit their large music catalogs. These algorithms aim at retrieving a ranked list of music tracks based on their similarity with a target music track. In this work, we propose a method for direct recommendation based on the audio content without explicitly tagging the music tracks. To that aim, we propose several strategies to perform triplet mining from ranked lists. We train a Convolutional Neural Network to learn the similarity via triplet loss. These different strategies are compared and validated on a large-scale experiment against an auto-tagging based approach. The results obtained highlight the efficiency of our system, especially when associated with an Auto-pooling layer.



### Omni-supervised Facial Expression Recognition via Distilled Data
- **Arxiv ID**: http://arxiv.org/abs/2005.08551v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.08551v5)
- **Published**: 2020-05-18 09:36:51+00:00
- **Updated**: 2021-12-09 04:07:04+00:00
- **Authors**: Ping Liu, Yunchao Wei, Zibo Meng, Weihong Deng, Joey Tianyi Zhou, Yi Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Facial expression plays an important role in understanding human emotions. Most recently, deep learning based methods have shown promising for facial expression recognition. However, the performance of the current state-of-the-art facial expression recognition (FER) approaches is directly related to the labeled data for training. To solve this issue, prior works employ the pretrain-and-finetune strategy, i.e., utilize a large amount of unlabeled data to pretrain the network and then finetune it by the labeled data. As the labeled data is in a small amount, the final network performance is still restricted. From a different perspective, we propose to perform omni-supervised learning to directly exploit reliable samples in a large amount of unlabeled data for network training. Particularly, a new dataset is firstly constructed using a primitive model trained on a small number of labeled samples to select samples with high confidence scores from a face dataset, i.e., MS-Celeb-1M, based on feature-wise similarity. We experimentally verify that the new dataset created in such an omni-supervised manner can significantly improve the generalization ability of the learned FER model. However, as the number of training samples grows, computational cost and training time increase dramatically. To tackle this, we propose to apply a dataset distillation strategy to compress the created dataset into several informative class-wise images, significantly improving the training efficiency. We have conducted extensive experiments on widely used benchmarks, where consistent performance gains can be achieved under various settings using the proposed framework. More importantly, the distilled dataset has shown its capabilities of boosting the performance of FER with negligible additional computational costs.



### Learning to Model and Calibrate Optics via a Differentiable Wave Optics Simulator
- **Arxiv ID**: http://arxiv.org/abs/2005.08562v1
- **DOI**: 10.1109/ICIP40778.2020.9190870
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.08562v1)
- **Published**: 2020-05-18 10:23:04+00:00
- **Updated**: 2020-05-18 10:23:04+00:00
- **Authors**: Josue Page, Paolo Favaro
- **Comment**: 6 pages, 3 figures, for source code see
  https://github.com/pvjosue/WaveBlocks, to be published in IEEE 2020
  International Conference on Image Processing (ICIP 2020)
- **Journal**: 2020 IEEE International Conference on Image Processing (ICIP)
- **Summary**: We present a novel learning-based method to build a differentiable computational model of a real fluorescence microscope. Our model can be used to calibrate a real optical setup directly from data samples and to engineer point spread functions by specifying the desired input-output data. This approach is poised to drastically improve the design of microscopes, because the parameters of current models of optical setups cannot be easily fit to real data. Inspired by the recent progress in deep learning, our solution is to build a differentiable wave optics simulator as a composition of trainable modules, each computing light wave-front (WF) propagation due to a specific optical element. We call our differentiable modules WaveBlocks and show reconstruction results in the case of lenses, wave propagation in air, camera sensors and diffractive elements (e.g., phase-masks).



### DDD20 End-to-End Event Camera Driving Dataset: Fusing Frames and Events with Deep Learning for Improved Steering Prediction
- **Arxiv ID**: http://arxiv.org/abs/2005.08605v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.08605v1)
- **Published**: 2020-05-18 11:39:38+00:00
- **Updated**: 2020-05-18 11:39:38+00:00
- **Authors**: Yuhuang Hu, Jonathan Binas, Daniel Neil, Shih-Chii Liu, Tobi Delbruck
- **Comment**: Accepted in The 23rd IEEE International Conference on Intelligent
  Transportation Systems (Special Session: Beyond Traditional Sensing for
  Intelligent Transportation)
- **Journal**: None
- **Summary**: Neuromorphic event cameras are useful for dynamic vision problems under difficult lighting conditions. To enable studies of using event cameras in automobile driving applications, this paper reports a new end-to-end driving dataset called DDD20. The dataset was captured with a DAVIS camera that concurrently streams both dynamic vision sensor (DVS) brightness change events and active pixel sensor (APS) intensity frames. DDD20 is the longest event camera end-to-end driving dataset to date with 51h of DAVIS event+frame camera and vehicle human control data collected from 4000km of highway and urban driving under a variety of lighting conditions. Using DDD20, we report the first study of fusing brightness change events and intensity frame data using a deep learning approach to predict the instantaneous human steering wheel angle. Over all day and night conditions, the explained variance for human steering prediction from a Resnet-32 is significantly better from the fused DVS+APS frames (0.88) than using either DVS (0.67) or APS (0.77) data alone.



### End-to-End Lip Synchronisation Based on Pattern Classification
- **Arxiv ID**: http://arxiv.org/abs/2005.08606v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2005.08606v2)
- **Published**: 2020-05-18 11:42:32+00:00
- **Updated**: 2021-03-19 06:55:05+00:00
- **Authors**: You Jin Kim, Hee Soo Heo, Soo-Whan Chung, Bong-Jin Lee
- **Comment**: slt 2021 accepted
- **Journal**: None
- **Summary**: The goal of this work is to synchronise audio and video of a talking face using deep neural network models. Existing works have trained networks on proxy tasks such as cross-modal similarity learning, and then computed similarities between audio and video frames using a sliding window approach. While these methods demonstrate satisfactory performance, the networks are not trained directly on the task. To this end, we propose an end-to-end trained network that can directly predict the offset between an audio stream and the corresponding video stream. The similarity matrix between the two modalities is first computed from the features, then the inference of the offset can be considered to be a pattern recognition problem where the matrix is considered equivalent to an image. The feature extractor and the classifier are trained jointly. We demonstrate that the proposed approach outperforms the previous work by a large margin on LRS2 and LRS3 datasets.



### Decoder Modulation for Indoor Depth Completion
- **Arxiv ID**: http://arxiv.org/abs/2005.08607v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.08607v2)
- **Published**: 2020-05-18 11:42:42+00:00
- **Updated**: 2021-02-08 08:20:51+00:00
- **Authors**: Dmitry Senushkin, Mikhail Romanov, Ilia Belikov, Anton Konushin, Nikolay Patakin
- **Comment**: None
- **Journal**: None
- **Summary**: Depth completion recovers a dense depth map from sensor measurements. Current methods are mostly tailored for very sparse depth measurements from LiDARs in outdoor settings, while for indoor scenes Time-of-Flight (ToF) or structured light sensors are mostly used. These sensors provide semi-dense maps, with dense measurements in some regions and almost empty in others. We propose a new model that takes into account the statistical difference between such regions. Our main contribution is a new decoder modulation branch added to the encoder-decoder architecture. The encoder extracts features from the concatenated RGB image and raw depth. Given the mask of missing values as input, the proposed modulation branch controls the decoding of a dense depth map from these features differently for different regions. This is implemented by modifying the spatial distribution of output signals inside the decoder via Spatially-Adaptive Denormalization (SPADE) blocks. Our second contribution is a novel training strategy that allows us to train on a semi-dense sensor data when the ground truth depth map is not available. Our model achieves the state of the art results on indoor Matterport3D dataset. Being designed for semi-dense input depth, our model is still competitive with LiDAR-oriented approaches on the KITTI dataset. Our training strategy significantly improves prediction quality with no dense ground truth available, as validated on the NYUv2 dataset.



### Learn Class Hierarchy using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2005.08622v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.08622v1)
- **Published**: 2020-05-18 12:06:43+00:00
- **Updated**: 2020-05-18 12:06:43+00:00
- **Authors**: Riccardo La Grassa, Ignazio Gallo, Nicola Landro
- **Comment**: 7 pages
- **Journal**: None
- **Summary**: A large amount of research on Convolutional Neural Networks has focused on flat Classification in the multi-class domain. In the real world, many problems are naturally expressed as problems of hierarchical classification, in which the classes to be predicted are organized in a hierarchy of classes. In this paper, we propose a new architecture for hierarchical classification of images, introducing a stack of deep linear layers with cross-entropy loss functions and center loss combined. The proposed architecture can extend any neural network model and simultaneously optimizes loss functions to discover local hierarchical class relationships and a loss function to discover global information from the whole class hierarchy while penalizing class hierarchy violations. We experimentally show that our hierarchical classifier presents advantages to the traditional classification approaches finding application in computer vision tasks.



### Universalization of any adversarial attack using very few test examples
- **Arxiv ID**: http://arxiv.org/abs/2005.08632v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.08632v2)
- **Published**: 2020-05-18 12:17:38+00:00
- **Updated**: 2022-10-28 17:37:32+00:00
- **Authors**: Sandesh Kamath, Amit Deshpande, K V Subrahmanyam, Vineeth N Balasubramanian
- **Comment**: Appeared in ACM CODS-COMAD 2022 (Research Track)
- **Journal**: None
- **Summary**: Deep learning models are known to be vulnerable not only to input-dependent adversarial attacks but also to input-agnostic or universal adversarial attacks. Dezfooli et al. \cite{Dezfooli17,Dezfooli17anal} construct universal adversarial attack on a given model by looking at a large number of training data points and the geometry of the decision boundary near them. Subsequent work \cite{Khrulkov18} constructs universal attack by looking only at test examples and intermediate layers of the given model. In this paper, we propose a simple universalization technique to take any input-dependent adversarial attack and construct a universal attack by only looking at very few adversarial test examples. We do not require details of the given model and have negligible computational overhead for universalization. We theoretically justify our universalization technique by a spectral property common to many input-dependent adversarial perturbations, e.g., gradients, Fast Gradient Sign Method (FGSM) and DeepFool. Using matrix concentration inequalities and spectral perturbation bounds, we show that the top singular vector of input-dependent adversarial directions on a small test sample gives an effective and simple universal adversarial attack. For VGG16 and VGG19 models trained on ImageNet, our simple universalization of Gradient, FGSM, and DeepFool perturbations using a test sample of 64 images gives fooling rates comparable to state-of-the-art universal attacks \cite{Dezfooli17,Khrulkov18} for reasonable norms of perturbation. Code available at https://github.com/ksandeshk/svd-uap .



### Building BROOK: A Multi-modal and Facial Video Database for Human-Vehicle Interaction Research
- **Arxiv ID**: http://arxiv.org/abs/2005.08637v2
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.08637v2)
- **Published**: 2020-05-18 12:20:17+00:00
- **Updated**: 2020-05-19 14:42:30+00:00
- **Authors**: Xiangjun Peng, Zhentao Huang, Xu Sun
- **Comment**: Conference: ACM CHI Conference on Human Factors in Computing Systems
  Workshops (CHI'20 Workshops)At: Honolulu, Hawaii, USA
  URL:https://emergentdatatrails.com
- **Journal**: None
- **Summary**: With the growing popularity of Autonomous Vehicles, more opportunities have bloomed in the context of Human-Vehicle Interactions. However, the lack of comprehensive and concrete database support for such specific use case limits relevant studies in the whole design spaces. In this paper, we present our work-in-progress BROOK, a public multi-modal database with facial video records, which could be used to characterize drivers' affective states and driving styles. We first explain how we over-engineer such database in details, and what we have gained through a ten-month study. Then we showcase a Neural Network-based predictor, leveraging BROOK, which supports multi-modal prediction (including physiological data of heart rate and skin conductance and driving status data of speed)through facial videos. Finally, we discuss related issues when building such a database and our future directions in the context of BROOK. We believe BROOK is an essential building block for future Human-Vehicle Interaction Research.



### Improving Named Entity Recognition in Tor Darknet with Local Distance Neighbor Feature
- **Arxiv ID**: http://arxiv.org/abs/2005.08746v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.08746v1)
- **Published**: 2020-05-18 14:21:22+00:00
- **Updated**: 2020-05-18 14:21:22+00:00
- **Authors**: Mhd Wesam Al-Nabki, Francisco Jañez-Martino, Roberto A. Vasco-Carofilis, Eduardo Fidalgo, Javier Velasco-Mata
- **Comment**: 2 pages, 1 figure, to be published in conference JNIC 2020
- **Journal**: None
- **Summary**: Name entity recognition in noisy user-generated texts is a difficult task usually enhanced by incorporating an external resource of information, such as gazetteers. However, gazetteers are task-specific, and they are expensive to build and maintain. This paper adopts and improves the approach of Aguilar et al. by presenting a novel feature, called Local Distance Neighbor, which substitutes gazetteers. We tested the new approach on the W-NUT-2017 dataset, obtaining state-of-the-art results for the Group, Person and Product categories of Named Entities. Next, we added 851 manually labeled samples to the W-NUT-2017 dataset to account for named entities in the Tor Darknet related to weapons and drug selling. Finally, our proposal achieved an entity and surface F1 scores of 52.96% and 50.57% on this extended dataset, demonstrating its usefulness for Law Enforcement Agencies to detect named entities in the Tor hidden services.



### Learning Spatial-Spectral Prior for Super-Resolution of Hyperspectral Imagery
- **Arxiv ID**: http://arxiv.org/abs/2005.08752v2
- **DOI**: 10.1109/TCI.2020.2996075
- **Categories**: **eess.IV**, cs.CV, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2005.08752v2)
- **Published**: 2020-05-18 14:25:50+00:00
- **Updated**: 2020-05-24 03:26:38+00:00
- **Authors**: Junjun Jiang, He Sun, Xianming Liu, Jiayi Ma
- **Comment**: Accepted for publication at IEEE Transactions on Computational
  Imaging
- **Journal**: None
- **Summary**: Recently, single gray/RGB image super-resolution reconstruction task has been extensively studied and made significant progress by leveraging the advanced machine learning techniques based on deep convolutional neural networks (DCNNs). However, there has been limited technical development focusing on single hyperspectral image super-resolution due to the high-dimensional and complex spectral patterns in hyperspectral image. In this paper, we make a step forward by investigating how to adapt state-of-the-art residual learning based single gray/RGB image super-resolution approaches for computationally efficient single hyperspectral image super-resolution, referred as SSPSR. Specifically, we introduce a spatial-spectral prior network (SSPN) to fully exploit the spatial information and the correlation between the spectra of the hyperspectral data. Considering that the hyperspectral training samples are scarce and the spectral dimension of hyperspectral image data is very high, it is nontrivial to train a stable and effective deep network. Therefore, a group convolution (with shared network parameters) and progressive upsampling framework is proposed. This will not only alleviate the difficulty in feature extraction due to high-dimension of the hyperspectral data, but also make the training process more stable. To exploit the spatial and spectral prior, we design a spatial-spectral block (SSB), which consists of a spatial residual module and a spectral attention residual module. Experimental results on some hyperspectral images demonstrate that the proposed SSPSR method enhances the details of the recovered high-resolution hyperspectral images, and outperforms state-of-the-arts. The source code is available at \url{https://github.com/junjun-jiang/SSPSR



### Evaluating Performance of an Adult Pornography Classifier for Child Sexual Abuse Detection
- **Arxiv ID**: http://arxiv.org/abs/2005.08766v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.08766v1)
- **Published**: 2020-05-18 14:32:33+00:00
- **Updated**: 2020-05-18 14:32:33+00:00
- **Authors**: Mhd Wesam Al-Nabki, Eduardo Fidalgo, Roberto A. Vasco-Carofilis, Francisco Jañez-Martino, Javier Velasco-Mata
- **Comment**: 4 pages, 8 figures, to be published in conference JNIC 2020
- **Journal**: None
- **Summary**: The information technology revolution has facilitated reaching pornographic material for everyone, including minors who are the most vulnerable in case they were abused. Accuracy and time performance are features desired by forensic tools oriented to child sexual abuse detection, whose main components may rely on image or video classifiers. In this paper, we identify which are the hardware and software requirements that may affect the performance of a forensic tool. We evaluated the adult porn classifier proposed by Yahoo, based on Deep Learning, into two different OS and four Hardware configurations, with two and four different CPU and GPU, respectively. The classification speed on Ubuntu Operating System is $~5$ and $~2$ times faster than on Windows 10, when a CPU and GPU are used, respectively. We demonstrate the superiority of a GPU-based machine rather than a CPU-based one, being $7$ to $8$ times faster. Finally, we prove that the upward and downward interpolation process conducted while resizing the input images do not influence the performance of the selected prediction model.



### Color Visual Illusions: A Statistics-based Computational Model
- **Arxiv ID**: http://arxiv.org/abs/2005.08772v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.08772v2)
- **Published**: 2020-05-18 14:39:48+00:00
- **Updated**: 2020-10-22 10:45:03+00:00
- **Authors**: Elad Hirsch, Ayellet Tal
- **Comment**: None
- **Journal**: None
- **Summary**: Visual illusions may be explained by the likelihood of patches in real-world images, as argued by input-driven paradigms in Neuro-Science. However, neither the data nor the tools existed in the past to extensively support these explanations. The era of big data opens a new opportunity to study input-driven approaches. We introduce a tool that computes the likelihood of patches, given a large dataset to learn from. Given this tool, we present a model that supports the approach and explains lightness and color visual illusions in a unified manner. Furthermore, our model generates visual illusions in natural images, by applying the same tool, reversely.



### Noise-Sampling Cross Entropy Loss: Improving Disparity Regression Via Cost Volume Aware Regularizer
- **Arxiv ID**: http://arxiv.org/abs/2005.08806v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.08806v2)
- **Published**: 2020-05-18 15:29:55+00:00
- **Updated**: 2020-05-28 09:59:11+00:00
- **Authors**: Yang Chen, Zongqing Lu, Xuechen Zhang, Lei Chen, Qingmin Liao
- **Comment**: Accepted by IEEE ICIP 2020
- **Journal**: None
- **Summary**: Recent end-to-end deep neural networks for disparity regression have achieved the state-of-the-art performance. However, many well-acknowledged specific properties of disparity estimation are omitted in these deep learning algorithms. Especially, matching cost volume, one of the most important procedure, is treated as a normal intermediate feature for the following softargmin regression, lacking explicit constraints compared with those traditional algorithms. In this paper, inspired by previous canonical definition of cost volume, we propose the noise-sampling cross entropy loss function to regularize the cost volume produced by deep neural networks to be unimodal and coherent. Extensive experiments validate that the proposed noise-sampling cross entropy loss can not only help neural networks learn more informative cost volume, but also lead to better stereo matching performance compared with several representative algorithms.



### Hierarchical and Efficient Learning for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2005.08812v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.08812v1)
- **Published**: 2020-05-18 15:45:25+00:00
- **Updated**: 2020-05-18 15:45:25+00:00
- **Authors**: Jiangning Zhang, Liang Liu, Chao Xu, Yong Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Recent works in the person re-identification task mainly focus on the model accuracy while ignore factors related to the efficiency, e.g. model size and latency, which are critical for practical application. In this paper, we propose a novel Hierarchical and Efficient Network (HENet) that learns hierarchical global, partial, and recovery features ensemble under the supervision of multiple loss combinations. To further improve the robustness against the irregular occlusion, we propose a new dataset augmentation approach, dubbed Random Polygon Erasing (RPE), to random erase irregular area of the input image for imitating the body part missing. We also propose an Efficiency Score (ES) metric to evaluate the model efficiency. Extensive experiments on Market1501, DukeMTMC-ReID, and CUHK03 datasets shows the efficiency and superiority of our approach compared with epoch-making methods.



### Visual Memorability for Robotic Interestingness via Unsupervised Online Learning
- **Arxiv ID**: http://arxiv.org/abs/2005.08829v3
- **DOI**: 10.1007/978-3-030-58536-5_4
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2005.08829v3)
- **Published**: 2020-05-18 16:00:27+00:00
- **Updated**: 2020-07-18 16:43:35+00:00
- **Authors**: Chen Wang, Wenshan Wang, Yuheng Qiu, Yafei Hu, Sebastian Scherer
- **Comment**: Oral paper in ECCV 2020
- **Journal**: 2020 European Conference on Computer Vision (ECCV)
- **Summary**: In this paper, we explore the problem of interesting scene prediction for mobile robots. This area is currently underexplored but is crucial for many practical applications such as autonomous exploration and decision making. Inspired by industrial demands, we first propose a novel translation-invariant visual memory for recalling and identifying interesting scenes, then design a three-stage architecture of long-term, short-term, and online learning. This enables our system to learn human-like experience, environmental knowledge, and online adaption, respectively. Our approach achieves much higher accuracy than the state-of-the-art algorithms on challenging robotic interestingness datasets.



### MMFashion: An Open-Source Toolbox for Visual Fashion Analysis
- **Arxiv ID**: http://arxiv.org/abs/2005.08847v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.08847v2)
- **Published**: 2020-05-18 16:19:00+00:00
- **Updated**: 2020-05-19 02:33:36+00:00
- **Authors**: Xin Liu, Jiancheng Li, Jiaqi Wang, Ziwei Liu
- **Comment**: Codes and models are available at:
  https://github.com/open-mmlab/mmfashion
- **Journal**: None
- **Summary**: We present MMFashion, a comprehensive, flexible and user-friendly open-source visual fashion analysis toolbox based on PyTorch. This toolbox supports a wide spectrum of fashion analysis tasks, including Fashion Attribute Prediction, Fashion Recognition and Retrieval, Fashion Landmark Detection, Fashion Parsing and Segmentation and Fashion Compatibility and Recommendation. It covers almost all the mainstream tasks in fashion analysis community. MMFashion has several appealing properties. Firstly, MMFashion follows the principle of modular design. The framework is decomposed into different components so that it is easily extensible for diverse customized modules. In addition, detailed documentations, demo scripts and off-the-shelf models are available, which ease the burden of layman users to leverage the recent advances in deep learning-based fashion analysis. Our proposed MMFashion is currently the most complete platform for visual fashion analysis in deep learning era, with more functionalities to be added. This toolbox and the benchmark could serve the flourishing research community by providing a flexible toolkit to deploy existing models and develop new ideas and approaches. We welcome all contributions to this still-growing efforts towards open science: https://github.com/open-mmlab/mmfashion.



### Deep Implicit Volume Compression
- **Arxiv ID**: http://arxiv.org/abs/2005.08877v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.08877v1)
- **Published**: 2020-05-18 16:46:13+00:00
- **Updated**: 2020-05-18 16:46:13+00:00
- **Authors**: Danhang Tang, Saurabh Singh, Philip A. Chou, Christian Haene, Mingsong Dou, Sean Fanello, Jonathan Taylor, Philip Davidson, Onur G. Guleryuz, Yinda Zhang, Shahram Izadi, Andrea Tagliasacchi, Sofien Bouaziz, Cem Keskin
- **Comment**: Danhang Tang and Saurabh Singh have equal contribution
- **Journal**: None
- **Summary**: We describe a novel approach for compressing truncated signed distance fields (TSDF) stored in 3D voxel grids, and their corresponding textures. To compress the TSDF, our method relies on a block-based neural network architecture trained end-to-end, achieving state-of-the-art rate-distortion trade-off. To prevent topological errors, we losslessly compress the signs of the TSDF, which also upper bounds the reconstruction error by the voxel size. To compress the corresponding texture, we designed a fast block-based UV parameterization, generating coherent texture maps that can be effectively compressed using existing video compression algorithms. We demonstrate the performance of our algorithms on two 4D performance capture datasets, reducing bitrate by 66% for the same distortion, or alternatively reducing the distortion by 50% for the same bitrate, compared to the state-of-the-art.



### Generative Tweening: Long-term Inbetweening of 3D Human Motions
- **Arxiv ID**: http://arxiv.org/abs/2005.08891v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2005.08891v2)
- **Published**: 2020-05-18 17:04:34+00:00
- **Updated**: 2020-05-28 05:36:46+00:00
- **Authors**: Yi Zhou, Jingwan Lu, Connelly Barnes, Jimei Yang, Sitao Xiang, Hao li
- **Comment**: None
- **Journal**: None
- **Summary**: The ability to generate complex and realistic human body animations at scale, while following specific artistic constraints, has been a fundamental goal for the game and animation industry for decades. Popular techniques include key-framing, physics-based simulation, and database methods via motion graphs. Recently, motion generators based on deep learning have been introduced. Although these learning models can automatically generate highly intricate stylized motions of arbitrary length, they still lack user control. To this end, we introduce the problem of long-term inbetweening, which involves automatically synthesizing complex motions over a long time interval given very sparse keyframes by users. We identify a number of challenges related to this problem, including maintaining biomechanical and keyframe constraints, preserving natural motions, and designing the entire motion sequence holistically while considering all constraints. We introduce a biomechanically constrained generative adversarial network that performs long-term inbetweening of human motions, conditioned on keyframe constraints. This network uses a novel two-stage approach where it first predicts local motion in the form of joint angles, and then predicts global motion, i.e. the global path that the character follows. Since there are typically a number of possible motions that could satisfy the given user constraints, we also enable our network to generate a variety of outputs with a scheme that we call Motion DNA. This approach allows the user to manipulate and influence the output content by feeding seed motions (DNA) to the network. Trained with 79 classes of captured motion data, our network performs robustly on a variety of highly complex motion styles.



### Deep Snow: Synthesizing Remote Sensing Imagery with Generative Adversarial Nets
- **Arxiv ID**: http://arxiv.org/abs/2005.08892v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.08892v1)
- **Published**: 2020-05-18 17:05:00+00:00
- **Updated**: 2020-05-18 17:05:00+00:00
- **Authors**: Christopher X. Ren, Amanda Ziemann, James Theiler, Alice M. S. Durieux
- **Comment**: None
- **Journal**: None
- **Summary**: In this work we demonstrate that generative adversarial networks (GANs) can be used to generate realistic pervasive changes in remote sensing imagery, even in an unpaired training setting. We investigate some transformation quality metrics based on deep embedding of the generated and real images which enable visualization and understanding of the training dynamics of the GAN, and may provide a useful measure in terms of quantifying how distinguishable the generated images are from real images. We also identify some artifacts introduced by the GAN in the generated images, which are likely to contribute to the differences seen between the real and generated samples in the deep embedding feature space even in cases where the real and generated samples appear perceptually similar.



### Portrait Shadow Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2005.08925v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2005.08925v2)
- **Published**: 2020-05-18 17:51:34+00:00
- **Updated**: 2020-05-20 17:49:55+00:00
- **Authors**: Xuaner Cecilia Zhang, Jonathan T. Barron, Yun-Ta Tsai, Rohit Pandey, Xiuming Zhang, Ren Ng, David E. Jacobs
- **Comment**: (updated version); SIGGRAPH 2020;Project webpage:
  https://people.eecs.berkeley.edu/~cecilia77/project-pages/portrait Video:
  https://youtu.be/M_qYTXhzyac
- **Journal**: None
- **Summary**: Casually-taken portrait photographs often suffer from unflattering lighting and shadowing because of suboptimal conditions in the environment. Aesthetic qualities such as the position and softness of shadows and the lighting ratio between the bright and dark parts of the face are frequently determined by the constraints of the environment rather than by the photographer. Professionals address this issue by adding light shaping tools such as scrims, bounce cards, and flashes. In this paper, we present a computational approach that gives casual photographers some of this control, thereby allowing poorly-lit portraits to be relit post-capture in a realistic and easily-controllable way. Our approach relies on a pair of neural networks---one to remove foreign shadows cast by external objects, and another to soften facial shadows cast by the features of the subject and to add a synthetic fill light to improve the lighting ratio. To train our first network we construct a dataset of real-world portraits wherein synthetic foreign shadows are rendered onto the face, and we show that our network learns to remove those unwanted shadows. To train our second network we use a dataset of Light Stage scans of human subjects to construct input/output pairs of input images harshly lit by a small light source, and variably softened and fill-lit output images of each face. We propose a way to explicitly encode facial symmetry and show that our dataset and training procedure enable the model to generalize to images taken in the wild. Together, these networks enable the realistic and aesthetically pleasing enhancement of shadows and lights in real-world portrait images



### Joint Multi-Dimension Pruning via Numerical Gradient Update
- **Arxiv ID**: http://arxiv.org/abs/2005.08931v2
- **DOI**: 10.1109/TIP.2021.3112041
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.08931v2)
- **Published**: 2020-05-18 17:57:09+00:00
- **Updated**: 2021-09-25 16:03:11+00:00
- **Authors**: Zechun Liu, Xiangyu Zhang, Zhiqiang Shen, Zhe Li, Yichen Wei, Kwang-Ting Cheng, Jian Sun
- **Comment**: Accepted to IEEE Transactions on Image Processing (TIP) 2021
- **Journal**: None
- **Summary**: We present joint multi-dimension pruning (abbreviated as JointPruning), an effective method of pruning a network on three crucial aspects: spatial, depth and channel simultaneously. To tackle these three naturally different dimensions, we proposed a general framework by defining pruning as seeking the best pruning vector (i.e., the numerical value of layer-wise channel number, spacial size, depth) and construct a unique mapping from the pruning vector to the pruned network structures. Then we optimize the pruning vector with gradient update and model joint pruning as a numerical gradient optimization process. To overcome the challenge that there is no explicit function between the loss and the pruning vectors, we proposed self-adapted stochastic gradient estimation to construct a gradient path through network loss to pruning vectors and enable efficient gradient update. We show that the joint strategy discovers a better status than previous studies that focused on individual dimensions solely, as our method is optimized collaboratively across the three dimensions in a single end-to-end training and it is more efficient than the previous exhaustive methods. Extensive experiments on large-scale ImageNet dataset across a variety of network architectures MobileNet V1&V2&V3 and ResNet demonstrate the effectiveness of our proposed method. For instance, we achieve significant margins of 2.5% and 2.6% improvement over the state-of-the-art approach on the already compact MobileNet V1&V2 under an extremely large compression ratio.



### InterFaceGAN: Interpreting the Disentangled Face Representation Learned by GANs
- **Arxiv ID**: http://arxiv.org/abs/2005.09635v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.09635v2)
- **Published**: 2020-05-18 18:01:22+00:00
- **Updated**: 2020-10-29 08:36:47+00:00
- **Authors**: Yujun Shen, Ceyuan Yang, Xiaoou Tang, Bolei Zhou
- **Comment**: Accepted by TPAMI 2020
- **Journal**: None
- **Summary**: Although Generative Adversarial Networks (GANs) have made significant progress in face synthesis, there lacks enough understanding of what GANs have learned in the latent representation to map a random code to a photo-realistic image. In this work, we propose a framework called InterFaceGAN to interpret the disentangled face representation learned by the state-of-the-art GAN models and study the properties of the facial semantics encoded in the latent space. We first find that GANs learn various semantics in some linear subspaces of the latent space. After identifying these subspaces, we can realistically manipulate the corresponding facial attributes without retraining the model. We then conduct a detailed study on the correlation between different semantics and manage to better disentangle them via subspace projection, resulting in more precise control of the attribute manipulation. Besides manipulating the gender, age, expression, and presence of eyeglasses, we can even alter the face pose and fix the artifacts accidentally made by GANs. Furthermore, we perform an in-depth face identity analysis and a layer-wise analysis to evaluate the editing results quantitatively. Finally, we apply our approach to real face editing by employing GAN inversion approaches and explicitly training feed-forward models based on the synthetic data established by InterFaceGAN. Extensive experimental results suggest that learning to synthesize faces spontaneously brings a disentangled and controllable face representation.



### U$^2$-Net: Going Deeper with Nested U-Structure for Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2005.09007v3
- **DOI**: 10.1016/j.patcog.2020.107404
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.09007v3)
- **Published**: 2020-05-18 18:08:26+00:00
- **Updated**: 2022-03-08 19:14:49+00:00
- **Authors**: Xuebin Qin, Zichen Zhang, Chenyang Huang, Masood Dehghan, Osmar R. Zaiane, Martin Jagersand
- **Comment**: Accepted in Pattern Recognition 2020
- **Journal**: None
- **Summary**: In this paper, we design a simple yet powerful deep network architecture, U$^2$-Net, for salient object detection (SOD). The architecture of our U$^2$-Net is a two-level nested U-structure. The design has the following advantages: (1) it is able to capture more contextual information from different scales thanks to the mixture of receptive fields of different sizes in our proposed ReSidual U-blocks (RSU), (2) it increases the depth of the whole architecture without significantly increasing the computational cost because of the pooling operations used in these RSU blocks. This architecture enables us to train a deep network from scratch without using backbones from image classification tasks. We instantiate two models of the proposed architecture, U$^2$-Net (176.3 MB, 30 FPS on GTX 1080Ti GPU) and U$^2$-Net$^{\dagger}$ (4.7 MB, 40 FPS), to facilitate the usage in different environments. Both models achieve competitive performance on six SOD datasets. The code is available: https://github.com/NathanUA/U-2-Net.



### Patch based Colour Transfer using SIFT Flow
- **Arxiv ID**: http://arxiv.org/abs/2005.09015v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.09015v1)
- **Published**: 2020-05-18 18:22:36+00:00
- **Updated**: 2020-05-18 18:22:36+00:00
- **Authors**: Hana Alghamdi, Rozenn Dahyot
- **Comment**: 8 pages, 7 figures, 4 tables
- **Journal**: None
- **Summary**: We propose a new colour transfer method with Optimal Transport (OT) to transfer the colour of a sourceimage to match the colour of a target image of the same scene that may exhibit large motion changes betweenimages. By definition OT does not take into account any available information about correspondences whencomputing the optimal solution. To tackle this problem we propose to encode overlapping neighborhoodsof pixels using both their colour and spatial correspondences estimated using motion estimation. We solvethe high dimensional problem in 1D space using an iterative projection approach. We further introducesmoothing as part of the iterative algorithms for solving optimal transport namely Iterative DistributionTransport (IDT) and its variant the Sliced Wasserstein Distance (SWD). Experiments show quantitative andqualitative improvements over previous state of the art colour transfer methods.



### A Novel Technique Combining Image Processing, Plant Development Properties, and the Hungarian Algorithm, to Improve Leaf Detection in Maize
- **Arxiv ID**: http://arxiv.org/abs/2005.09022v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.09022v1)
- **Published**: 2020-05-18 18:43:50+00:00
- **Updated**: 2020-05-18 18:43:50+00:00
- **Authors**: Nazifa Khan, Oliver A. S. Lyon, Mark Eramian, Ian McQuillan
- **Comment**: to be published in the IEEE CVPR 2020 Workshop Proceedings, and
  accepted by The 1st International Workshop and Prize Challenge on
  Agriculture-Vision: Challenges & Opportunities for Computer Vision in
  Agriculture in conjunction with IEEE/CVF CVPR 2020
- **Journal**: None
- **Summary**: Manual determination of plant phenotypic properties such as plant architecture, growth, and health is very time consuming and sometimes destructive. Automatic image analysis has become a popular approach. This research aims to identify the position (and number) of leaves from a temporal sequence of high-quality indoor images consisting of multiple views, focussing in particular of images of maize. The procedure used a segmentation on the images, using the convex hull to pick the best view at each time step, followed by a skeletonization of the corresponding image. To remove skeleton spurs, a discrete skeleton evolution pruning process was applied. Pre-existing statistics regarding maize development was incorporated to help differentiate between true leaves and false leaves. Furthermore, for each time step, leaves were matched to those of the previous and next three days using the graph-theoretic Hungarian algorithm. This matching algorithm can be used to both remove false positives, and also to predict true leaves, even if they were completely occluded from the image itself. The algorithm was evaluated using an open dataset consisting of 13 maize plants across 27 days from two different views. The total number of true leaves from the dataset was 1843, and our proposed techniques detect a total of 1690 leaves including 1674 true leaves, and only 16 false leaves, giving a recall of 90.8%, and a precision of 99.0%.



### On the effectiveness of GAN generated cardiac MRIs for segmentation
- **Arxiv ID**: http://arxiv.org/abs/2005.09026v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.09026v2)
- **Published**: 2020-05-18 18:48:38+00:00
- **Updated**: 2020-05-22 09:28:39+00:00
- **Authors**: Youssef Skandarani, Nathan Painchaud, Pierre-Marc Jodoin, Alain Lalande
- **Comment**: 4 pages, Accepted for MIDL 2020
- **Journal**: None
- **Summary**: In this work, we propose a Variational Autoencoder (VAE) - Generative Adversarial Networks (GAN) model that can produce highly realistic MRI together with its pixel accurate groundtruth for the application of cine-MR image cardiac segmentation. On one side of our model is a Variational Autoencoder (VAE) trained to learn the latent representations of cardiac shapes. On the other side is a GAN that uses "SPatially-Adaptive (DE)Normalization" (SPADE) modules to generate realistic MR images tailored to a given anatomical map. At test time, the sampling of the VAE latent space allows to generate an arbitrary large number of cardiac shapes, which are fed to the GAN that subsequently generates MR images whose cardiac structure fits that of the cardiac shapes. In other words, our system can generate a large volume of realistic yet labeled cardiac MR images. We show that segmentation with CNNs trained with our synthetic annotated images gets competitive results compared to traditional techniques. We also show that combining data augmentation with our GAN-generated images lead to an improvement in the Dice score of up to 12 percent while allowing for better generalization capabilities on other datasets.



### Efficient Image Gallery Representations at Scale Through Multi-Task Learning
- **Arxiv ID**: http://arxiv.org/abs/2005.09027v3
- **DOI**: 10.1145/3397271.3401433
- **Categories**: **cs.CV**, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.09027v3)
- **Published**: 2020-05-18 18:49:22+00:00
- **Updated**: 2020-07-24 10:24:02+00:00
- **Authors**: Benjamin Gutelman, Pavel Levin
- **Comment**: Proceedings of the 43rd International ACM SIGIR Conference on
  Research and Development in Information Retrieval
- **Journal**: None
- **Summary**: Image galleries provide a rich source of diverse information about a product which can be leveraged across many recommendation and retrieval applications. We study the problem of building a universal image gallery encoder through multi-task learning (MTL) approach and demonstrate that it is indeed a practical way to achieve generalizability of learned representations to new downstream tasks. Additionally, we analyze the relative predictive performance of MTL-trained solutions against optimal and substantially more expensive solutions, and find signals that MTL can be a useful mechanism to address sparsity in low-resource binary tasks.



### Distillation of neural network models for detection and description of key points of images
- **Arxiv ID**: http://arxiv.org/abs/2006.10502v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.10502v1)
- **Published**: 2020-05-18 18:59:35+00:00
- **Updated**: 2020-05-18 18:59:35+00:00
- **Authors**: A. V. Yashchenko, A. V. Belikov, M. V. Peterson, A. S. Potapov
- **Comment**: in Russian
- **Journal**: None
- **Summary**: Image matching and classification methods, as well as synchronous location and mapping, are widely used on embedded and mobile devices. Their most resource-intensive part is the detection and description of the key points of the images. And if the classical methods of detecting and describing key points can be executed in real time on mobile devices, then for modern neural network methods with the best quality, such use is difficult. Thus, it is important to increase the speed of neural network models for the detection and description of key points. The subject of research is distillation as one of the methods for reducing neural network models. The aim of thestudy is to obtain a more compact model of detection and description of key points, as well as a description of the procedure for obtaining this model. A method for the distillation of neural networks for the task of detecting and describing key points was tested. The objective function and training parameters that provide the best results in the framework of the study are proposed. A new data set has been introduced for testing key point detection methods and a new quality indicator of the allocated key points and their corresponding local features. As a result of training in the described way, the new model, with the same number of parameters, showed greater accuracy in comparing key points than the original model. A new model with a significantly smaller number of parameters shows the accuracy of point matching close to the accuracy of the original model.



### Webpage Segmentation for Extracting Images and Their Surrounding Contextual Information
- **Arxiv ID**: http://arxiv.org/abs/2005.09639v1
- **DOI**: 10.1145/1631272.1631379
- **Categories**: **cs.MM**, cs.CV, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2005.09639v1)
- **Published**: 2020-05-18 19:00:03+00:00
- **Updated**: 2020-05-18 19:00:03+00:00
- **Authors**: F. Fauzi, H. J. Long, M. Belkhatir
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2005.02156
- **Journal**: None
- **Summary**: Web images come in hand with valuable contextual information. Although this information has long been mined for various uses such as image annotation, clustering of images, inference of image semantic content, etc., insufficient attention has been given to address issues in mining this contextual information. In this paper, we propose a webpage segmentation algorithm targeting the extraction of web images and their contextual information based on their characteristics as they appear on webpages. We conducted a user study to obtain a human-labeled dataset to validate the effectiveness of our method and experiments demonstrated that our method can achieve better results compared to an existing segmentation algorithm.



### Cross-filter compression for CNN inference acceleration
- **Arxiv ID**: http://arxiv.org/abs/2005.09034v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.09034v1)
- **Published**: 2020-05-18 19:06:14+00:00
- **Updated**: 2020-05-18 19:06:14+00:00
- **Authors**: Fuyuan Lyu, Shien Zhu, Weichen Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Convolution neural network demonstrates great capability for multiple tasks, such as image classification and many others. However, much resource is required to train a network. Hence much effort has been made to accelerate neural network by reducing precision of weights, activation, and gradient. However, these filter-wise quantification methods exist a natural upper limit, caused by the size of the kernel. Meanwhile, with the popularity of small kernel, the natural limit further decrease. To address this issue, we propose a new cross-filter compression method that can provide $\sim32\times$ memory savings and $122\times$ speed up in convolution operations. In our method, all convolution filters are quantized to given bits and spatially adjacent filters share the same scaling factor. Our compression method, based on Binary-Weight and XNOR-Net separately, is evaluated on CIFAR-10 and ImageNet dataset with widely used network structures, such as ResNet and VGG, and witness tolerable accuracy loss compared to state-of-the-art quantification methods.



### Translating Video Recordings of Mobile App Usages into Replayable Scenarios
- **Arxiv ID**: http://arxiv.org/abs/2005.09057v1
- **DOI**: 10.1145/3377811.3380328
- **Categories**: **cs.SE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.09057v1)
- **Published**: 2020-05-18 20:11:36+00:00
- **Updated**: 2020-05-18 20:11:36+00:00
- **Authors**: Carlos Bernal-Cárdenas, Nathan Cooper, Kevin Moran, Oscar Chaparro, Andrian Marcus, Denys Poshyvanyk
- **Comment**: In proceedings of the 42nd International Conference on Software
  Engineering (ICSE'20), 13 pages
- **Journal**: None
- **Summary**: Screen recordings of mobile applications are easy to obtain and capture a wealth of information pertinent to software developers (e.g., bugs or feature requests), making them a popular mechanism for crowdsourced app feedback. Thus, these videos are becoming a common artifact that developers must manage. In light of unique mobile development constraints, including swift release cycles and rapidly evolving platforms, automated techniques for analyzing all types of rich software artifacts provide benefit to mobile developers. Unfortunately, automatically analyzing screen recordings presents serious challenges, due to their graphical nature, compared to other types of (textual) artifacts. To address these challenges, this paper introduces V2S, a lightweight, automated approach for translating video recordings of Android app usages into replayable scenarios. V2S is based primarily on computer vision techniques and adapts recent solutions for object detection and image classification to detect and classify user actions captured in a video, and convert these into a replayable test scenario. We performed an extensive evaluation of V2S involving 175 videos depicting 3,534 GUI-based actions collected from users exercising features and reproducing bugs from over 80 popular Android apps. Our results illustrate that V2S can accurately replay scenarios from screen recordings, and is capable of reproducing $\approx$ 89% of our collected videos with minimal overhead. A case study with three industrial partners illustrates the potential usefulness of V2S from the viewpoint of developers.



### Two-View Fine-grained Classification of Plant Species
- **Arxiv ID**: http://arxiv.org/abs/2005.09110v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.09110v2)
- **Published**: 2020-05-18 21:57:47+00:00
- **Updated**: 2021-10-04 17:51:21+00:00
- **Authors**: Voncarlos M. Araujo, Alceu S. Britto Jr., Luiz E. S. Oliveira, Alessandro L. Koerich
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic plant classification is a challenging problem due to the wide biodiversity of the existing plant species in a fine-grained scenario. Powerful deep learning architectures have been used to improve the classification performance in such a fine-grained problem, but usually building models that are highly dependent on a large training dataset and which are not scalable. In this paper, we propose a novel method based on a two-view leaf image representation and a hierarchical classification strategy for fine-grained recognition of plant species. It uses the botanical taxonomy as a basis for a coarse-to-fine strategy applied to identify the plant genus and species. The two-view representation provides complementary global and local features of leaf images. A deep metric based on Siamese convolutional neural networks is used to reduce the dependence on a large number of training samples and make the method scalable to new plant species. The experimental results on two challenging fine-grained datasets of leaf images (i.e. LifeCLEF 2015 and LeafSnap) have shown the effectiveness of the proposed method, which achieved recognition accuracy of 0.87 and 0.96 respectively.



### Domain Adaptive Relational Reasoning for 3D Multi-Organ Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2005.09120v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.09120v2)
- **Published**: 2020-05-18 22:44:34+00:00
- **Updated**: 2020-07-11 20:23:19+00:00
- **Authors**: Shuhao Fu, Yongyi Lu, Yan Wang, Yuyin Zhou, Wei Shen, Elliot Fishman, Alan Yuille
- **Comment**: Accepted at MICCAL 2020
- **Journal**: None
- **Summary**: In this paper, we present a novel unsupervised domain adaptation (UDA) method, named Domain Adaptive Relational Reasoning (DARR), to generalize 3D multi-organ segmentation models to medical data collected from different scanners and/or protocols (domains). Our method is inspired by the fact that the spatial relationship between internal structures in medical images is relatively fixed, e.g., a spleen is always located at the tail of a pancreas, which serves as a latent variable to transfer the knowledge shared across multiple domains. We formulate the spatial relationship by solving a jigsaw puzzle task, i.e., recovering a CT scan from its shuffled patches, and jointly train it with the organ segmentation task. To guarantee the transferability of the learned spatial relationship to multiple domains, we additionally introduce two schemes: 1) Employing a super-resolution network also jointly trained with the segmentation model to standardize medical images from different domain to a certain spatial resolution; 2) Adapting the spatial relationship for a test image by test-time jigsaw puzzle training. Experimental results show that our method improves the performance by 29.60% DSC on target datasets on average without using any data from the target domain during training.



### Improve robustness of DNN for ECG signal classification:a noise-to-signal ratio perspective
- **Arxiv ID**: http://arxiv.org/abs/2005.09134v3
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.09134v3)
- **Published**: 2020-05-18 23:37:33+00:00
- **Updated**: 2021-04-15 23:10:37+00:00
- **Authors**: Linhai Ma, Liang Liang
- **Comment**: This paper is accepted at ICLR 2020 workshop on Artificial
  Intelligence for Affordable Healthcare. 14 pages, 7 figures
- **Journal**: None
- **Summary**: Electrocardiogram (ECG) is the most widely used diagnostic tool to monitor the condition of the cardiovascular system. Deep neural networks (DNNs), have been developed in many research labs for automatic interpretation of ECG signals to identify potential abnormalities in patient hearts. Studies have shown that given a sufficiently large amount of data, the classification accuracy of DNNs could reach human-expert cardiologist level. A DNN-based automated ECG diagnostic system would be an affordable solution for patients in developing countries where human-expert cardiologist are lacking. However, despite of the excellent performance in classification accuracy, it has been shown that DNNs are highly vulnerable to adversarial attacks: subtle changes in input of a DNN can lead to a wrong classification output with high confidence. Thus, it is challenging and essential to improve adversarial robustness of DNNs for ECG signal classification, a life-critical application. In this work, we proposed to improve DNN robustness from the perspective of noise-to-signal ratio (NSR) and developed two methods to minimize NSR during training process. We evaluated the proposed methods on PhysionNets MIT-BIH dataset, and the results show that our proposed methods lead to an enhancement in robustness against PGD adversarial attack and SPSA attack, with a minimal change in accuracy on clean data.



