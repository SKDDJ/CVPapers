# Arxiv Papers in cs.CV on 2020-05-22
### A Concise Review of Recent Few-shot Meta-learning Methods
- **Arxiv ID**: http://arxiv.org/abs/2005.10953v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.10953v1)
- **Published**: 2020-05-22 00:39:14+00:00
- **Updated**: 2020-05-22 00:39:14+00:00
- **Authors**: Xiaoxu Li, Zhuo Sun, Jing-Hao Xue, Zhanyu Ma
- **Comment**: 7 pages
- **Journal**: None
- **Summary**: Few-shot meta-learning has been recently reviving with expectations to mimic humanity's fast adaption to new concepts based on prior knowledge. In this short communication, we give a concise review on recent representative methods in few-shot meta-learning, which are categorized into four branches according to their technical characteristics. We conclude this review with some vital current challenges and future prospects in few-shot meta-learning.



### Head2Head: Video-based Neural Head Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2005.10954v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.10954v1)
- **Published**: 2020-05-22 00:44:43+00:00
- **Updated**: 2020-05-22 00:44:43+00:00
- **Authors**: Mohammad Rami Koujan, Michail Christos Doukas, Anastasios Roussos, Stefanos Zafeiriou
- **Comment**: To be published in 15th IEEE International Conference on Automatic
  Face and Gesture Recognition (FG 2020)
- **Journal**: None
- **Summary**: In this paper, we propose a novel machine learning architecture for facial reenactment. In particular, contrary to the model-based approaches or recent frame-based methods that use Deep Convolutional Neural Networks (DCNNs) to generate individual frames, we propose a novel method that (a) exploits the special structure of facial motion (paying particular attention to mouth motion) and (b) enforces temporal consistency. We demonstrate that the proposed method can transfer facial expressions, pose and gaze of a source actor to a target video in a photo-realistic fashion more accurately than state-of-the-art methods.



### ReenactNet: Real-time Full Head Reenactment
- **Arxiv ID**: http://arxiv.org/abs/2006.10500v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.10500v1)
- **Published**: 2020-05-22 00:51:38+00:00
- **Updated**: 2020-05-22 00:51:38+00:00
- **Authors**: Mohammad Rami Koujan, Michail Christos Doukas, Anastasios Roussos, Stefanos Zafeiriou
- **Comment**: to be published in 15th IEEE International Conference on Automatic
  Face and Gesture Recognition (FG 2020)
- **Journal**: None
- **Summary**: Video-to-video synthesis is a challenging problem aiming at learning a translation function between a sequence of semantic maps and a photo-realistic video depicting the characteristics of a driving video. We propose a head-to-head system of our own implementation capable of fully transferring the human head 3D pose, facial expressions and eye gaze from a source to a target actor, while preserving the identity of the target actor. Our system produces high-fidelity, temporally-smooth and photo-realistic synthetic videos faithfully transferring the human time-varying head attributes from the source to the target actor. Our proposed implementation: 1) works in real time ($\sim 20$ fps), 2) runs on a commodity laptop with a webcam as the only input, 3) is interactive, allowing the participant to drive a target person, e.g. a celebrity, politician, etc, instantly by varying their expressions, head pose, and eye gaze, and visualising the synthesised video concurrently.



### Classification of Epithelial Ovarian Carcinoma Whole-Slide Pathology Images Using Deep Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/2005.10957v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.10957v2)
- **Published**: 2020-05-22 01:14:05+00:00
- **Updated**: 2020-06-29 03:03:56+00:00
- **Authors**: Yiping Wang, David Farnell, Hossein Farahani, Mitchell Nursey, Basile Tessier-Cloutier, Steven J. M. Jones, David G. Huntsman, C. Blake Gilks, Ali Bashashati
- **Comment**: None
- **Journal**: None
- **Summary**: Ovarian cancer is the most lethal cancer of the female reproductive organs. There are $5$ major histological subtypes of epithelial ovarian cancer, each with distinct morphological, genetic, and clinical features. Currently, these histotypes are determined by a pathologist's microscopic examination of tumor whole-slide images (WSI). This process has been hampered by poor inter-observer agreement (Cohen's kappa $0.54$-$0.67$). We utilized a \textit{two}-stage deep transfer learning algorithm based on convolutional neural networks (CNN) and progressive resizing for automatic classification of epithelial ovarian carcinoma WSIs. The proposed algorithm achieved a mean accuracy of $87.54\%$ and Cohen's kappa of $0.8106$ in the slide-level classification of $305$ WSIs; performing better than a standard CNN and pathologists without gynecology-specific training.



### Real-Time Monocular 4D Face Reconstruction using the LSFM models
- **Arxiv ID**: http://arxiv.org/abs/2006.10499v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.10499v1)
- **Published**: 2020-05-22 02:14:45+00:00
- **Updated**: 2020-05-22 02:14:45+00:00
- **Authors**: Mohammad Rami Koujan, Nikolai Dochev, Anastasios Roussos
- **Comment**: Published in Proceedings of the 15th ACM SIGGRAPH European Conference
  on Visual Media Production
- **Journal**: None
- **Summary**: 4D face reconstruction from a single camera is a challenging task, especially when it is required to be performed in real time. We demonstrate a system of our own implementation that solves this task accurately and runs in real time on a commodity laptop, using a webcam as the only input. Our system is interactive, allowing the user to freely move their head and show various expressions while standing in front of the camera. As a result, the put forward system both reconstructs and visualises the identity of the subject in the correct pose along with the acted facial expressions in real-time. The 4D reconstruction in our framework is based on the recently-released Large-Scale Facial Models (LSFM) \cite{LSFM1, LSFM2}, which are the largest-scale 3D Morphable Models of facial shapes ever constructed, based on a dataset of more than 10,000 facial identities from a wide range of gender, age and ethnicity combinations. This is the first real-time demo that gives users the opportunity to test in practice the capabilities of the recently-released Large-Scale Facial Models (LSFM)



### SEED: Semantics Enhanced Encoder-Decoder Framework for Scene Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/2005.10977v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.10977v1)
- **Published**: 2020-05-22 03:02:46+00:00
- **Updated**: 2020-05-22 03:02:46+00:00
- **Authors**: Zhi Qiao, Yu Zhou, Dongbao Yang, Yucan Zhou, Weiping Wang
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: Scene text recognition is a hot research topic in computer vision. Recently, many recognition methods based on the encoder-decoder framework have been proposed, and they can handle scene texts of perspective distortion and curve shape. Nevertheless, they still face lots of challenges like image blur, uneven illumination, and incomplete characters. We argue that most encoder-decoder methods are based on local visual features without explicit global semantic information. In this work, we propose a semantics enhanced encoder-decoder framework to robustly recognize low-quality scene texts. The semantic information is used both in the encoder module for supervision and in the decoder module for initializing. In particular, the state-of-the art ASTER method is integrated into the proposed framework as an exemplar. Extensive experiments demonstrate that the proposed framework is more robust for low-quality text images, and achieves state-of-the-art results on several benchmark datasets.



### Focus Longer to See Better:Recursively Refined Attention for Fine-Grained Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2005.10979v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.10979v1)
- **Published**: 2020-05-22 03:14:18+00:00
- **Updated**: 2020-05-22 03:14:18+00:00
- **Authors**: Prateek Shroff, Tianlong Chen, Yunchao Wei, Zhangyang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Neural Network has shown great strides in the coarse-grained image classification task. It was in part due to its strong ability to extract discriminative feature representations from the images. However, the marginal visual difference between different classes in fine-grained images makes this very task harder. In this paper, we tried to focus on these marginal differences to extract more representative features. Similar to human vision, our network repetitively focuses on parts of images to spot small discriminative parts among the classes. Moreover, we show through interpretability techniques how our network focus changes from coarse to fine details. Through our experiments, we also show that a simple attention model can aggregate (weighted) these finer details to focus on the most dominant discriminative part of the image. Our network uses only image-level labels and does not need bounding box/part annotation information. Further, the simplicity of our network makes it an easy plug-n-play module. Apart from providing interpretability, our network boosts the performance (up to 2%) when compared to its baseline counterparts. Our codebase is available at https://github.com/TAMU-VITA/Focus-Longer-to-See-Better



### RankPose: Learning Generalised Feature with Rank Supervision for Head Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2005.10984v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.10984v1)
- **Published**: 2020-05-22 03:27:30+00:00
- **Updated**: 2020-05-22 03:27:30+00:00
- **Authors**: Donggen Dai, Wangkit Wong, Zhuojun Chen
- **Comment**: None
- **Journal**: None
- **Summary**: We address the challenging problem of RGB image-based head pose estimation. We first reformulate head pose representation learning to constrain it to a bounded space. Head pose represented as vector projection or vector angles shows helpful to improving performance. Further, a ranking loss combined with MSE regression loss is proposed. The ranking loss supervises a neural network with paired samples of the same person and penalises incorrect ordering of pose prediction. Analysis on this new loss function suggests it contributes to a better local feature extractor, where features are generalised to Abstract Landmarks which are pose-related features instead of pose-irrelevant information such as identity, age, and lighting. Extensive experiments show that our method significantly outperforms the current state-of-the-art schemes on public datasets: AFLW2000 and BIWI. Our model achieves significant improvements over previous SOTA MAE on AFLW2000 and BIWI from 4.50 to 3.66 and from 4.0 to 3.71 respectively. Source code will be made available at: https://github.com/seathiefwang/RankHeadPose.



### Apply VGGNet-based deep learning model of vibration data for prediction model of gravity acceleration equipment
- **Arxiv ID**: http://arxiv.org/abs/2005.10985v2
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.10985v2)
- **Published**: 2020-05-22 03:36:06+00:00
- **Updated**: 2020-08-19 02:49:31+00:00
- **Authors**: SeonWoo Lee, HyeonTak Yu, HoJun Yang, JaeHeung Yang, GangMin Lim, KyuSung Kim, ByeongKeun Choi, JangWoo Kwon
- **Comment**: 15 pages, 10 figures "for associated publication of paper is as
  follow: Journal of Mechanics in Medicine and Biology,
  https://www.worldscientific.com/worldscinet/jmmb"
- **Journal**: None
- **Summary**: Hypergravity accelerators are a type of large machinery used for gravity training or medical research. A failure of such large equipment can be a serious problem in terms of safety or costs. This paper proposes a prediction model that can proactively prevent failures that may occur in a hypergravity accelerator. The method proposed in this paper was to convert vibration signals to spectograms and perform classification training using a deep learning model. An experiment was conducted to evaluate the performance of the method proposed in this paper. A 4-channel accelerometer was attached to the bearing housing, which is a rotor, and time-amplitude data were obtained from the measured values by sampling. The data were converted to a two-dimensional spectrogram, and classification training was performed using a deep learning model for four conditions of the equipment: Unbalance, Misalignment, Shaft Rubbing, and Normal. The experimental results showed that the proposed method had a 99.5% F1-Score, which was up to 23% higher than the 76.25% for existing feature-based learning models.



### A Convolutional Neural Network with Parallel Multi-Scale Spatial Pooling to Detect Temporal Changes in SAR Images
- **Arxiv ID**: http://arxiv.org/abs/2005.10986v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.10986v1)
- **Published**: 2020-05-22 03:37:30+00:00
- **Updated**: 2020-05-22 03:37:30+00:00
- **Authors**: Jia-Wei Chen, Rongfang Wang, Fan Ding, Bo Liu, Licheng Jiao, Jie Zhang
- **Comment**: None
- **Journal**: Remote Sens. 2020, 12, 1619
- **Summary**: In synthetic aperture radar (SAR) image change detection, it is quite challenging to exploit the changing information from the noisy difference image subject to the speckle. In this paper, we propose a multi-scale spatial pooling (MSSP) network to exploit the changed information from the noisy difference image. Being different from the traditional convolutional network with only mono-scale pooling kernels, in the proposed method, multi-scale pooling kernels are equipped in a convolutional network to exploit the spatial context information on changed regions from the difference image. Furthermore, to verify the generalization of the proposed method, we apply our proposed method to the cross-dataset bitemporal SAR image change detection, where the MSSP network (MSSP-Net) is trained on a dataset and then applied to an unknown testing dataset. We compare the proposed method with other state-of-arts and the comparisons are performed on four challenging datasets of bitemporal SAR images. Experimental results demonstrate that our proposed method obtains comparable results with S-PCA-Net on YR-A and YR-B dataset and outperforms other state-of-art methods, especially on the Sendai-A and Sendai-B datasets with more complex scenes. More important, MSSP-Net is more efficient than S-PCA-Net and convolutional neural networks (CNN) with less executing time in both training and testing phases.



### Investigating Vulnerability to Adversarial Examples on Multimodal Data Fusion in Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2005.10987v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.10987v1)
- **Published**: 2020-05-22 03:45:06+00:00
- **Updated**: 2020-05-22 03:45:06+00:00
- **Authors**: Youngjoon Yu, Hong Joo Lee, Byeong Cheon Kim, Jung Uk Kim, Yong Man Ro
- **Comment**: None
- **Journal**: None
- **Summary**: The success of multimodal data fusion in deep learning appears to be attributed to the use of complementary in-formation between multiple input data. Compared to their predictive performance, relatively less attention has been devoted to the robustness of multimodal fusion models. In this paper, we investigated whether the current multimodal fusion model utilizes the complementary intelligence to defend against adversarial attacks. We applied gradient based white-box attacks such as FGSM and PGD on MFNet, which is a major multispectral (RGB, Thermal) fusion deep learning model for semantic segmentation. We verified that the multimodal fusion model optimized for better prediction is still vulnerable to adversarial attack, even if only one of the sensors is attacked. Thus, it is hard to say that existing multimodal data fusion models are fully utilizing complementary relationships between multiple modalities in terms of adversarial robustness. We believe that our observations open a new horizon for adversarial attack research on multimodal data fusion.



### A CNN-LSTM Architecture for Detection of Intracranial Hemorrhage on CT scans
- **Arxiv ID**: http://arxiv.org/abs/2005.10992v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.10992v3)
- **Published**: 2020-05-22 04:00:04+00:00
- **Updated**: 2020-06-25 19:03:36+00:00
- **Authors**: Nhan T. Nguyen, Dat Q. Tran, Nghia T. Nguyen, Ha Q. Nguyen
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel method that combines a convolutional neural network (CNN) with a long short-term memory (LSTM) mechanism for accurate prediction of intracranial hemorrhage on computed tomography (CT) scans. The CNN plays the role of a slice-wise feature extractor while the LSTM is responsible for linking the features across slices. The whole architecture is trained end-to-end with input being an RGB-like image formed by stacking 3 different viewing windows of a single slice. We validate the method on the recent RSNA Intracranial Hemorrhage Detection challenge and on the CQ500 dataset. For the RSNA challenge, our best single model achieves a weighted log loss of 0.0522 on the leaderboard, which is comparable to the top 3% performances, almost all of which make use of ensemble learning. Importantly, our method generalizes very well: the model trained on the RSNA dataset significantly outperforms the 2D model, which does not take into account the relationship between slices, on CQ500. Our codes and models is publicly avaiable at https://github.com/VinBDI-MedicalImagingTeam/midl2020-cnnlstm-ich.



### Spoof Face Detection Via Semi-Supervised Adversarial Training
- **Arxiv ID**: http://arxiv.org/abs/2005.10999v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.10999v1)
- **Published**: 2020-05-22 04:32:33+00:00
- **Updated**: 2020-05-22 04:32:33+00:00
- **Authors**: Chengwei Chen, Wang Yuan, Xuequan Lu, Lizhuang Ma
- **Comment**: Submitted
- **Journal**: None
- **Summary**: Face spoofing causes severe security threats in face recognition systems. Previous anti-spoofing works focused on supervised techniques, typically with either binary or auxiliary supervision. Most of them suffer from limited robustness and generalization, especially in the cross-dataset setting. In this paper, we propose a semi-supervised adversarial learning framework for spoof face detection, which largely relaxes the supervision condition. To capture the underlying structure of live faces data in latent representation space, we propose to train the live face data only, with a convolutional Encoder-Decoder network acting as a Generator. Meanwhile, we add a second convolutional network serving as a Discriminator. The generator and discriminator are trained by competing with each other while collaborating to understand the underlying concept in the normal class(live faces). Since the spoof face detection is video based (i.e., temporal information), we intuitively take the optical flow maps converted from consecutive video frames as input. Our approach is free of the spoof faces, thus being robust and general to different types of spoof, even unknown spoof. Extensive experiments on intra- and cross-dataset tests show that our semi-supervised method achieves better or comparable results to state-of-the-art supervised techniques.



### SODA: Detecting Covid-19 in Chest X-rays with Semi-supervised Open Set Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2005.11003v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.11003v2)
- **Published**: 2020-05-22 04:58:28+00:00
- **Updated**: 2020-08-05 05:17:57+00:00
- **Authors**: Jieli Zhou, Baoyu Jing, Zeya Wang
- **Comment**: BIOKDD 2020: 19th International Workshop on Data Mining in
  Bioinformatics
- **Journal**: None
- **Summary**: Due to the shortage of COVID-19 viral testing kits and the long waiting time, radiology imaging is used to complement the screening process and triage patients into different risk levels. Deep learning based methods have taken an active role in automatically detecting COVID-19 disease in chest x-ray images, as witnessed in many recent works in early 2020. Most of these works first train a Convolutional Neural Network (CNN) on an existing large-scale chest x-ray image dataset and then fine-tune it with a COVID-19 dataset at a much smaller scale. However, direct transfer across datasets from different domains may lead to poor performance for CNN due to two issues, the large domain shift present in the biomedical imaging datasets and the extremely small scale of the COVID-19 chest x-ray dataset. In an attempt to address these two important issues, we formulate the problem of COVID-19 chest x-ray image classification in a semi-supervised open set domain adaptation setting and propose a novel domain adaptation method, Semi-supervised Open set Domain Adversarial network (SODA). SODA is able to align the data distributions across different domains in a general domain space and also in a common subspace of source and target data. In our experiments, SODA achieves a leading classification performance compared with recent state-of-the-art models in separating COVID-19 with common pneumonia. We also present initial results showing that SODA can produce better pathology localizations in the chest x-rays.



### Event-based visual place recognition with ensembles of temporal windows
- **Arxiv ID**: http://arxiv.org/abs/2006.02826v2
- **DOI**: 10.1109/LRA.2020.3025505
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2006.02826v2)
- **Published**: 2020-05-22 05:33:35+00:00
- **Updated**: 2020-09-17 23:23:38+00:00
- **Authors**: Tobias Fischer, Michael Milford
- **Comment**: 8 pages, 8 figures, additional 8 pages supplementary material
- **Journal**: IEEE Robotics and Automation Letters 2020
- **Summary**: Event cameras are bio-inspired sensors capable of providing a continuous stream of events with low latency and high dynamic range. As a single event only carries limited information about the brightness change at a particular pixel, events are commonly accumulated into spatio-temporal windows for further processing. However, the optimal window length varies depending on the scene, camera motion, the task being performed, and other factors. In this research, we develop a novel ensemble-based scheme for combining temporal windows of varying lengths that are processed in parallel. For applications where the increased computational requirements of this approach are not practical, we also introduce a new "approximate" ensemble scheme that achieves significant computational efficiencies without unduly compromising the original performance gains provided by the ensemble approach. We demonstrate our ensemble scheme on the visual place recognition (VPR) task, introducing a new Brisbane-Event-VPR dataset with annotated recordings captured using a DAVIS346 color event camera. We show that our proposed ensemble scheme significantly outperforms all the single-window baselines and conventional model-based ensembles, irrespective of the image reconstruction and feature extraction methods used in the VPR pipeline, and evaluate which ensemble combination technique performs best. These results demonstrate the significant benefits of ensemble schemes for event camera processing in the VPR domain and may have relevance to other related processes, including feature tracking, visual-inertial odometry, and steering prediction in driving.



### Feature selection for gesture recognition in Internet-of-Things for healthcare
- **Arxiv ID**: http://arxiv.org/abs/2005.11031v1
- **DOI**: 10.1109/ICC40277.2020.9149381
- **Categories**: **cs.CV**, cs.LG, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2005.11031v1)
- **Published**: 2020-05-22 06:54:53+00:00
- **Updated**: 2020-05-22 06:54:53+00:00
- **Authors**: Giulia Cisotto, Martina Capuzzo, Anna V. Guglielmi, Andrea Zanella
- **Comment**: None
- **Journal**: ICC 2020 - 2020 IEEE International Conference on Communications
  (ICC)
- **Summary**: Internet of Things is rapidly spreading across several fields, including healthcare, posing relevant questions related to communication capabilities, energy efficiency and sensors unobtrusiveness. Particularly, in the context of recognition of gestures, e.g., grasping of different objects, brain and muscular activity could be simultaneously recorded via EEG and EMG, respectively, and analyzed to identify the gesture that is being accomplished, and the quality of its performance. This paper proposes a new algorithm that aims (i) to robustly extract the most relevant features to classify different grasping tasks, and (ii) to retain the natural meaning of the selected features. This, in turn, gives the opportunity to simplify the recording setup to minimize the data traffic over the communication network, including Internet, and provide physiologically significant features for medical interpretation. The algorithm robustness is ensured both by consensus clustering as a feature selection strategy, and by nested cross-validation scheme to evaluate its classification performance.



### Real-time Semantic Segmentation via Spatial-detail Guided Context Propagation
- **Arxiv ID**: http://arxiv.org/abs/2005.11034v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.11034v5)
- **Published**: 2020-05-22 07:07:26+00:00
- **Updated**: 2022-03-19 05:18:29+00:00
- **Authors**: Shijie Hao, Yuan Zhou, Yanrong Guo, Richang Hong, Jun Cheng, Meng Wang
- **Comment**: The paper has been accepted in the IEEE Transactions on Neural
  Networks and Learning Systems
- **Journal**: None
- **Summary**: Nowadays, vision-based computing tasks play an important role in various real-world applications. However, many vision computing tasks, e.g. semantic segmentation, are usually computationally expensive, posing a challenge to the computing systems that are resource-constrained but require fast response speed. Therefore, it is valuable to develop accurate and real-time vision processing models that only require limited computational resources. To this end, we propose the Spatial-detail Guided Context Propagation Network (SGCPNet) for achieving real-time semantic segmentation. In SGCPNet, we propose the strategy of spatial-detail guided context propagation. It uses the spatial details of shallow layers to guide the propagation of the low-resolution global contexts, in which the lost spatial information can be effectively reconstructed. In this way, the need for maintaining high-resolution features along the network is freed, therefore largely improving the model efficiency. On the other hand, due to the effective reconstruction of spatial details, the segmentation accuracy can be still preserved. In the experiments, we validate the effectiveness and efficiency of the proposed SGCPNet model. On the Citysacpes dataset, for example, our SGCPNet achieves 69.5% mIoU segmentation accuracy, while its speed reaches 178.5 FPS on 768x1536 images on a GeForce GTX 1080 Ti GPU card. In addition, SGCPNet is very lightweight and only contains 0.61 M parameters.



### Position-based Scaled Gradient for Model Quantization and Pruning
- **Arxiv ID**: http://arxiv.org/abs/2005.11035v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.11035v4)
- **Published**: 2020-05-22 07:11:27+00:00
- **Updated**: 2020-11-11 03:43:25+00:00
- **Authors**: Jangho Kim, KiYoon Yoo, Nojun Kwak
- **Comment**: Advances in Neural Information Processing Systems
- **Journal**: None
- **Summary**: We propose the position-based scaled gradient (PSG) that scales the gradient depending on the position of a weight vector to make it more compression-friendly. First, we theoretically show that applying PSG to the standard gradient descent (GD), which is called PSGD, is equivalent to the GD in the warped weight space, a space made by warping the original weight space via an appropriately designed invertible function. Second, we empirically show that PSG acting as a regularizer to a weight vector is favorable for model compression domains such as quantization and pruning. PSG reduces the gap between the weight distributions of a full-precision model and its compressed counterpart. This enables the versatile deployment of a model either as an uncompressed mode or as a compressed mode depending on the availability of resources. The experimental results on CIFAR-10/100 and ImageNet datasets show the effectiveness of the proposed PSG in both domains of pruning and quantization even for extremely low bits. The code is released in Github.



### Style Normalization and Restitution for Generalizable Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2005.11037v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.11037v1)
- **Published**: 2020-05-22 07:15:10+00:00
- **Updated**: 2020-05-22 07:15:10+00:00
- **Authors**: Xin Jin, Cuiling Lan, Wenjun Zeng, Zhibo Chen, Li Zhang
- **Comment**: Accepted by CVPR2020
- **Journal**: None
- **Summary**: Existing fully-supervised person re-identification (ReID) methods usually suffer from poor generalization capability caused by domain gaps. The key to solving this problem lies in filtering out identity-irrelevant interference and learning domain-invariant person representations. In this paper, we aim to design a generalizable person ReID framework which trains a model on source domains yet is able to generalize/perform well on target domains. To achieve this goal, we propose a simple yet effective Style Normalization and Restitution (SNR) module. Specifically, we filter out style variations (e.g., illumination, color contrast) by Instance Normalization (IN). However, such a process inevitably removes discriminative information. We propose to distill identity-relevant feature from the removed information and restitute it to the network to ensure high discrimination. For better disentanglement, we enforce a dual causal loss constraint in SNR to encourage the separation of identity-relevant features and identity-irrelevant features. Extensive experiments demonstrate the strong generalization capability of our framework. Our models empowered by the SNR modules significantly outperform the state-of-the-art domain generalization approaches on multiple widely-used person ReID benchmarks, and also show superiority on unsupervised domain adaptation.



### Arbitrary-sized Image Training and Residual Kernel Learning: Towards Image Fraud Identification
- **Arxiv ID**: http://arxiv.org/abs/2005.11043v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2005.11043v1)
- **Published**: 2020-05-22 07:57:24+00:00
- **Updated**: 2020-05-22 07:57:24+00:00
- **Authors**: Hongyu Li, Xiaogang Huang, Zhihui Fu, Xiaolin Li
- **Comment**: None
- **Journal**: None
- **Summary**: Preserving original noise residuals in images are critical to image fraud identification. Since the resizing operation during deep learning will damage the microstructures of image noise residuals, we propose a framework for directly training images of original input scales without resizing. Our arbitrary-sized image training method mainly depends on the pseudo-batch gradient descent (PBGD), which bridges the gap between the input batch and the update batch to assure that model updates can normally run for arbitrary-sized images.   In addition, a 3-phase alternate training strategy is designed to learn optimal residual kernels for image fraud identification. With the learnt residual kernels and PBGD, the proposed framework achieved the state-of-the-art results in image fraud identification, especially for images with small tampered regions or unseen images with different tampering distributions.



### Polarimetric image augmentation
- **Arxiv ID**: http://arxiv.org/abs/2005.11044v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.11044v2)
- **Published**: 2020-05-22 08:00:12+00:00
- **Updated**: 2020-07-10 15:12:40+00:00
- **Authors**: Marc Blanchon, Olivier Morel, Fabrice Meriaudeau, Ralph Seulin, Désiré Sidibé
- **Comment**: 7 pages, submitted to ICPR2020 second round
- **Journal**: None
- **Summary**: Robotics applications in urban environments are subject to obstacles that exhibit specular reflections hampering autonomous navigation. On the other hand, these reflections are highly polarized and this extra information can successfully be used to segment the specular areas. In nature, polarized light is obtained by reflection or scattering. Deep Convolutional Neural Networks (DCNNs) have shown excellent segmentation results, but require a significant amount of data to achieve best performances. The lack of data is usually overcomed by using augmentation methods. However, unlike RGB images, polarization images are not only scalar (intensity) images and standard augmentation techniques cannot be applied straightforwardly. We propose to enhance deep learning models through a regularized augmentation procedure applied to polarimetric data in order to characterize scenes more effectively under challenging conditions. We subsequently observe an average of 18.1% improvement in IoU between non augmented and regularized training procedures on real world data.



### Vulnerability of deep neural networks for detecting COVID-19 cases from chest X-ray images to universal adversarial attacks
- **Arxiv ID**: http://arxiv.org/abs/2005.11061v1
- **DOI**: 10.1371/journal.pone.0243963
- **Categories**: **cs.CV**, cs.CR, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.11061v1)
- **Published**: 2020-05-22 08:54:41+00:00
- **Updated**: 2020-05-22 08:54:41+00:00
- **Authors**: Hokuto Hirano, Kazuki Koga, Kazuhiro Takemoto
- **Comment**: 17 pages, 5 figures, 3 tables
- **Journal**: PLoS ONE 5(12), e0243963 (2020)
- **Summary**: Under the epidemic of the novel coronavirus disease 2019 (COVID-19), chest X-ray computed tomography imaging is being used for effectively screening COVID-19 patients. The development of computer-aided systems based on deep neural networks (DNNs) has been advanced, to rapidly and accurately detect COVID-19 cases, because the need for expert radiologists, who are limited in number, forms a bottleneck for the screening. However, so far, the vulnerability of DNN-based systems has been poorly evaluated, although DNNs are vulnerable to a single perturbation, called universal adversarial perturbation (UAP), which can induce DNN failure in most classification tasks. Thus, we focus on representative DNN models for detecting COVID-19 cases from chest X-ray images and evaluate their vulnerability to UAPs generated using simple iterative algorithms. We consider nontargeted UAPs, which cause a task failure resulting in an input being assigned an incorrect label, and targeted UAPs, which cause the DNN to classify an input into a specific class. The results demonstrate that the models are vulnerable to nontargeted and targeted UAPs, even in case of small UAPs. In particular, 2% norm of the UPAs to the average norm of an image in the image dataset achieves >85% and >90% success rates for the nontargeted and targeted attacks, respectively. Due to the nontargeted UAPs, the DNN models judge most chest X-ray images as COVID-19 cases. The targeted UAPs make the DNN models classify most chest X-ray images into a given target class. The results indicate that careful consideration is required in practical applications of DNNs to COVID-19 diagnosis; in particular, they emphasize the need for strategies to address security concerns. As an example, we show that iterative fine-tuning of the DNN models using UAPs improves the robustness of the DNN models against UAPs.



### Driver Identification through Stochastic Multi-State Car-Following Modeling
- **Arxiv ID**: http://arxiv.org/abs/2005.11077v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2005.11077v1)
- **Published**: 2020-05-22 09:39:00+00:00
- **Updated**: 2020-05-22 09:39:00+00:00
- **Authors**: Donghao Xu, Zhezhang Ding, Chenfeng Tu, Huijing Zhao, Mathieu Moze, François Aioun, Franck Guillemard
- **Comment**: 13 pages, 4 figures. Submitted to T.ITS
- **Journal**: None
- **Summary**: Intra-driver and inter-driver heterogeneity has been confirmed to exist in human driving behaviors by many studies. In this study, a joint model of the two types of heterogeneity in car-following behavior is proposed as an approach of driver profiling and identification. It is assumed that all drivers share a pool of driver states; under each state a car-following data sequence obeys a specific probability distribution in feature space; each driver has his/her own probability distribution over the states, called driver profile, which characterize the intradriver heterogeneity, while the difference between the driver profile of different drivers depict the inter-driver heterogeneity. Thus, the driver profile can be used to distinguish a driver from others. Based on the assumption, a stochastic car-following model is proposed to take both intra-driver and inter-driver heterogeneity into consideration, and a method is proposed to jointly learn parameters in behavioral feature extractor, driver states and driver profiles. Experiments demonstrate the performance of the proposed method in driver identification on naturalistic car-following data: accuracy of 82.3% is achieved in an 8-driver experiment using 10 car-following sequences of duration 15 seconds for online inference. The potential of fast registration of new drivers are demonstrated and discussed.



### Point2Mesh: A Self-Prior for Deformable Meshes
- **Arxiv ID**: http://arxiv.org/abs/2005.11084v1
- **DOI**: 10.1145/3386569.3392415
- **Categories**: **cs.GR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.11084v1)
- **Published**: 2020-05-22 10:01:04+00:00
- **Updated**: 2020-05-22 10:01:04+00:00
- **Authors**: Rana Hanocka, Gal Metzer, Raja Giryes, Daniel Cohen-Or
- **Comment**: SIGGRAPH 2020; Project page:
  https://ranahanocka.github.io/point2mesh/
- **Journal**: None
- **Summary**: In this paper, we introduce Point2Mesh, a technique for reconstructing a surface mesh from an input point cloud. Instead of explicitly specifying a prior that encodes the expected shape properties, the prior is defined automatically using the input point cloud, which we refer to as a self-prior. The self-prior encapsulates reoccurring geometric repetitions from a single shape within the weights of a deep neural network. We optimize the network weights to deform an initial mesh to shrink-wrap a single input point cloud. This explicitly considers the entire reconstructed shape, since shared local kernels are calculated to fit the overall object. The convolutional kernels are optimized globally across the entire shape, which inherently encourages local-scale geometric self-similarity across the shape surface. We show that shrink-wrapping a point cloud with a self-prior converges to a desirable solution; compared to a prescribed smoothness prior, which often becomes trapped in undesirable local minima. While the performance of traditional reconstruction approaches degrades in non-ideal conditions that are often present in real world scanning, i.e., unoriented normals, noise and missing (low density) parts, Point2Mesh is robust to non-ideal conditions. We demonstrate the performance of Point2Mesh on a large variety of shapes with varying complexity.



### Improving Co-registration for Sentinel-1 SAR and Sentinel-2 Optical images
- **Arxiv ID**: http://arxiv.org/abs/2005.11092v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.11092v2)
- **Published**: 2020-05-22 10:35:11+00:00
- **Updated**: 2021-06-14 07:52:42+00:00
- **Authors**: Yuanxin Ye, Chao Yang, Bai Zhu, Youquan He, Huarong Jia
- **Comment**: None
- **Journal**: None
- **Summary**: Co-registering the Sentinel-1 SAR and Sentinel-2 optical data of European Space Agency (ESA) is of great importance for many remote sensing applications. However, we find that there are evident misregistration shifts between the Sentinel-1 SAR and Sentinel-2 optical images that are directly downloaded from the official website. To address that, this paper presents a fast and effective registration method for the two types of images. In the proposed method, a block-based scheme is first designed to extract evenly distributed interest points. Then the correspondences are detected by using the similarity of structural features between the SAR and optical images, where the three dimension (3D) phase correlation (PC) is used as the similarity measure for accelerating image matching. Finally, the obtained correspondences are employed to measure the misregistration shifts between the images. Moreover, to eliminate the misregistration, we use some representative geometric transformation models such as polynomial models, projective models, and rational function models for the co-registration of the two types of images, and compare and analyze their registration accuracy under different numbers of control points and different terrains. Six pairs of the Sentinel-1 SAR L1 and Sentinel-2 optical L1C images covering three different terrains are tested in our experiments. Experimental results show that the proposed method can achieve precise correspondences between the images, and the 3rd. Order polynomial achieves the most satisfactory registration results. Its registration accuracy of the flat areas is less than 1.0 10m pixels, and that of the hilly areas is about 1.5 10m pixels, and that of the mountainous areas is between 1.7 and 2.3 10m pixels, which significantly improves the co-registration accuracy of the Sentinel-1 SAR and Sentinel-2 optical images.



### Deep Learning Based Detection and Localization of Intracranial Aneurysms in Computed Tomography Angiography
- **Arxiv ID**: http://arxiv.org/abs/2005.11098v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.11098v2)
- **Published**: 2020-05-22 10:49:23+00:00
- **Updated**: 2021-12-14 20:03:17+00:00
- **Authors**: Dufan Wu, Daniel Montes, Ziheng Duan, Yangsibo Huang, Javier M. Romero, Ramon Gilberto Gonzalez, Quanzheng Li
- **Comment**: None
- **Journal**: None
- **Summary**: Purpose: To develop CADIA, a supervised deep learning model based on a region proposal network coupled with a false-positive reduction module for the detection and localization of intracranial aneurysms (IA) from computed tomography angiography (CTA), and to assess our model's performance to a similar detection network. Methods: In this retrospective study, we evaluated 1,216 patients from two separate institutions who underwent CT for the presence of saccular IA>=2.5 mm. A two-step model was implemented: a 3D region proposal network for initial aneurysm detection and 3D DenseNetsfor false-positive reduction and further determination of suspicious IA. Free-response receiver operative characteristics (FROC) curve and lesion-/patient-level performance at established false positive per volume (FPPV) were also performed. Fisher's exact test was used to compare with a similar available model. Results: CADIA's sensitivities at 0.25 and 1 FPPV were 63.9% and 77.5%, respectively. Our model's performance varied with size and location, and the best performance was achieved in IA between 5-10 mm and in those at anterior communicating artery, with sensitivities at 1 FPPV of 95.8% and 94%, respectively. Our model showed statistically higher patient-level accuracy, sensitivity, and specificity when compared to the available model at 0.25 FPPV and the best F-1 score (P<=0.001). At 1 FPPV threshold, our model showed better accuracy and specificity (P<=0.001) and equivalent sensitivity. Conclusions: CADIA outperformed a comparable network in the detection task of IA. The addition of a false-positive reduction module is a feasible step to improve the IA detection models.



### A Comparative Evaluation of Heart Rate Estimation Methods using Face Videos
- **Arxiv ID**: http://arxiv.org/abs/2005.11101v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2005.11101v1)
- **Published**: 2020-05-22 10:54:49+00:00
- **Updated**: 2020-05-22 10:54:49+00:00
- **Authors**: Javier Hernandez-Ortega, Julian Fierrez, Aythami Morales, David Diaz
- **Comment**: Accepted in "IEEE International Workshop on Medical Computing
  (MediComp) 2020"
- **Journal**: None
- **Summary**: This paper presents a comparative evaluation of methods for remote heart rate estimation using face videos, i.e., given a video sequence of the face as input, methods to process it to obtain a robust estimation of the subjects heart rate at each moment. Four alternatives from the literature are tested, three based in hand crafted approaches and one based on deep learning. The methods are compared using RGB videos from the COHFACE database. Experiments show that the learning-based method achieves much better accuracy than the hand crafted ones. The low error rate achieved by the learning based model makes possible its application in real scenarios, e.g. in medical or sports environments.



### Deep covariate-learning: optimising information extraction from terrain texture for geostatistical modelling applications
- **Arxiv ID**: http://arxiv.org/abs/2005.11194v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML, I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2005.11194v2)
- **Published**: 2020-05-22 14:00:28+00:00
- **Updated**: 2020-06-15 11:19:48+00:00
- **Authors**: Charlie Kirkwood
- **Comment**: 14 pages, 8 figures, submitted to journal
- **Journal**: None
- **Summary**: Where data is available, it is desirable in geostatistical modelling to make use of additional covariates, for example terrain data, in order to improve prediction accuracy in the modelling task. While elevation itself may be important, additional explanatory power for any given problem can be sought (but not necessarily found) by filtering digital elevation models to extract higher-order derivatives such as slope angles, curvatures, and roughness. In essence, it would be beneficial to extract as much task-relevant information as possible from the elevation grid. However, given the complexities of the natural world, chance dictates that the use of 'off-the-shelf' filters is unlikely to derive covariates that provide strong explanatory power to the target variable at hand, and any attempt to manually design informative covariates is likely to be a trial-and-error process -- not optimal. In this paper we present a solution to this problem in the form of a deep learning approach to automatically deriving optimal task-specific terrain texture covariates from a standard SRTM 90m gridded digital elevation model (DEM). For our target variables we use point-sampled geochemical data from the British Geological Survey: concentrations of potassium, calcium and arsenic in stream sediments. We find that our deep learning approach produces covariates for geostatistical modelling that have surprisingly strong explanatory power on their own, with R-squared values around 0.6 for all three elements (with arsenic on the log scale). These results are achieved without the neural network being provided with easting, northing, or absolute elevation as inputs, and purely reflect the capacity of our deep neural network to extract task-specific information from terrain texture. We hope that these results will inspire further investigation into the capabilities of deep learning within geostatistical applications.



### Semi-supervised Medical Image Classification with Global Latent Mixing
- **Arxiv ID**: http://arxiv.org/abs/2005.11217v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.11217v1)
- **Published**: 2020-05-22 14:49:13+00:00
- **Updated**: 2020-05-22 14:49:13+00:00
- **Authors**: Prashnna Kumar Gyawali, Sandesh Ghimire, Pradeep Bajracharya, Zhiyuan Li, Linwei Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Computer-aided diagnosis via deep learning relies on large-scale annotated data sets, which can be costly when involving expert knowledge. Semi-supervised learning (SSL) mitigates this challenge by leveraging unlabeled data. One effective SSL approach is to regularize the local smoothness of neural functions via perturbations around single data points. In this work, we argue that regularizing the global smoothness of neural functions by filling the void in between data points can further improve SSL. We present a novel SSL approach that trains the neural network on linear mixing of labeled and unlabeled data, at both the input and latent space in order to regularize different portions of the network. We evaluated the presented model on two distinct medical image data sets for semi-supervised classification of thoracic disease and skin lesion, demonstrating its improved performance over SSL with local perturbations and SSL with global mixing but at the input space only. Our code is available at https://github.com/Prasanna1991/LatentMixing.



### KL-Divergence-Based Region Proposal Network for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2005.11220v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.11220v1)
- **Published**: 2020-05-22 14:59:26+00:00
- **Updated**: 2020-05-22 14:59:26+00:00
- **Authors**: Geonseok Seo, Jaeyoung Yoo, Jaeseok Choi, Nojun Kwak
- **Comment**: 5 pages, 3 figures, Accepted to ICIP 2020
- **Journal**: None
- **Summary**: The learning of the region proposal in object detection using the deep neural networks (DNN) is divided into two tasks: binary classification and bounding box regression task. However, traditional RPN (Region Proposal Network) defines these two tasks as different problems, and they are trained independently. In this paper, we propose a new region proposal learning method that considers the bounding box offset's uncertainty in the objectness score. Our method redefines RPN to a problem of minimizing the KL-divergence, difference between the two probability distributions. We applied KL-RPN, which performs region proposal using KL-Divergence, to the existing two-stage object detection framework and showed that it can improve the performance of the existing method. Experiments show that it achieves 2.6% and 2.0% AP improvements on MS COCO test-dev in Faster R-CNN with VGG-16 and R-FCN with ResNet-101 backbone, respectively.



### Convolutional Neural Networks applied to sky images for short-term solar irradiance forecasting
- **Arxiv ID**: http://arxiv.org/abs/2005.11246v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, I.2.10; I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2005.11246v1)
- **Published**: 2020-05-22 15:57:39+00:00
- **Updated**: 2020-05-22 15:57:39+00:00
- **Authors**: Quentin Paletta, Joan Lasenby
- **Comment**: 4 pages, 7 figures, 1 table, accepted for European PV Solar Energy
  Conference and Exhibition (EU-PVSEC) 2020
- **Journal**: None
- **Summary**: Despite the advances in the field of solar energy, improvements of solar forecasting techniques, addressing the intermittent electricity production, remain essential for securing its future integration into a wider energy supply. A promising approach to anticipate irradiance changes consists of modeling the cloud cover dynamics from ground taken or satellite images. This work presents preliminary results on the application of deep Convolutional Neural Networks for 2 to 20 min irradiance forecasting using hemispherical sky images and exogenous variables. We evaluate the models on a set of irradiance measurements and corresponding sky images collected in Palaiseau (France) over 8 months with a temporal resolution of 2 min. To outline the learning of neural networks in the context of short-term irradiance forecasting, we implemented visualisation techniques revealing the types of patterns recognised by trained algorithms in sky images. In addition, we show that training models with past samples of the same day improves their forecast skill, relative to the smart persistence model based on the Mean Square Error, by around 10% on a 10 min ahead prediction. These results emphasise the benefit of integrating previous same-day data in short-term forecasting. This, in turn, can be achieved through model fine tuning or using recurrent units to facilitate the extraction of relevant temporal features from past data.



### PruneNet: Channel Pruning via Global Importance
- **Arxiv ID**: http://arxiv.org/abs/2005.11282v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.11282v1)
- **Published**: 2020-05-22 17:09:56+00:00
- **Updated**: 2020-05-22 17:09:56+00:00
- **Authors**: Ashish Khetan, Zohar Karnin
- **Comment**: 12 pages, 3 figures, Published in ICLR 2020 NAS Workshop
- **Journal**: None
- **Summary**: Channel pruning is one of the predominant approaches for accelerating deep neural networks. Most existing pruning methods either train from scratch with a sparsity inducing term such as group lasso, or prune redundant channels in a pretrained network and then fine tune the network. Both strategies suffer from some limitations: the use of group lasso is computationally expensive, difficult to converge and often suffers from worse behavior due to the regularization bias. The methods that start with a pretrained network either prune channels uniformly across the layers or prune channels based on the basic statistics of the network parameters. These approaches either ignore the fact that some CNN layers are more redundant than others or fail to adequately identify the level of redundancy in different layers. In this work, we investigate a simple-yet-effective method for pruning channels based on a computationally light-weight yet effective data driven optimization step that discovers the necessary width per layer. Experiments conducted on ILSVRC-$12$ confirm effectiveness of our approach. With non-uniform pruning across the layers on ResNet-$50$, we are able to match the FLOP reduction of state-of-the-art channel pruning results while achieving a $0.98\%$ higher accuracy. Further, we show that our pruned ResNet-$50$ network outperforms ResNet-$34$ and ResNet-$18$ networks, and that our pruned ResNet-$101$ outperforms ResNet-$50$.



### From ImageNet to Image Classification: Contextualizing Progress on Benchmarks
- **Arxiv ID**: http://arxiv.org/abs/2005.11295v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.11295v1)
- **Published**: 2020-05-22 17:39:16+00:00
- **Updated**: 2020-05-22 17:39:16+00:00
- **Authors**: Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Andrew Ilyas, Aleksander Madry
- **Comment**: None
- **Journal**: None
- **Summary**: Building rich machine learning datasets in a scalable manner often necessitates a crowd-sourced data collection pipeline. In this work, we use human studies to investigate the consequences of employing such a pipeline, focusing on the popular ImageNet dataset. We study how specific design choices in the ImageNet creation process impact the fidelity of the resulting dataset---including the introduction of biases that state-of-the-art models exploit. Our analysis pinpoints how a noisy data collection pipeline can lead to a systematic misalignment between the resulting benchmark and the real-world task it serves as a proxy for. Finally, our findings emphasize the need to augment our current model training and evaluation toolkit to take such misalignments into account. To facilitate further research, we release our refined ImageNet annotations at https://github.com/MadryLab/ImageNetMultiLabel.



### Pulmonary Nodule Malignancy Classification Using its Temporal Evolution with Two-Stream 3D Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2005.11341v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.11341v1)
- **Published**: 2020-05-22 18:19:32+00:00
- **Updated**: 2020-05-22 18:19:32+00:00
- **Authors**: Xavier Rafael-Palou, Anton Aubanell, Ilaria Bonavita, Mario Ceresa, Gemma Piella, Vicent Ribas, Miguel A. González Ballester
- **Comment**: None
- **Journal**: None
- **Summary**: Nodule malignancy assessment is a complex, time-consuming and error-prone task. Current clinical practice requires measuring changes in size and density of the nodule at different time-points. State of the art solutions rely on 3D convolutional neural networks built on pulmonary nodules obtained from single CT scan per patient. In this work, we propose a two-stream 3D convolutional neural network that predicts malignancy by jointly analyzing two pulmonary nodule volumes from the same patient taken at different time-points. Best results achieve 77% of F1-score in test with an increment of 9% and 12% of F1-score with respect to the same network trained with images from a single time-point.



### Stable and expressive recurrent vision models
- **Arxiv ID**: http://arxiv.org/abs/2005.11362v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.11362v2)
- **Published**: 2020-05-22 19:31:28+00:00
- **Updated**: 2020-10-22 23:15:14+00:00
- **Authors**: Drew Linsley, Alekh Karkada Ashok, Lakshmi Narasimhan Govindarajan, Rex Liu, Thomas Serre
- **Comment**: Published at NeurIPS 2020
- **Journal**: None
- **Summary**: Primate vision depends on recurrent processing for reliable perception. A growing body of literature also suggests that recurrent connections improve the learning efficiency and generalization of vision models on classic computer vision challenges. Why then, are current large-scale challenges dominated by feedforward networks? We posit that the effectiveness of recurrent vision models is bottlenecked by the standard algorithm used for training them, "back-propagation through time" (BPTT), which has O(N) memory-complexity for training an N step model. Thus, recurrent vision model design is bounded by memory constraints, forcing a choice between rivaling the enormous capacity of leading feedforward models or trying to compensate for this deficit through granular and complex dynamics. Here, we develop a new learning algorithm, "contractor recurrent back-propagation" (C-RBP), which alleviates these issues by achieving constant O(1) memory-complexity with steps of recurrent processing. We demonstrate that recurrent vision models trained with C-RBP can detect long-range spatial dependencies in a synthetic contour tracing task that BPTT-trained models cannot. We further show that recurrent vision models trained with C-RBP to solve the large-scale Panoptic Segmentation MS-COCO challenge outperform the leading feedforward approach, with fewer free parameters. C-RBP is a general-purpose learning algorithm for any application that can benefit from expansive recurrent dynamics. Code and data are available at https://github.com/c-rbp.



### Gleason Grading of Histology Prostate Images through Semantic Segmentation via Residual U-Net
- **Arxiv ID**: http://arxiv.org/abs/2005.11368v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.11368v1)
- **Published**: 2020-05-22 19:49:10+00:00
- **Updated**: 2020-05-22 19:49:10+00:00
- **Authors**: Amartya Kalapahar, Julio Silva-Rodríguez, Adrián Colomer, Fernando López-Mir, Valery Naranjo
- **Comment**: None
- **Journal**: None
- **Summary**: Worldwide, prostate cancer is one of the main cancers affecting men. The final diagnosis of prostate cancer is based on the visual detection of Gleason patterns in prostate biopsy by pathologists. Computer-aided-diagnosis systems allow to delineate and classify the cancerous patterns in the tissue via computer-vision algorithms in order to support the physicians' task. The methodological core of this work is a U-Net convolutional neural network for image segmentation modified with residual blocks able to segment cancerous tissue according to the full Gleason system. This model outperforms other well-known architectures, and reaches a pixel-level Cohen's quadratic Kappa of 0.52, at the level of previous image-level works in the literature, but providing also a detailed localisation of the patterns.



### Image Translation by Latent Union of Subspaces for Cross-Domain Plaque Detection
- **Arxiv ID**: http://arxiv.org/abs/2005.11384v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.11384v1)
- **Published**: 2020-05-22 20:35:34+00:00
- **Updated**: 2020-05-22 20:35:34+00:00
- **Authors**: Yingying Zhu, Daniel C. Elton, Sungwon Lee, Perry J. Pickhardt, Ronald M. Summers
- **Comment**: accepted as a short paper in the 2020 Medical Imaging with Deep
  Learning (MIDL) conference
- **Journal**: None
- **Summary**: Calcified plaque in the aorta and pelvic arteries is associated with coronary artery calcification and is a strong predictor of heart attack. Current calcified plaque detection models show poor generalizability to different domains (ie. pre-contrast vs. post-contrast CT scans). Many recent works have shown how cross domain object detection can be improved using an image translation model which translates between domains using a single shared latent space. However, while current image translation models do a good job preserving global/intermediate level structures they often have trouble preserving tiny structures. In medical imaging applications, preserving small structures is important since these structures can carry information which is highly relevant for disease diagnosis. Recent works on image reconstruction show that complex real-world images are better reconstructed using a union of subspaces approach. Since small image patches are used to train the image translation model, it makes sense to enforce that each patch be represented by a linear combination of subspaces which may correspond to the different parts of the body present in that patch. Motivated by this, we propose an image translation network using a shared union of subspaces constraint and show our approach preserves subtle structures (plaques) better than the conventional method. We further applied our method to a cross domain plaque detection task and show significant improvement compared to the state-of-the art method.



### One of these (Few) Things is Not Like the Others
- **Arxiv ID**: http://arxiv.org/abs/2005.11405v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.11405v1)
- **Published**: 2020-05-22 21:49:35+00:00
- **Updated**: 2020-05-22 21:49:35+00:00
- **Authors**: Nat Roth, Justin Wagle
- **Comment**: None
- **Journal**: None
- **Summary**: To perform well, most deep learning based image classification systems require large amounts of data and computing resources. These constraints make it difficult to quickly personalize to individual users or train models outside of fairly powerful machines. To deal with these problems, there has been a large body of research into teaching machines to learn to classify images based on only a handful of training examples, a field known as few-shot learning. Few-shot learning research traditionally makes the simplifying assumption that all images belong to one of a fixed number of previously seen groups. However, many image datasets, such as a camera roll on a phone, will be noisy and contain images that may not be relevant or fit into any clear group. We propose a model which can both classify new images based on a small number of examples and recognize images which do not belong to any previously seen group. We adapt previous few-shot learning work to include a simple mechanism for learning a cutoff that determines whether an image should be excluded or classified. We examine how well our method performs in a realistic setting, benchmarking the approach on a noisy and ambiguous dataset of images. We evaluate performance over a spectrum of model architectures, including setups small enough to be run on low powered devices, such as mobile phones or web browsers. We find that this task of excluding irrelevant images poses significant extra difficulty beyond that of the traditional few-shot task. We decompose the sources of this error, and suggest future improvements that might alleviate this difficulty.



### Novel Human-Object Interaction Detection via Adversarial Domain Generalization
- **Arxiv ID**: http://arxiv.org/abs/2005.11406v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.11406v1)
- **Published**: 2020-05-22 22:02:56+00:00
- **Updated**: 2020-05-22 22:02:56+00:00
- **Authors**: Yuhang Song, Wenbo Li, Lei Zhang, Jianwei Yang, Emre Kiciman, Hamid Palangi, Jianfeng Gao, C. -C. Jay Kuo, Pengchuan Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: We study in this paper the problem of novel human-object interaction (HOI) detection, aiming at improving the generalization ability of the model to unseen scenarios. The challenge mainly stems from the large compositional space of objects and predicates, which leads to the lack of sufficient training data for all the object-predicate combinations. As a result, most existing HOI methods heavily rely on object priors and can hardly generalize to unseen combinations. To tackle this problem, we propose a unified framework of adversarial domain generalization to learn object-invariant features for predicate prediction. To measure the performance improvement, we create a new split of the HICO-DET dataset, where the HOIs in the test set are all unseen triplet categories in the training set. Our experiments show that the proposed framework significantly increases the performance by up to 50% on the new split of HICO-DET dataset and up to 125% on the UnRel dataset for auxiliary evaluation in detecting novel HOIs.



### Approaching Bio Cellular Classification for Malaria Infected Cells Using Machine Learning and then Deep Learning to compare & analyze K-Nearest Neighbours and Deep CNNs
- **Arxiv ID**: http://arxiv.org/abs/2005.11417v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.11417v1)
- **Published**: 2020-05-22 23:02:36+00:00
- **Updated**: 2020-05-22 23:02:36+00:00
- **Authors**: Rishabh Malhotra, Dhron Joshi, Ku Young Shin
- **Comment**: 7 Pages
- **Journal**: None
- **Summary**: Malaria is a deadly disease which claims the lives of hundreds of thousands of people every year. Computational methods have been proven to be useful in the medical industry by providing effective means of classification of diagnostic imaging and disease identification. This paper examines different machine learning methods in the context of classifying the presence of malaria in cell images. Numerous machine learning methods can be applied to the same problem; the question of whether one machine learning method is better suited to a problem relies heavily on the problem itself and the implementation of a model. In particular, convolutional neural networks and k nearest neighbours are both analyzed and contrasted in regards to their application to classifying the presence of malaria and each models empirical performance. Here, we implement two models of classification; a convolutional neural network, and the k nearest neighbours algorithm. These two algorithms are compared based on validation accuracy. For our implementation, CNN (95%) performed 25% better than kNN (75%).



### Multi-view polarimetric scattering cloud tomography and retrieval of droplet size
- **Arxiv ID**: http://arxiv.org/abs/2005.11423v1
- **DOI**: None
- **Categories**: **physics.comp-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.11423v1)
- **Published**: 2020-05-22 23:39:21+00:00
- **Updated**: 2020-05-22 23:39:21+00:00
- **Authors**: Aviad Levis, Yoav Y. Schechner, Anthony B. Davis, Jesse Loveridge
- **Comment**: None
- **Journal**: None
- **Summary**: Tomography aims to recover a three-dimensional (3D) density map of a medium or an object. In medical imaging, it is extensively used for diagnostics via X-ray computed tomography (CT). Optical diffusion tomography is an alternative to X-ray CT that uses multiply scattered light to deliver coarse density maps for soft tissues. We define and derive tomography of cloud droplet distributions via passive remote sensing. We use multi-view polarimetric images to fit a 3D polarized radiative transfer (RT) forward model. Our motivation is 3D volumetric probing of vertically-developed convectively-driven clouds that are ill-served by current methods in operational passive remote sensing. These techniques are based on strictly 1D RT modeling and applied to a single cloudy pixel, where cloud geometry is assumed to be that of a plane-parallel slab. Incident unpolarized sunlight, once scattered by cloud-droplets, changes its polarization state according to droplet size. Therefore, polarimetric measurements in the rainbow and glory angular regions can be used to infer the droplet size distribution. This work defines and derives a framework for a full 3D tomography of cloud droplets for both their mass concentration in space and their distribution across a range of sizes. This 3D retrieval of key microphysical properties is made tractable by our novel approach that involves a restructuring and differentiation of an open-source polarized 3D RT code to accommodate a special two-step optimization technique. Physically-realistic synthetic clouds are used to demonstrate the methodology with rigorous uncertainty quantification.



### Hashing-based Non-Maximum Suppression for Crowded Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2005.11426v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.11426v1)
- **Published**: 2020-05-22 23:45:59+00:00
- **Updated**: 2020-05-22 23:45:59+00:00
- **Authors**: Jianfeng Wang, Xi Yin, Lijuan Wang, Lei Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose an algorithm, named hashing-based non-maximum suppression (HNMS) to efficiently suppress the non-maximum boxes for object detection. Non-maximum suppression (NMS) is an essential component to suppress the boxes at closely located locations with similar shapes. The time cost tends to be huge when the number of boxes becomes large, especially for crowded scenes. The basic idea of HNMS is to firstly map each box to a discrete code (hash cell) and then remove the boxes with lower confidences if they are in the same cell. Considering the intersection-over-union (IoU) as the metric, we propose a simple yet effective hashing algorithm, named IoUHash, which guarantees that the boxes within the same cell are close enough by a lower IoU bound. For two-stage detectors, we replace NMS in region proposal network with HNMS, and observe significant speed-up with comparable accuracy. For one-stage detectors, HNMS is used as a pre-filter to speed up the suppression with a large margin. Extensive experiments are conducted on CARPK, SKU-110K, CrowdHuman datasets to demonstrate the efficiency and effectiveness of HNMS. Code is released at \url{https://github.com/microsoft/hnms.git}.



