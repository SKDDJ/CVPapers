# Arxiv Papers in cs.CV on 2020-05-05
### 3D Tomographic Pattern Synthesis for Enhancing the Quantification of COVID-19
- **Arxiv ID**: http://arxiv.org/abs/2005.01903v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.01903v1)
- **Published**: 2020-05-05 01:31:40+00:00
- **Updated**: 2020-05-05 01:31:40+00:00
- **Authors**: Siqi Liu, Bogdan Georgescu, Zhoubing Xu, Youngjin Yoo, Guillaume Chabin, Shikha Chaganti, Sasa Grbic, Sebastian Piat, Brian Teixeira, Abishek Balachandran, Vishwanath RS, Thomas Re, Dorin Comaniciu
- **Comment**: None
- **Journal**: None
- **Summary**: The Coronavirus Disease (COVID-19) has affected 1.8 million people and resulted in more than 110,000 deaths as of April 12, 2020. Several studies have shown that tomographic patterns seen on chest Computed Tomography (CT), such as ground-glass opacities, consolidations, and crazy paving pattern, are correlated with the disease severity and progression. CT imaging can thus emerge as an important modality for the management of COVID-19 patients. AI-based solutions can be used to support CT based quantitative reporting and make reading efficient and reproducible if quantitative biomarkers, such as the Percentage of Opacity (PO), can be automatically computed. However, COVID-19 has posed unique challenges to the development of AI, specifically concerning the availability of appropriate image data and annotations at scale. In this paper, we propose to use synthetic datasets to augment an existing COVID-19 database to tackle these challenges. We train a Generative Adversarial Network (GAN) to inpaint COVID-19 related tomographic patterns on chest CTs from patients without infectious diseases. Additionally, we leverage location priors derived from manually labeled COVID-19 chest CTs patients to generate appropriate abnormality distributions. Synthetic data are used to improve both lung segmentation and segmentation of COVID-19 patterns by adding 20% of synthetic data to the real COVID-19 training data. We collected 2143 chest CTs, containing 327 COVID-19 positive cases, acquired from 12 sites across 7 countries. By testing on 100 COVID-19 positive and 100 control cases, we show that synthetic data can help improve both lung segmentation (+6.02% lesion inclusion rate) and abnormality segmentation (+2.78% dice coefficient), leading to an overall more accurate PO computation (+2.82% Pearson coefficient).



### Generating Thermal Image Data Samples using 3D Facial Modelling Techniques and Deep Learning Methodologies
- **Arxiv ID**: http://arxiv.org/abs/2005.01923v2
- **DOI**: 10.1109/QoMEX48832.2020.9123079
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.01923v2)
- **Published**: 2020-05-05 02:55:14+00:00
- **Updated**: 2020-05-07 11:02:04+00:00
- **Authors**: Muhammad Ali Farooq, Peter Corcoran
- **Comment**: Paper accpeted in QOMEX IEEE 2020 Conference copyright submitted to
  IEEE
- **Journal**: None
- **Summary**: Methods for generating synthetic data have become of increasing importance to build large datasets required for Convolution Neural Networks (CNN) based deep learning techniques for a wide range of computer vision applications. In this work, we extend existing methodologies to show how 2D thermal facial data can be mapped to provide 3D facial models. For the proposed research work we have used tufts datasets for generating 3D varying face poses by using a single frontal face pose. The system works by refining the existing image quality by performing fusion based image preprocessing operations. The refined outputs have better contrast adjustments, decreased noise level and higher exposedness of the dark regions. It makes the facial landmarks and temperature patterns on the human face more discernible and visible when compared to original raw data. Different image quality metrics are used to compare the refined version of images with original images. In the next phase of the proposed study, the refined version of images is used to create 3D facial geometry structures by using Convolution Neural Networks (CNN). The generated outputs are then imported in blender software to finally extract the 3D thermal facial outputs of both males and females. The same technique is also used on our thermal face data acquired using prototype thermal camera (developed under Heliaus EU project) in an indoor lab environment which is then used for generating synthetic 3D face data along with varying yaw face angles and lastly facial depth map is generated.



### StereoGAN: Bridging Synthetic-to-Real Domain Gap by Joint Optimization of Domain Translation and Stereo Matching
- **Arxiv ID**: http://arxiv.org/abs/2005.01927v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.01927v1)
- **Published**: 2020-05-05 03:11:38+00:00
- **Updated**: 2020-05-05 03:11:38+00:00
- **Authors**: Rui Liu, Chengxi Yang, Wenxiu Sun, Xiaogang Wang, Hongsheng Li
- **Comment**: Accepted to CVPR2020
- **Journal**: None
- **Summary**: Large-scale synthetic datasets are beneficial to stereo matching but usually introduce known domain bias. Although unsupervised image-to-image translation networks represented by CycleGAN show great potential in dealing with domain gap, it is non-trivial to generalize this method to stereo matching due to the problem of pixel distortion and stereo mismatch after translation. In this paper, we propose an end-to-end training framework with domain translation and stereo matching networks to tackle this challenge. First, joint optimization between domain translation and stereo matching networks in our end-to-end framework makes the former facilitate the latter one to the maximum extent. Second, this framework introduces two novel losses, i.e., bidirectional multi-scale feature re-projection loss and correlation consistency loss, to help translate all synthetic stereo images into realistic ones as well as maintain epipolar constraints. The effective combination of above two contributions leads to impressive stereo-consistent translation and disparity estimation accuracy. In addition, a mode seeking regularization term is added to endow the synthetic-to-real translation results with higher fine-grained diversity. Extensive experiments demonstrate the effectiveness of the proposed framework on bridging the synthetic-to-real domain gap on stereo matching.



### Modal features for image texture classification
- **Arxiv ID**: http://arxiv.org/abs/2005.01928v1
- **DOI**: 10.1016/j.patrec.2020.04.036
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.01928v1)
- **Published**: 2020-05-05 03:13:54+00:00
- **Updated**: 2020-05-05 03:13:54+00:00
- **Authors**: Thomas Lacombe, Hugues Favreliere, Maurice Pillet
- **Comment**: 12 pages, 5 figures. Accepted and to be published in Pattern
  Recognition Letters
- **Journal**: None
- **Summary**: Feature extraction is a key step in image processing for pattern recognition and machine learning processes. Its purpose lies in reducing the dimensionality of the input data through the computing of features which accurately describe the original information. In this article, a new feature extraction method based on Discrete Modal Decomposition (DMD) is introduced, to extend the group of space and frequency based features. These new features are called modal features. Initially aiming to decompose a signal into a modal basis built from a vibration mechanics problem, the DMD projection is applied to images in order to extract modal features with two approaches. The first one, called full scale DMD, consists in exploiting directly the decomposition resulting coordinates as features. The second one, called filtering DMD, consists in using the DMD modes as filters to obtain features through a local transformation process. Experiments are performed on image texture classification tasks including several widely used data bases, compared to several classic feature extraction methods. We show that the DMD approach achieves good classification performances, comparable to the state of the art techniques, with a lower extraction time.



### From Image Collections to Point Clouds with Self-supervised Shape and Pose Networks
- **Arxiv ID**: http://arxiv.org/abs/2005.01939v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.01939v1)
- **Published**: 2020-05-05 04:25:16+00:00
- **Updated**: 2020-05-05 04:25:16+00:00
- **Authors**: K L Navaneet, Ansu Mathew, Shashank Kashyap, Wei-Chih Hung, Varun Jampani, R. Venkatesh Babu
- **Comment**: Accepted to CVPR 2020; Codes are available at
  https://github.com/val-iisc/ssl_3d_recon
- **Journal**: None
- **Summary**: Reconstructing 3D models from 2D images is one of the fundamental problems in computer vision. In this work, we propose a deep learning technique for 3D object reconstruction from a single image. Contrary to recent works that either use 3D supervision or multi-view supervision, we use only single view images with no pose information during training as well. This makes our approach more practical requiring only an image collection of an object category and the corresponding silhouettes. We learn both 3D point cloud reconstruction and pose estimation networks in a self-supervised manner, making use of differentiable point cloud renderer to train with 2D supervision. A key novelty of the proposed technique is to impose 3D geometric reasoning into predicted 3D point clouds by rotating them with randomly sampled poses and then enforcing cycle consistency on both 3D reconstructions and poses. In addition, using single-view supervision allows us to do test-time optimization on a given test image. Experiments on the synthetic ShapeNet and real-world Pix3D datasets demonstrate that our approach, despite using less supervision, can achieve competitive performance compared to pose-supervised and multi-view supervised approaches.



### Small, Sparse, but Substantial: Techniques for Segmenting Small Agricultural Fields Using Sparse Ground Data
- **Arxiv ID**: http://arxiv.org/abs/2005.01947v1
- **DOI**: 10.1080/01431161.2020.1834166
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.01947v1)
- **Published**: 2020-05-05 05:26:19+00:00
- **Updated**: 2020-05-05 05:26:19+00:00
- **Authors**: Smit Marvaniya, Umamaheswari Devi, Jagabondhu Hazra, Shashank Mujumdar, Nitin Gupta
- **Comment**: 7 pages, 6 figures
- **Journal**: None
- **Summary**: The recent thrust on digital agriculture (DA) has renewed significant research interest in the automated delineation of agricultural fields. Most prior work addressing this problem have focused on detecting medium to large fields, while there is strong evidence that around 40\% of the fields world-wide and 70% of the fields in Asia and Africa are small. The lack of adequate labeled images for small fields, huge variations in their color, texture, and shape, and faint boundary lines separating them make it difficult to develop an end-to-end learning model for detecting such fields. Hence, in this paper, we present a multi-stage approach that uses a combination of machine learning and image processing techniques. In the first stage, we leverage state-of-the-art edge detection algorithms such as holistically-nested edge detection (HED) to extract first-level contours and polygons. In the second stage, we propose image-processing techniques to identify polygons that are non-fields, over-segmentations, or noise and eliminate them. The next stage tackles under-segmentations using a combination of a novel ``cut-point'' based technique and localized second-level edge detection to obtain individual parcels. Since a few small, non-cropped but vegetated or constructed pockets can be interspersed in areas that are predominantly croplands, in the final stage, we train a classifier for identifying each parcel from the previous stage as an agricultural field or not. In an evaluation using high-resolution imagery, we show that our approach has a high F-Score of 0.84 in areas with large fields and reasonable accuracy with an F-Score of 0.73 in areas with small fields, which is encouraging.



### AlignShift: Bridging the Gap of Imaging Thickness in 3D Anisotropic Volumes
- **Arxiv ID**: http://arxiv.org/abs/2005.01969v2
- **DOI**: 10.1007/978-3-030-59719-1_55
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.01969v2)
- **Published**: 2020-05-05 06:54:26+00:00
- **Updated**: 2020-07-08 10:03:05+00:00
- **Authors**: Jiancheng Yang, Yi He, Xiaoyang Huang, Jingwei Xu, Xiaodan Ye, Guangyu Tao, Bingbing Ni
- **Comment**: MICCAI 2020 (early accepted). Camera ready version. Code is available
  at https://github.com/M3DV/AlignShift
- **Journal**: None
- **Summary**: This paper addresses a fundamental challenge in 3D medical image processing: how to deal with imaging thickness. For anisotropic medical volumes, there is a significant performance gap between thin-slice (mostly 1mm) and thick-slice (mostly 5mm) volumes. Prior arts tend to use 3D approaches for the thin-slice and 2D approaches for the thick-slice, respectively. We aim at a unified approach for both thin- and thick-slice medical volumes. Inspired by recent advances in video analysis, we propose AlignShift, a novel parameter-free operator to convert theoretically any 2D pretrained network into thickness-aware 3D network. Remarkably, the converted networks behave like 3D for the thin-slice, nevertheless degenerate to 2D for the thick-slice adaptively. The unified thickness-aware representation learning is achieved by shifting and fusing aligned "virtual slices" as per the input imaging thickness. Extensive experiments on public large-scale DeepLesion benchmark, consisting of 32K lesions for universal lesion detection, validate the effectiveness of our method, which outperforms previous state of the art by considerable margins without whistles and bells. More importantly, to our knowledge, this is the first method that bridges the performance gap between thin- and thick-slice volumes by a unified framework. To improve research reproducibility, our code in PyTorch is open source at https://github.com/M3DV/AlignShift.



### NTIRE 2020 Challenge on Real-World Image Super-Resolution: Methods and Results
- **Arxiv ID**: http://arxiv.org/abs/2005.01996v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.01996v1)
- **Published**: 2020-05-05 08:17:04+00:00
- **Updated**: 2020-05-05 08:17:04+00:00
- **Authors**: Andreas Lugmayr, Martin Danelljan, Radu Timofte, Namhyuk Ahn, Dongwoon Bai, Jie Cai, Yun Cao, Junyang Chen, Kaihua Cheng, SeYoung Chun, Wei Deng, Mostafa El-Khamy, Chiu Man Ho, Xiaozhong Ji, Amin Kheradmand, Gwantae Kim, Hanseok Ko, Kanghyu Lee, Jungwon Lee, Hao Li, Ziluan Liu, Zhi-Song Liu, Shuai Liu, Yunhua Lu, Zibo Meng, Pablo Navarrete Michelini, Christian Micheloni, Kalpesh Prajapati, Haoyu Ren, Yong Hyeok Seo, Wan-Chi Siu, Kyung-Ah Sohn, Ying Tai, Rao Muhammad Umer, Shuangquan Wang, Huibing Wang, Timothy Haoning Wu, Haoning Wu, Biao Yang, Fuzhi Yang, Jaejun Yoo, Tongtong Zhao, Yuanbo Zhou, Haijie Zhuo, Ziyao Zong, Xueyi Zou
- **Comment**: None
- **Journal**: None
- **Summary**: This paper reviews the NTIRE 2020 challenge on real world super-resolution. It focuses on the participating methods and final results. The challenge addresses the real world setting, where paired true high and low-resolution images are unavailable. For training, only one set of source input images is therefore provided along with a set of unpaired high-quality target images. In Track 1: Image Processing artifacts, the aim is to super-resolve images with synthetically generated image processing artifacts. This allows for quantitative benchmarking of the approaches \wrt a ground-truth image. In Track 2: Smartphone Images, real low-quality smart phone images have to be super-resolved. In both tracks, the ultimate goal is to achieve the best perceptual quality, evaluated using a human study. This is the second challenge on the subject, following AIM 2019, targeting to advance the state-of-the-art in super-resolution. To measure the performance we use the benchmark protocol from AIM 2019. In total 22 teams competed in the final testing phase, demonstrating new and innovative solutions to the problem.



### On Interpretability of Deep Learning based Skin Lesion Classifiers using Concept Activation Vectors
- **Arxiv ID**: http://arxiv.org/abs/2005.02000v1
- **DOI**: 10.1109/IJCNN48605.2020.9206946
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.02000v1)
- **Published**: 2020-05-05 08:27:16+00:00
- **Updated**: 2020-05-05 08:27:16+00:00
- **Authors**: Adriano Lucieri, Muhammad Naseer Bajwa, Stephan Alexander Braun, Muhammad Imran Malik, Andreas Dengel, Sheraz Ahmed
- **Comment**: Accepted for the IEEE International Joint Conference on Neural
  Networks (IJCNN) 2020
- **Journal**: 2020 International Joint Conference on Neural Networks (IJCNN)
- **Summary**: Deep learning based medical image classifiers have shown remarkable prowess in various application areas like ophthalmology, dermatology, pathology, and radiology. However, the acceptance of these Computer-Aided Diagnosis (CAD) systems in real clinical setups is severely limited primarily because their decision-making process remains largely obscure. This work aims at elucidating a deep learning based medical image classifier by verifying that the model learns and utilizes similar disease-related concepts as described and employed by dermatologists. We used a well-trained and high performing neural network developed by REasoning for COmplex Data (RECOD) Lab for classification of three skin tumours, i.e. Melanocytic Naevi, Melanoma and Seborrheic Keratosis and performed a detailed analysis on its latent space. Two well established and publicly available skin disease datasets, PH2 and derm7pt, are used for experimentation. Human understandable concepts are mapped to RECOD image classification model with the help of Concept Activation Vectors (CAVs), introducing a novel training and significance testing paradigm for CAVs. Our results on an independent evaluation set clearly shows that the classifier learns and encodes human understandable concepts in its latent representation. Additionally, TCAV scores (Testing with CAVs) suggest that the neural network indeed makes use of disease-related concepts in the correct way when making predictions. We anticipate that this work can not only increase confidence of medical practitioners on CAD but also serve as a stepping stone for further development of CAV-based neural network interpretation methods.



### Multi-task pre-training of deep neural networks for digital pathology
- **Arxiv ID**: http://arxiv.org/abs/2005.02561v2
- **DOI**: 10.1109/JBHI.2020.2992878
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.02561v2)
- **Published**: 2020-05-05 08:50:17+00:00
- **Updated**: 2020-05-07 08:16:31+00:00
- **Authors**: Romain Mormont, Pierre Geurts, Raphaël Marée
- **Comment**: Accepted for publication in the IEEE Journal of Biomedical and Health
  Informatics, special issue on Computational Pathology
- **Journal**: None
- **Summary**: In this work, we investigate multi-task learning as a way of pre-training models for classification tasks in digital pathology. It is motivated by the fact that many small and medium-size datasets have been released by the community over the years whereas there is no large scale dataset similar to ImageNet in the domain. We first assemble and transform many digital pathology datasets into a pool of 22 classification tasks and almost 900k images. Then, we propose a simple architecture and training scheme for creating a transferable model and a robust evaluation and selection protocol in order to evaluate our method. Depending on the target task, we show that our models used as feature extractors either improve significantly over ImageNet pre-trained models or provide comparable performance. Fine-tuning improves performance over feature extraction and is able to recover the lack of specificity of ImageNet features, as both pre-training sources yield comparable performance.



### Automatic Plant Image Identification of Vietnamese species using Deep Learning Models
- **Arxiv ID**: http://arxiv.org/abs/2005.02832v1
- **DOI**: 10.14445/22315381/IJETT-V68I4P205S
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.02832v1)
- **Published**: 2020-05-05 09:59:10+00:00
- **Updated**: 2020-05-05 09:59:10+00:00
- **Authors**: Nguyen Van Hieu, Ngo Le Huy Hien
- **Comment**: 7 pages, 8 figures, 2 tables, Published with International Journal of
  Engineering Trends and Technology (IJETT)
- **Journal**: International Journal of Engineering Trends and Technology
  68.4(2020):25-31. Published by Seventh Sense Research Group
- **Summary**: It is complicated to distinguish among thousands of plant species in the natural ecosystem, and many efforts have been investigated to address the issue. In Vietnam, the task of identifying one from 12,000 species requires specialized experts in flora management, with thorough training skills and in-depth knowledge. Therefore, with the advance of machine learning, automatic plant identification systems have been proposed to benefit various stakeholders, including botanists, pharmaceutical laboratories, taxonomists, forestry services, and organizations. The concept has fueled an interest in research and application from global researchers and engineers in both fields of machine learning and computer vision. In this paper, the Vietnamese plant image dataset was collected from an online encyclopedia of Vietnamese organisms, together with the Encyclopedia of Life, to generate a total of 28,046 environmental images of 109 plant species in Vietnam. A comparative evaluation of four deep convolutional feature extraction models, which are MobileNetV2, VGG16, ResnetV2, and Inception Resnet V2, is presented. Those models have been tested on the Support Vector Machine (SVM) classifier to experiment with the purpose of plant image identification. The proposed models achieve promising recognition rates, and MobilenetV2 attained the highest with 83.9%. This result demonstrates that machine learning models are potential for plant species identification in the natural environment, and future works need to examine proposing higher accuracy systems on a larger dataset to meet the current application demand.



### Unsupervised Instance Segmentation in Microscopy Images via Panoptic Domain Adaptation and Task Re-weighting
- **Arxiv ID**: http://arxiv.org/abs/2005.02066v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.02066v1)
- **Published**: 2020-05-05 11:08:26+00:00
- **Updated**: 2020-05-05 11:08:26+00:00
- **Authors**: Dongnan Liu, Donghao Zhang, Yang Song, Fan Zhang, Lauren O'Donnell, Heng Huang, Mei Chen, Weidong Cai
- **Comment**: Accepted by CVPR2020
- **Journal**: None
- **Summary**: Unsupervised domain adaptation (UDA) for nuclei instance segmentation is important for digital pathology, as it alleviates the burden of labor-intensive annotation and domain shift across datasets. In this work, we propose a Cycle Consistency Panoptic Domain Adaptive Mask R-CNN (CyC-PDAM) architecture for unsupervised nuclei segmentation in histopathology images, by learning from fluorescence microscopy images. More specifically, we first propose a nuclei inpainting mechanism to remove the auxiliary generated objects in the synthesized images. Secondly, a semantic branch with a domain discriminator is designed to achieve panoptic-level domain adaptation. Thirdly, in order to avoid the influence of the source-biased features, we propose a task re-weighting mechanism to dynamically add trade-off weights for the task-specific loss functions. Experimental results on three datasets indicate that our proposed method outperforms state-of-the-art UDA methods significantly, and demonstrates a similar performance as fully supervised methods.



### Effect of the sEMG electrode (re)placement and feature set size on the hand movement recognition
- **Arxiv ID**: http://arxiv.org/abs/2005.02105v2
- **DOI**: 10.1016/j.bspc.2020.102292
- **Categories**: **eess.SP**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.02105v2)
- **Published**: 2020-05-05 12:36:26+00:00
- **Updated**: 2020-11-17 07:56:24+00:00
- **Authors**: Nadica Miljković, Milica S. Isaković
- **Comment**: 19 pages, 4 figures, 5 tables
- **Journal**: Nadica Miljkovi\'c, Milica S. Isakovi\'c, Effect of the sEMG
  electrode (re)placement and feature set size on the hand movement
  recognition, Biomedical Signal Processing and Control, Volume 64, 2021,
  102292, ISSN 1746-8094
- **Summary**: Repositioning of recording electrode array across repeated electromyography measurements may result in a displacement error in hand movement classification systems. In order to examine if the classifier re-training could reach satisfactory results when electrode array is translated along or rotated around subject's forearm for varying number of features, we recorded surface electromyography signals in 10 healthy volunteers for three types of grasp and 6 wrist movements. For feature extraction we applied principal component analysis and the feature set size varied from one to 8 principal components. We compared results of re-trained classifier with results from leave-one-out cross-validation classification procedure for three classifiers: LDA (Linear Discriminant Analysis), QDA (Quadratic Discriminant Analysis), and ANN (Artificial Neural Network). Our results showed that there was no significant difference in classification accuracy when the array electrode was repositioned indicating successful classification re-training and optimal feature set selection. The results also indicate expectedly that the number of principal components plays a key role for acceptable classification accuracy ~90 %. For the largest dataset (9 hand movements), LDA and QDA outperformed ANN, while for three grasping movements ANN showed promising results. Interestingly, we showed that interaction between electrode array position and the feature set size is not statistically significant. This study emphasizes the importance of testing the interaction of factors that influence classification accuracy and classifier selection altogether with their impact independently in order to establish guiding principles for design of hand movement recognition system. Data recorded for this study are stored on Zenodo repository (doi: 10.5281/zenodo.4039550).



### Adaptive Interaction Modeling via Graph Operations Search
- **Arxiv ID**: http://arxiv.org/abs/2005.02113v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.02113v1)
- **Published**: 2020-05-05 13:01:09+00:00
- **Updated**: 2020-05-05 13:01:09+00:00
- **Authors**: Haoxin Li, Wei-Shi Zheng, Yu Tao, Haifeng Hu, Jian-Huang Lai
- **Comment**: None
- **Journal**: None
- **Summary**: Interaction modeling is important for video action analysis. Recently, several works design specific structures to model interactions in videos. However, their structures are manually designed and non-adaptive, which require structures design efforts and more importantly could not model interactions adaptively. In this paper, we automate the process of structures design to learn adaptive structures for interaction modeling. We propose to search the network structures with differentiable architecture search mechanism, which learns to construct adaptive structures for different videos to facilitate adaptive interaction modeling. To this end, we first design the search space with several basic graph operations that explicitly capture different relations in videos. We experimentally demonstrate that our architecture search framework learns to construct adaptive interaction modeling structures, which provides more understanding about the relations between the structures and some interaction characteristics, and also releases the requirement of structures design efforts. Additionally, we show that the designed basic graph operations in the search space are able to model different interactions in videos. The experiments on two interaction datasets show that our method achieves competitive performance with state-of-the-arts.



### AGE Challenge: Angle Closure Glaucoma Evaluation in Anterior Segment Optical Coherence Tomography
- **Arxiv ID**: http://arxiv.org/abs/2005.02258v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.02258v3)
- **Published**: 2020-05-05 14:55:01+00:00
- **Updated**: 2020-07-31 17:04:35+00:00
- **Authors**: Huazhu Fu, Fei Li, Xu Sun, Xingxing Cao, Jingan Liao, Jose Ignacio Orlando, Xing Tao, Yuexiang Li, Shihao Zhang, Mingkui Tan, Chenglang Yuan, Cheng Bian, Ruitao Xie, Jiongcheng Li, Xiaomeng Li, Jing Wang, Le Geng, Panming Li, Huaying Hao, Jiang Liu, Yan Kong, Yongyong Ren, Hrvoje Bogunovic, Xiulan Zhang, Yanwu Xu
- **Comment**: Accepted to Medical Image Analysis (MedIA). AGE Challenge website at:
  https://age.grand-challenge.org
- **Journal**: None
- **Summary**: Angle closure glaucoma (ACG) is a more aggressive disease than open-angle glaucoma, where the abnormal anatomical structures of the anterior chamber angle (ACA) may cause an elevated intraocular pressure and gradually lead to glaucomatous optic neuropathy and eventually to visual impairment and blindness. Anterior Segment Optical Coherence Tomography (AS-OCT) imaging provides a fast and contactless way to discriminate angle closure from open angle. Although many medical image analysis algorithms have been developed for glaucoma diagnosis, only a few studies have focused on AS-OCT imaging. In particular, there is no public AS-OCT dataset available for evaluating the existing methods in a uniform way, which limits progress in the development of automated techniques for angle closure detection and assessment. To address this, we organized the Angle closure Glaucoma Evaluation challenge (AGE), held in conjunction with MICCAI 2019. The AGE challenge consisted of two tasks: scleral spur localization and angle closure classification. For this challenge, we released a large dataset of 4800 annotated AS-OCT images from 199 patients, and also proposed an evaluation framework to benchmark and compare different models. During the AGE challenge, over 200 teams registered online, and more than 1100 results were submitted for online evaluation. Finally, eight teams participated in the onsite challenge. In this paper, we summarize these eight onsite challenge methods and analyze their corresponding results for the two tasks. We further discuss limitations and future directions. In the AGE challenge, the top-performing approach had an average Euclidean Distance of 10 pixels (10um) in scleral spur localization, while in the task of angle closure classification, all the algorithms achieved satisfactory performances, with two best obtaining an accuracy rate of 100%.



### LandCover.ai: Dataset for Automatic Mapping of Buildings, Woodlands, Water and Roads from Aerial Imagery
- **Arxiv ID**: http://arxiv.org/abs/2005.02264v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.02264v4)
- **Published**: 2020-05-05 15:00:49+00:00
- **Updated**: 2022-04-21 19:59:27+00:00
- **Authors**: Adrian Boguszewski, Dominik Batorski, Natalia Ziemba-Jankowska, Tomasz Dziedzic, Anna Zambrzycka
- **Comment**: None
- **Journal**: None
- **Summary**: Monitoring of land cover and land use is crucial in natural resources management. Automatic visual mapping can carry enormous economic value for agriculture, forestry, or public administration. Satellite or aerial images combined with computer vision and deep learning enable precise assessment and can significantly speed up change detection. Aerial imagery usually provides images with much higher pixel resolution than satellite data allowing more detailed mapping. However, there is still a lack of aerial datasets made for the segmentation, covering rural areas with a resolution of tens centimeters per pixel, manual fine labels, and highly publicly important environmental instances like buildings, woods, water, or roads.   Here we introduce LandCover.ai (Land Cover from Aerial Imagery) dataset for semantic segmentation. We collected images of 216.27 sq. km rural areas across Poland, a country in Central Europe, 39.51 sq. km with resolution 50 cm per pixel and 176.76 sq. km with resolution 25 cm per pixel and manually fine annotated four following classes of objects: buildings, woodlands, water, and roads. Additionally, we report simple benchmark results, achieving 85.56% of mean intersection over union on the test set. It proves that the automatic mapping of land cover is possible with a relatively small, cost-efficient, RGB-only dataset. The dataset is publicly available at https://landcover.ai.linuxpolska.com/



### Towards explainable classifiers using the counterfactual approach -- global explanations for discovering bias in data
- **Arxiv ID**: http://arxiv.org/abs/2005.02269v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.02269v2)
- **Published**: 2020-05-05 15:05:33+00:00
- **Updated**: 2020-10-23 11:47:07+00:00
- **Authors**: Agnieszka Mikołajczyk, Michał Grochowski, Arkadiusz Kwasigroch
- **Comment**: Accepted for publication in Journal of Artificial Intelligence and
  Soft Computing Research; 12 pages, 4 figures, code available, 8-pages
  appendix
- **Journal**: None
- **Summary**: The paper proposes summarized attribution-based post-hoc explanations for the detection and identification of bias in data. A global explanation is proposed, and a step-by-step framework on how to detect and test bias is introduced. Since removing unwanted bias is often a complicated and tremendous task, it is automatically inserted, instead. Then, the bias is evaluated with the proposed counterfactual approach. The obtained results are validated on a sample skin lesion dataset. Using the proposed method, a number of possible bias causing artifacts are successfully identified and confirmed in dermoscopy images. In particular, it is confirmed that black frames have a strong influence on Convolutional Neural Network's prediction: 22% of them changed the prediction from benign to malignant.



### NTIRE 2020 Challenge on Video Quality Mapping: Methods and Results
- **Arxiv ID**: http://arxiv.org/abs/2005.02291v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.02291v3)
- **Published**: 2020-05-05 15:45:16+00:00
- **Updated**: 2020-06-15 22:12:40+00:00
- **Authors**: Dario Fuoli, Zhiwu Huang, Martin Danelljan, Radu Timofte, Hua Wang, Longcun Jin, Dewei Su, Jing Liu, Jaehoon Lee, Michal Kudelski, Lukasz Bala, Dmitry Hrybov, Marcin Mozejko, Muchen Li, Siyao Li, Bo Pang, Cewu Lu, Chao Li, Dongliang He, Fu Li, Shilei Wen
- **Comment**: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
  Workshops
- **Journal**: None
- **Summary**: This paper reviews the NTIRE 2020 challenge on video quality mapping (VQM), which addresses the issues of quality mapping from source video domain to target video domain. The challenge includes both a supervised track (track 1) and a weakly-supervised track (track 2) for two benchmark datasets. In particular, track 1 offers a new Internet video benchmark, requiring algorithms to learn the map from more compressed videos to less compressed videos in a supervised training manner. In track 2, algorithms are required to learn the quality mapping from one device to another when their quality varies substantially and weakly-aligned video pairs are available. For track 1, in total 7 teams competed in the final test phase, demonstrating novel and effective solutions to the problem. For track 2, some existing methods are evaluated, showing promising solutions to the weakly-supervised video quality mapping problem.



### Adversarial Training against Location-Optimized Adversarial Patches
- **Arxiv ID**: http://arxiv.org/abs/2005.02313v2
- **DOI**: 10.1007/978-3-030-68238-5_32
- **Categories**: **cs.CV**, cs.CR, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.02313v2)
- **Published**: 2020-05-05 16:17:00+00:00
- **Updated**: 2020-12-14 08:00:26+00:00
- **Authors**: Sukrut Rao, David Stutz, Bernt Schiele
- **Comment**: 20 pages, 6 tables, 4 figures, 2 algorithms, European Conference on
  Computer Vision Workshops 2020
- **Journal**: None
- **Summary**: Deep neural networks have been shown to be susceptible to adversarial examples -- small, imperceptible changes constructed to cause mis-classification in otherwise highly accurate image classifiers. As a practical alternative, recent work proposed so-called adversarial patches: clearly visible, but adversarially crafted rectangular patches in images. These patches can easily be printed and applied in the physical world. While defenses against imperceptible adversarial examples have been studied extensively, robustness against adversarial patches is poorly understood. In this work, we first devise a practical approach to obtain adversarial patches while actively optimizing their location within the image. Then, we apply adversarial training on these location-optimized adversarial patches and demonstrate significantly improved robustness on CIFAR10 and GTSRB. Additionally, in contrast to adversarial training on imperceptible adversarial examples, our adversarial patch training does not reduce accuracy.



### Multi-interactive Dual-decoder for RGB-thermal Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2005.02315v3
- **DOI**: 10.1109/TIP.2021.3087412
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.02315v3)
- **Published**: 2020-05-05 16:21:17+00:00
- **Updated**: 2021-06-04 12:56:34+00:00
- **Authors**: Zhengzheng Tu, Zhun Li, Chenglong Li, Yang Lang, Jin Tang
- **Comment**: Accepted by IEEE TIP
- **Journal**: None
- **Summary**: RGB-thermal salient object detection (SOD) aims to segment the common prominent regions of visible image and corresponding thermal infrared image that we call it RGBT SOD. Existing methods don't fully explore and exploit the potentials of complementarity of different modalities and multi-type cues of image contents, which play a vital role in achieving accurate results. In this paper, we propose a multi-interactive dual-decoder to mine and model the multi-type interactions for accurate RGBT SOD. In specific, we first encode two modalities into multi-level multi-modal feature representations. Then, we design a novel dual-decoder to conduct the interactions of multi-level features, two modalities and global contexts. With these interactions, our method works well in diversely challenging scenarios even in the presence of invalid modality. Finally, we carry out extensive experiments on public RGBT and RGBD SOD datasets, and the results show that the proposed method achieves the outstanding performance against state-of-the-art algorithms. The source code has been released at:https://github.com/lz118/Multi-interactive-Dual-decoder.



### Manifold Proximal Point Algorithms for Dual Principal Component Pursuit and Orthogonal Dictionary Learning
- **Arxiv ID**: http://arxiv.org/abs/2005.02356v2
- **DOI**: 10.1109/TSP.2021.3099643
- **Categories**: **math.OC**, cs.CV, cs.LG, eess.SP, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.02356v2)
- **Published**: 2020-05-05 17:40:03+00:00
- **Updated**: 2021-07-21 13:40:06+00:00
- **Authors**: Shixiang Chen, Zengde Deng, Shiqian Ma, Anthony Man-Cho So
- **Comment**: Accepted in IEEE Transactions on Signal Processing
- **Journal**: None
- **Summary**: We consider the problem of maximizing the $\ell_1$ norm of a linear map over the sphere, which arises in various machine learning applications such as orthogonal dictionary learning (ODL) and robust subspace recovery (RSR). The problem is numerically challenging due to its nonsmooth objective and nonconvex constraint, and its algorithmic aspects have not been well explored. In this paper, we show how the manifold structure of the sphere can be exploited to design fast algorithms for tackling this problem. Specifically, our contribution is threefold. First, we present a manifold proximal point algorithm (ManPPA) for the problem and show that it converges at a sublinear rate. Furthermore, we show that ManPPA can achieve a quadratic convergence rate when applied to the ODL and RSR problems. Second, we propose a stochastic variant of ManPPA called StManPPA, which is well suited for large-scale computation, and establish its sublinear convergence rate. Both ManPPA and StManPPA have provably faster convergence rates than existing subgradient-type methods. Third, using ManPPA as a building block, we propose a new approach to solving a matrix analog of the problem, in which the sphere is replaced by the Stiefel manifold. The results from our extensive numerical experiments on the ODL and RSR problems demonstrate the efficiency and efficacy of our proposed methods.



### Sub-Image Anomaly Detection with Deep Pyramid Correspondences
- **Arxiv ID**: http://arxiv.org/abs/2005.02357v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.02357v3)
- **Published**: 2020-05-05 17:43:35+00:00
- **Updated**: 2021-02-03 16:28:51+00:00
- **Authors**: Niv Cohen, Yedid Hoshen
- **Comment**: None
- **Journal**: None
- **Summary**: Nearest neighbor (kNN) methods utilizing deep pre-trained features exhibit very strong anomaly detection performance when applied to entire images. A limitation of kNN methods is the lack of segmentation map describing where the anomaly lies inside the image. In this work we present a novel anomaly segmentation approach based on alignment between an anomalous image and a constant number of the similar normal images. Our method, Semantic Pyramid Anomaly Detection (SPADE) uses correspondences based on a multi-resolution feature pyramid. SPADE is shown to achieve state-of-the-art performance on unsupervised anomaly detection and localization while requiring virtually no training time.



### Classification-Based Anomaly Detection for General Data
- **Arxiv ID**: http://arxiv.org/abs/2005.02359v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.02359v1)
- **Published**: 2020-05-05 17:44:40+00:00
- **Updated**: 2020-05-05 17:44:40+00:00
- **Authors**: Liron Bergman, Yedid Hoshen
- **Comment**: ICLR'20
- **Journal**: None
- **Summary**: Anomaly detection, finding patterns that substantially deviate from those seen previously, is one of the fundamental problems of artificial intelligence. Recently, classification-based methods were shown to achieve superior results on this task. In this work, we present a unifying view and propose an open-set method, GOAD, to relax current generalization assumptions. Furthermore, we extend the applicability of transformation-based methods to non-image data using random affine transformations. Our method is shown to obtain state-of-the-art accuracy and is applicable to broad data types. The strong performance of our method is extensively validated on multiple datasets from different domains.



### Data Augmentation via Mixed Class Interpolation using Cycle-Consistent Generative Adversarial Networks Applied to Cross-Domain Imagery
- **Arxiv ID**: http://arxiv.org/abs/2005.02436v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.02436v2)
- **Published**: 2020-05-05 18:53:38+00:00
- **Updated**: 2021-01-01 22:29:13+00:00
- **Authors**: Hiroshi Sasaki, Chris G. Willcocks, Toby P. Breckon
- **Comment**: 9 pages, 9 figures, accepted at the 25th International Conference on
  Pattern Recognition (ICPR 2020)
- **Journal**: None
- **Summary**: Machine learning driven object detection and classification within non-visible imagery has an important role in many fields such as night vision, all-weather surveillance and aviation security. However, such applications often suffer due to the limited quantity and variety of non-visible spectral domain imagery, in contrast to the high data availability of visible-band imagery that readily enables contemporary deep learning driven detection and classification approaches. To address this problem, this paper proposes and evaluates a novel data augmentation approach that leverages the more readily available visible-band imagery via a generative domain transfer model. The model can synthesise large volumes of non-visible domain imagery by image-to-image (I2I) translation from the visible image domain. Furthermore, we show that the generation of interpolated mixed class (non-visible domain) image examples via our novel Conditional CycleGAN Mixup Augmentation (C2GMA) methodology can lead to a significant improvement in the quality of non-visible domain classification tasks that otherwise suffer due to limited data availability. Focusing on classification within the Synthetic Aperture Radar (SAR) domain, our approach is evaluated on a variation of the Statoil/C-CORE Iceberg Classifier Challenge dataset and achieves 75.4% accuracy, demonstrating a significant improvement when compared against traditional data augmentation strategies (Rotation, Mixup, and MixCycleGAN).



### Iris segmentation techniques to recognize the behavior of a vigilant driver
- **Arxiv ID**: http://arxiv.org/abs/2005.02450v1
- **DOI**: 10.1109/AECT47998.2020.9194159
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.02450v1)
- **Published**: 2020-05-05 19:49:46+00:00
- **Updated**: 2020-05-05 19:49:46+00:00
- **Authors**: Abdullatif Baba
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we clarify how to recognize different levels of vigilance for vehicle drivers. In order to avoid the classical problems of crisp logic, we preferred to employ a fuzzy logic-based system that depends on two variables to make the final decision. Two iris segmentation techniques are well illustrated. A new technique for pupil position detection is also provided here with the possibility to correct the pupil detected position when dealing with some noisy cases.



### A new design of a flying robot, with advanced computer vision techniques to perform self-maintenance of smart grids
- **Arxiv ID**: http://arxiv.org/abs/2005.02460v1
- **DOI**: 10.1016/j.jksuci.2020.07.009
- **Categories**: **cs.RO**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.02460v1)
- **Published**: 2020-05-05 20:06:32+00:00
- **Updated**: 2020-05-05 20:06:32+00:00
- **Authors**: Abdullatif Baba
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a full design of a flying robot to investigate the state of power grid components and to perform the appropriate maintenance procedures according to each fail or defect that could be recognized. To realize this purpose; different types of sensors including thermal and aerial vision-based systems are employed in this design. The main features and technical specifications of this robot are presented and discussed here in detail. Some essential and advanced computer vision techniques are exploited in this work to take some readings and measurements from the robot's surroundings. From each given image, many sub-images containing different electrical components are extracted using a new region proposal approach that relies on Discrete Wavelet Transform, to be classified later by utilizing a Convolutional Neural Network.



### Spatio-Temporal Event Segmentation and Localization for Wildlife Extended Videos
- **Arxiv ID**: http://arxiv.org/abs/2005.02463v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.02463v4)
- **Published**: 2020-05-05 20:11:48+00:00
- **Updated**: 2021-07-18 19:35:14+00:00
- **Authors**: Ramy Mounir, Roman Gula, Jörn Theuerkauf, Sudeep Sarkar
- **Comment**: None
- **Journal**: None
- **Summary**: Using offline training schemes, researchers have tackled the event segmentation problem by providing full or weak-supervision through manually annotated labels or self-supervised epoch-based training. Most works consider videos that are at most 10's of minutes long. We present a self-supervised perceptual prediction framework capable of temporal event segmentation by building stable representations of objects over time and demonstrate it on long videos, spanning several days. The approach is deceptively simple but quite effective. We rely on predictions of high-level features computed by a standard deep learning backbone. For prediction, we use an LSTM, augmented with an attention mechanism, trained in a self-supervised manner using the prediction error. The self-learned attention maps effectively localize and track the event-related objects in each frame. The proposed approach does not require labels. It requires only a single pass through the video, with no separate training set. Given the lack of datasets of very long videos, we demonstrate our method on video from 10 days (254 hours) of continuous wildlife monitoring data that we had collected with required permissions. We find that the approach is robust to various environmental conditions such as day/night conditions, rain, sharp shadows, and windy conditions. For the task of temporally locating events, we had an 80% recall rate at 20% false-positive rate for frame-level segmentation. At the activity level, we had an 80% activity recall rate for one false activity detection every 50 minutes. We will make the dataset, which is the first of its kind, and the code available to the research community.



### Cross-media Structured Common Space for Multimedia Event Extraction
- **Arxiv ID**: http://arxiv.org/abs/2005.02472v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CL, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.02472v1)
- **Published**: 2020-05-05 20:21:53+00:00
- **Updated**: 2020-05-05 20:21:53+00:00
- **Authors**: Manling Li, Alireza Zareian, Qi Zeng, Spencer Whitehead, Di Lu, Heng Ji, Shih-Fu Chang
- **Comment**: Accepted as an oral paper at ACL 2020
- **Journal**: None
- **Summary**: We introduce a new task, MultiMedia Event Extraction (M2E2), which aims to extract events and their arguments from multimedia documents. We develop the first benchmark and collect a dataset of 245 multimedia news articles with extensively annotated events and arguments. We propose a novel method, Weakly Aligned Structured Embedding (WASE), that encodes structured representations of semantic information from textual and visual data into a common embedding space. The structures are aligned across modalities by employing a weakly supervised training strategy, which enables exploiting available resources without explicit cross-media annotation. Compared to uni-modal state-of-the-art methods, our approach achieves 4.0% and 9.8% absolute F-score gains on text event argument role labeling and visual event extraction. Compared to state-of-the-art multimedia unstructured representations, we achieve 8.3% and 5.0% absolute F-score gains on multimedia event extraction and argument role labeling, respectively. By utilizing images, we extract 21.4% more event mentions than traditional text-only methods.



### Mimicry: Towards the Reproducibility of GAN Research
- **Arxiv ID**: http://arxiv.org/abs/2005.02494v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.02494v1)
- **Published**: 2020-05-05 21:07:26+00:00
- **Updated**: 2020-05-05 21:07:26+00:00
- **Authors**: Kwot Sin Lee, Christopher Town
- **Comment**: Accepted to the AI for Content Creation Workshop at CVPR 2020
- **Journal**: None
- **Summary**: Advancing the state of Generative Adversarial Networks (GANs) research requires one to make careful and accurate comparisons with existing works. Yet, this is often difficult to achieve in practice when models are often implemented differently using varying frameworks, and evaluated using different procedures even when the same metric is used. To mitigate these issues, we introduce Mimicry, a lightweight PyTorch library that provides implementations of popular state-of-the-art GANs and evaluation metrics to closely reproduce reported scores in the literature. We provide comprehensive baseline performances of different GANs on seven widely-used datasets by training these GANs under the same conditions, and evaluating them across three popular GAN metrics using the same procedures. The library can be found at https://github.com/kwotsin/mimicry.



### Partly Supervised Multitask Learning
- **Arxiv ID**: http://arxiv.org/abs/2005.02523v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.02523v1)
- **Published**: 2020-05-05 22:42:12+00:00
- **Updated**: 2020-05-05 22:42:12+00:00
- **Authors**: Abdullah-Al-Zubaer Imran, Chao Huang, Hui Tang, Wei Fan, Yuan Xiao, Dingjun Hao, Zhen Qian, Demetri Terzopoulos
- **Comment**: 10 pages, 8 figures, 3 tables
- **Journal**: None
- **Summary**: Semi-supervised learning has recently been attracting attention as an alternative to fully supervised models that require large pools of labeled data. Moreover, optimizing a model for multiple tasks can provide better generalizability than single-task learning. Leveraging self-supervision and adversarial training, we propose a novel general purpose semi-supervised, multiple-task model---namely, self-supervised, semi-supervised, multitask learning (S$^4$MTL)---for accomplishing two important tasks in medical imaging, segmentation and diagnostic classification. Experimental results on chest and spine X-ray datasets suggest that our S$^4$MTL model significantly outperforms semi-supervised single task, semi/fully-supervised multitask, and fully-supervised single task models, even with a 50\% reduction of class and segmentation labels. We hypothesize that our proposed model can be effective in tackling limited annotation problems for joint training, not only in medical imaging domains, but also for general-purpose vision tasks.



