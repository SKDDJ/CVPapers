# Arxiv Papers in cs.CV on 2020-05-30
### StressGAN: A Generative Deep Learning Model for 2D Stress Distribution Prediction
- **Arxiv ID**: http://arxiv.org/abs/2006.11376v1
- **DOI**: 10.1115/1.4049805
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.11376v1)
- **Published**: 2020-05-30 00:28:21+00:00
- **Updated**: 2020-05-30 00:28:21+00:00
- **Authors**: Haoliang Jiang, Zhenguo Nie, Roselyn Yeo, Amir Barati Farimani, Levent Burak Kara
- **Comment**: None
- **Journal**: None
- **Summary**: Using deep learning to analyze mechanical stress distributions has been gaining interest with the demand for fast stress analysis methods. Deep learning approaches have achieved excellent outcomes when utilized to speed up stress computation and learn the physics without prior knowledge of underlying equations. However, most studies restrict the variation of geometry or boundary conditions, making these methods difficult to be generalized to unseen configurations. We propose a conditional generative adversarial network (cGAN) model for predicting 2D von Mises stress distributions in solid structures. The cGAN learns to generate stress distributions conditioned by geometries, load, and boundary conditions through a two-player minimax game between two neural networks with no prior knowledge. By evaluating the generative network on two stress distribution datasets under multiple metrics, we demonstrate that our model can predict more accurate high-resolution stress distributions than a baseline convolutional neural network model, given various and complex cases of geometry, load and boundary conditions.



### Deep Fusion Siamese Network for Automatic Kinship Verification
- **Arxiv ID**: http://arxiv.org/abs/2006.00143v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, 65D19, I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2006.00143v2)
- **Published**: 2020-05-30 01:43:59+00:00
- **Updated**: 2020-06-07 12:19:00+00:00
- **Authors**: Jun Yu, Mengyan Li, Xinlong Hao, Guochen Xie
- **Comment**: 8 pages, 8 figures
- **Journal**: 2020 15th IEEE Conference on Automatic Face and Gesture
  Recognition; 4th Recognizing Families In the Wild (RFIW)
- **Summary**: Automatic kinship verification aims to determine whether some individuals belong to the same family. It is of great research significance to help missing persons reunite with their families. In this work, the challenging problem is progressively addressed in two respects. First, we propose a deep siamese network to quantify the relative similarity between two individuals. When given two input face images, the deep siamese network extracts the features from them and fuses these features by combining and concatenating. Then, the fused features are fed into a fully-connected network to obtain the similarity score between two faces, which is used to verify the kinship. To improve the performance, a jury system is also employed for multi-model fusion. Second, two deep siamese networks are integrated into a deep triplet network for tri-subject (i.e., father, mother and child) kinship verification, which is intended to decide whether a child is related to a pair of parents or not. Specifically, the obtained similarity scores of father-child and mother-child are weighted to generate the parent-child similarity score for kinship verification. Recognizing Families In the Wild (RFIW) is a challenging kinship recognition task with multiple tracks, which is based on Families in the Wild (FIW), a large-scale and comprehensive image database for automatic kinship recognition. The Kinship Verification (track I) and Tri-Subject Verification (track II) are supported during the ongoing RFIW2020 Challenge. Our team (ustc-nelslip) ranked 1st in track II, and 3rd in track I. The code is available at https://github.com/gniknoil/FG2020-kinship.



### Challenge report: Recognizing Families In the Wild Data Challenge
- **Arxiv ID**: http://arxiv.org/abs/2006.00154v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.00154v1)
- **Published**: 2020-05-30 03:01:56+00:00
- **Updated**: 2020-05-30 03:01:56+00:00
- **Authors**: Zhipeng Luo, Zhiguang Zhang, Zhenyu Xu, Lixuan Che
- **Comment**: RFIW,IEEE FG2020
- **Journal**: None
- **Summary**: This paper is a brief report to our submission to the Recognizing Families In the Wild Data Challenge (4th Edition), in conjunction with FG 2020 Forum. Automatic kinship recognition has attracted many researchers' attention for its full application, but it is still a very challenging task because of the limited information that can be used to determine whether a pair of faces are blood relatives or not. In this paper, we studied previous methods and proposed our method. We try many methods, like deep metric learning-based, to extract deep embedding feature for every image, then determine if they are blood relatives by Euclidean distance or method based on classes. Finally, we find some tricks like sampling more negative samples and high resolution that can help get better performance. Moreover, we proposed a symmetric network with a binary classification based method to get our best score in all tasks.



### Joint Person Objectness and Repulsion for Person Search
- **Arxiv ID**: http://arxiv.org/abs/2006.00155v1
- **DOI**: 10.1109/TIP.2020.3038347
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.00155v1)
- **Published**: 2020-05-30 03:04:33+00:00
- **Updated**: 2020-05-30 03:04:33+00:00
- **Authors**: Hantao Yao, Changsheng Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Person search targets to search the probe person from the unconstrainted scene images, which can be treated as the combination of person detection and person matching. However, the existing methods based on the Detection-Matching framework ignore the person objectness and repulsion (OR) which are both beneficial to reduce the effect of distractor images. In this paper, we propose an OR similarity by jointly considering the objectness and repulsion information. Besides the traditional visual similarity term, the OR similarity also contains an objectness term and a repulsion term. The objectness term can reduce the similarity of distractor images that not contain a person and boost the performance of person search by improving the ranking of positive samples. Because the probe person has a different person ID with its \emph{neighbors}, the gallery images having a higher similarity with the \emph{neighbors of probe} should have a lower similarity with the probe person. Based on this repulsion constraint, the repulsion term is proposed to reduce the similarity of distractor images that are not most similar to the probe person. Treating the Faster R-CNN as the person detector, the OR similarity is evaluated on PRW and CUHK-SYSU datasets by the Detection-Matching framework with six description models. The extensive experiments demonstrate that the proposed OR similarity can effectively reduce the similarity of distractor samples and further boost the performance of person search, e.g., improve the mAP from 92.32% to 93.23% for CUHK-SYSY dataset, and from 50.91% to 52.30% for PRW datasets.



### MetaInv-Net: Meta Inversion Network for Sparse View CT Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2006.00171v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, math.OC, physics.med-ph, 65F10, 68T05, 92B20, 94A08
- **Links**: [PDF](http://arxiv.org/pdf/2006.00171v3)
- **Published**: 2020-05-30 04:19:09+00:00
- **Updated**: 2020-09-18 01:17:18+00:00
- **Authors**: Haimiao Zhang, Baodong Liu, Hengyong Yu, Bin Dong
- **Comment**: 14 pages
- **Journal**: None
- **Summary**: X-ray Computed Tomography (CT) is widely used in clinical applications such as diagnosis and image-guided interventions. In this paper, we propose a new deep learning based model for CT image reconstruction with the backbone network architecture built by unrolling an iterative algorithm. However, unlike the existing strategy to include as many data-adaptive components in the unrolled dynamics model as possible, we find that it is enough to only learn the parts where traditional designs mostly rely on intuitions and experience. More specifically, we propose to learn an initializer for the conjugate gradient (CG) algorithm that involved in one of the subproblems of the backbone model. Other components, such as image priors and hyperparameters, are kept as the original design. Since a hypernetwork is introduced to inference on the initialization of the CG module, it makes the proposed model a certain meta-learning model. Therefore, we shall call the proposed model the meta-inversion network (MetaInv-Net). The proposed MetaInv-Net can be designed with much less trainable parameters while still preserves its superior image reconstruction performance than some state-of-the-art deep models in CT imaging. In simulated and real data experiments, MetaInv-Net performs very well and can be generalized beyond the training setting, i.e., to other scanning settings, noise levels, and data sets.



### Retrieval of Family Members Using Siamese Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2006.00174v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.00174v1)
- **Published**: 2020-05-30 04:32:16+00:00
- **Updated**: 2020-05-30 04:32:16+00:00
- **Authors**: Jun Yu, Guochen Xie, Mengyan Li, Xinlong Hao
- **Comment**: 5 pages, 3 figures
- **Journal**: None
- **Summary**: Retrieval of family members in the wild aims at finding family members of the given subject in the dataset, which is useful in finding the lost children and analyzing the kinship. However, due to the diversity in age, gender, pose and illumination of the collected data, this task is always challenging. To solve this problem, we propose our solution with deep Siamese neural network. Our solution can be divided into two parts: similarity computation and ranking. In training procedure, the Siamese network firstly takes two candidate images as input and produces two feature vectors. And then, the similarity between the two vectors is computed with several fully connected layers. While in inference procedure, we try another similarity computing method by dropping the followed several fully connected layers and directly computing the cosine similarity of the two feature vectors. After similarity computation, we use the ranking algorithm to merge the similarity scores with the same identity and output the ordered list according to their similarities. To gain further improvement, we try different combinations of backbones, training methods and similarity computing methods. Finally, we submit the best combination as our solution and our team(ustc-nelslip) obtains favorable result in the track3 of the RFIW2020 challenge with the first runner-up, which verifies the effectiveness of our method. Our code is available at: https://github.com/gniknoil/FG2020-kinship



### When2com: Multi-Agent Perception via Communication Graph Grouping
- **Arxiv ID**: http://arxiv.org/abs/2006.00176v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MA, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2006.00176v2)
- **Published**: 2020-05-30 04:41:32+00:00
- **Updated**: 2020-06-02 19:32:30+00:00
- **Authors**: Yen-Cheng Liu, Junjiao Tian, Nathaniel Glaser, Zsolt Kira
- **Comment**: Accepted to CVPR 2020; for the project page, see
  https://ycliu93.github.io/projects/multi-agent-perception.html
- **Journal**: None
- **Summary**: While significant advances have been made for single-agent perception, many applications require multiple sensing agents and cross-agent communication due to benefits such as coverage and robustness. It is therefore critical to develop frameworks which support multi-agent collaborative perception in a distributed and bandwidth-efficient manner. In this paper, we address the collaborative perception problem, where one agent is required to perform a perception task and can communicate and share information with other agents on the same task. Specifically, we propose a communication framework by learning both to construct communication groups and decide when to communicate. We demonstrate the generalizability of our framework on two different perception tasks and show that it significantly reduces communication bandwidth while maintaining superior performance.



### Advanced Single Image Resolution Upsurging Using a Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/2006.00186v1
- **DOI**: 10.5121/sipij.2020.11105
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.00186v1)
- **Published**: 2020-05-30 05:40:44+00:00
- **Updated**: 2020-05-30 05:40:44+00:00
- **Authors**: Md. Moshiur Rahman, Samrat Kumar Dey, Kabid Hassan Shibly
- **Comment**: 10 pages, 4 figures, 1 Table
- **Journal**: Signal & Image Processing: An International Journal (SIPIJ)
  Vol.11, No.1, February 2020
- **Summary**: The resolution of an image is a very important criterion for evaluating the quality of the image. A higher resolution of an image is always preferable as images of lower resolution are unsuitable due to fuzzy quality. A higher resolution of an image is important for various fields such as medical imaging; astronomy works and so on as images of lower resolution becomes unclear and indistinct when their sizes are enlarged. In recent times, various research works are performed to generate a higher resolution of an image from its lower resolution. In this paper, we have proposed a technique of generating higher resolution images form lower resolution using Residual in Residual Dense Block network architecture with a deep network. We have also compared our method with other methods to prove that our method provides better visual quality images.



### An Efficient Planar Bundle Adjustment Algorithm
- **Arxiv ID**: http://arxiv.org/abs/2006.00187v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG
- **Links**: [PDF](http://arxiv.org/pdf/2006.00187v2)
- **Published**: 2020-05-30 05:54:22+00:00
- **Updated**: 2020-08-16 07:52:42+00:00
- **Authors**: Lipu Zhou, Daniel Koppel, Hui Ju, Frank Steinbruecker, Michael Kaess
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents an efficient algorithm for the least-squares problem using the point-to-plane cost, which aims to jointly optimize depth sensor poses and plane parameters for 3D reconstruction. We call this least-squares problem \textbf{Planar Bundle Adjustment} (PBA), due to the similarity between this problem and the original Bundle Adjustment (BA) in visual reconstruction. As planes ubiquitously exist in the man-made environment, they are generally used as landmarks in SLAM algorithms for various depth sensors. PBA is important to reduce drift and improve the quality of the map. However, directly adopting the well-established BA framework in visual reconstruction will result in a very inefficient solution for PBA. This is because a 3D point only has one observation at a camera pose. In contrast, a depth sensor can record hundreds of points in a plane at a time, which results in a very large nonlinear least-squares problem even for a small-scale space. Fortunately, we find that there exist a special structure of the PBA problem. We introduce a reduced Jacobian matrix and a reduced residual vector, and prove that they can replace the original Jacobian matrix and residual vector in the generally adopted Levenberg-Marquardt (LM) algorithm. This significantly reduces the computational cost. Besides, when planes are combined with other features for 3D reconstruction, the reduced Jacobian matrix and residual vector can also replace the corresponding parts derived from planes. Our experimental results verify that our algorithm can significantly reduce the computational time compared to the solution using the traditional BA framework. Besides, our algorithm is faster, more accuracy, and more robust to initialization errors compared to the start-of-the-art solution using the plane-to-plane cost



### OPAL-Net: A Generative Model for Part-based Object Layout Generation
- **Arxiv ID**: http://arxiv.org/abs/2006.00190v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2006.00190v1)
- **Published**: 2020-05-30 06:25:19+00:00
- **Updated**: 2020-05-30 06:25:19+00:00
- **Authors**: Rishabh Baghel, Ravi Kiran Sarvadevabhatla
- **Comment**: Code repository at https://github.com/atmacvit/opalnet
- **Journal**: None
- **Summary**: We propose OPAL-Net, a novel hierarchical architecture for part-based layout generation of objects from multiple categories using a single unified model. We adopt a coarse-to-fine strategy involving semantically conditioned autoregressive generation of bounding box layouts and pixel-level part layouts for objects. We use Graph Convolutional Networks, Deep Recurrent Networks along with custom-designed Conditional Variational Autoencoders to enable flexible, diverse and category-aware generation of object layouts. We train OPAL-Net on PASCAL-Parts dataset. The generated samples and corresponding evaluation scores demonstrate the versatility of OPAL-Net compared to ablative variants and baselines.



### Blended Multi-Modal Deep ConvNet Features for Diabetic Retinopathy Severity Prediction
- **Arxiv ID**: http://arxiv.org/abs/2006.00197v1
- **DOI**: 10.3390/electronics9060914
- **Categories**: **eess.IV**, cs.CV, cs.LG, 68T10, 62H30, I.4.7; I.5; I.2.10; G.3
- **Links**: [PDF](http://arxiv.org/pdf/2006.00197v1)
- **Published**: 2020-05-30 06:46:26+00:00
- **Updated**: 2020-05-30 06:46:26+00:00
- **Authors**: J. D. Bodapati, N. Veeranjaneyulu, S. N. Shareef, S. Hakak, M. Bilal, P. K. R. Maddikunta, O. Jo
- **Comment**: 18 pages, 8 figures, published in Electronics MDPI journal
- **Journal**: Electronics 2020, 9, 914
- **Summary**: Diabetic Retinopathy (DR) is one of the major causes of visual impairment and blindness across the world. It is usually found in patients who suffer from diabetes for a long period. The major focus of this work is to derive optimal representation of retinal images that further helps to improve the performance of DR recognition models. To extract optimal representation, features extracted from multiple pre-trained ConvNet models are blended using proposed multi-modal fusion module. These final representations are used to train a Deep Neural Network (DNN) used for DR identification and severity level prediction. As each ConvNet extracts different features, fusing them using 1D pooling and cross pooling leads to better representation than using features extracted from a single ConvNet. Experimental studies on benchmark Kaggle APTOS 2019 contest dataset reveals that the model trained on proposed blended feature representations is superior to the existing methods. In addition, we notice that cross average pooling based fusion of features from Xception and VGG16 is the most appropriate for DR recognition. With the proposed model, we achieve an accuracy of 97.41%, and a kappa statistic of 94.82 for DR identification and an accuracy of 81.7% and a kappa statistic of 71.1% for severity level prediction. Another interesting observation is that DNN with dropout at input layer converges more quickly when trained using blended features, compared to the same model trained using uni-modal deep features.



### Attention-Guided Discriminative Region Localization and Label Distribution Learning for Bone Age Assessment
- **Arxiv ID**: http://arxiv.org/abs/2006.00202v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.00202v2)
- **Published**: 2020-05-30 07:04:49+00:00
- **Updated**: 2020-08-13 04:58:22+00:00
- **Authors**: Chao Chen, Zhihong Chen, Xinyu Jin, Lanjuan Li, William Speier, Corey W. Arnold
- **Comment**: codes are available at
  https://github.com/chenchao666/Bone-Age-Assessment
- **Journal**: None
- **Summary**: Bone age assessment (BAA) is clinically important as it can be used to diagnose endocrine and metabolic disorders during child development. Existing deep learning based methods for classifying bone age use the global image as input, or exploit local information by annotating extra bounding boxes or key points. However, training with the global image underutilizes discriminative local information, while providing extra annotations is expensive and subjective. In this paper, we propose an attention-guided approach to automatically localize the discriminative regions for BAA without any extra annotations. Specifically, we first train a classification model to learn the attention maps of the discriminative regions, finding the hand region, the most discriminative region (the carpal bones), and the next most discriminative region (the metacarpal bones). Guided by those attention maps, we then crop the informative local regions from the original image and aggregate different regions for BAA. Instead of taking BAA as a general regression task, which is suboptimal due to the label ambiguity problem in the age label space, we propose using joint age distribution learning and expectation regression, which makes use of the ordinal relationship among hand images with different individual ages and leads to more robust age estimation. Extensive experiments are conducted on the RSNA pediatric bone age data set. Using no training annotations, our method achieves competitive results compared with existing state-of-the-art semi-automatic deep learning-based methods that require manual annotation. Code is available at https: //github.com/chenchao666/Bone-Age-Assessment.



### Complex Sequential Understanding through the Awareness of Spatial and Temporal Concepts
- **Arxiv ID**: http://arxiv.org/abs/2006.00212v1
- **DOI**: 10.1038/s42256-020-0168-3
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.00212v1)
- **Published**: 2020-05-30 07:51:50+00:00
- **Updated**: 2020-05-30 07:51:50+00:00
- **Authors**: Bo Pang, Kaiwen Zha, Hanwen Cao, Jiajun Tang, Minghui Yu, Cewu Lu
- **Comment**: 15 pages, 5 figures, 8 tables
- **Journal**: Nat Mach Intell 2, 24-253 (2020)
- **Summary**: Understanding sequential information is a fundamental task for artificial intelligence. Current neural networks attempt to learn spatial and temporal information as a whole, limited their abilities to represent large scale spatial representations over long-range sequences. Here, we introduce a new modeling strategy called Semi-Coupled Structure (SCS), which consists of deep neural networks that decouple the complex spatial and temporal concepts learning. Semi-Coupled Structure can learn to implicitly separate input information into independent parts and process these parts respectively. Experiments demonstrate that a Semi-Coupled Structure can successfully annotate the outline of an object in images sequentially and perform video action recognition. For sequence-to-sequence problems, a Semi-Coupled Structure can predict future meteorological radar echo images based on observed images. Taken together, our results demonstrate that a Semi-Coupled Structure has the capacity to improve the performance of LSTM-like models on large scale sequential tasks.



### Self-adaptive Re-weighted Adversarial Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2006.00223v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.00223v2)
- **Published**: 2020-05-30 08:35:18+00:00
- **Updated**: 2020-06-02 03:08:41+00:00
- **Authors**: Shanshan Wang, Lei Zhang
- **Comment**: to appear in IJCAI2020
- **Journal**: None
- **Summary**: Existing adversarial domain adaptation methods mainly consider the marginal distribution and these methods may lead to either under transfer or negative transfer. To address this problem, we present a self-adaptive re-weighted adversarial domain adaptation approach, which tries to enhance domain alignment from the perspective of conditional distribution. In order to promote positive transfer and combat negative transfer, we reduce the weight of the adversarial loss for aligned features while increasing the adversarial force for those poorly aligned measured by the conditional entropy. Additionally, triplet loss leveraging source samples and pseudo-labeled target samples is employed on the confusing domain. Such metric loss ensures the distance of the intra-class sample pairs closer than the inter-class pairs to achieve the class-level alignment. In this way, the high accurate pseudolabeled target samples and semantic alignment can be captured simultaneously in the co-training process. Our method achieved low joint error of the ideal source and target hypothesis. The expected target error can then be upper bounded following Ben-David's theorem. Empirical evidence demonstrates that the proposed model outperforms state of the arts on standard domain adaptation datasets.



### Web page classification with Google Image Search results
- **Arxiv ID**: http://arxiv.org/abs/2006.00226v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.00226v2)
- **Published**: 2020-05-30 09:16:20+00:00
- **Updated**: 2020-12-27 06:23:37+00:00
- **Authors**: Fahri Aydos, A. Murat Özbayoğlu, Yahya Şirin, M. Fatih Demirci
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we introduce a novel method that combines multiple neural network results to decide the class of the input. This is the first study which used the method for web pages classification. In our model, each element is represented by multiple descriptive images. After the training process of the neural network model, each element is classified by calculating its descriptive image results. We apply our idea to the web page classification problem using Google Image Search results as descriptive images. We obtained a classification rate of 94.90% on the WebScreenshots dataset that contains 20000 web sites in 4 classes. The method is easily applicable to similar problems.



### Integrating global spatial features in CNN based Hyperspectral/SAR imagery classification
- **Arxiv ID**: http://arxiv.org/abs/2006.00234v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.00234v2)
- **Published**: 2020-05-30 10:00:10+00:00
- **Updated**: 2020-06-15 09:00:59+00:00
- **Authors**: Fan Zhang, MinChao Yan, Chen Hu, Jun Ni, Fei Ma
- **Comment**: None
- **Journal**: None
- **Summary**: The land cover classification has played an important role in remote sensing because it can intelligently identify things in one huge remote sensing image to reduce the work of humans. However, a lot of classification methods are designed based on the pixel feature or limited spatial feature of the remote sensing image, which limits the classification accuracy and universality of their methods. This paper proposed a novel method to take into the information of remote sensing image, i.e., geographic latitude-longitude information. In addition, a dual-branch convolutional neural network (CNN) classification method is designed in combination with the global information to mine the pixel features of the image. Then, the features of the two neural networks are fused with another fully neural network to realize the classification of remote sensing images. Finally, two remote sensing images are used to verify the effectiveness of our method, including hyperspectral imaging (HSI) and polarimetric synthetic aperture radar (PolSAR) imagery. The result of the proposed method is superior to the traditional single-channel convolutional neural network.



### Hyperspectral Image Denoising via Global Spatial-Spectral Total Variation Regularized Nonconvex Local Low-Rank Tensor Approximation
- **Arxiv ID**: http://arxiv.org/abs/2006.00235v1
- **DOI**: 10.1109/TGRS.2020.3007945
- **Categories**: **eess.IV**, cs.CV, 94A12
- **Links**: [PDF](http://arxiv.org/pdf/2006.00235v1)
- **Published**: 2020-05-30 10:03:39+00:00
- **Updated**: 2020-05-30 10:03:39+00:00
- **Authors**: Haijin Zeng, Xiaozhen Xie, Jifeng Ning
- **Comment**: None
- **Journal**: Signal Processing Volume 178, January 2021, 107805
- **Summary**: Hyperspectral image (HSI) denoising aims to restore clean HSI from the noise-contaminated one. Noise contamination can often be caused during data acquisition and conversion. In this paper, we propose a novel spatial-spectral total variation (SSTV) regularized nonconvex local low-rank (LR) tensor approximation method to remove mixed noise in HSIs. From one aspect, the clean HSI data have its underlying local LR tensor property, even though the real HSI data may not be globally low-rank due to out-liers and non-Gaussian noise. According to this fact, we propose a novel tensor $L_{\gamma}$-norm to formulate the local LR prior. From another aspect, HSIs are assumed to be piecewisely smooth in the global spatial and spectral domains. Instead of traditional bandwise total variation, we use the SSTV regularization to simultaneously consider global spatial structure and spectral correlation of neighboring bands. Results on simulated and real HSI datasets indicate that the use of local LR tensor penalty and global SSTV can boost the preserving of local details and overall structural information in HSIs.



### Reconstructing undersampled photoacoustic microscopy images using deep learning
- **Arxiv ID**: http://arxiv.org/abs/2006.00251v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.00251v1)
- **Published**: 2020-05-30 12:39:52+00:00
- **Updated**: 2020-05-30 12:39:52+00:00
- **Authors**: Anthony DiSpirito III, Daiwei Li, Tri Vu, Maomao Chen, Dong Zhang, Jianwen Luo, Roarke Horstmeyer, Junjie Yao
- **Comment**: 12 pages, 7 main figures, 3 supplemental figures (see last 2 pages)
- **Journal**: None
- **Summary**: One primary technical challenge in photoacoustic microscopy (PAM) is the necessary compromise between spatial resolution and imaging speed. In this study, we propose a novel application of deep learning principles to reconstruct undersampled PAM images and transcend the trade-off between spatial resolution and imaging speed. We compared various convolutional neural network (CNN) architectures, and selected a fully dense U-net (FD U-net) model that produced the best results. To mimic various undersampling conditions in practice, we artificially downsampled fully-sampled PAM images of mouse brain vasculature at different ratios. This allowed us to not only definitively establish the ground truth, but also train and test our deep learning model at various imaging conditions. Our results and numerical analysis have collectively demonstrated the robust performance of our model to reconstruct PAM images with as few as 2% of the original pixels, which may effectively shorten the imaging time without substantially sacrificing the image quality.



### Is Depth Really Necessary for Salient Object Detection?
- **Arxiv ID**: http://arxiv.org/abs/2006.00269v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.00269v2)
- **Published**: 2020-05-30 13:40:03+00:00
- **Updated**: 2020-06-02 01:07:49+00:00
- **Authors**: Jiawei Zhao, Yifan Zhao, Jia Li, Xiaowu Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Salient object detection (SOD) is a crucial and preliminary task for many computer vision applications, which have made progress with deep CNNs. Most of the existing methods mainly rely on the RGB information to distinguish the salient objects, which faces difficulties in some complex scenarios. To solve this, many recent RGBD-based networks are proposed by adopting the depth map as an independent input and fuse the features with RGB information. Taking the advantages of RGB and RGBD methods, we propose a novel depth-aware salient object detection framework, which has following superior designs: 1) It only takes the depth information as training data while only relies on RGB information in the testing phase. 2) It comprehensively optimizes SOD features with multi-level depth-aware regularizations. 3) The depth information also serves as error-weighted map to correct the segmentation process. With these insightful designs combined, we make the first attempt in realizing an unified depth-aware framework with only RGB information as input for inference, which not only surpasses the state-of-the-art performances on five public RGB SOD benchmarks, but also surpasses the RGBD-based methods on five benchmarks by a large margin, while adopting less information and implementation light-weighted. The code and model will be publicly available.



### Positron Emission Tomography (PET) image enhancement using a gradient vector orientation based nonlinear diffusion filter (GVOF) for accurate quantitation of radioactivity concentration
- **Arxiv ID**: http://arxiv.org/abs/2006.00273v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.00273v1)
- **Published**: 2020-05-30 13:57:02+00:00
- **Updated**: 2020-05-30 13:57:02+00:00
- **Authors**: Mahbubunnabi Tamal
- **Comment**: None
- **Journal**: None
- **Summary**: To accurately quantify in vivo radiotracer uptake using Positron Emission Tomography (PET) is a challenging task due to low signal-to-noise ratio (SNR) and poor spatial resolution of PET camera along with the finite image sampling constraint. Furthermore, inter lesion variations of the SNR and contrast along with the variations in size of the lesion make the quantitation even more difficult. One of the ways to improve the quantitation is via post reconstruction filtering with Gaussian Filter (GF). Edge preserving Bilateral Filter (BF) and Nonlinear Diffusion Filter (NDF) are the alternatives to GF that can improve the SNR without degrading the image resolution. However, the performance of these edge preserving methods are only optimum for high count and low noise cases. A novel parameter free gradient vector orientation based nonlinear diffusion filter (GVOF) is proposed in this paper that is insensitive to statistical fluctuations (e. g., SNR, contrast, size etc.). GVOF method applied on the PET images collected with the NEMA phantom with varying levels of contrast and noise reveals that the GVOF method provides the highest SNR, CNR (contrast-to-noise ratio) and resolution compared to the original and other filtered images. The percentage bias in estimating the maximum activity representing SUVmax (Maximum Standardized Uptake Value) for the spheres with diameter > 2cm where the partial volume effects (PVE) is negligible is the lowest for the GVOF method. The GVOF method also improves the maximum intensity reproducibility. Robustness of the GVOF against variation in sizes, contrast levels and SNR makes it a suitable post filtering method for both accurate diagnosis and response assessment. Furthermore, its capability to provide accurate quantitative measurements irrespective of the SNR, it can also be effective in reduction of radioactivity dose.



### Super-BPD: Super Boundary-to-Pixel Direction for Fast Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2006.00303v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.00303v1)
- **Published**: 2020-05-30 16:00:54+00:00
- **Updated**: 2020-05-30 16:00:54+00:00
- **Authors**: Jianqiang Wan, Yang Liu, Donglai Wei, Xiang Bai, Yongchao Xu
- **Comment**: Accepted to CVPR 2020. 10 pages, 9 figures. Code available at https:
  //github.com/JianqiangWan/Super-BPD
- **Journal**: None
- **Summary**: Image segmentation is a fundamental vision task and a crucial step for many applications. In this paper, we propose a fast image segmentation method based on a novel super boundary-to-pixel direction (super-BPD) and a customized segmentation algorithm with super-BPD. Precisely, we define BPD on each pixel as a two-dimensional unit vector pointing from its nearest boundary to the pixel. In the BPD, nearby pixels from different regions have opposite directions departing from each other, and adjacent pixels in the same region have directions pointing to the other or each other (i.e., around medial points). We make use of such property to partition an image into super-BPDs, which are novel informative superpixels with robust direction similarity for fast grouping into segmentation regions. Extensive experimental results on BSDS500 and Pascal Context demonstrate the accuracy and efficency of the proposed super-BPD in segmenting images. In practice, the proposed super-BPD achieves comparable or superior performance with MCG while running at ~25fps vs. 0.07fps. Super-BPD also exhibits a noteworthy transferability to unseen scenes. The code is publicly available at https://github.com/JianqiangWan/Super-BPD.



### SDCT-AuxNet$^θ$: DCT Augmented Stain Deconvolutional CNN with Auxiliary Classifier for Cancer Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/2006.00304v2
- **DOI**: 10.1016/j.media.2020.101661
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.00304v2)
- **Published**: 2020-05-30 16:01:31+00:00
- **Updated**: 2020-06-08 01:47:54+00:00
- **Authors**: Shiv Gehlot, Anubha Gupta, Ritu Gupta
- **Comment**: The final version of this preprint has been published in Medical
  Image Analysis
- **Journal**: Medical Image Analysis, 61, 101661, 2020
- **Summary**: Acute lymphoblastic leukemia (ALL) is a pervasive pediatric white blood cell cancer across the globe. With the popularity of convolutional neural networks (CNNs), computer-aided diagnosis of cancer has attracted considerable attention. Such tools are easily deployable and are cost-effective. Hence, these can enable extensive coverage of cancer diagnostic facilities. However, the development of such a tool for ALL cancer was challenging so far due to the non-availability of a large training dataset. The visual similarity between the malignant and normal cells adds to the complexity of the problem. This paper discusses the recent release of a large dataset and presents a novel deep learning architecture for the classification of cell images of ALL cancer. The proposed architecture, namely, SDCT-AuxNet$^{\theta}$ is a 2-module framework that utilizes a compact CNN as the main classifier in one module and a Kernel SVM as the auxiliary classifier in the other one. While CNN classifier uses features through bilinear-pooling, spectral-averaged features are used by the auxiliary classifier. Further, this CNN is trained on the stain deconvolved quantity images in the optical density domain instead of the conventional RGB images. A novel test strategy is proposed that exploits both the classifiers for decision making using the confidence scores of their predicted class labels. Elaborate experiments have been carried out on our recently released public dataset of 15114 images of ALL cancer and healthy cells to establish the validity of the proposed methodology that is also robust to subject-level variability. A weighted F1 score of 94.8$\%$ is obtained that is best so far on this challenging dataset.



### Probabilistic self-learning framework for Low-dose CT Denoising
- **Arxiv ID**: http://arxiv.org/abs/2006.00327v2
- **DOI**: 10.1002/mp.14796
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.00327v2)
- **Published**: 2020-05-30 17:47:10+00:00
- **Updated**: 2021-01-22 04:41:30+00:00
- **Authors**: Ti Bai, Dan Nguyen, Biling Wang, Steve Jiang
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the indispensable role of X-ray computed tomography (CT) in diagnostic medicine field, the associated ionizing radiation is still a major concern considering that it may cause genetic and cancerous diseases. Decreasing the exposure can reduce the dose and hence the radiation-related risk, but will also induce higher quantum noise. Supervised deep learning can be used to train a neural network to denoise the low-dose CT (LDCT). However, its success requires massive pixel-wise paired LDCT and normal-dose CT (NDCT) images, which are rarely available in real practice. To alleviate this problem, in this paper, a shift-invariant property based neural network was devised to learn the inherent pixel correlations and also the noise distribution by only using the LDCT images, shaping into our probabilistic self-learning framework. Experimental results demonstrated that the proposed method outperformed the competitors, producing an enhanced LDCT image that has similar image style as the routine NDCT which is highly-preferable in clinic practice.



### Semi-Supervised Fine-Tuning for Deep Learning Models in Remote Sensing Applications
- **Arxiv ID**: http://arxiv.org/abs/2006.00345v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.00345v1)
- **Published**: 2020-05-30 19:54:32+00:00
- **Updated**: 2020-05-30 19:54:32+00:00
- **Authors**: Eftychios Protopapadakis, Anastasios Doulamis, Nikolaos Doulamis, Evangelos Maltezos
- **Comment**: None
- **Journal**: None
- **Summary**: A combinatory approach of two well-known fields: deep learning and semi supervised learning is presented, to tackle the land cover identification problem. The proposed methodology demonstrates the impact on the performance of deep learning models, when SSL approaches are used as performance functions during training. Obtained results, at pixel level segmentation tasks over orthoimages, suggest that SSL enhanced loss functions can be beneficial in models' performance.



### Critical Assessment of Transfer Learning for Medical Image Segmentation with Fully Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2006.00356v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.00356v2)
- **Published**: 2020-05-30 20:36:05+00:00
- **Updated**: 2022-04-03 16:45:27+00:00
- **Authors**: Davood Karimi, Simon K. Warfield, Ali Gholipour
- **Comment**: None
- **Journal**: None
- **Summary**: Transfer learning is widely used for training machine learning models. Here, we study the role of transfer learning for training fully convolutional networks (FCNs) for medical image segmentation. Our experiments show that although transfer learning reduces the training time on the target task, the improvement in segmentation accuracy is highly task/data-dependent. Larger improvements in accuracy are observed when the segmentation task is more challenging and the target training data is smaller. We observe that convolutional filters of an FCN change little during training for medical image segmentation, and still look random at convergence. We further show that quite accurate FCNs can be built by freezing the encoder section of the network at random values and only training the decoder section. At least for medical image segmentation, this finding challenges the common belief that the encoder section needs to learn data/task-specific representations. We examine the evolution of FCN representations to gain a better insight into the effects of transfer learning on the training dynamics. Our analysis shows that although FCNs trained via transfer learning learn different representations than FCNs trained with random initialization, the variability among FCNs trained via transfer learning can be as high as that among FCNs trained with random initialization. Moreover, feature reuse is not restricted to the early encoder layers; rather, it can be more significant in deeper layers. These findings offer new insights and suggest alternative ways of training FCNs for medical image segmentation.



### Entropy Decision Fusion for Smartphone Sensor based Human Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/2006.00367v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.00367v1)
- **Published**: 2020-05-30 21:09:38+00:00
- **Updated**: 2020-05-30 21:09:38+00:00
- **Authors**: Olasimbo Ayodeji Arigbabu
- **Comment**: None
- **Journal**: None
- **Summary**: Human activity recognition serves an important part in building continuous behavioral monitoring systems, which are deployable for visual surveillance, patient rehabilitation, gaming, and even personally inclined smart homes. This paper demonstrates our efforts to develop a collaborative decision fusion mechanism for integrating the predicted scores from multiple learning algorithms trained on smartphone sensor based human activity data. We present an approach for fusing convolutional neural network, recurrent convolutional network, and support vector machine by computing and fusing the relative weighted scores from each classifier based on Tsallis entropy to improve human activity recognition performance. To assess the suitability of this approach, experiments are conducted on two benchmark datasets, UCI-HAR and WISDM. The recognition results attained using the proposed approach are comparable to existing methods.



