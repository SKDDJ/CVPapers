# Arxiv Papers in cs.CV on 2020-05-02
### Stochastic Neighbor Embedding of Multimodal Relational Data for Image-Text Simultaneous Visualization
- **Arxiv ID**: http://arxiv.org/abs/2005.00670v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV, cs.HC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.00670v1)
- **Published**: 2020-05-02 00:39:29+00:00
- **Updated**: 2020-05-02 00:39:29+00:00
- **Authors**: Morihiro Mizutani, Akifumi Okuno, Geewook Kim, Hidetoshi Shimodaira
- **Comment**: 20 pages, 23 figures
- **Journal**: None
- **Summary**: Multimodal relational data analysis has become of increasing importance in recent years, for exploring across different domains of data, such as images and their text tags obtained from social networking services (e.g., Flickr). A variety of data analysis methods have been developed for visualization; to give an example, t-Stochastic Neighbor Embedding (t-SNE) computes low-dimensional feature vectors so that their similarities keep those of the observed data vectors. However, t-SNE is designed only for a single domain of data but not for multimodal data; this paper aims at visualizing multimodal relational data consisting of data vectors in multiple domains with relations across these vectors. By extending t-SNE, we herein propose Multimodal Relational Stochastic Neighbor Embedding (MR-SNE), that (1) first computes augmented relations, where we observe the relations across domains and compute those within each of domains via the observed data vectors, and (2) jointly embeds the augmented relations to a low-dimensional space. Through visualization of Flickr and Animal with Attributes 2 datasets, proposed MR-SNE is compared with other graph embedding-based approaches; MR-SNE demonstrates the promising performance.



### PAMTRI: Pose-Aware Multi-Task Learning for Vehicle Re-Identification Using Highly Randomized Synthetic Data
- **Arxiv ID**: http://arxiv.org/abs/2005.00673v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.00673v1)
- **Published**: 2020-05-02 01:29:09+00:00
- **Updated**: 2020-05-02 01:29:09+00:00
- **Authors**: Zheng Tang, Milind Naphade, Stan Birchfield, Jonathan Tremblay, William Hodge, Ratnesh Kumar, Shuo Wang, Xiaodong Yang
- **Comment**: Accepted by ICCV 2019
- **Journal**: None
- **Summary**: In comparison with person re-identification (ReID), which has been widely studied in the research community, vehicle ReID has received less attention. Vehicle ReID is challenging due to 1) high intra-class variability (caused by the dependency of shape and appearance on viewpoint), and 2) small inter-class variability (caused by the similarity in shape and appearance between vehicles produced by different manufacturers). To address these challenges, we propose a Pose-Aware Multi-Task Re-Identification (PAMTRI) framework. This approach includes two innovations compared with previous methods. First, it overcomes viewpoint-dependency by explicitly reasoning about vehicle pose and shape via keypoints, heatmaps and segments from pose estimation. Second, it jointly classifies semantic vehicle attributes (colors and types) while performing ReID, through multi-task learning with the embedded pose representations. Since manually labeling images with detailed pose and attribute information is prohibitive, we create a large-scale highly randomized synthetic dataset with automatically annotated vehicle attributes for training. Extensive experiments validate the effectiveness of each proposed component, showing that PAMTRI achieves significant improvement over state-of-the-art on two mainstream vehicle ReID benchmarks: VeRi and CityFlow-ReID. Code and models are available at https://github.com/NVlabs/PAMTRI.



### On the Generalization Effects of Linear Transformations in Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2005.00695v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.00695v3)
- **Published**: 2020-05-02 04:10:21+00:00
- **Updated**: 2023-07-26 22:58:34+00:00
- **Authors**: Sen Wu, Hongyang R. Zhang, Gregory Valiant, Christopher RÃ©
- **Comment**: 22 pages. Appeared in ICML 2020
- **Journal**: None
- **Summary**: Data augmentation is a powerful technique to improve performance in applications such as image and text classification tasks. Yet, there is little rigorous understanding of why and how various augmentations work. In this work, we consider a family of linear transformations and study their effects on the ridge estimator in an over-parametrized linear regression setting. First, we show that transformations that preserve the labels of the data can improve estimation by enlarging the span of the training data. Second, we show that transformations that mix data can improve estimation by playing a regularization effect. Finally, we validate our theoretical insights on MNIST. Based on the insights, we propose an augmentation scheme that searches over the space of transformations by how uncertain the model is about the transformed data. We validate our proposed scheme on image and text datasets. For example, our method outperforms random sampling methods by 1.24% on CIFAR-100 using Wide-ResNet-28-10. Furthermore, we achieve comparable accuracy to the SoTA Adversarial AutoAugment on CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets.



### A Benchmark for Structured Procedural Knowledge Extraction from Cooking Videos
- **Arxiv ID**: http://arxiv.org/abs/2005.00706v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.00706v2)
- **Published**: 2020-05-02 05:15:20+00:00
- **Updated**: 2020-10-09 13:54:27+00:00
- **Authors**: Frank F. Xu, Lei Ji, Botian Shi, Junyi Du, Graham Neubig, Yonatan Bisk, Nan Duan
- **Comment**: Accepted by NLP Beyond Text - First International Workshop on Natural
  Language Processing Beyond Text @ EMNLP 2020
- **Journal**: None
- **Summary**: Watching instructional videos are often used to learn about procedures. Video captioning is one way of automatically collecting such knowledge. However, it provides only an indirect, overall evaluation of multimodal models with no finer-grained quantitative measure of what they have learned. We propose instead, a benchmark of structured procedural knowledge extracted from cooking videos. This work is complementary to existing tasks, but requires models to produce interpretable structured knowledge in the form of verb-argument tuples. Our manually annotated open-vocabulary resource includes 356 instructional cooking videos and 15,523 video clip/sentence-level annotations. Our analysis shows that the proposed task is challenging and standard modeling approaches like unsupervised segmentation, semantic role labeling, and visual action detection perform poorly when forced to predict every action of a procedure in a structured form.



### Obtaining Faithful Interpretations from Compositional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2005.00724v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.00724v2)
- **Published**: 2020-05-02 06:50:35+00:00
- **Updated**: 2020-09-08 15:52:28+00:00
- **Authors**: Sanjay Subramanian, Ben Bogin, Nitish Gupta, Tomer Wolfson, Sameer Singh, Jonathan Berant, Matt Gardner
- **Comment**: ACL 2020; first three authors contributed equally
- **Journal**: None
- **Summary**: Neural module networks (NMNs) are a popular approach for modeling compositionality: they achieve high accuracy when applied to problems in language and vision, while reflecting the compositional structure of the problem in the network architecture. However, prior work implicitly assumed that the structure of the network modules, describing the abstract reasoning process, provides a faithful explanation of the model's reasoning; that is, that all modules perform their intended behaviour. In this work, we propose and conduct a systematic evaluation of the intermediate outputs of NMNs on NLVR2 and DROP, two datasets which require composing multiple reasoning steps. We find that the intermediate outputs differ from the expected output, illustrating that the network structure does not provide a faithful explanation of model behaviour. To remedy that, we train the model with auxiliary supervision and propose particular choices for module architecture that yield much better faithfulness, at a minimal cost to accuracy.



### Cross-View Image Retrieval -- Ground to Aerial Image Retrieval through Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2005.00725v1
- **DOI**: 10.1007/978-3-030-36711-4_19
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.00725v1)
- **Published**: 2020-05-02 06:52:16+00:00
- **Updated**: 2020-05-02 06:52:16+00:00
- **Authors**: Numan Khurshid, Talha Hanif, Mohbat Tharani, Murtaza Taj
- **Comment**: International Conference on Neural Information Processing
  (ICONIP-2019)
- **Journal**: None
- **Summary**: Cross-modal retrieval aims to measure the content similarity between different types of data. The idea has been previously applied to visual, text, and speech data. In this paper, we present a novel cross-modal retrieval method specifically for multi-view images, called Cross-view Image Retrieval CVIR. Our approach aims to find a feature space as well as an embedding space in which samples from street-view images are compared directly to satellite-view images (and vice-versa). For this comparison, a novel deep metric learning based solution "DeepCVIR" has been proposed. Previous cross-view image datasets are deficient in that they (1) lack class information; (2) were originally collected for cross-view image geolocalization task with coupled images; (3) do not include any images from off-street locations. To train, compare, and evaluate the performance of cross-view image retrieval, we present a new 6 class cross-view image dataset termed as CrossViewRet which comprises of images including freeway, mountain, palace, river, ship, and stadium with 700 high-resolution dual-view images for each class. Results show that the proposed DeepCVIR outperforms conventional matching approaches on the CVIR task for the given dataset and would also serve as the baseline for future research.



### Heterogeneous Knowledge Distillation using Information Flow Modeling
- **Arxiv ID**: http://arxiv.org/abs/2005.00727v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.00727v1)
- **Published**: 2020-05-02 06:56:56+00:00
- **Updated**: 2020-05-02 06:56:56+00:00
- **Authors**: Nikolaos Passalis, Maria Tzelepi, Anastasios Tefas
- **Comment**: Accepted at CVPR 2020
- **Journal**: None
- **Summary**: Knowledge Distillation (KD) methods are capable of transferring the knowledge encoded in a large and complex teacher into a smaller and faster student. Early methods were usually limited to transferring the knowledge only between the last layers of the networks, while latter approaches were capable of performing multi-layer KD, further increasing the accuracy of the student. However, despite their improved performance, these methods still suffer from several limitations that restrict both their efficiency and flexibility. First, existing KD methods typically ignore that neural networks undergo through different learning phases during the training process, which often requires different types of supervision for each one. Furthermore, existing multi-layer KD methods are usually unable to effectively handle networks with significantly different architectures (heterogeneous KD). In this paper we propose a novel KD method that works by modeling the information flow through the various layers of the teacher model and then train a student model to mimic this information flow. The proposed method is capable of overcoming the aforementioned limitations by using an appropriate supervision scheme during the different phases of the training process, as well as by designing and training an appropriate auxiliary teacher model that acts as a proxy model capable of "explaining" the way the teacher works to the student. The effectiveness of the proposed method is demonstrated using four image datasets and several different evaluation setups.



### RMM: A Recursive Mental Model for Dialog Navigation
- **Arxiv ID**: http://arxiv.org/abs/2005.00728v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2005.00728v2)
- **Published**: 2020-05-02 06:57:14+00:00
- **Updated**: 2020-10-06 02:16:27+00:00
- **Authors**: Homero Roman Roman, Yonatan Bisk, Jesse Thomason, Asli Celikyilmaz, Jianfeng Gao
- **Comment**: Findings of Empirical Methods in Natural Language Processing (EMNLP
  Findings), 2020
- **Journal**: None
- **Summary**: Language-guided robots must be able to both ask humans questions and understand answers. Much existing work focuses only on the latter. In this paper, we go beyond instruction following and introduce a two-agent task where one agent navigates and asks questions that a second, guiding agent answers. Inspired by theory of mind, we propose the Recursive Mental Model (RMM). The navigating agent models the guiding agent to simulate answers given candidate generated questions. The guiding agent in turn models the navigating agent to simulate navigation steps it would take to generate answers. We use the progress agents make towards the goal as a reinforcement learning reward signal to directly inform not only navigation actions, but also both question and answer generation. We demonstrate that RMM enables better generalization to novel environments. Interlocutor modelling may be a way forward for human-agent dialogue where robots need to both ask and answer questions.



### CoMoGCN: Coherent Motion Aware Trajectory Prediction with Graph Representation
- **Arxiv ID**: http://arxiv.org/abs/2005.00754v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.00754v2)
- **Published**: 2020-05-02 09:10:30+00:00
- **Updated**: 2020-05-05 08:47:12+00:00
- **Authors**: Yuying Chen, Congcong Liu, Bertram Shi, Ming Liu
- **Comment**: 12 pages, 3 figures
- **Journal**: None
- **Summary**: Forecasting human trajectories is critical for tasks such as robot crowd navigation and autonomous driving. Modeling social interactions is of great importance for accurate group-wise motion prediction. However, most existing methods do not consider information about coherence within the crowd, but rather only pairwise interactions. In this work, we propose a novel framework, coherent motion aware graph convolutional network (CoMoGCN), for trajectory prediction in crowded scenes with group constraints. First, we cluster pedestrian trajectories into groups according to motion coherence. Then, we use graph convolutional networks to aggregate crowd information efficiently. The CoMoGCN also takes advantage of variational autoencoders to capture the multimodal nature of the human trajectories by modeling the distribution. Our method achieves state-of-the-art performance on several different trajectory prediction benchmarks, and the best average performance among all benchmarks considered.



### Projection Inpainting Using Partial Convolution for Metal Artifact Reduction
- **Arxiv ID**: http://arxiv.org/abs/2005.00762v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.00762v1)
- **Published**: 2020-05-02 09:32:35+00:00
- **Updated**: 2020-05-02 09:32:35+00:00
- **Authors**: Lin Yuan, Yixing Huang, Andreas Maier
- **Comment**: Technical Report
- **Journal**: None
- **Summary**: In computer tomography, due to the presence of metal implants in the patient body, reconstructed images will suffer from metal artifacts. In order to reduce metal artifacts, metals are typically removed in projection images. Therefore, the metal corrupted projection areas need to be inpainted. For deep learning inpainting methods, convolutional neural networks (CNNs) are widely used, for example, the U-Net. However, such CNNs use convolutional filter responses on both valid and corrupted pixel values, resulting in unsatisfactory image quality. In this work, partial convolution is applied for projection inpainting, which only relies on valid pixels values. The U-Net with partial convolution and conventional convolution are compared for metal artifact reduction. Our experiments demonstrate that the U-Net with partial convolution is able to inpaint the metal corrupted areas better than that with conventional convolution.



### Deep Feature Mining via Attention-based BiLSTM-GCN for Human Motor Imagery Recognition
- **Arxiv ID**: http://arxiv.org/abs/2005.00777v3
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.00777v3)
- **Published**: 2020-05-02 10:03:40+00:00
- **Updated**: 2021-12-02 09:10:11+00:00
- **Authors**: Yimin Hou, Shuyue Jia, Xiangmin Lun, Shu Zhang, Tao Chen, Fang Wang, Jinglei Lv
- **Comment**: None
- **Journal**: None
- **Summary**: Recognition accuracy and response time are both critically essential ahead of building practical electroencephalography (EEG) based brain-computer interface (BCI). Recent approaches, however, have either compromised in the classification accuracy or responding time. This paper presents a novel deep learning approach designed towards remarkably accurate and responsive motor imagery (MI) recognition based on scalp EEG. Bidirectional Long Short-term Memory (BiLSTM) with the Attention mechanism manages to derive relevant features from raw EEG signals. The connected graph convolutional neural network (GCN) promotes the decoding performance by cooperating with the topological structure of features, which are estimated from the overall data. The 0.4-second detection framework has shown effective and efficient prediction based on individual and group-wise training, with 98.81% and 94.64% accuracy, respectively, which outperformed all the state-of-the-art studies. The introduced deep feature mining approach can precisely recognize human motion intents from raw EEG signals, which paves the road to translate the EEG based MI recognition to practical BCI systems.



### PyRetri: A PyTorch-based Library for Unsupervised Image Retrieval by Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2005.02154v2
- **DOI**: None
- **Categories**: **cs.IR**, cs.CV, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2005.02154v2)
- **Published**: 2020-05-02 10:17:18+00:00
- **Updated**: 2020-08-05 13:12:10+00:00
- **Authors**: Benyi Hu, Ren-Jie Song, Xiu-Shen Wei, Yazhou Yao, Xian-Sheng Hua, Yuehu Liu
- **Comment**: Accepted by ACM Multimedia Conference 2020. PyRetri is open-source
  and available at https://github.com/PyRetri/PyRetri
- **Journal**: None
- **Summary**: Despite significant progress of applying deep learning methods to the field of content-based image retrieval, there has not been a software library that covers these methods in a unified manner. In order to fill this gap, we introduce PyRetri, an open source library for deep learning based unsupervised image retrieval. The library encapsulates the retrieval process in several stages and provides functionality that covers various prominent methods for each stage. The idea underlying its design is to provide a unified platform for deep learning based image retrieval research, with high usability and extensibility. To the best of our knowledge, this is the first open-source library for unsupervised image retrieval by deep learning.



### Comparing SNNs and RNNs on Neuromorphic Vision Datasets: Similarities and Differences
- **Arxiv ID**: http://arxiv.org/abs/2005.02183v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.02183v1)
- **Published**: 2020-05-02 10:19:37+00:00
- **Updated**: 2020-05-02 10:19:37+00:00
- **Authors**: Weihua He, YuJie Wu, Lei Deng, Guoqi Li, Haoyu Wang, Yang Tian, Wei Ding, Wenhui Wang, Yuan Xie
- **Comment**: None
- **Journal**: None
- **Summary**: Neuromorphic data, recording frameless spike events, have attracted considerable attention for the spatiotemporal information components and the event-driven processing fashion. Spiking neural networks (SNNs) represent a family of event-driven models with spatiotemporal dynamics for neuromorphic computing, which are widely benchmarked on neuromorphic data. Interestingly, researchers in the machine learning community can argue that recurrent (artificial) neural networks (RNNs) also have the capability to extract spatiotemporal features although they are not event-driven. Thus, the question of "what will happen if we benchmark these two kinds of models together on neuromorphic data" comes out but remains unclear. In this work, we make a systematic study to compare SNNs and RNNs on neuromorphic data, taking the vision datasets as a case study. First, we identify the similarities and differences between SNNs and RNNs (including the vanilla RNNs and LSTM) from the modeling and learning perspectives. To improve comparability and fairness, we unify the supervised learning algorithm based on backpropagation through time (BPTT), the loss function exploiting the outputs at all timesteps, the network structure with stacked fully-connected or convolutional layers, and the hyper-parameters during training. Especially, given the mainstream loss function used in RNNs, we modify it inspired by the rate coding scheme to approach that of SNNs. Furthermore, we tune the temporal resolution of datasets to test model robustness and generalization. At last, a series of contrast experiments are conducted on two types of neuromorphic datasets: DVS-converted (N-MNIST) and DVS-captured (DVS Gesture).



### A neural network walks into a lab: towards using deep nets as models for human behavior
- **Arxiv ID**: http://arxiv.org/abs/2005.02181v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2005.02181v1)
- **Published**: 2020-05-02 11:17:36+00:00
- **Updated**: 2020-05-02 11:17:36+00:00
- **Authors**: Wei Ji Ma, Benjamin Peters
- **Comment**: None
- **Journal**: None
- **Summary**: What might sound like the beginning of a joke has become an attractive prospect for many cognitive scientists: the use of deep neural network models (DNNs) as models of human behavior in perceptual and cognitive tasks. Although DNNs have taken over machine learning, attempts to use them as models of human behavior are still in the early stages. Can they become a versatile model class in the cognitive scientist's toolbox? We first argue why DNNs have the potential to be interesting models of human behavior. We then discuss how that potential can be more fully realized. On the one hand, we argue that the cycle of training, testing, and revising DNNs needs to be revisited through the lens of the cognitive scientist's goals. Specifically, we argue that methods for assessing the goodness of fit between DNN models and human behavior have to date been impoverished. On the other hand, cognitive science might have to start using more complex tasks (including richer stimulus spaces), but doing so might be beneficial for DNN-independent reasons as well. Finally, we highlight avenues where traditional cognitive process models and DNNs may show productive synergy.



### DroTrack: High-speed Drone-based Object Tracking Under Uncertainty
- **Arxiv ID**: http://arxiv.org/abs/2005.00828v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2005.00828v1)
- **Published**: 2020-05-02 13:16:16+00:00
- **Updated**: 2020-05-02 13:16:16+00:00
- **Authors**: Ali Hamdi, Flora Salim, Du Yong Kim
- **Comment**: 10 pages, 12 figures, FUZZ-IEEE 2020
- **Journal**: None
- **Summary**: We present DroTrack, a high-speed visual single-object tracking framework for drone-captured video sequences. Most of the existing object tracking methods are designed to tackle well-known challenges, such as occlusion and cluttered backgrounds. The complex motion of drones, i.e., multiple degrees of freedom in three-dimensional space, causes high uncertainty. The uncertainty problem leads to inaccurate location predictions and fuzziness in scale estimations. DroTrack solves such issues by discovering the dependency between object representation and motion geometry. We implement an effective object segmentation based on Fuzzy C Means (FCM). We incorporate the spatial information into the membership function to cluster the most discriminative segments. We then enhance the object segmentation by using a pre-trained Convolution Neural Network (CNN) model. DroTrack also leverages the geometrical angular motion to estimate a reliable object scale. We discuss the experimental results and performance evaluation using two datasets of 51,462 drone-captured frames. The combination of the FCM segmentation and the angular scaling increased DroTrack precision by up to $9\%$ and decreased the centre location error by $162$ pixels on average. DroTrack outperforms all the high-speed trackers and achieves comparable results in comparison to deep learning trackers. DroTrack offers high frame rates up to 1000 frame per second (fps) with the best location precision, more than a set of state-of-the-art real-time trackers.



### Towards Deep Learning Methods for Quality Assessment of Computer-Generated Imagery
- **Arxiv ID**: http://arxiv.org/abs/2005.00836v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.00836v1)
- **Published**: 2020-05-02 14:08:39+00:00
- **Updated**: 2020-05-02 14:08:39+00:00
- **Authors**: Markus Utke, Saman Zadtootaghaj, Steven Schmidt, Sebastian MÃ¶ller
- **Comment**: 4 pages
- **Journal**: None
- **Summary**: Video gaming streaming services are growing rapidly due to new services such as passive video streaming, e.g. Twitch.tv, and cloud gaming, e.g. Nvidia Geforce Now. In contrast to traditional video content, gaming content has special characteristics such as extremely high motion for some games, special motion patterns, synthetic content and repetitive content, which makes the state-of-the-art video and image quality metrics perform weaker for this special computer generated content. In this paper, we outline our plan to build a deep learningbased quality metric for video gaming quality assessment. In addition, we present initial results by training the network based on VMAF values as a ground truth to give some insights on how to build a metric in future. The paper describes the method that is used to choose an appropriate Convolutional Neural Network architecture. Furthermore, we estimate the size of the required subjective quality dataset which achieves a sufficiently high performance. The results show that by taking around 5k images for training of the last six modules of Xception, we can obtain a relatively high performance metric to assess the quality of distorted video games.



### Derivation of a Constant Velocity Motion Model for Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/2005.00844v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.00844v4)
- **Published**: 2020-05-02 14:40:18+00:00
- **Updated**: 2020-10-20 22:05:50+00:00
- **Authors**: Nathanael L. Baisa
- **Comment**: None
- **Journal**: None
- **Summary**: Motion models play a great role in visual tracking applications for predicting the possible locations of objects in the next frame. Unlike target tracking in radar or aerospace domain which considers only points, object tracking in computer vision involves sizes of objects. Constant velocity motion model is the most widely used motion model for visual tracking, however, there is no clear and understandable derivation involving sizes of objects specially for new researchers joining this research field. In this document, we derive the constant velocity motion model that incorporates sizes of objects that, we think, can help the new researchers to adapt to it very quickly.



### BeCAPTCHA-Mouse: Synthetic Mouse Trajectories and Improved Bot Detection
- **Arxiv ID**: http://arxiv.org/abs/2005.00890v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2005.00890v2)
- **Published**: 2020-05-02 17:40:49+00:00
- **Updated**: 2021-03-02 18:35:31+00:00
- **Authors**: Alejandro Acien, Aythami Morales, Julian Fierrez, Ruben Vera-Rodriguez
- **Comment**: None
- **Journal**: None
- **Summary**: We first study the suitability of behavioral biometrics to distinguish between computers and humans, commonly named as bot detection. We then present BeCAPTCHA-Mouse, a bot detector based on: i) a neuromotor model of mouse dynamics to obtain a novel feature set for the classification of human and bot samples; and ii) a learning framework involving real and synthetically generated mouse trajectories. We propose two new mouse trajectory synthesis methods for generating realistic data: a) a function-based method based on heuristic functions, and b) a data-driven method based on Generative Adversarial Networks (GANs) in which a Generator synthesizes human-like trajectories from a Gaussian noise input. Experiments are conducted on a new testbed also introduced here and available in GitHub: BeCAPTCHA-Mouse Benchmark; useful for research in bot detection and other mouse-based HCI applications. Our benchmark data consists of 15,000 mouse trajectories including real data from 58 users and bot data with various levels of realism. Our experiments show that BeCAPTCHA-Mouse is able to detect bot trajectories of high realism with 93% of accuracy in average using only one mouse trajectory. When our approach is fused with state-of-the-art mouse dynamic features, the bot detection accuracy increases relatively by more than 36%, proving that mouse-based bot detection is a fast, easy, and reliable tool to complement traditional CAPTCHA systems.



### Clue: Cross-modal Coherence Modeling for Caption Generation
- **Arxiv ID**: http://arxiv.org/abs/2005.00908v1
- **DOI**: 10.18653/v1/2020.acl-main.583
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.00908v1)
- **Published**: 2020-05-02 19:28:52+00:00
- **Updated**: 2020-05-02 19:28:52+00:00
- **Authors**: Malihe Alikhani, Piyush Sharma, Shengjie Li, Radu Soricut, Matthew Stone
- **Comment**: Accepted as a long paper to ACL 2020
- **Journal**: None
- **Summary**: We use coherence relations inspired by computational models of discourse to study the information needs and goals of image captioning. Using an annotation protocol specifically devised for capturing image--caption coherence relations, we annotate 10,000 instances from publicly-available image--caption pairs. We introduce a new task for learning inferences in imagery and text, coherence relation prediction, and show that these coherence annotations can be exploited to learn relation classifiers as an intermediary step, and also train coherence-aware, controllable image captioning models. The results show a dramatic improvement in the consistency and quality of the generated captions with respect to information needs specified via coherence relations.



### SAMP: Shape and Motion Priors for 4D Vehicle Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2005.00922v1
- **DOI**: 10.1109/WACV.2017.51
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.00922v1)
- **Published**: 2020-05-02 21:23:54+00:00
- **Updated**: 2020-05-02 21:23:54+00:00
- **Authors**: Francis Engelmann, JÃ¶rg StÃ¼ckler, Bastian Leibe
- **Comment**: None
- **Journal**: IEEE Winter Conference on Applications of Computer Vision (WACV),
  2017
- **Summary**: Inferring the pose and shape of vehicles in 3D from a movable platform still remains a challenging task due to the projective sensing principle of cameras, difficult surface properties e.g. reflections or transparency, and illumination changes between images. In this paper, we propose to use 3D shape and motion priors to regularize the estimation of the trajectory and the shape of vehicles in sequences of stereo images. We represent shapes by 3D signed distance functions and embed them in a low-dimensional manifold. Our optimization method allows for imposing a common shape across all image observations along an object track. We employ a motion model to regularize the trajectory to plausible object motions. We evaluate our method on the KITTI dataset and show state-of-the-art results in terms of shape reconstruction and pose estimation accuracy.



### Multi-Modality Generative Adversarial Networks with Tumor Consistency Loss for Brain MR Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2005.00925v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.00925v1)
- **Published**: 2020-05-02 21:33:15+00:00
- **Updated**: 2020-05-02 21:33:15+00:00
- **Authors**: Bingyu Xin, Yifan Hu, Yefeng Zheng, Hongen Liao
- **Comment**: 5 pages, 3 figures, accepted to IEEE ISBI 2020
- **Journal**: None
- **Summary**: Magnetic Resonance (MR) images of different modalities can provide complementary information for clinical diagnosis, but whole modalities are often costly to access. Most existing methods only focus on synthesizing missing images between two modalities, which limits their robustness and efficiency when multiple modalities are missing. To address this problem, we propose a multi-modality generative adversarial network (MGAN) to synthesize three high-quality MR modalities (FLAIR, T1 and T1ce) from one MR modality T2 simultaneously. The experimental results show that the quality of the synthesized images by our proposed methods is better than the one synthesized by the baseline model, pix2pix. Besides, for MR brain image synthesis, it is important to preserve the critical tumor information in the generated modalities, so we further introduce a multi-modality tumor consistency loss to MGAN, called TC-MGAN. We use the synthesized modalities by TC-MGAN to boost the tumor segmentation accuracy, and the results demonstrate its effectiveness.



### An Information-theoretic Visual Analysis Framework for Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2005.02186v1
- **DOI**: 10.2312/stag.20211486
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2005.02186v1)
- **Published**: 2020-05-02 21:36:50+00:00
- **Updated**: 2020-05-02 21:36:50+00:00
- **Authors**: Jingyi Shen, Han-Wei Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the great success of Convolutional Neural Networks (CNNs) in Computer Vision and Natural Language Processing, the working mechanism behind CNNs is still under extensive discussions and research. Driven by a strong demand for the theoretical explanation of neural networks, some researchers utilize information theory to provide insight into the black box model. However, to the best of our knowledge, employing information theory to quantitatively analyze and qualitatively visualize neural networks has not been extensively studied in the visualization community. In this paper, we combine information entropies and visualization techniques to shed light on how CNN works. Specifically, we first introduce a data model to organize the data that can be extracted from CNN models. Then we propose two ways to calculate entropy under different circumstances. To provide a fundamental understanding of the basic building blocks of CNNs (e.g., convolutional layers, pooling layers, normalization layers) from an information-theoretic perspective, we develop a visual analysis system, CNNSlicer. CNNSlicer allows users to interactively explore the amount of information changes inside the model. With case studies on the widely used benchmark datasets (MNIST and CIFAR-10), we demonstrate the effectiveness of our system in opening the blackbox of CNNs.



### An Intelligent and Low-cost Eye-tracking System for Motorized Wheelchair Control
- **Arxiv ID**: http://arxiv.org/abs/2005.02118v1
- **DOI**: 10.3390/s20143936
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.02118v1)
- **Published**: 2020-05-02 23:08:33+00:00
- **Updated**: 2020-05-02 23:08:33+00:00
- **Authors**: Mahmoud Dahmani, Muhammad E. H. Chowdhury, Amith Khandakar, Tawsifur Rahman, Khaled Al-Jayyousi, Abdalla Hefny, Serkan Kiranyaz
- **Comment**: Accepted for publication in Sensor, 19 Figure, 3 Tables
- **Journal**: Sensors 2020, 20(14), 3936
- **Summary**: In the 34 developed and 156 developing countries, there are about 132 million disabled people who need a wheelchair constituting 1.86% of the world population. Moreover, there are millions of people suffering from diseases related to motor disabilities, which cause inability to produce controlled movement in any of the limbs or even head.The paper proposes a system to aid people with motor disabilities by restoring their ability to move effectively and effortlessly without having to rely on others utilizing an eye-controlled electric wheelchair. The system input was images of the users eye that were processed to estimate the gaze direction and the wheelchair was moved accordingly. To accomplish such a feat, four user-specific methods were developed, implemented and tested; all of which were based on a benchmark database created by the authors.The first three techniques were automatic, employ correlation and were variants of template matching, while the last one uses convolutional neural networks (CNNs). Different metrics to quantitatively evaluate the performance of each algorithm in terms of accuracy and latency were computed and overall comparison is presented. CNN exhibited the best performance (i.e. 99.3% classification accuracy), and thus it was the model of choice for the gaze estimator, which commands the wheelchair motion. The system was evaluated carefully on 8 subjects achieving 99% accuracy in changing illumination conditions outdoor and indoor. This required modifying a motorized wheelchair to adapt it to the predictions output by the gaze estimation algorithm. The wheelchair control can bypass any decision made by the gaze estimator and immediately halt its motion with the help of an array of proximity sensors, if the measured distance goes below a well-defined safety margin.



### Neural Computing for Online Arabic Handwriting Character Recognition using Hard Stroke Features Mining
- **Arxiv ID**: http://arxiv.org/abs/2005.02171v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2005.02171v3)
- **Published**: 2020-05-02 23:17:08+00:00
- **Updated**: 2021-01-15 10:58:36+00:00
- **Authors**: Amjad Rehman
- **Comment**: 16 pages
- **Journal**: IJICIC 2021
- **Summary**: Online Arabic cursive character recognition is still a big challenge due to the existing complexities including Arabic cursive script styles, writing speed, writer mood and so forth. Due to these unavoidable constraints, the accuracy of online Arabic character's recognition is still low and retain space for improvement. In this research, an enhanced method of detecting the desired critical points from vertical and horizontal direction-length of handwriting stroke features of online Arabic script recognition is proposed. Each extracted stroke feature divides every isolated character into some meaningful pattern known as tokens. A minimum feature set is extracted from these tokens for classification of characters using a multilayer perceptron with a back-propagation learning algorithm and modified sigmoid function-based activation function. In this work, two milestones are achieved; firstly, attain a fixed number of tokens, secondly, minimize the number of the most repetitive tokens. For experiments, handwritten Arabic characters are selected from the OHASD benchmark dataset to test and evaluate the proposed method. The proposed method achieves an average accuracy of 98.6% comparable in state of art character recognition techniques.



### Tensor optimal transport, distance between sets of measures and tensor scaling
- **Arxiv ID**: http://arxiv.org/abs/2005.00945v2
- **DOI**: None
- **Categories**: **cs.CV**, math.OC, 15A39, 15A69, 52A41, 62H17, 65D19, 65F35, 65K05, 90C05, 90C25
- **Links**: [PDF](http://arxiv.org/pdf/2005.00945v2)
- **Published**: 2020-05-02 23:49:31+00:00
- **Updated**: 2021-07-24 22:27:51+00:00
- **Authors**: Shmuel Friedland
- **Comment**: 32 pages, some of the results in arXiv:1905.11384 are repeated
- **Journal**: None
- **Summary**: We study the optimal transport problem for $d>2$ discrete measures. This is a linear programming problem on $d$-tensors. It gives a way to compute a "distance" between two sets of discrete measures. We introduce an entropic regularization term, which gives rise to a scaling of tensors. We give a variation of the celebrated Sinkhorn scaling algorithm. We show that this algorithm can be viewed as a partial minimization algorithm of a strictly convex function. Under appropriate conditions the rate of convergence is geometric and we estimate the rate. Our results are generalizations of known results for the classical case of two discrete measures.



### Towards Occlusion-Aware Multifocal Displays
- **Arxiv ID**: http://arxiv.org/abs/2005.00946v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/2005.00946v1)
- **Published**: 2020-05-02 23:51:11+00:00
- **Updated**: 2020-05-02 23:51:11+00:00
- **Authors**: Jen-Hao Rick Chang, Anat Levin, B. V. K. Vijaya Kumar, Aswin C. Sankaranarayanan
- **Comment**: SIGGRAPH 2020
- **Journal**: None
- **Summary**: The human visual system uses numerous cues for depth perception, including disparity, accommodation, motion parallax and occlusion. It is incumbent upon virtual-reality displays to satisfy these cues to provide an immersive user experience. Multifocal displays, one of the classic approaches to satisfy the accommodation cue, place virtual content at multiple focal planes, each at a di erent depth. However, the content on focal planes close to the eye do not occlude those farther away; this deteriorates the occlusion cue as well as reduces contrast at depth discontinuities due to leakage of the defocus blur. This paper enables occlusion-aware multifocal displays using a novel ConeTilt operator that provides an additional degree of freedom -- tilting the light cone emitted at each pixel of the display panel. We show that, for scenes with relatively simple occlusion con gurations, tilting the light cones provides the same e ect as physical occlusion. We demonstrate that ConeTilt can be easily implemented by a phase-only spatial light modulator. Using a lab prototype, we show results that demonstrate the presence of occlusion cues and the increased contrast of the display at depth edges.



