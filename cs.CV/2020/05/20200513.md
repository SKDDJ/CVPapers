# Arxiv Papers in cs.CV on 2020-05-13
### Adversarial examples are useful too!
- **Arxiv ID**: http://arxiv.org/abs/2005.06107v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.06107v1)
- **Published**: 2020-05-13 01:38:56+00:00
- **Updated**: 2020-05-13 01:38:56+00:00
- **Authors**: Ali Borji
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning has come a long way and has enjoyed an unprecedented success. Despite high accuracy, however, deep models are brittle and are easily fooled by imperceptible adversarial perturbations. In contrast to common inference-time attacks, Backdoor (\aka Trojan) attacks target the training phase of model construction, and are extremely difficult to combat since a) the model behaves normally on a pristine testing set and b) the augmented perturbations can be minute and may only affect few training samples. Here, I propose a new method to tell whether a model has been subject to a backdoor attack. The idea is to generate adversarial examples, targeted or untargeted, using conventional attacks such as FGSM and then feed them back to the classifier. By computing the statistics (here simply mean maps) of the images in different categories and comparing them with the statistics of a reference model, it is possible to visually locate the perturbed regions and unveil the attack.



### Project RISE: Recognizing Industrial Smoke Emissions
- **Arxiv ID**: http://arxiv.org/abs/2005.06111v8
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.06111v8)
- **Published**: 2020-05-13 01:51:15+00:00
- **Updated**: 2021-03-07 06:59:54+00:00
- **Authors**: Yen-Chia Hsu, Ting-Hao 'Kenneth' Huang, Ting-Yao Hu, Paul Dille, Sean Prendi, Ryan Hoffman, Anastasia Tsuhlares, Jessica Pachuta, Randy Sargent, Illah Nourbakhsh
- **Comment**: Accepted by AAAI 2021
- **Journal**: None
- **Summary**: Industrial smoke emissions pose a significant concern to human health. Prior works have shown that using Computer Vision (CV) techniques to identify smoke as visual evidence can influence the attitude of regulators and empower citizens to pursue environmental justice. However, existing datasets are not of sufficient quality nor quantity to train the robust CV models needed to support air quality advocacy. We introduce RISE, the first large-scale video dataset for Recognizing Industrial Smoke Emissions. We adopted a citizen science approach to collaborate with local community members to annotate whether a video clip has smoke emissions. Our dataset contains 12,567 clips from 19 distinct views from cameras that monitored three industrial facilities. These daytime clips span 30 days over two years, including all four seasons. We ran experiments using deep neural networks to establish a strong performance baseline and reveal smoke recognition challenges. Our survey study discussed community feedback, and our data analysis displayed opportunities for integrating citizen scientists and crowd workers into the application of Artificial Intelligence for Social Impact.



### Self-Supervised Deep Visual Odometry with Online Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2005.06136v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.06136v1)
- **Published**: 2020-05-13 03:39:29+00:00
- **Updated**: 2020-05-13 03:39:29+00:00
- **Authors**: Shunkai Li, Xin Wang, Yingdian Cao, Fei Xue, Zike Yan, Hongbin Zha
- **Comment**: Accepted by CVPR 2020 oral
- **Journal**: None
- **Summary**: Self-supervised VO methods have shown great success in jointly estimating camera pose and depth from videos. However, like most data-driven methods, existing VO networks suffer from a notable decrease in performance when confronted with scenes different from the training data, which makes them unsuitable for practical applications. In this paper, we propose an online meta-learning algorithm to enable VO networks to continuously adapt to new environments in a self-supervised manner. The proposed method utilizes convolutional long short-term memory (convLSTM) to aggregate rich spatial-temporal information in the past. The network is able to memorize and learn from its past experience for better estimation and fast adaptation to the current frame. When running VO in the open world, in order to deal with the changing environment, we propose an online feature alignment method by aligning feature distributions at different time. Our VO network is able to seamlessly adapt to different environments. Extensive experiments on unseen outdoor scenes, virtual to real world and outdoor to indoor environments demonstrate that our method consistently outperforms state-of-the-art self-supervised VO baselines considerably.



### 3D Scene Geometry-Aware Constraint for Camera Localization with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2005.06147v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.06147v1)
- **Published**: 2020-05-13 04:15:14+00:00
- **Updated**: 2020-05-13 04:15:14+00:00
- **Authors**: Mi Tian, Qiong Nie, Hao Shen
- **Comment**: Accepted for ICRA 2020
- **Journal**: None
- **Summary**: Camera localization is a fundamental and key component of autonomous driving vehicles and mobile robots to localize themselves globally for further environment perception, path planning and motion control. Recently end-to-end approaches based on convolutional neural network have been much studied to achieve or even exceed 3D-geometry based traditional methods. In this work, we propose a compact network for absolute camera pose regression. Inspired from those traditional methods, a 3D scene geometry-aware constraint is also introduced by exploiting all available information including motion, depth and image contents. We add this constraint as a regularization term to our proposed network by defining a pixel-level photometric loss and an image-level structural similarity loss. To benchmark our method, different challenging scenes including indoor and outdoor environment are tested with our proposed approach and state-of-the-arts. And the experimental results demonstrate significant performance improvement of our method on both prediction accuracy and convergence efficiency.



### Attribute-guided Feature Extraction and Augmentation Robust Learning for Vehicle Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2005.06184v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.06184v1)
- **Published**: 2020-05-13 07:13:53+00:00
- **Updated**: 2020-05-13 07:13:53+00:00
- **Authors**: Chaoran Zhuge, Yujie Peng, Yadong Li, Jiangbo Ai, Junru Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Vehicle re-identification is one of the core technologies of intelligent transportation systems and smart cities, but large intra-class diversity and inter-class similarity poses great challenges for existing method. In this paper, we propose a multi-guided learning approach which utilizing the information of attributes and meanwhile introducing two novel random augments to improve the robustness during training. What's more, we propose an attribute constraint method and group re-ranking strategy to refine matching results. Our method achieves mAP of 66.83% and rank-1 accuracy 76.05% in the CVPR 2020 AI City Challenge.



### Context Learning for Bone Shadow Exclusion in CheXNet Accuracy Improvement
- **Arxiv ID**: http://arxiv.org/abs/2005.06189v1
- **DOI**: 10.1109/KSE.2018.8573393
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.06189v1)
- **Published**: 2020-05-13 07:29:03+00:00
- **Updated**: 2020-05-13 07:29:03+00:00
- **Authors**: Minh-Chuong Huynh, Trung-Hieu Nguyen, Minh-Triet Tran
- **Comment**: KSE 2018 long paper
- **Journal**: None
- **Summary**: Chest X-ray examination plays an important role in lung disease detection. The more accuracy of this task, the more experienced radiologists are required. After ChestX-ray14 dataset containing over 100,000 frontal-view X-ray images of 14 diseases was released, several models were proposed with high accuracy. In this paper, we develop a work flow for lung disease diagnosis in chest X-ray images, which can improve the average AUROC of the state-of-the-art model from 0.8414 to 0.8445. We apply image preprocessing steps before feeding to the 14 diseases detection model. Our project includes three models: the first one is DenseNet-121 to predict whether a processed image has a better result, a convolutional auto-encoder model for bone shadow exclusion is the second one, and the last is the original CheXNet.



### Recognition of 26 Degrees of Freedom of Hands Using Model-based approach and Depth-Color Images
- **Arxiv ID**: http://arxiv.org/abs/2005.07068v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.07068v1)
- **Published**: 2020-05-13 08:05:30+00:00
- **Updated**: 2020-05-13 08:05:30+00:00
- **Authors**: Cong Hoang Quach, Minh Trien Pham, Anh Viet Dang, Dinh Tuan Pham, Thuan Hoang Tran, Manh Duong Phung
- **Comment**: in Proceedings of the 2014 National Conference on Electronics,
  Communications and Information Technology (REV-ECIT). in Vietnamese language
- **Journal**: None
- **Summary**: In this study, we present an model-based approach to recognize full 26 degrees of freedom of a human hand. Input data include RGB-D images acquired from a Kinect camera and a 3D model of the hand constructed from its anatomy and graphical matrices. A cost function is then defined so that its minimum value is achieved when the model and observation images are matched. To solve the optimization problem in 26 dimensional space, the particle swarm optimization algorimth with improvements are used. In addition, parallel computation in graphical processing units (GPU) is utilized to handle computationally expensive tasks. Simulation and experimental results show that the system can recognize 26 degrees of freedom of hands with the processing time of 0.8 seconds per frame. The algorithm is robust to noise and the hardware requirement is simple with a single camera.



### Mean Oriented Riesz Features for Micro Expression Classification
- **Arxiv ID**: http://arxiv.org/abs/2005.06198v1
- **DOI**: None
- **Categories**: **cs.CV**, 68U10, 68T10
- **Links**: [PDF](http://arxiv.org/pdf/2005.06198v1)
- **Published**: 2020-05-13 08:23:34+00:00
- **Updated**: 2020-05-13 08:23:34+00:00
- **Authors**: Carlos Arango Duque, Olivier Alata, Rémi Emonet, Hubert Konik, Anne-Claire Legrand
- **Comment**: None
- **Journal**: None
- **Summary**: Micro-expressions are brief and subtle facial expressions that go on and off the face in a fraction of a second. This kind of facial expressions usually occurs in high stake situations and is considered to reflect a human's real intent. There has been some interest in micro-expression analysis, however, a great majority of the methods are based on classically established computer vision methods such as local binary patterns, histogram of gradients and optical flow. A novel methodology for micro-expression recognition using the Riesz pyramid, a multi-scale steerable Hilbert transform is presented. In fact, an image sequence is transformed with this tool, then the image phase variations are extracted and filtered as proxies for motion. Furthermore, the dominant orientation constancy from the Riesz transform is exploited to average the micro-expression sequence into an image pair. Based on that, the Mean Oriented Riesz Feature description is introduced. Finally the performance of our methods are tested in two spontaneous micro-expressions databases and compared to state-of-the-art methods.



### On the uncertainty of self-supervised monocular depth estimation
- **Arxiv ID**: http://arxiv.org/abs/2005.06209v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.06209v1)
- **Published**: 2020-05-13 09:00:55+00:00
- **Updated**: 2020-05-13 09:00:55+00:00
- **Authors**: Matteo Poggi, Filippo Aleotti, Fabio Tosi, Stefano Mattoccia
- **Comment**: CVPR 2020. Code will be available
  https://github.com/mattpoggi/mono-uncertainty
- **Journal**: None
- **Summary**: Self-supervised paradigms for monocular depth estimation are very appealing since they do not require ground truth annotations at all. Despite the astonishing results yielded by such methodologies, learning to reason about the uncertainty of the estimated depth maps is of paramount importance for practical applications, yet uncharted in the literature. Purposely, we explore for the first time how to estimate the uncertainty for this task and how this affects depth accuracy, proposing a novel peculiar technique specifically designed for self-supervised approaches. On the standard KITTI dataset, we exhaustively assess the performance of each method with different self-supervised paradigms. Such evaluation highlights that our proposal i) always improves depth accuracy significantly and ii) yields state-of-the-art results concerning uncertainty estimation when training on sequences and competitive results uniquely deploying stereo pairs.



### DAugNet: Unsupervised, Multi-source, Multi-target, and Life-long Domain Adaptation for Semantic Segmentation of Satellite Images
- **Arxiv ID**: http://arxiv.org/abs/2005.06216v2
- **DOI**: 10.1109/TGRS.2020.3006161
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.06216v2)
- **Published**: 2020-05-13 09:11:22+00:00
- **Updated**: 2020-06-07 11:48:20+00:00
- **Authors**: Onur Tasar, Alain Giros, Yuliya Tarabalka, Pierre Alliez, Sébastien Clerc
- **Comment**: None
- **Journal**: None
- **Summary**: The domain adaptation of satellite images has recently gained an increasing attention to overcome the limited generalization abilities of machine learning models when segmenting large-scale satellite images. Most of the existing approaches seek for adapting the model from one domain to another. However, such single-source and single-target setting prevents the methods from being scalable solutions, since nowadays multiple source and target domains having different data distributions are usually available. Besides, the continuous proliferation of satellite images necessitates the classifiers to adapt to continuously increasing data. We propose a novel approach, coined DAugNet, for unsupervised, multi-source, multi-target, and life-long domain adaptation of satellite images. It consists of a classifier and a data augmentor. The data augmentor, which is a shallow network, is able to perform style transfer between multiple satellite images in an unsupervised manner, even when new data are added over the time. In each training iteration, it provides the classifier with diversified data, which makes the classifier robust to large data distribution difference between the domains. Our extensive experiments prove that DAugNet significantly better generalizes to new geographic locations than the existing approaches.



### Mitigating Gender Bias Amplification in Distribution by Posterior Regularization
- **Arxiv ID**: http://arxiv.org/abs/2005.06251v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.06251v1)
- **Published**: 2020-05-13 11:07:10+00:00
- **Updated**: 2020-05-13 11:07:10+00:00
- **Authors**: Shengyu Jia, Tao Meng, Jieyu Zhao, Kai-Wei Chang
- **Comment**: 7 pages, 3 figures, published in ACL 2020
- **Journal**: None
- **Summary**: Advanced machine learning techniques have boosted the performance of natural language processing. Nevertheless, recent studies, e.g., Zhao et al. (2017) show that these techniques inadvertently capture the societal bias hidden in the corpus and further amplify it. However, their analysis is conducted only on models' top predictions. In this paper, we investigate the gender bias amplification issue from the distribution perspective and demonstrate that the bias is amplified in the view of predicted probability distribution over labels. We further propose a bias mitigation approach based on posterior regularization. With little performance loss, our method can almost remove the bias amplification in the distribution. Our study sheds the light on understanding the bias amplification.



### Pose Proposal Critic: Robust Pose Refinement by Learning Reprojection Errors
- **Arxiv ID**: http://arxiv.org/abs/2005.06262v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.06262v2)
- **Published**: 2020-05-13 11:46:04+00:00
- **Updated**: 2020-05-14 10:41:36+00:00
- **Authors**: Lucas Brynte, Fredrik Kahl
- **Comment**: Added acknowledgements
- **Journal**: None
- **Summary**: In recent years, considerable progress has been made for the task of rigid object pose estimation from a single RGB-image, but achieving robustness to partial occlusions remains a challenging problem. Pose refinement via rendering has shown promise in order to achieve improved results, in particular, when data is scarce.   In this paper we focus our attention on pose refinement, and show how to push the state-of-the-art further in the case of partial occlusions. The proposed pose refinement method leverages on a simplified learning task, where a CNN is trained to estimate the reprojection error between an observed and a rendered image. We experiment by training on purely synthetic data as well as a mixture of synthetic and real data. Current state-of-the-art results are outperformed for two out of three metrics on the Occlusion LINEMOD benchmark, while performing on-par for the final metric.



### Towards Better Graph Representation: Two-Branch Collaborative Graph Neural Networks for Multimodal Marketing Intention Detection
- **Arxiv ID**: http://arxiv.org/abs/2005.08706v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.08706v3)
- **Published**: 2020-05-13 12:58:01+00:00
- **Updated**: 2021-03-31 06:16:54+00:00
- **Authors**: Lu Zhang, Jian Zhang, Zhibin Li, Jingsong Xu
- **Comment**: Accepted by ICME2020
- **Journal**: None
- **Summary**: Inspired by the fact that spreading and collecting information through the Internet becomes the norm, more and more people choose to post for-profit contents (images and texts) in social networks. Due to the difficulty of network censors, malicious marketing may be capable of harming society. Therefore, it is meaningful to detect marketing intentions online automatically. However, gaps between multimodal data make it difficult to fuse images and texts for content marketing detection. To this end, this paper proposes Two-Branch Collaborative Graph Neural Networks to collaboratively represent multimodal data by Graph Convolution Networks (GCNs) in an end-to-end fashion. Experimental results demonstrate that our proposed method achieves superior graph classification performance for marketing intention detection.



### A Biologically Inspired Feature Enhancement Framework for Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2005.08704v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.08704v1)
- **Published**: 2020-05-13 13:25:22+00:00
- **Updated**: 2020-05-13 13:25:22+00:00
- **Authors**: Zhongwu Xie, Weipeng Cao, Xizhao Wang, Zhong Ming, Jingjing Zhang, Jiyong Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Most of the Zero-Shot Learning (ZSL) algorithms currently use pre-trained models as their feature extractors, which are usually trained on the ImageNet data set by using deep neural networks. The richness of the feature information embedded in the pre-trained models can help the ZSL model extract more useful features from its limited training samples. However, sometimes the difference between the training data set of the current ZSL task and the ImageNet data set is too large, which may lead to the use of pre-trained models has no obvious help or even negative impact on the performance of the ZSL model. To solve this problem, this paper proposes a biologically inspired feature enhancement framework for ZSL. Specifically, we design a dual-channel learning framework that uses auxiliary data sets to enhance the feature extractor of the ZSL model and propose a novel method to guide the selection of the auxiliary data sets based on the knowledge of biological taxonomy. Extensive experimental results show that our proposed method can effectively improve the generalization ability of the ZSL model and achieve state-of-the-art results on three benchmark ZSL tasks. We also explained the experimental phenomena through the way of feature visualization.



### Binarizing MobileNet via Evolution-based Searching
- **Arxiv ID**: http://arxiv.org/abs/2005.06305v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2005.06305v2)
- **Published**: 2020-05-13 13:25:51+00:00
- **Updated**: 2020-05-15 15:48:58+00:00
- **Authors**: Hai Phan, Zechun Liu, Dang Huynh, Marios Savvides, Kwang-Ting Cheng, Zhiqiang Shen
- **Comment**: Accepted by CVPR2020
- **Journal**: None
- **Summary**: Binary Neural Networks (BNNs), known to be one among the effectively compact network architectures, have achieved great outcomes in the visual tasks. Designing efficient binary architectures is not trivial due to the binary nature of the network. In this paper, we propose a use of evolutionary search to facilitate the construction and training scheme when binarizing MobileNet, a compact network with separable depth-wise convolution. Inspired by one-shot architecture search frameworks, we manipulate the idea of group convolution to design efficient 1-Bit Convolutional Neural Networks (CNNs), assuming an approximately optimal trade-off between computational cost and model accuracy. Our objective is to come up with a tiny yet efficient binary neural architecture by exploring the best candidates of the group convolution while optimizing the model performance in terms of complexity and latency. The approach is threefold. First, we train strong baseline binary networks with a wide range of random group combinations at each convolutional layer. This set-up gives the binary neural networks a capability of preserving essential information through layers. Second, to find a good set of hyperparameters for group convolutions we make use of the evolutionary search which leverages the exploration of efficient 1-bit models. Lastly, these binary models are trained from scratch in a usual manner to achieve the final binary model. Various experiments on ImageNet are conducted to show that following our construction guideline, the final model achieves 60.09% Top-1 accuracy and outperforms the state-of-the-art CI-BCNN with the same computational cost.



### Multi-modal Embedding Fusion-based Recommender
- **Arxiv ID**: http://arxiv.org/abs/2005.06331v2
- **DOI**: 10.3390/electronics11091391
- **Categories**: **cs.IR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.06331v2)
- **Published**: 2020-05-13 14:13:35+00:00
- **Updated**: 2020-05-14 11:45:22+00:00
- **Authors**: Anna Wroblewska, Jacek Dabrowski, Michal Pastuszak, Andrzej Michalowski, Michal Daniluk, Barbara Rychalska, Mikolaj Wieczorek, Sylwia Sysko-Romanczuk
- **Comment**: 7 pages, 8 figures
- **Journal**: revised and improved version: Electronics MDPI -
  https://www.mdpi.com/2079-9292/11/9/1391
- **Summary**: Recommendation systems have lately been popularized globally, with primary use cases in online interaction systems, with significant focus on e-commerce platforms. We have developed a machine learning-based recommendation platform, which can be easily applied to almost any items and/or actions domain. Contrary to existing recommendation systems, our platform supports multiple types of interaction data with multiple modalities of metadata natively. This is achieved through multi-modal fusion of various data representations. We deployed the platform into multiple e-commerce stores of different kinds, e.g. food and beverages, shoes, fashion items, telecom operators. Here, we present our system, its flexibility and performance. We also show benchmark results on open datasets, that significantly outperform state-of-the-art prior work.



### Neural Architecture Search for Gliomas Segmentation on Multimodal Magnetic Resonance Imaging
- **Arxiv ID**: http://arxiv.org/abs/2005.06338v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.06338v2)
- **Published**: 2020-05-13 14:32:00+00:00
- **Updated**: 2020-05-20 06:00:43+00:00
- **Authors**: Feifan Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Past few years have witnessed the artificial intelligence inspired evolution in various medical fields. The diagnosis and treatment of gliomas -- one of the most commonly seen brain tumors with low survival rate -- rely heavily on the computer assisted segmentation process undertaken on the magnetic resonance imaging (MRI) scans. Although the encoder-decoder shaped deep learning networks have been the de facto standard style for semantic segmentation tasks in medical imaging analysis, enormous effort is still required to be spent on designing the detailed architecture of the down-sampling and up-sampling blocks. In this work, we propose a neural architecture search (NAS) based solution to brain tumor segmentation tasks on multimodal volumetric MRI scans. Three sets of candidate operations are composed respectively for three kinds of basic building blocks in which each operation is assigned with a specific probabilistic parameter to be learned. Through alternately updating the weights of operations and the other parameters in the network, the searching mechanism ends up with two optimal structures for the upward and downward blocks. Moreover, the developed solution also integrates normalization and patching strategies tailored for brain MRI processing. Extensive comparative experiments on the BraTS 2019 dataset demonstrate that the proposed algorithm not only could relieve the pressure of fabricating block architectures but also possesses competitive feasibility and scalability.



### Towards segmentation and spatial alignment of the human embryonic brain using deep learning for atlas-based registration
- **Arxiv ID**: http://arxiv.org/abs/2005.06368v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.06368v1)
- **Published**: 2020-05-13 15:23:44+00:00
- **Updated**: 2020-05-13 15:23:44+00:00
- **Authors**: Wietske A. P. Bastiaansen, Melek Rousian, Régine P. M. Steegers-Theunissen, Wiro J. Niessen, Anton Koning, Stefan Klein
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an unsupervised deep learning method for atlas based registration to achieve segmentation and spatial alignment of the embryonic brain in a single framework. Our approach consists of two sequential networks with a specifically designed loss function to address the challenges in 3D first trimester ultrasound. The first part learns the affine transformation and the second part learns the voxelwise nonrigid deformation between the target image and the atlas. We trained this network end-to-end and validated it against a ground truth on synthetic datasets designed to resemble the challenges present in 3D first trimester ultrasound. The method was tested on a dataset of human embryonic ultrasound volumes acquired at 9 weeks gestational age, which showed alignment of the brain in some cases and gave insight in open challenges for the proposed method. We conclude that our method is a promising approach towards fully automated spatial alignment and segmentation of embryonic brains in 3D ultrasound.



### Super-Resolution Domain Adaptation Networks for Semantic Segmentation via Pixel and Output Level Aligning
- **Arxiv ID**: http://arxiv.org/abs/2005.06382v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.06382v4)
- **Published**: 2020-05-13 15:48:41+00:00
- **Updated**: 2022-05-13 09:09:45+00:00
- **Authors**: Junfeng Wu, Zhenjie Tang, Congan Xu, Enhai Liu, Long Gao, Wenjun Yan
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, Unsupervised Domain Adaptation (UDA) has attracted increasing attention to address the domain shift problem in the semantic segmentation task. Although previous UDA methods have achieved promising performance, they still suffer from the distribution gaps between source and target domains, especially the resolution discrepany in the remote sensing images. To address this problem, this paper designs a novel end-to-end semantic segmentation network, namely Super-Resolution Domain Adaptation Network (SRDA-Net). SRDA-Net can simultaneously achieve the super-resolution task and the domain adaptation task, thus satisfying the requirement of semantic segmentation for remote sensing images which usually involve various resolution images. The proposed SRDA-Net includes three parts: a Super-Resolution and Segmentation (SRS) model which focuses on recovering high-resolution image and predicting segmentation map, a Pixel-level Domain Classifier (PDC) for determining which domain the pixel belongs to, and an Output-space Domain Classifier (ODC) for distinguishing which domain the pixel contribution is from. By jointly optimizing SRS with two classifiers, the proposed method can not only eliminates the resolution difference between source and target domains, but also improve the performance of the semantic segmentation task. Experimental results on two remote sensing datasets with different resolutions demonstrate that SRDA-Net performs favorably against some state-of-the-art methods in terms of accuracy and visual quality. Code and models are available at https://github.com/tangzhenjie/SRDA-Net.



### A global method to identify trees outside of closed-canopy forests with medium-resolution satellite imagery
- **Arxiv ID**: http://arxiv.org/abs/2005.08702v2
- **DOI**: 10.1080/01431161.2020.1841324
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.08702v2)
- **Published**: 2020-05-13 15:58:01+00:00
- **Updated**: 2020-07-24 13:56:50+00:00
- **Authors**: John Brandt, Fred Stolle
- **Comment**: None
- **Journal**: None
- **Summary**: Scattered trees outside of dense, closed-canopy forests are very important for carbon sequestration, supporting livelihoods, maintaining ecosystem integrity, and climate change adaptation and mitigation. In contrast to trees inside of closed-canopy forests, not much is known about the spatial extent and distribution of scattered trees at a global scale. Due to the cost of high-resolution satellite imagery, global monitoring systems rely on medium-resolution satellites to monitor land use. Here we present a globally consistent method to identify trees with canopy diameters greater than three meters with medium-resolution optical and radar imagery. Biweekly cloud-free, pan-sharpened 10 meter Sentinel-2 optical imagery and Sentinel-1 radar imagery are used to train a fully convolutional network, consisting of a convolutional gated recurrent unit layer and a feature pyramid attention layer. Tested across more than 215,000 Sentinel-1 and Sentinel-2 pixels distributed from -60 to +60 latitude, the proposed model exceeds 75% user's and producer's accuracy identifying trees in hectares with a low to medium density (less than 40%) of tree cover, and 95% user's and producer's accuracy in hectares with dense (greater than 40%) tree cover. The proposed method increases the accuracy of monitoring tree presence in areas with sparse and scattered tree cover (less than 40%) by as much as 20%, and reduces commission and omission error in mountainous and very cloudy regions by nearly half. When applied across large, heterogeneous landscapes, the results demonstrate potential to map trees in high detail and accuracy over diverse landscapes across the globe. This information is important for understanding current land cover and can be used to detect changes in land cover such as agroforestry, buffer zones around biological hotspots, and expansion or encroachment of forests.



### Subsampled Fourier Ptychography using Pretrained Invertible and Untrained Network Priors
- **Arxiv ID**: http://arxiv.org/abs/2005.07026v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.IR, cs.LG, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2005.07026v1)
- **Published**: 2020-05-13 16:13:01+00:00
- **Updated**: 2020-05-13 16:13:01+00:00
- **Authors**: Fahad Shamshad, Asif Hanif, Ali Ahmed
- **Comment**: Part of this work has been accepted in NeurIPS Deep Inverse Workshop,
  2019
- **Journal**: None
- **Summary**: Recently pretrained generative models have shown promising results for subsampled Fourier Ptychography (FP) in terms of quality of reconstruction for extremely low sampling rate and high noise. However, one of the significant drawbacks of these pretrained generative priors is their limited representation capabilities. Moreover, training these generative models requires access to a large number of fully-observed clean samples of a particular class of images like faces or digits that is prohibitive to obtain in the context of FP. In this paper, we propose to leverage the power of pretrained invertible and untrained generative models to mitigate the representation error issue and requirement of a large number of example images (for training generative models) respectively. Through extensive experiments, we demonstrate the effectiveness of proposed approaches in the context of FP for low sampling rates and high noise levels.



### FaR-GAN for One-Shot Face Reenactment
- **Arxiv ID**: http://arxiv.org/abs/2005.06402v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.06402v1)
- **Published**: 2020-05-13 16:15:37+00:00
- **Updated**: 2020-05-13 16:15:37+00:00
- **Authors**: Hanxiang Hao, Sriram Baireddy, Amy R. Reibman, Edward J. Delp
- **Comment**: This paper has been accepted to the AI for content creation workshop
  at CVPR 2020
- **Journal**: None
- **Summary**: Animating a static face image with target facial expressions and movements is important in the area of image editing and movie production. This face reenactment process is challenging due to the complex geometry and movement of human faces. Previous work usually requires a large set of images from the same person to model the appearance. In this paper, we present a one-shot face reenactment model, FaR-GAN, that takes only one face image of any given source identity and a target expression as input, and then produces a face image of the same source identity but with the target expression. The proposed method makes no assumptions about the source identity, facial expression, head pose, or even image background. We evaluate our method on the VoxCeleb1 dataset and show that our method is able to generate a higher quality face image than the compared methods.



### Dense-Caption Matching and Frame-Selection Gating for Temporal Localization in VideoQA
- **Arxiv ID**: http://arxiv.org/abs/2005.06409v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.06409v1)
- **Published**: 2020-05-13 16:35:27+00:00
- **Updated**: 2020-05-13 16:35:27+00:00
- **Authors**: Hyounghun Kim, Zineng Tang, Mohit Bansal
- **Comment**: ACL 2020 (11 pages)
- **Journal**: None
- **Summary**: Videos convey rich information. Dynamic spatio-temporal relationships between people/objects, and diverse multimodal events are present in a video clip. Hence, it is important to develop automated models that can accurately extract such information from videos. Answering questions on videos is one of the tasks which can evaluate such AI abilities. In this paper, we propose a video question answering model which effectively integrates multi-modal input sources and finds the temporally relevant information to answer questions. Specifically, we first employ dense image captions to help identify objects and their detailed salient regions and actions, and hence give the model useful extra information (in explicit textual format to allow easier matching) for answering questions. Moreover, our model is also comprised of dual-level attention (word/object and frame level), multi-head self/cross-integration for different sources (video and dense captions), and gates which pass more relevant information to the classifier. Finally, we also cast the frame selection problem as a multi-label classification task and introduce two loss functions, In-andOut Frame Score Margin (IOFSM) and Balanced Binary Cross-Entropy (BBCE), to better supervise the model with human importance annotations. We evaluate our model on the challenging TVQA dataset, where each of our model components provides significant gains, and our overall model outperforms the state-of-the-art by a large margin (74.09% versus 70.52%). We also present several word, object, and frame level visualization studies. Our code is publicly available at: https://github.com/hyounghk/VideoQADenseCapFrameGate-ACL2020



### Designing a Color Filter via Optimization of Vora-Value for Making a Camera more Colorimetric
- **Arxiv ID**: http://arxiv.org/abs/2005.06421v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.06421v1)
- **Published**: 2020-05-13 16:51:21+00:00
- **Updated**: 2020-05-13 16:51:21+00:00
- **Authors**: Yuteng Zhu, Graham D. Finlayson
- **Comment**: 6 pages, 6 figures, 1 table, conference
- **Journal**: None
- **Summary**: The Luther condition states that if the spectral sensitivity responses of a camera are a linear transform from the color matching functions of the human visual system, the camera is colorimetric. Previous work proposed to solve for a filter which, when placed in front of a camera, results in sensitivities that best satisfy the Luther condition. By construction, the prior art solves for a filter for a given set of human visual sensitivities, e.g. the XYZ color matching functions or the cone response functions. However, depending on the target spectral sensitivity set, a different optimal filter is found.   This paper begins with the observation that the cone fundamentals, XYZ color matching functions or any linear combination thereof span the same 3-dimensional subspace. Thus, we set out to solve for a filter that makes the vector space spanned by the filtered camera sensitivities as similar as possible to the space spanned by human vision sensors. We argue that the Vora-Value is a suitable way to measure subspace similarity and we develop an optimization method for finding a filter that maximizes the Vora-Value measure.   Experiments demonstrate that our new optimization leads to filtered camera sensitivities which have a significantly higher Vora-Value compared with antecedent methods.



### Multiple Attentional Pyramid Networks for Chinese Herbal Recognition
- **Arxiv ID**: http://arxiv.org/abs/2005.06423v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.06423v1)
- **Published**: 2020-05-13 16:55:01+00:00
- **Updated**: 2020-05-13 16:55:01+00:00
- **Authors**: Yingxue Xu, Guihua Wen, Yang Hu, Mingnan Luo, Dan Dai, Yishan Zhuang, Wendy Hall
- **Comment**: 14 pages, 8 figures
- **Journal**: None
- **Summary**: Chinese herbs play a critical role in Traditional Chinese Medicine. Due to different recognition granularity, they can be recognized accurately only by professionals with much experience. It is expected that they can be recognized automatically using new techniques like machine learning. However, there is no Chinese herbal image dataset available. Simultaneously, there is no machine learning method which can deal with Chinese herbal image recognition well. Therefore, this paper begins with building a new standard Chinese-Herbs dataset. Subsequently, a new Attentional Pyramid Networks (APN) for Chinese herbal recognition is proposed, where both novel competitive attention and spatial collaborative attention are proposed and then applied. APN can adaptively model Chinese herbal images with different feature scales. Finally, a new framework for Chinese herbal recognition is proposed as a new application of APN. Experiments are conducted on our constructed dataset and validate the effectiveness of our methods.



### Local Fiber Orientation from X-ray Region-of-Interest Computed Tomography of large Fiber Reinforced Composite Components
- **Arxiv ID**: http://arxiv.org/abs/2005.06431v1
- **DOI**: 10.1016/j.compscitech.2019.107786
- **Categories**: **eess.IV**, cs.CV, physics.ins-det
- **Links**: [PDF](http://arxiv.org/pdf/2005.06431v1)
- **Published**: 2020-05-13 17:08:39+00:00
- **Updated**: 2020-05-13 17:08:39+00:00
- **Authors**: Thomas Baranowski, Dascha Dobrovolskij, Kilian Dremel, Astrid Hölzing, Günter Lohfink, Katja Schladitz, Simon Zabler
- **Comment**: None
- **Journal**: Composites Science and Technology, 183: 107786, 2019
- **Summary**: The local fiber orientation is a micro-structural feature crucial for the mechanical properties of parts made from fiber reinforced polymers. It can be determined from micro-computed tomography data and subsequent quantitative analysis of the resulting 3D images. However, although being by nature non-destructive, this method so far has required to cut samples of a few millimeter edge length in order to achieve the high lateral resolution needed for the analysis. Here, we report on the successful combination of region-of-interest scanning with structure texture orientation analysis rendering the above described approach truly non-destructive. Several regions of interest in a large bearing part from the automotive industry made of fiber reinforced polymer are scanned and analyzed. Differences of these regions with respect to local fiber orientation are quantified. Moreover, consistency of the analysis based on scans at varying lateral resolutions is proved. Finally, measured and numerically simulated orientation tensors are compared for one of the regions.



### A Generative Model for Generic Light Field Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2005.06508v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.06508v2)
- **Published**: 2020-05-13 18:27:42+00:00
- **Updated**: 2020-06-17 17:10:11+00:00
- **Authors**: Paramanand Chandramouli, Kanchana Vaishnavi Gandikota, Andreas Goerlitz, Andreas Kolb, Michael Moeller
- **Comment**: None
- **Journal**: None
- **Summary**: Recently deep generative models have achieved impressive progress in modeling the distribution of training data. In this work, we present for the first time a generative model for 4D light field patches using variational autoencoders to capture the data distribution of light field patches. We develop a generative model conditioned on the central view of the light field and incorporate this as a prior in an energy minimization framework to address diverse light field reconstruction tasks. While pure learning-based approaches do achieve excellent results on each instance of such a problem, their applicability is limited to the specific observation model they have been trained on. On the contrary, our trained light field generative model can be incorporated as a prior into any model-based optimization approach and therefore extend to diverse reconstruction tasks including light field view synthesis, spatial-angular super resolution and reconstruction from coded projections. Our proposed method demonstrates good reconstruction, with performance approaching end-to-end trained networks, while outperforming traditional model-based approaches on both synthetic and real scenes. Furthermore, we show that our approach enables reliable light field recovery despite distortions in the input.



### Robust Visual Object Tracking with Two-Stream Residual Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/2005.06536v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.06536v1)
- **Published**: 2020-05-13 19:05:42+00:00
- **Updated**: 2020-05-13 19:05:42+00:00
- **Authors**: Ning Zhang, Jingen Liu, Ke Wang, Dan Zeng, Tao Mei
- **Comment**: None
- **Journal**: None
- **Summary**: The current deep learning based visual tracking approaches have been very successful by learning the target classification and/or estimation model from a large amount of supervised training data in offline mode. However, most of them can still fail in tracking objects due to some more challenging issues such as dense distractor objects, confusing background, motion blurs, and so on. Inspired by the human "visual tracking" capability which leverages motion cues to distinguish the target from the background, we propose a Two-Stream Residual Convolutional Network (TS-RCN) for visual tracking, which successfully exploits both appearance and motion features for model update. Our TS-RCN can be integrated with existing deep learning based visual trackers. To further improve the tracking performance, we adopt a "wider" residual network ResNeXt as its feature extraction backbone. To the best of our knowledge, TS-RCN is the first end-to-end trainable two-stream visual tracking system, which makes full use of both appearance and motion features of the target. We have extensively evaluated the TS-RCN on most widely used benchmark datasets including VOT2018, VOT2019, and GOT-10K. The experiment results have successfully demonstrated that our two-stream model can greatly outperform the appearance based tracker, and it also achieves state-of-the-art performance. The tracking system can run at up to 38.1 FPS.



### Detector-SegMentor Network for Skin Lesion Localization and Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2005.06550v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.06550v1)
- **Published**: 2020-05-13 19:41:27+00:00
- **Updated**: 2020-05-13 19:41:27+00:00
- **Authors**: Shreshth Saini, Divij Gupta, Anil Kumar Tiwari
- **Comment**: 9 pages, 7 figures, accepted at NCVPRIPG 2019
- **Journal**: None
- **Summary**: Melanoma is a life-threatening form of skin cancer when left undiagnosed at the early stages. Although there are more cases of non-melanoma cancer than melanoma cancer, melanoma cancer is more deadly. Early detection of melanoma is crucial for the timely diagnosis of melanoma cancer and prohibit its spread to distant body parts. Segmentation of skin lesion is a crucial step in the classification of melanoma cancer from the cancerous lesions in dermoscopic images. Manual segmentation of dermoscopic skin images is very time consuming and error-prone resulting in an urgent need for an intelligent and accurate algorithm. In this study, we propose a simple yet novel network-in-network convolution neural network(CNN) based approach for segmentation of the skin lesion. A Faster Region-based CNN (Faster RCNN) is used for preprocessing to predict bounding boxes of the lesions in the whole image which are subsequently cropped and fed into the segmentation network to obtain the lesion mask. The segmentation network is a combination of the UNet and Hourglass networks. We trained and evaluated our models on ISIC 2018 dataset and also cross-validated on PH\textsuperscript{2} and ISBI 2017 datasets. Our proposed method surpassed the state-of-the-art with Dice Similarity Coefficient of 0.915 and Accuracy 0.959 on ISIC 2018 dataset and Dice Similarity Coefficient of 0.947 and Accuracy 0.971 on ISBI 2017 dataset.



### Sustainable Recreational Fishing Using a Novel Electrical Muscle Stimulation (EMS) Lure and Ensemble Network Algorithm to Maximize Catch and Release Survivability
- **Arxiv ID**: http://arxiv.org/abs/2006.10125v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.10125v2)
- **Published**: 2020-05-13 20:35:33+00:00
- **Updated**: 2023-01-16 22:33:52+00:00
- **Authors**: Petteri Haverinen, Krithik Ramesh, Nathan Wang
- **Comment**: This was a high school hackathon project that although interesting,
  lacks sufficient rigor and data as a research paper
- **Journal**: None
- **Summary**: With 200-700 million anglers in the world, sportfishing is nearly five times more common than commercial trawling. Worldwide, hundreds of thousands of jobs are linked to the sportfishing industry, which generates billions of dollars for water-side communities and fisheries conservatories alike. However, the sheer popularity of recreational fishing poses threats to aquatic biodiversity that are hard to regulate. For example, as much as 25% of overfished populations can be traced to anglers. This alarming statistic is explained by the average catch and release mortality rate of 43%, which primarily results from hook-related injuries and careless out-of-water handling. The provisional-patented design proposed in this paper addresses both these problems separately First, a novel, electrical muscle stimulation based fishing lure is proposed as a harmless and low cost alternative to sharp hooks. Early prototypes show a constant electrical current of 90 mA applied through a 200g European perch's jaw can support a reeling tension of 2N - safely within the necessary ranges. Second, a fish-eye camera bob is designed to wirelessly relay underwater footage to a smartphone app, where an ensemble convolutional neural network automatically classifies the fish's species, estimates its length, and cross references with local and state fishing regulations (ie. minimum size, maximum bag limit, and catch season). This capability reduces overfishing by helping anglers avoid accidentally violating guidelines and eliminates the need to reel the fish in and expose it to negligent handling. IN conjunction, this cheap, lightweight, yet high-tech invention is a paradigm shift in preserving a world favorite pastime; while at the same time making recreational fishing more sustainable.



### Pedestrian Action Anticipation using Contextual Feature Fusion in Stacked RNNs
- **Arxiv ID**: http://arxiv.org/abs/2005.06582v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2005.06582v1)
- **Published**: 2020-05-13 20:59:37+00:00
- **Updated**: 2020-05-13 20:59:37+00:00
- **Authors**: Amir Rasouli, Iuliia Kotseruba, John K. Tsotsos
- **Comment**: This paper was accepted and presented at British Machine Vision
  Conference (BMVC) 2019
- **Journal**: None
- **Summary**: One of the major challenges for autonomous vehicles in urban environments is to understand and predict other road users' actions, in particular, pedestrians at the point of crossing. The common approach to solving this problem is to use the motion history of the agents to predict their future trajectories. However, pedestrians exhibit highly variable actions most of which cannot be understood without visual observation of the pedestrians themselves and their surroundings. To this end, we propose a solution for the problem of pedestrian action anticipation at the point of crossing. Our approach uses a novel stacked RNN architecture in which information collected from various sources, both scene dynamics and visual features, is gradually fused into the network at different levels of processing. We show, via extensive empirical evaluations, that the proposed algorithm achieves a higher prediction accuracy compared to alternative recurrent network architectures. We conduct experiments to investigate the impact of the length of observation, time to event and types of features on the performance of the proposed method. Finally, we demonstrate how different data fusion strategies impact prediction accuracy.



### Do Saliency Models Detect Odd-One-Out Targets? New Datasets and Evaluations
- **Arxiv ID**: http://arxiv.org/abs/2005.06583v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.06583v2)
- **Published**: 2020-05-13 20:59:53+00:00
- **Updated**: 2021-05-05 17:27:23+00:00
- **Authors**: Iuliia Kotseruba, Calden Wloka, Amir Rasouli, John K. Tsotsos
- **Comment**: Published in BMVC 2019. 14 pages, 5 figures
- **Journal**: None
- **Summary**: Recent advances in the field of saliency have concentrated on fixation prediction, with benchmarks reaching saturation. However, there is an extensive body of works in psychology and neuroscience that describe aspects of human visual attention that might not be adequately captured by current approaches. Here, we investigate singleton detection, which can be thought of as a canonical example of salience. We introduce two novel datasets, one with psychophysical patterns and one with natural odd-one-out stimuli. Using these datasets we demonstrate through extensive experimentation that nearly all saliency algorithms do not adequately respond to singleton targets in synthetic and natural images. Furthermore, we investigate the effect of training state-of-the-art CNN-based saliency models on these types of stimuli and conclude that the additional training data does not lead to a significant improvement of their ability to find odd-one-out targets. Datasets are available at http://data.nvision2.eecs.yorku.ca/P3O3/.



### Structured Query-Based Image Retrieval Using Scene Graphs
- **Arxiv ID**: http://arxiv.org/abs/2005.06653v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.06653v1)
- **Published**: 2020-05-13 22:40:32+00:00
- **Updated**: 2020-05-13 22:40:32+00:00
- **Authors**: Brigit Schroeder, Subarna Tripathi
- **Comment**: Accepted to Diagram Image Retrieval and Analysis (DIRA) Workshop at
  CVPR 2020
- **Journal**: None
- **Summary**: A structured query can capture the complexity of object interactions (e.g. 'woman rides motorcycle') unlike single objects (e.g. 'woman' or 'motorcycle'). Retrieval using structured queries therefore is much more useful than single object retrieval, but a much more challenging problem. In this paper we present a method which uses scene graph embeddings as the basis for an approach to image retrieval. We examine how visual relationships, derived from scene graphs, can be used as structured queries. The visual relationships are directed subgraphs of the scene graph with a subject and object as nodes connected by a predicate relationship. Notably, we are able to achieve high recall even on low to medium frequency objects found in the long-tailed COCO-Stuff dataset, and find that adding a visual relationship-inspired loss boosts our recall by 10% in the best case.



### Flexible Example-based Image Enhancement with Task Adaptive Global Feature Self-Guided Network
- **Arxiv ID**: http://arxiv.org/abs/2005.06654v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.06654v2)
- **Published**: 2020-05-13 22:45:07+00:00
- **Updated**: 2020-09-28 12:59:50+00:00
- **Authors**: Dario Kneubuehler, Shuhang Gu, Luc Van Gool, Radu Timofte
- **Comment**: None
- **Journal**: None
- **Summary**: We propose the first practical multitask image enhancement network, that is able to learn one-to-many and many-to-one image mappings. We show that our model outperforms the current state of the art in learning a single enhancement mapping, while having significantly fewer parameters than its competitors. Furthermore, the model achieves even higher performance on learning multiple mappings simultaneously, by taking advantage of shared representations. Our network is based on the recently proposed SGN architecture, with modifications targeted at incorporating global features and style adaption. Finally, we present an unpaired learning method for multitask image enhancement, that is based on generative adversarial networks (GANs).



### Exploiting Multi-Layer Grid Maps for Surround-View Semantic Segmentation of Sparse LiDAR Data
- **Arxiv ID**: http://arxiv.org/abs/2005.06667v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.06667v1)
- **Published**: 2020-05-13 23:50:34+00:00
- **Updated**: 2020-05-13 23:50:34+00:00
- **Authors**: Frank Bieder, Sascha Wirges, Johannes Janosovits, Sven Richter, Zheyuan Wang, Christoph Stiller
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we consider the transformation of laser range measurements into a top-view grid map representation to approach the task of LiDAR-only semantic segmentation. Since the recent publication of the SemanticKITTI data set, researchers are now able to study semantic segmentation of urban LiDAR sequences based on a reasonable amount of data. While other approaches propose to directly learn on the 3D point clouds, we are exploiting a grid map framework to extract relevant information and represent them by using multi-layer grid maps. This representation allows us to use well-studied deep learning architectures from the image domain to predict a dense semantic grid map using only the sparse input data of a single LiDAR scan. We compare single-layer and multi-layer approaches and demonstrate the benefit of a multi-layer grid map input. Since the grid map representation allows us to predict a dense, 360{\deg} semantic environment representation, we further develop a method to combine the semantic information from multiple scans and create dense ground truth grids. This method allows us to evaluate and compare the performance of our models not only based on grid cells with a detection, but on the full visible measurement range.



