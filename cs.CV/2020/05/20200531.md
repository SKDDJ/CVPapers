# Arxiv Papers in cs.CV on 2020-05-31
### Attribute-Induced Bias Eliminating for Transductive Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2006.00412v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.00412v1)
- **Published**: 2020-05-31 02:08:01+00:00
- **Updated**: 2020-05-31 02:08:01+00:00
- **Authors**: Hantao Yao, Shaobo Min, Yongdong Zhang, Changsheng Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Transductive Zero-shot learning (ZSL) targets to recognize the unseen categories by aligning the visual and semantic information in a joint embedding space. There exist four kinds of domain biases in Transductive ZSL, i.e., visual bias and semantic bias between two domains and two visual-semantic biases in respective seen and unseen domains, but existing work only focuses on the part of them, which leads to severe semantic ambiguity during the knowledge transfer. To solve the above problem, we propose a novel Attribute-Induced Bias Eliminating (AIBE) module for Transductive ZSL. Specifically, for the visual bias between two domains, the Mean-Teacher module is first leveraged to bridge the visual representation discrepancy between two domains with unsupervised learning and unlabelled images. Then, an attentional graph attribute embedding is proposed to reduce the semantic bias between seen and unseen categories, which utilizes the graph operation to capture the semantic relationship between categories. Besides, to reduce the semantic-visual bias in the seen domain, we align the visual center of each category, instead of the individual visual data point, with the corresponding semantic attributes, which further preserves the semantic relationship in the embedding space. Finally, for the semantic-visual bias in the unseen domain, an unseen semantic alignment constraint is designed to align visual and semantic space in an unsupervised manner. The evaluations on several benchmarks demonstrate the effectiveness of the proposed method, e.g., obtaining the 82.8%/75.5%, 97.1%/82.5%, and 73.2%/52.1% for Conventional/Generalized ZSL settings for CUB, AwA2, and SUN datasets, respectively.



### DC-UNet: Rethinking the U-Net Architecture with Dual Channel Efficient CNN for Medical Images Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2006.00414v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.00414v1)
- **Published**: 2020-05-31 02:23:55+00:00
- **Updated**: 2020-05-31 02:23:55+00:00
- **Authors**: Ange Lou, Shuyue Guan, Murray Loew
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, deep learning has become much more popular in computer vision area. The Convolution Neural Network (CNN) has brought a breakthrough in images segmentation areas, especially, for medical images. In this regard, U-Net is the predominant approach to medical image segmentation task. The U-Net not only performs well in segmenting multimodal medical images generally, but also in some tough cases of them. However, we found that the classical U-Net architecture has limitation in several aspects. Therefore, we applied modifications: 1) designed efficient CNN architecture to replace encoder and decoder, 2) applied residual module to replace skip connection between encoder and decoder to improve based on the-state-of-the-art U-Net model. Following these modifications, we designed a novel architecture--DC-UNet, as a potential successor to the U-Net architecture. We created a new effective CNN architecture and build the DC-UNet based on this CNN. We have evaluated our model on three datasets with tough cases and have obtained a relative improvement in performance of 2.90%, 1.49% and 11.42% respectively compared with classical U-Net. In addition, we used the Tanimoto similarity to replace the Jaccard similarity for gray-to-gray image comparisons.



### EBBINNOT: A Hardware Efficient Hybrid Event-Frame Tracker for Stationary Dynamic Vision Sensors
- **Arxiv ID**: http://arxiv.org/abs/2006.00422v4
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.00422v4)
- **Published**: 2020-05-31 03:01:35+00:00
- **Updated**: 2022-05-10 02:58:45+00:00
- **Authors**: Vivek Mohan, Deepak Singla, Tarun Pulluri, Andres Ussa, Pradeep Kumar Gopalakrishnan, Pao-Sheng Sun, Bharath Ramesh, Arindam Basu
- **Comment**: 16 pages, 13 figures
- **Journal**: None
- **Summary**: As an alternative sensing paradigm, dynamic vision sensors (DVS) have been recently explored to tackle scenarios where conventional sensors result in high data rate and processing time. This paper presents a hybrid event-frame approach for detecting and tracking objects recorded by a stationary neuromorphic sensor, thereby exploiting the sparse DVS output in a low-power setting for traffic monitoring. Specifically, we propose a hardware efficient processing pipeline that optimizes memory and computational needs that enable long-term battery powered usage for IoT applications. To exploit the background removal property of a static DVS, we propose an event-based binary image creation that signals presence or absence of events in a frame duration. This reduces memory requirement and enables usage of simple algorithms like median filtering and connected component labeling for denoise and region proposal respectively. To overcome the fragmentation issue, a YOLO inspired neural network based detector and classifier to merge fragmented region proposals has been proposed. Finally, a new overlap based tracker was implemented, exploiting overlap between detections and tracks is proposed with heuristics to overcome occlusion. The proposed pipeline is evaluated with more than 5 hours of traffic recording spanning three different locations on two different neuromorphic sensors (DVS and CeleX) and demonstrate similar performance. Compared to existing event-based feature trackers, our method provides similar accuracy while needing approx 6 times less computes. To the best of our knowledge, this is the first time a stationary DVS based traffic monitoring solution is extensively compared to simultaneously recorded RGB frame-based methods while showing tremendous promise by outperforming state-of-the-art deep learning solutions.



### Pseudo-Representation Labeling Semi-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2006.00429v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.00429v1)
- **Published**: 2020-05-31 03:55:41+00:00
- **Updated**: 2020-05-31 03:55:41+00:00
- **Authors**: Song-Bo Yang, Tian-li Yu
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, semi-supervised learning (SSL) has shown tremendous success in leveraging unlabeled data to improve the performance of deep learning models, which significantly reduces the demand for large amounts of labeled data. Many SSL techniques have been proposed and have shown promising performance on famous datasets such as ImageNet and CIFAR-10. However, some exiting techniques (especially data augmentation based) are not suitable for industrial applications empirically. Therefore, this work proposes the pseudo-representation labeling, a simple and flexible framework that utilizes pseudo-labeling techniques to iteratively label a small amount of unlabeled data and use them as training data. In addition, our framework is integrated with self-supervised representation learning such that the classifier gains benefits from representation learning of both labeled and unlabeled data. This framework can be implemented without being limited at the specific model structure, but a general technique to improve the existing model. Compared with the existing approaches, the pseudo-representation labeling is more intuitive and can effectively solve practical problems in the real world. Empirically, it outperforms the current state-of-the-art semi-supervised learning methods in industrial types of classification problems such as the WM-811K wafer map and the MIT-BIH Arrhythmia dataset.



### Quasi-conformal Geometry based Local Deformation Analysis of Lateral Cephalogram for Childhood OSA Classification
- **Arxiv ID**: http://arxiv.org/abs/2006.11408v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.11408v1)
- **Published**: 2020-05-31 04:14:38+00:00
- **Updated**: 2020-05-31 04:14:38+00:00
- **Authors**: Hei-Long Chan, Hoi-Man Yuen, Chun-Ting Au, Kate Ching-Ching Chan, Albert Martin Li, Lok-Ming Lui
- **Comment**: None
- **Journal**: None
- **Summary**: Craniofacial profile is one of the anatomical causes of obstructive sleep apnea(OSA). By medical research, cephalometry provides information on patients' skeletal structures and soft tissues. In this work, a novel approach to cephalometric analysis using quasi-conformal geometry based local deformation information was proposed for OSA classification. Our study was a retrospective analysis based on 60 case-control pairs with accessible lateral cephalometry and polysomnography (PSG) data. By using the quasi-conformal geometry to study the local deformation around 15 landmark points, and combining the results with three linear distances between landmark points, a total of 1218 information features were obtained per subject. A L2 norm based classification model was built. Under experiments, our proposed model achieves 92.5% testing accuracy.



### Fast Enhancement for Non-Uniform Illumination Images using Light-weight CNNs
- **Arxiv ID**: http://arxiv.org/abs/2006.00439v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.00439v1)
- **Published**: 2020-05-31 05:14:29+00:00
- **Updated**: 2020-05-31 05:14:29+00:00
- **Authors**: Feifan Lv, Bo Liu, Feng Lu
- **Comment**: 9 pages, 12 figures, 2 tables
- **Journal**: None
- **Summary**: This paper proposes a new light-weight convolutional neural network (5k parameters) for non-uniform illumination image enhancement to handle color, exposure, contrast, noise and artifacts, etc., simultaneously and effectively. More concretely, the input image is first enhanced using Retinex model from dual different aspects (enhancing under-exposure and suppressing over-exposure), respectively. Then, these two enhanced results and the original image are fused to obtain an image with satisfactory brightness, contrast and details. Finally, the extra noise and compression artifacts are removed to get the final result. To train this network, we propose a semi-supervised retouching solution and construct a new dataset (82k images) contains various scenes and light conditions. Our model can enhance 0.5 mega-pixel (like 600*800) images in real time (50 fps), which is faster than existing enhancement methods. Extensive experiments show that our solution is fast and effective to deal with non-uniform illumination images.



### Modified Segmentation Algorithm for Recognition of Older Geez Scripts Written on Vellum
- **Arxiv ID**: http://arxiv.org/abs/2006.00465v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/2006.00465v1)
- **Published**: 2020-05-31 08:16:27+00:00
- **Updated**: 2020-05-31 08:16:27+00:00
- **Authors**: Girma Negashe, Adane Mamuye
- **Comment**: 7 pages, 12 figures, AfricaNLP2020 Workshop
- **Journal**: None
- **Summary**: Recognition of handwritten document aims at transforming document images into a machine understandable format. Handwritten document recognition is the most challenging area in the field of pattern recognition. It becomes more complex when a document was written on vellum before hundreds of years, like older Geez scripts. In this study, we introduced a modified segmentation approach to recognize older Geez scripts. We used adaptive filtering for noise reduction, Isodata iterative global thresholding for document image binarization, modified bounding box projection to segment distinct strokes between Geez characters, numbers, and punctuation marks. SVM multiclass classifier scored 79.32% recognition accuracy with the modified segmentation algorithm.



### End-to-End Change Detection for High Resolution Drone Images with GAN Architecture
- **Arxiv ID**: http://arxiv.org/abs/2006.00467v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.00467v1)
- **Published**: 2020-05-31 08:19:11+00:00
- **Updated**: 2020-05-31 08:19:11+00:00
- **Authors**: Yura Zharkovsky, Ovadya Menadeva
- **Comment**: This paper will be presented at IMVC 2020
  (https://www.imvc.co.il/Program/GeneralProgram.aspx)
- **Journal**: None
- **Summary**: Monitoring large areas is presently feasible with high resolution drone cameras, as opposed to time-consuming and expensive ground surveys. In this work we reveal for the first time, the potential of using a state-of-the-art change detection GAN based algorithm with high resolution drone images for infrastructure inspection. We demonstrate this concept on solar panel installation. A deep learning, data-driven algorithm for identifying changes based on a change detection deep learning algorithm was proposed. We use the Conditional Adversarial Network approach to present a framework for change detection in images. The proposed network architecture is based on pix2pix GAN framework. Extensive experimental results have shown that our proposed approach outperforms the other state-of-the-art change detection methods.



### Exemplar-based Generative Facial Editing
- **Arxiv ID**: http://arxiv.org/abs/2006.00472v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.00472v1)
- **Published**: 2020-05-31 09:15:28+00:00
- **Updated**: 2020-05-31 09:15:28+00:00
- **Authors**: Jingtao Guo, Yi Liu, Zhenzhen Qian, Zuowei Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Image synthesis has witnessed substantial progress due to the increasing power of generative model. This paper we propose a novel generative approach for exemplar based facial editing in the form of the region inpainting. Our method first masks the facial editing region to eliminates the pixel constraints of the original image, then exemplar based facial editing can be achieved by learning the corresponding information from the reference image to complete the masked region. In additional, we impose the attribute labels constraint to model disentangled encodings in order to avoid undesired information being transferred from the exemplar to the original image editing region. Experimental results demonstrate our method can produce diverse and personalized face editing results and provide far more user control flexibility than nearly all existing methods.



### Face Authentication from Grayscale Coded Light Field
- **Arxiv ID**: http://arxiv.org/abs/2006.00473v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.00473v1)
- **Published**: 2020-05-31 09:21:17+00:00
- **Updated**: 2020-05-31 09:21:17+00:00
- **Authors**: Dana Weitzner, David Mendlovic, Raja Giryes
- **Comment**: To be published at ICIP 2020
- **Journal**: None
- **Summary**: Face verification is a fast-growing authentication tool for everyday systems, such as smartphones. While current 2D face recognition methods are very accurate, it has been suggested recently that one may wish to add a 3D sensor to such solutions to make them more reliable and robust to spoofing, e.g., using a 2D print of a person's face. Yet, this requires an additional relatively expensive depth sensor. To mitigate this, we propose a novel authentication system, based on slim grayscale coded light field imaging. We provide a reconstruction free fast anti-spoofing mechanism, working directly on the coded image. It is followed by a multi-view, multi-modal face verification network that given grayscale data together with a low-res depth map achieves competitive results to the RGB case. We demonstrate the effectiveness of our solution on a simulated 3D (RGBD) version of LFW, which will be made public, and a set of real faces acquired by a light field computational camera.



### A Review on End-To-End Methods for Brain Tumor Segmentation and Overall Survival Prediction
- **Arxiv ID**: http://arxiv.org/abs/2006.01632v1
- **DOI**: 10.32010/26166127
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.01632v1)
- **Published**: 2020-05-31 11:12:14+00:00
- **Updated**: 2020-05-31 11:12:14+00:00
- **Authors**: Snehal Rajput, Mehul S Raval
- **Comment**: 22 pages. Azerbaijan Journal for High Performance Computing, 2020
- **Journal**: None
- **Summary**: Brain tumor segmentation intends to delineate tumor tissues from healthy brain tissues. The tumor tissues include necrosis, peritumoral edema, and active tumor. In contrast, healthy brain tissues include white matter, gray matter, and cerebrospinal fluid. The MRI based brain tumor segmentation research is gaining popularity as; 1. It does not irradiate ionized radiation like X-ray or computed tomography imaging. 2. It produces detailed pictures of internal body structures. The MRI scans are input to deep learning-based approaches which are useful for automatic brain tumor segmentation. The features from segments are fed to the classifier which predict the overall survival of the patient. The motive of this paper is to give an extensive overview of state-of-the-art jointly covering brain tumor segmentation and overall survival prediction.



### Multilevel Image Thresholding Using a Fully Informed Cuckoo Search Algorithm
- **Arxiv ID**: http://arxiv.org/abs/2006.09987v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.09987v1)
- **Published**: 2020-05-31 13:22:27+00:00
- **Updated**: 2020-05-31 13:22:27+00:00
- **Authors**: Xiaotao Huang, Liang Shen, Chongyi Fan, Jiahua zhu, Sixian Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Though effective in the segmentation, conventional multilevel thresholding methods are computationally expensive as exhaustive search are used for optimal thresholds to optimize the objective functions. To overcome this problem, population-based metaheuristic algorithms are widely used to improve the searching capacity. In this paper, we improve a popular metaheuristic called cuckoo search using a ring topology based fully informed strategy. In this strategy, each individual in the population learns from its neighborhoods to improve the cooperation of the population and the learning efficiency. Best solution or best fitness value can be obtained from the initial random threshold values, whose quality is evaluated by the correlation function. Experimental results have been examined on various numbers of thresholds. The results demonstrate that the proposed algorithm is more accurate and efficient than other four popular methods.



### Motion2Vec: Semi-Supervised Representation Learning from Surgical Videos
- **Arxiv ID**: http://arxiv.org/abs/2006.00545v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.00545v1)
- **Published**: 2020-05-31 15:46:01+00:00
- **Updated**: 2020-05-31 15:46:01+00:00
- **Authors**: Ajay Kumar Tanwani, Pierre Sermanet, Andy Yan, Raghav Anand, Mariano Phielipp, Ken Goldberg
- **Comment**: IEEE International Conference on Robotics and Automation (ICRA), 2020
- **Journal**: None
- **Summary**: Learning meaningful visual representations in an embedding space can facilitate generalization in downstream tasks such as action segmentation and imitation. In this paper, we learn a motion-centric representation of surgical video demonstrations by grouping them into action segments/sub-goals/options in a semi-supervised manner. We present Motion2Vec, an algorithm that learns a deep embedding feature space from video observations by minimizing a metric learning loss in a Siamese network: images from the same action segment are pulled together while pushed away from randomly sampled images of other segments, while respecting the temporal ordering of the images. The embeddings are iteratively segmented with a recurrent neural network for a given parametrization of the embedding space after pre-training the Siamese network. We only use a small set of labeled video segments to semantically align the embedding space and assign pseudo-labels to the remaining unlabeled data by inference on the learned model parameters. We demonstrate the use of this representation to imitate surgical suturing motions from publicly available videos of the JIGSAWS dataset. Results give 85.5 % segmentation accuracy on average suggesting performance improvement over several state-of-the-art baselines, while kinematic pose imitation gives 0.94 centimeter error in position per observation on the test set. Videos, code and data are available at https://sites.google.com/view/motion2vec



### A General-Purpose Dehazing Algorithm based on Local Contrast Enhancement Approaches
- **Arxiv ID**: http://arxiv.org/abs/2006.00568v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.00568v1)
- **Published**: 2020-05-31 17:25:22+00:00
- **Updated**: 2020-05-31 17:25:22+00:00
- **Authors**: Bangyong Sun, Vincent Whannou de Dravo, Zhe Yu
- **Comment**: We draw the attention of the reader that this is a work in progress
- **Journal**: None
- **Summary**: Dehazing is in the image processing and computer vision communities, the task of enhancing the image taken in foggy conditions. To better understand this type of algorithm, we present in this document a dehazing method which is suitable for several local contrast adjustment algorithms. We base it on two filters. The first filter is built with a step of normalization with some other statistical tricks while the last represents the local contrast improvement algorithm. Thus, it can work on both CPU and GPU for real-time applications. We hope that our approach will open the door to new ideas in the community. Other advantages of our method are first that it does not need to be trained, then it does not need additional optimization processing. Furthermore, it can be used as a pre-treatment or post-processing step in many vision tasks. In addition, it does not need to convert the problem into a physical interpretation, and finally that it is very fast. This family of defogging algorithms is fairly simple, but it shows promising results compared to state-of-the-art algorithms based not only on a visual assessment but also on objective criteria.



### Limited-angle CT reconstruction via the L1/L2 minimization
- **Arxiv ID**: http://arxiv.org/abs/2006.00601v4
- **DOI**: None
- **Categories**: **math.OC**, cs.CV, cs.NA, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/2006.00601v4)
- **Published**: 2020-05-31 20:22:30+00:00
- **Updated**: 2021-03-18 02:00:03+00:00
- **Authors**: Chao Wang, Min Tao, James Nagy, Yifei Lou
- **Comment**: 29 pages
- **Journal**: None
- **Summary**: In this paper, we consider minimizing the L1/L2 term on the gradient for a limited-angle scanning problem in computed tomography (CT) reconstruction. We design a specific splitting framework for an unconstrained optimization model so that the alternating direction method of multipliers (ADMM) has guaranteed convergence under certain conditions. In addition, we incorporate a box constraint that is reasonable for imaging applications, and the convergence for the additional box constraint can also be established. Numerical results on both synthetic and experimental datasets demonstrate the effectiveness and efficiency of our proposed approaches, showing significant improvements over the state-of-the-art methods in the limited-angle CT reconstruction.



### In the Eye of the Beholder: Gaze and Actions in First Person Video
- **Arxiv ID**: http://arxiv.org/abs/2006.00626v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.00626v2)
- **Published**: 2020-05-31 22:06:06+00:00
- **Updated**: 2020-10-31 05:00:32+00:00
- **Authors**: Yin Li, Miao Liu, James M. Rehg
- **Comment**: Submitted to TPAMI
- **Journal**: None
- **Summary**: We address the task of jointly determining what a person is doing and where they are looking based on the analysis of video captured by a headworn camera. To facilitate our research, we first introduce the EGTEA Gaze+ dataset. Our dataset comes with videos, gaze tracking data, hand masks and action annotations, thereby providing the most comprehensive benchmark for First Person Vision (FPV). Moving beyond the dataset, we propose a novel deep model for joint gaze estimation and action recognition in FPV. Our method describes the participant's gaze as a probabilistic variable and models its distribution using stochastic units in a deep network. We further sample from these stochastic units, generating an attention map to guide the aggregation of visual features for action recognition. Our method is evaluated on our EGTEA Gaze+ dataset and achieves a performance level that exceeds the state-of-the-art by a significant margin. More importantly, we demonstrate that our model can be applied to larger scale FPV dataset---EPIC-Kitchens even without using gaze, offering new state-of-the-art results on FPV action recognition.



### Pedestrian Tracking with Gated Recurrent Units and Attention Mechanisms
- **Arxiv ID**: http://arxiv.org/abs/2006.11407v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.11407v1)
- **Published**: 2020-05-31 23:58:43+00:00
- **Updated**: 2020-05-31 23:58:43+00:00
- **Authors**: Mahdi Elhousni, Xinming Huang
- **Comment**: Accepted by ISCAS2020
- **Journal**: None
- **Summary**: Pedestrian tracking has long been considered an important problem, especially in security applications. Previously,many approaches have been proposed with various types of sensors. One popular method is Pedestrian Dead Reckoning(PDR) [1] which is based on the inertial measurement unit(IMU) sensor. However PDR is an integration and threshold based method, which suffers from accumulation errors and low accuracy. In this paper, we propose a novel method in which the sensor data is fed into a deep learning model to predict the displacements and orientations of the pedestrian. We also devise a new apparatus to collect and construct databases containing synchronized IMU sensor data and precise locations measured by a LIDAR. The preliminary results are promising, and we plan to push this forward by collecting more data and adapting the deep learning model for all general pedestrian motions.



