# Arxiv Papers in cs.CV on 2020-05-11
### The Visual Social Distancing Problem
- **Arxiv ID**: http://arxiv.org/abs/2005.04813v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.04813v1)
- **Published**: 2020-05-11 00:04:34+00:00
- **Updated**: 2020-05-11 00:04:34+00:00
- **Authors**: Marco Cristani, Alessio Del Bue, Vittorio Murino, Francesco Setti, Alessandro Vinciarelli
- **Comment**: 9 pages, 5 figures. All the authors equally contributed to this
  manuscript and they are listed by alphabetical order. Under submission
- **Journal**: None
- **Summary**: One of the main and most effective measures to contain the recent viral outbreak is the maintenance of the so-called Social Distancing (SD). To comply with this constraint, workplaces, public institutions, transports and schools will likely adopt restrictions over the minimum inter-personal distance between people. Given this actual scenario, it is crucial to massively measure the compliance to such physical constraint in our life, in order to figure out the reasons of the possible breaks of such distance limitations, and understand if this implies a possible threat given the scene context. All of this, complying with privacy policies and making the measurement acceptable. To this end, we introduce the Visual Social Distancing (VSD) problem, defined as the automatic estimation of the inter-personal distance from an image, and the characterization of the related people aggregations. VSD is pivotal for a non-invasive analysis to whether people comply with the SD restriction, and to provide statistics about the level of safety of specific areas whenever this constraint is violated. We then discuss how VSD relates with previous literature in Social Signal Processing and indicate which existing Computer Vision methods can be used to manage such problem. We conclude with future challenges related to the effectiveness of VSD systems, ethical implications and future application scenarios.



### Learning Descriptors Invariance Through Equivalence Relations Within Manifold: A New Approach to Expression Invariant 3D Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2005.04823v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.04823v1)
- **Published**: 2020-05-11 01:23:39+00:00
- **Updated**: 2020-05-11 01:23:39+00:00
- **Authors**: Faisal R. Al-Osaimi
- **Comment**: This paper was submitted to pattern recognition in 2015
- **Journal**: None
- **Summary**: This paper presents a unique approach for the dichotomy between useful and adverse variations of key-point descriptors, namely the identity and the expression variations in the descriptor (feature) space. The descriptors variations are learned from training examples. Based on the labels of the training data, the equivalence relations among the descriptors are established. Both types of descriptor variations are represented by a graph embedded in the descriptor manifold. The invariant recognition is then conducted as a graph search problem. A heuristic graph search algorithm suitable for the recognition under this setup was devised. The proposed approach was tests on the FRGC v2.0, the Bosphorus and the 3D TEC datasets. It has shown to enhance the recognition performance, under expression variations in particular, by considerable margins.



### Non-iterative Simultaneous Rigid Registration Method for Serial Sections of Biological Tissue
- **Arxiv ID**: http://arxiv.org/abs/2005.04848v1
- **DOI**: 10.1109/ISBI.2018.8363610
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.04848v1)
- **Published**: 2020-05-11 03:44:10+00:00
- **Updated**: 2020-05-11 03:44:10+00:00
- **Authors**: Chang Shu, Xi Chen, Qiwei Xie, Chi Xiao, Hua Han
- **Comment**: appears in IEEE International Symposium on Biomedical Imaging 2018
  (ISBI 2018)
- **Journal**: 2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI
  2018), Washington, DC, 2018, pp. 436-440
- **Summary**: In this paper, we propose a novel non-iterative algorithm to simultaneously estimate optimal rigid transformation for serial section images, which is a key component in volume reconstruction of serial sections of biological tissue. In order to avoid error accumulation and propagation caused by current algorithms, we add extra condition that the position of the first and the last section images should remain unchanged. This constrained simultaneous registration problem has not been solved before. Our algorithm method is non-iterative, it can simultaneously compute rigid transformation for a large number of serial section images in a short time. We prove that our algorithm gets optimal solution under ideal condition. And we test our algorithm with synthetic data and real data to verify our algorithm's effectiveness.



### Scope Head for Accurate Localization in Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2005.04854v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.04854v2)
- **Published**: 2020-05-11 04:00:09+00:00
- **Updated**: 2020-05-12 02:07:38+00:00
- **Authors**: Geng Zhan, Dan Xu, Guo Lu, Wei Wu, Chunhua Shen, Wanli Ouyang
- **Comment**: None
- **Journal**: None
- **Summary**: Existing anchor-based and anchor-free object detectors in multi-stage or one-stage pipelines have achieved very promising detection performance. However, they still encounter the design difficulty in hand-crafted 2D anchor definition and the learning complexity in 1D direct location regression. To tackle these issues, in this paper, we propose a novel detector coined as ScopeNet, which models anchors of each location as a mutually dependent relationship. This approach quantizes the prediction space and employs a coarse-to-fine strategy for localization. It achieves superior flexibility as in the regression based anchor-free methods, while produces more precise prediction. Besides, an inherit anchor selection score is learned to indicate the localization quality of the detection result, and we propose to better represent the confidence of a detection box by combining the category-classification score and the anchor-selection score. With our concise and effective design, the proposed ScopeNet achieves state-of-the-art results on COCO



### Celeganser: Automated Analysis of Nematode Morphology and Age
- **Arxiv ID**: http://arxiv.org/abs/2005.04884v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.04884v1)
- **Published**: 2020-05-11 06:57:58+00:00
- **Updated**: 2020-05-11 06:57:58+00:00
- **Authors**: Linfeng Wang, Shu Kong, Zachary Pincus, Charless Fowlkes
- **Comment**: Computer Vision for Microscopy Image Analysis (CVMI) 2020
- **Journal**: None
- **Summary**: The nematode Caenorhabditis elegans (C. elegans) serves as an important model organism in a wide variety of biological studies. In this paper we introduce a pipeline for automated analysis of C. elegans imagery for the purpose of studying life-span, health-span and the underlying genetic determinants of aging. Our system detects and segments the worm, and predicts body coordinates at each pixel location inside the worm. These coordinates provide dense correspondence across individual animals to allow for meaningful comparative analysis. We show that a model pre-trained to perform body-coordinate regression extracts rich features that can be used to predict the age of individual worms with high accuracy. This lays the ground for future research in quantifying the relation between organs' physiologic and biochemical state, and individual life/health-span.



### Gleason Score Prediction using Deep Learning in Tissue Microarray Image
- **Arxiv ID**: http://arxiv.org/abs/2005.04886v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.04886v1)
- **Published**: 2020-05-11 07:00:42+00:00
- **Updated**: 2020-05-11 07:00:42+00:00
- **Authors**: Yi-hong Zhang, Jing Zhang, Yang Song, Chaomin Shen, Guang Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Prostate cancer (PCa) is one of the most common cancers in men around the world. The most accurate method to evaluate lesion levels of PCa is microscopic inspection of stained biopsy tissue and estimate the Gleason score of tissue microarray (TMA) image by expert pathologists. However, it is time-consuming for pathologists to identify the cellular and glandular patterns for Gleason grading in large TMA images. We used Gleason2019 Challenge dataset to build a convolutional neural network (CNN) model to segment TMA images to regions of different Gleason grades and predict the Gleason score according to the grading segmentation. We used a pre-trained model of prostate segmentation to increase the accuracy of the Gleason grade segmentation. The model achieved a mean Dice of 75.6% on the test cohort and ranked 4th in the Gleason2019 Challenge with a score of 0.778 combined of Cohen's kappa and the f1-score.



### An Inductive Transfer Learning Approach using Cycle-consistent Adversarial Domain Adaptation with Application to Brain Tumor Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2005.04906v1
- **DOI**: 10.1145/3375923.3375948
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.04906v1)
- **Published**: 2020-05-11 08:01:59+00:00
- **Updated**: 2020-05-11 08:01:59+00:00
- **Authors**: Yuta Tokuoka, Shuji Suzuki, Yohei Sugawara
- **Comment**: None
- **Journal**: Proceedings of the 2019 6th International Conference on Biomedical
  and Bioinformatics Engineering, November 2019, Pages 44-48
- **Summary**: With recent advances in supervised machine learning for medical image analysis applications, the annotated medical image datasets of various domains are being shared extensively. Given that the annotation labelling requires medical expertise, such labels should be applied to as many learning tasks as possible. However, the multi-modal nature of each annotated image renders it difficult to share the annotation label among diverse tasks. In this work, we provide an inductive transfer learning (ITL) approach to adopt the annotation label of the source domain datasets to tasks of the target domain datasets using Cycle-GAN based unsupervised domain adaptation (UDA). To evaluate the applicability of the ITL approach, we adopted the brain tissue annotation label on the source domain dataset of Magnetic Resonance Imaging (MRI) images to the task of brain tumor segmentation on the target domain dataset of MRI. The results confirm that the segmentation accuracy of brain tumor segmentation improved significantly. The proposed ITL approach can make significant contribution to the field of medical image analysis, as we develop a fundamental tool to improve and promote various tasks using medical images.



### Conditional Image Generation and Manipulation for User-Specified Content
- **Arxiv ID**: http://arxiv.org/abs/2005.04909v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.04909v1)
- **Published**: 2020-05-11 08:05:00+00:00
- **Updated**: 2020-05-11 08:05:00+00:00
- **Authors**: David Stap, Maurits Bleeker, Sarah Ibrahimi, Maartje ter Hoeve
- **Comment**: Accepted to the AI for content creation workshop at CVPR 2020
- **Journal**: None
- **Summary**: In recent years, Generative Adversarial Networks (GANs) have improved steadily towards generating increasingly impressive real-world images. It is useful to steer the image generation process for purposes such as content creation. This can be done by conditioning the model on additional information. However, when conditioning on additional information, there still exists a large set of images that agree with a particular conditioning. This makes it unlikely that the generated image is exactly as envisioned by a user, which is problematic for practical content creation scenarios such as generating facial composites or stock photos. To solve this problem, we propose a single pipeline for text-to-image generation and manipulation. In the first part of our pipeline we introduce textStyleGAN, a model that is conditioned on text. In the second part of our pipeline we make use of the pre-trained weights of textStyleGAN to perform semantic facial image manipulation. The approach works by finding semantic directions in latent space. We show that this method can be used to manipulate facial images for a wide range of attributes. Finally, we introduce the CelebTD-HQ dataset, an extension to CelebA-HQ, consisting of faces and corresponding textual descriptions.



### Learning to hash with semantic similarity metrics and empirical KL divergence
- **Arxiv ID**: http://arxiv.org/abs/2005.04917v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2005.04917v1)
- **Published**: 2020-05-11 08:20:26+00:00
- **Updated**: 2020-05-11 08:20:26+00:00
- **Authors**: Heikki Arponen, Tom E. Bishop
- **Comment**: 17 pages, 5 figures
- **Journal**: None
- **Summary**: Learning to hash is an efficient paradigm for exact and approximate nearest neighbor search from massive databases. Binary hash codes are typically extracted from an image by rounding output features from a CNN, which is trained on a supervised binary similar/ dissimilar task. Drawbacks of this approach are: (i) resulting codes do not necessarily capture semantic similarity of the input data (ii) rounding results in information loss, manifesting as decreased retrieval performance and (iii) Using only class-wise similarity as a target can lead to trivial solutions, simply encoding classifier outputs rather than learning more intricate relations, which is not detected by most performance metrics. We overcome (i) via a novel loss function encouraging the relative hash code distances of learned features to match those derived from their targets. We address (ii) via a differentiable estimate of the KL divergence between network outputs and a binary target distribution, resulting in minimal information loss when the features are rounded to binary. Finally, we resolve (iii) by focusing on a hierarchical precision metric. Efficiency of the methods is demonstrated with semantic image retrieval on the CIFAR-100, ImageNet and Conceptual Captions datasets, using similarities inferred from the WordNet label hierarchy or sentence embeddings.



### Fake face detection via adaptive manipulation traces extraction network
- **Arxiv ID**: http://arxiv.org/abs/2005.04945v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.04945v2)
- **Published**: 2020-05-11 09:16:39+00:00
- **Updated**: 2020-12-16 06:18:36+00:00
- **Authors**: Zhiqing Guo, Gaobo Yang, Jiyou Chen, Xingming Sun
- **Comment**: None
- **Journal**: None
- **Summary**: With the proliferation of face image manipulation (FIM) techniques such as Face2Face and Deepfake, more fake face images are spreading over the internet, which brings serious challenges to public confidence. Face image forgery detection has made considerable progresses in exposing specific FIM, but it is still in scarcity of a robust fake face detector to expose face image forgeries under complex scenarios such as with further compression, blurring, scaling, etc. Due to the relatively fixed structure, convolutional neural network (CNN) tends to learn image content representations. However, CNN should learn subtle manipulation traces for image forensics tasks. Thus, we propose an adaptive manipulation traces extraction network (AMTEN), which serves as pre-processing to suppress image content and highlight manipulation traces. AMTEN exploits an adaptive convolution layer to predict manipulation traces in the image, which are reused in subsequent layers to maximize manipulation artifacts by updating weights during the back-propagation pass. A fake face detector, namely AMTENnet, is constructed by integrating AMTEN with CNN. Experimental results prove that the proposed AMTEN achieves desirable pre-processing. When detecting fake face images generated by various FIM techniques, AMTENnet achieves an average accuracy up to 98.52%, which outperforms the state-of-the-art works. When detecting face images with unknown post-processing operations, the detector also achieves an average accuracy of 95.17%.



### Prototypical Contrastive Learning of Unsupervised Representations
- **Arxiv ID**: http://arxiv.org/abs/2005.04966v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.04966v5)
- **Published**: 2020-05-11 09:53:36+00:00
- **Updated**: 2021-03-30 04:07:54+00:00
- **Authors**: Junnan Li, Pan Zhou, Caiming Xiong, Steven C. H. Hoi
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents Prototypical Contrastive Learning (PCL), an unsupervised representation learning method that addresses the fundamental limitations of instance-wise contrastive learning. PCL not only learns low-level features for the task of instance discrimination, but more importantly, it implicitly encodes semantic structures of the data into the learned embedding space. Specifically, we introduce prototypes as latent variables to help find the maximum-likelihood estimation of the network parameters in an Expectation-Maximization framework. We iteratively perform E-step as finding the distribution of prototypes via clustering and M-step as optimizing the network via contrastive learning. We propose ProtoNCE loss, a generalized version of the InfoNCE loss for contrastive learning, which encourages representations to be closer to their assigned prototypes. PCL outperforms state-of-the-art instance-wise contrastive learning methods on multiple benchmarks with substantial improvement in low-resource transfer learning. Code and pretrained models are available at https://github.com/salesforce/PCL.



### Quantitative Analysis of Image Classification Techniques for Memory-Constrained Devices
- **Arxiv ID**: http://arxiv.org/abs/2005.04968v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, I.4.8; I.5.4; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2005.04968v4)
- **Published**: 2020-05-11 09:54:54+00:00
- **Updated**: 2020-11-15 15:36:42+00:00
- **Authors**: Sebastian Müksch, Theo Olausson, John Wilhelm, Pavlos Andreadis
- **Comment**: 9 pages, 4 figures, 7 tables
- **Journal**: None
- **Summary**: Convolutional Neural Networks, or CNNs, are the state of the art for image classification, but typically come at the cost of a large memory footprint. This limits their usefulness in applications relying on embedded devices, where memory is often a scarce resource. Recently, there has been significant progress in the field of image classification on such memory-constrained devices, with novel contributions like the ProtoNN, Bonsai and FastGRNN algorithms. These have been shown to reach up to 98.2% accuracy on optical character recognition using MNIST-10, with a memory footprint as little as 6KB. However, their potential on more complex multi-class and multi-channel image classification has yet to be determined. In this paper, we compare CNNs with ProtoNN, Bonsai and FastGRNN when applied to 3-channel image classification using CIFAR-10. For our analysis, we use the existing Direct Convolution algorithm to implement the CNNs memory-optimally and propose new methods of adjusting the FastGRNN model to work with multi-channel images. We extend the evaluation of each algorithm to a memory size budget of 8KB, 16KB, 32KB, 64KB and 128KB to show quantitatively that Direct Convolution CNNs perform best for all chosen budgets, with a top performance of 65.7% accuracy at a memory footprint of 58.23KB.



### Deep Reinforcement Learning for Organ Localization in CT
- **Arxiv ID**: http://arxiv.org/abs/2005.04974v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.04974v1)
- **Published**: 2020-05-11 10:06:13+00:00
- **Updated**: 2020-05-11 10:06:13+00:00
- **Authors**: Fernando Navarro, Anjany Sekuboyina, Diana Waldmannstetter, Jan C. Peeken, Stephanie E. Combs, Bjoern H. Menze
- **Comment**: Accepted paper in MIDL 2020
- **Journal**: https://openreview.net/forum?id=0vDeD2UD0S&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3DMIDL.io%2F2020%2FConference%2FAuthors%23your-submissions)
- **Summary**: Robust localization of organs in computed tomography scans is a constant pre-processing requirement for organ-specific image retrieval, radiotherapy planning, and interventional image analysis. In contrast to current solutions based on exhaustive search or region proposals, which require large amounts of annotated data, we propose a deep reinforcement learning approach for organ localization in CT. In this work, an artificial agent is actively self-taught to localize organs in CT by learning from its asserts and mistakes. Within the context of reinforcement learning, we propose a novel set of actions tailored for organ localization in CT. Our method can use as a plug-and-play module for localizing any organ of interest. We evaluate the proposed solution on the public VISCERAL dataset containing CT scans with varying fields of view and multiple organs. We achieved an overall intersection over union of 0.63, an absolute median wall distance of 2.25 mm, and a median distance between centroids of 3.65 mm.



### HiFaceGAN: Face Renovation via Collaborative Suppression and Replenishment
- **Arxiv ID**: http://arxiv.org/abs/2005.05005v2
- **DOI**: 10.1145/3394171.3413965
- **Categories**: **cs.CV**, I.3.3; I.4.4
- **Links**: [PDF](http://arxiv.org/pdf/2005.05005v2)
- **Published**: 2020-05-11 11:33:17+00:00
- **Updated**: 2021-05-22 12:12:36+00:00
- **Authors**: Lingbo Yang, Chang Liu, Pan Wang, Shanshe Wang, Peiran Ren, Siwei Ma, Wen Gao
- **Comment**: Published in ACM Multimedia 2020
- **Journal**: None
- **Summary**: Existing face restoration researches typically relies on either the degradation prior or explicit guidance labels for training, which often results in limited generalization ability over real-world images with heterogeneous degradations and rich background contents. In this paper, we investigate the more challenging and practical "dual-blind" version of the problem by lifting the requirements on both types of prior, termed as "Face Renovation"(FR). Specifically, we formulated FR as a semantic-guided generation problem and tackle it with a collaborative suppression and replenishment (CSR) approach. This leads to HiFaceGAN, a multi-stage framework containing several nested CSR units that progressively replenish facial details based on the hierarchical semantic guidance extracted from the front-end content-adaptive suppression modules. Extensive experiments on both synthetic and real face images have verified the superior performance of HiFaceGAN over a wide range of challenging restoration subtasks, demonstrating its versatility, robustness and generalization ability towards real-world face processing applications.



### Autonomous Tissue Scanning under Free-Form Motion for Intraoperative Tissue Characterisation
- **Arxiv ID**: http://arxiv.org/abs/2005.05050v3
- **DOI**: 10.1109/icra40945.2020.9197294
- **Categories**: **cs.RO**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.05050v3)
- **Published**: 2020-05-11 12:50:13+00:00
- **Updated**: 2020-05-22 12:37:53+00:00
- **Authors**: Jian Zhan, Joao Cartucho, Stamatia Giannarou
- **Comment**: 7 pages, 5 figures, ICRA 2020
- **Journal**: None
- **Summary**: In Minimally Invasive Surgery (MIS), tissue scanning with imaging probes is required for subsurface visualisation to characterise the state of the tissue. However, scanning of large tissue surfaces in the presence of deformation is a challenging task for the surgeon. Recently, robot-assisted local tissue scanning has been investigated for motion stabilisation of imaging probes to facilitate the capturing of good quality images and reduce the surgeon's cognitive load. Nonetheless, these approaches require the tissue surface to be static or deform with periodic motion. To eliminate these assumptions, we propose a visual servoing framework for autonomous tissue scanning, able to deal with free-form tissue deformation. The 3D structure of the surgical scene is recovered and a feature-based method is proposed to estimate the motion of the tissue in real-time. A desired scanning trajectory is manually defined on a reference frame and continuously updated using projective geometry to follow the tissue motion and control the movement of the robotic arm. The advantage of the proposed method is that it does not require the learning of the tissue motion prior to scanning and can deal with free-form deformation. We deployed this framework on the da Vinci surgical robot using the da Vinci Research Kit (dVRK) for Ultrasound tissue scanning. Since the framework does not rely on information from the Ultrasound data, it can be easily extended to other probe-based imaging modalities.



### A New Computer-Aided Diagnosis System with Modified Genetic Feature Selection for BI-RADS Classification of Breast Masses in Mammograms
- **Arxiv ID**: http://arxiv.org/abs/2005.05074v1
- **DOI**: 10.1155/2020/7695207
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.05074v1)
- **Published**: 2020-05-11 13:06:25+00:00
- **Updated**: 2020-05-11 13:06:25+00:00
- **Authors**: Said Boumaraf, Xiabi Liu, Chokri Ferkous, Xiaohong Ma
- **Comment**: None
- **Journal**: None
- **Summary**: Mammography remains the most prevalent imaging tool for early breast cancer screening. The language used to describe abnormalities in mammographic reports is based on the breast Imaging Reporting and Data System (BI-RADS). Assigning a correct BI-RADS category to each examined mammogram is a strenuous and challenging task for even experts. This paper proposes a new and effective computer-aided diagnosis (CAD) system to classify mammographic masses into four assessment categories in BI-RADS. The mass regions are first enhanced by means of histogram equalization and then semiautomatically segmented based on the region growing technique. A total of 130 handcrafted BI-RADS features are then extrcated from the shape, margin, and density of each mass, together with the mass size and the patient's age, as mentioned in BI-RADS mammography. Then, a modified feature selection method based on the genetic algorithm (GA) is proposed to select the most clinically significant BI-RADS features. Finally, a back-propagation neural network (BPN) is employed for classification, and its accuracy is used as the fitness in GA. A set of 500 mammogram images from the digital database of screening mammography (DDSM) is used for evaluation. Our system achieves classification accuracy, positive predictive value, negative predictive value, and Matthews correlation coefficient of 84.5%, 84.4%, 94.8%, and 79.3%, respectively. To our best knowledge, this is the best current result for BI-RADS classification of breast masses in mammography, which makes the proposed system promising to support radiologists for deciding proper patient management based on the automatically assigned BI-RADS categories.



### Fine-Grained Visual Classification with Efficient End-to-end Localization
- **Arxiv ID**: http://arxiv.org/abs/2005.05123v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.05123v1)
- **Published**: 2020-05-11 14:07:06+00:00
- **Updated**: 2020-05-11 14:07:06+00:00
- **Authors**: Harald Hanselmann, Hermann Ney
- **Comment**: None
- **Journal**: None
- **Summary**: The term fine-grained visual classification (FGVC) refers to classification tasks where the classes are very similar and the classification model needs to be able to find subtle differences to make the correct prediction. State-of-the-art approaches often include a localization step designed to help a classification network by localizing the relevant parts of the input images. However, this usually requires multiple iterations or passes through a full classification network or complex training schedules. In this work we present an efficient localization module that can be fused with a classification network in an end-to-end setup. On the one hand the module is trained by the gradient flowing back from the classification network. On the other hand, two self-supervised loss functions are introduced to increase the localization accuracy. We evaluate the new model on the three benchmark datasets CUB200-2011, Stanford Cars and FGVC-Aircraft and are able to achieve competitive recognition performance.



### FroDO: From Detections to 3D Objects
- **Arxiv ID**: http://arxiv.org/abs/2005.05125v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.05125v1)
- **Published**: 2020-05-11 14:08:29+00:00
- **Updated**: 2020-05-11 14:08:29+00:00
- **Authors**: Kejie Li, Martin Rünz, Meng Tang, Lingni Ma, Chen Kong, Tanner Schmidt, Ian Reid, Lourdes Agapito, Julian Straub, Steven Lovegrove, Richard Newcombe
- **Comment**: To be published in CVPR 2020. The first two authors contributed
  equally
- **Journal**: None
- **Summary**: Object-oriented maps are important for scene understanding since they jointly capture geometry and semantics, allow individual instantiation and meaningful reasoning about objects. We introduce FroDO, a method for accurate 3D reconstruction of object instances from RGB video that infers object location, pose and shape in a coarse-to-fine manner. Key to FroDO is to embed object shapes in a novel learnt space that allows seamless switching between sparse point cloud and dense DeepSDF decoding. Given an input sequence of localized RGB frames, FroDO first aggregates 2D detections to instantiate a category-aware 3D bounding box per object. A shape code is regressed using an encoder network before optimizing shape and pose further under the learnt shape priors using sparse and dense shape representations. The optimization uses multi-view geometric, photometric and silhouette losses. We evaluate on real-world datasets, including Pix3D, Redwood-OS, and ScanNet, for single-view, multi-view, and multi-object reconstruction.



### A Contrast-Adaptive Method for Simultaneous Whole-Brain and Lesion Segmentation in Multiple Sclerosis
- **Arxiv ID**: http://arxiv.org/abs/2005.05135v2
- **DOI**: 10.1016/j.neuroimage.2020.117471
- **Categories**: **eess.IV**, cs.CV, cs.LG, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2005.05135v2)
- **Published**: 2020-05-11 14:25:35+00:00
- **Updated**: 2020-10-16 14:57:36+00:00
- **Authors**: Stefano Cerri, Oula Puonti, Dominik S. Meier, Jens Wuerfel, Mark Mühlau, Hartwig R. Siebner, Koen Van Leemput
- **Comment**: None
- **Journal**: None
- **Summary**: Here we present a method for the simultaneous segmentation of white matter lesions and normal-appearing neuroanatomical structures from multi-contrast brain MRI scans of multiple sclerosis patients. The method integrates a novel model for white matter lesions into a previously validated generative model for whole-brain segmentation. By using separate models for the shape of anatomical structures and their appearance in MRI, the algorithm can adapt to data acquired with different scanners and imaging protocols without retraining. We validate the method using four disparate datasets, showing robust performance in white matter lesion segmentation while simultaneously segmenting dozens of other brain structures. We further demonstrate that the contrast-adaptive method can also be safely applied to MRI scans of healthy controls, and replicate previously documented atrophy patterns in deep gray matter structures in MS. The algorithm is publicly available as part of the open-source neuroimaging package FreeSurfer.



### Keep off the Grass: Permissible Driving Routes from Radar with Weak Audio Supervision
- **Arxiv ID**: http://arxiv.org/abs/2005.05175v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.05175v2)
- **Published**: 2020-05-11 15:11:20+00:00
- **Updated**: 2020-09-22 07:28:19+00:00
- **Authors**: David Williams, Daniele De Martini, Matthew Gadd, Letizia Marchegiani, Paul Newman
- **Comment**: accepted for publication at the IEEE Intelligent Transportation
  Systems Conference (ITSC) 2020
- **Journal**: None
- **Summary**: Reliable outdoor deployment of mobile robots requires the robust identification of permissible driving routes in a given environment. The performance of LiDAR and vision-based perception systems deteriorates significantly if certain environmental factors are present e.g. rain, fog, darkness. Perception systems based on FMCW scanning radar maintain full performance regardless of environmental conditions and with a longer range than alternative sensors. Learning to segment a radar scan based on driveability in a fully supervised manner is not feasible as labelling each radar scan on a bin-by-bin basis is both difficult and time-consuming to do by hand. We therefore weakly supervise the training of the radar-based classifier through an audio-based classifier that is able to predict the terrain type underneath the robot. By combining odometry, GPS and the terrain labels from the audio classifier, we are able to construct a terrain labelled trajectory of the robot in the environment which is then used to label the radar scans. Using a curriculum learning procedure, we then train a radar segmentation network to generalise beyond the initial labelling and to detect all permissible driving routes in the environment.



### Reference Pose Generation for Long-term Visual Localization via Learned Features and View Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2005.05179v4
- **DOI**: 10.1007/s11263-020-01399-8
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.05179v4)
- **Published**: 2020-05-11 15:13:07+00:00
- **Updated**: 2020-12-30 14:29:28+00:00
- **Authors**: Zichao Zhang, Torsten Sattler, Davide Scaramuzza
- **Comment**: 25 pages, 16 figures. Int J Comput Vis (2020)
- **Journal**: None
- **Summary**: Visual Localization is one of the key enabling technologies for autonomous driving and augmented reality. High quality datasets with accurate 6 Degree-of-Freedom (DoF) reference poses are the foundation for benchmarking and improving existing methods. Traditionally, reference poses have been obtained via Structure-from-Motion (SfM). However, SfM itself relies on local features which are prone to fail when images were taken under different conditions, e.g., day/ night changes. At the same time, manually annotating feature correspondences is not scalable and potentially inaccurate. In this work, we propose a semi-automated approach to generate reference poses based on feature matching between renderings of a 3D model and real images via learned features. Given an initial pose estimate, our approach iteratively refines the pose based on feature matches against a rendering of the model from the current pose estimate. We significantly improve the nighttime reference poses of the popular Aachen Day-Night dataset, showing that state-of-the-art visual localization methods perform better (up to $47\%$) than predicted by the original reference poses. We extend the dataset with new nighttime test images, provide uncertainty estimates for our new reference poses, and introduce a new evaluation criterion. We will make our reference poses and our framework publicly available upon publication.



### Reference-Based Sketch Image Colorization using Augmented-Self Reference and Dense Semantic Correspondence
- **Arxiv ID**: http://arxiv.org/abs/2005.05207v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.05207v1)
- **Published**: 2020-05-11 15:52:50+00:00
- **Updated**: 2020-05-11 15:52:50+00:00
- **Authors**: Junsoo Lee, Eungyeup Kim, Yunsung Lee, Dongjun Kim, Jaehyuk Chang, Jaegul Choo
- **Comment**: Accepted to CVPR 2020
- **Journal**: None
- **Summary**: This paper tackles the automatic colorization task of a sketch image given an already-colored reference image. Colorizing a sketch image is in high demand in comics, animation, and other content creation applications, but it suffers from information scarcity of a sketch image. To address this, a reference image can render the colorization process in a reliable and user-driven manner. However, it is difficult to prepare for a training data set that has a sufficient amount of semantically meaningful pairs of images as well as the ground truth for a colored image reflecting a given reference (e.g., coloring a sketch of an originally blue car given a reference green car). To tackle this challenge, we propose to utilize the identical image with geometric distortion as a virtual reference, which makes it possible to secure the ground truth for a colored output image. Furthermore, it naturally provides the ground truth for dense semantic correspondence, which we utilize in our internal attention mechanism for color transfer from reference to sketch input. We demonstrate the effectiveness of our approach in various types of sketch image colorization via quantitative as well as qualitative evaluation against existing methods.



### Medical Image Segmentation Using a U-Net type of Architecture
- **Arxiv ID**: http://arxiv.org/abs/2005.05218v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.05218v1)
- **Published**: 2020-05-11 16:10:18+00:00
- **Updated**: 2020-05-11 16:10:18+00:00
- **Authors**: Eshal Zahra, Bostan Ali, Wajahat Siddique
- **Comment**: 6 Pages
- **Journal**: None
- **Summary**: Deep convolutional neural networks have been proven to be very effective in image related analysis and tasks, such as image segmentation, image classification, image generation, etc. Recently many sophisticated CNN based architectures have been proposed for the purpose of image segmentation. Some of these newly designed networks are used for the specific purpose of medical image segmentation, models like V-Net, U-Net and their variants. It has been shown that U-Net produces very promising results in the domain of medical image segmentation.However, in this paper, we argue that the architecture of U-Net, when combined with a supervised training strategy at the bottleneck layer, can produce comparable results with the original U-Net architecture. More specifically, we introduce a fully supervised FC layers based pixel-wise loss at the bottleneck of the encoder branch of U-Net. The two layer based FC sub-net will train the bottleneck representation to contain more semantic information, which will be used by the decoder layers to predict the final segmentation map. The FC layer based sub-net is trained by employing the pixel-wise cross entropy loss, while the U-Net architectures trained by using L1 loss.



### iUNets: Fully invertible U-Nets with Learnable Up- and Downsampling
- **Arxiv ID**: http://arxiv.org/abs/2005.05220v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.05220v3)
- **Published**: 2020-05-11 16:14:13+00:00
- **Updated**: 2020-06-30 09:32:29+00:00
- **Authors**: Christian Etmann, Rihuan Ke, Carola-Bibiane Schönlieb
- **Comment**: None
- **Journal**: None
- **Summary**: U-Nets have been established as a standard architecture for image-to-image learning problems such as segmentation and inverse problems in imaging. For large-scale data, as it for example appears in 3D medical imaging, the U-Net however has prohibitive memory requirements. Here, we present a new fully-invertible U-Net-based architecture called the iUNet, which employs novel learnable and invertible up- and downsampling operations, thereby making the use of memory-efficient backpropagation possible. This allows us to train deeper and larger networks in practice, under the same GPU memory restrictions. Due to its invertibility, the iUNet can furthermore be used for constructing normalizing flows.



### On the Transferability of Winning Tickets in Non-Natural Image Datasets
- **Arxiv ID**: http://arxiv.org/abs/2005.05232v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.05232v2)
- **Published**: 2020-05-11 16:26:00+00:00
- **Updated**: 2020-11-20 12:41:28+00:00
- **Authors**: Matthia Sabatelli, Mike Kestemont, Pierre Geurts
- **Comment**: None
- **Journal**: None
- **Summary**: We study the generalization properties of pruned neural networks that are the winners of the lottery ticket hypothesis on datasets of natural images. We analyse their potential under conditions in which training data is scarce and comes from a non-natural domain. Specifically, we investigate whether pruned models that are found on the popular CIFAR-10/100 and Fashion-MNIST datasets, generalize to seven different datasets that come from the fields of digital pathology and digital heritage. Our results show that there are significant benefits in transferring and training sparse architectures over larger parametrized models, since in all of our experiments pruned networks, winners of the lottery ticket hypothesis, significantly outperform their larger unpruned counterparts. These results suggest that winning initializations do contain inductive biases that are generic to some extent, although, as reported by our experiments on the biomedical datasets, their generalization properties can be more limiting than what has been so far observed in the literature.



### Fundus2Angio: A Conditional GAN Architecture for Generating Fluorescein Angiography Images from Retinal Fundus Photography
- **Arxiv ID**: http://arxiv.org/abs/2005.05267v2
- **DOI**: 10.1007/978-3-030-64559-5_10
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.05267v2)
- **Published**: 2020-05-11 17:09:29+00:00
- **Updated**: 2020-09-29 08:46:05+00:00
- **Authors**: Sharif Amit Kamran, Khondker Fariha Hossain, Alireza Tavakkoli, Stewart Lee Zuckerbrod, Salah A. Baker, Kenton M. Sanders
- **Comment**: 14 pages, Accepted to 15th International Symposium on Visual
  Computing 2020
- **Journal**: None
- **Summary**: Carrying out clinical diagnosis of retinal vascular degeneration using Fluorescein Angiography (FA) is a time consuming process and can pose significant adverse effects on the patient. Angiography requires insertion of a dye that may cause severe adverse effects and can even be fatal. Currently, there are no non-invasive systems capable of generating Fluorescein Angiography images. However, retinal fundus photography is a non-invasive imaging technique that can be completed in a few seconds. In order to eliminate the need for FA, we propose a conditional generative adversarial network (GAN) to translate fundus images to FA images. The proposed GAN consists of a novel residual block capable of generating high quality FA images. These images are important tools in the differential diagnosis of retinal diseases without the need for invasive procedure with possible side effects. Our experiments show that the proposed architecture outperforms other state-of-the-art generative networks. Furthermore, our proposed model achieves better qualitative results indistinguishable from real angiograms.



### Deep-Learning-based Automated Palm Tree Counting and Geolocation in Large Farms from Aerial Geotagged Images
- **Arxiv ID**: http://arxiv.org/abs/2005.05269v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.05269v1)
- **Published**: 2020-05-11 17:11:49+00:00
- **Updated**: 2020-05-11 17:11:49+00:00
- **Authors**: Adel Ammar, Anis Koubaa
- **Comment**: First version of the paper, 3 pages, 2 figures
- **Journal**: None
- **Summary**: In this paper, we propose a deep learning framework for the automated counting and geolocation of palm trees from aerial images using convolutional neural networks. For this purpose, we collected aerial images in a palm tree Farm in the Kharj region, in Riyadh Saudi Arabia, using DJI drones, and we built a dataset of around 10,000 instances of palms trees. Then, we developed a convolutional neural network model using the state-of-the-art, Faster R-CNN algorithm. Furthermore, using the geotagged metadata of aerial images, we used photogrammetry concepts and distance corrections to detect the geographical location of detected palms trees automatically. This geolocation technique was tested on two different types of drones (DJI Mavic Pro, and Phantom 4 Pro), and was assessed to provide an average geolocation accuracy of 2.8m. This GPS tagging allows us to uniquely identify palm trees and count their number from a series of drone images, while correctly dealing with the issue of image overlapping. Moreover, it can be generalized to the geolocation of any other objects in UAV images.



### Normalized Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2005.05274v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.05274v3)
- **Published**: 2020-05-11 17:20:26+00:00
- **Updated**: 2020-05-18 10:19:32+00:00
- **Authors**: Dongsuk Kim, Geonhee Lee, Myungjae Lee, Shin Uk Kang, Dongmin Kim
- **Comment**: 6pages typo errors ,errata are fixed.(p1,2,4,5)
- **Journal**: None
- **Summary**: In this paper, we propose Normalized Convolutional Neural Network(NCNN). NCNN is more fitted to a convolutional operator than other nomralizaiton methods. The normalized process is similar to a normalization methods, but NCNN is more adapative to sliced-inputs and corresponding the convolutional kernel. Therefor NCNN can be targeted to micro-batch training. Normalizaing of NC is conducted during convolutional process. In short, NC process is not usual normalization and can not be realized in deep learning framework optimizing standard convolution process. Hence we named this method 'Normalized Convolution'. As a result, NC process has universal property which means NC can be applied to any AI tasks involving convolution neural layer . Since NC don't need other normalization layer, NCNN looks like convolutional version of Self Normalizing Network.(SNN). Among micro-batch trainings, NCNN outperforms other batch-independent normalization methods. NCNN archives these superiority by standardizing rows of im2col matrix of inputs, which theoretically smooths the gradient of loss. The code need to manipulate standard convolution neural networks step by step. The code is available : https://github.com/kimdongsuk1/ NormalizedCNN.



### Using Computer Vision to enhance Safety of Workforce in Manufacturing in a Post COVID World
- **Arxiv ID**: http://arxiv.org/abs/2005.05287v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.05287v2)
- **Published**: 2020-05-11 17:40:58+00:00
- **Updated**: 2020-05-25 12:16:12+00:00
- **Authors**: Prateek Khandelwal, Anuj Khandelwal, Snigdha Agarwal, Deep Thomas, Naveen Xavier, Arun Raghuraman
- **Comment**: 6 pages, 7 figure, 1 table
- **Journal**: None
- **Summary**: The COVID-19 pandemic forced governments across the world to impose lockdowns to prevent virus transmissions. This resulted in the shutdown of all economic activity and accordingly the production at manufacturing plants across most sectors was halted. While there is an urgency to resume production, there is an even greater need to ensure the safety of the workforce at the plant site. Reports indicate that maintaining social distancing and wearing face masks while at work clearly reduces the risk of transmission. We decided to use computer vision on CCTV feeds to monitor worker activity and detect violations which trigger real time voice alerts on the shop floor. This paper describes an efficient and economic approach of using AI to create a safe environment in a manufacturing setup. We demonstrate our approach to build a robust social distancing measurement algorithm using a mix of modern-day deep learning and classic projective geometry techniques. We have deployed our solution at manufacturing plants across the Aditya Birla Group (ABG). We have also described our face mask detection approach which provides a high accuracy across a range of customized masks.



### Adipose Tissue Segmentation in Unlabeled Abdomen MRI using Cross Modality Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2005.05761v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2005.05761v1)
- **Published**: 2020-05-11 17:41:39+00:00
- **Updated**: 2020-05-11 17:41:39+00:00
- **Authors**: Samira Masoudi, Syed M. Anwar, Stephanie A. Harmon, Peter L. Choyke, Baris Turkbey, Ulas Bagci
- **Comment**: 5 pages,7 figures, EMBC 2020 conference
- **Journal**: None
- **Summary**: Abdominal fat quantification is critical since multiple vital organs are located within this region. Although computed tomography (CT) is a highly sensitive modality to segment body fat, it involves ionizing radiations which makes magnetic resonance imaging (MRI) a preferable alternative for this purpose. Additionally, the superior soft tissue contrast in MRI could lead to more accurate results. Yet, it is highly labor intensive to segment fat in MRI scans. In this study, we propose an algorithm based on deep learning technique(s) to automatically quantify fat tissue from MR images through a cross modality adaptation. Our method does not require supervised labeling of MR scans, instead, we utilize a cycle generative adversarial network (C-GAN) to construct a pipeline that transforms the existing MR scans into their equivalent synthetic CT (s-CT) images where fat segmentation is relatively easier due to the descriptive nature of HU (hounsfield unit) in CT images. The fat segmentation results for MRI scans were evaluated by expert radiologist. Qualitative evaluation of our segmentation results shows average success score of 3.80/5 and 4.54/5 for visceral and subcutaneous fat segmentation in MR images.



### A Survey on Patch-based Synthesis: GPU Implementation and Optimization
- **Arxiv ID**: http://arxiv.org/abs/2005.06278v1
- **DOI**: 10.13140/RG.2.2.29490.86729/1
- **Categories**: **cs.GR**, cs.CV, eess.IV, A.1; I.3.2; I.3.3; I.3.4; I.3.8; I.2.6; I.2.8; I.2.10; H.5.1; I.5.1;
  I.5.4; I.5.5; J.5
- **Links**: [PDF](http://arxiv.org/pdf/2005.06278v1)
- **Published**: 2020-05-11 19:25:28+00:00
- **Updated**: 2020-05-11 19:25:28+00:00
- **Authors**: Hadi Abdi Khojasteh
- **Comment**: 117 pages, 38 figures, in Persian
- **Journal**: None
- **Summary**: This thesis surveys the research in patch-based synthesis and algorithms for finding correspondences between small local regions of images. We additionally explore a large kind of applications of this new fast randomized matching technique. One of the algorithms we have studied in particular is PatchMatch, can find similar regions or "patches" of an image one to two orders of magnitude faster than previous techniques. The algorithmic program is driven by applying mathematical properties of nearest neighbors in natural images. It is observed that neighboring correspondences tend to be similar or "coherent" and use this observation in algorithm in order to quickly converge to an approximate solution. The algorithm is the most general form can find k-nearest neighbor matching, using patches that translate, rotate, or scale, using arbitrary descriptors, and between two or more images. Speed-ups are obtained over various techniques in an exceeding range of those areas. We have explored many applications of PatchMatch matching algorithm. In computer graphics, we have explored removing unwanted objects from images, seamlessly moving objects in images, changing image aspect ratios, and video summarization. In computer vision we have explored denoising images, object detection, detecting image forgeries, and detecting symmetries. We conclude by discussing the restrictions of our algorithmic program, GPU implementation and areas for future analysis.



### MART: Memory-Augmented Recurrent Transformer for Coherent Video Paragraph Captioning
- **Arxiv ID**: http://arxiv.org/abs/2005.05402v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.05402v1)
- **Published**: 2020-05-11 20:01:41+00:00
- **Updated**: 2020-05-11 20:01:41+00:00
- **Authors**: Jie Lei, Liwei Wang, Yelong Shen, Dong Yu, Tamara L. Berg, Mohit Bansal
- **Comment**: ACL 2020 (12 pages)
- **Journal**: None
- **Summary**: Generating multi-sentence descriptions for videos is one of the most challenging captioning tasks due to its high requirements for not only visual relevance but also discourse-based coherence across the sentences in the paragraph. Towards this goal, we propose a new approach called Memory-Augmented Recurrent Transformer (MART), which uses a memory module to augment the transformer architecture. The memory module generates a highly summarized memory state from the video segments and the sentence history so as to help better prediction of the next sentence (w.r.t. coreference and repetition aspects), thus encouraging coherent paragraph generation. Extensive experiments, human evaluations, and qualitative analyses on two popular datasets ActivityNet Captions and YouCookII show that MART generates more coherent and less repetitive paragraph captions than baseline methods, while maintaining relevance to the input video events. All code is available open-source at: https://github.com/jayleicn/recurrent-transformer



### Identifying Mechanical Models through Differentiable Simulations
- **Arxiv ID**: http://arxiv.org/abs/2005.05410v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2005.05410v1)
- **Published**: 2020-05-11 20:19:20+00:00
- **Updated**: 2020-05-11 20:19:20+00:00
- **Authors**: Changkyu Song, Abdeslam Boularias
- **Comment**: to be published in Learning for DynamIcs & Control (L4DC), June
  10-11th, 2020
- **Journal**: None
- **Summary**: This paper proposes a new method for manipulating unknown objects through a sequence of non-prehensile actions that displace an object from its initial configuration to a given goal configuration on a flat surface. The proposed method leverages recent progress in differentiable physics models to identify unknown mechanical properties of manipulated objects, such as inertia matrix, friction coefficients and external forces acting on the object. To this end, a recently proposed differentiable physics engine for two-dimensional objects is adopted in this work and extended to deal forces in the three-dimensional space. The proposed model identification technique analytically computes the gradient of the distance between forecasted poses of objects and their actual observed poses and utilizes that gradient to search for values of the mechanical properties that reduce the reality gap. Experiments with real objects using a real robot to gather data show that the proposed approach can identify the mechanical properties of heterogeneous objects on the fly.



### Optimizing Vessel Trajectory Compression
- **Arxiv ID**: http://arxiv.org/abs/2005.05418v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.05418v1)
- **Published**: 2020-05-11 20:38:56+00:00
- **Updated**: 2020-05-11 20:38:56+00:00
- **Authors**: Giannis Fikioris, Kostas Patroumpas, Alexander Artikis
- **Comment**: None
- **Journal**: None
- **Summary**: In previous work we introduced a trajectory detection module that can provide summarized representations of vessel trajectories by consuming AIS positional messages online. This methodology can provide reliable trajectory synopses with little deviations from the original course by discarding at least 70% of the raw data as redundant. However, such trajectory compression is very sensitive to parametrization. In this paper, our goal is to fine-tune the selection of these parameter values. We take into account the type of each vessel in order to provide a suitable configuration that can yield improved trajectory synopses, both in terms of approximation error and compression ratio. Furthermore, we employ a genetic algorithm converging to a suitable configuration per vessel type. Our tests against a publicly available AIS dataset have shown that compression efficiency is comparable or even better than the one with default parametrization without resorting to a laborious data inspection.



### Deep Medical Image Analysis with Representation Learning and Neuromorphic Computing
- **Arxiv ID**: http://arxiv.org/abs/2005.05431v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.05431v1)
- **Published**: 2020-05-11 20:56:37+00:00
- **Updated**: 2020-05-11 20:56:37+00:00
- **Authors**: Neil Getty, Thomas Brettin, Dong Jin, Rick Stevens, Fangfang Xia
- **Comment**: 8 pages, 7 figures
- **Journal**: None
- **Summary**: We explore three representative lines of research and demonstrate the utility of our methods on a classification benchmark of brain cancer MRI data. First, we present a capsule network that explicitly learns a representation robust to rotation and affine transformation. This model requires less training data and outperforms both the original convolutional baseline and a previous capsule network implementation. Second, we leverage the latest domain adaptation techniques to achieve a new state-of-the-art accuracy. Our experiments show that non-medical images can be used to improve model performance. Finally, we design a spiking neural network trained on the Intel Loihi neuromorphic chip (Fig. 1 shows an inference snapshot). This model consumes much lower power while achieving reasonable accuracy given model reduction. We posit that more research in this direction combining hardware and learning advancements will power future medical imaging (on-device AI, few-shot prediction, adaptive scanning).



### Target-Independent Domain Adaptation for WBC Classification using Generative Latent Search
- **Arxiv ID**: http://arxiv.org/abs/2005.05432v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.05432v2)
- **Published**: 2020-05-11 20:58:23+00:00
- **Updated**: 2020-07-13 18:30:20+00:00
- **Authors**: Prashant Pandey, Prathosh AP, Vinay Kyatham, Deepak Mishra, Tathagato Rai Dastidar
- **Comment**: IEEE TMI 2020
- **Journal**: None
- **Summary**: Automating the classification of camera-obtained microscopic images of White Blood Cells (WBCs) and related cell subtypes has assumed importance since it aids the laborious manual process of review and diagnosis. Several State-Of-The-Art (SOTA) methods developed using Deep Convolutional Neural Networks suffer from the problem of domain shift - severe performance degradation when they are tested on data (target) obtained in a setting different from that of the training (source). The change in the target data might be caused by factors such as differences in camera/microscope types, lenses, lighting-conditions etc. This problem can potentially be solved using Unsupervised Domain Adaptation (UDA) techniques albeit standard algorithms presuppose the existence of a sufficient amount of unlabelled target data which is not always the case with medical images. In this paper, we propose a method for UDA that is devoid of the need for target data. Given a test image from the target data, we obtain its 'closest-clone' from the source data that is used as a proxy in the classifier. We prove the existence of such a clone given that infinite number of data points can be sampled from the source distribution. We propose a method in which a latent-variable generative model based on variational inference is used to simultaneously sample and find the 'closest-clone' from the source distribution through an optimization procedure in the latent space. We demonstrate the efficacy of the proposed method over several SOTA UDA methods for WBC classification on datasets captured using different imaging modalities under multiple settings.



### Online Monitoring for Neural Network Based Monocular Pedestrian Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2005.05451v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.05451v1)
- **Published**: 2020-05-11 21:40:41+00:00
- **Updated**: 2020-05-11 21:40:41+00:00
- **Authors**: Arjun Gupta, Luca Carlone
- **Comment**: Accepted to ITSC 2020
- **Journal**: None
- **Summary**: Several autonomy pipelines now have core components that rely on deep learning approaches. While these approaches work well in nominal conditions, they tend to have unexpected and severe failure modes that create concerns when used in safety-critical applications, including self-driving cars. There are several works that aim to characterize the robustness of networks offline, but currently there is a lack of tools to monitor the correctness of network outputs online during operation. We investigate the problem of online output monitoring for neural networks that estimate 3D human shapes and poses from images. Our first contribution is to present and evaluate model-based and learning-based monitors for a human-pose-and-shape reconstruction network, and assess their ability to predict the output loss for a given test input. As a second contribution, we introduce an Adversarially-Trained Online Monitor ( ATOM ) that learns how to effectively predict losses from data. ATOM dominates model-based baselines and can detect bad outputs, leading to substantial improvements in human pose output quality. Our final contribution is an extensive experimental evaluation that shows that discarding outputs flagged as incorrect by ATOM improves the average error by 12.5%, and the worst-case error by 126.5%.



### VIDIT: Virtual Image Dataset for Illumination Transfer
- **Arxiv ID**: http://arxiv.org/abs/2005.05460v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.05460v2)
- **Published**: 2020-05-11 21:58:03+00:00
- **Updated**: 2020-05-13 10:17:15+00:00
- **Authors**: Majed El Helou, Ruofan Zhou, Johan Barthas, Sabine Süsstrunk
- **Comment**: For further information and data, see
  https://github.com/majedelhelou/VIDIT
- **Journal**: None
- **Summary**: Deep image relighting is gaining more interest lately, as it allows photo enhancement through illumination-specific retouching without human effort. Aside from aesthetic enhancement and photo montage, image relighting is valuable for domain adaptation, whether to augment datasets for training or to normalize input test data. Accurate relighting is, however, very challenging for various reasons, such as the difficulty in removing and recasting shadows and the modeling of different surfaces. We present a novel dataset, the Virtual Image Dataset for Illumination Transfer (VIDIT), in an effort to create a reference evaluation benchmark and to push forward the development of illumination manipulation methods. Virtual datasets are not only an important step towards achieving real-image performance but have also proven capable of improving training even when real datasets are possible to acquire and available. VIDIT contains 300 virtual scenes used for training, where every scene is captured 40 times in total: from 8 equally-spaced azimuthal angles, each lit with 5 different illuminants.



### Combining Deep Learning with Geometric Features for Image based Localization in the Gastrointestinal Tract
- **Arxiv ID**: http://arxiv.org/abs/2005.05481v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.05481v2)
- **Published**: 2020-05-11 23:04:00+00:00
- **Updated**: 2020-05-13 19:25:49+00:00
- **Authors**: Jingwei Song, Mitesh Patel, Andreas Girgensohn, Chelhwon Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Tracking monocular colonoscope in the Gastrointestinal tract (GI) is a challenging problem as the images suffer from deformation, blurred textures, significant changes in appearance. They greatly restrict the tracking ability of conventional geometry based methods. Even though Deep Learning (DL) can overcome these issues, limited labeling data is a roadblock to state-of-art DL method. Considering these, we propose a novel approach to combine DL method with traditional feature based approach to achieve better localization with small training data. Our method fully exploits the best of both worlds by introducing a Siamese network structure to perform few-shot classification to the closest zone in the segmented training image set. The classified label is further adopted to initialize the pose of scope. To fully use the training dataset, a pre-generated triangulated map points within the zone in the training set are registered with observation and contribute to estimating the optimal pose of the test image. The proposed hybrid method is extensively tested and compared with existing methods, and the result shows significant improvement over traditional geometric based or DL based localization. The accuracy is improved by 28.94% (Position) and 10.97% (Orientation) with respect to state-of-art method.



### Monotone Boolean Functions, Feasibility/Infeasibility, LP-type problems and MaxCon
- **Arxiv ID**: http://arxiv.org/abs/2005.05490v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CG, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.05490v1)
- **Published**: 2020-05-11 23:51:15+00:00
- **Updated**: 2020-05-11 23:51:15+00:00
- **Authors**: David Suter, Ruwan Tennakoon, Erchuan Zhang, Tat-Jun Chin, Alireza Bab-Hadiashar
- **Comment**: Parts under conference review, work in progress. Keywords: Monotone
  Boolean Functions, Consensus Maximisation, LP-Type Problem, Computer Vision,
  Robust Fitting, Matroid, Simplicial Complex, Independence Systems
- **Journal**: None
- **Summary**: This paper outlines connections between Monotone Boolean Functions, LP-Type problems and the Maximum Consensus Problem. The latter refers to a particular type of robust fitting characterisation, popular in Computer Vision (MaxCon). Indeed, this is our main motivation but we believe the results of the study of these connections are more widely applicable to LP-type problems (at least 'thresholded versions', as we describe), and perhaps even more widely. We illustrate, with examples from Computer Vision, how the resulting perspectives suggest new algorithms. Indeed, we focus, in the experimental part, on how the Influence (a property of Boolean Functions that takes on a special form if the function is Monotone) can guide a search for the MaxCon solution.



