# Arxiv Papers in cs.CV on 2020-05-10
### A Robust Matching Pursuit Algorithm Using Information Theoretic Learning
- **Arxiv ID**: http://arxiv.org/abs/2005.04541v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.04541v1)
- **Published**: 2020-05-10 01:36:00+00:00
- **Updated**: 2020-05-10 01:36:00+00:00
- **Authors**: Miaohua Zhang, Yongsheng Gao, Changming Sun, Michael Blumenstein
- **Comment**: Accepted by "Pattern Recognition"
- **Journal**: None
- **Summary**: Current orthogonal matching pursuit (OMP) algorithms calculate the correlation between two vectors using the inner product operation and minimize the mean square error, which are both suboptimal when there are non-Gaussian noises or outliers in the observation data. To overcome these problems, a new OMP algorithm is developed based on the information theoretic learning (ITL), which is built on the following new techniques: (1) an ITL-based correlation (ITL-Correlation) is developed as a new similarity measure which can better exploit higher-order statistics of the data, and is robust against many different types of noise and outliers in a sparse representation framework; (2) a non-second order statistic measurement and minimization method is developed to improve the robustness of OMP by overcoming the limitation of Gaussianity inherent in cost function based on second-order moments. The experimental results on both simulated and real-world data consistently demonstrate the superiority of the proposed OMP algorithm in data recovery, image reconstruction, and classification.



### Epipolar Transformers
- **Arxiv ID**: http://arxiv.org/abs/2005.04551v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.04551v1)
- **Published**: 2020-05-10 02:22:54+00:00
- **Updated**: 2020-05-10 02:22:54+00:00
- **Authors**: Yihui He, Rui Yan, Katerina Fragkiadaki, Shoou-I Yu
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: A common approach to localize 3D human joints in a synchronized and calibrated multi-view setup consists of two-steps: (1) apply a 2D detector separately on each view to localize joints in 2D, and (2) perform robust triangulation on 2D detections from each view to acquire the 3D joint locations. However, in step 1, the 2D detector is limited to solving challenging cases which could potentially be better resolved in 3D, such as occlusions and oblique viewing angles, purely in 2D without leveraging any 3D information. Therefore, we propose the differentiable "epipolar transformer", which enables the 2D detector to leverage 3D-aware features to improve 2D pose estimation. The intuition is: given a 2D location p in the current view, we would like to first find its corresponding point p' in a neighboring view, and then combine the features at p' with the features at p, thus leading to a 3D-aware feature at p. Inspired by stereo matching, the epipolar transformer leverages epipolar constraints and feature matching to approximate the features at p'. Experiments on InterHand and Human3.6M show that our approach has consistent improvements over the baselines. Specifically, in the condition where no external data is used, our Human3.6M model trained with ResNet-50 backbone and image size 256 x 256 outperforms state-of-the-art by 4.23 mm and achieves MPJPE 26.9 mm.



### Compact Neural Representation Using Attentive Network Pruning
- **Arxiv ID**: http://arxiv.org/abs/2005.04559v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.04559v1)
- **Published**: 2020-05-10 03:20:01+00:00
- **Updated**: 2020-05-10 03:20:01+00:00
- **Authors**: Mahdi Biparva, John Tsotsos
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks have evolved to become power demanding and consequently difficult to apply to small-size mobile platforms. Network parameter reduction methods have been introduced to systematically deal with the computational and memory complexity of deep networks. We propose to examine the ability of attentive connection pruning to deal with redundancy reduction in neural networks as a contribution to the reduction of computational demand. In this work, we describe a Top-Down attention mechanism that is added to a Bottom-Up feedforward network to select important connections and subsequently prune redundant ones at all parametric layers. Our method not only introduces a novel hierarchical selection mechanism as the basis of pruning but also remains competitive with previous baseline methods in the experimental evaluation. We conduct experiments using different network architectures on popular benchmark datasets to show high compression ratio is achievable with negligible loss of accuracy.



### Efficient Privacy Preserving Edge Computing Framework for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2005.04563v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.04563v2)
- **Published**: 2020-05-10 03:36:32+00:00
- **Updated**: 2021-09-04 05:35:44+00:00
- **Authors**: Omobayode Fagbohungbe, Sheikh Rufsan Reza, Xishuang Dong, Lijun Qian
- **Comment**: None
- **Journal**: None
- **Summary**: In order to extract knowledge from the large data collected by edge devices, traditional cloud based approach that requires data upload may not be feasible due to communication bandwidth limitation as well as privacy and security concerns of end users. To address these challenges, a novel privacy preserving edge computing framework is proposed in this paper for image classification. Specifically, autoencoder will be trained unsupervised at each edge device individually, then the obtained latent vectors will be transmitted to the edge server for the training of a classifier. This framework would reduce the communications overhead and protect the data of the end users. Comparing to federated learning, the training of the classifier in the proposed framework does not subject to the constraints of the edge devices, and the autoencoder can be trained independently at each edge device without any server involvement. Furthermore, the privacy of the end users' data is protected by transmitting latent vectors without additional cost of encryption. Experimental results provide insights on the image classification performance vs. various design parameters such as the data compression ratio of the autoencoder and the model complexity.



### Class-Aware Domain Adaptation for Improving Adversarial Robustness
- **Arxiv ID**: http://arxiv.org/abs/2005.04564v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.04564v1)
- **Published**: 2020-05-10 03:45:19+00:00
- **Updated**: 2020-05-10 03:45:19+00:00
- **Authors**: Xianxu Hou, Jingxin Liu, Bolei Xu, Xiaolong Wang, Bozhi Liu, Guoping Qiu
- **Comment**: None
- **Journal**: None
- **Summary**: Recent works have demonstrated convolutional neural networks are vulnerable to adversarial examples, i.e., inputs to machine learning models that an attacker has intentionally designed to cause the models to make a mistake. To improve the adversarial robustness of neural networks, adversarial training has been proposed to train networks by injecting adversarial examples into the training data. However, adversarial training could overfit to a specific type of adversarial attack and also lead to standard accuracy drop on clean images. To this end, we propose a novel Class-Aware Domain Adaptation (CADA) method for adversarial defense without directly applying adversarial training. Specifically, we propose to learn domain-invariant features for adversarial examples and clean images via a domain discriminator. Furthermore, we introduce a class-aware component into the discriminator to increase the discriminative power of the network for adversarial examples. We evaluate our newly proposed approach using multiple benchmark datasets. The results demonstrate that our method can significantly improve the state-of-the-art of adversarial robustness for various attacks and maintain high performances on clean images.



### Non-recurrent Traffic Congestion Detection with a Coupled Scalable Bayesian Robust Tensor Factorization Model
- **Arxiv ID**: http://arxiv.org/abs/2005.04567v1
- **DOI**: None
- **Categories**: **physics.soc-ph**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.04567v1)
- **Published**: 2020-05-10 03:58:18+00:00
- **Updated**: 2020-05-10 03:58:18+00:00
- **Authors**: Qin Li, Huachun Tan, Xizhu Jiang, Yuankai Wu, Linhui Ye
- **Comment**: None
- **Journal**: None
- **Summary**: Non-recurrent traffic congestion (NRTC) usually brings unexpected delays to commuters. Hence, it is critical to accurately detect and recognize the NRTC in a real-time manner. The advancement of road traffic detectors and loop detectors provides researchers with a large-scale multivariable temporal-spatial traffic data, which allows the deep research on NRTC to be conducted. However, it remains a challenging task to construct an analytical framework through which the natural spatial-temporal structural properties of multivariable traffic information can be effectively represented and exploited to better understand and detect NRTC. In this paper, we present a novel analytical training-free framework based on coupled scalable Bayesian robust tensor factorization (Coupled SBRTF). The framework can couple multivariable traffic data including traffic flow, road speed, and occupancy through sharing a similar or the same sparse structure. And, it naturally captures the high-dimensional spatial-temporal structural properties of traffic data by tensor factorization. With its entries revealing the distribution and magnitude of NRTC, the shared sparse structure of the framework compasses sufficiently abundant information about NRTC. While the low-rank part of the framework, expresses the distribution of general expected traffic condition as an auxiliary product. Experimental results on real-world traffic data show that the proposed method outperforms coupled Bayesian robust principal component analysis (coupled BRPCA), the rank sparsity tensor decomposition (RSTD), and standard normal deviates (SND) in detecting NRTC. The proposed method performs even better when only traffic data in weekdays are utilized, and hence can provide more precise estimation of NRTC for daily commuters.



### A Survey on Deep Learning for Neuroimaging-based Brain Disorder Analysis
- **Arxiv ID**: http://arxiv.org/abs/2005.04573v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.04573v1)
- **Published**: 2020-05-10 04:20:50+00:00
- **Updated**: 2020-05-10 04:20:50+00:00
- **Authors**: Li Zhang, Mingliang Wang, Mingxia Liu, Daoqiang Zhang
- **Comment**: 30 pages, 7 figures
- **Journal**: None
- **Summary**: Deep learning has been recently used for the analysis of neuroimages, such as structural magnetic resonance imaging (MRI), functional MRI, and positron emission tomography (PET), and has achieved significant performance improvements over traditional machine learning in computer-aided diagnosis of brain disorders. This paper reviews the applications of deep learning methods for neuroimaging-based brain disorder analysis. We first provide a comprehensive overview of deep learning techniques and popular network architectures, by introducing various types of deep neural networks and recent developments. We then review deep learning methods for computer-aided analysis of four typical brain disorders, including Alzheimer's disease, Parkinson's disease, Autism spectrum disorder, and Schizophrenia, where the first two diseases are neurodegenerative disorders and the last two are neurodevelopmental and psychiatric disorders, respectively. More importantly, we discuss the limitations of existing studies and present possible future directions.



### An Integrated Enhancement Solution for 24-hour Colorful Imaging
- **Arxiv ID**: http://arxiv.org/abs/2005.04580v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.04580v1)
- **Published**: 2020-05-10 05:11:34+00:00
- **Updated**: 2020-05-10 05:11:34+00:00
- **Authors**: Feifan Lv, Yinqiang Zheng, Yicheng Li, Feng Lu
- **Comment**: AAAI 2020 (Oral)
- **Journal**: None
- **Summary**: The current industry practice for 24-hour outdoor imaging is to use a silicon camera supplemented with near-infrared (NIR) illumination. This will result in color images with poor contrast at daytime and absence of chrominance at nighttime. For this dilemma, all existing solutions try to capture RGB and NIR images separately. However, they need additional hardware support and suffer from various drawbacks, including short service life, high price, specific usage scenario, etc. In this paper, we propose a novel and integrated enhancement solution that produces clear color images, whether at abundant sunlight daytime or extremely low-light nighttime. Our key idea is to separate the VIS and NIR information from mixed signals, and enhance the VIS signal adaptively with the NIR signal as assistance. To this end, we build an optical system to collect a new VIS-NIR-MIX dataset and present a physically meaningful image processing algorithm based on CNN. Extensive experiments show outstanding results, which demonstrate the effectiveness of our solution.



### Intracranial Hemorrhage Detection Using Neural Network Based Methods With Federated Learning
- **Arxiv ID**: http://arxiv.org/abs/2005.08644v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.08644v3)
- **Published**: 2020-05-10 05:35:15+00:00
- **Updated**: 2022-03-17 05:22:26+00:00
- **Authors**: Utkarsh Chandra Srivastava, Anshuman Singh, Dr. K. Sree Kumar
- **Comment**: 3 pages
- **Journal**: None
- **Summary**: Intracranial hemorrhage, bleeding that occurs inside the cranium, is a serious health problem requiring rapid and often intensive medical treatment. Such a condition is traditionally diagnosed by highly-trained specialists analyzing computed tomography (CT) scan of the patient and identifying the location and type of hemorrhage if one exists. We propose a neural network approach to find and classify the condition based upon the CT scan. The model architecture implements a time distributed convolutional network. We observed accuracy above 92% from such an architecture, provided enough data. We propose further extensions to our approach involving the deployment of federated learning. This would be helpful in pooling learned parameters without violating the inherent privacy of the data involved.



### A Hybrid Swarm and Gravitation based feature selection algorithm for Handwritten Indic Script Classification problem
- **Arxiv ID**: http://arxiv.org/abs/2005.04596v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.04596v1)
- **Published**: 2020-05-10 07:27:55+00:00
- **Updated**: 2020-05-10 07:27:55+00:00
- **Authors**: Ritam Guha, Manosij Ghosh, Pawan Kumar Singh, Ram Sarkar, Mita Nasipuri
- **Comment**: 37 pages, 22 figures, submitted to Multimedia Tools and Applications,
  Springer
- **Journal**: None
- **Summary**: In any multi-script environment, handwritten script classification is of paramount importance before the document images are fed to their respective Optical Character Recognition (OCR) engines. Over the years, this complex pattern classification problem has been solved by researchers proposing various feature vectors mostly having large dimension, thereby increasing the computation complexity of the whole classification model. Feature Selection (FS) can serve as an intermediate step to reduce the size of the feature vectors by restricting them only to the essential and relevant features. In our paper, we have addressed this issue by introducing a new FS algorithm, called Hybrid Swarm and Gravitation based FS (HSGFS). This algorithm is made to run on 3 feature vectors introduced in the literature recently - Distance-Hough Transform (DHT), Histogram of Oriented Gradients (HOG) and Modified log-Gabor (MLG) filter Transform. Three state-of-the-art classifiers namely, Multi-Layer Perceptron (MLP), K-Nearest Neighbour (KNN) and Support Vector Machine (SVM) are used for the handwritten script classification. Handwritten datasets, prepared at block, text-line and word level, consisting of officially recognized 12 Indic scripts are used for the evaluation of our method. An average improvement in the range of 2-5 % is achieved in the classification accuracies by utilizing only about 75-80 % of the original feature vectors on all three datasets. The proposed methodology also shows better performance when compared to some popularly used FS models.



### Duality in Persistent Homology of Images
- **Arxiv ID**: http://arxiv.org/abs/2005.04597v2
- **DOI**: None
- **Categories**: **math.AT**, cs.CG, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.04597v2)
- **Published**: 2020-05-10 07:29:24+00:00
- **Updated**: 2020-05-12 07:16:26+00:00
- **Authors**: Adélie Garin, Teresa Heiss, Kelly Maggs, Bea Bleile, Vanessa Robins
- **Comment**: This is an extended abstract for the SoCG Young Researchers Forum
  2020
- **Journal**: None
- **Summary**: We derive the relationship between the persistent homology barcodes of two dual filtered CW complexes. Applied to greyscale digital images, we obtain an algorithm to convert barcodes between the two different (dual) topological models of pixel connectivity.



### Atom Search Optimization with Simulated Annealing -- a Hybrid Metaheuristic Approach for Feature Selection
- **Arxiv ID**: http://arxiv.org/abs/2005.08642v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2005.08642v1)
- **Published**: 2020-05-10 07:56:58+00:00
- **Updated**: 2020-05-10 07:56:58+00:00
- **Authors**: Kushal Kanti Ghosh, Ritam Guha, Soulib Ghosh, Suman Kumar Bera, Ram Sarkar
- **Comment**: 39 pages, submitted to Expert Systems with Applications, Elsevier
- **Journal**: None
- **Summary**: 'Hybrid meta-heuristics' is one of the most interesting recent trends in the field of optimization and feature selection (FS). In this paper, we have proposed a binary variant of Atom Search Optimization (ASO) and its hybrid with Simulated Annealing called ASO-SA techniques for FS. In order to map the real values used by ASO to the binary domain of FS, we have used two different transfer functions: S-shaped and V-shaped. We have hybridized this technique with a local search technique called, SA We have applied the proposed feature selection methods on 25 datasets from 4 different categories: UCI, Handwritten digit recognition, Text, non-text separation, and Facial emotion recognition. We have used 3 different classifiers (K-Nearest Neighbor, Multi-Layer Perceptron and Random Forest) for evaluating the strength of the selected featured by the binary ASO, ASO-SA and compared the results with some recent wrapper-based algorithms. The experimental results confirm the superiority of the proposed method both in terms of classification accuracy and number of selected features.



### Robust Tensor Decomposition for Image Representation Based on Generalized Correntropy
- **Arxiv ID**: http://arxiv.org/abs/2005.04605v1
- **DOI**: 10.1109/TIP.2020.3033151
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.04605v1)
- **Published**: 2020-05-10 08:46:52+00:00
- **Updated**: 2020-05-10 08:46:52+00:00
- **Authors**: Miaohua Zhang, Yongsheng Gao, Changming Sun, Michael Blumenstein
- **Comment**: 13 pages
- **Journal**: None
- **Summary**: Traditional tensor decomposition methods, e.g., two dimensional principal component analysis and two dimensional singular value decomposition, that minimize mean square errors, are sensitive to outliers. To overcome this problem, in this paper we propose a new robust tensor decomposition method using generalized correntropy criterion (Corr-Tensor). A Lagrange multiplier method is used to effectively optimize the generalized correntropy objective function in an iterative manner. The Corr-Tensor can effectively improve the robustness of tensor decomposition with the existence of outliers without introducing any extra computational cost. Experimental results demonstrated that the proposed method significantly reduces the reconstruction error on face reconstruction and improves the accuracies on handwritten digit recognition and facial image clustering.



### Variational Clustering: Leveraging Variational Autoencoders for Image Clustering
- **Arxiv ID**: http://arxiv.org/abs/2005.04613v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.04613v1)
- **Published**: 2020-05-10 09:34:48+00:00
- **Updated**: 2020-05-10 09:34:48+00:00
- **Authors**: Vignesh Prasad, Dipanjan Das, Brojeshwar Bhowmick
- **Comment**: None
- **Journal**: IJCNN 2020
- **Summary**: Recent advances in deep learning have shown their ability to learn strong feature representations for images. The task of image clustering naturally requires good feature representations to capture the distribution of the data and subsequently differentiate data points from one another. Often these two aspects are dealt with independently and thus traditional feature learning alone does not suffice in partitioning the data meaningfully. Variational Autoencoders (VAEs) naturally lend themselves to learning data distributions in a latent space. Since we wish to efficiently discriminate between different clusters in the data, we propose a method based on VAEs where we use a Gaussian Mixture prior to help cluster the images accurately. We jointly learn the parameters of both the prior and the posterior distributions. Our method represents a true Gaussian Mixture VAE. This way, our method simultaneously learns a prior that captures the latent distribution of the images and a posterior to help discriminate well between data points. We also propose a novel reparametrization of the latent space consisting of a mixture of discrete and continuous variables. One key takeaway is that our method generalizes better across different datasets without using any pre-training or learnt models, unlike existing methods, allowing it to be trained from scratch in an end-to-end manner. We verify our efficacy and generalizability experimentally by achieving state-of-the-art results among unsupervised methods on a variety of datasets. To the best of our knowledge, we are the first to pursue image clustering using VAEs in a purely unsupervised manner on real image datasets.



### MOMBAT: Heart Rate Monitoring from Face Video using Pulse Modeling and Bayesian Tracking
- **Arxiv ID**: http://arxiv.org/abs/2005.04618v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.04618v1)
- **Published**: 2020-05-10 09:41:16+00:00
- **Updated**: 2020-05-10 09:41:16+00:00
- **Authors**: Puneet Gupta, Brojeshwar Bhowmick, Arpan Pal
- **Comment**: None
- **Journal**: Computers in Biology and Medicine, 2020
- **Summary**: A non-invasive yet inexpensive method for heart rate (HR) monitoring is of great importance in many real-world applications including healthcare, psychology understanding, affective computing and biometrics. Face videos are currently utilized for such HR monitoring, but unfortunately this can lead to errors due to the noise introduced by facial expressions, out-of-plane movements, camera parameters (like focus change) and environmental factors. We alleviate these issues by proposing a novel face video based HR monitoring method MOMBAT, that is, MOnitoring using Modeling and BAyesian Tracking. We utilize out-of-plane face movements to define a novel quality estimation mechanism. Subsequently, we introduce a Fourier basis based modeling to reconstruct the cardiovascular pulse signal at the locations containing the poor quality, that is, the locations affected by out-of-plane face movements. Furthermore, we design a Bayesian decision theory based HR tracking mechanism to rectify the spurious HR estimates. Experimental results reveal that our proposed method, MOMBAT outperforms state-of-the-art HR monitoring methods and performs HR monitoring with an average absolute error of 1.329 beats per minute and the Pearson correlation between estimated and actual heart rate is 0.9746. Moreover, it demonstrates that HR monitoring is significantly



### A Unified Weight Learning and Low-Rank Regression Model for Robust Complex Error Modeling
- **Arxiv ID**: http://arxiv.org/abs/2005.04619v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.04619v4)
- **Published**: 2020-05-10 09:50:14+00:00
- **Updated**: 2020-09-23 00:51:20+00:00
- **Authors**: Miaohua Zhang, Yongsheng Gao, Jun Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: One of the most important problems in regression-based error model is modeling the complex representation error caused by various corruptions and environment changes in images. For example, in robust face recognition, images are often affected by varying types and levels of corruptions, such as random pixel corruptions, block occlusions, or disguises. However, existing works are not robust enough to solve this problem due to they cannot model the complex corrupted errors very well. In this paper, we address this problem by a unified sparse weight learning and low-rank approximation regression model, which enables the random noises and contiguous occlusions in images to be treated simultaneously. For the random noise, we define a generalized correntropy (GC) function to match the error distribution. For the structured error caused by occlusions or disguises, we propose a GC function based rank approximation to measure the rank of error matrices. Since the proposed objective function is non-convex, an effective iterative optimization algorithm is developed to achieve the optimal weight learning and low-rank approximation. Extensive experimental results on three public face databases show that the proposed model can fit the error distribution and structure very well, thus obtain better recognition accuracies in comparison with the existing methods.



### A Comparison of Few-Shot Learning Methods for Underwater Optical and Sonar Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2005.04621v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.04621v2)
- **Published**: 2020-05-10 10:11:16+00:00
- **Updated**: 2020-10-26 15:18:43+00:00
- **Authors**: Mateusz Ochal, Jose Vazquez, Yvan Petillot, Sen Wang
- **Comment**: Accepted to IEEE Global OCEANS 2020: Singapore - U.S. Gulf Coast
- **Journal**: None
- **Summary**: Deep convolutional neural networks generally perform well in underwater object recognition tasks on both optical and sonar images. Many such methods require hundreds, if not thousands, of images per class to generalize well to unseen examples. However, obtaining and labeling sufficiently large volumes of data can be relatively costly and time-consuming, especially when observing rare objects or performing real-time operations. Few-Shot Learning (FSL) efforts have produced many promising methods to deal with low data availability. However, little attention has been given in the underwater domain, where the style of images poses additional challenges for object recognition algorithms. To the best of our knowledge, this is the first paper to evaluate and compare several supervised and semi-supervised Few-Shot Learning (FSL) methods using underwater optical and side-scan sonar imagery. Our results show that FSL methods offer a significant advantage over the traditional transfer learning methods that fine-tune pre-trained models. We hope that our work will help apply FSL to autonomous underwater systems and expand their learning capabilities.



### A Simple and Scalable Shape Representation for 3D Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2005.04623v1
- **DOI**: None
- **Categories**: **cs.CV**, 65D19
- **Links**: [PDF](http://arxiv.org/pdf/2005.04623v1)
- **Published**: 2020-05-10 10:22:50+00:00
- **Updated**: 2020-05-10 10:22:50+00:00
- **Authors**: Mateusz Michalkiewicz, Eugene Belilovsky, Mahsa Baktashmotlagh, Anders Eriksson
- **Comment**: 9 pages plus 3 pages of references. 4 figures
- **Journal**: None
- **Summary**: Deep learning applied to the reconstruction of 3D shapes has seen growing interest. A popular approach to 3D reconstruction and generation in recent years has been the CNN encoder-decoder model usually applied in voxel space. However, this often scales very poorly with the resolution limiting the effectiveness of these models. Several sophisticated alternatives for decoding to 3D shapes have been proposed typically relying on complex deep learning architectures for the decoder model. In this work, we show that this additional complexity is not necessary, and that we can actually obtain high quality 3D reconstruction using a linear decoder, obtained from principal component analysis on the signed distance function (SDF) of the surface. This approach allows easily scaling to larger resolutions. We show in multiple experiments that our approach is competitive with state-of-the-art methods. It also allows the decoder to be fine-tuned on the target task using a loss designed specifically for SDF transforms, obtaining further gains.



### BabyWalk: Going Farther in Vision-and-Language Navigation by Taking Baby Steps
- **Arxiv ID**: http://arxiv.org/abs/2005.04625v2
- **DOI**: None
- **Categories**: **cs.AI**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.04625v2)
- **Published**: 2020-05-10 10:46:41+00:00
- **Updated**: 2020-06-14 22:02:05+00:00
- **Authors**: Wang Zhu, Hexiang Hu, Jiacheng Chen, Zhiwei Deng, Vihan Jain, Eugene Ie, Fei Sha
- **Comment**: Accepted by ACL 2020
- **Journal**: None
- **Summary**: Learning to follow instructions is of fundamental importance to autonomous agents for vision-and-language navigation (VLN). In this paper, we study how an agent can navigate long paths when learning from a corpus that consists of shorter ones. We show that existing state-of-the-art agents do not generalize well. To this end, we propose BabyWalk, a new VLN agent that is learned to navigate by decomposing long instructions into shorter ones (BabySteps) and completing them sequentially. A special design memory buffer is used by the agent to turn its past experiences into contexts for future steps. The learning process is composed of two phases. In the first phase, the agent uses imitation learning from demonstration to accomplish BabySteps. In the second phase, the agent uses curriculum-based reinforcement learning to maximize rewards on navigation tasks with increasingly longer instructions. We create two new benchmark datasets (of long navigation tasks) and use them in conjunction with existing ones to examine BabyWalk's generalization ability. Empirical results show that BabyWalk achieves state-of-the-art results on several metrics, in particular, is able to follow long instructions better. The codes and the datasets are released on our project page https://github.com/Sha-Lab/babywalk.



### Learning Context-Based Non-local Entropy Modeling for Image Compression
- **Arxiv ID**: http://arxiv.org/abs/2005.04661v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.04661v1)
- **Published**: 2020-05-10 13:28:18+00:00
- **Updated**: 2020-05-10 13:28:18+00:00
- **Authors**: Mu Li, Kai Zhang, Wangmeng Zuo, Radu Timofte, David Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: The entropy of the codes usually serves as the rate loss in the recent learned lossy image compression methods. Precise estimation of the probabilistic distribution of the codes plays a vital role in the performance. However, existing deep learning based entropy modeling methods generally assume the latent codes are statistically independent or depend on some side information or local context, which fails to take the global similarity within the context into account and thus hinder the accurate entropy estimation. To address this issue, we propose a non-local operation for context modeling by employing the global similarity within the context. Specifically, we first introduce the proxy similarity functions and spatial masks to handle the missing reference problem in context modeling. Then, we combine the local and the global context via a non-local attention block and employ it in masked convolutional networks for entropy modeling. The entropy model is further adopted as the rate loss in a joint rate-distortion optimization to guide the training of the analysis transform and the synthesis transform network in transforming coding framework. Considering that the width of the transforms is essential in training low distortion models, we finally produce a U-Net block in the transforms to increase the width with manageable memory consumption and time complexity. Experiments on Kodak and Tecnick datasets demonstrate the superiority of the proposed context-based non-local attention block in entropy modeling and the U-Net block in low distortion compression against the existing image compression standards and recent deep image compression models.



### Domain Adaptation for Image Dehazing
- **Arxiv ID**: http://arxiv.org/abs/2005.04668v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.04668v1)
- **Published**: 2020-05-10 13:54:56+00:00
- **Updated**: 2020-05-10 13:54:56+00:00
- **Authors**: Yuanjie Shao, Lerenhan Li, Wenqi Ren, Changxin Gao, Nong Sang
- **Comment**: Accepted by IEEE Conference on Computer Vision and Patten Recognition
  (CVPR), 2020
- **Journal**: None
- **Summary**: Image dehazing using learning-based methods has achieved state-of-the-art performance in recent years. However, most existing methods train a dehazing model on synthetic hazy images, which are less able to generalize well to real hazy images due to domain shift. To address this issue, we propose a domain adaptation paradigm, which consists of an image translation module and two image dehazing modules. Specifically, we first apply a bidirectional translation network to bridge the gap between the synthetic and real domains by translating images from one domain to another. And then, we use images before and after translation to train the proposed two image dehazing networks with a consistency constraint. In this phase, we incorporate the real hazy image into the dehazing training via exploiting the properties of the clear image (e.g., dark channel prior and image gradient smoothing) to further improve the domain adaptivity. By training image translation and dehazing network in an end-to-end manner, we can obtain better effects of both image translation and dehazing. Experimental results on both synthetic and real-world images demonstrate that our model performs favorably against the state-of-the-art dehazing algorithms.



### A Generalized Kernel Risk Sensitive Loss for Robust Two-Dimensional Singular Value Decomposition
- **Arxiv ID**: http://arxiv.org/abs/2005.04671v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.04671v2)
- **Published**: 2020-05-10 14:02:40+00:00
- **Updated**: 2020-07-04 04:20:50+00:00
- **Authors**: Miaohua Zhang, Yongsheng Gao
- **Comment**: Under Consideration by "Pattern Recognition"
- **Journal**: None
- **Summary**: Two-dimensional singular decomposition (2DSVD) has been widely used for image processing tasks, such as image reconstruction, classification, and clustering. However, traditional 2DSVD algorithm is based on the mean square error (MSE) loss, which is sensitive to outliers. To overcome this problem, we propose a robust 2DSVD framework based on a generalized kernel risk sensitive loss (GKRSL-2DSVD) which is more robust to noise and and outliers. Since the proposed objective function is non-convex, a majorization-minimization algorithm is developed to efficiently solve it with guaranteed convergence. The proposed framework has inherent properties of processing non-centered data, rotational invariant, being easily extended to higher order spaces. Experimental results on public databases demonstrate that the performance of the proposed method on different applications significantly outperforms that of all the benchmarks.



### Deep Learning Based Vehicle Tracking System Using License Plate Detection And Recognition
- **Arxiv ID**: http://arxiv.org/abs/2005.08641v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, 68T45, I.2.10; I.4.6; I.2.7
- **Links**: [PDF](http://arxiv.org/pdf/2005.08641v1)
- **Published**: 2020-05-10 14:03:33+00:00
- **Updated**: 2020-05-10 14:03:33+00:00
- **Authors**: Lalit Lakshmanan, Yash Vora, Raj Ghate
- **Comment**: 6 pages, 13 figures
- **Journal**: None
- **Summary**: Vehicle tracking is an integral part of intelligent traffic management systems. Previous implementations of vehicle tracking used Global Positioning System(GPS) based systems that gave location of the vehicle of an individual on their smartphones.The proposed system uses a novel approach to vehicle tracking using Vehicle License plate detection and recognition (VLPR) technique, which can be integrated on a large scale with traffic management systems. Initial methods of implementing VLPR used simple image processing techniques which were quite experimental and heuristic. With the onset of Deep learning and Computer Vision, one can create robust VLPR systems that can produce results close to human efficiency. Previous implementations, based on deep learning, made use of object detection and support vector machines for detection and a heuristic image processing based approach for recognition. The proposed system makes use of scene text detection model architecture for License plate detection and for recognition it uses the Optical character recognition engine (OCR) Tesseract. The proposed system obtained extraordinary results when it was tested on a highway video using NVIDIA Ge-force RTX 2080ti GPU, results were obtained at a speed of 30 frames per second with accuracy close to human.



### Non-Autoregressive Image Captioning with Counterfactuals-Critical Multi-Agent Learning
- **Arxiv ID**: http://arxiv.org/abs/2005.04690v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.04690v1)
- **Published**: 2020-05-10 15:09:44+00:00
- **Updated**: 2020-05-10 15:09:44+00:00
- **Authors**: Longteng Guo, Jing Liu, Xinxin Zhu, Xingjian He, Jie Jiang, Hanqing Lu
- **Comment**: IJCAI 2020 (copyright held by IJCAI)
- **Journal**: None
- **Summary**: Most image captioning models are autoregressive, i.e. they generate each word by conditioning on previously generated words, which leads to heavy latency during inference. Recently, non-autoregressive decoding has been proposed in machine translation to speed up the inference time by generating all words in parallel. Typically, these models use the word-level cross-entropy loss to optimize each word independently. However, such a learning process fails to consider the sentence-level consistency, thus resulting in inferior generation quality of these non-autoregressive models. In this paper, we propose a Non-Autoregressive Image Captioning (NAIC) model with a novel training paradigm: Counterfactuals-critical Multi-Agent Learning (CMAL). CMAL formulates NAIC as a multi-agent reinforcement learning system where positions in the target sequence are viewed as agents that learn to cooperatively maximize a sentence-level reward. Besides, we propose to utilize massive unlabeled images to boost captioning performance. Extensive experiments on MSCOCO image captioning benchmark show that our NAIC model achieves a performance comparable to state-of-the-art autoregressive models, while brings 13.9x decoding speedup.



### Segmentation of Macular Edema Datasets with Small Residual 3D U-Net Architectures
- **Arxiv ID**: http://arxiv.org/abs/2005.04697v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.04697v1)
- **Published**: 2020-05-10 15:34:46+00:00
- **Updated**: 2020-05-10 15:34:46+00:00
- **Authors**: Jonathan Frawley, Chris G. Willcocks, Maged Habib, Caspar Geenen, David H. Steel, Boguslaw Obara
- **Comment**: 7 pages, 5 figures
- **Journal**: None
- **Summary**: This paper investigates the application of deep convolutional neural networks with prohibitively small datasets to the problem of macular edema segmentation. In particular, we investigate several different heavily regularized architectures. We find that, contrary to popular belief, neural architectures within this application setting are able to achieve close to human-level performance on unseen test images without requiring large numbers of training examples. Annotating these 3D datasets is difficult, with multiple criteria required. It takes an experienced clinician two days to annotate a single 3D image, whereas our trained model achieves similar performance in less than a second. We found that an approach which uses targeted dataset augmentation, alongside architectural simplification with an emphasis on residual design, has acceptable generalization performance - despite relying on fewer than 15 training examples.



### Hierarchical Regression Network for Spectral Reconstruction from RGB Images
- **Arxiv ID**: http://arxiv.org/abs/2005.04703v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.04703v1)
- **Published**: 2020-05-10 16:06:11+00:00
- **Updated**: 2020-05-10 16:06:11+00:00
- **Authors**: Yuzhi Zhao, Lai-Man Po, Qiong Yan, Wei Liu, Tingyu Lin
- **Comment**: 1st Place in CVPRW 2020 NTIRE Spectral Reconstruction Challenge
- **Journal**: None
- **Summary**: Capturing visual image with a hyperspectral camera has been successfully applied to many areas due to its narrow-band imaging technology. Hyperspectral reconstruction from RGB images denotes a reverse process of hyperspectral imaging by discovering an inverse response function. Current works mainly map RGB images directly to corresponding spectrum but do not consider context information explicitly. Moreover, the use of encoder-decoder pair in current algorithms leads to loss of information. To address these problems, we propose a 4-level Hierarchical Regression Network (HRNet) with PixelShuffle layer as inter-level interaction. Furthermore, we adopt a residual dense block to remove artifacts of real world RGB images and a residual global block to build attention mechanism for enlarging perceptive field. We evaluate proposed HRNet with other architectures and techniques by participating in NTIRE 2020 Challenge on Spectral Reconstruction from RGB Images. The HRNet is the winning method of track 2 - real world images and ranks 3rd on track 1 - clean images. Please visit the project web page https://github.com/zhaoyuzhi/Hierarchical-Regression-Network-for-Spectral-Reconstruction-from-RGB-Images to try our codes and pre-trained models.



### A Simple Semi-Supervised Learning Framework for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2005.04757v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.04757v2)
- **Published**: 2020-05-10 19:15:51+00:00
- **Updated**: 2020-12-03 04:12:25+00:00
- **Authors**: Kihyuk Sohn, Zizhao Zhang, Chun-Liang Li, Han Zhang, Chen-Yu Lee, Tomas Pfister
- **Comment**: None
- **Journal**: None
- **Summary**: Semi-supervised learning (SSL) has a potential to improve the predictive performance of machine learning models using unlabeled data. Although there has been remarkable recent progress, the scope of demonstration in SSL has mainly been on image classification tasks. In this paper, we propose STAC, a simple yet effective SSL framework for visual object detection along with a data augmentation strategy. STAC deploys highly confident pseudo labels of localized objects from an unlabeled image and updates the model by enforcing consistency via strong augmentations. We propose experimental protocols to evaluate the performance of semi-supervised object detection using MS-COCO and show the efficacy of STAC on both MS-COCO and VOC07. On VOC07, STAC improves the AP$^{0.5}$ from $76.30$ to $79.08$; on MS-COCO, STAC demonstrates $2{\times}$ higher data efficiency by achieving 24.38 mAP using only 5\% labeled data than supervised baseline that marks 23.86\% using 10\% labeled data. The code is available at https://github.com/google-research/ssl_detection/.



### Photometric Multi-View Mesh Refinement for High-Resolution Satellite Images
- **Arxiv ID**: http://arxiv.org/abs/2005.04777v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.04777v2)
- **Published**: 2020-05-10 20:37:54+00:00
- **Updated**: 2020-05-12 20:26:34+00:00
- **Authors**: Mathias Rothermel, Ke Gong, Dieter Fritsch, Konrad Schindler, Norbert Haala
- **Comment**: Accepted for publication in ISPRS Journal of Photogrammetry and
  Remote Sensing
- **Journal**: None
- **Summary**: Modern high-resolution satellite sensors collect optical imagery with ground sampling distances (GSDs) of 30-50cm, which has sparked a renewed interest in photogrammetric 3D surface reconstruction from satellite data. State-of-the-art reconstruction methods typically generate 2.5D elevation data. Here, we present an approach to recover full 3D surface meshes from multi-view satellite imagery. The proposed method takes as input a coarse initial mesh and refines it by iteratively updating all vertex positions to maximize the photo-consistency between images. Photo-consistency is measured in image space, by transferring texture from one image to another via the surface. We derive the equations to propagate changes in texture similarity through the rational function model (RFM), often also referred to as rational polynomial coefficient (RPC) model. Furthermore, we devise a hierarchical scheme to optimize the surface with gradient descent. In experiments with two different datasets, we show that the refinement improves the initial digital elevation models (DEMs) generated with conventional dense image matching. Moreover, we demonstrate that our method is able to reconstruct true 3D geometry, such as facade structures, if off-nadir views are available.



### Supervision and Source Domain Impact on Representation Learning: A Histopathology Case Study
- **Arxiv ID**: http://arxiv.org/abs/2005.08629v1
- **DOI**: 10.1109/EMBC44109.2020.9176279
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.08629v1)
- **Published**: 2020-05-10 21:27:38+00:00
- **Updated**: 2020-05-10 21:27:38+00:00
- **Authors**: Milad Sikaroudi, Amir Safarpoor, Benyamin Ghojogh, Sobhan Shafiei, Mark Crowley, H. R. Tizhoosh
- **Comment**: Accepted for presentation at the 42nd Annual International Conference
  of the IEEE Engineering in Medicine and Biology Society (EMBC'20)
- **Journal**: 2020 42nd Annual International Conference of the IEEE Engineering
  in Medicine & Biology Society (EMBC), pp. 1400-1403
- **Summary**: As many algorithms depend on a suitable representation of data, learning unique features is considered a crucial task. Although supervised techniques using deep neural networks have boosted the performance of representation learning, the need for a large set of labeled data limits the application of such methods. As an example, high-quality delineations of regions of interest in the field of pathology is a tedious and time-consuming task due to the large image dimensions. In this work, we explored the performance of a deep neural network and triplet loss in the area of representation learning. We investigated the notion of similarity and dissimilarity in pathology whole-slide images and compared different setups from unsupervised and semi-supervised to supervised learning in our experiments. Additionally, different approaches were tested, applying few-shot learning on two publicly available pathology image datasets. We achieved high accuracy and generalization when the learned representations were applied to two different pathology datasets.



### The Hateful Memes Challenge: Detecting Hate Speech in Multimodal Memes
- **Arxiv ID**: http://arxiv.org/abs/2005.04790v3
- **DOI**: None
- **Categories**: **cs.AI**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.04790v3)
- **Published**: 2020-05-10 21:31:00+00:00
- **Updated**: 2021-04-07 18:43:54+00:00
- **Authors**: Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet Singh, Pratik Ringshia, Davide Testuggine
- **Comment**: NeurIPS 2020
- **Journal**: None
- **Summary**: This work proposes a new challenge set for multimodal classification, focusing on detecting hate speech in multimodal memes. It is constructed such that unimodal models struggle and only multimodal models can succeed: difficult examples ("benign confounders") are added to the dataset to make it hard to rely on unimodal signals. The task requires subtle reasoning, yet is straightforward to evaluate as a binary classification problem. We provide baseline performance numbers for unimodal models, as well as for multimodal models with various degrees of sophistication. We find that state-of-the-art methods perform poorly compared to humans (64.73% vs. 84.7% accuracy), illustrating the difficulty of the task and highlighting the challenge that this important problem poses to the community.



### A Closed-Form Uncertainty Propagation in Non-Rigid Structure from Motion
- **Arxiv ID**: http://arxiv.org/abs/2005.04810v5
- **DOI**: 10.1109/LRA.2022.3173733
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.04810v5)
- **Published**: 2020-05-10 23:22:58+00:00
- **Updated**: 2022-09-05 13:54:42+00:00
- **Authors**: Jingwei Song, Mitesh Patel, Ashkan Jasour, Maani Ghaffari
- **Comment**: Accepted for publication by IEEE Robotics and Automation Letters.
  Code can be accessed at https://github.com/JingweiSong/NRSfM_uncertainty
- **Journal**: [J]. IEEE Robotics and Automation Letters, 2022, 7(3): 6479-6486
- **Summary**: Semi-Definite Programming (SDP) with low-rank prior has been widely applied in Non-Rigid Structure from Motion (NRSfM). Based on a low-rank constraint, it avoids the inherent ambiguity of basis number selection in conventional base-shape or base-trajectory methods. Despite the efficiency in deformable shape reconstruction, it remains unclear how to assess the uncertainty of the recovered shape from the SDP process. In this paper, we present a statistical inference on the element-wise uncertainty quantification of the estimated deforming 3D shape points in the case of the exact low-rank SDP problem. A closed-form uncertainty quantification method is proposed and tested. Moreover, we extend the exact low-rank uncertainty quantification to the approximate low-rank scenario with a numerical optimal rank selection method, which enables solving practical application in SDP based NRSfM scenario. The proposed method provides an independent module to the SDP method and only requires the statistic information of the input 2D tracked points. Extensive experiments prove that the output 3D points have identical normal distribution to the 2D trackings, the proposed method and quantify the uncertainty accurately, and supports that it has desirable effects on routinely SDP low-rank based NRSfM solver.



