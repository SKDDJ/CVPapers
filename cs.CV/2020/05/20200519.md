# Arxiv Papers in cs.CV on 2020-05-19
### 3D Augmented Reality-Assisted CT-Guided Interventions: System Design and Preclinical Trial on an Abdominal Phantom using HoloLens 2
- **Arxiv ID**: http://arxiv.org/abs/2005.09146v1
- **DOI**: 10.1038/s41598-020-75676-4
- **Categories**: **physics.med-ph**, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2005.09146v1)
- **Published**: 2020-05-19 00:22:24+00:00
- **Updated**: 2020-05-19 00:22:24+00:00
- **Authors**: Brian J. Park, Stephen J. Hunt, Gregory J. Nadolski, Terence P. Gade
- **Comment**: 16 pages, 6 figures, 2 tables
- **Journal**: None
- **Summary**: Background: Out-of-plane lesions pose challenges for CT-guided interventions. Augmented reality (AR) headset devices have evolved and are readily capable to provide virtual 3D guidance to improve CT-guided targeting.   Purpose: To describe the design of a three-dimensional (3D) AR-assisted navigation system using HoloLens 2 and evaluate its performance through CT-guided simulations.   Materials and Methods: A prospective trial was performed assessing CT-guided needle targeting on an abdominal phantom with and without AR guidance. A total of 8 operators with varying clinical experience were enrolled and performed a total of 86 needle passes. Procedure efficiency, radiation dose, and complication rates were compared with and without AR guidance. Vector analysis of the first needle pass was also performed.   Results: Average total number of needle passes to reach the target reduced from 7.4 passes without AR to 3.4 passes with AR (54.2% decrease, p=0.011). Average dose-length product (DLP) decreased from 538 mGy-cm without AR to 318 mGy-cm with AR (41.0% decrease, p=0.009). Complication rate of hitting a non-targeted lesion decreased from 11.9% without AR (7/59 needle passes) to 0% with AR (0/27 needle passes). First needle passes were more nearly aligned with the ideal target trajectory with AR versus without AR (4.6{\deg} vs 8.0{\deg} offset, respectively, p=0.018). Medical students, residents, and attendings all performed at the same level with AR guidance.   Conclusions: 3D AR guidance can provide significant improvements in procedural efficiency and radiation dose savings for targeting challenging, out-of-plane lesions. AR guidance elevated the performance of all operators to the same level irrespective of prior clinical experience.



### Increasing-Margin Adversarial (IMA) Training to Improve Adversarial Robustness of Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2005.09147v10
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.09147v10)
- **Published**: 2020-05-19 00:26:52+00:00
- **Updated**: 2023-02-10 00:03:26+00:00
- **Authors**: Linhai Ma, Liang Liang
- **Comment**: 26 pages
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) are vulnerable to adversarial noises. Adversarial training is a general and effective strategy to improve DNN robustness (i.e., accuracy on noisy data) against adversarial noises. However, DNN models trained by the current existing adversarial training methods may have much lower standard accuracy (i.e., accuracy on clean data), compared to the same models trained by the standard method on clean data, and this phenomenon is known as the trade-off between accuracy and robustness and is considered unavoidable. This issue prevents adversarial training from being used in many application domains, such as medical image analysis, as practitioners do not want to sacrifice standard accuracy too much in exchange for adversarial robustness. Our objective is to lift (i.e., alleviate or even avoid) this trade-off between standard accuracy and adversarial robustness for medical image classification and segmentation. We propose a novel adversarial training method, named Increasing-Margin Adversarial (IMA) Training, which is supported by an equilibrium state analysis about the optimality of adversarial training samples. Our method aims to preserve accuracy while improving robustness by generating optimal adversarial training samples. We evaluate our method and the other eight representative methods on six publicly available image datasets corrupted by noises generated by AutoAttack and white-noise attack. Our method achieves the highest adversarial robustness for image classification and segmentation with the smallest reduction in accuracy on clean data. For one of the applications, our method improves both accuracy and robustness. Our study has demonstrated that our method can lift the trade-off between standard accuracy and adversarial robustness for the image classification and segmentation applications.



### Associating Multi-Scale Receptive Fields for Fine-grained Recognition
- **Arxiv ID**: http://arxiv.org/abs/2005.09153v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.09153v1)
- **Published**: 2020-05-19 01:16:31+00:00
- **Updated**: 2020-05-19 01:16:31+00:00
- **Authors**: Zihan Ye, Fuyuan Hu, Yin Liu, Zhenping Xia, Fan Lyu, Pengqing Liu
- **Comment**: Accepted by ICIP2020
- **Journal**: None
- **Summary**: Extracting and fusing part features have become the key of fined-grained image recognition. Recently, Non-local (NL) module has shown excellent improvement in image recognition. However, it lacks the mechanism to model the interactions between multi-scale part features, which is vital for fine-grained recognition. In this paper, we propose a novel cross-layer non-local (CNL) module to associate multi-scale receptive fields by two operations. First, CNL computes correlations between features of a query layer and all response layers. Second, all response features are weighted according to the correlations and are added to the query features. Due to the interactions of cross-layer features, our model builds spatial dependencies among multi-level layers and learns more discriminative features. In addition, we can reduce the aggregation cost if we set low-dimensional deep layer as query layer. Experiments are conducted to show our model achieves or surpasses state-of-the-art results on three benchmark datasets of fine-grained classification. Our codes can be found at github.com/FouriYe/CNL-ICIP2020.



### Sketch-BERT: Learning Sketch Bidirectional Encoder Representation from Transformers by Self-supervised Learning of Sketch Gestalt
- **Arxiv ID**: http://arxiv.org/abs/2005.09159v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.09159v1)
- **Published**: 2020-05-19 01:35:44+00:00
- **Updated**: 2020-05-19 01:35:44+00:00
- **Authors**: Hangyu Lin, Yanwei Fu, Yu-Gang Jiang, Xiangyang Xue
- **Comment**: Accepted to CVPR 2020
- **Journal**: None
- **Summary**: Previous researches of sketches often considered sketches in pixel format and leveraged CNN based models in the sketch understanding. Fundamentally, a sketch is stored as a sequence of data points, a vector format representation, rather than the photo-realistic image of pixels. SketchRNN studied a generative neural representation for sketches of vector format by Long Short Term Memory networks (LSTM). Unfortunately, the representation learned by SketchRNN is primarily for the generation tasks, rather than the other tasks of recognition and retrieval of sketches. To this end and inspired by the recent BERT model, we present a model of learning Sketch Bidirectional Encoder Representation from Transformer (Sketch-BERT). We generalize BERT to sketch domain, with the novel proposed components and pre-training algorithms, including the newly designed sketch embedding networks, and the self-supervised learning of sketch gestalt. Particularly, towards the pre-training task, we present a novel Sketch Gestalt Model (SGM) to help train the Sketch-BERT. Experimentally, we show that the learned representation of Sketch-BERT can help and improve the performance of the downstream tasks of sketch recognition, sketch retrieval, and sketch gestalt.



### Spatiotemporal Attacks for Embodied Agents
- **Arxiv ID**: http://arxiv.org/abs/2005.09161v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.09161v3)
- **Published**: 2020-05-19 01:38:47+00:00
- **Updated**: 2020-11-17 14:13:55+00:00
- **Authors**: Aishan Liu, Tairan Huang, Xianglong Liu, Yitao Xu, Yuqing Ma, Xinyun Chen, Stephen J. Maybank, Dacheng Tao
- **Comment**: Accepted on ECCV 2020
- **Journal**: None
- **Summary**: Adversarial attacks are valuable for providing insights into the blind-spots of deep learning models and help improve their robustness. Existing work on adversarial attacks have mainly focused on static scenes; however, it remains unclear whether such attacks are effective against embodied agents, which could navigate and interact with a dynamic environment. In this work, we take the first step to study adversarial attacks for embodied agents. In particular, we generate spatiotemporal perturbations to form 3D adversarial examples, which exploit the interaction history in both the temporal and spatial dimensions. Regarding the temporal dimension, since agents make predictions based on historical observations, we develop a trajectory attention module to explore scene view contributions, which further help localize 3D objects appeared with the highest stimuli. By conciliating with clues from the temporal dimension, along the spatial dimension, we adversarially perturb the physical properties (e.g., texture and 3D shape) of the contextual objects that appeared in the most important scene views. Extensive experiments on the EQA-v1 dataset for several embodied tasks in both the white-box and black-box settings have been conducted, which demonstrate that our perturbations have strong attack and generalization abilities.



### A New Validity Index for Fuzzy-Possibilistic C-Means Clustering
- **Arxiv ID**: http://arxiv.org/abs/2005.09162v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML, I.5.1; I.2.1; I.5.3; J.3; G.1.6
- **Links**: [PDF](http://arxiv.org/pdf/2005.09162v1)
- **Published**: 2020-05-19 01:48:13+00:00
- **Updated**: 2020-05-19 01:48:13+00:00
- **Authors**: Mohammad Hossein Fazel Zarandi, Shahabeddin Sotudian, Oscar Castillo
- **Comment**: The following article has been accepted by Scientia Iranica
- **Journal**: None
- **Summary**: In some complicated datasets, due to the presence of noisy data points and outliers, cluster validity indices can give conflicting results in determining the optimal number of clusters. This paper presents a new validity index for fuzzy-possibilistic c-means clustering called Fuzzy-Possibilistic (FP) index, which works well in the presence of clusters that vary in shape and density. Moreover, FPCM like most of the clustering algorithms is susceptible to some initial parameters. In this regard, in addition to the number of clusters, FPCM requires a priori selection of the degree of fuzziness and the degree of typicality. Therefore, we presented an efficient procedure for determining their optimal values. The proposed approach has been evaluated using several synthetic and real-world datasets. Final computational results demonstrate the capabilities and reliability of the proposed approach compared with several well-known fuzzy validity indices in the literature. Furthermore, to clarify the ability of the proposed method in real applications, the proposed method is implemented in microarray gene expression data clustering and medical image segmentation.



### Learning from a Lightweight Teacher for Efficient Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2005.09163v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.09163v1)
- **Published**: 2020-05-19 01:54:15+00:00
- **Updated**: 2020-05-19 01:54:15+00:00
- **Authors**: Yuang Liu, Wei Zhang, Jun Wang
- **Comment**: 11 pages, 3 figures, 9 tables
- **Journal**: None
- **Summary**: Knowledge Distillation (KD) is an effective framework for compressing deep learning models, realized by a student-teacher paradigm requiring small student networks to mimic the soft target generated by well-trained teachers. However, the teachers are commonly assumed to be complex and need to be trained on the same datasets as students. This leads to a time-consuming training process. The recent study shows vanilla KD plays a similar role as label smoothing and develops teacher-free KD, being efficient and mitigating the issue of learning from heavy teachers. But because teacher-free KD relies on manually-crafted output distributions kept the same for all data instances belonging to the same class, its flexibility and performance are relatively limited. To address the above issues, this paper proposes en efficient knowledge distillation learning framework LW-KD, short for lightweight knowledge distillation. It firstly trains a lightweight teacher network on a synthesized simple dataset, with an adjustable class number equal to that of a target dataset. The teacher then generates soft target whereby an enhanced KD loss could guide student learning, which is a combination of KD loss and adversarial loss for making student output indistinguishable from the output of the teacher. Experiments on several public datasets with different modalities demonstrate LWKD is effective and efficient, showing the rationality of its main design principles.



### Regularization Methods for Generative Adversarial Networks: An Overview of Recent Studies
- **Arxiv ID**: http://arxiv.org/abs/2005.09165v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2005.09165v1)
- **Published**: 2020-05-19 01:59:24+00:00
- **Updated**: 2020-05-19 01:59:24+00:00
- **Authors**: Minhyeok Lee, Junhee Seok
- **Comment**: None
- **Journal**: None
- **Summary**: Despite its short history, Generative Adversarial Network (GAN) has been extensively studied and used for various tasks, including its original purpose, i.e., synthetic sample generation. However, applying GAN to different data types with diverse neural network architectures has been hindered by its limitation in training, where the model easily diverges. Such a notorious training of GANs is well known and has been addressed in numerous studies. Consequently, in order to make the training of GAN stable, numerous regularization methods have been proposed in recent years. This paper reviews the regularization methods that have been recently introduced, most of which have been published in the last three years. Specifically, we focus on general methods that can be commonly used regardless of neural network architectures. To explore the latest research trends in the regularization for GANs, the methods are classified into several groups by their operation principles, and the differences between the methods are analyzed. Furthermore, to provide practical knowledge of using these methods, we investigate popular methods that have been frequently employed in state-of-the-art GANs. In addition, we discuss the limitations in existing methods and propose future research directions.



### MOTS: Multiple Object Tracking for General Categories Based On Few-Shot Method
- **Arxiv ID**: http://arxiv.org/abs/2005.09167v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.09167v1)
- **Published**: 2020-05-19 02:18:01+00:00
- **Updated**: 2020-05-19 02:18:01+00:00
- **Authors**: Xixi Xu, Chao Lu, Liang Zhu, Xiangyang Xue, Guanxian Chen, Qi Guo, Yining Lin, Zhijian Zhao
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: Most modern Multi-Object Tracking (MOT) systems typically apply REID-based paradigm to hold a balance between computational efficiency and performance. In the past few years, numerous attempts have been made to perfect the systems. Although they presented favorable performance, they were constrained to track specified category. Drawing on the ideas of few shot method, we pioneered a new multi-target tracking system, named MOTS, which is based on metrics but not limited to track specific category. It contains two stages in series: In the first stage, we design the self-Adaptive-matching module to perform simple targets matching, which can complete 88.76% assignments without sacrificing performance on MOT16 training set. In the second stage, a Fine-match Network was carefully designed for unmatched targets. With a newly built TRACK-REID data-set, the Fine-match Network can perform matching of 31 category targets, even generalizes to unseen categories.



### Retrieving and Highlighting Action with Spatiotemporal Reference
- **Arxiv ID**: http://arxiv.org/abs/2005.09183v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2005.09183v1)
- **Published**: 2020-05-19 03:12:31+00:00
- **Updated**: 2020-05-19 03:12:31+00:00
- **Authors**: Seito Kasai, Yuchi Ishikawa, Masaki Hayashi, Yoshimitsu Aoki, Kensho Hara, Hirokatsu Kataoka
- **Comment**: Accepted to ICIP 2020
- **Journal**: None
- **Summary**: In this paper, we present a framework that jointly retrieves and spatiotemporally highlights actions in videos by enhancing current deep cross-modal retrieval methods. Our work takes on the novel task of action highlighting, which visualizes where and when actions occur in an untrimmed video setting. Action highlighting is a fine-grained task, compared to conventional action recognition tasks which focus on classification or window-based localization. Leveraging weak supervision from annotated captions, our framework acquires spatiotemporal relevance maps and generates local embeddings which relate to the nouns and verbs in captions. Through experiments, we show that our model generates various maps conditioned on different actions, in which conventional visual reasoning methods only go as far as to show a single deterministic saliency map. Also, our model improves retrieval recall over our baseline without alignment by 2-3% on the MSR-VTT dataset.



### An Innovative Approach to Determine Rebar Depth and Size by Comparing GPR Data with a Theoretical Database
- **Arxiv ID**: http://arxiv.org/abs/2005.09643v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.09643v1)
- **Published**: 2020-05-19 03:35:30+00:00
- **Updated**: 2020-05-19 03:35:30+00:00
- **Authors**: Zhongming Xiang, Ge Ou, Abbas Rashidi
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Ground penetrating radar (GPR) is an efficient technique used for rapidly recognizing embedded rebar in concrete structures. However, due to the difficulty in extracting signals from GPR data and the intrinsic coupling between the rebar depth and size showing in the data, simultaneously determining rebar depth and size is challenging. This paper proposes an innovative algorithm to address this issue. First, the hyperbola signal from the GPR data is identified by direct wave removal, signal reconstruction and separation. Subsequently, a database is developed from a series of theoretical hyperbolas and then compared with the extracted hyperbola outlines. Finally, the rebar depth and size are determined by searching for the closest counterpart in the database. The obtained results are very promising and indicate that: (1) implementing the method presented in this paper can completely remove the direct wave noise from the GPR data, and can successfully extract the outlines from the interlaced hyperbolas; and (2) the proposed method can simultaneously determine the rebar depth and size with the accuracy of 100% and 95.11%, respectively.



### A Self-ensembling Framework for Semi-supervised Knee Cartilage Defects Assessment with Dual-Consistency
- **Arxiv ID**: http://arxiv.org/abs/2005.09212v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.09212v2)
- **Published**: 2020-05-19 04:47:25+00:00
- **Updated**: 2020-10-12 11:08:17+00:00
- **Authors**: Jiayu Huo, Liping Si, Xi Ouyang, Kai Xuan, Weiwu Yao, Zhong Xue, Qian Wang, Dinggang Shen, Lichi Zhang
- **Comment**: accepted by International Workshop on PRedictive Intelligence In
  MEdicine, 2020
- **Journal**: None
- **Summary**: Knee osteoarthritis (OA) is one of the most common musculoskeletal disorders and requires early-stage diagnosis. Nowadays, the deep convolutional neural networks have achieved greatly in the computer-aided diagnosis field. However, the construction of the deep learning models usually requires great amounts of annotated data, which is generally high-cost. In this paper, we propose a novel approach for knee cartilage defects assessment, including severity classification and lesion localization. This can be treated as a subtask of knee OA diagnosis. Particularly, we design a self-ensembling framework, which is composed of a student network and a teacher network with the same structure. The student network learns from both labeled data and unlabeled data and the teacher network averages the student model weights through the training course. A novel attention loss function is developed to obtain accurate attention masks. With dual-consistency checking of the attention in the lesion classification and localization, the two networks can gradually optimize the attention distribution and improve the performance of each other, whereas the training relies on partially labeled data only and follows the semi-supervised manner. Experiments show that the proposed method can significantly improve the self-ensembling performance in both knee cartilage defects classification and localization, and also greatly reduce the needs of annotated data.



### Deep Learning Guided Building Reconstruction from Satellite Imagery-derived Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2005.09223v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.09223v1)
- **Published**: 2020-05-19 05:38:06+00:00
- **Updated**: 2020-05-19 05:38:06+00:00
- **Authors**: Bo Xu, Xu Zhang, Zhixin Li, Matt Leotta, Shih-Fu Chang, Jie Shan
- **Comment**: None
- **Journal**: None
- **Summary**: 3D urban reconstruction of buildings from remotely sensed imagery has drawn significant attention during the past two decades. While aerial imagery and LiDAR provide higher resolution, satellite imagery is cheaper and more efficient to acquire for large scale need. However, the high, orbital altitude of satellite observation brings intrinsic challenges, like unpredictable atmospheric effect, multi view angles, significant radiometric differences due to the necessary multiple views, diverse land covers and urban structures in a scene, small base-height ratio or narrow field of view, all of which may degrade 3D reconstruction quality. To address these major challenges, we present a reliable and effective approach for building model reconstruction from the point clouds generated from multi-view satellite images. We utilize multiple types of primitive shapes to fit the input point cloud. Specifically, a deep-learning approach is adopted to distinguish the shape of building roofs in complex and yet noisy scenes. For points that belong to the same roof shape, a multi-cue, hierarchical RANSAC approach is proposed for efficient and reliable segmenting and reconstructing the building point cloud. Experimental results over four selected urban areas (0.34 to 2.04 sq km in size) demonstrate the proposed method can generate detailed roof structures under noisy data environments. The average successful rate for building shape recognition is 83.0%, while the overall completeness and correctness are over 70% with reference to ground truth created from airborne lidar. As the first effort to address the public need of large scale city model generation, the development is deployed as open source software.



### Holistic Parameteric Reconstruction of Building Models from Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2005.09226v1
- **DOI**: 10.5194/isprs-archives-XLIII-B2-2020-689-2020
- **Categories**: **cs.CV**, cs.GR, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.09226v1)
- **Published**: 2020-05-19 05:42:23+00:00
- **Updated**: 2020-05-19 05:42:23+00:00
- **Authors**: Zhixin Li, Wenyuan Zhang, Jie Shan
- **Comment**: Remote Sens. Spatial Inf. Sci., 2020
- **Journal**: None
- **Summary**: Building models are conventionally reconstructed by building roof points planar segmentation and then using a topology graph to group the planes together. Roof edges and vertices are then mathematically represented by intersecting segmented planes. Technically, such solution is based on sequential local fitting, i.e., the entire data of one building are not simultaneously participating in determining the building model. As a consequence, the solution is lack of topological integrity and geometric rigor. Fundamentally different from this traditional approach, we propose a holistic parametric reconstruction method which means taking into consideration the entire point clouds of one building simultaneously. In our work, building models are reconstructed from predefined parametric (roof) primitives. We first use a well-designed deep neural network to segment and identify primitives in the given building point clouds. A holistic optimization strategy is then introduced to simultaneously determine the parameters of a segmented primitive. In the last step, the optimal parameters are used to generate a watertight building model in CityGML format. The airborne LiDAR dataset RoofN3D with predefined roof types is used for our test. It is shown that PointNet++ applied to the entire dataset can achieve an accuracy of 83% for primitive classification. For a subset of 910 buildings in RoofN3D, the holistic approach is then used to determine the parameters of primitives and reconstruct the buildings. The achieved overall quality of reconstruction is 0.08 meters for point-surface-distance or 0.7 times RMSE of the input LiDAR points. The study demonstrates the efficiency and capability of the proposed approach and its potential to handle large scale urban point clouds.



### Structural Residual Learning for Single Image Rain Removal
- **Arxiv ID**: http://arxiv.org/abs/2005.09228v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.09228v1)
- **Published**: 2020-05-19 05:52:13+00:00
- **Updated**: 2020-05-19 05:52:13+00:00
- **Authors**: Hong Wang, Yichen Wu, Qi Xie, Qian Zhao, Yong Liang, Deyu Meng
- **Comment**: None
- **Journal**: None
- **Summary**: To alleviate the adverse effect of rain streaks in image processing tasks, CNN-based single image rain removal methods have been recently proposed. However, the performance of these deep learning methods largely relies on the covering range of rain shapes contained in the pre-collected training rainy-clean image pairs. This makes them easily trapped into the overfitting-to-the-training-samples issue and cannot finely generalize to practical rainy images with complex and diverse rain streaks. Against this generalization issue, this study proposes a new network architecture by enforcing the output residual of the network possess intrinsic rain structures. Such a structural residual setting guarantees the rain layer extracted by the network finely comply with the prior knowledge of general rain streaks, and thus regulates sound rain shapes capable of being well extracted from rainy images in both training and predicting stages. Such a general regularization function naturally leads to both its better training accuracy and testing generalization capability even for those non-seen rain configurations. Such superiority is comprehensively substantiated by experiments implemented on synthetic and real datasets both visually and quantitatively as compared with current state-of-the-art methods.



### An Auto-Context Deformable Registration Network for Infant Brain MRI
- **Arxiv ID**: http://arxiv.org/abs/2005.09230v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.09230v2)
- **Published**: 2020-05-19 06:00:13+00:00
- **Updated**: 2020-07-05 08:56:54+00:00
- **Authors**: Dongming Wei, Sahar Ahmad, Yunzhi Huang, Lei Ma, Zhengwang Wu, Gang Li, Li Wang, Qian Wang, Pew-Thian Yap, Dinggang Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Deformable image registration is fundamental to longitudinal and population analysis. Geometric alignment of the infant brain MR images is challenging, owing to rapid changes in image appearance in association with brain development. In this paper, we propose an infant-dedicated deep registration network that uses the auto-context strategy to gradually refine the deformation fields to obtain highly accurate correspondences. Instead of training multiple registration networks, our method estimates the deformation fields by invoking a single network multiple times for iterative deformation refinement. The final deformation field is obtained by the incremental composition of the deformation fields. Experimental results in comparison with state-of-the-art registration methods indicate that our method achieves higher accuracy while at the same time preserves the smoothness of the deformation fields. Our implementation is available online.



### On the Value of Out-of-Distribution Testing: An Example of Goodhart's Law
- **Arxiv ID**: http://arxiv.org/abs/2005.09241v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.09241v1)
- **Published**: 2020-05-19 06:45:50+00:00
- **Updated**: 2020-05-19 06:45:50+00:00
- **Authors**: Damien Teney, Kushal Kafle, Robik Shrestha, Ehsan Abbasnejad, Christopher Kanan, Anton van den Hengel
- **Comment**: None
- **Journal**: None
- **Summary**: Out-of-distribution (OOD) testing is increasingly popular for evaluating a machine learning system's ability to generalize beyond the biases of a training set. OOD benchmarks are designed to present a different joint distribution of data and labels between training and test time. VQA-CP has become the standard OOD benchmark for visual question answering, but we discovered three troubling practices in its current use. First, most published methods rely on explicit knowledge of the construction of the OOD splits. They often rely on ``inverting'' the distribution of labels, e.g. answering mostly 'yes' when the common training answer is 'no'. Second, the OOD test set is used for model selection. Third, a model's in-domain performance is assessed after retraining it on in-domain splits (VQA v2) that exhibit a more balanced distribution of labels. These three practices defeat the objective of evaluating generalization, and put into question the value of methods specifically designed for this dataset. We show that embarrassingly-simple methods, including one that generates answers at random, surpass the state of the art on some question types. We provide short- and long-term solutions to avoid these pitfalls and realize the benefits of OOD evaluation.



### Assertion Detection in Multi-Label Clinical Text using Scope Localization
- **Arxiv ID**: http://arxiv.org/abs/2005.09246v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.09246v1)
- **Published**: 2020-05-19 06:56:02+00:00
- **Updated**: 2020-05-19 06:56:02+00:00
- **Authors**: Rajeev Bhatt Ambati, Ahmed Ada Hanifi, Ramya Vunikili, Puneet Sharma, Oladimeji Farri
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-label sentences (text) in the clinical domain result from the rich description of scenarios during patient care. The state-of-theart methods for assertion detection mostly address this task in the setting of a single assertion label per sentence (text). In addition, few rules based and deep learning methods perform negation/assertion scope detection on single-label text. It is a significant challenge extending these methods to address multi-label sentences without diminishing performance. Therefore, we developed a convolutional neural network (CNN) architecture to localize multiple labels and their scopes in a single stage end-to-end fashion, and demonstrate that our model performs atleast 12% better than the state-of-the-art on multi-label clinical text.



### Bias-based Universal Adversarial Patch Attack for Automatic Check-out
- **Arxiv ID**: http://arxiv.org/abs/2005.09257v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.09257v3)
- **Published**: 2020-05-19 07:38:54+00:00
- **Updated**: 2020-08-03 13:06:03+00:00
- **Authors**: Aishan Liu, Jiakai Wang, Xianglong Liu, Bowen Cao, Chongzhi Zhang, Hang Yu
- **Comment**: This paper has been accepted on ECCV 2020
- **Journal**: None
- **Summary**: Adversarial examples are inputs with imperceptible perturbations that easily misleading deep neural networks(DNNs). Recently, adversarial patch, with noise confined to a small and localized patch, has emerged for its easy feasibility in real-world scenarios. However, existing strategies failed to generate adversarial patches with strong generalization ability. In other words, the adversarial patches were input-specific and failed to attack images from all classes, especially unseen ones during training. To address the problem, this paper proposes a bias-based framework to generate class-agnostic universal adversarial patches with strong generalization ability, which exploits both the perceptual and semantic bias of models. Regarding the perceptual bias, since DNNs are strongly biased towards textures, we exploit the hard examples which convey strong model uncertainties and extract a textural patch prior from them by adopting the style similarities. The patch prior is more close to decision boundaries and would promote attacks. To further alleviate the heavy dependency on large amounts of data in training universal attacks, we further exploit the semantic bias. As the class-wise preference, prototypes are introduced and pursued by maximizing the multi-class margin to help universal training. Taking AutomaticCheck-out (ACO) as the typical scenario, extensive experiments including white-box and black-box settings in both digital-world(RPC, the largest ACO related dataset) and physical-world scenario(Taobao and JD, the world' s largest online shopping platforms) are conducted. Experimental results demonstrate that our proposed framework outperforms state-of-the-art adversarial patch attack methods.



### Synthesizing Unrestricted False Positive Adversarial Objects Using Generative Models
- **Arxiv ID**: http://arxiv.org/abs/2005.09294v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.09294v1)
- **Published**: 2020-05-19 08:58:58+00:00
- **Updated**: 2020-05-19 08:58:58+00:00
- **Authors**: Martin Kotuliak, Sandro E. Schoenborn, Andrei Dan
- **Comment**: None
- **Journal**: None
- **Summary**: Adversarial examples are data points misclassified by neural networks. Originally, adversarial examples were limited to adding small perturbations to a given image. Recent work introduced the generalized concept of unrestricted adversarial examples, without limits on the added perturbations. In this paper, we introduce a new category of attacks that create unrestricted adversarial examples for object detection. Our key idea is to generate adversarial objects that are unrelated to the classes identified by the target object detector. Different from previous attacks, we use off-the-shelf Generative Adversarial Networks (GAN), without requiring any further training or modification. Our method consists of searching over the latent normal space of the GAN for adversarial objects that are wrongly identified by the target object detector. We evaluate this method on the commonly used Faster R-CNN ResNet-101, Inception v2 and SSD Mobilenet v1 object detectors using logo generative iWGAN-LC and SNGAN trained on CIFAR-10. The empirical results show that the generated adversarial objects are indistinguishable from non-adversarial objects generated by the GANs, transferable between the object detectors and robust in the physical world. This is the first work to study unrestricted false positive adversarial examples for object detection.



### AdaptiveWeighted Attention Network with Camera Spectral Sensitivity Prior for Spectral Reconstruction from RGB Images
- **Arxiv ID**: http://arxiv.org/abs/2005.09305v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.09305v1)
- **Published**: 2020-05-19 09:21:01+00:00
- **Updated**: 2020-05-19 09:21:01+00:00
- **Authors**: Jiaojiao Li, Chaoxiong Wu, Rui Song, Yunsong Li, Fei Liu
- **Comment**: The 1st ranking on the Clean track and the 3rd place only 1.59106e-4
  more than the 1st on the Real World track of the NTIRE 2020 Spectral
  Reconstruction Challenge
- **Journal**: None
- **Summary**: Recent promising effort for spectral reconstruction (SR) focuses on learning a complicated mapping through using a deeper and wider convolutional neural networks (CNNs). Nevertheless, most CNN-based SR algorithms neglect to explore the camera spectral sensitivity (CSS) prior and interdependencies among intermediate features, thus limiting the representation ability of the network and performance of SR. To conquer these issues, we propose a novel adaptive weighted attention network (AWAN) for SR, whose backbone is stacked with multiple dual residual attention blocks (DRAB) decorating with long and short skip connections to form the dual residual learning. Concretely, we investigate an adaptive weighted channel attention (AWCA) module to reallocate channel-wise feature responses via integrating correlations between channels. Furthermore, a patch-level second-order non-local (PSNL) module is developed to capture long-range spatial contextual information by second-order non-local operations for more powerful feature representations. Based on the fact that the recovered RGB images can be projected by the reconstructed hyperspectral image (HSI) and the given CSS function, we incorporate the discrepancies of the RGB images and HSIs as a finer constraint for more accurate reconstruction. Experimental results demonstrate the effectiveness of our proposed AWAN network in terms of quantitative comparison and perceptual quality over other state-of-the-art SR methods. In the NTIRE 2020 Spectral Reconstruction Challenge, our entries obtain the 1st ranking on the Clean track and the 3rd place on the Real World track. Codes are available at https://github.com/Deep-imagelab/AWAN.



### Localizing Firearm Carriers by Identifying Human-Object Pairs
- **Arxiv ID**: http://arxiv.org/abs/2005.09329v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.09329v2)
- **Published**: 2020-05-19 09:50:23+00:00
- **Updated**: 2020-05-20 09:49:30+00:00
- **Authors**: Abdul Basit, Muhammad Akhtar Munir, Mohsen Ali, Arif Mahmood
- **Comment**: 5 pages, accepted in IEEE ICIP 2020
- **Journal**: None
- **Summary**: Visual identification of gunmen in a crowd is a challenging problem, that requires resolving the association of a person with an object (firearm). We present a novel approach to address this problem, by defining human-object interaction (and non-interaction) bounding boxes. In a given image, human and firearms are separately detected. Each detected human is paired with each detected firearm, allowing us to create a paired bounding box that contains both object and the human. A network is trained to classify these paired-bounding-boxes into human carrying the identified firearm or not. Extensive experiments were performed to evaluate effectiveness of the algorithm, including exploiting full pose of the human, hand key-points, and their association with the firearm. The knowledge of spatially localized features is key to success of our method by using multi-size proposals with adaptive average pooling. We have also extended a previously firearm detection dataset, by adding more images and tagging in extended dataset the human-firearm pairs (including bounding boxes for firearms and gunmen). The experimental results ($AP_{hold} = 78.5$) demonstrate effectiveness of the proposed method.



### Uncertainty Estimation in Deep 2D Echocardiography Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2005.09349v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.09349v1)
- **Published**: 2020-05-19 10:19:23+00:00
- **Updated**: 2020-05-19 10:19:23+00:00
- **Authors**: Lavsen Dahal, Aayush Kafle, Bishesh Khanal
- **Comment**: None
- **Journal**: None
- **Summary**: 2D echocardiography is the most common imaging modality for cardiovascular diseases. The portability and relatively low-cost nature of Ultrasound (US) enable the US devices needed for performing echocardiography to be made widely available. However, acquiring and interpreting cardiac US images is operator dependent, limiting its use to only places where experts are present. Recently, Deep Learning (DL) has been used in 2D echocardiography for automated view classification, and structure and function assessment. Although these recent works show promise in developing computer-guided acquisition and automated interpretation of echocardiograms, most of these methods do not model and estimate uncertainty which can be important when testing on data coming from a distribution further away from that of the training data. Uncertainty estimates can be beneficial both during the image acquisition phase (by providing real-time feedback to the operator on acquired image's quality), and during automated measurement and interpretation. The performance of uncertainty models and quantification metric may depend on the prediction task and the models being compared. Hence, to gain insight of uncertainty modelling for left ventricular segmentation from US images, we compare three ensembling based uncertainty models quantified using four different metrics (one newly proposed) on state-of-the-art baseline networks using two publicly available echocardiogram datasets. We further demonstrate how uncertainty estimation can be used to automatically reject poor quality images and improve state-of-the-art segmentation results.



### Learning to segment clustered amoeboid cells from brightfield microscopy via multi-task learning with adaptive weight selection
- **Arxiv ID**: http://arxiv.org/abs/2005.09372v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.09372v1)
- **Published**: 2020-05-19 11:31:53+00:00
- **Updated**: 2020-05-19 11:31:53+00:00
- **Authors**: Rituparna Sarkar, Suvadip Mukherjee, Elisabeth Labruyère, Jean-Christophe Olivo-Marin
- **Comment**: None
- **Journal**: None
- **Summary**: Detecting and segmenting individual cells from microscopy images is critical to various life science applications. Traditional cell segmentation tools are often ill-suited for applications in brightfield microscopy due to poor contrast and intensity heterogeneity, and only a small subset are applicable to segment cells in a cluster. In this regard, we introduce a novel supervised technique for cell segmentation in a multi-task learning paradigm. A combination of a multi-task loss, based on the region and cell boundary detection, is employed for an improved prediction efficiency of the network. The learning problem is posed in a novel min-max framework which enables adaptive estimation of the hyper-parameters in an automatic fashion. The region and cell boundary predictions are combined via morphological operations and active contour model to segment individual cells.   The proposed methodology is particularly suited to segment touching cells from brightfield microscopy images without manual interventions. Quantitatively, we observe an overall Dice score of 0.93 on the validation set, which is an improvement of over 15.9% on a recent unsupervised method, and outperforms the popular supervised U-net algorithm by at least $5.8\%$ on average.



### hidden markov random fields and cuckoo search method for medical image segmentation
- **Arxiv ID**: http://arxiv.org/abs/2005.09377v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.09377v1)
- **Published**: 2020-05-19 11:54:03+00:00
- **Updated**: 2020-05-19 11:54:03+00:00
- **Authors**: EL-Hachemi Guerrout, Ramdane Mahiou, Dominique Michelucci, Boukabene Randa, Ouali Assia
- **Comment**: 5 pages, 2 figures, 8 tables
- **Journal**: None
- **Summary**: Segmentation of medical images is an essential part in the process of diagnostics. Physicians require an automatic, robust and valid results. Hidden Markov Random Fields (HMRF) provide powerful model. This latter models the segmentation problem as the minimization of an energy function. Cuckoo search (CS) algorithm is one of the recent nature-inspired meta-heuristic algorithms. It has shown its efficiency in many engineering optimization problems. In this paper, we use three cuckoo search algorithm to achieve medical image segmentation.



### MaskFace: multi-task face and landmark detector
- **Arxiv ID**: http://arxiv.org/abs/2005.09412v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.09412v1)
- **Published**: 2020-05-19 13:09:28+00:00
- **Updated**: 2020-05-19 13:09:28+00:00
- **Authors**: Dmitry Yashunin, Tamir Baydasov, Roman Vlasov
- **Comment**: None
- **Journal**: None
- **Summary**: Currently in the domain of facial analysis single task approaches for face detection and landmark localization dominate. In this paper we draw attention to multi-task models solving both tasks simultaneously. We present a highly accurate model for face and landmark detection. The method, called MaskFace, extends previous face detection approaches by adding a keypoint prediction head. The new keypoint head adopts ideas of Mask R-CNN by extracting facial features with a RoIAlign layer. The keypoint head adds small computational overhead in the case of few faces in the image while improving the accuracy dramatically. We evaluate MaskFace's performance on a face detection task on the AFW, PASCAL face, FDDB, WIDER FACE datasets and a landmark localization task on the AFLW, 300W datasets. For both tasks MaskFace achieves state-of-the-art results outperforming many of single-task and multi-task models.



### The Skincare project, an interactive deep learning system for differential diagnosis of malignant skin lesions. Technical Report
- **Arxiv ID**: http://arxiv.org/abs/2005.09448v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.09448v1)
- **Published**: 2020-05-19 13:51:17+00:00
- **Updated**: 2020-05-19 13:51:17+00:00
- **Authors**: Daniel Sonntag, Fabrizio Nunnari, Hans-Jürgen Profitlich
- **Comment**: 20 pages, 15 figures
- **Journal**: None
- **Summary**: A shortage of dermatologists causes long wait times for patients who seek dermatologic care. In addition, the diagnostic accuracy of general practitioners has been reported to be lower than the accuracy of artificial intelligence software. This article describes the Skincare project (H2020, EIT Digital). Contributions include enabling technology for clinical decision support based on interactive machine learning (IML), a reference architecture towards a Digital European Healthcare Infrastructure (also cf. EIT MCPS), technical components for aggregating digitised patient information, and the integration of decision support technology into clinical test-bed environments. However, the main contribution is a diagnostic and decision support system in dermatology for patients and doctors, an interactive deep learning system for differential diagnosis of malignant skin lesions. In this article, we describe its functionalities and the user interfaces to facilitate machine learning from human input. The baseline deep learning system, which delivers state-of-the-art results and the potential to augment general practitioners and even dermatologists, was developed and validated using de-identified cases from a dermatology image data base (ISIC), which has about 20000 cases for development and validation, provided by board-certified dermatologists defining the reference standard for every case. ISIC allows for differential diagnosis, a ranked list of eight diagnoses, that is used to plan treatments in the common setting of diagnostic ambiguity. We give an overall description of the outcome of the Skincare project, and we focus on the steps to support communication and coordination between humans and machine in IML. This is an integral part of the development of future cognitive assistants in the medical domain, and we describe the necessary intelligent user interfaces.



### Self-supervised Transfer Learning for Instance Segmentation through Physical Interaction
- **Arxiv ID**: http://arxiv.org/abs/2005.09484v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2005.09484v1)
- **Published**: 2020-05-19 14:31:24+00:00
- **Updated**: 2020-05-19 14:31:24+00:00
- **Authors**: Andreas Eitel, Nico Hauff, Wolfram Burgard
- **Comment**: Extended version and code release of accepted IROS 2019 paper
- **Journal**: None
- **Summary**: Instance segmentation of unknown objects from images is regarded as relevant for several robot skills including grasping, tracking and object sorting. Recent results in computer vision have shown that large hand-labeled datasets enable high segmentation performance. To overcome the time-consuming process of manually labeling data for new environments, we present a transfer learning approach for robots that learn to segment objects by interacting with their environment in a self-supervised manner. Our robot pushes unknown objects on a table and uses information from optical flow to create training labels in the form of object masks. To achieve this, we fine-tune an existing DeepMask network for instance segmentation on the self-labeled training data acquired by the robot. We evaluate our trained network (SelfDeepMask) on a set of real images showing challenging and cluttered scenes with novel objects. Here, SelfDeepMask outperforms the DeepMask network trained on the COCO dataset by 9.5% in average precision. Furthermore, we combine our approach with recent approaches for training with noisy labels in order to better cope with induced label noise.



### Built Infrastructure Monitoring and Inspection Using UAVs and Vision-based Algorithms
- **Arxiv ID**: http://arxiv.org/abs/2005.09486v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.09486v1)
- **Published**: 2020-05-19 14:37:48+00:00
- **Updated**: 2020-05-19 14:37:48+00:00
- **Authors**: Khai Ky Ly, Manh Duong Phung
- **Comment**: None
- **Journal**: None
- **Summary**: This study presents an inspecting system using real-time control unmanned aerial vehicles (UAVs) to investigate structural surfaces. The system operates under favourable weather conditions to inspect a target structure, which is the Wentworth light rail base structure in this study. The system includes a drone, a GoPro HERO4 camera, a controller and a mobile phone. The drone takes off the ground manually in the testing field to collect the data requiring for later analysis. The images are taken through HERO 4 camera and then transferred in real time to the remote processing unit such as a ground control station by the wireless connection established by a Wi-Fi router. An image processing method has been proposed to detect defects or damages such as cracks. The method based on intensity histogram algorithms to exploit the pixel group related to the crack contained in the low intensity interval. Experiments, simulation and comparisons have been conducted to evaluate the performance and validity of the proposed system.



### RoadText-1K: Text Detection & Recognition Dataset for Driving Videos
- **Arxiv ID**: http://arxiv.org/abs/2005.09496v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.09496v1)
- **Published**: 2020-05-19 14:51:25+00:00
- **Updated**: 2020-05-19 14:51:25+00:00
- **Authors**: Sangeeth Reddy, Minesh Mathew, Lluis Gomez, Marcal Rusinol, Dimosthenis Karatzas., C. V. Jawahar
- **Comment**: to be published in ICRA 2020
- **Journal**: None
- **Summary**: Perceiving text is crucial to understand semantics of outdoor scenes and hence is a critical requirement to build intelligent systems for driver assistance and self-driving. Most of the existing datasets for text detection and recognition comprise still images and are mostly compiled keeping text in mind. This paper introduces a new "RoadText-1K" dataset for text in driving videos. The dataset is 20 times larger than the existing largest dataset for text in videos. Our dataset comprises 1000 video clips of driving without any bias towards text and with annotations for text bounding boxes and transcriptions in every frame. State of the art methods for text detection, recognition and tracking are evaluated on the new dataset and the results signify the challenges in unconstrained driving videos compared to existing datasets. This suggests that RoadText-1K is suited for research and development of reading systems, robust enough to be incorporated into more complex downstream tasks like driver assistance and self-driving. The dataset can be found at http://cvit.iiit.ac.in/research/projects/cvit-projects/roadtext-1k



### Toward Automated Classroom Observation: Multimodal Machine Learning to Estimate CLASS Positive Climate and Negative Climate
- **Arxiv ID**: http://arxiv.org/abs/2005.09525v3
- **DOI**: 10.1109/TAFFC.2021.3059209
- **Categories**: **cs.CV**, cs.LG, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2005.09525v3)
- **Published**: 2020-05-19 15:36:32+00:00
- **Updated**: 2021-07-23 15:24:49+00:00
- **Authors**: Anand Ramakrishnan, Brian Zylich, Erin Ottmar, Jennifer LoCasale-Crouch, Jacob Whitehill
- **Comment**: The authors discovered that the results are not reproducible
- **Journal**: IEEE Transactions on Affective Computing, 2021
- **Summary**: In this work we present a multi-modal machine learning-based system, which we call ACORN, to analyze videos of school classrooms for the Positive Climate (PC) and Negative Climate (NC) dimensions of the CLASS observation protocol that is widely used in educational research. ACORN uses convolutional neural networks to analyze spectral audio features, the faces of teachers and students, and the pixels of each image frame, and then integrates this information over time using Temporal Convolutional Networks. The audiovisual ACORN's PC and NC predictions have Pearson correlations of $0.55$ and $0.63$ with ground-truth scores provided by expert CLASS coders on the UVA Toddler dataset (cross-validation on $n=300$ 15-min video segments), and a purely auditory ACORN predicts PC and NC with correlations of $0.36$ and $0.41$ on the MET dataset (test set of $n=2000$ videos segments). These numbers are similar to inter-coder reliability of human coders. Finally, using Graph Convolutional Networks we make early strides (AUC=$0.70$) toward predicting the specific moments (45-90sec clips) when the PC is particularly weak/strong. Our findings inform the design of automatic classroom observation and also more general video activity recognition and summary recognition systems.



### Differentiable Mapping Networks: Learning Structured Map Representations for Sparse Visual Localization
- **Arxiv ID**: http://arxiv.org/abs/2005.09530v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2005.09530v1)
- **Published**: 2020-05-19 15:43:39+00:00
- **Updated**: 2020-05-19 15:43:39+00:00
- **Authors**: Peter Karkus, Anelia Angelova, Vincent Vanhoucke, Rico Jonschkowski
- **Comment**: ICRA 2020
- **Journal**: None
- **Summary**: Mapping and localization, preferably from a small number of observations, are fundamental tasks in robotics. We address these tasks by combining spatial structure (differentiable mapping) and end-to-end learning in a novel neural network architecture: the Differentiable Mapping Network (DMN). The DMN constructs a spatially structured view-embedding map and uses it for subsequent visual localization with a particle filter. Since the DMN architecture is end-to-end differentiable, we can jointly learn the map representation and localization using gradient descent. We apply the DMN to sparse visual localization, where a robot needs to localize in a new environment with respect to a small number of images from known viewpoints. We evaluate the DMN using simulated environments and a challenging real-world Street View dataset. We find that the DMN learns effective map representations for visual localization. The benefit of spatial structure increases with larger environments, more viewpoints for mapping, and when training data is scarce. Project website: http://sites.google.com/view/differentiable-mapping



### Ultrasound Video Summarization using Deep Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2005.09531v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.09531v1)
- **Published**: 2020-05-19 15:44:18+00:00
- **Updated**: 2020-05-19 15:44:18+00:00
- **Authors**: Tianrui Liu, Qingjie Meng, Athanasios Vlontzos, Jeremy Tan, Daniel Rueckert, Bernhard Kainz
- **Comment**: Accepted by MICCAI'20
- **Journal**: None
- **Summary**: Video is an essential imaging modality for diagnostics, e.g. in ultrasound imaging, for endoscopy, or movement assessment. However, video hasn't received a lot of attention in the medical image analysis community. In the clinical practice, it is challenging to utilise raw diagnostic video data efficiently as video data takes a long time to process, annotate or audit. In this paper we introduce a novel, fully automatic video summarization method that is tailored to the needs of medical video data. Our approach is framed as reinforcement learning problem and produces agents focusing on the preservation of important diagnostic information. We evaluate our method on videos from fetal ultrasound screening, where commonly only a small amount of the recorded data is used diagnostically. We show that our method is superior to alternative video summarization methods and that it preserves essential information required by clinical diagnostic standards.



### CIAGAN: Conditional Identity Anonymization Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2005.09544v2
- **DOI**: 10.1109/CVPR42600.2020.00549
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.09544v2)
- **Published**: 2020-05-19 15:56:08+00:00
- **Updated**: 2020-11-30 17:12:44+00:00
- **Authors**: Maxim Maximov, Ismail Elezi, Laura Leal-Taixé
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: The unprecedented increase in the usage of computer vision technology in society goes hand in hand with an increased concern in data privacy. In many real-world scenarios like people tracking or action recognition, it is important to be able to process the data while taking careful consideration in protecting people's identity. We propose and develop CIAGAN, a model for image and video anonymization based on conditional generative adversarial networks. Our model is able to remove the identifying characteristics of faces and bodies while producing high-quality images and videos that can be used for any computer vision task, such as detection or tracking. Unlike previous methods, we have full control over the de-identification (anonymization) procedure, ensuring both anonymization as well as diversity. We compare our method to several baselines and achieve state-of-the-art results.



### Identifying Statistical Bias in Dataset Replication
- **Arxiv ID**: http://arxiv.org/abs/2005.09619v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.09619v2)
- **Published**: 2020-05-19 17:48:32+00:00
- **Updated**: 2020-09-02 06:38:04+00:00
- **Authors**: Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Jacob Steinhardt, Aleksander Madry
- **Comment**: None
- **Journal**: None
- **Summary**: Dataset replication is a useful tool for assessing whether improvements in test accuracy on a specific benchmark correspond to improvements in models' ability to generalize reliably. In this work, we present unintuitive yet significant ways in which standard approaches to dataset replication introduce statistical bias, skewing the resulting observations. We study ImageNet-v2, a replication of the ImageNet dataset on which models exhibit a significant (11-14%) drop in accuracy, even after controlling for a standard human-in-the-loop measure of data quality. We show that after correcting for the identified statistical bias, only an estimated $3.6\% \pm 1.5\%$ of the original $11.7\% \pm 1.0\%$ accuracy drop remains unaccounted for. We conclude with concrete recommendations for recognizing and avoiding bias in dataset replication. Code for our study is publicly available at http://github.com/MadryLab/dataset-replication-analysis .



### Focus on defocus: bridging the synthetic to real domain gap for depth estimation
- **Arxiv ID**: http://arxiv.org/abs/2005.09623v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.09623v1)
- **Published**: 2020-05-19 17:52:37+00:00
- **Updated**: 2020-05-19 17:52:37+00:00
- **Authors**: Maxim Maximov, Kevin Galim, Laura Leal-Taixé
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: Data-driven depth estimation methods struggle with the generalization outside their training scenes due to the immense variability of the real-world scenes. This problem can be partially addressed by utilising synthetically generated images, but closing the synthetic-real domain gap is far from trivial. In this paper, we tackle this issue by using domain invariant defocus blur as direct supervision. We leverage defocus cues by using a permutation invariant convolutional neural network that encourages the network to learn from the differences between images with a different point of focus. Our proposed network uses the defocus map as an intermediate supervisory signal. We are able to train our model completely on synthetic data and directly apply it to a wide range of real-world images. We evaluate our model on synthetic and real datasets, showing compelling generalization results and state-of-the-art depth prediction.



### Symbolic Pregression: Discovering Physical Laws from Distorted Video
- **Arxiv ID**: http://arxiv.org/abs/2005.11212v2
- **DOI**: 10.1103/PhysRevE.103.043307
- **Categories**: **cs.CV**, cs.AI, cs.LG, physics.comp-ph, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.11212v2)
- **Published**: 2020-05-19 18:00:52+00:00
- **Updated**: 2020-09-11 17:52:02+00:00
- **Authors**: Silviu-Marian Udrescu, Max Tegmark
- **Comment**: Expanded and improved physics discussion, additional method details.
  9 pages, 7 figs
- **Journal**: Phys. Rev. E 103, 043307 (2021)
- **Summary**: We present a method for unsupervised learning of equations of motion for objects in raw and optionally distorted unlabeled video. We first train an autoencoder that maps each video frame into a low-dimensional latent space where the laws of motion are as simple as possible, by minimizing a combination of non-linearity, acceleration and prediction error. Differential equations describing the motion are then discovered using Pareto-optimal symbolic regression. We find that our pre-regression ("pregression") step is able to rediscover Cartesian coordinates of unlabeled moving objects even when the video is distorted by a generalized lens. Using intuition from multidimensional knot-theory, we find that the pregression step is facilitated by first adding extra latent space dimensions to avoid topological problems during training and then removing these extra dimensions via principal component analysis.



### Weakly Supervised Representation Learning with Coarse Labels
- **Arxiv ID**: http://arxiv.org/abs/2005.09681v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.09681v3)
- **Published**: 2020-05-19 18:05:20+00:00
- **Updated**: 2021-08-24 04:33:45+00:00
- **Authors**: Yuanhong Xu, Qi Qian, Hao Li, Rong Jin, Juhua Hu
- **Comment**: accepted by ICCV'21
- **Journal**: None
- **Summary**: With the development of computational power and techniques for data collection, deep learning demonstrates a superior performance over most existing algorithms on visual benchmark data sets. Many efforts have been devoted to studying the mechanism of deep learning. One important observation is that deep learning can learn the discriminative patterns from raw materials directly in a task-dependent manner. Therefore, the representations obtained by deep learning outperform hand-crafted features significantly. However, for some real-world applications, it is too expensive to collect the task-specific labels, such as visual search in online shopping. Compared to the limited availability of these task-specific labels, their coarse-class labels are much more affordable, but representations learned from them can be suboptimal for the target task. To mitigate this challenge, we propose an algorithm to learn the fine-grained patterns for the target task, when only its coarse-class labels are available. More importantly, we provide a theoretical guarantee for this. Extensive experiments on real-world data sets demonstrate that the proposed method can significantly improve the performance of learned representations on the target task, when only coarse-class information is available for training. Code is available at \url{https://github.com/idstcv/CoIns}.



### Contextual Residual Aggregation for Ultra High-Resolution Image Inpainting
- **Arxiv ID**: http://arxiv.org/abs/2005.09704v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2005.09704v1)
- **Published**: 2020-05-19 18:55:32+00:00
- **Updated**: 2020-05-19 18:55:32+00:00
- **Authors**: Zili Yi, Qiang Tang, Shekoofeh Azizi, Daesik Jang, Zhan Xu
- **Comment**: CVPR 2020 oral paper. 22 pages, 11 figures
- **Journal**: None
- **Summary**: Recently data-driven image inpainting methods have made inspiring progress, impacting fundamental image editing tasks such as object removal and damaged image repairing. These methods are more effective than classic approaches, however, due to memory limitations they can only handle low-resolution inputs, typically smaller than 1K. Meanwhile, the resolution of photos captured with mobile devices increases up to 8K. Naive up-sampling of the low-resolution inpainted result can merely yield a large yet blurry result. Whereas, adding a high-frequency residual image onto the large blurry image can generate a sharp result, rich in details and textures. Motivated by this, we propose a Contextual Residual Aggregation (CRA) mechanism that can produce high-frequency residuals for missing contents by weighted aggregating residuals from contextual patches, thus only requiring a low-resolution prediction from the network. Since convolutional layers of the neural network only need to operate on low-resolution inputs and outputs, the cost of memory and computing power is thus well suppressed. Moreover, the need for high-resolution training datasets is alleviated. In our experiments, we train the proposed model on small images with resolutions 512x512 and perform inference on high-resolution images, achieving compelling inpainting quality. Our model can inpaint images as large as 8K with considerable hole sizes, which is intractable with previous learning-based approaches. We further elaborate on the light-weight design of the network architecture, achieving real-time performance on 2K images on a GTX 1080 Ti GPU. Codes are available at: Atlas200dk/sample-imageinpainting-HiFill.



### Inverse problems with second-order Total Generalized Variation constraints
- **Arxiv ID**: http://arxiv.org/abs/2005.09725v1
- **DOI**: None
- **Categories**: **math.NA**, cs.CV, cs.NA
- **Links**: [PDF](http://arxiv.org/pdf/2005.09725v1)
- **Published**: 2020-05-19 19:48:28+00:00
- **Updated**: 2020-05-19 19:48:28+00:00
- **Authors**: Kristian Bredies, Tuomo Valkonen
- **Comment**: Published in 2011 as a conference proceeding. Uploaded in 2020 on
  arXiv to ensure availability: the original proceedings are no longer online
- **Journal**: Proceedings of the 9th International Conference on Sampling Theory
  and Applications (SampTA) 2011, Singapore (2011)
- **Summary**: Total Generalized Variation (TGV) has recently been introduced as penalty functional for modelling images with edges as well as smooth variations. It can be interpreted as a "sparse" penalization of optimal balancing from the first up to the $k$-th distributional derivative and leads to desirable results when applied to image denoising, i.e., $L^2$-fitting with TGV penalty. The present paper studies TGV of second order in the context of solving ill-posed linear inverse problems. Existence and stability for solutions of Tikhonov-functional minimization with respect to the data is shown and applied to the problem of recovering an image from blurred and noisy data.



### On Evaluating Weakly Supervised Action Segmentation Methods
- **Arxiv ID**: http://arxiv.org/abs/2005.09743v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.09743v3)
- **Published**: 2020-05-19 20:30:31+00:00
- **Updated**: 2021-10-21 17:16:34+00:00
- **Authors**: Yaser Souri, Alexander Richard, Luca Minciullo, Juergen Gall
- **Comment**: Technical Report
- **Journal**: None
- **Summary**: Action segmentation is the task of temporally segmenting every frame of an untrimmed video. Weakly supervised approaches to action segmentation, especially from transcripts have been of considerable interest to the computer vision community. In this work, we focus on two aspects of the use and evaluation of weakly supervised action segmentation approaches that are often overlooked: the performance variance over multiple training runs and the impact of selecting feature extractors for this task. To tackle the first problem, we train each method on the Breakfast dataset 5 times and provide average and standard deviation of the results. Our experiments show that the standard deviation over these repetitions is between 1 and 2.5% and significantly affects the comparison between different approaches. Furthermore, our investigation on feature extraction shows that, for the studied weakly-supervised action segmentation methods, higher-level I3D features perform worse than classical IDT features.



### Medical Image Generation using Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2005.10687v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.10687v1)
- **Published**: 2020-05-19 20:31:57+00:00
- **Updated**: 2020-05-19 20:31:57+00:00
- **Authors**: Nripendra Kumar Singh, Khalid Raza
- **Comment**: 19 pages, 3 figures, 5 tables
- **Journal**: None
- **Summary**: Generative adversarial networks (GANs) are unsupervised Deep Learning approach in the computer vision community which has gained significant attention from the last few years in identifying the internal structure of multimodal medical imaging data. The adversarial network simultaneously generates realistic medical images and corresponding annotations, which proven to be useful in many cases such as image augmentation, image registration, medical image generation, image reconstruction, and image-to-image translation. These properties bring the attention of the researcher in the field of medical image analysis and we are witness of rapid adaption in many novel and traditional applications. This chapter provides state-of-the-art progress in GANs-based clinical application in medical image generation, and cross-modality synthesis. The various framework of GANs which gained popularity in the interpretation of medical images, such as Deep Convolutional GAN (DCGAN), Laplacian GAN (LAPGAN), pix2pix, CycleGAN, and unsupervised image-to-image translation model (UNIT), continue to improve their performance by incorporating additional hybrid architecture, has been discussed. Further, some of the recent applications of these frameworks for image reconstruction, and synthesis, and future research directions in the area have been covered.



### Self-supervised Dynamic CT Perfusion Image Denoising with Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2005.09766v1
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.09766v1)
- **Published**: 2020-05-19 21:44:07+00:00
- **Updated**: 2020-05-19 21:44:07+00:00
- **Authors**: Dufan Wu, Hui Ren, Quanzheng Li
- **Comment**: 13 pages, 9 figures
- **Journal**: None
- **Summary**: Dynamic computed tomography perfusion (CTP) imaging is a promising approach for acute ischemic stroke diagnosis and evaluation. Hemodynamic parametric maps of cerebral parenchyma are calculated from repeated CT scans of the first pass of iodinated contrast through the brain. It is necessary to reduce the dose of CTP for routine applications due to the high radiation exposure from the repeated scans, where image denoising is necessary to achieve a reliable diagnosis. In this paper, we proposed a self-supervised deep learning method for CTP denoising, which did not require any high-dose reference images for training. The network was trained by mapping each frame of CTP to an estimation from its adjacent frames. Because the noise in the source and target was independent, this approach could effectively remove the noise. Being free from high-dose training images granted the proposed method easier adaptation to different scanning protocols. The method was validated on both simulation and a public real dataset. The proposed method achieved improved image quality compared to conventional denoising methods. On the real data, the proposed method also had improved spatial resolution and contrast-to-noise ratio compared to supervised learning which was trained on the simulation data



### Unsupervised anomaly localization using VAE and beta-VAE
- **Arxiv ID**: http://arxiv.org/abs/2005.10686v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.10686v1)
- **Published**: 2020-05-19 21:58:59+00:00
- **Updated**: 2020-05-19 21:58:59+00:00
- **Authors**: Leixin Zhou, Wenxiang Deng, Xiaodong Wu
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2002.03734 by
  other authors
- **Journal**: None
- **Summary**: Variational Auto-Encoders (VAEs) have shown great potential in the unsupervised learning of data distributions. An VAE trained on normal images is expected to only be able to reconstruct normal images, allowing the localization of anomalous pixels in an image via manipulating information within the VAE ELBO loss. The ELBO consists of KL divergence loss (image-wise) and reconstruction loss (pixel-wise). It is natural and straightforward to use the later as the predictor. However, usually local anomaly added to a normal image can deteriorate the whole reconstructed image, causing segmentation using only naive pixel errors not accurate. Energy based projection was proposed to increase the reconstruction accuracy of normal regions/pixels, which achieved the state-of-the-art localization accuracy on simple natural images. Another possible predictors are ELBO and its components gradients with respect to each pixels. Previous work claimed that KL gradient is a robust predictor. In this paper, we argue that the energy based projection in medical imaging is not as useful as on natural images. Moreover, we observe that the robustness of KL gradient predictor totally depends on the setting of the VAE and dataset. We also explored the effect of the weight of KL loss within beta-VAE and predictor ensemble in anomaly localization.



