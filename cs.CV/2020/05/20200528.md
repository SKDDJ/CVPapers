# Arxiv Papers in cs.CV on 2020-05-28
### Anomaly Detection Based on Deep Learning Using Video for Prevention of Industrial Accidents
- **Arxiv ID**: http://arxiv.org/abs/2005.13734v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.13734v1)
- **Published**: 2020-05-28 01:53:57+00:00
- **Updated**: 2020-05-28 01:53:57+00:00
- **Authors**: Satoshi Hashimoto, Yonghoon Ji, Kenichi Kudo, Takayuki Takahashi, Kazunori Umeda
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes an anomaly detection method for the prevention of industrial accidents using machine learning technology.



### L^2UWE: A Framework for the Efficient Enhancement of Low-Light Underwater Images Using Local Contrast and Multi-Scale Fusion
- **Arxiv ID**: http://arxiv.org/abs/2005.13736v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.13736v2)
- **Published**: 2020-05-28 01:57:32+00:00
- **Updated**: 2020-11-05 21:26:23+00:00
- **Authors**: Tunai Porto Marques, Alexandra Branzan Albu
- **Comment**: Presented on the 2020 IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR) Workshop NTIRE: New Trends in Image Restoration
  and Enhancement. Code and dataset available at:
  https://github.com/tunai/l2uwe
- **Journal**: Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition Workshops, 2020, pp. 538-539
- **Summary**: Images captured underwater often suffer from suboptimal illumination settings that can hide important visual features, reducing their quality. We present a novel single-image low-light underwater image enhancer, L^2UWE, that builds on our observation that an efficient model of atmospheric lighting can be derived from local contrast information. We create two distinct models and generate two enhanced images from them: one that highlights finer details, the other focused on darkness removal. A multi-scale fusion process is employed to combine these images while emphasizing regions of higher luminance, saliency and local contrast. We demonstrate the performance of L^2UWE by using seven metrics to test it against seven state-of-the-art enhancement methods specific to underwater and low-light scenes. Code available at: https://github.com/tunai/l2uwe.



### Universal Lesion Detection by Learning from Multiple Heterogeneously Labeled Datasets
- **Arxiv ID**: http://arxiv.org/abs/2005.13753v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.13753v1)
- **Published**: 2020-05-28 02:56:00+00:00
- **Updated**: 2020-05-28 02:56:00+00:00
- **Authors**: Ke Yan, Jinzheng Cai, Adam P. Harrison, Dakai Jin, Jing Xiao, Le Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Lesion detection is an important problem within medical imaging analysis. Most previous work focuses on detecting and segmenting a specialized category of lesions (e.g., lung nodules). However, in clinical practice, radiologists are responsible for finding all possible types of anomalies. The task of universal lesion detection (ULD) was proposed to address this challenge by detecting a large variety of lesions from the whole body. There are multiple heterogeneously labeled datasets with varying label completeness: DeepLesion, the largest dataset of 32,735 annotated lesions of various types, but with even more missing annotation instances; and several fully-labeled single-type lesion datasets, such as LUNA for lung nodules and LiTS for liver tumors. In this work, we propose a novel framework to leverage all these datasets together to improve the performance of ULD. First, we learn a multi-head multi-task lesion detector using all datasets and generate lesion proposals on DeepLesion. Second, missing annotations in DeepLesion are retrieved by a new method of embedding matching that exploits clinical prior knowledge. Last, we discover suspicious but unannotated lesions using knowledge transfer from single-type lesion detectors. In this way, reliable positive and negative regions are obtained from partially-labeled and unlabeled images, which are effectively utilized to train ULD. To assess the clinically realistic protocol of 3D volumetric ULD, we fully annotated 1071 CT sub-volumes in DeepLesion. Our method outperforms the current state-of-the-art approach by 29% in the metric of average sensitivity.



### Stereo Vision Based Single-Shot 6D Object Pose Estimation for Bin-Picking by a Robot Manipulator
- **Arxiv ID**: http://arxiv.org/abs/2005.13759v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2005.13759v1)
- **Published**: 2020-05-28 03:15:20+00:00
- **Updated**: 2020-05-28 03:15:20+00:00
- **Authors**: Yoshihiro Nakano
- **Comment**: 7 pages, 8 figures
- **Journal**: None
- **Summary**: We propose a fast and accurate method of 6D object pose estimation for bin-picking of mechanical parts by a robot manipulator. We extend the single-shot approach to stereo vision by application of attention architecture. Our convolutional neural network model regresses to object locations and rotations from either a left image or a right image without depth information. Then, a stereo feature matching module, designated as Stereo Grid Attention, generates stereo grid matching maps. The important point of our method is only to calculate disparity of the objects found by the attention from stereo images, instead of calculating a point cloud over the entire image. The disparity value is then used to calculate the depth to the objects by the principle of triangulation. Our method also achieves a rapid processing speed of pose estimation by the single-shot architecture and it is possible to process a 1024 x 1024 pixels image in 75 milliseconds on the Jetson AGX Xavier implemented with half-float model. Weakly textured mechanical parts are used to exemplify the method. First, we create original synthetic datasets for training and evaluating of the proposed model. This dataset is created by capturing and rendering numerous 3D models of several types of mechanical parts in virtual space. Finally, we use a robotic manipulator with an electromagnetic gripper to pick up the mechanical parts in a cluttered state to verify the validity of our method in an actual scene. When a raw stereo image is used by the proposed method from our stereo camera to detect black steel screws, stainless screws, and DC motor parts, i.e., cases, rotor cores and commutator caps, the bin-picking tasks are successful with 76.3%, 64.0%, 50.5%, 89.1% and 64.2% probability, respectively.



### A Feature-map Discriminant Perspective for Pruning Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2005.13796v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.13796v1)
- **Published**: 2020-05-28 06:25:22+00:00
- **Updated**: 2020-05-28 06:25:22+00:00
- **Authors**: Zejiang Hou, Sun-Yuan Kung
- **Comment**: None
- **Journal**: None
- **Summary**: Network pruning has become the de facto tool to accelerate deep neural networks for mobile and edge applications. Recently, feature-map discriminant based channel pruning has shown promising results, as it aligns well with the CNN objective of differentiating multiple classes and offers better interpretability of the pruning decision. However, existing discriminant-based methods are challenged by computation inefficiency, as there is a lack of theoretical guidance on quantifying the feature-map discriminant power. In this paper, we present a new mathematical formulation to accurately and efficiently quantify the feature-map discriminativeness, which gives rise to a novel criterion,Discriminant Information(DI). We analyze the theoretical property of DI, specifically the non-decreasing property, that makes DI a valid selection criterion. DI-based pruning removes channels with minimum influence to DI value, as they contain little information regarding to the discriminant power. The versatility of DI criterion also enables an intra-layer mixed precision quantization to further compress the network. Moreover, we propose a DI-based greedy pruning algorithm and structure distillation technique to automatically decide the pruned structure that satisfies certain resource budget, which is a common requirement in reality. Extensive experiments demonstratethe effectiveness of our method: our pruned ResNet50 on ImageNet achieves 44% FLOPs reduction without any Top-1 accuracy loss compared to unpruned model



### 3D human pose estimation with adaptive receptive fields and dilated temporal convolutions
- **Arxiv ID**: http://arxiv.org/abs/2005.13797v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.13797v1)
- **Published**: 2020-05-28 06:25:41+00:00
- **Updated**: 2020-05-28 06:25:41+00:00
- **Authors**: Michael Shin, Eduardo Castillo, Irene Font Peradejordi, Shobhna Jayaraman
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we demonstrate that receptive fields in 3D pose estimation can be effectively specified using optical flow. We introduce adaptive receptive fields, a simple and effective method to aid receptive field selection in pose estimation models based on optical flow inference. We contrast the performance of a benchmark state-of-the-art model running on fixed receptive fields with their adaptive field counterparts. By using a reduced receptive field, our model can process slow-motion sequences (10x longer) 23% faster than the benchmark model running at regular speed. The reduction in computational cost is achieved while producing a pose prediction accuracy to within 0.36% of the benchmark model.



### Explainable deep learning models in medical image analysis
- **Arxiv ID**: http://arxiv.org/abs/2005.13799v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.13799v1)
- **Published**: 2020-05-28 06:31:05+00:00
- **Updated**: 2020-05-28 06:31:05+00:00
- **Authors**: Amitojdeep Singh, Sourya Sengupta, Vasudevan Lakshminarayanan
- **Comment**: Preprint submitted to J.Imaging, MDPI
- **Journal**: None
- **Summary**: Deep learning methods have been very effective for a variety of medical diagnostic tasks and has even beaten human experts on some of those. However, the black-box nature of the algorithms has restricted clinical use. Recent explainability studies aim to show the features that influence the decision of a model the most. The majority of literature reviews of this area have focused on taxonomy, ethics, and the need for explanations. A review of the current applications of explainable deep learning for different medical imaging tasks is presented here. The various approaches, challenges for clinical deployment, and the areas requiring further research are discussed here from a practical standpoint of a deep learning researcher designing a system for the clinical end-users.



### TOAN: Target-Oriented Alignment Network for Fine-Grained Image Categorization with Few Labeled Samples
- **Arxiv ID**: http://arxiv.org/abs/2005.13820v2
- **DOI**: 10.1109/TCSVT.2021.3065693
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.13820v2)
- **Published**: 2020-05-28 07:48:44+00:00
- **Updated**: 2021-03-10 05:40:46+00:00
- **Authors**: Huaxi Huang, Junjie Zhang, Jian Zhang, Qiang Wu, Chang Xu
- **Comment**: T-CSVT Accepted
- **Journal**: None
- **Summary**: The challenges of high intra-class variance yet low inter-class fluctuations in fine-grained visual categorization are more severe with few labeled samples, \textit{i.e.,} Fine-Grained categorization problems under the Few-Shot setting (FGFS). High-order features are usually developed to uncover subtle differences between sub-categories in FGFS, but they are less effective in handling the high intra-class variance. In this paper, we propose a Target-Oriented Alignment Network (TOAN) to investigate the fine-grained relation between the target query image and support classes. The feature of each support image is transformed to match the query ones in the embedding feature space, which reduces the disparity explicitly within each category. Moreover, different from existing FGFS approaches devise the high-order features over the global image with less explicit consideration of discriminative parts, we generate discriminative fine-grained features by integrating compositional concept representations to global second-order pooling. Extensive experiments are conducted on four fine-grained benchmarks to demonstrate the effectiveness of TOAN compared with the state-of-the-art models.



### Boosting Few-Shot Learning With Adaptive Margin Loss
- **Arxiv ID**: http://arxiv.org/abs/2005.13826v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.13826v1)
- **Published**: 2020-05-28 07:58:41+00:00
- **Updated**: 2020-05-28 07:58:41+00:00
- **Authors**: Aoxue Li, Weiran Huang, Xu Lan, Jiashi Feng, Zhenguo Li, Liwei Wang
- **Comment**: Accepted by CVPR 2020
- **Journal**: None
- **Summary**: Few-shot learning (FSL) has attracted increasing attention in recent years but remains challenging, due to the intrinsic difficulty in learning to generalize from a few examples. This paper proposes an adaptive margin principle to improve the generalization ability of metric-based meta-learning approaches for few-shot learning problems. Specifically, we first develop a class-relevant additive margin loss, where semantic similarity between each pair of classes is considered to separate samples in the feature embedding space from similar classes. Further, we incorporate the semantic context among all classes in a sampled training task and develop a task-relevant additive margin loss to better distinguish samples from different classes. Our adaptive margin method can be easily extended to a more realistic generalized FSL setting. Extensive experiments demonstrate that the proposed method can boost the performance of current metric-based meta-learning approaches, under both the standard FSL and generalized FSL settings.



### Enhanced nonconvex low-rank approximation of tensor multi-modes for tensor completion
- **Arxiv ID**: http://arxiv.org/abs/2005.14521v2
- **DOI**: None
- **Categories**: **cs.CV**, 94A12
- **Links**: [PDF](http://arxiv.org/pdf/2005.14521v2)
- **Published**: 2020-05-28 08:53:54+00:00
- **Updated**: 2020-06-22 08:58:14+00:00
- **Authors**: Haijin Zeng, Xiaozhen Xie, Jifeng Ning
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2004.08747
- **Journal**: None
- **Summary**: Higher-order low-rank tensor arises in many data processing applications and has attracted great interests. Inspired by low-rank approximation theory, researchers have proposed a series of effective tensor completion methods. However, most of these methods directly consider the global low-rankness of underlying tensors, which is not sufficient for a low sampling rate; in addition, the single nuclear norm or its relaxation is usually adopted to approximate the rank function, which would lead to suboptimal solution deviated from the original one. To alleviate the above problems, in this paper, we propose a novel low-rank approximation of tensor multi-modes (LRATM), in which a double nonconvex $L_{\gamma}$ norm is designed to represent the underlying joint-manifold drawn from the modal factorization factors of the underlying tensor. A block successive upper-bound minimization method-based algorithm is designed to efficiently solve the proposed model, and it can be demonstrated that our numerical scheme converges to the coordinatewise minimizers. Numerical results on three types of public multi-dimensional datasets have tested and shown that our algorithm can recover a variety of low-rank tensors with significantly fewer samples than the compared methods.



### Traditional Method Inspired Deep Neural Network for Edge Detection
- **Arxiv ID**: http://arxiv.org/abs/2005.13862v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.13862v1)
- **Published**: 2020-05-28 09:20:37+00:00
- **Updated**: 2020-05-28 09:20:37+00:00
- **Authors**: Jan Kristanto Wibisono, Hsueh-Ming Hang
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, Deep-Neural-Network (DNN) based edge prediction is progressing fast. Although the DNN based schemes outperform the traditional edge detectors, they have much higher computational complexity. It could be that the DNN based edge detectors often adopt the neural net structures designed for high-level computer vision tasks, such as image segmentation and object recognition. Edge detection is a rather local and simple job, the over-complicated architecture and massive parameters may be unnecessary. Therefore, we propose a traditional method inspired framework to produce good edges with minimal complexity. We simplify the network architecture to include Feature Extractor, Enrichment, and Summarizer, which roughly correspond to gradient, low pass filter, and pixel connection in the traditional edge detection schemes. The proposed structure can effectively reduce the complexity and retain the edge prediction quality. Our TIN2 (Traditional Inspired Network) model has an accuracy higher than the recent BDCN2 (Bi-Directional Cascade Network) but with a smaller model.



### Learning Various Length Dependence by Dual Recurrent Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2005.13867v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CL, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.13867v1)
- **Published**: 2020-05-28 09:30:01+00:00
- **Updated**: 2020-05-28 09:30:01+00:00
- **Authors**: Chenpeng Zhang, Shuai Li, Mao Ye, Ce Zhu, Xue Li
- **Comment**: None
- **Journal**: None
- **Summary**: Recurrent neural networks (RNNs) are widely used as a memory model for sequence-related problems. Many variants of RNN have been proposed to solve the gradient problems of training RNNs and process long sequences. Although some classical models have been proposed, capturing long-term dependence while responding to short-term changes remains a challenge. To this problem, we propose a new model named Dual Recurrent Neural Networks (DuRNN). The DuRNN consists of two parts to learn the short-term dependence and progressively learn the long-term dependence. The first part is a recurrent neural network with constrained full recurrent connections to deal with short-term dependence in sequence and generate short-term memory. Another part is a recurrent neural network with independent recurrent connections which helps to learn long-term dependence and generate long-term memory. A selection mechanism is added between two parts to help the needed long-term information transfer to the independent neurons. Multiple modules can be stacked to form a multi-layer model for better performance. Our contributions are: 1) a new recurrent model developed based on the divide-and-conquer strategy to learn long and short-term dependence separately, and 2) a selection mechanism to enhance the separating and learning of different temporal scales of dependence. Both theoretical analysis and extensive experiments are conducted to validate the performance of our model, and we also conduct simple visualization experiments and ablation analyses for the model interpretability. Experimental results indicate that the proposed DuRNN model can handle not only very long sequences (over 5000 time steps), but also short sequences very well. Compared with many state-of-the-art RNN models, our model has demonstrated efficient and better performance.



### CGGAN: A Context Guided Generative Adversarial Network For Single Image Dehazing
- **Arxiv ID**: http://arxiv.org/abs/2005.13884v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.13884v1)
- **Published**: 2020-05-28 10:14:30+00:00
- **Updated**: 2020-05-28 10:14:30+00:00
- **Authors**: Zhaorun Zhou, Zhenghao Shi, Mingtao Guo, Yaning Feng, Minghua Zhao
- **Comment**: 12 pages, 7 figures, 3 tables
- **Journal**: None
- **Summary**: Image haze removal is highly desired for the application of computer vision. This paper proposes a novel Context Guided Generative Adversarial Network (CGGAN) for single image dehazing. Of which, an novel new encoder-decoder is employed as the generator. And it consists of a feature-extraction-net, a context-extractionnet, and a fusion-net in sequence. The feature extraction-net acts as a encoder, and is used for extracting haze features. The context-extraction net is a multi-scale parallel pyramid decoder, and is used for extracting the deep features of the encoder and generating coarse dehazing image. The fusion-net is a decoder, and is used for obtaining the final haze-free image. To obtain more better results, multi-scale information obtained during the decoding process of the context extraction decoder is used for guiding the fusion decoder. By introducing an extra coarse decoder to the original encoder-decoder, the CGGAN can make better use of the deep feature information extracted by the encoder. To ensure our CGGAN work effectively for different haze scenarios, different loss functions are employed for the two decoders. Experiments results show the advantage and the effectiveness of our proposed CGGAN, evidential improvements over existing state-of-the-art methods are obtained.



### P2B: Point-to-Box Network for 3D Object Tracking in Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2005.13888v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.13888v1)
- **Published**: 2020-05-28 10:25:12+00:00
- **Updated**: 2020-05-28 10:25:12+00:00
- **Authors**: Haozhe Qi, Chen Feng, Zhiguo Cao, Feng Zhao, Yang Xiao
- **Comment**: Accepted by CVPR 2020 (oral)
- **Journal**: None
- **Summary**: Towards 3D object tracking in point clouds, a novel point-to-box network termed P2B is proposed in an end-to-end learning manner. Our main idea is to first localize potential target centers in 3D search area embedded with target information. Then point-driven 3D target proposal and verification are executed jointly. In this way, the time-consuming 3D exhaustive search can be avoided. Specifically, we first sample seeds from the point clouds in template and search area respectively. Then, we execute permutation-invariant feature augmentation to embed target clues from template into search area seeds and represent them with target-specific features. Consequently, the augmented search area seeds regress the potential target centers via Hough voting. The centers are further strengthened with seed-wise targetness scores. Finally, each center clusters its neighbors to leverage the ensemble power for joint 3D target proposal and verification. We apply PointNet++ as our backbone and experiments on KITTI tracking dataset demonstrate P2B's superiority (~10%'s improvement over state-of-the-art). Note that P2B can run with 40FPS on a single NVIDIA 1080Ti GPU. Our code and model are available at https://github.com/HaozheQi/P2B.



### Deep Learning for Automatic Pneumonia Detection
- **Arxiv ID**: http://arxiv.org/abs/2005.13899v1
- **DOI**: 10.1109/CVPRW50498.2020.00183
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.13899v1)
- **Published**: 2020-05-28 10:54:34+00:00
- **Updated**: 2020-05-28 10:54:34+00:00
- **Authors**: Tatiana Gabruseva, Dmytro Poplavskiy, Alexandr A. Kalinin
- **Comment**: to appear in CVPR 2020 Workshops proceedings
- **Journal**: 2020 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition Workshops (CVPRW), Seattle, WA, USA, 2020, pp. 1436-1443
- **Summary**: Pneumonia is the leading cause of death among young children and one of the top mortality causes worldwide. The pneumonia detection is usually performed through examine of chest X-ray radiograph by highly-trained specialists. This process is tedious and often leads to a disagreement between radiologists. Computer-aided diagnosis systems showed the potential for improving diagnostic accuracy. In this work, we develop the computational approach for pneumonia regions detection based on single-shot detectors, squeeze-and-excitation deep convolution neural networks, augmentations and multi-task learning. The proposed approach was evaluated in the context of the Radiological Society of North America Pneumonia Detection Challenge, achieving one of the best results in the challenge.



### CNN-based Approach for Cervical Cancer Classification in Whole-Slide Histopathology Images
- **Arxiv ID**: http://arxiv.org/abs/2005.13924v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.13924v1)
- **Published**: 2020-05-28 11:45:23+00:00
- **Updated**: 2020-05-28 11:45:23+00:00
- **Authors**: Ferdaous Idlahcen, Mohammed Majid Himmi, Abdelhak Mahmoudi
- **Comment**: Presented at the ICLR 2020 Workshop on AI for Overcoming Global
  Disparities in Cancer Care (AI4CC)
- **Journal**: None
- **Summary**: Cervical cancer will cause 460 000 deaths per year by 2040, approximately 90% are Sub-Saharan African women. A constantly increasing incidence in Africa making cervical cancer a priority by the World Health Organization (WHO) in terms of screening, diagnosis, and treatment. Conventionally, cancer diagnosis relies primarily on histopathological assessment, a deeply error-prone procedure requiring intelligent computer-aided systems as low-cost patient safety mechanisms but lack of labeled data in digital pathology limits their applicability. In this study, few cervical tissue digital slides from TCGA data portal were pre-processed to overcome whole-slide images obstacles and included in our proposed VGG16-CNN classification approach. Our results achieved an accuracy of 98,26% and an F1-score of 97,9%, which confirm the potential of transfer learning on this weakly-supervised task.



### Early Screening of SARS-CoV-2 by Intelligent Analysis of X-Ray Images
- **Arxiv ID**: http://arxiv.org/abs/2005.13928v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.13928v1)
- **Published**: 2020-05-28 11:46:31+00:00
- **Updated**: 2020-05-28 11:46:31+00:00
- **Authors**: D. Gil, K. Díaz-Chito, C. Sánchez, A. Hernández-Sabaté
- **Comment**: None
- **Journal**: None
- **Summary**: Future SARS-CoV-2 virus outbreak COVID-XX might possibly occur during the next years. However the pathology in humans is so recent that many clinical aspects, like early detection of complications, side effects after recovery or early screening, are currently unknown. In spite of the number of cases of COVID-19, its rapid spread putting many sanitary systems in the edge of collapse has hindered proper collection and analysis of the data related to COVID-19 clinical aspects. We describe an interdisciplinary initiative that integrates clinical research, with image diagnostics and the use of new technologies such as artificial intelligence and radiomics with the aim of clarifying some of SARS-CoV-2 open questions. The whole initiative addresses 3 main points: 1) collection of standardize data including images, clinical data and analytics; 2) COVID-19 screening for its early diagnosis at primary care centers; 3) define radiomic signatures of COVID-19 evolution and associated pathologies for the early treatment of complications. In particular, in this paper we present a general overview of the project, the experimental design and first results of X-ray COVID-19 detection using a classic approach based on HoG and feature selection. Our experiments include a comparison to some recent methods for COVID-19 screening in X-Ray and an exploratory analysis of the feasibility of X-Ray COVID-19 screening. Results show that classic approaches can outperform deep-learning methods in this experimental setting, indicate the feasibility of early COVID-19 screening and that non-COVID infiltration is the group of patients most similar to COVID-19 in terms of radiological description of X-ray. Therefore, an efficient COVID-19 screening should be complemented with other clinical data to better discriminate these cases.



### Quantifying the Complexity of Standard Benchmarking Datasets for Long-Term Human Trajectory Prediction
- **Arxiv ID**: http://arxiv.org/abs/2005.13934v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.13934v4)
- **Published**: 2020-05-28 12:00:41+00:00
- **Updated**: 2021-05-20 08:17:40+00:00
- **Authors**: Ronny Hug, Stefan Becker, Wolfgang Hübner, Michael Arens
- **Comment**: None
- **Journal**: None
- **Summary**: Methods to quantify the complexity of trajectory datasets are still a missing piece in benchmarking human trajectory prediction models. In order to gain a better understanding of the complexity of trajectory prediction tasks and following the intuition, that more complex datasets contain more information, an approach for quantifying the amount of information contained in a dataset from a prototype-based dataset representation is proposed. The dataset representation is obtained by first employing a non-trivial spatial sequence alignment, which enables a subsequent learning vector quantization (LVQ) stage. A large-scale complexity analysis is conducted on several human trajectory prediction benchmarking datasets, followed by a brief discussion on indications for human trajectory prediction and benchmarking.



### Disentanglement Then Reconstruction: Learning Compact Features for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2005.13947v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.13947v1)
- **Published**: 2020-05-28 12:30:12+00:00
- **Updated**: 2020-05-28 12:30:12+00:00
- **Authors**: Lihua Zhou, Mao Ye, Xinpeng Li, Ce Zhu, Yiguang Liu, Xue Li
- **Comment**: None
- **Journal**: None
- **Summary**: Recent works in domain adaptation always learn domain invariant features to mitigate the gap between the source and target domains by adversarial methods. The category information are not sufficiently used which causes the learned domain invariant features are not enough discriminative. We propose a new domain adaptation method based on prototype construction which likes capturing data cluster centers. Specifically, it consists of two parts: disentanglement and reconstruction. First, the domain specific features and domain invariant features are disentangled from the original features. At the same time, the domain prototypes and class prototypes of both domains are estimated. Then, a reconstructor is trained by reconstructing the original features from the disentangled domain invariant features and domain specific features. By this reconstructor, we can construct prototypes for the original features using class prototypes and domain prototypes correspondingly. In the end, the feature extraction network is forced to extract features close to these prototypes. Our contribution lies in the technical use of the reconstructor to obtain the original feature prototypes which helps to learn compact and discriminant features. As far as we know, this idea is proposed for the first time. Experiment results on several public datasets confirm the state-of-the-art performance of our method.



### Improving Generalized Zero-Shot Learning by Semantic Discriminator
- **Arxiv ID**: http://arxiv.org/abs/2005.13956v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.13956v2)
- **Published**: 2020-05-28 12:48:38+00:00
- **Updated**: 2020-06-11 14:43:10+00:00
- **Authors**: Xinpeng Li
- **Comment**: None
- **Journal**: None
- **Summary**: It is a recognized fact that the classification accuracy of unseen classes in the setting of Generalized Zero-Shot Learning (GZSL) is much lower than that of traditional Zero-Shot Leaning (ZSL). One of the reasons is that an instance is always misclassified to the wrong domain. Here we refer to the seen and unseen classes as two domains respectively. We propose a new approach to distinguish whether the instances come from the seen or unseen classes. First the visual feature of instance is projected into the semantic space. Then the absolute norm difference between the projected semantic vector and the class semantic embedding vector, and the minimum distance between the projected semantic vectors and the semantic embedding vectors of the seen classes are used as discrimination basis. This approach is termed as SD (Semantic Discriminator) because domain judgement of instance is performed in the semantic space. Our approach can be combined with any existing ZSL method and fully supervision classification model to form a new GZSL method. Furthermore, our approach is very simple and does not need any fixed parameters.



### Robust Modeling of Epistemic Mental States
- **Arxiv ID**: http://arxiv.org/abs/2005.13982v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2005.13982v1)
- **Published**: 2020-05-28 13:34:45+00:00
- **Updated**: 2020-05-28 13:34:45+00:00
- **Authors**: AKMMahbubur Rahman, ASM Iftekhar Anam, Mohammed Yeasin
- **Comment**: Accepted for Publication in Multimedia Tools and Application, Special
  Issue: Socio-Affective Technologies
- **Journal**: None
- **Summary**: This work identifies and advances some research challenges in the analysis of facial features and their temporal dynamics with epistemic mental states in dyadic conversations. Epistemic states are: Agreement, Concentration, Thoughtful, Certain, and Interest. In this paper, we perform a number of statistical analyses and simulations to identify the relationship between facial features and epistemic states. Non-linear relations are found to be more prevalent, while temporal features derived from original facial features have demonstrated a strong correlation with intensity changes. Then, we propose a novel prediction framework that takes facial features and their nonlinear relation scores as input and predict different epistemic states in videos. The prediction of epistemic states is boosted when the classification of emotion changing regions such as rising, falling, or steady-state are incorporated with the temporal features. The proposed predictive models can predict the epistemic states with significantly improved accuracy: correlation coefficient (CoERR) for Agreement is 0.827, for Concentration 0.901, for Thoughtful 0.794, for Certain 0.854, and for Interest 0.913.



### Uncertainty-Aware Blind Image Quality Assessment in the Laboratory and Wild
- **Arxiv ID**: http://arxiv.org/abs/2005.13983v6
- **DOI**: 10.1109/TIP.2021.3061932
- **Categories**: **cs.CV**, cs.LG, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.13983v6)
- **Published**: 2020-05-28 13:35:23+00:00
- **Updated**: 2021-02-23 09:45:41+00:00
- **Authors**: Weixia Zhang, Kede Ma, Guangtao Zhai, Xiaokang Yang
- **Comment**: Accepted to IEEE TIP. The implementations are available at
  https://github.com/zwx8981/UNIQUE
- **Journal**: None
- **Summary**: Performance of blind image quality assessment (BIQA) models has been significantly boosted by end-to-end optimization of feature engineering and quality regression. Nevertheless, due to the distributional shift between images simulated in the laboratory and captured in the wild, models trained on databases with synthetic distortions remain particularly weak at handling realistic distortions (and vice versa). To confront the cross-distortion-scenario challenge, we develop a \textit{unified} BIQA model and an approach of training it for both synthetic and realistic distortions. We first sample pairs of images from individual IQA databases, and compute a probability that the first image of each pair is of higher quality. We then employ the fidelity loss to optimize a deep neural network for BIQA over a large number of such image pairs. We also explicitly enforce a hinge constraint to regularize uncertainty estimation during optimization. Extensive experiments on six IQA databases show the promise of the learned method in blindly assessing image quality in the laboratory and wild. In addition, we demonstrate the universality of the proposed training strategy by using it to improve existing BIQA models.



### Perception-aware time optimal path parameterization for quadrotors
- **Arxiv ID**: http://arxiv.org/abs/2005.13986v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.13986v1)
- **Published**: 2020-05-28 13:40:07+00:00
- **Updated**: 2020-05-28 13:40:07+00:00
- **Authors**: Igor Spasojevic, Varun Murali, Sertac Karaman
- **Comment**: Accepted to appear at ICRA 2020
- **Journal**: None
- **Summary**: The increasing popularity of quadrotors has given rise to a class of predominantly vision-driven vehicles. This paper addresses the problem of perception-aware time optimal path parametrization for quadrotors. Although many different choices of perceptual modalities are available, the low weight and power budgets of quadrotor systems makes a camera ideal for on-board navigation and estimation algorithms. However, this does come with a set of challenges. The limited field of view of the camera can restrict the visibility of salient regions in the environment, which dictates the necessity to consider perception and planning jointly. The main contribution of this paper is an efficient time optimal path parametrization algorithm for quadrotors with limited field of view constraints. We show in a simulation study that a state-of-the-art controller can track planned trajectories, and we validate the proposed algorithm on a quadrotor platform in experiments.



### A Normalized Fully Convolutional Approach to Head and Neck Cancer Outcome Prediction
- **Arxiv ID**: http://arxiv.org/abs/2005.14017v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.14017v2)
- **Published**: 2020-05-28 14:00:47+00:00
- **Updated**: 2020-05-29 14:31:06+00:00
- **Authors**: William Le, Francisco Perdigón Romero, Samuel Kadoury
- **Comment**: 6 pages, 1 figure, 1 table, Medical Imaging with Deep Learning 2020
  conference
- **Journal**: None
- **Summary**: In medical imaging, radiological scans of different modalities serve to enhance different sets of features for clinical diagnosis and treatment planning. This variety enriches the source information that could be used for outcome prediction. Deep learning methods are particularly well-suited for feature extraction from high-dimensional inputs such as images. In this work, we apply a CNN classification network augmented with a FCN preprocessor sub-network to a public TCIA head and neck cancer dataset. The training goal is survival prediction of radiotherapy cases based on pre-treatment FDG PET-CT scans, acquired across 4 different hospitals. We show that the preprocessor sub-network in conjunction with aggregated residual connection leads to improvements over state-of-the-art results when combining both CT and PET input images.



### G1020: A Benchmark Retinal Fundus Image Dataset for Computer-Aided Glaucoma Detection
- **Arxiv ID**: http://arxiv.org/abs/2006.09158v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.09158v1)
- **Published**: 2020-05-28 14:29:03+00:00
- **Updated**: 2020-05-28 14:29:03+00:00
- **Authors**: Muhammad Naseer Bajwa, Gur Amrit Pal Singh, Wolfgang Neumeier, Muhammad Imran Malik, Andreas Dengel, Sheraz Ahmed
- **Comment**: Accepted in IJCNN-2020, 7 pages, 5 figures
- **Journal**: None
- **Summary**: Scarcity of large publicly available retinal fundus image datasets for automated glaucoma detection has been the bottleneck for successful application of artificial intelligence towards practical Computer-Aided Diagnosis (CAD). A few small datasets that are available for research community usually suffer from impractical image capturing conditions and stringent inclusion criteria. These shortcomings in already limited choice of existing datasets make it challenging to mature a CAD system so that it can perform in real-world environment. In this paper we present a large publicly available retinal fundus image dataset for glaucoma classification called G1020. The dataset is curated by conforming to standard practices in routine ophthalmology and it is expected to serve as standard benchmark dataset for glaucoma detection. This database consists of 1020 high resolution colour fundus images and provides ground truth annotations for glaucoma diagnosis, optic disc and optic cup segmentation, vertical cup-to-disc ratio, size of neuroretinal rim in inferior, superior, nasal and temporal quadrants, and bounding box location for optic disc. We also report baseline results by conducting extensive experiments for automated glaucoma diagnosis and segmentation of optic disc and optic cup.



### Unsupervised learning of multimodal image registration using domain adaptation with projected Earth Move's discrepancies
- **Arxiv ID**: http://arxiv.org/abs/2005.14107v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.14107v1)
- **Published**: 2020-05-28 15:57:21+00:00
- **Updated**: 2020-05-28 15:57:21+00:00
- **Authors**: Mattias P Heinrich, Lasse Hansen
- **Comment**: Medical Imaging with Deep Learning (accepted short paper)
  https://openreview.net/forum?id=wbZM-DcJB9
- **Journal**: None
- **Summary**: Multimodal image registration is a very challenging problem for deep learning approaches. Most current work focuses on either supervised learning that requires labelled training scans and may yield models that bias towards annotated structures or unsupervised approaches that are based on hand-crafted similarity metrics and may therefore not outperform their classical non-trained counterparts. We believe that unsupervised domain adaptation can be beneficial in overcoming the current limitations for multimodal registration, where good metrics are hard to define. Domain adaptation has so far been mainly limited to classification problems. We propose the first use of unsupervised domain adaptation for discrete multimodal registration. Based on a source domain for which quantised displacement labels are available as supervision, we transfer the output distribution of the network to better resemble the target domain (other modality) using classifier discrepancies. To improve upon the sliced Wasserstein metric for 2D histograms, we present a novel approximation that projects predictions into 1D and computes the L1 distance of their cumulative sums. Our proof-of-concept demonstrates the applicability of domain transfer from mono- to multimodal (multi-contrast) 2D registration of canine MRI scans and improves the registration accuracy from 33% (using sliced Wasserstein) to 44%.



### Heatmap-Based Method for Estimating Drivers' Cognitive Distraction
- **Arxiv ID**: http://arxiv.org/abs/2005.14136v2
- **DOI**: 10.1109/ICCICC50026.2020.9450216
- **Categories**: **cs.HC**, cs.CV, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2005.14136v2)
- **Published**: 2020-05-28 16:37:30+00:00
- **Updated**: 2020-10-31 16:47:18+00:00
- **Authors**: Antonyo Musabini, Mounsif Chetitah
- **Comment**: Accepted at IEEE ICCI*CC 2020 (matching camera-ready version)
- **Journal**: None
- **Summary**: In order to increase road safety, among the visual and manual distractions, modern intelligent vehicles need also to detect cognitive distracted driving (i.e., the drivers mind wandering). In this study, the influence of cognitive processes on the drivers gaze behavior is explored. A novel image-based representation of the driver's eye-gaze dispersion is proposed to estimate cognitive distraction. Data are collected on open highway roads, with a tailored protocol to create cognitive distraction. The visual difference of created shapes shows that a driver explores a wider area in neutral driving compared to distracted driving. Thus, support vector machine (SVM)-based classifiers are trained, and 85.2% of accuracy is achieved for a two-class problem, even with a small dataset. Thus, the proposed method has the discriminative power to recognize cognitive distraction using gaze information. Finally, this work details how this image-based representation could be useful for other cases of distracted driving detection.



### QEBA: Query-Efficient Boundary-Based Blackbox Attack
- **Arxiv ID**: http://arxiv.org/abs/2005.14137v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.14137v1)
- **Published**: 2020-05-28 16:41:12+00:00
- **Updated**: 2020-05-28 16:41:12+00:00
- **Authors**: Huichen Li, Xiaojun Xu, Xiaolu Zhang, Shuang Yang, Bo Li
- **Comment**: Accepted by CVPR 2020
- **Journal**: None
- **Summary**: Machine learning (ML), especially deep neural networks (DNNs) have been widely used in various applications, including several safety-critical ones (e.g. autonomous driving). As a result, recent research about adversarial examples has raised great concerns. Such adversarial attacks can be achieved by adding a small magnitude of perturbation to the input to mislead model prediction. While several whitebox attacks have demonstrated their effectiveness, which assume that the attackers have full access to the machine learning models; blackbox attacks are more realistic in practice. In this paper, we propose a Query-Efficient Boundary-based blackbox Attack (QEBA) based only on model's final prediction labels. We theoretically show why previous boundary-based attack with gradient estimation on the whole gradient space is not efficient in terms of query numbers, and provide optimality analysis for our dimension reduction-based gradient estimation. On the other hand, we conducted extensive experiments on ImageNet and CelebA datasets to evaluate QEBA. We show that compared with the state-of-the-art blackbox attacks, QEBA is able to use a smaller number of queries to achieve a lower magnitude of perturbation with 100% attack success rate. We also show case studies of attacks on real-world APIs including MEGVII Face++ and Microsoft Azure.



### Modeling the Distribution of Normal Data in Pre-Trained Deep Features for Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2005.14140v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.14140v2)
- **Published**: 2020-05-28 16:43:41+00:00
- **Updated**: 2020-10-23 13:51:36+00:00
- **Authors**: Oliver Rippel, Patrick Mertens, Dorit Merhof
- **Comment**: Camera-ready for ICPR2020 (8 pages + 4 pages appendix). First two
  authors contributed equally to this work
- **Journal**: None
- **Summary**: Anomaly Detection (AD) in images is a fundamental computer vision problem and refers to identifying images and image substructures that deviate significantly from the norm. Popular AD algorithms commonly try to learn a model of normality from scratch using task specific datasets, but are limited to semi-supervised approaches employing mostly normal data due to the inaccessibility of anomalies on a large scale combined with the ambiguous nature of anomaly appearance.   We follow an alternative approach and demonstrate that deep feature representations learned by discriminative models on large natural image datasets are well suited to describe normality and detect even subtle anomalies in a transfer learning setting. Our model of normality is established by fitting a multivariate Gaussian (MVG) to deep feature representations of classification networks trained on ImageNet using normal data only. By subsequently applying the Mahalanobis distance as the anomaly score we outperform the current state of the art on the public MVTec AD dataset, achieving an AUROC value of $95.8 \pm 1.2$ (mean $\pm$ SEM) over all 15 classes. We further investigate why the learned representations are discriminative to the AD task using Principal Component Analysis. We find that the principal components containing little variance in normal data are the ones crucial for discriminating between normal and anomalous instances. This gives a possible explanation to the often sub-par performance of AD approaches trained from scratch using normal data only. By selectively fitting a MVG to these most relevant components only, we are able to further reduce model complexity while retaining AD performance. We also investigate setting the working point by selecting acceptable False Positive Rate thresholds based on the MVG assumption.   Code available at https://github.com/ORippler/gaussian-ad-mvtec



### Map-Guided Curriculum Domain Adaptation and Uncertainty-Aware Evaluation for Semantic Nighttime Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2005.14553v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.14553v2)
- **Published**: 2020-05-28 16:54:38+00:00
- **Updated**: 2021-01-07 15:26:43+00:00
- **Authors**: Christos Sakaridis, Dengxin Dai, Luc Van Gool
- **Comment**: IEEE T-PAMI 2020
- **Journal**: None
- **Summary**: We address the problem of semantic nighttime image segmentation and improve the state-of-the-art, by adapting daytime models to nighttime without using nighttime annotations. Moreover, we design a new evaluation framework to address the substantial uncertainty of semantics in nighttime images. Our central contributions are: 1) a curriculum framework to gradually adapt semantic segmentation models from day to night through progressively darker times of day, exploiting cross-time-of-day correspondences between daytime images from a reference map and dark images to guide the label inference in the dark domains; 2) a novel uncertainty-aware annotation and evaluation framework and metric for semantic segmentation, including image regions beyond human recognition capability in the evaluation in a principled fashion; 3) the Dark Zurich dataset, comprising 2416 unlabeled nighttime and 2920 unlabeled twilight images with correspondences to their daytime counterparts plus a set of 201 nighttime images with fine pixel-level annotations created with our protocol, which serves as a first benchmark for our novel evaluation. Experiments show that our map-guided curriculum adaptation significantly outperforms state-of-the-art methods on nighttime sets both for standard metrics and our uncertainty-aware metric. Furthermore, our uncertainty-aware evaluation reveals that selective invalidation of predictions can improve results on data with ambiguous content such as our benchmark and profit safety-oriented applications involving invalid inputs.



### Self-supervised Modal and View Invariant Feature Learning
- **Arxiv ID**: http://arxiv.org/abs/2005.14169v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.14169v1)
- **Published**: 2020-05-28 17:35:14+00:00
- **Updated**: 2020-05-28 17:35:14+00:00
- **Authors**: Longlong Jing, Yucheng Chen, Ling Zhang, Mingyi He, Yingli Tian
- **Comment**: None
- **Journal**: None
- **Summary**: Most of the existing self-supervised feature learning methods for 3D data either learn 3D features from point cloud data or from multi-view images. By exploring the inherent multi-modality attributes of 3D objects, in this paper, we propose to jointly learn modal-invariant and view-invariant features from different modalities including image, point cloud, and mesh with heterogeneous networks for 3D data. In order to learn modal- and view-invariant features, we propose two types of constraints: cross-modal invariance constraint and cross-view invariant constraint. Cross-modal invariance constraint forces the network to maximum the agreement of features from different modalities for same objects, while the cross-view invariance constraint forces the network to maximum agreement of features from different views of images for same objects. The quality of learned features has been tested on different downstream tasks with three modalities of data including point cloud, multi-view images, and mesh. Furthermore, the invariance cross different modalities and views are evaluated with the cross-modal retrieval task. Extensive evaluation results demonstrate that the learned features are robust and have strong generalizability across different tasks.



### Depth-aware Blending of Smoothed Images for Bokeh Effect Generation
- **Arxiv ID**: http://arxiv.org/abs/2005.14214v1
- **DOI**: 10.1016/j.jvcir.2021.103089
- **Categories**: **cs.CV**, cs.AI, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.14214v1)
- **Published**: 2020-05-28 18:11:05+00:00
- **Updated**: 2020-05-28 18:11:05+00:00
- **Authors**: Saikat Dutta
- **Comment**: None
- **Journal**: Journal of Visual Communication and Image Representation 2021
- **Summary**: Bokeh effect is used in photography to capture images where the closer objects look sharp and every-thing else stays out-of-focus. Bokeh photos are generally captured using Single Lens Reflex cameras using shallow depth-of-field. Most of the modern smartphones can take bokeh images by leveraging dual rear cameras or a good auto-focus hardware. However, for smartphones with single-rear camera without a good auto-focus hardware, we have to rely on software to generate bokeh images. This kind of system is also useful to generate bokeh effect in already captured images. In this paper, an end-to-end deep learning framework is proposed to generate high-quality bokeh effect from images. The original image and different versions of smoothed images are blended to generate Bokeh effect with the help of a monocular depth estimation network. The proposed approach is compared against a saliency detection based baseline and a number of approaches proposed in AIM 2019 Challenge on Bokeh Effect Synthesis. Extensive experiments are shown in order to understand different parts of the proposed algorithm. The network is lightweight and can process an HD image in 0.03 seconds. This approach ranked second in AIM 2019 Bokeh effect challenge-Perceptual Track.



### FCN+RL: A Fully Convolutional Network followed by Refinement Layers to Offline Handwritten Signature Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2005.14229v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.14229v1)
- **Published**: 2020-05-28 18:47:10+00:00
- **Updated**: 2020-05-28 18:47:10+00:00
- **Authors**: Celso A. M. Lopes Junior, Matheus Henrique M. da Silva, Byron Leite Dantas Bezerra, Bruno Jose Torres Fernandes, Donato Impedovo
- **Comment**: 7 pages, 6 figures, Accepted at IJCNN 2020: International Joint
  Conference on Neural Networks
- **Journal**: None
- **Summary**: Although secular, handwritten signature is one of the most reliable biometric methods used by most countries. In the last ten years, the application of technology for verification of handwritten signatures has evolved strongly, including forensic aspects. Some factors, such as the complexity of the background and the small size of the region of interest - signature pixels - increase the difficulty of the targeting task. Other factors that make it challenging are the various variations present in handwritten signatures such as location, type of ink, color and type of pen, and the type of stroke. In this work, we propose an approach to locate and extract the pixels of handwritten signatures on identification documents, without any prior information on the location of the signatures. The technique used is based on a fully convolutional encoder-decoder network combined with a block of refinement layers for the alpha channel of the predicted image. The experimental results demonstrate that the technique outputs a clean signature with higher fidelity in the lines than the traditional approaches and preservation of the pertinent characteristics to the signer's spelling. To evaluate the quality of our proposal, we use the following image similarity metrics: SSIM, SIFT, and Dice Coefficient. The qualitative and quantitative results show a significant improvement in comparison with the baseline system.



### Fuzziness-based Spatial-Spectral Class Discriminant Information Preserving Active Learning for Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2005.14236v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.14236v1)
- **Published**: 2020-05-28 18:58:11+00:00
- **Updated**: 2020-05-28 18:58:11+00:00
- **Authors**: Muhammad Ahmad
- **Comment**: 13 pages, 7 figures
- **Journal**: None
- **Summary**: Traditional Active/Self/Interactive Learning for Hyperspectral Image Classification (HSIC) increases the size of the training set without considering the class scatters and randomness among the existing and new samples. Second, very limited research has been carried out on joint spectral-spatial information and finally, a minor but still worth mentioning is the stopping criteria which not being much considered by the community. Therefore, this work proposes a novel fuzziness-based spatial-spectral within and between for both local and global class discriminant information preserving (FLG) method. We first investigate a spatial prior fuzziness-based misclassified sample information. We then compute the total local and global for both within and between class information and formulate it in a fine-grained manner. Later this information is fed to a discriminative objective function to query the heterogeneous samples which eliminate the randomness among the training samples. Experimental results on benchmark HSI datasets demonstrate the effectiveness of the FLG method on Generative, Extreme Learning Machine and Sparse Multinomial Logistic Regression (SMLR)-LORSAL classifiers.



### Human Recognition Using Face in Computed Tomography
- **Arxiv ID**: http://arxiv.org/abs/2005.14238v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.14238v1)
- **Published**: 2020-05-28 18:59:59+00:00
- **Updated**: 2020-05-28 18:59:59+00:00
- **Authors**: Jiuwen Zhu, Hu Han, S. Kevin Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: With the mushrooming use of computed tomography (CT) images in clinical decision making, management of CT data becomes increasingly difficult. From the patient identification perspective, using the standard DICOM tag to track patient information is challenged by issues such as misspelling, lost file, site variation, etc. In this paper, we explore the feasibility of leveraging the faces in 3D CT images as biometric features. Specifically, we propose an automatic processing pipeline that first detects facial landmarks in 3D for ROI extraction and then generates aligned 2D depth images, which are used for automatic recognition. To boost the recognition performance, we employ transfer learning to reduce the data sparsity issue and to introduce a group sampling strategy to increase inter-class discrimination when training the recognition network. Our proposed method is capable of capturing underlying identity characteristics in medical images while reducing memory consumption. To test its effectiveness, we curate 600 3D CT images of 280 patients from multiple sources for performance evaluation. Experimental results demonstrate that our method achieves a 1:56 identification accuracy of 92.53% and a 1:1 verification accuracy of 96.12%, outperforming other competing approaches.



### Joint Total Variation ESTATICS for Robust Multi-Parameter Mapping
- **Arxiv ID**: http://arxiv.org/abs/2005.14247v1
- **DOI**: 10.1007/978-3-030-59713-9_6
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.14247v1)
- **Published**: 2020-05-28 19:08:42+00:00
- **Updated**: 2020-05-28 19:08:42+00:00
- **Authors**: Yaël Balbastre, Mikael Brudfors, Michela Azzarito, Christian Lambert, Martina F. Callaghan, John Ashburner
- **Comment**: 11 pages, 2 figures, 1 table, conference paper, accepted at MICCAI
  2020
- **Journal**: None
- **Summary**: Quantitative magnetic resonance imaging (qMRI) derives tissue-specific parameters -- such as the apparent transverse relaxation rate R2*, the longitudinal relaxation rate R1 and the magnetisation transfer saturation -- that can be compared across sites and scanners and carry important information about the underlying microstructure. The multi-parameter mapping (MPM) protocol takes advantage of multi-echo acquisitions with variable flip angles to extract these parameters in a clinically acceptable scan time. In this context, ESTATICS performs a joint loglinear fit of multiple echo series to extract R2* and multiple extrapolated intercepts, thereby improving robustness to motion and decreasing the variance of the estimators. In this paper, we extend this model in two ways: (1) by introducing a joint total variation (JTV) prior on the intercepts and decay, and (2) by deriving a nonlinear maximum \emph{a posteriori} estimate. We evaluated the proposed algorithm by predicting left-out echoes in a rich single-subject dataset. In this validation, we outperformed other state-of-the-art methods and additionally showed that the proposed approach greatly reduces the variance of the estimated maps, without introducing bias.



### Overview: Computer vision and machine learning for microstructural characterization and analysis
- **Arxiv ID**: http://arxiv.org/abs/2005.14260v1
- **DOI**: 10.1007/s11661-020-06008-4
- **Categories**: **cs.CV**, cond-mat.mtrl-sci
- **Links**: [PDF](http://arxiv.org/pdf/2005.14260v1)
- **Published**: 2020-05-28 19:51:23+00:00
- **Updated**: 2020-05-28 19:51:23+00:00
- **Authors**: Elizabeth A. Holm, Ryan Cohn, Nan Gao, Andrew R. Kitahara, Thomas P. Matson, Bo Lei, Srujana Rao Yarasi
- **Comment**: submitted to Materials and Metallurgical Transactions A
- **Journal**: None
- **Summary**: The characterization and analysis of microstructure is the foundation of microstructural science, connecting the materials structure to its composition, process history, and properties. Microstructural quantification traditionally involves a human deciding a priori what to measure and then devising a purpose-built method for doing so. However, recent advances in data science, including computer vision (CV) and machine learning (ML) offer new approaches to extracting information from microstructural images. This overview surveys CV approaches to numerically encode the visual information contained in a microstructural image, which then provides input to supervised or unsupervised ML algorithms that find associations and trends in the high-dimensional image representation. CV/ML systems for microstructural characterization and analysis span the taxonomy of image analysis tasks, including image classification, semantic segmentation, object detection, and instance segmentation. These tools enable new approaches to microstructural analysis, including the development of new, rich visual metrics and the discovery of processing-microstructure-property relationships.



### Uncertainty Evaluation Metric for Brain Tumour Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2005.14262v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.14262v1)
- **Published**: 2020-05-28 19:53:32+00:00
- **Updated**: 2020-05-28 19:53:32+00:00
- **Authors**: Raghav Mehta, Angelos Filos, Yarin Gal, Tal Arbel
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we develop a metric designed to assess and rank uncertainty measures for the task of brain tumour sub-tissue segmentation in the BraTS 2019 sub-challenge on uncertainty quantification. The metric is designed to: (1) reward uncertainty measures where high confidence is assigned to correct assertions, and where incorrect assertions are assigned low confidence and (2) penalize measures that have higher percentages of under-confident correct assertions. Here, the workings of the components of the metric are explored based on a number of popular uncertainty measures evaluated on the BraTS 2019 dataset.



### LR-CNN: Local-aware Region CNN for Vehicle Detection in Aerial Imagery
- **Arxiv ID**: http://arxiv.org/abs/2005.14264v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.14264v1)
- **Published**: 2020-05-28 19:57:34+00:00
- **Updated**: 2020-05-28 19:57:34+00:00
- **Authors**: Wentong Liao, Xiang Chen, Jingfeng Yang, Stefan Roth, Michael Goesele, Michael Ying Yang, Bodo Rosenhahn
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: State-of-the-art object detection approaches such as Fast/Faster R-CNN, SSD, or YOLO have difficulties detecting dense, small targets with arbitrary orientation in large aerial images. The main reason is that using interpolation to align RoI features can result in a lack of accuracy or even loss of location information. We present the Local-aware Region Convolutional Neural Network (LR-CNN), a novel two-stage approach for vehicle detection in aerial imagery. We enhance translation invariance to detect dense vehicles and address the boundary quantization issue amongst dense vehicles by aggregating the high-precision RoIs' features. Moreover, we resample high-level semantic pooled features, making them regain location information from the features of a shallower convolutional block. This strengthens the local feature invariance for the resampled features and enables detecting vehicles in an arbitrary orientation. The local feature invariance enhances the learning ability of the focal loss function, and the focal loss further helps to focus on the hard examples. Taken together, our method better addresses the challenges of aerial imagery. We evaluate our approach on several challenging datasets (VEDAI, DOTA), demonstrating a significant improvement over state-of-the-art methods. We demonstrate the good generalization ability of our approach on the DLR 3K dataset.



### Two-stage framework for optic disc localization and glaucoma classification in retinal fundus images using deep learning
- **Arxiv ID**: http://arxiv.org/abs/2005.14284v1
- **DOI**: 10.1186/s12911-019-0842-8
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.14284v1)
- **Published**: 2020-05-28 20:40:19+00:00
- **Updated**: 2020-05-28 20:40:19+00:00
- **Authors**: Muhammad Naseer Bajwa, Muhammad Imran Malik, Shoaib Ahmed Siddiqui, Andreas Dengel, Faisal Shafait, Wolfgang Neumeier, Sheraz Ahmed
- **Comment**: 16 Pages, 10 Figures
- **Journal**: BMC medical informatics and decision making 19.1 (2019): 136
- **Summary**: With the advancement of powerful image processing and machine learning techniques, CAD has become ever more prevalent in all fields of medicine including ophthalmology. Since optic disc is the most important part of retinal fundus image for glaucoma detection, this paper proposes a two-stage framework that first detects and localizes optic disc and then classifies it into healthy or glaucomatous. The first stage is based on RCNN and is responsible for localizing and extracting optic disc from a retinal fundus image while the second stage uses Deep CNN to classify the extracted disc into healthy or glaucomatous. In addition to the proposed solution, we also developed a rule-based semi-automatic ground truth generation method that provides necessary annotations for training RCNN based model for automated disc localization. The proposed method is evaluated on seven publicly available datasets for disc localization and on ORIGA dataset, which is the largest publicly available dataset for glaucoma classification. The results of automatic localization mark new state-of-the-art on six datasets with accuracy reaching 100% on four of them. For glaucoma classification we achieved AUC equal to 0.874 which is 2.7% relative improvement over the state-of-the-art results previously obtained for classification on ORIGA. Once trained on carefully annotated data, Deep Learning based methods for optic disc detection and localization are not only robust, accurate and fully automated but also eliminates the need for dataset-dependent heuristic algorithms. Our empirical evaluation of glaucoma classification on ORIGA reveals that reporting only AUC, for datasets with class imbalance and without pre-defined train and test splits, does not portray true picture of the classifier's performance and calls for additional performance metrics to substantiate the results.



### ePillID Dataset: A Low-Shot Fine-Grained Benchmark for Pill Identification
- **Arxiv ID**: http://arxiv.org/abs/2005.14288v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2005.14288v2)
- **Published**: 2020-05-28 20:53:36+00:00
- **Updated**: 2020-09-07 22:29:24+00:00
- **Authors**: Naoto Usuyama, Natalia Larios Delgado, Amanda K. Hall, Jessica Lundin
- **Comment**: CVPR 2020 VL3. Project Page:
  https://github.com/usuyama/ePillID-benchmark
- **Journal**: None
- **Summary**: Identifying prescription medications is a frequent task for patients and medical professionals; however, this is an error-prone task as many pills have similar appearances (e.g. white round pills), which increases the risk of medication errors. In this paper, we introduce ePillID, the largest public benchmark on pill image recognition, composed of 13k images representing 9804 appearance classes (two sides for 4902 pill types). For most of the appearance classes, there exists only one reference image, making it a challenging low-shot recognition setting. We present our experimental setup and evaluation results of various baseline models on the benchmark. The best baseline using a multi-head metric-learning approach with bilinear features performed remarkably well; however, our error analysis suggests that they still fail to distinguish particularly confusing classes. The code and data are available at https://github.com/usuyama/ePillID-benchmark.



### Monocular Depth Estimators: Vulnerabilities and Attacks
- **Arxiv ID**: http://arxiv.org/abs/2005.14302v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.14302v1)
- **Published**: 2020-05-28 21:25:21+00:00
- **Updated**: 2020-05-28 21:25:21+00:00
- **Authors**: Alwyn Mathew, Aditya Prakash Patra, Jimson Mathew
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advancements of neural networks lead to reliable monocular depth estimation. Monocular depth estimated techniques have the upper hand over traditional depth estimation techniques as it only needs one image during inference. Depth estimation is one of the essential tasks in robotics, and monocular depth estimation has a wide variety of safety-critical applications like in self-driving cars and surgical devices. Thus, the robustness of such techniques is very crucial. It has been shown in recent works that these deep neural networks are highly vulnerable to adversarial samples for tasks like classification, detection and segmentation. These adversarial samples can completely ruin the output of the system, making their credibility in real-time deployment questionable. In this paper, we investigate the robustness of the most state-of-the-art monocular depth estimation networks against adversarial attacks. Our experiments show that tiny perturbations on an image that are invisible to the naked eye (perturbation attack) and corruption less than about 1% of an image (patch attack) can affect the depth estimation drastically. We introduce a novel deep feature annihilation loss that corrupts the hidden feature space representation forcing the decoder of the network to output poor depth maps. The white-box and black-box test compliments the effectiveness of the proposed attack. We also perform adversarial example transferability tests, mainly cross-data transferability.



### Combining Fine- and Coarse-Grained Classifiers for Diabetic Retinopathy Detection
- **Arxiv ID**: http://arxiv.org/abs/2005.14308v1
- **DOI**: 10.1007/978-3-030-39343-4_21
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.14308v1)
- **Published**: 2020-05-28 21:37:36+00:00
- **Updated**: 2020-05-28 21:37:36+00:00
- **Authors**: Muhammad Naseer Bajwa, Yoshinobu Taniguchi, Muhammad Imran Malik, Wolfgang Neumeier, Andreas Dengel, Sheraz Ahmed
- **Comment**: Pages 12, Figures 5
- **Journal**: None
- **Summary**: Visual artefacts of early diabetic retinopathy in retinal fundus images are usually small in size, inconspicuous, and scattered all over retina. Detecting diabetic retinopathy requires physicians to look at the whole image and fixate on some specific regions to locate potential biomarkers of the disease. Therefore, getting inspiration from ophthalmologist, we propose to combine coarse-grained classifiers that detect discriminating features from the whole images, with a recent breed of fine-grained classifiers that discover and pay particular attention to pathologically significant regions. To evaluate the performance of this proposed ensemble, we used publicly available EyePACS and Messidor datasets. Extensive experimentation for binary, ternary and quaternary classification shows that this ensemble largely outperforms individual image classifiers as well as most of the published works in most training setups for diabetic retinopathy detection. Furthermore, the performance of fine-grained classifiers is found notably superior than coarse-grained image classifiers encouraging the development of task-oriented fine-grained classifiers modelled after specialist ophthalmologists.



### Predicting Goal-directed Human Attention Using Inverse Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2005.14310v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.14310v2)
- **Published**: 2020-05-28 21:46:27+00:00
- **Updated**: 2020-06-25 10:56:15+00:00
- **Authors**: Zhibo Yang, Lihan Huang, Yupei Chen, Zijun Wei, Seoyoung Ahn, Gregory Zelinsky, Dimitris Samaras, Minh Hoai
- **Comment**: 16 pages, 13 figures, CVPR 2020
- **Journal**: None
- **Summary**: Being able to predict human gaze behavior has obvious importance for behavioral vision and for computer vision applications. Most models have mainly focused on predicting free-viewing behavior using saliency maps, but these predictions do not generalize to goal-directed behavior, such as when a person searches for a visual target object. We propose the first inverse reinforcement learning (IRL) model to learn the internal reward function and policy used by humans during visual search. The viewer's internal belief states were modeled as dynamic contextual belief maps of object locations. These maps were learned by IRL and then used to predict behavioral scanpaths for multiple target categories. To train and evaluate our IRL model we created COCO-Search18, which is now the largest dataset of high-quality search fixations in existence. COCO-Search18 has 10 participants searching for each of 18 target-object categories in 6202 images, making about 300,000 goal-directed fixations. When trained and evaluated on COCO-Search18, the IRL model outperformed baseline models in predicting search fixation scanpaths, both in terms of similarity to human search behavior and search efficiency. Finally, reward maps recovered by the IRL model reveal distinctive target-dependent patterns of object prioritization, which we interpret as a learned object context.



### Self-Attention Dense Depth Estimation Network for Unrectified Video Sequences
- **Arxiv ID**: http://arxiv.org/abs/2005.14313v1
- **DOI**: 10.1109/ICIP40778.2020.9190764
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.14313v1)
- **Published**: 2020-05-28 21:53:53+00:00
- **Updated**: 2020-05-28 21:53:53+00:00
- **Authors**: Alwyn Mathew, Aditya Prakash Patra, Jimson Mathew
- **Comment**: None
- **Journal**: None
- **Summary**: The dense depth estimation of a 3D scene has numerous applications, mainly in robotics and surveillance. LiDAR and radar sensors are the hardware solution for real-time depth estimation, but these sensors produce sparse depth maps and are sometimes unreliable. In recent years research aimed at tackling depth estimation using single 2D image has received a lot of attention. The deep learning based self-supervised depth estimation methods from the rectified stereo and monocular video frames have shown promising results. We propose a self-attention based depth and ego-motion network for unrectified images. We also introduce non-differentiable distortion of the camera into the training pipeline. Our approach performs competitively when compared to other established approaches that used rectified images for depth estimation.



### Bipartite Distance for Shape-Aware Landmark Detection in Spinal X-Ray Images
- **Arxiv ID**: http://arxiv.org/abs/2005.14330v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.14330v1)
- **Published**: 2020-05-28 22:34:24+00:00
- **Updated**: 2020-05-28 22:34:24+00:00
- **Authors**: Abdullah-Al-Zubaer Imran, Chao Huang, Hui Tang, Wei Fan, Kenneth M. C. Cheung, Michael To, Zhen Qian, Demetri Terzopoulos
- **Comment**: Presented at Med-NeurIPS 2019
- **Journal**: None
- **Summary**: Scoliosis is a congenital disease that causes lateral curvature in the spine. Its assessment relies on the identification and localization of vertebrae in spinal X-ray images, conventionally via tedious and time-consuming manual radiographic procedures that are prone to subjectivity and observational variability. Reliability can be improved through the automatic detection and localization of spinal landmarks. To guide a CNN in the learning of spinal shape while detecting landmarks in X-ray images, we propose a novel loss based on a bipartite distance (BPD) measure, and show that it consistently improves landmark detection performance.



