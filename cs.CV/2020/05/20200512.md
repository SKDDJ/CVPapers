# Arxiv Papers in cs.CV on 2020-05-12
### Train and Deploy an Image Classifier for Disaster Response
- **Arxiv ID**: http://arxiv.org/abs/2005.05495v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.05495v1)
- **Published**: 2020-05-12 00:45:48+00:00
- **Updated**: 2020-05-12 00:45:48+00:00
- **Authors**: Jianyu Mao, Kiana Harris, Nae-Rong Chang, Caleb Pennell, Yiming Ren
- **Comment**: 5 pages, 6 figures
- **Journal**: None
- **Summary**: With Deep Learning Image Classification becoming more powerful each year, it is apparent that its introduction to disaster response will increase the efficiency that responders can work with. Using several Neural Network Models, including AlexNet, ResNet, MobileNet, DenseNets, and 4-Layer CNN, we have classified flood disaster images from a large image data set with up to 79% accuracy. Our models and tutorials for working with the data set have created a foundation for others to classify other types of disasters contained in the images.



### Jigsaw-VAE: Towards Balancing Features in Variational Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/2005.05496v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.05496v1)
- **Published**: 2020-05-12 00:46:54+00:00
- **Updated**: 2020-05-12 00:46:54+00:00
- **Authors**: Saeid Asgari Taghanaki, Mohammad Havaei, Alex Lamb, Aditya Sanghi, Ara Danielyan, Tonya Custis
- **Comment**: None
- **Journal**: None
- **Summary**: The latent variables learned by VAEs have seen considerable interest as an unsupervised way of extracting features, which can then be used for downstream tasks. There is a growing interest in the question of whether features learned on one environment will generalize across different environments. We demonstrate here that VAE latent variables often focus on some factors of variation at the expense of others - in this case we refer to the features as ``imbalanced''. Feature imbalance leads to poor generalization when the latent variables are used in an environment where the presence of features changes. Similarly, latent variables trained with imbalanced features induce the VAE to generate less diverse (i.e. biased towards dominant features) samples. To address this, we propose a regularization scheme for VAEs, which we show substantially addresses the feature imbalance problem. We also introduce a simple metric to measure the balance of features in generated images.



### 3DV: 3D Dynamic Voxel for Action Recognition in Depth Video
- **Arxiv ID**: http://arxiv.org/abs/2005.05501v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.05501v1)
- **Published**: 2020-05-12 01:04:34+00:00
- **Updated**: 2020-05-12 01:04:34+00:00
- **Authors**: Yancheng Wang, Yang Xiao, Fu Xiong, Wenxiang Jiang, Zhiguo Cao, Joey Tianyi Zhou, Junsong Yuan
- **Comment**: Accepted by CVPR2020
- **Journal**: None
- **Summary**: To facilitate depth-based 3D action recognition, 3D dynamic voxel (3DV) is proposed as a novel 3D motion representation. With 3D space voxelization, the key idea of 3DV is to encode 3D motion information within depth video into a regular voxel set (i.e., 3DV) compactly, via temporal rank pooling. Each available 3DV voxel intrinsically involves 3D spatial and motion feature jointly. 3DV is then abstracted as a point set and input into PointNet++ for 3D action recognition, in the end-to-end learning way. The intuition for transferring 3DV into the point set form is that, PointNet++ is lightweight and effective for deep feature learning towards point set. Since 3DV may lose appearance clue, a multi-stream 3D action recognition manner is also proposed to learn motion and appearance feature jointly. To extract richer temporal order information of actions, we also divide the depth video into temporal splits and encode this procedure in 3DV integrally. The extensive experiments on 4 well-established benchmark datasets demonstrate the superiority of our proposition. Impressively, we acquire the accuracy of 82.4% and 93.5% on NTU RGB+D 120 [13] with the cross-subject and crosssetup test setting respectively. 3DV's code is available at https://github.com/3huo/3DV-Action.



### Real-time Facial Expression Recognition "In The Wild'' by Disentangling 3D Expression from Identity
- **Arxiv ID**: http://arxiv.org/abs/2005.05509v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.05509v1)
- **Published**: 2020-05-12 01:32:55+00:00
- **Updated**: 2020-05-12 01:32:55+00:00
- **Authors**: Mohammad Rami Koujan, Luma Alharbawee, Giorgos Giannakakis, Nicolas Pugeault, Anastasios Roussos
- **Comment**: to be published in 15th IEEE International Conference on Automatic
  Face and Gesture Recognition (FG 2020)
- **Journal**: None
- **Summary**: Human emotions analysis has been the focus of many studies, especially in the field of Affective Computing, and is important for many applications, e.g. human-computer intelligent interaction, stress analysis, interactive games, animations, etc. Solutions for automatic emotion analysis have also benefited from the development of deep learning approaches and the availability of vast amount of visual facial data on the internet. This paper proposes a novel method for human emotion recognition from a single RGB image. We construct a large-scale dataset of facial videos (\textbf{FaceVid}), rich in facial dynamics, identities, expressions, appearance and 3D pose variations. We use this dataset to train a deep Convolutional Neural Network for estimating expression parameters of a 3D Morphable Model and combine it with an effective back-end emotion classifier. Our proposed framework runs at 50 frames per second and is capable of robustly estimating parameters of 3D expression variation and accurately recognizing facial expressions from in-the-wild images. We present extensive experimental evaluation that shows that the proposed method outperforms the compared techniques in estimating the 3D expression parameters and achieves state-of-the-art performance in recognising the basic emotions from facial images, as well as recognising stress from facial videos. %compared to the current state of the art in emotion recognition from facial images.



### A Novel Granular-Based Bi-Clustering Method of Deep Mining the Co-Expressed Genes
- **Arxiv ID**: http://arxiv.org/abs/2005.05519v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2005.05519v1)
- **Published**: 2020-05-12 02:04:40+00:00
- **Updated**: 2020-05-12 02:04:40+00:00
- **Authors**: Kaijie Xu, Witold Pedrycz, Zhiwu Li, Yinghui Quan, Weike Nie
- **Comment**: None
- **Journal**: None
- **Summary**: Traditional clustering methods are limited when dealing with huge and heterogeneous groups of gene expression data, which motivates the development of bi-clustering methods. Bi-clustering methods are used to mine bi-clusters whose subsets of samples (genes) are co-regulated under their test conditions. Studies show that mining bi-clusters of consistent trends and trends with similar degrees of fluctuations from the gene expression data is essential in bioinformatics research. Unfortunately, traditional bi-clustering methods are not fully effective in discovering such bi-clusters. Therefore, we propose a novel bi-clustering method by involving here the theory of Granular Computing. In the proposed scheme, the gene data matrix, considered as a group of time series, is transformed into a series of ordered information granules. With the information granules we build a characteristic matrix of the gene data to capture the fluctuation trend of the expression value between consecutive conditions to mine the ideal bi-clusters. The experimental results are in agreement with the theoretical analysis, and show the excellent performance of the proposed method.



### Making Robots Draw A Vivid Portrait In Two Minutes
- **Arxiv ID**: http://arxiv.org/abs/2005.05526v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.05526v3)
- **Published**: 2020-05-12 03:02:24+00:00
- **Updated**: 2020-07-21 07:20:32+00:00
- **Authors**: Fei Gao, Jingjie Zhu, Zeyuan Yu, Peng Li, Tao Wang
- **Comment**: 7 pages, 7 figures; accepted by IROS2020
- **Journal**: None
- **Summary**: Significant progress has been made with artistic robots. However, existing robots fail to produce high-quality portraits in a short time. In this work, we present a drawing robot, which can automatically transfer a facial picture to a vivid portrait, and then draw it on paper within two minutes averagely. At the heart of our system is a novel portrait synthesis algorithm based on deep learning. Innovatively, we employ a self-consistency loss, which makes the algorithm capable of generating continuous and smooth brush-strokes. Besides, we propose a componential sparsity constraint to reduce the number of brush-strokes over insignificant areas. We also implement a local sketch synthesis algorithm, and several pre- and post-processing techniques to deal with the background and details. The portrait produced by our algorithm successfully captures individual characteristics by using a sparse set of continuous brush-strokes. Finally, the portrait is converted to a sequence of trajectories and reproduced by a 3-degree-of-freedom robotic arm. The whole portrait drawing robotic system is named AiSketcher. Extensive experiments show that AiSketcher can produce considerably high-quality sketches for a wide range of pictures, including faces in-the-wild and universal images of arbitrary content. To our best knowledge, AiSketcher is the first portrait drawing robot that uses neural style transfer techniques. AiSketcher has attended a quite number of exhibitions and shown remarkable performance under diverse circumstances.



### PSDet: Efficient and Universal Parking Slot Detection
- **Arxiv ID**: http://arxiv.org/abs/2005.05528v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.05528v1)
- **Published**: 2020-05-12 03:06:25+00:00
- **Updated**: 2020-05-12 03:06:25+00:00
- **Authors**: Zizhang Wu, Weiwei Sun, Man Wang, Xiaoquan Wang, Lizhu Ding, Fan Wang
- **Comment**: Accpeted to IV 2020, i.e., the 31st IEEE Intelligent Vehicles
  Symposium
- **Journal**: None
- **Summary**: While real-time parking slot detection plays a critical role in valet parking systems, existing methods have limited success in real-world applications. We argue two reasons accounting for the unsatisfactory performance: \romannumeral1, The available datasets have limited diversity, which causes the low generalization ability. \romannumeral2, Expert knowledge for parking slot detection is under-estimated. Thus, we annotate a large-scale benchmark for training the network and release it for the benefit of community. Driven by the observation of various parking lots in our benchmark, we propose the circular descriptor to regress the coordinates of parking slot vertexes and accordingly localize slots accurately. To further boost the performance, we develop a two-stage deep architecture to localize vertexes in the coarse-to-fine manner. In our benchmark and other datasets, it achieves the state-of-the-art accuracy while being real-time in practice. Benchmark is available at: https://github.com/wuzzh/Parking-slot-dataset



### 3D Face Anti-spoofing with Factorized Bilinear Coding
- **Arxiv ID**: http://arxiv.org/abs/2005.06514v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.06514v3)
- **Published**: 2020-05-12 03:09:20+00:00
- **Updated**: 2020-12-13 16:21:24+00:00
- **Authors**: Shan Jia, Xin Li, Chuanbo Hu, Guodong Guo, Zhengquan Xu
- **Comment**: arXiv admin note: text overlap with arXiv:1910.05457
- **Journal**: None
- **Summary**: We have witnessed rapid advances in both face presentation attack models and presentation attack detection (PAD) in recent years. When compared with widely studied 2D face presentation attacks, 3D face spoofing attacks are more challenging because face recognition systems are more easily confused by the 3D characteristics of materials similar to real faces. In this work, we tackle the problem of detecting these realistic 3D face presentation attacks, and propose a novel anti-spoofing method from the perspective of fine-grained classification. Our method, based on factorized bilinear coding of multiple color channels (namely MC\_FBC), targets at learning subtle fine-grained differences between real and fake images. By extracting discriminative and fusing complementary information from RGB and YCbCr spaces, we have developed a principled solution to 3D face spoofing detection. A large-scale wax figure face database (WFFD) with both images and videos has also been collected as super-realistic attacks to facilitate the study of 3D face presentation attack detection. Extensive experimental results show that our proposed method achieves the state-of-the-art performance on both our own WFFD and other face spoofing databases under various intra-database and inter-database testing scenarios.



### DeepFaceLab: Integrated, flexible and extensible face-swapping framework
- **Arxiv ID**: http://arxiv.org/abs/2005.05535v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.05535v5)
- **Published**: 2020-05-12 03:26:55+00:00
- **Updated**: 2021-06-29 07:07:57+00:00
- **Authors**: Ivan Perov, Daiheng Gao, Nikolay Chervoniy, Kunlin Liu, Sugasa Marangonda, Chris Umé, Mr. Dpfks, Carl Shift Facenheim, Luis RP, Jian Jiang, Sheng Zhang, Pingyu Wu, Bo Zhou, Weiming Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Deepfake defense not only requires the research of detection but also requires the efforts of generation methods. However, current deepfake methods suffer the effects of obscure workflow and poor performance. To solve this problem, we present DeepFaceLab, the current dominant deepfake framework for face-swapping. It provides the necessary tools as well as an easy-to-use way to conduct high-quality face-swapping. It also offers a flexible and loose coupling structure for people who need to strengthen their pipeline with other features without writing complicated boilerplate code. We detail the principles that drive the implementation of DeepFaceLab and introduce its pipeline, through which every aspect of the pipeline can be modified painlessly by users to achieve their customization purpose. It is noteworthy that DeepFaceLab could achieve cinema-quality results with high fidelity. We demonstrate the advantage of our system by comparing our approach with other face-swapping methods.For more information, please visit:https://github.com/iperov/DeepFaceLab/.



### High-Fidelity Accelerated MRI Reconstruction by Scan-Specific Fine-Tuning of Physics-Based Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2005.05550v1
- **DOI**: 10.1109/EMBC44109.2020.9176241
- **Categories**: **eess.IV**, cs.CV, cs.LG, eess.SP, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2005.05550v1)
- **Published**: 2020-05-12 05:10:10+00:00
- **Updated**: 2020-05-12 05:10:10+00:00
- **Authors**: Seyed Amir Hossein Hosseini, Burhaneddin Yaman, Steen Moeller, Mehmet Akçakaya
- **Comment**: None
- **Journal**: Proceedings of IEEE EMBC, 2020
- **Summary**: Long scan duration remains a challenge for high-resolution MRI. Deep learning has emerged as a powerful means for accelerated MRI reconstruction by providing data-driven regularizers that are directly learned from data. These data-driven priors typically remain unchanged for future data in the testing phase once they are learned during training. In this study, we propose to use a transfer learning approach to fine-tune these regularizers for new subjects using a self-supervision approach. While the proposed approach can compromise the extremely fast reconstruction time of deep learning MRI methods, our results on knee MRI indicate that such adaptation can substantially reduce the remaining artifacts in reconstructed images. In addition, the proposed approach has the potential to reduce the risks of generalization to rare pathological conditions, which may be unavailable in the training data.



### Effective and Robust Detection of Adversarial Examples via Benford-Fourier Coefficients
- **Arxiv ID**: http://arxiv.org/abs/2005.05552v1
- **DOI**: 10.1007/s11633-022-1328-1
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.05552v1)
- **Published**: 2020-05-12 05:20:59+00:00
- **Updated**: 2020-05-12 05:20:59+00:00
- **Authors**: Chengcheng Ma, Baoyuan Wu, Shibiao Xu, Yanbo Fan, Yong Zhang, Xiaopeng Zhang, Zhifeng Li
- **Comment**: None
- **Journal**: None
- **Summary**: Adversarial examples have been well known as a serious threat to deep neural networks (DNNs). In this work, we study the detection of adversarial examples, based on the assumption that the output and internal responses of one DNN model for both adversarial and benign examples follow the generalized Gaussian distribution (GGD), but with different parameters (i.e., shape factor, mean, and variance). GGD is a general distribution family to cover many popular distributions (e.g., Laplacian, Gaussian, or uniform). It is more likely to approximate the intrinsic distributions of internal responses than any specific distribution. Besides, since the shape factor is more robust to different databases rather than the other two parameters, we propose to construct discriminative features via the shape factor for adversarial detection, employing the magnitude of Benford-Fourier coefficients (MBF), which can be easily estimated using responses. Finally, a support vector machine is trained as the adversarial detector through leveraging the MBF features. Extensive experiments in terms of image classification demonstrate that the proposed detector is much more effective and robust on detecting adversarial examples of different crafting methods and different sources, compared to state-of-the-art adversarial detection methods.



### Multi-Channel Transfer Learning of Chest X-ray Images for Screening of COVID-19
- **Arxiv ID**: http://arxiv.org/abs/2005.05576v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.05576v1)
- **Published**: 2020-05-12 07:03:46+00:00
- **Updated**: 2020-05-12 07:03:46+00:00
- **Authors**: Sampa Misra, Seungwan Jeon, Seiyon Lee, Ravi Managuli, Chulhong Kim
- **Comment**: 7 pages, 3 figures, 1 Table
- **Journal**: None
- **Summary**: The 2019 novel coronavirus (COVID-19) has spread rapidly all over the world and it is affecting the whole society. The current gold standard test for screening COVID-19 patients is the polymerase chain reaction test. However, the COVID-19 test kits are not widely available and time-consuming. Thus, as an alternative, chest X-rays are being considered for quick screening. Since the presentation of COVID-19 in chest X-rays is varied in features and specialization in reading COVID-19 chest X-rays are required thus limiting its use for diagnosis. To address this challenge of reading chest X-rays by radiologists quickly, we present a multi-channel transfer learning model based on ResNet architecture to facilitate the diagnosis of COVID-19 chest X-ray. Three ResNet-based models (Models a, b, and c) were retrained using Dataset_A (1579 normal and 4429 diseased), Dataset_B (4245 pneumonia and 1763 non-pneumonia), and Dataset_C (184 COVID-19 and 5824 Non-COVID19), respectively, to classify (a) normal or diseased, (b) pneumonia or non-pneumonia, and (c) COVID-19 or non-COVID19. Finally, these three models were ensembled and fine-tuned using Dataset_D (1579 normal, 4245 pneumonia, and 184 COVID-19) to classify normal, pneumonia, and COVID-19 cases. Our results show that the ensemble model is more accurate than the single ResNet model, which is also re-trained using Dataset_D as it extracts more relevant semantic features for each class. Our approach provides a precision of 94 % and a recall of 100%. Thus, our method could potentially help clinicians in screening patients for COVID-19, thus facilitating immediate triaging and treatment for better outcomes.



### Discriminative Multi-modality Speech Recognition
- **Arxiv ID**: http://arxiv.org/abs/2005.05592v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.SD, eess.AS, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.05592v2)
- **Published**: 2020-05-12 07:56:03+00:00
- **Updated**: 2020-05-13 07:55:21+00:00
- **Authors**: Bo Xu, Cheng Lu, Yandong Guo, Jacob Wang
- **Comment**: CVPR2020
- **Journal**: None
- **Summary**: Vision is often used as a complementary modality for audio speech recognition (ASR), especially in the noisy environment where performance of solo audio modality significantly deteriorates. After combining visual modality, ASR is upgraded to the multi-modality speech recognition (MSR). In this paper, we propose a two-stage speech recognition model. In the first stage, the target voice is separated from background noises with help from the corresponding visual information of lip movements, making the model 'listen' clearly. At the second stage, the audio modality combines visual modality again to better understand the speech by a MSR sub-network, further improving the recognition rate. There are some other key contributions: we introduce a pseudo-3D residual convolution (P3D)-based visual front-end to extract more discriminative features; we upgrade the temporal convolution block from 1D ResNet with the temporal convolutional network (TCN), which is more suitable for the temporal tasks; the MSR sub-network is built on the top of Element-wise-Attention Gated Recurrent Unit (EleAtt-GRU), which is more effective than Transformer in long sequences. We conducted extensive experiments on the LRS3-TED and the LRW datasets. Our two-stage model (audio enhanced multi-modality speech recognition, AE-MSR) consistently achieves the state-of-the-art performance by a significant margin, which demonstrates the necessity and effectiveness of AE-MSR.



### Modeling and Enhancing Low-quality Retinal Fundus Images
- **Arxiv ID**: http://arxiv.org/abs/2005.05594v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.05594v3)
- **Published**: 2020-05-12 08:01:16+00:00
- **Updated**: 2020-12-09 10:39:09+00:00
- **Authors**: Ziyi Shen, Huazhu Fu, Jianbing Shen, Ling Shao
- **Comment**: None
- **Journal**: None
- **Summary**: Retinal fundus images are widely used for the clinical screening and diagnosis of eye diseases. However, fundus images captured by operators with various levels of experience have a large variation in quality. Low-quality fundus images increase uncertainty in clinical observation and lead to the risk of misdiagnosis. However, due to the special optical beam of fundus imaging and structure of the retina, natural image enhancement methods cannot be utilized directly to address this. In this paper, we first analyze the ophthalmoscope imaging system and simulate a reliable degradation of major inferior-quality factors, including uneven illumination, image blurring, and artifacts. Then, based on the degradation model, a clinically oriented fundus enhancement network (cofe-Net) is proposed to suppress global degradation factors, while simultaneously preserving anatomical retinal structures and pathological characteristics for clinical observation and analysis. Experiments on both synthetic and real images demonstrate that our algorithm effectively corrects low-quality fundus images without losing retinal details. Moreover, we also show that the fundus correction method can benefit medical image analysis applications, e.g., retinal vessel segmentation and optic disc/cup detection.



### Unsupervised Multi-label Dataset Generation from Web Data
- **Arxiv ID**: http://arxiv.org/abs/2005.05623v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.05623v1)
- **Published**: 2020-05-12 08:57:59+00:00
- **Updated**: 2020-05-12 08:57:59+00:00
- **Authors**: Carlos Roig, David Varas, Issey Masuda, Juan Carlos Riveiro, Elisenda Bou-Balust
- **Comment**: The 3rd Workshop on Visual Understanding by Learning from Web Data
  2019
- **Journal**: The 3rd Workshop on Visual Understanding by Learning from Web Data
  (CVPR 2019)
- **Summary**: This paper presents a system towards the generation of multi-label datasets from web data in an unsupervised manner. To achieve this objective, this work comprises two main contributions, namely: a) the generation of a low-noise unsupervised single-label dataset from web-data, and b) the augmentation of labels in such dataset (from single label to multi label). The generation of a single-label dataset uses an unsupervised noise reduction phase (clustering and selection of clusters using anchors) obtaining a 85% of correctly labeled images. An unsupervised label augmentation process is then performed to assign new labels to the images in the dataset using the class activation maps and the uncertainty associated with each class. This process is applied to the dataset generated in this paper and a public dataset (Places365) achieving a 9.5% and 27% of extra labels in each dataset respectively, therefore demonstrating that the presented system can robustly enrich the initial dataset.



### Detecting CNN-Generated Facial Images in Real-World Scenarios
- **Arxiv ID**: http://arxiv.org/abs/2005.05632v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.05632v1)
- **Published**: 2020-05-12 09:18:28+00:00
- **Updated**: 2020-05-12 09:18:28+00:00
- **Authors**: Nils Hulzebosch, Sarah Ibrahimi, Marcel Worring
- **Comment**: Accepted to the workshop on Media Forensics at CVPR 2020
- **Journal**: None
- **Summary**: Artificial, CNN-generated images are now of such high quality that humans have trouble distinguishing them from real images. Several algorithmic detection methods have been proposed, but these appear to generalize poorly to data from unknown sources, making them infeasible for real-world scenarios. In this work, we present a framework for evaluating detection methods under real-world conditions, consisting of cross-model, cross-data, and post-processing evaluation, and we evaluate state-of-the-art detection methods using the proposed framework. Furthermore, we examine the usefulness of commonly used image pre-processing methods. Lastly, we evaluate human performance on detecting CNN-generated images, along with factors that influence this performance, by conducting an online survey. Our results suggest that CNN-based detection methods are not yet robust enough to be used in real-world scenarios.



### Invertible Image Rescaling
- **Arxiv ID**: http://arxiv.org/abs/2005.05650v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.05650v1)
- **Published**: 2020-05-12 09:55:53+00:00
- **Updated**: 2020-05-12 09:55:53+00:00
- **Authors**: Mingqing Xiao, Shuxin Zheng, Chang Liu, Yaolong Wang, Di He, Guolin Ke, Jiang Bian, Zhouchen Lin, Tie-Yan Liu
- **Comment**: None
- **Journal**: None
- **Summary**: High-resolution digital images are usually downscaled to fit various display screens or save the cost of storage and bandwidth, meanwhile the post-upscaling is adpoted to recover the original resolutions or the details in the zoom-in images. However, typical image downscaling is a non-injective mapping due to the loss of high-frequency information, which leads to the ill-posed problem of the inverse upscaling procedure and poses great challenges for recovering details from the downscaled low-resolution images. Simply upscaling with image super-resolution methods results in unsatisfactory recovering performance. In this work, we propose to solve this problem by modeling the downscaling and upscaling processes from a new perspective, i.e. an invertible bijective transformation, which can largely mitigate the ill-posed nature of image upscaling. We develop an Invertible Rescaling Net (IRN) with deliberately designed framework and objectives to produce visually-pleasing low-resolution images and meanwhile capture the distribution of the lost information using a latent variable following a specified distribution in the downscaling process. In this way, upscaling is made tractable by inversely passing a randomly-drawn latent variable with the low-resolution image through the network. Experimental results demonstrate the significant improvement of our model over existing methods in terms of both quantitative and qualitative evaluations of image upscaling reconstruction from downscaled images.



### Very High Resolution Land Cover Mapping of Urban Areas at Global Scale with Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2005.05652v1
- **DOI**: 10.5194/isprs-archives-XLIII-B3-2020-201-2020
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.05652v1)
- **Published**: 2020-05-12 10:03:20+00:00
- **Updated**: 2020-05-12 10:03:20+00:00
- **Authors**: Thomas Tilak, Arnaud Braun, David Chandler, Nicolas David, Sylvain Galopin, Amélie Lombard, Michaël Michaud, Camille Parisel, Matthieu Porte, Marjorie Robert
- **Comment**: 8 pages, 14 figures, ISPRS Archives of the Photogrammetry, Remote
  Sensing and Spatial Information Sciences
- **Journal**: XXIV ISPRS Congress, Commission III, Volume XLIII-B3-2020, 2020
- **Summary**: This paper describes a methodology to produce a 7-classes land cover map of urban areas from very high resolution images and limited noisy labeled data. The objective is to make a segmentation map of a large area (a french department) with the following classes: asphalt, bare soil, building, grassland, mineral material (permeable artificialized areas), forest and water from 20cm aerial images and Digital Height Model. We created a training dataset on a few areas of interest aggregating databases, semi-automatic classification, and manual annotation to get a complete ground truth in each class. A comparative study of different encoder-decoder architectures (U-Net, U-Net with Resnet encoders, Deeplab v3+) is presented with different loss functions. The final product is a highly valuable land cover map computed from model predictions stitched together, binarized, and refined before vectorization.



### Stillleben: Realistic Scene Synthesis for Deep Learning in Robotics
- **Arxiv ID**: http://arxiv.org/abs/2005.05659v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.05659v1)
- **Published**: 2020-05-12 10:11:00+00:00
- **Updated**: 2020-05-12 10:11:00+00:00
- **Authors**: Max Schwarz, Sven Behnke
- **Comment**: Accepted for ICRA 2020
- **Journal**: None
- **Summary**: Training data is the key ingredient for deep learning approaches, but difficult to obtain for the specialized domains often encountered in robotics. We describe a synthesis pipeline capable of producing training data for cluttered scene perception tasks such as semantic segmentation, object detection, and correspondence or pose estimation. Our approach arranges object meshes in physically realistic, dense scenes using physics simulation. The arranged scenes are rendered using high-quality rasterization with randomized appearance and material parameters. Noise and other transformations introduced by the camera sensors are simulated. Our pipeline can be run online during training of a deep neural network, yielding applications in life-long learning and in iterative render-and-compare approaches. We demonstrate the usability by learning semantic segmentation on the challenging YCB-Video dataset without actually using any training frames, where our method achieves performance comparable to a conventionally trained model. Additionally, we show successful application in a real-world regrasping system.



### RetinotopicNet: An Iterative Attention Mechanism Using Local Descriptors with Global Context
- **Arxiv ID**: http://arxiv.org/abs/2005.05701v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.05701v1)
- **Published**: 2020-05-12 11:54:56+00:00
- **Updated**: 2020-05-12 11:54:56+00:00
- **Authors**: Thomas Kurbiel, Shahrzad Khaleghian
- **Comment**: 7 pages, 23 figures
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) were the driving force behind many advancements in Computer Vision research in recent years. This progress has spawned many practical applications and we see an increased need to efficiently move CNNs to embedded systems today. However traditional CNNs lack the property of scale and rotation invariance: two of the most frequently encountered transformations in natural images. As a consequence CNNs have to learn different features for same objects at different scales. This redundancy is the main reason why CNNs need to be very deep in order to achieve the desired accuracy. In this paper we develop an efficient solution by reproducing how nature has solved the problem in the human brain. To this end we let our CNN operate on small patches extracted using the log-polar transform, which is known to be scale and rotation equivariant. Patches extracted in this way have the nice property of magnifying the central field and compressing the periphery. Hence we obtain local descriptors with global context information. However the processing of a single patch is usually not sufficient to achieve high accuracies in e.g. classification tasks. We therefore successively jump to several different locations, called saccades, thus building an understanding of the whole image. Since log-polar patches contain global context information, we can efficiently calculate following saccades using only the small patches. Saccades efficiently compensate for the lack of translation equivariance of the log-polar transform.



### Automatic clustering of Celtic coins based on 3D point cloud pattern analysis
- **Arxiv ID**: http://arxiv.org/abs/2005.05705v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.05705v1)
- **Published**: 2020-05-12 11:59:16+00:00
- **Updated**: 2020-05-12 11:59:16+00:00
- **Authors**: Sofiane Horache, Jean-Emmanuel Deschaud, François Goulette, Katherine Gruel, Thierry Lejars
- **Comment**: None
- **Journal**: None
- **Summary**: The recognition and clustering of coins which have been struck by the same die is of interest for archeological studies. Nowadays, this work can only be performed by experts and is very tedious. In this paper, we propose a method to automatically cluster dies, based on 3D scans of coins. It is based on three steps: registration, comparison and graph-based clustering. Experimental results on 90 coins coming from a Celtic treasury from the II-Ith century BC show a clustering quality equivalent to expert's work.



### IterDet: Iterative Scheme for Object Detection in Crowded Environments
- **Arxiv ID**: http://arxiv.org/abs/2005.05708v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.05708v2)
- **Published**: 2020-05-12 12:04:27+00:00
- **Updated**: 2021-01-29 07:26:14+00:00
- **Authors**: Danila Rukhovich, Konstantin Sofiiuk, Danil Galeev, Olga Barinova, Anton Konushin
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning-based detectors usually produce a redundant set of object bounding boxes including many duplicate detections of the same object. These boxes are then filtered using non-maximum suppression (NMS) in order to select exactly one bounding box per object of interest. This greedy scheme is simple and provides sufficient accuracy for isolated objects but often fails in crowded environments, since one needs to both preserve boxes for different objects and suppress duplicate detections. In this work we develop an alternative iterative scheme, where a new subset of objects is detected at each iteration. Detected boxes from the previous iterations are passed to the network at the following iterations to ensure that the same object would not be detected twice. This iterative scheme can be applied to both one-stage and two-stage object detectors with just minor modifications of the training and inference procedures. We perform extensive experiments with two different baseline detectors on four datasets and show significant improvement over the baseline, leading to state-of-the-art performance on CrowdHuman and WiderPerson datasets. The source code and the trained models are available at https://github.com/saic-vul/iterdet.



### Skeleton-Aware Networks for Deep Motion Retargeting
- **Arxiv ID**: http://arxiv.org/abs/2005.05732v1
- **DOI**: 10.1145/3386569.3392462
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.05732v1)
- **Published**: 2020-05-12 12:51:40+00:00
- **Updated**: 2020-05-12 12:51:40+00:00
- **Authors**: Kfir Aberman, Peizhuo Li, Dani Lischinski, Olga Sorkine-Hornung, Daniel Cohen-Or, Baoquan Chen
- **Comment**: SIGGRAPH 2020. Project page:
  https://deepmotionediting.github.io/retargeting , Video:
  https://www.youtube.com/watch?v=ym8Tnmiz5N8
- **Journal**: None
- **Summary**: We introduce a novel deep learning framework for data-driven motion retargeting between skeletons, which may have different structure, yet corresponding to homeomorphic graphs. Importantly, our approach learns how to retarget without requiring any explicit pairing between the motions in the training set. We leverage the fact that different homeomorphic skeletons may be reduced to a common primal skeleton by a sequence of edge merging operations, which we refer to as skeletal pooling. Thus, our main technical contribution is the introduction of novel differentiable convolution, pooling, and unpooling operators. These operators are skeleton-aware, meaning that they explicitly account for the skeleton's hierarchical structure and joint adjacency, and together they serve to transform the original motion into a collection of deep temporal features associated with the joints of the primal skeleton. In other words, our operators form the building blocks of a new deep motion processing framework that embeds the motion into a common latent space, shared by a collection of homeomorphic skeletons. Thus, retargeting can be achieved simply by encoding to, and decoding from this latent space. Our experiments show the effectiveness of our framework for motion retargeting, as well as motion processing in general, compared to existing approaches. Our approach is also quantitatively evaluated on a synthetic dataset that contains pairs of motions applied to different skeletons. To the best of our knowledge, our method is the first to perform retargeting between skeletons with differently sampled kinematic chains, without any paired examples.



### Angular Triplet Loss-based Camera Network for ReID
- **Arxiv ID**: http://arxiv.org/abs/2005.05740v2
- **DOI**: 10.1109/IJCNN52387.2021.9534270
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.05740v2)
- **Published**: 2020-05-12 13:14:15+00:00
- **Updated**: 2021-09-23 03:01:55+00:00
- **Authors**: Yitian Li, Ruini Xue, Mengmeng Zhu, Jing Xu, Zenglin Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Person re-identification (ReID) is a challenging crosscamera retrieval task to identify pedestrians. Many complex network structures are proposed recently and many of them concentrate on multi-branch features to achieve high performance. However, they are too heavy-weight to deploy in realworld applications. Additionally, pedestrian images are often captured by different surveillance cameras, so the varied lights, perspectives and resolutions result in inevitable multi-camera domain gaps for ReID. To address these issues, this paper proposes ATCN, a simple but effective angular triplet loss-based camera network, which is able to achieve compelling performance with only global features. In ATCN, a novel angular distance is introduced to learn a more discriminative feature representation in the embedding space. Meanwhile, a lightweight camera network is designed to transfer global features to more discriminative features. ATCN is designed to be simple and flexible so it can be easily deployed in practice. The experiment results on various benchmark datasets show that ATCN outperforms many SOTA approaches.



### Unpaired Motion Style Transfer from Video to Animation
- **Arxiv ID**: http://arxiv.org/abs/2005.05751v1
- **DOI**: 10.1145/3386569.3392469
- **Categories**: **cs.GR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.05751v1)
- **Published**: 2020-05-12 13:21:27+00:00
- **Updated**: 2020-05-12 13:21:27+00:00
- **Authors**: Kfir Aberman, Yijia Weng, Dani Lischinski, Daniel Cohen-Or, Baoquan Chen
- **Comment**: SIGGRAPH 2020. Project page:
  https://deepmotionediting.github.io/style_transfer , Video:
  https://www.youtube.com/watch?v=m04zuBSdGrc , Code:
  https://github.com/DeepMotionEditing/deep-motion-editing
- **Journal**: None
- **Summary**: Transferring the motion style from one animation clip to another, while preserving the motion content of the latter, has been a long-standing problem in character animation. Most existing data-driven approaches are supervised and rely on paired data, where motions with the same content are performed in different styles. In addition, these approaches are limited to transfer of styles that were seen during training. In this paper, we present a novel data-driven framework for motion style transfer, which learns from an unpaired collection of motions with style labels, and enables transferring motion styles not observed during training. Furthermore, our framework is able to extract motion styles directly from videos, bypassing 3D reconstruction, and apply them to the 3D input motion. Our style transfer network encodes motions into two latent codes, for content and for style, each of which plays a different role in the decoding (synthesis) process. While the content code is decoded into the output motion by several temporal convolutional layers, the style code modifies deep features via temporally invariant adaptive instance normalization (AdaIN). Moreover, while the content code is encoded from 3D joint rotations, we learn a common embedding for style from either 3D or 2D joint positions, enabling style extraction from videos. Our results are comparable to the state-of-the-art, despite not requiring paired training data, and outperform other methods when transferring previously unseen styles. To our knowledge, we are the first to demonstrate style transfer directly from videos to 3D animations - an ability which enables one to extend the set of style examples far beyond motions captured by MoCap systems.



### Adaptive Mixture Regression Network with Local Counting Map for Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/2005.05776v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.05776v2)
- **Published**: 2020-05-12 13:54:05+00:00
- **Updated**: 2020-05-13 06:53:41+00:00
- **Authors**: Xiyang Liu, Jie Yang, Wenrui Ding
- **Comment**: None
- **Journal**: None
- **Summary**: The crowd counting task aims at estimating the number of people located in an image or a frame from videos. Existing methods widely adopt density maps as the training targets to optimize the point-to-point loss. While in testing phase, we only focus on the differences between the crowd numbers and the global summation of density maps, which indicate the inconsistency between the training targets and the evaluation criteria. To solve this problem, we introduce a new target, named local counting map (LCM), to obtain more accurate results than density map based approaches. Moreover, we also propose an adaptive mixture regression framework with three modules in a coarse-to-fine manner to further improve the precision of the crowd estimation: scale-aware module (SAM), mixture regression module (MRM) and adaptive soft interval module (ASIM). Specifically, SAM fully utilizes the context and multi-scale information from different convolutional features; MRM and ASIM perform more precise counting regression on local patches of images. Compared with current methods, the proposed method reports better performances on the typical datasets. The source code is available at https://github.com/xiyang1012/Local-Crowd-Counting.



### HDD-Net: Hybrid Detector Descriptor with Mutual Interactive Learning
- **Arxiv ID**: http://arxiv.org/abs/2005.05777v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.05777v2)
- **Published**: 2020-05-12 13:55:04+00:00
- **Updated**: 2020-11-26 09:14:34+00:00
- **Authors**: Axel Barroso-Laguna, Yannick Verdie, Benjamin Busam, Krystian Mikolajczyk
- **Comment**: None
- **Journal**: Asian Conference on Computer Vision (ACCV), 2020
- **Summary**: Local feature extraction remains an active research area due to the advances in fields such as SLAM, 3D reconstructions, or AR applications. The success in these applications relies on the performance of the feature detector and descriptor. While the detector-descriptor interaction of most methods is based on unifying in single network detections and descriptors, we propose a method that treats both extractions independently and focuses on their interaction in the learning process rather than by parameter sharing. We formulate the classical hard-mining triplet loss as a new detector optimisation term to refine candidate positions based on the descriptor map. We propose a dense descriptor that uses a multi-scale approach and a hybrid combination of hand-crafted and learned features to obtain rotation and scale robustness by design. We evaluate our method extensively on different benchmarks and show improvements over the state of the art in terms of image matching on HPatches and 3D reconstruction quality while keeping on par on camera localisation tasks.



### One-Shot Recognition of Manufacturing Defects in Steel Surfaces
- **Arxiv ID**: http://arxiv.org/abs/2005.05815v1
- **DOI**: 10.1016/j.promfg.2020.05.146
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.05815v1)
- **Published**: 2020-05-12 14:30:03+00:00
- **Updated**: 2020-05-12 14:30:03+00:00
- **Authors**: Aditya M. Deshpande, Ali A. Minai, Manish Kumar
- **Comment**: Accepted for publication in NAMRC 48
- **Journal**: Procedia Manufacturing 48 (2020) 1064-1071
- **Summary**: Quality control is an essential process in manufacturing to make the product defect-free as well as to meet customer needs. The automation of this process is important to maintain high quality along with the high manufacturing throughput. With recent developments in deep learning and computer vision technologies, it has become possible to detect various features from the images with near-human accuracy. However, many of these approaches are data intensive. Training and deployment of such a system on manufacturing floors may become expensive and time-consuming. The need for large amounts of training data is one of the limitations of the applicability of these approaches in real-world manufacturing systems. In this work, we propose the application of a Siamese convolutional neural network to do one-shot recognition for such a task. Our results demonstrate how one-shot learning can be used in quality control of steel by identification of defects on the steel surface. This method can significantly reduce the requirements of training data and can also be run in real-time.



### A Distributed Approximate Nearest Neighbor Method for Real-Time Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2005.05824v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.05824v2)
- **Published**: 2020-05-12 14:39:31+00:00
- **Updated**: 2020-08-27 22:14:18+00:00
- **Authors**: Aysan Aghazadeh, Maryam Amirmazlaghani
- **Comment**: None
- **Journal**: None
- **Summary**: Nowadays, face recognition and more generally image recognition have many applications in the modern world and are widely used in our daily tasks. This paper aims to propose a distributed approximate nearest neighbor (ANN) method for real-time face recognition using a big dataset that involves a lot of classes. The proposed approach is based on using a clustering method to separate the dataset into different clusters and on specifying the importance of each cluster by defining cluster weights. To this end, reference instances are selected from each cluster based on the cluster weights using a maximum likelihood approach. This process leads to a more informed selection of instances, so it enhances the performance of the algorithm. Experimental results confirm the efficiency of the proposed method and its out-performance in terms of accuracy and the processing time.



### Bayesian Fusion for Infrared and Visible Images
- **Arxiv ID**: http://arxiv.org/abs/2005.05839v1
- **DOI**: 10.1016/j.sigpro.2020.107734
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.05839v1)
- **Published**: 2020-05-12 14:57:19+00:00
- **Updated**: 2020-05-12 14:57:19+00:00
- **Authors**: Zixiang Zhao, Shuang Xu, Chunxia Zhang, Junmin Liu, Jiangshe Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Infrared and visible image fusion has been a hot issue in image fusion. In this task, a fused image containing both the gradient and detailed texture information of visible images as well as the thermal radiation and highlighting targets of infrared images is expected to be obtained. In this paper, a novel Bayesian fusion model is established for infrared and visible images. In our model, the image fusion task is cast into a regression problem. To measure the variable uncertainty, we formulate the model in a hierarchical Bayesian manner. Aiming at making the fused image satisfy human visual system, the model incorporates the total-variation(TV) penalty. Subsequently, the model is efficiently inferred by the expectation-maximization(EM) algorithm. We test our algorithm on TNO and NIR image fusion datasets with several state-of-the-art approaches. Compared with the previous methods, the novel model can generate better fused images with high-light targets and rich texture details, which can improve the reliability of the target automatic detection and recognition system.



### Probabilistic Semantic Segmentation Refinement by Monte Carlo Region Growing
- **Arxiv ID**: http://arxiv.org/abs/2005.05856v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.05856v1)
- **Published**: 2020-05-12 15:23:57+00:00
- **Updated**: 2020-05-12 15:23:57+00:00
- **Authors**: Philipe A. Dias, Henry Medeiros
- **Comment**: Submitted to IEEE Transactions on Image Processing (April 2020)
- **Journal**: None
- **Summary**: Semantic segmentation with fine-grained pixel-level accuracy is a fundamental component of a variety of computer vision applications. However, despite the large improvements provided by recent advances in the architectures of convolutional neural networks, segmentations provided by modern state-of-the-art methods still show limited boundary adherence. We introduce a fully unsupervised post-processing algorithm that exploits Monte Carlo sampling and pixel similarities to propagate high-confidence pixel labels into regions of low-confidence classification. Our algorithm, which we call probabilistic Region Growing Refinement (pRGR), is based on a rigorous mathematical foundation in which clusters are modelled as multivariate normally distributed sets of pixels. Exploiting concepts of Bayesian estimation and variance reduction techniques, pRGR performs multiple refinement iterations at varied receptive fields sizes, while updating cluster statistics to adapt to local image features. Experiments using multiple modern semantic segmentation networks and benchmark datasets demonstrate the effectiveness of our approach for the refinement of segmentation predictions at different levels of coarseness, as well as the suitability of the variance estimates obtained in the Monte Carlo iterations as uncertainty measures that are highly correlated with segmentation accuracy.



### Neural Architecture Transfer
- **Arxiv ID**: http://arxiv.org/abs/2005.05859v2
- **DOI**: 10.1109/TPAMI.2021.3052758
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2005.05859v2)
- **Published**: 2020-05-12 15:30:36+00:00
- **Updated**: 2021-03-22 00:32:53+00:00
- **Authors**: Zhichao Lu, Gautam Sreekumar, Erik Goodman, Wolfgang Banzhaf, Kalyanmoy Deb, Vishnu Naresh Boddeti
- **Comment**: Code is available at
  https://github.com/human-analysis/neural-architecture-transfer
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence,
  2021
- **Summary**: Neural architecture search (NAS) has emerged as a promising avenue for automatically designing task-specific neural networks. Existing NAS approaches require one complete search for each deployment specification of hardware or objective. This is a computationally impractical endeavor given the potentially large number of application scenarios. In this paper, we propose Neural Architecture Transfer (NAT) to overcome this limitation. NAT is designed to efficiently generate task-specific custom models that are competitive under multiple conflicting objectives. To realize this goal we learn task-specific supernets from which specialized subnets can be sampled without any additional training. The key to our approach is an integrated online transfer learning and many-objective evolutionary search procedure. A pre-trained supernet is iteratively adapted while simultaneously searching for task-specific subnets. We demonstrate the efficacy of NAT on 11 benchmark image classification tasks ranging from large-scale multi-class to small-scale fine-grained datasets. In all cases, including ImageNet, NATNets improve upon the state-of-the-art under mobile settings ($\leq$ 600M Multiply-Adds). Surprisingly, small-scale fine-grained datasets benefit the most from NAT. At the same time, the architecture search and transfer is orders of magnitude more efficient than existing NAS methods. Overall, the experimental evaluation indicates that, across diverse image classification tasks and computational objectives, NAT is an appreciably more effective alternative to conventional transfer learning of fine-tuning weights of an existing network architecture learned on standard datasets. Code is available at https://github.com/human-analysis/neural-architecture-transfer



### Recurrent and Spiking Modeling of Sparse Surgical Kinematics
- **Arxiv ID**: http://arxiv.org/abs/2005.05868v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.05868v2)
- **Published**: 2020-05-12 15:41:45+00:00
- **Updated**: 2020-06-11 16:01:48+00:00
- **Authors**: Neil Getty, Zixuan Zhao, Stephan Gruessner, Liaohai Chen, Fangfang Xia
- **Comment**: 5 pages, 8 figures, accepted ICONS 2020
- **Journal**: None
- **Summary**: Robot-assisted minimally invasive surgery is improving surgeon performance and patient outcomes. This innovation is also turning what has been a subjective practice into motion sequences that can be precisely measured. A growing number of studies have used machine learning to analyze video and kinematic data captured from surgical robots. In these studies, models are typically trained on benchmark datasets for representative surgical tasks to assess surgeon skill levels. While they have shown that novices and experts can be accurately classified, it is not clear whether machine learning can separate highly proficient surgeons from one another, especially without video data. In this study, we explore the possibility of using only kinematic data to predict surgeons of similar skill levels. We focus on a new dataset created from surgical exercises on a simulation device for skill training. A simple, efficient encoding scheme was devised to encode kinematic sequences so that they were amenable to edge learning. We report that it is possible to identify surgical fellows receiving near perfect scores in the simulation exercises based on their motion characteristics alone. Further, our model could be converted to a spiking neural network to train and infer on the Nengo simulation framework with no loss in accuracy. Overall, this study suggests that building neuromorphic models from sparse motion features may be a potentially useful strategy for identifying surgeons and gestures with chips deployed on robotic systems to offer adaptive assistance during surgery and training with additional latency and privacy benefits.



### Latent Fingerprint Registration via Matching Densely Sampled Points
- **Arxiv ID**: http://arxiv.org/abs/2005.05878v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.05878v2)
- **Published**: 2020-05-12 15:51:59+00:00
- **Updated**: 2021-01-20 02:54:13+00:00
- **Authors**: Shan Gu, Jianjiang Feng, Jiwen Lu, Jie Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Latent fingerprint matching is a very important but unsolved problem. As a key step of fingerprint matching, fingerprint registration has a great impact on the recognition performance. Existing latent fingerprint registration approaches are mainly based on establishing correspondences between minutiae, and hence will certainly fail when there are no sufficient number of extracted minutiae due to small fingerprint area or poor image quality. Minutiae extraction has become the bottleneck of latent fingerprint registration. In this paper, we propose a non-minutia latent fingerprint registration method which estimates the spatial transformation between a pair of fingerprints through a dense fingerprint patch alignment and matching procedure. Given a pair of fingerprints to match, we bypass the minutiae extraction step and take uniformly sampled points as key points. Then the proposed patch alignment and matching algorithm compares all pairs of sampling points and produces their similarities along with alignment parameters. Finally, a set of consistent correspondences are found by spectral clustering. Extensive experiments on NIST27 database and MOLF database show that the proposed method achieves the state-of-the-art registration performance, especially under challenging conditions.



### Efficient and Model-Based Infrared and Visible Image Fusion Via Algorithm Unrolling
- **Arxiv ID**: http://arxiv.org/abs/2005.05896v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.05896v2)
- **Published**: 2020-05-12 16:15:56+00:00
- **Updated**: 2021-04-23 12:04:44+00:00
- **Authors**: Zixiang Zhao, Shuang Xu, Jiangshe Zhang, Chengyang Liang, Chunxia Zhang, Junmin Liu
- **Comment**: Accepted by IEEE Transactions on Circuits and Systems for Video
  Technology
- **Journal**: None
- **Summary**: Infrared and visible image fusion (IVIF) expects to obtain images that retain thermal radiation information from infrared images and texture details from visible images. In this paper, a model-based convolutional neural network (CNN) model, referred to as Algorithm Unrolling Image Fusion (AUIF), is proposed to overcome the shortcomings of traditional CNN-based IVIF models. The proposed AUIF model starts with the iterative formulas of two traditional optimization models, which are established to accomplish two-scale decomposition, i.e., separating low-frequency base information and high-frequency detail information from source images. Then the algorithm unrolling is implemented where each iteration is mapped to a CNN layer and each optimization model is transformed into a trainable neural network. Compared with the general network architectures, the proposed framework combines the model-based prior information and is designed more reasonably. After the unrolling operation, our model contains two decomposers (encoders) and an additional reconstructor (decoder). In the training phase, this network is trained to reconstruct the input image. While in the test phase, the base (or detail) decomposed feature maps of infrared/visible images are merged respectively by an extra fusion layer, and then the decoder outputs the fusion image. Qualitative and quantitative comparisons demonstrate the superiority of our model, which can robustly generate fusion images containing highlight targets and legible details, exceeding the state-of-the-art methods. Furthermore, our network has fewer weights and faster speed.



### Localized convolutional neural networks for geospatial wind forecasting
- **Arxiv ID**: http://arxiv.org/abs/2005.05930v3
- **DOI**: 10.3390/en13133440
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML, 68T05, I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/2005.05930v3)
- **Published**: 2020-05-12 17:14:49+00:00
- **Updated**: 2020-07-10 16:13:17+00:00
- **Authors**: Arnas Uselis, Mantas Lukoševičius, Lukas Stasytis
- **Comment**: None
- **Journal**: Energies, 13 (13), pp. 3440, 2020
- **Summary**: Convolutional Neural Networks (CNN) possess many positive qualities when it comes to spatial raster data. Translation invariance enables CNNs to detect features regardless of their position in the scene. However, in some domains, like geospatial, not all locations are exactly equal. In this work, we propose localized convolutional neural networks that enable convolutional architectures to learn local features in addition to the global ones. We investigate their instantiations in the form of learnable inputs, local weights, and a more general form. They can be added to any convolutional layers, easily end-to-end trained, introduce minimal additional complexity, and let CNNs retain most of their benefits to the extent that they are needed. In this work we address spatio-temporal prediction: test the effectiveness of our methods on a synthetic benchmark dataset and tackle three real-world wind prediction datasets. For one of them, we propose a method to spatially order the unordered data. We compare the recent state-of-the-art spatio-temporal prediction models on the same data. Models that use convolutional layers can be and are extended with our localizations. In all these cases our extensions improve the results, and thus often the state-of-the-art. We share all the code at a public repository.



### Planning to Explore via Self-Supervised World Models
- **Arxiv ID**: http://arxiv.org/abs/2005.05960v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.05960v2)
- **Published**: 2020-05-12 17:59:45+00:00
- **Updated**: 2020-06-30 23:05:50+00:00
- **Authors**: Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, Deepak Pathak
- **Comment**: Accepted at ICML 2020. Videos and code at
  https://ramanans1.github.io/plan2explore/
- **Journal**: None
- **Summary**: Reinforcement learning allows solving complex tasks, however, the learning tends to be task-specific and the sample efficiency remains a challenge. We present Plan2Explore, a self-supervised reinforcement learning agent that tackles both these challenges through a new approach to self-supervised exploration and fast adaptation to new tasks, which need not be known during exploration. During exploration, unlike prior methods which retrospectively compute the novelty of observations after the agent has already reached them, our agent acts efficiently by leveraging planning to seek out expected future novelty. After exploration, the agent quickly adapts to multiple downstream tasks in a zero or a few-shot manner. We evaluate on challenging control tasks from high-dimensional image inputs. Without any training supervision or task-specific interaction, Plan2Explore outperforms prior self-supervised exploration methods, and in fact, almost matches the performances oracle which has access to rewards. Videos and code at https://ramanans1.github.io/plan2explore/



### Fast Deep Multi-patch Hierarchical Network for Nonhomogeneous Image Dehazing
- **Arxiv ID**: http://arxiv.org/abs/2005.05999v1
- **DOI**: 10.1109/CVPRW50498.2020.00249
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.05999v1)
- **Published**: 2020-05-12 18:26:51+00:00
- **Updated**: 2020-05-12 18:26:51+00:00
- **Authors**: Sourya Dipta Das, Saikat Dutta
- **Comment**: CVPR Workshops Proceedings 2020
- **Journal**: 2020 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition Workshops (CVPRW)
- **Summary**: Recently, CNN based end-to-end deep learning methods achieve superiority in Image Dehazing but they tend to fail drastically in Non-homogeneous dehazing. Apart from that, existing popular Multi-scale approaches are runtime intensive and memory inefficient. In this context, we proposed a fast Deep Multi-patch Hierarchical Network to restore Non-homogeneous hazed images by aggregating features from multiple image patches from different spatial sections of the hazed image with fewer number of network parameters. Our proposed method is quite robust for different environments with various density of the haze or fog in the scene and very lightweight as the total size of the model is around 21.7 MB. It also provides faster runtime compared to current multi-scale methods with an average runtime of 0.0145s to process 1200x1600 HD quality image. Finally, we show the superiority of this network on Dense Haze Removal to other state-of-the-art models.



### Increased-confidence adversarial examples for deep learning counter-forensics
- **Arxiv ID**: http://arxiv.org/abs/2005.06023v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2005.06023v2)
- **Published**: 2020-05-12 19:29:03+00:00
- **Updated**: 2022-01-06 12:37:19+00:00
- **Authors**: Wenjie Li, Benedetta Tondi, Rongrong Ni, Mauro Barni
- **Comment**: None
- **Journal**: None
- **Summary**: Transferability of adversarial examples is a key issue to apply this kind of attacks against multimedia forensics (MMF) techniques based on Deep Learning (DL) in a real-life setting. Adversarial example transferability, in fact, would open the way to the deployment of successful counter forensics attacks also in cases where the attacker does not have a full knowledge of the to-be-attacked system. Some preliminary works have shown that adversarial examples against CNN-based image forensics detectors are in general non-transferrable, at least when the basic versions of the attacks implemented in the most popular libraries are adopted. In this paper, we introduce a general strategy to increase the strength of the attacks and evaluate their transferability when such a strength varies. We experimentally show that, in this way, attack transferability can be largely increased, at the expense of a larger distortion. Our research confirms the security threats posed by the existence of adversarial examples even in multimedia forensics scenarios, thus calling for new defense strategies to improve the security of DL-based MMF techniques.



### Computer Vision Toolkit for Non-invasive Monitoring of Factory Floor Artifacts
- **Arxiv ID**: http://arxiv.org/abs/2005.06037v1
- **DOI**: 10.1016/j.promfg.2020.05.141
- **Categories**: **cs.CV**, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2005.06037v1)
- **Published**: 2020-05-12 20:25:34+00:00
- **Updated**: 2020-05-12 20:25:34+00:00
- **Authors**: Aditya M. Deshpande, Anil Kumar Telikicherla, Vinay Jakkali, David A. Wickelhaus, Manish Kumar, Sam Anand
- **Comment**: Accepted for publication in 48th SME North American Manufacturing
  Research Conference (NAMRC48)
- **Journal**: Procedia Manufacturing 48 (2020) 1020-1028
- **Summary**: Digitization has led to smart, connected technologies be an integral part of businesses, governments and communities. For manufacturing digitization, there has been active research and development with a focus on Cloud Manufacturing (CM) and the Industrial Internet of Things (IIoT). This work presents a computer vision toolkit (CV Toolkit) for non-invasive digitization of the factory floor in line with Industry 4.0 requirements for factory data collection. Currently, technical challenges persist towards digitization of legacy systems due to the limitation for changes in their design and sensors. This novel toolkit is developed to facilitate easy integration of legacy production machinery and factory floor artifacts with the digital and smart manufacturing environment with no requirement of any physical changes in the machines. The system developed is modular, and allows real-time monitoring of production machinery. Modularity aspect allows the incorporation of new software applications in the current framework of CV Toolkit. To allow connectivity of this toolkit with manufacturing floors in a simple, deployable and cost-effective manner, the toolkit is integrated with a known manufacturing data standard, MTConnect, to "translate" the digital inputs into data streams that can be read by commercial status tracking and reporting software solutions. The proposed toolkit is demonstrated using a mock-panel environment developed in house at the University of Cincinnati to highlight its usability.



### Generalized Multi-view Shared Subspace Learning using View Bootstrapping
- **Arxiv ID**: http://arxiv.org/abs/2005.06038v1
- **DOI**: 10.1109/TSP.2021.3102751
- **Categories**: **cs.LG**, cs.CV, cs.SD, eess.AS, eess.SP, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.06038v1)
- **Published**: 2020-05-12 20:35:14+00:00
- **Updated**: 2020-05-12 20:35:14+00:00
- **Authors**: Krishna Somandepalli, Shrikanth Narayanan
- **Comment**: None
- **Journal**: None
- **Summary**: A key objective in multi-view learning is to model the information common to multiple parallel views of a class of objects/events to improve downstream learning tasks. In this context, two open research questions remain: How can we model hundreds of views per event? Can we learn robust multi-view embeddings without any knowledge of how these views are acquired? We present a neural method based on multi-view correlation to capture the information shared across a large number of views by subsampling them in a view-agnostic manner during training. To provide an upper bound on the number of views to subsample for a given embedding dimension, we analyze the error of the bootstrapped multi-view correlation objective using matrix concentration theory. Our experiments on spoken word recognition, 3D object classification and pose-invariant face recognition demonstrate the robustness of view bootstrapping to model a large number of views. Results underscore the applicability of our method for a view-agnostic learning setting.



### Occlusion-Adaptive Deep Network for Robust Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/2005.06040v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.06040v1)
- **Published**: 2020-05-12 20:42:55+00:00
- **Updated**: 2020-05-12 20:42:55+00:00
- **Authors**: Hui Ding, Peng Zhou, Rama Chellappa
- **Comment**: None
- **Journal**: None
- **Summary**: Recognizing the expressions of partially occluded faces is a challenging computer vision problem. Previous expression recognition methods, either overlooked this issue or resolved it using extreme assumptions. Motivated by the fact that the human visual system is adept at ignoring the occlusion and focus on non-occluded facial areas, we propose a landmark-guided attention branch to find and discard corrupted features from occluded regions so that they are not used for recognition. An attention map is first generated to indicate if a specific facial part is occluded and guide our model to attend to non-occluded regions. To further improve robustness, we propose a facial region branch to partition the feature maps into non-overlapping facial blocks and task each block to predict the expression independently. This results in more diverse and discriminative features, enabling the expression recognition system to recover even though the face is partially occluded. Depending on the synergistic effects of the two branches, our occlusion-adaptive deep network significantly outperforms state-of-the-art methods on two challenging in-the-wild benchmark datasets and three real-world occluded expression datasets.



### Compositional Few-Shot Recognition with Primitive Discovery and Enhancing
- **Arxiv ID**: http://arxiv.org/abs/2005.06047v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2005.06047v3)
- **Published**: 2020-05-12 21:01:25+00:00
- **Updated**: 2020-09-22 00:03:47+00:00
- **Authors**: Yixiong Zou, Shanghang Zhang, Ke Chen, Yonghong Tian, Yaowei Wang, José M. F. Moura
- **Comment**: None
- **Journal**: None
- **Summary**: Few-shot learning (FSL) aims at recognizing novel classes given only few training samples, which still remains a great challenge for deep learning. However, humans can easily recognize novel classes with only few samples. A key component of such ability is the compositional recognition that human can perform, which has been well studied in cognitive science but is not well explored in FSL. Inspired by such capability of humans, to imitate humans' ability of learning visual primitives and composing primitives to recognize novel classes, we propose an approach to FSL to learn a feature representation composed of important primitives, which is jointly trained with two parts, i.e. primitive discovery and primitive enhancing. In primitive discovery, we focus on learning primitives related to object parts by self-supervision from the order of image splits, avoiding extra laborious annotations and alleviating the effect of semantic gaps. In primitive enhancing, inspired by current studies on the interpretability of deep networks, we provide our composition view for the FSL baseline model. To modify this model for effective composition, inspired by both mathematical deduction and biological studies (the Hebbian Learning rule and the Winner-Take-All mechanism), we propose a soft composition mechanism by enlarging the activation of important primitives while reducing that of others, so as to enhance the influence of important primitives and better utilize these primitives to compose novel classes. Extensive experiments on public benchmarks are conducted on both the few-shot image classification and video recognition tasks. Our method achieves the state-of-the-art performance on all these datasets and shows better interpretability.



### Class-Incremental Learning for Semantic Segmentation Re-Using Neither Old Data Nor Old Labels
- **Arxiv ID**: http://arxiv.org/abs/2005.06050v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.06050v1)
- **Published**: 2020-05-12 21:03:29+00:00
- **Updated**: 2020-05-12 21:03:29+00:00
- **Authors**: Marvin Klingner, Andreas Bär, Philipp Donn, Tim Fingscheidt
- **Comment**: ITSC 2020 Conference Paper
- **Journal**: None
- **Summary**: While neural networks trained for semantic segmentation are essential for perception in autonomous driving, most current algorithms assume a fixed number of classes, presenting a major limitation when developing new autonomous driving systems with the need of additional classes. In this paper we present a technique implementing class-incremental learning for semantic segmentation without using the labeled data the model was initially trained on. Previous approaches still either rely on labels for both old and new classes, or fail to properly distinguish between them. We show how to overcome these problems with a novel class-incremental learning technique, which nonetheless requires labels only for the new classes. Specifically, (i) we introduce a new loss function that neither relies on old data nor on old labels, (ii) we show how new classes can be integrated in a modular fashion into pretrained semantic segmentation models, and finally (iii) we re-implement previous approaches in a unified setting to compare them to ours. We evaluate our method on the Cityscapes dataset, where we exceed the mIoU performance of all baselines by 3.5% absolute reaching a result, which is only 2.2% absolute below the upper performance limit of single-stage training, relying on all data and labels simultaneously.



### Apple Defect Detection Using Deep Learning Based Object Detection For Better Post Harvest Handling
- **Arxiv ID**: http://arxiv.org/abs/2005.06089v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.06089v1)
- **Published**: 2020-05-12 23:34:43+00:00
- **Updated**: 2020-05-12 23:34:43+00:00
- **Authors**: Paolo Valdez
- **Comment**: Paper presented at the ICLR 2020 Workshop on Computer Vision for
  Agriculture (CV4A)
- **Journal**: None
- **Summary**: The inclusion of Computer Vision and Deep Learning technologies in Agriculture aims to increase the harvest quality, and productivity of farmers. During postharvest, the export market and quality evaluation are affected by assorting of fruits and vegetables. In particular, apples are susceptible to a wide range of defects that can occur during harvesting or/and during the post-harvesting period. This paper aims to help farmers with post-harvest handling by exploring if recent computer vision and deep learning methods such as the YOLOv3 (Redmon & Farhadi (2018)) can help in detecting healthy apples from apples with defects.



