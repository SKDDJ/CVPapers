# Arxiv Papers in cs.CV on 2020-05-24
### ShapeAdv: Generating Shape-Aware Adversarial 3D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2005.11626v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.11626v1)
- **Published**: 2020-05-24 00:03:27+00:00
- **Updated**: 2020-05-24 00:03:27+00:00
- **Authors**: Kibok Lee, Zhuoyuan Chen, Xinchen Yan, Raquel Urtasun, Ersin Yumer
- **Comment**: 3D Point Clouds, Adversarial Learning
- **Journal**: None
- **Summary**: We introduce ShapeAdv, a novel framework to study shape-aware adversarial perturbations that reflect the underlying shape variations (e.g., geometric deformations and structural differences) in the 3D point cloud space. We develop shape-aware adversarial 3D point cloud attacks by leveraging the learned latent space of a point cloud auto-encoder where the adversarial noise is applied in the latent space. Specifically, we propose three different variants including an exemplar-based one by guiding the shape deformation with auxiliary data, such that the generated point cloud resembles the shape morphing between objects in the same category. Different from prior works, the resulting adversarial 3D point clouds reflect the shape variations in the 3D point cloud space while still being close to the original one. In addition, experimental evaluations on the ModelNet40 benchmark demonstrate that our adversaries are more difficult to defend with existing point cloud defense methods and exhibit a higher attack transferability across classifiers. Our shape-aware adversarial attacks are orthogonal to existing point cloud based attacks and shed light on the vulnerability of 3D deep neural networks.



### MVStylizer: An Efficient Edge-Assisted Video Photorealistic Style Transfer System for Mobile Phones
- **Arxiv ID**: http://arxiv.org/abs/2005.11630v2
- **DOI**: 10.1145/3397166.3409140
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.11630v2)
- **Published**: 2020-05-24 00:54:27+00:00
- **Updated**: 2020-06-01 19:16:08+00:00
- **Authors**: Ang Li, Chunpeng Wu, Yiran Chen, Bin Ni
- **Comment**: None
- **Journal**: None
- **Summary**: Recent research has made great progress in realizing neural style transfer of images, which denotes transforming an image to a desired style. Many users start to use their mobile phones to record their daily life, and then edit and share the captured images and videos with other users. However, directly applying existing style transfer approaches on videos, i.e., transferring the style of a video frame by frame, requires an extremely large amount of computation resources. It is still technically unaffordable to perform style transfer of videos on mobile phones. To address this challenge, we propose MVStylizer, an efficient edge-assisted photorealistic video style transfer system for mobile phones. Instead of performing stylization frame by frame, only key frames in the original video are processed by a pre-trained deep neural network (DNN) on edge servers, while the rest of stylized intermediate frames are generated by our designed optical-flow-based frame interpolation algorithm on mobile phones. A meta-smoothing module is also proposed to simultaneously upscale a stylized frame to arbitrary resolution and remove style transfer related distortions in these upscaled frames. In addition, for the sake of continuously enhancing the performance of the DNN model on the edge server, we adopt a federated learning scheme to keep retraining each DNN model on the edge server with collected data from mobile clients and syncing with a global DNN model on the cloud server. Such a scheme effectively leverages the diversity of collected data from various mobile clients and efficiently improves the system performance. Our experiments demonstrate that MVStylizer can generate stylized videos with an even better visual quality compared to the state-of-the-art method while achieving 75.5$\times$ speedup for 1920$\times$1080 videos.



### PoliteCamera: Respecting Strangers' Privacy in Mobile Photographing
- **Arxiv ID**: http://arxiv.org/abs/2005.11634v1
- **DOI**: 10.1007/978-3-030-01701-9_13
- **Categories**: **cs.CR**, cs.CV, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2005.11634v1)
- **Published**: 2020-05-24 01:18:13+00:00
- **Updated**: 2020-05-24 01:18:13+00:00
- **Authors**: Ang Li, Wei Du, Qinghua Li
- **Comment**: None
- **Journal**: None
- **Summary**: Camera is a standard on-board sensor of modern mobile phones. It makes photo taking popular due to its convenience and high resolution. However, when users take a photo of a scenery, a building or a target person, a stranger may also be unintentionally captured in the photo. Such photos expose the location and activity of strangers, and hence may breach their privacy. In this paper, we propose a cooperative mobile photographing scheme called PoliteCamera to protect strangers' privacy. Through the cooperation between a photographer and a stranger, the stranger's face in a photo can be automatically blurred upon his request when the photo is taken. Since multiple strangers nearby the photographer might send out blurring requests but not all of them are in the photo, an adapted balanced convolutional neural network (ABCNN) is proposed to determine whether the requesting stranger is in the photo based on facial attributes. Evaluations demonstrate that the ABCNN can accurately predict facial attributes and PoliteCamera can provide accurate privacy protection for strangers.



### Robust Object Detection under Occlusion with Context-Aware CompositionalNets
- **Arxiv ID**: http://arxiv.org/abs/2005.11643v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.11643v2)
- **Published**: 2020-05-24 02:57:34+00:00
- **Updated**: 2020-05-30 14:33:14+00:00
- **Authors**: Angtian Wang, Yihong Sun, Adam Kortylewski, Alan Yuille
- **Comment**: Accepted to CVPR 2020
- **Journal**: None
- **Summary**: Detecting partially occluded objects is a difficult task. Our experimental results show that deep learning approaches, such as Faster R-CNN, are not robust at object detection under occlusion. Compositional convolutional neural networks (CompositionalNets) have been shown to be robust at classifying occluded objects by explicitly representing the object as a composition of parts. In this work, we propose to overcome two limitations of CompositionalNets which will enable them to detect partially occluded objects: 1) CompositionalNets, as well as other DCNN architectures, do not explicitly separate the representation of the context from the object itself. Under strong object occlusion, the influence of the context is amplified which can have severe negative effects for detection at test time. In order to overcome this, we propose to segment the context during training via bounding box annotations. We then use the segmentation to learn a context-aware CompositionalNet that disentangles the representation of the context and the object. 2) We extend the part-based voting scheme in CompositionalNets to vote for the corners of the object's bounding box, which enables the model to reliably estimate bounding boxes for partially occluded objects. Our extensive experiments show that our proposed model can detect objects robustly, increasing the detection performance of strongly occluded vehicles from PASCAL3D+ and MS-COCO by 41% and 35% respectively in absolute performance relative to Faster R-CNN.



### Master-Auxiliary: an efficient aggregation strategy for video anomaly detection
- **Arxiv ID**: http://arxiv.org/abs/2005.11645v2
- **DOI**: 10.1109/LSP.2021.3107750
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.11645v2)
- **Published**: 2020-05-24 03:09:08+00:00
- **Updated**: 2020-09-19 13:34:56+00:00
- **Authors**: Zhiguo Wang, Zhongliang Yang, Yujin Zhang
- **Comment**: 7 pages
- **Journal**: None
- **Summary**: The aim of surveillance video anomaly detection is to detect events that rarely or never happened in a certain scene. Generally, different detectors can detect different anomalies. This paper proposes an efficient strategy to aggregate multiple detectors. First, the aggregation strategy chooses one detector as master detector by experience, and sets the remaining detectors as auxiliary detectors. Then, the aggregation strategy extracts credible information from auxiliary detectors, including credible abnormal (Cred-a) frames and credible normal (Cred-n) frames. After that, the frequencies that each video frame being judged as Cred-a and Cred-n are counted. Applying the events' time continuity property, more Cred-a and Cred-n frames can be inferred. Finally, the aggregation strategy utilizes the Cred-a and Cred-n frequencies to vote to calculate soft weights, and uses the soft weights to assist the master detector. Experiments are carried out on multiple datasets. Comparing with existing aggregation strategies, the proposed strategy achieves state-of-the-art performance.



### Benefits of temporal information for appearance-based gaze estimation
- **Arxiv ID**: http://arxiv.org/abs/2005.11670v1
- **DOI**: 10.1145/3379156.3391376
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.11670v1)
- **Published**: 2020-05-24 07:19:53+00:00
- **Updated**: 2020-05-24 07:19:53+00:00
- **Authors**: Cristina Palmero, Oleg V. Komogortsev, Sachin S. Talathi
- **Comment**: In ACM Symposium on Eye Tracking Research & Applications (ETRA), 2020
- **Journal**: None
- **Summary**: State-of-the-art appearance-based gaze estimation methods, usually based on deep learning techniques, mainly rely on static features. However, temporal trace of eye gaze contains useful information for estimating a given gaze point. For example, approaches leveraging sequential eye gaze information when applied to remote or low-resolution image scenarios with off-the-shelf cameras are showing promising results. The magnitude of contribution from temporal gaze trace is yet unclear for higher resolution/frame rate imaging systems, in which more detailed information about an eye is captured. In this paper, we investigate whether temporal sequences of eye images, captured using a high-resolution, high-frame rate head-mounted virtual reality system, can be leveraged to enhance the accuracy of an end-to-end appearance-based deep-learning model for gaze estimation. Performance is compared against a static-only version of the model. Results demonstrate statistically-significant benefits of temporal information, particularly for the vertical component of gaze.



### Networks with pixels embedding: a method to improve noise resistance in images classification
- **Arxiv ID**: http://arxiv.org/abs/2005.11679v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.11679v3)
- **Published**: 2020-05-24 07:55:08+00:00
- **Updated**: 2022-01-14 06:41:13+00:00
- **Authors**: Yang Liu, Hai-Long Tu, Chi-Chun Zhou, Yi Liu, Fu-Lin Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: In the task of image classification, usually, the network is sensitive to noises. For example, an image of cat with noises might be misclassified as an ostrich. Conventionally, to overcome the problem of noises, one uses the technique of data augmentation, that is, to teach the network to distinguish noises by adding more images with noises in the training dataset. In this work, we provide a noise-resistance network in images classification by introducing a technique of pixel embedding. We test the network with pixel embedding, which is abbreviated as the network with PE, on the mnist database of handwritten digits. It shows that the network with PE outperforms the conventional network on images with noises. The technique of pixel embedding can be used in many tasks of image classification to improve noise resistance.



### Learning Camera Miscalibration Detection
- **Arxiv ID**: http://arxiv.org/abs/2005.11711v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.11711v1)
- **Published**: 2020-05-24 10:32:49+00:00
- **Updated**: 2020-05-24 10:32:49+00:00
- **Authors**: Andrei Cramariuc, Aleksandar Petrov, Rohit Suri, Mayank Mittal, Roland Siegwart, Cesar Cadena
- **Comment**: ICRA 2020
- **Journal**: None
- **Summary**: Self-diagnosis and self-repair are some of the key challenges in deploying robotic platforms for long-term real-world applications. One of the issues that can occur to a robot is miscalibration of its sensors due to aging, environmental transients, or external disturbances. Precise calibration lies at the core of a variety of applications, due to the need to accurately perceive the world. However, while a lot of work has focused on calibrating the sensors, not much has been done towards identifying when a sensor needs to be recalibrated. This paper focuses on a data-driven approach to learn the detection of miscalibration in vision sensors, specifically RGB cameras. Our contributions include a proposed miscalibration metric for RGB cameras and a novel semi-synthetic dataset generation pipeline based on this metric. Additionally, by training a deep convolutional neural network, we demonstrate the effectiveness of our pipeline to identify whether a recalibration of the camera's intrinsic parameters is required or not. The code is available at http://github.com/ethz-asl/camera_miscalib_detection.



### A Lightweight CNN and Joint Shape-Joint Space (JS2) Descriptor for Radiological Osteoarthritis Detection
- **Arxiv ID**: http://arxiv.org/abs/2005.11715v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.11715v1)
- **Published**: 2020-05-24 10:48:38+00:00
- **Updated**: 2020-05-24 10:48:38+00:00
- **Authors**: Neslihan Bayramoglu, Miika T. Nieminen, Simo Saarakkala
- **Comment**: MIUA2020
- **Journal**: None
- **Summary**: Knee osteoarthritis (OA) is very common progressive and degenerative musculoskeletal disease worldwide creates a heavy burden on patients with reduced quality of life and also on society due to financial impact. Therefore, any attempt to reduce the burden of the disease could help both patients and society. In this study, we propose a fully automated novel method, based on combination of joint shape and convolutional neural network (CNN) based bone texture features, to distinguish between the knee radiographs with and without radiographic osteoarthritis. Moreover, we report the first attempt at describing the bone texture using CNN. Knee radiographs from Osteoarthritis Initiative (OAI) and Multicenter Osteoarthritis (MOST) studies were used in the experiments. Our models were trained on 8953 knee radiographs from OAI and evaluated on 3445 knee radiographs from MOST. Our results demonstrate that fusing the proposed shape and texture parameters achieves the state-of-the art performance in radiographic OA detection yielding area under the ROC curve (AUC) of 95.21%



### Multi-view Alignment and Generation in CCA via Consistent Latent Encoding
- **Arxiv ID**: http://arxiv.org/abs/2005.11716v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.11716v1)
- **Published**: 2020-05-24 10:50:15+00:00
- **Updated**: 2020-05-24 10:50:15+00:00
- **Authors**: Yaxin Shi, Yuangang Pan, Donna Xu, Ivor W. Tsang
- **Comment**: 37 pages, 22 figures
- **Journal**: None
- **Summary**: Multi-view alignment, achieving one-to-one correspondence of multi-view inputs, is critical in many real-world multi-view applications, especially for cross-view data analysis problems. Recently, an increasing number of works study this alignment problem with Canonical Correlation Analysis (CCA). However, existing CCA models are prone to misalign the multiple views due to either the neglect of uncertainty or the inconsistent encoding of the multiple views. To tackle these two issues, this paper studies multi-view alignment from the Bayesian perspective. Delving into the impairments of inconsistent encodings, we propose to recover correspondence of the multi-view inputs by matching the marginalization of the joint distribution of multi-view random variables under different forms of factorization. To realize our design, we present Adversarial CCA (ACCA) which achieves consistent latent encodings by matching the marginalized latent encodings through the adversarial training paradigm. Our analysis based on conditional mutual information reveals that ACCA is flexible for handling implicit distributions. Extensive experiments on correlation analysis and cross-view generation under noisy input settings demonstrate the superiority of our model.



### Fast and automated biomarker detection in breath samples with machine learning
- **Arxiv ID**: http://arxiv.org/abs/2006.01772v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2006.01772v1)
- **Published**: 2020-05-24 11:44:28+00:00
- **Updated**: 2020-05-24 11:44:28+00:00
- **Authors**: Angelika Skarysz, Dahlia Salman, Michael Eddleston, Martin Sykora, Eugenie Hunsicker, William H Nailon, Kareen Darnley, Duncan B McLaren, C L Paul Thomas, Andrea Soltoggio
- **Comment**: None
- **Journal**: None
- **Summary**: Volatile organic compounds (VOCs) in human breath can reveal a large spectrum of health conditions and can be used for fast, accurate and non-invasive diagnostics. Gas chromatography-mass spectrometry (GC-MS) is used to measure VOCs, but its application is limited by expert-driven data analysis that is time-consuming, subjective and may introduce errors. We propose a system to perform GC-MS data analysis that exploits deep learning pattern recognition ability to learn and automatically detect VOCs directly from raw data, thus bypassing expert-led processing. The new proposed approach showed to outperform the expert-led analysis by detecting a significantly higher number of VOCs in just a fraction of time while maintaining high specificity. These results suggest that the proposed method can help the large-scale deployment of breath-based diagnosis by reducing time and cost, and increasing accuracy and consistency.



### High-Resolution Image Inpainting with Iterative Confidence Feedback and Guided Upsampling
- **Arxiv ID**: http://arxiv.org/abs/2005.11742v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2005.11742v2)
- **Published**: 2020-05-24 13:23:45+00:00
- **Updated**: 2020-07-14 05:52:30+00:00
- **Authors**: Yu Zeng, Zhe Lin, Jimei Yang, Jianming Zhang, Eli Shechtman, Huchuan Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Existing image inpainting methods often produce artifacts when dealing with large holes in real applications. To address this challenge, we propose an iterative inpainting method with a feedback mechanism. Specifically, we introduce a deep generative model which not only outputs an inpainting result but also a corresponding confidence map. Using this map as feedback, it progressively fills the hole by trusting only high-confidence pixels inside the hole at each iteration and focuses on the remaining pixels in the next iteration. As it reuses partial predictions from the previous iterations as known pixels, this process gradually improves the result. In addition, we propose a guided upsampling network to enable generation of high-resolution inpainting results. We achieve this by extending the Contextual Attention module to borrow high-resolution feature patches in the input image. Furthermore, to mimic real object removal scenarios, we collect a large object mask dataset and synthesize more realistic training data that better simulates user inputs. Experiments show that our method significantly outperforms existing methods in both quantitative and qualitative evaluations. More results and Web APP are available at https://zengxianyu.github.io/iic.



### Domain Specific, Semi-Supervised Transfer Learning for Medical Imaging
- **Arxiv ID**: http://arxiv.org/abs/2005.11746v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.11746v1)
- **Published**: 2020-05-24 13:50:00+00:00
- **Updated**: 2020-05-24 13:50:00+00:00
- **Authors**: Jitender Singh Virk, Deepti R. Bathula
- **Comment**: 9 pages 4 figures
- **Journal**: None
- **Summary**: Limited availability of annotated medical imaging data poses a challenge for deep learning algorithms. Although transfer learning minimizes this hurdle in general, knowledge transfer across disparate domains is shown to be less effective. On the other hand, smaller architectures were found to be more compelling in learning better features. Consequently, we propose a lightweight architecture that uses mixed asymmetric kernels (MAKNet) to reduce the number of parameters significantly. Additionally, we train the proposed architecture using semi-supervised learning to provide pseudo-labels for a large medical dataset to assist with transfer learning. The proposed MAKNet provides better classification performance with $60 - 70\%$ less parameters than popular architectures. Experimental results also highlight the importance of domain-specific knowledge for effective transfer learning.



### Deep learning approach to describe and classify fungi microscopic images
- **Arxiv ID**: http://arxiv.org/abs/2005.11772v1
- **DOI**: 10.1371/journal.pone.0234806
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.11772v1)
- **Published**: 2020-05-24 15:15:07+00:00
- **Updated**: 2020-05-24 15:15:07+00:00
- **Authors**: Bartosz Zieliński, Agnieszka Sroka-Oleksiak, Dawid Rymarczyk, Adam Piekarczyk, Monika Brzychczy-Włoch
- **Comment**: None
- **Journal**: None
- **Summary**: Preliminary diagnosis of fungal infections can rely on microscopic examination. However, in many cases, it does not allow unambiguous identification of the species by microbiologist due to their visual similarity. Therefore, it is usually necessary to use additional biochemical tests. That involves additional costs and extends the identification process up to 10 days. Such a delay in the implementation of targeted therapy may be grave in consequence as the mortality rate for immunosuppressed patients is high. In this paper, we apply a machine learning approach based on deep neural networks and Fisher Vector (advanced bag-of-words method) to classify microscopic images of various fungi species. Our approach has the potential to make the last stage of biochemical identification redundant, shortening the identification process by 2-3 days, and reducing the cost of the diagnosis.



### Deep Convolutional Neural Network-based Bernoulli Heatmap for Head Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2005.11780v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2005.11780v1)
- **Published**: 2020-05-24 15:36:29+00:00
- **Updated**: 2020-05-24 15:36:29+00:00
- **Authors**: Zhongxu Hu, Yang Xing, Chen Lv, Peng Hang, Jie Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Head pose estimation is a crucial problem for many tasks, such as driver attention, fatigue detection, and human behaviour analysis. It is well known that neural networks are better at handling classification problems than regression problems. It is an extremely nonlinear process to let the network output the angle value directly for optimization learning, and the weight constraint of the loss function will be relatively weak. This paper proposes a novel Bernoulli heatmap for head pose estimation from a single RGB image. Our method can achieve the positioning of the head area while estimating the angles of the head. The Bernoulli heatmap makes it possible to construct fully convolutional neural networks without fully connected layers and provides a new idea for the output form of head pose estimation. A deep convolutional neural network (CNN) structure with multiscale representations is adopted to maintain high-resolution information and low-resolution information in parallel. This kind of structure can maintain rich, high-resolution representations. In addition, channelwise fusion is adopted to make the fusion weights learnable instead of simple addition with equal weights. As a result, the estimation is spatially more precise and potentially more accurate. The effectiveness of the proposed method is empirically demonstrated by comparing it with other state-of-the-art methods on public datasets.



### Vision-based control of a knuckle boom crane with online cable length estimation
- **Arxiv ID**: http://arxiv.org/abs/2005.11794v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2005.11794v2)
- **Published**: 2020-05-24 16:38:14+00:00
- **Updated**: 2020-05-26 07:26:30+00:00
- **Authors**: Geir Ole Tysse, Andrej Cibicik, Olav Egeland
- **Comment**: None
- **Journal**: None
- **Summary**: A vision-based controller for a knuckle boom crane is presented. The controller is used to control the motion of the crane tip and at the same time compensate for payload oscillations. The oscillations of the payload are measured with three cameras that are fixed to the crane king and are used to track two spherical markers fixed to the payload cable. Based on color and size information, each camera identifies the image points corresponding to the markers. The payload angles are then determined using linear triangulation of the image points. An extended Kalman filter is used for estimation of payload angles and angular velocity. The length of the payload cable is also estimated using a least squares technique with projection. The crane is controlled by a linear cascade controller where the inner control loop is designed to damp out the pendulum oscillation, and the crane tip is controlled by the outer loop. The control variable of the controller is the commanded crane tip acceleration, which is converted to a velocity command using a velocity loop. The performance of the control system is studied experimentally using a scaled laboratory version of a knuckle boom crane.



### Recognizing Families through Images with Pretrained Encoder
- **Arxiv ID**: http://arxiv.org/abs/2005.11811v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.11811v1)
- **Published**: 2020-05-24 17:59:19+00:00
- **Updated**: 2020-05-24 17:59:19+00:00
- **Authors**: Tuan-Duy H. Nguyen, Huu-Nghia H. Nguyen, Hieu Dao
- **Comment**: Will appear as part of RFIW2020 in the Proceedings of 2020
  International Conference on Automatic Face and Gesture Recognition (IEEE
  AMFG)
- **Journal**: None
- **Summary**: Kinship verification and kinship retrieval are emerging tasks in computer vision. Kinship verification aims at determining whether two facial images are from related people or not, while kinship retrieval is the task of retrieving possible related facial images to a person from a gallery of images. They introduce unique challenges because of the hidden relations and features that carry inherent characteristics between the facial images. We employ 3 methods, FaceNet, Siamese VGG-Face, and a combination of FaceNet and VGG-Face models as feature extractors, to achieve the 9th standing for kinship verification and the 5th standing for kinship retrieval in the Recognizing Family in The Wild 2020 competition. We then further experimented using StyleGAN2 as another encoder, with no improvement in the result.



### JPD-SE: High-Level Semantics for Joint Perception-Distortion Enhancement in Image Compression
- **Arxiv ID**: http://arxiv.org/abs/2005.12810v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.12810v3)
- **Published**: 2020-05-24 20:46:38+00:00
- **Updated**: 2022-08-09 05:53:12+00:00
- **Authors**: Shiyu Duan, Huaijin Chen, Jinwei Gu
- **Comment**: Accepted by IEEE Transactions on Image Processing
- **Journal**: None
- **Summary**: While humans can effortlessly transform complex visual scenes into simple words and the other way around by leveraging their high-level understanding of the content, conventional or the more recent learned image compression codecs do not seem to utilize the semantic meanings of visual content to their full potential. Moreover, they focus mostly on rate-distortion and tend to underperform in perception quality especially in low bitrate regime, and often disregard the performance of downstream computer vision algorithms, which is a fast-growing consumer group of compressed images in addition to human viewers. In this paper, we (1) present a generic framework that can enable any image codec to leverage high-level semantics and (2) study the joint optimization of perception quality and distortion. Our idea is that given any codec, we utilize high-level semantics to augment the low-level visual features extracted by it and produce essentially a new, semantic-aware codec. We propose a three-phase training scheme that teaches semantic-aware codecs to leverage the power of semantic to jointly optimize rate-perception-distortion (R-PD) performance. As an additional benefit, semantic-aware codecs also boost the performance of downstream computer vision algorithms. To validate our claim, we perform extensive empirical evaluations and provide both quantitative and qualitative results.



